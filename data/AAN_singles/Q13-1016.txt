Transactions of the Association for Computational Linguistics, 1 (2013) 193?206.
Action Editor: Jason Eisner.Submitted 10/2012; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Jointly Learning to Parse and Perceive: ConnectingNatural Language to the Physical WorldJayant KrishnamurthyComputer Science DepartmentCarnegie Mellon Universityjayantk@cs.cmu.eduThomas KollarComputer Science DepartmentCarnegie Mellon Universitytkollar@andrew.cmu.eduAbstractThis paper introduces Logical Semantics withPerception (LSP), a model for grounded lan-guage acquisition that learns to map natu-ral language statements to their referents ina physical environment.
For example, givenan image, LSP can map the statement ?bluemug on the table?
to the set of image seg-ments showing blue mugs on tables.
LSPlearns physical representations for both cate-gorical (?blue,?
?mug?)
and relational (?on?
)language, and also learns to compose theserepresentations to produce the referents of en-tire statements.
We further introduce a weaklysupervised training procedure that estimatesLSP?s parameters using annotated referentsfor entire statements, without annotated ref-erents for individual words or the parse struc-ture of the statement.
We perform experimentson two applications: scene understanding andgeographical question answering.
We findthat LSP outperforms existing, less expressivemodels that cannot represent relational lan-guage.
We further find that weakly supervisedtraining is competitive with fully supervisedtraining while requiring significantly less an-notation effort.1 IntroductionLearning the mapping from natural language tophysical environments is a central problem for natu-ral language semantics.
Understanding this mappingis necessary to enable natural language interactionswith robots and other embodied systems.
For exam-ple, for an autonomous robot to understand the sen-tence ?The blue mug is on the table,?
it must be ableto identify (1) the objects in its environment corre-sponding to ?blue mug?
and ?table,?
and (2) the ob-jects which participate in the spatial relation denotedby ?on.?
If the robot can successfully identify theseobjects, it understands the meaning of the sentence.The problem of learning to map from natural lan-guage expressions to their referents in an environ-ment is known as grounded language acquisition.In embodied settings, environments consist of rawsensor data ?
for example, an environment could bean image collected from a robot?s camera.
In suchapplications, grounded language acquisition has twosubproblems: parsing, learning the compositionalstructure of natural language; and perception, learn-ing the environmental referents of individual words.Acquiring both kinds of knowledge is necessary tounderstand novel language in novel environments.Unfortunately, perception is often ignored in workon language acquisition.
Other variants of groundedlanguage acquisition eliminate the need for percep-tion by assuming access to a logical representationof the environment (Zettlemoyer and Collins, 2005;Wong and Mooney, 2006; Matuszek et al 2010;Chen and Mooney, 2011; Liang et al 2011).
Theexisting work which has jointly addressed both pars-ing and perception has significant drawbacks, in-cluding: (1) fully supervised models requiring largeamounts of manual annotation and (2) limited se-mantic representations (Kollar et al 2010; Tellex etal., 2011; Matuszek et al 2012).This paper introduces Logical Semantics withPerception (LSP), a model for grounded languageacquisition that jointly learns to semantically parselanguage and perceive the world.
LSP models amapping from natural language queries to sets of ob-jects in a real-world environment.
The input to LSPis an environment containing objects, such as a seg-193(a) An environment containing 4objects (image segments).Environment:(image on left)Knowledge BaseQuery:?things to the rightof the blue mug?Semantic ParseGrounding: {(2, 1), (3, 1)}Denotation: {2, 3}(b) LSP predicting the environmental referents ofa natural language query.Language DenotationThe mugs {1, 3}The objects on the table {1, 2, 3}There is an LCD monitor {2}Is the blue mug right {}of the monitor?The monitor is behind {2}the blue cup.
(c) Training examples for weakly su-pervised training.Figure 1: LSP applied to scene understanding.
Given an environment containing a set of objects (left), and anatural language query, LSP produces a semantic parse, logical knowledge base, grounding and denotation(middle), using only language/denotation pairs (right) for training.mented image (Figure 1a), and a natural languagequery, such as ?the things to the right of the bluemug.?
Given these inputs, LSP produces (1) a logi-cal knowledge base describing objects and relation-ships in the environment and (2) a semantic parse ofthe query capturing its compositional structure.
LSPcombines these two outputs to produce the query?sgrounding, which is the set of object referents of thequery?s noun phrases, and its denotation, which isthe query?s answer (Figure 1b).1 Weakly supervisedtraining estimates parameters for LSP using queriesannotated with their denotations in an environment(Figure 1c).This work has two contributions.
The first con-tribution is LSP, which is more expressive than pre-vious models, representing both one-argument cat-egories and two-argument relations over sets of ob-jects in the environment.
The second contributionis a weakly supervised training procedure that esti-mates LSP?s parameters without annotated semanticparses, noun phrase/object mappings, or manually-constructed knowledge bases.We perform experiments on two different applica-tions.
The first application is scene understanding,where LSP grounds descriptions of images in im-age segments.
The second application is geograph-ical question answering, where LSP learns to an-swer questions about locations, represented as poly-gons on a map.
In geographical question answering,1We treat declarative sentences as if they were queries abouttheir subject, e.g., the denotation of ?the mug is on the table?
isthe set of mugs on tables.
Typically, the denotation of a sentenceis either true or false; our treatment is strictly more general, asa sentence?s denotation is nonempty if and only if the sentenceis true.LSP correctly answers 34% more questions than themost comparable state-of-the-art model (Matuszeket al 2012).
In scene understanding, accuracy sim-ilarly improves by 16%.
Furthermore, weakly su-pervised training achieves an accuracy within 6% ofthat achieved by fully supervised training, while re-quiring significantly less annotation effort.2 Prior WorkLogical Semantics with Perception (LSP) is relatedto work from planning, natural language processing,computer vision and robotics.
Much of the relatedwork focuses on interpreting natural language us-ing a fixed formal representation.
Some work con-structs integrated systems which execute plans in re-sponse to natural language commands (Winograd,1970; Hsiao et al 2003; Roy et al 2003; Skubicet al 2004; MacMahon et al 2006; Levit and Roy,2007; Kruijff et al 2007).
These systems parsenatural language to a formal representation whichcan be executed using a set of fixed control pro-grams.
Similarly, work on semantic parsing learnsto map natural language to a given formal repre-sentation.
Semantic parsers can be trained usingsentences annotated with their formal representation(Zelle and Mooney, 1996; Zettlemoyer and Collins,2005; Kate and Mooney, 2006; Kwiatkowski et al2010) or various less restrictive annotations (Clarkeet al 2010; Liang et al 2011; Krishnamurthy andMitchell, 2012).
Finally, work on grounded lan-guage acquisition leverages semantic parsing to mapfrom natural language to a formal representation ofan environment (Kate and Mooney, 2007; Chen andMooney, 2008; Shimizu and Haas, 2009; Matuszek194Environment d Know.
Base ?mug(1)mug(3)blue(1)table(4)on-rel(1, 4)on-rel(3, 4)...(a) Perception fper produces a logical knowl-edge base ?
from the environment d using anindependent classifier for each category andrelation.Language z?blue mug on table?Logical form `?x.
?y.blue(x) ?mug(x) ?on-rel(x, y) ?table(y)(b) Semantic parsing fprsmaps language z to a log-ical form `.Grounding: g = {(1, 4)}, Denotation: ?
= {1}{1}{1}blue(x){1, 3}mug(x){(1, 4), (3, 4)}{(1, 4), (3, 4)}on-rel(x, y){4}table(y)(c) Evaluation feval evaluates a logical form ` on alogical knowledge base ?
to produce a grounding gand denotation ?.Figure 2: Overview of Logical Semantics with Perception (LSP).et al 2010; Dzifcak et al 2009; Cantrell et al2010; Chen and Mooney, 2011).
All of this work as-sumes that the formal environment representation isgiven, while LSP learns to produce this formal rep-resentation from raw sensor input.Most similar to LSP is work on simultaneouslyunderstanding natural language and perceiving theenvironment.
This problem has been addressed inthe context of robot direction following (Kollar etal., 2010; Tellex et al 2011) and visual attributelearning (Matuszek et al 2012).
However, thiswork is less semantically expressive than LSP andtrained using more supervision.
TheG3 model (Kol-lar et al 2010; Tellex et al 2011) assumes a one-to-one mapping from noun phrases to entities andis trained using full supervision, while LSP allowsone-to-many mappings from noun phrases to entitiesand can be trained using minimal annotation.
Ma-tuszek et al(2012) learns only one-argument cate-gories (?attributes?)
and requires a fully supervisedinitial training phase.
In contrast, LSP models two-argument relations and allows for weakly supervisedsupervised training throughout.3 Logical Semantics with PerceptionLogical Semantics with Perception (LSP) is a modelfor grounded language acquisition.
LSP accepts asinput a natural language statement and an environ-ment and outputs the objects in the environment de-noted by the statement.
The LSP model has threecomponents: perception, parsing and evaluation (seeFigure 2).
The perception component constructslogical knowledge bases from low-level feature-based representations of environments.
The pars-ing component semantically parses natural languageinto lambda calculus queries against the constructedknowledge base.
Finally, the evaluation compo-nent deterministically executes this query against theknowledge base to produce LSP?s output.The output of LSP can be either a denotation ora grounding.
A denotation is the set of entity refer-ents for the phrase as a whole, while a grounding isthe set of entity referents for each component of thephrase.
The distinction between these two outputs isshown in Figure 1b.
In this example, the denotationis the set of ?things to the right of the blue mug,?which does not include the blue mug itself.
On theother hand, the grounding includes both the refer-ents of ?things?
and ?blue mug.?
Only denotationsare used during training, so we ignore groundings inthe following model description.
However, ground-ings are used in our evaluation, as they are a morecomplete description of the model?s understanding.Formally, LSP is a linear model f that predicts adenotation ?
given a natural language statement z inan environment d. As shown in Figure 3, the struc-ture of LSP factors into perception (fper), semanticparsing (fprs) and evaluation (feval) components us-ing several latent variables:f(?,?, `, t, z,d; ?)
= fper(?, d; ?per)+fprs(`, t, z; ?prs) + feval(?,?, `)LSP assumes access to a set of predicates thattake either one argument, called categories (c ?
C)or two arguments, called relations (r ?
R).2 Thesepredicates are the interface between LSP?s percep-tion and parsing components.
The perception func-tion fper takes an environment d and produces a log-2The set of predicates are derived from our training data.See Section 5.3.195?feval?d fper?
zfprstFigure 3: Factor graph of LSP.
The environment dand language z are given as input, from which themodel predicts a logical knowledge base ?, logicalform `, syntactic tree t and denotation ?.ical knowledge base ?
that assigns truth values toinstances of these predicates using parameters ?per.This function uses an independent classifier to pre-dict the instances of each predicate.
The seman-tic parser fprs takes a natural language statement zand produces a logical form ` and syntactic parset using parameters ?prs.
The logical form ` is adatabase query expressed in lambda calculus nota-tion, constructed by logically combining the givenpredicates.
Finally, the evaluation function feval de-terministically evaluates the logical form ` on theknowledge base ?
to produce a denotation ?.
Thesecomponents are illustrated in Figure 2.The following sections describe the percep-tion function (Section 3.1), semantic parser (Sec-tion 3.2), evaluation function (Section 3.3), and in-ference (Section 3.4) in more detail.3.1 Perception FunctionThe perception function fper constructs a logicalknowledge base ?
given an environment d. The per-ception function assumes that an environment con-tains a collection of entities e ?
Ed.
The knowl-edge base produced by perception is a collection ofground predicate instances using these entities.
Forexample, in Figure 2a, the entire image is the envi-ronment, and each image segment is an entity.
Thelogical knowledge base ?
contains the shown pred-icate instances, where the categories include blue,mug and table, and the relations include on-rel.The perception function scores logical knowledgebases using a set of per-predicate binary classifiers.These classifiers independently assign a score towhether each entity (entity pair) is an element ofeach category (relation).
Let ?c ?
?
denote the setof entities which are elements of category c; simi-larly, let ?r ?
?
denote the set of entity pairs whichare elements of the relation r. Given these sets, thescore of a logical knowledge base ?
factors into per-relation and per-category scores h:fper(?, d; ?per) =?c?Ch(?c, d; ?cper)+?r?Rh(?r, d; ?rper)The per-predicate scores are in turn given by asum of per-element classification scores:h(?c, d; ?cper) =?e?Ed?c(e)(?cper)T?cat(e)h(?r, d; ?rper) =?
(e1,e2)?Ed?r(e1, e2)(?rper)T?rel(e1, e2)Each term in the above sums represents a singlebinary classification, determining the score for a sin-gle entity (entity pair) belonging to a particular cat-egory (relation).
We treat ?c and ?r as indicatorfunctions for the sets they denote, i.e., ?c(e) = 1for entities e in the set, and 0 otherwise.
Similarly,?r(e1, e2) = 1 for entity pairs (e1, e2) in the set,and 0 otherwise.
The features of these classifiers aregiven by ?cat and ?rel, which are feature functionsthat map entities and entity pairs to feature vectors.The parameters of these classifiers are given by ?cperand ?rper.
The perception parameters ?per containone such set of parameters for every category and re-lation, i.e., ?per = {?cper : c ?
C} ?
{?rper : r ?
R}.3.2 Semantic ParserThe goal of semantic parsing is to identify whichportions of the input natural language denote enti-ties and relationships between entities in the envi-ronment.
Semantic parsing accomplishes this goalby mapping from natural language to a logical formthat explicitly describes the language?s entity refer-ents using one- and two-argument predicates.
Thelogical form is combined with instances of thesepredicates to produce the statement?s denotation.LSP?s semantic parser is defined using Combina-tory Categorial Grammar (CCG) (Steedman, 1996).The grammar of the parser is given by a lexicon ?which maps words to syntactic categories and logi-cal forms.
For example, ?mug?
may have the syn-tactic category N for noun, and the logical form?x.mug(x), denoting the set of all entities x suchthat mug is true.
During parsing, the logical formsfor adjacent phrases are combined to produce thelogical form for the complete statement.196theN/N?f.fmugsN?x.mug(x)N : ?x.mug(x)are(S\N)/N?f.?g.
?x.g(x) ?
f(x)rightN/PP?f.?x.
?y.right-rel(x, y) ?
f(y)ofPP/N?f.ftheN/N?f.fmonitorN?x.monitor(x)N : ?x.monitor(x)PP : ?x.monitor(x)N : ?x.
?y.right-rel(x, y) ?
monitor(y)S\N : ?g.?x.
?y.g(x) ?
right-rel(x, y) ?
monitor(y)S : ?x.
?y.mug(x) ?
right-rel(x, y) ?
monitor(y)Figure 4: Example parse of ?the mugs are right of the monitor.?
The first row of the derivation retrieveslexical categories from the lexicon, while the remaining rows represent applications of CCG combinators.Figure 4 illustrates how CCG parsing produces asyntactic tree t and a logical form `.
The top rowof the parse represents retrieving a lexicon entry foreach word.
Each successive row combines a pair ofentries by applying a logical form to an adjacent ar-gument.
A given sentence may have multiple parseslike the one shown, using a different set of lexiconentries or a different order of function applications.The semantic parser scores each such parse, learningto distinguish correct and incorrect parses.The semantic parser in LSP is a linear model overCCG parses (`, t) given language z:fprs(`, t, z; ?prs) = ?Tprs?prs(`, t, z)Here, ?prs(`, t, z) represents a feature function map-ping CCG parses to vectors of feature values.
?prsfactorizes according to the tree structure of the CCGparse; it contains features for local parsing opera-tions which are summed to produce the feature val-ues for a tree.
If the parse tree is a terminal, then:?prs(`, t, z) = 1(lexicon entry)The notation 1(x) denotes a vector with a single oneentry whose position is determined by x.
The termi-nal features are indicator features for each lexiconentry, as shown in the top row of Figure 4.
Thesefeatures allow the model to learn the correct syntac-tic and semantic function of each word.
If the parsetree is a nonterminal, then:?prs(`, t, z) = ?prs(left(`, t, z))+ ?prs(right(`, t, z)) + 1(combinator)These nonterminal features are defined over combi-nator rules in the parse tree, as in the remaining rowsof Figure 4.
These features allow the model to learnwhich adjacent parse trees are likely to combine.
Werefer the reader to Zettlemoyer and Collins (2005)for more information about CCG semantic parsing.3.3 Evaluation FunctionThe evaluation function feval deterministicallyscores denotations given a logical form ` and alogical knowledge base ?.
Intuitively, the evalu-ation function simply evaluates the query ` on thedatabase ?
to produce a denotation.
The evaluationfunction then assigns score 0 to this denotation, andscore ??
to all other denotations.We describe feval by giving a recurrence for com-puting the denotation ?
of a logical form ` on a log-ical knowledge base ?.
This evaluation takes theform of a tree, as in Figure 2c.
The base cases are:?
If ` = ?x.c(x) then ?
= ?c.?
If ` = ?x.
?y.r(x, y), then ?
= ?r.The denotations for more complex logical formsare computed recursively by decomposing ` accord-ing to its logical structure.
Our logical forms con-tain only conjunctions and existential quantifiers;the corresponding recursive computations are:?
If ` = ?x.`1(x) ?
`2(x), then?
(e) = 1 iff ?1(e) = 1 ?
?2(e) = 1.?
If ` = ?x.
?y.`1(x, y), then?
(e1) = 1 iff ?e2.
?1(e1, e2) = 1.Note that a similar recurrence can be used to com-pute groundings: simply retain the satisfying assign-ments to existentially-quantified variables.3.4 InferenceThe basic inference problem in LSP is to predict adenotation ?
given language z and an environmentd.
This inference problem is straightforward dueto the deterministic structure of feval.
The highest-scoring ?
can be found by independently maximiz-ing fprs and fper to find the highest-scoring logicalform ` and logical knowledge base ?.
Deterministi-cally evaluating the recurrence for feval using thesevalues yields the highest-scoring denotation.197Another inference problem occurs during train-ing: identify the highest-scoring logical form andknowledge base which produce a particular denota-tion.
Our approximate inference algorithm for thisproblem is described in Section 4.2.4 Weakly Supervised ParameterEstimationThis section describes a weakly supervised trainingprocedure for LSP, which estimates parameters us-ing a corpus of sentences with annotated denota-tions.
The algorithm jointly trains both the pars-ing and the perception components of LSP to bestpredict the denotations of the observed training sen-tences.
Our approach trains LSP as a maximummargin Markov network using the stochastic subgra-dient method.
The main difficulty is computing thesubgradient, which requires computing values forthe model?s hidden variables, i.e., the logical knowl-edge base ?
and semantic parse ` that are responsi-ble for the model?s prediction.4.1 Stochastic Subgradient MethodThe training procedure trains LSP as a maximummargin Markov network (Taskar et al 2004), astructured analog of a support vector machine.
Thetraining data for our weakly supervised algorithm isa collection {(zi, ?i, di)}ni=1, consisting of languagezi paired with its denotation ?i in environment di.Given this data, the parameters ?
= [?prs, ?per]are estimated by minimizing the following objectivefunction:O(?)
= ?2 ||?||2 + 1n[ n?i=1?i](1)where ?
is a regularization parameter that controlsthe trade-off between model complexity and slackpenalties.
The slack variable ?i represents a marginviolation penalty for the ith training example, de-fined as:?i = max?,?,`,t[f(?,?, `, t, zi, di; ?)
+ cost(?, ?i)]?max?,`,tf(?i,?, `, t, zi, di; ?
)The above expression is the structured counterpartof the hinge loss, where cost(?, ?i) is the marginby which ?i?s score must exceed ?
?s score.
We letcost(?, ?i) be the Hamming cost; it adds a cost of 1for each entity e such that ?i(e) 6= ?
(e).We optimize this objective using the stochasticsubgradient method (Ratliff et al 2006).
To com-pute the subgradient gi, first compute the highest-scoring assignments to the model?s hidden variables:?
?, ?
?, ?`, t??
arg max?,?,`,tf(?,?, `, t, zi, di; ?j)+ cost(?, ?i) (2)?
?, `?, t?
?
arg max?,`,tf(?i,?, `, t, zi, di; ?j) (3)The first set of values (e.g., ?`) are the best ex-planation for the denotation ??
which most violatesthe margin constraint.
The second set of values(e.g., `?)
are the best explanation for the true de-notation ?i.
The subgradient update increases theweights of features that explain the true denotation,while decreasing the weights of features that explainthe denotation violating the margin.
The subgradi-ent factors into parsing and perception components:gi = [giprs, giper].
The parsing subgradient is:giprs = ?prs(?`, t?, zi)?
?prs(`?, t?, zi)The subgradient of the perception parameters ?perfactors into subgradients of the category and relationclassifier parameters.
Recall that ?per = {?cper : c ?C}?
{?rper : r ?
R}.
Let ?
?c ?
??
be the best margin-violating set of entities for c, and ?c?
?
??
be thebest truth-explaining set of entities.
Similarly define?
?r and ?r?.
The subgradients of the category andrelation classifier parameters are:gi,cper =?e?Edi(??c(e)?
?c?
(e))?cat(e)gi,rper =?(e1,e2)?Edi(?
?r(e1, e2)?
?r?
(e1, e2))?rel(e1, e2)4.2 Inference: Computing the SubgradientSolving the maximizations in Equations 2 and 3 ischallenging because the weights placed on the de-notation ?
couple fprs and fper.
Due to this cou-pling, exactly solving these problems requires (1)enumerating all possible logical forms `, and (2) foreach logical form, finding the highest-scoring logi-cal knowledge base ?
by propagating the weights on?
back through feval.We use a two-step approximate inference algo-rithm for both maximizations.
The first step per-forms a beam search over CCG parses, producing198k possible logical forms `1, ..., `k. The second stepuses an integer linear program (ILP) to find the bestlogical knowledge base ?
given each parse `i. Inour experiments, we parse with a beam size of 1000,then solve an ILP for each of the 10 highest-scoringlogical forms.
The highest-scoring parse/logicalknowledge base pair is the approximate maximizer.Given a logical form ` output by beam search, thesecond step of inference computes the best valuesfor the logical knowledge base ?
and denotation ?
:max?,?fper(?, d; ?per) + feval(?, `,?)
+ ?(?)
(4)Here, ?(?)
= ?e?Ed ?e?
(e) represents a set ofweights on the entities in the predicted denotation ?.For Equation 2, ?
represents cost(?, ?i).
For Equa-tion 3, ?
is a hard constraint encoding ?
= ?i (i.e.,?(?)
= ??
when ?
6= ?i and 0 otherwise).We encode the maximization in Equation 4 as anILP.
For each category c and relation r, we create bi-nary variables ?c(e1) and ?r(e1, e2) for each entityin the environment, e1, e2 ?
Ed.
We similarly createbinary variables ?
(e) for the denotation ?.
Using thefact that fper is a linear function of these variables,we write the ILP objective as:fper(?, d; ?per) + ?(?)
=?e1?Ed?c?Cwce1?c(e1)+?e1,e2?Ed?r?Rwre1,e2?r(e1, e2) +?e1?Ed?e1?
(e1)where the weights wce1 and wre1,e2 determine howlikely it is that each entity or entity pair belongs tothe predicates c and r:wce1 = (?cper)T?cat(e1)wre1,e2 = (?rper)T?rel(e1, e2)The ILP also includes constraints and additionalauxiliary variables that represent feval.
These con-straints couple the denotation ?
and the logicalknowledge base ?
such that ?
is the result of evaluat-ing ` on ?.
` is recursively decomposed as in Section3.3, and each intermediate set of entities in the recur-rence is given its own set of |Ed| (or |Ed|2) variables.These variables are then logically constrained to en-force `?s structure.5 EvaluationOur evaluation performs three major comparisons.First, we examine the performance impact of weaklysupervised training by comparing weakly and fullysupervised variants of LSP.
Second, we examine theperformance impact of modelling relations by com-paring against a category-only baseline, which is anablated version of LSP similar to the model of Ma-tuszek et al(2012).
Finally, we examine the causesof errors by performing an error analysis of LSP?ssemantic parser and perceptual classifiers.Before describing our results, we first describesome necessary set-up for the experiments.
Thesesections describe the data sets, features, construc-tion of the CCG lexicon, and details of the models.Our data sets and additional evaluation resources areavailable online from http://rtw.ml.cmu.edu/tacl2013_lsp/.5.1 Data SetsWe evaluate LSP on two applications: scene un-derstanding (SCENE) and geographical question an-swering (GEOQA).
These data sets are collections{(zi, ?i, di, `i,?i)}ni=1, consisting of a number ofnatural language statements zi with annotated deno-tations ?i in environments di.
For fully supervisedtraining, each statement is annotated with a goldstandard logical form `i, and each environment witha gold standard logical knowledge base ?i.
Statisticsof these data sets are shown in Table 1, and exampleenvironments and statements are shown in Figure 5.The SCENE data set consists of segmented imagesof indoor environments containing a number of or-dinary objects such as mugs and monitors.
Eachimage is an environment, and each image segment(bounding box) is an entity.
We collected naturallanguage descriptions of each scene from membersof our lab and Amazon Mechanical Turk, askingsubjects to describe the objects in each scene.
Theauthors then manually annotated the collected state-ments with their denotations and logical forms.
Inthis data set, each image contains the same set of ob-jects; note that this does not trivialize the task, as themodel only observes visual features of the objects,which are not consistent across environments.The GEOQA data set consists of several mapscontaining entities such as cities, states, nationalparks, lakes and oceans.
Each map is an envi-ronment, and its component entities are given bypolygons of latitude/longitude coordinates marking199Data Set Statistics SCENE GEOQA# of environments 15 10Mean entities / environment d 4.1 6.9Mean # of entities in denotation ?
1.5 1.2# of statements 284 263Mean words / statement 6.3 6.3Mean predicates / log.
form 2.6 2.8# of preds.
in annotated worlds 46 38Lexicon Statistics# of words in lexicon 169 288# of lexicon entries 712 876Mean parses / statement 15.0 8.9Table 1: Statistics of the two data sets used in ourevaluation, and of the generated lexicons.their boundaries.3 Furthermore, each entity has oneknown name (e.g., ?Greenville?).
In this data set,distinct entities occur on average in 1.25 environ-ments; repeated entities are mostly large locations,such as states and oceans.
The language for thisdata set was contributed by members of our researchgroup, who were instructed to provide a mixture ofsimple and complex geographical questions.
Theauthors then manually annotated each question witha denotation (its answer) and a logical form.5.2 FeaturesThe features of both applications are intended tocapture properties of entities and relations betweenthem.
As such, both applications share a set of phys-ical features which are functions of the boundingpolygons of each entity.
Example category features(?cat) are the area and perimeter of the entity, andan example relation feature (?rel) is the distance be-tween entity centroids.The SCENE data set additionally includes visualappearance features in ?cat to capture visual proper-ties of objects.
These features include a Histogramof Oriented Gradients (HOG) (Dalal and Triggs,2005) and an RGB color histogram.The GEOQA data set additionally includes dis-tributional semantic features to distinguish betweendifferent types of entities (e.g., states vs. lakes)and to capture non-spatial relations (e.g., capitalsof states).
These features are derived from phraseco-occurrences with entity names in the Clueweb093Polygons were extracted from OpenStreetMap, http://www.openstreetmap.org/.web corpus.4 The category features ?cat include in-dicators for the 20 contexts which most frequentlyoccur with an entity?s name (e.g., ?X is a city?
).Similarly, the relation features ?rel include indica-tors for the 20 most frequent contexts between twoentities?
names (e.g., ?X in eastern Y ?
).5.3 Lexicon InductionOne of the inputs to the semantic parser (Section 3.2)is a lexicon that lists possible syntactic and seman-tic functions for each word.
Together, these per-word entries define the set of possible logical formsfor every statement.
Each word may have mul-tiple lexicon entries to capture uncertainty aboutits meaning.
For example, the word ?right?
mayhave entries N : ?x.right(x) and N/PP :?f.?x.
?y.right-rel(x, y)?
f(y).
The semanticparser learns to distinguish among these interpreta-tions to produce good logical forms.We automatically generated lexicons for bothapplications using part-of-speech tag heuristics.5These heuristics map words to lexicon entries con-taining category and relation predicates derivedfrom the word?s lemma.
Nouns and adjectives pro-duce lexicon entries containing either categories orrelations (as shown above for ?right?).
Mappingthese parts-of-speech to relations is necessary forphrases like ?to the right of,?
where the noun ?right?denotes a relation.
Verbs and prepositions pro-duce lexicon entries containing relations.
Additionalheuristics generate semantically-empty lexicon en-tries, allowing words like determiners to have nophysical interpretation.
Finally, there are specialheuristics for forms of ?to be?
and, in GEOQA, tohandle known entity names.
The complete set oflexicon induction heuristics is available online.The automatically generated lexicon makes it dif-ficult to compare semantic parses across models,since the correctness of a semantic parse depends onthe learned perceptual classifiers.
To facilitate sucha comparison (Section 5.6), we filtered out lexiconentries containing predicates which were not used inany of the annotated logical forms.
Statistics of thefiltered lexicons are shown in Table 1.4http://www.lemurproject.org/clueweb09/5We used the Stanford POS tagger (Toutanova et al 2003).200Environment d Language z and predicted logical form ` Predicted grounding True groundingmonitor to the left of the mugs {(2,1), (2,3)} {(2,1), (2,3)}?x.
?y.monitor(x) ?
left-rel(x, y) ?
mug(y)mug to the left of the other mug {(3,1)} {(3,1)}?x.
?y.mug(x) ?
left-rel(x, y) ?
mug(y)objects on the table {(1,4), (2,4) {(1,4), (2,4),?x.
?y.object(x) ?
on-rel(x, y) ?
table(y) (3,4)} (3,4)}two blue cups are placed near to the computer screen {(1)} {(1,2), (3,2)}?x.blue(x) ?
cup(x) ?
comp.
(x) ?
screen(x)What cities are in North Carolina?
{(CH,NC), (GB,NC) {(CH,NC), (GB,NC)?x.
?y.city(x) ?
in-rel(x, y) ?
y = NC (RA,NC)} (RA,NC)}What city is east of Greensboro in North Carolina?
{(RA,GB,NC), {(RA,GB,NC)}?x.
?y, z.city(x) ?
east-rel(x, y) (MB,GB,NC)}?
y = GB ?
in-rel(y, z) ?
z = NCWhat cities are on the ocean?
{(CH,AO), (GB,AO), {(MB,AO)}?x.
?y.city(x) ?
on-rel(x, y) ?
ocean(y) (MB,AO), (RA,AO)}Figure 5: Example environments, statements, and model predictions from the SCENE and GEOQA data sets.5.4 Models and TrainingThe evaluation compares three models.
The firstmodel is LSP-W, which is LSP trained using theweakly supervised algorithm described in Section 4.The second model, LSP-CAT, replicates the modelof Matuszek et al(2012) by restricting LSP touse category predicates.
LSP-CAT is constructed byremoving all relation predicates in lexicon entries,mapping entries like ?f.?g.?x.
?y.r(x, y) ?
g(x) ?f(y) to ?f.?g.?x.
?y.g(x) ?
f(y).
This model isalso trained using our weakly supervised algorithm.The third model, LSP-F, is LSP trained with fullsupervision, using the manually annotated semanticparses and logical knowledge bases in our data sets.Given these annotations, training LSP amounts toindependently training a semantic parser (using sen-tences with annotated logical forms, {(zi, `i)}) anda set of perceptual classifiers (using environmentswith annotated logical knowledge bases, {(di,?i)}).This model measures the performance achievablewith LSP given significantly more supervision.All three variants of LSP were trained using thesame hyperparameters.
For SCENE, we computedsubgradients in 5 example minibatches and per-formed 100 passes over the data using ?
= 0.03.
ForGEOQA, we computed subgradients in 8 exampleminibatches, again performing 100 passes over thedata using ?
= 0.02.
We tried varying the regular-ization parameter, but found that performance wasrelatively stable under ?
?
0.05.
All experimentsuse leave-one-environment-out cross-validation toestimate model performance.
We hold out each en-vironment in turn, train each model on the remainingenvironments, then test on the held-out environment.5.5 ResultsWe consider two prediction problems in the eval-uation.
The first problem is to predict the correctdenotation ?i for a statement zi in an environmentdi.
A correct prediction on this task correspondsto a correctly answered question.
A weakness ofthis task is that it is possible to guess the right de-notation without fully understanding the language.For example, given a query like ?mugs on the ta-ble,?
it might be possible to guess the denotationbased solely on ?mugs,?
ignoring ?table?
altogether.The grounding prediction task corrects for this prob-lem.
Here, each model predicts a grounding, whichis the set of all satisfying assignments to the vari-ables in a logical form.
For example, for the log-ical form ?x.
?y.left-rel(x, y) ?
mug(y), thegrounding is the set of (x, y) tuples for which bothleft-rel(x, y) and mug(y) return true.
Notethat, if the predicted semantic parse is incorrect, thepredicted grounding for a statement may contain adifferent number of variables than the true ground-ing; such groundings are incorrect.
Figure 5 showsmodel predictions for the grounding task.Performance on both tasks is measured using ex-act match accuracy.
This metric is the fraction ofexamples for which the predicted set of entities (beit the denotation or grounding) exactly equals theannotated set.
This is a challenging metric, as the201Denotation ?
0 rel.
1 rel.
other totalLSP-CAT 0.94 0.45 0.20 0.51LSP-F 0.89 0.81 0.20 0.70LSP-W 0.89 0.77 0.16 0.67Grounding g 0 rel.
1 rel.
other totalLSP-CAT 0.94 0.37 0.00 0.42LSP-F 0.89 0.80 0.00 0.65LSP-W 0.89 0.70 0.00 0.59% of data 23 56 21 100(a) Results on the SCENE data set.Denotation ?
0 rel.
1 rel.
other totalLSP-CAT 0.22 0.19 0.07 0.17LSP-F 0.64 0.53 0.21 0.48LSP-W 0.64 0.58 0.21 0.51Grounding g 0 rel.
1 rel.
other totalLSP-CAT 0.22 0.19 0.00 0.16LSP-F 0.64 0.53 0.17 0.47LSP-W 0.64 0.58 0.15 0.50% of data 8 72 20 100(b) Results on the GEOQA data set.Table 2: Model performance on the SCENE and GEOQA datasets.
LSP-CAT is an ablated version of LSPthat only learns categories (similar to Matuszek et al(2012)), LSP-F is LSP trained with annotated seman-tic parses and logical knowledge bases, and LSP-W is LSP trained on sentences with annotated denotations.Results are separated by the number of relations in each test natural language statement.number of possible sets grows exponentially in thenumber of entities in the environment.
Say an en-vironment has 5 entities and a logical form has twovariables; then there are 25 possible denotations and225 possible groundings.
To quantify this difficulty,note that selecting a denotation uniformly at randomachieves 6% accuracy on SCENE, and 1% accuracyon GEOQA; selecting a random grounding achieves1% and 0% accuracy, respectively.Table 2 shows results for both applications us-ing exact match accuracy.
To better understand theperformance of each model, we break down perfor-mance according to linguistic complexity.
We com-pute the number of relations in the annotated logicalform for each statement, and show separate resultsfor 0 and 1 relations.
We also include an ?other?category to capture sentences with more than onerelation (very infrequent), or that include quanti-fiers, comparatives, or other linguistic phenomenanot captured by LSP.The results from these experiments suggest threeconclusions.
First, we find that modelling relationsis important for both applications, as (1) the major-ity of examples contain relational language, and (2)LSP-W and LSP-F dramatically outperform LSP-CAT on these examples.
The low performance ofLSP-CAT suggests that many denotations cannotbe predicted from only the first noun phrase in astatement, demonstrating that both applications re-quire an understanding of relations.
Second, we findthat weakly supervised training and fully supervisedtraining perform similarly, with accuracy differencesin the range of 3%-6%.
Finally, we find that LSP-Wperforms similarly on both the denotation and com-plete grounding tasks; this result suggests that whenLSP-W predicts a correct denotation, it does so be-cause it has identified the correct entity referents ofeach portion of the statement.5.6 Component Error AnalysisWe performed an error analysis of each model com-ponent to better understand the causes of errors.
Ta-ble 3 shows the accuracy of the semantic parser fromeach trained model.
Each held-out sentence zi isparsed to produce a logical form `, which is markedcorrect if it exactly matches our manual annotation`i.
A correct logical form implies a correct ground-ing for the statement when the parse is evaluated inthe gold standard logical knowledge base.
These re-sults show that both LSP-W and LSP-F have rea-sonably accurate semantic parsers, given the restric-tions on possible logical forms.
Common mistakesinclude missing lexicon entries (e.g., ?borders?
isPOS-tagged as a noun, so the GEOQA lexicon doesnot include a verb for it) and prepositional phraseattachments (e.g., 6th example in Figure 5).Table 4 shows the precision and recall of the in-dividual perceptual classifiers.
We computed thesemetrics by comparing each annotated predicate inthe held-out environment with the model?s predic-tions for the same predicate, treating each entity (orentity pair) as an independent example for classifi-202SCENE GEOQALSP-CAT 0.21 0.17LSP-W 0.72 0.71LSP-F 0.73 0.75Upper Bound 0.79 0.87Table 3: Accuracy of the semantic parser from eachtrained model.
Upper bound is the highest accu-racy achievable without modelling comparatives andother linguistic phenomena not captured by LSP.cation.
Fully supervised training appears to producebetter perceptual classifiers than weakly supervisedtraining; however, this result conflicts with the fullsystem evaluation in Table 2, where both systemsperform equally well.
There are two causes for thisphenomenon: uninformative adjectives and unim-portant relation instances.Uninformative adjective predicates are responsi-ble for the low performance of the category classi-fiers in SCENE.
Phrases like ?LCD screen?
in thisdomain are annotated with logical forms such as?x.lcd(x) ?
screen(x).
Here, lcd is uninfor-mative, since screen already denotes a unique ob-ject in the environment.
Therefore, it is not impor-tant to learn an accurate classifier for lcd.
Weaklysupervised training learns that lcd is meaningless,yet predicts the correct denotation for ?x.lcd(x) ?screen(x) using its screen classifier.The discrepancy in relation performance occursbecause the relation evaluation weights every rela-tion equally, whereas in reality some relations aremore frequent.
Furthermore, even within a singlerelation, each entity pair is not equally important ?for example, people tend to ask what is in a state, butnot what is in an ocean.
To account for these factors,we define a reweighted relation metric using the an-notated logical forms containing only one relation,of the form ?x.
?y.c1(x) ?
r(x, y) ?
c2(y).
Usingthese logical forms, we measure the performance ofr on the set of x, y pairs such that c1(x)?c2(y), thenaverage this over all examples.
Table 4 shows that,using this metric, both training regimes have similarperformance.
This result suggests that weakly su-pervised training adapts LSP?s relation classifiers tothe relation instances which are empirically impor-tant for grounding natural language.SCENE GEOQACategories P R F1 P R F1LSP-CAT 0.40 0.86 0.55 0.78 0.25 0.38LSP-W 0.40 0.84 0.54 0.85 0.63 0.73LSP-F 0.98 0.96 0.97 0.89 0.63 0.74Relations P R F1 P R F1LSP-W 0.40 0.42 0.41 0.34 0.51 0.41LSP-F 0.99 0.87 0.92 0.70 0.46 0.55Relations (rw) P R F1 P R F1LSP-W 0.98 0.98 0.98 0.86 0.72 0.79LSP-F 0.98 0.95 0.96 0.89 0.66 0.76Table 4: Perceptual classifier performance, mea-sured against the gold standard logical knowledgebases.
LSP-CAT is excluded from the relation eval-uations, since it does not learn relations.
Relations(rw) is the reweighted metric (see text for details).6 ConclusionsThis paper introduces Logical Semantics with Per-ception (LSP), a model for mapping natural lan-guage statements to their referents in a physical en-vironment.
LSP jointly models perception and lan-guage understanding, simultaneously learning (1)to map from environments to logical knowledgebases containing instances of both one-argumentcategories and two-argument relations, and (2) tosemantically parse natural language.
Furthermore,we introduce a weakly supervised training proce-dure that trains LSP using only sentences with anno-tated denotations, without annotated semantic parsesor noun phrase/entity mappings.
An experimen-tal evaluation reveals that this procedure performsnearly as well fully supervised training, while re-quiring significantly less annotation effort.
Our ex-periments also find that LSP?s ability to learn rela-tions improves performance over comparable priorwork (Matuszek et al 2012).AcknowledgmentsThis research has been supported in part by DARPAunder award FA8750-13-2-0005, and in part by agift from Google.
We also gratefully acknowledgethe CMU Read the Web group for assistance withdata set construction, and Tom Mitchell, ManuelaVeloso, Brendan O?Connor, Felix Duvallet, RobertFisher and the anonymous reviewers for helpful dis-cussions and comments on the paper.203ReferencesRehj Cantrell, Matthias Scheutz, Paul Schermerhorn,and Xuan Wu.
2010.
Robust spoken instruc-tion understanding for HRI.
In Proceedings of the5th ACM/IEEE International Conference on Human-Robot Interaction.David L. Chen and Raymond J. Mooney.
2008.
Learningto sportscast: a test of grounded language acquisition.In Proceedings of the 25th International Conferenceon Machine learning.David L. Chen and Raymond J. Mooney.
2011.
Learn-ing to interpret natural language navigation instruc-tions from observations.
In Proceedings of the 25thAAAI Conference on Artificial Intelligence.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning.Navneet Dalal and Bill Triggs.
2005.
Histograms oforiented gradients for human detection.
In Proceed-ings of the 2005 IEEE Computer Society Conferenceon Computer Vision and Pattern Recognition.Juraj Dzifcak, Matthias Scheutz, Chitta Baral, and PaulSchermerhorn.
2009.
What to do and how to doit: translating natural language directives into tempo-ral and dynamic logic representation for goal manage-ment and action execution.
In Proceedings of the 2009IEEE International Conference on Robotics and Au-tomation.Kai-yuh Hsiao, Nikolaos Mavridis, and Deb Roy.2003.
Coupling perception and simulation: Stepstowards conversational robotics.
In Proceedings ofthe IEEE/RSJ International Conference on IntelligentRobots and Systems.Rohit J. Kate and Raymond J. Mooney.
2006.
Usingstring-kernels for learning semantic parsers.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe ACL.Rohit J. Kate and Raymond J. Mooney.
2007.
Learninglanguage semantics from ambiguous supervision.
InProceedings of the 22nd Conference on Artificial In-telligence.Thomas Kollar, Stefanie Tellex, Deb Roy, and NicholasRoy.
2010.
Toward understanding natural languagedirections.
In Proceedings of the 5th ACM/IEEE In-ternational Conference on Human-Robot Interaction.Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning.Geert-Jan M. Kruijff, Hendrik Zender, Patric Jensfelt,and Henrik I. Christensen.
2007.
Situated dialogueand spatial organization: What, where... and why.
In-ternational Journal of Advanced Robotic Systems.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilisticCCG grammars from logical form with higher-orderunification.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing.Michael Levit and Deb Roy.
2007.
Interpretation of spa-tial language in a map navigation task.
IEEE Transac-tions on Systems, Man, and Cybernetics, Part B.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional semantics.In Proceedings of the Association for ComputationalLinguistics.Matthew MacMahon, Brian Stankiewicz, and BenjaminKuipers.
2006.
Walk the talk: connecting language,knowledge, and action in route instructions.
In Pro-ceedings of the 21st National Conference on ArtificialIntelligence.Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010.Following directions using statistical machine transla-tion.
In Proceedings of the 5th ACM/IEEE Interna-tional Conference on Human-Robot Interaction.Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-moyer, Liefeng Bo, and Dieter Fox.
2012.
A jointmodel of language and perception for grounded at-tribute learning.
In Proceedings of the 29th Interna-tional Conference on Machine Learning.Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.Zinkevich.
2006.
(online) subgradient methodsfor structured prediction.
Artificial Intelligence andStatistics.Deb Roy, Kai-Yuh Hsiao, and Nikolaos Mavridis.
2003.Conversational robots: building blocks for groundingword meaning.
In Proceedings of the HLT-NAACL2003 Workshop on Learning Word Meaning from Non-linguistic Data.Nobuyuki Shimizu and Andrew Haas.
2009.
Learning tofollow navigational route instructions.
In Proceedingsof the 21st international joint conference on artificalintelligence.Marjorie Skubic, Dennis Perzanowski, Sam Blisard, AlanSchultz, William Adams, Magda Bugajska, and DerekBrock.
2004.
Spatial language for human-robot di-alogs.
IEEE Transactions on Systems, Man, and Cy-bernetics, Part C: Applications and Reviews.Mark Steedman.
1996.
Surface Structure and Interpre-tation.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin markov networks.
In Advances in NeuralInformation Processing Systems.204Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew Walter, Ashis Banerjee, Seth Teller, andNicholas Roy.
2011.
Understanding natural languagecommands for robotic navigation and mobile manipu-lation.
In AAAI Conference on Artificial Intelligence.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology.Terry Winograd.
1970.
Procedures as a representationfor data in a computer program for understanding nat-ural language.
Ph.D. thesis, Massachusetts Institute ofTechnology.Yuk Wah Wong and Raymond J. Mooney.
2006.
Learn-ing for semantic parsing with statistical machine trans-lation.
In Proceedings of the main conference on Hu-man Language Technology Conference of NAACL.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logic pro-gramming.
In Proceedings of the Thirteenth NationalConference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammars.
InProceedings of the 21st Conference in Uncertainty inArtificial Intelligence.205206
