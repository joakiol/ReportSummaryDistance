Proceedings of the Workshop on Statistical Machine Translation, pages 150?153,New York City, June 2006. c?2006 Association for Computational LinguisticsLanguage Models and Reranking for Machine TranslationMarian Olteanu, Pasin Suriyentrakorn and Dan MoldovanLanguage Computer Corp.Richardson, TX 75080{marian,psuri,moldovan}@languagecomputer.comAbstractComplex Language Models cannot be eas-ily integrated in the first pass decoding ofa Statistical Machine Translation system ?the decoder queries the LM a very largenumber of times; the search process in thedecoding builds the hypotheses incremen-tally and cannot make use of LMs thatanalyze the whole sentence.
We presentin this paper the Language Computer?ssystem for WMT06 that employs LM-powered reranking on hypotheses gener-ated by phrase-based SMT systems1 IntroductionStatistical machine translation (SMT) systems com-bine a number of translation models with one ormore language models.
Adding complex languagemodels in the incremental process of decoding is avery challenging task.
Some language models canonly score sentences as a whole.
Also, SMT de-coders generate during the search process a verylarge number of partial hypotheses and query thelanguage model/models 1.The solution to these problems is either to usemultiple iterations for decoding, to make use of thecomplex LMs only for complete hypotheses in thesearch space or to generate n-best lists and to rescorethe hypotheses using also the additional LMs.
For1During the translation of the first 10 sentences of the de-vtest2006.de dataset using Phramer and the configuration de-scribed in Section 3, the 3-gram LM was queried 27 milliontimes (3 million distinct queries).the WMT 2006 shared task we opted for the rerank-ing solution.
This paper describes our solution andresults.2 System DescriptionWe developed for the WMT 2006 shared task a sys-tem that is trained on a (a) word-aligned bilingualcorpus, (b) a large monolingual (English) corpus and(c) an English treebank and it is capable of translat-ing from a source language (German, Spanish andFrench) into English.Our system embeds Phramer2 (used for mini-mum error rate training, decoding, decoding tools),Pharaoh (Koehn, 2004) (decoding), Carmel 3(helper for Pharaoh in n-best generation), Char-niak?s parser (Charniak, 2001) (language model) andSRILM4 (n-gram LM construction).2.1 Translation table constructionWe developed a component that builds a translationtable from a word-aligned parallel corpus.
The com-ponent generates the translation table according tothe process described in the Pharaoh training man-ual5.
It generates a vector of 5 numeric values foreach phrase pair:?
phrase translation probability:?(f?
|e?)
= count(f?
, e?)count(e?)
, ?(e?|f?)
=count(f?
, e?)count(f?
)2http://www.phramer.org/ ?
Java-based open-source phrasebased SMT system3http://www.isi.edu/licensed-sw/carmel/4http://www.speech.sri.com/projects/srilm/5http://www.iccs.inf.ed.ac.uk/?pkoehn/training.tgz150?
lexical weighting (Koehn et al, 2003):lex(f?
|e?, a) =n?i=11|{j|(i, j) ?
a}|??(i,j)?aw(fi|ej)lex(e?|f?
, a) =m?j=11|{i|(i, j) ?
a}|??
(i,j)?aw(ej |fi)?
phrase penalty: ?(f?
|e?)
= e; log(?(f?
|e?))
= 12.2 DecodingWe used the Pharaoh decoder for both the Min-imum Error Rate Training (Och, 2003) and testdataset decoding.
Although Phramer provides de-coding functionality equivalent to Pharaoh?s, wepreferred to use Pharaoh for this task because itis much faster than Phramer ?
between 2 and 15times faster, depending on the configuration ?
andpreliminary tests showed that there is no noticeabledifference between the output of these two in termsof BLEU (Papineni et al, 2002) score.The log-linear model uses 8 features: one distor-tion feature, one basic LM feature, 5 features fromthe translation table and one sentence length feature.2.3 Minimum Error Rate TrainingTo determine the best coefficients of the log-linearmodel (?)
for both the initial stage decoding andthe second stage reranking, we used the unsmoothedMinimum Error Rate Training (MERT) componentpresent in the Phramer package.
The MERT com-ponent is highly efficient; the time required to searcha set of 200,000 hypotheses is less than 30 secondsper iteration (search from a previous/random ?
toa local maximum) on a 3GHz P4 machine.
Wealso used the distributed decoding component fromPhramer to speed up the search process.We generated the n-best lists required for MERTusing the Carmel toolkit.
Pharaoh outputs a lat-tice for each input sentence, from which Carmelextracts a specific number of hypotheses.
We usedthe europarl.en.srilm language model for decodingthe n-best lists.The weighting vector is calculated individuallyfor each subtask (pair of source and target lan-guages).No.
of sentences 96.7 MNo.
of tokens 2.3 BVocabulary size 1.6 MDistinct grams 1 BTable 1: English Gigaword LM statistics2.4 Language Models for rerankingWe employed both syntactic language models andn-gram based language models extracted from verylarge corpora for improving the quality of the trans-lation through reranking of the n-best list.
These lan-guage models add a total of 13 new features to thelog-linear model.2.4.1 English GigawordWe created large-scale n-gram language modelsusing English Gigaword Second Edition6 (EGW).We split the corpus into sentences, tokenized thecorpus, lower-cased the sentences, replaced everydigit with ?9?
to cluster different numbers into thesame unigram entry, filtered noisy sentences and wecollected n-gram counts (up to 4-grams).
Table 1presents the statistics related to this process.We pruned the unigrams that appeared less than15 times in the corpus and all the n-grams that con-tain the pruned unigrams.
We also pruned 3-gramsand 4-grams that appear only once in the corpus.Based on these counts, we calculated 4 features foreach sentence: the logarithm of the probability ofthe sentence based on unigrams, on bigrams, on 3-grams and on 4-grams.
The probabilities of eachword in the analyzed translation hypotheses werebounded by 10?5 (to avoid overall zero probabilityof a sentence caused by zero-counts).Based on the unpruned counts, we calculated 8additional features: how many of the n-grams in thethe hypothesis appear in the EGW corpus and alsohow many of the n-grams in the hypotheses don?tappear in the Gigaword corpus (n = 1..4).
Thetwo types of counts will have different behavior onlywhen they are used to discriminate between two hy-potheses with different length.The number of n-grams in each of the two casesis presented in Table 2.6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T12151sentence probability n-gram hit/missmodel model1-grams 310 K 310 K2-grams 45 M 45 M3-grams 123 M 283 M4-grams 235 M 675 MTable 2: Number of n-gram entries in the EGW LM2.4.2 Charniak parsingWe used Charniak?s parser as an additional LM(Charniak, 2001) in reranking.
The parser pro-vides one feature for our model ?
the log-grammar-probability of the sentence.We retrained the parser on lowercased Penn Tree-bank II (Marcus et al, 1993), to match the lower-cased output of the MT decoder.Considering the huge number of hypotheses thatneeded to be parsed for this task, we set it to parsevery fast (using the command-line parameter -T107).2.5 Reranking and votingA ?
weights vector trained over the 8 basic features(?1) is used to decode a n-best list.
Then, a ?
vectortrained over all 21 features (?2) is used to rerankthe n-best list, potentially generating a new first-besthypothesis.To improve the results, we generated during train-ing a set of distinct ?2 weight vectors (4-10 differentweight vectors).
Each ?2 picks a preferred hypoth-esis.
The final hypothesis is chosen using a votingmechanism.
The computational cost of the votingprocess is very low - each of the ?2 is applied on thesame set of hypotheses - generated by a single ?1.2.6 PreprocessingThe vocabulary of languages like English, Frenchand Spanish is relatively small.
Most of the newwords that appear in a text and didn?t appear in a pre-defined large text (i.e.
: translation table) are abbre-viations and proper nouns, that usually don?t changetheir form when they are translated into another lan-guage.
Thus Pharaoh and Phramer deal without-of-vocabulary (OOV) words ?
words that don?tappear in the translation table ?
by copying theminto the output translation.
German is a compound-ing language, thus the German vocabulary is virtu-7Time factor.
Higher is better.
Default: 210ally infinite.
In order to avoid OOV issues for newtext, we applied a heuristic to improve the probabil-ity of properly translating compound words that arenot present in the translation table.
We extracted theGerman vocabulary from the translation table.
Then,for each word in a text to be translated (developmentset or test set), we checked if it is present in the trans-lation dictionary.
If it was not present, we checkedif it can be obtained by concatenating two words inthe dictionary.
If we found at least one variant ofsplitting the unknown word, we altered the text bydividing the word into the corresponding pieces.
Ifthere are multiple ways of splitting, we randomlytook one.
The minimum length for the generatedword is 3 letters.In order to minimize the risk of inserting wordsthat are not in the reference translation into the out-put translation, we applied a OOV pruning algorithm(Koehn et al, 2005) ?
we removed every word in thetext to be translated that we know we cannot trans-late (doesn?t appear either in the foreign part of theparallel corpus used for training) or in what we ex-pect to be present in an English text (doesn?t appearin the English Gigaword corpus).
This method wasapplied to all the input text that was automaticallytranslated ?
development and test; German, Frenchand Spanish.For the German-to-English translation, the com-pound word splitting algorithm was applied beforethe unknown word removal process.3 Experimental SetupWe generated the translation tables for each pairof languages using the alignment provided for thisshared task.We split the dev2006 files into two halves.
Thefirst half was used to determine ?1.
Using ?1, wecreated a 500-best list for each sentence in the sec-ond half.
We calculated the value of the enhancedfeatures (EGW and Charniak) for each of these hy-potheses.
Over this set of almost 500 K hypothe-ses, we computed 10 different ?2 using MERT.
Thesearch process was seeded using ?1 padded with 0for the new 13 features.
We sorted the ?2s by theBLEU score estimated by the MERT algorithm.
Wepruned manually the ?2s that diverge too much fromthe overall set of ?2s (based on the observation that152500-best best voting WPT05oracle ?1 ?2 ?2 bestDE-EN?
no split 25.70?
split 33.63 25.81 26.29 26.28 24.77FR-EN 37.33 30.90 31.21 31.21 30.27ES-EN 38.06 31.13 31.15 31.22 30.95Table 3: BLEU scores on the devtest2006 datasets.Comparison with WPT05 results500-best oracle ?1 voting ?2DE-EN (split) 30.93 23.03 23.55FR-EN 34.71 27.83 28.00ES-EN 37.68 29.97 30.12Table 4: BLEU scores on the test2006 datasets.
Sub-mitted results are bolded.these weights are overfitting).
We picked from theremaining set the best ?2 and a preferred subset of?2s to be used in voting.The ?1 was also used to decode a 500-best list foreach sentence in the devtest2006 and test2006 sets.After computing value of the enhanced features foreach of these hypotheses, we applied the rerankingalgorithm to pick a new first-best hypothesis ?
theoutput of our system.We used the following parameters for decoding:-dl 5 -b 0.0001 -ttable-limit 30 -s 200 for French andSpanish and -dl 9 -b 0.00001 -ttable-limit 30 -s 200for German.4 ResultsTable 3 presents the detailed results of our system onthe devtest2006 datasets and comparison with WMT2006 best results 8.
The final results, on the test setof the shared task, are reported in Table 4.5 ConclusionsBy analyzing the results, we observe that a verypowerful component of our system is the MERTcomponent of Phramer.
It provided a very highbaseline for the devtest2006 sets (WPT05 test sets).The additional language models seem to consis-tently improve the results, although the increase isnot very significant on FR-EN and ES-EN subtasks.The cause might be the specifics of the data involved8http://www.statmt.org/wpt05/mt-shared-task/in this shared task ?
mostly European Parliamentproceedings, which is different than the domain ofboth Treebank and English Gigaword ?
newswire.The enhanced LMs compete with the default LM(which is also part of the model) that is trained onEuropean Parliament data.The word splitting heuristics offers also a smallimprovement for the performance on DE-EN sub-task.Voting seems to slightly improve the results insome cases (ES-EN subtask).
We believe that thevoting implementation reduces ?
weights overfit-ting, by combining the output of multiple local max-ima of the development set.
The size of the de-velopment set used to generate ?1 and ?2 (1000sentences) compensates the tendency of the un-smoothed MERT algorithm to overfit (Och, 2003)by providing a high ratio between number of vari-ables and number of parameters to be estimated.ReferencesEugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of 39th AnnualMeeting of the Association for Computational Linguis-tics, pages 124?131.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL 2003, Edmonton, Canada.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, DavidTalbot, and Michael White.
2005.
Edinburgh systemdescription for the 2005 NIST MT Evaluation.Philipp Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Erhard Hinrichsand Dan Roth, editors, Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 311?318.153
