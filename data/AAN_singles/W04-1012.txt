Automatic Evaluation of Summaries Using Document GraphsEugene Santos Jr., Ahmed A. Mohamed, and Qunhua ZhaoComputer Science and Engineering DepartmentUniversity of Connecticut191 Auditorium Road, U-155, Storrs, CT 06269-3155{eugene, amohamed, qzhao}@engr.uconn.eduAbstractSummarization evaluation has been always a chal-lenge to researchers in the document summariza-tion field.
Usually, human involvement isnecessary to evaluate the quality of a summary.Here we present a new method for automaticevaluation of text summaries by using documentgraphs.
Data from Document Understanding Con-ference 2002 (DUC-2002) has been used in theexperiment.
We propose measuring the similaritybetween two summaries or between a summaryand a document based on the concepts/entities andrelations between them in the text.1 IntroductionDocument summarization has been the focus ofmany researchers for the last decade, due to theincrease in on-line information and the need tofind the most important information in a (set of)document(s).
One of the biggest challenges in textsummarization research is how to evaluate thequality of a summary or the performance of asummarization tool.
There are different approa-ches to evaluate overall quality of a summariza-tion system.
In general, there are two types ofevaluation categories: intrinsic and extrinsic(Sparck-Jones and Galliers, 1996).
Extrinsic ap-proaches measure the quality of a summary basedon how it affects certain tasks.
In intrinsic approa-ches, the quality of the summarization is evaluatedbased on analysis of the content of a summaryitself.
In both categories human involvement isused to judge the summarization outputs.
Theproblem with having humans involved in evalua-ting summaries is that we can not hire human jud-ges every time we want to evaluate summaries(Mani and Maybury, 1999).
In this paper, we dis-cuss a new automated way to evaluate machine-generated summaries without the need to havehuman judges being involved which decreases thecost of determining which summarization systemis best.
In our experiment, we used data from Do-cument Understanding Conference 2002 (DUC-2002).2 Related WorkResearchers in the field of document summariza-tion have been trying for many years to define ametric for evaluating the qua lity of a machine-generated summary.
Most of these attempts invol-ve human interference, which make the process ofevaluation expensive and time-consuming.
Wediscuss some important work in the intrinsic cate-gory.2.1 Sentence Precision-Recall MeasureSentence precision and recall have been widelyused to evaluate the quality of a summarizer (Jinget al, 1998).
Sentence precision measures the per-cent of the summary that contains sentences mat-ched with the model summary.
Recall, on theother hand, measures the percent of sentences inthe ideal summary that have been recalled in thesummary.
Even though sentence precision/recallfactors can give us an idea about a summary?squality, they are not the best metrics to evaluate asystem?s quality.
This is due to the fact that asmall change in the output summary can dramati-cally affect the quality of a summary (Jing et al,1998).
For example, it is possible that a systemwill pick a sentence that does not match with amodel sentence chosen by an assessor, but isequivalent to it in meaning.
This, of course, willaffect the score assigned to the system dramatica-lly.
It is also obvious that sentence precision/recallis only applicable to the summaries that are gene-rated by sentence extraction, not abstraction (Ma-ni, 2001).2.2 Content-Based MeasureContent-based measure computes the similarity atthe vocabulary level (Donaway, 2000 and Mani,2001).
The evaluation is done by creating termfrequency vectors for both the summary and themodel summary, and measuring the cosine simila-rity (Salton, 1988) between these two vectors.
Ofcourse, the higher the cosine similarity measure,the higher the quality of the summary is.
Lin andHovy (2002) used accumulative n-gram matchingscores between model summaries and the summa-ries to be evaluated as a performance indicator inmulti-document summaries.
They achieved theirbest results by giving more credit to longer n-gram matches with the use of Porter stemmer.A problem raised in the evaluation approachesthat use the cosine measure is that the summariesmay use different key terms than those in the ori-ginal documents or model summaries.
Since termfrequency is the base to score summaries, it ispossible that a high quality summary will get alower score if the terms used in the summary arenot the same terms used in most of the document?stext.
Donaway et al (2000) discussed using acommon tool in information retrieval: latent se-mantic indexing (LSI) (Deerwester et al, 1990) toaddress this problem.
The use of LSI reduces theeffect of near-synonymy problem on the similarityscore.
This is done by penalizing the summaryless in the reduced dimension model when thereare infrequent terms synonymous to frequentterms.
LSI averages the weights of terms that co-occur frequently with other mutual terms.
Forexample, both ?bank?
and ?financial institution?often occur with the term ?account?
(Deerwesteret al, 1990).
Even though using LSI can be usefulin some cases, it can produce unexpected resultswhen the document contains terms that are notsynonymous to each other, but, however, they co-occur with other mutual terms.2.3 Document Graph2.3.1 Representing Content by DocumentGraphCurrent approaches in content-based summariza-tion evaluation ignore the relations between thekeywords that are expressed in the document.Here, we introduce our approach, which measuresthe similarity between two summaries or a sum-mary and a document based on the relations (bet-ween the keywords).
In our approach, eachdocument/summary is represented as a documentgraph (DG), which is a directed graph of con-cepts/entities and the relations between them.
ADG contains two kinds of nodes, concept/entitynodes and relation nodes.
Currently, only twokinds of relations, ?isa?
and ?related to?, are cap-tured (Santos et al 2001) for simplicity.To generate a DG, a document/summary inplain text format is first tokenized into sentences;and then, each sentence is parsed using Link Par-ser (Sleator and Temperley, 1993), and the nounphrases (NP) are extracted from the parsing re-sults.
The relations are generated based on threeheuristic rules:?
The NP-heuristic helps to set up the hierar-chical relations.
For example, from a nounphrase ?folk hero stature?, we generate re-lations ?folk hero stature isa stature?, ?folkhero stature related to  folk hero?, and ?folkhero isa hero?.?
The NP-PP-heuristic attaches all preposi-tional phrases to adjacent noun phrases.
Forexample, from ?workers at a coal mine?,we generate a relation, ?worker related tocoal mine?.?
The sentence-heuristic rela tes con-cepts/entities contained in one sentence.The relations created by sentence-heuristicare then sensitive to verbs, since the inter-val between two noun phrases usually con-tains a verb.
For example, from a sentence?Workers at a coal mine went on strike?,we generate a relation ?worker related tostrike?.
Another example, from ?The usualcause of heart attacks is a blockage of thecoronary arteries?, we generate ?heart at-tack cause related to  coronary artery bloc-kage?.
Figure 1 shows a  example of apartial DG.Figure 1: A partial DG.2.3.2 Similarity Comparison between twoDocument GraphsThe similarity of DG1 to DG2 is given by theequation:MmNnDGDGSim22),( 21 +=which is modified from Montes-y-G?mez et al(2000).
N is the number of concept/entity nodes inDG1, and M stands for number of relations inDG1; n is the number of matched concept/entitynodes in two DGs, and m is the number of mat-ched relations.
We say we find a matched relationin two different DGs, only when both of the twoconcept/entity nodes linked to the relation nodeare matched, and the relation node is also mat-ched.
Since we might compare two DGs that aresignificantly different in size (for example, DGsfor an extract vs. its source document), we usedthe number of concept/entity nodes and relationnodes in the target DG as N and M, instead of thetotal number of nodes in both DGs.
The target DGis the one for the extract in comparing an extractwith its source text.
Otherwise, the similarity willalways be very low.
Currently, we weight all theconcepts/entities and relations equally.
This canbe fine tuned in the future.3 Data, and Experimental Design3.1 DataBecause the data from DUC-2003 were short(~100 words per extract for multi-document task),we chose to use multi-document extracts fromDUC-2002 (~200 words and ~400 words per ex-tract for multi-document task) in our experiment.In this corpus, each of ten information analystsfrom the National Institute of Standards andTechnology (NIST) chose one set of newswi-re/paper articles in the following topics (Over andLiggett, 2002):?
A single natural disaster event with docu-ments created within at most a 7-day win-dow?
A single event of any type with documentscreated within at most a 7-day window?
Multiple distinct events of the same type (notime limit)?
Biographical (discuss a single person)Each assessor chose 2 more sets of articles sothat we ended up with a total of 15 document setsof each type.
Each set contains about 10 docu-ments.
All documents in a set are mainly about aspecific ?concept.
?A total of ten automatic summarizers participa-ted to produce machine-generated summaries.Two extracts of different lengths, 200 and 400words, have been generated for each document-set.3.2 Experimental DesignA total of 10 different automatic summarizationsystems submitted their summaries to DUC.
Weobtained a ranking order of these 10 systems ba-sed on sentence precision/recall by comparing themachine generated extracts to the human genera-ted model summaries.
The F-factor is calculatedfrom the following equation (Rijsbergen, 1979):)(2RPRPF+?
?=where P is the precision and R is the recall.
Wethink this ranking order gives us some idea onhow human judges think about the performance ofdifferent systems.For our evaluation based on DGs, we alsocalculated F-factors based on precision and recall,where P = Sim(DG1, DG2) and R = Sim(DG2,DG1).
In the first experiment, we ranked the 10automatic summarization systems by comparingDGs generated from their outputs to the DGs gen-erated from model summaries.
In this case, DG1 isthe machine generated extract and DG2 is the hu-man generated extract.
In the second experiment,we ranked the systems by comparing machinegenerated extracts to the original documents.
Inthis case, DG1 is an extract and DG2 is the corre-sponding  original document.
Since the extractswere  generated  from multi-document  sets,  weused the average of the F-factors for ranking pur-poses.4  ResultsThe ranking orders obtained based on sentenceprecisions and recalls are shown in Tables 1 and2.
The results indicate that for sentence precisionand recall, the ranking order for different summa-rization systems is not affected by the summariza-tion compression ratio.
The ranking results for200-word extracts and 400-word extracts are ex-actly the same.Since the comparison is between the machinegenerated extracts and the human created modelextracts, we believe that the rankings should rep-resent the performance of 10 different automatedsummarization systems, to some degree.
The ex-periments using DGs instead of sentence matchinggive two very similar ranking orders (Spearmanrank correlation coefficient [Myers and Well,1995] is 0.988) where only systems 24 and 19 arereversed in their ranks (Tables 1 and 2).
The re-sults show that when the evaluation is based onthe comparison between machine generated ex-tracts and the model extracts, our DG-basedevaluation approach will provide roughly thesame ranking results as the sentence precision andrecall approach.
Notice that the F-factors obtainedby experiments using DGs are higher than thosecalculated based on sentence matching.
This isbecause our DG-based evaluation approach com-pares the two extracts at a more fine grained levelthan sentence matching does since we comparethe similarity at the level of concepts/entities andtheir relations, not just whole sentences.
The simi-larity of the two extracts should actually be higherthan the score obtained with sentence matchingbecause there are sentences that are equivalent inmeaning but not syntactically identical.Since we believe that the DGs captures the se-mantic information content contained in the res-pective documents, we rank the automaticsummarization systems by comparing the DGs oftheir extract outputs against the DGs of the orig i-nal documents.
This approach does not need themodel summaries, and hence no human involve-ment is needed in the evaluation.
The results areshown in Tables 3 and 4.
As we can see, our ran-kings are different from the ranking results basedon comparison against the model extracts.
System28 has the largest change in rank in both 200-wordand 400-word summaries.
It was ranked as theworst by our DG based approach instead of num-ber 7 (10 is the best) by the approaches comparingto the model extracts.
We investigated the extractcontent of system 28 and found that many extractsSystemrankSentence-basedRankingSentence-basedF-factorDG-basedRankingDG-basedF-factorSystemrankSentence-basedRankingSentence-basedF-factorDG-basedRankingDG-basedF-factor1(worst) 22 0.000 22 0.1221(worst) 22 0.000 22 0.1812 16 0.062 16 0.167  2 16 0.128 16 0.2353 31 0.081 31 0.180  3 25 0.147 25 0.2564 25 0.081 25 0.188  4 31 0.150 31 0.2665 29 0.090 29 0.200  5 29 0.155 20 0.2736 20 0.125 20 0.226  6 20 0.172 29 0.2797 28 0.138 28 0.255  7 28 0.197 28 0.3168 24 0.171 19 0.283  8 24 0.223 19 0.3379 19 0.184 24 0.283  9 19 0.224 24 0.35510(best) 21 0.188 21 0.30810(best) 21 0.258 21 0.372Table 1: Model Summaries vs. machine-generated summaries.
Ranking results for 200words extractsTable 2: Model Summaries vs. machine-generated summaries.
Ranking results for 400words extractsgenerated by system 28 included sentences thatcontain little information, e.g., author?s names,publishers, date of publication, etc.
The followingare sample extracts produced for document 120 bysystems 28, 29 (the best ranked) and a human jud-ge, at 200-words.
[Extract for Document 120 by System 28]John Major, endorsed by Margaret Thatcher asthe politician closest to her heart, was elected bythe Conservative Party Tuesday night to succeedher as prime minister.Hong Kong WEN WEI POBy MICHAEL CASSELL and IVOR OWENBy MICHAEL THOMPSON-NOELBy DOMINIC LAWSONFrom Times Wire ServicesBy WILLIAM TUOHY, TIMES STAFF WRITERFrom Associated Press[Extract for Document 120 by System 29]John Major, endorsed by Margaret Thatcher asthe politician closest to her heart, was elected bythe Conservative Party Tuesday night to succeedher as prime minister.Aides said Thatcher is "thrilled".Hurd also quickly conceded.ONE year ago tomorrow, Mr John Major surpri-sed everyone but himself by winning the generalelection.It has even been suggested that the recording ofthe prime minister's conversation with MichaelBrunson, ITN's political editor, in which Majorused a variety of four-, six- and eight-letter wordsto communicate his lack of fondness for certaincolleagues, may do him good.BFN[Colin Brown article: "Cabinet Allies Close RanksBut BringRight-wing MPs confirmed the findings in anINDEPENDENT ON SUNDAY/NOP [NationalOpinion Poll] poll that Michael Heseltine was thefavourite to replace Mr Major, if he is forced out.The Labour Party controls 90 local councils, whe-reas the Conservatives only control 13, with asharp contrast in strength between the two sides.If he did not see the similarity, that is still morerevealing.
[Extract for Document 120 by a human judge --model extract]John Major, endorsed by Margaret Thatcher asthe politician closest to her heart, was elected bythe Conservative Party Tuesday night to succeedher as prime minister.While adopting a gentler tone on the contentiousissue of Britain's involvement in Europe, he sharesher opposition to a single European currency andshares her belief in tight restraint on governmentspending.FT 08 APR 93 / John Major's Year: Major's blueperiod - A year on from success at the polls, theprime minister's popularity has plunged.The past 12 months have been hijacked by inter-nal party differences over Europe, by the debaclesurrounding UK withdrawal from the exchangerates mechanism of the European Monetary Sys-tem, and by a continuing, deep recession whichhas disappointed and alienated many traditionalTory supporters in business.Its Leader"] [Text] In local government electionsacross Britain yesterday, the Conservatives suffe-red their worst defeat ever, losing control of 17regional councils and 444 seats.Even before all of the results were known, someTories openly announced their determination tochallenge John Major's position and remove himfrom office as early as possible.The extract generated by system 28 has 8 sen-tences of which only one of them contained rele-vant information.
When comparing using sentenceprecision and recall, all three extracts only haveone sentence match which is the first sentence.
Ifwe calculate the F-factors based on the model ex-tract shown above, system 28 has a score of 0.143and system 29 has a lower score of 0.118.
Afterreading all three extracts, the extract generated bysystem 29 contains much more relevant informa-tion than that generated by system 28.
The mis-sing information in system 28 is ---John Majorand the Conservatives were losing the popularityin 1993, after John Major won the election oneyear ago,-- which should be the most importantcontent in the extract.
In our DG-based approach,the scores assigned to system 28 and 29 are 0.063and 0.100, respectively; which points out that sys-tems 29 did a better job than system 28.200-word 400-wordSystem F-factor System F-factor28 0.092 22 0.13722 0.101 28 0.14116 0.111 16 0.16020 0.115 25 0.16325 0.115 20 0.16421 0.122 31 0.165Model 0.124 Model 0.16531 0.124 21 0.16724 0.125 29 0.16819 0.129 19 0.16829 0.132 24 0.169Table 5: Average F-factors for the model sum-maries and machine-generated summaries.Of the 59 submitted 200-word extracts by sys-tem 28, 39 extracts suffer the problem of havingless informative sentences.
The number of suchsentences is 103, where the total number of sen-tences is 406 from all the extracts for system 28.On average, each extract contains 1.75 such sen-tences, where each extract has 6.88 sentences.
Forthe 400-words extracts, we found 54 extractsamong the 59 submitted summaries also have thisproblem.
The total number of such sentences  was206, and the total number of sentences was 802sentences.
So,  about 3.49 sentences do not con-tain much information, where the average lengthof each extract is 13.59 sentences.
Thus, a largeportion of each extract does not contribute to thedo example, will not be considered a good sum-mary, either on the criterion of summary coheren-ce or summary informativeness, where coherenceis how the summary reads and informativeness ishow much information from the source is preser-ved in the summary (Mani, 2001).From the results based on comparing extractsagainst original documents, we found that severalsystems perform very similarly, especially in theexperiments with 400-word extracts (Table 4).The results show that except for systems 22 and28 which perform significantly worse, all othersystems are very similar, from the point of view ofinformativeness.Finally, we generated DGs for the model extra-cts and then compared them against their originaldocuments.
The average F-factors are calculated,which are listed in Table 5 along with the scoresfor different automatic summarization systems.Intuitively, a system provides extracts that containmore information than other systems will get ahigher score.
As we can see from the data, at 200-words, the extracts generated by systems 21, 31,24, 19, and 29 contain roughly the same amountof information as those created by humans, whilethe other five systems performed worse thanhuman judges.
At 400-words, when the compres-sion ratio of the extracts is decreased, more sys-tems perform well; only systems 22 and 28SystemrankSentence-basedRankingSentence-basedF-factorDG-basedRankingDG-basedF-factorSystemrankSentence-basedRankingSentence-basedF-factorDG-basedRankingDG-basedF-factor1(worst) 22 0.000 28 0.0921(worst) 22 0.000 22 0.1372 16 0.062 22 0.101  2 16 0.128 28 0.1413 31 0.081 16 0.111  3 25 0.147 16 0.1604 25 0.081 20 0.115  4 31 0.150 25 0.1635 29 0.090 25 0.115  5 29 0.155 20 0.1646 20 0.125 21 0.122  6 20 0.172 31 0.1657 28 0.138 31 0.124  7 28 0.197 21 0.1678 24 0.171 24 0.125  8 24 0.223 29 0.1689 19 0.184 19 0.129  9 19 0.224 19 0.16810(best) 21 0.188 29 0.13210(best) 21 0.258 24 0.169Table 3: Machine-generated summaries vs.source documents.
Ranking results for 200words extractsTable 4: Machine-generated summaries vs.source documents.
Ranking results for 400words extractsgenerated summaries that contain much less in-formation than the model summaries.5 Discussion and Future WorkIn DUC 2002 data collection, 9 human judgeswere involved in creating model extracts; how-ever, there are only 2 model extracts generated foreach document set.
The sentence precisions andrecalls obtained from comparing the machine gen-erated extracts and human generated model ex-tracts are distributed along with raw data (DUC-2002.
http://www-nlpir.nist.gov/projects/duc),with the intent to use them in system performancecomparison.
Van Halteren (2002) argued that onlytwo manually created extracts could not be used toform a sufficient basis for a good benchmark.
Toexplore this issue, we obtained a ranking order foreach human judge based on the extracts he/shegenerated.
The results showed that the rankingorders obtained from 9 different judges are actu-ally similar to each other, with the averageSpearman correlation efficient to be 0.901.
Fromthis point of view, if the ranking orders obtainedby sentence precision and recall based on themodel extracts could not form a good basis for abenchmark, it is because of its binary nature (Jinget al, 1998), not the lack of sufficient model ex-tracts in DUC 2002 data.Van Halteren and Teufel (2003) proposed toevaluate summaries via factoids, a pseudo-semantic representation based on atomic informa-tion units.
However, sufficient manually createdmodel summaries are need; and factoids are alsomanually annotated.
Donaway et al (2000) sug-gested that it might be possible to use content-based measures for summarization evaluation wit-hout generating model summaries.
Here, we pre-sented our approach to evaluate the summariesbase on document graphs, which is generated au-tomatically.
It is not very surprising that differentmeasures rank summaries differently.
A similarobservation has been reported previously (Radev,et al 2003).
Our document graph approach onsummarization evaluation is a new automatic wayto evaluate machine-generated summaries, whichmeasures the summaries from the point of view ofinformativeness.
It has the potential to evaluatethe quality of summaries, including extracts, abs-tracts, and multi-document summaries, withouthuman involvement.
To improve the performanceof our system and better represent the content ofthe summaries and source documents, we areworking in several areas: 1) Improve the results ofnatural language processing to capture informa-tion more accurately; 2) Incorporate a knowledgebase, such as WordNet (Fellbaum, 1998), to ad-dress the synonymy problem; and, 3) Use moreheuristics in our relation extraction and genera-tion.
We are also going to extend our experimentsby comparing our approach to content-based mea-sure approaches, such as cosine similarity basedon term frequencies and LSI approaches, in bothextracts and abstracts.6 AcknowledgmentsThis work was supported in part by the AdvancedResearch and Development Activity (ARDA) U.S.Government.
Any opinions, findings and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of the U. S. Government.ReferencesScott Deerwester, Susan T. Dumais, George W.Furnas, Thomas K. Landauer, and RichardHarshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for In-formation Science, 41(6): 391-407.Robert L. Donaway, Kevin W. Drummey, andLaura A. Mather.
2000.
A comparison of rank-ings produced by summarization evaluationmeasures.
In Proceedings of the Workshop onAutomatic Summarization, pages 69-78.Christiane Fellbaum.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal,and Jaime Carbonell.
1999.
Summarizing textdocuments: Sentence selection and evaluationmetrics.
In Proceedings the 24th Annual Interna-tional ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages121-128, ACM, New York..Hans van Haltern.
2002.
Writing style recognitionand sentence extraction.
DUC?02 ConferenceProceedings.Hans Van Halteren and Simone Teufel, 2003.
Ex-amining the consensus between human summa-ries: Initial experiments with factoid analysis.
InHLT/NAACL-2003 Workshop on AutomaticSummarization.Hongyan Jing, Kathleen McKeown, Regina Barzi-lay, and Michael Elhadad.
1998.
Summarizationevaluation methods: experiments and analysis.In American Association for Artificial Intelli-gence Spring Symposium Series, pages 60-68.Chen-Yew Lin.
2001.
Summary evaluationenvironment.
http://www.isi.edu/~cyl/SEE.Chin-Yew Lin and Eduard Hovy.
2002.
Manualand automatic evaluation of summaries.
In Pro-ceedings of the Workshop on Automatic Summa-rization post conference workshop of ACL-02,Philadelphia, PA, U.S.A., July 11-12 (DUC2002).Inderjeet Mani.
2001.
Summarization evaluation:An overview.
In Proceedings of the NTCIRWorkshop 2 Meeting on Evaluation of Chineseand Japanese Text Retrieval and Test Summari-zation, National Institute of Informatics.Inderjeet Mani and Mark T. Maybury.
1999.
Ad-vances in Automatic Text Summarization.
TheMIT Press.Manuel Montes-y-G?mez, Alexander Gelbukh,and Aurelio L?pez-L?pez.
2000.
Comparison ofconceptual graphs.
In Proceedings of MICAI-2000, 1st Mexican International Conference onArtificial Intelligence, Acapulco, Mexico.Jerome L. Myers and Arnold D. Well.
1995.
Re-search Design and Statistical Analysis, pages,488-490, Lawrence Erlbaum Associates, NewJerseyPaul Over and Walter Liggett.
2002.
Introductionto DUC-2002: An intrinsic evaluation of genericnews text summarization systems.
DocumentUnderstanding Conferences website(http://duc.nist.gov/)Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
BLEU: A Method for Auto-matic Evaluation of Machine Translation.Technical Report RC22176 (W0109-022), IBMResearch Division, Thomas J. Watson ResearchCenter.Dragomir R. Radev, Simone Teufel, Horacio Sag-gion, Wai Lam, John Blitzer, Hong Qi, ArdaCelebi, Danyu Liu, and Elliott Drabek.
2003.Evaluation challenges in large-scale documentsummarization.
In Proceedings of the 41st An-nual Meetring of the Association for Computa-tional Linguistics, July 2003, pages 375-382.Keith van Rijsbergen.
1979.
Information Re-trieval.
Second Edition Butterworths, London.Gerard Salton.
1988.
Automatic Text Processing.Addison-Wesley Publishing Company.Eugene Santos Jr., Hien Nguyen, and Scott M.Brown.
2001.
Kavanah: An active user interfaceInformation Retrieval Agent Technology.
Mae-bashi, Japan, October 2001, pages 412-423.Danny Sleator and Davy Temperley.
1993.
Pars-ing English with a link grammar.
In Proceed-ings of the Third International Workshop onParsing Technologies, pages 277-292.Karen Sparck-Jones and Julia R. Galliers.
1996.Evaluating Natural Language Processing Sys-tems: An Analysis and Review (Lecture Notes inArtificial Inte lligence 1083).
Springer-Verlag
