English-Japanese Example-Based Machine Translation Using AbstractLinguistic RepresentationsChris Brockett, Takako Aikawa, Anthony Aue, Arul Menezes, Chris Quirkand Hisami SuzukiNatural Language Processing Group, Microsoft ResearchOne Microsoft WayRedmond, WA 98052, USA{chrisbkt,takakoa,anthaue,arulm,chrisq,hisamis}@microsoft.comAbstractThis presentation describes an example-based English-Japanese machine trans-lation system in which an abstractlinguistic representation layer is used toextract and store bilingual translationknowledge, transfer patterns betweenlanguages, and generate output strings.Abstraction permits structural neutral-izations that facilitate learning of trans-lation examples across languages withradically different surface structure charac-teristics, and allows MT development toproceed within a largely language-independent NLP architecture.
Com-parative evaluation indicates that aftertraining in a domain the English-Japanesesystem is statistically indistinguishablefrom a non-customized commerciallyavailable MT system in the same domain.IntroductionIn the wake of the pioneering work of Nagao(1984), Brown et al (1990) and Sato andNagao (1990), Machine Translation (MT)research has increasingly focused on the issueof how to acquire translation knowledge fromaligned parallel texts.
While much of thisresearch effort has focused on acquisition ofcorrespondences between individual lexicalitems or between unstructured strings of words,closer attention has begun to be paid to thelearning of structured phrasal units: Yamamotoand Matsumoto (2000), for example, describe amethod for automatically extracting correspon-dences between dependency relations inJapanese and English.
Similarly, Imamura(2001a, 2001b) seeks to match correspondingJapanese and English phrases containinginformation about hierarchical structures,including partially completed parses.Yamamoto and Matsumoto (2000) explicitlyassume that dependency relations betweenwords will generally be preserved acrosslanguages.
However, when languages are asdifferent as Japanese and English with respectto their syntactic and informational structures,grammatical or dependency relations may notalways be preserved: the English sentence ?thenetwork failed?
has quite a differentgrammatical structure from its Japanesetranslation equivalent ??????????????
?a defect arose in the network.?
Oneissue for example-based MT, then, is to capturesystematic divergences through genericlearning applicable to multiple language pairs.In this presentation we describe the MSR-MTEnglish-Japanese system, an example-basedMT system that learns structured phrase-sizedtranslation units.
Unlike the systems discussedin Yamamoto and Matsumoto (2000) andImamura (2001a, 2001b), MSR-MT places thelocus of translation knowledge acquisition at agreater level of abstraction than surfacerelations, pushing it into a semantically-motivated layer called LOGICAL FORM (LF)(Heidorn 2000; Campbell & Suzuki 2002a,2002b).
Abstraction has the effect ofneutralizing (or at least minimizing) differencesin word order and syntactic structure, so thatmappings between structural relationsassociated with lexical items can readily beacquired within a general MT architecture.In Section 1 below, we present an overview ofthe characteristics of the system, with specialreference to English-Japanese MT.
Section 2discusses a class of structures learned throughphrase alignment, Section 3 presents the resultsof comparative evaluation, and Section 4 somefactors that contributed to the evaluation results.Section 5 addresses directions for future work.1 The MSR-MT SystemThe MSR-MT English-Japanese system is ahybrid example-based machine translationsystem that employs handcrafted broad-coverage augmented phrase structure grammarsfor parsing, and statistical and heuristictechniques to capture translation knowlege andfor transfer between languages.
The parsers aregeneral purpose: the English parser, forexample, forms the core of the grammarcheckers used in Microsoft Word (Heidorn2000).
The Japanese grammar utilizes much ofthe same codebase, but contains language-specific grammar rules and additional featuresowing to the need for word-breaking inJapanese (Suzuki et al 2000; Kacmarcik et al2000).
These parsers are robust in that if theanalysis grammar fails to find an appropriateparse, it outputs a best-guess ?fitted?
parse.System development is not confined toEnglish-Japanese: MSR-MT is part of abroader natural language processing projectinvolving three Asian languages (Japanese,Chinese, and Korean) and four Europeanlanguages (English, French, German, andSpanish).
Development of the MSR-MTsystems proceeds more or less simultaneouslyacross these languages and in multipledirections, including Japanese-English.
TheSpanish-English version of MSR-MT has beendescribed in Richardson et al 2001a, Richardsonet al2001b, and the reader is referred to thesepapers for more information concerningalgorithms employed during phrase alignment.A description of the French-Spanish MTsystem is found in Pinkham & Smets.
2002.1.1 Training DataMSR-MT requires that a large corpus ofaligned sentences be available as examples fortraining.
For English-Japanese MT, the systemcurrently trains on a corpus of approximately596,000 pre-aligned sentence pairs.
About274,000 of these are sentence pairs extractedfrom Microsoft technical documentation thathad been professionally translated fromEnglish into Japanese.
The remaining 322,000are sentence examples or sentence fragmentsextracted from electronic versions of studentdictionaries.11.2  Logical FormMSR-MT employs a post-parsing layer ofsemantic representation called LOGICAL FORM(LF) to handle core components of thetranslation process, namely acquisition andstorage of translation knowledge, transferbetween languages, and generation of targetoutput.
LF can be viewed as a representation ofthe various roles played by the content wordsafter neutralizing word order and localmorphosyntactic variation (Heidorn 2000;Campbell & Suzuki 2002a; 2002b).
These canbe seen in the Tsub (Typical Subject) and Tobj(Typical Object) relations in Fig.
1 in thesentence ?Mary eats pizza?
and its Japanesecounterpart.
The graphs are simplified forexpository purposes.Although our hypothesis is that equivalentsentences in two languages will tend toresemble each other at LF more than they do inthe surface parse, we do not adopt a na?vereductionism that would attempt to make LFscompletely identical.
In Fig.
2, for example, theLFs of the quantified nouns differ in that theJapanese LF preserves the classifier, yet aresimilar enough that learning the mappingbetween the two structures is straightforward.It will be noted that since the LF for eachlanguage stores words or morphemes of thatlanguage, this level of representation is not inany sense an interlingua.1 Kodansha?s Basic English-Japanese Dictionary,1999; Kenkyusha?s New College Japanese-EnglishDictionary, 4th Edition, 1995 ; and Kenkyusha?sNew College English-Japanese Dictionary, 6thEdition, 1994.Fig.
1  Canonical English and JapaneseLogical Forms1.3  Mapping Logical FormsIn the training phase, MSR-MT learns transfermappings from the sentence-aligned bilingualcorpus.
First, the system deploys thegeneral-purpose parsers to analyze the Englishand Japanese sentence pairs and generate LFsfor each sentence.
In the next step, an LFalignment algorithm is used to match sourcelanguage and target language LFs at thesub-sentence level.The LF alignment algorithm first establishestentative lexical correspondences betweennodes in the source and target LFs on the basisof lexical matching over dictionary informationand approximately 31,000 ?word associations,?that is, lexical mappings extracted from thetraining corpora using statistical techniquesbased on mutual information (Moore 2001).From these possible lexical correspondences,the algorithm uses a small grammar of(language-pair-independent) rules to align LFnodes on lexical and structural principles.
Thealigned LF pairs are then partitioned intosmaller aligned LF segments, with individualnode mappings captured in a relationship wecall ?sublinking.?
Finally, the aligned LFsegments are filtered on the basis of frequency,and compiled into a database known as aMindnet.
(See Menezes & Richardson 2001 for adetailed description of this process.
)The Mindnet is a general-purpose database ofsemantic information (Richardson et al 1998)that has been repurposed as the primaryrepository of translation information for MTapplications.
The process of building theMindnet is entirely automated; there is nohuman vetting of candidate entries.
At the endof a typical training session, 1,816,520 transferpatterns identified in the training corpus mayyield 98,248 final entries in the Mindnet.
Onlythe output of successful parses is consideredfor inclusion, and each mapping of LFsegments must have been encountered twice inthe corpus before it is incorporated into theMindnet.In the Mindnet, LF segments from the sourcelanguage are represented as linked to thecorresponding LF segment from the targetlanguages.
These can be seen in Figs.
3 and 4,discussed below in Section 2.1.4  Transfer and GenerationAt translation time, the broad-coverage sourcelanguage parser processes the English inputsentence, and creates a source-language LF.This LF is then checked against the Mindnetentries.
2  The best matching structures areextracted and stitched together determinist-ically into a new target-language ?transferredLF?
that is then submitted to the Japanesesystem for generation of the output string.The generation module is language-specificand used for both monolingual generation andMT.
In the context of MT, generation takes asinput the transferred LF and converts it into abasic syntactic tree.
A small set of heuristicrules preprocesses the transferred LF to?nativize?
some structural differences, such aspro-drop phenomena in Japanese.
A series ofcore generation rules then applies to the LF tree,transforming it into a Japanese sentence string.Generation rules operate on a single tree only,are application-independent and are developedin a monolingual environment (see Aikawa etal.
2001a, 2001b for further details.
)Generation of inflectional morphology is alsohandled in this component.
The generationcomponent has no explicit knowledge of thesource language.2 Acquisition of Complex StructuralMappingsThe generalization provided by LF makes itpossible for MSR-MT to handle complexstructural relations in cases where English andJapanese are systematically divergent.
This is2 MSR-MT resorts to lexical lookup only when aterm is not found in the Mindnet.
The handcrafteddictionary is slated for replacement by purelystatistically generated data.Fig.
2  Cross-Linguistic Variation in LogicalFormillustrated by the sample training pair in thelefthand column of Table 1.
In Japanese,inanimate nouns tend to be avoided as subjectsof transitive verbs; the word ?URL?, which issubject in the English sentence, thuscorresponds to an oblique relation in theJapanese.
(The Japanese sentence, although anatural and idiomatic translation of the English,is literally equivalent to ?one can access publicfolders with this URL.?
)Nonetheless, mappings turn out to be learnableeven where the information is structured soradically differently.
Fig.
3 shows the Mindnetentry for ?provide,?
which is result of trainingon sentence pairs like those in the lefthandcolumn of Table 1.
The system learns not onlythe mapping between the phrase ?provideaccess?
and the potential form of ????
?access?, but also the crucial sublinking of theTsub node of the English sentence and the nodeheaded by ?
(underspecified for semanticrole) in the Japanese.
At translation time thesystem is able to generalize on the basis of thefunctional roles stored in the Mindnet; it cansubstitute lexical items to achieve a relativelynatural translation of similar sentences such asshown in the right-hand side of Table 1.Differences of the kind seen in Fig 3 areendemic in our Japanese and English corpora.Fig.
4 shows part of the example Mindnet entryfor the English word ?fail?
referred to in theIntroduction, which exhibits another mismatchin grammatical roles somewhat similar to thatin observed in Fig.
3.
Here again, the lexicalmatching and generic alignment heuristics haveallowed the match to be captured into theMindnet.
Although the techniques employedmay have been informed by analysis oflanguage-specific data, they are in principle ofgeneral application.3 EvaluationIn May 2002, we compared output of theMSR-MT English-Japanese system with acommercially available desktop MT system.33 Toshiba?s The Honyaku Office v2.0 desktop MTsystem was selected for this purpose.
The Honyakuis a trademark of the Toshiba Corporation.
Anotherdesktop system was also considered for evaluation;however, comparative evaluation with that systemindicated that the Toshiba system performedmarginally, though not significantly, better on ourtechnical documentation.Training Data Translation OutputThis URL provides access to public folders.This computer provides access to the internet.??
URL ??????
???????????????????????????????????????
?Table 1.
Sample Input and OutputFig.
3.
Part of the Mindnet Entry for ?provide?Fig.
4.
Part of the Mindnet Entry for ?fail?A total of 238 English-Japanese sentence pairswere randomly extracted from held-outsoftware manual data of the same kinds usedfor training the system.
4  The Japanesesentences, which had been translated by humantranslators, were taken as reference sentences(and were assumed to be correct translations).The English sentences were then translated bythe two MT systems into Japanese for blindevaluation performed by seven outside vendorsunfamiliar with either system?s characteristics.No attempt was made to constrain or modifythe English input sentences on the basis oflength or other characteristics.
Both systemsprovided a translation for each sentence.5For each of the Japanese reference sentences,evaluators were asked to select whichtranslation was closer to the reference sentence.A value of +1 was assigned if the evaluatorconsidered MSR-MT output sentence betterand ?1 if they considered the comparisonsystem better.
If two translated sentences wereconsidered equally good or bad in comparison4  250 sentences were originally selected forevaluation; 12 were later discarded when it wasdiscovered by evaluators that the Japanese referencesentences (not the input sentences) were defectiveowing to the presence of junk characters (mojibake)and other deficiencies.5 In MSR-MT, Mindnet coverage is sufficientlycomplete with respect to the domain that anuntranslated sentence normally represents acomplete failure to parse the input, typically owingto excessive length.to the reference, a value of 0 was assigned.
Onthis metric, MSR-MT scored slightly worsethan the comparison system rating of ?0.015.At a two-way confidence measure of +/?0.16,the difference between the systems isstatistically insignificant.
By contrast, anearlier evaluation conducted in October 2001yielded a score of ?0.34 vis-?-vis thecomparison system.In addition, the evaluators were asked to ratethe translation quality on an absolute scale of 1through 4, according to the following criteria:1.
Unacceptable: Absolutely not comprehen-sible and/or little or no information trans-ferred accurately.2.
Possibly Acceptable: Possibly compre-hensible (given enough context and/ortime to work it out); some informationtransferred accurately.3.
Acceptable: Not perfect, but definitelycomprehensible, and with accurate transferof all important information.4.
Ideal: Not necessarily a perfect translation,but grammatically correct, and with allinformation accurately transferred.On this absolute scale, neither systemperformed exceptionally well: MSR-MT scoredan average 2.25 as opposed to 2.32 for thecomparison system.
Again, the differencebetween the two is statistically insignificant.
Itshould be added that the comparison presentedhere is not ideal, since MSR-MT was trainedprincipally on technical manual sentences,EvaluationDateTransfersper SentenceNodesPer TransferOct.
2001 5.8 1.6May 2002 6.7 2.0Table 2.
Number of Transfers and Nodes Transferred per SentenceEvaluation Date Word Class Total FromMindnetFromDictionaryUntranslatedPrepositions 410 17.1% 77.1% 5.9%Oct.
2001(250 sentences) Content Lemmas 2124 88.4% 7.8% 3.9%Prepositions 842 61.9% 37.5% 0.6%May 2002(520 sentences) Content Lemmas 4429 95.9% 1.5% 2.6%Table 3.
Sources of Different Word Classes at Transferwhile the comparison system was notspecifically tuned to this corpus.
Accordinglythe results of the evaluation need to beinterpreted narrowly, as demonstrating that:l  A viable example-based English-JapaneseMT system can be developed that appliesgeneral-purpose alignment rules to semanticrepresentations; andl  Given general-purpose grammars, arepresentation of what the sentence means,and suitable learning techniques, it ispossible to achieve in a domain, resultsanalogous with those of a maturecommercial product, and within a relativelyshort time frame.4 DiscussionIt is illustrative to consider some of the factorsthat contributed to these results.
Table 2 showsthe number of transfers per sentence and thenumber of LF nodes per transfer in versions ofthe system evaluated in October 2001 and May2002.
Not only is the MSR-MT finding moreLF segments in the Mindnet, crucially thenumber of nodes transferred has also grown.An average of two connected nodes are nowtransferred with each LF segment, indicatingthat the system is increasingly learning itstranslation knowledge in terms of complexstructures rather than simple lexicalcorrespondences.It has been our experience that the greaterMSR-MT?s reliance on the Mindnet, the betterthe quality of its output.
Table 2 shows thesources of selected word classes in the twosystems.
Over time, reliance on the Mindnethas increased overall, while reliance ondictionary lookup has now diminished to thepoint where, in the case of content words, itshould be possible to discard the handcrafteddictionary altogether and draw exclusively onthe contextualized resources of the Mindnetand statistically-generated lexical data.
Alsostriking in Table 2 is the gain shown inpreposition handling: a majority of Englishprepositions are now being transferred only inthe context of LF structures found in theMindnet.The important observation underlying the gainsshown in these tables is that they haveprimarily been obtained either as the result ofLF improvements in English or Japanese (i.e.,from better sentence analysis or LFconstruction), or as a result of genericimprovements to the algorithms that mapbetween LF segments (notably bettercoindexation and improved learning ofmappings involving lexical attributes).
In thelatter case, although certain modifications mayhave been driven by phenomena observedbetween Japanese and English, the heuristicsapply across all seven languages on which ourgroup is currently working.
Adaptation to thecase of Japanese-English MT usually takes theform of loosening rather than tightening ofconstraints.5 Future WorkUltimately it is probably desirable that thesystem?s mean absolute score should approach3 (Acceptable) within the training domain: thisis a high quality bar that is not attained byoff-the-shelf systems.
Much of the work will beof a general nature: improving the parses andLF structures of source and target languageswill bring automatic benefits to both alignmentof structured phrases and runtime translation.For example, efforts are currently underway toredesign LF to better represent scopalproperties of quantifiers and negation(Campbell & Suzuki 2002a, 2002b).Work to improve the quality of alignment andtransfer is ongoing within our group.
Inaddition to improvement of alignment itself,we are also exploring techniques to ensure thatthe transferred LF is consistent with knownLFs in the target language, with the eventualgoal of obviating the need for heuristic rulesused in preprocessing generation.
Again, theseimprovements are likely to be system-wide andgeneric, and not specific to theEnglish-Japanese case.ConclusionsUse of abstract semantically-motivatedlinguistic representations (Logical Form)permits MSR-MT to align, store, and translatesentence patterns reflecting widely varyingsyntactic and information structures inJapanese and English, and to do so within theframework of a general-purpose NLParchitecture applicable to both Europeanlanguages and Asian languages.Our experience with English-Japanese examplebased MT suggests that the problem of MTamong Asian languages may be recast as aproblem of implementing a general represen-tation of structured meaning across languagesthat neutralizes differences where possible, andwhere this is not possible, readily permitsresearchers to identify general-purposetechniques of bridging the disparities that areviable across multiple languages.AcknowledgementsWe would like to thank Bill Dolan and RichCampbell for their comments on a draft of thispaper.
Our appreciation also goes to themembers of the Butler Hill Group for theirassistance with conducting evaluations.ReferencesAikawa, T., M. Melero, L. Schwartz, and A. Wu.2001a.
Multilingual sentence generation.
InProceedings of 8th European Workshop onNatural Language Generation, Toulouse, France.Aikawa, T., M. Melero, L. Schwartz, and A. Wu.2001b.
Sentence generation for multilingualmachine translation.
In Proceedings of the MTSummit VIII, Santiago de Compostela, Spain.Brown, P. F.,  J. Cocke, S. A. D. Pietra, V. J. D.Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer,and P. S. Roossin.
1990.
A statistical approach tomachine translation.
Computational Linguistics,16(2): 79-85.Campbell, R. and H. Suzuki.
2002a.
Language-neutral representation of syntactic structure.
InProceedings of the First International Workshopon Scalable Natural Language Understanding(SCANALU 2002), Heidelberg, Germany.Campbell, R. and H. Suzuki.
2002b.
Language-Neutral Syntax: An Overview.
Microsoft ResearchTechreport: MSR-TR-2002-76.Heidorn, G. 2000.
Intelligent writing assistance.
InR.
Dale, H. Moisl and H. Somers (eds.
), AHandbook of Natural Language Processing:Techniques and Applications for the Processingof Language as Text.
Marcel Dekker, New York.pp.
181-207.Imamura, K. 2001a.
Application of translationknowledge acquired by ierarchical phrasealignment.
In Proceedings of TMI.Imamura, K. 2001b.
Hierarchical phrase alignmentharmonized with parsing.
In Proceedings ofNLPRS, Tokyo, Japan, pp 377-384.Kacmarcik, G., C. Brockett, and H. Suzuki.
2000.Robust segmentation of Japanese text into alattice for parsing.
In Proceedings of COLING2000, Saarbrueken, Germany, pp.
390-396.Menezes, A. and S. D. Richardson.
2001.
Abest-first alignment algorithm for automaticextraction of transfer mappings from bilingualcorpora.
In Proceedings of the Workshop onData-driven Machine Translation at 39th AnnualMeeting of the Association for ComputationalLinguistics, Toulouse, France, pp.
39-46.Moore, R. C. 2001.
Towards a simple and accuratestatistical approach to learning translationrelationships among words," in Proceedings,Workshop on Data-driven Machine Translation,39th Annual Meeting and 10th Conference of theEuropean Chapter, Association forComputational Linguistics, Toulouse, France, pp.79-86.Nagao, M. 1984.
A framework of a mechanicaltranslation between Japanese and English byanalogy principle.
In A. Elithorn.
and R.
Bannerji(eds.)
Artificial and Human Intelligence.
NatoPublications.
pp.
181-207.Pinkham, J., M. Corston-Oliver, M. Smets and M.Pettenaro.
2001.
Rapid Assembly of a Large-scaleFrench-English MT system.
In Proceedings of theMT Summit VIII, Santiago de Compostela, Spain.Pinkham, J., and M. Smets.
2002.
Machinetranslation without a bilingual dictionary.
InProceedings of the 9th International Conferenceon Theoretical and Methodological Issues inMachine Translation.
Kyoto, Japan, pp.
146-156.Richardson, S. D., W. B. Dolan, A. Menezes, and M.Corston-Oliver.
2001.
Overcoming thecustomization bottleneck using example-basedMT.
In Proceedings, Workshop on Data-drivenMachine Translation, 39th Annual Meeting and10th Conference of the European Chapter,Association for Computational Linguistics.Toulouse, France, pp.
9-16.Richardson, S. D., W. B. Dolan, A. Menezes, and J.Pinkham.
2001.
Achieving commercial-qualitytranslation with example-based methods.
InProceedings of MT Summit VIII, Santiago DeCompostela, Spain, pp.
293-298.Richardson, S. D., W. B. Dolan, and L.Vanderwende.
1998 MindNet: Acquiring andstructuring semantic information from text,ACL-98.
pp.
1098-1102.Sato, S. and Nagao M. 1990.
Towardmemory-based translation.
In Proceedings ofCOLING 1990, Helsinki, Finland, pp.
247-252.Suzuki, H., C. Brockett, and G. Kacmarcik.
2000.Using a broad-coverage parser for word-breakingin Japanese.
In Proceedings of COLING 2000,Saarbrueken, Germany, pp.
822-827.Yamamoto K., and Y Matsumoto.
2000.Acquisition of phrase-level bilingualcorrespondence using dependency structure.
InProceedings of COLING 2000, Saarbrueken,Germany, pp.
933-939.
