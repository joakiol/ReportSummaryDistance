Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 72?80,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPData Cleaning for Word AlignmentTsuyoshi OkitaCNGL / School of ComputingDublin City University, Glasnevin, Dublin 9tokita@computing.dcu.ieAbstractParallel corpora are made by human be-ings.
However, as an MT system is anaggregation of state-of-the-art NLP tech-nologies without any intervention of hu-man beings, it is unavoidable that quite afew sentence pairs are beyond its analy-sis and that will therefore not contributeto the system.
Furthermore, they in turnmay act against our objectives to make theoverall performance worse.
Possible unfa-vorable items are n : m mapping objects,such as paraphrases, non-literal transla-tions, and multiword expressions.
Thispaper presents a pre-processing methodwhich detects such unfavorable items be-fore supplying them to the word alignerunder the assumption that their frequencyis low, such as below 5 percent.
We showan improvement of Bleu score from 28.0to 31.4 in English-Spanish and from 16.9to 22.1 in German-English.1 IntroductionPhrase alignment (Marcu and Wong, 02) has re-cently attracted researchers in its theory, althoughit remains in infancy in its practice.
However, aphrase extraction heuristic such as grow-diag-final(Koehn et al, 05; Och and Ney, 03), which is a sin-gle difference between word-based SMT (Brownet al, 93) and phrase-based SMT (Koehn et al,03) where we construct word-based SMT by bi-directional word alignment, is nowadays consid-ered to be a key process which leads to an over-all improvement of MT systems.
However, tech-nically, this phrase extraction process after wordalignment is known to have at least two limita-tions: 1) the objectives of uni-directional wordalignment is limited only in 1 : n mappings and2) an atomic unit of phrase pair used by phrase ex-traction is thus basically restricted in 1 : n or n : 1with small exceptions.Firstly, the posterior-based approach (Liang,06) looks at the posterior probability and partiallydelays the alignment decision.
However, this ap-proach does not have any extension in its 1 : nuni-directional mappings in its word alignment.Secondly, the aforementioned phrase alignment(Marcu and Wong, 02) considers the n : m map-ping directly bilingually generated by some con-cepts without word alignment.
However, this ap-proach has severe computational complexity prob-lems.
Thirdly, linguistic motivated phrases, suchas a tree aligner (Tinsley et al, 06), provides n : mmappings using some information of parsing re-sults.
However, as the approach runs somewhat ina reverse direction to ours, we omit it from the dis-cussion.
Hence, this paper will seek for the meth-ods that are different from those approaches andwhose computational cost is cheap.n : m mappings in our discussion include para-phrases (Callison-Burch, 07; Lin and Pantel, 01),non-literal translations (Imamura et al, 03), mul-tiword expressions (Lambert and Banchs, 05), andsome other noise in one side of a translation pair(from now on, we call these ?outliers?, meaningthat these are not systematic noise).
One com-mon characteristic of these n : m mappings isthat they tend to be so flexible that even an ex-haustive list by human beings tends to be incom-plete (Lin and Pantel, 01).
There are two caseswhich we should like to distinguish: when we useexternal resources and when we do not.
For ex-ample, Quirk et al employ external resources bydrawing pairs of English sentences from a compa-rable corpus (Quirk et al, 04), while Bannard andCallison-Burch (Bannard and Callison-Burch, 05)identified English paraphrases by pivoting throughphrases in another language.
However, in this pa-per our interest is rather the case when our re-sources are limited within our parallel corpus.72Imamura et al (Imamura et al, 03), on the otherhand, do not use external resources and present amethod based on literalness measure called TCR(Translation Correspondence Rate).
Let us de-fine literal translation as a word-to-word transla-tion, and non-literal translation as a non word-to-word translation.
Literalness is defined as a de-gree of literal translation.
Literalness measure ofImamura et al is trained from a parallel corpususing word aligned results, and then sentences areselected which should either be translated by a ?lit-eral translation?
decoder or by a ?non-literal trans-lation?
decoder based on this literalness measure.Apparently, their definition of literalness measureis designed to be high recall since this measureincorporates all the possible correspondence pairs(via realizability of lexical mappings) rather thanall the possible true positives (via realizability ofsentences).
Adding to this, the notion of literaltranslation may be broader than this.
For exam-ple, literal translation of ?C?est la vie.?
in Frenchis ?That?s life.?
or ?It is the life.?
in English.If literal translation can not convey the originalmeaning correctly, non-literal translation can beapplied: ?This is just the way life is.
?, ?That?s howthings happen.
?, ?Love story.
?, and so forth.
Non-literal translation preserves the original meaning1as much as possible, ignoring the exact word-to-word correspondence.
As is indicated by this ex-ample, the choice of literal translation or non-literal translation seems rather a matter of trans-lator preference.This paper presents a pre-processing method us-ing the alternative literalness score aiming for highprecision.
We assume that the percentages of thesen : m mappings are relatively low.
Finally, itturned out that if we focus on outlier ratio, thismethod becomes a well-known sentence cleaningapproach.
We refer to this in Section 5.This paper is organized as follows.
Section 2outlines the 1 : n characteristics of word align-ment by IBM Model 4.
Section 3 reviews anatomic unit of phrase extraction.
Section 4 ex-plains our Good Points Algorithm.
Experimen-tal results are presented in Section 5.
Section 6discusses a sentence cleaning algorithm.
Section7 concludes and provides avenues for further re-search.1Dictionary goes as follows: something that you say whensomething happens that you do not like but which you haveto accept because you cannot change it [Cambridge IdiomsDictionary 2nd Edition, 06].CBADFigure 1: Figures A and C show the results ofword alignment for DE-EN where outliers de-tected by Algorithm 1 are shown in blue at the bot-tom.
We check all the alignment cept pairs in thetraining corpus inspecting so-called A3 final filesby type of alignment from 1:1 to 1:13 (or NULLalignment).
It is noted that outliers are minisculein A and C because each count is only 3 percent.Most of them are NULL alignment or 1:1 align-ment, while there are small numbers of alignmentswith 1:3 and 1:4 (up to 1:13 in the DE-EN direc-tion in Figure A).
In Figure C, 1:11 is the greatest.Figure B and D show the ratio of outliers over allthe counts.
Figure B shows that in the case of 1:10alignments, 1/2 of the alignments are consideredto be outliers by Algorithm 1, while 100 percentof alignment from 1:11 to 1:13 are considered tobe outliers (false negative).
Figure D shows that inthe case of EN-DE, most of the outlier ratios areless than 20 percent.2 1 : n Word AlignmentOur discussion of uni-directional alignments ofword alignment is limited to IBM Model 4.Definition 1 (Word alignment task) Let eibethe i-th sentence in target language, e?i,jbe the j-th word in i-th sentence, and e?ibe the i-th word inparallel corpus (Similarly for fi,?fi,j, and ?fi).
Let|ei| be a sentence length of ei, and similarly for|fi|.
We are given a pair of sentence aligned bilin-gual texts (f1, e1), .
.
.
, (fn, en) ?
X ?
Y , wherefi= (?fi,1, .
.
.
,?fi,|fi|) and ei= (e?i,1, .
.
.
, e?i,|ei|).It is noted that eiand fimay include more thanone sentence.
The task of word alignment is tofind a lexical translation probability p?fi: e?i?p?fj(e?i) such that ?p?fj(e?i) = 1 and ?e?i: 0 ?p?fj(e?i) ?
1 (It is noted that some models such73to my regret i cannot go today .i am sorry that i cannot visit today .it is a pity that i cannot go today .i am sorry that i cannot visit today .it is a pity that i cannot go today .sorry , today i will not be availableSource LanguageGIZA++ alignment results for IBM Model 4i NULL 0.667cannot available 0.272it am 1is am 1sorry go 0.667, go 1that regret 0.25cannot regret 0.18visit regret 1regret not 1be pity 1available pity 1cannot sorry 0.55go sorry 0.667am to 1sorry to 0.33to , 1my , 1will is 1not is 1a that 1pity that 1today .
1. .
1i cannot 0.33that cannot 0.75Target Languageto my regret i cannot go today .sorry , today i will not be availableFigure 2: Example shows an example alignmentof paraphrases in a monolingual case.
Source andtarget use the same set of sentences.
Results showthat only the matching between the colon is cor-rect3.as IBM Model 3 and 4 have deficiency problems).It is noted that there may be several words insource language and target language which do notmap to any words, which are called unaligned (ornull aligned) words.
Triples ( ?fi, e?i, p?fi(e?1)) (or(?fi, e?i,?
log10p?fi(e?1))) are called T-tables.As the above definition shows, the purpose ofthe word alignment task is to obtain a lexicaltranslation probability p( ?fi|e?i), which is a 1 : nuni-directional word alignment.
The initial ideaunderlying the IBM Models, consisting of fivedistinctive models, is that it introduces an align-ment function a(j|i), or alternatively the distor-tion function d(j|i) or d(j ?
?i), when the task isviewed as a missing value problem, where i and jdenote the position of a cept in a sentence and ?idenotes the center of a cept.
d(j|i) denotes a dis-tortion of the absolute position, while d(j?
?i) de-notes the distortion of relative position.
Then thismissing value problem can be solved by EM algo-rithms : E-step is to take expectation of all the pos-sible alignments and M-step is to estimate maxi-mum likelihood of parameters by maximizing theexpected likelihood obtained in the E-step.
Thesecond idea of IBM Models is in the mechanismof fertility and a NULL insertion, which makes theperformance of IBM Models competitive.
Fertilityand a NULL insertion is used to adjust the length3It is noted that there might be a criticism that this is not afair comparison because we do not have sufficient data.
Un-der a transductive setting (where we can access the test data),we believe that our statement is valid.
Considering the natureof the 1 : n mapping, it would be quite lucky if we obtainn : m mapping after phrase extraction (Our focus is not onthe incorrect probability, but rather on the incorrect match-ing.
)n when the length of the source sentence is differ-ent from this n. Fertility is a mechanism to aug-ment one source word into several source wordsor delete a source word, while a NULL insertionis a mechanism of generating several words fromblank words.
Fertility uses a conditional probabil-ity depending only on the lexicon.
For example,the length of ?today?
can be conditioned only onthe lexicon ?today?.As is already mentioned, the resulting align-ments are 1 : n (shown in the upper figure inFigure 1).
For DE-EN News Commentary cor-pus, most of the alignments fall in either 1:1 map-ping or NULL mappings whereas small numbersare 1:2 mappings and miniscule numbers are from1:3 to 1:13.
However, this 1 : n nature of wordalignment will cause problems if we encountern : m mapping objects, such as a paraphrase, non-literal translation, or multiword expression.
Figure2 shows such difficulties where we show a mono-lingual paraphrase.
Without loss of generality thiscan be easily extended to bilingual paraphrases.
Inthis case, results of word alignment are completelywrong, with the exception of the example consist-ing of a colon.
Although these paraphrases, non-literal translations, and multiword expressions donot always become outliers, they may face thepotential danger of producing the incorrect wordalignments with incorrect probabilities.3 Phrase Extraction and Atomic Unit ofPhrasesThe phrase extraction is a process to exploitphrases for a given bi-directional word alignment(Koehn et al, 05; Och and Ney, 03).
If we focus onits generative process, this would become as fol-lows: 1) add intersection of two word alignmentsas an alignment point, 2) add new alignment pointsthat exist in the union with the constraint that anew alignment point connects at least one previ-ously unaligned word, 3) check the unaligned row(or column) as unaligned row (or column, respec-tively), 4) if n alignment points are contiguous inhorizontal (or vertical) direction we consider thatthis is a contiguous 1 : n (or n : 1) phrase pair(let us call these type I phrase pairs), 5) if a neigh-borhood of a contiguous 1 : n phrase pair is (an)unaligned row(s) or (an) unaligned column(s) wegrow this region (with consistency constraint) (letus call these type II phrase pair), and 6) we con-sider all the diagonal combinations of type I and74type II phrase pairs generatively.The atomic unit of type I phrase pairs is 1 : nor n : 1, while that of type II phrase pairs is n : mif unaligned row(s) and column(s) exist in neigh-borhood.
So, whether they form a n : m map-ping or not depends on the existence of unalignedrow(s) and column(s).
And at the same time, n orm should be restricted to a small value.
There isa chance that a n : m phrase pair can be createdin this way.
This is because around one third ofword alignments, which is quite a large figure, are1 : 0 as is shown in Figure 1.
Nevertheless, ourconcern is if the results of word alignment is verylow quality, e.g.
similar to the situation depictedin Figure 2, this mechanism will not work.
Fur-thermore, this mechanism is only restricted in theunaligned row(s) and column(s).4 Our Approach: Good Points ApproachOur approach aims at removing outliers by the lit-eralness score, which we defined in Section 1, be-tween a pair of sentences.
Sentence pairs with lowliteralness score should be removed.
Followingtwo propositions are the theory behind this.
Leta word-based MT system be MWBand a phrase-based MT system be MPB.
Then,Proposition 1 Under an ideal MT system MPB, aparaphrase is an inlier (or realizable), andProposition 2 Under an ideal MT system MWB,a paraphrase is an outlier (or not realizable).Based on these propositions, we could assumethat if we measure the literalness score under aword-based MT MWBwe will be able to deter-mine the degree of outlier-ness whatever the mea-sure we use for it.
Hence, what we should do is,initially, to score it under a word-based MT MWBusing Bleu, for example.
(Later we replace it witha variant of Bleu, i.e.
cumulative n-gram score).However, despite Proposition 1, our MT systemat hand is unfortunately not ideal.
What we cancurrently do is the following: if we witness badsentence-based scores in word-based MT, we canconsider our MT system failing to incorporating an : m mapping object for those sentences.
Laterin our revised version, we use both of word-basedMT and phrase-based MT.
The summary of ourfirst approach becomes as follows: 1) employingthe mechanism of word-based MT trained on thesame parallel corpus, we measure the literalnessbetween a pair of sentences, 2) we use the variantsFigure 3: Left figure shows sentence-based Bleuscore of word-based SMT and right figure showsthat of phrase-based SMT.
Each row shows the cu-mulative n-gram score (n = 1,2,3,4) and we useNews Commentary parallel corpus (DE-EN).Figure 4: Each row shows Bleu, NIST, and TER,while each column shows different language pairs(EN-ES, EN-DE and FR-DE).
These figures showthe scores of all the training sentences by theword-based SMT system.
In the row for Bleu,note that the area of rectangle shows the num-ber of sentence pairs whose Bleu scores are zero.
(There are a lot of sentence pairs whose Bleu scoreare zero: if we draw without en-folding the coor-dinate, these heights reach to 25,000 to 30,000.
)There is a smooth probability distribution in themiddle, while there are two non-smoothed connec-tions at 1.0 and 0.0.
Notice there is a small num-ber of sentences whose score is 1.0.
In the middlerow for NIST score, similarly, there is a smoothprobability distribution in the middle and we havea non-smoothed connection at 0.0.
In the bottomrow for TER score, the 0.0 is the best score unlikeBleu and NIST, and we omit scores more than 2.5in these figures.
(The maximum was 27.0.
)75of Bleu score as the measure of literalness, and3) based on this score, we reduce the sentences inparallel corpus.
Our algorithm is as follows:Algorithm 1 Good Points AlgorithmStep 1: Train word-based MT.Step 2: Translate all training sentences by theabove trained word-based MT decoder.Step 3: Obtain the cumulative X-gram score foreach pair of sentences where X is 4, 3, 2, and 1.Step 4: By the threshold described in Table 1,we produce new reduced parallel corpus.
(Step 5: Do the whole procedure of phrase-based SMT using the reduced parallel corpuswhich we obtain from Step 1 to 4.
)conf A1 A2 A3 A4Ours 0.05 0.05 0.1 0.21 0.12 0.1 0.23 0.1 0.2 0.3 0.54 0.05 0.1 0.2 0.45 0.22 0.3 0.4 0.66 0.25 0.4 0.5 0.77 0.2 0.4 0.5 0.88 0.6Table 1: Table shows our threshold where A1, A2,A3, and A4 correspond to the absolute cumulativen-gram precision value (n=1,2,3,4 respectively).In experiments, we compare ours with eight con-figurations above in Table 6.but this does not matter .peu importe !we may find ourselves there once again .va-t-il en e?tre de me?me cette fois-ci ?all for the good .et c?
est tant mieux !but if the ceo is not accountable , who is ?mais s?
il n?
est pas responsable , qui alors ?Table 2: Sentences judged as outliers by Algo-rithm 1 (ENFR News Commentary corpus).We would like to mention our motivation forchoosing the variant of Bleu.
In Step 3 weneed to set up a threshold in MWBto determineoutliers.
Natural intuition is that this distribu-tion takes some smooth distribution as Bleu takesweighted geometric mean.
However, as is showncumulative 4?gram scorescumulative 1?gram scorescumulative 2?gram scorescumulative 3?gram scores4?gram scores2?gram scores3?gram scores 3?gram scores1?gram scores2?gram scores 1?gram scoresof MT_PBof MT_PBof MT_PBof MT_PBof MT_WBof MT_WBof MT_WBof MT_WB4?gram scorescount countcount countFigure 5: Four figures show the sentence-basedcumulative n-gram scores: x-axis is phrase-basedSMT and y-axis is word-based SMT.
Focus is onthe worst point (0,0) where both scores are zero.Many points reside in (0,0) in cumulative 4-gramscores, while only small numbers of point residein (0,0) in cumulative 1-gram scores.in the first row of Figure 4, typical distribution ofwords in this space MWBis separated in two clus-ters: one looks like a geometric distribution andthe other one contains a lot of points whose valueis zero.
(Especially in the case of Bleu, if the sen-tence length is less than 3 the Bleu score is zero.
)For this reason, we use the variants of Bleu score:we decompose Bleu score in cumulative n-gramscore (n=1,2,3,4), which is shown in Figure 3.
It isnoted that the following relation holds: S4(e, f) ?S3(e, f) ?
S2(e, f) ?
S1(e, f) where e denotesan English sentence, f denotes a foreign sentence,and SXdenotes cumulative X-gram scores.
For 3-gram scores, the tendency to separate in two clus-ters is slightly decreased.
Furthermore, for 1-gramscores, the distribution approaches to normal dis-tribution.
We model P(outlier) taking care of thequantity of S2(e, f), where we choose 0.1: otherconfigurations in Table 1 are used in experiments.It is noted that although we choose the variantsof Bleu score, it is clear, in this context, that wecan replace Bleu with any other measure, such asMETEOR (Banerjee and Lavie, 05), NIST (Dod-dington, 02), GTM (Melamed et al, 03), TER(Snover et al, 06), labeled dependency approach(Owczarzak et al, 07) and so forth (see Figure 4).Table 2 shows outliers detected by Algorithm 1.Finally, a revised algorithm which incorporatessentence-based X-gram scores of phrase-basedMT is shown in Algorithm 2.
Figure 5 tells us76that there are many sentence pair scores actuallyimproved in phrase-based MT even if word-basedscore is zero.Algorithm 2 Revised Good Points AlgorithmStep 1: Train word-based MT for full parallelcorpus.
Translate all training sentences by theabove trained word-based MT decoder.Step 2: Obtain the cumulative X-gram scoreSWB,Xfor each pair of sentences where X is4, 3, 2, and 1 for word-based MT decoder.Step 3: Train phrase-based MT for full parallelcorpus.
Note that we do not need to run a wordaligner again in here, but use the results of Step1.
Translate all training sentences by the abovetrained phrase-based MT decoder.Step 4: Obtain the cumulative X-gram scoreSPB,Xfor each pair of sentences where X is4, 3, 2, and 1 for phrase-based MT decoder.Step 5: Remove sentences whose (SWB,2,SPB,2) = (0, 0).
We produce new reduced par-allel corpus.
(Step 6: Do the whole procedure of phrase-based SMT using the reduced parallel corpuswhich we obtain from Step 1 to 5.
)5 ResultsWe evaluate our algorithm using the News Com-mentary parallel corpus used in 2007 StatisticalMachine Translation Workshop shared task (cor-pus size and average sentence length are shown inTable 8).
We use the devset and the evaluation setalignment ENFR ESENgrow-diag-final 0.058 0.115union 0.205 0.116intersection 0.164 0.116Table 3: Performance of word-based MT systemin different alignment methods.
The above is be-tween ENFR and ESEN.pair ENFR FRENscore 0.205 0.176ENES ENDE DEEN0.276 0.134 0.208Table 4: Performance of word-based MT systemfor different language pairs with union alignmentmethod.provided by this workshop.
We use Moses (Koehnet al, 07) as the baseline system, with mgiza (Gaoand Vogel, 08) as its word alignment tool.
We doMERT in all the experiments below.Step 1 of Algorithm 1 produces, for a givenparallel corpus, a word-based MT.
We do this us-ing Moses with option max-phrase-length set to 1,alignment as union as we would like to extract thebi-directional results of word alignment with highrecall.
Although we have chosen union, other se-lection options may be possible as Table 3 sug-gests.
Performance of this word-based MT systemis as shown in Table 4.Step 2 is to obtain the cumulative n-gram scorefor the entire training parallel corpus by using theword-based MT system trained in Step 1.
Table 5shows the first two sentences of News Commen-tary corpus.
We score for all the sentence pairs.c score = [0.4213,0.4629,0.5282,0.6275]consider the number of clubs that havequalified for the european champions ?league top eight slots .conside?rons le nombre de clubs qui se sontqualifie?s parmi les huit meilleurs de la liguedes champions europenne .c score = [0.0000,0.0000,0.0000,0.3298]estonia did not need to ponder longabout the options it faced .l?
estonie n?
a pas eu besoin de longuementrflchir sur les choix qui s?
offraient a` elle .Table 5: Four figures marked as score shows thecumulative n-gram score from left to right.
Thefollowing EN and FR are the calculated sentencesused by word-based MT system trained on Step 1.In Step 3, we obtain the cumulative n-gramscore (shown in Figure 3).
As is already men-tioned, there are a lot of sentence pairs whose cu-mulative 4-gram score is zero.
In the cumulative3-gram score, this tendency is slightly decreased.For 1-gram scores, the distribution approaches tonormal distribution.
In Step 4, other than our con-figuration we used 8 different configurations in Ta-ble 6 to reduce our parallel corpus.Now we obtain the reduced parallel corpus.
InStep 5, using this reduced parallel corpus we car-ried out training of MT system from the begin-ning: we again started from the word alignment,followed by phrase extraction, and so forth.
Theresults corresponding to these configurations areshown in Table 6.
In Table 6, in the case of77ENES Bleu effective sent UNKBase 0.280 99.30 % 1.60%Ours 0.314 96.54% 1.61%1 0.297 56.21% 2.21%2 0.294 60.37% 2.09%3 0.301 66.20% 1.97%4 0.306 84.60% 1.71%5 0.299 56.12% 2.20%6 0.271 25.05% 2.40%7 0.283 35.28% 2.26%8 0.264 19.78% 4.22%DEEN % ENFR %Base 0.169 99.10% 0.180 91.81%Ours 0.221 96.42% 0.192 96.38%1 0.201 40.49% 0.187 49.37%2 0.205 48.53% 0.188 55.03%3 0.208 58.07% 0.187 61.22%4 0.215 83.10% 0.190 81.57%5 0.192 29.03% 0.180 31.52%6 0.174 17.69% 0.162 29.97%7 0.186 24.60% 0.179 30.52%8 0.177 18.29% 0.167 17.11%Table 6: Table shows Bleu score for ENES,DEEN, and ENFR: 0.314, 0.221, and 0.192, re-spectively.
All of these are better than baseline.Effective ratio can be considered to be the inlierratio, which is equivalent to 1 - (outlier ratio).
Thedetails for the baseline system are shown in Table8.ENES Bleu effective sentBase 0.280 99.30 %Ours 0.317 97.80 %DEEN Bleu effective sentBase 0.169 99.10 %Ours 0.218 97.14 %Table 7: This table shows results for the revisedGood Points Algorithm.English-Spanish our configuration discards 3.46percent of sentences, and the performance reaches0.314 which is the best among other configura-tions.
Similarly in the case of German-English ourconfiguration attains the best performance amongconfigurations.
It is noted that results for the base-line system are shown in Table 8 where we pickedup the score where n is 100.
It is noted that thebaseline system as well as other configurations useMERT.
Similarly, results for a revised Good PointsFigure 6: Three figures in the left show the his-togram of sentence length (main figures) and his-togram of sentence length of outliers (at the bot-tom).
(As the numbers of outliers are less than5 percent in each case, outliers are miniscule.
Inthe case of EN-ES, we can observe the blue smalldistributions at the bottom from 2 to 16 sentencelength.)
Three figures in the right show that if wesee this by ratio of outliers over all the counts, allof three figures tend to be more than 20 to 30 per-cent from 80 to 100 sentence length.
The lowertwo figures show that sentence length 1 to 4 tendto be more than 10 percent.Algorithm is shown in Table 7.6 DiscussionIn Section 1, we mentioned that if we aim at out-lier ratio using the indirect feature sentence length,this method reduces to a well-known sentencecleaning approach shown below in Algorithm 3.Algorithm 3 Sentence Cleaning AlgorithmRemove sentences with lengths greater than X(or remove sentences with lengths smaller thanX in the case of short sentences).This approach is popular although the reasonbehind why this approach works is not well un-derstood.
Our explanation is shown in the right-hand side of Figure 6 where outliers are shown atthe bottom (almost invisible) which are extractedby Algorithm 1.
The region that Algorithm 3 re-moves via sentence length X is possibly the regionwhere the ratio of outliers is high.This method is a high recall method.
Thismethod does not check whether the removed sen-tences are really sentences whose behavior is bador not.
For example, look at Figure 6 for sen-78X ENFR FREN ESEN DEEN ENDE10 0.167 0.088 0.143 0.097 0.07920 0.087 0.195 0.246 0.138 0.12730 0.145 0.229 0.279 0.157 0.13740 0.175 0.242 0.295 0.168 0.14250 0.229 0.250 0.297 0.170 0.14560 0.178 0.253 0.297 0.171 0.14670 0.179 0.251 0.298 0.170 0.14680 0.181 0.252 0.301 0.169 0.14790 0.180 0.252 0.297 0.171 0.147100 0.180 0.251 0.302 0.169 0.146# 51k 51k 51k 60k 60kave 21.0/23.8(EN/FR) 20.9/24.5(EN/ES)len 20.6/21.6(EN/DE)Table 8: Bleu score after cleaning of sen-tences with length greater than X.
The rowshows X, while the column shows the languagepair.
Parallel corpus is News Commentary par-allel corpus.
It is noted that the default set-ting of MAX SENTENCE LENTH ALLOWEDin GIZA++ is 101.tence length 10 to 30 where there are considerablymany outliers in the region that a lot of inliers re-side.
However, this method cannot cope with suchoutliers.
Instead, the method cope with the regionthat the outlier ratio is possibly high at both ends,e.g.
sentence length > 60 or sentence length < 5.The advantage is that sentence length informationis immediately available from the sentence whichis easy to implement.
The results of this algorithmis shown in Table 8 where we varies X and lan-guage pair.
This table also suggests that we shouldrefrain from saying that X = 60 is best or X = 80is best.7 Conclusions and Further WorkThis paper shows some preliminary results thatdata cleaning may be a useful pre-processing tech-nique for word alignment.
At this moment, we ob-serve two positive results, improvement of Bleuscore from 28.0 to 31.4 in English-Spanish and16.9 to 22.1 in German-English which are shownin Table 6.
Our method checks the realizability oftarget sentences in training sentences.
If we wit-ness bad cumulative X-gram scores we suspectthat this is due to some problems caused by then : m mapping objects during word alignment fol-lowed by phrase extraction process.Firstly, although we removed training sentenceswhose n-gram scores are low, we can dupli-cate such training sentences in word alignment.This method is appealing, but unfortunately if weuse mgiza or GIZA++, our training process of-ten ceased in the middle by unrecognized errors.However, if we succeed in training, the results of-ten seem comparable to our results.
Although wedid not supply back removed sentences, it is pos-sible to examine such sentences using the T-tablesto extract phrase pairs.Secondly, it seems that one of the key matterslies in the quantities of n : m mapping objectswhich are difficult to learn by word-based MT (orby phrase-based MT).
It is possible that such quan-tities are different depending on their languagepairs and on their corpora size.
A rough estimationis that this quantity may be somewhere less than10 percent (in FR-EN Hansard corpus, recall andprecision reach around 90 percent (Moore, 05)),or less than 5 percent (in News Commentary cor-pus, the best Bleu scores by Algorithm 1 are whenthis percentage is less than 5 percent ).
As furtherstudy, we intend to examine this issue further.Thirdly, this method has other aspects that itremoves discontinuous points: such discontinu-ous points may relate to the smoothness of opti-mization surface.
One of the assumptions of themethod such as Wang et al (Wang et al, 07) re-lates to smoothness.
Then, our method may im-prove their results, which is our further study.In addition, although our algorithm runs a wordaligner more than once, this process can be re-duced since removed sentences are less than 5 per-cent or so.Finally, we did not compare our method withTCR of Imamura.
In our case, the focus was 2-gram scores rather than other n-gram scores.
Weintend to investigate this further.8 AcknowledgementsThis work is supported by Science FoundationIreland (Grant No.
07/CE/I1142).
Thanks toYvette Graham and Sudip Naskar for proof read-ing, Andy Way, Khalil Sima?an, Yanjun Ma, andannonymous reviewers for comments, and Ma-chine Translation Marathon.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
ACL.79Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation With Im-proved Correlation With Human Judgments.
Work-shop On Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization.Peter F. Brown, Vincent J.D.
Pietra, Stephen A.D.Pietra, and Robert L. Mercer.
1993.
The Mathe-matics of Statistical Machine Translation: Parame-ter Estimation,?
Computational Linguistics, Vol.19,Issue 2.Chris Callison-Burch.
2007.
Paraphrasing and Trans-lation.
PhD Thesis, University of Edinburgh.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Transla-tion Using Paraphrases.
NAACL.Chris Callison-Burch, Trevor Cohn, and Mirella La-pala.
2008.
ParaMetric: An Automatic EvaluationMetric for Paraphrasing.
COLING.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from Incomplete Data via theEM algorithm.
Journal of the Royal Statistical Soci-ety.Yonggang Deng and William Byrne.
2005.
HMMWord and Phrase Alignment for Statistical MachineTranslation.
Proc.
Human Language TechnologyConference and Empirical Methods in Natural Lan-guage Processing.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
HLT.David A. Forsyth and Jean Ponce.
2003.
ComputerVision.
Pearson Education.Qin Gao and Stephan Vogel.
2008.
Parallel Imple-mentations of Word Alignment Tool.
Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing.Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto.2003.
Automatic Construction of Machine Trans-lation Knowledge Using Translation Literalness.EACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.HLT/NAACL.Philipp Koehn, Amittai Axelrod, Alexandra Birch,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
In-ternational Workshop on Spoken Language Transla-tion.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.ACL.Patrik Lambert and Rafael E. Banchs.
2005.
DataInferred Multiword Expressions for Statistical Ma-chine Translation.
Machine Translation Summit X.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
HLT/NAACL.Dekang Lin and Patrick Pantel.
1999.
Induction of Se-mantic Classes from Natural Language Text.
In Pro-ceedings of ACM Conference on Knowledge Dis-covery and Data Mining (KDD-01).Daniel Marcu and William Wong.
2002.
A Phrase-based, Joint Probability Model for Statistical Ma-chine Translation.
In Proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP).I.
Dan Melamed, Ryan Green, and Joseph Turian.2003.
Precision and Recall of Machine Translation.NAACL/HLT 2003.Robert C. Moore.
2005.
A Discriminative Frameworkfor Bilingual Word Alignment.
HLT/EMNLP.Franz Josef Och and Hermann Ney.
2003.
A Sys-tematic Comparison of Various Statistical Align-ment Models.
Computational Linguistics, volume20,number 1.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007.
Evaluating Machine Translation withLFG Dependencies.
Machine Translation, Springer,Volume 21, Number 2.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method For AutomaticEvaluation of Machine Translation ACL.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrasegeneration.
EMNLP-2004.Matthew Snover.
Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Anno-tation.
Association for Machine Translation in theAmericas.John Tinsley, Ventsisiav Zhechev, Mary Hearne, andAndy Way.
2006.
Robust Language Pair-Independent Sub-Tree Alignment.
Translation Sum-mit XI.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based Word Alignment in StatisticalTranslation.
COLING 96.Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-mak.
2007.
Kernel Regression Based MachineTranslation.
Proceedings of NAACL-HLT 2007.80
