Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 788?798,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTransforming Dependencies into Phrase StructuresLingpeng KongSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA, USAlingpenk@cs.cmu.eduAlexander M. RushFacebook AI ResearchNew York, NY, USAsrush@seas.harvard.eduNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA, USAnasmith@cs.cmu.eduAbstractWe present a new algorithm for transformingdependency parse trees into phrase-structureparse trees.
We cast the problem as struc-tured prediction and learn a statistical model.Our algorithm is faster than traditional phrase-structure parsing and achieves 90.4% Englishparsing accuracy and 82.4% Chinese parsingaccuracy, near to the state of the art on bothbenchmarks.1 IntroductionNatural language parsers typically produce phrase-structure (constituent) trees or dependency trees.These representations capture some of the same syn-tactic phenomena, and the two can be producedjointly (Klein and Manning, 2002; Hall and Nivre,2008; Carreras et al, 2008; Rush et al, 2010).
Yetit appears to be completely unpredictable which willbe preferred by a particular subcommunity or usedin a particular application.
Both continue to receivethe attention of parsing researchers.Further, it appears to be a historical accidentthat phrase-structure syntax was used in annotatingthe Penn Treebank, and that English dependencyannotations are largely derived through mechani-cal, rule-based transformations (reviewed in Sec-tion 2).
Indeed, despite extensive work on direct-to-dependency parsing algorithms (which we call d-parsing), the most accurate dependency parsers forEnglish still involve phrase-structure parsing (whichwe call c-parsing) followed by rule-based extractionof dependencies (Kong and Smith, 2014).What if dependency annotations had come first?Because d-parsers are generally much faster thanc-parsers, we consider an alternate pipeline (Sec-tion 3): d-parse first, then transform the depen-dency representation into a phrase-structure treeconstrained to be consistent with the dependencyparse.
This idea was explored by Xia and Palmer(2001) and Xia et al (2009) using hand-writtenrules.
Instead, we present a data-driven algorithmusing the structured prediction framework (Sec-tion 4).
The approach can be understood as aspecially-trained coarse-to-fine decoding algorithmwhere a d-parser provides ?coarse?
structure and thesecond stage refines it (Charniak and Johnson, 2005;Petrov and Klein, 2007).Our lexicalized phrase-structure parser, PAD, isasymptotically faster than parsing with a lexical-ized context-free grammar: O(n2) plus d-parsing,vs.
O(n5) worst case runtime in sentence lengthn, with the same grammar constant.
Experimentsshow that our approach achieves linear observableruntime, and accuracy similar to state-of-the-artphrase-structure parsers without reranking or semi-supervised training (Section 7).2 BackgroundWe begin with the conventional development by firstintroducing c-parsing and then defining d-parsesthrough a mechanical conversion using head rules.In the next section, we consider the reverse transfor-mation.2.1 CFG ParsingThe phrase-structure trees annotated in the PennTreebank are derivation trees from a context-freegrammar.
Define a binary1context-free grammar1For notational simplicity, we defer discussion of non-binary rules to Section 3.3.788(CFG) as a 4-tuple (N ,G, T , r) where N is a setof nonterminal symbols (e.g.
NP, VP), T is a setof terminal symbols, consisting of the words in thelanguage, G is a set of binary rules of the formA ?
?1?2, and r ?
N is a distinguished root non-terminal symbol.Given an input sentence x1, .
.
.
, xnof terminalsymbols from T , define the set of c-parses for thesentence as Y(x).
This set consists of all binary or-dered trees with fringe x1, .
.
.
, xn, internal nodes la-beled from N , all tree productions A ?
?1?2con-sisting of members of G, and root label r.For a c-parse y ?
Y(x), we further associate aspan ?v?, v??
with each vertex in the tree.
Thisspecifies the subsequence {xv?, .
.
.
, xv?}
of thesentence covered by this vertex.2.2 Dependency ParsingDependency parses provide an alternative, and insome sense simpler, representation of sentencestructure.
These d-parses can be derived throughmechanical transformation from context-free trees.There are several popular transformations in wideuse; each provides a different representation of asentence?s structure (Collins, 2003; De Marneffeand Manning, 2008; Yamada and Matsumoto, 2003;Johansson and Nugues, 2007).We consider the class of transformations that aredefined through local head rules.
For a binary CFG,define a collection of head rules as a mapping fromeach CFG rule to a head preference for its left orright child.
We use the notation A ?
?
?1?2andA ?
?1?
?2to indicate a left- or right-headed rule,respectively.The head rules can be used to map a c-parse to adependency tree (d-parse).
In a d-parse, each wordin the sentence is assigned as a dependent to a headword, h ?
{0, .
.
.
, n}, where 0 is a special symbolindicating the pseudo-root of the sentence.
For eachh we define L(h) ?
{1, .
.
.
, h?
1} as the set of leftdependencies of h, and R(h) ?
{h + 1, .
.
.
, n} asthe set of right dependencies.A d-parse can be constructed recursively from ac-parse and the head rules.
For each c-parse vertexv with potential children vLand vRin bottom-up or-der, we apply the following procedure to both assignheads to the c-parse and construct the d-parse:S(3).
.
.VP(3)VBD?(3)sold3NP(2)NN?(2)automaker2DT(1)The1The1automaker2sold3.
.
.Figure 1: Illustration of c-parse to d-parse conversion with headrules {VP?
NP VBD?,NP?
DT NN?, .
.
.}.
The c-parse is anordered tree with fringe x1, .
.
.
, xn.
Each vertex is annotatedwith a terminal or nonterminal symbol and a derived head index.The blue and red vertices have the words automaker2andsold3as heads respectively.
The vertex VP(3) implies thatautomaker2is a left-dependent of sold3, and that 2 ?
L(3)in the d-parse.1.
If the vertex is leaf xm, then head(v) = m.2.
If the next rule is A ?
?
?1?2then head(v) =head(vL) and head(vR) ?
R(head(v)), i.e.the head of the right-child is a dependent of thehead word.3.
If the next rule is A ?
?1?
?2then head(v) =head(vR) and head(vL) ?
L(head(v)), i.e.the head of the left-child is a dependent of thehead word.Figure 1 shows an example conversion of a c-parseto d-parse using this procedure.By construction, these dependencies form a di-rected tree with arcs (h,m) for all h ?
{0, .
.
.
, n}and m ?
L(h) ?
R(h).
While this tree differsfrom the original c-parse, we can relate the two treesthrough their spans.
Define the dependency treespan ?h?, h??
as the contiguous sequence of wordsreachable from word h in this tree.2This span isequivalent to the maximal span ?v?, v??
of any c-parse vertex with head(v) = h. This property willbe important for the parsing algorithm presented inthe next section.2The conversion from a standard CFG tree to a d-parse pre-serves this sequence property, known as projectivity.
We leavethe question of non-projective d-parses and the related questionof traces and co-indexation in c-parses to future work.2789I1saw2the3man4X(2)X(4)Nman4Dthe3Vsaw2X(1)NI1X(2)X(2)X(4)Nman4Dthe3Vsaw2NI1X(2)X(4)Nman4Dthe3X(2)Vsaw2NI1Figure 2: [Adapted from (Collins et al, 1999).]
A d-parse(left) and several c-parses consistent with it (right).
Our goal isto select the best parse from this set.3 Parsing DependenciesNow we consider flipping this setup.
There has beensignificant progress in developing efficient direct-to-dependency parsers.
These d-parsers are trainedonly on dependency annotations and do not requirefull phrase-structure trees.3Some prefer this setup,since it allows easy selection of the specific depen-dencies of interest in a downstream task (e.g., infor-mation extraction), and perhaps even training specif-ically for those dependencies.
Other applicationsmake use of phrase structures, so c-parsers enjoywide use as well.With these latter applications in mind, we con-sider the problem of converting a fixed d-parseinto a c-parse, with the intent of using off-the-shelfd-parsers for constructing phrase-structure parses.Since this problem is more challenging than its in-verse, we use a structured prediction setup: we learna function to score possible c-parse conversions, andthen generate the highest-scoring c-parse given a d-parse.
A toy example of the problem is shown inFigure 2.3.1 Parsing AlgorithmConsider the classical problem of predicting the bestc-parse under a CFG with head rules, known as lex-icalized context-free parsing.
Assume that we aregiven a binary CFG defining a set of valid c-parsesY(x).
The parsing problem is to find the highest-scoring parse in this set, i.e.
arg maxy?Y(x)s(y;x)3For English these parsers are still often trained ontrees converted from c-parses; however, for other languages,dependency-only treebanks of directly-annotated d-parses arecommon.where s is a scoring function that factors over lexi-calized tree productions.This problem can be solved by extending theCKY algorithm to propagate head information.
Thealgorithm can be compactly defined by the produc-tions in Figure 3 (left).
For example, one type ofproduction is of the form(?i, k?,m, ?1) (?k + 1, j?, h, ?2)(?i, j?, h, A)for all rules A ?
?1??2?
G and spans i ?
k < j.This particular production indicates that rule A ??1?
?2was applied at a vertex covering ?i, j?
to pro-duce two vertices covering ?i, k?
and ?k+ 1, j?, andthat the new head is index h has dependent index m.We say this production ?completes?
word m since itcan no longer be the head of a larger span.Running the algorithm consists of bottom-up dy-namic programming over these productions.
How-ever, applying this version of the CKY algorithmrequires O(n5|G|) time (linear in the number ofproductions), which is not practical to run withoutheavy pruning.
Most lexicalized parsers thereforemake further assumptions on the scoring functionwhich can lead to asymptotically faster algorithms(Eisner and Satta, 1999).Instead, we consider the same objective, but con-strain the c-parses to be consistent with a given d-parse, d. By ?consistent,?
we mean that the c-parse will be converted by the head rules to this ex-act d-parse.4Define the set of consistent c-parsesas Y(x, d) and the constrained search problem asarg maxy?Y(x,d)s(y;x, d).Figure 3 (right) shows the algorithm for this newproblem.
The algorithm has several nice proper-ties.
All rules now must select words h and m thatare consistent with the dependency parse (i.e., thereis an arc (h,m)) so these variables are no longerfree.
Furthermore, since we have the full d-parse,we can precompute the dependency span of eachword ?m?,m??.
By our definition of consistency,this gives us the c-parse span of m before it is com-pleted, and fixes two more free variables.
Finally thehead item must have its alternative side index match4An alternative, soft version of consistency, might enforcethat the c-parse is close to the d-parse.
While this allows the al-gorithm to potentially correct d-parse mistakes, it is much morecomputationally expensive.3790Premise:(?i, i?, i, A) ?i ?
{1 .
.
.
n}, A ?
NRules:For i ?
h ?
k < m ?
j, and rule A?
?
?1?2,(?i, k?, h, ?1) (?k + 1, j?,m, ?2)(?i, j?, h, A)For i ?
m ?
k < h ?
j, rule A?
?1?
?2,(?i, k?,m, ?1) (?k + 1, j?, h, ?2)(?i, j?, h, A)Goal:(?1, n?,m, r) for any mPremise:(?i, i?, i, A) ?i ?
{1 .
.
.
n}, A ?
NRules:For all h, m ?
R(h), rule A?
?
?1?2,and i ?
{m??
: m??
L(h)} ?
{h},(?i,m??
1?, h, ?1) (?m?,m?
?,m, ?2)(?i,m?
?, h, A)For all h, m ?
L(h), rule A?
?1?
?2,and j ?
{m??
: m??
R(h)} ?
{h},(?m?,m?
?,m, ?1) (?m?+ 1, j?, h, ?2)(?m?, j?, h, A)Goal:(?1, n?,m, r) for any m ?
R(0)Figure 3: The two algorithms written as deductive parsers.
Starting from the premise, any valid application of rules that leads to agoal is a valid parse.
Left: lexicalized CKY algorithm for CFG parsing with head rules.
For this algorithm there areO(n5|G|) ruleswhere n is the length of the sentence.
Right: the constrained CKY parsing algorithm for Y(x, d).
The algorithm is nearly identicalexcept that many of the free indices are now fixed given the dependency parse.
Finding the optimal c-parse with the new algorithmnow requires O((?h|L(h)||R(h)|)|G|)time where L(h) andR(h) are the left and right dependents of word h.a valid dependency span.
For example, if for a wordh there are |L(h)| = 3 left dependents, then whentaking the next right-dependent there can only be 4valid left boundary indices.The runtime of the final algorithm reduces toO(?h|L(h)||R(h)||G|).
While the terms |L(h)|and |R(h)| could in theory make the runtimequadratic, in practice the number of dependents isalmost always constant in the length of the sentence.This leads to linear observed runtime in practice aswe will show in Section 7.3.2 PruningIn addition to constraining the number of c-parses,the d-parse also provides valuable information aboutthe labeling and structure of the c-parse.
We can usethis information to further prune the search space.We employ two pruning methods:Method 1 uses the part-of-speech tag of xh,tag(h), to limit the possible rule productions at agiven span.
We build tables Gtag(h)and restrict thesearch to rules seen in training for a particular part-of-speech tag.Method 2 prunes based on the order in which de-pendent words are added.
By the constraints of thealgorithm, a head word xhmust combine with eachof its left and right dependents.
However, the or-der of combination can lead to different tree struc-tures (as illustrated in Figure 2).
In total there are|L(h)| ?
|R(h)| possible orderings of dependents.In practice, though, it is often easy to predictwhich side, left or right, will come next.
We do thisby estimating the distribution,p(side | tag(h), tag(m), tag(m?
)),wherem ?
L(h) is the next left dependent andm?
?R(h) is the next right dependent.
If the conditionalprobability of left or right is greater than a thresholdparameter ?, we make a hard decision to combinewith that side next.
This pruning further reduces theimpact of outliers with multiple dependents on bothsides.We empirically measure how these pruning meth-ods affect observed runtime and oracle parsing per-formance (i.e., how well a perfect scoring functioncould do with a pruned Y(x, d)).
Table 1 showsa comparison of these pruning methods on devel-opment data.
The constrained parsing algorithm ismuch faster than standard lexicalized parsing, and4791Model Complexity Sent./s.
Ora.
F1LEX CKY?n5|G| 0.25 100.0DEP CKY?h|L(h)||R(h)||G| 71.2 92.6PRUNE1?h|L(h)||R(h)||GT| 336.0 92.5PRUNE2 ?
96.6 92.5PRUNE1+2 ?
425.1 92.5Table 1: Comparison of three parsing setups: LEX CKY?is the complete lexicalized c-parser on Y(x), but limited toonly sentences less than 20 words for tractability, DEP CKYis the constrained c-parser on Y(x, d), PRUNE1, PRUNE2, andPRUNE1+2 are combinations of the pruning methods describedin Section 3.2.
The oracle is the best labeled F1achievable onthe development data (?22, see Section 7).pruning contributes even greater speed-ups.
The or-acle experiments show that the d-parse constraintsdo contribute a large drop in oracle accuracy, whilepruning contributes a relatively small one.
Still, thisupper-bound on accuracy is high enough to make itpossible to still recover c-parses at least as accurateas state-of-the-art c-parsers.
We will return to thisdiscussion in Section 7.3.3 Binarization and Unary RulesWe have to this point developed the algorithm fora strictly binary-branching grammar; however, weneed to produce trees have rules with varying size.In order to apply the algorithm, we binarize thegrammar and add productions to handle unary rules.Consider a non-binarized rule of the form A ??1.
.
.
?mwith head child ??k.
Relative to the headchild ?kthe rule has left-side ?1.
.
.
?k?1and right-side ?k+1.
.
.
?m.
We replace this rule with newbinary rules and non-terminal symbols to produceeach side independently as a simple chain, left-sidefirst.
The transformation introduces the followingnew rules:5A ?
?1?A?,?A ?
?i?A?for i ?
{2, .
.
.
, k}, and?A??A?
?ifor i ?
{k, .
.
.
,m}.As an example consider the transformation of arule with four children:SNPNPVP?NP?
S?S?NP?S?NPVP?NPThese rules can then be reversed deterministically toproduce a non-binary tree.5These rules are slightly modified when k = 1.We also explored binarization using horizontaland vertical markovization to include additionalcontext of the tree, as found useful in unlexicalizedapproaches (Klein and Manning, 2003).
Preliminaryexperiments showed that this increased the size ofthe grammar, and the runtime of the algorithm, with-out leading to improvements in accuracy.Phrase-structure trees also include unary rules ofthe form A?
??1.
To handle unary rules we modifythe parsing algorithms in Figure 3 to include a unarycompletion rule,(?i, j?, h, ?1)(?i, j?, h, A)for all indices i ?
h ?
j that are consistent withthe dependency parse.
In order to avoid unary re-cursion, we limit the number of applications of thisrule at each span (preserving the runtime of the algo-rithm).
Preliminary experiments looked at collaps-ing the unary rules into the nonterminal symbols,but we found that this hurt performance comparedto explicit unary rules.4 Structured PredictionWe learn the d-parse to c-parse conversion us-ing a standard structured prediction setup.
Definethe linear scoring function s for a conversion ass(y;x, d, ?)
= ?>f(x, d, y) where ?
is a parametervector and f(x, d, y) is a feature function that mapsparse productions to sparse feature vectors.
Whilethe parser only requires a d-parse at prediction time,the parameters of this scoring function are learneddirectly from a treebank of c-parses and a set of headrules.
The structured prediction model, in effect,learns to invert the head rule transformation.4.1 FeaturesThe scoring function requires specifying a set ofparse features f which, in theory, could be directlyadapted from existing lexicalized c-parsers.
How-ever, the structure of the dependency parse greatlylimits the number of decisions that need to be made,and allows for a smaller set of features.We model our features after two bare-bones pars-ing systems.
The first set is the basic arc-factoredfeatures used by McDonald (2006).
These featuresinclude combinations of: rule and top nonterminal,5792For a production(?i, k?,m, ?1) (?k + 1, j?, h, ?2)(?i, j?, h, A)Nonterm Features(A, ?1) (A, ?1, tag(m))(A, ?2) (A, ?2, tag(h))Span Features(rule, xi) (rule, xi?1)(rule, xj) (rule, xj+1)(rule, xk) (rule, xk+1)(rule, bin(j ?
i))Rule Features(rule)(rule, xh, tag(m))(rule, tag(h), xm)(rule, tag(h), tag(m))(rule, xh)(rule, tag(h))(rule, xm)(rule, tag(m))Figure 4: The feature templates used in the function f(x, d, y).For the span features, the symbol rule is expanded into bothA ?
B C and backoff symbol A.
The function bin(i) parti-tions a span length into one of 10 bins.modifier word and part-of-speech, and head wordand part-of-speech.The second set of features is modeled after thespan features described in the X-bar-style parser ofHall et al (2014).
These include conjunctions of therule with: first and last word of current span, pre-ceding and following word of current span, adjacentwords at split of current span, and binned length ofthe span.The full feature set is shown in Figure 4.
Aftertraining, there are a total of around 2 million non-zero features.
For efficiency, we use lossy featurehashing.
We found this had no impact on parsingaccuracy but made the parsing significantly faster.4.2 TrainingThe parameters ?
are estimated using a struc-tural support vector machine (Taskar et al, 2004).Given a set of gold-annotated c-parse examples,(x1, y1), .
.
.
, (xD, yD), and d-parses d1.
.
.
dDin-duced from the head rules, we estimate the parame-ters to minimize the regularized empirical riskmin?D?i=1`(xi, di, yi, ?)
+ ?||?||1where we define ` as `(x, d, y, ?)
= ?s(y) +maxy??Y(x,d)(s(y?)
+ ?
(y, y?))
and where ?
is aproblem specific cost-function.
In experiments, weuse a Hamming loss ?
(y, y?)
= |y ?
y?| where y isan indicator for production rules firing over pairs ofadjacent spans (i.e., i, j, k).PTB ?22Model Prec.
Rec.
F1Xia et al (2009) 88.1 90.7 89.4PAD (?19) 95.9 95.9 95.9PAD (?2?21) 97.5 97.8 97.7Table 2: Comparison with the rule-based system of Xia et al(2009).
Results are shown using gold-standard tags and depen-dencies.
Xia et al report results consulting only ?19 in devel-opment and note that additional data had little effect.
We showour system?s results using ?19 and the full training set.The objective is optimized using AdaGrad (Duchiet al, 2011).
The gradient calculation requires com-puting a loss-augmented max-scoring c-parse foreach training example which is done using the al-gorithm of Figure 3 (right).5 Related WorkThe problem of converting dependency to phrase-structured trees has been studied previously from theperspective of building multi-representational tree-banks.
Xia and Palmer (2001) and Xia et al (2009)develop a rule-based system for the conversion ofhuman-annotated dependency parses.
This work fo-cuses on modeling the conversion decisions madeand capturing how researchers annotate specific phe-nomena.
Our work focuses on a different problem oflearning a data-driven structured prediction modelthat is also able to handle automatically predicteddependency parses as input.
While the aim is dif-ferent, Table 2 does give a direct comparison of oursystem to that of Xia et al (2009) on gold d-parsedata.An important line of previous work also uses de-pendency parsers to produce phrase-structure trees.In particular Hall et al (2007) and Hall and Nivre(2008) develop a specialized dependency label set toencode phrase-structure information in the d-parse.After predicting a d-parse this label information canbe used to assemble a predicted c-parse.
Our workdiffers in that it does not make any assumptions onthe labeling of the dependency tree used and it usesstructured prediction to produce the final c-parse.Very recently, Fern?andez-Gonz?alez and Martins(2015) also show that an off-the-shelf, trainable,dependency parser is enough to build a highly-competitive constituent parser.
They proposed6793a new intermediate representation called ?head-ordered dependency trees?, which encode head or-dering information in dependeny labels.
Their al-gorithm is based on a reduction of the constituentparsing to dependency parsing of such trees.There has been successful work combining de-pendency and phrase-structure information to buildaccurate c-parsers.
Klein and Manning (2002) con-struct a factored generative model that scores bothcontext-free syntactic productions and semantic de-pendencies.
Carreras et al (2008) construct a state-of-the-art parser that uses a dependency parsingmodel both for pruning and within a richer lexical-ized parser.
Similarly, Rush et al (2010) use dualdecomposition to combine a powerful dependencyparser with a lexicalized phrase-structure model.This work differs in that we treat the dependencyparse as a hard constraint, hence largely reduce theruntime of a fully lexicalized phrase structure pars-ing model while maintaining the ability, at leastin principle, to generate highly accurate phrase-structure parses.Finally there have also been several papers thatuse ideas from dependency parsing to simplify andspeed up phrase-structure prediction.
Zhu et al(2013) build a high-accuracy phrase-structure parserusing a transition-based system.
Hall et al (2014)use a stripped down parser based on a simple X-bargrammar and a small set of lexicalized features.6 MethodsWe ran a series of experiments to assess the accu-racy, efficiency, and applicability of our parser, PAD,to several tasks.
These experiments use the follow-ing setup.For English experiments we use the standard PennTreebank (PTB) experimental setup (Marcus et al,1993).
Training is done on ?2?21, development on?22, and testing on ?23.
We use the development setto tune the regularization parameter, ?
= 1e?8, andthe pruning threshold, ?
= 0.95.For Chinese experiments, we use version 5.1 ofthe Penn Chinese Treebank 5.1 (CTB) (Xue et al,2005).
We followed previous work and used articles001?270 and 440?1151 for training, 301?325 for de-velopment, and 271?300 for test.
We also use thedevelopment set to tune the regularization parame-ter, ?
= 1e?
3.Part-of-speech tagging is performed for all mod-els using TurboTagger (Martins et al, 2013).
Priorto training the d-parser, the training sections areautomatically processed using 10-fold jackknifing(Collins and Koo, 2005) for both dependency andphrase structure trees.
Zhu et al (2013) found thissimple technique gives an improvement to depen-dency accuracy of 0.4% on English and 2.0% onChinese in their system.During training, we use the d-parses induced bythe head rules from the gold c-parses as constraints.There is a slight mismatch here with test, since thesed-parses are guaranteed to be consistent with the tar-get c-parse.
We also experimented with using 10-fold jacknifing of the d-parser during training to pro-duce more realistic parses; however, we found thatthis hurt performance of the parser.Unless otherwise noted, in English the test d-parsing is done using the RedShift implementation6of the parser of Zhang and Nivre (2011), trainedto follow the conventions of Collins head rules(Collins, 2003).
This parser is a transition-basedbeam search parser, and the size of the beam k con-trols a speed/accuracy trade-off.
By default we usea beam of k = 16.
We found that dependency la-bels have a significant impact on the performance ofthe RedShift parser, but not on English dependencyconversion.
We therefore train a labeled parser, butdiscard the labels.For Chinese, we use the head rules compiled byDing and Palmer (2005)7.
For this data-set wetrained the d-parser using the YaraParser implemen-tation8of the parser of Zhang and Nivre (2011), be-cause it has a better Chinese implementation.
Weuse a beam of k = 64.
In experiments, we foundthat Chinese labels were quite helpful, and addedfour additional features templates conjoining the la-bel with the non-terminals of a rule.Evaluation for phrase-structure parses is per-formed using the evalb9script with the standardsetup.
We report labeled F1scores as well as recalland precision.
For dependency parsing, we report6https://github.com/syllog1sm/redshift7http://stp.lingfil.uu.se/?nivre/research/chn_headrules.txt8https://github.com/yahoo/YaraParser9http://nlp.cs.nyu.edu/evalb7794PTB ?23Model F1Sent./s.Charniak (2000) 89.5 ?Stanford PCFG (2003) 85.5 5.3Petrov (2007) 90.1 8.6Zhu (2013) 90.3 39.0Carreras (2008) 91.1 ?CJ Reranking (2005) 91.5 4.3Stanford RNN (2013) 90.0 2.8PAD 90.4 34.3PAD (Pruned) 90.3 58.6CTBModel F1Charniak (2000) 80.8Bikel (2004) 80.6Petrov (2007) 83.3Zhu (2013) 83.2PAD 82.4Table 3: Accuracy and speed on PTB ?23 and CTB 5.1 testsplit.
Comparisons are to state-of-the-art non-reranking super-vised phrase-structure parsers (Charniak, 2000; Klein and Man-ning, 2003; Petrov and Klein, 2007; Carreras et al, 2008; Zhuet al, 2013; Bikel, 2004), and semi-supervised and rerankingparsers (Charniak and Johnson, 2005; Socher et al, 2013).unlabeled accuracy score (UAS).We implemented the grammar binarization, headrules, and pruning tables in Python, and the parser,features, and training in C++.
Experiments are per-formed on a Lenovo ThinkCentre desktop computerwith 32GB of memory and Core i7-3770 3.4GHz8M cache CPU.7 ExperimentsWe ran experiments to assess the accuracy of themethod, its runtime efficiency, the effect of depen-dency parsing accuracy, and the effect of the amountof annotated phrase-structure data.Parsing Accuracy Table 3 compares the accuracyand speed of the phrase-structure trees produced bythe parser.
For these experiments we treat our sys-tem and the Zhang-Nivre parser as an independentlytrained, but complete end-to-end c-parser.
Runtimefor these experiments includes both the time for d-parsing and conversion.
Despite the fixed depen-Model UAS F1Sent./s.
OracleMALTPARSER 89.7 85.5 240.7 87.8RS-K1 90.1 86.6 233.9 87.6RS-K4 92.5 90.1 151.3 91.5RS-K16 93.1 90.6 58.6 92.5YARA-K1 89.7 85.3 1265.8 86.7YARA-K16 92.9 89.8 157.5 91.7YARA-K32 93.1 90.4 48.3 92.0YARA-K64 93.1 90.5 47.3 92.2TP-BASIC 92.8 88.9 132.8 90.8TP-STANDARD 93.3 90.9 27.2 92.6TP-FULL 93.5 90.8 13.2 92.9Table 4: The effect of d-parsing accuracy (PTB ?22) on PADand an oracle converter.
Runtime includes d-parsing and c-parsing.
Inputs include MaltParser (Nivre et al, 2006), theRedShift and the Yara implementations of the parser of Zhangand Nivre (2011) with various beam size, and three versions ofTurboParser trained with projective constraints (Martins et al,2013).dency constraints, the English results show that theparser is comparable in accuracy to many widely-used systems, and is significantly faster.
The parsermost competitive in both speed and accuracy is thatof Zhu et al (2013), a fast shift-reduce phrase-structure parser.Furthermore, the Chinese results suggest that,even without making language-specific changes inthe feature system we can still achieve competitiveparsing accuracy.Effect of Dependencies Table 4 shows experi-ments comparing the effect of different input d-parses.
For these experiments we used the same ver-sion of PAD with 11 different d-parsers of varyingquality and speed.
We measure for each parser: itsUAS, speed, and labeled F1when used with PADand with an oracle converter.10The paired figure10For a gold parse y and predicted dependencies?d, define theoracle parse as y?= arg miny??Y(x,?d)?
(y, y?
)8795Figure 5: Empirical runtime of the parser on sentences of vary-ing length, with and without pruning.
Despite a worst-casequadratic complexity, observed runtime is linear.shows that there is a direct correlation between theUAS of the inputs and labeled F1.Runtime In Section 3 we considered the theoret-ical complexity of the parsing model and presentedthe main speed results in Table 1.
Despite havinga quadratic theoretical complexity, the practical run-time was quite fast.
Here we consider the empiri-cal complexity of the model by measuring the timespent on individual sentences.
Figure 5 shows parserspeed for sentences of varying length for both thefull algorithm and with pruning.
In both cases theobserved runtime is linear.Recovering Phrase-Structure Treebanks Anno-tating phrase-structure trees is often more expensiveand slower than annotating unlabeled dependencytrees (Schneider et al, 2013).
For low-resource lan-guages, an alternative approach to developing fullyannotated phrase-structure treebanks might be to la-bel a small amount of c-parses and a large amount ofcheaper d-parses.
Assuming this setup, we ask howmany c-parses would be necessary to obtain reason-able performance?For this experiment, we train PAD on only 5%of the PTB training set and apply it to predicted d-parses from a fully-trained model.
Even with thissmall amount of data, we obtain a parser with de-velopment score of F1= 89.1%, which is compa-rable to Charniak (2000) and Stanford PCFG (Kleinand Manning, 2003) trained on the complete c-parsetraining set.
Additionally, if the gold dependenciesare available, PAD with 5% training achieves F1=95.8% on development, demonstrating a strong abil-Class ResultsDep.
Span Split Count Acc.
(h,m) ?i, j?
k A+ + + 32853 97.9?
+ + 381 69.3+ + ?
802 83.3?
+ ?
496 85.9+ ?
?
1717 0.0?
?
?
1794 0.0Table 5: Error analysis of binary CFG rules.
Rules used are splitinto classes based on correct (+) identification of dependency(h,m), span ?i, j?, and split k. ?Count?
is the size of eachclass.
?Acc.?
is the accuracy of span nonterminal identification.ity to recover the phrase-structure trees from depen-dency annotations.Analysis Finally we consider an internal erroranalysis of the parser.
For this analysis, we groupeach binary rule production selected by the parserby three properties: Is its dependency (h,m) cor-rect?
Is its span ?i, j?
correct?
Is its split k correct?The first property is fully determined by the inputd-parse, the others are partially determined by PADitself.Table 5 shows the breakdown.
The conversionis almost always accurate (?98%) when the parserhas correct span and dependency information.
Asexpected, the difficult cases come when the depen-dency was fully incorrect, or there is a propagatedspan mistake.
As dependency parsers improve, theperformance of PAD should improve as well.8 ConclusionWith recent advances in statistical dependency pars-ing, we find that fast, high-quality phrase-structureparsing is achievable using dependency parsing first,followed by a statistical conversion algorithm tofill in phrase-structure trees.
Our implementationis available as open-source software at https://github.com/ikekonglp/PAD.Acknowledgments The authors thank the anony-mous reviewers and Andr?e Martins, Chris Dyer, andSlav Petrov for helpful feedback.
This research wassupported in part by NSF grant IIS-1352440 andcomputing resources provided by Google and thePittsburgh Supercomputing Center.9796ReferencesDaniel M Bikel.
2004.
On the parameter space of gen-erative lexicalized statistical parsing models.
Ph.D.thesis, University of Pennsylvania.Xavier Carreras, Michael Collins, and Terry Koo.
2008.Tag, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, pages 9?16.
Association for Compu-tational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 132?139.
Association for Com-putational Linguistics.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?70.Michael Collins, Lance Ramshaw, Jan Haji?c, andChristoph Tillmann.
1999.
A statistical parser forczech.
In Proceedings of the 37th annual meetingof the Association for Computational Linguistics onComputational Linguistics, pages 505?512.
Associa-tion for Computational Linguistics.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational linguis-tics, 29(4):589?637.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-Domain ParserEvaluation, pages 1?8.
Association for ComputationalLinguistics.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 541?548.
Association for ComputationalLinguistics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In Proceedings of the 37th annualmeeting of the Association for Computational Linguis-tics on Computational Linguistics, pages 457?464.
As-sociation for Computational Linguistics.Daniel Fern?andez-Gonz?alez and Andr?e FT Martins.2015.
Parsing as reduction.
arXiv preprintarXiv:1503.00030.Johan Hall and Joakim Nivre.
2008.
A dependency-driven parser for german dependency and constituencyrepresentations.
In Proceedings of the Workshop onParsing German, pages 47?54.
Association for Com-putational Linguistics.Johan Hall, Joakim Nivre, and Jens Nilsson.
2007.
A hy-brid constituency-dependency parser for swedish.
InProceedings of NODALIDA, pages 284?287.David Hall, Greg Durrett, and Dan Klein.
2014.
Lessgrammar, more features.
In ACL.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for english.
In16th Nordic Conference of Computational Linguistics,pages 105?112.
University of Tartu.Dan Klein and Christopher D Manning.
2002.
Fast exactinference with a factored model for natural languageparsing.
In Advances in neural information processingsystems, pages 3?10.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Lingpeng Kong and Noah A Smith.
2014.
An empiricalcomparison of parsing methods for stanford dependen-cies.
arXiv preprint arXiv:1404.4314.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The penn treebank.
Computational lin-guistics, 19(2):313?330.Andr?e FT Martins, Miguel Almeida, and Noah A Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In ACL (2), pages 617?622.Ryan McDonald.
2006.
Discriminative learning andspanning tree algorithms for dependency parsing.Ph.D.
thesis, University of Pennsylvania.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of LREC, volume 6, pages2216?2219.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL, pages 404?411.
Citeseer.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 1?11.
Association for Computational Linguis-tics.10797Nathan Schneider, Brendan O?Connor, Naomi Saphra,David Bamman, Manaal Faruqui, Noah A Smith,Chris Dyer, and Jason Baldridge.
2013.
Aframework for (under) specifying dependency syn-tax without overloading annotators.
arXiv preprintarXiv:1306.2091.Richard Socher, John Bauer, Christopher D Manning, andAndrew Y Ng.
2013.
Parsing with compositional vec-tor grammars.
In In Proceedings of the ACL confer-ence.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin Markov networks.
In Advances in NeuralInformation Processing Systems 16.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structures to phrase structures.
In Proceedingsof the first international conference on Human lan-guage technology research, pages 1?5.
Association forComputational Linguistics.Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,and Dipti Misra Sharma.
2009.
Towards a multi-representational treebank.
In The 7th InternationalWorkshop on Treebanks and Linguistic Theories.Groningen, Netherlands.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(02):207?238.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT, volume 3.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, pages188?193.
Association for Computational Linguistics.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, andJingbo Zhu.
2013.
Fast and accurate shift-reduce con-stituent parsing.
In ACL (1), pages 434?443.11798
