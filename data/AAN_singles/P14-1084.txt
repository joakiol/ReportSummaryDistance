Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 892?901,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsModelling Events through Memory-based, Open-IE Patternsfor Abstractive SummarizationDaniele PighinGoogle Inc.biondo@google.comMarco Cornolti?University of Pisa, Italycornolti@di.unipi.itEnrique AlfonsecaGoogle Inc.ealfonseca@google.comKatja FilippovaGoogle Inc.katjaf@google.comAbstractAbstractive text summarization of newsrequires a way of representing events, suchas a collection of pattern clusters in whichevery cluster represents an event (e.g.,marriage) and every pattern in the clus-ter is a way of expressing the event (e.g.,X married Y, X and Y tied the knot).
Wecompare three ways of extracting eventpatterns: heuristics-based, compression-based and memory-based.
While the for-mer has been used previously in multi-document abstraction, the latter two havenever been used for this task.
Comparedwith the first two techniques, the memory-based method allows for generating sig-nificantly more grammatical and informa-tive sentences, at the cost of searching avast space of hundreds of millions of parsetrees of known grammatical utterances.
Tothis end, we introduce a data structure anda search method that make it possible toefficiently extrapolate from every sentencethe parse sub-trees that match against anyof the stored utterances.1 IntroductionText summarization beyond extraction requires asemantic representation that abstracts away fromwords and phrases and from which a summary canbe generated (Mani, 2001; Sp?arck-Jones, 2007).Following and extending recent work in semanticparsing, information extraction (IE), paraphrasegeneration and summarization (Titov and Klemen-tiev, 2011; Alfonseca et al, 2013; Zhang andWeld, 2013; Mehdad et al, 2013), the represen-tation we consider in this paper is a large collec-?Work done during an internship at Google Zurich.
[John Smith] and[Mary Brown] wedin [Baltimore]...[Smith] tied theknot with [Brown]this Monday...#21: death#22: divorce#23: marriagePER married PERPER and PER wedPER tied the knot with PERPER has married PERJohn Smith married Mary Browne1: J. Smith (PER)e2: M. Brown (PER)e3: Baltimore, MD (LOC)Figure 1: An example of abstracting from inputsentences to an event representation and genera-tion from that representation.tion of clusters of event patterns.
An abstractivesummarization system relying on such a represen-tation proceeds by (1) detecting the most relevantevent cluster for a given sentence or sentence col-lection, and (2) using the most representative pat-tern from the cluster to generate a concise sum-mary sentence.
Figure 1 illustrates the summa-rization architecture we are assuming in this pa-per.
Given input text(s) with resolved and typedentity mentions, event mentions and the most rele-vant event cluster are detected (first arrow).
Then,a summary sentence is generated from the eventand entity representations (second arrow).However, the utility of such a representation forsummarization depends on the quality of patternclusters.
In particular, event patterns must cor-respond to grammatically correct sentences.
In-troducing an incomplete or incomprehensible pat-tern (e.g., PER said PER) may negatively affectboth event detection and sentence generation.
Re-lated work on paraphrase detection and relationextraction is mostly heuristics-based and has re-lied on hand-crafted rules to collect such patterns(see Sec.
2).
A standard approach is to focuson binary relations between entities and extract892EYentmoGelPatternFlXVterinJ1ewVFlXVterVPatterne[traFtion1ewVartiFlePatterne[traFtion,nIerenFe$EVtraFtiYeVXmmar\Figure 2: A generic pipeline for event-driven ab-stractive headline generation.the dependency path between the two entities asan event representation.
An obvious limitationof this approach is there is no guarantee that theextracted pattern corresponds to a grammaticallycorrect sentence, e.g., that an essential preposi-tional phrase is retained like in file for a divorce.In this paper we explore two novel, data-drivenmethods for event pattern extraction.
The first,compression-based method uses a robust sentencecompressor with an aggressive compression rateto get to the core of the sentence (Sec.
3).
Thesecond, memory-based method relies on a vastcollection of human-written headlines and sen-tences to find a substructure which is known tobe grammatically correct (Sec.
4).
While the lat-ter method comes closer to ensuring perfect gram-maticality, it introduces a problem of efficientlysearching the vast space of known well-formedpatterns.
Since standard iterative approaches com-paring every pattern with every sentence are pro-hibitive here, we present a search strategy whichscales well to huge collections (hundreds of mil-lions) of sentences.In order to evaluate the three methods, we con-sider an abstractive summarization task where thegoal is to get the gist of single sentences by recog-nizing the underlying event and generating a shortsummary sentence.
To the best of our knowledge,this is the first time that this task has been pro-posed; it can be considered as abstractive sentencecompression, in contrast to most existing sentencecompression systems which are based on selectingwords from the original sentence or rewriting withsimpler paraphrase tables.
An extensive evalua-tion with human raters demonstrates the utility ofthe new pattern extraction techniques.
Our analy-sis highlights advantages and disadvantages of thethree methods.To better isolate the qualities of the three ex-traction methodologies, all three methods use thesame training data and share components of theAlgorithm 1 HEURISTICEXTRACTOR(T,E): heuristi-cally extract relational patterns for the dependency parse Tand the set of entities E.1: /* Global constants /*2: global Vp, Vc, Np, Nc3: Vc?
{subj, nsubj, nsubjpass, dobj, iobj, xcomp,4: acomp, expl, neg, aux, attr, prt}5: Vp?
{xcomp}6: Nc?
{det, predet, num, ps, poss, nc, conj}7: Np?
{ps, poss, subj, nsubj, nsubjpass, dobj, iobj}8: /* Entry point /*9: P ?
?10: for all C ?
COMBINATIONS(E) do11: N ?
MENTIONNODES(T,C)12: N??
APPLYHEURISTICS(T, BUILDMST(T,N))13: P ?
P ?
{BUILDPATTERN(T,N?
)}14: return P15: /* Procedures /*16: procedure APPLYHEURISTICS(T,N )17: N??
N18: while |N?| > 0 do19: N???
?20: for all n ?
N?do21: if n.ISVERB() then22: N???
N???
INCLUDECHILDREN(n, Vc)23: N???
N???
INCLUDEPARENT(n, Vp)24: else if n.ISNOUN() then25: N???
N???
INCLUDECHILDREN(n,Nc)26: N???
N???
INCLUDEPARENT(n,Np)27: N??
N?
?\N?28: procedure INCLUDECHILDREN(n,L)29: R?
?30: for all c ?
n.CHILDREN() do31: if c.PARENTEDGELABEL() ?
L then32: R?
R ?
{c}33: return R34: procedure INCLUDEPARENT(n,L)35: if n.PARENTEDGELABEL() ?
L then36: return {n}37: else return ?very same summarization architecture, as shownin Figure 2: an event model is constructed by clus-tering the patterns extracted according to the se-lected extraction method.
Then, the same extrac-tion method is used to collect patterns from sen-tences in never-seen-before news articles.
Finally,the patterns are used to query the event model andgenerate an abstractive summary.
The three differ-ent pattern extractors are detailed in the next threesections.2 Heuristics-based pattern extractionIn order to be able to work in an Open-IE man-ner, applicable to different domains, most existingpattern extraction systems are based on linguisti-cally motivated heuristics.
Zhang and Weld (2013)is based on REVERB (Fader et al, 2011), whichuses a regular expression on part-of-speech tagsto produce the extractions.
An alternative system,893OLLIE (Schmitz et al, 2012), uses syntactic de-pendency templates to guide the pattern extractionprocess.The heuristics used in this paper are inspired byAlfonseca et al (2013), who built well formed re-lational patterns by extending minimum spanningtrees (MST) which connect entity mentions in adependency parse.
Algorithm 1 details our re-implementation of their method and the specificset of rules that we rely on to enforce pattern gram-maticality.
We use the standard Stanford-style setof dependency labels (de Marneffe et al, 2006).The input to the algorithm are a parse tree T anda set of target entities E. We first generate com-binations of 1-3 elements of E (line 10), then foreach combination C we identify all the nodes inT that mention any of the entities in C. We con-tinue by constructing the MST of these nodes, andfinally apply our heuristics to the nodes in theMST.
The procedure APPLYHEURISTICS (:16) re-cursively grows a nodeset N?by including chil-dren and parents of noun and verb nodes in N?based on dependency labels.
For example, we in-clude all children of verbs in N?whose label islisted in Vc(:3), e.g., active or passive subjects,direct or indirect objects, particles and auxiliaryverbs.
Similarly, we include the parent of a nounin N?if the dependency relation between the nodeand its parent is listed in Np.3 Pattern extraction by sentencecompressionSentence compression is a summarization tech-nique that shortens input sentences preserving themost important content (Grefenstette, 1998; Mc-Donald, 2006; Clarke and Lapata, 2008, interalia).
While first attempts at integrating a com-pression module into an extractive summarizationsystem were not particularly successful (Daum?eIII and Marcu, 2004, inter alia), recent workhas been very promising (Berg-Kirkpatrick et al,2011; Wang et al, 2013).
It has shown that drop-ping constituents of secondary importance fromselected sentences ?
e.g., temporal modifiers orrelative clauses ?
results in readable and more in-formative summaries.
Unlike this related work,our goal here is to compress sentences to obtainan event pattern ?
the minimal grammatical struc-ture expressing an event.
To our knowledge, thisapplication of sentence compressors is novel.
Asin Section 2, we only consider sentences mention-ing entities and require the compression (pattern)to retain at least one such mention.Sentence compression methods are abundantbut very few can be configured to produce out-put satisfying certain constraints.
For example,most compression algorithms do not accept com-pression rate as an argument.
In our case, sen-tence compressors which formulate the compres-sion task as an optimization problem and solve itwith integer linear programming (ILP) tools un-der a number of constraints are particularly attrac-tive (Clarke and Lapata, 2008; Filippova and Al-tun, 2013).
They can be extended relatively easilywith both the length constraint and the constrainton retaining certain words.
The method of Clarkeand Lapata (2008) uses a trigram language model(LM) to score compressions.
Since we are inter-ested in very short outputs, a LM trained on stan-dard, uncompressed text would not be suitable.
In-stead, we chose to modify the method of Filippovaand Altun (2013) because it relies on dependencyparse trees and does not use any LM scoring.Like other syntax-based compressors, the sys-tem of Filippova and Altun (2013) prunes depen-dency structures to obtain compression trees andhence sentences.
The objective function to maxi-mize in an ILP problem (Eq.
1) is formulated overweighted edges in a transformed dependency treeand is subject to a number of constraints.
Edgeweight is defined as a linear function over a fea-ture set: w(e) = w ?
f(e).F (X) =?e?Exe?
w(e) (1)In our reimplementation we followed the algo-rithm as described by Filippova and Altun (2013).The compression tree is obtained in two steps.First, the input tree is transformed with determin-istic rules, most of which aim at collapsing indis-pensable modifiers with their heads (determiners,auxiliary verbs, negation, multi-word expressions,etc.).
Then a sub-tree maximizing the objectivefunction is found under a number of constraints.Apart from the structural constrains from theoriginal system which ensure that the output is avalid tree, the constraints we add state that:1. tree size in edges must be in [3, 6],2. entity mentions must be retained,3.
subject of the clause must be retained,4.
the sub-tree must be covered by a singleclause ?
exactly one finite verb must used.894Since we consider compressions with differentlengths as candidates, from this set we select theone with the maximum averaged edge weight asthe final compression.
Figure 3 illustrates theuse of the compressor for obtaining event pat-terns.
Dashed edges are dropped as a result ofconstrained compression so that the output is JohnSmith married Mary Brown and the event patternis PER married PER.
Note that the root of a sub-clause is allowed to be the top-level node in theextracted compression.Compared with patterns obtaines with heuris-tics, compression patterns should retain preposi-tional verb arguments whose removal would ren-der the pattern ungrammatical.
As an exampleconsider [C. Zeta-Jones] and [M. Douglas] filedfor divorce.
The heuristics-based pattern is PERand PER filed which is incomplete.
Unlike it,the compression-based method keeps the essentialprepositional phrase for divorce in the pattern be-cause the average edge weight is greater for thetree with the prepositional phrase.4 Memory-based pattern extractionNeither heuristics-based, nor compression-basedmethods provide a guarantee that the extractedpattern is grammatically correct.
In this sec-tion we introduce an extraction technique whichmakes it considerably more likely because it onlyextracts patterns which have been observed asfull sentences in a human-written text (Sec.
4.1).However, this memory-based method also posesa problem not encountered by the two previousmethods: how to search over the vast space of ob-served headlines and sentences to extract a patternfrom a given sentence?
Our trie-based solution,which we present in the remainder of this sec-tion, makes it possible to compare a dependencygraph against millions of observed grammaticalutterances in a fraction of a second.4.1 A tree-trie to store them all.
.
.Our objective is to construct a compact representa-tion of hundreds of millions of observed sentencesthat can fit in the memory of a standard worksta-tion.
This data structure should make it possibleto efficiently identify the sub-trees of a sentencethat match any complete utterance previously ob-served.
To this end, we build a trie of depen-dency trees (which we call a tree-trie) by scan-ning all the dependency parses in the news trainingAlgorithm 2 STORE(T, I): store the dependency tree Tin the tree-trie I .1: /* Entry point /*2: L?
T.LINEARIZE()3: STORERECURSION(I.ROOT(), L, 0)4: return M5: /* Procedures /*6: procedure STORERECURSION(n,L, o)7: if o == L.LENGTH() then8: n.ADDTREESTRUCTURE(L.STRUCTURE())9: return10: if not n.HASCHILD(L.TOKEN(o)) then11: n.ADDCHILD(L.TOKEN(o))12: n??
n.GETCHILD(L.TOKEN(o))13: STORERECURSION(n?, L, o+ 1)data, and index each tree in the tree-trie accord-ing to Algorithm 2.
For better clarity, the processis also described graphically in Figure 4.
First,each dependency tree (a) is linearized, resultingin a data structure that consists of two aligned se-quences (b).
The first sequence (tokens) encodesword/parent-relation pairs, while the second se-quence (structure) encodes the offsets of parentnodes in the linearized tree.
As an example, thefirst word ?The?
is a determiner (?det?)
for the sec-ond node (offset 1) in the sequence, which is ?cat?.In turn, ?cat?
is the subject (?nsubj?)
of the nodein position 2, i.e., ?sleeps?.
As described in Algo-rithm 2, we recursively store the token sequencein the trie, each word/relation pair being stored ina node.
When the token sequence is completelyconsumed, we store in the current trie node thestructure of the linearized tree.
Combining struc-tural information with the sequential informationencoded by each path in the trie makes it possi-ble to rebuild a complete dependency graph.
Fig-ure 4(c) shows an example trie encoding 4 differ-ent sentences.
We highlighted in bold the path cor-responding to the linearized form (b) of the exam-ple parse tree (a).The figure shows that the tree contains twokinds of nodes: end-of-sentence (EOS) nodes(red) and non-terminal nodes (in blue).
EOS nodesdo not necessarily coincide with trie leaves, as itis possible to observe complete sentences embed-ded in longer ones.
EOS nodes differ from non-terminal nodes in that they store one or more struc-tural sequences corresponding to different syntac-tic representations of observed sentences with thesame tokens.Space-complexity and generalization.
Storingall the observed sentences in a single trie requireshuge amounts of memory.
To make it possible to895root Our sources report John Smith married Mary Brown in Baltimore yesterdayrootrootsubjsubjobjintmodFigure 3: Transformed dependency tree with a sub-tree expressing an event pattern.The cat sleeps under the tablerootnsubjdetpreppobjdet(a)ThedetcatnsubjsleepsROOTunderprepthedettablepobj1 2 -1 2 5 3(b)ThedetdognsubjbarkedROOT1,2,-1catnsubjsleepsROOT1,2,-1soundlyadvmod1,2,-1,2underprepthedettablepobj1,2,-1,2,5,3(c)Figure 4: A dependency tree (a), its linearized form (b) and the resulting path in a trie (c), in bold.store a complete tree-trie in memory, we adopt thefollowing strategy.
We replace the surface form ofentity nodes with the coarse entity type (e.g., PER,LOC, ORG) of the entity.
Similarly, we replaceproper nouns with the placeholder ?
[P]?, thus sig-nificantly reducing lexical sparsity.
Then, we en-code each distinct word/relation pair as a 32-bitunsigned integer.
Assuming a maximum tree sizeof 255 nodes, we represent structure sequences asvectors of type unsigned char (8 bit per element).Finally, we store trie-node children as sorted vec-tors instead of hash maps to reduce memory foot-print.
As a result, we are able to load a trie encod-ing 400M input dependency parses, 170M distinctnodes and 48M distinct sentence structures in un-der 10GB of RAM.4.2 .
.
.
and in the vastness match themAt lookup time, we want to use the tree-trie toidentify all sub-graphs of an input dependency treeT that match at least a complete observed sen-tence.
To do so, we need to identify all paths inthe trie that match any sub-sequence s of the lin-earized sequence of T nodes.
Whenever we en-counter an EOS node e, we verify if any of thestructures stored at e matches the sub-tree gener-ated by s. If so, then we have a positive match.As a sentence might embed many shorter utter-ances, each input T will generally yield multiplematches.
For example, querying the tree-trie inFigure 4(c) with the input tree shown in (a) wouldyield two results, as both The cat sleeps and Thecat sleeps under the table are complete utterancesstored in the trie.Algorithm 3 LOOKUP(T, I): Lookup for matches of sub-set of tree T in the trie index I .1: /* Entry point /*2: L?
T.LINEARIZE()3: M ?
?4: LOOKUPRECURSIVE(T,L, 0, I.ROOT(), ?,M)5: return M6: /* Procedures /*7: procedure LOOKUPRECURSIVE(T,L, o, n, P,M )8: for all i ?
[o, L.LENGTH()) do9: if n.HASCHILD(L.TOKEN(i)) then10: n??
n.GETCHILD(L.TOKEN(i))11: P??
P ?
{i}12: for all S ?
n?.TREESTRUCTURES() do13: if L.ISCOMPATIBLE(S, P?)
then14: M ?M ?
{T.GETNODES(P?
)}15: LOOKUPRECURSIVE(L, i, o+ 1, n?, P?,M)Algorithm 3 describes the lookup process inmore detail.
The first step consists in the lineariza-tion of the input tree T .
Then, we recursively tra-verse the trie calling LOOKUPRECURSIVE.
Theinputs of this procedure are: the input tree T , itslinearization L and an offset o (starting at 0), thetrie node currently being traversed n (starting withthe root), the set of offsets in L that constitute apartial match P (initially empty) and the set ofcomplete matches found M .
We recursively tra-verse all the nodes in the trie that yield a partialmatch with any sub-sequence of the linearized to-kens of T .
At each step, we scan all the tokensin L in the range [o, L.LENGTH()) looking for to-kens matching any of the children of n. If a match-ing node is found, a new partial match P?is con-structed by extending P with the matching token896Sheet6Page 10 10 20 30 40 50 60 70 80 901.0E-051.0E-041.0E-031.0E-021.0E-011.0E+001.0E+01f(x) = 1.9E-07 x^3.3E+00Tree size (number of nodes)Time (seconds)Figure 5: Time complexity of lookup operationsfor inputs of different sizes.offset i (line 11), and the recursion continues fromthe matching trie node n?and offset i (line 15).Every time a partial match is found, we verify ifthe partial match is compatible with any of thetree structures stored in the matching node.
If thatis the case, we identify the corresponding set ofmatching nodes in T and add it to the result M(lines 12-14).
A pattern is generated from eachcomplete match returned by LOOKUP after apply-ing a simple heuristic: for each verb node v in thematch, we enforce that negations and auxiliaries inT depending from x are also included in the pat-tern.Time complexity of lookups.
Let k be the max-imum fan-out of trie nodes, d be the depth ofthe trie and n be the size of an input tree (num-ber of nodes).
If trie node children are hashed(which has a negative effect on space complex-ity), then worst case complexity of LOOKUP() isO(nk)d?1.
If they are stored as sorted lists, as inour memory-efficient implementation, theoreticalcomplexity becomes O(nk log(k))d?1.
It shouldbe noted that worst case complexity can only beobserved under extremely unlikely circumstances,i.e., that at every step of the recursion all the nodesin the tail of the linearized tree match a child ofthe current node.
Also, in the actual trie used inour experiments the average branching factor k isvery small.
We observed that a trie storing 400Msentences (170M nodes) has an average branchingfactor of 1.02.
While the root of the trie has unsur-prisingly many children (210K, all the observedfirst sentence words), already at depth 2 the aver-age fan-out is 13.7, and at level 3 it is 4.9.For an empirical analysis of lookup complexity,Figure 5 plots, in black, wall-clock lookup timeas a function of tree size n for a random sampleof 1,600 inputs.
As shown by the polynomial re-gression curve (red), observed lookup complexityis approximately cubic with a very small constantfactor.
In general, we can see that for sentences ofcommon length (20-50 words) a lookup operationcan be completed in well under one second.5 Evaluation5.1 Experimental settingsAll the models for the experiments that we presenthave been trained using the same corpus ofnews crawled from the web between 2008 and2013.
The news have been processed with a to-kenizer, a sentence splitter (Gillick and Favre,2009), a part-of-speech tagger and dependencyparser (Nivre, 2006), a co-reference resolutionmodule (Haghighi and Klein, 2009) and an entitylinker based on Wikipedia and Freebase (Milneand Witten, 2008).
We use Freebase types as fine-grained named entity types, so we are also able tolabel e.g.
instances of sports teams as such insteadof the coarser label ORG.Next, the news have been grouped based ontemporal closeness (Zhang and Weld, 2013) andcosine similarity (using tf?idf weights).
For eachof the three pattern extraction methods we used thesame summarization pipeline (as shown above inFigure 2):1.
Run pattern extraction on the news.2.
For every news collection Coll and entity setE, generate a set containing all the extractedpatterns from news in Coll mentioning allthe entities in E. These are patterns that arelikely to be paraphrasing each other.3.
Run a clustering algorithm to group togetherpatterns that typically co-occur in the setsgenerated in the previous step.
There aremany choices for clustering algorithms (Al-fonseca et al, 2013; Zhang and Weld, 2013).Following Alfonseca et al (2013) we use inthis work a Noisy-OR Bayesian Network be-cause it has already been applied for abstrac-tive summarization (albeit multi-document),it provides an easily interpretable probabilis-tic clustering, and training can be easily par-allelized to be able to handle large trainingsets.
The hidden events in the Bayesian net-work represent pattern clusters.
When train-ing is done, for each extraction pattern pj897Original sentence Abstractive summary (method)Two-time defending overall World Cup champion Marcel Hirscher won thechallenging giant slalom on the Gran Risa course with two solid runs Sundayand attributed his victory to a fixed screw in his equipment setup.Marcel Hirscher has won the giantslalom.
(C)Zodiac Aerospace posted a 7.9 percent rise in first-quarter revenue, below mar-ket expectations, but reaffirmed its full-year financial targets.Zodiac Aerospace has reported a rise inprofits.
(C)Australian free-agent closer Grant Balfour has agreed to terms with the Balti-more Orioles on a two-year deal, the Baltimore Sun reported on Tuesday citingmultiple industry sources.Balfour will join the Baltimore Orioles.
(H)Paul Rudd is ?Ant-Man?
: 5 reasons he needs an ?Agents of SHIELD?
appear-ance.Paul Rudd to play Ant-Man.
(H)Millwall defender Karleigh Osborne has joined Bristol City on a two-and-a-halfyear deal after a successful loan spell.Bristol City have signed Karleigh Os-borne.
(M)Simon Hoggart, one of the Spectator?s best-loved columnists, died yesterdayafter fighting pancreatic cancer for over three years.Simon Hoggart passed away yesterday.
(M)Table 1: Abstraction examples from compression (C), heuristic (H) and memory-based (M) patterns.Method Extractions AbstractionsHEURISTIC 24,630 956COMPRESSION 15,687 657MEMORY-BASED 11,459 967Table 2: Patterns extracted in each method, beforeNoisy-OR inference.and pattern cluster ci, the network providesp(pj|ci) ?the probability that ciwill gener-ate pj?
and p(ci|pj) ?the probability that,given a pattern pj, ciwas the hidden eventthat generated it.At generation time we proceed in the followingway:1.
Given the title or first sentence of a news ar-ticle, run the same pattern extraction methodthat was used in training and, if possible, ob-tain a pattern p involving some entities.2.
Find the model clusters that contain this pat-tern, Cp= {cisuch that p(ci|p) > 0}.3.
Return a ranked list of model patternsoutput = {(pj, score(pj))}, scored as fol-lows:score(pj) =?ci?Cpp(pj|ci)p(ci|p)where p was the input pattern.4.
Replace the entity placeholders in the top-scored patterns pjwith the entities that wereactually mentioned in the input news article.In all cases the parameters of the network werepredefined as 20,000 nodes in the hidden layer(model clusters) and 40 Expectation Maximization(EM) training iterations.
Training was distributedacross 20 machines with 10 GB of memory each.For testing we used 37,584 news crawled dur-ing December 2013, which had not been used fortraining the models.
Table 3 shows one patterncluster example from each of the three trainedmodels.
The table shows only the surface formof the pattern for simplicity.Pattern cluster (MEMORY-BASED)organization1gets organization0nod for drugorganization1gets organization0nod for tabletsorganization0approves organization1drugorganizations0approves organization1?s drugorganization1gets organization0nod for capsulesPattern cluster (HEURISTIC)organization0to buy organization1organization0to acquire organization1organization0buys organization1organization0acquires organization1organization0to acquire organizations1organization0buys organizations1organization0acquires organizations1organization0agrees to buy organization1organization0snaps up organization1organization0to purchase organizations1organization0is to acquire organization1organization0has agreed to buy organization1organization0announces acquisition of organizations1organization0may bid for organization1organization1sold to organization0organization1acquired by organization0Pattern cluster (COMPRESSION)the sports team1have acquired person0from the sports team2the sports team1acquired person0from the sports team2the sports team2have traded person0to the sports team1sports team1acquired the rights to person0from sports team2sports team2acquired from sports team1in exchange for person0sports team2have acquired from the sports team1in exchange for person0Table 3: Examples of pattern clusters.
In eachcluster ci, patterns are sorted by p(pj|ci).8985.2 ResultsTable 2 shows the number of extracted patternsfrom the test set, and the number of abstractiveevent descriptions produced.As expected, the number of extracted patternsusing the memory-based model is smaller thanwith the two other models, which are based ongeneric rules and are less restricted in what theycan generate.
As mentioned, the memory-basedmodel can only extract previously-seen structures.Compared to this model, with heuristics we canobtain patterns for more than twice more news ar-ticles.
At the same time, looking at the numberof summary sentences generated they are com-parable, meaning that a larger proportion of thememory-based patterns actually appeared in thepattern clusters and could be used to produce sum-maries.
This is also consistent with the fact that us-ing heuristics the space of extracted patterns is ba-sically unbounded and many new patterns can begenerated that were previously unseen ?and thesecannot generate abstractions.
A positive outcomeis that restricting the syntactic structure of the ex-tracted patterns to what has been observed in pastnews does not negatively affect end-to-end cover-age when generating the abstractive summaries.Table 1 shows some of the abstractive sum-maries generated with the different methods.
Formanually evaluating their quality, a random sam-ple of 100 original sentences was selected for eachmethod.
The top ranked summary for each origi-nal sentence was sent to human raters for evalua-tion, and received three different ratings.
None ofthe raters had any involvement in the developmentof the work or the writing of the paper, and a con-straint was added that no rater could rate more than50 abstractions.
Raters were presented with theoriginal sentence and the compressed abstraction,and were asked to rate it along two dimensions, inboth cases using a 5-point Likert scale:?
Readability: whether the abstracted com-pression is grammatically correct.?
Informativeness: whether the abstractedcompression conveys the most important in-formation from the original sentence.Inter-judge agreement was measured using theIntra-Class Correlation (ICC) (Shrout and Fleiss,1979; Cicchetti, 1994).
The ICC for readabilitywas 0.37 (95% confidence interval [0.32, 0.41]),Method Readability InformativenessHEURISTIC 3.95 3.07COMPRESSION 3.98 2.35MEMORY-BASED 4.20 3.70Table 4: Results for the three methods when ratingthe top-ranked abstraction.and for informativeness it was 0.64 (95% confi-dence interval [0,60, 0.67]), representing fair andsubstantial reliability.Table 4 shows the results when rating the topranked abstraction using either of the three dif-ferent models for pattern extraction.
The abstrac-tions produced with the memory-based method aremore readable than those produced with the othertwo methods (statistically significant with 95%confidence).Regarding informativeness, the differences be-tween the methods are bigger, because the first twomethods have a proportionally larger number ofitems with a high readability but a low informa-tiveness score.
For each method, we have man-ually reviewed the 25 items where the differencebetween readability and informativeness was thelargest, to understand in which cases grammatical,yet irrelevant compressions are produced.
The re-sults are shown in Table 5.
Be+adjective includesexamples where the pattern is of the form Entity isAdjective, which the compression-based systemsextracts often represents an incomplete extraction.Wrong inference contains the cases where patternsthat are related but not equivalent are clustered,e.g.
Person arrived in Country and Person arrivedin Country for talks.
Info.
missing represents caseswhere very relevant information has been droppedand the summary sentence is not complete.
Pos-sibility contains cases where the original sentencedescribed a possibility and the compression statesit as a fact, or vice versa.
Disambiguation are en-tity disambiguations errors, and Opposite containscases of patterns clustered together that are op-posite along some dimension, e.g.
Person quitsTV Program and Person to return to TV Program.The method with the largest drop between thereadability and informativeness scores is COM-PRESSION.
As can be seen, many of these mis-takes are due to relevant information being miss-ing in the summary sentence.
This is also thelargest source of errors for the HEURISTIC system.For the MEMORY-BASED system, the drop in read-899Method Be+adjective Wrong inference Info.
missing Possibility Disambiguation OppositeHEURISTIC 0 7 14 3 1 0COMPRESSION 3 10 10 0 0 2MEMORY-BASED 0 17 4 2 0 2Table 5: Sources of errors for the top 25 items with high readability and low informativeness.Original sentence Pattern extracted (method) AbstractionDavid Moyes is happy to use tough love on Adnan Januzajto ensure the Manchester United youngster fulfils his mas-sive potential.David Moyes is happy.
(C)Fortune will start to favourDavid Moyes.The Democratic People?s Republic of Korea will ?achievenothing by making threats or provocation,?
the UnitedStates said Friday.The United States said Fri-day.
(C, H)United States officials saidFriday.EU targets Real and Barca over illegal state aid.EU targets Real Madrid.
(H)EU is going after RealMadrid.EU warns Israel over settlement constructionEU warns Israel.
(M)EU says Israel needs re-forms.Table 6: Examples of compression (C), heuristic (H) and memory-based (M) patterns that led to abstrac-tions with high readability but a low informativeness score.
Both incomplete summary sentences andwrong inferences can be observed.ability score is much smaller, so there were less ofthese examples.
And most of these examples be-long to the class of wrong inferences (patterns thatare related but not equivalent, so we should notabstract one of them from the other, but they wereclustered together in the model).
Our conclusionis that the examples with missing information arenot such a big problem with the MEMORY-BASEDsystem, as using the trie is an additional safeguardthat the generated titles are complete statements,but the method is not preventing the wrong infer-ence errors so this class of errors become the dom-inant class by a large margin.Some examples with high readability but lowinformativeness are shown in Table 6.6 ConclusionsMost Open-IE systems are based on linguistically-motivated heuristics for learning patterns that ex-press relations between entities or events.
How-ever, it is common for these patterns to be incom-plete or ungrammatical, and therefore they are notsuitable for abstractive summary generation of therelation or event mentioned in the text.In this paper, we describe a memory-based ap-proach in which we use a corpus of past newsto learn valid syntactic sentence structures.
Wediscuss the theoretical time complexity of look-ing up extraction patterns in a large corpus ofsyntactic structures stored as a trie and demon-strate empirically that this method is effective inpractice.
Finally, the evaluation shows that sum-mary sentences produced by this method outper-form heuristics and compression-based ones bothin terms of readability and informativeness.
Theproblem of generating incomplete summary sen-tences, which was the main source of informative-ness errors for the alternative methods, becomes aminor problem with the memory-based approach.Yet, there are some cases in which also the mem-ory based approach extracts correct but misleadingutterances, e.g., a pattern like PER passed awayfrom the sentence PER passed the ball away.
Tosolve this class of problems, a possible researchdirection would be the inclusion of more complexlinguistic features in the tree-trie, such as verb sub-categorization frames.As another direction for future work, more ef-fort is needed in making sure that no incorrect in-ferences are made with this model.
These happenwhen a more specific pattern is clustered togetherwith a less specific pattern, or when two non-equivalent patterns often co-occur in news as twoevents are somewhat correlated in real life, but it isgenerally incorrect to infer one from the other.
Im-provements in the pattern-clustering model, out-side the scope of this paper, will be required.900ReferencesEnrique Alfonseca, Daniele Pighin, and GuillermoGarrido.
2013.
HEADY: News headline abstractionthrough event pattern clustering.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics, Sofia, Bulgaria, 4?9 August2013, pages 1243?1253.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, Portland,OR, 19?24 June 2011.Domenic V Cicchetti.
1994.
Guidelines, criteria, andrules of thumb for evaluating normed and standard-ized assessment instruments in psychology.
Psycho-logical Assessment, 6(4):284.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
Journal of Artificial Intelli-gence Research, 31:399?429.Hal Daum?e III and Daniel Marcu.
2004.
A tree-position kernel for document compression.
In Pro-ceedings of the 2004 Document Understanding Con-ference held at the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics,, Boston,Mass., 6?7 May 2004.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation, Genoa, Italy,22?28 May 2006, pages 449?454.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, Edinburgh, UK, 27?29 July 2011, pages 1535?1545.Katja Filippova and Yasemin Altun.
2013.
Overcom-ing the lack of parallel data in sentence compression.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, Seattle,WA, USA, 18?21 October 2013, pages 1481?1491.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of the ILPfor NLP Workshop, Boulder, CO, June 4 2009, pages10?18.Gregory Grefenstette.
1998.
Producing intelligenttelegraphic text reduction to provide an audio scan-ning service for the blind.
In Working Notes of theWorkshop on Intelligent Text Summarization, PaloAlto, Cal., 23 March 1998, pages 111?117.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, Singapore, 6-7 August 2009, pages 1152?1161.Inderjeet Mani.
2001.
Automatic Summarization.John Benjamins, Amsterdam, Philadelphia.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics,Trento, Italy, 3?7 April 2006, pages 297?304.Yashar Mehdad, Giuseppe Carenini, and Frank W.Tompa.
2013.
Abstractive meeting summariza-tion with entailment and fusion.
In Proceedingsof the 14th European Workshop on Natural Lan-guage Generation, Sofia, Bulgaria, 8?9 August,2013, pages 136?146.David Milne and Ian H. Witten.
2008.
An effective,low-cost measure of semantic relatedness obtainedfrom Wikipedia links.
In Proceedings of the AAAI2008 Workshop on Wikipedia and Artificial Intelli-gence, Chicago, IL, 13-14 July, 2008.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Michael Schmitz, Robert Bart, Stephen Soderland,Oren Etzioni, et al 2012.
Open language learn-ing for information extraction.
In Proceedings ofthe 2012 Conference on Empirical Methods in Natu-ral Language Processing, Jeju Island, Korea, 12?14July 2012, pages 523?534.Patrick E Shrout and Joseph L Fleiss.
1979.
Intraclasscorrelations: uses in assessing rater reliability.
Psy-chological bulletin, 86(2):420.Karen Sp?arck-Jones.
2007.
Automatic summaris-ing: A review and discussion of the state of the art.Technical Report UCAM-CL-TR-679, University ofCambridge, Computer Laboratory, Cambridge, U.K.Ivan Titov and Alexandre Klementiev.
2011.
ABayesian model for unsupervised semantic parsing.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, Portland,OR, 19?24 June 2011, pages 1445?1455.Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-rian, and Claire Cardie.
2013.
A sentence com-pression based framework to query-focused multi-document summarization.
In Proceedings of the51st Annual Meeting of the Association for Com-putational Linguistics, Sofia, Bulgaria, 4?9 August2013, pages 1384?1394.Congle Zhang and Daniel S. Weld.
2013.
Harvest-ing parallel news streams to generate paraphrases ofevent relations.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle, WA, USA, 18?21 October 2013,pages 1776?1786.901
