An Incremental Connectionist Phrase Structure ParserJ ames  Henderson*U. of Pennsy lvan ia ,  Dept  of Computer  and In format ion  Science200 South  33rdPh i lade lph ia ,  PA 191041 IntroductionThis abstract outlines a parser implemented in a con-nectionist model of short term memory and reasoning 1 .This connectionist architecture, proposed by Shastri n\[Shastri and Ajjanagadde, 1990\], preserves the sym-bolic interpretation of the information it stores andmanipulates, but does its computations with nodeswhich have roughly the same computational proper-ties as neurons.
The parser recovers the phrase struc-ture of a sentence incrementally from beginning to endand is intended to be a plausible model of human sen-tence processing.
The formalism which defines thegrammars for the parser is expressive nough to in-corporate analyses from a wide variety of grammaticalinvestigations 2.
This combination gives a theory of hu-man syntactic processing which spans from the level oflinguistic theory to the level of neuron computations.2 The  Connect ion is t  Arch i tec-tureIn order to store and manipulate information in a con-nectionist net quickly, the information eeds to be rep-resented in the activation of nodes, not the connectionsbetween odes 3.
A property of an entity can be repre-sented by having a node for the entity and a node forthe property both active at the same time.
However,this only permits information about one entity to bestored at any one time.
The connectionist architectureused here solves this problem with nodes which, whenactive, fire at regular intervals.
A property is predi-cated of an entity only if their nodes are firing syn-*This research was supported by DARPA grant num-ber N0014-90-J-1863 and ARO grant number DAAL03-89-C0031PRI.lAs of this writing the parser has been designed, but notcoded.2A paper about a closely related formalism was submittedto this year's regular ACL session under the title "A CCG--LikeSystem of Types for Trees", and an older version of the laterwas discussed in my masters thesis (\[Henderson, 1990\]), whereits linguistic expressiveness i  demonstrated.3This section is a very brief characterization f the core sys-tem presented in \[Shastri and Ajjanagadde, 1990\].chronously.
This permits multiple ntities to be storedat one time by having their nodes firing in differentphases.
However, the number of entities is limited bythe number of distinct phases which can fit in the inter-val between periodic firings.
Such boundedness of hu-man conscious short term memory is well documented,where it is about seven entities.Computation using the information i  the memory isdone with pattern-action rules.
A rule is represented asa collection of nodes which look for a temporal patternof activation, and when it finds the pattern it modifiesthe memory contents.
Rules can compute in parallel.3 The Grammar  Formal ismI will describe grammar entries through the examplesgiven in figure 1.
Each entry must be a rooted treefragment.
Solid lines are immediate dominance links,dashed arrows are linear precedence constraints, anddotted lines, called dominance links, specify the needfor a chain of immediate dominance links.
Plus sub-scripts on nodes designate that the node is headed andminus subscripts that the node still needs a head.
Theparent of a dominance link must always be an:unheadednode.
Node labels are feature structures, but corefer-ence of features between odes is not allowed.
In thefigure, the structure for "likes" needs heads for bothits NP's, thereby expressing the subcategorization f rthese arguments.
The structure for '`white" expressesits modification of N's by having a headless root N.The structure for "who" subcategorizes for an S andspecifies that somewhere within that S there must be aheadless NP.
These four words can combine to form acomplete phrase structure tree through the appropriateequations of nodes.NP.
"~'VP+w~o NP?
p177aFigure 1: Example grammar entries.There are four operations for combining adjacent357tree f ragments  4.
The first equates  a node in the lefttree with the root of the right tree.
As in all equations,at least one of these nodes must be unheaded, and theirnode labels must unify.
If the node on the left is un-headed then it is subcategorisation, a d if the root isunheaded then it is modification.
The second combi-nation operation satisfies a dominance link in the lefttree by equating the node above the dominance link tothe root of the right tree and the node below the dom-inance link to another node in the right tree.
This isfor things such as attaching embedded subjects to theirverbs and filling subject gaps.
The third combinationoperation also involves a dominance link in the left sub-tree, but only the parent and root are equated and thedominance relationship is passed to an unheaded nodein the right tree.
This is for passing traces down thetree.
The last operation only involves one tree frag-ment.
This operation satisfies a dominance link byequating the child of the link with some node which isbelow the node which was the original parent of thisdominance link, regardless of what nodes the link hasbeen passed to with the third operation.
This is forgap filling.
Limitations on the purser's ability to de-termine what nodes are eligible for this equation forcesome known constraints on long distance movement.All these operations are restricted so that linear prece-dence constraints are never violated.The important properties of this formalism are itsuse of partiality in the specification of tree fragmentsand the limited domain affected by each combinationoperation.
Partiality is essential to allow the parser toincrementally specify what it knows so far about thestructure of the sentence.
The fact that each combi-nation operation is only dependent on a few nodes isimportant both because it simplifies the parser's rulesand because nodes which are no longer going to be in-volved in any equations can be forgotten.
Nodes mustbe forgotten in order to parse arbitrarily long sentenceswith the memory's very limited capacity.4 The  ParserThe parser builds the phrase structure for a sentenceincrementally from beginning to end.
After each wordthe short term memory contains information about thestructure bui l t  so far.
Pattern-action rules are thenused to compute how the next word's tree can be com-bined with the current ree.
In the memory, the nodesof the tree are the entities, and predicates are usedto specify the necessary information about these nodesand the relationships between them.
The tree in the4Thls formalism has both a structural interpretation and aCategorial Grmmnar style interpretation.
In the late~ interpre-tati~m these combination opez'atiorm have a more natural speci-fication.
Unfortunately space prevents me discueming it here.memory is used as the left tree for the combinationoperations.
The tree for the next word is the righttree.
For every grammar entry there are pattern-actionrules for each way it could participate in a combination.When the next word is identified its grammar entriesare activated and their rules each try to find a place inthe current ree where their combination can be done.The best match is chosen and that rule modifies thememory contents to represent the result of its combi-nation.
The gap filling combination operation is donewith a rule which can be activated at any time.
Ifthe parser does not have enough space to store all thenodes for the new word's tree, then any node which hasboth a head and an immediate parent can be removedfrom the memory without changing any predicationson other nodes.
When the parse is done it succeeds ifall nodes have heads and only the root doesn't have animmediate parent.Because nodes may be forgotten before the parse ofa sentence is finished, the output of the parser is not acomplete phrase structure tree.
The output is a list ofthe combinations which were done.
This is isomorphicto the complete phrase structure, since the structurecan be constructed from the combination i formation.It also provides incremental information about the pro-gression of the parse.
Such information could be usedby a separate short term memory module to constructthe semantic structure of the sentence in parallel withthe construction of the syntactic structure.Several characteristics make this parser interesting.Most importantly, the computational rchitecture ituses is compatible with what we know about the ar-chitecture of the human brain.
Also, its incrementalityconforms to our intuitions about the incrementality ofour sentence processing, even providing for incrementalsemantic analysis.
The parallelism in the combinationprocess provides for both lexical ambiguity and uncer-tainty about what word was heard.
Only further workcan determine the linguistic adequacy of the parser'sgrammars, but work on related formalisms provides ev-idence of its expressiveness.References\[Henderson, 1990\] James Henderson.
Structure Uni-fication Grammar: A Unifying Framework ForInvestigating Natural Language.
Technical Re-port MS-CIS-90-94, University of Pennsylvania,Philadelphia, PA, 1990.\[Shastri and Ajjanagadde, 1990\] Lokendra Shastri andVenkat Ajjanagadde.
From Simple Associationsto Systematic Reasoning: A Connectionist Repre-sentation of Rules, Variables and Dynamic Bind-ings.
Technical Report MS-CIS-90-05, Universityof Pennsylvania, Philadelphia, PA, 1990.358
