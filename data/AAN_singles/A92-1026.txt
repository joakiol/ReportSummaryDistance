Robust Processing of Real-World Natural-Language TextsJ e r ry  R .
Hobbs ,  Doug las  E .
Appe l t ,  J ohn  Bear ,  and  Mabry  TysonArt i f ic ia l  In te l l igence CenterSR, I In ternat iona lAbst rac tI1.
is often assumed that when natural anguageprocessing meets the real world, the ideal ofaiming for complete and correct interpretationshas to be abandoned.
However, our experiencewith TACITUS; especially in the MUC-3 eval-uation, has shown that.
principled techniquesfox' syntactic and pragmatic analysis can bebolstered with methods for achieving robust-ness.
We describe three techniques for mak-ing syntactic analysis more robust--an agenda-based scheduling parser, a recovery techniquefor failed parses, and a new technique called ter-minal substring parsing.
For pragmatics pro-cessing, we describe how the method of ab-ductive inference is inherently robust, in thatan interpretation is always possible, so that inthe absence of the required world knowledge,performance degrades gracefully.
Each of thesetechlfiques have been evaluated and the resultsof the evaluations are presented.1 In t roduct ionIf automatic text processing is to be a useful enterprise,it.
must be demonstrated that the completeness and ac-curacy of the information extracted is adequate for theapplication one has in nfind.
While it is clear that cer-tain applications require only a minimal level of com-petence from a system, it is also true that many appli-cationsrequire a very high degree of completeness anda.ccuracy in text processing, and an increase in capabilityin either area is a clear advantage.
Therefore we adoptan extremely lfigh standard against which the perfor-mance of a text processing system should be measured:it.
should recover all information that is implicitly or ex-plicitly present in the text, and it should do so withoutmaking mistakes.Tiffs standard is far beyond the state of the art.
It isan impossibly high standard for human beings, let alnemachines.
However, progress toward adequate text pro-cessing is best.
served by setting ambitious goals.
For thisreason we believe that, while it may be necessary in theintermediate term to settle for results that are far shortof this ultimate goal, any linguistic theory or systemarchitecture that is adopted should not be demonstra-bly inconsistent with attaining this objective.
However,if one is interested, as we are, in the potentially suc-cessful application of these intermediate-term systemsto real problems, it is impossible to ignore the questionof whether they can be made efficient enough and robustenough for application.1.1 The  TACITUS SystemThe TACITUS text processing system has been underdevelopment at SRI International for the last six years.This system has been designed as a first step towardthe realization of a system with very high completenessand accuracy in its ability to extract information fromtext.
The general philosophy underlying the design oIthis system is that the system, to the maximum extentpossible, should not discard any information that mightbe semantically or pragmatically relevant o a full, cor-rect interpretation.
The effect of this design philosophyon the system architecture is manifested in the followingcharacteristics:* TACITUS relies on a large, comprehensive l xiconcontaining detailed syntactic subcategorization i -formation for each lexieal item.. TACITUS produces a parse and semantic interpre-tation of each sentence using a comprehensive gram-mar of English in which different possible predicate-argument relations are associated with different syn-tactic structures.?
TACITUS relies on a general abductive reasoningmeclmnism to uncover the implicit assumptions nec-essary to explain the coherence of the explicit text.These basic design decisions do not by themselvesdistinguish TACITUS from a number of other natural-language processing systems.
However, they are some-what controversial given the intermediate goal of pro-ducing systems that are useful for existing applicationsCriticism of the overall design with respect to this goalcenters on the following observations:?
The syntactic structure of English is very complexand no grammar of English has been constructedthat has complete coverage of the syntax one en-counters in real-world texts.
Much of the text thaineeds to be processed will lie outside the scope olthe best grammars available, and therefore canno!be understood by a. system that relies on a complet(186syntactic analysis of each sentence as a prerequisiteto other processing.?
Typical sentences in newspaper articles are about25-30 words in length.
Many sentences are muchlonger.
Processing strategies that rely on producinga complete syntactic analysis of such sentences willbe faced with a combinatorially intractable task, as-suming in the first place that the sentences lie withinthe language described by the grammar.?
Any grammar that successfully accounts for therange of syntactic structures encountered in real-world texts will necessarily produce many alnbigt>ous analyses of most sentences.
Assuming that thesystem can find the possible analyses of a sentencein a reasonable period of time, it is still faced withthe problem of choosing the correct one from themany competing ones.Designers of application-oriented text processing sys-tems have adopted a number of strategies for (lea.l-ing with these problems.
Such strategies involve de-emphasizing the role of syntactic analysis (Jacobs et al,1991), producing partial parses with stochastic or heuris-tic parsers (de Marcken, 1990; Weischedel et al1991) orresorting to weaker syntactic processing methods uch asconceptual or case-frame based parsing (e.g., Schank andRiesbeck, 1981) or template matching techniques (Jack-son et M., 1991).
A common feature shared by theseweaker methods is that they ignore certain informationthat is present in the text, which could be extracted bya more comprehensive analysis.
The information thatis ignored may be irrelevant o a particular application,or relevant in only an insignificant handful of cases, andthus we cannot argue that approaches to text process-ing based on weak or even nonexistent syntactic and se-mantic analysis are doomed to failure in all cases andare not worthy of further investigation.
However, it isnot obvious how such methods can scale up to handlefine distinctions in attachment, scoping, and inference,although some recent attempts have been made in thisdirection (Cardie and Lehnert, 1991b).In the development of TACITUS, we have chosen adesign philosophy that assumes that a complete and ac-curate analysis of the text is being undertaken.
In thispaper we discuss how issues of robustness are approachedfrom this general design perspective.
In particular, wedemonstrate that?
useful partial analyses of the text can be obtained incases in which the text is not grammatical English,or lies outside the scope of the grammar's coverage,?
substantially correct parses of sentences can befound without exploring the entire search space foreach sentence,?
useful pragmatic interpretations can be obtainedusing general reasoning methods, even in cases inwhich the system lacks the necessary world knowl-edge to resolve all of the pragmatic problems posedin a sentence, and?
all of this processing can be done within acceptablebounds on computational resources.Our experience with TACITUS suggests that exten-sion of the system's capabilities to higher levels of com-pleteness and accuracy can be achieved through incre-mental modifications of the system's knowledge, lexiconand grammar, while the robust processing techniquesdiscussed in the following sections make the system us-able for intermediate term applications.
We have eva.lu-ated the success of the various techniques discussed here,and conclude fi'om this eva.hlation that TAC1TUS offerssubstantiatioll of our claim that a text.
processing systembased on principles of complete syntactic, semantic andpragmatic analysis need not.
be too brittle or computa-tionally expensive for practical applications.1.2 Eva luat ing  the  SystemSRI International participated in the recent M UC,-3 eval-uation of text-understanding systems (Sundheim, 1991).The methodolpgy chosen for this evaluation was to scorea system's ability to fill in slots in tenlplates ,nnmariz-ing the content of short (approximately 1 page) newspa-per articles on Latin American terrorism.
The template-filling task required identifying, among other things, theperpetrators and victims of each terrorist act describedin the articles, the occupation of the victims, the typ~of physical entity attacked or destroyed, the date, tilelocation, and the effect on the targets.
Frequently, arti-cles described multiple incidents, while other texts werecompletely irrelevant.A set of 1,300 such newspaper articles was selected onthe basis of the presence of keywords in the text, andgiven to participants as training data.
Several hundredtexts from the corpus were withheld for various phasesof testing.
Participants were scored on their ability tofill the templates correctly.
Recall and precision mea-sures were computed as an objective performance evalu-ation metric.
Variations in computing these metrics arepossible, but intuitively understood, recall measures thepercentage of correct fills a system finds (ignoring wrongand spurious answers), and precision measures the per-centage of correct fills provided out of the total numberof answers posited.
Thus, recall measures the complete-ness of a system's ability to extract information from atext, while precision measures it's accuracy.The TACITUS system achieved a recall of 44% with aprecision of 65% on templates for events correctly iden-tiffed, and a recall of 25% with a precision of 48% onall templates, including spurious templates the systemgenerated.
Our precision was the highest among theparticipating sites; our recall was somewhere in tile mid-dle.
Although pleased with these overall results, a sub-sequent detailed analysis of our performance on the first20 messages of the 100-message t st set is much moreilluminating for evaluating the success of the particu-lax robust processing strategies we have chosen.
In theremainder of this paper, we discuss the impact of therobust processing methods in the Tight of this detailedanalysis.2 Syntact i c  Ana lys i sRobust syntactic analysis requires a very large gram-mar and means for dealing with sentences that do not187parse, whether because they fall outside the coverageof the grammar or because they are too long for theparser.
The gral-nnaar used in TACITUS is that of theI)IAI~OCIC system, deweloped in 1980-81 essentially byconstructing the union of the Linguistic String ProjectG'ranmmr (Sager, 1981) and tile DIAGP~AM grammar(Robinson, 1982) which grew out of SRI's Speech Un-&:rst.anding System research in the 1970s.
Since thatt.imc il.
has been consid~'l'ably enhanced.
It consists ofabout 160 phrase structure rules.
Associated with eachrule is a "constructor" expressing the constraints on theapplicability of that rule, and a "translator" for produc-ing the logical form.The grannnar is comprehensive and includes subcat-egorization, sentential complements, adverbials, relativeclauses, complex determiners, the most common vari-eties of conjnnction and comparisou, selectional con-straints, some coreference resolution, and the most com-mon sentence fra.gments.
The parses are ordered accord-ing to heuristics encoded in the grammar.The parse tree is translated into a logical representa-tion of the nleaning of the sentence, encoding predicate-argument relations and grammatical subordination re-lations.
In addition, it regularizes to some extent therole assignments in the predicate-argument structure,and handles argnments inherited from control verbs.Our lexicon includes about 20,000 entries, includingabout 2000 personal names and about 2000 location,organization, or other names.
This number does notinclude morphological variants, which are handled in aseparate naorphological nalyzer.
(In addition, there arespecial procedures for handling unknown words, includ-ing unknown names, described in Hobbs et al, 1991.
)The syntactic analysis component was remarkably suc-cessful in the MUC,-3 evaluation.
This was due primarilyto three innovations.?
An agenda-based scheduling chart parser.?
A recovery heuristic for unparsable sentences thatfound the best sequence of gramnmtical fragments.?
The use of "ternfina.l substring parsing" for verylong sentences.Each of these techniques will be described in turn, withstatistics on their i)erformance in the MUC-a evaluation.2.1 Per fo rmance  of  the  Schedu l ing  Parser  andthe  GrammarTile fastest parsing algorithms for context-free grammarsmake use of prediction based on left context to limit thennmber of nodes and edges the parser must insert intotim chart.
However, if robustness in the face of pos-sibly ungramlnatical input or inadequate grammaticalcoverage is desired, such algorithms are inappropriate.Although the heuristic of choosing tile longest possiblesubstring beginning at the left, that can be parsed as asentence could be tried (e.g.
Grishman and Sterling,1989), solnetimes, the best fraglnentary analysis of asentence can only be found by parsing an intermediateor terminal substring that excludes the leftmost words.For this reason, we feel that bottom-up arsing withoutstrong constraints based on left context, are required forrobust syntactic analysis.Bottom-up parsing is favored for its robustness, andthis robustness derives from the fact that a bottom-upparser will construct nodes and edges in the chart thata parser with top-down prediction would not.
The obvi-ous problem is that these additional nodes do not comewithout an associated cost.
Moore and Dowding (1991)observed a ninefold increase ill time required to parsesentences with a straightforward C, KY parser as opposedto a shift-reduce parser.
Prior to November 1990, TAC-ITUS employed a simple, exhaustive, bottom-up arserwith the result that sentences of more than 15 to 20words were impossible to parse in reasonable time.
Sincethe average length of a sentence in the MUC-3 texts isapproximately 25 words, such techniqnes were clearly in-appropriate for the application.We addressed this problem by adding an agenda mech-anism to the bottom-up arser, based on Kaplan (1973),as described in Winograd (1983).
The purpose of theagenda is to allow us to order nodes (complete con-stituents) and edges (incomplete constituents) in thechart for further processing.
As nodes and edges arebuilt, they are rated according to various criteria forhow likely they are to figure in a correct parse.
Thisallows us to schedule which constituents to work withfirst so that we can pursue only the most likely pathsin the search space and find a parse without exhaus-tively trying all possibilities.
The scheduling algorithmis simple: explore the ramifications of the highest scoringconstituents first.In addition, there is a facility for pruning the searchspace.
The user can set limits on the number of nodesand edges that are allowed to be stored in the chart.Nodes are indexed on their atomic grammatical cate-gory (i.e., excluding features) and the string position atwhich they begin.
Edges are indexed on their atomicgrammatical category and tim string position wherethey end.
The algorithm for pruning is simple: Throwaway all but the n highest scoring constituents for eachcategory/string-position pair.It has often been pointed out that various stan-dard parsing strategies correspond to various schedulingstrategies in an agenda-based parser.
However, in practi-cal parsing, what is needed is a scheduling strategy thatenables us to pursue only the most likely paths in thesearch space and to find the correct parse without ex-haustively trying all possibilities.
The literature has notbeen as ilhnninating on this issue.We designed our parser to score each node and edgeon the basis of three criteria:?
The length of the substring spanned by the con-stituent.?
Whether the constituent is a node or an edge, thatis, whether the constituent is complete or not.?
The scores derived from the preference heuristicsthat have been encoded in DIALOGIC over theyears, described and systematized in Hobbs andBear (1990).However, after considerable experimentation with var-188ious weightings, we concluded that tile length and com-pleteness factors failed to improve the performance a.tall over a broad range of sentences.
Evidence suggestedthat a score based on preference factor alone producesthe best results.
The reason a correct or nearly correctparse is found so often by this method is that these pref-erence heuristics are so effective.In the frst  20 messages of the test set., 131 sentenceswere given to the scheduling parser, after statisticallybased relevance filtering.
A parse was produced for 81of the 131 sentences, or 62%.
Of these, 4:3 (or 33%)were completely correct, and 30 more had three or fewererrors.
Thus, 56% of the sentences were parsed correctlyor nearly correctly.These results naturally vary depending oil the lengthof the sentences.
There were 64 sentences of under 30naorphemes (where by "morpheme" we mean words plusinflectional affixes).
Of these, 37 (58%) had completelycorrect parses and 48 (75%) had three or fewer errors.By contrast, the scheduling parser attempted only 8 sen-tences of more than 50 morphemes, and only two of theseparsed, neither of them even nearly correctly.Of the 44 sentences that would not parse, nine weredue to problems in lexical entries.
Eighteen were due toshortcomings in the grammar, primarily involving adver-bial placement and less than fully general treatment ofconjunction and comparatives.
Six were due to garbledtext.
The causes of eleven failures to parse have not beendetermined.
These errors are spread out evenly acrosssentence lengths.
In addition, seven sentences of over 30lnorphemes hit the time limit we had set, and terminalsubstring parsing, as described below, was invoked.A majority of the errors in parsing can be attributedto five or six causes.
Two prominent causes are the ten-dency of the scheduling parser to lose favored close at-tachments of conjuncts and adjuncts near the end of longsentences, and the tendency to misanalyze the string\[\[Noun Noun\]Np Verbt,.an, NP\]sas\[Noun\]Np \[Noun Verbditran8 0 NP\]s/Np,again contrary to the grammar's preference heuristics.We believe that most of these problems are due to thefact that the work of the scheduling parser is not dis-tributed evenly enough across the different parts of thesentence, and we expect that this difficulty could besolved with relatively little effort.Our results in syntactic analysis are quite encouragingsince they show that a high proportion of a corpus oflong and very complex sentences can be parsed nearlycorrectly.
However, the situation is even better whenone considers the results for the best-fragment-sequenceheuristic and for terminal substring parsing.2.2 Recovery  f rom Fai led ParsesWhen a sentence does not parse, we attempt to spanit with the longest, best sequence of interpretable frag-ments.
The fragments we look for are main clauses, verhphrases, adverbial phrases, and noun phrases.
They arechosen on the basis of length and their preference scores,favoring length over preference score.
We do not attemptto find fragments for strings of less than five morphemes.The effect of this heuristic is that even for sentences thatdo not parse, we are able to extract nearly all of thepropositional content.For example, the sentenceThe attacks today come afl.er Shining Pathattacks during which least 10 buses wereburned throughout Lima on 24 Oct.did not parse because of the use of "least" instead of "a.t.least".
Hence, the best.
Dagment sequence was sought.This consisted of the two fragments "The attacks todaycome after Shining Path attacks" and "10 buses wereburned thronghout Lima on 24 Oct." The parses forboth these fragments were completely correct.
Thus, theonly information lost was from the three words "duringwhich least".
Frequently such information can be recap-tured by the pragmatics component.
In this case, theburning would be recognized as a consequence of the at-tack.In tile first 20 messages of the test set, a best sequenceof fragments was sought for the 44 sentences that didnot parse for reasons other than timing.
A sequence wasfound for 41 of these; the other three were too short, withproblems in the middle.
The average number of frag-ments in a sequence was two.
This means that an averageof only one structural relationship was lost.
Moreover,the fragments covered 88% of the morphemes.
That is,even in the case of failed parses, 88% of the proposi-tional content of the sentences was made available topragmatics.
Frequently the lost propositional content isfrom a preposed or postposed, temporal or causal adver-bial, and the actual temporal or causal relationship isreplaced by simple logical conjunction of the fragments.In such cases, much useful information is still obtainedfl'om the partial results.For .37% of the 41 sentences, correct syntactic analysesof the fragments were produced.
For 74%, the analysescontained three or fewer errors.
Correctness did not cor-relate with length of sentence.These numbers could probably be improved.
Wefavored the longest fragment regardless of preferencescores.
Thus, frequently a high-scoring main clause wasrejected because by tacking a noun onto the front of thatfragment and reinterpreting the main clause bizarrelyas a relative clause, we could form a low-scoring nounphrase that was one word longer.
We therefore plan toexperiment with combining length and preference scorein a more intelligent manner.2.3 Termina l  Subst r ing  Pars ingFor sentences of longer than 60 words and for faster,though less accurate, parsing of shorter sentences, wedeveloped a technique we are calling lerminal subsiringparsing.
The sentence is segmented into substrings, bybreaking it at commas, conjunctions, relative pronouns,and certain instances of the word "that".
The substringsare then parsed, starting with the last one and workingback.
For each substring, we try either to parse thesubstring itself as one of several categories or to parsethe entire set of substrings parsed so far as one of thosecategories.
The best such structure is selected, and for189subsequent processing, that is the only analysis of thatportion of the sentence allowed.
The categories that welook for include main, subordinate, and relative clauses,infinitives, w'H) phrases, prepositional phrases, and nounp h rases.A simple exalnple is |,lie following, although we do nota.I)ply the technique to sentences or to fragments thisshort.
(.h>org(~ \]}US\]l, l.lie president, held a press con-feren(:e yesterda.y.This sentellc(~ would be segmented a.t the conunas.
First'<hehl a. press conference yesterday" would be recognizedas a VP.
We next try to parse both <<the president" and"the presidellt, VP".
The string "the president, VP"would not be recognized as anything, but "the presi-dent" would be recognized as an NP.
Finally, we try toparse both "George Bush" and <<George Bush, NP, VP".
"George Bush, NP, VP" is recognized as a sentence withan appositive on t.he subject.This algorithm is superior to a more obvious a.lgorithnlwe had been considering earlier, llamely, to parse eachfragment individually in a left-to-right fashion and thento a.ttempt o piece the fi'agments together.
The lat-ter algorithm would have required looking inside all butthe last of tile fi'agments for possible attachment points.This problem of recombining parts is in general a diffi-culty that is faced by parsers thai, produce phrasal ratherthan sentential parses (e.g., Weischedel et al, 1991).ltowever, in terminal substring parsing, this recombiningis not, necessary, since the favored analyses of subsequentseginents are already available when a given segment isbeing parsed.The effect of this terminal substring parsing techniqueis to give only short inputs to the parser, without los-ing the possibility of getting a single parse for the entirelong sentence.
Suppose, for exa.lnple, we are parsing a60-word seni.ence that can be broken into six 10-wordsegments.
At.
each stage, we will only be parsing a stringof ten to fifteen "words", the ten words in the segment,phls the nonterminal symbols dominating the favoredanalyses of the subsequent segments.
When parsing thesentence-initial 10-word substring, we are in effect pars-ing at most a "IS-word" string covering the entire sen-tence, consisting of the 10 words plus the nontermina.1symbols covering the best analyses of the other five sub-strings.
In a. sense, rather than parsing one very longsentence, we are parsing six fairly short sentences, thusavoiding the combinatorial explosion.Although this algorithm has given us satisfactory re-suits in our development work, its nnmbers fl'om theMUC-3 evahiation do not look good.
This is not sur-prising, given that tile technique is called on only whenall else has already failed.
In tile first 20 messages of thetest set, terlninal substring parsing was applied to 14sentences, ranging fl'om 34 to 81 morphemes in length.Only one of these parsed, and that parse was not good.However, sequences of fragments were found for the other1:3 sentences.
The average number of fragments was 2.6,and the sequences covered 80% of the morphelnes.
Noneof the fragment sequences was without errors.
However,eight of the 13 had three or fewer mistakes.
The tech-nique therefore allowed us to make use of much of theinformation in sentences that have hitherto been beyondthe capability of virtually all parsers.3 Robust  Pragmatic InterpretationWhen a sentence is parsed and given a semantic interpre-tation, the relationship between this interpretation andthe information previously expressed in the text as wellas the interpreter's general knowledge must be estab-lished.
Establishing this relationship comes under tilegeneral heading of pragmatic interpretation.
The par-ticular problems that are solved during this step include* Making explicit information that is only implicit inthe text.
This includes, for example, explicatingthe relationship underlying a coinpound nominal, orexplicating causal consequences of events or statesmentioned explicitly ill the text.?
Determining the implicit entities and relationshipsreferred to metonymically in the text.?
Resolving anaphoric references and implicit argu-lnents.?
Viewing the text as an instance of a. schema thatmakes its various parts coherent.TACITUS interprets a sentence pragmatically byproving that its logical form follows fi'om general knowl-edge and the preceding text, allowing a lninimal set otassumptions to be made.
In addition, it is assuined thatthe set of events, abstract entities, and physical objectsmentioned in the text is to be consistently minimizedThe best set of assumptions necessary to find such aproof can be regarded as an explanation of its truth, andconstitutes the implicit information required to producethe interpretation (Hobbs, Stickel, et al, 1990).
Th(minimization of objects and events leads to anaphoreresolution by assuming that objects that share proper-ties are identical, when it is consistent o do so.In the MUC-3 domain, explaining a text involves view-ing it as an instance of one of a number of explanator)schemas representing terrorist incidents of various type,(e.g.
bombing, arson, assassination) or one of severa:event types that are similar to terrorist incidents, buiexplicitly excluded by the task requirements (e.g.
an ex-change of fire between military groups of opposing fac-tions).
This means that assumptions that fit into inci.dent schemas are preferred to a.ssun~ptions that do notand the schema that ties together the most assumption=is the best explanation.In this text interpretation task, the domain knowledg,performs two primary functions:1.
It relates the propositions expressed in the text t<the elements of the underlying explanatory schemas2.
It enables and restricts possible coreferences fo:anaphora resolution.It is clear that nmch domain knowledge may be required to perform these functions successfully, but it i~not necessarily the case that more knowledge is alwaybetter.
If axioms are incrementally added to the systento cover cases not accounted for in the existing domaiJ190theory, it is possiMe that they can interact with the exist-ing knowledge in such a way that the reasoning processbecomes computationally intractable, and the unhappyresult would be failure to find an interpretation i casesin which the correct interpretation is entailed by the sys-tem's knowledge.
In a. domain as broad and diffuse asthe terrorist domain, it is often impossible to guaranteeby inspection that a domain theory is not subject to suchcombinatorial problems.The goal of robustness in interpretation therefore re-quires one to address two problems: a system must per-mit a graceful degradation of performance in those casesin which knowledge is incomplete, and it must extractas much information as it can in the face of a possiblecombinatorial explosion.The general approach of abductive text interpretationaddresses the first problem through the notion of a "bestinterpretation."
The best explanation, given incompletedomain knowledge, can succeed at relating some propo-sitions contained in the text to the explanatory schemas,but may not succeed for all propositions.
The combina-torial problems are addressed through a particular searchstrategy for abductive reasoning described as incremen-tal refinement of minimal.informalion proofs.The abductive proof procedure as employed by TAC-ITUS (Stickel, 1988) will always be able to find some in-terpretation of the text.
In the worst cause--the absenceof any commonseuse knowledge that would be relevantto the interpretation of a sentence--the explanation of-fered would be found by a.ssunaing each of the literals tobe proved.
Such a proof is called a "minimal informa-tion proof" because no schema recognition or explicationof implicit relationships takes place.
However, the moreknowledge the system has, the more implicit informationca.n be recovered.Because a minimal information proof is always avail-able for any sentence of the text that is internally consis-tent, it provides a starting point for incremental refine-ment of explanations that can be obtained at next to nocost.
TACITUS explores the space of abductive proofsby finding incrementally better explanations for each ofthe constituent literMs.
A search strategy is adoptedthat finds successive xplanations, each of which is bet-ter than the minimal information proof.
This processcan be halted at any time in a state that will provide atleast some intermediate results that are useful for sub-sequent interpretation and template filling.Consider the following example taken'fi'om the MUC-3text corpus:A cargo train running kom Lima to Lorohiawas derailed before dawn today after hittinga dynamite charge.Inspector Eulogio Flores died in the explosion.The correct interpretation of this text requires recov-ering certain implicit information that relies on common-sense knowledge.
The compound nominal phrase "dyna-mite charge" nmst be interpreted as "charge composedof dynamite."
The interpretation requires knowing thatdynamite is a substance, that substances can be relatedvia compound nominal relations to objects composed ofthose substances, that things composed of dynamite arebombs, that hitting bombs causes them to explode, thatexploding causes damage, that derailing is a type of clam-age, and that planting a bomb is a terrorist act.
The sys-tem's commonsense knowledge base must be rich enoughto derive each of these conclusions if it is to recognizethe event described as a. terrorist act., since all derailingsare not the result of' bombings.
This example under-scores the need for fa.irly extensive world knowledge inthe comprehension of text.
If the knowledge is missing,the correct interpretation cannot be found.However, if there is Inissing knowledge, all is not nec-essarily lost.
If, for example, the knowledge was miss-ing that lilt.ring a boml~ causes it to explode, the sys-rein could still hyl.mthesize the relationship between tilecharge and tile (lynamite to reason that a bomb wasplaced.
When processing the next sentence, the systemmay have trouble figuring out tile time and place of Flo-res's death if it can't associate the explosion with hittingthe bomb.
However, if the second sentence were "TileShining Path claimed that their guerrillas had plantedthe bomb," the partial infornm.tion would be sufficient oallow "bomb" to be resolved to dynamite charge, therebyconnecting the event described in tile first, sentence with~che vent described ill the second.It is difficult to evahmte the pragmatic interpretationcomponent individually, since to a great extent its suc-cess depends on the adequacy of the semantic analysisit operates on.
Itowew~r, in examiuing the first, 20 mes-sages of the MUC-3 test set.
in detail, we attempted topinpoint the reason for each missing or incorrect entryin the required templates.There were 269 sucl~ mistakes, due to problems in 41sentences.
Of these, 124 are attributable to pragmaticinterpretation.
We have classified their causes into anumber of categories, and the results are as follows.l:{ea.sonSimple Axiom Missing 49Combinatorics 28Unconstrained Identity Assumptions 25Complex Axioms or Theory Missing 14Underconstrained Axiom 8MistakesAn example of a missing simple axiom is that "bishop"is a profession.
An exa.nlple of a. missing complex the-ory is one that assigns a default causality relationshipto events that are simultaneous at the granularity re-ported in the text.
An underconstrained axiom is onethat allows, for examl)le, "damage to the economy" tobe taken a.s a terrorist, incident.
Unconstrained identityassumptions result from the knowledge base's inabilityto rule out identity of two different objects with similarproperties, thus leading to incorrect anaphora resolution.
"Combinatorics" simply means that the theorem-provertimed out, and the nfinimal-information proof strategywas invoked to obtain a. partial interpretation.It is difficult to evaluate the precise impact of the ro-bustness trategies outlined here.
The robustness is aninherent feature of the overall al)proach, and we did nothave a non-robust control to test.
it against.
However, theimplementation of the mhlilnal information proof search191strategy virtually eliminated all of our complete t'a.iluresdue to lack of computational resources, and cut the errorrate attributable to this cause roughly in half.4 ConclusionIt is often assumed that when natural anguage process-tug meets the real world, the ideal of aiming for con>plete and correct interpretations has to be abandoned.llowcver, our experience with TACITUS, especially inthe M UC-3 evaluation, has shown that principled tech-niques for syntactic and pragmatic analysis can be bol-stered with methods for achieving robustness, yielding asystem with some utility in the short term and showingpromise of more in tim long term.AcknowledgmentsThis research has been funded by the Defense AdvancedResearch Projects Agency under Office of Naval Re-search contracts N00014-85-C-0013 and N00014-90-C-0220.References\[1\] Cardie, Claire and Wendy Lehnert, 1991.
"A Cogni-tively Plausible Approach to Understanding ComplexSyntax," Proceedings, Ninth National Conference onArtificial Intelligence, pp.
117-124.\[2\] Grishman, R., and J.
Sterling, 1989.
"PreferenceSemantics for Message Understanding, Proceedings,DARPA Speech and Natural-Language Workshop, pp.71-74.\[3\] Bobbs, Jerry R., 1978.
"Resolving Pronoun Refer-ences", Lingua, Vol.
44, pp.
311-338.
Also ill Readingsin Natural Language Processing, B. Grosz, K. Sparck-Jones, and B. Webber, editors, pp.
339-352, MorganKaufmann Publishers, Los Altos, Califonlia.\[4\] ltobbs, Jerry R., and John Bear, 1990.
"Two Princi-ples of Parse Preference", in H. Karlgren, ed., Proceed-tags, Thirteenth International Conference on Compu-tational Linguistics, Helsinki, Finland, Vol.
3, pp.
162-167, August, 1990.\[5\] Hobbs, Jerry R., Mark Stiekel, Douglas Appelt, andPaul Martin, 1990.
"Interpretation as Abduction",SRI International Artificial Intelligence Center Tech-nical Note 499, December 1990.\[6\] Jackson, Eric, Douglas Appelt, John Bear, RobertMoore, and Ann Podlozny, 1991.
"A TemplateMa.tcher for Robust NL Interpretation", Proceedings,DARPA Speech and Natural Language Workshop,February 1991, Asilomar, California, pp.
190-194.\[7\] Jacobs, Paul S., George R. Krupka, and Lisa.
F. Rau,1991.
"Lexico-Sen~a.ntic Pattern Matching as a Com-panion to Parsing in Text Understanding", Proceed-tags, DARPA Speech and Natural Language Work-shop, February 1991, Asilomar, California, pp.
337-341.\[8\] Kaplan, Ronald, 1973.
"A General Syntactic Proces-sor," in Ra.ndM1 Rustin, (Ed.)
Natural Language Pro-cessing, Algorithmics Press, New York, pp.
193-241.\[9\] de Marcken, C.G., 1990.
"Parsing the LOB Corpus,"Proceedings, 28th Annual Meeting of the Associationfor Computational Linguistics, pp.
243-251.\[10\] Moore, R.C., and J. Dowding, 1991.
"EfficientBottom-Up Parsing," Proceedings, DARPA Speechand Natural Language Workshop, February 1991,Asilomar, California, pp.
200-203.\[11\] Robinson, Jane, 1982.
"DIAGRAM: A Grammar forDialogues", Communications ofthe A CM, Vol.
25, No.1, pp.
27-47, January 1982.\[I2\] Sager, Naomi, 1981.
Natural Language Inform.a-lion Processing: A Computer Grammar of English.and Its Applications, Addison-Wesley, Reading, Mas-sachusetts.\[13\] Sehank, Roger and C. Riesbeck, 1981.
Inside Com-puter Understanding: Five Programs Plus Miniatures,Lawrence Erlbaum, Hillsdale, New Jersey.\[14\] Stickel, Mark E., 1988.
"A Prolog-like InferenceSystem for Computing Minimum-Cost AbductiveExplanations in Natural-Language Interpretation",Proceedings of the International Computer ScienceConference-88, pp.
343-350, Hong Kong, December1988.
Also published as Technical Note 451, ArtificialIntelligence Center, SRI International, Menlo Park,California, September 1988.\[15\] Sundheim, Beth (editor), 1991.
Proceedings, ThirdMessage Understanding Conference (MUC-3), SanDiego, California, May 1991.\[16\] Weisehedel, R., D. Ayuso, S. Boisen, R. Ingria, andJ.
Palmucci, 1991.
"Partial Parsing: A Report onWork in Progress, Proceedings, DARPA Speech andNatural Language Workshop, February 1991, Asilo-mar, California, pp.
204-209.\[17\] Winograd, Terry, 1983.
Language as a CognitiveProcess, Addison-Wesley, Menlo Park, California.192
