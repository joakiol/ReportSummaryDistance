Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 290?298,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemi-Automatic Entity Set RefinementVishnu Vyas and Patrick PantelYahoo!
LabsSanta Clara, CA 95054{vishnu,ppantel}@yahoo-inc.comAbstractState of the art set expansion algorithms pro-duce varying quality expansions for differententity types.
Even for the highest quality ex-pansions, errors still occur and manual re-finements are necessary for most practicaluses.
In this paper, we propose algorithms toaide this refinement process, greatly reducingthe amount of manual labor required.
The me-thods rely on the fact that most expansion er-rors are systematic, often stemming from thefact that some seed elements are ambiguous.Using our methods, empirical evidence showsthat average R-precision over random entitysets improves by 26% to 51% when givenfrom 5 to 10 manually tagged errors.
Bothproposed refinement models have linear timecomplexity in set size allowing for practicalonline use in set expansion systems.1 IntroductionSets of named entities are extremely useful in avariety of natural language and information re-trieval tasks.
For example, companies such as Ya-hoo!
and Google maintain sets of named entitiessuch as cities, products and celebrities to improvesearch engine relevance.Manually creating and maintaining large sets ofnamed entities is expensive and laborious.
In re-sponse, many automatic and semi-automatic me-thods of creating sets of named entities have beenproposed, some are supervised (Zhou and Su,2001), unsupervised (Pantel and Lin 2002, Nadeauet al 2006), and others semi-supervised (Kozarevaet al 2008).
Semi-supervised approaches are oftenused in practice since they allow for targeting spe-cific entity classes such as European Cities andFrench Impressionist Painters.
Methods differ incomplexity from simple ones using lexico-syntactic patterns (Hearst 1992) to more compli-cated techniques based on distributional similarity(Pa?ca 2007a).Even for state of the art methods, expansion er-rors inevitably occur and manual refinements arenecessary for most practical uses requiring highprecision (such as for query interpretation at com-mercial search engines).
Looking at expansionsfrom state of the art systems such as GoogleSets1 ,we found systematic errors such as those resultingfrom ambiguous seed instances.
For example, con-sider the following seed instances for the target setRoman Gods:Minerva, Neptune, Baccus, Juno,ApolloGoogleSet?s expansion as well others employingdistributional expansion  techniques consists of amishmash of Roman Gods and celestial bodies,originating most likely from the fact that Neptuneis both a Roman God and a Planet.
Below is anexcerpt of the GoogleSet expansion:Mars, Venus, *Moon, Mercury,*asteroid, Jupiter, *Earth,*comet, *Sonne, *Sun, ?The inherent semantic similarity between the errorscan be leveraged to quickly clean up the expan-sion.
For example, given a manually tagged error?asteroid?, a distributional similarity thesaurus1 http://labs.google.com/sets290such as (Lin 1998)2 can identify comet as similar toasteroid and therefore potentially also as an error.This method has its limitations since a manuallytagged error such as Earth would correctly removeMoon and Sun, but it would also incorrectly re-move Mars, Venus and Jupiter since they are alsosimilar to Earth3.In this paper, we propose two algorithms to im-prove the precision of automatically expanded enti-ty sets by using minimal human negativejudgments.
The algorithms leverage the fact thatset expansion errors are systematically caused byambiguous seed instances which attract incorrectinstances of an unintended entity type.
We use dis-tributional similarity and sense feature modeling toidentify such unintended entity types in order toquickly clean up errors with minimal manual labor.We show empirical evidence that average R-precision over random entity sets improves by 26%to 51% when given from 5 to 10 manually taggederrors.
Both proposed refinement models have li-near time complexity in set size allowing for prac-tical online use in set expansion systems.The remainder of this paper is organized as fol-lows.
In the next section we review related workand position our contribution within its landscape.Section 3 presents our task of dynamically model-ing the similarity of a set of words and describesalgorithms for refining sets of named entities.
Thedatasets and our evaluation methodology used toperform our experiments are presented in Section 4and in Section 5 we describe experimental results.Finally, we conclude with some discussion andfuture work.2 Related WorkThere is a large body of work for automaticallybuilding sets of named entities using various tech-niques including supervised, unsupervised andsemi-supervised methods.
Supervised techniquesuse large amounts of training data to detect andclassify entities into coarse grained classes such asPeople, Organizations, and Places (Bunescu andMooney 2004; Etzioni et al 2005).
On the otherhand, unsupervised methods require no training2 See http://demo.patrickpantel.com/ for a demonstration ofthe distributional thesaurus.3 In practice, this problem is rare since most terms that aresimilar in one of their senses tend not to be similar in theirother senses.data and rely on approaches such as clustering,targeted patterns and co-occurrences to extract setsof entities (Pantel and Lin 2002; Downey et al2007).Semi-supervised approaches are often used inpractice since they allow for targeting specific enti-ty classes.
These methods rely on a small set ofseed examples to extract sets of entities.
They ei-ther are based on distributional approaches or em-ploy lexico-syntactic patterns to expand a small setof seeds to a larger set of candidate expansions.Some methods such as (Riloff and Shepherd 1997;Riloff and Jones 1999; Banko et al 2007;Pa?ca2007a)  use lexico-syntactic patterns to expand aset of seeds from web text and query logs.
Otherssuch as (Pa?ca et al 2006; Pa?ca 2007b; Pa?ca andDurme 2008) use distributional approaches.
Wangand Cohen (2007) use structural cues in semi-structured text to expand sets of seed elements.
Inall methods however, expansion errors inevitablyoccur.
This paper focuses on the task of postprocessing any such system?s expansion outputusing minimal human judgments in order to re-move expansion errors.Using user feedback to improve a system?s per-formance is a common theme within many infor-mation retrieval and machine learning tasks.
Oneform of user feedback is active learning (Cohn etal.
1994), where one or more classifiers are used tofocus human annotation efforts on the most benefi-cial test cases.
Active learning has been successful-ly applied to various natural language tasks such asparsing (Tang et al 2001), POS tagging (Daganand Engelson 1995) and providing large amountsof annotations for common natural languageprocessing tasks such as word sense disambigua-tion (Banko and Brill 2001).
Relevance feedback isanother popular feedback paradigm commonlyused in information retrieval (Harman 1992),where user feedback (either explicit or implicit) isused to refine the search results of an IR system.Relevance feedback has been successfully appliedto many IR applications including content-basedimage retrieval (Zhouand Huang 2003) and websearch (Vishwa et al 2005).
Within NLP applica-tions relevance feedback has also been used togenerate sense tagged examples for WSD tasks(Stevenson et al 2008), and Question Answering(Negri 2004).
Our methods use relevance feedbackin the form of negative examples to refine the re-sults of a set expansion system.2913 Dynamic Similarity ModelingThe set expansion algorithms discussed in Section2 often produce high quality entity sets, howeverinevitably errors are introduced.
Applications re-quiring high precision sets must invest significant-ly in editorial efforts to clean up the sets.
Althoughcompanies like Yahoo!
and Google can afford toroutinely support such manual labor, there is alarge opportunity to reduce the refinement cost(i.e., number of required human judgments).Recall the set expansion example of RomanGods from Section 1.
Key to our approach is thehypothesis that most expansion errors result fromsome systematic cause.
Manual inspection of ex-pansions from GoogleSets and distributional setexpansion techniques revealed that most errors aredue to the inherent ambiguity of seed terms (suchas Neptune in our example) and data sparseness(such as Sonne in our example, a very rare term).The former kind of error is systematic and can beleveraged by an automatic method by assumingthat any entity semantically similar to an identifiederror will also be erroneous.In this section, we propose two methods for le-veraging this hypothesis.
In the first method, de-scribed in Section 3.1, we use a simpledistributional thesaurus and remove all entitieswhich are distributionally similar to manually iden-tified errors.
In the second method, described inSection 3.2, we model the semantics of the seedsusing distributional features and then dynamicallychange the feature space according to the manuallyidentified errors and rerank the entities in the set.Both methods rely on the following two observa-tions:a) Many expansion errors are systematicallycaused by ambiguous seed examples whichdraw in several incorrect entities of its unin-tended senses (such as seed Neptune in ourRoman Gods example which drew in celestialbodies such as Earth and Sun);b) Entities which are similar in one sense areusually not similar in their other senses.
Forexample, Apple and Sun are similar in theirCompany sense but their other senses (Fruitand Celestial Body) are not similar.
Our exam-ple in Section 1 illustrates a rare counterexam-ple where Neptune and Mercury are similar inboth their Planets and Roman Gods senses.Task Outline: Our task is to remove errors fromentity sets by using a minimal amount of manualjudgments.
Incorporating feedback into thisprocess can be done in multiple ways.
The mostflexible system would allow a judge to iterativelyremove as many errors as desired and then havethe system automatically remove other errors ineach iteration.
Because it is intractable to test arbi-trary numbers of manually identified errors in eachiteration, we constrain the judge to identify at mostone error in each iteration.Although this paper focuses solely on removingerrors in an entity set, it is also possible to improveexpanded sets by using feedback to add new ele-ments to the sets.
We consider this task out ofscope for this paper.3.1 Similarity Method (SIM)Our first method directly models observation a) inthe previous section.
Following Lin (1998), wemodel the similarity between entities using the dis-tributional hypothesis, which states that similarterms tend to occur in similar contexts (Harris1985).
A semantic model can be obtained by re-cording the surrounding contexts for each term in alarge collection of unstructured text.
Methods dif-fer in their definition of a context (e.g., text win-dow or syntactic relations), or a means to weighcontexts (e.g., frequency, tf-idf, pointwise mutualinformation), or ultimately in measuring the simi-larity between two context vectors (e.g., using Euc-lidean distance, Cosine, Dice).
In this paper, weuse a text window of size 1, we weigh our contextsusing pointwise mutual information, and we usethe cosine score to compute the similarity betweencontext vectors (i.e., terms).
Section 5.1 describesour source corpus and extraction details.
Compu-ting the full similarity matrix for many terms overa very large corpus is computationally intensive.Our specific implementation follows the one pre-sented in (Bayardo et al 2007).The similarity matrix computed above is thendirectly used to refine entity sets.
Given a manual-ly identified error at each iteration, we automatical-ly remove each entity in the set that is found to besemantically similar to the error.
The similaritythreshold was determined by manual inspectionand is reported in Section 5.1.Due to observation b) in the previous section,we expect that this method will perform poorly on292entity sets such as the one presented in our exam-ple of Section 1 where the manual removal ofEarth would likely remove correct entities such asMars, Venus and Jupiter.
The method presented inthe next section attempts to alleviate this problem.3.2 Feature Modification Method (FMM)Under the distributional hypothesis, the semanticsof a term are captured by the contexts in which itoccurs.
The Feature Modification Method (FMM),in short, attempts to automatically discover theincorrect contexts of the unintended senses of seedelements and then filters out expanded termswhose contexts do not overlap with the other con-texts of the seed elements.Consider the set of seed terms S and an errone-ous expanded instance e. In the SIM method ofSection 3.1 all set elements that have a feature vec-tor (i.e., context vector) similar to e are removed.The Feature Modification Method (FMM) insteadtries to identify the subset of features of the error ewhich represent the unintended sense of the seedterms S. For example, let S = {Minerva, Neptune,Baccus, Juno, Apollo}.
Looking at the contexts ofthese words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S. InWikipedia articles we see contexts (i.e., features)such as4:attack, kill, *planet, destroy,Goddess, *observe, statue, *launch,Rome, *orbit, ?Given an erroneous expansion such as e = Earth,we postulate that removing the intersecting fea-tures from Earth?s feature vector and the abovefeature vector will remove the unintended Planetsense of the seed set caused by the seed elementNeptune.
The intersecting features that are re-moved are bolded in the above feature vector for S.The similarity between this modified feature vectorfor S and all entities in the expansion set can berecomputed as described in Section 3.1.
Entitieswith a low similarity score are removed from theexpanded set since they are assumed to be part ofthe unintended semantic class (Planet in this ex-ample).Unlike the SIM method from Section 3.1, thismethod is more stable with respect to observation4 The full feature vector for these and all other terms in Wiki-pedia can be found at http://demo.patrickpantel.com/..b) in Section 3.
We showed that SIM would incor-rectly remove expansions such as Mars, Venus andJupiter given the erroneous expansion Earth.
TheFMM method would instead remove the Planetfeatures from the seed feature vectors and the re-maining features would still overlap with Mars,Venus and Jupiter?s Roman God sense.Efficiency: FMM requires online similarity com-putations between centroid vectors and all ele-ments of the expanded set.
For large corpora suchas Wikipedia articles or the Web, feature vectorsare large and storing them in memory and perform-ing similarity computations repeatedly for eacheditorial judgment is computationally intensive.For example, the size of the feature vector for asingle word extracted from Wikipedia can be in theorder of a few gigabytes.
Storing the feature vec-tors for all candidate expansions and the seed set isinefficient and too slow for an interactive system.The next section proposes a solution that makesthis computation very fast, requires little memory,and produces near perfect approximations of thesimilarity scores.3.3 Approximating Cosine SimilarityThere are engineering optimizations that are avail-able that allow us to perform a near perfect approx-imation of the similarity computation from theprevious section.
The proposed method requires usto only store the shared features between the cen-troid and the words rather than the complete fea-ture vectors, thus reducing our space requirementsdramatically.
Also, FMM requires us to repeatedlycalculate the cosine similarity between a modifiedcentroid feature vector and each candidate expan-sion at each iteration.
Without the full context vec-tors of all candidate expansions, computing theexact cosine similarity is impossible.
Given, how-ever, the original cosine scores between the seedelements and the candidate expansions before thefirst refinement iteration as well as the shared fea-tures, we can approximate with very high accuracythe updated cosine score between the modifiedcentroid and each candidate expansion.
Our me-thod relies on the fact that features (i.e., contexts)are only ever removed from the original centroid ?no new features are ever added.Let ?
be the original centroid representing theseed instances.
Given an expansion error e, FMMcreates a modified centroid by removing all fea-293tures intersecting between e and ?.
Let ?'
be thismodified centroid.
FMM requires us to computethe similarity between ?'
and all candidate expan-sions x as:cos x, ?
?
( )= xi ?
?
i?x ?
?
?where  i iterates over the feature space.In our efficient setting, the only element that wedo not have for calculating the exact cosine simi-larity is the norm of x, x .
Given that we have theoriginal cosine similarity score, cos(x, ?)
and thatwe have the shared features between the originalcentroid ?
and the candidate expansion x we cancalculate x  as:x = xi?i??
?
cos x,?
( )Combining the two equations, have:cos x, ?
?
( )= cos x,?
( )?
xi ?
?
i?xi?i?
???
?In the above equation, the modified cosine scorecan be considered as an update to the original co-sine score, where the update depends only on theshared features and the original centroid.
Theabove update equation can be used to recalculatethe similarity scores without resorting to an expen-sive computation involving complete feature vec-tors.Storing the original centroid is expensive andcan be approximated instead from only the sharedfeatures between the centroid and all instances inthe expanded set.
We empirically tested this ap-proximation by comparing the cosine scores be-tween the candidate expansions and both the truecentroid and the approximated centroid.
The aver-age error in cosine score was 9.5E-04 ?
7.83E-05(95% confidence interval).4 Datasets and Baseline AlgorithmWe evaluate our algorithms against manuallyscraped gold standard sets, which were extractedfrom Wikipedia to represent a random collection ofconcepts.
Section 4.1 discusses the gold standardsets and the criteria behind their selection.
Topresent a statistically significant view of our resultswe generated a set of trials from gold standard setsto use as seeds for our seed set expansion algo-rithm.
Also, in section 4.2 we discuss how we cansimulate editorial feedback using our gold standardsets.4.1 Gold Standard Entity SetsThe gold standard sets form an essential part of ourevaluation.
These sets were chosen to represent asingle concept such as Countries and Archbishopsof Canterbury.
These sets were selected from theList of pages from Wikipedia5.
We randomlysorted the list of every noun occurring in Wikipe-dia.
Then, for each noun we verified whether ornot it existed in a Wikipedia list, and if so we ex-tracted this list ?
up to a maximum of 50 lists.
If anoun belonged to multiple lists, the authors chosethe list that seemed most appropriate.
Althoughthis does not generate a perfect random sample,diversity is ensured by the random selection ofnouns and relevancy is ensured by the author adju-dication.Lists were then scraped from the Wikipediawebsite and they went through a manual cleanupprocess which included merging variants.
.
The 50sets contain on average 208 elements (with a min-imum of 11 and a maximum of 1116 elements) fora total of 10,377 elements.
The final gold standardlists contain 50 sets including classical pianists,Spanish provinces, Texas counties, male tennisplayers, first ladies, cocktails, bottled waterbrands, and Archbishops of Canterbury6.4.2 Generation of Experimental TrialsTo provide a statistically significant view of theperformance of our algorithm, we created morethan 1000 trials as follows.
For each of the goldstandard seed sets, we created 30 random sortings.These 30 random sortings were then used to gener-ate trial seed sets with a maximum size of 20seeds.4.3 Simulating User Feedback and BaselineAlgorithmUser feedback forms an integral part of our algo-rithm.
We used the gold standard sets to judge the5 In this paper, extractions from Wikipedia are taken from asnapshot of the resource in December 2007.6 The gold standard is available for download athttp://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&id=sse-gold/wikipedia.20071218.goldsets.tgz294candidate expansions.
The judged expansions wereused to simulate user feedback by marking thosecandidate expansions that were incorrect.
The firstcandidate expansion that was marked incorrect ineach editorial iteration was used as the editor?snegative example and was given to the system asan error.In the next section, we report R-precision gainsat each iteration in the editorial process for our twomethods described in Section 3.
Our baseline me-thod simply measures the gains obtained by re-moving the first incorrect entry in a candidateexpansion set at each iteration.
This simulates theprocess of manually cleaning a set by removingone error at a time.5 Experimental Results5.1 Experimental SetupWikipedia5 served as the source corpus for our al-gorithms described in Sections 3.1 and 3.2.
Allarticles were POS-tagged using (Brill 1995) andlater chunked using a variant of (Abney 1991).Corpus statistics from this processed text were col-lected to build the similarity matrix for the SIMmethod (Section 3.1) and to extract the featuresrequired for the FMM method (Section 3.2).
Inboth cases corpus statistics were extracted over thesemi-syntactic contexts (chunks) to approximateterm meanings.
The minimum similarity thresholdswere experimentally set to 0.15 and 0.11 for theSIM and FMM algorithms respectively.Each experimental trial described in Section4.2, which consists of a set of seed instances of oneof our 50 random semantic classes, was expandedusing a variant of the distributional set expansionalgorithm from Sarmento et al (2007).
The expan-sions were judged against the gold standard andeach candidate expansion was marked as eithercorrect or incorrect.
This set of expanded andjudged candidate files were used as inputs to thealgorithms described in Sections 3.1 and 3.2.Choosing the first candidate expansion that wasjudged as incorrect simulated our user feedback.This process was repeated for each iteration of thealgorithm and results are reported for 10 iterations.The outputs of our algorithms were againjudged against the gold standard lists and the per-formance was measured in terms of precision gainsover the baseline at various ranks.
Precision gainfor an algorithm over a baseline is the percentageincrease in precision for the same values of para-meters of the algorithm over the baseline.
Also, asthe size of our gold standard lists vary, we reportanother commonly used statistic, R-precision.
R-precision for any set is the precision at the size ofthe gold standard set.
For example, if a gold stan-dard set contains 20 elements, then R-precision forany set expansion is measured as the precision atrank 20.
The average R-precision over each set isthen reported.5.2 Quantitative AnalysisTable 1 lists the performance of our baseline algo-rithm (Section 4.3) and our proposed methods SIMand FMM (Sections 3.1 and 3.2) in terms of theirR-precision with 95% confidence bounds over 10iterations of each algorithm.The FMM of Section 3.2 is the best performingmethod in terms of R-precision reaching a maxi-mum value of 0.322 after the 10th iteration.
Forsmall numbers of iterations, however, the SIM me-thod outperforms FMM since it is bolder in its re-finements by removing all elements similar to thetagged error.
Inspection of FMM results showedthat bad instances get ranked lower in early itera-tions but it is only after 4 or 5 iterations that theyget pushed passed the similarity threshold (ac-counting for the low marginal increase in precisiongain for FMM in the first 4 to 5 iterations).FMM outperforms the SIM method by an aver-age of 4% increase in performance (13% im-provement after 10 iterations).
However both theFMM and the SIM method are able to outperformTable 1.
R-precision of the three methods with 95% confi-dence bounds.ITERATION BASELINE SIM FMM1 0.219?0.012 0.234?0.013 0.220?0.0152 0.223?0.013 0.242?0.014 0.227?0.0173 0.227?0.013 0.251?0.015 0.235?0.0194 0.232?0.013 0.26?0.016 0.252?0.0215 0.235?0.014 0.266?0.017 0.267?0.0226 0.236?0.014 0.269?0.017 0.282?0.0237 0.238?0.014 0.273?0.018 0.294?0.0238 0.24?0.014 0.28?0.018 0.303?0.0249 0.242?0.014 0.285?0.018 0.315?0.02510 0.243?0.014 0.286?0.018 0.322?0.025295the baseline method.
Using the FMM method onewould achieve an average of 17% improvement inR-precision over manually cleaning up the set(32.5% improvement after 10 iterations).
Using theSIM method one would achieve an average of 13%improvement in R-precision over manually clean-ing up the set (17.7% improvement after 10 itera-tions).5.3 Intrinsic Analysis of the SIM AlgorithmFigure 1 shows the precision gain of the similaritymatrix based algorithm over the baseline algo-rithm.
The results are shown for precision at ranks1, 2, 5, 10, 25, 50 and 100, as well as for R-precision.
The results are also shown for the first10 iterations of the algorithm.SIM outperforms the baseline algorithm for allranks and increases in gain throughout the 10 itera-tions.
As the number of iterations increases thechange in precision gain levels off.
This behaviorcan be attributed to the fact that we start removingerrors from top to bottom and in each iteration therank of the error candidate provided to the algo-rithm is lower than in the previous iteration.
Thisresults in errors which are not similar to any othercandidate expansions.
These are random errors andthe discriminative capacity of this method reducesseverely.Figure 1 also shows that the precision gain ofthe similarity matrix algorithm over the baselinealgorithm is higher at ranks 1, 2 and 5.
Also, theperformance increase drops at ranks 50 and 100.This is because low ranks contain candidate expan-sions that are random errors introduced due to datasparsity.
Such unsystematic errors are not detecta-ble by the SIM method.5.4 Intrinsic Analysis of the FMM AlgorithmThe feature modification method of Section 3.2shows similar behavior to the SIM method, how-ever as Figure 2 shows, it outperforms SIM me-thod in terms of precision gain for all values ofranks tested.
This is because the FMM method isable to achieve fine-grained control over what itremoves and what it doesn?t, as described in Sec-tion 5.2.Another interesting aspect of FMM is illu-strated in the R-precision curve.
There is a suddenjump in precision gain after the fifth iteration ofthe algorithm.
In the first iterations only few errorsare pushed beneath the similarity threshold as cen-troid features intersecting with tagged errors areslowly removed.
As the feature vector for the cen-troid gets smaller and smaller, remaining featureslook more and more unambiguous to the targetentity type and erroneous example have lesschance of overlapping with the centroid causingthem to be pushed pass the conservative similaritythreshold.
Different conservative thresholdsyielded similar curves.
High thresholds yield badperformance since all but the only very prototypi-cal set instances are removed as errors.The R-precision measure indirectly models re-call as a function of the target coverage of each set.We also directly measured recall at various ranksFigure 1.
Precision gain over baseline algorithm for SIMmethod.Figure 2.
Precision gain over baseline algorithm for FMMmethod.296and FMM outperformed SIM at all ranks and itera-tions.5.5 DiscussionIn this paper we proposed two techniques whichuse user feedback to remove systematic errors inset expansion systems caused by ambiguous seedinstances.
Inspection of expansion errors yieldedother types of errors.First, model errors are introduced in candidateexpansion sets by noise from various pre-processing steps involved in generating the expan-sions.
Such errors cause incorrect contexts (or fea-tures) to be extracted for seed instances andultimately can cause erroneous expansions to beproduced.
These errors do not seem to be systemat-ic and are hence not discoverable by our proposedmethod.Other errors are due to data sparsity.
As the fea-ture space can be very large, the difference in simi-larity between a correct candidate expansion andan incorrect expansion can be very small for sparseentities.
Previous approaches have suggested re-moving candidate expansions for which too fewstatistics can be extracted, however at the greatcost of recall (and lower R-precision).6 ConclusionIn this paper we presented two algorithms for im-proving the precision of automatically expandedentity sets by using minimal human negativejudgments.
We showed that systematic errorswhich arise from the semantic ambiguity inherentin seed instances can be leveraged to automaticallyrefine entity sets.
We proposed two techniques:SIM which boldly removes instances that are dis-tributionally similar to errors, and FMM whichmore conservatively removes features from theseed set representing its unintended (ambiguous)concept in order to rank lower potential errors.We showed empirical evidence that average R-precision over random entity sets improves by 26%to 51% when given from 5 to 10 manually taggederrors.
These results were reported by testing therefinement algorithms on a set of 50 randomlychosen entity sets expanded using a state of the artexpansion algorithm.
Given very small amounts ofmanual judgments, the SIM method outperformedFMM (up to 4 manual judgments).
FMM outper-formed the SIM method given more than 6 manualjudgments.
Both proposed refinement models havelinear time complexity in set size allowing forpractical online use in set expansion systems.This paper only addresses techniques for re-moving erroneous entities from expanded entitysets.
A complimentary way to improve perfor-mance would be to investigate the addition of rele-vant candidate expansions that are not already inthe initial expansion.
We are currently investigat-ing extensions to FMM that can efficiently addnew candidate expansions to the set by computingthe similarity between modified centroids and allterms occurring in a large body of text.We are also investigating ways to use the find-ings of this work to a priori remove ambiguousseed instances (or their ambiguous contexts) beforerunning the initial expansion algorithm.
It is ourhope that most of the errors identified in this workcould be automatically discovered without anymanual judgments.ReferencesAbney, S. Parsing by Chunks.
1991.
In: Robert Ber-wick, Steven Abney and Carol Tenny (eds.
), Prin-ciple-Based Parsing.
Kluwer Academic Publishers,Dordrecht.Banko, M. and Brill, E. 2001.
Scaling to very large cor-pora for natural language disambiguation.
In Pro-ceedings of ACL-2001.pp 26-33.
Morristown, NJ.Banko, M.; Cafarella, M.; Soderland, S.; Broadhead,M.
; Etzioni, O.
2007.
Open Information Extractionfrom the Web.
In Proceedings of IJCAI-07.Bayardo, R. J; Yiming Ma,; Ramakrishnan Srikant.
;Scaling Up All-Pairs Similarity Search.
In Proc.
ofthe 16th Int'l Conf.
on World Wide Web.
pp 131-1402007.Brill, E. 1995.
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part of Speech Tagging.
ComputationalLinguistics.Bunescu, R. and Mooney, R. 2004 Collective Informa-tion Extraction with Relational Markov Networks.
InProceedings of ACL-04.pp.
438-445.Cohn, D. A., Atlas, L., and Ladner, R. E. 1994.
Improv-ing Generalization with Active Learning.
MachineLearning, 15(2):201-221.
Springer, Netherlands.Dagan, I. and Engelson, S. P. 1995.
Selective Samplingin Natural Language Learning.
In Proceedings ofIJCAI-95 Workshop on New Approaches to Learningfor Natural Language Processing.
Montreal, Canada.Downey, D.; Broadhead, M; Etzioni, O.
2007.
LocatingComplex Named Entities in Web Text.
In Proceed-ings of IJCAI-07.297Etzioni, O.; Cafarella, M.; Downey.
D.; Popescu, A.;Shaked, T; Soderland, S.; Weld, D.; Yates, A.
2005.Unsupervised named-entity extraction from the Web:An Experimental Study.
In Artificial Intelligence,165(1):91-134.Harris, Z.
1985.
Distributional Structure.
In: Katz, J.
J.(ed.
), The Philosophy of Linguistics.
New York: Ox-ford University Press.
pp.
26-47.Harman, D. 1992.
Relevance feedback revisited.
InProceeedings of SIGIR-92.
Copenhagen, Denmark.Hearst, M. A.
1992.Automatic acquisition of hyponymsfrom large text corpora.In Proceedings of COLING-92.
Nantes, France.Kozareva, Z., Riloff, E. and Hovy, E. 2008.
SemanticClass Learning from the Web with Hyponym PatternLinkage Graphs.In Proceedings of ACL-08.pp 1048-1056.
Columbus, OHLin, D. 1998.Automatic retrieval and clustering of simi-lar words.In Proceedings of COLING/ACL-98.pp.768?774.
Montreal, Canada.Nadeau, D., Turney, P. D. and Matwin., S. 2006.
Unsu-pervised Named-Entity Recognition: Generating Ga-zetteers and Resolving Ambiguity.
In Advances inArtifical Intelligence.pp 266-277.
Springer Berlin,Heidelberg.Negri, M. 2004.
Sense-based blind relevance feedbackfor question answering.
In Proceedings of SIGIR-04Workshop on Information Retrieval For QuestionAnswering (IR4QA).
Sheffield, UK,Pantel, P. and Lin, D. 2002.
Discovering Word Sensesfrom Text.
In Proceedings of KDD-02.pp.
613-619.Edmonton, Canada.Pa?ca, M. 2007a.Weakly-supervised discovery ofnamed entities using web search queries.
In Proceed-ings of CIKM-07.pp.
683-690.Pasca, M. 2007b.
Organizing and Searching the WorldWide Web of Facts - Step Two: Harnessing the Wis-dom of the Crowds.
In Proceedings of WWW-07.
pp.101-110.Pa?ca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A.2006.
Names and Similarities on the Web: Fact Ex-traction in the Fast Lane.
In Proceedings of ACL-2006.pp.
113-120.Pa?ca, M. and Durme, B.J.
2008.
Weakly-supervisedAcquisition of Open-Domain Classes and ClassAttributes from Web Documents and Query Logs.
InProceedings of ACL-08.Riloff, E. and Jones, R. 1999 Learning Dictionaries forInformation Extraction by Multi-Level Boostrap-ping.In Proceedings of AAAI/IAAAI-99.Riloff, E. and Shepherd, J.
1997.
A corpus-based ap-proach for building semantic lexicons.In Proceedingsof EMNLP-97.Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E.2007.
?More like these?
: growing entity classes fromseeds.
In Proceedings of CIKM-07.
pp.
959-962.
Lis-bon, Portugal.Stevenson, M., Guo, Y. and  Gaizauskas, R. 2008.
Ac-quiring Sense Tagged Examples using RelevanceFeedback.
In Proceedings ofCOLING-08.
Manches-ter UK.Tang, M., Luo, X., and Roukos, S. 2001.
Active learn-ing for statistical natural language parsing.In Pro-ceedings of ACL-2001.pp 120 -127.
Philadelphia,PA.Vishwa.
V, Wood, K., Milic-Frayling, N. and Cox, I. J.2005.
Comparing Relevance Feedback Algorithmsfor Web Search.
In Proceedings of WWW 2005.
Chi-ba, Japan.Wang.
R.C.
and Cohen, W.W. 2007.Language-Independent Set Expansion of Named Entities Usingthe Web.In Proceedings of ICDM-07.Zhou, X. S. and Huang, S. T. 2003.
Relevance Feedbackin Image Retrieval: A Comprehensive Review -Xiang Sean Zhou, Thomas S. Huang MultimediaSystems.
pp 8:536-544.Zhou, G. and Su, J.
2001.
Named entity recognitionusing an HMM-based chunk tagger.
In Proceedingsof ACL-2001.pp.
473-480.
Morristown, NJ.298
