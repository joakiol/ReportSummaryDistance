Automatically Merging Lexicons that haveIncompatible Part-of-Speech CategoriesDaniel Ka-Leung CHAN and Dekai WUHKUSTHuman Language Technology CenterUniversity of Science and TechnologyClear Water Bay, Hong Kong{dklchan, dekai}@cs.ust.hkAbstractWe present a new method to automaticallymerge lexicons that employ different incom-patible POS categories.
Such incompatibil-ities have hindered efforts to combine lexi-cons to maximize coverage with reasonablehuman effort.
Given an "original exicon",our method is able to merge lexemes froman "additional lexicon" into the original ex-icon, converting lexemes from the additionallexicon with about 89% precision.
This levelof precision is achieved with the aid of adevice we introduce called an anti-lexicon,which neatly summarizes all the essential in-formation we need about the co-occurrenceof tags and lemmas.
Our model is intuitive,fast, easy to implement, and does not requireheavy computational resources nor trainingcorpus.lemma I tagapple INN boy NNcalculate VBExample entries in Brill lexicon1 MotivationWe present a new, accurate method to auto-matically merge lexicons that contain incom-patible POS categories.
In this paper, welook specifically at the problem that differ-ent lexicons employ their own part-of-speech(POS) tagsets that are incompatible witheach other, owing to their different linguisticbackgrounds, application domains, and/orlexical acquisition methods.Consider the way that lemmas are typ-ically marked with POS information inmachine-readable lexicons.
For example,here are a few entries from the lexicon inBrill's tagger (Brill, 1994) and the Moby lex-icon (Ward, 1996), showing simple pairs oflemmas and POS tags:lemma tagboy Nhold VExample entries in Moby lexiconPerhaps the most natural first approach tomerging the lexicons is to construct a set ofPOS mapping rules.For example, we might wish to acquire thefollowing mapping rules:("NN", "N" ), ("VB",  "V" ) .
.
.Here, the first rule says that the "NN"POS in the Brill lexicon should be mappedto the "N" POS in the Moby lexicon.
Ofcourse, not all POS tags can be accuratelytranslated this way, but the strategy is a rea-sonable first approximation.In order to incorporate ntries from otherlexicons into the current knowledge base, themapping rules between different POS tagsetsare usually formulated by hand in ad hocways.
In view of this heterogeneity and hu-man subjectiveness, some people had begunto investigate and develop methods of learn-ing the mapping rules between different POScategories in different lexicons.
Teufel de-scribed a tool to support manual mappingbetween different agsets using a rule-basedapproach (Teufel, 1995).
This approach re-quires heavy human intervention, and there-fore does not scale up easily.
Another ap-247proach was proposed by Hughes et al, to au-tomatically extract mapping rules from cor-pora tagged with more than one annotationscheme (Hughes et al, 1995).
However, thedependence on multiply-annotated corporarequires heavy annotation and/or computa-tion resources, whereas we are investigatingmethods with only the information found inexisting lexicons.In this paper, we will begin by presentinga basic method that generates a set of map-ping rules.
Experimental results on a vari-ety of lexicons will be presented.
We willthen introduce a mechanism called an "anti-lexicon" that significantly improves precisionon the learned rules and merged lexicons,though at a cost to recall.2 Bas icsOur general strategy is to inspect the co-occurrence of tags on those lemmas that arefound in both lexicons, and to use that infor-mation as a basis for generalizing, thus yield-ing POS mapping rules.
To do this requiresseveral steps, as described in the followingsubsections.
As a preliminary step, we willintroduce a way to represent POS tags usingfeature vectors.
We then use these vectorsto generate mapping rules.
To obtain betteraccuracy, we can restrict he training exam-ples to entries that occur in both lexicons.The generation algorithm also requires us todefine a similarity metric between POS fea-ture vectors.2.1 Part -of -speech feature vectorA necessary preliminary step of our methodis to introduce POS feature vectors.
A fea-ture vector is a useful representation of aPOS tag, because it neatly summarizes allthe information we need about which lem-mas can and cannot have that POS tag, il-lustrated as follows.Given:?
a lemma set .h4 ={ "apple", "boy","calculate"}?
a set of POS tags T' ={"NN","VB"}A tiny example lexicon consisting of lemmaand POS tag pairs might be as follows, whereeach cell with ?
indicates the existence ofthat lemma-POS pair in the lexicon:II apple boy calculateNN ?
?VB ?which, when represented as POS feature vec-tors, will be:p l :  < 1, 1, 0, >p2: < 0, 0 1, >where p1 here is the "NN" POS representedby the set of words that can be nouns ina given lexicon, in this example { "apple","boy" } and p2 similarly is the "VB" POS.The feature value for feature f in g can beeither:?
0 to indicate that we are not surewhether p is a tag of f;?
1 to indicate that p is a tag for f;?
2 to indicate that p can never be a tagfor lemma f.Obtaining information about the last ofthese (the value 2) is a non-trivial problem,which we will return to later in this paper.With ordinary lexicons, we only directly ob-tain feature vectors containing 0 and 1 val-ues.2.2 Mapping rule learning algorithmGiven a feature vector for every POS tag inboth lexicons--say, Brill's lexicon and theMoby lexicon--we use the following algo-rithm to learn mapping rules from POS tagsin Brill's tagset to POS tags in the Mobytagset.
The idea is to assume that a mappingrule between two POS tags holds if the sim-ilarity between their feature vectors exceedsa preset threshold, called a sim-threshold T.The similarity metric (SimScore) will be de-scribed later, but let's first look at the learn-ing algorithm, as described in algorithm 1.This algorithm does not exclude m-to-nmappings; that is, any Brill POS tag couldin principle get mapped to any number ofMoby POS tags.248mapper( 'P,Q)input  ?
Two sets of feature vectors of POS tagsp =e= , (  }output  ?
A mapping table represented as a set of pairs of feature vectorsB= {<F,?>,...}algor i thm:foreach ~ in P doforeach ~ e Q doif SimScore(~,~) > sire_threshold ~-thenB Bu >};endendend.Algorithm 1: Mapping rule learning algorithm2.3 Improv ing the t ra in ing  set byintersect ing the lexiconsWe can obtain better results by consideringonly those lemmas that occur in both lexi-cons.
This has the effect of eliminating un-reliable features in the POS feature vectors,since lemmas that do not occur in both lex-icons cannot be relied upon when judgingsimilarity.
This results in pruned versions ofboth lexicons.For example, pretend that the followingare the only entries in the Brill and Mobylexicons:lemma tagapple NNboy NNcalculate VBBrill lexiconlemma tagboy Nhold VMoby lexiconIn this case, intersecting the lexiconswould result in the following pruned lexi-cons:lemma I tagboy INNBrill' lexiconlemma I tagboy \[ NMoby' lexiconAfter pruning, the only remaining lemmais "boy", and the new POS feature vectorsfor "NN" and "N" have just one dimensioncorresponding to "boy":NN: < 1 >N: < 1 >Of course, in reality the lexicons are muchbigger and the effect is not so drastic.In all experiments in this paper, we usedlexicon intersection to prune the lexicons.2.4 S imi lar i ty  metr icThe similarity function we use calculates asimilarity score between two feature vectorsby counting the number of features with thesame feature value 1 or 2, indicating thata lemma either can or cannot belong tothat POS category.
(Recall that the value2490 means "don't know", so we simply ignoreany features with value 0.)
The score is nor-malized by the length of the feature vector.We also require that there be at least onepositive match in the sense that some lemmais shared by both of the POS categories; oth-erwise, if there are only negative matches(i.e., lemmas that cannot belong to eitherPOS category), we consider the evidence tobe too weak and the similarity score is thendefined to be zero.
The whole algorithm isdescribed in algorithm 2.2.5 The "complete  lexicon"assumpt ionAs mentioned earlier, ordinary lexicons donot explicitly contain information aboutwhich parts of speech a lemma can notbe used as.
We have two choices.
Inthe examples up till now, we used a valueof 0 for any lemma-tag pair not explicitlylisted in the lexicon, signifying that we don'tknow whether the POS category can includethat lemma.
However, having many "don'tknow" values significantly weakens our sim-ilarity scoring method.
Alternatively, wecan choose to assume that our lexicons arecomplete--a kind of closed world assump-tion.
In this case, we assume that anylemma-tag pair not found in the lexicon isnot merely an omission, but really can neveroccur.
This means we use the value 2 insteadof the value 0.The "complete lexicon" assumption onlymakes ense when we are dealing with large,broad coverage l xicons (as is the case in thispaper).
It is not reasonable when dealingwith small or specialized sublexicons.3 Anti- lexiconBased on the above intuition on utilizingnegative information, we propose an im-proved model using something we call ananti-lexicon, that indicates the POS tagsthat a lemma cannot have, which we will callits anti-tags.
A POS tag p is called an anti-tag a of a lemma m if p can never be a tagof m.The anti-lexicon consists of a set of piecesof this negative information, each called ananti-lexeme ~l:dejwhere -~p is the anti-tag of lemma m, and pis a POS used in the lexicon.Some examples of anti-lexemes are:( happy,-~IN )( run, -~JJ }(in,--~VB )where "IN","JJ" and "VB" are the preposi-tion, adjective and verb tags in Brill lexiconrespectively.Similar to a traditional lexicon which con-tains lexemes in the form of pairs of lemmasand their corresponding possible POS tag(s),an anti-lexicon contains anti-lexemes whichare simple pairs that associate a lemma withan anti-tag.The anti-lexicon can be automaticallygenerated quickly and easily.
To illustratethe idea, consider an example lexicon wherewe add the lemma "Central" and the POS"NP" to the example lexicon we have beenworking with.II apple boy calculateNN ?
?VBNPCentralSuppose we want to know whether "Cen-tral" can be a "NN", and whether "calcu-late" can be a "NN".
The fact that "apple"can be tagged by both "NN" and "NP", butnot "VB", gives "Central" (a lemma thatis already known to be able to serve as an"NP") a higher likelihood of possibly serv-ing as an "NN"  than "calculate" (a lemmathat is not known to be able to serve as an"NP") .Based  on this assumpt ion  that lexemeswith similar semantics will have similar POStags, we  conceptualize this kind of patternin terms of "cohesion" between lemmas andPOS tags in a lexicon.
The  "cohesion" of alemma I and  a POS tag p measures  the like-l ihood of a POS tag p being a possible tagof a lemma l, and  is defined as:250SimScore(/Y, q~input  : Two POS feature vectors of integers with values {0, 1, 2}(representing POS tags that may come from different tagsets)P= {Pl,P2,..-,Pn}q'= {ql,q2,.. .
,q,}?
A score in the range (0, 1) representing the similarity between iff and ( outputa lgor i thm:num_agree +-- 0num_known +- 0all_negative_agree +-- trueforeach i from 1 to n doif Pi ?
0 and qi ?
0 thennum_known ~ num_known + 1if p~ = qi thennum_agree +- num_agree + 1if Pi = 1 thenall_negative_agree +-- falseendendendendif all_negative_agree thenreturn 0endelsereturn num_agree -- num_knownendAlgorithm 2: Similarity scoring algorithmcohesion (/, p) =Pr(plPl,P2, .... ,Pn),~0,if (l, p) in lexicon;if F(p l ,p2 , .
.
.
,Pn) > 0;otherwiseset {P, pl,p2, .
.
.
.
.
.
,p~} to the the lem-mas that can have all the POS in the set{P l ,P2 ,  .
?
.
, Pn} .In the last example,whereF (p, Pl , P2 , .
?
- , P__.__n )Pr(p\]pl,p2, .... ' P " )= ~ i -  ,P,)which F(p l ,p2 , .
.
.
,p~) denotes the totalnumber of lemmas in the lexicon for whichp l ,p2 , .
.
.
,p,~ are all legal POS tags of l,and the probability Pr (p lp l ,p2 , .
.
.
,Pn) isjust a simple relative frequency of the lem-mas that can have all the POS in thecohesion( "Central" , "NN") = 0.5cohesion( "calculate" , "NN") = 0Therefore "NN" is more likely to be asso-ciated to "Central" than "calculate", whichimplies "NN" will be less likely to be thevalid POS to "calculate" than to "Central"?Under this intuition, we create an anti-lexicon by considering the cohesion of allpossible combinations of lemmas and POS251tags.
Entries with low cohesion will be con-sidered as anti-lexemes and inserted into theanti-lexicon.An anti-lexicon A is created by:A = ,,4 U (l, a) iff cohesion(l, a) < Awhere A is a threshold called anti-threshold,usually a very small real number between 0and 1.In our example, if we set anti-thresholdto 0.4, "NN" will become an anti-tag for"calculate" but not for "Central".
Sincethe lemmas in actual lexicons usually havemany possible POS tags, their cohesion toany POS tag will in turn be smaller thanthe cohesion in our simple example.
To cre-ate a more accurate anti-lexicon, we shouldset the anti-threshold to smaller value.4 Lex icon  merg ing  a lgor i thmGiven a POS mapping table B between thePOS tagset 7:' used by the original exicon 12 qand the POS tagset Q used by the additionallexicon ?P, we merge the entries from theadditional lexicon into the original lexiconby an algorithm as shown in algorithm 3.This algorithm does not exclude m-to-nPOS mappings; that is, a lexeme in the ad-ditional exicon can generate more than onelexeme and we can merge all of them intothe original exicon.5 Exper iment5.1 SetupWe tested the above method in a setof experiments using four commonly-usedmachine-readable dictionaries.
They areBrill's lexicon, the Moby lexicon, the Collinslexicon, the Oxford machine-readable dic-tionary, with characteristics a summarizedin table 1.
The lexicons use distinct POStagsets of different ag granularities, as sum-marized in table 2.With these four training lexicons we cantest twelve pairwise lexicon merging tasks,as shown in table 3.
For each pairs of lex-icon combination, we intersect them by thestrategy mentioned before and produced alexicon POS tagset number oftagset lexemesBrill Penn TreeBank 105199Collins Collins tagset 100566250441 MobyOxfordMoby tagsetOxford tagset 84588Table 1: Summary of English monolinguallexiconsnew set of training lexicons in each task.Note that the trimmed down Brill lexi-con in the "Brill-to-Collins" task is not thesame as the trimmed down Brill lexicon in"Brill-to-Moby".In order to evaluate the accuracy of ourmethods, we asked a linguist to manuallycreate twelve "gold standard" sets of POSmapping rules, TO, one for each of the twelvepairwise lexicons on the semantics betweenthe POS tag only.
We then ran the exper-iments to automatically generate two setsof POS mapping tables, with one under thecomplete world assumption and another us-ing an anti-lexicon i  each merging task.
Weevaluated precision and recall on POS map-ping rules as follows:precision on POS mapping rules - I$' I IEIwhere?
g is the resulting tagset mapping tablecontaining all mapping rules obtainedfrom experiment;?
?t is the subset of ?
which contains allcorrect mapping rules in "R,.
(?
E 7~)recall on POS mapping rules - \[g' \[ JnlUsing an anti-threshold A = 0.00001, we cre-ated twelve anti-lexicons which can then beused in our algorithm.
We obtained the POSmapping results as shown in table 4.In the baseline model, the precision is verylow, mainly due to data sparseness caused252tagset granularityPenn TreeBank fineCollins tagset fineMoby tagset coarseOxford tagset coarsesize example tags onnoun, proper noun, adjective, verb43 NN, NP, J J, VV32 n, n, adj, vb15 n, n, a, v20 K, G, M,  NTable 2: Summary of original POS tagsets in lexiconstaskbmbcbocmomcoocmomeobcbmbadditional size after original size afterlexicon lexicon lexicon lexiconintersection intersectionBrill 48097 Moby 47486Brill 29861 Collins 35933Brill 48154 Oxford 46952Collins 96149 Moby 90255Oxford 50562 Moby 52508Collins 42146 Oxford 33056Oxford 33056 Collins 42146Moby 5250890255 MobyOxfordCollins5056296149Oxford 46952 Brill 48154Collins 35933 Brill 29861Moby 47486 Brill 48097Table 3: Size of trimmed lexicons after lexicon intersectiontask precisionW/o anti-lexiconprecisionw/anti-lexiconrecallw/o anti-lexiconrecallw/anti-lexiconbm 0.1606 1.0000 0.4070 0.0349bc 0.1399 1.0000 0.4409 0.0215bo 0.1944 0.2727 0.6222 0.0667cm 0.1419 0.7143 0.3929 0.1786om 0.1811 1.0000 0.5897 0.0513co 0.1358 0.5714 0.4314 0.0784oc 0.1420 0.7500 0.5227 0.0682mo 0.1811 0.6667 0.6216 0.1081me 0.1290 1.0000 0.4762 0.1904ob 0.1979 0.3333 0.6333 0.0444cb 0.1434 0.2500 0.4118 0.0392mb 0.1651 0.3750 0.4932 0.0822average II 0.1594 0.6611 I 0.5036 0.0803Table 4: Results for POS mapping rule learning253lexico n_insertor (?P,?
q )input  : 1.
Two sets of lexemes, each lexeme in the form of a pair of lemma andPOS.z:p =C q : {<mk,ql>, .
.
.
}2.
A POS mapping table B from the POS tagset P to POS tagset QB = {(p~,qu)..
.
}output :An enlarged set of lexemes in ?
q, which contains newly inserted lexemesconverted from ?P.cq '= .
.
,  (mr,where (Pj,qs) E B, and (mr, qs) ~ ~q for all k, l ,p,q,r ,salgor i thm:foreach (mi,pj) in ?P doforeach (pj, qsI in B doif (mi, qs) not in ?q then12 q +-- ~q U (mi, q~) }endendendAlgorithm 3: Lexicon merging algorithmby the fact that machine readable lexiconsusually do not contain full lexeme coverage.This means our "complete lexicon assump-tion" which says that we can interpret en-tries not being in the lexicon as "negativeexamples" is not correct.In the anti-lexicon model, the precisiongreatly improves, with some experimentseven achieving 100% precision.
Unfortu-nately, the recall suffers sharply.After automatically constructing the POSmapping tables from training, we proceededto merge lexicons in each testing task us-ing the lexicon merging algorithm describedabove, and evaluated the accuracy of themerged lexicons as follow.In each merging task, we randomly se-lected 100 lexemes from the additional lexi-con.
Given these 100 lexemes, a linguist firstmanually constructs a set of correctly con-verted lexemes, which will be used as the"gold standard" set of lexemes, T~ n. Similarto the evaluation criteria outlined for POSmappings, we define the precision and recallon lexicon merging as the following:IEL'Iprecision on lexicon merg ing-  i$  L Iwhere?
E L is the set of lexemes generated by thelexicon insertor.?
E L' is the subset of E L that contains alllexemes in ~n.J EL'Irecall on lexicon merging- 17~L I5.2 ResultsWe obtain the results on lexicon merging asshown at table 5.The anti-lexicon model significantly im-proves the precision in both the generatedPOS mapping rules and merged lexicons.Most of the 12 lexicon merging tasks achieve254taskbmbcbocmomCOOCprecisionw/o anti-lexicon0.22630.11110.11470.27580.14130.13550.1625precisionw/anti-lexicon0.98410.10290.94290.88000.82761.00000.86671.0000recallw/o anti-lexicon0.34100.37890.96150.63560.73330.45140.35920.6193mo 0.1233 0.6000 0.4907mc 0.1813 0.9899 0.6946ob 0.0592 0.7692 0.5315cb 0.1114 0.9286 0.3546mb0.8959 average 0.14540.3165I 0.4922recallw/anti-lexicon0.28570.17370.18640.17780.31940.12620.20810.22220.58680.06990.09220.15820.2172Table 5: Results for lexicon mergingnearly more than 92% precision, which can-not be obtained by using even the gold stan-dard mapping rules, as shown in table 6.The recall degradation using anti-lexiconis lower in lexicon merging than in POS map-ping rule learning, owing to the fact thatnot all POS tags appear in lexicons withsame frequency.
For example, nouns andverbs occur far more frequently than prepo-sitions and adverbs.
High recall in POSmapping rules will not necessarily yield moreaccurate converted lexemes, if all the map-ping rules obtained are only those rarely-occurring POS tags.
Conversely, the suc-cessful generation of a single correct map-ping rule for a frequently-occuring POS taggreatly improves recall.
The mapping rulesgenerated by our anti-lexicon model confirmthis assumption: recall for POS mappingrules is 8%, but for lexicon merging it im-proves to about 22%.Recall suffers sharply, but precision ismore important han recall in lexicon merg-ing.
This is because the cost of post-lexiconclean up on lexemes with incorrect POS tagin a lexicon after merging is very expensive.A set of high precision POS mapping rulesguarantees a much cleaner esulting lexiconafter merging.
Thus during lexicon merg-ing, a conservative algorithm, which gener-ates fewer but more exact lexemes is prefer-able.task I precision recallbm 0.6953 0.8203bc 0.5081 0.8263bo 0.3478 0.8136cm 0.3697 0.9037om 0.4006 0.9236co 0.3103 0.9612oc 0.4804 0.9340mo 0.3160 0.9537mc 0.3996 0.9162ob 0.1590 0.8671cb 0.2272 0.9007mb 0.2157 0.8861average I 0.3664 0.8922Table 6: Lexicon merging results usinggold standard POS mapping rulesTo show how anti-lexicons affect the pre-cision and recall on lexicon merging, we alsoran experiments using different combina-tions of sim-thresholds and anti-thresholds.In most cases, the precision of lexicon merg-ing obtained from anti-lexicon are muchhigher than those without.
The results aresummarized in table 7 and table 8.
The255T0.50.60.70.80.90.9911 baseline II A = o.10.1159 0.08030.1178 = 0.13160.1208 0.11660.1454 0.09180.1636 0.09800.2450 ~ 0.21710.0010.22940.53920.57470.64440.54750.14880.00010.70680.79810.87110.89450.54540.24490.000010.71890.80940.88320.89590.54570.1408Table 7: Average precision on lexicon merging using different sire-thresholds 7 and anti-thresholds ATable 8: Averagethresholds AT0.50.60.70.80.90.99\[\[ baseline \[\[ A = 0.1 0.001 0.0001 0.000010.8082 ' 0.5591 0.4152 0.4063 0.41810.7512 I 0.4493 0.3475 0.3383 0.35010.6318 0.64440.1847 0.49220.27450.20920.09180.21520.28640.21720.2404 0.1090 0.0884 0.0973 0.09730.0458 0.0341 0.0135 0.0458 0.0366recall on lexicon merging using different sim-thresholds v and anti-best precision for lexicon merging is obtainedfrom 7 = 0.8 and A = 0.00001 in a grid-search.5.3 D iscuss ionAs mentioned earlier, the mapping rulelearning algorithm we used permitsm-to-n mappings so long as the map-ping rules created for every tag in alexicon reach the sim-threshold, that is,the confidence level specified by the lexi-cographer.
An alternative approach thatwe are experimenting with is to allowonly m-to-1 mappings, by simply choosingthe mapping rule with highest similarityscore.
In theory, this would seem to limitthe possible accuracy of the algorithm,but empirically we have found that thisapproach often yields higher precision andrecall.
Further investigation is needed.Different similarity scoring functions canalso be used.
If data sparseness i a seriousproblem, we can use a similarity score whichcounts only the lemmas which are tagged,but not the lemmas which are not tagged.One effect of ignoring unlikely tags in thisway is that the need for an anti-lexicon iseliminated.
We are also currently investi-gating the mapping power of such variantmethods.In general, we have observed ifferent be-haviors depending on factors such as thegranularity of the tagsets, the linguistic the-ories behind the tagsets, and the coverage ofthe lexicons.Finally, in addition to lexicon merging,POS mapping table is also useful in otherapplications.
Wu and Wong apply themin their SITG channel model to give bet-ter performance in their translation applica-tion (Wu and Wong, 1998).There is a serious problem of low recall onour anti-lexicon model.
This is because ourmodel prunes out many possible POS map-ping rules which results in very conservativelexeme selection during the lexicon merg-ing process.
Moreover, our model cannotdiscover which POS tags in original lexiconhave no corresponding tag in the additionallexicon.Our model took POS mapping rules asa natural starting point since this repre-256sentation has been used in earlier relatedwork.
However, our experiments showinglow precision on lexicon merging even usingthe human-generated gold standard map-ping rules indicates it might not be a goodapproach to use POS mapping rules at allto tackle the lexicon merging problems.
Ournext step will be to investigate models thatare not constrained by the POS mappingrule representation.6 ConclusionWe present a new method to automaticallymerge lexicons that employ different incom-patible POS categories, which merges lex-emes from an additional lexicon into an orig-inal lexicon with 89% in average precision.We showed how precision in the final mergedlexicon can be improved by introducing amodel called anti-lexicon, which neatly sum-marizes all the essential information we needabout he co-occurrence of tags and lemmas.Our model is intuitive, fast, easy to imple-ment, and does not require heavy computa-tional resources nor training corpus.7 AcknowledgmentsMany thanks to Josephine Kwan and HarrietLee for their help on hand-crafting the goldstandard POS mapping rules and lexemesets in the evaluation phrase.
We also thankthe SILC members, Aboy Wong, Hong-singWong, Vincent Chow, James Pang for theirrigorous constructive criticisms on our modeland experiments.ReferencesEric Brill.
1994.
Some Advances inTransformation-Based Part of SpeechTagging.
In Twelfth National Conferenceon Artificial Intelligence (AAAI-9~).Edward Fox, R. Wohlwend, P. Sheldon,Q.
Chen, , and R. France.
1986.
CoderLexicon: The Collins English Dictionaryand its Adverb Definitions.
Technical Re-port TR-86-23, Department of Computerand Information Science, Virginia Tech,Oct.A.
S. Hornby.
1995.
Oxford Machine Read-able Dictionary http://info.ox.ac.uk/.archive/ota.html.John Hughes, Clive Souter, and E. Atwell.1995.
Automatic Extraction of TagsetMappings from Parallel-Annotated Cor-pora.
Computation and Language.Beatrice Santorini.
1990.
Part-of-speechTagging Guidelines for the Penn TreebankProject.
Technical Report MS-CIS-90-47,Department ofComputer and InformationScience, University of Pennsylvania.Simone Teufel.
1995.
A Support Tool forTagset Mapping.
In EACL-Sigdat 95.Grady Ward.
1996.
Moby Lexicon.http://www.dcs.shef.ac.uk/research/ilash/Moby/.Dekai Wu and H. S. Wong.
1998.
Ma-chine Translation with a Stochastic Gram-matical Channel.
In Coling-ACL98, pages1408-1415.257
