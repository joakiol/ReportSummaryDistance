LET'S PUT THE AI BACK IN NLPLawrence BimbaumYale UniversityDepartment ofComputer ScienceNew Haven, ConnecticutArtificial intelligence is, or should be, at the heart of natural language processing research.After all, it is AI more than any other cognitive science that has made processing a central issue inthe study of the mind.
Yet, it seems to me that there has been a tendency recently on the part ofmany natural language researchers -- even, rather inexplicably, on the part of some within the AIcommunity itself -- to view AI as playing a secondary role in the study of language, at best auseful engineering adjunct o the more important " heoretical" studies carried out elsewhere.
Oneneed not reach as far back as the blunderbuss attack of Dresher and Hornstein -- an attack whoseferocity in fact reflected acertain amount of healthy respect for AI, or at least anxiety about itssuccess .- to find signs of this tendency.
Consider, for example, the title of a recently publishedbook, Natural Language Parsing: Computational, Psychological, and Theoretical Perspectives.It seems reasonably clear what the "computational" and "psychological" perspectives mentionedin the title are intended to refer to, but what does "theoretical" mean in this context?
Thatbecomes clearer, perhaps, when we observe that the book was edited by several linguists.
Theuse of the term "theoretical" inplace of "linguistic" reflects, I suspect, not only a desire to rhymebut also an unconscious assumption on the part of the editors that they amount o the same thingwhen it comes to the study of parsing.Such an assumption may be pardonable as reflecting anatural pride in their field on the partof some linguists.
What is much more surprising is evidence that this attitude xists within AI aswell.
For example, a recent monograph on natural language processing begins by propoundingthe following historical view: When NLP research first began, linguists were preoccupied withsyntax, so AI researchers had no choice but to cobble together semantic theories as best theycould.
But now that the linguists (and philosophers) have turned their attention to semantics in aserious way, these ad hoc AI theories can and should be replaced by implementations of the farmore rigorous products of our brethren sciences.
This is only a slight exaggeration of anargument which seems quite seriously intended.It should be obvious that in my view this attitude is detrimental to progress in N IPresearch.
AI's unique contribution to the study of  the mind stems from its dedication to theproposition that functional considerations arising from the need to perform realistic tasks, rather120than considerations of parsimonious empirical description, should be the primary constraints oncognitive theories.
To take the view that Ars job is to "implement" he theories produced byother cognitive sciences is therefore to abandon what makes AI worth doing in the first place.Natural language processing may include computational linguistics, but there is a lot more to itthan that.One unfortunate consequence of the tendency to ignore what AI can genuinely contribute isthat a great deal of effort gets devoted to implementing theories (primarily linguistic theories) thatwere never intended to be process models in the f'n-st place, while somewhat paradoxicallyattempting to stick as close to the original conception as possible.
The results are generallyuninteresting both from the perspective oflinguistics -- since such an implementation is likely tobe, at best, only a somewhat more rigorous reformulation of the original theory -- and from theview of AI -- since the original theory was not formulated with a view towards making anyinteresting functional claims.To return to an old controversy, consider the case of AI models which draw theirinspiration from linguistic theories of syntactic ompetence -- that is, theories which attempt tocapture the content of our knowledge of language structures -- which are based on theassumption of syntac~c autonomy.
I do not question the substantial empirical contributions madein pursuit of these linguistic theories themselves.
The question is, what additional contributionsare made by the AI theories based on them?The majority of the parsing models which are inspired by these linguistic theories, such asATNs and Prolog-based parsers, depend quite explicitly on nondeterminism.
The rules that theyemploy, and the representations that they build, seem for the most part taken over frompre-existing linguistic theory.
Very few of these models eem to have anything to say about howthe space of hypotheses generated by the grammars that they implement are to be searched, orhow they are to be integrated into the understanding process as a whole, or how these factorsmight impinge on the rules and representations employed.
None of them, that is, has much tosay about the most specifically A1 issues involved.
The indiscriminate reliance onnondeterminism is particularly troubling in this respect.
As process models, these theoriessimply fall back on a general model of symbolic omputation -- namely, backward chaining withback-up.On the other hand, Marcus's theory of deterministic syntactic analysis is a far moreprofound attempt to build an AI model of parsing based on linguistic theory.
Marcus tries toprovide a genuinely computational explanation for certain putative properties of English syntax,121by arguing that they are a natural consequence of functionally motivated aspects of his model.
Ihappen to think he fails -- largely because his overall claim that autonomous, deterministicsyntactic analysis is possible is seriously undercut by the failure to address uch issues as lexicalambiguity or genuine structural mbiguity, particularly prepositional phrase attachment -- but atleast some genuine claims are being made.
Unfortunately, the more recent work of Marcus,Hindle, and Fleck marks a step backwards in this regard.
In order to maintain the position ofautonomous syntactic processing, Marcus et al propose that the output of the parser be asomewhat vague description of the syntactic structure of the input sentence, capturing whateversmactural information can be gleaned without semantics or nondeterminism.
What claim is beingmade here?
It is tautological that autonomous deterministic syntactic analysis is possible if theoutput is defined to be whatever can be yielded in such a fashion.
In order to support ameaningful c aim of syntactic autonomy, certain functional criteria must be met: It is necessary toshow that such an output will be useful, and that it can be used without violating the assumptionof syntactic autonomy -- i.e., that the rest of the language processing system will not need toemploy syntactic knowledge.
There is good reason to believe that his is not the case.In the case of generation, the attempt o maintain syntactic autonomy is equallycounterproductive.
A generator must be able to relate semantic representations and pragmaticgoals to the syntactic onstructions that can be used to express the appropriate meanings andachieve those goals.
If a model of generation deals only with purely syntactic rules andrepresentations then it cannot, by definition, deal with such relations, and therefore it cannotaddress the interesting planning issues raised by generation - that is, the specifically AI issues.Thus, for example, McDonald's model of autonomous syntactic generation makes virtually nodecisions itself -- everything from what to say to which words to use has already been decided.If this theory is to make any meaningful processing claims, therefore, it must at least be shownthat the rest of the language processing system can make all of these decisions without anyrecourse to knowledge of the syntactic options offered by the language.
Particularly in the caseof lexical selection, there is good reason to doubt his.The real tragedy in all of this, of course, is that many genuine, and genuinely important, AIproblems get lost in the shuffle.
NLP is desperate for good methods whereby contextualconstraints can be brought to bear in a timely fashion to help resolve such problems as lexical andstructural ambiguity in language analysis.
Lexical selection has received far too little attention ingeneration research.
The problem of controlling search in conversational planning remainsvirtually untouched.
Aside from Chamiak's recent work and a few other attempts, the problemof controlling explanatory inference in understanding has largely been put aside since the heydayof script/frame theory, despite its centrality.
What are the criteria (e.g., parsimony,122completeness, etc.)
by which explanations are judged, compared, chosen?
What kinds ofconstraints do they impose on knowledge representations?
All of these issues are crucial tonatural language processing, and AI is crucial to their solution.
Let's put the AI back in NLP.We might even put some of the fun back in at the same time.123
