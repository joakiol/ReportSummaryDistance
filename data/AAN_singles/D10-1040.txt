Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410?419,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Game-Theoretic Approach to Generating Spatial DescriptionsDave GollandUC BerkeleyBerkeley, CA 94720dsg@cs.berkeley.eduPercy LiangUC BerkeleyBerkeley, CA 94720pliang@cs.berkeley.eduDan KleinUC BerkeleyBerkeley, CA 94720klein@cs.berkeley.eduAbstractLanguage is sensitive to both semantic andpragmatic effects.
To capture both effects,we model language use as a cooperative gamebetween two players: a speaker, who gener-ates an utterance, and a listener, who respondswith an action.
Specifically, we consider thetask of generating spatial references to ob-jects, wherein the listener must accuratelyidentify an object described by the speaker.We show that a speaker model that acts op-timally with respect to an explicit, embeddedlistener model substantially outperforms onethat is trained to directly generate spatial de-scriptions.1 IntroductionLanguage is about successful communication be-tween a speaker and a listener.
For example, if thegoal is to reference the target object O1 in Figure 1,a speaker might choose one of the following two ut-terances:(a) right of O2 (b) on O3Although both utterances are semantically correct,(a) is ambiguous between O1 and O3, whereas (b)unambiguously identifies O1 as the target object,and should therefore be preferred over (a).
In thispaper, we present a game-theoretic model that cap-tures this communication-oriented aspect of lan-guage interpretation and generation.Successful communication can be broken downinto semantics and pragmatics.
Most computationalFigure 1: An example of a 3D model of a room.
Thespeaker?s goal is to reference the target object O1 by de-scribing its spatial relationship to other object(s).
Thelistener?s goal is to guess the object given the speaker?sdescription.work on interpreting language focuses on compo-sitional semantics (Zettlemoyer and Collins, 2005;Wong and Mooney, 2007; Piantadosi et al, 2008),which is concerned with verifying the truth of a sen-tence.
However, what is missing from this truth-oriented view is the pragmatic aspect of language?that language is used to accomplish an end goal, asexemplified by speech acts (Austin, 1962).
Indeed,although both utterances (a) and (b) are semanticallyvalid, only (b) is pragmatically felicitous: (a) is am-biguous and therefore violates the Gricean maximof manner (Grice, 1975).
To capture this maxim, wedevelop a model of pragmatics based on game the-ory, in the spirit of Ja?ger (2008) but extended to thestochastic setting.
We show that Gricean maxims410fall out naturally as consequences of the model.An effective way to empirically explore the prag-matic aspects of language is to work in the groundedsetting, where the basic idea is to map language tosome representation of the non-linguistic world (Yuand Ballard, 2004; Feldman and Narayanan, 2004;Fleischman and Roy, 2007; Chen and Mooney,2008; Frank et al, 2009; Liang et al, 2009).
Alongsimilar lines, past work has also focused on inter-preting natural language instructions (Branavan etal., 2009; Eisenstein et al, 2009; Kollar et al, 2010),which takes into account the goal of the communi-cation.
This work differs from ours in that it doesnot clarify the formal relationship between pragmat-ics and the interpretation task.
Pragmatics has alsobeen studied in the context of dialog systems.
Forinstance, DeVault and Stone (2007) present a modelof collaborative language between multiple agentsthat takes into account contextual ambiguities.We present our pragmatic model in a groundedsetting where a speaker must describe a target objectto a listener via spatial description (such as in theexample given above).
Though we use some of thetechniques from work on the semantics of spatial de-scriptions (Regier and Carlson, 2001; Gorniak andRoy, 2004; Tellex and Roy, 2009), we empiricallydemonstrate that having a model of pragmatics en-ables more successful communication.2 Language as a GameTo model Grice?s cooperative principle (Grice,1975), we formulate the interaction between aspeaker S and a listener L as a cooperative game, thatis, one in which S and L share the same utility func-tion.
For simplicity, we focus on the production andinterpretation of single utterances, where the speakerand listener have access to a shared context.
To sim-plify notation, we suppress writing the dependenceon the context.The Communication Game1.
In order to communicate a target o to L, S pro-duces an utterance w chosen according to astrategy pS(w | o).2.
L interprets w and responds with a guess g ac-cording to a strategy pL(g | w).3.
S and L collectively get a utility of U(o, g).o w gUspeaker listenerps(w | o) pl(g | w)target utterance guessutilityFigure 2: Diagram representing the communicationgame.
A target, o, is given to the speaker that generatesan utterance w. Based on this utterance, the listener gen-erates a guess g. If o = g, then both the listener andspeaker get a utility of 1, otherwise they get a utility of 0.This communication game is described graphi-on O31near O30right of O20Figure 3: Three instances of the communication game onthe scenario in Figure 1.
For each instance, the target o,utterancew, guess g, and the resulting utilityU are shownin their respective positions.
A utility of 1 is awarded onlywhen the guess matches the target.cally in Figure 2.
Figure 3 shows several instances ofthe communication game being played for the sce-nario in Figure 1.Grice?s maxim of manner encourages utterancesto be unambiguous, which motivates the followingutility, which we call (communicative) success:U(o, g)def= I[o = g], (1)where the indicator function I[o = g] is 1 if o =g and 0 otherwise.
Hence, a utility-maximizingspeaker will attempt to produce unambiguous utter-ances because they increase the probability that thelistener will correctly guess the target.411Given a speaker strategy pS(w | o), a listenerstrategy pL(g | w), and a prior distribution over tar-gets p(o), the expected utility obtained by S and L isas follows:EU(S, L) =?o,w,gp(o)pS(w|o)pL(g|w)U(o, g)=?o,wp(o)pS(w|o)pL(o|w).
(2)3 From Reflex Speaker to RationalSpeakerHaving formalized the language game, we now ex-plore various speaker and listener strategies.
First,let us consider literal strategies.
A literal speaker(denoted S:LITERAL) chooses uniformly from theset of utterances consistent with a target object, i.e.,the ones which are semantically valid;1 a literal lis-tener (denoted L:LITERAL) guesses an object con-sistent with the utterance uniformly at random.In the running example (Figure 1), where the tar-get object is O1, there are two semantically valid ut-terances:(a) right of O2 (b) on O3S:LITERAL selects (a) or (b) each with probability12 .
If S:LITERAL chooses (a), L:LITERAL will guessthe target object O1 correctly with probability 12 ; ifS:LITERAL chooses (b), L:LITERAL will guess cor-rectly with probability 1.
Therefore, the expectedutility EU(S:LITERAL, L:LITERAL) = 34 .We say S:LITERAL is an example of a reflexspeaker because it chooses an utterance withouttaking the listener into account.
A general reflexspeaker is depicted in Figure 4(a), where each edgerepresents a potential utterance.Suppose we now have a model of some listenerL.
Motivated by game theory, we would optimizethe expected utility (2) given pL(g | w).
We callthe resulting speaker S(L) the rational speaker withrespect to listener L. Solving for this strategy yields:pS(L)(w | o) = I[w = w?
], wherew?
= argmaxw?pL(o | w?).
(3)1Semantic validity is approximated by a set of heuristic rules(e.g.
left is all positions with smaller x-coordinates).Sw1o w2w3S(L)w1oLg1w2 g2g3w3(a) Reflex speaker (b) Rational speakerFigure 4: (a) A reflex speaker (S) directly selects an ut-terance based only on the target object.
Each edge rep-resents a different choice of utterance.
(b) A rationalspeaker (S(L)) selects an utterance based on an embed-ded model of the listener (L).
Each edge in the first layerrepresents a different choice the speaker can make, andeach edge in the second layer represents a response of thelistener.Intuitively, S(L) chooses an utterance, w?, such that,if listener L were to interpret w?, the probability ofL guessing the target would be maximized.2 The ra-tional speaker is depicted in Figure 4(b), where, asbefore, each edge at the first level represents a possi-ble choice for the speaker, but there is now a secondlayer representing the response of the listener.To see how an embedded model of the listenerimproves communication, again consider our run-ning example in Figure 1.
A speaker can describethe target object O1 using either w1 = on O3 orw2 = right of O2.
Suppose the embedded listeneris L:LITERAL, which chooses uniformly from theset of objects consistent with the given utterance.In this scenario, pL:LITERAL(O1 | w1) = 1 becausew1 unambiguously describes the target object, butpL:LITERAL(O1 | w2) = 12 .
The rational speakerS(L:LITERAL) would therefore choose w1, achiev-ing a utility of 1, which is an improvement over thereflex speaker S:LITERAL?s utility of 34 .2If there are ties, any distribution over the utterances havingthe same utility is optimal.4124 From Literal Speaker to LearnedSpeakerIn the previous section, we showed that a literalstrategy, one that considers only semantically validchoices, can be used to directly construct a reflexspeaker S:LITERAL or an embedded listener in arational speaker S(L:LITERAL).
This section fo-cuses on an orthogonal direction: improving literalstrategies with learning.
Specifically, we constructlearned strategies from log-linear models trained onhuman annotations.
These learned strategies canthen be used to construct reflex and rational speakervariants?S:LEARNED and S(L:LEARNED), respec-tively.4.1 Training a Log-Linear Speaker/ListenerWe train the speaker, S:LEARNED, (similarly, lis-tener, L:LEARNED) on training examples whichcomprise the utterances produced by the human an-notators (see Section 6.1 for details on how thisdata was collected).
Each example consists of a 3Dmodel of a room in a house that specifies the 3D po-sitions of each object and the coordinates of a 3Dcamera.
When training the speaker, each example isa pair (o, w), where o is the input target object andw is the output utterance.
When training the listener,each example is (w, g), where w is the input utter-ance and g is the output guessed object.For now, an utterance w consists of two parts:?
A spatial preposition w.r (e.g., right of) from aset of possible prepositions.3?
A reference object w.o (e.g., O3) from the setof objects in the room.We consider more complex utterances in Section 5.Both S:LEARNED and L:LEARNED areparametrized by log-linear models:pS:LEARNED(w|o; ?S) ?
exp{?>S ?
(o, w)} (4)pL:LEARNED(g|w; ?L) ?
exp{?>L ?
(g, w)} (5)where ?
(?, ?)
is the feature vector (see below), ?Sand ?L are the parameter vectors for speaker and lis-tener.
Note that the speaker and listener use the same3We chose 10 prepositions commonly used by people to de-scribe objects in a preliminary data gathering experiment.
Thislist includes multi-word units, which function equivalently toprepositions, such as left of.set of features, but they have different parameters.Furthermore, the first normalization sums over pos-sible utterances w while the second normalizationsums over possible objects g in the scene.
The twoparameter vectors are trained to optimize the log-likelihood of the training data under the respectivemodels.Features We now describe the features ?
(o, w).These features draw inspiration from Landau andJackendoff (1993) and Tellex and Roy (2009).Each object o in the 3D scene is represented byits bounding box, which is the smallest rectangularprism containing o.
The following are functions ofthe camera, target (or guessed object) o, and the ref-erence object w.o in the utterance.
The full set offeatures is obtained by conjoining these functionswith indicator functions of the form I[w.r = r],where r ranges over the set of valid prepositions.?
Proximity functions measure the distance be-tween o and w.o.
This is implemented as theminimum over all the pairwise Euclidean dis-tances between the corners of the boundingboxes.
We also have indicator functions forwhether o is the closest object, among the top5 closest objects, and among the top 10 closestobjects to w.o.?
Topological functions measure containment be-tween o and w.o: vol(o ?
w.o)/vol(o) andvol(o ?
w.o)/vol(w.o).
To simplify volumecomputation, we approximate each object by abounding box that is aligned with the cameraaxes.?
Projection functions measure the relative posi-tion of the bounding boxes with respect to oneanother.
Specifically, let v be the vector fromthe center of w.o to the center of o.
There is afunction for the projection of v onto each of theaxes defined by the camera orientation (see Fig-ure 5).
Additionally, there is a set of indicatorfunctions that capture the relative magnitude ofthese projections.
For example, there is a indi-cator function denoting whether the projectionof v onto the camera?s x-axis is the largest ofall three projections.413Figure 5: The projection features are computed by pro-jecting a vector v extending from the center of the ref-erence object to the center of the target object onto thecamera axes fx and fy .5 Handling Complex UtterancesSo far, we have only considered speakers and lis-teners that deal with utterances consisting of onepreposition and one reference object.
We now ex-tend these strategies to handle more complex utter-ances.
Specifically, we consider utterances that con-form to the following grammar:4[noun] N ?
something | O1 | O2 | ?
?
?
[relation] R ?
in front of | on | ?
?
?
[conjunction] NP ?
N RP?
[relativization] RP ?
R NPThis grammar captures two phenomena of lan-guage use, conjunction and relativization.?
Conjunction is useful when one spatial relationis insufficient to disambiguate the target object.For example, in Figure 1, right of O2 could re-fer to the vase or the table, but using the con-junction right of O2 and on O3 narrows downthe target object to just the vase.?
The main purpose of relativization is to referto objects without a precise nominal descrip-tor.
With complex utterances, it is possible tochain relative prepositional phrases, for exam-ple, using on something right of O2 to refer tothe vase.4Naturally, we disallow direct reference to the target object.Given an utterancew, we define its complexity |w|as the number of applications of the relativizationrule, RP ?
R NP, used to produce w. We had onlyconsidered utterances of complexity 1 in previoussections.5.1 Example UtterancesTo illustrate the types of utterances available underthe grammar, again consider the scene in Figure 1.Utterances of complexity 2 can be generated ei-ther using the relativization rule exclusively, or boththe conjunction and relativization rules.
The rela-tivization rule can be used to generate the followingutterances:?
on something that is right of O2?
right of something that is left of O3Applying the conjunction rule leads to the followingutterances:?
right of O2 and on O3?
right of O2 and under O1?
left of O1 and left of O3Note that we inserted the words that is after each Nand the word and between every adjacent pair of RPsgenerated via the conjunction rule.
This is to help ahuman listener interpret an utterance.5.2 Extending the Rational SpeakerSuppose we have a rational speaker S(L) defined interms of an embedded listener L which operates overutterances of complexity 1.
We first extend L to in-terpret arbitrary utterances of our grammar.
The ra-tional speaker (defined in (2)) automatically inheritsthis extension.Compositional semantics allows us to define theinterpretation of complex utterances in terms of sim-pler ones.
Specifically, each node in the parse treehas a denotation, which is computed recursivelyin terms of the node?s children via a set of sim-ple rules.
Usually, denotations are represented aslambda-calculus functions, but for us, they will bedistributions over objects in the scene.
As a basecase for interpreting utterances of complexity 1, wecan use either L:LITERAL or L:LEARNED (definedin Sections 3 and 4).414Given a subtree w rooted at u ?
{N, NP, RP}, wedefine the denotation of w, JwK, to be a distributionover the objects in the scene in which the utterancewas generated.
The listener strategy pL(g|w) = JwKis recursively as follows:?
If w is rooted at N with a single child x, then JwKis the uniform distribution over N (x), the set ofobjects consistent with the word x.?
If w is rooted at NP, we recursively compute thedistributions over objects g for each child tree,multiply the probabilities, and renormalize (Hin-ton, 1999).?
Ifw is rooted at RP with relation r, we recursivelycompute the distribution over objects g?
for thechild NP tree.
We then appeal to the base caseto produce a distribution over objects g which arerelated to g?
via relation r.This strategy is defined formally as follows:pL(g | w) ???????????
?I[g ?
N (x)] w = (N x)k?j=1pL(g | wj) w = (NP w1 .
.
.
wk)?g?pL(g | (r, g?))pL(g?
| w?)
w = (RP (R r)w?
)(6)Figure 6 shows an example of this bottom-up denotation computation for the utteranceon something right of O2 with respect to the scenein Figure 1.
The denotation starts with the lowestNP node JO2K, which places all the mass on O2in the scene.
Moving up the tree, we computethe denotation of the RP, Jright of O2K, using theRP case of (6), which results in a distribution thatplaces equal mass on O1 and O3.5 The denotationof the N node JsomethingK is a flat distribution overall the objects in the scene.
Continuing up the tree,the denotation of the NP is computed by taking aproduct of the object distributions, and turns outto be exactly the same split distribution as its RPchild.
Finally, the denotation at the root is computedby applying the base case to on and the resultingdistribution from the previous step.5It is worth mentioning that this split distribution betweenO1 and O3 represents the ambiguity mentioned in Section 3when discussing the shortcomings of S:LITERAL.Figure 6: The listener model maps an utterance to a dis-tribution over objects in the room.
Each internal NP or RPnode is a distribution over objects in the room.Generation So far, we have defined the listenerstrategy pL(g | w).
Given target o, the rationalspeaker S(L) with respect to this listener needs tocompute argmaxw pL(o | w) as dictated by (3).
Thismaximization is performed by enumerating all utter-ances of bounded complexity.5.3 Modeling Listener ConfusionOne shortcoming of the previous approach for ex-tending a listener is that it falsely assumes that a lis-tener can reliably interpret a simple utterance just aswell as it can a complex utterance.We now describe a more realistic speaker whichis robust to listener confusion.
Let ?
?
[0, 1] bea focus parameter which determines the confusionlevel.
Suppose we have a listener L. When presentedwith an utterance w, for each application of the rela-tivization rule, we have a 1??
probability of losingfocus.
If we stay focused for the entire utterance(with probability ?|w|), then we interpret the utter-ance according to pL.
Otherwise (with probability1 ?
?|w|), we guess an object at random accordingto prnd(g | w).
We then use (3) to define the rationalspeaker S(L) with respect the following ?confusedlistener?
strategy:p?L(g | w) = ?|w|pL(g | w) + (1 ?
?|w|)prnd(g | w).
(7)As ?
?
0, the confused listener is more likely tomake a random guess, and thus there is a strongerpenalty against using more complex utterances.
As415?
?
1, the confused listener converges to pL and thepenalty for using complex utterances vanishes.5.4 The Taboo SettingNotice that the rational speaker as defined so fardoes not make full use of our grammar.
Specifi-cally, the rational speaker will never use the ?wild-card?
noun something nor the relativization rule inthe grammar because an NP headed by the wildcardsomething can always be replaced by the object IDto obtain a higher utility.
For instance, in Figure 6,the NP spanning something right of O2 can be re-placed by O3.However, it is not realistic to assume that all ob-jects can be referenced directly.
To simulate scenar-ios where some objects cannot be referenced directly(and to fully exercise our grammar), we introducethe taboo setting.
In this setting, we remove fromthe lexicon some fraction of the object IDs which areclosest to the target object.
Since the tabooed objectscannot be referenced directly, a speaker must resortto use of the wildcard something and relativization.For example, in Figure 7, we enable tabooingaround the target O1.
This prevents the speaker fromreferring directly to O3, so the speaker is forced todescribe O3 via the relativization rule, for example,producing something right of O2.Figure 7: With tabooing enabled around O1, O3 can nolonger be referred to directly (represented by an X).6 ExperimentsWe now present our empirical results, showing thatrational speakers, who have embedded models of lis-Figure 8: Mechanical Turk speaker task: Given the tar-get object (e.g., O1), a human speaker must choose anutterance to describe the object (e.g., right of O2).teners, can communicate more successfully than re-flex speakers, who do not.6.1 SetupWe collected 43 scenes (rooms) from the GoogleSketchup 3D Warehouse, each containing an aver-age of 22 objects (household items and pieces of fur-niture arranged in a natural configuration).
For eachobject o in a scene, we create a scenario, which rep-resents an instance of the communication game witho as the target object.
There are a total of 2,860 sce-narios, which we split evenly into a training set (de-noted TR) and a test set (denoted TS).We created the following two Amazon Mechani-cal Turk tasks, which enable humans to play the lan-guage game on the scenarios:Speaker Task In this task, human annotators playthe role of speakers in the language game.
They areprompted with a target object o and asked to eachproduce an utterance w (by selecting a prepositionw.r from a dropdown list and clicking on a referenceobjectw.o) that best informs a listener of the identityof the target object.For each training scenario o, we asked threespeakers to produce an utterancew.
The three result-ing (o, w) pairs are used to train the learned reflexspeaker (S:LITERAL).
These pairs were also used totrain the learned reflex listener (L:LITERAL), wherethe target o is treated as the guessed object.
See Sec-tion 4.1 for the details of the training procedure.Listener Task In this task, human annotators playthe role of listeners.
Given an utterance generated bya speaker (human or not), the human listener must416O2O1O3Question: What object is right of           ?O2Figure 9: Mechanical Turk listener task: a human listeneris prompted with an utterance generated by a speaker(e.g., right of O2), and asked to click on an object (shownby the red arrow).guess the target object that the speaker saw by click-ing on an object.
The purpose of the listener task isto evaluate speakers, as described in the next section.6.2 EvaluationUtility (Communicative Success) We primarilyevaluate a speaker by its ability to communicate suc-cessfully with a human listener.
For each test sce-nario, we asked three listeners to guess the object.We use pL:HUMAN(g | w) to denote the distributionover guessed objects g given prompt w. For exam-ple, if two of the three listeners guessed O1, thenpL:HUMAN(O1 | w) = 23 .
The expected utility (2) isthen computed by averaging the utility (communica-tive success) over the test scenarios TS:SUCCESS(S) = EU(S, L:HUMAN) (8)=1|TS|?o?TS?wpS(w|o)pL:HUMAN(o|w).Exact Match As a secondary evaluation metric,we also measure the ability of our speaker to exactlymatch an utterance produced by a human speaker.Note that since there are many ways of describingan object, exact match is neither necessary nor suffi-cient for successful communication.We asked three human speakers to each pro-duce an utterance w given a target o.
We usepS:HUMAN(w | o) to denote this distribution; for ex-ample, pS:HUMAN(right of O2 | o) = 13 if exactly oneof the three speakers uttered right of O2.
We thenSpeaker Success Exact MatchS:LITERAL [reflex] 4.62% 1.11%S(L:LITERAL) [rational] 33.65% 2.91%S:LEARNED [reflex] 38.36% 5.44%S(L:LEARNED) [rational] 52.63% 14.03%S:HUMAN 41.41% 19.95%Table 1: Comparison of various speakers on communica-tive success and exact match, where only utterances ofcomplexity 1 are allowed.
The rational speakers (withrespect to both the literal listener L:LITERAL and thelearned listener L:LEARNED) perform better than theirreflex counterparts.
While the human speaker (composedof three people) has higher exact match (it is better atmimicking itself), the rational speaker S(L:LEARNED)actually achieves higher communicative success than thehuman listener.define the exact match of a speaker S as follows:MATCH(S) =1|TS|?o?TS?wpS:HUMAN(w | o)pS(w | o).
(9)6.3 Reflex versus Rational SpeakersWe first evaluate speakers in the setting where onlyutterances of complexity 1 are allowed.
Table 1shows the results on both success and exact match.First, our main result is that the two rational speak-ers S(L:LITERAL) and S(L:LEARNED), which eachmodel a listener explicitly, perform significantly bet-ter than the corresponding reflex speakers, both interms of success and exact match.Second, it is natural that the speakers that in-volve learning (S:LITERAL and S(L:LITERAL))outperform the speakers that only consider theliteral meaning of utterances (S:LEARNED andS(L:LEARNED)), as the former models capture sub-tler preferences using features.Finally, we see that in terms of exact match, thehuman speaker S:HUMAN performs the best (thisis not surprising because human exact match is es-sentially the inter-annotator agreement), but in termsof communicative success, S(L:LEARNED) achievesa higher success rate than S:HUMAN, suggestingthat the game-theoretic modeling undertaken by therational speakers is effective for communication,which is ultimate goal of language.Note that exact match is low even for the ?humanspeaker?, since there are often many equally good4170.2 0.4 0.6 0.8 1.0?0.490.50.510.52successFigure 10: Communicative success as a function of focusparameter ?
without tabooing on TSDEV.
The optimalvalue of ?
is obtained at 0.79.ways to evoke an object.
At the same time, the suc-cess rates for all speakers are rather low, reflectingthe fundamental difficulty of the setting: sometimesit is impossible to unambiguously evoke the targetobject via short utterances.
In the next section, weshow that we can improve the success rate by al-lowing the speakers to generate more complex utter-ances.6.4 Generating More Complex UtterancesWe now evaluate the rational speakerS(L:LEARNED) when it is allowed to generateutterances of complexity 1 or 2.
Recall fromSection 5.3 that the speaker depends on a focusparameter ?, which governs the embedded listener?sability to interpret the utterance.
We divided the testset (TS) in two halves: TSDEV, which we used totune the value of ?
and TSFINAL, which we used toevaluate success rates.Figure 10 shows the communicative success asa function of ?
on TSDEV.
When ?
is small, theembedded listener is confused more easily by morecomplex utterances; therefore the speaker tends tochoose mostly utterances of complexity 1.
As ?increases, the utterances increase in complexity, asdoes the success rate.
However, when ?
approaches1, the utterances are too complex and the successrate decreases.
The dependence between ?
and av-erage utterance complexity is shown in Figure 11.Table 2 shows the success rates on TSFINAL for?
?
0 (all utterances have complexity 1), ?
= 1 (allutterances have complexity 2), and ?
tuned to max-imize the success rate based on TSDEV.
Setting ?in this manner allows us to effectively balance com-plexity and ambiguity, resulting in an improvementin the success rate.0.2 0.4 0.6 0.8 1.0?1.21.41.61.82.0average|w|Figure 11: Average utterance complexity as a function ofthe focus parameter ?
on TSDEV.
Higher values of ?yield more complex utterances.Taboo Success Success SuccessAmount (?
?
0) (?
= 1) (?
= ??)
?
?0% 51.78% 50.99% 54.53% 0.795% 38.75% 40.83% 43.12% 0.8910% 29.57% 29.69% 30.30% 0.8030% 12.40% 13.04% 12.98% 0.81Table 2: Communicative success (on TSFINAL) of therational speaker S(L:LEARNED) for various values of ?across different taboo amounts.
When the taboo amountis small, small values of ?
lead to higher success rates.
Asthe taboo amount increases, larger values of ?
(resultingin more complex utterances) are better.7 ConclusionStarting with the view that the purpose of languageis successful communication, we developed a game-theoretic model in which a rational speaker gener-ates utterances by explicitly taking the listener intoaccount.
On the task of generating spatial descrip-tions, we showed the rational speaker substantiallyoutperforms a baseline reflex speaker that does nothave an embedded model.
Our results therefore sug-gest that a model of the pragmatics of communica-tion is an important factor to consider for generation.Acknowledgements This work was supported bythe National Science Foundation through a Gradu-ate Research Fellowship to the first two authors.
Wealso would like to acknowledge Surya Murali, thedesigner of the 3D Google Sketchup models, andthank the anonymous reviewers for their comments.ReferencesJ.
L. Austin.
1962.
How to do Things with Words: TheWilliam James Lectures delivered at Harvard Univer-418sity in 1955.
Oxford, Clarendon, UK.S.
Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.2009.
Reinforcement learning for mapping instruc-tions to actions.
In Association for Computational Lin-guistics and International Joint Conference on NaturalLanguage Processing (ACL-IJCNLP), Singapore.
As-sociation for Computational Linguistics.D.
L. Chen and R. J. Mooney.
2008.
Learning tosportscast: A test of grounded language acquisition.In International Conference on Machine Learning(ICML), pages 128?135.
Omnipress.David DeVault and Matthew Stone.
2007.
Managingambiguities across utterances in dialogue.J.
Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.2009.
Reading to learn: Constructing features fromsemantic abstracts.
In Empirical Methods in NaturalLanguage Processing (EMNLP), Singapore.J.
Feldman and S. Narayanan.
2004.
Embodied meaningin a neural theory of language.
Brain and Language,89:385?392.M.
Fleischman and D. Roy.
2007.
Representing inten-tions in a cognitive model of language acquisition: Ef-fects of phrase structure on situated verb learning.
InAssociation for the Advancement of Artificial Intelli-gence (AAAI), Cambridge, MA.
MIT Press.M.
C. Frank, N. D. Goodman, and J.
B. Tenenbaum.2009.
Using speakers?
referential intentions to modelearly cross-situational word learning.
PsychologicalScience, 20(5):578?585.Peter Gorniak and Deb Roy.
2004.
Grounded semanticcomposition for visual scenes.
In Journal of ArtificialIntelligence Research, volume 21, pages 429?470.H.
P. Grice.
1975.
Syntax and Semantics; Logic andConversation.
3:Speech Acts:41?58.G.
Hinton.
1999.
Products of experts.
In InternationalConference on Artificial Neural Networks (ICANN).G.
Ja?ger.
2008.
Game theory in semantics and pragmat-ics.
Technical report, University of Tu?bingen.T.
Kollar, S. Tellex, D. Roy, and N. Roy.
2010.
Towardunderstanding natural language directions.
In Human-Robot Interaction, pages 259?266.Barbara Landau and Ray Jackendoff.
1993.
?what?and ?where?
in spatial language and spatial cognition.Behavioral and Brain Sciences, 16(2spatial preposi-tions analysis, cross linguistic conceptual similarities;comments/response):217?238.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learning se-mantic correspondences with less supervision.
In As-sociation for Computational Linguistics and Interna-tional Joint Conference on Natural Language Process-ing (ACL-IJCNLP), Singapore.
Association for Com-putational Linguistics.S.
T. Piantadosi, N. D. Goodman, B.
A. Ellis, and J. B.Tenenbaum.
2008.
A Bayesian model of the acquisi-tion of compositional semantics.
In Proceedings of theThirtieth Annual Conference of the Cognitive ScienceSociety.T Regier and LA Carlson.
2001.
Journal of experimen-tal psychology.
general; grounding spatial language inperception: an empirical and computational investiga-tion.
130(2):273?298.Stefanie Tellex and Deb Roy.
2009.
Grounding spatialprepositions for video search.
In ICMI.Y.
W. Wong and R. J. Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Association for Computational Linguis-tics (ACL), pages 960?967, Prague, Czech Republic.Association for Computational Linguistics.C.
Yu and D. H. Ballard.
2004.
On the integration ofgrounding language and learning objects.
In Asso-ciation for the Advancement of Artificial Intelligence(AAAI), pages 488?493, Cambridge, MA.
MIT Press.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Uncer-tainty in Artificial Intelligence (UAI), pages 658?666.419
