Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1152?1158,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDependency Parsing for Weibo:An Efficient Probabilistic Logic Programming ApproachWilliam Yang Wang, Lingpeng Kong, Kathryn Mazaitis, William W. CohenLanguage Technologies Institute & Machine Learning DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213, USA.
{yww,lingpenk,krivard,wcohen}@cs.cmu.eduAbstractDependency parsing is a core task in NLP,and it is widely used by many applica-tions such as information extraction, ques-tion answering, and machine translation.In the era of social media, a big chal-lenge is that parsers trained on traditionalnewswire corpora typically suffer from thedomain mismatch issue, and thus performpoorly on social media data.
We present anew GFL/FUDG-annotated Chinese tree-bank with more than 18K tokens from SinaWeibo (the Chinese equivalent of Twit-ter).
We formulate the dependency pars-ing problem as many small and paralleliz-able arc prediction tasks: for each task,we use a programmable probabilistic first-order logic to infer the dependency arc of atoken in the sentence.
In experiments, weshow that the proposed model outperformsan off-the-shelf Stanford Chinese parser,as well as a strong MaltParser baseline thatis trained on the same in-domain data.1 IntroductionWeibo, in particular Sina Weibo1, has attractedmore than 30% of Internet users (Yang et al.,2012), making it one of the most popular socialmedia services in the world.
While Weibo postsare abundantly available, NLP techniques for ana-lyzing Weibo posts have not been well-studied inthe past.Syntactic analysis of Weibo is made difficultfor three reasons: first, in the last few decades,Computational Linguistics researchers have pri-marily focused on building resources and tools us-ing standard English newswire corpora2, and thus,1http://en.wikipedia.org/wiki/Sina Weibo2For example, Wall Street Journal articles are used forbuilding the Penn Treebank (Marcus et al., 1993).there are fewer resources in other languages ingeneral.
Second, microblog posts are typicallyshort, noisy (Gimpel et al., 2011), and can beconsidered as a ?dialect?, which is very differ-ent from news data.
Due to the differences ingenre, part-of-speech taggers and parsers trainedon newswire corpora typically fail on social mediatexts.
Third, most existing parsers use language-independent standard features (McDonald et al.,2005), and these features may not be optimal forChinese (Martins, 2012).
To most of the applica-tion developers, the parser is more like a blackbox,which is not directly programmable.
Therefore,it is non-trivial to adapt these generic parsers tolanguage-specific social media text.In this paper, we present a new probabilistic de-pendency parsing approach for Weibo, with thefollowing contributions:?
We present a freely available Chinese Weibodependency treebank3, manually annotatedwith more than 18,000 tokens;?
We introduce a novel probabilistic logicprogramming approach for dependency arcprediction, making the parser directly pro-grammable for theory engineering;?
We show that the proposed approach outper-forms an off-the-shelf dependency parser, aswell as a strong baseline trained on the samein-domain data.In the next section, we describe existing workon dependency parsing for Chinese.
In Section 3,we present the new Chinese Weibo Treebank tothe research community.
In Section 4, we intro-duce the proposed efficient probabilistic program-ming approach for parsing Weibo.
We show theexperimental results in Section 5, and conclude inSection 6.3http://www.cs.cmu.edu/?yww/data/WeiboTreebank.zip11522 Related WorkChinese dependency parsing has attracted manyinterests in the last fifteen years.
Bikel and Chi-ang (2000; 2002) are among the first to use PennChinese Tree Bank for dependency parsing, wherethey adapted Xia?s head rules (Xia, 1999).
An im-portant milestone for Chinese dependency pars-ing is that, a few years later, the CoNLL sharedtask launched a track for multilingual dependencyparsing, which also included Chinese (Buchholzand Marsi, 2006; Nilsson et al., 2007).
Theseshared tasks soon popularized Chinese depen-dency parsing by making datasets available, andthere has been growing amount of literature sincethen (Zhang and Clark, 2008; Nivre et al., 2007;Sagae and Tsujii, 2007; Che et al., 2010; Carreras,2007; Duan et al., 2007).Besides the CoNLL shared tasks, there are alsomany interesting studies on Chinese dependencyparsing.
For example, researchers have studiedcase (Yu et al., 2008) and morphological (Li andZhou, 2012) structures for learning a Chinese de-pendency parser.
Another direction is to performjoint learning and inference for POS tagging anddependency parsing (Li et al., 2011; Hatori et al.,2011; Li et al., 2011; Ma et al., 2012).
In recentyears, there has been growing interests in depen-dency arc prediction in Chinese (Che et al., 2014),and researchers have also investigated character-level Chinese dependency parsing (Zhang et al.,2014).
However, even though the above methodsall have merits, the results are reported only onstandard newswire based Chinese Treebank (e.g.from People?s Daily (Liu et al., 2006)), and it isunclear how they would perform on Weibo data.To the best of our knowledge, together with therecent study on parsing tweets (Kong et al., 2014),we are among the first to study the problem of de-pendency parsing for social media text.3 The Chinese Weibo TreebankWe use the publicly available ?topia dataset (Linget al., 2013) for dependency annotation.
An in-teresting aspect of this Weibo dataset is that, be-sides the Chinese posts, it also includes a copy ofthe English translations.
This allows us to observesome interesting phenomena that mark the differ-ences of the two languages.
For example:?
Function words are more frequently used inEnglish than in Chinese.
When examin-Figure 1: An example of pro-drop phenomenonfrom the Weibo data.ing this English version of the Weibo cor-pus for the total counts of the word ?the?,there are 2,084 occurrences in 2,003 sen-tences.
Whereas in Chinese, there are only52 occurrences of the word ?the?
out of the2,003 sentences.?
The other interesting thing is the position ofthe head.
In English, the head of the treeoccurs more frequent on the left-to-middleof the sentence, while the distribution of thehead is more complicated in Chinese.
This isalso verified from the parallel Weibo data.?
Another well-known issue in Chinese is thatChinese is a pro-drop topical language.
Thisis extremely prominent in the short text,which clearly creates a problem for parsing.For example, in the Chinese Weibo data, wehave observed the sentence in Figure 1.To facilitate the annotation process, we firstpreprocess the Weibo posts using the StanfordNLP pipeline, including a Chinese Word Seg-menter (Tseng et al., 2005) and a Chinese Part-of-Speech tagger (Toutanova and Manning, 2000).Two native speakers of Chinese with strong lin-guistic backgrounds have annotated the depen-dency relations from 1,000 posts of the ?topiadataset, using the FUDG (Schneider et al., 2013)and GFL annotation tool (Mordowanec et al.,2014).
The annotators communicate regularly dur-ing the annotation process, and a coding man-ual that relies majorly on the Stanford Dependen-cies (Chang et al., 2009) is designed.
The anno-tation process has two stages: in the first stage,we rely on the word segmentation produced bythe segmenter, and produce a draft version of thetreebank; in the second stage, the annotators ac-tively discuss the difficult cases to reach agree-ments, manually correct the mis-segmented wordtokens, and revise the annotations of the trickycases.
The final inter-annotator agreement rate ona randomly-selected subset of 373 tokens in this1153treebank is 82.31%.Fragmentary Unlabeled Dependency Grammar(FUDG) is a newly proposed flexible frameworkthat offers a relative easy way to annotate the syn-tactic structure of text.
Beyond the traditional treeview of dependency syntax in which the tokensof a sentence form nodes in a tree, FUDG alsoallows the annotation of additional lexical itemssuch as multiword expressions.
It provides specialdevices for coordination and coreference; and fa-cilitates underspecified (partial) annotations whereproducing a complete parse would be difficult.Graph Fragment Language (GFL) is an implemen-tation of unlabeled dependency annotations in theFUDG framework, which fully supports Chinese,English and other languages.
The training set ofour Chinese Weibo Treebank4includes 14,774 to-kens, while the development and test sets include1,846 and 1,857 tokens respectively.4 A Programmable Parser withPersonalized PageRank InferenceA key problem in multilingual dependency parsingis that generic feature templates may not work wellfor every language.
For example, Martins (2012)shows that for Chinese dependency parsing, whenadding the generic grandparents and siblings fea-tures, the performance was worse than using thestandard bilexical, unilexical, and part-of-speechfeatures.
Unfortunately, for many parsers suchas Stanford Chinese Parser (Levy and Manning,2003) and MaltParser (Nivre et al., 2007), it isvery difficult for programmers to specify the fea-ture templates and inference rules for dependencyarc prediction.In this work, we present a Chinese dependencyparsing method for Weibo, based on efficient prob-abilistic first-order logic programming (Wang etal., 2013).
The advantage of probabilistic pro-gramming for parsing is that, software engineerscan simply conduct theory engineering, and op-timize the performance of the parser for a spe-cific genre of the target language.
Recently, proba-bilistic programming approaches (Goodman et al.,2012; Wang et al., 2013; Lloyd et al., 2014) havedemonstrated its efficiency and effectiveness inmany areas such as information extraction (Wanget al., 2014), entity linking, and text classifica-tion (Wang et al., 2013).4The corpus is freely available for download at the URLspecified in Section 1.Algorithm 1 A Dependency Arc Inference Algo-rithm for Parsing WeiboGiven:(1) a sentence with tokens Ti, where i is the in-dex, and L is the length;(2) a databaseD of token relations from the cor-pus;(3) first-order logic inference rule set R.for i = 1?
L tokens doS?
ConstructSearchSpace(Ti, R,D);~Pi?
InferParentUsingProPPR(Ti,S);end forGreedy Global Inferencefor i = 1?
L tokens doYi= arg max~Pi;end for4.1 Problem FormulationWe formulate the dependency parsing prob-lem as many small dependency arc predictionproblems.
For each token, we form the par-ent inference problem of a token Tias solving aquery edge(Ti, ?)
using stochastic theorem prov-ing on a search graph.
Our approach relies on adatabase D of inter-token relations.
To constructthe database, we automatically extract the tokenrelations from the text data.
For example, to de-note the adjacency of two tokens T1and T2, westore the entry adjacent(T1, T2) in D. One canalso store the part-of-speech tag of a token in theform haspos(T1, DT ).
There is no limitationson the arity and the types of the predicates in thedatabase.Given the database of token relations, one thenneeds to construct the first-order logic inferencetheory R for predicting dependency arcs.
For ex-ample, to construct simple bilexical and bi-POSinference rules to model the dependency of an ad-jacent head and a modifier, one can write first-order clauses such as:edge(V1,V2) :-adjacent(V1,V2),hasword(V1,W1),hasword(V2,W2),keyword(W1,W2) #adjWord.edge(V1,V2) :-adjacent(V1,V2),haspos(V1,W1),haspos(V2,W2),keypos(W1,W2) #adjPos.keyword(W1,W2) :- # kw(W1,W2).keypos(W1,W2) :- # kp(W1,W2).1154Figure 2: After mapping the database D to theory R, here is an example of search space for dependencyarc inference.
The query is edge(S1T5, X), and there exists one correct and multiple incorrect solutions(highlighted in bold).Here, we associate a feature vector ?cwith eachclause, which is annotated using the # symbol af-ter each clause in the theory set.
Note that the lasttwo (keyword and keypos) clauses are feature tem-plates that allow us to learn the specific bi-POStags and bilexical words from the data.
In orderfor one to solve the query edge(Ti, ?
), we firstneed to map the entities from D to R to constructthe search space.
The details for constructing andsearching in the graph can be found in previousstudies on probabilistic first-order logic (Wang etal., 2013) and stochastic logic programs (Cussens,2001).
An example search space is illustrated inFigure 2.
Note that now the edges in the searchgraph correspond to the feature vector ?cin R.The overall dependency arc inference algorithmcan be found in Algorithm 1.
For each of the par-ent inference subtask, we use ProPPR (Wang et al.,2013) to perform efficient personalized PageRankinference.
Note that to ensure the validity of thedependency tree, we break the loops in the finalparse graph into a parse tree using the maximumpersonalized PageRank score criteria.
When mul-tiple roots are predicted, we also select the mostlikely root by comparing the personalized PageR-ank solution scores.To learn the more plausible theories, one needsto upweight weights for relevant features, sothat they have higher transition probabilities onthe corresponding edges.
To do this, we usestochastic gradient descent to learn from trainingqueries, where the correct and incorrect solutionsare known.
The details of the learning algorithmare described in the last part of this section.4.2 Personalized PageRank InferenceFor the inference of the parent of each token, weutilize ProPPR (Wang et al., 2013).
ProPPR al-lows a fast approximate proof procedure, in whichonly a small subset of the full proof graph isgenerated.
In particular, if ?
upper-bounds thereset probability, and d upperbounds the degreeof nodes in the graph, then one can efficientlyfind a subgraph with O(1?) nodes which approx-imates the weight for every node within an er-ror of d (Wang et al., 2013), using a variant ofthe PageRank-Nibble algorithm of Andersen et al(2008).4.3 Parameter EstimationOur parameter learning algorithm is implementedusing a parallel stochastic gradient descent vari-ant to optimize the log loss using the supervisedpersonalized PageRank algorithm (Backstrom and1155Method Dev.
TestStanford Parser (Xinhua) 0.507 0.489Stanford Parser (Chinese) 0.597 0.581MaltParser (Full) 0.669 0.654Our methods ?
ProPPRReLU (Bi-POS) 0.506 0.517ReLU (Bilexical) 0.635 0.616ReLU (Full) 0.668 0.666Truncated tanh (Bi-POS) 0.601 0.594Truncated tanh (Bilexical) 0.650 0.634Truncated tanh (Full) 0.667 0.675*Table 1: Comparing our Weibo parser to otherbaselines (UAS).
The off-the-shelf Stanford parseruses its attached Xinhua and Chinese factoredmodels, which are trained on external Chinesetreebank of newswire data.
MaltParser was trainedon the same in-domain data as our proposed ap-proach.
* indicates p < .001 comparing to theMaltParser.Leskovec, 2011).
The idea is that, given thetraining queries, we perform a random walk withrestart process, and upweight the edges that aremore likely to end up with a known correct parent.We learn the transition probability from two nodes(u, v) in the search graph using: Prw(v|u) =1Zf(w,?crestart), where we use two popular non-linear parameter learning functions from the deeplearning community:?
Rectified Linear Unit (ReLU) (Nair and Hin-ton, 2010): max(0, x);?
The Hyperbolic Function (Glorot and Ben-gio, 2010): tanh(x).as the f in this study.
ReLU is a desirablenon-linear function, because it does not have thevanishing gradient problem, and produces sparseweights.
For the weights learned from tanh(x),we truncate the negative weights on the edges,since the default weight on the feature edges isw = 1.0 (existence), and w = 0.0 means that theedge does not exist in the inference stage.5 ExperimentsIn this experiment, we compare the proposedparser with two well-known baselines.
First,we compare with an off-the-shelf Stanford Chi-nese Parser (Levy and Manning, 2003).
Second,we compare with the MaltParser (Nivre et al.,2007) that is trained on the same in-domain Weibodataset.
The train, development, and test splits aredescribed in Section 3.
We tune the regulariza-tion hyperparameters of the models on the dev.
set,and report Unlabeled Attachment Score (UAS) re-sults for both the dev.
set and the hold-out test set.We experiment with the bilexical and bi-POS first-order logic theory separately, as well as a com-bined full model with directional and distance fea-tures.The results are shown in Table 1.
We see thatboth of the two attached pre-trained models fromthe Stanford parser do not perform very well onthis Weibo dataset, probably because of the mis-matched training and test data.
MaltParser iswidely considered as one of the most popular de-pendency parsers, not only because of its speed,but also the acclaimed accuracy.
We see that whenusing the full model, the UAS results between ourmethods and MaltParser are very similar on the de-velopment set, but both of our approaches outper-form the Maltparser in the holdout test set.
Thetruncated tanh variant of ProPPR obtains the bestUAS score of 0.675.6 ConclusionIn this paper, we present a novel Chinese de-pendency treebank, annotated using Weibo data.We introduce a probabilistic programming depen-dency arc prediction approach, where theory en-gineering is made easy.
In experiments, we showthat our methods outperform an off-the-shelf Stan-ford Chinese Parser, as well a strong MaltParserthat is trained on the same in-domain data.
TheChinese Weibo Treebank is made freely availableto the research community.
In the future, we planto apply the proposed approaches to dependencyand semantic parsing of other languages.AcknowledgementsWe are grateful to anonymous reviewers for usefulcomments.
This research was supported in partby DARPA grant FA8750-12-2-0342 funded un-der the DEFT program, and a Google ResearchAward.
The authors are solely responsible for thecontents of the paper, and the opinions expressedin this publication do not reflect those of the fund-ing agencies.1156ReferencesReid Andersen, Fan R. K. Chung, and Kevin J. Lang.2008.
Local partitioning for directed graphs usingpagerank.
Internet Mathematics, 5(1):3?22.Lars Backstrom and Jure Leskovec.
2011.
Supervisedrandom walks: predicting and recommending linksin social networks.
In Proceedings of the fourthACM international conference on Web search anddata mining, pages 635?644.
ACM.Daniel M Bikel and David Chiang.
2000.
Two statis-tical parsing models applied to the chinese treebank.In Proceedings of the second workshop on Chineselanguage processing: held in conjunction with the38th Annual Meeting of the Association for Compu-tational Linguistics-Volume 12, pages 1?6.
Associa-tion for Computational Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, pages 149?164.Association for Computational Linguistics.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In EMNLP-CoNLL, pages 957?961.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D Manning.
2009.
Discriminativereordering with chinese grammatical relations fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation, pages51?59.
Association for Computational Linguistics.Wanxiang Che, Zhenghua Li, and Ting Liu.
2010.
Ltp:A chinese language technology platform.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics: Demonstrations, pages13?16.
Association for Computational Linguistics.Wanxiang Che, Jiang Guo, and Ting Liu.
2014.
Re-liable dependency arc recognition.
Expert Systemswith Applications, 41(4):1716?1722.David Chiang and Daniel M. Bikel.
2002.
Recover-ing latent information in treebanks.
In Proceedingsof the 19th International Conference on Computa-tional Linguistics - Volume 1, COLING ?02, pages1?7, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.James Cussens.
2001.
Parameter estimation instochastic logic programs.
Machine Learning,44(3):245?271.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic parsing action models for multi-lingual de-pendency parsing.
In EMNLP-CoNLL, pages 940?946.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flani-gan, and Noah A Smith.
2011.
Part-of-speech tag-ging for twitter: Annotation, features, and experi-ments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, pages 42?47.
Association for Computa-tional Linguistics.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In International Conference on ArtificialIntelligence and Statistics, pages 249?256.Noah Goodman, Vikash Mansinghka, Daniel Roy,Keith Bonawitz, and Daniel Tarlow.
2012.
Church:a language for generative models.
arXiv preprintarXiv:1206.3255.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint pos tag-ging and dependency parsing in chinese.
In IJC-NLP, pages 1216?1224.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A. Smith.
2014.
A dependency parser fortweets.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2014), Doha, Qatar, October.
ACL.Roger Levy and Christopher Manning.
2003.
Is itharder to parse chinese, or the chinese treebank?In Proceedings of the 41st Annual Meeting on As-sociation for Computational Linguistics-Volume 1,pages 439?446.
Association for Computational Lin-guistics.Zhongguo Li and Guodong Zhou.
2012.
Unified de-pendency parsing of chinese morphological and syn-tactic structures.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 1445?1454.
Association forComputational Linguistics.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,Wenliang Chen, and Haizhou Li.
2011.
Joint mod-els for chinese pos tagging and dependency pars-ing.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1180?1191.
Association for Computational Linguis-tics.Wang Ling, Guang Xiang, Chris Dyer, Alan Black, andIsabel Trancoso.
2013.
Microblogs as parallel cor-pora.
In Proceedings of the 51st Annual Meetingon Association for Computational Linguistics, ACL?13.
Association for Computational Linguistics.Ting Liu, Jinshan Ma, and Sheng Li.
2006.
Build-ing a dependency treebank for improving chineseparser.
Journal of Chinese Language and Comput-ing, 16(4):207?224.1157James Robert Lloyd, David Duvenaud, Roger Grosse,Joshua B Tenenbaum, and Zoubin Ghahramani.2014.
Automatic construction and natural-languagedescription of nonparametric regression models.arXiv preprint arXiv:1402.4304.Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren.
2012.Easy-first chinese pos tagging and dependency pars-ing.
In COLING, pages 1731?1746.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.Andr?e Filipe Torres Martins.
2012.
The Geometry ofConstrained Structured Prediction: Applications toInference and Learning of Natural Language Syn-tax.
Ph.D. thesis, Columbia University.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training ofdependency parsers.
In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 91?98.
Association for Computa-tional Linguistics.Michael T. Mordowanec, Nathan Schneider, ChrisDyer, and Noah A. Smith.
2014.
Simplified de-pendency annotations with gfl-web.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics: System Demonstrations.ACL.Vinod Nair and Geoffrey E Hinton.
2010.
Rectifiedlinear units improve restricted boltzmann machines.In Proceedings of the 27th International Conferenceon Machine Learning (ICML-10), pages 807?814.Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.The conll 2007 shared task on dependency parsing.In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL, pages 915?932.
sn.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Kenji Sagae and Jun?ichi Tsujii.
2007.
Depen-dency parsing and domain adaptation with lr modelsand parser ensembles.
In EMNLP-CoNLL, volume2007, pages 1044?1050.Nathan Schneider, Brendan O?Connor, Naomi Saphra,David Bamman, Manaal Faruqui, Noah A Smith,Chris Dyer, and Jason Baldridge.
2013.
Aframework for (under) specifying dependency syn-tax without overloading annotators.
arXiv preprintarXiv:1306.2091.Kristina Toutanova and Christopher D Manning.
2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger.
In Proceedingsof the 2000 Joint SIGDAT conference on Empiricalmethods in natural language processing and verylarge corpora: held in conjunction with the 38th An-nual Meeting of the Association for ComputationalLinguistics-Volume 13, pages 63?70.
Association forComputational Linguistics.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, volume171.William Yang Wang, Kathryn Mazaitis, and William WCohen.
2013.
Programming with personalizedpagerank: a locally groundable first-order proba-bilistic logic.
In Proceedings of the 22nd ACM in-ternational conference on Conference on informa-tion & knowledge management, pages 2129?2138.ACM.William Yang Wang, Kathryn Mazaitis, Ni Lao, TomMitchell, and William W Cohen.
2014.
Effi-cient inference and learning in a large knowledgebase: Reasoning with extracted information usinga locally groundable first-order probabilistic logic.arXiv preprint arXiv:1404.3301.Fei Xia.
1999.
Extracting tree adjoining grammarsfrom bracketed corpora.
In Proceedings of the 5thNatural Language Processing Pacific Rim Sympo-sium (NLPRS-99), pages 398?403.Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang.
2012.Automatic detection of rumor on sina weibo.
In Pro-ceedings of the ACM SIGKDD Workshop on MiningData Semantics, page 13.
ACM.Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.2008.
Chinese dependency parsing with large scaleautomatically constructed case structures.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics-Volume 1, pages 1049?1056.
Association for Computational Linguistics.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.
Association for Computa-tional Linguistics.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2014.
Character-level chinese dependencyparsing.
In Proceedings of the 52th Annual Meet-ing of the Association for Computational Linguistics(ACL 2014), Baltimore, MD, USA, June.
ACL.1158
