The PEACE SLDS understanding evaluation paradigm of the FrenchMEDIA campaignLaurence Devillers, He?le`ne Maynard, Patrick Paroubek, Sophie RossetLIMSI-CNRSBt 508 University of Paris XI - BP 133 F-91403 ORSAY Cedex, Francefdevil,hbm,pap,rossetg@limsi.frAbstractThis paper presents a paradigm forevaluating the context-sensitive under-standing capability of any spoken lan-guage dialog system: PEACE (Frenchacronym for Paradigme d?EvaluationAutomatique de la Compre?hension horset En-contexte).
This paradigm will bethe basis of the French TechnolangueMEDIA project, in which dialog sys-tems from various academic and indus-trial sites will be tested in an evaluationcampaign coordinated by ELRA/ELDA(over the next two years).
Despite pre-vious efforts such as EAGLES, DISC,AUPELF ARCB2 or the ongoing Ameri-can DARPA COMMUNICATOR project,the spoken dialog community still lackscommon reference tasks and widelyagreed upon methods for comparingand diagnosing systems and techniques.Automatic solutions are nowadays be-ing sought both to make possible thecomparison of different approaches bymeans of reliable indicators with genericevaluation methodologies and also to re-duce system development costs.
How-ever achieving independence from boththe dialog system and the task per-formed seems to be more and more autopia.
Most of the evaluations haveup to now either tackled the system asa whole, or based the measurementson dialog-context-free information.
ThePEACE proposal aims at bypassing someof these shortcomings by extracting,from real dialog corpora, test sets thatsynthesize contextual information.1 IntroductionGenerally speaking common referencetasks (Whittaker et al, 2002) and methodsto compare and diagnose spoken language dialogsystems (SLDS) and spoken dialog techniquesare lacking despite previous efforts futher dis-cussed in the next section such as EAGLES,DISC, AUPELF ARCB2 or the ongoing Americanproject DARPA COMMUNICATOR.
Withoutan objective assessment of dialog systems, it isdifficult to reuse previous work and to advancetheories.
The assessment of a dialog system iscomplex in part to the high integration factorand tight coupling between the various modulespresent in any SLDS, for which unfortunatelytoday, no common accepted reference architectureexists.
Nevertheless, a major problem remains thedynamic nature of dialog.
Consequently to theseshortcomings, researchers are often unable toprovide principled design and system capabilitiesfor technology transfer.
In other research areas,such as speech recognition and information re-trieval, common reference tasks have been highlyeffective in sharing research costs and efforts.
Asimilar development is highly needed in the dialogcommunity.In this contribution which addresses only a partof the SLDS evaluation problem, a paradigm forevaluating the context-sensitive understanding ca-pability of any spoken language dialog system isproposed.
PEACE (Devillers et al, 2002a) de-scribed in section 3, is based on test sets extractedfrom real corpora, and has three main aspects: itis generic, contextual and it offers diagnostic ca-pabilities.
Here genericity is envisaged in a con-text of information dialogs access.
The diagnos-tic aspect is important in order to determine thedifferent qualities of the systems under test.
Thecontextual aspect of evaluation is a crucial pointsince dialog is dynamic by nature.
We proposeto simulate/synthesize the contextual information.The PEACE paradigm will be tested in the FrenchTechnolangue MEDIA project and will serve asbasis in the comparison and diagnostic evaluationof systems presented by various academic and in-dustrial sites (section 4).
ELRA/ELDA is the co-ordinator of the larger scope evaluation campaignEVALDA, which includes the MEDIA campaignthat began in January 2003.2 Overview of SLDS evaluationWithout an attempt to be exhaustive, we overviewsome recent efforts for evaluation of SLDS.The objective of the European DISC project wasto write the best-practice guidelines for SLDS de-velopment and evaluation of its time.
DISC hascollected a systematic list of bottom-up evalua-tion criteria, each corresponding to a partially or-dered list of properties likely to be encounteredin any SLDS.
This properties are positioned on agrid defining an SLDS abstract architecture and re-late to various phases of the generic DISC SLDSdevelopment life-cycle (Dybkj?r and al., 1998).They are complemented by a standard evaluationpattern made of 10 generic questions (e.g.
?Whichsymptoms need to be observed?? )
which has beeninstantiated for all the evaluation criteria.
If theDISC results are quite extensive and presented inan homogeneous way, they do not provide a di-rect answer to the question of SLDS evaluation.Its contribution lies more at the specification level.Although the approach and the goals of the Euro-pean EAGLES project were different, one couldforward the same remark about the results of thespeech evaluation work group (D. Gibbon, 1997).In (Fraser, 1998), one find a set of evaluation cri-teria for voice oriented products and services, or-ganized in four broad categories.
: 1) voice com-mand, 2) document generation, 3) phone services4) other.To the best of our knowledge, the MADCOW(Multi Site Data COllection Working group) co-ordination group set up in the USA by ARPA inthe context of the ATIS (Air Travel InformationServices) task to collect corpora, was the first topropose a common infrastructure for SLDS auto-matic evaluation (MADCOW, 1992), which alsoaddressed the problem of language understand-ing evaluation, based on system answer compar-ison.
Unfortunately no direct diagnostic informa-tion can be produced, since understanding is ap-preciated by gauging the distance from the answerto a pair of minimal and a maximal reference an-swers.
In ATIS, the protocol was only been ap-plied to context free sentences.
Up to now it hasbeen one of the most used by the community sinceit is relatively objective and generic because it re-lies on counts of explicit information and allowsfor a certain variation in the answers.
On the otherhand, the method displays a bias toward silenceand does not give the means to appreciate errorseverity.In ARISE (Automatic Railway Information Sys-tems for Europe) (Lamel, 1998), a corpus ofroughly 10,000 calls has been used in conjunc-tion with user debriefing questionnaire analysis todiagnose different versions of a phone informa-tion server.
The hand-tagging objective measuresof the corpus include understanding error counts(glass box methodology).
Although it providesfine grained diagnostic information, this procedurecannot be easily generalized since it requires hand-annotated corpus and access to the internal repre-sentation of the system.Two metrics have been developped at MIT(Glass et al, 2000): the Query Density (QD)and the Concept Efficiency (CE), which measurerespectively over the course of a dialogue: themean number of new concepts introduced per userquery, and the number of turns necessary for eachconcept to be understood by the system.
Con-cepts are generated automatically for each utter-ance with a parsable orthographic transcription asa series of keyword-value pairs.
The higher theQD, the more effectively a user is able to commu-nicate information to the system.
The CE is an in-dicator of recognition or understanding errors; thehigher it is, the fewer times a user needs to repeathimself.
These metrics were evaluated on singlesystems (JUPITER and and MERCURY); to com-pare different systems of the same type, one wouldneed a common ontology.
In (Glass et al, 2000),the authors believe that CE should be related touser frustation, but to show it they would need touse the PARADISE framework.PARADISE (Walker et al, 1998) can be seenas a sort of meta-paradigm which correlates ob-jective and subjective measurements.
Its ground-ing hypothesis states that the goal of any SLDS isto achieve user-satisfaction, which in turn can bepredicted through task success and various interac-tion costs.
With the help of the kappa coefficient(Carletta, 1996) proposes to represent the dialogsuccess independently from the task intrinsic com-plexity, thus opening the way to task generic com-parative evaluation.
PARADISE has been testedin the COMMUNICATOR project (Walker et al,2001) with 9 systems working on the same taskover different databases.
With four basic measures(e.g.
task completion) the protocol has been ableto predict 37% of user satisfaction variation, and42% with the help of a few extra measurements ondialog acts and subtasks.
One critic, one can makeabout PARADISE concern its cost (real user testsare costly) and the use of subjective assessment.The adaption of the DQR text understandingevaluation methodology (Sabatier et al, 2000) tospeech resulted in a generic and qualitative proce-dure.
Each element of its test set holds three parts,the Declaration to define the context, a Questionwhich bears on point present in the context and theResponse.
The test set is organized through sevenlevels of test, from basic explicit understandingto semantic interpretation and reply pertinence as-sessment.
This protocol is task and system genericbut test set construction is not straightforward andthe bias introduced by the wording of the questionis difficult to assess.Recently the GDR-13 work group of CNRSon spoken dialog understanding, has proposed anevaluation methodology for literal understanding.According to (Antoine and al., 2002), DEFI triesto remedy two important weaknesses of the MAD-COW methodology, namely the lack of genericityand the lack of diagnostic information, by craft-ing system specific test sets from a primary set ofenunciations representative of the task (providedby the developers).
Secondary enunciations arethen derived from the primary ones in order to ex-hibit particular language phenomena.
Afterwards,the systems are evaluated by their developers us-ing specific test set and their own metrics.
Thevarious results can be mapped over a generic ab-stract architecture for comparison (although thismapping is still unspecified at the time of writ-ing).
DEFI has already been used in one evalua-tion campaign, with 5 systems presented by 4 lab-oratories.
(Antoine and al., 2002) has reported thefollowing weaknesses of the protocol: how to con-trol the bias introduced by the derivation of enun-ciations, how to guaranty that derived enunciationwill remain in the task scope (this prevented somesystem from being evaluated over the completetest set) and finally how to restrict and organizethe language phenomena used in the test set.3 The PEACE paradigmWe first describe the paradigm and relate prelim-inary experiments with PEACE.
This paradigmwhich is as basement for the MEDIA project willbe refined by all the partners and use for an evalua-tion campaign between seven systems of industrialand academic sites.3.1 DescriptionThe PEACE paradigm relies on the idea that fordatabase querying tasks, it is possible to define acommon semantic representation, onto which allthe systems are able to convert their own repre-sentation (Moore, 1994).
The paradigm based ondata extracted from real corpus, includes both lit-eral and contextual understanding test sets.
Moreprecisely, it provides: the definition of a semantic representation(see 3.1.1), the definition of a model for dialogic contexts(see 3.1.2), the definition and typology of linguistic phe-nomena and dialogic functions used to selec-tively diagnoze the system language capabil-ities (anaphora resolution, constraints relax-ation, etc.)
(see 3.1.3), a data structuring method.
The format of theannotated data will be adapted to languageresource standard annotations implemented(see 3.1.4), and evaluation metrics with the correspond-ing evaluation tool (see 3.1.5).3.1.1 Generic semantic representationThe difficulty of choosing a semantic represen-tation lies in finding a complete and simple repre-sentation of a user utterance meaning in a unifiedformat.
A frame Attribute Value Representation(AVR) has been chosen, allowing a fast and re-liable annotation.
The values are either numericunits, proper names, or semantic classes, thatgroup together lexical units which are synonymsfor the task.
The order of the (attribute, value)pairs in the semantic representation matches theirrespective position in the utterance.
A modal in-formation (positive (+) and negative(-)) is also as-signed to each (attribute, value) pair.
The semanticrepresentation of an utterance consists then in a listof triplets of the form (mode, attribute, normalizedvalue).
An example is given in figure 1.
In orderto take into account for long-time dependencies orto allow multiple referenced objects, the semanticrepresentation may be enriched by adding a refer-ence value to each triplet for the representation oflinks between 2 attributes of the utterance.Attributes can grouped into different classes: the database attributes (the most frequent)correspond to the attributes of the databasetables (e.g.
category for an hotel); the modifier attributes are associated tothe database concepts.
Their values areused to modify the database concept in-terpretation values (e.g.
the attributecategory-modifier with possible val-ues: >; <, =, Max, Min); the discursive attributes are introduced tohandle various aspects of dialogic interactionUser c?est pas Paris c?est PassyQuery it is not Paris it is Passy(LU) AVR (-, place, Paris)(+, place, Passy)Figure 1: Example of a semantic representation of an ut-terance with positive and negative information for the ARISEtask.
Place is an database attribute,Paris and Passy arevalues and +/- modal markers.(e.g.
commandwith values cancelation, cor-rection, error specification: : :, or responsewith values yes or no); the argument attribute which represents thetopic at the focus of the utterance.When dealing with information retrieval appli-cations, defining the database and modifier at-tributes and the appropriate values can be donein a rather straightforward way.
Most of thoseattributes are derived directly from the informa-tion stored in the database.
Furthermore, most ofthe discursive attributes are domain-independent.Some database attributes remain unchanged acrossmany tasks, such as those dealing with dates orprices.This semantic representation has been used atLIMSI for PARIS-SITI TASK (touristic informa-tion) and ARISE TASK (traintable information)both with triplet representation.
More recently inthe context of the AMITIES project, quadrupletswere used.3.1.2 Contextual understanding modelingContextual understanding evaluation providesinformation about the capability of the systemto take into account the dialog history in orderto properly interpret the user query.
Contextualunderstanding evaluation is rarely performed be-cause of the dynamic nature of the dialog makethe dialog context depend on the system?s dialogstrategy.Nevertheless PEACE proposes a system-independent way to evaluate local contextualinterpretation.
Given U1:::Utthe user inter-actions, and S1:::Stthe answers of the agentor system, the context a time t is a functionf(U1; S1; U2; S2; :::Ut; St).
In the PEACEparadigm, a paraphrase of the context is derivedfrom the semantic representation (Bonneau-Maynard et al, 2000).The dialog contexts are extracted from real di-alogs in three steps.
First, the internal semanticframes representing the dialog contexts are auto-matically extracted from the log files of the ses-sion recordings.
Secondly, the semantic framesare converted into AVR format and then hand-corrected to faithfully represent the dialog history.The last step consists in the writing of a sentencefor each context (the context paraphrase), whichresults in the same AVR representation as the oneof the dialog context.Two possibilities may be investigated for build-ing the paraphrase from the internal semantic rep-resentation of the dialog context.
A rule-based ortemplate-based natural language generation mod-ule can be used to automatically produce the para-phrase.
The paraphrase can also be obtainedby concatenating the sentences preceding the ex-tracted dialog state.
In both cases, a manual veri-fication is needed.3.1.3 A typology of linguistic phenomena anddialogic functionsFor dialog system evaluation, it is essential tobuild test sets randomly extracted from real cor-pus.
For dialog system diagnosis, it is also crucialto build test sets labeled with the linguistic phe-nomena and dialogic functions.
Thus, the capabil-ities of system?s contextual understanding can beassessed for the main linguistic and dialogic dif-ficulties such as, for instance, anaphora or ellipsisresolution.3.1.4 A data structuring methodTwo types of units, one for literal understanding(LU), the other for contextual understanding (CU)are defined.
The format of the annotated data willbe adapted to language resource standard annota-tions implemented in XML, e.g.
(Geoffrois et al,2000), (Ide and Romary, 2002).Each unit is extracted from a real dialog cor-pus.
LU units are composed of the user query,the corresponding audio signal, an automatic tran-scription obtained with a recognition system, andfinally the literal semantic representation of the ut-terance (see Figure 1).
CU units are composed ofContext je voudrais un ho?tel 4paraphrase e?toiles dans le neuvie`meI would like a 4 categoryhotel in the ninth(LU) AVR (+, argument, hotel)(+, district, 9)(+, category, 4)User la me?me cate?gorie dansquery un autre arrondissementthe same category inanother district(LU) AVR (+, other, district)(+, same, category)(CU) AVR (+, argument, hotel)(-, district, 9)(+, category, 4)Figure 2: Example of a contextual understanding unit com-posed of a context paraphrase, a user query and the resultingAVR.
AVR of context paraphrase and user query are given inTYPEWRITING MODE.
Ellipsis (?in the ninth?)
and anaphora(?same category?, ?another district?)
may be observed.the dialog context (given by the paraphrase), theuser query and the resulting AVR of the user queryin the given context (see Figure 2).
Those units arealso labeled with linguistic and dialogic phenom-ena.3.1.5 Evaluation metrics and scoring toolCommon evaluation metrics are essential foranalyzing the system capabilities.
The scoring toolfor AVR comparison is able to compare betweentwo AVR frame representation sets.
For evalu-ation, system outputs translated in AVR formatcomposed one set, the other one contains the AVRreferences which are manually annotated.
Bothframe sets have the form of a list of AVRs (fixedlength records).
Each record is composed of threeor four fields (mode, attribute, value, reference).The comparison consists in applying a set of pre-defined operators each assigned with a cost value.The comparison process looks for operator liststo be applied to the test frame in order to obtainthe reference frame that minimizes the final costvalue.
For a global evaluation, the classical opera-tors from speech evaluation (DELetion, INSertionand SUBstitution) may be used (as used for firsttwo values of Accuracy percentage in Table 1).With our scoring tool the definition of new opera-tors is quite easy.
It is then also possible to distin-guish between different types of errors by definingspecific operators (as used to estimate Topic iden-tification in Table 1), or by using different cost val-ues (for example a substitution is often consideredmore costly for dialog management).3.2 Example use of PEACEIn order to validate the evaluation paradigm, aset of approximatively 1,700 literal units and aset of 100 contextual units has been used forthe PARIS-SITI task (Bonneau-Maynard and Dev-illers, 2000).
Results for both literal and contex-tual understanding test sets are given in Table 1.
Inorder to observe the ability of the systems to dealwith recognition errors, each literal understand-ing unit also contains the ASR transcription of theoriginal user utterance.
The various measures ofunderstanding accuracy are computed as the ratiobetween the sum of the number of deleted, insertedand substituted attributes, and the total number ofAVR attributes in the test set.
The possibility ofan automatic evaluation of the LU accuracy andthe ability of the scoring tool to point out the er-rors allowed us to easily improve the literal un-derstanding accuracy from 89.0% to 93.5%.
Dueto a 26.5% ASR error rate, the LU accuracy goesdown from 93.5% to 72% after ASR transcription.The contextual understanding accuracy on the 100test units is 82.6% on exact transcription.
Forinstance, anaphoric references are relatively wellsolved, with 80.4% accuracy on the 50 units con-taining at least one anaphoric reference.
For eachexample, the anaphoric referenced object is gen-erally correctly identified and remaining errors areoften due to a bad history constraint management.3.3 Discussing the PEACE paradigmThe PEACE paradigm enables automatic evalua-tion of literal and contextual dialog understand-ing.
The evaluation paradigm makes the distinc-tion between different types of errors, allowing aqualitative and diagnostic analysis of the perfor-mances of a speech understanding module.
Veryfew evaluation paradigms propose automatic di-agnosis of contextual interpretation (Glass et al,2000).
The proposed methodology is based on#Units #Attr.
%Acc.
Prec.LU exact 1 681 3 991 93.5% 0.7LU ASR .
1 681 3 991 72.0% 1.4Topic id.
680 833 94.3% 1.6Modifier id.
323 445 95.7% 1.9CU exact 100 430 86.8% 3.2Anaphoric 50 245 84.4% 4.5resolutionEllipsis 25 106 85.3% 6.7resolutionTable 1: Literal understanding (LU) accuracy on both exactand ASR transcription, and contextual understanding (CU)accuracy.
Second column indicates the number of units in-cluded in the test set (i.e # of user utterances), third col-umn gives the total number of attributes in the correct AVRtest sets.
Details, using specific operators, are given forargument (topic) and modifier identification for LU onexact transcription, and for anaphoric reference and ellipsisresolution for CU.
Last column gives the 95% precision ofthe accuracy estimation (Montacie?
and Chollet, 1997).semi-automatically built reference test sets, andtherefore is much more time effective than manualevaluation.
Furthermore, it provides reproducibletests.Although the semantic representation is task de-pendent, the example described above shows thefeasibility of the paradigm for any dialog systeminterfacing to a database.
Robustness to manylinguistic phenomena such as repetitions, hesita-tions or auto-corrections may be evaluated withthis method.
XML coding will facilitate the gener-icity and the reusability of the test sets, by al-lowing the selection of the dialogic contexts to bestudied.The representation of the dialog context with asingle paraphrase, derived from a ?flat?
structuredAVR, may have some limitations in case of long-time dialog dependencies.
It does not allow formemorizing all the steps of the dialog.
For ex-ample, if the speaker says first ?I would like a2 star hotel?, then ?no I prefer 3 stars?
and fi-nally says ?give me again my first choice?, theCU unit cannot take into account this successionof queries.
However, this kind of interaction israrely observed in dialogue corpora: the user usu-ally repeats the constraint value (?give me againa 2 star hotel?).
To represent more precisely thedialog state, the representation of the dialog con-text should incorporate some meta-information in-spired for example from the DAMSL annotationstandard 1 (Devillers et al, 2002b).Another point is the representativity of the testsets.
This may be considered as a limitation asfar as PEACE paradigm is built on the idea thatthe test units are extracted from real dialogs.
Ob-viously, the larger the test sets are, the better.
Adiagnostic evaluation may need a very large testcorpora to validate system performance against thewide range of phenomena present in spontaneousdialog.The ability to automatically diagnose the per-formances of contextual understanding moduleson local difficulties such as ellipsis, negations,anaphoric reference or constraint relaxation is oneof the major advantages of the PEACE paradigm,which has not been investigated by other method-ologies.
This is why it has been chosen for theMEDIA project described in the next section.4 The MEDIA projectThe MEDIA project proposes a paradigm basedon a reference task and on test sets extracted fromreal corpora for evaluating literal and contextualunderstanding in dialog systems.
The PEACEparadigm will serve as basis for the MEDIAproject.
The consortium is composed of IRIT,LIA, LIMSI, LORIA, VALORIA for the Frenchacademic sites and France Telecom R&D andTELIP for the industrial sites.
The scientific com-mittee contains representatives of AT&T (USA),Tilburg University (Netherlands), IBM, IMAG,LIUM and VECSYS (France).The project has four main parts.
First, the selec-tion of reference task such as for example a taskof web-based travel agency.
The reference taskhas to correspond to a real-life application allow-ing real user tests.
Secondly, multi-level represen-tation such as the semantic representation, the ty-pology of linguistic phenomena and dialogic func-tions, the dialog context model... will be com-monly refined and adapted to the reference task.The third part deals with the recording and la-beling of a dialog corpus which will be used for1http://www.cs.rochester.edu/research/trains/annotationboth system adaptation and test set selection.
Thelast part is the organisation of the evaluation cam-paigns by ELRA/ELDA for the participating sites.ELRA/ELDA is the coordinator of a largerscope project: EVALDA which includes amongothers, the MEDIA project.
ELDA with VEC-SYS will provide transcribed and annotated cor-pora and evaluation tools according to consortiumspecifications.
The recording of 1200 French di-alogs (240 speakers, 5 dialogs each, 15k userqueries) is planned.
Three sets of LU and CUunits will be built from this corpus.
A large sizeadaptation set will be used by the participants toadapt their system to the task and the semanticrepresentation.
The development set (around 1KLU (resp.
CU) units) will be used to validate theevaluation protocole.
The size of the test set isplanned to be around 3K LU (resp.
CU) units.
Var-ious approaches are currently used at the partici-pating sites; stochastic or syntactic and semanticrule-based modeling.
The project started in Jan-uary 2003 and will last two years.5 ConclusionAssessing the dialog system understanding capa-bilities requires to evaluate the transition betweensuccessive states of the dialog.
At least, we mustbe able to test a sequence of two states at anypoint in the dialog.
The dynamic and interac-tive nature of the dialog makes construction andreuse of test sets difficult.
Furthermore, to eval-uate one particular dialog transition, the systemhas to be put in a particular state corresponding tothe original dialog context.
The variable describ-ing the dialog state can be composed of complexinformation such as the current semantic frame(list of triplets (mode,attribute,value) or quadru-ples (mode, attribute, value, reference)), the dialoghistory semantic frame and potentially other infor-mation like recognition scores, dialog acts, etc.The PEACE paradigm allows the evaluation oftwo successive simplified dialog states.
It has beensuccessfully tested with test samples focusing onlinguistic difficulties of literal and contextual un-derstanding.
For these tests, the dialog state isthe dialog history semantic frame.
The contextualunderstanding modeling in PEACE is system inde-pendent since the context is given by a paraphraseof queries.
PEACE allows a diagnostic evaluationof specific semantic attributes and particular lin-guistic phenomena.In our opinion, it is crucial for the dialog com-munity to agree on a common reference task andreference test sets in order to be able to compareand diagnose dialog systems.
Both evaluation withreal users and artificial simulation of successivedialog states using test sets extracted from real cor-pora have to be carried out in parallel.
The use oftest sets reduces the global cost of dialog systemevaluation, moreover such tests are reproducible.The PEACE protocol will be used as basis forthe French Technolangue MEDIA project in a twoyear evaluation campaign where dialog systemsfrom both academia and industry will be evalu-ated.
In other domains, it could be related with(Hirschman, 2000) propositions for Question An-swering evaluation.ReferencesJ.Y.
Antoine and al.
2002.
Predictive and objective evalua-tion of speech understanding: the ?challenge?
evaluationcampaign of the i3 speech workgroup of th french cnrs.
InLREC2002, Spain, May.
ELRA.H.
Bonneau-Maynard and L. Devillers.
2000.
A frameworkfor evaluating contextual understanding.
In ICSLP.H.
Bonneau-Maynard, L. Devillers, and S. Rosset.
2000.Predictive performance of dialog systems.
In LREC2000,volume 1, pages 177?181, Athens, Greece, May.
ELRA.J.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistics.
Computational Linguistics,2(22):249?254.R.
Winski D. Gibbon, R. Moore.
1997.
Handbook of Stan-dards and Ressources for Spoken Language Ressources.Mouton de Gruyter, New York.L.
Devillers, H. Maynard, and P. Paroubek.
2002a.Me?thodologies d?e?valuation des syste`mes de dia-logue parle?
: re?flexions et expe?riences autour de lacompre?hension.
In Traitement Automatique des Langues,volume 43, pages 155?184.L.
Devillers, S. Rosset, H. Bonneau-Maynard, and L. Lamel.2002b.
Annotations for dynamic diagnosis of the dialogstate.
In LREC2002, Spain, May.
ELRA.L.
Dybkj?r and al.
1998.
The disc approach to spo-ken language systems development and evaluation.
InLREC1998), volume 1, pages 185?189, Spain, May.ELRA.N.
Fraser.
1998.
Spoken Language System Assessment, vol-ume 3.
Mouton de Gruyter, New York.E.
Geoffrois, C. Barras, S. Bird, and Z. Wu.
2000.
Tran-scribing with annotation graphs.
In LREC2000, volume 2,pages 1517?1521, Greece, May.
ELRA.J.
Glass, J. Polifroni, S. Seneff, and V. Zue.
2000.
Datacollection and performance evaluation of spoken dialoguesystems: the MIT experience.Lynette Hirschman.
2000.
Reading comprehension andquestion answering new evaluation paradigms for humanlanguage technology.
In LREC2000 Workshop ?UsingEvaluation within HLT Programs: Results and Trends?,pages 54?59, Greece, May.
ELRA.N.
Ide and L. Romary.
2002.
Towards multimodal contentrepresentation.
In LREC 2002.L.
Lamel.
1998.
Spoken language dialog system develop-ment and evaluation at limsi.
In Actes de l?InternationalSymposium on Spoken Dialogue, Sydney, Australia,November.MADCOW.
1992.
Multi-site data collection for a spokenlanguage corpus.
In DARPA Speech and Natural Lan-guage Workshop.C.
Montacie?
and G. Chollet.
1997.
Syste`mes de re?fe?rencepour l?e?valuation d?applications et la caracte?risation debases de donne?es en reconnaissance de la parole.
In16e`me JEP.R.C.
Moore.
1994.
Semantic evaluation for spoken-languagesystems.
In DARPA Speech and Natural Language Work-shop.P.
Sabatier, Ph.
Blache, J. Guizol, F. Le?vy, A. Nazarenko,and S. N?Guema.
2000. e?valuer des syste`mes decompre?hension de textes.
In Ressources et Evaluation enInge?nierie Linguistique, pages 265?275.
Chibout K. et al(Eds) Duculot.M.
Walker, D. Litman, C. Kamm, and A. Abella.
1998.
Eval-uating spoken dialogue agents with paradise: 2 cases stud-ies.
Computer Speech and Language, 3(12):317?347.M.
Walker, R. Passonneau, and J.E.
Boland.
2001.
Quantita-tive and qualitative evaluation of darpa communicatorspo-ken dialog systems.
In Actes du 39me ACL, pages 515?522, Toulouse, France, July.
ACL.S.
Whittaker, L. Terveen, and B. Nardi.
2002.
Reference taskagenda for HCI.
In ISLE workshop 2002.
