Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 737?744,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Generation of Domain Models for Call Centers from NoisyTranscriptionsShourya Roy and L Venkata SubramaniamIBM ResearchIndia Research LabIIT Delhi, Block-1New Delhi 110016Indiarshourya,lvsubram@in.ibm.comAbstractCall centers handle customer queries from variousdomains such as computer sales and support, mo-bile phones, car rental, etc.
Each such domaingenerally has a domain model which is essentialto handle customer complaints.
These modelscontain common problem categories, typical cus-tomer issues and their solutions, greeting styles.Currently these models are manually created overtime.
Towards this, we propose an unsupervisedtechnique to generate domain models automati-cally from call transcriptions.
We use a state ofthe art Automatic Speech Recognition system totranscribe the calls between agents and customers,which still results in high word error rates (40%)and show that even from these noisy transcrip-tions of calls we can automatically build a domainmodel.
The domain model is comprised of pri-marily a topic taxonomy where every node is char-acterized by topic(s), typical Questions-Answers(Q&As), typical actions and call statistics.
Weshow how such a domain model can be used fortopic identification of unseen calls.
We also pro-pose applications for aiding agents while handlingcalls and for agent monitoring based on the do-main model.1 IntroductionCall center is a general term for help desks, infor-mation lines and customer service centers.
Manycompanies today operate call centers to handlecustomer issues.
It includes dialog-based (bothvoice and online chat) and email support a userreceives from a professional agent.
Call centershave become a central focus of most companies asthey allow them to be in direct contact with theircustomers to solve product-related and services-related issues and also for grievance redress.
Atypical call center agent handles over a hundredcalls in a day.
Gigabytes of data is produced ev-ery day in the form of speech audio, speech tran-scripts, email, etc.
This data is valuable for doinganalysis at many levels, e.g., to obtain statisticsabout the type of problems and issues associatedwith different products and services.
This data canalso be used to evaluate agents and train them toimprove their performance.Today?s call centers handle a wide variety of do-mains such as computer sales and support, mobilephones and apparels.
To analyze the calls in anydomain, analysts need to identify the key issuesin the domain.
Further, there may be variationswithin a domain, say mobile phones, based on theservice providers.
The analysts generate a domainmodel through inspection of the call records (au-dio, transcripts and emails).
Such a model can in-clude a listing of the call categories, types of prob-lems solved in each category, listing of the cus-tomer issues, typical questions-answers, appropri-ate call opening and closing styles, etc.
In essence,these models provide a structured view of the do-main.
Manually building such models for vari-ous domains may become prohibitively resourceintensive.
Another important point to note is thatthese models are dynamic in nature and changeover time.
As a new version of a mobile phoneis introduced, software is launched in a country, asudden attack of a virus, the model may need to berefined.
Hence, an automated way of creating andmaintaining such a model is important.In this paper, we have tried to formalize the es-sential aspects of a domain model.
It comprisesof primarily a topic taxonomy where every nodeis characterized by topic(s), typical Questions-737Answers (Q&As), typical actions and call statis-tics.
To build the model, we first automaticallytranscribe the calls.
Current automatic speechrecognition technology for telephone calls havemoderate to high word error rates (Padmanabhanet al, 2002).
We applied various feature engi-neering techniques to combat the noise introducedby the speech recognition system and applied textclustering techniques to group topically similarcalls together.
Using clustering at different gran-ularity and identifying the relationship betweengroups at different granularity we generate a tax-onomy of call types.
This taxonomy is augmentedwith various meta information related to each nodeas mentioned above.
Such a model can be usedfor identification of topics of unseen calls.
To-wards this, we envision an aiding tool for agentsto increase agent effectiveness and an administra-tive tool for agent appraisal and training.Organization of the paper: We start by de-scribing related work in relevant areas.
Section 3talks about the call center dataset and the speechrecognition system used.
The following sectioncontains the definition and describes an unsuper-vised mechanism for building a topical modelfrom automatically transcribed calls.
Section 5demonstrates the usability of such a topical modeland proposes possible applications.
Section 6 con-cludes the paper.2 Background and Related WorkIn this work, we are trying to bridge the gap be-tween a few seemingly unrelated research areasviz.
(1) Automatic Speech Recognition(ASR), (2)Text Clustering and Automatic Taxonomy Gener-ation (ATG) and (3) Call Center Analytics.
Wepresent some relevant work done in each of theseareas.Automatic Speech Recognition(ASR): Auto-matic transcription of telephonic conversations isproven to be more difficult than the transcriptionof read speech.
According to (Padmanabhan etal., 2002), word-error rates are in the range of 7-8% for read speech whereas for telephonic speechit is more than 30%.
This degradation is dueto the spontaneity of speech as well as the tele-phone channel.
Most speech recognition systemsperform well when trained for a particular accent(Lawson et al, 2003).
However, with call cen-ters now being located in different parts of theworld, the requirement of handling different ac-cents by the same speech recognition system fur-ther increases word error rates.Automatic Taxonomy Generation (ATG): In re-cent years there has been some work relating tomining domain specific documents to build an on-tology.
Mostly these systems rely on parsing (bothshallow and deep) to extract relationships betweenkey concepts within the domain.
The ontology isconstructed from this by linking the extracted con-cepts and relations (Jiang and Tan, 2005).
How-ever, the documents contain well formed sentenceswhich allow for parsers to be used.Call Center Analytics: A lot of work on auto-matic call type classification for the purpose ofcategorizing calls (Tang et al, 2003), call rout-ing (Kuo and Lee, 2003; Haffner et al, 2003), ob-taining call log summaries (Douglas et al, 2005),agent assisting and monitoring (Mishne et al,2005) has appeared in the past.
In some cases, theyhave modeled these as text classification problemswhere topic labels are manually obtained (Tang etal., 2003) and used to put the calls into differentbuckets.
Extraction of key phrases, which can beused as features, from the noisy transcribed callsis an important issue.
For manually transcribedcalls, which do not have any noise, in (Mishne etal., 2005) a phrase level significance estimate isobtained by combining word level estimates thatwere computed by comparing the frequency of aword in a domain-specific corpus to its frequencyin an open-domain corpus.
In (Wright et al, 1997)phrase level significance was obtained for noisytranscribed data where the phrases are clusteredand combined into finite state machines.
Otherapproaches use n-gram features with stop word re-moval and minimum support (Kuo and Lee, 2003;Douglas et al, 2005).
In (Bechet et al, 2004) callcenter dialogs have been clustered to learn aboutdialog traces that are similar.Our Contribution: In the call center scenario, theauthors are not aware of any work that deals withautomatically generating a taxonomy from tran-scribed calls.
In this paper, we have tried to for-malize the essential aspects of a domain model.We show an unsupervised method for building adomain model from noisy unlabeled data, which isavailable in abundance.
This hierarchical domainmodel contains summarized topic specific detailsfor topics of different granularity.
We show howsuch a model can be used for topic identificationof unseen calls.
We propose two applications for738aiding agents while handling calls and for agentmonitoring based on the domain model.3 Issues with Call Center DataWe obtained telephonic conversation data col-lected from the internal IT help desk of a com-pany.
The calls correspond to users making spe-cific queries regarding problems with computersoftware such as Lotus Notes, Net Client, MS Of-fice, MS Windows, etc.
Under these broad cate-gories users faced specific problems e.g.
in LotusNotes users had problems with their passwords,mail archiving, replication, installation, etc.
It ispossible that many of the sub problem categoriesare similar, e.g.
password issues can occur withLotus Notes, Net Client and MS Windows.We obtained automatic transcriptions of the di-alogs using an Automatic Speech Recognition(ASR) system.
The transcription server, used fortranscribing the call center data, is an IBM re-search prototype.
The speech recognition systemwas trained on 300 hours of data comprising ofhelp desk calls sampled at 6KHz.
The transcrip-tion output comprises information about the rec-ognized words along with their durations, i.e., be-ginning and ending times of the words.
Further,speaker turns are marked, so the agent and cus-tomer portions of speech are demarcated withoutexactly naming which part is the agent and whichthe customer.
It should be noted that the call cen-ter agents and the customers were of different na-tionalities having varied accents and this furthermade the job of the speech recognizer hard.
Theresultant transcriptions have a word error rate ofabout 40%.
This high error rate implies that manywrong deletions of actual words and wrong inser-tion of dictionary words have taken place.
Alsooften speaker turns are not correctly identified andvoice portions of both speakers are assigned to asingle speaker.
Apart from speech recognition er-rors there are other issues related to spontaneousspeech recognition in the transcriptions.
There areno punctuation marks, silence periods are markedbut it is not possible to find sentence boundariesbased on these.
There are repeats, false starts, alot of pause filling words such as um and uh, etc.Portion of a transcribed call is shown in figure 1.Generally, at these noise levels such data is hardto interpret by a human.
We used over 2000 callsthat have been automatically transcribed for ouranalysis.
The average duration of a call is about 9SPEAKER 1: windows thanks for calling and you canlearn yes i don?t mind it so then i went toSPEAKER 2: well and ok bring the machine frontend loaded with a standard um and that?s um it?sa desktop machine and i did that everything wasworking wonderfully um I went ahead connectedinto my my network um so i i changed my networksettings to um to my home network so i i can youknow it?s showing me for my workroom um and thenit is said it had to reboot in order for changesto take effect so i rebooted and now it?s askingme for a password which i never i never saidanything upSPEAKER 1: ok just press the escape key i candoesn?t do anything can you pull up so that i meanFigure 1: Partial transcript of a help desk dialogminutes.
For 125 of these calls, call topics weremanually assigned.4 Generation of Domain ModelFig 2 shows the steps for generating a domainmodel in the call center scenario.
This section ex-plains different modules shown in the figure.4.1 Description of ModelWe propose the Domain Model to be comprisedof primarily a topic taxonomy where every nodeis characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statis-tics.
Generating such a taxonomy manually fromscratch requires significant effort.
Further, thechanging nature of customer problems requiresfrequent changes to the taxonomy.
In the next sub-section, we show that meaningful taxonomies canbe built without any manual supervision from acollection of noisy call transcriptions.4.2 Taxonomy GenerationAs mentioned in section 3, automatically tran-scribed data is noisy and requires a good amountof feature engineering before applying any textanalytics technique.
Each transcription is passedthrough a Feature Engineering Component to per-form noise removal.
We performed a sequence ofcleansing operations to remove stopwords such asthe, of, seven, dot, january, hello.
We also removepause filling words such as um, uh, huh .
The re-maining words in every transcription are passedthrough a stemmer (using Porter?s stemming algo-739StopwordRemovalN-gramExtractionDatabase,archive,replicateCanyouaccessyahoo?Is modemon?CallstatisticsFeature EngineeringASRClusterer TaxonomyBuilderModelBuilderComponentClusters of differentgranularityVoice help-desk data123 45Figure 2: 5 Steps to automatically build domain model from a collection of telephonic conversationrecordingsrithm 1) to extract the root form of every word e.g.call from called.
We extract all n-grams whichoccur more frequently than a threshold and do notcontain any stopword.
We observed that usingall n-grams without thresholding deteriorates thequality of the generated taxonomy.
a t & t, lotusnotes, and expense reimbursement are some exam-ples of extracted n-grams.The Clusterer generates individual levels ofthe taxonomy by using text clustering.
We usedCLUTO package 2 for doing text clustering.
Weexperimented with all the available clusteringfunctions in CLUTO but no one clustering al-gorithm consistently outperformed others.
Also,there was not much difference between variousalgorithms based on the available goodness met-rics.
Hence, we used the default repeated bisec-tion technique with cosine function as the similar-ity metric.
We ran this algorithm on a collectionof 2000 transcriptions multiple times.
First wegenerate 5 clusters from the 2000 transcriptions.Next we generate 10 clusters from the same setof transcriptions and so on.
At the finest level wesplit them into 100 clusters.
To generate the topic1http://www.tartarus.org/?martin/PorterStemmer2http://glaros.dtc.umn.edu/gkhome/views/clutotaxonomy, these sets containing 5 to 100 clustersare passed through the Taxonomy Builder compo-nent.
This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v1 to v2 if v1 and v2share at least one document between them, andwhere v2 is one level finer than v1.
Now v1 and v2become nodes in adjacent layers in the taxonomy.Here we found the taxonomy to be a tree but ingeneral it can be a DAG.
Now onwards, each nodein the taxonomy will be referred to as a topic.This kind of top-down approach was preferredover a bottom-up approach because it not onlygives the linkage between clusters of various gran-ularity but also gives the most descriptive and dis-criminative set of features associated with eachnode.
CLUTO defines descriptive (and discrimi-native) features as the set of features which con-tribute the most to the average similarity (dissim-ilarity) between documents belonging to the samecluster (different clusters).
In general, there is alarge overlap between descriptive and discrimina-tive features.
These features, topic features, arelater used for generating topic specific informa-tion.
Figure 3 shows a part of the taxonomy ob-tained from the IT help desk dataset.
The labels740atandtconnect lotusnotclick clientconnectwirelessnetworkdefaultpropertinetnetclientlocalareaareaconnectroutercabldatabasserver foldercopi archivreplicmailslashfolderfilearchivdatabasservercopilocalcopiFigure 3: A part of the automatically generatedontology along with descriptive features.shown in Figure 3 are the most descriptive and dis-criminative features of a node given the labels ofits ancestors.4.3 Topic Specific InformationThe Model Builder component in Figure 2 createsan augmented taxonomy with topic specific infor-mation extracted from noisy transcriptions.
Topicspecific information includes phrases that describetypical actions, typical Q&As and call statistics(for each topic in the taxonomy).Typical Actions: Actions correspond to typical is-sues raised by the customer, problems and strate-gies for solving them.
We observed that action re-lated phrases are mostly found around topic fea-tures.
Hence, we start by searching and collect-ing all the phrases containing topic words fromthe documents belonging to the topic.
We definea 10-word window around the topic features andharvest all phrases from the documents.
The setof collected phrases are then searched for n-gramswith support above a preset threshold.
For exam-ple, both the 10-grams note in click button to setup for all stops and to action settings and click thebutton to set up increase the support count of the5-gram click button to set up.The search for the n-grams proceeds based ona threshold on a distance function that counts theinsertions necessary to match the two phrases.
Forexample can you is closer to can < ... > you thanto can < ... >< ... > you.
Longer n-grams areallowed a higher distance threshold than shorter n-grams.
After this stage we extracted all the phrasesthat frequently occur within the cluster.In the second step, phrase tiling and ordering,we prune and merge the extracted phrases and or-der them.
Tiling constructs longer n-grams fromsequences of overlapping shorter n-grams.
Wenoted that the phrases have more meaning if theyare ordered by their appearance.
For example, ifgo to the program menu typically appears beforeselect options from program menu then it is morethank you for calling this isproblem with our serial number softwareQ: may i have your serial numberQ: how may i help you todayA: i?m having trouble with my at&t network........................click on advance log in propertiesi want you to right clickcreate a connection across an existing internetconnectionin d. n. s. use default network........................Q: would you like to have your ticketA: ticket number is twothank you for calling and have a great daythank you for calling bye byeanything else i can help you withhave a great day you tooFigure 4: Topic specific informationuseful to present them in the order of their appear-ance.
We establish this order based on the averageturn number where a phrase occurs.Typical Questions-Answers: To understand acustomer?s issue the agent needs to ask the rightset of questions.
Asking the right questions is thekey to effective call handling.
We search for all thequestions within a topic by defining question tem-plates.
The question templates basically look forall phrases beginning with how, what, can I, canyou, were there, etc.
This set comprised of 127such templates for questions.
All 10-word phrasesconforming to the question templates are collectedand phrase harvesting, tiling and ordering is doneon them as described above.
For the answers wesearch for phrases in the vicinity immediately fol-lowing the question.Figure 4 shows a part of the topic specific in-formation that has been generated for the defaultproperti node in Fig 3.
There are 123 documentsin this node.
We have selected phrases that occurat least 5 times in these 123 documents.
We havecaptured the general opening and closing stylesused by the agents in addition to typical actionsand Q&As for the topic.
In this node the docu-ments pertain to queries on setting up a new A T &T network connection.
Most of the topic specificissues that have been captured relate to the agent741leading the customer through the steps for settingup the connection.
In the absence of tagged datasetwe could not quantify our observation.
However,when we compared the automatically generatedtopic specific information to the extracted infor-mation from the hand labeled calls, we noted thatalmost all the issues have been captured.
In factthere are some issues in the automatically gener-ated set that are missing from the hand labeled set.The following observations can be made from thetopic specific information that has been generated:?
The phrases that have been captured turn outto be quite well formed.
Even though theASR system introduces a lot of noise, the re-sulting phrases when collected over the clus-ters are clean.?
Some phrases appear in multiple forms thankyou for calling how can i help you, how mayi help you today, thanks for calling can ibe of help today.
While tiling is able tomerge matching phrases, semantically simi-lar phrases are not merged.?
The list of topic specific phrases, as alreadynoted, matched and at times was more ex-haustive than similar hand generated sets.Call Statistics: We compute various aggregatestatistics for each node in the topic taxonomy aspart of the model viz.
(1) average call duration(inseconds), (2) average transcription length(numberof words) (3) average number of speaker turns and(4) number of calls.
We observed that call dura-tions and number of speaker turns varies signifi-cantly from one topic to another.
Figure 5 showsaverage call duration and corresponding averagetranscription lengths for a few interesting topics.
Itcan be seen that in topic cluster-1, which is aboutexpense reimbursement and related stuff, most ofthe queries can be answered quickly in standardways.
However, some connection related issues(topic cluster-5) require more information fromcustomers and are generally longer in duration.
In-terestingly, topic cluster-2 and topic cluster-4 havesimilar average call durations but quite differentaverage transcription lengths.
On investigation wefound that cluster-4 is primarily about printer re-lated queries where the customer many a times isnot ready with details like printer name, ip addressof the printer, resulting in long hold time whereasfor cluster-2, which is about online courses, users01002003004005006007008009005432102004006008001000120014001600CallDuration(secs)TranscriptionLength(no.ofwords)Topic ClusterFigure 5: Call duration and transcription length forsome topic clustersgenerally have details like course name, etc.
readywith them and are interactive in nature.We build a hierarchical index of type{topic?information} based on this automat-ically generated model for each topic in the topictaxonomy.
An entry of this index contains topicspecific information viz.
(1) typical Q&As, (2)typical actions, and (3) call statistics.
As wego down this hierarchical index the informationassociated with each topic becomes more andmore specific.
In (Mishne et al, 2005) a manuallydeveloped collection of issues and their solutionsis indexed so that they can be matched to thecall topic.
In our work the indexed collection isautomatically obtained from the call transcrip-tions.
Also, our index is more useful because ofits hierarchical nature where information can beobtained for topics of various granularity unlike(Mishne et al, 2005) where there is no concept oftopics at all.5 Application of Domain ModelInformation retrieval from spoken dialog data is animportant requirement for call centers.
Call cen-ters constantly endeavor to improve the call han-dling efficiency and identify key problem areas.The described model provides a comprehensiveand structured view of the domain that can be usedto do both.
It encodes three levels of informationabout the domain:?
General: The taxonomy along with the la-bels gives a general view of the domain.
Thegeneral information can be used to monitortrends on how the number of calls in differ-ent categories change over time e.g.
daily,weekly, monthly.742?
Topic level: This includes a listing of the spe-cific issues related to the topic, typical cus-tomer questions and problems, usual strate-gies for solving the problems, average calldurations, etc.
It can be used to identify pri-mary issues, problems and solutions pertain-ing to any category.?
Dialog level: This includes information onhow agents typically open and close calls, askquestions and guide customers, average num-ber of speaker turns, etc.
The dialog levelinformation can be used to monitor whetheragents are using courteous language in theircalls, whether they ask pertinent questions,etc.The {topic?information} index requires iden-tification of the topic for each call to make useof information available in the model.
Below weshow examples of the use of the model for topicidentification.5.1 Topic IdentificationMany of the customer complaints can be catego-rized into coarse as well as fine topic categoriesby listening to only the initial part of the call.
Ex-ploiting this observation we do fast topic identi-fication using a simple technique based on distri-bution of topic specific descriptive and discrimi-native features (Sec 4.2) within the initial portionof the call.
Figure 6 shows variation in predictionaccuracy using this technique as a function of thefraction of a call observed for 5, 10 and 25 clus-ters verified over the 125 hand-labeled transcrip-tions.
It can be seen, at coarse level, nearly 70%prediction accuracy can be achieved by listeningto the initial 30% of the call and more than 80% ofthe calls can be correctly categorized by listeningonly to the first half of the call.
Also calls relatedto some categories can be quickly detected com-pared to some other clusters as shown in Figure 7.5.2 Aiding and Administrative ToolUsing the techniques presented in this paper so farit is possible to put together many applications fora call center.
In this section we give some exam-ple applications and describe ways in which theycan be implemented.
Based on the hierarchicalmodel described in Section 4 and topic identifica-tion mentioned in the last sub-section we describe102030405060708090100100908070605040302010Predictionaccuracy(%)Fraction of call observed(%)?5-Clusters??10-Clusters?
?25-Clusters?Figure 6: Variation in prediction accuracy withfraction of call observed for 5, 10 and 25 clusters010203040506070809010010987654321Predictionaccuracy(%)Cluster ID25% observed50% observed75% observed100% observedFigure 7: Cluster wise variation in prediction ac-curacy for 10 clusters(1) a tool capable of aiding agents for efficienthandling of calls to improve customer satisfactionas well as to reduce call handling time, (2) an ad-ministrative tool for agent appraisal and training.Agent aiding is done based on the automati-cally generated domain model.
The hierarchicalnature of the model helps to provide generic tospecific information to the agent as the call pro-gresses.
During call handling the agent can beprovided the automatically generated taxonomyand the agent can get relevant information asso-ciated with different nodes by say clicking on thenodes.
For example, once the agent identifies acall to be about {lotusnot} in Fig 3 then he cansee the generic Lotus Notes related Q&As and ac-tions.
By interacting further with the customer theagent identifies it to be of {copi archiv replic}topic and typical Q&As and actions change ac-cordingly.
Finally, the agent narrows down to thetopic as {servercopi localcopi} and suggest solu-tion for replication problem in Lotus Notes.The concept of administrative tool is primar-ily driven by Dialog and Topic level information.We envision this post-processing tool to be used743for comparing completed individual calls with cor-responding topics based on the distribution ofQ&As, actions and call statistics.
Based on thetopic level information we can check whether theagent identified the issues and offered the knownsolutions on a given topic.
We can use the dialoglevel information to check whether the agent usedcourteous opening and closing sentences.
Callsthat deviate from the topic specific distributions,can be identified in this way and agents handlingthese calls can be offered further training on thesubject matter, courtesy, etc.
This kind of post-processing tool can also help us to catch abnor-mally long calls, agents with high average callhandle time, etc.6 Discussion and Future WorkWe have shown that it is possible to retrieve use-ful information from noisy transcriptions of callcenter voice conversations.
We have shown thatthe extracted information can be put in the form ofa model that succinctly captures the domain andprovides a comprehensive view of it.
We brieflyshowed through experiments that this model is anaccurate description of the domain.
We have alsosuggested useful scenarios where the model can beused to aid and improve call center performance.A call center handles several hundred-thousandcalls per year in various domains.
It is very diffi-cult to monitor the performance based on manualprocessing of the calls.
The framework presentedin this paper, allows a large part of this workto be automated.
A domain specific model thatis automatically learnt and updated based on thevoice conversations allows the call center to iden-tify problem areas quickly and allocate resourcesmore effectively.In future we would like to semantically clus-ter the topic specific information so that redundanttopics are eliminated from the list.
We can use Au-tomatic Taxonomy Generation(ATG) algorithmsfor document summarization (Kummamuru et al,2004) to build topic taxonomies.
We would alsolike to link our model to technical manuals, cata-logs, etc.
already available on the different topicsin the given domain.Acknowledgements: We thank our colleaguesRaghuram Krishnapuram and Sreeram Balakrish-nan for helpful discussions.
We also thank OlivierSiohan from the IBM T. J. Watson Research Cen-ter for providing us with call transcriptions.ReferencesF.
Bechet, G. Riccardi and D. Hakkani-Tur 2004.
Min-ing Spoken Dialogue Corpora for System Evaluationand Modeling.
Conference on Empirical Methodsin Natural Language Processing (EMNLP).
July,Barcelona, Spain.S.
Douglas, D. Agarwal, T. Alonso, R. M. Bell, M.Gilbert, D. F. Swayne and C. Volinsky.
2005.
Min-ing Customer Care Dialogs for ?Daily News?.
IEEETrans.
on Speech and Audio Processing, 13(5):652?660.P.
Haffner, G. Tur and J. H. Wright 2003.
Optimiz-ing SVMs for Complex Call Classification.
IEEEInternational Conference on Acoustics, Speech, andSignal Processing.
April 6-10, Hong Kong.X.
Jiang and A.-H. Tan.
2005.
Mining Ontolog-ical Knowledge from Domain-Specific Text Doc-uments.
IEEE International Conference on DataMining, November 26-30, New Orleans, Louisiana,USA.K.
Kummamuru, R. Lotlikar, S. Roy, K. Singal and R.Krishnapuram.
2004.
A hierarchical monotheticdocument clustering algorithm for summarizationand browsing search results.
International Confer-ence on World Wide Web.
New York, NY, USA.H.-K J. Kuo and C.-H. Lee.
2003.
DiscriminativeTraining of Natural Language Call Routers.
IEEETrans.
on Speech and Audio Processing, 11(1):24?35.A.
D. Lawson, D. M. Harris, J. J. Grieco.
2003.
Ef-fect of Foreign Accent on Speech Recognition inthe NATO N-4 Corpus.
Eurospeech.
September 1-4, Geneva, Switzerland.G.
Mishne, D. Carmel, R. Hoory, A. Roytman and A.Soffer.
2005.
Automatic Analysis of Call-centerConversations.
Conference on Information andKnowledge Management.
October 31-November 5,Bremen, Germany.M.
Padmanabhan, G. Saon, J. Huang, B. Kingsburyand L. Mangu.. 2002.
Automatic Speech Recog-nition Performance on a Voicemail TranscriptionTask.
IEEE Trans.
on Speech and Audio Process-ing, 10(7):433?442.M.
Tang, B. Pellom and K. Hacioglu.
2003.
Call-type Classification and Unsupervised Training forthe Call Center Domain.
Automatic Speech Recog-nition and Understanding Workshop.
November 30-December 4, St. Thomas, U S Virgin Islands.J.
Wright, A. Gorin and G. Riccardi.
1997.
Auto-matic Acquisition of Salient Grammar Fragmentsfor Call-type Classification.
Eurospeech.
Septem-ber, Rhodes, Greece.744
