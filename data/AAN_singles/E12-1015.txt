Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 141?151,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCharacter-Based Pivot Translation for Under-Resourced Languages andDomainsJo?rg TiedemannDepartment of Linguistics and PhilologyUppsala University, Uppsala/Swedenjorg.tiedemann@lingfil.uu.seAbstractIn this paper we investigate the use ofcharacter-level translation models to sup-port the translation from and to under-resourced languages and textual domainsvia closely related pivot languages.
Our ex-periments show that these low-level modelscan be successful even with tiny amountsof training data.
We test the approach onmovie subtitles for three language pairs andlegal texts for another language pair in a do-main adaptation task.
Our pivot translationsoutperform the baselines by a large margin.1 IntroductionData-driven approaches have been extremely suc-cessful in most areas of natural language pro-cessing (NLP) and can be considered the mainparadigm in application-oriented research and de-velopment.
Research in machine translation is atypical example with the dominance of statisti-cal models over the last decade.
This is even en-forced due to the availability of toolboxes such asMoses (Koehn et al 2007) which make it pos-sible to build translation engines within days oreven hours for any language pair provided that ap-propriate training data is available.
However, thisreliance on training data is also the most severelimitation of statistical approaches.
Resources inlarge quantities are only available for a few lan-guages and domains.
In the case of SMT, thedilemma is even more apparent as parallel cor-pora are rare and usually quite sparse.
Some lan-guages can be considered lucky, for example, be-cause of political situations that lead to the pro-duction of freely available translated material ona large scale.
A lot of research and developmentwould not have been possible without the Euro-pean Union and its language policies to give anexample.One of the main challenges of current NLP re-search is to port data-driven techniques to under-resourced languages, which refers to the major-ity of the world?s languages.
One obvious ap-proach is to create appropriate data resources evenfor those languages in order to enable the use ofsimilar techniques designed for high-density lan-guages.
However, this is usually too expensiveand often impossible with the quantities needed.Another idea is to develop new models that canwork with (much) less data but still make useof resources and techniques developed for otherwell-resourced languages.In this paper, we explore pivot translation tech-niques for the translation from and to resource-poor languages with the help of intermediateresource-rich languages.
We explore the factthat many poorly resourced languages are closelyrelated to well equipped languages, which en-ables low-level techniques such as character-based translation.
We can show that these tech-niques can boost the performance enormously,tested for several language pairs.
Furthermore, weshow that pivoting can also be used to overcomedata sparseness in specific domains.
Even highdensity languages are under-resourced in mosttextual domains and pivoting via in-domain dataof another language can help to adapt statisticalmodels.
In our experiments, we observe that re-lated languages have the largest impact in such asetup.The remaining parts of the paper are organizedas follows: First we describe the pivot translationapproach used in this study.
Thereafter, we dis-141cuss character-based translation models followedby a detailed presentation of our experimentalresults.
Finally, we briefly summarize relatedwork and conclude the paper with discussions andprospects for future work.2 Pivot ModelsInformation from pivot languages can be incorpo-rated in SMT models in various ways.
The mainprinciple refers to the combination of source-to-pivot and pivot-to-target translation models.In our setup, one of these models includes aresource-poor language (source or target) and theother one refers to a standard model with ap-propriate data resources.
A condition is that wehave at least some training data for the translationbetween pivot and the resource-poor language.However, for the original task (source-to-targettranslation) we do not require any data resourcesexcept for purposes of comparison.We will explore various models for the transla-tion between the resource-poor language and thepivot language and most of them are not compat-ible with standard phrase-based translation mod-els.
Hence, triangulation methods (Cohn and La-pata, 2007) for combining phrase tables are notapplicable in our case.
Instead, we explore acascaded approach (also called ?transfer method?
(Wu and Wang, 2009)) in which we translate theinput text in two steps using a linear interpo-lation for rescoring N-best lists.
Following themethod described in (Utiyama and Isahara, 2007)and (Wu and Wang, 2009), we use the best n hy-potheses from the translation of source sentencess to pivot sentences p and combine them with thetop m hypotheses for translating these pivot sen-tences to target sentences t:t?
?
argmaxtL?k=1?
?spkhspk(s, p) + (1?
?
)?ptkhptk(p, t)where hxyk are feature functions for model xywith appropriate weights ?xyk .1 Basically, thismeans that we simply add the scores and, sim-ilar to related work, we assume that the featureweights can be set independently for each modelusing minimum error rate training (MERT) (Och,1Note, that we do not require the same feature functionsin both models even though the formula above implies thisfor simplicity of representation.2003).
In our setup we added the parameter ?that can be used to weight the importance of onemodel over the other.
This can be useful as wedo not consider the entire hypothesis space butonly a small subset of N-best lists.
In the sim-plest case, this weight is set to 0.5 making bothmodels equally important.
An alternative to fit-ting the interpolation weight would be to per-form a global optimization procedure.
However,a straightforward implementation of pivot-basedMERT would be prohibitively slow due to theexpensive two-step translation procedure over n-best lists.A general condition for the pivot approach is toassume independent training sets for both transla-tion models as already pointed out by (Bertoldiet al 2008).
In contrast to research presentedin related work (see, for example, (Koehn et al2009)) this condition is met in our setup in whichall data sets represent different samples over thelanguages considered (see section 4).23 Character-Based SMTThe basic idea behind character-based translationmodels is to take advantage of the strong lexi-cal and syntactic similarities between closely re-lated languages.
Consider, for example, Figure1.
Related languages like Catalan and Spanish orDanish and Norwegian have common roots and,therefore, use similar concepts and express themin similar grammatical structures.
Spelling con-ventions can still be quite different but those dif-ferences are often very consistent.
The Bosnian-Macedonian example also shows that we do nothave to require any alphabetic overlap in order toobtain character-level similarities.Regularities between such closely related lan-guages can be captured below the word level.
Wecan also assume a more or less monotonic rela-tion between the two languages which motivatesthe idea of translation models over character N-grams treating translation as a transliteration task(Vilar et al 2007).
Conceptually it is straightfor-ward to think of phrase-based models on the char-acter level.
Sequences of characters can be usedinstead of word N-grams for both, translation andlanguage models.
Training can proceed with thesame tools and approaches.
The basic task is to2Note that different samples may still include commonsentences.142Figure 1: Some examples of movie subtitle transla-tions between closely related languages (either sharingparts of the same alphabet or not).prepare the data to comply with the training pro-cedures (see Figure 2).Figure 2: Data pre-processing for training models onthe character level.
Spaces are represented by ?
?
andeach sentence is treated as one sequence of characters.3.1 Character AlignmentOne crucial difference is the alignment of charac-ters, which is required instead of an alignment ofwords.
Clearly, the traditional IBM word align-ment models are not designed for this task es-pecially with respect to distortion.
However, thesame generative story can still be applied in gen-eral.
Vilar et al(2007) explore a two-step proce-dure where words are aligned first (with the tradi-tional IBM models) to divide sentence pairs intoaligned segments of reasonable size and the char-acters are then aligned with the same algorithm.An alternative is to use models designed fortransliteration or related character-level transfor-mation tasks.
Many approaches are based ontransducer models that resemble string edit oper-ations such as insertions, deletions and substitu-tions (Ristad and Yianilos, 1998).
Weighted fi-nite state transducers (WFST?s) can be trained onunaligned pairs of character sequences and havebeen shown to be very effective for transliterationtasks or letter-to-phoneme conversions (Jiampoja-marn et al 2007).
The training procedure usuallyemploys an expectation maximization (EM) pro-cedure and the resulting transducer can be used tofind the Viterbi alignment between characters ac-cording to the best sequence of edit operations ap-plied to transform one string into the other.
Exten-sions to this model are possible, for example theuse of many-to-many alignments which have beenshown to be very effective in letter-to-phonemealignment tasks (Jiampojamarn et al 2007).One advantage of the edit-distance-based trans-ducer models is that the alignments they pre-dict are strictly monotonic and cannot easily beconfused by spurious relations between charac-ters over longer distances.
Long distance align-ments are only possible in connection with a se-ries of insertions and deletions that usually in-crease the alignment costs in such a way that theyare avoided if possible.
On the other hand, IBMword alignment models also prefer monotonicalignments over non-monotonic ones if there is nogood reason to do otherwise (i.e., there is frequentevidence of distorted alignments).
However, thesize of the vocabulary in a character-level modelis very small (several orders of magnitude smallerthan on the word level) and this may cause seriousconfusion of the word alignment model that verymuch relies on context-independent lexical trans-lation probabilities.
Hence, for character align-ment, the lexical evidence is much less reliablewithout their context.It is certainly possible to find a compromise be-tween word-level and character-level models inorder to generalize below word boundaries butavoiding alignment problems as discussed above.Morpheme-based translation models have beenexplored in several studies with similar motiva-tions as in our approach, a better generalizationfrom sparse training data (Fishel and Kirik, 2010;Luong et al 2010).
However, these approacheshave the drawback that they require proper mor-phological analyses.
Data-driven techniques ex-ist even for morphology, but their use in SMTstill needs to be shown (Fishel, 2009).
The sit-uation is comparable to the problems of integrat-ing linguistically motivated phrases into phrase-based SMT (Koehn et al 2003).
Instead we optfor a more general approach to extend context tofacilitate, especially, the alignment step.
Figure 3shows how we can transform texts into sequencesof bigrams that can be aligned with standard ap-proaches without making any assumptions aboutlinguistically motivated segmentations.143cu ur rs so o c co on nf fi ir rm ma ad do o .
.?
q qu ue?
e?
e es s e es so o ?
?Figure 3: Two Spanish sentences as sequences of char-acter bigrams with a final ?
?
marking the end of a sen-tence.In this way we can construct a parallel corpus withslightly richer contextual information as input tothe alignment program.
The vocabulary remainssmall (for example, 1267 bigrams in the case ofSpanish compared to 84 individual characters inour experiments) but lexical translation probabili-ties become now much more differentiated.With this, it is now possible to use the align-ment between bigrams to train a character-leveltranslation system as we have the same number ofbigrams as we have characters (and the first char-acter in each bigram corresponds to the charac-ter at that position).
Certainly, it is also possibleto train a bigram translation model (and languagemodel).
This has the (one and only) advantagethat one character of context across phrase bound-aries (i.e.
character N-grams) is used in the se-lection of translation alternatives from the phrasetable.33.2 Tuning Character-Level ModelsA final remark on training character-based SMTmodels is concerned with feature weight tun-ing.
It certainly makes not much sense to com-pute character-level BLEU scores for tuning fea-ture weights especially with the standard settingsof matching relatively short N-grams.
Insteadwe would still like to measure performance interms of word-level BLEU scores (or any otherMT evaluation metric used in minimum errorrate training).
Therefore, it is important to post-process character-translated development sets be-fore adjusting weights.
This is simply doneby merging characters accordingly and replacingthe place-holders with spaces again.
Thereafter,MERT can run as usual.3.3 EvaluationCharacter-level translations can be evaluated inthe same way as other translation hypotheses,for example using automatic measures such as3Using larger units (trigrams, for example) led to lowerscores in our experiments (probably due to data sparseness)and, therefore, are not reported here.BLEU, NIST, METEOR etc.
The same simplepost-processing as mentioned in the previous sec-tion can be applied to turn the character transla-tions into ?normal?
text.
However, it can be use-ful to look at some other measures as well thatconsider near matches on the character level in-stead of matching words and word N-grams only.Character-level models have the ability to producestrings that may be close to the reference and stilldo not match any of the words contained.
Theymay generate non-words that include mistakeswhich look like spelling-errors or minor gram-matical mistakes.
Those words are usually closeenough to the correct target words to be recog-nized by the user, which is often more acceptablethan leaving foreign words untranslated.
This isespecially true as many unknown words representimportant content words that bear a lot of infor-mation.
The problem of unknown words is evenmore severe for morphologically rich language asmany word forms are simply not part of (sparse)training data sets.
Untranslated words are espe-cially annoying when translating languages thatuse different writing systems.
Consider, for ex-ample, the following subtitles in Macedonian (us-ing Cyrillic letters) that have been translated fromBosnian (written in Latin characters):reference: ?
????
???
?, ????
?
??????
?.word-based: ?
c?as?u vina, ????
??????
?.char-based: ?
????
???
?, ????
??????
?.reference: ??
???????
?????????
?.word-based: ??
starom svetili?stu.char-based: ??
????
???????????
?.The underlined parts mark examples of character-level differences with respect to the referencetranslation.
For the pivot translation approach, itis important that the translations generated in thefirst step can be handled by the second one.
Thismeans, that words generated by a character-basedmodel should at least be valid input words for thesecond step, even though they might refer to er-roneous inflections in that context.
Therefore, weadd another measure to our experimental resultspresented below ?
the number of unknown wordswith respect to the input language of the secondstep.
This applies only to models that are usedas the first step in pivot-based translations.
Forother models, we include a string similarity mea-sure based on the longest common subsequenceratio (LCSR) (Stephen, 1992) in order to give animpression about the ?closeness?
of the system144output to the reference translations.4 ExperimentsWe conducted a series of experiments to testthe ideas of (character-level) pivot translation forresource-poor languages.
We chose to use datafrom a collection of translated subtitles com-piled in the freely available OPUS corpus (Tiede-mann, 2009b).
This collection includes a largevariety of languages and contains mainly shortsentences and sentence fragments, which suitscharacter-level alignment very well.
The selectedsettings represent translation tasks between lan-guages (and domains) for which only very limitedtraining data is available or none at all.Below we present results from two generaltasks:4 (i) Translating between English and aresource-poor language (in both directions) viaa pivot language that is close related to theresource-poor language.
(ii) Translating betweentwo languages in a domain for which no in-domain training data is available via a pivot lan-guage with in-domain data.
We will start withthe presentation of the first task and the character-based translation between closely related lan-guages.4.1 Task 1: Pivoting via Related LanguagesWe decided to look at resource-poor languagesfrom two language families: Macedonian repre-senting a Slavic language from the Balkan re-gion, Catalan and Galician representing two Ro-mance languages spoken mainly in Spain.
Thereis only little or no data available for translatingfrom or to English for these languages.
However,there are related languages with medium or largeamounts of training data.
For Macedonian, weuse Bulgarian (which also uses a Cyrillic alpha-bet) and Bosnian (another related language thatmainly uses Latin characters) as the pivot lan-guage.
For Catalan and Galician, the obviouschoice was Spanish (however, Portuguese would,for example, have been another reasonable op-tion for Galician).
Table 1 lists the data avail-able for training the various models.
Furthermore,we reserved 2000 sentences for tuning parameters4In all experiments we use standard tools like Moses,Giza++, SRILM, mteval etc.
Details about basic settings areomitted here due to space constraints but can be found inthe supplementary material.
The data sets are available fromhere: http://stp.lingfil.uu.se/?joerg/index.php?resourcesand another 2000 sentences for testing.
For Gali-cian, we only used 1000 sentences for each setdue to the lack of additional data.
We were espe-cially careful when preparing the data to excludeall sentences from tuning and test sets that couldbe found in any pivot or direct translation model.Hence, all test sentences are unseen strings for allmodels presented in this paper (but they are notcomparable with each other as they are sampledindividually from independent data sets).language pair #sent?s #wordsGalician ?
English ?
?Galician ?
Spanish 2k 15kCatalan ?
English 50k 400kCatalan ?
Spanish 64k 500kSpanish ?
English 30M 180MMacedonian ?
English 220k 1.2MMacedonian ?
Bosnian 12k 60kMacedonian ?
Bulgarian 155k 800kBosnian ?
English 2.1M 11MBulgarian ?
English 14M 80MTable 1: Training data for the translation task betweenclosely related languages in the domain of movie sub-titles.
Number of sentences (#sent?s) and number ofwords (#words) in thousands (k) and millions (M) (av-erages of source and target language).The data sets represent several interesting testcases: Galician is the least supported languagewith extremely little training data for building ourpivot model.
There is no data for the direct modeland, therefore, no explicit baseline for this task.There is 30 times more data available for Catalan-English, but still too little for a decent standardSMT model.
Interesting here is that we have moreor less the same amount of data available for thebaseline and for the pivot translation between therelated languages.
The data set for Macedonian?
English is by far the largest among the baselinemodels and also bigger than the sets available forthe related pivot languages.
Especially Macedo-nian ?
Bosnian is not well supported.
The inter-esting questions is whether tiny amounts of pivotdata can still be competitive.
In all three cases,there is much more data available for the trans-lation models between English and the pivot lan-guage.In the following section we will look at thetranslation between related languages with vari-ous models and training setups before we con-sider the actual translation task via the bridge lan-guages.145bs-mk bg-mk es-gl es-caModel BLEU % ?LCSR BLEU % ?LCSR BLEU % ?LCSR BLEU % ?LCSRword-based 15.43 0.5067 14.66 0.6225 41.11 0.7966 62.73 0.8526char ?
WFST1:1 21.37++ 0.6903 13.33??
0.6159 36.94 0.7832 73.17++ 0.8728char ?
WFST2:2 19.17++ 0.6737 12.67??
0.6190 43.39++ 0.8083 70.64++ 0.8684char ?
IBMchar 23.17++ 0.6968 14.57 0.6347 45.21++ 0.8171 73.12++ 0.8767char ?
IBMbigram 24.84++ 0.7046 15.01++ 0.6374 44.06++ 0.8144 74.21++ 0.8803Table 2: Translating from a related pivot language to the target language.
Bosnian (bs) / Bulgarian (bg) ?Macedonian (mk); Galician (gl) / Catalan (ca) ?
Spanish (es).
Word-based refers to standard phrase-based SMTmodels.
All other models use phrases over character sequences.
The WFSTx:y models use weighted finite statetransducers for character alignment with units that are at most x and y characters long, respectively.
Othermodels use Viterbi alignments created by IBM model 4 using GIZA++ (Och and Ney, 2003) between characters(IBMchar) or bigrams (IBMbigram).
LCSR refers to the averaged longest common subsequence ratio betweensystem translations and references.
Results are significantly better (p < 0.01++, p < 0.05+) or worse (p <0.01?
?, p < 0.05?)
than the word-based baseline.mk-bs mk-bg gl-es ca-esModel BLEU % ?UNK BLEU % ?UNK BLEU % ?UNK BLEU % ?UNKword-based 14.22 17.83% 14.77 5.29% 43.22 10.18% 59.34 3.80%char ?
WFST1:1 21.74++ 1.50% 16.04++ 0.77% 50.24++ 1.17% 62.87++ 0.45%char ?
WFST2:2 19.19++ 2.05% 15.32 0.96% 50.59++ 1.28% 59.84 0.47%char ?
IBMchar 24.15++ 1.30% 17.12++ 0.80% 51.18++ 1.38% 64.35 ++ 0.59%char ?
IBMbigram 24.82++ 1.00% 17.28++ 0.77% 50.70++ 1.36% 65.14++ 0.48%Table 3: Translating from the source language to a related pivot language.
UNK gives the proportion of unknownwords with respect to the translation model from the pivot language to English.4.1.1 Translating Related LanguagesThe main challenge for the translation mod-els between related languages is the restriction tovery limited parallel training data.
Character-levelmodels make it possible to generalize to very ba-sic translation units leading to robust models inthe sense of models without unknown events.
Thebasic question is whether they provide reasonabletranslations with respect to given accepted refer-ences.
Tables 2 and 3 give a comprehensive sum-mary of various models for the languages selectedin our experiments.We can see that at least one character-basedtranslation model outperforms the standard word-based model in all cases.
This is true (and not verysurprising) for the language pairs with very littletraining data but it is also the case for languagepairs with slightly more reasonable data sets likeBulgarian-Macedonian.
The automatic measuresindicate decent translation performances at thisstage which encourages their use in pivot trans-lation that we will discuss in the next section.Furthermore, we can also see the influence ofdifferent character alignment algorithms.
Some-what surprisingly, the best results are achievedwith IBM alignment models that are not designedfor this purpose.
Transducer-based alignmentsproduce consistently worse translation models (atleast in terms of BLEU scores).
The reason forthis might be that the IBM models can handlenoise in the training data more robustly.
How-ever, in terms of unknown words, WFST-basedalignment is very competitive and often the bestchoice (but not much different from the best IBMbased models).
The use of character bigramsleads to further BLEU improvements for all datasets except Galician-Spanish.
However, this dataset is extremely small, which may cause unpre-dictable results.
In any case, the differencesbetween character-based alignments and bigram-based ones are rather small and our experimentsdo not lead to conclusive results.4.1.2 Pivot TranslationIn this section we now look at cascaded transla-tions via the related pivot language.
Tables 4 and5 summarize the results for various settings.As we can see, the pivot translations for Cata-lan and Galician outperform the baselines by alarge margin.
Here, the baselines are, of course,very weak due to the minimal amount of train-ing data.
Furthermore, the Catalan-English testset appears to be very easy considering the rela-tively high BLEU scores achieved even with tiny146Model (BLEU in %) 1x1 10x10English ?
Catalan (baseline) 26.70English ?
(Spanish = Catalan) 8.38English ?
Spanish -word- Catalan 38.91++ 39.59++English ?
Spanish -char- Catalan 44.46++ 46.82++Catalan ?
English (baseline) 27.86(Catalan = Spanish) ?
English 9.52Catalan -word- Spanish ?
English 38.41++ 38.65++Catalan -char- Spanish ?
English 40.43++ 40.73++English ?
Galician (baseline) ?English ?
(Spanish = Galician) 7.46English ?
Spanish -word- Galician 20.55 20.76English ?
Spanish -char- Galician 21.12 21.09Galician ?
English (baseline) ?
(Galician = Spanish) ?
English 5.76Galician -word- Spanish ?
English 13.16 13.20Galician -char- Spanish ?
English 16.04 16.02Table 4: Translating between Galician/Catalan and En-glish via Spanish using a standard phrase-based SMTbaseline, Spanish?English SMT models to translatefrom/to Catalan/Galician and pivot-based approachesusing word-level models or character-level models(based on IBMbigramalignments) with either one-best(1x1) or N-best lists (10x10 with ?
= 0.85).amounts of training data for the baseline.
Still, notest sentence appears in any training or develop-ment set for either direct translation or pivot mod-els.
From the results, we can also see that Catalanand Galician are quite different from Spanish andrequire language-specific treatment.
Using a largeSpanish ?
English model (with over 30% BLEUin both directions) to translate from or to Cata-lan or Galician is not an option.
The experimentsshow that character-based pivot models lead tobetter translations than word-based pivot models(in terms of BLEU scores).
This reflects the per-formance gains presented in Table 2.
Rescoringof N-best lists, on the other hand, does not havea big impact on our results.
However, we did notspend time optimizing the parameters of N-bestsize and interpolation weight.The results from the Macedonian task are not asclear.
This is especially due to the different setupin which the baseline uses more training data thanany of the related language pivot models.
How-ever, we can still see that the pivot translation viaBulgarian clearly outperforms the baseline.
Forthe case of translating to Macedonian via Bulgar-ian, the word-based model seems to be more ro-bust than the character-level model.
This may bedue to a larger number of non-words generatedby the character-based pivot model.
In general,Model (BLEU in %) 1x1 10x10English ?
Maced.
(baseline) 11.04English ?
Bosn.
-word- Maced.
7.33??
7.64English ?
Bosn.
-char- Maced.
9.99 10.34English ?
Bulg.
-word- Maced.
12.49++ 12.62++English ?
Bulg.
-char- Maced.
11.57++ 11.59+Maced.
?
English (baseline) 20.24Maced.
-word- Bosn.
?
English 12.36??
12.48??Maced.
-char- Bosn.
?
English 18.73?
18.64??Maced.
-word- Bulg.
?
English 19.62 19.74Maced.
-char- Bulg.
?
English 21.05 21.10Table 5: Translating between Macedonian (Maced)and English via Bosnian (Bosn) / Bulgarian (Bulg).the BLEU scores are much lower for all modelsinvolved (even for the high-density languages),which indicates larger problems with the gener-ation of correct output and intermediate transla-tions.Interesting is the fact that we can achieve al-most the same performance as the baseline whentranslating via Bosnian even though we had muchless training data at our disposal for the translationbetween Macedonian and Bosnian.
In this setup,we can see that a character-based model was nec-essary in order to obtain the desired abstractionfrom the tiny amount of training data.4.2 Task 2: Pivoting for Domain AdaptationSparse resources are not only a problem for spe-cific languages but also for specific domains.SMT models are very sensitive to domain shiftsand domain-specific data is often rare.
In the fol-lowing, we investigate a test case of translatingbetween two languages (English and Norwegian)with reasonable amounts of data resources but inthe wrong domain (movie subtitles instead of le-gal texts).
Here again, we facilitate the transla-tion process by a pivot language, this time withdomain-specific data.The task is to translate legal texts from Norwe-gian (Bokma?l) to English and vice versa.
The testset is taken from the English?Norwegian ParallelCorpus (ENPC) (Johansson et al 1996) and con-tains 1493 parallel sentences (a selection of Eu-ropean treaties, directives and agreements).
Oth-erwise, there is no training data available in thisdomain for English and Norwegian.
Table 6 liststhe other data resources we used in our study.As we can see, there is decent amount of train-ing data for English ?
Norwegian, but the domainis strikingly different.
On the other hand, there147Language pair Domain #sent?s #wordsEnglish?Norwegian subtitles 2.4M 18MNorwegian?Danish subtitles 1.5M 10MDanish?English DGT-TM 430k 9MTable 6: Training data available for the domain adapta-tion task.
DGT-TM refers to the translation memoriesprovided by the JRC (Steinberger et al 2006)is in-domain data for other languages like Danishthat may act as an intermediate pivot.
Further-more, we have out-of-domain data for the transla-tion between pivot and Norwegian.
The sizes ofthe training data sets for the pivot models are com-parable (in terms of words).
The in-domain pivotdata is controlled and very consistent and, there-fore, high quality translations can be expected.The subtitle data is noisy and includes variousmovie genres.
It is important to mention that thepivot data still does not contain any sentence in-cluded in the English?Norwegian test set.Table 7 summarizes the results of our experi-ments when using Danish and in-domain data asa pivot in translations from and to Norwegian.Model (task: English ?
Norwegian) BLEU(step 1) English ?dgt?
Danish 52.76(step 2) Danish ?subswo?
Norwegian 29.87(step 2) Danish ?subsch?
Norwegian 29.65(step 2) Danish ?subsbi?
Norwegian 25.65English ?subs?
Norwegian (baseline) 7.20English ?dgt?
(Danish = Norwegian) 9.44++English ?dgt?
Danish -subswo- Norwegian 17.49++English ?dgt?
Danish -subsch- Norwegian 17.61++English ?dgt?
Danish -subsbi- Norwegian 14.07++Model (task: Norwegian ?
English) BLEU(step 1) Norwegian ?subswo?
Danish 30.15(step 1) Norwegian ?subsch?
Danish 27.81(step 1) Norwegian ?subsbi?
Danish 28.52(step 2) Danish ?dgt?
English 57.23Norwegian ?subs?
English (baseline) 11.41(Norwegian = Danish) ?dgt?
English 13.21++Norwegian ?subs+dgtLM?
English 13.33++Norwegian ?subswo?
Danish ?dgt?
English 25.75++(Norwegian ?subsch?
Danish ?dgt?
English 23.77++Norwegian ?subsbi?
Danish ?dgt?
English 26.29++Table 7: Translating out-of-domain data via Dan-ish.
Models using in-domain data are marked withdgt and out-of-domain models are marked with subs.subs+dgtLM refers to a model with an out-of-domaintranslation model and an added in-domain languagemodel.
The subscripts wo, ch and bi refer to word,character and bigram models, respectively.The influence of in-domain data in the transla-tion process is enormous.
As expected, the out-of-domain baseline does not perform well eventhough it uses the largest amount of training datain our setup.
It is even outperformed by the in-domain pivot model when pretending that Norwe-gian is in fact Danish.
For the translation into En-glish, the in-domain language model helps a lit-tle bit (similar resources are not available for theother direction).
However, having the strong in-domain model for translating to (and from) thepivot language improves the scores dramatically.The out-of-domain model in the other part of thecascaded translation does not destroy this advan-tage completely and the overall score is muchhigher than any other baseline.In our setup, we used again a closely relatedlanguage as a pivot.
However, this time wehad more data available for training the pivottranslation model.
Naturally, the advantages ofthe character-level approach diminishes and theword-level model becomes a better alternative.However, there can still be a good reason for theuse of a character-based model as we can see inthe success of the bigram model (?subsbi?)
in thetranslation from Norwegian to English (via Dan-ish).
A character-based model may generalize be-yond domain-specific terminology which leads toa reduction of unknown words when applied toa new domain.
Note that using a character-basedmodel in step two could possibly cause more harmthan using it in step one of the pivot-based pro-cedure.
Using n-best lists for a subsequent word-based translation in step two may fix errors causedby character-based translation simply by ignoringhypotheses containing them, which makes such amodel more robust to noisy input.Finally, as an alternative, we can also look atother pivot languages.
The domain adaptationtask is not at all restricted to closely related pivotlanguages especially considering the success ofword-based models in the experiments above.
Ta-ble 8 lists results for three other pivot languages.Surprisingly, the results are much worse thanfor the Danish test case.
Apparently, these mod-els are strongly influenced by the out-of-domaintranslation between Norwegian and the pivot lan-guage.
The only success can be seen with an-other closely related language, Swedish.
Lexicaland syntactic similarity seems to be important tocreate models that are robust enough for domainshifts in the cascaded translation setup.148Pivot=xx en?xx xx?no en?xx?noGerman 53.09 23.60 3.15?
?French 66.47 17.84 5.03?
?Swedish 52.62 24.79 10.07++Pivot=xx no?xx xx?en no?xx?enGerman 15.02 53.02 5.52?
?French 17.69 65.85 8.78?
?Swedish 19.72 59.55 16.35++Table 8: Alternative word-based pivot translations be-tween Norwegian (no) and English (en).5 Related WorkThere is a wide range of pivot language ap-proaches to machine translation and a numberof strategies have been proposed.
One of themis often called triangulation and usually refersto the combination of phrase tables (Cohn andLapata, 2007).
Phrase translation probabilitiesare merged and lexical weights are estimated bybridging word alignment models (Wu and Wang,2007; Bertoldi et al 2008).
Cascaded translationvia pivot languages are discussed by (Utiyamaand Isahara, 2007) and are frequently used by var-ious researchers (de Gispert and Marin?o, 2006;Koehn et al 2009; Wu and Wang, 2009) andcommercial systems such as Google Translate.A third strategy is to generate or augment datasets with the help of pivot models.
This is, forexample, explored by (de Gispert and Marin?o,2006) and (Wu and Wang, 2009) (who call it thesynthetic method).
Pivoting has also been usedfor paraphrasing and lexical adaptation (Bannardand Callison-Burch, 2005; Crego et al 2010).
(Nakov and Ng, 2009) investigate pivot languagesfor resource-poor languages (but only when trans-lating from the resource-poor language).
Theyalso use transliteration for adapting models to anew (related) language.
Character-level SMT hasbeen used for transliteration (Matthews, 2007;Tiedemann and Nabende, 2009) and also for thetranslation between closely related languages (Vi-lar et al 2007; Tiedemann, 2009a).6 Conclusions and DiscussionIn this paper, we have discussed possibilities totranslate via pivot languages on the characterlevel.
These models are useful to support under-resourced languages and explore strong lexicaland syntactic similarities between closely relatedlanguages.
Such an approach makes it possibleto train reasonable translation models even withextremely sparse data sets.
Moreover, charac-ter level models introduce an abstraction that re-duce the number of unknown words dramatically.In most cases, these unknown words representinformation-rich units that bear large portions ofthe meaning to be translated.
The following illus-trates this effect on example translations with andwithout pivot model:Example: Catalan  English (via Spanish)Referene: I have to grade these papers.Baseline: Tinque qualiar these ex amens.Pivotword: Tinque qualiar these tests.Pivotchar: I have to grade these papers.Example: Maedonian  English (via Bulgarian)Referene: It's a simple matter of self-preservation.Baseline: It's simply a question of ?????????????
?.Pivotword: That's a matter of ?????????????
?.Pivotchar: It's just a question of yourself.Leaving unseen words untranslated is not only an-noying (especially if the input language uses adifferent writing system) but often makes transla-tions completely incomprehensible.
Pivot trans-lations will still not be perfect (see exampletwo above), but can at least be more intelli-gible.
Character-based models can even takecare of tokenization errors as the one shownabove (?Tincque?
should be two words ?Tincque?).
Fortunately, the generation of non-wordsequences (observed as unknown words) does notseem to be a big problem and no special treatmentis required to avoid such output.
We would stilllike to address this issue in future work by addinga word level LM in character-based SMT.
How-ever, (Vilar et al 2007) already showed that thisdid not have any positive effect in their character-based system.
In a second study, we also showedthat pivot models can be useful for adapting toa new domain.
The use of in-domain pivot dataleads to systems that outperform out-of-domaintranslation models by a large margin.
Our find-ings point to many prospects for future work.For example, we would like to investigate combi-nations of character-based and word-based mod-els.
Character-based models may also be used fortreating unknown words only.
Multiple source ap-proaches via several pivots is another possibilityto be explored.
Finally, we also need to furtherinvestigate the robustness of the approach with re-spect to other language pairs, data sets and learn-ing parameters.149ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages597?604, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-erico, and Roldano Cattoni.
2008.
Phrase-BasedStatistical Machine Translation with Pivot Lan-guages.
In Proceedings of the International Work-shop on Spoken Language Translation, pages 143?149, Hawaii, USA.Trevor Cohn and Mirella Lapata.
2007.
Machinetranslation by triangulation: Making effective useof multi-parallel corpora.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 728?735, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Josep Maria Crego, Aure?lien Max, and Franc?ois Yvon.2010.
Local lexical adaptation in machine transla-tion through triangulation: SMT helping SMT.
InProceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages232?240, Beijing, China, August.
Coling 2010 Or-ganizing Committee.A.
de Gispert and J.B. Marin?o.
2006.
Catalan-englishstatistical machine translation without parallel cor-pus: Bridging through spanish.
In Proceedings ofthe 5th Workshop on Strategies for developing Ma-chine Translation for Minority Languages (SALT-MIL?06) at LREC, pages 65?68, Genova, Italy.Mark Fishel and Harri Kirik.
2010.
Linguisticallymotivated unsupervised segmentation for machinetranslation.
In Proceedings of the InternationalConference on Language Resources and Evaluation(LREC), pages 1741?1745, Valletta, Malta.Mark Fishel.
2009.
Deeper than words: Morph-basedalignment for statistical machine translation.
InProceedings of the Conference of the Pacific Associ-ation for Computational Linguistics PacLing 2009,Sapporo, Japan.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phonemeconversion.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguis-tics; Proceedings of the Main Conference, pages372?379, Rochester, New York, April.
Associationfor Computational Linguistics.Stig Johansson, Jarle Ebeling, and Knut Hofland.1996.
Coding and aligning the English-NorwegianParallel Corpus.
In K. Aijmer, B. Altenberg,and M. Johansson, editors, Languages in Contrast,pages 87?112.
Lund University Press.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions, pages 177?180, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Philipp Koehn, Alexandra Birch, and Ralf Steinberger.2009.
462 machine translation systems for europe.In Proceedings of MT Summit XII, pages 65?72, Ot-tawa, Canada.Minh-Thang Luong, Preslav Nakov, and Min-YenKan.
2010.
A hybrid morpheme-word represen-tation for machine translation of morphologicallyrich languages.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 148?157, Cambridge, MA, Octo-ber.
Association for Computational Linguistics.David Matthews.
2007.
Machine transliteration ofproper names.
Master?s thesis, School of Informat-ics, University of Edinburgh.Preslav Nakov and Hwee Tou Ng.
2009.
Im-proved statistical machine translation for resource-poor languages using related resource-rich lan-guages.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 1358?1367, Singapore, August.
Associ-ation for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Eric Sven Ristad and Peter N. Yianilos.
1998.Learning string edit distance.
IEEE Transactionson Pattern Recognition and Machine Intelligence,20(5):522?532, May.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz?
Erjavec, and Dan Tufis?.2006.
The JRC-Acquis: A multilingual aligned par-allel corpus with 20+ languages.
In Proceedings of150the 5th International Conference on Language Re-sources and Evaluation (LREC), pages 2142?2147.Graham A. Stephen.
1992.
String Search.
Technicalreport, School of Electronic Engineering Science,University College of North Wales, Gwynedd.Jo?rg Tiedemann and Peter Nabende.
2009.
Translat-ing transliterations.
International Journal of Com-puting and ICT Research, 3(1):33?41.Jo?rg Tiedemann.
2009a.
Character-based PSMT forclosely related languages.
In Proceedings of 13thAnnual Conference of the European Association forMachine Translation (EAMT?09), pages 12 ?
19,Barcelona, Spain.Jo?rg Tiedemann.
2009b.
News from OPUS - A col-lection of multilingual parallel corpora with toolsand interfaces.
In Recent Advances in Natural Lan-guage Processing, volume V, pages 237?248.
JohnBenjamins, Amsterdam/Philadelphia.Masao Utiyama and Hitoshi Isahara.
2007.
A com-parison of pivot methods for phrase-based statisti-cal machine translation.
In Human Language Tech-nologies 2007: The Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics; Proceedings of the Main Conference,pages 484?491, Rochester, New York, April.
Asso-ciation for Computational Linguistics.David Vilar, Jan-Thorsten Peter, and Hermann Ney.2007.
Can we translate letters?
In Proceedings ofthe Second Workshop on Statistical Machine Trans-lation, pages 33?39, Prague, Czech Republic, June.Association for Computational Linguistics.Hua Wu and Haifeng Wang.
2007.
Pivot language ap-proach for phrase-based statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages856?863, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Hua Wu and Haifeng Wang.
2009.
Revisiting pivotlanguage approach for machine translation.
In Pro-ceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 154?162, Suntec, Singapore,August.
Association for Computational Linguistics.151
