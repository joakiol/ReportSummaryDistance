Incremental Sentence Production with a Parallel Marker-Passing AlgorithmHiroaki  K i tanoCenter  for  Mach ine  Translat ionCarnegie  Mel lon  Univers i tyPittsburgh, PA 15213, U.S.A.h i roak i@cs .cmu.eduABSTRACT ~DMDIALOG \[Kitano, 1989a\], developed at the Centerfor Machine Translation at Carnegie Mellon University.This paper describes a method of incremen-tal natural anguage generation using a paral-lel marker-passing algorithm for modeling si-multaneous interpretation.
Semantic and syn-tactic knowledge are represented in a memorynetwork in which several types of markers arepassed around in order to make inference, andexplore implicit parallelism of sentence produc-tion.
The model is consistent with several psy-cholinguistic studies.
The model is actuallyimplemented asa part of the ~DMDIALOG real-time speech-to-speech dialog translation sys-tem developed atthe Center for Machine Trans-lation at Carnegie Mellon University, and pub-licly demonstrated since March 1989.1 IntroductionIncremented sentence production has been gaining moreattention in recent years.
It is particulary important inapplication areas such as speech-to-speech translationwhere real-.time transaction is essential.
The ~DMDIALOGproject is a research project o develop aspeech-to-speechdialog translation system with simultaneous interpreta-tion capability.
At the outset of the project, we haveinvestigated actual simultaneous interpretation sessionsand telephone dialogs.
As a result, we found that oneutterance in a real dialog can be quite long (15 secondsfor one sentence is not rare in Japanese).
This impliesthat if we adopt a sequential rchitecture, in which gen-eration starts only after the entire parsing is completed,this inevitably creates unendurable delay in translation.Suppose one speaker made an utterance of 15 seconds,and the other esponded with an utterance of 15 secondsin length, the first speaker must wait at least 30 seconds tostart hearing the translation of the utterance of his/her di-alog partner.
It is inconceivable that such a system couldbe practially deployed.
Introduction of a simultaneousinterpretation scheme by coupling an incremental gener-ation and all incremental parsing technologies is the onlyway to minimize this problem 1.Incremental sentence production is interesting from thestandpoint of psycholinguistics a  well.
There are manypsycholinguistic studies which support incremental sen-tence production as a psycholinguistically plausible ap-proach.
We will discuss the psycholinguistic relevancyof our model ater.In this paper, we describe a model of incrementalsentence l:Woduction which is actually implemented asa part of the speech-to-speech dialog translation system1Although there is a problem of how to resolve ambiguitiesin parsing, discussion on such topic is beyond the scope of thispaper.
For those who are interested inthis topic, refer to \[Kitanoet.
al., 1989a\]\[Kitano et.
al., 1989b\].2 Basic Organization of the ModelWe use a hybrid parallel paradigm \[Kitano, 1989b\],which is an integration of a parallel marker-passingscheme and a connectionist network, as a basic algo-rithm.
Five types of markers (two types for parsing, twoother types for generation, and an another type for con-textual priming) are passed around the memory networkwhich represents knowledge from morphophonetic-levelto discourse-level.
A connectionist network performssub-symbolic computations with a massive parallelism.Use of the hybrid parallel scheme on the memory net-work has its merit in exploring implicit parallelism in theprocess of natural language generation and parsing.2.1 The Memory NetworkThe memory network incorporates knowledge from mor-phophonetics toplan hierarchies of each participant of adialog.
Each node is a type and represents either a concept(Concept Class node; CC) or a sequence of concepts (Con-cept Sequence Class node; CSC).
Strictly speaking, bothCC and CSC are a collection or family since they are, forthe most part, sets of classes.
CCs represent such knowl-edge as concepts (i.e.
*Conference, *Event, *Mtrans-Action), and plans (i.e.
*Declare-Want-Attend).
CSCsrepresent sequences ofconcepts and their elations uch asconcept sequences 2 (i.e.
<*Conference *Goal-Role *At-tend *Want>) or plan sequences (i.e.
<*Declare-Want-Attend *Listen-Instruction>) 3 of the two participants ofthe dialog.
CSCs have an internal structure composed of aconcept sequence, constraint equations, presuppositions,and effects.
This internal structure provides our schemewith the capability to handle unification-based processingas well as case-based processing, so that ypical criticismsagainst DMAP-type NLP \[Riesbeck and Martin, 1985\],such as weak linguistic overage and incapability of han-dling linguistically complex sentences, do not apply to ourmodel 4.
Each type of node creates instances during pars-ing which are called concept instances (CI) and conceptsequence instances (CSI), respectively.
CIs correspond todiscourse ntities.
They are connected through labelledlinks such as IS-A or PART-OF, and weighted links whichform a connectionist network.
CSIs record specific asesof utterances indexed into the memory network whereasZConcept sequences are the representation f an integratedsyntax/semantics level of knowledge in our model.3This should not be confused with 'discourse segments'\[Grosz and Sidner, 1985\].
In our model, information repre-sented in discourse segments i distributively incorporated inthe memory network.4Indeed, our model is substantially different from DMAP-type marker-passing or any other naive marker-passing models,because linguistic features are carried up by markers to conductsubstantial linguistic analysis as well as case-based processing.1 217CSCs represent generalized cases and syntactic rules.
Useof cases for generation is one of the unique features of ourmodel while most generators solely depend upon syntac-tic rules.2.2 Tile MarkersA guided marker-passing scheme is employed for infer-ence in the memory network.
Basically, our model usesfour types of markers.
These markers are (1) activationmarkers, (2) prediction markers, (3) generation markers,and (4) verbalization markers.Activation Markers (A-Markers) are created based onthe input of the source language.
These are passed upthrough IS-A links and carry instance, features and cost.This type of marker is used for parsing.Prediction Markers (P-Markers) are passed along theconceptual nd phonemic sequences to make predictionsabout which nodes are to be activated next.
Each P-Marker carries constraints, cost, and the informationstructure of the utterance which is built incrementallyduring parsing.Generation Markers (G-Markers) show activation ofnodes in the target language, and each contains a surfacestring, features, cost and an instance which the surfacestring represents.
G-Markers are passed up through IS-Alinks.Verbalization Markers (V-Markers) anticipate andkeep track of verbalization of surface strings.
Final sur-face realizations, cost and constraints are carried by V-Markers.Besides these markers, we assume Contextual Markers(C-Markers) \[Tomabechi, 1987\] which are used when aconnectionist network is computationally too expensive.The C.-Markers are passed through weighted links to in-dicate contextually relevant nodes.2.3 A Baseline AlgorithmGenerally, natural anguage generation i volves severalstages: content deliniation, text structuring, lexical se-lection, syntactic selection, coreference treatment, con-stituent ordering, and realization.
In our model, the con-tent is determined atthe parsing stage, and most other pro-cesses are unified into one stage, because, in our model.lexica~ item, phrase, and sentence are treated in the samemechanism.
The common thrust in our model is thehypothesis-activation-selection yclein which multiplehypotheses are activated and where one of them is finallyselected.
Thus, the translation process of our model iscomposed of processes of (1) concept activation, (2) lex-ical and phrasal hypotheses activation, (3) propositionalcontent activation, (4) syntactic and lexical selection, and(5) realization.1.
Concept Activation: A part of the parsing processas well as an initial process of generation.
Individualconcepts represented by CCs are activated as a result ofparsing speech inputs.
A-Markers are created and passedup by activating the concept.2.
Lexieal and Phrasal Hypotheses Activation: Hy-potheses for lexicons and phrases which represent theactivated concept are searched for, and G-Markers arecreated and passed up as a result of this process.
Usually,multiple candidates are activated at a time.3.
Propositional Content Activation: A part of theparsing process by which propositional content of the ut-terance is determined.4.
Syntactic and Lexical Selection: Selection of onehypothesis from multiple candidates of lexical entries orphrases.
First, the syntactic and semantic onstraints arechecked to ensure the correctness of the hypotheses, andthe final selection is made using a cost/activation-basedselection.5.
Realization: The surface string (which can be either asequence of words or a sequence of phonological signs)is formed from the selected hypothesis and scmt o thespeech synthesis device.The movement of V-Markers is important in under-standing our algorithm.
First, a V-Marker is located onthe first element of the CSC.
When a G-Marker hits theelement with the V-Marker, the V-Marker is moved tothe next element of the CSC (figure la), and unificationis performed to ensure syntactic soundness of the sen-tence.
In figure lb, dl is a closed class lexical item s.When a G-Marker hits the first element, a V-Marker onthe first element is moved to the third element by pass-ing through the second element which is a closed classitem.
In this case, the element for the closed class itemneed not have a G-Marker.
The lexical realization for theelement is retrieved when the V-Marker passes throughthe element.
In the case where the G-Marker hits an el-ement without a V-Marker, the G-Marker is stored in theelement.
When another G-Marker hits the element witha V-Marker, the V-Marker is moved to the next element.Since the next element already has a G-Marker, the V-Marker is further moved to the subsequent element of theCSC (figure lc).
Although, in most cases, a bottom upprocess by G-Markers handles generation process, thereare cases where a bottom up process alone can not iden-tify syntactic structure and lexieal items to express agivenmeaning.
In such cases, a top-down process is invokedwhich identifies the best syntactic structure and lexiealitems by searching downward from each element of theactivated CSC.
Each retrieval procedure is similar to thesearch of a closed class lexical item.There are cases in which an element oftheCSC is linkedto other CSCs, and forms hierarchies of CSCs.
Sup-pose each CSC represents a phrase structure nile, then thedynamically organized CSC hierarchy provides produc-tive power so that various types of structures of complexsentences can be generated.
In the hierarchy of CSCs,G-Markers are passed up when a CSC is accepted, andcarry feature structures which represent mourning frag-ments expressed by the CSC.
V-Markers are passed ownto lower CSCs when an element is predicted, and imposeconstraints on each elements of the lower CSCs.
The hi-erarchical organization of CSCs allows all types of treeexpansions: upward, downward and insertion.Figure 2 shows an example of how an analysis treecan be constructed in our model.
In this example, we as-sume Lexical-Functional Grammar (LFG) as a grammarformalism, and the order which conceptual fl'agments aregiven is based on an order that conceptual fragments canbe identified when parsing acorresponding Japanese sen-tence incrementally.
Notice that all three types of exten-sions are involved even in such a simple sentence.SClosed class lexical items refer to function words such asin, of, at in English and wo, ga, ni in Japanese.
These wordsare non-referential and their number do not grow, whereas openclass lexical items are mostly referential nd their number growsas vocabulary expands.218  2(a) v< ~oa l  a2  .
.
.
an >/G(C) V<~ (/0 ~1 a2 ?
?
?
an >lGv (b) v::~ < at al a2 ?
.. a, > < ~0 a-1 a2 .
.
.
a, >!GV=:~ < a0 dl a2 .-.
a, >V V< qo a l  a2 ?
?
?
an > : :~ < at  a t  a2  ?
?
?
a, >T GGFigure 1: Movement of V-Marker in the CSCNPINIShe(~)NP ppIN P NPI !Z2xShe a the hotel(4)NP ppIN P NPI IShe at  the hotel(2)PPAP VPV NP' I Lxunpack her luggageNP PPIN P NPShe a the hotel(3)NPher luggageSANP VPN V PP  PPi I / xShe s layed P NP Ia, the he,e, , \[unpack her luggage(a)Figure 2: An Incremental Tree Construction3 Activation of Lexical and PhrasalltIypolheses and Propositional ContentsWhen a concept is recognizexl by the parsing process, anhypotheses for translation will be activated.
Tile conceptcan be an individual concept, a phrase or a sentence.
Inour model, they are all represented as CC nodes, and eachinshqnce of the concept is represented as a CI node.
Thebasic process is for each of the activated CCs, LEX nodes 6in the target language to be activated.
There are fern'possible mappings between source language nodes andtarget language nodes which are activated; word-to-word,phrase-to-word, word-to-phrase, and phrase-to-phrase.
Inour model, hypotheses for sentences and phrases are rep-resented as CSCs.
From the viewpoint of generation,either LEX nodes representing words or CSC nodes rep-resenting phrases or entire sentences are activat~xl.LEX node activation: There are cases when a word ora phrase can be translated into a word in the target lan-guage.
In figure 3a and c, the word LEXsc or the phraseCSCsL activates CC~.
LEXlrL is activated as a hypothesisof translation for LEXsc or CSCsL interpreted as CC~.
AG-Marker is createxl at LEXT,L containing a surface real-ization, cost, features, and an instance which the LEXlrcrepresents (CI).
The G-Marker is passed up through anIS-A link.
When a CCI does not have LEXlrL, a CC2 isactivated and a LEX2.tL will be activated.
Thus, the mostspecific word in the target language will be activated as ahypothesis.CSC node activation: When a CC can be representedby a phrase or sentence, a CSC node is activated and a6LEX nodes are a kind of CSC which represent a lexicalentry and phonological realization of the word.G-Marker which contains that phrase or sentence will becreated.
In figure 3b and d, LEXsL and CSCsL activatesCCl which has CSCI.rL.
In this case, CSClrL will beactivated as a hypothesis to translate LEXsL or CSCsL in-terpreted as CC1.
In particular, activation of CSCrL byCSCsL is interesting because it covers cases where twoexpressions can be translated only at phrasal or sentenialcorrespondence, not at the lexical level.
Such cases areoften found in greetings or canned phrases.
It should benoted that CSCs represent either syntactic rules or eases ofutterance.
Assuming eases are acquired from legitimateutterances of native speakers, use of  cases for a generationprocess hould be preferred over purely syntactic formu-lation of sentences because use of cases avoids generationof sentences which are syntactically sound but never ut-tered by native speakers.4 Syntactic and Lexical SelectionsSyntactic and lexical selections are conducted involvingthree processes: feature aggregation, constraint satisfac-tion, and competitive activation.
Feature aggregation andconstraint satisfaction correspond to a symbolic approachto syntactic and lexieal selection which guarantee gram-maticality and local semantic accuracy of the generatedsentences, and the competitive activation process is addedin order to select ile best decision among multiple candi-dates.4.1 Feature AggregationFeature aggregation is an operation which combines fea-tures in the process of passing up G-Markers so thatminimal features are carried up.
Due to the hierarchi-cal organization of the memory network, features which3 219II%LEXsL CI LEX1TLI ILEXsL Cl CSC1TL CSCsL CI LEX17LII% / c sc2CSCsL CI CSCI~(a) ?o) (c) (d)Figure 3: Activation of Syntactic and Lexical Hypothesesneed to be carried by G-Markers are different dependingupon which level of abstraction is used for generation'.Given the fact that unification is a computationally ex-pensive operation, aggregation is an efficient mechanismfor propagating features because it ensures only minimalfeatures are aggregated when features are unified, and ag-gregation itself is a cheap operation since it simply addsnew features to existing features in the G-Marker.
Oneother advantage of this mechanism is that the case-basedprocess and the constraint-based process are treated inone mechanism because features required for each levelof processing are incrementally added in G-Markers.4.2 Constraint SatisfactionConstraint is a central notion in modern syntax theories.Each CSC has constraint equations which define the con-straints imposed for that CSC depending on their levelof abstraction 8.
Feature structures and constraint equa-tions interact at two stages.
At the prediction stage, if aV-Marker placed on the first element of the CSC alreadycontains a feature structure that is non-nil, the featurestructure determines, according to the constraint equa-tions, possible feature structures of G-Markers whichsubsequent elements of the CSC can accept.
At a G-V-collision stage, a feature structure in the G-Marker istested to see if it can meet what was anticipated.
If thefeature structure passes this test, information in the G-Marker and the V-Marker are combined and more precisepredictions are made as to what will be acceptable insubsequent elements.
Thus, the grammaticality of thegenerated sentences i guaranteed.
Semantic restrictionsare considered in this stage.4.3 Competitive ActivationThe competitive activation process introduced either bya C-Marker-passing or by the connectionist network de-termines the final syntactic and lexical realization of thesentence.
Here, we have adopted a cost?based scheme aswe have employed in parsing \[Kitano et.
at., 1989a\].
Inthe cost-based scheme, the hypothesis with the least costwill be selected.
This idea reflects our view that pars-ing and generation are dynamic processes in which thestate of the system tends to a global minima, and that acost represents dispersion of energy so that higher costhypotheses are less likely to be taken as the state of thesystem.
In the actual implementation, we compute a cost7When knowledge of cases, similar to the phrasal lexicon,is used for generation, features are not necessary because thisknowledge is already indexed to specific discourse ntities.8However, CSCs representing specific cases do not havecontraint equations ince they axe already instanfiated and theCSCs are indexed in the memory network.of each hypothesis which is determined by a C-Marker-passing scheme or a connectionist network.The C-Marker passing scheme puts C-Markers at con-textually relevant nodes when a conceptual root node isactivated.
A G-Marker which goes through a node with-out a C-Marker will be added with larger cost than others.When there are multiple hypothesis for the specific CCnode; i.e.
when multiple CSCs are linked with the CC,we will add up the cost of each G-Marker used for eachlinearization combined with pragmatic onstraints whichmay be assigned to each CSC, and the preference for eachCSC, and the hypothesis with least cost will ~; selectedas the translated result.The Connectionist Network will be adopted with somecomputational costs.
When a connectionist network isfully deployed, every node in the network is connectedwith weighted links.
A competitive excitation and inhibi-tion process is performed to select one hypothesis.
Finalinterpretation and translation in the target language areselected through a winner-take-all mechanism.5 Committment and AmbiguitiesOne of the most significant issues is how to resolve ambi-guities of the parsing process as early as possible, so thatthe final translation hypothesis can be determined as earlyas possible.
Since many sentences are ambiguous un-til, at least, the entire clause is analyzed, disambiguationnecessarily imposes constraints upon scheduling of thegeneration process, However, it should be noted that thehuman interpreter does not start ranslating unless she/heis sure about what the sentence means.
This allows ourmodel to take a wait-and-see strategy when multiple hy-potheses are present during processing of input utterances.However, when some ambiguities still remain., the gen-erator needs to commit o one of the hypotheses, whichmay turn out to be false.
This would be even compli-cated when a source language mid a target language havesubstantially different linguistic structures.
For exam-ple, in English, negation comes before a verb, whereasJapanese negation comes after a verb, and the verb comesat the very end of a sentence.
In such case, translationcannot be started until the verb, which comes the end ofthe sentence, was processed, and existance of negationafter the verb is checked.
Decision has to be made, forthis case, to wait translation until these ambiguities areresolved by encountering a clause which follows the ini-tial clause.
Fortunately, most Japanese utterance consistof multiple clauses which makes imultaneous interpreta-tion possible.
In order to cope with these ambiguities, asimultaneous interpretation system should have capabili-ties such as (1) anticipating the possiblity of negation atthe end, (2) incorporating some heuristics which recover220  4false translation to correct one, and (3) making decisionson when to start or wait translations.
Theories of com-mitment in ambiguity resolution and generation are notestablished, yet, thus they are a subject of further investi-gations.
One possible solution which we are investigatingis to use probabilistic speed control of marker propaga-tion as seem in \[Wu, 1989\] so that the best hypothesispresented first.
This would allow the generator to commitupon present hypothesis within its local decisions.6 Psychological PlausibilityPsychological studies of sentence production \[Garrett,1975\] \[Garrett, 1980\] \[Levelt and Maassen, 1981\] \[Bock,1982\] \[Bock, 1987\] and \[Kempen and Huijbers, 1983\]were taken into account in designing the model.
In\[Kempen and Huijbers, 1983\], two independent retrievalprocesses are assumed, one accounting for abstract pre-phonologicalitems (L 1-items) and the other for phonolog-ical items (L2-items).
The lexicalization i  their modelfollows: (1) a simultaneous multiple Ll-item retrieval,(2) a monitoring process which watches the output ofLl-lexicalization to check that it is keeping within con-straints upon utterance format, (3) retrieval of L2-itemsafter waiting until the Ll-item has been checked by themonitor, and all other Ll-items become available.
In ourmodel, a CCs activation stage corresponds to multipleLl-item retrieval, constraint checks by V-Markers corre-spond to the monitoring, and the realization stage whichconcatenates the surface string in a V-Marker correspondsto the L2-item retrieval stage.
The difference between ourmodel and their model is that, in our model, L2-items arealready incorporated in G-Markers whereas they assumeL2-items are accessed only after the monitoring.
Phe-nomenologically, this does not make a significant differ-ence because L2-items (phonological realization) in ourmodel are not explicitly selected until constraints are met;atwhichpointthemonitoringis completed.
However, thisdifference may be more explicit in tbe production of sen-tences because of the difference in the scheduling of theL2-itern retrieval and the monitoring.
This is due to thefact that our model retains interaction between two levelsas investigated by \[Bock, 1987\].
Our model also explainscontradictory observations by \[Bock, 1982\] and \[Levoelt and Maassen, 1981\] because activation of CC nodes(Ll-iteras) and LEX nodes (L2-items) are separated withsome interactions.
Also, our model is consistent witha two-stage model \[Garrett, 1975\] \[Garrett, 1980\].
Thefunctior~alandpositionallevels of processing inhis modelcorrespond tothe parallel activation of CCs and CSCs, theV-Marker movement which is left to right, and the surfacestring concatenation during that movement.Studies of the planning unit in sentence production\[Ford and Holmes, 1978\] give additional support o thepsychological p ausibility of our model.
They report hatdeep clause instead of surface clause is the unit of sen-tence planning.
This is consistent to our model which em-ploys CSCs, which account for deep propositional unitsand the realization of deep clauses as the basic units of sen-tence planning.
They also report hat people are planningthe next clause while speaking the current clause.
This isexactly what our model is performing, and is consistentwith our observations from transcripts of simultaneousinterpretation.7 Relevant StudiesSince most machine translation systems assume sequen-tial parsing and generation, a simple extension of exist-ing systems to combine speech recognition and synthesiswould not suffice for interpreting telephony.
The mainproblem is in previously existing systems' inability toattain simultaneous interpretation (whereas partial trans-lation is performed while parsing is in progress), becausein other systems a parser and a generator are indepen-dent modules, and the generation process is only invokedwhen the entire parse is completed and full semantic rep-resentation is given to the generator.
Our model servesas an example of approaches counter to the modular ap-proach, and attains simultaneous interpretation capabil-ity by employing incremental parsing and a generationmodel.
Pioneer studies of parallel incremental sentenceproduction are seem in \[Kempen and Hoekamp, 1987\]\[Kempen, 1987\].
They use a segment grammar whichis composed of Node-Arc-Node building blocks to attainincremental formation of trees.
Their studies parallel ourmodel in many aspects.
The segment grammar is a kindof semantic grammar since the arc label of each segmentmakes each segment a syntax/semantic object.
Featureaggregation and constraint satisfaction by G-Markers andV-Markers in our model corresponds toa distributed uni-fication \[De Smedt, 1989\] in the segment grammar.
\[DeSmedt, 1990\] reports extensively on their approach to in-cremental sentence generation which parallel to our modelin many aspects.8 Current  Imp lementat ionThe model of generation described in this paper hasbeen implemented asa part of #DMDIALOG, a speech-to-speech dialog translation system developed at the Centerfor Machine Translation at Carnegie Mellon University.#DMDIALOG is implemented on an IBM RT-PC worksta-tion using CMU CommonLisp run on Mach OS.
Speechinput and voice synthesis are done by connected hardwaresystems, currently, we are using Matsushita Institute'sJapanese speech recognition hardware and DECTalk.Figure 4 is an example of how sentences with multi?pie clauses are translated simultaneously in giDMDIALOG.Although an input is shown as a word sequence, realrun takes speech inputs and a phoneme sequence is usedto interface between the speech recognition device andthe software.
Current implementation translates betweenJapanese and English and operates on the conference r g-istration domain based on the corpus provided by the ATRInterpreting Telephony Research Laboratories.
For moredetails of the generation scheme described in this paper,refer to \[Kitano, 1990\].Currently, we are designing a version of our model tobe implemented on massively parallel machines: IXM\[Higuchi et.
al., 1989\] and SNAP \[Moldovan et.
al.,1989\].9 ConclusionWe described a parallel incremental model of natural lan-guage generation designed for the speech-to-speech di-alog translation system ~DMDIALOG.
We demonstratedthat a parallel marker-passing scheme is one desirable wayof exploring inherent parallelism of sentence production.All types of tree expansion are attained, and ability to in-5 221Input Utterance TranslationIwanttoattendtheconferencebecauseIaminterestedininterpreting telephonywatashi ha (I Role-Agent; This is ellipsed in the actual translation)kaigi ni sanka shitai (want to attend the conference)toiunoha (because)watashi ha (I Role-Agent; This is ellipsed in the actual translation)tuuyaku denwa ni kyoumi ga arukara desu (interested in interpreting telephony)Figure 4: An Example of Simultaneous Interpretationcrementally generate complex sentences has been shown.It should be noted that, in our model, activations and se-lections of syntactic structure and lexical items are treatedin an uniform mechanism.
Psychological plausibility isanother notable feature of our model since most researchin natural language generation has not taken into accountpsychological studies.
We believe our parallel incremen-tal generation model is a promising approach toward thedevelopment of interpreting telephony where simultane-ous interpretation is required.References\[Bock, 1987\] "Exploring Levels of Processing in Sentence Pro-duction," In Kempen, G.
(Ed.)
Natural Language Genera-tion, Nijhoff, 1987\[Bock, 198:2\] "Toward a Cognitive Psychology of Syntax: In-formation Processing Contributions to Sentence Formula-tion," Psycho.
Rev., 89, ppl-47, 1982.\[De Smedt, 1989\] De Smedt, K., "Distributed Unification inParallel Incremetnal Syntactic Tree Fomlation," In Proceed-ings of the Second European Workshop on Natural LangaugeGeneration, 1989.\[De Smedt, 1990\] De Smedt, K., Incremental Sentence Gener-ation, NICI Technical Report 90o01, Nijmegen Institute forCognition Research and Information Technology, 1990.\[Ford and Holmes, 1978\] Ford, M. and Holmes, V., "PlanningUnits and Syntax in Sentence Production," Cognition, 6,pp35-53, 1978.\[Garrett, 1980\] Garrett, M.F., "Levels of Processing in Sen-tence Production," In Butterworth, B.
(Ed.)
Language Pro~duction (Vol.
1 Speech and Talk), Academic Press, 1980.\[Garrett, 1975\] Garrett, M.F., ''The Analysis of Sentence Pro-duction," In Bower, G.
(Ed.)
The Psychology of Learningand Motivation, Vol.
9, Academic Press, 1975.\[Grosz and Sidner, 1985\] Grosz, B. and Sidner, C., ''The Struc-ture of Discourse Structure," CSLI Report No.
CSLI-85-39,1985.\[Higuchi et.
al., 1989\] Higuchi, T., Furuya, T., Kusumoto, H.,Handa, K. and Kokubu~ A., "The Prototype of a SemanticNetwork Machine IXM," In Proceedings of the InternationalConference on Parallel Processing, 1989.\[Kempen, t987\] Kempen, G., "A Framework for IncrementalSyntactic Tree Formation," In Proceedings of the Interna-tional Joint Conference onArtiftcial Intelligence (IJCAI-87),1987.\[Kempen and Hoekamp, 1987\] Kempen, G. and Hoenkamp,E., "An Incremental Procedural Grammar for Sentence For-mulation," Cognitive Science, 11,201-258, 1987.\[Kempen and Huijbers, 1983\] Kempen, G. and Huijbers, P.,"The Lexicalization Process in Sentence Production andNaming: Indirect Election of Words," Cognition, 14, pp185-209, 1983.\[Kitano, 1990\] Kitano, H., "Parallel Incremental SentencePro-duction for a Model of Simultaneous Interpretation," In Dale,R.
et.
al.
(Eds.)
Current Research in Natural Language Gen-eration, Academic Press, 1990.\[Kitano, 1989a\] Kitano, H., A Massively Parallel Model of Si-multaneous Interpretation: The #DMDIALOO System, Tech-nical Report CMU-CMT-89-116, Carnegie Mellon Univer-sity, Pittsburgh, 1989.\[Kitano, 1989b\] Kitano, H., "Hybrid Parallelism: A Case ofSpeech-to-Speech Dialog Translation," In Proceedings ofthe IJCAI-89 Workshop on Parallel Algorithms for MachineIntelligence, 1989.\[Kitano et.
al., 1989a\] Kitano, H., Tomabechi, H. and Levin,L., "Ambiguity Resolution in DMTRANS PLUS," In Proceed-ings of the Fourth Conference of the European Chapter ofthe Association for Computational Linguistics, 1989.\[Kitano et.
al., 1989b\] Kitano, H., Mitamura, T. and Tomita,M., "Massively Parallel Parsing in ~liDMDIALOG: IntegratedArchitecture for Parsing Speech Inputs," In Proceedings ofthe International Workshop on Parsing Technologies, 1989.\[Levelt and Maassen, 1981\] Levelt, WJ.M.
and Maassen, B.,"Lexical Search and Order of Mention in sentence Produc-lion," In Klein, W. and Levelt, WJ.M.
(Eds.
), Crossing theBoundaries in Linguistics: Studies Presented to ManfredBierwisch, Dordrecht, Reidel, 1981.\[Moldovan et.
al., 1989\] Moldovan, D., Lee, W. and Lin, C.,SNAP: A Marker-Propagation Architecture for KrwwledgeProcessing, Technical Report CENG 89-10, University ofSouthern California, 1989.\[Riesbeck and Marlin, 1985\] Riesbeck, C. and Marlin, C., Di-rect Memory Access Parsing, Yale Un&ersity Report 354,1985.\[Tomabechi, 1987\] Tomabechi, H., "Direct Memory AccessTranslation," Proceedings of lJCAl87, 1987.\[Wu, 1989\] Wu, D., "A Probabilisite Apporach to Marker Prop-agation" In Proceedings oflJCAI-89, 1989.222  6
