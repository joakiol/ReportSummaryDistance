Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 337?344,Sydney, July 2006. c?2006 Association for Computational LinguisticsReranking and Self-Training for Parser AdaptationDavid McClosky, Eugene Charniak, and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{dmcc|ec|mj}@cs.brown.eduAbstractStatistical parsers trained and tested on thePenn Wall Street Journal (WSJ) treebankhave shown vast improvements over thelast 10 years.
Much of this improvement,however, is based upon an ever-increasingnumber of features to be trained on (typi-cally) the WSJ treebank data.
This has ledto concern that such parsers may be toofinely tuned to this corpus at the expenseof portability to other genres.
Such wor-ries have merit.
The standard ?Charniakparser?
checks in at a labeled precision-recall f -measure of 89.7% on the PennWSJ test set, but only 82.9% on the test setfrom the Brown treebank corpus.This paper should allay these fears.
In par-ticular, we show that the reranking parserdescribed in Charniak and Johnson (2005)improves performance of the parser onBrown to 85.2%.
Furthermore, use of theself-training techniques described in (Mc-Closky et al, 2006) raise this to 87.8%(an error reduction of 28%) again with-out any use of labeled Brown data.
Thisis remarkable since training the parser andreranker on labeled Brown data achievesonly 88.4%.1 IntroductionModern statistical parsers require treebanks totrain their parameters, but their performance de-clines when one parses genres more distant fromthe training data?s domain.
Furthermore, the tree-banks required to train said parsers are expensiveand difficult to produce.Naturally, one of the goals of statistical parsingis to produce a broad-coverage parser which is rel-atively insensitive to textual domain.
But the lackof corpora has led to a situation where much ofthe current work on parsing is performed on a sin-gle domain using training data from that domain?
the Wall Street Journal (WSJ) section of thePenn Treebank (Marcus et al, 1993).
Given theaforementioned costs, it is unlikely that many sig-nificant treebanks will be created for new genres.Thus, parser adaptation attempts to leverage ex-isting labeled data from one domain and create aparser capable of parsing a different domain.Unfortunately, the state of the art in parserportability (i.e.
using a parser trained on one do-main to parse a different domain) is not good.
The?Charniak parser?
has a labeled precision-recallf -measure of 89.7% on WSJ but a lowly 82.9%on the test set from the Brown corpus treebank.Furthermore, the treebanked Brown data is mostlygeneral non-fiction and much closer to WSJ than,e.g., medical corpora would be.
Thus, most workon parser adaptation resorts to using some labeledin-domain data to fortify the larger quantity of out-of-domain data.In this paper, we present some encouraging re-sults on parser adaptation without any in-domaindata.
(Though we also present results with in-domain data as a reference point.)
In particular wenote the effects of two comparatively recent tech-niques for parser improvement.The first of these, parse-reranking (Collins,2000; Charniak and Johnson, 2005) starts with a?standard?
generative parser, but uses it to gener-ate the n-best parses rather than a single parse.Then a reranking phase uses more detailed fea-tures, features which would (mostly) be impossi-ble to incorporate in the initial phase, to reorder337the list and pick a possibly different best parse.At first blush one might think that gathering evenmore fine-grained features from a WSJ treebankwould not help adaptation.
However, we find thatreranking improves the parsers performance from82.9% to 85.2%.The second technique is self-training ?
pars-ing unlabeled data and adding it to the trainingcorpus.
Recent work, (McClosky et al, 2006),has shown that adding many millions of wordsof machine parsed and reranked LA Times arti-cles does, in fact, improve performance of theparser on the closely related WSJ data.
Here weshow that it also helps the father-afield Browndata.
Adding it improves performance yet-again,this time from 85.2% to 87.8%, for a net error re-duction of 28%.
It is interesting to compare this toour results for a completely Brown trained system(i.e.
one in which the first-phase parser is trainedon just Brown training data, and the second-phasereranker is trained on Brown 50-best lists).
Thissystem performs at a 88.4% level ?
only slightlyhigher than that achieved by our system with onlyWSJ data.2 Related WorkWork in parser adaptation is premised on the as-sumption that one wants a single parser that canhandle a wide variety of domains.
While this is thegoal of the majority of parsing researchers, it is notquite universal.
Sekine (1997) observes that forparsing a specific domain, data from that domainis most beneficial, followed by data from the sameclass, data from a different class, and data froma different domain.
He also notes that differentdomains have very different structures by lookingat frequent grammar productions.
For these rea-sons he takes the position that we should, instead,simply create treebanks for a large number of do-mains.
While this is a coherent position, it is farfrom the majority view.There are many different approaches to parseradaptation.
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains.
The need toparse biomedical literature inspires (Clegg andShepherd, 2005; Lease and Charniak, 2005).Clegg and Shepherd (2005) provide an extensiveside-by-side performance analysis of several mod-ern statistical parsers when faced with such data.They find that techniques which combine differ-Training Testing f -measureGildea BacchianiWSJ WSJ 86.4 87.0WSJ Brown 80.6 81.1Brown Brown 84.0 84.7WSJ+Brown Brown 84.3 85.6Table 1: Gildea and Bacchiani results on WSJ andBrown test corpora using different WSJ and Browntraining sets.
Gildea evaluates on sentences oflength ?
40, Bacchiani on all sentences.ent parsers such as voting schemes and parse se-lection can improve performance on biomedicaldata.
Lease and Charniak (2005) use the Charniakparser for biomedical data and find that the use ofout-of-domain trees and in-domain vocabulary in-formation can considerably improve performance.However, the work which is most directly com-parable to ours is that of (Ratnaparkhi, 1999; Hwa,1999; Gildea, 2001; Bacchiani et al, 2006).
Allof these papers look at what happens to mod-ern WSJ-trained statistical parsers (Ratnaparkhi?s,Collins?, Gildea?s and Roark?s, respectively) astraining data varies in size or usefulness (becausewe are testing on something other than WSJ).
Weconcentrate particularly on the work of (Gildea,2001; Bacchiani et al, 2006) as they provide re-sults which are directly comparable to those pre-sented in this paper.Looking at Table 1, the first line shows usthe standard training and testing on WSJ ?
bothparsers perform in the 86-87% range.
The nextline shows what happens when parsing Brown us-ing a WSJ-trained parser.
As with the Charniakparser, both parsers take an approximately 6% hit.It is at this point that our work deviates fromthese two papers.
Lacking alternatives, both(Gildea, 2001) and (Bacchiani et al, 2006) giveup on adapting a pure WSJ trained system, insteadlooking at the issue of how much of an improve-ment one gets over a pure Brown system by addingWSJ data (as seen in the last two lines of Table 1).Both systems use a ?model-merging?
(Bacchianiet al, 2006) approach.
The different corpora are,in effect, concatenated together.
However, (Bac-chiani et al, 2006) achieve a larger gain by weight-ing the in-domain (Brown) data more heavily thanthe out-of-domain WSJ data.
One can imagine, forinstance, five copies of the Brown data concate-nated with just one copy of WSJ data.3383 CorporaWe primarily use three corpora in this paper.
Self-training requires labeled and unlabeled data.
Weassume that these sets of data must be in similardomains (e.g.
news articles) though the effective-ness of self-training across domains is currently anopen question.
Thus, we have labeled (WSJ) andunlabeled (NANC) out-of-domain data and labeledin-domain data (BROWN).
Unfortunately, lackinga corresponding corpus to NANC for BROWN, wecannot perform the opposite scenario and adaptBROWN to WSJ.3.1 BrownThe BROWN corpus (Francis and Kuc?era, 1979)consists of many different genres of text, intendedto approximate a ?balanced?
corpus.
While thefull corpus consists of fiction and nonfiction do-mains, the sections that have been annotated inTreebank II bracketing are primarily those con-taining fiction.
Examples of the sections annotatedinclude science fiction, humor, romance, mystery,adventure, and ?popular lore.?
We use the samedivisions as Bacchiani et al (2006), who basetheir divisions on Gildea (2001).
Each division ofthe corpus consists of sentences from all availablegenres.
The training division consists of approx-imately 80% of the data, while held-out develop-ment and testing divisions each make up 10% ofthe data.
The treebanked sections contain approx-imately 25,000 sentences (458,000 words).3.2 Wall Street JournalOur out-of-domain data is the Wall Street Journal(WSJ) portion of the Penn Treebank (Marcus et al,1993) which consists of about 40,000 sentences(one million words) annotated with syntactic in-formation.
We use the standard divisions: Sec-tions 2 through 21 are used for training, section 24for held-out development, and section 23 for finaltesting.3.3 North American News CorpusIn addition to labeled news data, we make useof a large quantity of unlabeled news data.
Theunlabeled data is the North American News Cor-pus, NANC (Graff, 1995), which is approximately24 million unlabeled sentences from various newssources.
NANC contains no syntactic informationand sentence boundaries are induced by a simplediscriminative model.
We also perform some basiccleanups on NANC to ease parsing.
NANC containsnews articles from various news sources includingthe Wall Street Journal, though for this paper, weonly use articles from the LA Times portion.To use the data from NANC, we use self-training(McClosky et al, 2006).
First, we take a WSJtrained reranking parser (i.e.
both the parser andreranker are built from WSJ training data) andparse the sentences from NANC with the 50-best(Charniak and Johnson, 2005) parser.
Next, the50-best parses are reordered by the reranker.
Fi-nally, the 1-best parses after reranking are com-bined with the WSJ training set to retrain the first-stage parser.1 McClosky et al (2006) find that theself-trained models help considerably when pars-ing WSJ.4 ExperimentsWe use the Charniak and Johnson (2005) rerank-ing parser in our experiments.
Unless mentionedotherwise, we use the WSJ-trained reranker (as op-posed to a BROWN-trained reranker).
To evaluate,we report bracketing f -scores.2 Parser f -scoresreported are for sentences up to 100 words long,while reranking parser f -scores are over all sen-tences.
For simplicity and ease of comparison,most of our evaluations are performed on the de-velopment section of BROWN.4.1 Adapting self-trainingOur first experiment examines the performanceof the self-trained parsers.
While the parsers arecreated entirely from labeled WSJ data and unla-beled NANC data, they perform extremely well onBROWN development (Table 2).
The trends are thesame as in (McClosky et al, 2006): Adding NANCdata improves parsing performance on BROWNdevelopment considerably, improving the f -scorefrom 83.9% to 86.4%.
As more NANC data isadded, the f -score appears to approach an asymp-tote.
The NANC data appears to help reduce datasparsity and fill in some of the gaps in the WSJmodel.
Additionally, the reranker provides fur-ther benefit and adds an absolute 1-2% to the f -score.
The improvements appear to be orthogonal,as our best performance is reached when we usethe reranker and add 2,500k self-trained sentencesfrom NANC.1We trained a new reranker from this data as well, but itdoes not seem to get significantly different performance.2The harmonic mean of labeled precision (P) and labeledrecall (R), i.e.
f = 2?P?RP+R339Sentences added Parser Reranking ParserBaseline BROWN 86.4 87.4Baseline WSJ 83.9 85.8WSJ+50k 84.8 86.6WSJ+250k 85.7 87.2WSJ+500k 86.0 87.3WSJ+750k 86.1 87.5WSJ+1,000k 86.2 87.3WSJ+1,500k 86.2 87.6WSJ+2,000k 86.1 87.7WSJ+2,500k 86.4 87.7Table 2: Effects of adding NANC sentences to WSJtraining data on parsing performance.
f -scoresfor the parser with and without the WSJ rerankerare shown when evaluating on BROWN develop-ment.
For this experiment, we use the WSJ-trainedreranker.The results are even more surprising when wecompare against a parser3 trained on the labeledtraining section of the BROWN corpus, with pa-rameters tuned against its held-out section.
De-spite seeing no in-domain data, the WSJ basedparser is able to match the BROWN based parser.For the remainder of this paper, we will referto the model trained on WSJ+2,500k sentences ofNANC as our ?best WSJ+NANC?
model.
We alsonote that this ?best?
parser is different from the?best?
parser for parsing WSJ, which was trainedon WSJ with a relative weight4 of 5 and 1,750ksentences from NANC.
For parsing BROWN, thedifference between these two parsers is not large,though.Increasing the relative weight of WSJ sentencesversus NANC sentences when testing on BROWNdevelopment does not appear to have a significanteffect.
While (McClosky et al, 2006) showed thatthis technique was effective when testing on WSJ,the true distribution was closer to WSJ so it madesense to emphasize it.4.2 Incorporating In-Domain DataUp to this point, we have only considered the sit-uation where we have no in-domain data.
We now3In this case, only the parser is trained on BROWN.
In sec-tion 4.3, we compare against a fully BROWN-trained rerank-ing parser as well.4A relative weight of n is equivalent to using n copies ofthe corpus, i.e.
an event that occurred x times in the corpuswould occur x?n times in the weighted corpus.
Thus, largercorpora will tend to dominate smaller corpora of the samerelative weight in terms of event counts.explore different ways of making use of labeledand unlabeled in-domain data.Bacchiani et al (2006) applies self-training toparser adaptation to utilize unlabeled in-domaindata.
The authors find that it helps quite a bit whenadapting from BROWN to WSJ.
They use a parsertrained from the BROWN train set to parse WSJ andadd the parsed WSJ sentences to their training set.We perform a similar experiment, using our WSJ-trained reranking parser to parse BROWN train andtesting on BROWN development.
We achieved aboost from 84.8% to 85.6% when we added theparsed BROWN sentences to our training.
Addingin 1,000k sentences from NANC as well, we saw afurther increase to 86.3%.
However, the techniquedoes not seem as effective in our case.
While theself-trained BROWN data helps the parser, it ad-versely affects the performance of the rerankingparser.
When self-trained BROWN data is added toWSJ training, the reranking parser?s performancedrops from 86.6% to 86.1%.
We see a similardegradation as NANC data is added to the train-ing set as well.
We are not yet able to explain thisunusual behavior.We now turn to the scenario where we havesome labeled in-domain data.
The most obviousway to incorporate labeled in-domain data is tocombine it with the labeled out-of-domain data.We have already seen the results (Gildea, 2001)and (Bacchiani et al, 2006) achieve in Table 1.We explore various combinations of BROWN,WSJ, and NANC corpora.
Because we aremainly interested in exploring techniques withself-trained models rather than optimizing perfor-mance, we only consider weighting each corpuswith a relative weight of one for this paper.
Themodels generated are tuned on section 24 fromWSJ.
The results are summarized in Table 3.While both WSJ and BROWN models bene-fit from a small amount of NANC data, addingmore than 250k NANC sentences to the BROWNor combined models causes their performance todrop.
This is not surprising, though, since adding?too much?
NANC overwhelms the more accurateBROWN or WSJ counts.
By weighting the countsfrom each corpus appropriately, this problem canbe avoided.Another way to incorporate labeled data is totune the parser back-off parameters on it.
Bac-chiani et al (2006) report that tuning on held-outBROWN data gives a large improvement over tun-340ing on WSJ data.
The improvement is mostly (butnot entirely) in precision.
We do not see the sameimprovement (Figure 1) but this is likely due todifferences in the parsers.
However, we do seea similar improvement for parsing accuracy onceNANC data has been added.
The reranking parsergenerally sees an improvement, but it does not ap-pear to be significant.4.3 Reranker PortabilityWe have shown that the WSJ-trained reranker isactually quite portable to the BROWN fiction do-main.
This is surprising given the large numberof features (over a million in the case of the WSJreranker) tuned to adjust for errors made in the 50-best lists by the first-stage parser.
It would seemthe corrections memorized by the reranker are notas domain-specific as we might expect.As further evidence, we present the results ofapplying the WSJ model to the Switchboard cor-pus ?
a domain much less similar to WSJ thanBROWN.
In Table 4, we see that while the parser?sperformance is low, self-training and rerankingprovide orthogonal benefits.
The improvementsrepresent a 12% error reduction with no additionalin-domain data.
Naturally, in-domain data andspeech-specific handling (e.g.
disfluency model-ing) would probably help dramatically as well.Finally, to compare against a model fullytrained on BROWN data, we created a BROWNreranker.
We parsed the BROWN training set with20-fold cross-validation, selected features that oc-curred 5 times or more in the training set, andfed the 50-best lists from the parser to a numeri-cal optimizer to estimate feature weights.
The re-sulting reranker model had approximately 700,000features, which is about half as many as the WSJtrained reranker.
This may be due to the smallersize of the BROWN training set or because thefeature schemas for the reranker were developedon WSJ data.
As seen in Table 5, the BROWNreranker is not a significant improvement over theWSJ reranker for parsing BROWN data.5 AnalysisWe perform several types of analysis to measuresome of the differences and similarities betweenthe BROWN-trained and WSJ-trained rerankingparsers.
While the two parsers agree on a largenumber of parse brackets (Section 5.2), there arecategorical differences between them (as seen inParser model Parser f -score Reranker f -scoreWSJ 74.0 75.9WSJ+NANC 75.6 77.0Table 4: Parser and reranking parser performanceon the SWITCHBOARD development corpus.
Inthis case, WSJ+NANC is a model created from WSJand 1,750k sentences from NANC.Model 1-best 10-best 25-best 50-bestWSJ 82.6 88.9 90.7 91.9WSJ+NANC 86.4 92.1 93.5 94.3BROWN 86.3 92.0 93.3 94.2Table 6: Oracle f -scores of top n parses pro-duced by baseline WSJ parser, a combined WSJ andNANC parser, and a baseline BROWN parser.Section 5.3).5.1 Oracle ScoresTable 6 shows the f -scores of an ?oracle reranker??
i.e.
one which would always choose the parsewith the highest f -score in the n-best list.
Whilethe WSJ parser has relatively low f -scores, addingNANC data results in a parser with comparable ora-cle scores as the parser trained from BROWN train-ing.
Thus, the WSJ+NANC model has better oraclerates than the WSJ model (McClosky et al, 2006)for both the WSJ and BROWN domains.5.2 Parser AgreementIn this section, we compare the output of theWSJ+NANC-trained and BROWN-trained rerank-ing parsers.
We use evalb to calculate how sim-ilar the two sets of output are on a bracket level.Table 7 shows various statistics.
The two parsersachieved an 88.0% f -score between them.
Ad-ditionally, the two parsers agreed on all bracketsalmost half the time.
The part of speech taggingagreement is fairly high as well.
Considering theywere created from different corpora, this seemslike a high level of agreement.5.3 Statistical AnalysisWe conducted randomization tests for the signifi-cance of the difference in corpus f -score, based onthe randomization version of the paired sample t-test described by Cohen (1995).
The null hypoth-esis is that the two parsers being compared are infact behaving identically, so permuting or swap-ping the parse trees produced by the parsers for341WSJ tuned parserBROWN tuned parserWSJ tuned reranking parserBROWN tuned reranking parserNANC sentences addedf-score2000k1750k1500k1250k1000k750k500k250k0k87.887.086.085.083.8Figure 1: Precision and recall f -scores when testing on BROWN development as a function of the numberof NANC sentences added under four test conditions.
?BROWN tuned?
indicates that BROWN training datawas used to tune the parameters (since the normal held-out section was being used for testing).
For ?WSJtuned,?
we tuned the parameters from section 24 of WSJ.
Tuning on BROWN helps the parser, but not forthe reranking parser.Parser model Parser alone Reranking parserWSJ alone 83.9 85.8WSJ+2,500k NANC 86.4 87.7BROWN alone 86.3 87.4BROWN+50k NANC 86.8 88.0BROWN+250k NANC 86.8 88.1BROWN+500k NANC 86.7 87.8WSJ+BROWN 86.5 88.1WSJ+BROWN+50k NANC 86.8 88.1WSJ+BROWN+250k NANC 86.8 88.1WSJ+BROWN+500k NANC 86.6 87.7Table 3: f -scores from various combinations of WSJ, NANC, and BROWN corpora on BROWN develop-ment.
The reranking parser used the WSJ-trained reranker model.
The BROWN parsing model is naturallybetter than the WSJ model for this task, but combining the two training corpora results in a better model(as in Gildea (2001)).
Adding small amounts of NANC further improves the models.Parser model Parser alone WSJ-reranker BROWN-rerankerWSJ 82.9 85.2 85.2WSJ+NANC 87.1 87.8 87.9BROWN 86.7 88.2 88.4Table 5: Performance of various combinations of parser and reranker models when evaluated on BROWNtest.
The WSJ+NANC parser with the WSJ reranker comes close to the BROWN-trained reranking parser.The BROWN reranker provides only a small improvement over its WSJ counterpart, which is not statisti-cally significant.342Bracketing agreement f -score 88.03%Complete match 44.92%Average crossing brackets 0.94POS Tagging agreement 94.85%Table 7: Agreement between the WSJ+NANCparser with the WSJ reranker and the BROWNparser with the BROWN reranker.
Complete matchis how often the two reranking parsers returned theexact same parse.the same test sentence should not affect the cor-pus f -scores.
By estimating the proportion of per-mutations that result in an absolute difference incorpus f -scores at least as great as that observedin the actual output, we obtain a distribution-free estimate of significance that is robust againstparser and evaluator failures.
The results of thistest are shown in Table 8.
The table shows thatthe BROWN reranker is not significantly differentfrom the WSJ reranker.In order to better understand the difference be-tween the reranking parser trained on Brown andthe WSJ+NANC/WSJ reranking parser (a rerankingparser with the first-stage trained on WSJ+NANCand the second-stage trained on WSJ) on Browndata, we constructed a logistic regression modelof the difference between the two parsers?
f -scores on the development data using the R sta-tistical package5.
Of the 2,078 sentences in thedevelopment data, 29 sentences were discardedbecause evalb failed to evaluate at least one ofthe parses.6 A Wilcoxon signed rank test on theremaining 2,049 paired sentence level f -scoreswas significant at p = 0.0003.
Of these 2,049sentences, there were 983 parse pairs with thesame sentence-level f -score.
Of the 1,066 sen-tences for which the parsers produced parses withdifferent f -scores, there were 580 sentences forwhich the BROWN/BROWN parser produced aparse with a higher sentence-level f -score and 486sentences for which the WSJ+NANC/WSJ parserproduce a parse with a higher f -score.
Weconstructed a generalized linear model with abinomial link with BROWN/BROWN f -score >WSJ+NANC/WSJ f -score as the predicted variable,and sentence length, the number of prepositions(IN), the number of conjunctions (CC) and Brown5http://www.r-project.org6This occurs when an apostrophe is analyzed as a posses-sive marker in the gold tree and a punctuation symbol in theparse tree, or vice versa.Feature Estimate z-value Pr(> |z|)(Intercept) 0.054 0.3 0.77IN -0.134 -4.4 8.4e-06 ***ID=G 0.584 2.5 0.011 *ID=K 0.697 2.9 0.003 **ID=L 0.552 2.3 0.021 *ID=M 0.376 0.9 0.33ID=N 0.642 2.7 0.0055 **ID=P 0.624 2.7 0.0069 **ID=R 0.040 0.1 0.90Table 9: The logistic model of BROWN/BROWNf -score > WSJ+NANC/WSJ f -score identified bymodel selection.
The feature IN is the num-ber prepositions in the sentence, while ID identi-fies the Brown subcorpus that the sentence comesfrom.
Stars indicate significance level.subcorpus ID as explanatory variables.
Modelselection (using the ?step?
procedure) discardedall but the IN and Brown ID explanatory vari-ables.
The final estimated model is shown in Ta-ble 9.
It shows that the WSJ+NANC/WSJ parserbecomes more likely to have a higher f -scorethan the BROWN/BROWN parser as the numberof prepositions in the sentence increases, and thatthe BROWN/BROWN parser is more likely to havea higher f -score on Brown sections K, N, P, Gand L (these are the general fiction, adventure andwestern fiction, romance and love story, letters andmemories, and mystery sections of the Brown cor-pus, respectively).
The three sections of BROWNnot in this list are F, M, and R (popular lore, sci-ence fiction, and humor).6 Conclusions and Future WorkWe have demonstrated that rerankers and self-trained models can work well across domains.Models self-trained on WSJ appear to be betterparsing models in general, the benefits of whichare not limited to the WSJ domain.
The WSJ-trained reranker using out-of-domain LA Timesparses (produced by the WSJ-trained reranker)achieves a labeled precision-recall f -measure of87.8% on Brown data, nearly equal to the per-formance one achieves by using a purely Browntrained parser-reranker.
The 87.8% f -score onBrown represents a 24% error reduction on thecorpus.Of course, as corpora differences go, Brown isrelatively close to WSJ.
While we also find that our343WSJ+NANC/WSJ BROWN/WSJ BROWN/BROWNWSJ/WSJ 0.025 (0) 0.030 (0) 0.031 (0)WSJ+NANC/WSJ 0.004 (0.1) 0.006 (0.025)BROWN/WSJ 0.002 (0.27)Table 8: The difference in corpus f -score between the various reranking parsers, and the significance ofthe difference in parentheses as estimated by a randomization test with 106 samples.
?x/y?
indicates thatthe first-stage parser was trained on data set x and the second-stage reranker was trained on data set y.?best?
WSJ-parser-reranker improves performanceon the Switchboard corpus, it starts from a muchlower base (74.0%), and achieves a much less sig-nificant improvement (3% absolute, 11% error re-duction).
Bridging these larger gaps is still for thefuture.One intriguing idea is what we call ?self-trainedbridging-corpora.?
We have not yet experimentedwith medical text but we expect that the ?best?WSJ+NANC parser will not perform very well.However, suppose one does self-training on a bi-ology textbook instead of the LA Times.
Onemight hope that such a text will split the differ-ence between more ?normal?
newspaper articlesand the specialized medical text.
Thus, a self-trained parser based upon such text might do muchbetter than our standard ?best.?
This is, of course,highly speculative.AcknowledgmentsThis work was supported by NSF grants LIS9720368, andIIS0095940, and DARPA GALE contract HR0011-06-2-0001.
We would like to thank the BLLIP team for their com-ments.ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proc.
of the 2005 Meeting of theAssoc.
for Computational Linguistics (ACL), pages173?180.Andrew B. Clegg and Adrian Shepherd.
2005.
Evalu-ating and integrating treebank parsers on a biomedi-cal corpus.
In Proceedings of the ACL Workshop onSoftware.Paul R. Cohen.
1995.
Empirical Methods for Artifi-cial Intelligence.
The MIT Press, Cambridge, Mas-sachusetts.Michael Collins.
2000.
Discriminative rerankingfor natural language parsing.
In Machine Learn-ing: Proceedings of the Seventeenth InternationalConference (ICML 2000), pages 175?182, Stanford,California.W.
Nelson Francis and Henry Kuc?era.
1979.
Manualof Information to accompany a Standard Corpus ofPresent-day Edited American English, for use withDigital Computers.
Brown University, Providence,Rhode Island.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 167?202.David Graff.
1995.
North American News Text Cor-pus.
Linguistic Data Consortium.
LDC95T21.Rebecca Hwa.
1999.
Supervised grammar inductionusing training data with limited constituent infor-mation.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 72?80, University of Maryland.Matthew Lease and Eugene Charniak.
2005.
Parsingbiomedical literature.
In Second International JointConference on Natural Language Processing (IJC-NLP?05).Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Comp.
Lin-guistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of HLT-NAACL 2006.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1-3):151?175.Satoshi Sekine.
1997.
The domain dependence ofparsing.
In Proc.
Applied Natural Language Pro-cessing (ANLP), pages 96?102.Mark Steedman, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Steven Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proc.
of European ACL (EACL), pages331?338.344
