Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 631?636,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLabelling Topics using Unsupervised Graph-based MethodsNikolaos Aletras and Mark StevensonDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield, S1 4DPUnited Kingdom{n.aletras, m.stevenson}@dcs.shef.ac.ukAbstractThis paper introduces an unsupervisedgraph-based method that selects textuallabels for automatically generated topics.Our approach uses the topic keywords toquery a search engine and generate a graphfrom the words contained in the results.PageRank is then used to weigh the wordsin the graph and score the candidate labels.The state-of-the-art method for this task issupervised (Lau et al, 2011).
Evaluationon a standard data set shows that the per-formance of our approach is consistentlysuperior to previously reported methods.1 IntroductionTopic models (Hofmann, 1999; Blei et al, 2003)have proved to be a useful way to represent thecontent of document collections, e.g.
(Chaney andBlei, 2012; Ganguly et al, 2013; Gretarsson etal., 2012; Hinneburg et al, 2012; Snyder et al,2013).
In these interfaces, topics need to be pre-sented to users in an easily interpretable way.
Acommon way to represent topics is as set of key-words generated from the n terms with the highestmarginal probabilities.
For example, a topic aboutthe global financial crisis could be representedby its top 10 most probable terms: FINANCIAL,BANK, MARKET, GOVERNMENT, MORTGAGE,BAILOUT, BILLION, STREET, WALL, CRISIS.
Butinterpreting such lists is not always straightfor-ward, particularly since background knowledgemay be required (Chang et al, 2009).Textual labels could assist with the interpre-tations of topics and researchers have developedmethods to generate these automatically (Mei etal., 2007; Lau et al, 2010; Lau et al, 2011).
Forexample, a topic which has keywords SCHOOL,STUDENT, UNIVERSITY, COLLEGE, TEACHER,CLASS, EDUCATION, LEARN, HIGH, PROGRAM,could be labelled as EDUCATION and a suitable la-bel for the topic shown above would be GLOBALFINANCIAL CRISIS.
Approaches that make use ofalternative modalities, such as images (Aletras andStevenson, 2013), have also been proposed.Mei et al (2007) label topics using statisticallysignificant bigrams identified in a reference collec-tion.
Magatti et al (2009) introduced an approachfor labelling topics that relied on two hierarchicalknowledge resources labelled by humans, whileLau et al (2010) proposed selecting the most rep-resentative word from a topic as its label.
Hulpuset al (2013) make use of structured data from DB-pedia to label topics.Lau et al (2011) proposed a method for auto-matically labelling topics using information fromWikipedia.
A set of candidate labels is gener-ated from Wikipedia article titles by querying us-ing topic terms.
Additional labels are then gen-erated by chunk parsing the article titles to iden-tify n-grams that represent Wikipedia articles aswell.
Outlier labels (less relevant to the topic) areidentified and removed.
Finally, the top-5 topicterms are added to the candidate set.
The la-bels are ranked using Support Vector Regression(SVR) (Vapnik, 1998) and features extracted us-ing word association measures (i.e.
PMI, t-test, ?2and Dice coefficient), lexical features and searchengine ranking.
Lau et al (2011) report two ver-sions of their approach, one unsupervised (whichis used as a baseline) and another which is super-vised.
They reported that the supervised versionachieves better performance than a previously re-ported approach (Mei et al, 2007).This paper introduces an alternative graph-based approach which is unsupervised and lesscomputationally intensive than Lau et al (2011).Our method uses topic keywords to form a query.A graph is generated from the words contained inthe search results and these are then ranked usingthe PageRank algorithm (Page et al, 1999; Mihal-631{?Description?
: ?Microsoft will accelerate your journey to cloud computing with anagile and responsive datacenter built from your existing technology investments.?,?DisplayUrl?
: ?www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx?,?ID?
: ?a42b0908-174e-4f25-b59c-70bdf394a9da?,?Title?
: ?Microsoft | Server & Cloud | Datacenter | Virtualization ...?,?Url?
: ?http://www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx?,...
}Figure 1: Sample of the metadata associated with a search result.cea and Tarau, 2004).
Evaluation on a standarddata set shows that our method consistently out-performs the best performing previously reportedmethod, which is supervised (Lau et al, 2011).2 MethodologyWe use the topic keywords to query a search en-gine.
We assume that the search results returnedare relevant to the topic and can be used to identifyand weigh relevant keywords.
The most impor-tant keywords can be used to generate keyphrasesfor labelling the topic or weight pre-existing can-didate labels.2.1 Retrieving and Processing TextInformationWe use the approach described by Lau et al (2011)to generate candidate labels from Wikipedia arti-cles.
The 10 terms with the highest marginal prob-abilities in the topic are used to query Wikipediaand the titles of the articles retrieved used as candi-date labels.
Further candidate labels are generatedby processing the titles of these articles to identifynoun chunks and n-grams within the noun chunksthat are themselves the titles of Wikipedia arti-cles.
Outlier labels, identified using a similaritymeasure (Grieser et al, 2011), are removed.
Thismethod has been proved to produce labels whicheffectively summarise a topic?s main subject.However, it should be noted that our method isflexible and could be applied to any set of can-didate labels.
We have experimented with variousapproaches to candidate label generation but choseto report results using the approach described byLau et al (2011) to allow direct comparison of ap-proaches.Information obtained from web searches is usedto identify the best labels from the set of candi-dates.
The top n keywords, i.e.
those with highestmarginal probability within the topic, are used toform a query which was submitted to the Bing1search engine.
Textual information included in theTitle field2of the search results metadata was ex-tracted.
Each title was tokenised using openNLP3and stop words removed.Figure 1 shows a sample of the metadata asso-ciated with a search result for the topic: VMWARE,SERVER, VIRTUAL, ORACLE, UPDATE, VIRTU-ALIZATION, APPLICATION, INFRASTRUCTURE,MANAGEMENT, MICROSOFT.2.2 Creating a Text GraphWe consider any remaining words in the searchresult metadata as nodes, v ?
V , in a graphG = (V,E).
Each node is connected to its neigh-bouring words in a context window of ?n words.In the previous example, the words added to thegraph from the Title of the search result are mi-crosoft, server, cloud, datacenter and virtualiza-tion.We consider both unweighted and weightedgraphs.
When the graph is unweighted we assumethat all the edges have a weight e = 1.
In addi-tion, we weight the edges of the graph by comput-ing the relatedness between two nodes, viand vj,as their normalised Pointwise Mutual Information(NPMI) (Bouma, 2009).
Word co-occurrences arecomputed using Wikipedia as a a reference cor-pus.
Pairs of words are connected with edges onlyif NPMI(wi, wj) > 0.2 avoiding connections be-tween words co-occurring by chance and hence in-troducing noise.2.3 Identifying Important TermsImportant terms are identified by applying thePageRank algorithm (Page et al, 1999) in a sim-ilar way to the approach used by Mihalcea and1http://www.bing.com/2We also experimented with using the Description fieldbut found that this reduced performance.3http://opennlp.apache.org/632Tarau (2004) for document keyphrase extraction.The PageRank score (Pr) over G for a word (vi)can be computed by the following equation:Pr(vi) = d ?
?vj?C(vi)sim(vi, vj)?vk?C(vj)sim(vj, vk)Pr(vj)+ (1?
d)v (1)where C(vi) denotes the set of vertices which areconnected to the vertex vi.
d is the damping factorwhich is set to the default value of d = 0.85 (Pageet al, 1999).
In standard PageRank all elementsof the vector v are the same,1Nwhere N is thenumber of nodes in the graph.2.4 Ranking LabelsGiven a candidate label L = {w1, ..., wm} con-taining m keywords, we compute the score of Lby simply adding the PageRank scores of its con-stituent keywords:Score(L) =m?i=1Pr(wi) (2)The label with the highest score amongst the setof candidates is selected to represent the topic.
Wealso experimented with normalised versions of thescore, e.g.
mean of the PageRank scores.
How-ever, this has a negative effect on performancesince it favoured short labels of one or two wordswhich were not sufficiently descriptive of the top-ics.
In addition, we expect that candidate labelscontaining words that do not appear in the graph(with the exception of stop words) are unlikely tobe good labels for the topic.
In these cases thescore of the candidate label is set to 0.
We alsoexperimented with removing this restriction butfound that it lowered performance.3 Experimental Evaluation3.1 DataWe evaluate our method on the publicly avail-able data set published by Lau et al (2011).
Thedata set consists of 228 topics generated usingtext documents from four domains, i.e.
blogposts (BLOGS), books (BOOKS), news articles(NEWS) and scientific articles from the biomedi-cal domain (PUBMED).
Each topic is representedby its ten most probable keywords.
It is also as-sociated with candidate labels and human ratingsdenoting the appropriateness of a label given thetopic.
The full data set consists of approximately6,000 candidate labels (27 labels per topic).3.2 Evaluation MetricsOur evaluation follows the framework proposedby Lau et al (2011) using two metrics, i.e.
Top-1 average rating and nDCG, to compare variouslabelling methods.Top-1 average rating is the average human rat-ing (between 0 and 3) assigned to the top-rankedlabel proposed by the system.
This provides an in-dication of the overall quality of the label the sys-tem judges as the best one.Normalised discounted cumulative gain(nDCG) (J?arvelin and Kek?al?ainen, 2002; Croft etal., 2009) compares the label ranking proposedby the system to the ranking provided by humanannotators.
The discounted cumulative gainat position p, DCGp, is computed using thefollowing equation:DCGp= rel1+p?i=2relilog2(i)(3)where reliis the relevance of the label to the topicin position i.
Then nDCG is computed as:nDCGp=DCGpIDCGp(4)where IDCGpis the superviseed ranking of theimage labels, in our experiments this is the rank-ing provided by the scores in the human annotateddata set.3.3 Model ParametersOur proposed model requires two parameters tobe set: the context window size when connectingneighbouring words in the graph and the numberof the search results considered when constructingthe graph.We experimented with different sizes of contextwindow, n, between?1 words to the left and rightand all words in the title.
The best results were ob-tained when n = 2 for all of the domains.
In addi-tion, we experimented with varying the number ofsearch results between 10 and 300.
We observedno noticeable difference in the performance whenthe number of search results is equal or greaterthan 30 (see below).
We choose to report resultsobtained using 30 search results for each topic.
In-cluding more results did not improve performancebut required additional processing.633Domain Model Top-1 Av.
Rating nDCG-1 nDCG-3 nDCG-5BLOGSLau et al (2011)-U 1.84 0.75 0.77 0.79Lau et al (2011)-S 1.98 0.81 0.82 0.83PR 2.05?
0.83 0.84 0.83PR-NPMI 2.08?
0.84 0.84 0.83Upper bound 2.45 1.00 1.00 1.00BOOKSLau et al (2011)-U 1.75 0.77 0.77 0.79Lau et al (2011)-S 1.91 0.84 0.81 0.83PR 1.98?
0.86 0.88 0.87PR-NPMI 2.01?
0.87 0.88 0.87Upper bound 2.29 1.00 1.00 1.00NEWSLau et al (2011)-U 1.96 0.80 0.79 0.78Lau et al (2011)-S 2.02 0.82 0.82 0.84PR 2.04?
0.83 0.81 0.81PR-NPMI 2.05?
0.83 0.81 0.81Upper bound 2.45 1.00 1.00 1.00PUBMEDLau et al (2011)-U 1.73 0.75 0.77 0.79Lau et al (2011)-S 1.79 0.77 0.82 0.84PR 1.88??
0.80 0.80 0.80PR-NPMI 1.90??
0.81 0.80 0.80Upper bound 2.31 1.00 1.00 1.00Table 1: Results for Various Approaches to Topic Labelling (?
: significant difference (t-test, p < 0.05)to Lau et al (2011)-U; ?
: significant difference (p < 0.05) to Lau et al (2011)-S).4 Results and DiscussionResults are shown in Table 1.
Performance whenPageRank is applied to the unweighted (PR) andNPMI-weighted graphs (PR-NPMI) (see Section2.2) is shown.
Performance of the best unsuper-vised (Lau et al (2011)-U) and supervised (Lauet al (2011)-S) methods reported by Lau et al(2011) are shown.
Lau et al (2011)-U uses the av-erage ?2scores between the topic keywords andthe label keywords while Lau et al (2011)-S usesSVR to combine evidence from all features.
Inaddition, upper bound figures, the maximum pos-sible value given the scores assigned by the anno-tators, are also shown.The results obtained by applying PageRankover the unweighted graph (2.05, 1.98, 2.04 and1.88) are consistently better than the supervisedand unsupervised methods reported by Lau et al(2011) for the Top-1 Average scores and this im-provement is observed in all domains.
The differ-ence is significant (t-test, p < 0.05) for the un-supervised method.
A slight improvement in per-formance is observed when the weighted graph isused (2.08, 2.01, 2.05 and 1.90).
This is expectedsince the weighted graph contains additional in-formation about word relatedness.
For example,the word hardware is more related and, therefore,closer in the graph to the word virtualization thanto the word investments.Results from the nDCG metric imply that ourmethods provide better rankings of the candidatelabels in the majority of the cases.
It is outper-formed by the best supervised approach in two do-mains, NEWS and PUBMED, using the nDCG-3 and nDCG-5 metrics.
However, the best labelproposed by our methods is judged to be better(as shown by the nDCG-1 and Top-1 Av.
Rat-ing scores), demonstrating that it is only the lowerranked labels in our approach that are not as goodas the supervised approach.An interesting finding is that, although limitedin length, the textual information in the search re-sult?s metadata contain enough salient terms rel-evant to the topic to provide reliable estimates of63450 100 150 200 250 3001.71.81.922.12.2Number of Search ResultsTop-1Av.Rating(a) BLOGS50 100 150 200 250 3001.71.81.922.12.2Number of Search Results(b) BOOKS50 100 150 200 250 3001.71.81.922.12.2Number of Search ResultsTop-1Av.Rating(c) NEWS50 100 150 200 250 3001.71.81.922.12.2Number of Search ResultsLau et al (2011)-ULau et al (2011)-SPRPR-NPMI(d) PUBMEDFigure 2: Top-1 Average Rating obtained for different number of search results.term importance.
Consequently, it is not necessaryto measure semantic similarity between topic key-words and candidate labels as previous approacheshave done.
In addition, performance improvementgained from using the weighted graph is mod-est, suggesting that the computation of associationscores over a large reference corpus could be omit-ted if resources are limited.In Figure 2, we show the scores of Top-1 av-erage rating obtained in the different domains byexperimenting with the number of search resultsused to generate the text graph.
The most inter-esting finding is that performance is stable when30 or more search results are considered.
In addi-tion, we observe that quality of the topic labels inthe four domains remains stable, and higher thanthe supervised method, when the number of searchresults used is between 150 and 200.
The onlydomain in which performance of the supervisedmethod is sometimes better than the approach pro-posed here is NEWS.
The main reason is that newstopics are more fine grained and the candidatelabels of better quality (Lau et al, 2011) whichhas direct impact in good performance of rankingmethods.5 ConclusionWe described an unsupervised graph-basedmethod to associate textual labels with automati-cally generated topics.
Our approach uses resultsretrieved from a search engine using the topickeywords as a query.
A graph is generated fromthe words contained in the search results metadataand candidate labels ranked using the PageRankalgorithm.
Evaluation on a standard data setshows that our method consistently outperformsthe supervised state-of-the-art method for the task.AcknowledgmentsWe would like to thank Jey Han Lau for providingus with the labels selected by Lau et al (2011)-U and Lau et al (2011)-S. We also thank DanielPreot?iuc-Pietro for his useful comments on earlydrafts of this paper.635ReferencesNikolaos Aletras and Mark Stevenson.
2013.
Rep-resenting topics using images.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 158?167, At-lanta, Georgia.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
In Proceed-ings of GSCL.Allison June-Barlow Chaney and David M. Blei.
2012.Visualizing topic models.
In Proceedings of theSixth International AAAI Conference on Weblogsand Social Media, Dublin, Ireland.Jonathan Chang, Jordan Boyd-Graber, and Sean Ger-rish.
2009.
Reading Tea Leaves: How Humans In-terpret Topic Models.
Neural Information, pages 1?9.Bruce W. Croft, Donald Metzler, and Trevor Strohman.2009.
Search engines: Information retrieval inpractice.
Addison-Wesley.Debasis Ganguly, Manisha Ganguly, Johannes Level-ing, and Gareth J.F.
Jones.
2013.
TopicVis: A GUIfor Topic-based feedback and navigation.
In Pro-ceedings of the Thirty-Sixth Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR 13), Dublin,Ireland.Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-jiev, Tobias H?ollerer, Arthur Asuncion, David New-man, and Padhraic Smyth.
2012.
TopicNets: Visualanalysis of large text corpora with topic modeling.ACM Trans.
Intell.
Syst.
Technol., 3(2):23:1?23:26.Karl Grieser, Timothy Baldwin, Fabian Bohnert, andLiz Sonenberg.
2011.
Using Ontological and Doc-ument Similarity to Estimate Museum Exhibit Re-latedness.
Journal on Computing and Cultural Her-itage (JOCCH), 3(3):10:1?10:20.Alexander Hinneburg, Rico Preiss, and Ren?e Schr?oder.2012.
TopicExplorer: Exploring document collec-tions with topic models.
In Peter A. Flach, TijlBie, and Nello Cristianini, editors, Machine Learn-ing and Knowledge Discovery in Databases, volume7524 of Lecture Notes in Computer Science, pages838?841.
Springer Berlin Heidelberg.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR ?99),pages 50?57, Berkeley, California, United States.Ioana Hulpus, Conor Hayes, Marcel Karnstedt, andDerek Greene.
2013.
Unsupervised graph-basedtopic labelling using DBpedia.
In Proceedings of the6th ACM International Conference on Web Searchand Data Mining (WSDM ?13), pages 465?474,Rome, Italy.Kalervo J?arvelin and Jaana Kek?al?ainen.
2002.
Cumu-lated gain-based evaluation of IR techniques.
ACMTrans.
Inf.
Syst., 20(4):422?446.Jey Han Lau, David Newman, Sarvnaz Karimi, andTimothy Baldwin.
2010.
Best topic word selec-tion for topic labelling.
In The 23rd InternationalConference on Computational Linguistics (COLING?10), pages 605?613, Beijing, China.Jey Han Lau, Karl Grieser, David Newman, and Tim-othy Baldwin.
2011.
Automatic labelling of topicmodels.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1536?1545, Portland, Oregon, USA.Davide Magatti, Silvia Calegari, Davide Ciucci, andFabio Stella.
2009.
Automatic Labeling of Top-ics.
In Proceedings of the 9th International Confer-ence on Intelligent Systems Design and Applications(ICSDA ?09), pages 1227?1232, Pisa, Italy.Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.2007.
Automatic Labeling of Multinomial TopicModels.
In Proceedings of the 13th ACM Inter-national Conference on Knowledge Discovery andData Mining (SIGKDD ?07), pages 490?499, SanJose, California.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings of Inter-national Conference on Empirical Methods in Natu-ral Language Processing (EMNLP ?04), pages 404?411, Barcelona, Spain.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The PageRank citationranking: Bringing order to the web.
Technical Re-port 1999-66, Stanford InfoLab.Justin Snyder, Rebecca Knowles, Mark Dredze,Matthew Gormley, and Travis Wolfe.
2013.
Topicmodels and metadata for visualizing text corpora.
InProceedings of the 2013 NAACL-HLT Demonstra-tion Session, pages 5?9, Atlanta, Georgia.
Associa-tion for Computational Linguistics.Vladimir N Vapnik.
1998.
Statistical learning theory.Wiley, New York.636
