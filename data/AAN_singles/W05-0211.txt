Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 69?76, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Evaluating State-of-the-Art Treebank-style Parsers forCoh-Metrix and Other Learning Technology EnvironmentsChristian F. Hempelmann, Vasile Rus, Arthur C. Graesser, and Danielle S. McNamaraInstitute for Intelligent SystemsDepartments of Computer Science and PsychologyThe University of MemphisMemphis, TN 38120, USA{chmplmnn, vrus, a-graesser, dsmcnamr}@memphis.eduAbstractThis paper evaluates a series of freelyavailable, state-of-the-art parsers on astandard benchmark as well as withrespect to a set of data relevant formeasuring text cohesion.
We outlineadvantages and disadvantages of exist-ing technologies and make recommen-dations.
Our performance report usestraditional measures based on a goldstandard as well as novel dimensionsfor parsing evaluation.
To our knowl-edge this is the first attempt to eval-uate parsers accross genres and gradelevels for the implementation in learn-ing technology.1 IntroductionThe task of syntactic parsing is valuable tomost natural language understanding applica-tions, e.g., anaphora resolution, machine trans-lation, or question answering.
Syntactic parsingin its most general definition may be viewed asdiscovering the underlying syntactic structure ofa sentence.
The specificities include the typesof elements and relations that are retrieved bythe parsing process and the way in which theyare represented.
For example, Treebank-styleparsers retrieve a bracketed form that encodesa hierarchical organization (tree) of smaller el-ements (called phrases), while Grammatical-Relations(GR)-style parsers explicitly output re-lations together with elements involved in therelation (subj(John,walk)).The present paper presents an evaluation ofparsers for the Coh-Metrix project (Graesser etal., 2004) at the Institute for Intelligent Systemsof the University of Memphis.
Coh-Metrix is atext-processing tool that provides new methodsof automatically assessing text cohesion, read-ability, and difficulty.
In its present form, v1.1,few cohesion measures are based on syntacticinformation, but its next incarnation, v2.0, willdepend more heavily on hierarchical syntacticinformation.
We are developing these measures.Thus, our current goal is to provide the mostreliable parser output available for them, whilestill being able to process larger texts in realtime.
The usual trade-off between accuracy andspeed has to be taken into account.In the first part of the evaluation, we adopta constituent-based approach for evaluation, asthe output parses are all derived in one way oranother from the same data and generate simi-lar, bracketed output.
The major goal is to con-sistently evaluate the freely available state-of-the-art parsers on a standard data set and acrossgenre on corpora typical for learning technologyenvironments.
We report parsers?
competitive-ness along an array of dimensions including per-formance, robustness, tagging facility, stability,and length of input they can handle.Next, we briefly address particular types ofmisparses and mistags in their relation to mea-sures planned for Coh-Metrix 2.0 and assumedto be typical for learning technology applica-tions.
Coh-Metrix 2.0 measures that centrallyrely on good parses include:causal and intentional cohesion, for which themain verb and its subject must be identified;69anaphora resolution, for which the syntactic re-lations of pronoun and referent must be identi-fied;temporal cohesion, for which the main verb andits tense/aspect must be identified.These measures require complex algorithmsoperating on the cleanest possible sentenceparse, as a faulty parse will lead to a cascad-ing error effect.1.1 Parser TypesWhile the purpose of this work is not to proposea taxonomy of all available parsers, we considerit necessary to offer a brief overview of the var-ious parser dimensions.
Parsers can be classi-fied according to their general approach (hand-built-grammar-based versus statistical), the wayrules in parses are built (selective vs. genera-tive), the parsing algorithm they use (LR, chartparser, etc.
), type of grammar (unification-basedgrammars, context-free grammars, lexicalizedcontext-free grammars, etc.
), the representationof the output (bracketed, list of relations, etc.
),and the type of output itself (phrases vs gram-matical relations).
Of particular interest to ourwork are Treebank-style parsers, i.e., parsersproducing an output conforming to the PennTreebank (PTB) annotation guidelines.
ThePTB project defined a tag set and bracketedform to represent syntactic trees that became astandard for parsers developed/trained on PTB.It also produced a treebank, a collection of hand-annotated texts with syntactic information.Given the large number of dimensions alongwhich parsers can be distinguished, an evalua-tion framework that would provide both parser-specific (to understand the strength of differ-ent technologies) and parser-independent (to beable to compare different parsers) performancefigures is desirable and commonly used in theliterature.1.2 General Parser Evaluation MethodsEvaluation methods can be broadly dividedinto non-corpus- and corpus-based methodswith the latter subdivided into unannotatedand annotated corpus-based methods (Carrollet al, 1999).
The non-corpus method sim-ply lists linguistic constructions covered by theparser/grammar.
It is well-suited for hand-built grammars because during the constructionphase the covered cases can be recorded.
How-ever, it has problems with capturing complex-ities occuring from the interaction of coveredcases.The most widely used corpus-based eval-uation methods are: (1) the constituent-based (phrase structure) method, and (2) thedependency/GR-based method.
The former hasits roots in the Grammar Evaluation InterestGroup (GEIG) scheme (Grishman et al, 1992)developed to compare parsers with different un-derlying grammatical formalisms.
It promotedthe use of phrase-structure bracketed informa-tion and defined Precision, Recall, and Cross-ing Brackets measures.
The GEIG measureswere extended later to constituent information(bracketing information plus label) and havesince become the standard for reporting auto-mated syntactic parsing performance.
Amongthe advantages of constituent-based evaluationare generality (less parser specificity) and finegrain size of the measures.
On the other hand,the measures of the method are weaker than ex-act sentence measures (full identity), and it isnot clear if they properly measure how well aparser identifies the true structure of a sentence.Many phrase boundary mismatches spawn fromdifferences between parsers/grammars and cor-pus annotation schemes (Lin, 1995).
Usually,treebanks are constructed with respect to infor-mal guidelines.
Annotators often interpret themdifferently leading to a large number of differentstructural configurations.There are two major approaches to evaluateparsers using the constituent-based method.
Onthe one hand, there is the expert-only approachin which an expert looks at the output of aparser, counts errors, and reports different mea-sures.
We use a variant of this approach forthe directed parser evaluation (see next section).Using a gold standard, on the other hand, is amethod that can be automated to a higher de-gree.
It replaces the counting part of the formermethod with a software system that comparesthe output of the parser to the gold standard,70highly accurate data, manually parsed ?
or au-tomatically parsed and manually corrected ?
byhuman experts.
The latter approach is moreuseful for scaling up evaluations to large collec-tions of data while the expert-only approach ismore flexible, allowing for evaluation of parsersfrom new perspectives and with a view to spe-cial applications, e.g., in learning technology en-vironments.In the first part of this work we use the goldstandard approach for parser evaluation.
Theevaluation is done from two different points ofview.
First, we offer a uniform evaluation for theparsers on section 23 from the Wall Street Jour-nal (WSJ) section of PTB, the community normfor reporting parser performance.
The goal ofthis first evaluation is to offer a good estimationof the parsers when evaluated in identical en-vironments (same configuration parameters forthe evaluator software).
We also observe the fol-lowing features which are extremely importantfor using the parsers in large-scale text process-ing and to embed them as components in largersystems.Self-tagging: whether or not the parser does tag-ging itself.
It is advantageous to take in raw textsince it eliminates the need for extra modules.Performance: if the performance is in the midand upper 80th percentiles.Long sentences: the ability of the parser to han-dle sentences longer than 40 words.Robustness: relates to the property of a parserto handle any type of input sentence and returna reasonable output for it and not an empty lineor some other useless output.Second, we evaluate the parsers on narrativeand expository texts to study their performanceacross the two genres.
This second evaluationstep will provide additional important resultsfor learning technology projects.
We use evalb(http://nlp.cs.nyu.edu/evalb/) to evaluate thebracketing performance of the output of a parseragainst a gold standard.
The software evaluatorreports numerous measures of which we only re-port the two most important: labelled precision(LR), labelled recall (LR) which are discussed inmore detail below.1.3 Directed Parser Evaluation MethodFor the third step of this evaluation we lookedfor specific problems that will affect Coh-Metrix2.0, and presumably learning technology appli-cations in general, with a view to amendingthem by postprocessing the parser output.
Thefollowing four classes of problems in a sentence?sparse were distinguished:None: The parse is generally correct, unambigu-ous, poses no problem for Coh-Metrix 2.0.One: There was one minor problem, e.g., a mis-labeled terminal or a wrong scope of an adver-bial or prepositional phrase (wrong attachmentsite) that did not affect the overall parse of thesentence, which is therefore still usable for Coh-Metrix 2.0 measures.Two: There were two or three problems of thetype one, or a problem with the tree structurethat affected the overall parse of the sentence,but not in a fatal manner, e.g., a wrong phraseboundary, or a mislabelled higher constituent.Three: There were two or more problems of thetype two, or two or more of the type one aswell as one or more of the type two, or anotherfundamental problem that made the parse of thesentence completely useless, unintelligible, e.g.,an omitted sentence or a sentence split into two,because a sentence boundary was misidentified.2 Evaluated Parsers2.1 Apple PieApple Pie (AP) (Sekine and Grishman, 1995)extracts a grammar from PTB v.2 in which Sand NP are the only true non-terminals (theothers are included into the right-hand side ofS and NP rules).
The rules extracted from thePTB have S or NP on the left-hand side and aflat structure on the right-hand side, for instanceS ?
NP VBX JJ.
Each such rule has the mostcommon structure in the PTB associated withit, and if the parser uses the rule it will gener-ate its corresponding structure.
The parser isa chart parser and factors grammar rules withcommon prefixes to reduce the number of activenodes.
Although the underlying model of theparser is simple, it can?t handle sentences over40 words due to the large variety of linguistic71constructs in the PTB.2.2 Charniak?s ParserCharniak presents a parser (CP) based on prob-abilities gathered from the WSJ part of the PTB(Charniak, 1997).
It extracts the grammar andprobabilities and with a standard context-freechart-parsing mechanism generates a set of pos-sible parses for each sentence retaining the onewith the highest probability (probabilities arenot computed for all possible parses).
The prob-abilities of an entire tree are computed bottom-up.
In (Charniak, 2000), he proposes a gen-erative model based on a Markov-grammar.
Ituses a standard bottom-up, best-first probabilis-tic parser to first generate possible parses beforeranking them with a probabilistic model.2.3 Collins?s (Bikel?s) ParserCollins?s statistical parser (CBP; (Collins,1997)), improved by Bikel (Bikel, 2004), is basedon the probabilities between head-words in parsetrees.
It explicitly represents the parse proba-bilities in terms of basic syntactic relationshipsof these lexical heads.
Collins defines a map-ping from parse trees to sets of dependencies,on which he defines his statistical model.
Aset of rules defines a head-child for each nodein the tree.
The lexical head of the head-child of each node becomes the lexical head ofthe parent node.
Associated with each node isa set of dependencies derived in the followingway.
For each non-head child, a dependency isadded to the set where the dependency is identi-fied by a triplet consisting of the non-head-childnon-terminal, the parent non-terminal, and thehead-child non-terminal.
The parser is a CYK-style dynamic programming chart parser.2.4 Stanford ParserThe Stanford Parser (SP) is an unlexical-ized parser that rivals state-of-the-art lexical-ized ones (Klein and Manning, 2003).
Ituses a context-free grammar with state splits.The parsing algorithm is simpler, the grammarsmaller and fewer parameters are needed for theestimation.
It uses a CKY chart parser whichexhaustively generates all possible parses for asentence before it selects the highest probabil-ity tree.
Here we used the default lexicalizedversion.3 Experiments and Results3.1 Text CorpusWe performed experiments on three data sets.First, we chose the norm for large scale parserevaluation, the 2416 sentences of WSJ section23.
Since parsers have different parameters thatcan be tuned leading to (slightly) different re-sults we first report performance values on thestandard data set and then use same parametersettings on the second data set for more reliablecomparison.The second experiment is on a set of three nar-rative and four expository texts.
The gold stan-dard for this second data set was built manuallyby the authors starting from CP?s as well as SP?soutput on those texts.
The four texts used ini-tially are two expository and two narrative textsof reasonable length for detailed evaluation:The Effects of Heat (SRA Real Science Grade 2Elementary Science): expository; 52 sentences,392 words: 7.53 words/sentence;The Needs of Plants (McGraw-Hill Science):expository; 46 sentences, 458 words: 9.96words/sentence;Orlando (Addison Wesley Phonics Take-HomeReader Grade 2): narrative; 65 sentences, 446words: 6.86 words/sentence;Moving (McGraw-Hill Reading - TerraNova TestPreparation and Practice - Teachers EditionGrade 3): narrative, 33 sentences, 433 words:13.12 words/sentence.An additional set of three texts was cho-sen from the Touchstone Applied Science As-sociates, Inc., (TASA) corpus with an averagesentence length of 13.06 (overall TASA average)or higher.Barron17: expository; DRP=75.14 (collegegrade); 13 sentences, 288 words: 22.15words/sentence;Betty03: narrative; DRP=56.92 (5th grade); 14sentences, 255 words: 18.21 words/sentence;Olga91: expository; DRP=74.22 (college grade);12 sentences, 311 words: 25.92 words/sentence.72We also tested all four parsers for speed on acorpus of four texts chosen randomly from theMetametrix corpus of school text books, acrosshigh and low grade levels and across narrativeand science texts (see Section 3.2.2).G4: 4th grade narrative text, 1,500 sentences,18,835 words: 12.56 words/sentence;G6: 6th grade science text, 1,500 sentences,18,237 words: 12.16 words/sentence;G11: 11th grade narrative text, 1,558 sentences,18,583 words: 11.93 words/sentence;G12: 12th grade science text, 1,520 sentences,25,098 words: 16.51 words/sentence.3.2 General Parser Evaluation Results3.2.1 AccuracyThe parameters file we used for evalb wasthe standard one that comes with the package.Some parsers are not robust, meaning that forsome input they do not output anything, leadingto empty lines that are not handled by the evalu-ator.
Those parses had to be ?aligned?
with thegold standard files so that empty lines are elim-inated from the output file together with theirpeers in the corresponding gold standard files.In Table 1 we report the performance valueson Section 23 of WSJ.
Table 2 shows the resultsfor our own corpus.
The table gives the averagevalues of two test runs, one against the SP-basedgold standard, the other against the CP-basedgold standard, to counterbalance the bias of thestandards.
Note that CP and SP possibly stillscore high because of this bias.
However, CBPis clearly a contender despite the bias, while APis not.1 The reported metrics are Labelled Pre-cision (LP) and Labelled Recall (LR).
Let us de-note by a the number of correct phrases in theoutput from a parser for a sentence, by b thenumber of incorrect phrases in the output andby c the number of phrases in the gold standardfor the same sentence.
LP is defined as a/(a+b)and LR is defined as a/c.
A summary of theother dimensions of the evaluation is offered inTable 3.
A stability dimension is not reported1AP?s performance is reported for sentences < 40words in length, 2,250 out of 2,416.
SP is also not ro-bust enough and the performance reported is only on2,094 out of 2,416 sentences in section 23 of WSJ.because we were not able to find a bullet-proofparser so far, but we must recognize that someparsers are significantly more stable than oth-ers, namely CP and CBP.
In terms of resourcesneeded, the parsers are comparable, except forAP which uses less memory and processing time.The LP/LR of AP is significantly lower, partlydue to its outputting partial trees for longer sen-tences.
Overall, CP offers the best performance.Note in Table 1 that CP?s tagging accuracy isworst among the three top parsers but still de-livers best overall parsing results.
This meansthat its parsing-only performance is slighstlybetter than the numbers in the table indicate.The numbers actually represent the tagging andparsing accuracy of the tested parsing systems.Nevertheless, this is what we would most likelywant to know since one would prefer to inputraw text as opposed to tagged text.
If morefinely grained comparisons of only the parsingaspects of the parsers are required, perfect tagsextracted from PTB must be provided to mea-sure performance.Table 4 shows average measures for each ofthe parsers on the PTB and seven expositoryand narrative texts in the second column andfor expository and narrative in the fourth col-umn.
The third and fifth columns contain stan-dard deviations for the previous columns, re-spectively.
Here too, CP shows the best result.3.2.2 SpeedAll parsers ran on the same Linux Debian ma-chine: P4 at 3.4GHz with 1.0GB of RAM.2 AP?sand SP?s high speeds can be explained to a largedegree by their skipping longer sentences, thevery ones that lead to the longer times for theother two candidates.
Taking this into account,SP is clearly the fastest, but the large range ofprocessing times need to be heeded.3.3 Directed Parser Evaluation ResultsThis section reports the results of expert ratingof texts for specific problems (see Section 1.3).The best results are produced by CP with an av-erage of 88.69% output useable for Coh-Metrix2.0 (Table 6).
CP also produces good output2Some of the parsers also run under Windows.73Table 1: Accuracy of Parsers.Parser Performance(LP/LR/Tagging - %)WSJ 23 Expository NarrativeApplie Pie 43.71/44.29/90.26 41.63/42.70 42.84/43.84Charniak?s 84.35/88.28/92.58 91.91/93.94 93.74/96.18Collins/Bikel?s 84.97/87.30/93.24 82.08/85.35 67.75/85.19Stanford 84.41/87.00/95.05 75.38/85.12 62.65/87.56Table 2: Performance of parsers on the narrative and expository text (average against CP-basedand SP-based gold standard).File Performance (LR/LP - %)AP CP CBP SPHeat 48.25/47.59 91.96/93.77 92.47/94.14 92.44/91.85Plants 41.85/45.89 85.34/88.02 78.24/88.45 81.00/85.62Orlando 45.82/49.03 85.83/91.88 65.87/93.97 57.75/90.72Moving 37.77/41.45 88.93/92.74 53.94/91.68 76.56/84.97Barron17 43.22/42.95 89.74/91.32 80.49/89.32 87.22/86.31Betty03 46.53/44.67 90.77/90.74 87.95/85.21 74.53/80.91Olga91 32.29/32.69 77.65/80.04 61.61/75.43 61.65/70.60Table 3: Evaluation of Parsers with Respect to the Criteria Listed at the Top of Each Column.Parser Self-tagging Performance Long-sentences RobustnessAP Yes No No NoCP Yes Yes Yes YesCBP Yes Yes Yes YesSP Yes Yes No NoTable 4: Average Performance of Parsers.Parser Ave. (LR/LP - %) S.D.
(%) Ave. on S.D.
onExp+Nar (LR/LP - %) Exp+Nar (%)AP 42.73/43.61 1.04/0.82 42.24/43.46 5.59/5.41CP 90.00/92.80 4.98/4.07 87.17/89.79 4.85/4.66CBP 78.27/85.95 9.22/1.17 74.36/88.31 14.24/6.51SP 74.14/86.56 10.93/1.28 75.88/84.42 12.66/7.1174Table 5: Parser Speed in Seconds.G4 G6 G11 G12#sent 619 3336 4976 2215AP 144 89 144 242CP 647 499 784 1406CBP 485 1947 1418 1126SP 449 391 724 651Ave.
431 732 768 856most consistently at a standard deviation overthe seven texts of 8.86%.
The other three candi-dates are clearly trailing behing, namely by be-tween 5% (SP) and 11% (AP).
The distributionof severe problems is comparable for all parsers.Table 6: Average Performance of Parsers overall Texts (Directed Evaluation).Ave.
(%) S.D.
(%)AP 77.31 15.00CP 88.69 8.86CBP 79.82 18.94SP 83.43 11.42As expected, longer sentences are more prob-lematic for all parsers, as can be seen in Ta-ble 7.
No significant trends in performance dif-ferences with respect to genre difference, narra-tive (Orlando, Moving, Betty03) vs. expositorytexts (Heat, Plants, Barron17, Olga91), were de-tected (cf.
also speed results in Table 5).
Butwe assume that the difference in average sen-tence length obscures any genre differences inour small sample.The most common non-fatal problems (typeone) involved the well-documented adjunct at-tachment site issue, in particular for preposi-tional phrases ((Abney et al, 1999), (Brill andResnik, 1994), (Collins and Brooks, 1995)) aswell as adjectival phrases (Table 8)3.
Similarmisattachment issues for adjuncts are encoun-tered with adverbial phrases, but they were rare3PP = wrong attachment site for a prepositionalphrase; ADV = wrong attachment site for an adverbialphrase; cNP = misparsed complex noun phrase; &X =wrong coordinationTable 7: Correlation of Average Performanceper Text for all Parsers and Average SentenceLength (Directed Evaluation).Text perf.
(%) length (#words)Heat 92.31 7.54Plants 90.76 9.96Orlando 93.46 6.86Moving 90.91 13.12Barron17 76.92 22.15Betty03 71.43 18.21Olga91 60.42 25.92in our corpus.Another common problem are deverbal nounsand denominal verbs, as well as -ing/VBGforms.
They share surface forms leading to am-biguous part of speech assignments.
For manyCoh-Metrix 2.0 measures, most obviously tem-poral cohesion, it is necessary to be able to dis-tinguish gerunds from gerundives and deverbaladjectives and deverbal nouns.Table 8: Specific Problems by Parser.PP ADV cNP &XAP 13 10 8 9CP 15 1 2 7CBP 10 0 0 13SP 22 6 3 4Sum 60 17 13 33Problems with NP misidentification are par-ticularly detrimental in view of the impor-tant role of NPs in Coh-Metrix 2.0 mea-sures.
This pertains in particular to the mistag-ging/misparsing of complex NPs and the coor-dination of NPs.
Parses with fatal problemsare expected to produce useless results for algo-rithms operating with them.
Wrong coordina-tion is another notorious problem of parsers (cf.
(Cremers, 1993), (Grootveld, 1994)).
In our cor-pus we found 33 instances of miscoordination,of which 23 involved NPs.
Postprocessing ap-proaches that address these issues are currentlyunder investigation.754 ConclusionThe paper presented the evaluation of freelyavailable, Treebank-style, parsers.
We offereda uniform evaluation for four parsers: ApplePie, Charniak?s, Collins/Bikel?s, and the Stan-ford parser.
A novelty of this work is the evalua-tion of the parsers along new dimensions such asstability and robustness and across genre, in par-ticular narrative and expository.
For the latterpart we developed a gold standard for narrativeand expository texts from the TASA corpus.
Nosignificant effect, not already captured by vari-ation in sentence length, could be found here.Another novelty is the evaluation of the parserswith respect to particular error types that areanticipated to be problematic for a given use ofthe resulting parses.
The reader is invited tohave a closer look at the figures our tables pro-vide.
We lack the space in the present paper todiscuss them in more detail.
Overall, Charniak?sparser emerged as the most succesful candidateof a parser to be integrated where learning tech-nology requires syntactic information from realtext in real time.ACKNOWLEDGEMENTSThis research was funded by Institute for Educa-tions Science Grant IES R3056020018-02.
Anyopinions, findings, and conclusions or recom-mendations expressed in this article are thoseof the authors and do not necessarily reflect theviews of the IES.
We are grateful to Philip M.McCarthy for his assistance in preparing someof our data.ReferencesS.
Abney, R. E. Schapire, and Y.
Singer.
1999.Boosting applied to tagging and pp attachment.Proceedings of the 1999 Joint SIGDAT Confer-ence on Empirical Methods in Natural LanguageProcessing and Very Large Corpora, pages 38?45.D.
M. Bikel.
2004.
Intricacies of collins?
parsingmodel.
Computational Linguistics, 30-4:479?511.E.
Brill and P. Resnik.
1994.
A rule-based approachto prepositional phrase attachment disambigua-tion.
In Proceedings of the 15th International Con-ference on Computational Linguistics.J.
Carroll, E. Briscoe, and A. Sanfilippo, 1999.Parser evaluation: current practice, pages 140?150.
EC DG-XIII LRE EAGLES Document EAG-II-EWG-PR.1.E.
Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
Pro-ceedings of the Fourteenth National Conferenceon Artificial Intelligence, AAAI Press/MIT Press,Menlo Park.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the North-AmericanChapter of Association for Computational Lin-guistics, Seattle, Washington.M.
Collins and J. Brooks.
1995.
Prepositional phraseattachment through a backed-off model.
In Pro-ceedings of the Third Workshop on Very LargeCorpora, Cambridge.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Com-putational Linguistic, Madrid, Spain.C.
Cremers.
1993.
On Parsing Coordination Cate-gorially.
Ph.D. thesis, Leiden University.A.
C. Graesser, D.S.
McNamara, M. M. Louwerse,and Z. Cai.
2004.
Coh-metrix: Analysis of text oncohesion and language.
Behavior Research Meth-ods, Instruments, and Computers, 36-2:193?202.R.
Grishman, C. MacLeod, and J. .
Sterling.
1992.Evaluating parsing strategies using standardizedparse files.
In Proceedings of the Third Conferenceon Applied Natural Language Processing, pages156?161.M.
Grootveld.
1994.
Parsing Coordination Genera-tively.
Ph.D. thesis, Leiden University.D.
Klein and C. Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistic, Sapporo, Japan.D.
Lin.
1995.
A dependency-based method for eval-uating broad-coverage parsers.
Proceedings of In-ternational Joint Conference on Artificial Intelli-gence, pages 1420?1427.A.
Ratnaparkhi, J. Renyar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phraseattachment.
In Proceedings of the ARPA Work-shop on Human Language Technology.S.
Sekine and R. Grishman.
1995.
A corpus-based probabilistic grammar with only two non-terminals.
Proceedings of the International Work-shop on Parsing Technologies, pages 216?223.76
