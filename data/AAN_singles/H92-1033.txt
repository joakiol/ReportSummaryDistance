Vocabulary and Environment Adaptationin Vocabulary-Independent Speech RecognitionHsiao-Wuen Hon Kai-Fu LeeSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213Speech & Language GroupApple Computer, Inc.Cupertino, CA 950141 AbstractIn this paper, we are looking into the adaptation issues ofvocabulary-independent (VI) systems.
Just as with speaker-adaptation i  speaker-independent system, two vocabularyadaptation algorithms \[5\] are implemented in order to tailorthe VI subword models to the target vocabulary.
The firstalgorithm is to generate vocabulary-adapted clustering de-cision trees by focusing on relevant allophones during treegeneration and reduces the VI error rate by 9%.
The secondalgorithm, vocabulary-bias training, is to give the relevantallophones more prominence by assign more weight o themduring Baum-Welch training of the generalized allophonicmodels and reduces the VI error ate by 15%.
Finally, in orderto overcome the degradation caused by the different acousticenvironments u ed for VI training and testing, CDCN andISDCN originally designed for microphone adaptation are in-corporated into our VI system and both reduce the degradationof VI cross-environment recognition by 50%.2 IntroductionIn 89' and 91' DARPA Speech and Natural Language Work-shops \[8, 7\], we have shown that accurate vocabulary-independent (VI) speech recognition is possible.
However,there are many anatomical differences between tasks (vocab-ularies), such as the size of the vocabulary and the frequencyof confusable words., which might affect he acoustic model-ing techniques toachieve optimal performance in vocabulary-dependent (VD) systems.
For example, whole-word modelsare often used in small-vocabulary tasks, while subword mod-els must be used in large-vocabulary tasks.
Moreover, withina limited vocabulary, it is possible to design some special fea-tures to separate the confusable models.
Therefore, discrimi-native training techniques, uch as neural networks \[10\], andmaximum utual information estimator (MMIE) \[4\], have somuch success in small-vocabulary tasks.Just as with speaker adaptation in speaker-independentsystems, it is desirable to implement vocabulary adapta-tion to make the VI system tailored to the target vocabulary(task).
Our first vocabulary adaptation algorithm is to buildvocabulary-adapted allophonic lustering decision trees for168the target vocabulary based on only the relevant allophones.The adapted trees would only focus on the relevant contextsto separate the relevant allophones, thus give the resultingallophonic lusters more discriminative power for the targetvocabulary.
In an experiment of adapting allophone cluster-ing tree for the Resource Management task, this algorithmachieved an 9% error reduction.Our second vocabulary adaptation algorithm is to focuson the relevant allophones during training of generalized allo-phonic models, instead of focusing on them during generationof allophonic lustering decision trees.
To achieve that, wegive the relevant allophones more prominence by assigningmore weight o the relevant allophones during Baum-Welchtraining of generalized allophonic models.
With vocabulary-bias training we are able to reduce the VI error rate by 15%for the Resource Management task.We have found that different recording environments be-tween training and testing (CMU vs. TI) will degrade the per-formance significantly \[6\], even when the same microphoneis used in either case.
Based on the framework of semi-continuous HMMs, we proposed to update codebook proto-types in discrete HMMs in order to fit speech vectors fromnew environments \[5\].
Moreover, codebook-dependent cep-stral normalization (CDCN) and interpolated SNR-dependentcepstral normalization (ISDCN) proposed by Acero et al \[2\]for microphone adaptation are incorporated into the our VIsystem to achieve environmental robustness.
CDCN usesthe speech knowledge represented in a codebook to estimatethe noise and spectral equalization correction vectors for en-vironmental normalization.
In ISDCN, the SNR-dependentcorrection vectors are obtained via EM algorithm to minimizethe VQ distortion.
Both algorithms reduced the degradationof VI cross-environment recognition by 50%.In this paper, we first describe our two vocabulary adap-tation algorithms , vocabulary-adapted decision trees andvocabulary-bias training.
Then we describe the codebookadaptation algorithm and two cepstral normalization tech-niques, CDCN and ISDCN for environmental robustness.
Wewill also present results with these vocabulary and environ-ment adaptation algorithms.
Finally, we will close with someconcluding remark about his work and future work.3 Vocabulary AdaptationUnlike most speaker adaptation techniques, our vocabularyadaptation algorithms only take advantage of analyzing thetarget vocabulary and thus do not require any additionalvocabulary-specific data.
Two terminologies which play anessential role in our algorithms are defined as follows.relevant allophones Those allophones which occur in thetarget vocabulary (task).irrelevant allophones Those allophone which occur in theVI training set, but not in the target vocabulary(task).In 91' DARPA Speech and Natural Language Workshop\[7\], we have shown the decision-tree based generalized allo-phone is a adequate VI subword model.
Figure 1 is an exampleof our VI subword unit, generalized allophone, which is ac-tually an allophonic luster.
The allophones in the white areaare relevant allophones and the rest are irrelevant ones.Figure 1: A generalized allophone (allophonic luster)3.1 Vocabulary.Adapted Decision TreeOur first vocabulary adaptation algorithm is to change theallophone clustering (the decision trees) so that the brandnew set of subword models would have a more discriminativepower for the target vocabulary.
Since the clustering decisiontree was built on the entire VI training set, the existence of theenormous irrelevant aUophones might result in sub-optimallyclustering of allophones for the target vocabulary.To reveal such facts, let's look at the following scenario.Figure 2 is a split in the original decision tree for phone/ k / generated from vocabulary-independent training set andthe associated question for this split is "Is the left context avowel".
Suppose all the left contexts for phone/k /  in thetarget vocabulary are vowels.
Thus, the question for this splitis totally unsuitable for the target vocabulary because the splitassigns all the allophones for /k /  in the target vocabularyto one branch and discrimination among those allophonesbecomes impossible.On the other hand, if only the relevant ailophones are con-sidered for this split, the associated split question would turnsout to be the one of relevant questions which separates therelevant allophones appropriately and therefore possesses thegreatest discriminative ability among the relevant allophones.Figure 3 just shows uch optimal split for relevant allophones.The generation of the clustering decision trees are recursive.The existence of enormous irrelevant allophones would pre-vent he generation ofthe decision trees from concentrating onthose relevant allophones and relevant questions, and resultsin sub-optimal trees for those relevant allophones.Left = Vowel?IN!
irrelevant allophonesrelevant allophnonesFigure 2: An split(question) in the original decision tree forphone / k /Right = Liquid?NN\[~\] relevant allophnonesirrelevant allophonesFigure 3: the correspondent optimal split(question) for rele-vant allophones of phone / k /Based on the analysis, our first adaptation algorithm is tobuild vocabulary-adapted (VA) decision trees by using onlyrelevant allophones during the generation of decision trees.The adapted trees would not only be automatically generated,but also focus on the relevant questions to separate the relevantallophones, therefore give the resulting allophonic lustersmore discriminative power for the target vocabulary.Three potential problems are brought up when one exam-ining the algorithm closely.
First of all, some relevant allo-phones might not occur in the VI training set since we can'texpect 100% allophone coverage for every task, especiallyfor large-vocabulary task.
Nevertheless, it is essential to haveall the models for relevant allophones ready before generatingthe VA decision trees because we need the entropy informa-tion of models for each split.
It is trivial for those relevantallophones which also occur in VI training set.
The correspon-dent allophonic models trained from the training data can be169used directly.
Because of the nature of decision trees, everyallophone could find its closest generalized allophonic lusterby traw~rsing the decision trees.
Therefore, the correspondentgeneralized allophonic models could be used as the modelsfor those relevant allophones not occurring in the VI trainingset during the generation of the VA clustering trees.Secondly, if only the part of VI training set which con-rains the relevant allophones i used to train new generalizedallophonic models, the new adapted generalized allophonicmodels would be under-trained and less robust.
Fortunately,we can retain the entire training set because of the the natureof decision trees.
All the allophones could find their gener-alized allophonic lusters by traversing the new VA decisiontrees, so the entire VI training set could actually contributeto the training of new adapted generalized allophonic modelsand make them well-trained and robust.The entropy criterion for splitting during the generation ofdecision trees is weighted by the counts (frequencies) ofallo-phones \[6\].
By preferring to split nodes with large counts (al-lophones appearing frequently), the counts of the allophoniccluster will become more balanced and the final generalizedallophonic models will be equally trainable.
Since the VA de-cision tress are generated from the set of relevant allophoneswhich is not the same as the set of allophones to train thegeneralized allophonic models.
The balance feature of thosemodels will be no longer valid.
Some generalized allophonicmodels might only have few (or even none) examples in the VItraining set and thus cannot be well-trained.
Fortunately, wecan enhance the trainability of VA subword models throughgross validation with the entire VI training set.
The grossvalidation for VA decision trees is somehow different than theconventional cross validation which uses one part of the datato grow the trees and the other part of independent data toprune the trees in order to predict new contexts.
Since rele-vant allophones i already only a small portion of the entire VItraining set, further dividing it will prevent the learning algo-rithm from generating reliable VA decision trees.
Instead, wegrow the VA decision trees very deeply; replace the entropyreduction information of each split by traversing through thetrees with all the allophones (including irrelevant ones); andfinally prune the trees based on the new entropy informa-tion.
This will prune out those splits of nodes without enoughtraining support (too few examples) even though they mightbe relevant to the target vocabulary.
Therefore the resultinggeneralized allophonic models will become more trainable.The vocabulary-adapted d cision tree learning algorithm,emphasizing the relevant allophones during growing of thedecision trees and using the gross validation with the entire VItraining set provides an ideal mean for finding the equilibriumbetween adaptability for the target vocabulary and trainabilitywith the VI training database.3.2 Vocabulary-Bias TrainingWhile the above adaptation algorithm tailors the subwordunits to the target vocabulary by focusing on the relevant al-lophones during the generation of clustering decision trees,it treated relevant and other irrelevant allophones equally inthe final training of generalized allophonic models.
Our nextadaptation algorithm is to give the relevant allophones moreprominence during the training of generalized allophonicmodels.Since the VI training database is supposed to be very large,it is reasonable to assume that the irrelevant allophones arethe majority of almost every cluster.
Thus, the resulting allo-phonic luster will more likely represent the acoustic behaviorof the set of irrelevant allophones, instead of the set of relevantallophones.In order to make relevant allophones become the majority ofthe allophonic luster without incorporating ew vocabulary-specific data, we must impose a bias toward the relevant al-lophones during training.
Since our VI system is based onHMM approach, it is trivial to give the relevant allophonesmore prominence by assigning more weight o them duringBaum-Welch training.
The simplest way is to multiply aprominent weight o the parametric re-estimation equationsfor relevant allophones.The prominent weight can be a pre-defined constant, like2.0 or 3.0, or a function of some variables.
However, it isbetter for the prominent weight to reflect the reliability ofthe relevant allophones toward which we imposed a bias.If a relevant allophone occur rarely in the training set, weshouldn't assign a large weight o it because the statistics ofit is not reliable.
On the other hand, we could assign largerweights to those relevant allophones with enough examplesin the training data.
In our experiments, we use a simplefunction based on the frequencies of relevant allophones.
Allthe irrelevant allophones have the weight 1.0 and the weightfor relevant allophones i given by the following function:1 + loya(Z) where x is the frequency of relevant allophonesa is chosen to be the minimum number of training examplesto train a reasonable model in our configuration.Imposing abias toward the relevant allophones i similar toduplicating the training data of relevant allophones.
For ex-ample, using aprominent weight of 2.0 for an training examplein the Baum-Welch re-estimation is like observing the sametraining example twice.
Therefore, our vocabulary-bias train-ing algorithm is identical to duplicating the training exam-ples of relevant allophones according to the weight function.Based on the same principle, this adaptation algorithm can beapplied to other non-HMM systems by duplicating the train-ing data of relevant allophones to make relevant allophones170become the majority of the training data during training.
Theresulting models will then be tailored to those relevant aUo-phones.4 Environment Adaptat ionIt is well known that when a system is trained and tested underdifferent environments, he performance ofrecognition dropsmoderately \[8\] However, it is very likely for training and test-ing taking place under different environments in VI systemsbecause the VI models can be used for any task which couldhappen anywhere.
Even if the recording hardware r mains un-changed, e.g., microphones, A/D converters, pre-amplifiers,etc, the other environmental f ctors, e.g.
the room size, back-ground noise, positions of microphones, reverberation fromsurface reflections, etc, are all out of the control realm.
For ex-ample, when comparing the recording environment ofTexasInstruments (TI) and Carnegie Mellon University (CMU), afew differences were observed although both used the sameclose-talk microphone (Sennheiser HMD-414).?
Recording equipment - TI and CMU used different A/Ddevices, filters and pre-amplifiers which might changethe overall transfer function and thus generate differentspectral tilts on speech signals.?
Room - The TI recording took place in a sound-proofroom, while the CMU recording took place in a big labo-ratory with much background noise (mostly paper ustle,keyboard noise, and other conversations).
Therefore;CMU's data tends to contain more additive noise thanTI's.?
Input level - The CMU recording process always ad-justed the amplifier's gain control for different speak-ers to compensate he varied sound volume of speakers.Since the sound volume of TI's female speakers tends tobe much lower, TI probably didn't adjust he gain controllike CMU did.
Therefore, the dynamic range of CMU'sdata tends to be larger.4.1 Codebook Adaptat ionThe speech signal processing of our VI system is based on acharacterization of speech in a codebook of prototypical moO-els \[7\].
Typically the performance of systems based on a code-book degrade over time as the speech signal drifts through en-vironmental changes due to the increased istortion betweenthe speech and the codebook.Therefore, two possible adaptation strategies include:1. continuously updating the cooebook prototypes tofit thetesting speech spectral vectors xt.2.
continuously transforming the testing speech spectralvectors x, into normalized vectors Yi, so that the dis-tribution of the y~ is close to that of the training datadescribed by the codebook prototypes.Our first environment adaptation algorithm belongs to the firststrategy, while two cepstral normalization algorithms whichwill be described in Section 4.2 belongs to the second strategy.Semi-continuous HMMs (SCHMMs) or tied mixture con-tinuous HMMs \[9, 3\] has been proposed to extend the dis-crete HMMs by replacing discrete output distributions with acombination of the original discrete output probability distri-butions and continuous pdf's of codebooks.
SCHMMs canjointly re-estimate both the codebooks and HMM parametersto achieve an optimal codebook/model combination accordingto a maximum likelihood criterion during training.
They havebeen applied to several recognition systems with improvedperformance over discrete HMMs \[9, 3\].The cooebooks of our vocabulary-independent system canbe modified to optimize the probability of generating datafrom new environment bythe vocabulary-independent HMMsaccording to the SCHMM framework.
Let #i denote the meanvector of cooebook index i in the original codebook, then thenew vector ~ can be obtained from the following equation- E (cT=  (1)where 7~ (t) denotes the posterior probability observed thecodeword i at time t using HMM m for speech vector xt.Note that we did not use continuous Gassian pdf's to rep-resent he cooebooks in the Equation 1.
Each mean vec-tor of the new codebook is computed from acoustic vectorxt associated with corresponding posterior probability in thediscrete forward-backward algorithm without involving con-tinuous pdf computation.
The new data from different envi-ronment, xt, can be automatically aligned with correspondingcodeword in the forward-backward t aining procedure.
If thealignment is not closely associated with the correspondingcodeword in the HMM training procedure, reestimation ofthe corresponding codeword will then be de-weighted by theposterior probability 7~ n(t) accordingly inorder to adjust henew cooebook to fit the new data.4.2 Cepst ra l  Normal i za t ionThe types of environmental f ctors which differ in TI's andCMU's recording environments can roughly be classified intotwo complementary categories :1. additive noise - noise from different sources, like paperrustle, keyboard noise, other conversations, etc.1712.
spectral equalization - distortions from the convolutionof the speech signal with an unknown channel, ike posi-tions of microphones, reverberation from surface reflec-tions, etc.Acero at al.
\[1,2\] proposed a series of environment ormal-ization algorithms based on joint compensation for additivenoise and equaliTation.
They has been implemented success-fully on SPHINX to achieve robustness to different micro-phones.
Among those algorithms, codeword-dependent cep-stral normalization (CDCN), is the most accurate one, whileinterpolated SNR-dependent cepstral normalization (ISDCN)is the most efficient one 1.
In this study, we incorporate hesetwo algorithms to make our vocabulary-independent sys emmore robust o environmental variations.x = z -  w(q ,n )  (2)Equation 2 is the environmental compensation model,where x, z, w, q and n represent respectively the normalizedvector, observed vector, correction vector, spectral equaliza-tion vector and noise vector.
The CDCN algorithm attemptsto determine q and n that provide an ensemble of compen-sated vectors x being collectively closest to the set of locationsof legitimate VQ codewords.
The correction vector w willbe obtained using MMSE estimator based on q, n and thecodebook.
In ISDCN, q and n were determined by an EMalgorithm aiming at minimizing VQ distortion.
The final cor-rection vector w also depends on the instantaneous SNR ofthe current input frame using a sigmoid function.Condition Error Rate Error ReductionBaseline 5.4% N/A%+VA decision trees 4.9% 9.3%+VB training 4.6% 14.8%+VA trees & VB training 4.6% 14.8%Table 1: The results for Resource Management usingvocabulary-adapted d cision trees and vocabulary-bias train-ing algorithmsto further tailor the vocabulary-independent models to theResource Management task, no compound improvement wasproduced.
It might be because ither both algorithms arelearning the similar characteristics of the target ask, or thecombination ofthese two algorithms already reaches the limi-tation of adaptation capability within our modeling techniquewithout he help of vocabulary-specific data.Adaptation Sentence CMU-TEST TI-TESTBaseline 5.4% 7.4%100 N/A 7.1%300 N/A 7.0%1000 N/A 7.0%2000 N/A 6.9%Table 2: The vocabulary-independent r sults on TI-TEST byadapting the codebooks for TI's data5 Experiments and ResultsAll the experiments are evaluated on the speaker-independentDARPA resource management task.
This task is a 991-wordcontinuous task and a standard word-pair grammar with per-plexity 60 was used throughout.
The test set, TI .TEST, con-sists of 320 sentences from 32 speakers (a random selectionfrom June 1988, February 1989 and October 1990 DARPAevaluation sets).In order to isolate the influence of cross-environment recog-nition, another identical same test set, CMU-TEST, from32 speakers (different from TI speakers) was collected atCMU.
Our baseline is using 4-codebook discrete SPHINXand decision-tree based generalized allophones as the VI sub-word units\[7\].
Table 1 shows that about 9% error reductionis achieved by adapting the decision trees for Resource Man-agement task, while about 15% error eduction is achieved byusing vocabulary-bias training for the same task.
Neverthe-less, when we try to combine these two adaptation algorithms1The reader isreferred to\[1\] for detailed CDCN and ISDCN algorithms172In codebook adaptation experiments, the 4 codebooks usedin our HMM-based system are updated according Equation1.
We randomly select 100, 300, 1000, 2000 sentences fromTIRM database to form different adaptation sets.
Two iter-ation were carried out for each adaptation sets to estimatedthe new codebooks for TI's data, while the HMM parametersare fixed.
Table 2 shows the adaptation recognition result onTI testing set.
It is indicated that only marginal improvementby adapting codebook for new environment even with lots ofadaptation data.
The result suggested that the adaptation ofcodebook alone fail to produce adequate adaptation becausethe HMM statistics used by recognizer have not been updated.Table 3 shows the recognition error rate on two test sets forVI systems incorporated with CDCN and ISDCN.
Be awarethat our VI training set was recorded at CMU.
The degradationof cross-environment recognition with TI -TEST is roughlyreduced by 50%.
Like most environment ormalization al-gorithms, there is also a minor performance degradation forsame-environment recognition when gaining robustness toother environments.Test Set CMU-TEST TI-TESTBaseline 5.4% 7.4%CDCN 5.6% 6A%ISDCN 5.7% 6.5%Table 3: The results for environment ormalization usingCDCN & ISDCN6 ConclusionsIn this paper, we have presented two vocabulary adaptationalgonthms, including vocabulary-adapted decision trees andvocabulary-bias training, that improve the performance ofthe vocabulary-independent system on the target ask by tai-loring the VI subword models to he target vocabulary.
In91' DARPA Speech and Natural Language Workshop \[7\], wehave shown that our VI system is already slightly better thanour VD system.
With these two adaptation algorithms whichled to 9% and 15% error reduction respectively on ResourceManagement task, the resulting VI system is far more ac-curate than our VD system.
In \[8\], we have demonstratedimproved vocabulary-independent r sults with vocabulary-specific adaptation data.
In the future, we plan to extend ouradaptation algorithms with the help of vocabulary-specificdata to achieve further adaptation with the target vocabulary(task).CDCN and ISDCN have been successfully incorporatedto the vocabulary-independent system and reduce the degra-dation of VI cross-environment recognition by 50%.
In thefuture, we will keep investigating new environment ormal-ization techniques to further educe the degradation and ulti-mately achieve the full environmental robustness across dif-ferent acoustic environments.
Moreover, environment adap-tation with environment-specific data will also be exploredfor adapting the VI system to the new environment once wehave more knowledge about it.To make the speech recognition system more robust fornew vocabularies and new environments is essential to makethe speech recognition application feasible.
Our results haveshown that plentiful training data, careful subword model-ing (decision-tree based generalized allophones) and suit-able environment ormalization have compensated for thelack of vocabulary and environment specific training.
Withthe additional help of vocabulary adaptation, the vocabulary-independent system can be further tailored to any task quicklyand cheaply, and therefore facilitates peech applicationstremendously.173AcknowledgementsThis research was sponsored by the Defense Advanced ResearchProjects Agency (DOD), Arpa Order No.
5167, under contractnumber N00039-85-C-0163.
The authors would like to express theirgratitude to Professor Raj Reddy and CMU speech research groupfor their support.References\[1\] Acero, A. Acoustical and Environmental Robustness inAuto-matic Speech Recognition.
Department ofElectrical Engineer-ing, Carnegie-Mellon University, September 1990.\[2\] Acero, A. and Stem, R. Environmental Robustness inAuto-matic Speech Recognition.
in: IEEE International Confer-ence on Acoustics, Speech, and Signal Processing.
1990,pp.
849-852.\[3\] Bellegarda, I. and Nahamoo, D. Tied Mixture Continuous Pa-rameter Models for Large Vocabulary Isolated Speech Recog-nition, in: IEEE International Conference on Acoustics,Speech, and Signal Processing.
1989, pp.
13-16.\[4\] Brown, P. The Acoustic-Modeling Problem in AutomaticSpeech Recognition.
Computer Science Department, CarnegieMellon University, May 1987.\[5\] Hon, H. Vocabulary-lndependentSpeech Recognition: : TheVOCIND System.
School of Computer Science, Carnegie Mel-lon University, February 1992.\[6\] Hon, H. and Lee, K. CMU Robust Vocabulary-IndependentSpeech Recognition System.
in: IEEE International Confer-ence on Acoustics, Speech, and Signal Processing.
Toronto,Ontario, CANADA, 1991, pp.
889-892.\[7\] Hon, H. and Lee, K. Recent Progress in Robust Vocabulary-Independent Speech Recognition.
in: DARPA Speech andLanguage Workshop.
Morgan Kaufmann Publishers, Asilo-mar, CA, 1991.\[8\] Hon, H. and Lee, K. Towards Speech Recognition WithoutVocabulary-Speci~c Training.
in: DARPA Speech and Lan-guage Workshop.
Morgan Kaufmann Publishers, Cape Cod,MA, 1989.\[9\] Huang, X., Lee, K., and Hon, H. On Semi-Continuous HiddenMarkov Modeling.
in: IEEE International Conference onAcoustics, Speech, and Signal Processing.
Albuquerque,NM, 1990, pp.
689-692.\[10\] Walbel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang,K.
Phoneme Recognition using Time-Delay Neural Networks.IEEE Transactions on Acoustics, Speech, and Signal Pro-cessing, vol.
ASSP-28 (1989), pp.
357-366.
