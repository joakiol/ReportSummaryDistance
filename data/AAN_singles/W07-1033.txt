BioNLP 2007: Biological, translational, and clinical language processing, pages 209?216,Prague, June 2007. c?2007 Association for Computational LinguisticsReranking for Biomedical Named-Entity RecognitionKazuhiro Yoshida?
Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo?School of Informatics, University of Manchester?National Center for Text MiningHongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN{kyoshida, tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper investigates improvement of au-tomatic biomedical named-entity recogni-tion by applying a reranking method to theCOLING 2004 JNLPBA shared task of bio-entity recognition.
Our system has a com-mon reranking architecture that consists of apipeline of two statistical classifiers whichare based on log-linear models.
The ar-chitecture enables the reranker to take ad-vantage of features which are globally de-pendent on the label sequences, and fea-tures from the labels of other sentences thanthe target sentence.
The experimental re-sults show that our system achieves the la-beling accuracies that are comparable to thebest performance reported for the same task,thanks to the 1.55 points of F-score improve-ment by the reranker.1 IntroductionDifficulty and potential application of biomedicalnamed-entity recognition has attracted many re-searchers of both natural language processing andbioinformatics.
The difficulty of the task largelystems from a wide variety of named entity expres-sions used in the domain.
It is common for practi-cal protein or gene databases to contain hundreds ofthousands of items.
Such a large variety of vocab-ulary naturally leads to long names with productiveuse of general words, making the task difficult to besolved by systems with naive Markov assumption oflabel sequences, because such systems must performtheir prediction without seeing the entire string ofthe entities.Importance of the treatment of long names mightbe implicitly indicated in the performance com-parison of the participants of JNLPBA sharedtask (Kim et al, 2004), where the best perform-ing system (Zhou and Su, 2004) attains their scoresby extensive post-processing, which enabled thesystem to make use of global information of theentity labels.
After the shared task, many re-searchers tackled the task by using conditional ran-dom fields (CRFs) (Lafferty et al, 2001), whichseemed to promise improvement over locally opti-mized models like maximum entropy Markov mod-els (MEMMs) (McCallum et al, 2000).
However,many of the CRF systems developed after the sharedtask failed to reach the best performance achievedby Zhou et al One of the reasons may be the defi-ciency of the dynamic programming-based systems,that the global information of sequences cannot beincorporated as features of the models.
Another rea-son may be that the computational complexity ofthe models prevented the developers to invent ef-fective features for the task.
We had to wait untilTsai et al (2006), who combine pattern-based post-processing with CRFs, for CRF-based systems toachieve the same level of performance as Zhou et alAs such, a key to further improvement of the perfor-mance of bio-entity recognition has been to employglobal features, which are effective to capture thefeatures of long names appearing in the bio domain.In this paper, we use reranking architecture,which was successfully applied to the task of nat-ural language parsing (Collins, 2000; Charniak and209Johnson, 2005), to address the problem.
Rerankingenables us to incorporate truly global features to themodel of named entity tagging, and we aim to real-ize the state-of-the-art performance without depend-ing on rule-based post-processes.Use of global features in named-entity recogni-tion systems is widely studied for sequence labelingincluding general named-entity tasks like CoNLL2003 shared task.
Such systems may be classifiedinto two kinds, one of them uses a single classifierwhich is optimized incorporating non-local features,and the other consists of pipeline of more than oneclassifiers.
The former includes Relational MarkovNetworks by Bunescu et al (2004) and skip-edgeCRFs by Sutton et al (2004).
A major drawbackof this kind of systems may be heavy computationalcost of inference both for training and running thesystems, because non-local dependency forces suchmodels to use expensive approximate inference in-stead of dynamic-programming-based exact infer-ence.
The latter, pipelined systems include a re-cent study by Krishnan et al (2006), as well asour reranking system.
Their method is a two stagemodel of CRFs, where the second CRF uses theglobal information of the output of the first CRF.Though their method is effective in capturing var-ious non-local dependencies of named entities likeconsistency of labels, we may be allowed to claimthat reranking is likely to be more effective in bio-entity tagging, where the treatment of long entitynames is also a problem.This paper is organized as follows.
First, webriefly overview the JNLPBA shared task of bio-entity recognition and its related work.
Then we ex-plain the components of our system, one of which isan MEMM n-best tagger, and the other is a rerankerbased on log-linear models.
Then we show the ex-periments to tune the performance of the system us-ing the development set.
Finally, we compare ourresults with the existing systems, and conclude thepaper with the discussion for further improvementof the system.2 JNLPBA shared task and related workThis section overviews the task of biomedical namedentity recognition as presented in JNLPBA sharedtask held at COLING 2004, and the systems thatwere successfully applied to the task.
The train-ing data provided by the shared task consisted of2000 abstracts of biomedical articles taken from theGENIA corpus version 3 (Ohta et al, 2002), whichconsists of the MEDLINE abstracts with publicationyears from 1990 to 1999.
The articles are annotatedwith named-entity BIO tags as an example shown inTable 1.
As usual, ?B?
and ?I?
tags are for beginningand internal words of named entities, and ?O?
tagsare for general English words that are not named en-tities.
?B?
and ?I?
tags are split into 5 sub-labels,each of which are used to represent proteins, genes,cell lines, DNAs, cell types, and RNAs.
The testset of the shared task consists of 404 MEDLINE ab-stracts whose publication years range from 1978 to2001.
The difference of publication years betweenthe training and test sets reflects the organizer?s in-tention to see the entity recognizers?
portability withregard to the differences of the articles?
publicationyears.Kim et al (Kim et al, 2004) compare the 8 sys-tems participated in the shared task.
The systemsuse various classification models including CRFs,hidden Markov models (HMMs), support vector ma-chines (SVMs), and MEMMs, with various featuresand external resources.
Though it is impossible toobserve clear correlation between the performanceand classification models or resources used, an im-portant characteristic of the best system by Zhou etal.
(2004) seems to be extensive use of rule-basedpost processing they apply to the output of their clas-sifier.After the shared task, several researchers tack-led the problem using the CRFs and their ex-tensions.
Okanohara et al (2006) applied semi-CRFs (Sarawagi and Cohen, 2004), which can treatmultiple words as corresponding to a single state.Friedrich et al (2006) used CRFs with features fromthe external gazetteer.
Current state-of-the-art forthe shared-task is achieved by Tsai et al (2006),whose improvement depends on careful design offeatures including the normalization of numeric ex-pressions, and use of post-processing by automati-cally extracted patterns.210IL-2 gene expression requires reactive oxygen production by 5-lipoxygenase .B-DNA I-DNA O O O O O O B-protein OFigure 1: Example sentence from the training data.State name Possible next stateBOS B-* or OB-protein I-protein, B-* or OB-cell type I-cell type, B-* or OB-DNA I-DNA, B-* or OB-cell line I-cell line, B-* or OB-RNA I-RNA, B-* or OI-protein I-protein, B-* or OI-cell type I-cell type, B-* or OI-DNA I-DNA, B-* or OI-cell line I-cell line, B-* or OI-RNA I-RNA, B-* or OO B-* or OTable 1: State transition of MEMM.3 N-best MEMM taggerAs our n-best tagger, we use a first order MEMMmodel (McCallum et al, 2000).
Though CRFs (Laf-ferty et al, 2001) can be regarded as improved ver-sion of MEMMs, we have chosen MEMMs becauseMEMMs are usually much faster to train comparedto CRFs, which enables extensive feature selection.Training a CRF tagger with features selected us-ing an MEMM may result in yet another perfor-mance boost, but in this paper we concentrate on theMEMM as our n-best tagger, and consider CRFs asone of our future extensions.Table 1 shows the state transition table of ourMEMM model.
Though existing studies suggestthat changing the tag set of the original corpus, suchas splitting of O tags, can contribute to the perfor-mances of named entity recognizers (Peshkin andPfefer, 2003), our system uses the original tagsetof the training data, except that the ?BOS?
label isadded to represent the state before the beginning ofsentences.Probability of state transition to the i-th label of asentence is calculated by the following formula:P (li|li?1, S) =exp(?j ?jfj(li, li?1, S))?l exp(?j ?jfj(l, li?1, S)).
(1)Features used Forward tagging Backward taggingunigrams, bi-grams and pre-vious labels(62.43/71.77/66.78) (66.02/74.73/70.10)unigrams andbigrams (61.64/71.73/66.30) (65.38/74.87/69.80)unigrams andprevious labels (62.17/71.67/66.58) (65.59/74.77/69.88)unigrams (61.31/71.81/66.15) (65.61/75.25/70.10)Table 2: (Recall/Precision/F-score) of forward andbackward tagging.where li is the next BIO tag, li?1 is the previousBIO tag, S is the target sentence, and fj and ljare feature functions and parameters of a log-linearmodel (Berger et al, 1996).
As a first order MEMM,the probability of a label li is dependent on the pre-vious label li?1, and when we calculate the normal-ization constant in the right hand side (i.e.
the de-nominator of the fraction), we limit the range of l tothe possible successors of the previous label.
Thisprobability is multiplied to obtain the probability ofa label sequence for a sentence:P (l1...n|S) =?iP (li|li?1).
(2)The probability in Eq.
1. is estimated as a singlelog-linear model, regardless to the types of the targetlabels.N-best tag sequences of input sentences are ob-tained by well-known combination of the Viterbi al-gorithm and A* algorithm.
We implemented twomethods for thresholding the best sequences: N -best takes the sequences whose ranks are higher thanN , and ?-best takes the sequences that have proba-bility higher than that of the best sequences with afactor ?, where ?
is a real value between 0 and 1.
The?-best method is used in combination with N -best tolimit the maximum number of selected sequences.3.1 Backward taggingThere remains one significant choice when we de-velop an MEMM tagger, that is, the direction of tag-ging.
The results of the preliminary experiment with211forward and backward MEMMs with word unigramand bigram features are shown in Table 2.
(The eval-uation is done using the same training and develop-ment set as used in Section 5.)
As can be seen, thebackward tagging outperformed forward tagging bya margin larger than 3 points, in all the cases.One of the reasons of these striking differencesmay be long names which appear in biomedicaltexts.
In order to recognize long entity names, for-ward tagging is preferable if we have strong clues ofentities which appear around their left boundaries,and backward tagging is preferable if clues appearat right boundaries.
A common example of this ef-fect is a gene expression like ?XXX YYY gene.?
Theright boundary of this expression is easy to detectbecause of the word ?gene.?
For a backward tagger,the remaining decision is only ?where to stop?
theentity.
But a forward tagger must decide not only?where to start,?
but also ?whether to start?
the en-tity, before the tagger encounter the word ?gene.?
Inbiomedical named-entity tagging, right boundariesare usually easier to detect, and it may be the reasonof the superiority of the backward tagging.We could have partially alleviated this effect byemploying head-word triggers as done in Zhou etal.
(2004), but we decided to use backward tag-ging because the results of a number of preliminaryexperiments, including the ones shown in Table 2above, seemed to be showing that the backward tag-ging is preferable in this task setting.3.2 Feature setIn our system, features of log-linear models are gen-erated by concatenating (or combining) the ?atomic?features, which belong to their corresponding atomicfeature classes.
Feature selection is done by de-ciding whether to include combination of featureclasses into the model.
We ensure that features in thesame atomic feature class do not co-occur, so that asingle feature-class combination generates only onefeature for each event.
The following is a list ofatomic feature classes implemented in our system.Label features The target and previous labels.
Wealso include the coarse-grained label distinction todistinguish five ?I?
labels of each entity classes fromthe other labels, expecting smoothing effect.Word-based features Surface strings, base forms,parts-of-speech (POSs), word shapes1, suffixes andprefixes of words in input sentence.
These featuresare extracted from five words around the word to betagged, and also from the words around NP-chunkboundaries as explained bellow.Chunk-based features Features dependent on theoutput of shallow parser.
Word-based features ofthe beginning and end of noun phrases, and the dis-tances of the target word from the beginning and endof noun phrases are used.4 RerankerOur reranker is based on a log-linear classifier.Given n-best tag sequences Li(1 ?
i ?
n), a log-linear model is used to estimate the probabilityP (Li|S) =exp(?j ?jfj(Li, S))?k exp(?j ?jfj(Lk, S)).
(3)From the n-best sequences, reranker selects a se-quence which maximize this probability.The features used by the reranker are explained inthe following sections.
Though most of the featuresare binary-valued (i.e.
the value of fj in Eq.
3. isexclusively 1 or 0), the logarithm of the probabilityof the sequence output by the n-best tagger is alsoused as a real-valued feature, to ensure the reranker?simprovement over the n-best tagger.4.1 Basic featuresBasic features of the reranker are straightforward ex-tension of the features used in the MEMM tagger.The difference is that we do not have to care the lo-cality of the features with regard to the labels.Characteristics of words that are listed as word-based features in the previous section is also usedfor the reranker.
Such features are chiefly extractedfrom around the left and right boundaries of entities.In our experiments, we used five words around theleftmost and rightmost words of the entities.
We alsouse the entire string, affixes, word shape, concatena-tion of POSs, and length of entities.
Some of our1The shape of a word is defined as a sequence of charactertypes contained in the word.
Character types include uppercaseletters, lowercase letters, numerics, space characters, and theother symbols.212features depend on two adjacent entities.
Such fea-tures include the word-based features of the wordsbetween the entities, and the verbs between the en-tities.
Most of the features are used in combinationwith entity types.4.2 N-best distribution featuresN-best tags of sentences other than the target sen-tence is available to the rerankers.
This informationis sometimes useful for recognizing the names inthe target sentence.
For example, proteins are oftenwritten as ?XXX protein?
where XXX is a proteinname, especially when they are first introduced in anarticle, and thereafter referred to simply as ?XXX.
?In such cases, the first appearance is easily identifiedas proteins only by local features, but the subsequentones might not, and the information of the first ap-pearance can be effectively used to identify the otherappearances.Our system uses the distribution of the tags ofthe 20 neighboring sentences of the target sentenceto help the tagging of the target sentence.
Tagdistributions are obtained by marginalizing the n-best tag sequences.
Example of an effective featureis a binary-valued feature which becomes 1 whenthe candidate entity names in the target sentence iscontained in the marginal distribution of the neigh-boring sentences with a probability which is abovesome threshold.We also use the information of overlappingnamed-entity candidates which appear in the targetsentence.
When there is an overlap between the en-tities in the target sequence and any of the named-entity candidates in the marginal distribution of thetarget sentence, the corresponding features are usedto indicate the existence of the overlapping entityand its entity type.5 ExperimentsWe evaluated the performance of the system on thedata set provided by the COLING 2004 JNLPBAshared-task.
which consists of 2000 abstracts fromthe MEDLINE articles.
GENIA tagger 2, a biomed-ical text processing tool which automatically anno-2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/.
Thetagger is trained on the GENIA corpus, so it is likely to showvery good performance on both training and development sets,but not on the test set.Features used (Recall/Precision/F-score)full set (73.90/77.58/75.69)w/o shallow parser (72.63/76.35/74.44)w/o previous labels (72.06/75.38/73.68)Table 3: Performance of MEMM tagger.tates POS tags, shallow parses and named-entity tagsis used to preprocess the corpus, and POS and shal-low parse information is used in our experiments.We divided the data into 20 contiguous andequally-sized sections, and used the first 18 sec-tions for training, and the last 2 sections for testingwhile development (henceforth the training and de-velopment sets, respectively).
The training data ofthe reranker is created by the n-best tagger, and ev-ery set of 17 sections from the training set is usedto train the n-best tagger for the remaining section(The same technique is used by previous studiesto avoid the n-best tagger?s ?unrealistically good?performance on the training set (Collins, 2000)).Among the n-best sequences output by the MEMMtagger, the sequence with the highest F-score is usedas the ?correct?
sequence for training the reranker.The two log-linear models for the MEMM taggerand reranker are estimated using a limited-memoryBFGS algorithm implemented in an open-sourcesoftware Amis3.
In both models, Gaussian prior dis-tributions are used to avoid overfitting (Chen andRosenfeld, 1999), and the standard deviations of theGaussian distributions are optimized to maximizethe performance on the development set.
We alsoused a thresholding technique which discards fea-tures with low frequency.
This is also optimized us-ing the development set, and the best threshold was4 for the MEMM tagger, and 50 for the reranker 4.For both of the MEMM tagger and reranker, com-binations of feature classes are manually selected toimprove the accuracies on the development set.
Ourfinal models include 49 and 148 feature class combi-nations for the MEMM tagger and reranker, respec-tively.Table 3 shows the performance of the MEMMtagger on the development set.
As reported in many3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/.4We treated feature occurrences both in positive and nega-tive examples as one occurrence.213Features used (Recall/Precision/F-score)oracle (94.62/96.07/95.34)full set (75.46/78.85/77.12)w/o features thatdepend on twoentities(74.67/77.99/76.29)w/o n-best distribu-tion features(74.99/78.38/76.65)baseline (73.90/77.58/75.69)Table 4: Performance of the reranker.of the previous studies (Kim et al, 2004; Okanoharaet al, 2006; Tzong-Han Tsai et al, 2006), features ofshallow parsers had a large contribution to the per-formance.
The information of the previous labelswas also quite effective, which indicates that labelunigram models (i.e.
0th order Markov models, soto speak) would have been insufficient for good per-formance.Then we developed the reranker, using the resultsof 50-best taggers as training data.
Table 4 shows theperformance of the reranker pipelined with the 50-best MEMM tagger, where the ?oracle?
row showsthe upper bound of reranker performance.
Here, wecan observe that the reranker successfully improvedthe performance by 1.43 points from the baseline(i.e.
the one-best of the MEMM tagger).
It is alsoshown that the global features that depend on twoadjacent entities, and the n-best distribution featuresfrom the outside of the target sentences, are bothcontributing to this performance improvement.We also conducted experimental comparison oftwo thresholding methods which are described inSection 3.
Since we can train and test the rerankerwith MEMM taggers that use different thresholdingmethods, we could make a table of the performanceof the reranker, changing the MEMM tagger usedfor both training and evaluation5.Tables 5 and 6 show the F-scores obtained byvarious MEMM taggers, where the ?oracle?
columnagain shows the performance upper bound.
(Allof the ?-best methods are combined with 200-bestthresholding.)
Though we can roughly state that thereranker can work better with n-best taggers which5These results might not be a fair comparison, because thefeature selection and hyper-parameter tuning are done using areranker which is trained and tested with a 50-best tagger.are more ambiguous than those used for their train-ing, the differences are so slight to see clear ten-dencies (For example, the columns for the rerankertrained using the 10-best MEMM tagger seems to bea counter example against the statement).We may also be able to say that the ?-best meth-ods are generally performing slightly better, and itcould be explained by the fact that we have bet-ter oracle performance with less ambiguity in ?-bestmethods.However, the scores in the column correspondingto the 50-best training seems to be as high as any ofthe scores of the ?-best methods, and the best scoreis also achieved in that column.
The reason may bebecause our performance tuning is done exclusivelyusing the 50-best-trained reranker.
Though we couldhave achieved better performance by doing featureselection and hyper-parameter tuning again using ?-best MEMMs, we use the reranker trained on 50-best tags run with 70-best MEMM tagger as the bestperforming system in the following.5.1 Comparison with existing systemsTable 7 shows the performance of our n-best tag-ger and reranker on the official test set, and the bestreported results on the same task.
As naturally ex-pected, our system outperformed the systems thatcannot accommodate truly global features (Note thatone point of F-score improvement is valuable in thistask, because inter-annotator agreement rate of hu-man experts in bio-entity recognition is likely to beabout 80%.
For example, Krauthammer et al (2004)report the inter-annotater agreement rate of 77.6%for the three way bio-entity classification task.)
andthe performance can be said to be at the same level asthe best systems.
However, in spite of our effort, oursystem could not outperform the best result achievedby Tsai et al What makes Tsai et al?s system per-form better than ours might be the careful treatmentof numeric expressions.It is also notable that our MEMM tagger scored71.10, which is comparable to the results of the sys-tems that use CRFs.
Considering the fact that thetagger?s architecture is a simple first-order MEMMwhich is far from state-of-the-art, and it uses onlyPOS taggers and shallow parsers as external re-sources, we can say that simple machine-learning-based method with carefully selected features could214Thresholding method for trainingThresholdingmethod fortestingoracle avg.
# of an-swers10-best 20-best 30-best 40-best 50-best 70-best 100-best10-best 91.00 10 76.51 76.53 76.85 76.73 77.01 76.68 76.8620-best 93.31 20 76.40 76.55 76.83 76.62 76.95 76.68 76.8530-best 94.40 30 76.34 76.52 76.91 76.63 77.06 76.75 76.9040-best 94.94 40 76.39 76.58 76.91 76.71 77.14 76.75 76.9250-best 95.34 50 76.37 76.58 76.90 76.65 77.12 76.78 76.9270-best 95.87 60 76.38 76.57 76.91 76.71 77.16 76.81 76.97100-best 96.26 70 76.38 76.59 76.95 76.74 77.10 76.82 76.98Table 5: Comparison of the F-scores of rerankers trained and evaluated with various N -best taggers.Thresholding method for trainingThresholdingmethod fortestingoracleavg.
#of an-swers0.05-best 0.02-best 0.008-best 0.004-best 0.002-best 0.0005-best 0.0002-best0.05-best 91.65 10.7 76.70 76.80 76.93 76.64 77.02 76.78 76.520.02-best 93.45 17.7 76.79 76.91 77.07 76.79 77.09 76.89 76.700.008-best 94.81 27.7 76.79 77.01 77.05 76.80 77.14 76.88 76.730.004-best 95.55 37.5 76.79 76.98 76.97 76.74 77.12 76.86 76.710.002-best 96.09 49.3 76.79 76.98 76.96 76.73 77.13 76.85 76.720.0005-best 96.82 77.7 76.79 76.98 76.96 76.73 77.13 76.85 76.700.0002-best 97.04 99.2 76.83 77.01 76.96 76.71 77.13 76.88 76.70Table 6: Comparison of the F-scores of rerankers trained and evaluated with various ?-best taggers.F-score Method71.10 MEMMThis paper72.65 rerankingTsai et al (2006) 72.98 CRF, post-processingZhou et al (2004) 72.55HMM,SVM, post-processing,gazetteerFriedrich et al (2006) 71.5 CRF,gazetteerOkanohara et al (2006) 71.48 semi-CRFTable 7: Performance comparison on the test set.be sufficient practical solutions for this kind of tasks.6 ConclusionThis paper showed that the named-entity recogni-tion, which have usually been solved by dynamic-programming-based sequence-labeling techniqueswith local features, can have innegligible perfor-mance improvement from reranking methods.
Oursystem showed clear improvement over many of themachine-learning-based systems reported to date,and also proved comparable to the existing state-of-the-art systems that use rule-based post-processing.Our future plans include further sophistication offeatures, such as the use of external gazetteers whichis reported to improve the F-score by 1.0 and 2.7points in (Zhou and Su, 2004) and (Friedrich etal., 2006), respectively.
We expect that rerankingarchitecture can readily accommodate dictionary-based features, because we can apply elaboratedstring-matching algorithms to the qualified candi-date strings available at reranking phase.We also plan to apply self-training of n-best tag-ger which successfully boosted the performanceof one of the best existing English syntacticparser (McClosky et al, 2006).
Since the test data ofthe shared-task consists of articles that represent thedifferent publication years, the effects of the publi-cation years of the texts used for self-training wouldbe interesting to study.ReferencesAdam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A Maximum Entropy Approach215to Natural Language Processing.
Computational Lin-guistics, 22(1).R.
Bunescu and R. Mooney.
2004.
Relational markovnetworks for collective information extraction.
In Pro-ceedings of ICML 2004.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of ACL 2005.S.
Chen and R. Rosenfeld.
1999.
A Gaussian prior forsmoothing maximum entropy models.
In TechnicalReport CMUCS.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
In Proceedings of 17th In-ternational Conference on Machine Learning, pages175?182.
Morgan Kaufmann, San Francisco, CA.Christoph M. Friedrich, Thomas Revillion, Martin Hof-mann, and Juliane Fluck.
2006.
Biomedical andChemical Named Entity Recognition with ConditionalRandom Fields: The Advantage of Dictionary Fea-tures.
In Proceedings of the Second International Sym-posium on Semantic Mining in Biomedicine.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introductionto the Bio-Entity Recognition Task at JNLPBA.
InProceedings of the International Workshop on Natu-ral Language Processing in Biomedicine and its Appli-cations (JNLPBA-04), pages 70?75, Geneva, Switzer-land.Michael Krauthammer and Goran Nenadic.
2004.
Termidentification in the biomedical literature.
Journal ofBiomedical Informatics, 37(6).Vijay Krishnan and Christopher D. Manning.
2006.
AnEffective Two-Stage Model for Exploiting Non-LocalDependencies in Named Entity Recognition.
In Pro-ceedings of ACL 2006.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProceedings of 18th International Conference on Ma-chine Learning, pages 282?289.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum Entropy Markov Models forInformation Extraction and Segmentation.
In ICML2000.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Ef-fective self-training for parsing.
In Proceedings ofNAACL 2006.Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichiTsujii.
2002.
GENIA Corpus: an Annotated ResearchAbstract Corpus in Molecular Biology Domain.
InProceedings of the Human Language Technology Con-ference (HLT 2002), March.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
2006.
Improving the Scala-bility of Semi-Markov Conditional Random Fields forNamed Entity Recognition.
In Proceedings of ACL2006, Sydney, Australia, July.Leonid Peshkin and Avi Pfefer.
2003.
Bayesian Infor-mation Extraction Network.
In Proceedings of theEighteenth International Joint Conf.
on Artificial In-telligence.S.
Sarawagi and W. Cohen.
2004.
Semimarkov con-ditional random fields for information extraction.
InProceedings of ICML 2004.Charles Sutton and Andrew McCallum.
2004.
Collec-tive Segmentation and Labeling of Distant Entities inInformation Extraction.
Technical report, Universityof Massachusetts.
Presented at ICML Workshop onStatistical Relational Learning and Its Connections toOther Fields.Richard Tzong-Han Tsai, Cheng-Lung Sung, Hong-JieDai, Hsieh-Chuan Hung, Ting-Yi Sung, and Wen-LianHsu.
2006.
NERBio: using selected word conjunc-tions, term normalization, and global patterns to im-prove biomedical named entity recognition.
In BMCBioinformatics 2006, 7(Suppl 5):S11.GuoDong Zhou and Jian Su.
2004.
Exploring deepknowledge resources in biomedical name recognition.In Proceedings of the International Workshop on Nat-ural Language Processing in Biomedicine and its Ap-plications (JNLPBA-04), pages 96?99.216
