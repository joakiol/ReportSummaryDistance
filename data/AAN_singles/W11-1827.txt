Proceedings of BioNLP Shared Task 2011 Workshop, pages 173?182,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsAdapting a General Semantic Interpretation Approach to Biological EventExtractionHalil Kilicoglu and Sabine BerglerDepartment of Computer Science and Software EngineeringConcordia University1455 de Maisonneuve Blvd.
WestMontre?al, Canada{h kilico,bergler}@cse.concordia.caAbstractThe second BioNLP Shared Task on EventExtraction (BioNLP-ST?11) follows up theprevious shared task competition with a focuson generalization with respect to text types,event types and subject domains.
In this spirit,we re-engineered and extended our event ex-traction system, emphasizing linguistic gener-alizations and avoiding domain-, event type-or text type-specific optimizations.
Similarto our earlier system, syntactic dependenciesform the basis of our approach.
However, di-verging from that system?s more pragmatic na-ture, we more clearly distinguish the sharedtask concerns from a general semantic com-position scheme, that is based on the no-tion of embedding.
We apply our methodol-ogy to core bio-event extraction and specu-lation/negation detection tasks in three maintracks.
Our results demonstrate that such ageneral approach is viable and pinpoint someof its shortcomings.1 IntroductionIn the past two years, largely due to the availabil-ity of GENIA event corpus (Kim et al, 2008) andthe resulting shared task competition (BioNLP?09Shared Task on Event Extraction (Kim et al,2009)), event extraction in biological domain hasbeen attracting greater attention.
One of the crit-icisms towards this paradigm of corpus annota-tion/competition has been that they are concernedwith narrow domains and specific representations,and that they may not generalize well.
For in-stance, GENIA event corpus contains only Medlineabstracts on transcription factors in human bloodcells.
Whether models trained on this corpus wouldperform well on full-text articles or on text focusingon other aspects of biomedicine (e.g., treatment oretiology of disease) remains largely unclear.
Sinceannotated corpora are not available for every con-ceivable domain, it is desirable for automatic eventextraction systems to be generally applicable to dif-ferent types of text and domains without requiringmuch training data or customization.GENIA EPI ID BB BI# core events 9 15 10 2 10Triggers?
Y Y Y N NFull-text?
Y N Y N NSpec/Neg?
Y Y Y N NTable 1: An overview of BioNLP-ST?11 tracksIn the follow-up event to BioNLP?09 SharedTask on Event Extraction, organizers of the secondBioNLP Shared Task on Event Extraction (BioNLP-ST?11) (Kim et al, 2011a) address this challenge tosome extent.
The theme of BioNLP-ST?11 is gen-eralization and the net is cast much wider.
Thereare 4 event extraction tracks: in addition to the GE-NIA track that again focuses on transcription fac-tors (Kim et al, 2011b), the epigenetics and post-translational modification track (EPI) focuses onevents relating to epigenetic change, such as DNAmethylation and histone modification, as well asother common post-translational protein modifica-tions (Ohta et al, 2011), whereas the infectious dis-eases track (ID) focuses on bio-molecular mecha-nisms of infectious diseases (Pyysalo et al, 2011a).Both GENIA and ID tracks include data pertainingto full-text articles, as well.
The fourth track, Bacte-ria, consists of two sub-tracks: Biotopes (BB) andInteractions (BI) (Bossy et al (2011) and Jourde173et al (2011), respectively).
A summary of theBioNLP-ST?11 tracks is given in Table (1).We participated in three tracks: GENIA, EPI, andID.
In the spirit of the competition, our aim was todemonstrate a methodology that was general and re-quired little, if any, customization or training for in-dividual tracks.
For this purpose, we used a two-phase approach: a syntax-driven composition phasethat exploits linguistic generalizations to create ageneral semantic representation in a bottom-up man-ner and a mapping phase, which relies on the sharedtask event definitions and constraints to map rele-vant parts of this semantic representation to eventinstances.
The composition phase takes as its inputsimple entities and syntactic dependency relationsand is intended to be fully general.
On the otherhand, the second phase is more task-specific eventhough the kind of task-specific knowledge it re-quires is largely limited to event definitions and trig-ger expressions.
In addition to extracting core bio-logical events, our system also addresses speculationand negation detection within the same framework.Our results demonstrate the feasibility of a method-ology that uses little training data or customization.2 MethodologyIn our general research, we are working towardsa linguistically-grounded, bottom-up discourse in-terpretation scheme.
In particular, we focus onlower level discourse phenomena, such as causation,modality, and negation, and investigate how they in-teract with each other, as well as their effect on ba-sic propositional semantic content (who did what towho?)
and higher discourse/pragmatics structure.
Inour model, we distinguish three layers of proposi-tions: atomic, embedding, and discourse.
An atomicproposition corresponds to the basic unit and low-est level of meaning: in other words, a semantic re-lation whose arguments correspond to ontologicallysimple entities.
Atomic propositions form the ba-sis for embedding propositions, that is, propositionstaking as arguments other propositions (embeddingthem).
In turn, embedding and atomic propositionsact as arguments for discourse relations1.
Our main1Discourse relations, also referred to as coherence or rhetor-ical relations (Mann and Thompson, 1988), are not relevant tothe shared task and, thus, we will not discuss them further inmotivation in casting the problem of discourse in-terpretation in this structural manner is two-fold: a)to explore the semantics of the embedding layer ina systematic way b) to allow a bottom-up semanticcomposition approach, which works its way fromatomic propositions towards discourse relations increating general semantic representations.The first phase of our event extraction system(composition) is essentially an implementation ofthis semantic composition approach.
Before delvinginto further details regarding our implementation forthe shared task, however, it is necessary to briefly ex-plain the embedding proposition categorization thatour interpretation scheme is based on.
With this cat-egorization, our goal is to make explicit the kindof semantic information expressed at the embeddinglayer.
We distinguish three basic classes of embed-ding propositions: MODAL, ATTRIBUTIVE, and RE-LATIONAL.
We provide a brief summary below.2.1 MODAL typeThe embedding propositions of MODAL type mod-ify the status of the embedded proposition with re-spect to its factuality, possibility, or necessity, andso on.
They typically involve a) judgement aboutthe status of the proposition, b) evidence for theproposition, c) ability or willingness, and d) obli-gations and permissions, corresponding roughly toEPISTEMIC, EVIDENTIAL, DYNAMIC and DEONTICtypes (cf.
Palmer (1986)), respectively.
Further sub-divisions are given in Figure (1).
In the shared taskcontext, the MODAL class is mostly relevant to thespeculation and negation detection tasks.2.2 ATTRIBUTIVE typeThe ATTRIBUTIVE type of embedding serves tospecify an attribute of an embedded proposition (se-mantic role of an argument).
They typically involvea verbal predicate (undergo in Example (1) below),which takes a nominalized predicate (degradation)as one of its syntactic arguments.
The other syntac-tic argument of the verbal predicate corresponds toa semantic argument of the embedded predicate.
InExample (1), p105 is a semantic argument of PA-TIENT type for the proposition indicated by degra-dation.this paper.174(1) .
.
.
p105 undergoes degradation .
.
.Verbs functioning in this way are plenty (e.g., per-form for the AGENT role, experience for experiencerrole).
With respect to the shared task, we found thatthe usefulness of the ATTRIBUTIVE type of embed-ding was largely limited to verbal predicates involveand require and their nominal forms.2.3 RELATIONAL typeThe RELATIONAL type of embedding serves to se-mantically link two propositions, providing a dis-course/pragmatic function.
It is characterized bypermeation of a limited set of discourse relations tothe clausal level, often signalled lexically by ?dis-course verbs?
(Danlos, 2006) (e.g., cause, mediate,lead, correlate), their nominal forms or other ab-stract nouns, such as role.
We categorize the RELA-TIONAL class into CAUSAL, TEMPORAL, CORREL-ATIVE, COMPARATIVE, and SALIENCY types.
In theexample below, the verbal predicate leads to indi-cates a CAUSAL relation between the propositionswhose predicates are highlighted.
(2) Stimulation of cells leads to a rapid phospho-rylation of I?B?
.
.
.While not all the subtypes of this class were relevantto the shared task, we found that CAUSAL, CORREL-ATIVE, and SALIENCY subtypes play a role, partic-ularly in complex regulatory events.
The portions ofthe classification that pertain to the shared task aregiven in Figure (1).3 ImplementationIn the shared task setting, embedding propositionscorrespond to complex regulatory events (e.g., Reg-ulation, Catalysis) as well as event modifications(Negation and Speculation), whereas atomic propo-sitions correspond to simple event types (e.g., Phos-phorylation).
While the treatment of these two typesdiffer in significant ways, they both require that sim-ple entities are recognized, syntactic dependenciesare identified and a dictionary of trigger expressionsis available.
We first briefly explain the constructionof the trigger dictionary.3.1 Dictionary of Trigger ExpressionsIn the previous shared task, we relied on trainingdata and simple statistical measures to identify goodFigure 1: Embedding proposition categorization relevantto the shared tasktrigger expressions for events and used a list of trig-gers that we manually compiled for speculation andnegation detection (see Kilicoglu and Bergler (2009)for details).
With respect to atomic propositions,our method of constructing a dictionary of triggerexpressions remains essentially the same, includingthe use of statistical measures to distinguish goodtriggers.
The only change we made was to consideraffixal negation and set polarity of several atomicproposition triggers to negative (e.g., nonexpression,unglycosylated).
On the other hand, we have beenextending our manually compiled list of specula-tion/negation triggers to include other types of em-bedding triggers and to encode finer grained distinc-tions in terms of their categorization and trigger be-haviors.
The training data provided for the sharedtask also helped us expand this trigger dictionary,particularly with respect to RELATIONAL trigger ex-pressions.
It is worth noting that we used the sameembedding trigger dictionary for all three tracks thatwe participated in.
Several entries from the embed-ding trigger dictionary are summarized in Table (2).Lexical polarity and strength values play a rolein the composition phase in associating a context-dependent scalar value with propositions.
Lexicalpolarity values are largely derived from a polaritylexicon (Wilson et al, 2005) and extended by us-175Trigger POS Semantic Type Lexical Polarity Strengthshow VB DEMONSTRATIVE positive 1.0unknown JJ EPISTEMIC negative 0.7induce VB CAUSAL positive 1.0fail VB SUCCESS negative 0.0effect NN CAUSAL neutral 0.5weakly RB HEDGE neutral -absence NN REVERSE negative -Table 2: Several entries from the embedding dictionarying heuristics involving the event types associatedwith the trigger2.
Some polarity values were as-signed manually.
Some strength values were basedon prior work (Kilicoglu and Bergler, 2008), oth-ers were manually assigned.
As Table (2) shows, insome cases, the semantic type (e.g., DEMONSTRA-TIVE, CAUSAL) is simply a mapping to the embed-ding categorization.
In other cases, such as weaklyor absence, the semantic type identifies the role thatthe trigger plays in the composition phase.
The em-bedding trigger dictionary incorporates ambiguity;however, for the shared task, we limit ourselves toone semantic type per trigger to avoid the issue ofdisambiguation.
For ambiguous triggers extractedfrom the training data, the semantic type with themaximum likelihood is used.
On the other hand, wedetermined the semantic type to use manually fortriggers that we compiled independent of the train-ing data.
In this way, we use 466 triggers for atomicpropositions and 908 for embedding ones3.3.2 CompositionAs mentioned above, the composition phase as-sumes simple entities, syntactic dependency rela-tions and trigger expressions.
Using these elements,we construct a semantic embedding graph of thedocument.
To obtain syntactic dependency relations,we segment documents into sentences, parse themusing the re-ranking parser of Charniak and John-son (2005) adapted to the biomedical domain (Mc-Closky and Charniak, 2008) and extract syntactic2For example, if the most likely event type associated withthe trigger is Negative regulation, its polarity is considered neg-ative.3Note, however, that not all embedding propositions (or theirtriggers) were directly relevant to the shared task.dependencies from parse trees using the Stanforddependency scheme (de Marneffe et al, 2006).
Inaddition to syntactic dependencies, we also requireinformation regarding individual tokens, includinglemma, part-of-speech, and positional information,for which we also rely on Stanford parser tools.
Wepresent a high level description of the compositionphase below.3.2.1 From syntactic dependencies toembedding graphsAs the first step in composition, we convert syn-tactic dependencies into embedding relations.
Anembedding relation, in our definition, is very simi-lar to a syntactic dependency; it is typed and holdsbetween two textual elements.
It diverges from asyntactic dependency in two ways: its elements canbe multi-word expressions and it is aimed at betterreflecting the direction of the semantic dependencybetween its elements.
Take, for example, the sen-tence fragment in Example (3a).
Syntactic depen-dencies are given in (3b) and the corresponding em-bedding relations in (3c).
The fact that the adjecti-val predicate in modifier position (possible) semanti-cally embeds its head (involvement) is captured withthe first embedding relation.
The second syntacticdependency already reflects the direction of the se-mantic dependency between its elements accuratelyand, thus, is unchanged as an embedding relation.
(3) (a) .
.
.
possible involvement of HCMV .
.
.
(b) amod(involvement,possible)prep of (involvement,HCMV)(c) amod(possible,involvement)prep of (involvement,HCMV)To obtain the embedding relations in a sentence,we apply a series of transformations to its syntactic176Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), theextracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in humanmonocytes.
in the context of the document embedding graph for the Medline abstract with PMID 10089566.dependencies.
A transformation may not be neces-sary, as with the prep of dependency in the exam-ple above.
It may result in collapsing several syn-tactic dependencies into one, as well, or in splittingone into several embedding relations.
In additionto capturing semantic dependency behavior explic-itly, these transformations serve to incorporate se-mantic information (entities and triggers) into theembedding structure and to correct syntactic depen-dencies that are systemically misidentified, such asthose that involve modifier coordination.After these transformations, the resulting directedacyclic embedding graph is, in the simplest case, atree, but more often a forest.
An example graph isgiven in Figure (2).
The edges are associated withthe embedding relation types, and the nodes withtextual elements.3.2.2 Composing PropositionsAfter constructing the embedding graph, we tra-verse it in a bottom-up manner and compose se-mantic propositions.
Before this procedure can takeplace, though, the embedding graph pertaining toeach sentence is further linked to the document em-bedding graph in a way to reflect the proximity ofsentences, as illustrated in Figure (2).
This is doneto enable discourse interpretation across sentences,including coreference resolution.Traversal of the embedding structure is guided byargument identification rules, which apply to non-leaf nodes in the embedding graph.
An argumentidentification rule is essentially a mapping from thetype of the embedding relation holding between aparent node and its child node and part-of-speech ofthe parent node to a logical argument type (logicalsubject, logical object or adjunct).
Constraints onand exclusions from a rule can be defined, as shownin Table (3).
We currently use about 80 such rules,mostly adapted from our previous shared task sys-tem (Kilicoglu and Bergler, 2009).After all the descendants of a non-leaf node arerecursively processed for arguments, a semanticproposition can be composed.
We define a seman-tic proposition as consisting of a trigger, a collection177Relation Applies to Argument Constrained to Exclusionsprep on NN Object influence,impact,effect -agent VB Subject - -nsubjpass VB Object - -whether comp VB Object INTERROGATIVE -prep in NN Adjunct - effect, role, influence, importanceTable 3: Several argument identification rules.
Note that constraints and exclusions may apply to trigger categories, aswell as to lemmas.of core and adjunct arguments as well as a polarityvalue and a scalar value.
The polarity value can bepositive, negative or neutral.
The scalar value is inthe (0,1) range.
Atomic propositions are simply as-signed polarity value of neutral4 and the scalar valueof 1.0.
On the other hand, in the context of embed-ding propositions, the computation of these values,through which we attempt to capture some of the in-teractions occurring at the embedding layer, is moreinvolved.
For the sentence depicted in Figure (2),the relevant resulting embedding and atomic propo-sitions are given below.
(4) DEMONSTRATIVE(em1,Trigger=show,Object=em2, Subject=Our previous results,Polarity=positive, Value=1.0)(5) CAUSAL(em2, Trigger=stimulates, Object=ap1,Subject=recombinant gp41, Polarity=positive,Value=1.0)(6) Gene expression(ap1, Trigger= production,Object= interleukin-10, Adjunct= humanmonocytes, Polarity=neutral, Value=1.0)The composition phase also deals with coordina-tion of entities and propositions as well as with prop-agation of arguments at the lower levels.3.3 Mapping Propositions to EventsThe goal of the mapping phase is to impose theshared task constraints on the partial interpretationachieved in the previous phase.
We achieve this inthree steps.The first step is to map embedding propositiontypes to event (or event modification) types.
We de-fined constraints that guide this mapping.
Some of4Unless affixal negation is involved, in which case the as-signed polarity value is negative.these mappings are presented in Table (4).
In thisway, Example (4) is pruned, since embedding propo-sitions of DEMONSTRATIVE type satisfy the con-straints only if they have negative polarity, as shownin Table (4).We then apply constraints concerned with the se-mantic roles of the participants.
For this step, wedefine a small number of logical argument/semanticrole mappings.
These are similar to argument identi-fication rules, in that the mapping can be constrainedto certain event types or event types can be excludedfrom it.
We provide some of these mappings in Ta-ble (5).
With these mappings, the Object and Sub-ject arguments of the proposition in Example (5) areconverted to Theme and Cause semantic roles, re-spectively.As the final step, we prune event participants thatdo not conform to the event definition as well as thepropositions whose types could not be mapped to ashared task event type.
For example, a Cause par-ticipant for a Gene expression event is pruned, sinceonly Theme participants are relevant for the sharedtask.
Further, a proposition with DEONTIC seman-tic type is pruned, because it cannot be mapped toa shared task type.
The infectious diseases track(ID) event type Process is interesting, because it maytake no participants at all, and we deal with this id-iosyncrasy at this step, as well.
This concludes theprogressive transformation of the graph to event andevent modification annotations.4 Results and DiscussionWith the two-phase methodology presented above,we participated in three tracks: GENIA (Tasks 1 and3), ID, and EPI.
The official evaluation results weobtained for the GENIA track are presented in Ta-ble (6) and the results for the EPI and ID tracks in178Track Prop.
Type Polarity Value Correspond.
Event (Modification) TypeGENIA,ID CAUSAL neutral - RegulationGENIA,ID,EPI SUCCESS negative - NegationEPI CAUSAL positive - CatalysisGENIA,ID,EPI SPECULATIVE - > 0.0 SpeculationGENIA,ID,EPI DEMONSTRATIVE negative - SpeculationTable 4: Several event (and event modification) mappingsLogicalArg.SemanticRoleConstraint ExclusionObject Theme - ProcessSubject Cause - -Subject Theme Binding -Object Participant Process -Object Scope Speculation,Negation-Table 5: Logical argument to semantic role mappingsTable (7).
With the official evaluation criteria, wewere ranked 5th in the GENIA track (5/15), 7th inthe EPI track (7/7) and 4th in the ID track (4/7).There were only two submissions for the GENIAspeculation/negation task (Task 3) and our resultsin this task were comparable to those of the otherparticipating group: our system performed slightlybetter with speculation, and theirs with negation.Our core module extracts adjunct arguments, us-ing ABNER (Settles, 2005) as its source for addi-tional named entities.
We experimented with map-ping these arguments to non-core event participants(Site, Contextgene, etc.
); however, we did not in-clude them in our official submission, because theyseemed to require more work with respect to map-ping to shared task specifications.
Due to this short-coming, the performance of our system suffered sig-nificantly in the EPI track.A particularly encouraging outcome for our sys-tem is that our results on the GENIA developmentset versus on the test set were very close (an F-score of 51.03 vs. 50.32), indicating that our gen-eral approach avoided overfitting, while capturingthe linguistic generalizations, as we intended.
Weobserve similar trends with the other tracks, as well.In the EPI track, development/test F-score resultswere 29.10 vs. 27.88; while, in the ID track, inter-Event Class Recall Precis.
F-scoreLocalization 39.27 90.36 54.74Binding 29.33 49.66 36.88Gene expression 65.87 86.84 74.91Transcription 32.18 58.95 41.64Protein catabolism 66.67 71.43 68.97Phosphorylation 75.14 94.56 83.73EVT-TOTAL 52.67 78.04 62.90Regulation 33.77 42.48 37.63Positive regulation 35.97 47.66 41.00Negative regulation 36.43 43.88 39.81REG-TOTAL 35.72 45.85 40.16Negation 18.77 44.26 26.36Speculation 21.10 38.46 27.25MOD-TOTAL 19.97 40.89 26.83ALL-TOTAL 43.55 59.58 50.32Table 6: Official GENIA track results, with approximatespan matching/approximate recursive matching evalua-tion criteriaestingly, our test set performance was better (39.64vs.
44.21).
We also obtained the highest recall inthe ID track, despite the fact that our system typi-cally favors precision.
We attribute this somewhatidiosyncratic performance in the ID track partly tothe fact that we did not use a track-specific triggerdictionary.
Most of the ID track event types arethe same as those of GENIA track, which probablyled to identification of some ID events with GENIA-only triggers5.One of the interesting aspects of the shared taskwas its inclusion of full-text articles in training andevaluation.
Cohen et al (2010) show that structureand content of biomedical abstracts and article bod-ies differ markedly and suggest that some of these5This clearly also led to low precision particularly in com-plex regulatory events.179Track-Eval.
Type Recall Precis.
F-scoreEPI-FULL 20.83 42.14 27.88EPI-CORE 40.28 76.71 52.83ID-FULL 49.00 40.27 44.21ID-CORE 50.77 43.25 46.71Table 7: Official evaluation results for EPI and ID tracks.Primary evaluation criteria underlined.differences may pose problems in processing full-text articles.
Since one of our goals was to determinethe generality of our system across text types, wedid not perform any full text-specific optimization.Our results on article bodies are notable: our systemhad stable performance across text types (in fact, wehad a very slight F-score improvement on full-textarticles: 50.40 vs. 50.28).
This contrasts with thedrop of a few points that seems to occur with otherwell-performing systems.
Taking only full-text arti-cles into consideration, we would be ranked 4th inthe GENIA track.
Furthermore, a preliminary erroranalysis with full-text articles seems to indicate thatparsing-related errors are more prevalent in the full-text article set than in the abstract set, consistent withCohen et al?s (2010) findings.
At the same time, ourresults confirm that we were able to abstract awayfrom this complexity to some degree with our ap-proach.We have a particular interest in speculation andnegation detection.
Therefore, we examined our re-sults on the GENIA development set with respect toTask 3 more closely.
Consistent with our previousshared task results, we determined that the majorityof errors were due to misidentified or missed baseevents (70% of the precision errors and 83% of therecall errors)6.
Task 3-specific precision errors in-cluded cases in which speculation or negation wasdebatable, as the examples below show.
In Exam-ple (7a), our system detected a Speculation instance,due to the verbal predicate suggesting, which scopesover the event indicated by role.
In Example (7b),our system detected a Negation instance, due to thenominal predicate lack, which scopes over the eventsindicated by expression.
Neither were annotated as6Even a bigger percentage of speculation/negation-relatederrors in the EPI and ID tracks were due to the same problem,as the overall accuracy in those tracks is lower.such in the shared task corpus.
(7) (a) .
.
.
suggesting a role of these 3?
elementsin beta-globin gene expression.
(b) .
.
.
DT40 B cell lines that lack expressionof either PKD1 or PKD3 .
.
.Another class of precision errors was due to argu-ment propagation up the embedding graph.
It seemsthe current algorithm may be too permissive in somecases and a more refined approach to argument prop-agation may be necessary.
In the following example,while suggest, an epistemic trigger, does not embedinduction directly (as shown in (8b)), the intermedi-ate nodes simply propagate the proposition associ-ated with the induction node up the graph, leadingus to conclude that the proposition triggered by in-duction is speculated, leading to a precision error.
(8) (a) .
.
.
these findings suggest that PWM is ableto initiate an intracytoplasmic signalingcascade and EGR-1 induction .
.
.
(b) suggest ?
able ?
initiate ?
inductionAmong the recall errors, some of them were dueto shortcomings of the composition algorithm, as itis currently implemented.
One recall problem in-volved the embedding status of and rules concern-ing copular constructions, which we had not yet ad-dressed.
Therefore, we miss the relatively straight-forward Speculation instances in the following ex-amples.
(9) (a) .
.
.
the A3G promoter appears constitu-tively active.
(b) .
.
.
the precise factors that mediate this in-duction mechanism remain unknown.Similarly, the lack of a trigger expression in our dic-tionary may cause recall errors.
The example belowshows an instance where this occurs, in addition tolack of an appropriate argument identification rule:(10) mRNA was quantified by real-time PCR forFOXP3 and GATA3 expression.Our system also missed an interesting, domain-specific type of negation, in which the minus signindicates negation of the event that the entity partic-ipates in.
(11) .
.
.
CD14- surface Ag expression .
.
.1805 Conclusions and Future WorkWe explored a two-phase approach to event ex-traction, distinguishing general linguistic principlesfrom task-specific aspects, in accordance with thegeneralization theme of the shared task.
Our resultsdemonstrate the viability of this approach on bothabstracts and article bodies, while also pinpointingsome of its shortcomings.
For example, our erroranalysis shows that some aspects of semantic com-position algorithm (argument propagation, in partic-ular) requires more refinement.
Furthermore, usingthe same trigger expression dictionary for all tracksseems to have negative effect on the overall perfor-mance.
The incremental nature of our system de-velopment ensures that some of these shortcomingswill be addressed in future work.We participated in three supporting tasks, twoof which (Co-reference (CO) and Entity Relations(REL) tasks (Nguyen et al (2011) and Pyysalo etal.
(2011b), respectively) were relevant to the mainportion of the shared task; however, due to time con-straints, we were not able to fully incorporate thesemodules into our general framework, with the ex-ception of the co-reference resolution of relative pro-nouns.
Since our goal is to move towards discourseinterpretation, we plan to incorporate these modules(inter-sentential co-reference resolution, in particu-lar) into our framework.
After applying the lessonswe learned in the shared task and fully incorporatingthese modules, we plan to make our system availableto the scientific community.ReferencesRobert Bossy, Julien Jourde, Philippe Bessie`res, Marteenvan de Guchte, and Claire Ne?dellec.
2011.
BioNLPShared Task 2011 - Bacteria Biotope.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Meeting of the Associ-ation for Computational Linguistics, pages 173?180.K Bretonnel Cohen, Helen L Johnson, Karin Verspoor,Christophe Roeder, and Lawrence E Hunter.
2010.The structural and content aspects of abstracts versusbodies of full text journal articles are different.
BMCBioinformatics, 11:492.Laurence Danlos.
2006.
?Discourse verbs?
and dis-course periphrastic links.
In C Sidner, J Harpur,A Benz, and P Ku?hnlein, editors, Second Workshopon Constraints in Discourse (CID06), pages 59?65.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation, pages 449?454.Julien Jourde, Alain-Pierre Manine, Philippe Veber,Kare?n Fort, Robert Bossy, Erick Alphonse, andPhilippe Bessie`res.
2011.
BioNLP Shared Task 2011- Bacteria Gene Interactions and Renaming.
In Pro-ceedings of the BioNLP 2011 Workshop CompanionVolume for Shared Task, Portland, Oregon, June.
As-sociation for Computational Linguistics.Halil Kilicoglu and Sabine Bergler.
2008.
Recognizingspeculative language in biomedical research articles:a linguistically motivated perspective.
BMC Bioinfor-matics, 9 Suppl 11:s10.Halil Kilicoglu and Sabine Bergler.
2009.
Syntactic de-pendency based heuristics for biological event extrac-tion.
In Proceedings of Natural Language Process-ing in Biomedicine (BioNLP) NAACL 2009 Workshop,pages 119?127.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9:10.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop, pages1?9.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Jun?ichi Tsujii.
2011a.
Overviewof BioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-nori Yonezawa.
2011b.
Overview of the Genia Eventtask in BioNLP Shared Task 2011.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings ofthe 46th Meeting of the Association for ComputationalLinguistics, pages 101?104.181Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii.
2011.Overview of the Protein Coreference task in BioNLPShared Task 2011.
In Proceedings of the BioNLP 2011Workshop Companion Volume for Shared Task, Port-land, Oregon, June.
Association for ComputationalLinguistics.Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii.
2011.Overview of the Epigenetics and Post-translationalModifications (EPI) task of BioNLP Shared Task2011.
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.Frank R Palmer.
1986.
Mood and modality.
CambridgeUniversity Press, Cambridge, UK.Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-livan, Chunhong Mao, Chunxia Wang, Bruno So-bral, Jun?ichi Tsujii, and Sophia Ananiadou.
2011a.Overview of the Infectious Diseases (ID) task ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.2011b.
Overview of the Entity Relations (REL) sup-porting task of BioNLP Shared Task 2011.
In Pro-ceedings of the BioNLP 2011 Workshop CompanionVolume for Shared Task, Portland, Oregon, June.
As-sociation for Computational Linguistics.Burr Settles.
2005.
ABNER: An open source tool forautomatically tagging genes, proteins, and other entitynames in text.
Bioinformatics, 21(14):3191?3192.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Hu-man Language Technologies Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT/EMNLP-2005), pages 347?354.182
