Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64?67,Columbus, June 2008. c?2008 Association for Computational LinguisticsRapidly Deploying Grammar-Based Speech Applicationswith Active Learning and Back-off GrammarsTim Paek1, Sudeep Gandhe2, David Maxwel Chickering11 Microsoft Research, One Microsoft Way, Redmond, WA 980522 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292{timpaek|dmax}@microsoft.com, gandhe@usc.edu2 Second author was partly sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM).
Statements and opi-nions expressed do not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred.AbstractGrammar-based approaches to spoken lan-guage understanding are utilized to a great ex-tent in industry, particularly when developersare confronted with data sparsity.
In order toensure wide grammar coverage, developerstypically modify their grammars in an itera-tive process of deploying the application, col-lecting and transcribing user utterances, andadjusting the grammar.
In this paper, we ex-plore enhancing this iterative process by leve-raging active learning with back-offgrammars.
Because the back-off grammarsexpand coverage of user utterances, develop-ers have a safety net for deploying applica-tions earlier.
Furthermore, the statistics relatedto the back-off can be used for active learning,thus reducing the effort and cost of data tran-scription.
In experiments conducted on acommercially deployed application, the ap-proach achieved levels of semantic accuracycomparable to transcribing all failed utter-ances with 87% less transcriptions.1 IntroductionAlthough research in spoken language understand-ing is typically pursued from a statistical perspec-tive, grammar-based approaches are utilized to agreat extent in industry (Knight et al, 2001).Speech recognition grammars are often manuallyauthored and iteratively modified as follows: Typi-cally, context-free grammars (CFG) are written ina format such as Speech Recognition GrammarSpecification (SRGS) (W3C, 2004) and deployed.Once user utterances are collected and transcribed,the grammars are then adjusted to improve theircoverage.
This process continues until minimalOOG utterances are observed.
In this paper, weexplore enhancing this iterative process of gram-mar modification by combining back-off gram-mars, which expand coverage of user utterances,with active learning, which reduces ?the number oftraining examples to be labeled by automaticallyprocessing unlabeled examples, and then selectingthe most informative ones with respect to a speci-fied cost function for a human to label?
(Hakkani-Tur et al, 2002).
This paper comprises three sec-tions.
In Section 2, we describe our overall ap-proach to rapid application development (RAD).
InSection 3, we explain how data transcription canbe reduced by leveraging active learning based onstatistics related to the usage of back-off gram-mars.
Finally, in Section 4, we evaluate the activelearning approach with simulation experimentsconducted on data collected from a commercialgrammar-based speech application.2 RAD Approach & Related WorkWorking under the assumption that developers inindustry will continue to use CFGs for rapid appli-cation development, our approach to grammarmodification is as follows:1.
Create a CFG (either manually or automatically).1.1 Generate a back-off grammar from the CFG.2.
Deploy the application.2.1 Use the back-off grammar for OOG utterances.3.
Gather data from users.4.
Selectively transcribe data by using statistics re-lated to the back-off for active learning; i.e., transcribeonly those utterances that satisfy the active learningcriterion.5.
Modify CFG either manually or automatically andgo to step 1.1.To begin with, developers start with a CFG in Step1.
If they had access to a grammatical platform64such as Regulus (Rayner et al, 2006), they couldin principle construct a CFG automatically for anynew domain, though most developers will probablymanually author the grammar.
Two steps are addedto the typical iterative process.
In Step 1.1, wegenerate a back-off grammar from the CFG.
Oneway to accomplish this is by constructing a back-off CFG using filler models (Paek et al, 2007),which when applied to the same command-and-control task in Section 4 can result in a 35% rela-tive reduction in semantic error rate for OOG ut-terances.
However, the back-off grammar couldalso be a SLM trained on artificial data createdfrom the CFG (Galescu et al, 1998).
Whateverback-off mechanism is employed, its coverageshould be wider than the original CFG so that ut-terances that fail to be recognized by the CFG, orfall below an acceptable confidence threshold, canbe handled by the back-off in a second or simulta-neous pass.
That is the gist of Step 2.1, the secondadditional step.
It is not only important to generatea back-off grammar, but it must be utilized forhandling possible OOG utterances.Our approach attempts to reduce the usual costassociated with grammar modification after theapplication has been deployed and data collected inStep 4.
The idea is simple: Exploit the fast and ac-curate CFG recognition of in-grammar (ING) ut-terances by making OOG utterances handled bythe back-off grammar ING.
In other words, expandCFG coverage to include whatever gets handled bythe back-off grammar.
This idea is very comple-mentary with a two-pass recognition approachwhere the goal is to get utterances correctly recog-nized by a CFG on the first pass so as to minimizecomputational expenses (Paek et al, 2007).All of this can be accomplished with reducedtranscription effort by keeping track of and leve-raging back-off statistics for active learning.
If theback-off is a CFG, we keep track of statistics re-lated to which CFG rules were utilized the most,whether they allowed the task to be successfullycompleted, etc.
If the back-off is a SLM, we keeptrack of similar statistics related to the semanticalignment and mapping in spoken language under-standing.
Given an active learning criterion, thesestatistics can be used to selectively transcribe ut-terances which can then be used to modify theCFG in Step 5 so that OOG utterances becomeING.
Section 3 covers this in more detail.Finally, in Step 5, the CFG grammar is mod-ified using the selectively transcribed utterances.Although developers will probably want to do thismanually, it is possible to automate much of thisstep by making grammar changes with minimaledit distance or Levenshtein distance.Leveraging a wider coverage back-off grammaris of course not new.
For grammar-based applica-tions, several researchers have investigated using aCFG along with a back-off grammar either simul-taneously via a domain-trained SLM (Gorrell eta1., 2002), or in two-pass recognition using eitheran SLM trained on CFG data (Gorrell, 2003) or adictation n-gram (Dusan & Flanagan, 2002).
Toour knowledge however, no prior research has con-sidered leveraging statistics related to the back-offgrammar for active learning, especially as part of aRAD approach.3 Active LearningOur overall approach utilizes back-off grammars toprovide developers with a safety net for deployingapplications earlier, and active learning to reducetranscription effort and cost.
We now elaborate onactive learning, demonstrate the concept with re-spect to a CFG back-off.Active learning aims at reducing transcriptionof training examples by selecting utterances thatare most likely to be informative according to aspecified cost function (Hakkani-Tur et al, 2002).In the speech community, active learning has beensuccessfully applied to reducing the transcriptioneffort for ASR (Hakkani-Tur et al, 2002), SLU(Tur et al, 2003b), as well as finding labeling er-rors (Tur et al, 2003).
In our case, the examplesare user utterances that need to be transcribed, andthe learning involves modifying a CFG to achievewider coverage of user expressions.
Instead of pas-sively transcribing everything and modifying theCFG as such, the grammar can ?actively?
partici-pate in which utterances are transcribed.The usual procedure for selecting utterances forgrammar modification is to transcribe at least allfailed utterances, such as those that fall below arejection threshold.
By leveraging a back-offgrammar, developers have more information withwhich to select utterances for transcription.
For aCFG back-off, how frequently a back-off rule firedcan serve as an active learning criterion becausethat is where OOG utterances are handled.
Given65this active learning criterion, the algorithm wouldproceed as follows (where i denotes iteration, Stdenotes the set of transcribed utterances, and Sudenotes the set of all utterances):[1] Modify CFGi using St and generate correspondingback-offi from the CFGi.
[2] Recognize utterances in set Su using CFGi + back-offi.
[3] Compute statistics on what back-off rules firedwhen and how frequently.
[4] Select the k utterances that were handled by themost frequently occurring back-off rule and tran-scribe them.
Call the new transcribed set as Si.
[5] ;t t i u u iS S S S S S?
?
??
[6] Stop when CFGi achieves a desired level of seman-tic accuracy, or alternatively when back-off rulesonly handle a desired percentage of Su, otherwisego to Step 1.Note that the set Su grows with each iteration andfollows as a result of deploying an application witha CFGi + back-offi.
Step [1] corresponds to Step 5,1.1, and 2.1 of our approach.
Steps [2-4] aboveconstitute the active learning criterion and can beadjusted depending on what developers want tooptimize.
This algorithm currently assumes thatruntime efficiency is the main objective (e.g., on amobile device); hence, it is critical to move utter-ances recognized in the second pass to the firstpass.
If developers are more interested in learningnew semantics, in Step [4] above they could tran-scribe utterances that failed in the back-off.
Withan active learning criterion in place, Step [6] pro-vides a stopping criterion.
This too can be adjusted,and may even target budgetary objectives.4 EvaluationFor evaluation, we used utterances collected from204 users of Microsoft Voice Command, a gram-mar-based command-and-control (C&C) applica-tion for high-end mobile devices (see Paek et al,2007 for details).
We partitioned 5061 transcribedutterances into five sets, one of which was usedexclusively for testing.
The remaining four wereused for iterative CFG modification.
For the firstiteration, we started with a CFG which was a de-graded version of the grammar currently shippedwith the Voice Command product.
It was obtainedby using the mode, or the most frequent user utter-ance, for each CFG rule.
We compared two ap-proaches: CFG_Full, where each iterative CFGwas modified using the full set of transcribed utter-ances that resulted in a failure state (i.e., when afalse recognition event occurred or the phrase con-fidence score fell below 45%, which was set by aproprietary tuning procedure for optimizing word-error rate), and CFG_Active, where each iterativeCFG was modified using only those transcribedutterances corresponding to the most frequentlyoccurring CFG back-off rules.
For both CFG_Fulland CFG_Active, CFGi was modified using thesame set of heuristics akin to minimal edit dis-tance.
In order to assess the value of using theback-off grammar as a safety net, we also com-pared CFG_Full+Back-off, where a derived CFGback-off was utilized whenever a failure state oc-curred with CFG_Full, and CFG_Active+Back-off,where again a CFG back-off was utilized, this timewith the back-off derived from the CFG trained onselective utterances.As our metric, we evaluated semantic accuracysince that is what matters most in C&C settings.Furthermore, because recognition of part of an ut-terance can increase the odds of ultimately achiev-ing task completion (Paek et al, 2007), we carriedout separate evaluations for the functional consti-tuents of a C&C utterance (i.e., keyword and slot)as well as the complete phrase (keyword + slot).We computed accuracy as follows: For any singleutterance, the recognizer can either accept or rejectit.
If it is accepted, then the semantics of the utter-ance can either be correct (i.e., it matches what theuser intended) or incorrect, hence:accuracy = CA / (CA + IA + R)   (1)where CA denotes accepted commands that arecorrect, IA denotes accepted commands that areincorrect, and R denotes the number of rejections.Table 2 displays semantic accuracies for bothCFG_Full and CFG_Active.
Standard errors aboutthe mean were computed using the jacknife proce-dure with 10 re-samples.
Notice that bothCFG_Full and CFG_Active initially have the sameaccuracy levels because they start off with thesame degraded CFG.
The highest accuracies ob-tained almost always occurred in the second itera-tion after modifying the CFG with the first batch oftranscriptions.
Thereafter, all accuracies seem todecrease.
In order to understand why this would becase, we computed the coverage of the ith CFG onthe holdout set.
This is reported in the ?OOG%?column.
Comparing CFG_Full to CFG_Active on66keyword + slot accuracy, CFG_Full decreases inaccuracy after the second iteration as doesCFG_Active.
However, the OOG% of CFG_Full ismuch lower than CFG_Active.
In fact, it seems tolevel off after the second iteration, suggesting thatperhaps the decrease in accuracies reflects the in-crease in grammar perplexity; that is, as the gram-mar covers more of the utterances, it has morehypotheses to consider, and as a result, performsslightly worse.
Interestingly, after the last iteration,CFG_Active for keyword + slot and slot accuracieswas slightly higher (69.06%) than CFG_Full(66.88%) (p = .05).
Furthermore, this was donewith 193 utterances as opposed to 1393, or 87%less transcriptions.
For keyword accuracy,CFG_Active (64.09%) was slightly worse thanCFG_Full (66.10%) (p < .05).With respect to the value of having a back-offgrammar as a safety net, we found that bothCFG_Full and CFG_Active achieved much higheraccuracies with the back-off for keyword, slot, andkeyword + slot accuracies.
Notice also that the dif-ferences between CFG_Full and CFG_Active afterthe last iteration were much closer to each otherthan without the back-off, suggesting applicationsshould always be deployed with a back-off.5 ConclusionIn this paper, we explored enhancing the usualiterative process of grammar modification by leve-raging active learning with back-off grammars.Because the back-off grammars expand coverageof user utterances to handle OOG occurrences, de-velopers have a safety net for deploying applica-tions earlier.
Furthermore, because statistics relatedto the back-off can be used for active learning, de-velopers can reduce the effort and cost of datatranscription.
In our simulation experiments, leve-raging active learning achieved levels of semanticaccuracy comparable to transcribing all failed ut-terances with 87% less transcriptions.ReferencesS.
Dusan & J. Flanagan.
2002.
Adaptive dialog based upon multimod-al language acquisition.
In Proc.
of ICMI.L.
Galescu, E. Ringger, & J. Allen.
1998.
Rapid language model de-velopment for new task domains.
In Proc.
of LREC.G.
Gorrell, I. Lewin, & M. Rayner.
2002.
Adding intelligent help tomixed initiative spoken dialogue systems.
In Proc.
of ICSLP.G.. Gorrell.
2003.
Using statistical language modeling to identify newvocabulary in a grammar-based speech recognition system.
InProc.
of Eurospeech.D.
Hakkani-Tur, G. Riccardi & A. Gorin.
2002.
Active learning forautomatic speech recognition.
In Proc.
of ICASSP.S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, & I. Le-win.
2001.
Comparing grammar-based and robust approaches tospeech understanding: A case study.
In Proc.
of Eurospeech.T.
Paek, S. Gandhe, D. Chickering & Y. Ju.
2007.
Handling out-of-grammar commands in mobile speech interaction using back-offfiller models.
In Proc.
of ACL Workshop on Grammar-Based Ap-proaches to Spoken Language Processing (SPEECHGRAM).M.
Rayner, B.A.
Hockey, & P. Bouillon.
2006.
Putting Linguisticsinto Speech Recognition: The Regulus Grammar Compiler.
CSLI.G.
Tur, M. Rahim & D. Hakkani-Tur.
2003.
Active labeling for spo-ken language understanding.
In Proc.
of Eurospeech.G.
Tur, R. Schapire, & D. Hakkani-Tur.
2003b.
Active learning forspoken language understanding.
In Proc.
of ICASSP.W3C.
2004.
Speech Recognition Grammar Specification Version 1.0.http://www.w3.org/TR/speech-grammarApproach iUtterancesTranscribedKeywordAccuracySlotAccuracyKeyword + SlotAccuracyProcessingTime (ms)OOG%CFG_Full1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10%2 590 66.20% (0.12%) 71.02% (0.23%) 70.59% (0.23%) 401 (4.0586) 31.92%3 1000 65.80% (0.15%) 69.72% (0.19%) 69.06% (0.19%) 422 (4.5804) 31.30%4 1393 66.10% (0.13%) 67.54% (0.22%) 66.88% (0.21%) 433 (4.7061) 30.95%CFG_Full +Back-off1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10%2 590 73.32% (0.11%) 72.11% (0.22%) 71.68% (0.23%) 562 (10.4696) 31.92%3 1000 72.52% (0.12%) 72.11% (0.21%) 71.46% (0.22%) 584 (10.4985) 31.30%4 1393 73.02% (0.10%) 71.02% (0.23%) 70.37% (0.23%) 592 (10.6805) 30.95%CFG_Active1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10%2 87 64.09% (0.13%) 74.29% (0.21%) 74.07% (0.22%) 395 (4.1469) 42.09%3 138 64.29% (0.15%) 70.15% (0.22%) 69.50% (0.24%) 409 (4.3375) 38.02%4 193 64.09% (0.15%) 69.72% (0.23%) 69.06% (0.24%) 413 (4.4015) 37.93%CFG_Active+ Back-off1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10%2 87 72.52% (0.10%) 76.91% (0.19%) 76.47% (0.21%) 568 (10.3494) 42.09%3 138 71.72% (0.14%) 71.90% (0.24%) 71.24% (0.27%) 581 (10.6330) 38.02%4 193 71.21% (0.15%) 71.90% (0.25%) 71.24% (0.26%) 580 (10.5266) 37.93%Table 2.
Semantic accuracies for partial (keyword or slot) and full phrase recognitions (keyword + slot) using a CFG trained on either?Full?
or ?Active?
transcriptions (i.e., selective transcriptions based on active learning).
Parentheses indicate standard error about the mean.The ?i?
column represents iteration.
The ?Utterances Transcribed?
column is cumulative.
The ?OOG%?
column represents coverage of theith CFG on the hold-out set.
Rows containing ?Back-off?
evaluate 2-pass recognition using both the CFG and a derived CFG back-off.67
