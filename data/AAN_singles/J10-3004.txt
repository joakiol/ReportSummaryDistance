Disentangling ChatMicha Elsner?Brown Laboratory for LinguisticInformation Processing (BLLIP)Eugene Charniak?
?Brown Laboratory for LinguisticInformation Processing (BLLIP)When multiple conversations occur simultaneously, a listener must decide which conversationeach utterance is part of in order to interpret and respond to it appropriately.
We refer to this taskas disentanglement.
We present a corpus of Internet Relay Chat dialogue in which the variousconversations have been manually disentangled, and evaluate annotator reliability.
We proposea graph-based clustering model for disentanglement, using lexical, timing, and discourse-basedfeatures.
The model?s predicted disentanglements are highly correlated with manual annotations.We conclude by discussing two extensions to the model, specificity tuning and conversation startdetection, both of which are promising but do not currently yield practical improvements.1.
MotivationSimultaneous conversations seem to arise naturally in both informal social interactionsand multi-party typed chat.
Aoki et al?s (2006) study of voice conversations among8?10 people found an average of 1.76 conversations (floors) active at a time, and amaximum of four.
In our chat corpus, the average is even higher, at 2.75.
The typicalconversation, therefore, does not form a contiguous segment of the chatroom transcript,but is frequently broken up by interposed utterances from other conversations.Disentanglement (also called thread detection [Shen et al 2006], thread extraction[Adams and Martell 2008], and thread/conversation management [Traum 2004]) isthe clustering task of dividing a transcript into a set of distinct conversations.
It isan essential prerequisite for any kind of higher-level dialogue analysis.
For instance,consider the multi-party exchange in Figure 1.Contextually, it is clear that this corresponds to two conversations, and Felicia?s1response excellent is intended for Chanel and Regine.
A straightforward reading of thetranscript, however, might interpret it as a response to Gale?s statement immediatelypreceding.?
Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.E-mail: melsner@cs.brown.edu.??
Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.E-mail: ec@cs.brown.edu.1 Real user nicknames are replaced with randomly selected identifiers for ethical reasons.Submission received: 27 January 2009; revised submission received: 1 November 2009; accepted forpublication: 3 March 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 3Figure 1Some (abridged) conversation from our corpus.Humans are adept at disentanglement, even in complicated environments likecrowded cocktail parties or chat rooms; in order to perform this task, they must maintaina complex mental representation of the ongoing discourse.
Moreover, they adjust theirconversational behavior to make the task easier, mentioning names more frequentlythan in spoken or two-party typed dialogues (O?Neill and Martin 2003).
This need foradaptation suggests that disentanglement can be challenging even for humans, andtherefore can serve as a useful stress test for computational models of discourse.Disentanglement has two practical applications.
One is the analysis of pre-recordedtranscripts in order to extract some kind of information, such as question?answer pairsor summaries.
These tasks should probably take as input each separate conversation,rather than the entire transcript.
Another application is as part of a user-interface systemfor active participants in the chat, in which users target a conversation of interestwhich is then highlighted for them.
Aoki et al (2003) created such a system for speech,which users generally preferred to a conventional system?when the disentanglementworked!We begin in Section 2 with an overview of related work.
In Section 3, we present anew corpus of manually annotated chat room data and evaluate annotator reliability.
Wegive a set of metrics (Section 3.2) describing structural similarity both locally and glob-ally.
In Section 4, we propose a model which uses supervised pairwise classification tolink utterances from the same conversation, followed by a greedy inference stage whichclusters the utterances into conversations.
Our system uses time gap and utterance con-tent features.
Experimental results (Section 5) show that its output is highly correlatedwith human annotations.
Finally, in Sections 6 and 7, we investigate two extensions tothe basic model, specificity tuning and automatic detection of conversation starts.To our knowledge, this is the first work to evaluate interannotator agreement forthe disentanglement task.
It is also the first to use a supervised method to learn weightsfor different feature types, rather than relying on cosine distance with uniform or hand-tuned feature weights.
It supplements standard word repetition and time gap featureswith other feature types, including very powerful features based on name mentioning,which is common in Internet Relay Chat.2.
Related WorkSeveral threads of research are direct attempts to solve the disentanglement problem.The closest to our own work is that of Shen et al (2006), which performs conversationdisentanglement on an online chat corpus.
Aoki et al (2003, 2006) disentangle speech,rather than chat.
Other work has slightly different goals than ours: Adams and Martell(2008) attempt to find all utterances of a specific single conversation in Internet andNavy tactical chat.
Camtepe et al (2005) and Acar et al (2005) perform social network390Elsner and Charniak Disentangling Chatanalysis, extracting groups of speakers who talk to one another.
This can be considereda disentanglement task, although, as we will see (Section 5), the assumption that eachspeaker participates in only one conversation is flawed.Adams and Martell (2008) and Shen et al (2006) publish results on human-annotated data; although we do not have their corpora, we discuss their evaluationmetrics (Section 3.2) and give a comparison to our own results herein (Section 5).
Aokiet al (2006) construct an annotated speech corpus, but they give no results for modelperformance, only user satisfaction with their conversational system.
Camtepe et al(2005) and Acar et al (2005) do give performance results, but only on synthetic data.Adams and Martell (2008) and Shen et al (2006) treat disentanglement in the sameway we do, as a clustering task where the objects to be clustered are the individualutterances.
Both their algorithms define a notion of distance (based on the cosine) anda threshold parameter determining how close to the cluster center an utterance must bebefore the clustering algorithm adds it.
This stands in contrast to our own supervisedapproach, where the distance metric is explicitly tuned on training data.
The super-vised method has the advantage that it can weigh individual feature types based ontheir predictivity?unsupervised methods combine features either uniformly or usingheuristic methods.
There is also no need for a separate tuning phase to determine thethreshold.
On the other hand, supervised methods require labeled training data, andmay be more difficult to adapt to novel domains or corpora.The remaining papers treat the problem as one of clustering speakers, rather thanutterances.
That is, they assume that during the window over which the system oper-ates, a particular speaker is engaging in only one conversation.
Camtepe et al (2005)state an explicit assumption that this is true throughout the entire transcript; real speak-ers, by contrast, often participate in many conversations, sequentially or sometimeseven simultaneously.
Aoki et al (2003) analyze each 30-second segment of the transcriptseparately.
This makes the single-conversation restriction somewhat less severe, but hasthe disadvantage of ignoring all events which occur outside the segment.Acar et al (2005) attempt to deal with this problem by using a fuzzy algorithm tocluster speakers; this assigns each speaker a distribution over conversations rather thanmaking a hard assignment.
However, the algorithm still deals with speakers rather thanutterances, and cannot determine which conversation any particular utterance is part of.Another problem with these two approaches is the information used for clustering.Aoki et al (2003) and Camtepe et al (2005) detect the arrival times of messages, anduse them to construct an affinity graph between participants by detecting turn-takingbehavior among pairs of speakers.
(Turn-taking is typified by short pauses betweenutterances; speakers aim neither to interrupt nor leave long gaps.)
Aoki et al (2006) findthat turn-taking on its own is inadequate.
They motivate a richer feature set, which,however, does not yet appear to be implemented.
Acar et al (2005) add word repetitionto their feature set.
However, their approach deals with all word repetitions on an equalbasis, and so degrades quickly in the presence of ?noise words?
(their term for wordswhich are shared across conversations) to almost complete failure when only half of thewords are shared.Adams and Martell (2008) and Shen et al (2006) use a more robust representationfor lexical features: Term frequency?inverse document frequency (TF?IDF) weightedunigrams, as often used in information extraction.
This feature set works fairly wellalone, although time is also a key feature as in the other studies.
Adams and Martell(2008) also investigate WordNet hypernyms (Miller et al 1990) as a measure of semanticrelatedness, and use the identity of the speaker of a particular utterance as a feature.
Itis unclear from their results whether these latter two features are effective or not.391Computational Linguistics Volume 36, Number 3To motivate our own approach, we examine some linguistic studies of discourse,especially analysis of multi-party conversation.
O?Neill and Martin (2003) point outseveral ways in which multi-party text chat differs from typical two-party conversation.One key difference is the frequency with which participants mention each others?names.
They hypothesize that name mentioning is a strategy which participants useto make disentanglement easier, compensating for the lack of cues normally present inface-to-face dialogue.
Mentions (such as Gale?s comments to Arlie in Figure 1) are verycommon in our corpus, occurring in 36% of comments, and provide a useful feature.Another key difference is that participants may create a new conversation (floor) atany time, a process which Sacks, Schegloff, and Jefferson (1974) calls schisming.
Duringa schism, a new conversation is formed, not necessarily because of a shift in the topic,but because certain participants have refocused their attention onto each other, andaway from whoever held the floor in the parent conversation.Despite these differences, there are still strong similarities between chat and otherconversations such as meetings.
Meetings do not typically allow multiple simultaneousconversations, but analogues to schisms do exist, in the form of digressions or ?sub-ordinate conversations,?
in which the speaker addresses someone specific, who is thenexpected to answer.
Some meeting analysis systems attempt to discover where thesedigressions begin, who is involved in them, and who has the floor when they end;features used in this work are relevant to disentanglement.The task of automatically determining the intended recipient of an utterance in ameeting is called addressee identification.
It requires detecting digressions and identi-fying their participants.
Several studies attempt this task.
Jovanovic and op den Akker(2004) and Jovanovic, op den Akker, and Nijholt (2006) perform addressee identificationusing a complex feature set including linguistic cues like pronouns and discourse mark-ers, temporal information, and gaze direction.
They also find that addressee identitycan be annotated with high reliability (?
= .7 for one set and .8 for another).
Traum(2004) discusses the necessity for addressee identification and disentanglement in thedesign of a system for military dialogues involving virtual agents.
Subsequent work(Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracyon addressee identification.Another meeting-related task is floor tracking, which attempts to determine whichspeaker has the floor after each utterance.
This task involves modeling the coordinationstrategies which speakers use to acquire or give up the floor, and so provides a goodmodel of an ongoing conversation.
A detailed analysis is given in Chen et al (2006);Chen (2008) also gives a model for detecting floor shifts.
Hawes, Lin, and Resnik (2008)use a conditional random fields (CRF) model to predict the next speaker in SupremeCourt oral argument transcripts.A somewhat related area of research involves environments with higher latencythan real-time chat: message boards and e-mail.
Content matching approaches tendto work better in these settings, because while many chat messages are backchannelresponses or discourse requests (Yes, Why?
and so forth), longer posts tend to becontentful.
Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristicinformation from message headers can be useful, as can content-based matching suchas detecting quotes from earlier messages.
Wang et al (2008), an analysis of a studentdiscussion group, is the work on which Adams and Martell (2008) is based, and usesvery similar methodology based on TF-IDF.Our two-stage classification and partitioning algorithm draws on work on corefer-ence resolution.
Many approaches to coreference (starting with Soon, Ng, and Lim 2001)use such an approach, building global clusters based on pairwise decisions made by a392Elsner and Charniak Disentangling Chatclassifier.
The global partitioning problem was identified as correlation clustering, anNP-hard problem, by McCallum and Wellner (2004).Finally, we briefly mention some work which appeared after we developed thesystem described here.
Wang and Oard (2009) is another system that uses TF?IDFunigrams, but augments these feature vectors using the information retrieval techniqueof message expansion.
They report results on our corpus which improve on our own.Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation todescribe semantic relatedness.
He finds both techniques ineffective.
In addition, heannotates a large corpus of Internet Relay Chat and similarly finds that annotators havetrouble agreeing.
Elsner and Schudy (2009) explore different partitioning strategies,improving on the greedy algorithm that we present.3.
Data SetOur data set is recorded from IRC channel ##LINUX at free?node.net, using the freelyavailable gaim client.
##LINUX is an unofficial tech support line for the Linux operatingsystem, selected because it is one of the most active chat rooms on freenode, leadingto many simultaneous conversations, and because its content is typically inoffensive.Although it is notionally intended only for tech support, it includes large amounts ofsocial chat as well, such as the conversation about factory work in Figure 1.The entire data set contains over 52 hours of chat, but we devote most of ourattention to three annotated sections: development (706 utterances; 2:06 hr) and test(800 utterances; 1:39 hr), plus a short pilot section on which we tested our annotationsystem (359 utterances; 0:58 hr).3.1 AnnotationWe recruited and paid seven university students to annotate the test section.
All hadat least some familiarity with the Linux OS, although in some cases very slight.
Anno-tation of the test data set typically took them about two hours.
In all, we produced sixannotations of the test set.2We have four annotations of the pilot set, by three volunteers and the experimenters.The pilot set was used to prototype our annotation software, and also as a validation cor-pus for our system.
The development set was annotated only once, by the experimenter.This data set is used for training.Our annotation scheme marks each utterance as part of a single conversation.Annotators are instructed to create as many or as few conversations as they need todescribe the data.
Our instructions state that a conversation can be between any numberof people, and that, ?We mean conversation in the typical sense: a discussion in whichthe participants are all reacting and paying attention to one another .
.
.
it should be clearthat the comments inside a conversation fit together.?
The annotation system itself is asimple Java program with a graphical interface, intended to appear somewhat similarto a typical chat client.
Each speaker?s name is displayed in a different color, and thesystem displays the elapsed time between comments, marking especially long pauses2 One additional annotation was discarded because the annotator misunderstood the task.393Computational Linguistics Volume 36, Number 3in red.
Annotators group utterances into conversations by clicking and dragging themonto each other.3.2 MetricsBefore discussing the annotations themselves, we will describe the metrics we use tocompare different annotations; these measure both how much our annotators agreewith each other, and how well our model and various baselines perform.
Comparingclusterings with different numbers of clusters is a non-trivial task, and metrics foragreement on supervised classification, such as the ?
statistic, are not applicable.To measure global similarity between annotations, we use one-to-one accuracy.This measure describes how well we can extract whole conversations intact, as requiredfor summarization or information extraction.
To compute it, we pair up conversationsfrom the two annotations to maximize the total overlap by computing an optimal max-weight bipartite matching, then report the percentage of overlap found.3 One-to-oneaccuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighiand Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreferenceresolution.If we intend to monitor or participate in the conversation as it occurs, we will caremore about local judgments.
The local agreement metric is a constrained form of theRand index for clusterings (Rand 1971) which counts agreements and disagreements forpairs within a context k. We consider a particular utterance: The previous k utterancesare each in either the same or a different conversation.
The lock score between twoannotators is their average agreement on these k same/different judgments, averagedover all utterances.
For example, loc1 counts pairs of adjacent utterances for which twoannotations agree.Several related papers use some variant of the F-score metric to measure accuracy.The most complete treatment is given in Shen et al (2006).
They use a micro-averagedF-score, which is defined by constructing a multiway matching between conversationsin the two annotations.
For a gold conversation i with size ni, and a proposed conver-sation j with size nj, with overlap of size nij, they define precision and recall (plus thestandard balanced F-score).
The F-score of an entire annotation is a weighted sum overthe matching:P =nijnjR =nijniF(i, j) = 2PRP+ RF =?inin maxjF(i, j) (1)This is the F-score we report for comparative purposes.
Because the match is multiway,the score is not symmetric; when measuring agreement between pairs of human anno-tators (where there is no reason for one to be considered gold), we map the high-entropytranscript to the lower one (the entropy of a transcript is defined subsequently, inEquation 2).
Micro-averaged F-scores are also popular in work on document clustering.In general, scores using this metric are correlated with our other measurement of globalconsistency, the one-to-one accuracy.3 The matching can be computed efficiently with the so-called Hungarian algorithm or by reduction to maxflow.
The widely used greedy algorithm is a two-approximation, although we have not found largedifferences in practice.394Elsner and Charniak Disentangling ChatAdams and Martell (2008) also report F-score, but using a somewhat differentdefinition.
They define F-score only between a particular pair of conversations, andreport the score for a single selected conversation.
They do not describe how thisreference conversation is chosen.
It is also unclear how they determine which proposedconversation to match to it?the one with the best F-score, or the one which containsthe first (or ?root?)
utterance of the reference conversation.
(The latter, although it maybe more useful for some applications, has an obvious problem?if the conversation isretrieved perfectly except for the root utterance, the score will be zero.)
For these reasonswe do not evaluate their metric.3.3 DiscussionA statistical examination of our data (Table 1) shows that there is a substantial amountof disentanglement to do: the average number of conversations active at a time (thedensity) is 2.75.
Our annotators have high agreement on the local metric (average of81.1%).
On the one-to-one metric, they disagree more, with a mean overlap of 53.0% anda maximum of only 63.5%.
Though this level of agreement is low, naive baselines scoreeven lower (see Section 5).
Therefore the metric does indeed distinguish human-likefrom baseline performance.
Thus measuring one-to-one overlap with our annotationsis a reasonable evaluation for computational models.
However, we feel that the majorsource of disagreement is one that can be remedied in future annotation schemes: thespecificity of the individual annotations.To measure the level of detail in an annotation, we use the information-theoreticentropy of the random variable, which indicates which conversation an utterance isin.
This variable has as many potential values as the number of conversations in thetranscript, each value having probability proportional to its size.
Thus, for a transcriptof length n, with conversations i each having size ni, the entropy is:H(c) =?inin log2nin (2)This quantity is non-negative, increasing as the number of conversations grow and theirsize becomes more balanced.
It reaches its maximum, 9.64 bits for this data set, whenTable 1Statistics on 6 annotations of 800 utterances of chat transcript.
Inter-annotator agreement metrics(below the line) are calculated between distinct pairs of annotations.Mean Max MinConversations 81.33 128 50Average conversation length 10.6 16.0 6.2Average conversation density 2.75 2.92 2.53Entropy 4.83 6.18 3.00one-to-one 52.98 63.50 35.63loc3 81.09 86.53 74.75Many-to-1 (by entropy) 86.70 94.13 75.50Shen F (by entropy) 53.87 66.08 35.43395Computational Linguistics Volume 36, Number 3each utterance is placed in a separate conversation.
In our annotations, it ranges from3.0 to 6.2.
This large variation shows that some annotators are more specific than others,but does not indicate how much they agree on the general structure.
To measure this,we introduce the many-to-one accuracy.
This measurement is asymmetrical, and mapseach of the conversations of the source annotation to the single conversation in thetargetwith which it has the greatest overlap, then counts the total percentage of overlap.This is not a statistic to be optimized (indeed, optimization is trivial: Simply make eachutterance in the source into its own conversation), but it can give us some intuitionabout specificity.
In particular, if one subdivides a coarse-grained annotation to make amore specific variant, the many-to-one accuracy from fine to coarse remains 1.
When wemap high-entropy annotations (fine) to lower ones (coarse), we find high many-to-oneaccuracy, with a mean of 86%, which implies that the more specific annotations havemostly the same large-scale boundaries as the coarser ones.By examining the local metric, we can see even more: Local correlations are good,at an average of 81.1%.
This means that, in the three-sentence window preceding eachsentence, the annotators are often in agreement.
If they recognize subdivisions of a largeconversation, these subdivisions tend to be contiguous, not mingled together, which iswhy they have little impact on the local measure.We find reasons for the annotators?
disagreement about appropriate levels of detailin the linguistic literature.
As mentioned, new conversations often break off from oldones in schisms.
Aoki et al (2006) discuss conversational features associated withschisming and the related process of affiliation, by which speakers attach themselves toa conversation.
Schisms often branch off from asides or even normal comments (toss-outs) within an existing conversation.
This means that there is no clear beginning to thenew conversation?at the time when it begins, it is not clear that there are two separatefloors, and this will not become clear until distinct sets of speakers and patterns of turn-taking are established.
Speakers, meanwhile, take time to orient themselves to the newconversation.
Example schisms are shown in Figures 2 and 3.Our annotation scheme requires annotators to mark each utterance as part of asingle conversation, and distinct conversations are not related in any way.
If a schismoccurs, the annotator is faced with two options: If it seems short, they may view it asa mere digression and label it as part of the parent conversation.
If it seems to deservea place of its own, they will have to separate it from the parent, but this severs theinitial comment (an otherwise unremarkable aside) from its context.
One or two of theannotators actually remarked that this made the task confusing.
Our annotators seemFigure 2A schism occurring in our corpus (abridged).
The schism-inducing turn is Kandra?s comment,marked by arrows.
Annotators 0 and 2 begin a new conversation with this turn; 1, 4, and 5 groupit with the other utterances shown; 3 creates new conversations for both this turn and Madison?squestion immediately following.396Elsner and Charniak Disentangling ChatFigure 3A schism occurring in our corpus (abridged): not all annotators agree on where the thread aboutcharging for answers to technical questions diverges from the one about setting up Paypalaccounts.
The schism begins just after Lai?s second comment (marked with arrows), to whichGale and Azzie both respond (marked with double arrows).
Annotators 1, 2, 4, and 5 begin a newconversation with Gale?s response.
Annotator 0 starts a new conversation with Azzie?s response.Annotator 3 makes an error, linking the two responses to each other, but not to the parent.Figure 4Utterances versus conversations participated in per speaker on development data.to be either ?splitters?
or ?lumpers?
?in other words, each annotator seems to aim fora consistent level of detail, but each one has their own idea of what this level should be.As a final observation about the data set, we test the appropriateness of the assump-tion (used in previous work) that each speaker takes part in only one conversation.
Inour data, the average speaker takes part in about 3.3 conversations (the actual numbervaries for each annotator).
The more talkative a speaker is, the more conversations theyparticipate in, as shown by a plot of conversations versus utterances (Figure 4).
Theassumption is not very accurate, especially for speakers with more than 10 utterances.4.
ModelOur model for disentanglement fits into the general class of graph partitioning algo-rithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, includingcoreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting seg-mentation (Malioutov and Barzilay 2006).
These algorithms operate in two stages: First,397Computational Linguistics Volume 36, Number 3Table 2Feature functions with performance on development data.Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66)Time The time between x and y in seconds, discretized into logarithmically sized bins.Speaker x and y have the same speaker.Mention x-y x mentions the speaker of y (or vice versa).
For example, this feature is true fora pair such as: Felicia ?Gale: ... and any utterance spoken by Gale.Mention same Both x and y mention the same name.Mention other either x or y mentions a third person?s name.Discourse (Acc: 52 Prec: 47 Rec: 77 F: 58)Cue words Either x or y uses a greeting (hello etc.
), an answer (yes, no etc.
), or thanks.Question Either asks a question (explicitly marked with ?
).Long Either is long (> 10 words).Content (Acc: 50 Prec: 45 Rec: 74 F: 56)Repeat(i) The number of words shared between x and y which have unigramprobability i, binned logarithmically.Tech Whether both x and y use technical jargon, neither do, or only one does.Combined (Acc: 75 Prec: 73 Rec: 68 F: 71)a binary classifier marks each pair of items as alike or different, and second, a consistentpartition is extracted.44.1 ClassificationWe use a maximum-entropy classifier (Daume?
2004) to decide whether a pair of utter-ances x and y are in same or different conversations.
The most likely class is different,which occurs 57% of the time in the development data.
We describe the classifier?s per-formance in terms of raw accuracy (correct decisions/total), precision and recall of thesame class, and F-score, the harmonic mean of precision and recall.
Our classifier usesseveral types of features (Table 2).
The chat-specific features yield the highest accuracyand precision.
Discourse and content-based features have poor accuracy on their own(worse than the baseline), because they work best on nearby pairs of utterances, andtend to fail on more distant pairs.
Paired with the time gap feature, however, they boostaccuracy somewhat and produce substantial gains in recall, encouraging the model togroup related utterances together.The classifier is trained on our single annotation of the 706-utterance developmentsection and validated against the 359-utterance pilot section.4 Our first attempt at this task used a Bayesian generative model.
However, we could not define a sharpenough posterior over new sentences, which made the model unstable and overly sensitive to its prior.398Elsner and Charniak Disentangling ChatFigure 5Distribution of pause length (log-scaled) between utterances in the same conversation.The time gap, as discussed earlier, is the most widely used feature in previous work.Our choice of a logarithmic binning scheme is intended to capture two characteristics ofthe distribution of pause lengths (shown in Figure 5).
The curve has its maximum at 1?3 seconds, and pauses shorter than a second are less common.
This reflects turn-takingbehavior among participants; participants in the same conversation prefer to wait foreach others?
responses before speaking again.
On the other hand, the curve is quiteheavy-tailed to the right, leading us to bucket long pauses fairly coarsely.
The specificdiscretization we adopt for a time gap ?
is bin(?)
= floor(log1.5(?+ 1)).
The particularchoice of 1.5 was chosen by hand to fit the observed scale of the curve.Our discourse-based features model some pairwise relationships: questions fol-lowed by answers, short comments reacting to longer ones, greetings at the beginning,and thanks at the end.Word repetition is a key feature in nearly every model for segmentation or coher-ence, so it is no surprise that it is useful here.
We discard the 50 most frequent words.Then we bin all words by their unigram probability (bin(w) = floor(log10(p(w))) andcreate an integer-valued feature for each bin, equal to the number of repeated words inthat bin.
Unigram probabilities are calculated over the entire 52 hours of transcript.
Thebinning scheme allows us to deal with ?noise words?
which are repeated coincidentally,because these occur in high-probability bins where repetitions are given less weight.The point of the repetition feature is of course to detect sentences with similartopics.
We also find that sentences with technical content are more likely to be relatedthan non-technical sentences.
We label an utterance as technical if it contains a Webaddress, a long string of digits, or a term present in a guide for novice Linux users5but not in a large news corpus (Graff 1995).6 This is a lightweight way to captureone ?semantic dimension?
or cluster of related words.
The technical word feature wasincluded because it improves our development classification score slightly, but it doesnot have a significant effect on overall performance.
Adams (2008) attempts to addmore semantic dimensions learned via Latent Dirichlet Allocation, and similarly findsno improvement.Pairs of utterances which are widely separated in the discourse are unlikely to bedirectly related?even if they are part of the same conversation, the link between themis probably a long chain of intervening utterances.
Thus, if we run our classifier on apair of very distant utterances, we expect it to default to the majority class, which in thiscase will be different, and this will damage our performance in case the two are reallypart of the same conversation.
To deal with this, we run our classifier only on utterances5 Introduction to Linux: A Hands-on Guide.
Machtelt Garrels.
Edition 1.25 fromhttp://tldp.org/LDP/intro-linux/html/intro-linux.html.6 Our data came from the LA Times, 1994?1997 ?
helpfully, this corpus predates the current wide coverageof Linux in the mainstream press.399Computational Linguistics Volume 36, Number 3Figure 6VOTE algorithm.separated by 129 seconds or less.
The cutoff of 129 seconds was chosen because, forutterances further apart than this, the classifier has no significant advantage over themajority baseline.
For 99.9% of utterances in an ongoing conversation, the previousutterance in that conversation is within this gap, and so the system has a chance ofcorrectly linking the two.On test data, the classifier has a mean accuracy of 68.2 (averaged over annotations).The mean precision of same conversation is 53.3 and the recall is 71.3, with a meanF-score of 60.
This error rate is high, but the partitioning procedure allows us to recoverfrom some of the errors, because if nearby utterances are grouped correctly, the baddecisions will be outvoted by good ones.4.2 PartitioningThe next step in the process is to cluster the utterances.
We wish to find a set of clustersfor which the weighted accuracy of the classifier would be maximal; this is an exampleof correlation clustering (Bansal, Blum, and Chawla 2004), which is NP-complete.
Theinput to our partitioning procedure is a graph with a node for each utterance; if theclassifier connects utterances i and j with probability p, we take the weight wij of edge ijto be the log odds log(pij/(1 ?
pij)).7 We create a variable xij for each pair of utterances,which is 1 if the utterances are placed in the same conversation, and 0 if they areseparated.
The log probability of the clustering, treating the edges as independent, is?ij:i<j wijxij.
We attempt to maximize this quantity, subject to the constraint that the xijmust form a legitimate clustering such that xij = xjk = 1 implies xij = xik.Finding an exact solution proves to be difficult; the problem has a quadratic numberof variables (one for each pair of utterances) and a cubic number of triangle inequalityconstraints (three for each triplet).8 With 800 utterances in our test set, even solvingthe linear relaxation of the problem with CPLEX (Ilog, Inc. 2003) is too expensive to bepractical.Experiments on a variety of heuristic algorithms (Elsner and Schudy 2009) showthat a relatively good solution can be obtained using a greedy voting algorithm (Fig-ure 6).
In this algorithm, we assign utterance j by examining all previously assigned7 The original version of our system used a different weighting scheme, wij = pij ?
.5.
The log-odds ratiobehaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithmsor tuning (see Section 6), so, for simplicity, we present it here as well.8 There is a triangle inequality constraint for each triplet i, j, k: (1 ?
xik ) ?
(1 ?
xij )+ (1 ?
xjk ).400Elsner and Charniak Disentangling Chatutterances i, and treating the classifier?s judgment wij as a vote for cluster(i).
If themaximum vote is greater than 0, we set cluster( j) = argmaxc votec.
Otherwise j is putin a new cluster.If the utterances are considered in order, this is a natural on-line algorithm?itassigns each utterance as it arrives, without reference to the future.
Elsner and Schudy(2009) show that performance can be improved by approximately 6% on the one-to-oneand F-score metrics using offline randomized and local search methods.
The loc3 metricis insensitive to these more complex search procedures.5.
ExperimentsWe annotate the 800-line test transcript using our system.
The annotation obtained has62 conversations, with mean length 12.90.
The average density of conversations is 2.86,and the entropy is 3.72.
This places it within the bounds of our human annotations (seeTable 1), toward the more general end of the spectrum.As a standard of comparison for our system, we provide results for severalbaselines?trivial systems which any useful annotation should outperform.All different Each utterance is a separate conversation.All same The whole transcript is a single conversation.Blocks of k Each consecutive group of k utterances is a conversation.Pause of k Each pause of k seconds or more separates two conversations.Speaker Each speaker?s utterances are treated as a monologue.For each particular metric, we calculate the best baseline result among all of these.To find the best block size or pause length, we search over multiples of five between5 and 300.
This makes these baselines appear better than they really are, because theirperformance is optimized with respect to the test data.
(A complete table of baselineresults is shown in Figure 7.
)We also calculate results for two more systems.
One is a non-trivial baseline:Time/mention Our system, using only time gap and mention-based features.The other is an oracle, designed to test how well a segmentation system designed formeeting or lecture data might possibly do on this task.
If no conversation were everinterrupted, such a system would be perfect (up to the limit of annotator agreement).Figure 7Metric values for all baselines.401Computational Linguistics Volume 36, Number 3Table 3Metric values between proposed annotations and human annotations.
Model scores typically fallbetween inter-annotator agreement and baseline performance.Annotators Model Time/ment.
Perf.
Seg.
Best BaselineMean one-to-one 52.98 41.23 38.62 26.20 35.08 (Pause 35)Max one-to-one 63.50 52.12 44.12 36.50 56.00 (Pause 65)Min one-to-one 35.63 31.62 30.62 15.38 27.50 (Blocks 80)Mean loc3 81.09 72.94 68.69 75.98 62.16 (Speaker)Max loc3 86.53 74.70 70.93 85.40 69.05 (Speaker)Min loc3 74.75 70.77 66.37 69.05 54.37 (Speaker)Mean Shen F 53.87 43.47 41.31 35.50 36.58 (Speaker)Max Shen F 66.08 57.57 48.85 46.70 46.79 (Speaker)Min Shen F 35.43 32.97 32.07 21.83 29.09 (Blocks 65)Perfect segments The transcript is divided into contiguous segments, where all utter-ances in a segment belong to the same conversation.
The conversation assign-ments are determined by the human annotation whose agreement with the othersis highest.Our results, in Table 3, are encouraging.
On average, annotators agree more witheach other than with any artificial annotation, and more with our model than withthe baselines.
For the one-to-one accuracy metric, we cannot claim much beyond thesegeneral results.
The range of human variation is quite wide, and there are annotatorswho are closer to baselines than to any other human annotator.
As explained earlier,this is because some human annotations are much more specific than others.
For veryspecific annotations, the best baselines are short blocks or pauses.
For the most general,marking all utterances the same does very well (although for all other annotations, it isextremely poor).For the local metric, the results are much clearer.
There is no overlap in the ranges;for every test annotation, agreement is highest with other annotators, then our model,and finally the baselines.
The most competitive baseline is one conversation per speaker,which makes sense, since if a speaker makes two comments in a four-utterance win-dow, they are very likely to be related.The Shen F-score metric seems to perform similarly to the one-to-one accuracy,which is unsurprising because they are both measures of global consistency.
The largestdifference between them is that the speaker baseline outperforms blocks and pauses inF-score (although not by very much), perhaps because it is more precise.Shen et al (2006) report higher F-scores for their own best model: It obtains anF-score of 61.2, whereas our model?s mean score is only 43.4.
Because of the differentcorpora, we are unable to explain this difference.
Better results are also reported in Wangand Oard (2009) and Elsner and Schudy (2009) (see Table 4).Mention information alone is not sufficient for disentanglement; with only namemention and time gap features, mean one-to-one is 38 and loc3 is 69.
However, namemention features are critical for our model.
Without them, the classifier?s developmentF-score drops from 71 to 56.
The disentanglement system?s test performance decreasesproportionally; mean one-to-one falls to 36, and mean loc3 to 63, essentially baseline per-formance.
For some utterances, of course, name mentions provide the only reasonable402Elsner and Charniak Disentangling ChatTable 4Results reported by others on the same task.Result F-score Notesthis model 43.4Elsner and Schudy (2009) 50 improved partitioning inferenceWang and Oard (2009) 54 message expansion featuresShen et al (2006) 61.2 different corpusclue to the correct decision, which is why humans mention names in the first place.But our system is probably overly dependent on them, because they are very reliablecompared to our other features.Because of the frequency with which conversations interleave, perfect segmenta-tion alone is not sufficient to optimize either global metric, and generally does notoutperform the baselines.
For the local metric, however, it generally does better than themodel.
Here, performance depends mainly on whether the system can find the bound-aries between one conversation and another, and it is less important to link the segmentsof a particular conversation to one another, since these different segments often lieoutside the three-utterance horizon.
Systems designed to detect segment boundaries,like those for meetings, might contribute to improvement of this metric.6.
Specificity TuningAlthough our analysis shows that individual annotators can produce more or lessspecific annotations of the same conversation, the system described here can produceonly a single annotation (for any given set of training data) with a fixed specificity.
Nowwe attempt to control the specificity parametrically, producing more and less specificannotations on demand, without retraining the classifier.The parameter we choose to alter is the bias of our pairwise classifier.
A maximum-entropy classifier has the form:y(x) = 11+ exp(?
(w ?
x+ b))(3)Here w represents the vector of feature weights and b is the bias term; a positive b shiftsall judgments toward high-confidence same conversation decisions and a negative bshifts them away.
To alter the classifier, we add a constant ?
to b.
In general, increasingthe number and confidence of same decisions leads to larger, coarser partitionings,and decreasing it creates smaller, finer ones.
We measure specificity by examining theentropy of the output annotation.
Although entropy is generally an increasing functionof ?, the relationship is not always smooth, nor is it completely monotonic.
Figure 8plots entropy as a function of ?.In Figure 9, we plot the one-to-one match between each test annotation and thealtered annotations produced by this method, as a function of the entropy.
The unbiasedsystem creates an annotation with entropy 3.7.
Although this yields reasonable resultsfor all human annotations, each of the annotations has a point of higher performanceat a different bias level.
For instance, the line uppermost on the left side of the plot403Computational Linguistics Volume 36, Number 3Figure 8Entropy of the output annotation produced with bias factor ?
on test data.
?
= 0 corresponds tothe unbiased system.shows overlap with a human transcript whose entropy is 3.0 bits; lower-entropy systemannotations correspond better with this annotator?s judgments.For each human annotation, we evaluate the tuned system?s performance at theentropy level of the original annotation.
(This point is marked by the large dot onFigure 9One-to-one accuracy between biased system annotations and each test annotation, as a functionof entropy.
The vertical line (at 3.72 bits) marks the scores obtained by the unbiased system with?
= 0.
The large dot on each line is the score obtained at the entropy of the human annotation.404Elsner and Charniak Disentangling ChatTable 5Metric values between proposed annotations and human annotations on test data.
The tunedmodel (evaluated at the entropy of the human annotations) improves on one-to-one accuracy butnot on loc3.Unbiased Model Tuned ModelMean one-to-one 41.23 48.52Max one-to-one 52.13 58.75Min one-to-one 31.66 40.88Mean loc3 72.94 73.64Max loc3 74.70 75.87Min loc3 70.77 69.95each line in the figure.)
To do this, we perform a line search over ?
until we producea clustering whose entropy is within .25 bits of the original?s, then evaluate.
In otherwords, we measure performance given an additional piece of supervision?the annota-tor?s preferred specificity level.Results on the one-to-one metric are fairly good: Extreme and average scores arelisted in Table 5.
The effects of this technique on the local metric are small (and inmany cases negative).
This is not entirely surprising, as the local metric is less sensitiveto specificity of annotations.
Slight positive effects occur only for the most and leastspecific annotation, which are presumably so extreme that specificity begins to have aslight effect even on local decisions.Despite fairly large performance increases on the test set, we do not consider thistechnique really reliable, because the relationship between the bias parameter and finalscore is not smooth.
Small changes in the bias can cause large shifts in entropy, and smallchanges in entropy can have large effects on quality.
(For instance, two annotations havea sharp decline in score at about entropy 5.7, losing about 5% of performance with achange of just over .1 bit.)
Therefore it is not clear exactly how to choose a bias parameterwhich will yield good performance.
Matching the entropy of a human annotation seemsto work on the test data, but fails to improve scores on our development data.
Moreover,although for methodological simplicity we assume access to the exact target entropy foreach annotation, it is unlikely that a real user could express their desired specificityso precisely.
Figuring out a way to let the user select the desired entropy remains achallenge.7.
Detecting Conversation StartsIn this section, we investigate better ways to find the beginnings of new conversations.In the pairwise-linkage representation presented earlier, a new conversation is begunwhen none of the previous utterances is strongly linked to the current utterance.
Thisrepresentation spreads out the responsibility for detecting a new conversation overmany pairwise decisions.
We are inspired by the use of discourse-new classifiers (alsocalled anaphoricity detectors) in coreference classification (Ng and Cardie 2002) to findNPs which begin coreferential chains.
Oracle experiments show that a similar detectorfor utterances which begin conversations could improve disentanglement scores if itwere available.
We attempt to develop such a detector, but without much success.405Computational Linguistics Volume 36, Number 3Table 6Metric values using an oracle new-conversation detector on test data.Original Model +Oracle New ConversationsMean one-to-one 41.23 46.75Max one-to-one 52.13 53.50Min one-to-one 31.66 42.13Mean loc3 72.94 73.90Max loc3 74.70 76.49Min loc3 70.77 70.72As a demonstration of the gains possible if a good classifier could be developed,we show the oracle improvements possible on the test data, using an optimal new-conversation detector as a hard constraint on inference (Table 6).
The oracle detectordetects a conversation start if it occurs in the majority of human annotations, and theinference algorithm is forced to start a new conversation if and only if the oracle hasdetected one.
Good conversation detection is capable of improving not only one-to-oneaccuracy but local accuracy as well.We can track the performance of realistic, non-oracle new-conversation detectionvia the precision, recall, and F-score of the new conversation class (Table 7).
As astarting point, we report the accuracy obtained by the pairwise-linkage model andgreedy inference already presented.
At 49% F-score, it is clearly not doing a good job.It is possible to do better than this using information already represented in thepairwise classifier: The time since the speaker of the utterance last spoke (logarithmi-cally bucketed), and whether the utterance mentions a name.
A better representation forthe problem allows the classifier to make somewhat more effective use of these features.For reasons we cannot explain, adding discourse features like the presence of a questionor greeting does not improve performance.
The simple classifier does improve slightlyon the baseline, up to 51%.
These test results, however, are somewhat surprising to us.On our development corpus, the corresponding scores are 69% and 75%.
Because thatcorpus contains an average (over three annotations) of 34 conversations, it is likely thatwe were misled by coincidentally good results.On the development set, where the classifier works well, its decisions can be inte-grated with inference to yield substantial improvements in actual system performance.Mean loc3 increases from 72% to about 78% and mean one-to-one accuracy from 41%to about 66%.
However, we find no improvement at all on the test data, becausethe classifier has very low recall, and the resulting test annotations have far too fewconversations.Table 7Precision, recall, and F-score of the new conversation class on test data (average 81conversations).Precision Recall F-scorePairwise system 56.08 43.44 48.96Time/Mention Features 68.06 40.16 50.52Human Annotators 64.30 61.70 61.14406Elsner and Charniak Disentangling Chat8.
Future WorkAlthough our annotators are reasonably reliable, it seems clear that they think of con-versations as a hierarchy, with digressions and schisms.
We are interested to see an an-notation protocol which more closely follows human intuition.
One suggestion (DavidTraum, personal communication) is to drop the idea of partitioning entirely and haveannotators mark the data as a graph, linking each utterance to its parents and childrenwith links of various strengths.
Such a scheme might yield more reliable annotationsthan our current one, although testing this hypothesis would require new annotationsoftware and a different set of metrics.
Any new annotation project should also investi-gate whether annotators can define their desired specificity, and with what precision.Our results on new conversation detection suggest that a high-performance clas-sifier for this task could improve results substantially.
It is also interesting to consider,given the weakness of our technical words feature and the disappointing results usingLatent Dirichlet Allocation from Adams (2008), how semantic similarity might be use-fully modeled.Finally, we are interested to see how well this feature set performs on speech data,as in Aoki et al (2003).
Spoken conversation is more natural than text chat, but evenwhen participants are face-to-face, disentanglement remains a problem.
On the otherhand, spoken dialogue contains new sources of information, such as prosody and gazedirection.
Turn-taking behavior is also more distinct, which makes the task easier, butaccording to Aoki et al (2006), it is certainly not sufficient.9.
ConclusionThis work provides a corpus of annotated data for chat disentanglement, which, alongwith our proposed metrics, should allow future researchers to evaluate and comparetheir results quantitatively.9 Our annotations are consistent with one another, especiallywith respect to local agreement.
We show that features based on discourse patterns andthe content of utterances are helpful in disentanglement.
The model we present canoutperform a variety of baselines.AcknowledgmentsOur thanks to Suman Karumuri, SteveSloman, Matt Lease, David McClosky, seventest annotators, three pilot annotators, threeanonymous conference reviewers, threeanonymous journal reviewers, and the NSFPIRE grant.
We would also like to thankCraig Martell and David Traum for theirvery useful comments at the conferencepresentation.ReferencesAcar, Evrim, Seyit Ahmet Camtepe,Mukkai S. Krishnamoorthy, and Bu?lentYener.
2005.
Modeling and multiwayanalysis of chatroom tensors.
In Paul B.Kantor, Gheorghe Muresan, Fred Roberts,Daniel Dajun Zeng, Fei-Yue Wang,Hsinchun Chen, and Ralph C. Merkle,editors, ISI, volume 3495 of Lecture Notesin Computer Science.
Springer, Berlin,pages 256?268.Adams, Paige H. 2008.
Conversation ThreadExtraction and Topic Detection in Text-basedChat.
Ph.D. thesis, Naval PostgraduateSchool.Adams, Paige H. and Craig H. Martell.
2008.Topic detection and extraction in chat.International Conference on SemanticComputing, 2:581?588.Aoki, Paul M., Matthew Romaine,Margaret H. Szymanski, James D.Thornton, Daniel Wilson, and AllisonWoodruff.
2003.
The mad hatter?scocktail party: A social mobile audio9 Our software and data set are publicly available from cs.brown.edu/?melsner.407Computational Linguistics Volume 36, Number 3space supporting multiple simultaneousconversations.
In CHI ?03: Proceedings ofthe SIGCHI Conference on Human Factorsin Computing Systems, pages 425?432,New York, NY.Aoki, Paul M., Margaret H. Szymanski,Luke D. Plurkowski, James D. Thornton,Allison Woodruff, and Weilie Yi.
2006.Where?s the ?party?
in ?multi-party??
:Analyzing the structure of small-groupsociable talk.
In CSCW ?06: Proceedings ofthe 2006 20th Anniversary Conference onComputer Supported Cooperative Work,pages 393?402, New York, NY.Bansal, Nikhil, Avrim Blum, and ShuchiChawla.
2004.
Correlation clustering.Machine Learning, 56(1-3):89?113.Camtepe, Seyit Ahmet, Mark K. Goldberg,Malik Magdon-Ismail, and MukkaiKrishnamoorty.
2005.
Detectingconversing groups of chatters: A model,algorithms, and tests.
In IADIS AC,pages 89?96, Algarve.Chen, Lei.
2008.
Incorporating NonverbalFeatures into Multimodal Models ofHuman-to-Human Communication.Ph.D.
thesis, Purdue University.Chen, Lei, Mary Harper, Amy Franklin,Travis R. Rose, Irene Kimbara,Zhongqiang Huang, and FrancisQuek.
2006.
A multimodal analysisof floor control in meetings.
InProceedings of MLMI 06, pages 36?49,Bethesda, MD.Daume?, III, Hal.
2004.
Notes on CG andLM-BFGS optimization of logisticregression.
Paper available at http://pub.hal3.name#daume04cg-bfgs.Implementation available at http://hal3.name/megam/.Elsner, Micha and Warren Schudy.
2009.Bounding and comparing methods forcorrelation clustering beyond ILP.
InProceedings of ILP-NLP, pages 19?27,Boulder, CO.Graff, David.
1995.
North American NewsText Corpus.
Linguistic Data Consortium.LDC95T21.Haghighi, Aria and Dan Klein.
2006.Prototype-driven learning for sequencemodels.
In Proceedings of HLT-NAACL,pages 320?327, New York, NY.Hawes, Timothy, Jimmy Lin, andPhilip Resnik.
2008.
Elements of acomputational model for multi-partydiscourse: The turn-taking behaviorof supreme court justices.
TechnicalReport LAMP-TR-147/HCIL-2008-02,University of Maryland, College Park.Ilog, Inc. 2003.
CPLEX solver.
Available atwww-01.ibm.com/software/websphere/ilog migration.html.Jovanovic, Natasa and Rieks op denAkker.
2004.
Towards automaticaddressee identification in multi-partydialogues.
In Proceedings of the5th SIGdial Workshop, pages 89?92,Cambridge, MA.Jovanovic, Natasa, Rieks op den Akker,and Anton Nijholt.
2006.
Addresseeidentification in face-to-face meetings.
InProceedings of EACL, Trento.Luo, Xiaoqiang.
2005.
On coreferenceresolution performance metrics.
InProceedings of HLT-EMNLP, pages 25?32,Morristown, NJ.Malioutov, Igor and Regina Barzilay.
2006.Minimum cut model for spoken lecturesegmentation.
In Proceedings of ACL,pages 25?32, Sydney.McCallum, Andrew and Ben Wellner.2004.
Conditional models of identityuncertainty with application to nouncoreference.
In Proceedings of the 18thAnnual Conference on Neural InformationProcessing Systems (NIPS), pages 905?912,Vancouver.Miller, G., A. R. Beckwith, C. Fellbaum,D.
Gross, and K. Miller.
1990.
Introductionto Wordnet: An on-line lexical database.International Journal of Lexicography,3(4):235?244.Ng, Vincent and Claire Cardie.
2002.Identifying anaphoric and non-anaphoricnoun phrases to improve coreferenceresolution.
In COLING, Taipei.O?Neill, Jacki and David Martin.
2003.
Textchat in action.
In GROUP ?03: Proceedingsof the 2003 International ACM SIGGROUPConference on Supporting Group Work,pages 40?49, New York, NY.Rand, William M. 1971.
Objective criteria forthe evaluation of clustering methods.Journal of the American StatisticalAssociation, 66(336):846?850.Roth, Dan and Wen-tau Yih.
2004.
A linearprogramming formulation for globalinference in natural language tasks.
InProceedings of CoNLL-2004, pages 1?8,Boston, MA.Sacks, Harvey, Emanuel A. Schegloff,and Gail Jefferson.
1974.
A simplestsystematics for the organization ofturn-taking for conversation.
Language,50(4):696?735.Shen, Dou, Qiang Yang, Jian-Tao Sun,and Zheng Chen.
2006.
Thread detectionin dynamic text message streams.
In408Elsner and Charniak Disentangling ChatSIGIR ?06: Proceedings of the 29th AnnualInternational ACM SIGIR Conference,pages 35?42, New York, NY.Soon, Wee Meng, Hwee Tou Ng, andDaniel Chung Yong Lim.
2001.
Amachine learning approach tocoreference resolution of nounphrases.
Computational Linguistics,27(4):521?544.Traum, D. 2004.
Issues in multi-partydialogues.
In F. Dignum, editor, Advancesin Agent Communication.
Springer Verlag,Berlin, pages 201?211.Traum, David R., Susan Robinson, andJens Stephan.
2004.
Evaluation ofmulti-party virtual reality dialogueinteraction.
In Proceedings of the FourthInternational Conference on LanguageResources and Evaluation (LREC),pages 1699?1702, Lisbon.Wang, Lidan and Douglas W. Oard.
2009.Context-based message expansion fordisentanglement of interleaved textconversations.
In Proceedings ofHLT-NAACL, pages 200?208, Boulder, CO.Wang, Yi-Chia, Mahesh Joshi, WilliamCohen, and Carolyn Rose?.
2008.Recovering implicit thread structure innewsgroup style conversations.
InProceedings of the 2nd InternationalConference on Weblogs and Social Media(ICWSM II), Seattle, WA.Yeh, Jen-Yuan and Aaron Harnly.
2006.Email thread reassembly using similaritymatching.
In Conference on Email andAnti-Spam, Mountain View, CA.409
