Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 828?838,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsMedical Relation Extraction with Manifold ModelsChang WangIBM T. J. Watson Research CenterYorktown Heights, New York, 10598changwangnk@gmail.comJames FanIBM T. J. Watson Research CenterYorktown Heights, New York, 10598fanj@us.ibm.comAbstractIn this paper, we present a manifold modelfor medical relation extraction.
Our modelis built upon a medical corpus containing80M sentences (11 gigabyte text) and de-signed to accurately and efficiently detectthe key medical relations that can facilitateclinical decision making.
Our approachintegrates domain specific parsing and typ-ing systems, and can utilize labeled as wellas unlabeled examples.
To provide userswith more flexibility, we also take labelweight into consideration.
Effectivenessof our model is demonstrated both theo-retically with a proof to show that the so-lution is a closed-form solution and exper-imentally with positive results in experi-ments.1 IntroductionThere exists a vast amount of knowledge sourcesand ontologies in the medical domain.
Such in-formation is also growing and changing extremelyquickly, making the information difficult for peo-ple to read, process and remember.
The combi-nation of recent developments in information ex-traction and the availability of unparalleled medi-cal resources thus offers us the unique opportunityto develop new techniques to help healthcare pro-fessionals overcome the cognitive challenges theyface in clinical decision making.Relation extraction plays a key role in informa-tion extraction.
Using question answering as anexample (Wang et al, 2012): in question analy-sis, the semantic relations between the questionfocus and each term in the clue can be used toidentify the weight of each term so that bettersearch queries can be generated.
In candidate an-swer generation, relations enable the backgroundknowledge base to be used for potential candidateanswer generation.
In candidate answer scoring,relation-based matching algorithms can go beyondexplicit lexical and syntactic information to detectimplicit semantic relations shared across the ques-tion and passages.To construct a medical relation extraction sys-tem, several challenges have to be addressed:?
The first challenge is how to identify a set ofrelations that has sufficient coverage in themedical domain.
To address this issue, westudy a real-world diagnosis related questionset and identify a set of relations that has agood coverage of the clinical questions.?
The second challenge is how to efficiently de-tect relations in a large amount of medicaltext.
The medical corpus underlying our re-lation extraction system contains 80M sen-tences (11 gigabytes pure text).
To extractrelations from a dataset at this scale, the re-lation detectors have to be fast.
In this paper,we speed up relation detectors through pars-ing adaptation and replacing non-linear clas-sifiers with linear classifiers.?
The third challenge is that the labeled rela-tion examples are often insufficient due to thehigh labeling cost.
When we build a na?
?vemodel to detect relations, the model tends tooverfit for the labeled data.
To address thisissue, we develop a manifold model (Belkinet al, 2006) that encourages examples (in-cluding both labeled and unlabeled exam-ples) with similar contents to be assignedwith similar scores.
Our model goes beyondregular regression models in that it appliesconstraints to those coefficients, such that thetopology of the given data manifold will berespected.
Computing the optimal weightsin a regression model and preserving mani-fold topology are conflicting objectives, we828present a closed-form solution to ideally bal-ance these two goals.The contributions of this paper on medical rela-tion extraction are three-fold:?
The problem setup is new.
There is a?fundamental?
difference between our prob-lem setup and the conventional setups, likei2b2 (Uzuner et al, 2011).
In i2b2 rela-tion extraction task, entity mentions are man-ually labeled, and each mention has 1 of 3concepts: ?treatment?, ?problem?, and ?test?.To resemble real-world medical relation ex-traction challenges where perfect entity men-tions do not exist, our new setup requiresthe entity mentions to be automatically de-tected.
The most well-known tool to detectmedical entity mentions is MetaMap (Aron-son, 2001), which considers all terms as en-tities and automatically associates each termwith a number of concepts from UMLS CUIdictionary (Lindberg et al, 1993) with morethan 2.7 million distinct concepts (comparedto 3 in i2b2).
The huge amount of entitymentions, concepts and noisy concept assign-ments provide a tough situation that peoplehave to face in real-world applications.?
From the perspective of relation extractionapplications, we identify ?super relations?-the key relations that can facilitate clinicaldecision making (Table 1).
We also presentapproaches to collect training data for theserelations with a small amount of labeling ef-fort.?
From the perspective of relation extractionmethodologies, we present a manifold modelfor relation extraction utilizing both labeledand unlabeled data.
Our approach can alsotake the label weight into consideration.The experimental results show that our relationdetectors are fast and outperform the state-of-the-art approaches on medical relation extraction by alarge margin.
We also apply our model to build anew medical relation knowledge base as a comple-ment to the existing knowledge bases.2 Background2.1 Medical Ontologies and SourcesMedical domain has a huge amount of natural lan-guage content found in textbooks, encyclopedias,guidelines, electronic medical records, and manyother sources.
It is also growing at an extremelyhigh speed.
Substantial understanding of the med-ical domain has already been included in the Uni-fied Medical Language System (UMLS) (Lind-berg et al, 1993), which includes medical con-cepts, relations, definitions, etc.
The 2012 versionof the UMLS contains information about morethan 2.7 million concepts from over 160 sourcevocabularies.
Softwares for using this knowledgealso exist: MetaMap (Aronson, 2001) is able toidentify concepts in text.
SEMREP (Rindfleschand Fiszman, 2003) can detect some relations us-ing hand-crafted rules.2.2 Relation ExtractionTo extract semantic relations from text, three typesof approaches have been applied.
Rule-basedmethods (Miller et al, 2000) employ a numberof linguistic rules to capture relation patterns.Feature-based methods (Kambhatla, 2004; Zhaoand Grishman, 2005) transform relation instancesinto a large amount of linguistic features like lex-ical, syntactic and semantic features, and capturethe similarity between these feature vectors.
Re-cent results mainly rely on kernel-based meth-ods.
Many of them focus on using tree kernels tolearn parse tree structure related features (Collinsand Duffy, 2001; Culotta and Sorensen, 2004;Bunescu and Mooney, 2005).Other researchers study how different ap-proaches can be combined to improve the extrac-tion performance.
For example, by combining treekernels and convolution string kernels, (Zhang etal., 2006) achieved the state of the art performanceon ACE data (ACE, 2004).
Recently, ?distant su-pervision?
has emerged to be a popular choice fortraining relation extractors without using manuallylabeled data (Mintz et al, 2009; Jiang, 2009; Chanand Roth, 2010; Wang et al, 2011; Riedel et al,2010; Ji et al, 2011; Hoffmann et al, 2011; Sur-deanu et al, 2012; Takamatsu et al, 2012; Min etal., 2013).Various relation extraction approaches havebeen adapted to the medical domain, most ofwhich focus on designing heuristic rules targetedfor diagnosis and integrating the medical ontologyin the existing extraction approaches.
Results ofsome of these approaches are reported on the i2b2data (Uzuner et al, 2011).8293 Identifying Key Medical Relations3.1 Super Relations in Medical DomainThe first step in building a relation extraction sys-tem for medical domain is to identify the relationsthat are important for clinical decision making.Four main clinical tasks that physicians engagein are discussed in (Demner-Fushman and Lin,2007).
They are Therapy- select treatments to of-fer a patient, taking consideration of effectiveness,risk, cost and other factors (prevention is under thegeneral category of Therapy), Diagnosis (includ-ing differential diagnosis based on findings and di-agnostic test), Etiology- identify the factors thatcause the disease and Prognosis- estimate the pa-tient?s likely course over time.
These activities canbe translated into ?search tasks?.
For example, thesearch for therapy is usually the therapy selectiongiven a disease.We did an independent study regarding whatclinical questions usually ask for on a set of 5,000Doctor Dilemma (DD) questions from the Ameri-can College of Physicians (ACP).
This set includesquestions about diseases, treatments, lab tests, andgeneral facts1.
Our analysis shows that about 15%of these questions ask for treatments, preventionsor contraindicated drugs for a disease or anotherway around, 4% are about diagnosis tests, 6% areabout the causes of a disease, 1% are about the lo-cations of a disease, 25% are about the symptomsof a disease, 8% are asking for definitions, 7% areabout guidelines and the remaining 34% questionseither express no relations or some relations thatare not very popular.Based on the analysis in (Demner-Fushman andLin, 2007) and our own results, we decided to fo-cus on seven key relations in the medical domain,which are described in Table 1.
We call these re-lations ?super relations?, since they cover mostquestions in the DD question set and align wellwith the analysis result in (Demner-Fushman andLin, 2007).3.2 Collect Training DataThis section presents how we collect training datafor each relation.
The overall procedure is illus-trated in Figure 1.1Here?s an example of these questions and its answer:Question: The syndrome characterized by joint pain, abdom-inal pain, palpable purpura, and a nephritic sediment.
An-swer: Henoch-Schonlein purpura.Large Amount ofNoisy RelationDataMedical TextRelation Knowledge inMedical DomainTraining Data forEach RelationFor each relation, choose asmall amount of the mostrepresentative examplesAnnotationUnlabeled DataLabeled DataFigure 1: Collect Training DataOur medical corpus has incorporated a setof medical books/journals2 and MEDLINE ab-stracts.
We also complemented these sources withWikipedia articles.
In total, the corpus contains80M sentences (11 gigabyte pure text).The UMLS 2012 Release contains more than600 relations and 50M relation instances underaround 15 categories.
The RO category (ROstands for ?has Relationship Other than synony-mous, narrower, or broader?)
is the most inter-esting one, and covers relations like ?may treat?,?has finding site?, etc.
Each relation has acertain number of Concept Unique Identifier(CUI) pairs that are known to bear that rela-tion.
In UMLS, some relation information isredundant.
Firstly, half of these relations aresimply inverse of each other (e.g.
the relation?may treat?
and ?may be treated by?).
Secondly,there is a significant amount of redundancy evenamong non-inverse relations (e.g.
the relation?has manifestation?
and ?disease has finding?
).From UMLS relations, we manually chose asubset of them that are directly related to the su-per relations discussed in Section 3.1.
The cor-respondences between them are given in Table 1.One thing to note is that super relations are moregeneral than the UMLS relations, and one superrelation might integrate multiple UMLS relations.Using the CUI pairs in the UMLS relation knowl-2This is a full list of the books and journals used inour corpus: ACP-Medical Knowledge Self-Assessment Pro-gram, EBSCO-Dynamed, EBSCO-Quick Lessons, EBSCO-EBCS, EBSCO-Clinical Review, Wiley-Essential EvidencePlus: EBMG Guidelines, Wiley-Essential Evidence Topics,Wiley-Essential Evidence Plus: EBMG Summaries, Wiley-POEMs, Wiley-The Breast Journal, New England Journalof Medicine, Journal Watch, NCCN-CME, NCCN-GUS,NCCN-Compendium, NCCN-Templates, NCCN-Guidelinesfor Patients, NCCN-Physician Guidelines, Merck Manual ofDiagnosis and Therapy, and UpToDate.830Table 1: Super relations & their arguments, UMLS sources and noise% in the annotation dataSuper Relations Argument 1 Argument 2 UMLS Sources Noise% in Annotation Datatreats disease treatments may treat, treats 16%prevents disease treatments may prevent 49%contraindicates disease treatments contraindicated drug 97%diagnoses disease tests may diagnose 63%causes disease causes cause of, causative agent of 66%location of disease locations has finding site 41%disease has primary anatomic sitesymptom of disease symptoms disease has finding 66%disease may have findinghas manifestationhas definitional manifestationedge base, we associate each super relation with aset of CUI pairs.To collect the training data for each super re-lation, we need to collect sentences that expressthe relation.
To achieve this, we parsed all 80Msentences in our medical corpus, looking for thesentences containing the terms that are associatedwith the CUI pairs in the knowledge base.
This(distant supervision) approach resulted in a hugeamount of sentences that contain the desired rela-tions, but also brought in a lot of noise in the formof false positives.
For example, we know fromthe knowledge base that ?antibiotic drug?
maytreat ?Lyme disease?.
However the sentence ?Thispaper studies the relationship between antibioticdrug and Lyme disease?
contains both terms butdoes not express the ?treats?
relation.The most reliable way to clean the training datais to ask annotators to go through the sentencesand assign the sentences with positive/negative la-bels.
However, it will not work well when we havemillions of sentences to vet.
To minimize the hu-man labeling effort, we ran a K-medoids clusteringon the sentences associated with each super rela-tion and kept the cluster centers as the most rep-resentative sentences for annotation.
Dependingon the number of the sentences we collected foreach relation, the #clusters was chosen from 3,000- 6,000.
The similarity of two sentences is definedas the bag-of-words similarity of the dependencypaths connecting arguments.
Part of the resultingdata was manually vetted by our annotators, andthe remaining was held as unlabeled data for fur-ther experiments.Our relation annotation task is quite straightfor-ward, since both arguments are given and the de-cision is a Yes-or-No decision.
The noise rate ofeach relation (#sentences expressing the relation/ #sentences) is reported in Table 1 based on theannotation results.
The noise rates differ signifi-cantly from one relation to another.
For ?treats?relation, only 16% of the sentences are false posi-tives.
For ?contraindicates?
relation, the noise rateis 97%.To grow the size of the negative training set foreach super relation, we also added a small amountof the most representative examples (also comingfrom K-medoids clustering) from each unrelatedUMLS relation to the training set as negative ex-amples.
This resulted in more than 10,000 extranegative examples for each relation.3.3 Parsing and TypingThe most well-known tool to detect medical en-tity mentions is MetaMap (Aronson, 2001), whichconsiders all terms as entities and automaticallyassociates each term with a number of conceptsfrom UMLS CUI dictionary (Lindberg et al,1993) with 2.7 million distinct concepts.The parser used in our system is Medi-calESG, an adaptation of ESG (English SlotGrammar) (McCord et al, 2012) to the medicaldomain with extensions of medical lexicons inte-grated in the UMLS 2012 Release.
Compared toMetaMap, MedicalESG is based on the same med-ical lexicons, 10 times faster and produces verysimilar parsing results.We use the semantic types defined inUMLS (Lindberg et al, 1993) to categorizeargument types.
The UMLS consists of a setof 133 subject categories, or semantic types,that provide a consistent categorization of morethan 2M concepts represented in the UMLSMetathesaurus.
Our system assigns each relationargument with one or more UMLS semantic typesthrough a two step process.
Firstly, we use Med-icalESG to process the input sentence, identifysegments of text that correspond to concepts in831Figure 2: A Parse Tree Examplethe UMLS Metathesaurus and associate each ofthem with one or more UMLS CUIs (ConceptUnique Identifier).
Then we do a CUI lookup inUMLS to find the corresponding semantic typesfor each CUI.Most relation arguments are associated withmultiple semantic types.
For example, the term?tetracycline hydrochloride?
has two types: ?Or-ganic Chemical?
and ?Antibiotic?.
Sometimes,the semantic types are noisy due to ambiguity ofterms.
For example, the term ?Hepatitis b?
is asso-ciated with both ?Pharmacologic Substance?
and?Disease or Syndrome?
based on UMLS.
The rea-son for this is that people use ?Hepatitis b?
to rep-resent both ?the disease of Hepatitis b?
and ?Hep-atitis b vaccine?, so UMLS assigns both types to it.This is a concern for relation extraction, since twotypes bear opposite meanings.
Our current strat-egy is to integrate all associated types, and rely onthe relation detector trained with the labeled datato decide how to weight different types based uponthe context.Here is an illustrative example.
Consider thesentence: ?Antibiotics are the standard therapyfor Lyme disease?
: MedicalESG first generatesa dependency parse tree (Figure 2) to representgrammatical relations between the words in thesentence, and then associates the words with CUIs.For example, ?Antibiotics?
is associated with CUI?C0003232?
and ?Lyme disease?
is associatedwith two CUIs: ?C0024198?
and ?C0717360?.CUI lookup will assign ?Antibiotics?
with a se-mantic type ?Antibiotic?, and ?Lyme disease?
withthree semantic types: ?Disease or Syndrome?,?Pharmacologic Substance?
and ?ImmunologicFactor?.
This sentence expresses a ?treats?
rela-tion between ?Antibiotics?
and ?Lyme disease?.4 Relation Extraction with ManifoldModels4.1 MotivationsGiven a few labeled examples and many unlabeledexamples for a relation, we want to build a re-lation detector leveraging both labeled and unla-beled data.
Following the manifold regularizationidea (Belkin et al, 2006), our strategy is to learna function that assigns a score to each example.Scores are fit so that examples (both labeled andunlabeled) with similar content get similar scores,and scores of labeled examples are close to theirlabels.
Integration of the unlabeled data can helpsolve overfitting problems when the labeled datais not sufficient.4.2 FeaturesWe use 8 groups of features to represent each rela-tion example.
These features are commonly usedfor relation extraction.?
(1) Semantic types of argument 1, such as?Antibiotic?.?
(2) Semantic types of argument 2.?
(3) Syntactic features representing the depen-dency path between two arguments, such as?subj?, ?pred?, ?mod nprep?
and ?objprep?
(between arguments ?antibiotic?
and ?lymedisease?)
in Figure 2.?
(4) Features modeling the incoming and out-going links of both arguments.
These fea-tures are useful to determine if a relation goesfrom argument 1 to argument 2 or vice versa.?
(5) Topic features modeling the words inthe dependency path.
In the example givenin Figure 2, the dependency path containsthe following words: ?be?, ?standard ther-apy?
and ?for?.
These features as well asthe features in (6) are achieved by projectingthe words onto a 100 dimensional LSI topicspace (Deerwester et al, 1990) constructedfrom our medical corpus.?
(6) Topic features modeling the words in thewhole sentence.?
(7) Bag-of-words features modeling the de-pendency path.
In (7) and (8), we only con-sider the words that have occurred in the pos-itive training data.832Notations:The input dataset X = {x1, ?
?
?
, xm} is repre-sented as a feature-instance matrix.The desired label vector Y = {y1, ?
?
?
, yl} repre-sents the labels of {x1, ?
?
?
, xl}, where l ?
m.W is a weight matrix, where Wi,j= e?
?xi?xj?2models the similarity of xiand xj.?xi?
xj?
stands for the Euclidean distance be-tween xiand xjin the vector space.D is a diagonal matrix: Di,i=?jWi,j.L = D?0.5(D ?W )D?0.5 is called normalizedgraph Laplacian matrix.?
is a user defined l ?
l diagonal matrix, where?irepresents the weight of label yi.A =(?
00 0)is an m?m matrix.V = [y1, ?
?
?
yl, 0, ?
?
?
, 0] is a 1?m matrix.?
is a weight scalar.
()+ represents pseudo inverse.Algorithm:1.
Represent each example using features:X = {x1, ?
?
?
, xm}, where xiis the ith ex-ample.2.
Construct graph Laplacian matrix Lmodeling the data manifold.3.
Construct vector V = [y1, ?
?
?
yl, 0, ?
?
?
, 0].4.
Compute projection function f for eachrelation: f = (X(A+ ?L)XT )+XAV T .Figure 3: Notations and the Algorithm to Train aManifold Model for Relation Extraction?
(8) Bag-of-words features modeling thewhole sentence.In relation extraction, many recent approachesuse non-linear kernels to get the similarity of tworelation examples.
To classify a relation exam-ple, a lot of dot product computations are required.This is very time consuming and becomes a bottle-neck in using relation extraction to facilitate clin-ical decision making.
To speed up the classifierduring the apply time, we decided to use a linearclassifier instead of non-linear classifiers.We represent all features in a single featurespace.
For example, we use a vector of 133 en-tries (UMLS contains 133 semantic types) to rep-resent the types of argument 1.
If argument 1 isassociated with two types: ?Organic Chemical?and ?Antibiotic?, we set the two corresponding en-tries to 1 and all the other entries to 0.
Similar ap-proaches are used to represent the other features.4.3 The Main AlgorithmThe problem we want to solve is formalized as fol-lows: given a relation dataset X = {x1, ?
?
?
, xm},and the desired label Y = {y1, ?
?
?
, yl} for{x1, ?
?
?
, xl}, where l ?
m, we want to constructa mapping function f to project any example xitoa new space, where fTximatches xi?s desired la-bel yi.
In addition, we also want f to preserve themanifold topology of the dataset, such that similarexamples (both labeled and unlabeled) get simi-lar scores.
Here, the label is ?+1?
for positive ex-amples, and ?-1?
for negative examples.
Notationsand the main algorithm to construct f for each re-lation are given in Figure 3.4.4 JustificationThe solution to the problem defined in Section 4.3is given by the mapping function f to minimizethe following cost function:C(f) =?i?l?i(fTxi?
yi)2+ ??i,jWi,j(fTxi?
fTxj)2.The first term of C(f) is based on labeled ex-amples, and penalizes the difference between themapping result of xiand its desired label yi.
?iisa user specified parameter, representing the weightof label yi.
The second term of C(f) does not takelabel information into account.
It encourages theneighborhood relationship (geometry of the man-ifold) within X to be preserved in the mapping.When xiand xjare similar, the correspondingWi,jis big.
If f maps xiand xjto different posi-tions, f will be penalized.
The second term is use-ful to bound the mapping function f and preventsoverfitting from happening.
Here ?
is the weightof the second term.
When ?
= 0, the model dis-regards the unlabeled data, and the data manifoldtopology is not respected.Compared to manifold regularization (Belkinet al, 2006), we do not include the RKHS normterm.
Instead, we associate each labeled examplewith an extra weight for label confidence.
Thisweight is particularly useful when the trainingdata comes from ?Crowdsourcing?, where we ask833multiple workers to complete the same task tocorrect errors.
In that scenario, weights can be as-signed to labels based upon annotator agreement.Theorem 1: f = (X(A + ?L)XT )+XAV Tminimizes the cost function C(f).Proof:Given the input X , we want to find the optimalmapping function f such that C(f) is minimized:f = argminfC(f).It can be verified that?i?l?i(fTxi?
yi)2= fTXAXTf ?
2fTXAVT+ VAVT.We can also verify that??i,j(fTxi?
fTxj)2Wi,j= ?fTXLXTf.So C(f) can be written asfTXAXTf ?
2fTXAVT+ VAVT+ ?fTXLXTf.Using the Lagrange multiplier trick to differentiateC(f) with respect to f , we have2XAXTf + 2?XLXTf = 2XAVT.This implies thatX(A+ ?L)XTf = XAVT.Sof = (X(A+ ?L)XT)+XAVT,where ?+?
represents pseudo inverse.4.5 AdvantagesOur algorithm offers the following advantages:?
The algorithm exploits unlabeled data, whichhelps prevent ?overfitting?
from happening.?
The algorithm provides users with the flex-ibility to assign different labels with differ-ent weights.
This feature is useful when thetraining data comes from ?crowdsourcing?
or?distant supervision?.?
Different from many approaches in this area,our algorithm provides a closed-form solu-tion of the result.
The solution is global opti-mal regarding the cost function C(f).?
The algorithm is computationally efficient atthe apply time (as fast as linear regressions).5 Experiments5.1 Cross-Validation TestWe use a cross-validation test3 with the relationdata generated in Section 3.2 to compare our ap-proaches against the state-of-the-art approaches.The task is to classify the examples into positiveor negative for each relation.
We applied a 5-foldcross-validation.
In each round of validation, weused 20% of the data for training and 80% for test-ing.
The F1scores reported here are the averageof all 5 rounds.
We used MedicalESG to processthe input text for all approaches.5.1.1 Data and ParametersThis dataset includes 7 relations.
We do not con-sider the relation of ?contraindicates?
in this test,since it has too few positive examples.
On average,each relation contains about 800 positive examplesand more than 13,000 negative examples.
To elim-inate the examples that are trivial to classify, weremoved the negative examples that do not bearthe valid argument types.
This removed the exam-ples that can be easily classified by a type filter,resulting in 3,000 negatives on average per rela-tion.
For each relation, we also collected 5,000unlabeled examples and put them into two sets:unlabeled set 1 and 2 (2,500 examples in each set).No parameter tuning was taken and no relationspecific heuristic rules were applied in all tests.
Inall manifold models, ?
= 1.
In SVM implemen-tations, the trade-off parameter between trainingerror and margin was set to 1 for all experiments.5.1.2 Baseline ApproachesWe compare our approaches to three state-of-the-art approaches including SVM with convolutiontree kernels (Collins and Duffy, 2001), linear re-gression and SVM with linear kernels (Scho?lkopfand Smola, 2002).
To adapt the tree kernel to med-ical domain, we followed the approach in (Nguyenet al, 2009) to take the syntactic structures intoconsideration.
We also added the argument typesas features to the tree kernel.
In the tree kernel im-plementation, we assigned the tree structure andthe vector corresponding to the argument types3If we take the perfect entity mentions and the associatedconcepts provided by i2b2 (Uzuner et al, 2011) as the input,our system can directly apply to i2b2 relation extraction data.However, the i2b2 data has a tough data use agreement.
Ourlegal team held several rounds of negotiations with the i2b2data owner and then decided we should not use it due to thehigh legal risks.
We are not aware of other available medicalrelation extraction datasets that fit for our evaluations.834Table 2: F1Scores from a Five-Fold Cross Validation ExperimentSVM SVM Linear Manifold Manifold Manifold ManifoldTree Linear Regression Unlabeled Predicted Labels Predicted Labels Unlabeled+PredictedKernel Kernel with Weights without Weights Labels with Weightstreats 0.7648 0.7850 0.7267 0.8025 0.8041 0.7884 0.8085prevents 0.2859 0.3887 0.3922 0.5502 0.5696 0.6349 0.6332causes 0.3885 0.5024 0.5219 0.5779 0.5088 0.3978 0.5081location of 0.6113 0.6009 0.4968 0.7275 0.7363 0.6964 0.7454diagnoses 0.5520 0.4934 0.3202 0.6468 0.6485 0.5720 0.6954symptom of 0.4398 0.5611 0.5984 0.6347 0.5314 0.4515 0.5968average 0.5071 0.5553 0.5094 0.6566 0.6331 0.5902 0.6646with equal weights.
The SVM with linear kernelsand the linear regression model used the same fea-tures as the manifold models.5.1.3 Settings for the Manifold ModelsWe tested our manifold model for each relation un-der three different settings:(1) Manifold Unlabeled: We combined the la-beled data and unlabeled set 1 in training.
We set?i= 1 for i ?
[1, l].
(2) Manifold Predicted Labels: We combinedlabeled data and unlabeled set 2 in training.
?i=1 for i ?
[1, l].
Different from the previous set-ting, we gave a label estimation to all the exam-ples in the unlabeled set 2 based on the noise rate(Noise%) from Table 1.
The label of all unla-beled examples was set to ?+1?
when 100% ?
2 ?Noise% > 0, or ?-1?
otherwise.
Two weightingstrategies were applied:?
With Weights: We let label weight ?i=|100%?
2 ?Noise%| for all xicoming fromthe unlabeled set 2.
This setting represents anempirical rule to estimate the label and con-fidence of each unlabeled example based onthe sampling result.?
Without Weights: ?iis always set to 1.
(3) Manifold UnLabeled+Predicted Labels: acombination of setting (1) and (2).
In this setting,the data from unlabeled set 1 was used as unla-beled data and the data from unlabeled set 2 wasused as labeled data (With Weights).5.1.4 ResultsThe results are summarized in Table 2.The tree kernel-based approach and linear re-gression achieved similar F1scores, while linearSVM made a 5% improvement over them.
Onething to note is that the results from these ap-proaches vary significantly.
The reason for this isthat the labeled training data is not sufficient.
Sothe approaches that completely depend on the la-beled data are likely to run into overfitting.
LinearSVM performed better than the other two, sincethe large-margin constraint together with the lin-ear model constraint can alleviate overfitting.By integrating unlabeled data, the manifoldmodel under setting (1) made a 15% improvementover linear regression model on F1score, wherethe improvement was significant across all rela-tions.Under setting (2), the With Weights strategyachieved a slightly worse F1score than the previ-ous setting but much better result than the baselineapproaches.
This tells us that estimating the labelof unlabeled examples based upon the samplingresult is one way to utilize unlabeled data and mayhelp improve the relation extraction results.
Theresults also show that the label weight is importantfor this setting, since the Without Weights strategydid not perform very well.Compared to setting (1) and (2), setting (3)made use of 2,500 more unlabeled examples,and achieved the best performance among all ap-proaches.
On one hand, this result shows thatusing more unlabeled data can further improvethe result.
On the other hand, the insignificantimprovement over (1) and (2) strongly indicatesthat how to utilize more unlabeled data to achievea significant improvement is non-trivial and de-serves more attention.
To what extensions the un-labeled data can help the learning process is anopen problem.
Generally speaking, when the ex-isting data is sufficient to characterize the datasetgeometry, adding more unlabeled data will nothelp (Singh et al, 2008).We tested the tree kernel-based approach with-out integrating the medical types as well.
That re-sulted in very poor performance: the average F1score was below 30%.
We also applied the rulesused in SEMREP (Rindflesch and Fiszman, 2003)to this dataset.
Since the relations detected by835SEMREP rules cannot be perfectly aligned withsuper relations, we cannot directly compare the re-sults.
Overall speaking, SEMREP rules are veryconservative and detect very few relations from thesame text.5.2 Knowledge Base (KB) ConstructionThe UMLS Metathesaurus (Lindberg et al, 1993)contains a large amount of manually extracted re-lation knowledge.
Such knowledge is invaluablefor people to collect training data to build newrelation detectors.
One downside of using thisKB is its incompleteness.
For example, it onlycontains the treatments for about 8,000 diseases,which are far from sufficient.
Further, the medicalknowledge is changing extremely quickly, makingpeople hard to understand it, and update it in theknowledge base in a timely manner.To address these challenges, we constructed ourown relation KB as a complement to the UMLSrelation KB.
We directly ran our relation detec-tors (trained with all labeled and unlabeled exam-ples) on our medical corpus to extract relations.Then we combined the results and put them in anew KB.
The new KB covers all super relationsand stores the knowledge in the format of (rela-tion name, argument 1, argument 2, confidence),where the confidence is computed based on the re-lation detector confidence score and relation pop-ularity in the corpus.
The most recent version ofour relation KB contains 3.4 million such entries.We compared this new KB against UMLS KBusing an answer generation task on a set of 742Doctor Dilemma questions.
We first ran our rela-tion detectors to detect the relation(s) in the ques-tion clue involving question focus (what the ques-tion asks for).
Then we searched against both KBsusing the relation name and the non-focus argu-ment for the missing argument.
The search re-sults were then generated as potential answers.
Weused the same relations to do KB lookup, so theresults are directly comparable.
Since most ques-tions only have one correct answer, the precisionnumber is not very important in this experiment.If we detect multiple relations in the question,and the same answer is generated from more thanone relations, we sum up all those confidencescores to make such answers more preferable.Sometimes, we may generate too many answersfrom KBs.
For example, if the detected relationis ?location of?
and the non-focus argument is?skin?, then thousands of answers can be gener-ated.
In this scenario, we sort the answers basedupon the confidence scores and only consider upto p answers for each question.
In our test, weconsidered three numbers for p: 20, 50 and 3,000.From Table 3, we can see that the new KB out-performs the most popularly-used UMLS KB atall recall levels by a large margin.
This result in-dicates that the new KB has a much better knowl-edge coverage.
The UMLS KB is manually cre-ated and thus more precise.
In our experiment, theUMLS KB generated fewer answers than the newKB.
For example, when up to 20 answers weregenerated for each question, the UMLS KB gen-erated around 4,700 answers for the whole ques-tion set, while the new KB generated about 7,600answers.Construction of the new KB cost 16 machines(using 4?2.8G cores per machine) 8 hours.
Thereported computation time is for the whole corpuswith 11G pure text.Table 3: Knowledge Base ComparisonRecall@20 Recall@50 Recall@3000Our KB 135/742 182/742 301/742UMLS KB 42/742 52/742 73/7426 ConclusionsIn this paper, we identify a list of key relations thatcan facilitate clinical decision making.
We alsopresent a new manifold model to efficiently extractthese relations from text.
Our model is developedto utilize both labeled and unlabeled examples.
Itfurther provides users with the flexibility to takelabel weight into consideration.
Effectiveness ofthe new model is demonstrated both theoreticallyand experimentally.
We apply the new model toconstruct a relation knowledge base (KB), and useit as a complement to the existing manually cre-ated KBs.AcknowledgmentsWe thank Siddharth Patwardhan for help on treekernels, Sugato Bagchi and Dr. Herbert Chase?steam for categorizing the Doctor Dilemma ques-tions.
We also thank Anthony Levas, Karen In-graffea, Mark Mergen, Katherine Modzelewski,Jonathan Hodax, Matthew Schoenfeld and AdarshThaker for vetting the training data.836ReferencesACE.
2004.
The automatic content extraction projects,http://projects.ldc.upenn.edu/ace/.A.
Aronson.
2001.
Effective mapping of biomedicaltext to the UMLS metathesaurus: the MetaMap pro-gram.
In Proceedings of the 2001 Annual Sympo-sium of the American Medical Informatics Associa-tion.M.
Belkin, P. Niyogi, and V. Sindhwani.
2006.Manifold regularization: a geometric frameworkfor learning from labeled and unlabeled exam-ples.
Journal of Machine Learning Research, pages2399?2434.R.
Bunescu and R. Mooney.
2005.
A shortest path de-pendency kernel for relation extraction.
In Proceed-ings of the Conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing.Y.
Chan and D. Roth.
2010.
Exploiting backgroundknowledge for relation extraction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 152?160.M.
Collins and N. Duffy.
2001.
Convolution ker-nels for natural language.
In Proceedings of theAdvances in Neural Information Processing Systems(NIPS), pages 625?632.A.
Culotta and J. Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proceedings of the42nd Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 423?429.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Societyfor Information Science, 41(6):391?407.D.
Demner-Fushman and J. Lin.
2007.
Answeringclinical questions with knowledge-based and statis-tical techniques.
Journal of Computational Linguis-tics, 56:63?103.R.
Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, andD.
S. Weld.
2011.
Knowledge-based weak supervi-sion for information extraction of overlapping rela-tions.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics (ACL).H.
Ji, R. Grishman, and H. T. Dang.
2011.
Overviewof the TAC 2011 knowledge base population track.In Proceedings of the Text Analytics Conference.J.
Jiang.
2009.
Multi-task transfer learning for weakly-supervised relation extraction.
In Proceedings ofthe Joint Conference of the 47th Annual Meetingof the Association for Computational Linguistics(ACL) and the 4th International Joint Conferenceon Natural Language Processing (IJCNLP), pages1012?1020.N.
Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy mod-els for extracting relations.
In Proceedings of theACL 2004 on Interactive poster and demonstrationsessions.D.
Lindberg, B. Humphreys, and A. McCray.
1993.The Unified Medical Language System.
Methods ofInformation in Medicine, 32:281?291.M.
McCord, J. W. Murdock, and B. K. Boguraev.2012.
Deep parsing in Watson.
IBM Journal of Re-search and Development, 56.S.
Miller, H. Fox, L. Ramshaw, and R. Weischedel.2000.
A novel use of statistical parsing to extract in-formation from text.
In Proceedings of the 1st NorthAmerican Chapter of the Association for Computa-tional Linguistics Conference.B.
Min, R. Grishman, L. Wan, C. Wang, andD.
Gondek.
2013.
Distant supervision for relationextraction with an incomplete knowledge base.
InThe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT).M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.Distant supervision for relation extraction withoutlabeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the Associationfor Computational Linguistics (ACL) and the 4th In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP), pages 1003?1011.T.
Nguyen, A. Moschitti, and G. Riccardi.
2009.
Con-volution kernels on constituent, dependency and se-quential structures for relation extraction.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).S.
Riedel, L. Yao, and A. McCallum.
2010.
Mod-eling relations and their mentions without labeledtext.
In Proceedings of the European Conferenceon Machine Learning and Knowledge Discovery inDatabases (ECML PKDD).T.
C. Rindflesch and M. Fiszman.
2003.
The inter-action of domain knowledge and linguistic structurein natural language processing: interpreting hyper-nymic propositions in biomedical text.
Journal ofBiomedical Informatics, 36:462?477.B.
Scho?lkopf and A. J. Smola.
2002.
Learning withKernels: Support Vector Machines, Regularization,Optimization, and Beyond.
MIT Press.A.
Singh, R. D. Nowak, and X. Zhu.
2008.
Unlabeleddata: now it helps, now it doesnot.
In Proceedingsof the Advances in Neural Information ProcessingSystems (NIPS).M.
Surdeanu, J. Tibshirani, R. Nallapati, and C. D.Manning.
2012.
Multi-instance multilabel learningfor relation extraction.
In Proceedings of the 2012837Conference on Empirical Methods in Natural Lan-guage Processing and Natural Language Learning(EMNLP).S.
Takamatsu, I. Sato, and H. Nakagawa.
2012.
Re-ducing wrong labels in distant supervision for rela-tion extraction.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).O?.
Uzuner, B. R. South, S. Shen, and S. L. DuVall.2011.
2010 i2b2/VA challenge on concepts, asser-tions, and relations in clinical text.
Journal of Amer-ican Medical Informatics Association, 18:552?556.C.
Wang, J.
Fan, A. Kalyanpur, and D. Gondek.
2011.Relation extraction with relation topics.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (EMNLP).C.
Wang, A. Kalyanpur, J.
Fan, B. Boguraev, andD.
Gondek.
2012.
Relation extraction and scoringin DeepQA.
IBM Journal of Research and Develop-ment, 56.M.
Zhang, J. Zhang, J. Su, and G. Zhou.
2006.
A com-posite kernel to extract relations between entitieswith both flat and structured features.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics (ACL).S.
Zhao and R. Grishman.
2005.
Extracting relationswith integrated information using kernel methods.In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 419?426.838
