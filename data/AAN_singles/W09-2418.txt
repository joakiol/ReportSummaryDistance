Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112?116,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 13:Evaluating Events, Time Expressions, and Temporal Relations(TempEval-2)James PustejovskyComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USAjamesp@cs.brandeis.eduMarc VerhagenComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USAmarc@cs.brandeis.eduAbstractWe describe the TempEval-2 task which iscurrently in preparation for the SemEval-2010evaluation exercise.
This task involves iden-tifying the temporal relations between eventsand temporal expressions in text.
Six distinctsubtasks are defined, ranging from identifyingtemporal and event expressions, to anchoringevents to temporal expressions, and orderingevents relative to each other.1 IntroductionNewspaper texts, narratives and other such texts de-scribe events which occur in time and specify thetemporal location and order of these events.
Textcomprehension, even at the most general level, in-volves the capability to identify the events describedin a text and locate these in time.
This capability iscrucial to a wide range of NLP applications, fromdocument summarization and question answering tomachine translation.
As in many areas of NLP, anopen evaluation challenge in the area of temporal an-notation will serve to drive research forward.The automatic identification of all temporal re-ferring expressions, events, and temporal relationswithin a text is the ultimate aim of research in thisarea.
However, addressing this aim in a first evalua-tion challenge was deemed too difficult and a stagedapproach was suggested.
The 2007 SemEval task,TempEval (henceforth TempEval-1), was an initialevaluation exercise based on three limited tasks thatwere considered realistic both from the perspectiveof assembling resources for development and test-ing and from the perspective of developing systemscapable of addressing the tasks.We are now preparing TempEval-2, a temporalevaluation task based on TempEval-1.
TempEval-2is more elaborate in two respects: (i) it is a multilin-gual task, and (ii) it consists of six subtasks ratherthan three.2 TempEval-1TempEval-1 consisted of three tasks:A. determine the relation between an event and atimex in the same sentence;B. determine the relation between an event and thedocument creation time;C. determine the relation between the main eventsof two consecutive sentences.The data sets were based on TimeBank (Puste-jovsky et al, 2003; Boguraev et al, 2007), a hand-built gold standard of annotated texts using theTimeML markup scheme.1 The data sets includedsentence boundaries, TIMEX3 tags (including thespecial document creation time tag), and EVENTtags.
For tasks A and B, a restricted set of eventswas used, namely those events that occur more than5 times in TimeBank.
For all three tasks, the re-lation labels used were BEFORE, AFTER, OVER-LAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTERand VAGUE.2 For a more elaborate description ofTempEval-1, see (Verhagen et al, 2007; Verhagenet al, 2009).1See www.timeml.org for details on TimeML, Time-Bank is distributed free of charge by the LinguisticData Consortium (www.ldc.upenn.edu), catalog numberLDC2006T08.2Which is different from the set of 13 labels from TimeML.The set of labels for TempEval-1 was simplified to aid datapreparation and to reduce the complexity of the task.112There were six systems competing in TempEval-1: University of Colorado at Boulder (CU-TMP);Language Computer Corporation (LCC-TE); NaraInstitute of Science and Technology (NAIST); Uni-versity of Sheffield (USFD); Universities of Wolver-hampton and Allicante (WVALI); and XEROX Re-search Centre Europe (XRCE-T).The difference between these systems was notlarge, and details of system performance, along withcomparisons and evaluation, are presented in (Ver-hagen et al, 2009).
The scores for WVALI?s hybridapproach were noticeably higher than those of theother systems in task B and, using relaxed scoring,in task C as well.
But for task A, the highest scoringsystems are barely ahead of the rest of the field.
Sim-ilarly, for task C using strict scoring, there is no sys-tem that clearly separates itself from the field.
Inter-estingly, the baseline is close to the average systemperformance on task A, but for other tasks the sys-tem scores noticeably exceed the baseline.
Note thatthe XRCE-T system is somewhat conservative in as-signing TLINKS for tasks A and B, producing lowerrecall scores than other systems, which in turn yieldlower f-measure scores.
For task A, this is mostlydue to a decision only to assign a temporal relationbetween elements that can also be linked by the syn-tactic analyzer.3 TempEval-2The set of tasks chosen for TempEval-1 was by nomeans complete, but was a first step towards a fullerset of tasks for temporal parsing of texts.
While themain goal of the division in subtasks was to aid eval-uation, the larger goal of temporal annotation in or-der to create a complete temporal characterization ofa document was not accomplished.
Results from thefirst competition indicate that task A was defined toogenerally.
As originally defined, it asks to tempo-rally link all events in a sentence to all time expres-sions in the same sentence.
A clearer task wouldhave been to solicit local anchorings and to sepa-rate these from the less well-defined temporal rela-tions between arbitrary events and times in the samesentence.
We expect both inter-annotator agree-ment and system performance to be higher with amore precise subtask.
Thus, the set of tasks usedin TempEval-1 is far from complete and the taskscould have been made more restrictive.
As a re-sult, inter-annotator agreement scores lag, makingprecise evaluation more challenging.The overall goal of temporal tagging of a text is toprovide a temporal characterization of a set of eventsthat is as complete as possible.
If the annotationgraph of a document is not completely connectedthen it is impossible to determine temporal relationsbetween two arbitrary events because these eventscould be in separate subgraphs.
Hence, for the cur-rent competition, TempEval-2, we have enriched thetask description to bring us closer to creating sucha temporal characterization for a text.
We have en-riched the TempEval-2 task definition to include sixdistinct subtasks:A.
Determine the extent of the time expressionsin a text as defined by the TimeML TIMEX3tag.
In addition, determine value of the fea-tures TYPE and VAL.
The possible values ofTYPE are TIME, DATE, DURATION, and SET;the value of VAL is a normalized value as de-fined by the TIMEX2 and TIMEX3 standards.B.
Determine the extent of the events in a text asdefined by the TimeML EVENT tag.
In addi-tion, determine the value of the features TENSE,ASPECT, POLARITY, and MODALITY.C.
Determine the temporal relation between anevent and a time expression in the same sen-tence.
For TempEval-2, this task is further re-stricted by requiring that either the event syn-tactically dominates the time expression or theevent and time expression occur in the samenoun phrase.D.
Determine the temporal relation between anevent and the document creation time.E.
Determine the temporal relation between twomain events in consecutive sentences.F.
Determine the temporal relation between twoevents where one event syntactically dominatesthe other event.
This refers to examples like?she heard an explosion?
and ?he said theypostponed the meeting?.The complete TimeML specification assumes thetemporal interval relations as defined by Allen(Allen, 1983) in Figure 1.113AB A EQUALS BAB A is BEFORE B;  B is AFTER AAB A MEETS B;  B is MET BY AAB A OVERLAPS B;  B is OVERLAPPED BY AAB A STARTS B;  B is STARTED BY AAB A FINISHES B;  B is FINISHED BY AAB A is DURING B;  B CONTAINS AFigure 1: Allen RelationsFor this task, however, we assume a reduced sub-set, as introduced in TempEval-1: BEFORE, AFTER,OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE.
However, we are investigat-ing whether for some tasks the more precise set ofTimeML relations could be used.Task participants may choose to either do alltasks, focus on the time expression task, focus onthe event task, or focus on the four temporal rela-tion tasks.
In addition, participants may choose oneor more of the five languages for which we providedata: English, Italian, Chinese, Spanish, and Ko-rean.3.1 Extent of Time ExpressionThis task involves identification of the EXTENT,TYPE, and VAL of temporal expressions in the text.Times can be expressed syntactically by adverbial orprepositional phrases, as shown in the following:(1) a. on Thursdayb.
November 15, 2004c.
Thursday eveningd.
in the late 80?se.
Later this afternoonf.
yesterdayThe TYPE of the temporal extent must be identified.There are four temporal types that will be distin-guished for this task;(2) a.
Time: at 2:45 p.m.b.
Date: January 27, 1920, yesterdayc.
Duration two weeksd.
Set: every Monday morningThe VAL attribute will assume values according toan extension of the ISO 8601 standard, as enhancedby TIMEX2.
(3) November 22, 2004<TIMEX3 tid="t1" type="DATE"value="2004-11-22"/>3.2 Extent of Event ExpressionThe EVENT tag is used to annotate those elements ina text that describe what is conventionally referred toas an eventuality.
Syntactically, events are typicallyexpressed as inflected verbs, although event nomi-nals, such as ?crash?
in killed by the crash, shouldalso be annotated as EVENTs.In this task, event extents must be identified andtagged with EVENT, along with values for the fea-tures TENSE, ASPECT, POLARITY, and MODALITY.Examples of these features are shown below:(4) should have bought<EVENT id="e1" pred="BUY" pos="VERB"tense="PAST" aspect="PERFECTIVE"modality="SHOULD" polarity="POS"/>(5) did not teach<EVENT id="e2" pred="TEACH" pos="VERB"tense="PAST" aspect="NONE"modality="NONE" polarity="NEG"/>The specifics on the definition of event extentwill follow the published TimeML guideline (cf.timeml.org).3.3 Within-sentence Event-Time AnchoringThis task involves determining the temporal relationbetween an event and a time expression in the samesentence.
This was present in TempEval-1, but here,in TempEval-2, this problem is further restricted byrequiring that the event either syntactically domi-nates the time expression or the event and time ex-pression occur in the same noun phrase.
For exam-ple, the following constructions will be targeted fortemporal labeling:114(6) Mary taughte1 on Tuesday morningt1OVERLAP(e1,t1)(7) They cancelled the eveningt2 classe2OVERLAP(e2,t2)3.4 Neighboring Sentence Event-EventOrderingIn this task, the goal is to identify the temporal re-lation between two main events in consecutive sen-tences.
This task was covered in the previous com-petition, and includes pairs such as that shown be-low:(8) The President spokee1 to the nation on Tuesdayon the financial crisis.
He had conferrede2 withhis cabinet regarding policy the day before.AFTER(e1,e2)3.5 Sentence Event-DCT OrderingThis task was also included in TempEval-1 and re-quires the identification of the temporal order be-tween the matrix event of the sentence and the Docu-ment Creation Time (DCT) of the article or text.
Forexample, the text fragment below specifies a fixedDCT, relative to which matrix events from the twosentences are ordered:(9) DCT: MARCH 5, 2009a.
Most troops will leavee1 Iraq by August of2010.
AFTER(e1,dct)b.
The country defaultede2 on debts for thatentire year.
BEFORE(e2,dct)3.6 Within-sentence Event-Event OrderingThe final task involves identifying the temporal re-lation between two events, where one event syntac-tically dominates the other event.
This includes ex-amples such as those illustrated below.
(10) The students hearde1 a fire alarme2.OVERLAP(e1,e2)(11) He saide1 they had postponede2 the meeting.AFTER(e1,e2)4 Resources and Evaluation4.1 DataThe development corpus will contain the followingdata:1.
Sentence boundaries;2.
The document creation time (DCT) for eachdocument;3.
All temporal expressions in accordance withthe TimeML TIMEX3 tag;4.
All events in accordance with the TimeMLEVENT tag;5.
Main event markers for each sentence;6.
All temporal relations defined by tasks Cthrough F.The data for the five languages are being preparedindependently of each other.
We do not provide aparallel corpus.
However, annotation specificationsand guidelines for the five languages will be devel-oped in conjunction with one other.
For some lan-guages, we may not use all four temporal linkingtasks.
Data preparation is currently underway forEnglish and will start soon for the other languages.Obviously, data preparation is a large task.
For En-glish and Chinese, the data are being developed atBrandeis University under three existing grants.For evaluation data, we will provide two data sets,each consisting of different documents.
DataSet1 isfor tasks A and B and will contain data item 1 and 2from the list above.
DataSet2 is for tasks C thoughF and will contain data items 1 through 5.4.2 Data PreparationFor all languages, annotation guidelines are definedfor all tasks, based on version 1.2.1 of the TimeMLannotation guidelines for English3.
The most no-table changes relative to the previous TimeMLguidelines are the following:?
The guidelines are not all presented in one doc-ument, but are split up according to the sevenTempEval-2 tasks.
Full temporal annotationhas proven to be a very complex task, split-ting it into subtasks with separate guidelines for3See http://www.timeml.org.115each task has proven to make temporal annota-tion more manageable.?
It is not required that all tasks for temporal link-ing (tasks C through F) use the same relationset.
One of the goals during the data prepara-tion phase is to determine what kind of relationset makes sense for each individual task.?
The guidelines can be different depending onthe language.
This is obviously required be-cause time expressions, events, and relationsare expressed differently across languages.Annotation proceeds in two phases: a dualannotation phase where two annotators annotateeach document and an adjudication phase where ajudge resolves disagreements between the annota-tors.
We are expanding the annotation tool used forTempEval-1, making sure that we can quickly an-notate data for all tasks while making it easy for alanguage to define an annotation task in a slightlydifferent way from another language.
The BrandeisAnnotation Tool (BAT) is a generic web-based anno-tation tool that is centered around the notion of an-notation tasks.
With the task decomposition allowedby BAT, it is possible to flexibly structure the com-plex task of temporal annotation by splitting it up inas many sub tasks as seems useful.
As such, BAT iswell-suited for TempEval-2 annotation.
Comparisonof annotation speed with tools that do not allow taskdecomposition showed that annotation with BAT isup to ten times faster.
Annotation has started forItalian and English.For all tasks, precision and recall are used as eval-uation metrics.
A scoring program will be suppliedfor participants.5 ConclusionIn this paper, we described the TempEval-2 taskwithin the SemEval 2010 competition.
This taskinvolves identifying the temporal relations betweenevents and temporal expressions in text.
Usinga subset of TimeML temporal relations, we showhow temporal relations and anchorings can be an-notated and identified in five different languages.The markup language adopted presents a descrip-tive framework with which to examine the tempo-ral aspects of natural language information, demon-strating in particular, how tense and temporal infor-mation is encoded in specific sentences, and howtemporal relations are encoded between events andtemporal expressions.
This work paves the way to-wards establishing a broad and open standard meta-data markup language for natural language texts, ex-amining events, temporal expressions, and their or-derings.ReferencesJames Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Communications of the ACM,26(11):832?843.Bran Boguraev, James Pustejovsky, Rie Ando, and MarcVerhagen.
2007.
Timebank evolution as a communityresource for timeml parsing.
Language Resource andEvaluation, 41(1):91?115.James Pustejovsky, David Day, Lisa Ferro, RobertGaizauskas, Patrick Hanks, Marcia Lazo, Roser Saur?
?,Andrew See, Andrea Setzer, and Beth Sundheim.2003.
The TimeBank Corpus.
Corpus Linguistics,March.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Proc.
of the Fourth Int.
Work-shop on Semantic Evaluations (SemEval-2007), pages75?80, Prague, Czech Republic, June.
Association forComputational Linguistics.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Jessica Moszkowicz, and James Puste-jovsky.
2009.
The tempeval challenge: identifyingtemporal relations in text.
Language Resources andEvaluation.116
