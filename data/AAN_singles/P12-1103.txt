Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 979?987,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsImprove SMT Quality with Automatically Extracted Paraphrase RulesWei He1, Hua Wu2, Haifeng Wang2, Ting Liu1*1Research Center for Social Computing and InformationRetrieval, Harbin Institute of Technology{whe,tliu}@ir.hit.edu.cn2Baidu{wu_hua,wanghaifeng}@baidu.comAbstract1We propose a novel approach to improveSMT via paraphrase rules which areautomatically extracted from the bilingualtraining data.
Without using extraparaphrase resources, we acquire the rulesby comparing the source side of the parallelcorpus with the target-to-sourcetranslations of the target side.
Besides theword and phrase paraphrases, the acquiredparaphrase rules mainly cover thestructured paraphrases on the sentencelevel.
These rules are employed to enrichthe SMT inputs for translation qualityimprovement.
The experimental resultsshow that our proposed approach achievessignificant improvements of 1.6~3.6 pointsof BLEU in the oral domain and 0.5~1points in the news domain.1 IntroductionThe translation quality of the SMT system ishighly related to the coverage of translation models.However, no matter how much data is used fortraining, it is still impossible to completely coverthe unlimited input sentences.
This problem ismore serious for online SMT systems in real-worldapplications.
Naturally, a solution to the coverageproblem is to bridge the gaps between the inputsentences and the translation models, either fromthe input side, which targets on rewriting the inputsentences to the MT-favored expressions, or fromThis work was done when the first author was visiting Baidu.
*Correspondence author: tliu@ir.hit.edu.cnthe side of translation models, which tries to enrichthe translation models to cover more expressions.In recent years, paraphrasing has been provenuseful for improving SMT quality.
The proposedmethods can be classified into two categoriesaccording to the paraphrase targets: (1) enrichtranslation models to cover more bilingualexpressions; (2) paraphrase the input sentences toreduce OOVs or generate multiple inputs.
In thefirst category, He et al (2011), Bond et al (2008)and Nakov (2008) enriched the SMT models viaparaphrasing the training corpora.
Kuhn et al(2010) and Max (2010) used paraphrases tosmooth translation models.
For the secondcategory, previous studies mainly focus on findingtranslations for unknown terms using phrasalparaphrases.
Callison-Burch et al (2006) andMarton et al (2009) paraphrase unknown terms inthe input sentences using phrasal paraphrasesextracted from bilingual and monolingual corpora.Mirkin et al (2009) rewrite OOVs withentailments and paraphrases acquired fromWordNet.
Onishi et al (2010) and Du et al (2010)use phrasal paraphrases to build a word lattice toget multiple input candidates.
In the abovemethods, only word or phrasal paraphrases areused for input sentence rewriting.
No structuredparaphrases on the sentence level have beeninvestigated.
However, the information in thesentence level is very important for disambiguation.For example, we can only substitute play withdrama in a context related to stage or theatre.Phrasal paraphrase substitutions can hardly solvesuch kind of problems.In this paper, we propose a method that rewrites979the input sentences of the SMT system usingautomatically extracted paraphrase rules which cancapture structures on sentence level in addition toparaphrases on the word or phrase level.
Withoutextra paraphrase resources, a novel approach isproposed to acquire paraphrase rules from thebilingual training corpus based on the results ofForward-Translation and Back-Translation.
Therules target on rewriting the input sentences to anMT-favored expression to ensure a bettertranslation.
The paraphrase rules cover all kinds ofparaphrases on the word, phrase and sentencelevels, enabling structure reordering, word orphrase insertion, deletion and substitution.
Theexperimental results show that our proposedapproach achieves significant improvements of1.6~3.6 points of BLEU in the oral domain and0.5~1 points in the news domain.The remainder of the paper is organized asfollows: Section 2 makes a comparison betweenthe Forward-Translation and Back-Translation.Section 3 introduces our methods that extractparaphrase rules from the bilingual corpus of SMT.Section 4 describes the strategies for constructingword lattice with paraphrase rules.
Theexperimental results and some discussions arepresented in Section 5 and Section 6.
Section 7compares our work to the previous researches.Finally, Section 8 concludes the paper and suggestsdirections for future work.2 Forward-Translation vs. Back-TranslationThe Back-Translation method is mainly used forautomatic MT evaluation (Rapp 2009).
Thisapproach is very helpful when no target languagereference is available.
The only requirement is thatthe MT system needs to be bidirectional.
Theprocedure includes translating a text into certainforeign language with the MT system (Forward-Translation), and translating it back into theoriginal language with the same system (Back-Translation).
Finally the translation quality ofBack-Translation is evaluated by using the originalsource texts as references.Sun et al (2010) reported an interestingphenomenon: given a bilingual text, the Back-Translation results of the target sentences is betterthan the Forward-Translation results of the sourcesentences.
Clearly, let (S0, T0) be the initial pair ofbilingual text.
A source-to-target translation systemSYS_ST and a target-to-source translation systemSYS_TS are trained using the bilingual corpus.?????
is a Forward-Translation function, and?????
is a function of Back-Translation which canbe deduced with two rounds of translations:?????
?
???_??????_??????.
In the first roundof translation, S0 and T0 are fed into SYS_ST andSYS_TS, and we get T1 and S1 as translation results.In the second round, we translate S1 back into thetarget side with SYS_ST, and get the translation T2.The procedure is illustrated in Figure 1, which canalso formally be described as:1.
T1 = FT(S0) = SYS_ST(S0).2.
T2 = BT(T0), which can be decomposed intotwo steps: S1 = SYS_TS(T0), T2 = SYS_ST(S1).Using T0 as reference, an interesting result isreported in Sun et al (2010) that T2 achieves ahigher score than T1 in automatic MT evaluation.This outcome is important because T2 is translatedFigure 1: Procedure of Forward-Translation and Back-Translation.S0 T0S1 T1T2Source Language Target LanguageInitial Parallel Text1st Round Translation2nd Round TranslationForward-TranslationBack-Translation980from a machine-generated text S1, but T1 istranslated from a human-write text S0.
Why themachine-generated text results in a bettertranslation than the human-write text?
Twopossible reasons may explain this phenomenon: (1)in the first round of translation T0 ?
S1, sometarget word orders are reserved due to thereordering failure, and these reserved orders lead toa better result in the second round of translation; (2)the text generated by an MT system is more likelyto be matched by the reversed but homologous MTsystem.Note that all the texts of S0, S1, S2, T0 and T1 aresentence aligned because the initial parallel corpus(S0, T0) is aligned in the sentence level.
The alignedsentence pairs in (S0, S1) can be considered asparaphrases.
Since S1 has some MT-favoredstructures which may result in a better translation,an intuitive idea is whether we can learn thesestructures by comparing S1 with S0.
This is themain assumption of this paper.
Taking (S0, S1) asparaphrase resource, we propose a method thatautomatically extracts paraphrase rules to capturethe MT-favored structures.3 Extraction of Paraphrase Rules3.1 Definition of Paraphrase RulesWe define a paraphrase rule as follows:1.
A paraphrase rule consists of two parts, left-hand-side (LHS) and right-hand-side (RHS).Both of LHS and RHS consist of non-terminals (slot) and terminals (words).2.
LHS must start/end with a terminal.3.
There must be at least one terminal betweentwo non-terminals in LHS.A paraphrase rule in the format of:LHS ?
RHSwhich means the words matched by LHS can beparaphrased to RHS.
Taking Chinese as a casestudy, some examples of paraphrase rules areshown in Table 1.3.2  Selecting Paraphrase Sentence PairsFollowing the methods in Section 2, the initialbilingual corpus is (S0, T0).
We train a source-to-target PBMT system (SYS_ST) and a target-to-source PBMT system (SYS_TS) on the parallelcorpus.
Then a Forward-Translation is performedon S0 using SYS_ST, and a Back-Translation isperformed on T0 using SYS_TS and SYS_ST.
Asmentioned above, the detailed procedure is: T1 =SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1).Finally we compute BLEU (Papineni et al 2002)score for every sentence in T2 and T1, using thecorresponding sentence in T0 as reference.
If thesentence in T2 has a higher BLEU score than thealigned sentence in T1, the corresponding sentencesin S0 and S1 are selected as candidate paraphrasesentence pairs, which are used in the followingsteps of paraphrase extractions.3.3 Word Alignments FilteringWe can construct word alignment between S0 andS1 through T0.
On the initial corpus of (S0, T0), weconduct word alignment with Giza++ (Och andNey, 2000) in both directions and then apply thegrow-diag-final heuristic (Koehn et al, 2005) forsymmetrization.
Because S1 is generated byfeeding T0 into the PBMT system SYS_TS, theword alignment between T0 and S1 can be acquiredfrom the verbose information of the decoder.The word alignments of S0 and S1 contain noiseswhich are produced by either wrong alignment ofGIZA++ or translation errors of SYS_TS.
To ensurethe alignment quality, we use some heuristics tofilter the alignment between S0 and S1:1.
If two identical words are aligned in S0 andS1, then remove all the other links to the twowords.No.
LHS RHS1 ?
?/ride   X1   ???
?/bus ?
?/ride    X1   ?
?/bus2    ?/at   X1  ?/location   ??
?/turn left  ??
?/turn left   ?/at   X1  ?/location3 ?/NULL   X1    ?/give    ?/me ?/give    ?/me    X14 ?/from  X1  ?/to  X2  ?/need ?
?/how long?
?/time?/need   ?/spend  ?
?/how long  ?
?/time?/from X1?/to X2Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word9812.
Stop words (including some function wordsand punctuations) can only be aligned toeither stop words or null.Figure 2 illustrates an example of using theheuristics to filter alignment.3.4 Extracting Paraphrase RulesFrom the word-aligned sentence pairs, we thenextract a set of rules that are consistent with theword alignments.
We use the rule extractingmethods of Chiang (2005).
Take the sentence pairin Figure 2 as an example, two initial phrase pairsPP1 = ??
?
??
???
||| ?
?
??
???
?and  PP2 = ??
?
?
??
???
?
??
||| ??
??
?
?
??
????
are identified, andPP1 is contained by PP2, then we could form therule:?
X1 ?
??
?
?
?
??
X1to  have interest  very feel interest4 Paraphrasing the Input SentencesThe extracted paraphrase rules aim to rewrite theinput sentences to an MT-favored form which maylead to a better translation.
However, it is risky todirectly replace the input sentence with aparaphrased sentence, since the errors in automaticparaphrase substitution may jeopardize thetranslation result seriously.
To avoid such damage,for a given input sentence, we first transform allparaphrase rules that match the input sentences tophrasal paraphrases, and then build a word latticefor SMT decoder using the phrasal paraphrases.
Inthis case, the decoder can search for the best resultamong all the possible paths.The input sentences are first segmented into sub-sentences by punctuations.
Then for each sub-sentence, the matched paraphrase rules are rankedaccording to: (1) the number of matched words; (2)the frequency of the paraphrase rule in the trainingdata.
Actually, the ranking strategy tends to selectparaphrase rules that have more matched words(therefore less ambiguity) and higher frequency(therefore more reliable).4.1 Applying Paraphrase RulesGiven an input sentence S and a paraphrase rule R<RLHS, RRHS>, if S matches RLHS, then the matchedpart can be replaced by RRHS.
An example forapplying the paraphrase rules is illustrated inFigure 3.From Figure 3, we can see that the words ofposition 1~3 are replaced to ???
10 ?
??
?.Actually, only the words at position 3 and 4 areparaphrased to the word ???
?, other words areleft unchanged.
Therefore, we can use a triple,<MIN_RP_TEXT, COVER_START, COVER_LEN>(<??
, 3, 1> in this example) to denote theparaphrase rule, which means the minimal text toreplace is ???
?, and the paraphrasing starts atposition 3 and covers 1 words.In this manner, all the paraphrase rules matchedfor a certain sentence can be converted to theformat of <MIN_RP_TEXT, COVER_START,COVER_LEN>, which can also be considered asphrasal paraphrases.
Then the methods of buildingphrasal paraphrases into word lattice for SMTinputs can be used in our approaches.??
??
[10?]
??????
[10?]
??RuleLHS:?
?/ride  X1 ????/busRHS:?
?/ride  X1  ?
?/busFigure 3: Example for Applying Paraphrase Rules0         1            2                3welcome  ride     No.10         busride       No.10        busI  very feel interest that N/A  blue   handbagI     to   that   N/A  blue  handbag have interest?
?
?
??
?
?
??
???
??
?
?
?
??
???
?
??
?Figure 2: Example for Word AlignmentFiltrationI     to   that   N/A  blue  handbag have interest?
?
?
?
??
???
?
??
?I  very feel interest that N/A  blue   handbag?
?
?
??
?
?
??
???
?9824.2 Construction of Paraphrase LatticeGiven an input sentence, all the matchedparaphrase rules are converted to phrasalparaphrases first.
Then we build the phrasalparaphrases into word lattices using the methodsproposed by Du et al (2010).
The constructionprocess takes advantage of the correspondencebetween detected phrasal paraphrases and positionsof the original words in the input sentence, andthen creates extra edges in the lattices to allow thedecoder to consider paths involving the paraphrasewords.
An example is illustrated in Figure 4: givena sequence of words {w1,?,wN} as the input, twophrases ?
={?1,?
?p} and ?
= {?1,?, ?q} aredetected as paraphrases for P1 = {wx,?, wy} (1 ?
x?
y ?
N) and P2 = {wm,?,wn} (1 ?
m ?
n ?
N)respectively.
The following steps are taken totransform them into word lattices:1.
Transform the original source sentence intoword lattice.
N + 1 nodes (?k, 0 ?
k ?
N) arecreated, and N edges labeled with wi (1 ?
i ?N) are generated to connect themsequentially.2.
Generate extra nodes and edges for each ofthe paraphrases.
Taking ?
as an example,firstly, p ?
1 nodes are created, and then pedges labeled with ?j (1 ?
j ?
p) aregenerated to connect node ?x-1, p-1 nodesand ?y-1.Via step 2, word lattices are generated by addingnew nodes and edges coming from paraphrases.4.3  Weight EstimationThe weights of new edges in the lattices areestimated by an empirical method base on rankingpositions.
Following Du et al (2010), supposingthat E = {e1,?,ek} are a set of new edgesconstructed from k paraphrase rules, which aresorted in a descending order.
Then the weight foran edge ei is calculated as:??e??
?
1?
?
?
??
?1 ?
?
?
??
where k is a predefined tradeoff parameter betweendecoding speed and the number of potentialparaphrases being considered.5  Experiments5.1  Experimental DataIn our experiments, we used Moses (Koehn et al,2007) as the baseline system which can supportlattice decoding.
The alignment was obtained usingGIZA++ (Och and Ney, 2003) and then wesymmetrized the word alignment using the grow-diag-final heuristic.
Parameters were tuned usingMinimum Error Rate Training (Och, 2003).
Tocomprehensively evaluate the proposed methods indifferent domains, two groups of experiments werecarried out, namely, the oral group (Goral) and thenews group (Gnews).
The experiments wereconducted in both Chinese-English and English-Chinese directions for the oral group, and Chinese-English direction for the news group.
The Englishsentences were all tokenized and lowercased, andthe Chinese sentences were segmented into wordsby Language Technology Platform (LTP) 1 .
Weused SRILM2 for the training of language models(5-gram in all the experiments).
The metrics forautomatic evaluation were BLEU 3  and TER 4(Snover et al, 2005).The detailed statistics of the training data in Goralare showed in Table 2.
For the bilingual corpus, weused the BTEC and PIVOT data of IWSLT 2008,HIT corpus 5  and other Chinese LDC (CLDC)1 http://ir.hit.edu.cn/ltp/2 http://www.speech.sri.com/projects/srilm/3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl4 http://www.umiacs.umd.edu/~snover/terp/5 The HIT corpus contains the CLDC Olympic corpus (2004-863-008) and the other HIT corpora available athttp://mitlab.hit.edu.cn/index.php/resources/29-the-resource/111-share-bilingual-corpus.html.Figure 4: An example of lattice-basedparaphrases for an input sentence.983corpora, including the Chinese-English SentenceAligned Bilingual Corpus (CLDC-LAC-2003-004)and the Chinese-English Parallel Corpora (CLDC-LAC-2003-006).
We trained a Chinese languagemodel for the E-C translation on the Chinese partof the bi-text.
For the English language model ofC-E translation, an extra corpus named Tanaka wasused besides the English part of the bilingualcorpora.
For testing and developing, we used sixChinese-English development corpora of IWSLT2008.
The statistics are shown in Table 3.In detail, we chose CSTAR03-test andIWSLT06-dev as the development set; and usedIWSLT04-test, IWSLT05-test, IWSLT06-dev andIWSLT07-test for testing.
For English-Chineseevaluation, we used IWSLT English-Chinese MTevaluation 2005 as the test set.
Due to the lackingof development set, we did not tune parameters onEnglish-Chinese side, instead, we just used thedefault parameters of Moses.In the experiments of the news group, we usedthe Sinorama and FBIS corpora (LDC2005T10 andLDC2003E14) for bilingual corpus.
Aftertokenization and filtering, this bilingual corpuscontained 319,694 sentence pairs (7.9M tokens onChinese side and 9.2M tokens on English side).We trained a 5-gram language model on theEnglish side of the bi-text.
The system was testedusing the Chinese-English MT evaluation sets ofNIST 2004, NIST 2006 and NIST 2008.
Fordevelopment, we used the Chinese-English MTevaluation sets of NIST 2002 and NIST 2005.Table 4 shows the statistics of test/developmentsets used in the news group.5.2 ResultsWe extract both Chinese and English rules in Goral,and Chinese paraphrase rules in Gnews bycomparing the results of Forward-Translation andBack-Translation as described in Section 3.
Duringthe extraction, some heuristics are used to ensurethe quality of paraphrase rules.
Take the extractionof Chinese paraphrase rules in Goral as a case study.Suppose (C0, E0) are the initial bilingual corpus ofGoral.
A Chinese-English and an English-ChineseMT system are trained on (C0, E0).
We performBack-Translation on E0 (??
????????
??
??
????????
??
??
), andForward-Translation on C0 (??
????????
??
??).
Suppose e1i and e2i are two aligned sentences in E1 and E2,c0i and c1i are the corresponding sentences in C0and C1.
(c0i, c1i) are selected for the extraction ofparaphrase rules if two conditions are satisfied: (1)BLEU(e2i) ?
BLEU(e1i) > ?1, and (2) BLEU(e2i) >?2, where BLEU???
is a function for computingBLEU score; ?1 and ?2 are thresholds for balancingthe rules number and the quality of paraphraserules.
In our experiment, ?1 and ?2 are empiricallyset to 0.1 and 0.3.As a result, we extract 912,625 Chinese and1,116,375 English paraphrase rules for Goral, andfor Gnews the number of Chinese paraphrase rules is2,877,960.
Then we use the extracted paraphraserules to improve SMT by building word lattices forthe input sentences.The Chinese-English experimental results ofGoral and Gnews are shown in Table 5 and Table 6,respectively.
It can be seen that our methodoutperforms the baselines in both oral and newsdomains.
Our system gains significantimprovements of 1.6~3.6 points of BLEU in theoral domain, and 0.5~1 points of BLEU in thenews domain.
Figure 5 shows the effect ofconsidered paraphrases (k) in the step of buildingCorpus #Sen. pairs #Ch.
words #En wordsBETC 19,972 174k 190kPIVOT 20,000 162k 196kHIT 80,868 788k 850kCLDC 190,447 1,167k 1,898kTanaka 149,207 - 1,375kTable 2: Statistics of training data in GoralCorpus #Sen.  #Ref.develop CSTAR03 test set 506 16 IWSLT06 dev set 489 7testIWSLT04 test set 500 16IWSLT05 test set 506 16IWSLT06 test set 500 7IWSLT07 test set 489 6Table 3: Statistics of test/develop sets in GoralCorpus #Sen.  #Ref.develop NIST 2002 878 10 NIST 2005 1,082 4testNIST 2004 1,788 5NIST 2006 1,664 4NIST 2008 1,357 4Table 4: Statistics of test/develop sets in Gnews984word lattices.
The result of English-Chineseexperiments in Goral is shown in Table 7.6 DiscussionWe make a detailed analysis on the Chinese-English translation results that are affected by ourparaphrase rules.
The aim is to investigate whatkinds of paraphrases have been captured in therules.
Firstly the input path is recovered from thetranslation results according to the tracinginformation of the decoder, and therefore we canexamine which path is selected by the SMTdecoder from the paraphrase lattice.
A humanannotator is asked to judge whether the recoveredparaphrase sentence keeps the same meaning as theoriginal input.
Then the annotator compares thebaseline translation with the translations proposedby our approach.
The analysis is carried out on theIWSLT 2007 Chinese-English test set, 84 out of489 input sentences have been affected byparaphrases, and the statistic of human evaluationis shown in Table 8.It can be seen in Table 8 that the paraphrasesachieve a relatively high accuracy, 60 (71.4%)paraphrased sentences retain the same meaning,and the other 24 (28.6%) are incorrect.
Among the60 correct paraphrases, 36 sentences finally resultin an improved translation.
We further analyzethese paraphrases and the translation results toinvestigate what kinds of transformation finallylead to the translation quality improvement.
Theparaphrase variations can be classified into fourcategories:(1) Reordering: The original source sentencesare reordered to be similar to the order ofthe target language.
(2) Word substitution: A phrase with multi-word translations is replaced by a phrasewith a single-word translation.
(3) Recovering omitted words: Ellipsis occursfrequently in spoken language.
Recoveringthe omitted words often leads to a bettertranslation.
(4) Removing redundant words: Mostly,translating redundant words may confusethe SMT system and would be unnecessary.Removing redundant words can mitigatethis problem.44.2?44.4?44.6?44.8?45.0?45.2?45.4?0 10 20 30 40BLEU?score?(%)Considered?paraprhases?
(k)Figure 5: Effect of considered paraphrases (k)on BLEU scoreModel BLEU TER iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07baseline 0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390para.
improved 0.5712 0.6107 0.2924 0.4193 0.3055 0.2722 0.5374 0.4217Model BLEU TER nist 04 nist 06 nist 08 nist 04 nist 06 nist 08baseline 0.2795 0.2389 0.1933 0.6554 0.6515 0.6652para.
improved 0.2891 0.2485 0.1978 0.6451 0.6407 0.6582model IWSLT 2005  BLEU TERbaseline 0.4644 0.4164para.
improved  0.4853 0.3883trans.para.
improve comparable worsen totalcorrect 36 20 4 60incorrect 1 9 14 24Table 8: Human analysis of the paraphrasingresults in IWSLT 2007 CE translationTable 5: Experimental results of Goral in Chinese-English directionTable 6: Experimental results of Gnews in Chinese-English directionTable 7: Experimental results of Goral inEnglish-Chinese direction985Four examples for category (1), (2), (3) and (4)are shown in Table 9, respectively.
The numbers inthe second column indicates the number of thesentences affected by the rules, among the 36sentences with improved paraphrasing andtranslation.
A sentence can be classified intomultiple categories.
Except category (2), the otherthree categories cannot be detected by the previousapproaches, which verify our statement that ourrules can capture structured paraphrases on thesentence level in addition to the paraphrases on theword or phrase level.Not all the paraphrased results are correct.Sometimes an ill paraphrased sentence can producebetter translations.
Take the first line of Table 9 asan example, the paraphrased sentence ??
?/Howmany ?
?/cigarettes ?
?/can ?
?/duty-free ?/take ?/NULL?
is not a fluent Chinese sentence,however, the rearranged word order is closer toEnglish, which finally results in a much bettertranslation.7 Related WorkPrevious studies on improving SMT usingparaphrase rules focus on hand-crafted rules.Nakov (2008) employs six rules for paraphrasingthe training corpus.
Bond et al (2008) usegrammars to paraphrase the source side of trainingdata, covering aspects like word order and minorlexical variations (tenses etc.)
but not contentwords.
The paraphrases are added to the sourceside of the corpus and the corresponding targetsentences are duplicated.A disadvantage for hand-crafted paraphraserules is that it is language dependent.
In contrast,our method that automatically extracted paraphraserules from bilingual corpus is flexible and suitablefor any language pairs.Our work is similar to Sun et al (2010).
Bothtried to capture the MT-favored structures frombilingual corpus.
However, a clear difference isthat Sun et al (2010) captures the structuresimplicitly by training an MT system on (S0, S1) and?translates?
the SMT input to an MT-favoredexpression.
Actually, the rewriting process isconsidered as a black box in Sun et al (2010).
Inthis paper, the MT-favored expressions arecaptured explicitly by automatically extractedparaphrase rules.
The advantages of the paraphraserules are: (1) Our method can explicitly capture thestructure information in the sentence level,enabling global reordering, which is impossible inSun et al (2010).
(2) For each rule, we can controlits quality automatically or manually.8 ConclusionIn this paper, we propose a novel method forextracting paraphrase rules by comparing thesource side of bilingual corpus to the target-to-source translation of the target side.
The acquiredparaphrase rules are employed to enrich the SMTinputs, which target on rewriting the inputsentences to an MT-favored form.
The paraphraserules cover all kinds of paraphrases on the word,phrase and sentence levels, enabling structurereordering, word or phrase insertion, deletion andsubstitution.
Experimental results show that theparaphrase rules can improve SMT quality in boththe oral and news domains.
The manualinvestigation on oral translation results indicatethat the paraphrase rules capture four kinds of MT-favored transformation to ensure translation qualityimprovement.Cate.
Num Original Sentence/Translation Paraphrased Sentence/Translation(1) 11?
?/cigarette ?
?/can ?
?/duty-free ?/take ?
?/how much ?/N/A ??
?/how much ?
?/cigarettes ?
?/can ?
?/duty-free ?/take ?/N/A ?what a cigarette can i take duty-free ?
how many cigarettes can i take duty-free  one ?
(2) 18?/you  ?/have  ?
?/how long  ?/N/A?
?/teaching ?
?/experience ?
?/you  ?/have  ?
?/how much  ??/teaching?
?/experience ?you have how long teaching experience ?
how much teaching experience you have ?
(3) 10 ?
?/need  ?
?/deposit  ?/N/A ?
?/you  ?
?/need  ?
?/deposit  ?/N/A ?
you need a deposit ?
do you need a deposit ?
(4) 4?
?/ring ?/fall ?/into ??
?/washbasin?/in ?/N/A ?ring off into the washbasin is in .?
?/ring  ?/fall  ?/into  ??
?/washbasin ?/N/A ?ring off into the washbasin .Table 9: Examples for classification of paraphrase rules986AcknowledgementThis work was supported by National NaturalScience Foundation of China (NSFC) (61073126,61133012), 863 High Technology Program(2011AA01A207).ReferencesFrancis Bond, Eric Nichols, Darren Scott Appling, andMichael Paul.
2008.
Improving Statistical MachineTranslation by Paraphrasing the Training Data.
InProceedings of the IWSLT, pages 150?157.Chris Callison-Burch, Philipp Koehn, and MilesOsborne.
2006.
Improved Statistical MachineTranslation Using Paraphrases.
In Proceedings ofNAACL, pages 17-24.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270.Jinhua Du, Jie Jiang, Andy Way.
2010.
FacilitatingTranslation Using Source Language ParaphraseLattices.
In Proceedings of EMNLP, pages 420-429.Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu.
2011.Enriching SMT Training Data via Paraphrasing.
InProceedings of IJCNLP, pages 803-810.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InProceedings of HLT/NAACL, pages 48?54Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388-395.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proceedings of IWSLT.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine MoranRichard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL Demo and Poster Sessions,pages 177?180.Roland Kuhn, Boxing Chen, George Foster and EvanStratford.
2010.
Phrase Clustering for Smoothing TMProbabilities-or, How to Extract Paraphrases fromPhrase Tables.
In Proceedings of COLING, pages608?616.Yuval Marton, Chris Callison-Burch, and Philip Resnik.2009.
Improved Statistical Machine TranslationUsing Monolingually-Dervied Paraphrases.
InProceedings of EMNLP, pages 381-390.Aur?lien Max.
2010.
Example-Based Paraphrasing forImproved Phrase-Based Statistical MachineTranslationIn Proceedings of EMNLP, pages 656-666.Shachar Mirkin, Lucia Specia, Nicola Cancedda, IdoDagan, Marc Dymetman, Idan Szpektor.
2009.Source-Language Entailment Modeling forTranslation Unknown Terms.
In Proceedings of ACL,pages 791-799.Preslav Nakov.
2008.
Improved Statistical MachineTranslation Using Monolingual Paraphrases.
InProceedings of ECAI, pages 338-342.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings of ACL,pages 440-447.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofACL, pages 160-167.Takashi Onishi, Masao Utiyama, Eiichiro Sumita.
2010.Paraphrase Lattice for Statistical MachineTranslation.
In Proceedings of ACL, pages 1-5.Kishore Papineni, Salim Roukos, Todd Ward, Wei-JingZhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedings ofACL, pages 311-318.Reinhard Rapp.
2009.
The Back-translation Score:Automatic MT Evaluation at the Sentence Levelwithout Reference Translations.
In Proceedings ofACL-IJCNLP, pages 133-136.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,John Makhoul, Linnea Micciulla, and RalphWeischedel.
2005.
A study of translation error ratewith targeted human annotation.
Technical ReportLAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland, July, 2005.Yanli Sun, Sharon O?Brien, Minako O?Hagan and FredHollowood.
2010.
A Novel Statistical Pre-ProcessingModel for Rule-Based Machine Translation System.In Proceedings of EAMT, 8pp.987
