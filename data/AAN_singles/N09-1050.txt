Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 442?449,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproved Pronunciation Features for Construct-driven Assessment ofNon-native Spontaneous SpeechLei Chen, Klaus Zechner, Xiaoming XiEducational Testing ServicePrinceton, NJ, USA{LChen,KZechner,XXi}@ets.orgAbstractThis paper describes research on automatic as-sessment of the pronunciation quality of spon-taneous non-native adult speech.
Since thespeaking content is not known prior to theassessment, a two-stage method is developedto first recognize the speaking content basedon non-native speech acoustic properties andthen forced-align the recognition results witha reference acoustic model reflecting nativeand near-native speech properties.
Featuresrelated to Hidden Markov Model likelihoodsand vowel durations are extracted.
Words withlow recognition confidence can be excludedin the extraction of likelihood-related fea-tures to minimize erroneous alignments dueto speech recognition errors.
Our experimentson the TOEFL R?Practice Online test, an En-glish language assessment, suggest that therecognition/forced-alignment method can pro-vide useful pronunciation features.
Our newpronunciation features are more meaningfulthan an utterance-based normalized acousticmodel score used in previous research from aconstruct point of view.1 IntroductionAutomated systems for evaluating highly pre-dictable speech (e.g.
read speech or speech thatis quite constrained in the use of vocabulary andsyntactic structures) have emerged in the pastdecade (Bernstein, 1999; Witt, 1999; Franco et al,2000; Hacker et al, 2005) due to the growing matu-rity of speech recognition and processing technolo-gies.
However, endeavors into automated scoringfor spontaneous speech have been sparse given thechallenge of both recognizing and assessing spon-taneous speech.
This paper addresses the develop-ment and evaluation of pronunciation features for anautomated system for scoring spontaneous speech.This system was deployed for the TOEFL R?PracticeOnline (TPO) assessment used by prospective testtakers to prepare for the official TOEFL R?test.A construct is a set of knowledge, skills, and abil-ities measured by a test.
The construct of the speak-ing test is embodied in the rubrics that human ratersuse to score the test.
It consists of three key cat-egories: delivery, language use, and topic devel-opment.
Delivery refers to the pace and the clar-ity of the speech, including performance on into-nation, rhythm, rate of speech, and degree of hesi-tancy.
Language use refers to the range, complex-ity, and precision of vocabulary and grammar use.Topic development refers to the coherence and full-ness of the response.
Most of the research on spon-taneous speech assessment focuses on the deliveryaspect given the low recognition accuracy on non-native spontaneous speech.The delivery aspect can be measured on four di-mensions: fluency, intonation, rhythm, and pronun-ciation.
For the TPO assessment, we have definedpronunciation as the quality of vowels, consonantsand word-level stress (segmentals).
Intonation andsentence-level stress patterns (supra-segmentals) arenot defined as part of pronunciation.
Pronuncia-tion is one of the key factors that impact the intelli-gibility and perceived comprehensibility of speech.Because pronunciation plays an important role inspeech perception, features measuring pronuncia-442tion using speech technologies have been exploredin many previous studies.
However, the bulk of theresearch on automatic pronunciation evaluation con-cerns read speech or highly predictable speech (Witt,1999; Franco et al, 2000; Hacker et al, 2005),where there is a high possibility of success in speechrecognition.
Automatic pronunciation evaluation ischallenging for spontaneous speech and has beenunder-explored.In this paper, we will describe a method forextracting pronunciation features based on sponta-neous speech that is well motivated by theories andsupported by empirical evaluations of feature per-formance.
In conceptualizing and computing thesefeatures, we draw on the literature on automatic pro-nunciation evaluation for constrained speech.
As de-scribed in the related work in Section 2, the widelyused features for measuring pronunciation are (1)likelihood (posterior probability) of a phoneme be-ing spoken given the observed audio sample thatis computed in a Viterbi decoding process, and (2)phoneme length measurements that are compared tostandard references based on native speech.However, we have also come up with unique solu-tions to address the issue of relatively low accuracyin recognizing spontaneous speech.
Our methods offeature extraction are designed with considerationsof how to best capture the quality of pronunciationgiven technological constraints.The remainder of the paper is organized as fol-lows: Section 2 reviews the related research; Sec-tion 3 describes our method to extract a set of fea-tures for measuring pronunciation; Section 4 de-scribes the design of the experiments, including thequestions investigated, the data, the speech process-ing technologies, and the measurement metrics; Sec-tion 5 reports on the experimental results; Section 6discusses the experimental results; and Section 7summaries the findings and future research planned.2 Related workThere is previous research on utilizing speech recog-nition technology to automatically assess non-nativespeakers?
communicative competence (e.g., fluency,intonation, and pronunciation).
Witt (Witt, 1999)developed the Goodness of Pronunciation (GOP)measurement for measuring pronunciation based onHidden Markov Model (HMM) log likelihood.
Us-ing a similar method, Neumeyer et al (Neumeyer etal., 2000) designed a series of likelihood related pro-nunciation features, e.g., the local average likelihoodand global average likelihood.
Hacker et al (Hackeret al, 2005) utilized a relatively large feature vectorfor scoring pronunciation.Pronunciation has been the focus of assessment inseveral automatic speech scoring systems.
Franco etal.
(Franco et al, 2000) presented a system for au-tomatic evaluation of pronunciation quality on thephoneme level and the sentence level of speech bynative and non-native speakers of English and otherlanguages (e.g., French).
A forced alignment be-tween the speech read by subjects and the ideal paththrough the HMM was computed.
Then, the logposterior probabilities for a certain position in thesignal were computed to achieve a local pronunci-ation score.
Cucchiarini et al (Cucchiarini et al,1997a; Cucchiarini et al, 1997b) designed a systemfor scoring Dutch pronunciation along a similar line.Their pronunciation feature set was more extensive,including various log likelihood HMM scores andphoneme duration scores.
In these two systems, thespeaking skill scores computed on features by ma-chine are found to have good agreement with scoresprovided by humans.A limited number of studies have been conductedon assessing speaking proficiency based on sponta-neous speech.
Moustroufas and Digalakis (Mous-troufas and Digalakis, 2007) designed a system toautomatically evaluate the pronunciation of foreignspeakers using unknown text.
The difference in therecognition results between a recognizer trained onspeakers?
native languages (L1) and another recog-nizer trained on their learned languages (L2) wasused for pronunciation scoring.
Zechner and Be-jar (Zechner and Bejar, 2006) presented a systemto score non-native spontaneous speech using fea-tures derived from the recognition results.
Follow-ing their work, an operational assessment system,SpeechRaterTM , was implemented with further im-provements (Zechner et al, 2007).There are some issues with the method to extractpronunciation features in the previous research onautomated assessment of spontaneous speech (Zech-ner and Bejar, 2006; Zechner et al, 2007).
For ex-443ample, the acoustic model (AM) that was used to es-timate a likelihood of a phoneme being spoken waswell-fitted to non-native speech acoustic properties.Further, other important aspects of pronunciation,e.g., vowel duration, have not been utilized as a fea-ture in the current SpeechRaterTMsystem.
Likeli-hoods estimated on non-words (such as silences andfillers) that were not central to the measurement ofpronunciation were used in the feature extraction.
Inaddition, mis-recognized words lead to wrong like-lihood estimation.
Our paper attempts to address allof these limitations described above.3 Extraction of Pronunciation FeaturesFigure 1 depicts our new method for extracting anexpanded set of pronunciation features in a moremeaning way.Figure 1: Two-stage pronunciation feature extractionWe used two different AMs for pronunciation fea-ture extraction.
First, we used an AM optimizedfor speech recognition (typically an AM adaptedon non-native speech to better fit non-native speak-ers?
acoustics patterns) to generate word hypotheses;then we used the other AM optimized for pronun-ciation scoring (typically trained on native or near-native speech to be a good reference model reflect-ing expected speech characteristics) to force alignthe speech signals to the word hypotheses and tocompute the likelihoods of individual words beingspoken and durations of phonemes; finally new pro-nunciation features were extracted based on thesemeasurements.Some notations used for computing the pronunci-ation features are listed in Table 1.
Based on thesenotations, the proposed new pronunciation featuresare described in Table 2.
To address the limita-tions of previous research on automated assessmentof pronunciation, which was described in Section 2,our proposed method has achieved improvements on(1) using the two-stage method to compute HMMlikelihoods using a reference acoustic model trainedon native and near-native speech, (2) expanding thecoverage of pronunciation features by using vowelduration shifts that are compared to standard normsof native speech, (3) and using likelihoods on theaudio portions that are recognized as words and ap-plying various normalizations.Table 1: Notations used for pronunciation feature extrac-tionVariable MeaningL(xi) the likelihood of word xi being spo-ken given the observed audio signalti the duration of word i in a responseTs the duration of the entire responseTn?i=1ti, the summation of the durationof all words, where T ?
Tsn the number of words in a responsem the number of letters in a responseR mTs , the frequency of letters (as the rateof speech)vi vowel iNv the total number of vowelsPvi the duration of vowel viP?
the average vowel duration (across allvowels in the response being scored)Dvi the standard average duration ofvowel vi (estimated on a nativespeech corpus)D?
the averaged vowel duration (on allvowels in a native speech corpus)Svi |Pvi ?
Dvi |, duration shift of vowelvi (measured as the absolute value ofthe difference between the duration ofvowel vi and its standard value)Snvi |PviP?
?DviD?
|, normalized duration shiftof vowel vi (measured as the absolutevalue of the normalized difference be-tween the duration of vowel vi and itsstandard value)4 Experiment designWe first raise three questions that we try to answerwith our experiments.
Then, we describe the datasets and the speech recognizers, especially the two444Table 2: A list of proposed pronunciation featuresFeature Formula MeaningL1n?i=1L(xi) summation of likeli-hoods of all the indi-vidual wordsL2 L1/n average likelihoodacross all wordsL3 L1/m average likelihoodacross all lettersL4 L1/T average likelihoodper secondL5n?i=1L(xi)tin average likelihooddensity across allwordsL6 L4/R L4 normalized by therate of speechL7 L5/R L5 normalized by therate of speechS?Nv?i=1SviNv average vowel dura-tion shiftsS?nNv?i=1SnviNv average normalizedvowel duration shiftsdifferent acoustic models fitted to non-native and ex-pected speech respectively.
Finally, we describe theevaluation criterion used in the experiment.4.1 Research questionsIn order to justify that the two-stage method for ex-tracting pronunciation features is a valid method thatprovides useful features for assessing pronunciation,the following questions need to be answered:Q1: Can the words hypothesized be used to approx-imate the human transcripts in the forced align-ment step?Q2: Are the new pronunciation features effectivefor assessment?Q3: Can the likelihood-related features be im-proved when using only words correctly recog-nized?4.2 DataTable 3 lists the data sets used in the experiment.Non-native speech collected in the TPO was used intraining a non-native AM.
For feature evaluations,we selected 1, 257 responses from the TPO data col-lected in 2006.
Within this set, 645 responses weretranscribed.
Holistic scores were assigned by humanraters based on a score scale of 1 (the lowest profi-ciency) to 4 (the highest proficiency).In the TOEFL R?Native Speaker Study, nativespeakers of primarily North American English(NaE) took the TOEFL R?test and their speech fileswere collected.
This TOEFL R?native speech dataand some high-scored TPO responses were usedin the adaptation of an AM representing expectedspeech properties.
In addition, 1, 602 responses ofnative speech, which had the highest speech profi-ciency scores in NaE, were used to estimate standardaverage vowel durations.Type Function Source Sizenon-nativespeechAM training TPO ?
30 hrsfeature evalua-tionTPO col-lected in20061, 257responses(645 withtran-scripts)nativeornear-nativespeechAM adaptation TPO andTOEFLNative?
2, 000responsesestimation ofstandard voweldurationsTOEFLNative1, 602 re-sponsesTable 3: Data sets used in the experiment4.3 Speech technologiesFor speech recognition and forced alignment, weused a gender-independent fully continuous HMMspeech recognizer.
Two different AMs were used inthe recognition and forced alignment steps respec-tively.The AM used in the recognition was trainedon about 30 hours of non-native speech from theTPO.
For language model training, a large corpusof non-native speech (about 100 hours) was used445and mixed with a large general-domain languagemodel (trained from the Broadcast News (BN) cor-pus (Graff et al, 1997) of the Linguistic Data Con-sortium (LDC)).
In the pronunciation feature extrac-tion process depicted in Figure 1, this AM was usedto recognize non-native speech to generate the wordhypotheses.The AM used in the forced alignment was trainedon native speech and high-scored non-native speech.It was trained as follows: starting from a genericrecognizer, which was trained on a large and var-ied native speech corpus, we adapted the AM usingbatch-mode MAP adaptation.
The adaptation corpuscontained about 2, 000 responses with high scores inprevious TPO tests and the TOEFL R?Native SpeakerStudy.
In addition, this AM was used to estimatestandard norms of vowels as described in Table 1.4.4 Measurement metricTo measure the quality of the developed features,a widely used metric is the Pearson correlation (r)computed between the features and human scores.In previous studies, human holistic scores of per-ceived proficiency have been widely used in esti-mating the correlations.
In our experiment, we willuse the absolute value of Pearson correlation withhuman holistic scores (|r|) to evaluate the features.Given the close relationship between pronunciationquality and overall speech proficiency, |r| is ex-pected to approximate the strength of its relationshipwith the human pronunciation scores.5 Experimental Results5.1 Results for Q1When assessing read speech, the transcription ofthe spoken content is known prior to the assess-ment and used to forced-align the speech for fea-ture extraction.
However, when assessing sponta-neous speech, we do not know the spoken contentand cannot provide a correct word transcription forthe forced alignment with imperfect speech recogni-tion.
A practical solution is to use the recognitionhypothesis to approximate the human transcript inthe forced alignment.
Since the recognition word ac-curacy on non-native spontaneous speech is not veryhigh (for example, a word accuracy of about 50% onthe TPO data was reported in (Zechner et al, 2007)),it is critical to verify that the approximation can pro-vide good enough pronunciation features comparedto the ones computed in an ideal scenario (by usingthe human transcript in the forced alignment step).We ran forced alignment on 645 TPO responseswith human transcriptions, using both the manualtranscription and the word hypotheses from the rec-ognizer described in Section 4.3.
Then, based onthese two forced alignment outputs, we extracted thepronunciation features as described in Section 3.Table 4 reports the |r|s between the proposedpronunciation features and human holistic scoreswhen using the forced alignment results from ei-ther transcriptions or recognition hypotheses.
Therelative |r| reduction (defined as (|r|transcriptions ?|r|hypotheses)/|r|transcriptions ?
100) is reported tomeasure the magnitude reduction.Based on the results shown in Table 4, we find thatthe pronunciation features computed based on theforced alignment results using transcriptions havehigher |r|s with the human holistic scores than thecorresponding features computed based on the FAresults using the recognition hypotheses.
This is notsurprising given that only 50% ?
60% word accu-racy can be achieved when recognizing non-nativespontaneous speech.
However, the pronunciationfeatures computed using the recognition hypothe-ses that is feasible in practice show some promisingcorrelations to human holistic scores.
For example,L3, L6, and L7 have |r|s larger than 0.45 and S?nhas an |r| larger than 0.35.
Compared to the cor-responding features computed using the FA resultsbased on transcriptions, these promising pronuncia-tion features that can be obtained practically, showsome reduction in quality (from 13.4% to 21.1%)but are still usable.
Therefore, our proposed two-stage method for pronunciation feature extraction isproven to be a practical way for the computation offeatures that have acceptable performance.5.2 Result for Q2Although our proposed modifications described inSection 3 have improved the meaningfulness of thefeatures, an empirical study is needed to examine theactual utility of these features for the assessment ofpronunciation.In the experiment described in Section 5.1, fourpronunciation features (including L3, L6, L7, and446Feature |r| usingtranscrip-tion|r| usingrecog-nitionhypothesisrelative |r|reduction(%)L1 0.216 0.107 50.5L2 0.443 0.416 6.1L3 0.506 0.473 6.5L4 0.363 0.294 19L5 0.333 0.287 13.8L6 0.549 0.475 13.5L7 0.546 0.473 13.4S?
0.396 0.296 25.3S?n 0.451 0.356 21.1Table 4: |r| between the pronunciation features and hu-man holistic scores under two forced alignment inputconditions (using transcriptions vs. using recognition hy-potheses) and relative |r| reductionS?n) show promising correlations to human holisticscores.
To check the quality of the newly developedpronunciation features, we compared these four fea-tures with the amscore feature used in (Zechner etal., 2007) on the TPO data set collected in 2006(with 1, 257 responses).
We first ran speech recog-nition using the recognizer designed for non-nativespeech.
The recognition results were used to com-pute the amscore, which is calculated by dividingthe likelihood over an entire response by the numberof letters.
Then, we used the recognition hypothe-ses to do the forced alignment using the other AMtrained on the native and near-native speech to ex-tract those four pronunciation features.
Finally, wecalculated the correlation coefficients between fea-tures and the human holistic scores.
The results arereported in Table 5.feature |r| to human holistic scoresamscore 0.434L3 0.369L6 0.444L7 0.443S?n 0.363Table 5: A comparison of new pronunciation features toamscore, the one used in SpeechRaterTMCompared to the feature amscore, L6 and L7have slightly higher |r|s with the human holisticscores.
This suggests that our construct-driven ap-proach yields pronunciation features that are empiri-cally comparable or even better than the amscore.
Inaddition, S?n, a new feature representing the vowelproduction aspect of pronunciation, shows a rela-tively high correlation with human holistic scores.This suggests that our new pronunciation feature sethas an expanded coverage of pronunciation.It is interesting to note that L3 has a lower |r|withhuman holistic scores than the amscore does.
Al-though the computation of L3 is quite similar to thatof amscore, the major difference is that likelihoodsof non-word portions (such as silences and fillers)are used to compute amscore but not L3.
This sug-gests that likelihood-related pronunciation featuresthat involve information related to non-words mayperform better in predicting human holistic scores.For example, for amscore, the likelihoods measuredon those non-word units were involved in the featurecalculation; for L6 and L7, the temporal informationof those non-word units (e.g., duration of units) wasinvolved in the feature calculation 1.5.3 Result for Q3In the feature extraction, we used the words hy-pothesized by the speech recognizer as the input forthe forced alignment.
Since a considerable num-ber of words are recognized incorrectly (especiallyfor non-native spontaneous speech), a natural wayto further improve the likelihood related features isto only consider words which are correctly recog-nized.
A useful metric associated with the recog-nition performance is the confidence score (CS) out-put by the recognizer, which reflects the recognizer?sestimation about the probability that a hypothesizedword is correctly recognized.
The recognized wordswith high confidence scores tend to be correctly rec-ognized.
Therefore, focusing on words recognizedwith high confidence scores may reduce the negativeimpact caused by recognition errors on the quality ofthe likelihood related features.On the TPO data with human transcripts, we usedthe NIST?s sclite scoring tool (Fiscus, 2009) to mea-sure the percentage of correct words (correct%),which is defined as the ratio of the number of words1L6 and L7 use R, which is computed as mTs , where Ts con-tains durations of non-words.447correctly recognized given the number of words inthe reference transcript.
On all words (correspond-ing to confidence scores ranging from 0.0 to 1.0), thecorrect% is 53.3%.
Figure 2 depicts the correct%corresponding to ten confidence score bins rangingfrom 0.0 to 1.0.
Clearly, with the increase of the con-fidence score, more words tend to be accurately rec-ognized.
Therefore, it is reasonable to only use like-lihoods estimated on the hypothesized words withhigh confidence scores for extracting likelihood re-lated features.0102030405060  00.20.40.60.81Correct% of words hypothesizedConfidence score (CS) binFigure 2: Correct% of words recognized across 10 confi-dence score binsOn the TPO data set collected in 2006, we com-puted three likelihood related features (including L3,L6, and L7) only on words whose SC is equal toor higher than a threshold (i.e., 0.5, 0.6, 0.7, 0.8,and 0.9) and measured the |r| of a feature with thehuman holistic scores.
Table 6 lists the confidencescore cutting thresholds, the percentage of wordswhose confidence scores are not lower than the cut-ting threshold selected, and |r| between each like-lihood feature to human holistic scores.
In the Ta-ble 6, we observe that only using words recognizedwith high confidence improves the correlations be-tween the features and the human holistic scores.One issue about only using words recognized withhigh confidence scores is that the number of wordsused in the feature extraction has been reduced andmay reduce the robustness of the feature calculation.Tc percentageof wordswhose CS?
Tc (%)L3|r|L6|r|L7|r|0.0 100 0.369 0.444 0.4430.5 84.21 0.38 0.462 0.4610.6 77.07 0.377 0.465 0.4640.7 69.31 0.363 0.461 0.4610.8 60.86 0.371 0.466 0.4660.9 50.76 0.426 0.477 0.475Table 6: |r| between L3, L6, and L7 and human holisticscores using only words recognized whose CSs are notlower than a threshold (Tc)6 DiscussionTo assess the pronunciation of spontaneous speech,we proposed a method for extracting a set of pro-nunciation features.
The method consists of twostages: (1) recognizing speech using an AM well fit-ted to non-native speech properties and (2) forced-aligning the hypothesized words using the otherAM, which was trained on native and near-nativespeech, and extracting features related to spectralproperties (HMM likelihood) and vowel production.This method of using one AM optimized for speechrecognition and another AM optimized for pronun-ciation evaluation is well motivated theoretically.The derived pronunciation features have also beenfound to have reasonably high correlations with hu-man holistic scores.
The results support the link-age of the features to the construct of pronunciationand their utility of being used in a scoring model topredict human holistic judgments.
Several contribu-tions of this paper are described as below.First, the two-stage method allows us to utilizean AM trained on native and near-native speech asa reference model when computing pronunciationfeatures.
The decision to include high-scored non-native speech was driven by the scoring rubrics de-rived from the construct, where the pronunciationquality of the highest level performance does notnecessarily require native-like accent, but highly in-telligible speech.
The way the reference model wastrained is consistent with the scoring rubrics, andmakes it an appropriate standard based on which thepronunciation quality of non-native speech can be448evaluated.
By using the recognition hypotheses fromthe recognition step as input in the forced alignmentstep, our experiments show a relatively small reduc-tion in correlations with human holistic scores incomparison to the features based on the human tran-scriptions.
This suggests that our method has po-tential to be implemented in a real-time operationalsetting.Second, a few decisions we have made in com-puting the pronunciation features are driven byconsiderations of how these features are meaning-fully linked to the construct of pronunciation as-sessment.
For example, we have excluded theHMM likelihoods on non-words (such as pausesand fillers) in the computations of likelihood-relatedfeatures.
In addition, only using words recognizedwith high confidence scores yields more informativelikelihood-related features for assessing the qualityof speech.
The inclusion of vowel duration measuresin the feature set expanded the coverage of the qual-ity of pronunciation.7 Summary and future workThis paper presents a method for computing featuresfor assessing the pronunciation quality of non-nativespontaneous speech, guided by construct considera-tions.
We were able to show that using a two-stagemethod of first recognizing speech with a non-nativeAM and then forced aligning of the hypothesis usinga native or near-native speech AM we can generatepronunciation features with promising correlationswith holistic scores assigned by human raters.We plan to continue our research in the follow-ing directions: (1) we will improve the native speechnorms for vowel durations, such as using the distri-bution of vowel durations rather than just the meanof durations in our feature computations; (2) wewill investigate other aspects of pronunciation, e.g.,consonant quality and word stress; (3) we will addother standard varieties of English (such as British,Canadian, Australian, etc) to the training corpus forthe reference pronunciation model as the currentmodel is trained on primarily North American En-glish (NaE).ReferencesJ.
Bernstein.
1999.
PhonePass testing: Structure andconstruct.
Technical report, Ordinate Corporation.C.
Cucchiarini, H. Strik, and L. Boves.
1997a.
Au-tomatic evaluation of Dutch Pronunciation by us-ing Speech Recognition Technology.
In IEEE Auto-matic Speech Recognition and Understanding Work-shop (ASRU), Santa Barbara, CA.C.
Cucchiarini, H. Strik, and L. Boves.
1997b.
Us-ing Speech Recognition Technology to Assess ForeignSpeakers?
Pronunciation of Dutch.
In 3rd interna-tional symosium on the acquision of second languagespeech, Klagenfurt, Austria.J.
Fiscus.
2009.
Speech Recognition Scoring Toolkit(SCTK) Version 2.3.10.H.
Franco, V. Abrash, K. Precoda, H. Bratt, R. Rao, andJ.
Butzberger.
2000.
The SRI EduSpeak system:Recognition and pronunciation scoring for languagelearning.
In InSTiLL (Intelligent Speech Technologyin Language Learning), Dundee, Stotland.D.
Graff, J. Garofolo, J. Fiscus, W. Fisher, and D. Pallett.1997.
1996 English Broadcast News Speech (HUB4).C.
Hacker, T. Cincarek, R. Grubn, S. Steidl, E. Noth, andH.
Niemann.
2005.
Pronunciation Feature Extraction.In Proceedings of DAGM 2005.N.
Moustroufas and V. Digalakis.
2007.
Automaticpronunciation evaluation of foreign speakers usingunknown text.
Computer Speech and Language,21(6):219?230.L.
Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.2000.
Automatic Scoring of Pronunciation Quality.Speech Communication, 6.S.
M. Witt.
1999.
Use of Speech Recognition inComputer-assisted Language Learning.
Ph.D. thesis,University of Cambridge.K.
Zechner and I. Bejar.
2006.
Towards Automatic Scor-ing of Non-Native Spontaneous Speech.
In NAACL-HLT, NewYork NY.K.
Zechner, D. Higgins, and Xiaoming Xi.
2007.SpeechRater: A Construct-Driven Approach to Scor-ing Spontaneous Non-Native Speech.
In Proc.
SLaTE.449
