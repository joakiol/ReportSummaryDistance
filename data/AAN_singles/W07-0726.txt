Proceedings of the Second Workshop on Statistical Machine Translation, pages 193?196,Prague, June 2007. c?2007 Association for Computational LinguisticsMulti-Engine Machine Translationwith an Open-Source Decoder for Statistical Machine TranslationYu Chen1, Andreas Eisele1,2, Christian Federmann2,Eva Hasler3, Michael Jellinghaus1, Silke Theison1(authors listed in alphabetical order)1: Saarland University, Saarbru?cken, Germany2: DFKI GmbH, Saarbru?cken, Germany3: University of Cologne, GermanyAbstractWe describe an architecture that allowsto combine statistical machine translation(SMT) with rule-based machine translation(RBMT) in a multi-engine setup.
We use avariant of standard SMT technology to aligntranslations from one or more RBMT sys-tems with the source text.
We incorporatephrases extracted from these alignments intothe phrase table of the SMT system and usethe open-source decoder Moses to find goodcombinations of phrases from SMT trainingdata with the phrases derived from RBMT.First experiments based on this hybrid archi-tecture achieve promising results.1 IntroductionRecent work on statistical machine translation hasled to significant progress in coverage and quality oftranslation technology, but so far, most of this workfocuses on translation into English, where relativelysimple morphological structure and abundance ofmonolingual training data helped to compensate forthe relative lack of linguistic sophistication of theunderlying models.
As SMT systems are trained onmassive amounts of data, they are typically quitegood at capturing implicit knowledge contained inco-occurrence statistics, which can serve as a shal-low replacement for the world knowledge that wouldbe required for the resolution of ambiguities and theinsertion of information that happens to be missingin the source text but is required to generate well-formed text in the target language.Already before, decades of work went into the im-plementation of MT systems (typically rule-based)for frequently used language pairs1, and these sys-tems quite often contain a wealth of linguisticknowledge about the languages involved, such asfairly complete mechanisms for morphological andsyntactic analysis and generation, as well as a largenumber of bilingual lexical entries spanning manyapplication domains.It is an interesting challenge to combine the differ-ent types of knowledge into integrated systems thatcould then exploit both explicit linguistic knowledgecontained in the rules of one or several conventionalMT system(s) and implicit knowledge that can beextracted from large amounts of text.The recently started EuroMatrix2 project will ex-plore this integration of rule-based and statisticalknowledge sources, and one of the approaches tobe investigated is the combination of existing rule-based MT systems into a multi-engine architecture.The work described in this paper is one of thefirst incarnations of such a multi-engine architec-ture within the project, and a careful analysis of theresults will guide us in the choice of further stepswithin the project.2 Architectures for multi-engine MTCombinations of MT systems into multi-engine ar-chitectures have a long tradition, starting perhapswith (Frederking and Nirenburg, 1994).
Multi-engine systems can be roughly divided into simple1See (Hutchins et al, 2006) for a list of commercial MTsystems2See http://www.euromatrix.net193Figure 1: Architecture for multi-engine MT drivenby a SMT decoderarchitectures that try to select the best output from anumber of systems, but leave the individual hypothe-ses as is (Tidhar and Ku?ssner, 2000; Akiba et al,2001; Callison-Burch and Flournoy, 2001; Akiba etal., 2002; Nomoto, 2004; Eisele, 2005) and more so-phisticated setups that try to recombine the best partsfrom multiple hypotheses into a new utterance thatcan be better than the best of the given candidates,as described in (Rayner and Carter, 1997; Hogan andFrederking, 1998; Bangalore et al, 2001; Jayaramanand Lavie, 2005; Matusov et al, 2006; Rosti et al,2007).Recombining multiple MT results requires find-ing the correspondences between alternative render-ings of a source-language expression proposed bydifferent MT systems.
This is generally not straight-forward, as different word order and errors in theoutput can make it hard to identify the alignment.Still, we assume that a good way to combine the var-ious MT outcomes will need to involve word align-ment between the MT output and the given sourcetext, and hence a specialized module for word align-ment is a central component of our setup.Additionally, a recombination system needs a wayto pick the best combination of alternative buildingblocks; and when judging the quality of a particu-lar configuration, both the plausibility of the build-ing blocks as such and their relation to the contextneed to be taken into account.
The required opti-mization process is very similar to the search in aSMT decoder that looks for naturally sounding com-binations of highly probable partial translations.
In-stead of implementing a special-purpose search pro-cedure from scratch, we transform the informationcontained in the MT output into a form that is suit-able as input for an existing SMT decoder.
This hasthe additional advantage that resources used in stan-dard phrase-based SMT can be flexibly combinedwith the material extracted from the rule-based MTresults; the optimal combination can essentially bereduced to the task of finding good relative weightsfor the various phrase table entries.A sketch of the overall architecture is given inFig.
1, where the blue (light) parts represent themodules and data sets used in purely statistical MT,and the red (dark) parts are the additional modulesand data sets derived from the rule-based engines.
Itshould be noted that this is by far not the only wayto combine systems.
In particular, as this proposedsetup gives the last word to the SMT decoder, werisk that linguistically well-formed constructs fromone of the rule-based engines will be deteriorated inthe final decoding step.
Alternative architectures areunder exploration and will be described elsewhere.3 MT systems and other knowledgesourcesFor the experiments, we used a set of six rule-basedMT engines that are partly available via web inter-faces and partly installed locally.
The web basedsystems are provided by Google (based on Systranfor the relevant language pairs), SDL, and ProMTwhich all deliver significantly different output.
Lo-cally installed systems are OpenLogos, Lucy (a re-cent offspring of METAL), and translate pro by lin-genio (only for German?
English).
In addition tothese engines, we also used the scripts included inthe Moses toolkit (Koehn et al, 2006)3 to generatephrase tables from the training data.
We enhancedthe phrase tables with information on whether agiven pair of phrases can also be derived via a third,intermediate language.
We assume that this can beuseful to distinguish different degrees of reliability,but due to lack of time for fine-tuning we could notyet show that it indeed helps in increasing the overallquality of the output.3see http://www.statmt.org/moses/1944 Implementation Details4.1 Alignment of MT outputThe input text and the output text of the MT systemswas aligned by means of GIZA++ (Och and Ney,2003), a tool with which statistical models for align-ment of parallel texts can be trained.
Since trainingnew models on merely short texts does not yield veryaccurate results, we applied a method where text canbe aligned based on existing models that have beentrained on the Europarl Corpus (Koehn, 2005) be-forehand.
This was achieved by using a modifiedversion of GIZA++ that is able to load given mod-els.The modified version of GIZA++ is embeddedinto a client-server setup.
The user can send twocorresponding files to the server, and specify twomodels for both translation directions from whichalignments should be generated.
After generatingalignments in both directions (by running GIZA++twice), the system also delivers a combination ofthese alignments which then serves as input to thefollowing steps described below.4.2 Phrase tables from MT outputWe then concatenated the phrase tables from theSMT baseline system and the phrase tables obtainedfrom the rule-based MT systems and augmentedthem by additional columns, one for each systemused.
With this additional information it is clearwhich of the MT systems a phrase pair stems from,enabling us to assign relative weights to the con-tributions of the different systems.
The optimalweights for the different columns can then be as-signed with the help of minimum error rate training(Och, 2003).5 ResultsWe compared the hybrid system to a purely statis-tical baseline system as well as two rule-based sys-tems.
The only differences between the baseline sys-tem and our hybrid system are the phrase table ?
thehybrid system includes more lexical entries than thebaseline ?
and the weights obtained from minimumerror rate training.For a statistical system, lexical coverage becomesan obstacle ?
especially when the bilingual lexicalentries are trained on documents from different do-mains.
However, due to the distinct mechanismsused to generate these entries, rule-based systemsand statistical systems usually differ in coverage.Our system managed to utilize lexical entries fromvarious sources by integrating the phrase tables de-rived from rule-based systems into the phrase tabletrained on a large parallel corpus.
Table 1 showsSystems Token #Ref.
2091 (4.21%)R-I 3886 (7.02%)R-II 3508 (6.30%)SMT 3976 (7.91%)Hybrid 2425 (5.59%)Table 1: Untranslated tokens (excl.
numbers andpunctuations) in output for news commentary task(de-en) from different systemsa rough estimation of the number of untranslatedwords in the respective output of different systems.The estimation was done by counting ?words?
(i.e.tokens excluding numbers and punctuations) that ap-pear in both the source document and the outputs.Note that, as we are investigating translations fromGerman to English, where the languages share a lotof vocabulary, e.g.
named entities such as ?USA?,there are around 4.21% of words that should stay thesame throughout the translation process.
In the hy-brid system, 5.59% of the words remain unchanged,which is is the lowest percentage among all systems.Our baseline system (SMT in Table 1), not compris-ing additional phrase tables, was the one to producethe highest number of such untranslated words.Baseline Hybridtest 18.07 21.39nc-test 21.17 22.86Table 2: Performance comparison (BLEU scores)between baseline and hybrid systems, on in-domain(test) and out-of-domain (nc-test) test dataHigher lexical coverage leads to better perfor-mance as can be seen in Table 2, which comparesBLEU scores of the baseline and hybrid systems,both measured on in-domain and out-of-domain testdata.
Due to time constraints these numbers reflect195results from using a single RBMT system (Lucy);using more systems would potentially further im-prove results.6 OutlookDue to lack of time for fine-tuning the parametersand technical difficulties in the last days before de-livery, the results submitted for the shared task donot yet show the full potential of our architecture.The architecture described here places a strongemphasis on the statistical models and can be seenas a variant of SMT where lexical information fromrule-based engines is used to increase lexical cover-age.
We are currently also exploring setups wherestatistical alignments are fed into a rule-based sys-tem, which has the advantage that well-formed syn-tactic structures generated via linguistic rules can-not be broken apart by the SMT components.
Butas rule-based systems typically lack mechanisms forruling out implausible results, they cannot easilycope with errors that creep into the lexicon due tomisalignments and similar problems.7 AcknowledgementsThis research has been supported by the EuropeanCommission in the FP6-IST project EuroMatrix.
Wealso want to thank Teresa Herrmann for helping uswith the Lucy system.ReferencesYasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.2001.
Using multiple edit distances to automaticallyrank machine translation output.
In Proceedings of MTSummit VIII, Santiago de Compostela, Spain.Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.2002.
Using language and translation models to selectthe best among outputs from multiple mt systems.
InCOLING.Srinivas Bangalore, German Bordel, and Giuseppe Ric-cardi.
2001.
Computing consensus translation frommultiple machine translation systems.
In ASRU, Italy.Chris Callison-Burch and Raymond S. Flournoy.
2001.A program for automatically selecting the best outputfrom multiple machine translation engines.
In Proc.
ofMT Summit VIII, Santiago de Compostela, Spain.Andreas Eisele.
2005.
First steps towards multi-enginemachine translation.
In Proceedings of the ACL Work-shop on Building and Using Parallel Texts, June.Robert E. Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In ANLP, pages 95?100.Christopher Hogan and Robert E. Frederking.
1998.
Anevaluation of the multi-engine MT architecture.
InProceedings of AMTA, pages 113?123.John Hutchins, Walter Hartmann, and Etsuo Ito.
2006.IAMT compendium of translation software.
TwelfthEdition, January.Shyamsundar Jayaraman and Alon Lavie.
2005.
Multi-engine machine translation guided by explicit wordmatching.
In Proc.
of EAMT, Budapest, Hungary.P.
Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bo-jar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang,R.
Zens, A. Constantin, C. C. Moran, and E. Herbst.2006.
Open source toolkit for statistical machine trans-lation: Factored translation models and confusion net-work decoding.
Final Report of the 2006 JHU SummerWorkshop.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In Proceedings of the MTSummit.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In In Proc.
EACL, pages 33?40.Tadashi Nomoto.
2004.
Multi-engine machine translationwith voted language model.
In Proc.
of ACL.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, March.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of ACL,Sapporo, Japan, July.Manny Rayner and David M. Carter.
1997.
Hybrid lan-guage processing in the spoken language translator.
InProc.
ICASSP ?97, pages 107?110, Munich, Germany.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.2007.
Combining translations from multiple machinetranslation systems.
In Proceedings of the Conferenceon Human Language Technology and North Americanchapter of the Association for Computational Linguis-tics Annual Meeting (HLT-NAACL?2007), pages 228?235, Rochester, NY, April 22-27.Dan Tidhar and Uwe Ku?ssner.
2000.
Learning to select agood translation.
In COLING, pages 843?849.196
