Social Goals in Conversational CooperationGuido  Boe l la ,  Rossana  Damiano  and  Leonardo  LesmoDipartimento dii informatica nd Centro di Scienza CognitivaUniversita' di TorinoCso Svizzera 185 10149 Torino, ITALYemail: {guido,rossana,lesmo}@di.unito.itAbst ractWe propose a model where dialogobligations arise from the interplayof social goals and intentions of theparticipants: when an agent is ad-dressed with a request, t:he agent'sdecision to commit o the requester'slinguistic and domain goals is mo-tivated by a trade-off between thepreference for preventing a negativereaction of the requester and thecost of the actions needed to satisfythe goals.1 In t roduct ionAs noticed by (Airenti et al, 1993), a dialog.participant, even when he does not commit othe domain goals of his partner (i.e., he does-n't cooperate behaviorally), typically contin-ues to cooperate at the conversational level.\[1\] A: Do you have Marlboros?B: Uh, no.
We ran out 1\[2\] A: Can you tell me the time??
B :  No.
My watch is broken 2\[3\] A: Could you give me some moneyfor the booze?B: I won't giv e you a dimeWhat leads people to exhibit these forms ofcooperation?
(Traum and Allen, 1994) havechallenged intention-based approaches to di-alog modeling ((Cohen and Levesque, 1990),(Lambert and Carberry, 1991), (Airenti et al,1993)) arguing that, in non-cooperative s t-tings (i.e., when the participants do not have1 (Merrit, 1976)~(Green and Carberry, 1999)shared goals), intention-based approachesleave unexplained why a participant shouldbother to be cooperative, both at the con-versational and at the behavioral level.
Inorder to overcome these difficulties, (Traumand Allen, 1994) claim that speech actspose obligations on the hearer: obligationsare pro-attitudes which provide the hearerwith a motivation to act, even if he is not- strictly speaking - cooperating with thespeaker.
Elaborating this proposal, (Poesioand Traum, 1998) propose to add obligationsto the illocutive effect of speech acts: for in-stance, a (successful) question would pose onthe addressee the obligation to answer; and,in general, a speech act poses the obligationto ground it.While we agree with (Traum and Allen, 1994)that cooperation between agents who are notpart of a group has to be explained by somemechanism which obliges an agent to answer- at least for refusing explicitly - we want togo deeper inside the notion of obligation andtry to show that it is strictly related to thatof intention.In order to explain obligations, we re-sort to the notion of social goals, startingfrom (Goffman, 1981)'s sociolinguistic anal-ysis of interactions.
We argue that, in non-cooperative situations, social goals provideagents with the motivation for committing toother agents' communicated goals.
As shownby (Brown and Levinson, 1987), an agent hasthe social goal of taking into account he faceof other people (and his own as well); thisconcern generates complementary needs forthe requester and for the requestee.
Fromthe requester's point of view, it results in84the production of complex linguistic formsaimed at reducing the potential offence in-trinsic to a demand to act (conversationallyor behaviorally); from the requestee's pointof view, while acceptance normally addressesthe requester's potential offence by a display-ing of good-tempered feelings, any refusal atthe conversational or behavioral level consti-tutes in turn a potential offence to the reques-tee's face, and sets up the social need for therefusing agent to act in order to nullify thispotential offence (Goffman, 1981).Differently from obligations, ocial goals in-fluence actions in an indirect way: in orderto evaluate the effects of an action on his in-terlocutor, an agent has to make a tentativeprediction of his reaction (anticipatory coordi-nation) (Castelfranchi, 1998).
This predictionallows the agent o keep the partner's possiblereaction into account when planning his next(domain or linguistic) action.
Social goals in-tervene as preferences during the action se-lection phase, by leading the planning agentto choose the actions which minimize the of-fence to the partner and address the potentialoffence conveyed by a refusal.2 The  In teract iona l  F ramework2.1 Goals and PreferencesWe assume that every agent A has a set ofgoals G, and a set of preferences P towardsstates of affairs.
Besides, an agent has at hisdisposal a set of action operators (recipes forachieving domain and linguistic goals, corre-sponding to behavioral nd conversational co-operation) organized in a hierarchical way.The preferences of an agent are expressedas functions which map states, represented assets of attribute-value pairs, to real numbers;an overall utility function, which consists ofthe weighted sum of the individual functions,expresses the utility of reaching the state de-picted by a certain configuration ofattributes,according to the results of the multi-attributeutility theory (Haddawy and Hanks, 1998).Goals provide the input to the planning pro-cess; in addition, they can appear in the pref-erences of the agent, i.e., they can be relatedto a utility function which evaluates the ex-pected utility of achieving them 3.
On the ba-sis of his goals and of the recipes he knows,an agent builds a set of plans, by selecting therecipes which have among their effects one (ormore) of the goals in the set.
4The planner we use is a modification of theDRIPS decision-theoretic h erarchical planner(Haddawy and Hanks, 1998).
The planningprocess tarts by applying to the current stateall selected recipes and recursively expandsthe partial plans until the appropriate l velof detail is reached.
When the planning algo-rithm concludes the refinement of the inputrecipes, it returns the preferred plan, i. e.,the one with the highest expected utility: theagent becomes committed to that plan, whichconstitutes his current intention.
The use ofpreferences allows a plan to be ewluated notonly with respect o the fact that it achievesthe goal it has been built for, but also withrespect o its side effects (for instance, con-suming less resources).2.2 Ant ic ipatory  Coord inat ion andAdopt ionThe planning situation depicted abovebecomes more complex when two or moreagents interact.
In particular, a goal of agentA may become known to agent B; a specialoccurrence of this situation arises when Ahas explicitly asked B for help.
If this is thecase, it is possible that agent B comes tochoose a plan to satisfy this goal, even if itdoes not yield any direct utility to him.Notice that if an agent evaluated the utilityof a plan for achieving a goal that has beenrequested by another agent only on the basisof its immediate outcome, he would neverchoose that plan in a non-cooperative s tting:performing an action for achieving anotheragent's goal often results only in a negativeutility, since the side effects of the actionZNot all goals are among the preferred states of af-fairs, since there are instrurnentalgoals which arise asa consequence of the intention to achieve some higher-level goal.4In the planning process, we distinguish betweenprimary effects, which are the goals that led to theselection of a given recipe, and side e_ffects, i.e., allother effects of the recipe.85cannot but affect resources and time.
Thereason why B m:topts a partner's goal is thefact that the satisfaction of an adopted goalcan have an indirect utility for B in the lightof A's reaction.
Here, the ability of an agentto predict the potential reactions of anotheragent is exploited to decide whether it isworth for him to commit to the satisfactionof the other agent's goal.In order to evaluate how the partner'sreaction affects his own preferences, likenot offending the partner and other socialgoals, an agent evaluates the utility of a planby considering the world states resultingfrom the other partner's reaction (one-levellookahead), both in case he has commit-ted to the partner's goals, and in case hehas decided that they are not worth pursuing.The DRIPS planner has been modified toimplement the following process of intentionformation in interactions with other agents(see figure 1):1. adoption: if A communicates to B a goalgA which he wants B to achieve, then the cur-rent set of B's goals, GB, becomes G~, theunion of {gA} and GB.2.
planning: B builds the set of plans PBwhich aim at achieving (all or some of) thegoals in G~ (in this way the plans achiev-ing also gA are compared with those whichdo not).3. anticipatory coordination: from the stateresulting from each plan pi in PB, B considersthe possible reaction of A: the world stateresulting from the reaction becomes the newoutcome of p~.4.
preference-driven choice: B chooses the Piin PB whose outcome maximizes his utility.For a detailed description of the planningalgorithm with anticipatory coordination, see(Boella, 2000).In the following Section, we will show how so-cial obligations arise spontaneously in a modelof conversational interaction which exploitsthe planning framework described above.3 Social  Goa ls  and  Conversat iona lCooperat ion3.1 Social GoalsIn this section, we exploit the framework de-scribed above to model the complex dynamicsof goals and social preferences that underliesexamples like \[1\].
In particular, we considerthe possibility that the partner is offended bythe agent's response to a request.
The offenceis not modeled as a direct effect of an actionof the agent.
Instead, during the planningphase, the agent makes a tentative predictionof the partner's attitude in the state wherehe is faced with a refusal, in order to evaluatehow this state complies with his preferencefor not offending: the partner is offended as aresult of his reaction to the agent's refusal.In our model, the preference for not offend-ing the partner corresponds to a social goal ~of an agent: this preference doesn't constitutean input to planning, but, by being embodiedin the utility function of the agent, it con-tributes to plan selection, by promoting theplans which do not have offending as a conse-quence.
This is in line with (Goffman, 1967)'sclaim that "Ordinarily, maintenance of face isa condition of interaction, not its objective"(p.12).Some authors ((Schegloff and Sacks, 1973),(Coulthard, 1977)) have characterized the or-ganization of conversation i terms of proto-typical pairs, adjacency pairs.
In our model,the existence of adjacency pairs is not mo-tivated by the action of specific groundingnorms, or obligations.
6 Rather, these ex-changes are explained by the interplay of thecommunicative intentions of the participants,and by their ability to recognize the intentionsof the interlocutors (Ardissono et al, 2000).Sin (Clark, 1996)'s terminology, goals like beingpolite are called interpersonal goals.o "Given a speaker's need to know whether his mes-sage has been received, and if so, whether or not hasbeen passably understood, and given a recipient's needto show that he has received the message and correctly- given these very fundamental requirements of talkas a communication system - we have the essential ra-tionale for the existence ofadjacency pairs, that is,for the organization f talk into two-part exchanges"((Goffman, 1981), p. 12).864. preference-driven choice; .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.?
=========================== _lf .
.
.
.
.
.
.
.
.
.
.
.
~ 3. anticipatory ~ J ~?~-~t~)~ 2. planning coordinta#oni , ~ Z ~ - .
A'S REACTION8', PuN2 .
.
.
.
.Figure 1: The intention formation process in interactionsIn general, the preference for not offendingwhich encompasses conversational phenom-ena like request-response pairs, is motivatedby the requestee's goal of displaying a good-tempered acceptance of the request itself: in(Goffman, 1981)'s terms, communicative ex-changes are subject to a set of "constraintsregarding how each individual ought to han-dle himself with respect to each others, so thathe not discredit his own tacit claim to good-character or the tacit claim of the others thatthey are persons of social worth (...)" (p. 16).Within an interaction, agents are aware ofthe fact that their actions have social effects,like conveying some information about theircharacter and about their attitude towardsthe partner: "An act is taken to carry im-plications regarding the character of the ac-tor and his evaluation of his listeners, as wellas reflecting on the relationship between himand them" ((Goffman, 1981), p. 21).
As aconsequence, agents are very cautious in theuse of the expressive means they have at dis-posal, namely verbal actions: besides moni-toring the partner's reactions, they try to an-ticipate them with the aim of not offendingthe partner.The preference for not offending holds aswell in the circumstances where an agent isforced to refuse his cooperation by the impos-sibility of executing the appropriate action toachieve the partner's goal.
However, if this isthe case, the requestee has to cope with theadditional fact that a simple, negative answercan be mistakenly taken to count as a refusalto cooperate at all:\[4\] A: Have you got a cigarette?B: NoFor this reason, the refusing agent is likelyto provide the requester with an acceptablereason, i.e.
a remedy or account (Levinson,1983), when the request is to be turned down.What remains to be explained is why ree-quests at behavioral level seem to pose lessconstraints on the addressee, if comparedto requests at conversational level: providedthat the interactants don't have shared goals,it is a matter of fact that it is easier to refusea request for money (see example \[3\]) ~ thana request o tell the time (see example \[2\]).In particular, conversational goals often forcethe hearer to satisfy them: it is aggressive notto answer at all or to ignore the speaker.The reason why paying attention to people,listening and understanding are not easily re-fused is that they are low cost actions, or "freegoods" in (Goffman, 1967) terminology, so noone can refuse them without threatening thespeaker's face and offending him.
A refusal atthe conversational level - ignoring a potentialpartner and not even responding to his verbalrequest - constitutes a menace to the face ofthe requester, so it is hardly justified.7We thank the anonymous reviewers for the ob-servation that this example lends itself to a deeperanalysis, involving further social and psycological pa-rameters.
However, we will not discuss the examplehere, due to space reasons.87Up to this point no explicit obligation iscreated: the "obligation to act" depends onthe utility of the action needed to establishcooperation; if the cost of the action is low(e.g., a conversational ~=tion): the refusal toexecute it can be motivated in the requester'seyes only by a requestee's negative attitudetowards him.
So, the requester, as a result ofhis ability to infer the requestee's reasoning,will be offended by a refusal; the preferencefor not threatening the face of the partnersand preserving one's own social face normallymakes the utility of offences negative, thusleading requestees toavoiding refusals.
At thesame time, this analysis, by making explicitthe underlying motivations for the preferencefor a certain type of response, accounts for theexistence of preferred and dispreferred secondturns in adjacency pairs.3.2 Conversat iona l  Cooperat ionThe effect on the requester is evaluated bythe planning agent by means of the antici-pation of his reaction.
In general, the situa-tion a requestee is faced with is constitutedby the choice between the alternative of sat-isfying the requester's conversational or be-havioral goals and the alternative of going onwith his own activity.Consider the situation depicted in example\[1\] from B's point of view, where B is facedwith A's indirect request:\[1\] A: Do you have Marlboros?B: Uh, no.
We ran outB can attribute to A two main goals (see fig-ure 2): s1.
the behavioral goal that B sells to A apacket of cigarettes (sell): however, since Acannot ake B's cooperation for granted, thisgoal is related in turn to the goal of know-ing whether B has committed to the perlocu-tionary intention of selling to A a packet ofcigarettes (knowif-satisfy), by committing toA's goal (satisfy), and, if this is the case, tothe goal of knowing whether he has completedthe corresponding plan to sell the cigarettes8The recognition ofdomain goals depends on therecognition ofthe linguistic goals, :i.e., on the successof the linguistic actions.
(hand the cigarettes, cash, ect.)
to A (knowif-completed);2. the conversational goal of knowing if therequest has been understood by B and is nowpart of the common ground (grounded); thisgoal directly relates to the management of di-alog: if A does not believe that the illocu-tionary effect of his question holds, he shouldrepeat or reformulate the question.Note that, at both levels, subsidiary goalsarise as part of the intentional behavior of anagent: for example, after performing an ac-tion for achieving some goal, it is rational tocheck whether this action has succeeded.
9B considers if it is possible for him to com-mit to the higher-level goal (sell), to which theremaining recognized goals are subordinated:although B is inclined to satisfy this goal, Bknows that one of the preconditions for ex-ecuting the selling action (has(B, Marlboros))is not true.At this point, besides the choice of not re-sponding at all, the alternative courses ofaction available to B consist in committingto A's goal to know if B has committed tohis (unachievable) sell goal (knowif-satisfy),and the subordinate goal to know if B hascompleted the plan to achieve it (knowif-completed), or to commit o A's goal - at con-versational level - to have his illocutionary actgrounded (grounded)3 ?
The choice betweenthe alternative of not responding at all andany of the other alternatives i  accomplishedby considering the reaction of the partner to arefusal at the conversational level; this choiceis enforced by the consideration that commu-nicative actions are "free goods", so they can-not be refused without incurring in a statewhere the partner is offended.Being committed to the satisfaction of theknowif-cornpleted goal, B has to choose be-9We will not describe here how these goals are iden-tified and kept ogether in a unified structure: workslike (Ardissono et al, 2000) show how the recognitionof the intentions stemming from the problem solvingactivity can constitute the required glue1?Note that, when producing an illocutionary act tosatisfy the know-satisfied or knowifieompleted goal, Bsatisfies the grounded goal as well: by displaying thereaction to the perlocutionary effect, he uptake of theillocutionary effect is granted.88trequest(A,B,(sell(B,A,cigarettes)))thave(A, cigarettes)sell(B,A,cigarettes).tsa~sfy(B,(sen(B,A,eigarettes)))knowif-~tisfy(A,B(sell(B,A,cigarettes)))grounded(request(A,B,(sell(B,A,cigarettes))))knowif-completed(A,B(sell(B,A,cigarettes)))Figure 2: The intentions of A in example \[1\]tween different ways to communicate the im-possibility to execute the plan.
In this case,two plans can apply: the simple plan for refus-ing, or the elaborated plan for refusing whichincludes a justification for the refusal.The first plan is less expensive, by beingshorter and by not requiring a mental effort;however, it is not fully explicit about the mo-tivations of the refusal, and so it is potentiallyoffensive in the partner's evaluation (A couldthink that B didn't want to sell the cigarettesto him).
On the contrary, the second plan,though more expensive, obeys to the prefer-ence for not offending, since it protects therefusing requestee from the accusation of non-cooperativeness.The existence of complex refusal acts has beenremarked on by (Gree n and Carberry, 1999).In their mechanism for initiative in answergeneration, the ambiguity of a negative an-swer to a pre-request between a literal answerand a refusal triggers the "Excuse-Indicated"rule, which generates the appropriate xpla-nation.4 Re la ted  Work(Traum and Allen, 1994) defined a model oflinguistic interaction based on the notion ofobligation.
Obligations are pro-attitudes thatimpose less commitment than intentions (sothat they can be violated), while their socialcharacter explains why humans are solicitedto act, in both cooperative and non cooper-ative contexts.
The notion of obligation hasbeen exploited also in applied dialog systems,like (Jameson et al, 1996), where they are as-sociated to move types.While in (Traum and Allen, 1994) discourseobligations are social norms that speakershave to learn, in our model, the speakers haveto learn in what conditions humans happen tobe offended; this same knowledge xplains theuse of indirect speech acts (as in (Ardissonoet al, 1999)).Moreover, obligations eem somehow redun-dant in cooperative contexts, where intentionsare sufficient o explain grounding and otherconversational phenomena.Differently from (Traum and Allen, 1994),(Allwood, 1994) introduces, besides the obli-gations associated to the communicative acts,two additional sources of obligation which arerelated, respectively, to ethical and rationalmotivations intrinsic to social relations andto the management of communication itself.Communication management obligations giverise to the mechanisms of turn-taking, inter-action sequencing, and so on, while the eth-ical obligation are socially desirable qualitiesof the interactional behavior: there exists astrong social expectation towards them, butan agent can decide to disobey them.
(Kreutel and Matheson, 2000) claim thatthe intentional structure in uncooperative di-alogues can be determined by resorting to dis-course obligations.
In order to do so, theydefine a set of inference rules which allow toreconstruct the participants' intentions epa-rately from obligations, then show how obli-gations account for the existence of conversa-tional preferences by addressing pending in-tentions.
However, the semantic rules they89propose seem to constitute a shortcut o therecognition of the communicative intentionsof the speaker, which has been proven tobe necessary to reconstruct dialog coherence(Levinson, 1981); the resulting representa-tion, since it lacks a model of the private in-tentions of the participants inadequately ac-counts for the presence of individual inten-tions which have to be traded-off against obli-gations in situations where cooperation is notgranted.5 An  Example  S i tuat ionIn order to verify the feasibility of exploit-ing social goals for motivating cooperation,we have implemented a prototype using a de-cision theoretic planner inspired to the ap-proach of (Haddawy and Hanks, 1998).
Theplanner exploits hierarchical plans to findthe optimal sequence of actions under uncer-tainty, based on a multi-attribute utility func-tion.
Goals can be by traded off against cost(waste of resources) and against each other.Five different attributes n have been intro-duced to depict the situation in example \[2\],where B is interrupted by A while he is ex-ecuting a generic action Act; this action isaimed at reaching one of B's private goal.\[2\] A: Can you tell me the time?B: No.
My watch is brokenThe following attributes model the statesinvolved in the example situation, and appearin the effects of the participants' actions.?
time: it models time as a bounded re-source; the utility decreases as a functionof time;?
grounded: it models A's goal of knowingthat B has successfully interpreted therequest;?
res: it models the consumption of (generic)resources;?
refused: it is true if A believes that Bhas refused, without any justification, tocommit o A's communicated goal;?
offended: it models A's degree of offence;n Note that the values 0 and 1 of the attributesground and satisfied.requestrepresent the truth-valuesof the corresponding propositions.Other goals like knowing whether B hascommitted to the achievement of the goal orwhether the achievement has been successfulor higher-level domain goals are not includedin this example for space reasons.In order to model the alternatives availableto B, we have introduced the following ac-tions (see figure 3).
Effects are representedas changes in the value of attributes: for ex-ample, (time=time+2) means that after theexecution of the Notify-motivation action, thevalue of the time attribute will increase by 2.?
Action Tell-time: it represents B's cooper-ation with A at the behavioral level (Bexecutes the requested action);?
Action Ground: it has the effect that Aknows that the illocutionary effect of hisrequest has been properly recognized bythe partner (the grounded attribute is setto "true").?
Action Notify-impossible: it models B'snotification that A's goal is impossibleto achieve; it specializes into two sub-actions, Notify-motivation and Notify-simple: both actions have a cost interms of resources and time and set thegrounded attribute to true, but the sec-ond one negatively affects the refused at-tribute, meaning that A considers it as a(possible) unjustified refusal.?
Action Act: it constitutes B's current planwhen he is interrupted by A's request.
Itaffects both the grounded and the refusedattribute, by setting the latter to "false".?
Action Refuse: it represents B's act ofcommunicating to A that he will not dowhat A requested, without any justifica-tion.
Among its effects, there is the factthat B comes to know A's choice (refusedand grounded attribute are set to "true").Before B replies to A's request, thegrounded attribute is set to false and the re-fused attribute is set to true.
Note that - withthe exception of Act - all actions affect thevalue of the grounded attribute, meaning that,after performing any of them, A's request re-sults grounded anyway, since all these actionsare coherent replies.90(ac t ion  Not i fy -mot ivat ion( t ime = t ime + 3)( res  = res  - 3)(g rounded = I)( re fused  = 0))(ac t ion  Ground( t ime = t ime + i)( res = res  - 2)(g rounded = I))(ac t ion(ac t ion  Not i fy -s imp le  (ac t ion  Refuse  (ac t ion( t ime = t ime + I) ( t ime = t ime + 2)( res  = res  - I) ( res = res  - 2)(g rounded = I) (g rounded = I)( re fused  = I)) ( re fused  = I))Te l l - t ime( t ime = t ime + 1)( res  = res  - 2)(g rounded = I)( re fused  = 0))Ac t( t ime = t ime +5)( res  = res  -5)(g rounded = 0)(goa l  = 1))Figure 3: A simplified representation of some of the actions that B can execute: the actionname is followed by the list of its effects.On A's side, we have introduced the actionReact 12 (see Figure 5), that models the changeof the offended parameter depending on B'schoice.
The key parameter affecting the levelof offence is the cost 13 of the requested ac-tions: the less the cost of the requested ac-tion, the greater the offence; this follows theprinciple that low-cost actions cannot be re-fused (Goffman, 1967), and, if they are, re-questers get offended.
The lack of groundingis interpreted by A as B is not cooperatingat the conversational level: since cooperatingat the conversational level (interpreting thesentence, grounding it) has a low cost, it isoffensive not to do it.Now, let's consider in detail the current sit-uation, i.e, the one where A has just asked toB to do something while B has just performedthe first step of Act.
In order to explore thedifferent alternatives, the planner builds andevaluates ome plans.
These plans differ inthat the actions for pursuing the partner'srecognized goal can be included or omitted.From the result state of each alternative, theplanner then tries to predict he reaction of Aby simulating the execution of the React ac-tion by A (see figure 5), and commits to theplan whose resulting state after the predictedreaction yields the greater utility according toB's preferences ( ee Figure 6).As explained in Section 2.1., an agent's util-ity function is a weighted sum of individualutility functions, which represent the prefer-*2We assume that weights Wi and Wj are set, re-spectively, to 20 and 10.
*aWhere (cost(action) = (res * 2) + time).ences of the agent.
The weights associated tothe individual functions reflect the strengthof each preference, by allowing for differenttrade-offs among preferences during the pro-cess of decision making.
14In figure 4, two alternative plans are repre-sented, where the utility of B is calculatedby using the utility function in figure 6.
As-suming that the weights W1, W2, W3; andW4 are set to 10, 5, 8, and 100, respectively,B will choose the plan which includes Notify-impossible as the first step, and Act - the pros-ecution of B's previous activity - as the sec-ond step.
This solution yields in fact a higherutility than the alternative of ignoring A's re-quest at all and continuing one's own activity.A change in the weights of the utility func-tion of B affects his behavior, by determininga variation in the degree of cooperation: thestronger is the preference for not offending,the more cooperative is the agent.
For exam-ple, if the utility function of B associates agreater utility to the achievement of B's pri-vate goal (by executing Act) than to the socialpreference for not offending, B will decide todisregard A's request, both at conversationaland behavioral level, is On the contrary, if the14 As (Traum, 1999) notices with reference to socialrules, "when they directly conflict with the agent'spersonal goals, the agent may choose to violate them(and perhaps suffer the consequences of not meetingits obligations)."
In our model, this roughly amountsto associating a greater utility to the achievement ofthe agent's own goals than to the preference for notoffending.
*STipically, this is the case in specific contexts whenprivate goals of the addressee are very relevant andcontrast with the satisfaction of the requester's goal;91$1Ares = 20off = 0Bres = 50goal =0SHgrounded = 0refused = 1time : 0NOTIFY -MOTIVAT ION (B)ACT (B)S2Ares = 20off = 0Bres = 47goal = 0SHgrounded = 1refused = 0t ime = 3$2Ares = 20off = 013res = 45goal = 1SHgrounded : O:refused = 11:~me = 5ACT (B)$3Ares : 20off = 0Bres = 42goal = 1SHgrounded = 1refused = 0time = 8S3Atee  = 19off  : IBres  z 45goa l  : 1SH~ou.~Led = 0: i re fusedL = 1t ime= 6REACT (A)REACT (A)ITB = 38'7$3Are8  = 19of f  = 0Btee  = 42goa l  = 1S~g ' ~ e d  = 1ze~ed : 0t ime : 9= 448Figure 4: Two of B's alternative plans in response to A's request(ac t ion  React( t ime = t ime + i)( res  = res  - i )(o f fended = o f fended +(not (grounded) )  * Wi  +( re fused  / cos t (ac t ion) )  * W~))Figure 5: The partner's reactionUs= ( ress  * Wi)  -( t ime * W~) -(o f fended * Ws)  +(goa l  * We)Figure 6: The utilitY function of Butility function of B models a more balancedtra~:le-off between the achievement of B's pri-vate goMs and social preferences, B will de-cide to ground A's request, at least, or to befully cooperative by satisfying A's request.6 Conc lus ionsIn this paper we proposed an intention-basedapproach to dialog that aims at overcomingthe critics posed by (Traum and Allen, 1994)by assuming the existence of social goals.
Oursolution does not rest on an explicit notionfor  example ,  B cou ld  miss ing  the  t ra in .of obligation, even if some similarities can befound with (Traum and Allen, 1994).
Theadvantage of not resorting to a primitive no-tion of obligation is to have a uniform sourceof motivations for explaining the behavior ofagents.With respect to approaches which stipu-late a primitive notion of obligation, here, thesame phenomena are accounted for withoutintroducing further propositional attitudes.This explanation of the motivations leadingto cooperation provides an explicative modelthat is uniform with the treatment of deon-tic reasoning in agent theories (Conte et al,1998), (Boella and Lesmo, 2000) and the def-inition of cooperation proposed in (Boella etal., 2000).It is clear that, by reducing the numberof propositional attitudes, the reasoning pro-cess becomes more complex, but our modelis aimed at constituting an explanation, andit does not exclude the possibility of compil-ing the reasoning in more compact form: as(Brown and Levinson, 1987) notice, %here isa rational basis for conventions, too".92ReferencesG.
Airenti, B. Bara, and M. Colombetti.
1993.Conversational nd behavior games in the prag-matics of discourse.
Cognitive Science, 17:197-256.J.
Allwood.
1994.
Obligations and options in di-alogue.
Think, 3.L.
Ardissono, G. Boella, and L. Lesmo.
1999.
Therole of social goals in planning polite speechacts.
In Workshop on Attitude, Personalityand Emotions in User-Adapted Interaction atUM'99 Conference, pages 41-55, Banff.L.
Ardissono, G. Boella, and L. Lesmo.
2000.Plan based agent architecture for interpretingnatural anguage dialogue.
International Jour-nal of Human- Computer Studies, (52) :583-636.G.
Boella and L. Lesmo.
2000.
Deliberate nor-mative agents.
In Proc.
of Autonomous Agents2000 Workshop on Norms and Institutions.,Barcelona.G.
Boella, R. Damiano, and L. Lesmo.
2000.Cooperation and group utility.
In N.R.
Jen-nings and Y. Lesp~rance, editors, IntelligentAgents VI - -  Proceedings of the Sixth Interna-tional Workshop on Agent Theories, Architec-tures, and Languages (ATAL-99, Orlando FL),pages 319-333.
Springer-Verlag, Berlin.G.
Boella.
2000.
Cooperation among economi-cally rational agents.
Ph.D. thesis, Universit~di Torino, Italy.P.
Brown and S. C. Levinson.
1987.
Politeness:some universals on language usage.
CambridgeUniversity Press, Cambridge.C.
Castelfranchi.
1998.
Modeling social action forAI agents.
Artificial Intelligence, 103:157-182.H.C.
Clark.
1996.
Using Language.
CambridgeUniversity Press.P.R.
Cohen and H.J.
Levesque.
1990.
Rationalinteraction as the basis for communication.
InP.R.
Cohen, J. Morgan, and M.E.
Pollack, ed-itors, Intentions in communication, pages 221-255.
MIT Press.R.
Conte, C. Castelfranchi, and F. Dignum.1998.
Autonomous norm-acceptance.
In J. P.Mueller, M.P.
Singh, and A.S. Rao, editors, In-telligent Agents V - -  Proc.
of 5th Int.
Work-shop on Agent Theories, Architectures, andLanguages (ATAL-98).
Springer Verlag, Berlin.M.
Coulthard.
1977.
An Introduction to Dis-course Analysis.
Longman, London.E.
Goffman.
1967.
Interaction Ritual.
Penguin,Harmondsworth.E.
Goffman.
1981.
Forms of Talk.
University ofPennsylavania Press.N.
Green and S. Carberry.
1999.
Interpret-ing and generating indirect answers.
Compu-tational Linguistics, 25(3):389-435.P.
Haddawy and S. Hanks.
1998.
Utility modelsfor goal-directed, decision-theoretic planners.Computational Intelligence, 14:392-429.A.
Jameson, R. Sharer, J. Simons, and T. Weis.1996.
How to juggle discourse obligations.
InR.
Meyer-Klabunde and C. yon Stutterheim,editors, Proceedings of the Symposium 'Concep-tual and Semantic Knowledge in Language Pro-duction', pages 171-185.J.
Kreutel and C. Matheson.
2000.
Obliga-tions, intentions and the notion of conversa-tional games.
In Proc.
Gotalog, 4th Workshopon the Semantics and Pragmatics of Dialogue.L.
Lambert and S. Carberry.
1991.
A tripartiteplan-based model of dialogue.
In Proc.
29thAnnual Meeting of A CL, pages 47-54, Berkeley,CA.S.C.
Levinson.
1981.
The essential inadequaciesof speech act models of dialogue.
In M. Par-ret, M. Sbis~, and J. Verschueren, editors, Pos-sibilities and Limitations of Pragmatics, pages473-492.
Benjamins, Amsterdam.S.C.
Levinson.
1983.
Pragmatics.
CambridgeUniversity Press, Cambridge.M.
Merrit.
1976.
On questions following questions(in service encounters).
Language in Society,5(3):315-357.M.
Poesio and D. Traum.
1998.
Towards anaxiomatization of dialogue acts.
In Proc.
of13th Twente Workshop on Language Technol-ogy, pages 207-222, Enschede.E.A.
Schegloff and H. Sacks.
1973.
Opening upclosings.
Semiotica, 7:289-327.D.R.
Traum and J.F.
Allen.
1994.
Discourse obli-gations in dialogue processing.
In Proc.
32ndAnnual Meeting of A CL, pages 1-8, Las Cruces,New Mexico.D.
Traum.
1999.
Speech acts for dialogue agents.In M. Wooldridge and A. Rao, editors, Foun-dations and Theories of rational Agents, pages169-201.
Kluwer.93
