The design of a spoken language interfaceJean-Michel Lunati Alexander I. RudnickySchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213Fast and accurate speech recognition systems ystems bring withthem the possibility of designing effective voice driven applica-tions.
Efforts to this date have involved the construction ofmonolithic systems, necessitating repetition of effort as each newsystem is implemented.
In this paper, we describe an initialimplementation of a general spoken language interface, theCarnegie Mellon Spoken Language Shell (CM-SLS) whichprovides voice interface services to a variable number of applica-tions running on the same computer.
We also present a systembuilt using CM-SLS, the Office Manager, which provides the userwith voice access to facilities uch as an appointment calendar, apersonal database, and voice mail.Speech interfaces need to provide services that are par-ticular to speech, either due to the intrinsic properties ofspeech or due to the characteristics of cur~nt recognitiontechnology.
We are interested in identifying these servicesand in understanding how they should be integrated intothe computer interface.
Ultimately, our goal is to under-stand how to make speech be a conventional form of inputto a computer, well integrated into a multimodal interface.A well-designed speech interface must respect four fun-damental rules of computer-human interface design:1.
Coherence across applications.
Different applica-tions must react similarly to requests that are similarin content and to always react to certain standard in-puts (such as requests for help).
Doing so allows theuser to maintain as much as possible a single style of(spoken) interaction.2.
Conciseness inside an application.
An applicationshould allow users to express requests in simpleeconomical forms.
Providing natural languageprocessing capabilities i one aspect of this.
Anotheraspect is allowing the user to use a variety of expres-sions, including minimal telegraphic forms.3.
A meaningful and appropriate system of feedback.The user must be able to easily maintain an accuratemodel of system state.
An explicit indication that therecognizer is available is an example, as is providing aread-out of the recognition result.
The ability torespond in real-time underlies the effectiveness offeedback.4.
A natural structuring of activities.
The system shouldbe able to guide the user into acceptable modes ofinteraction or to otherwise anticipate how the user willapproach it.
Developing a language that is suited tothe task is one aspect, while incorporating clarifica-tion dialogues i  another.To facilitate the exploration of these and other issues, wehave developed a system that provides a core of spokenlanguage interface services.
In this paper, we describe thedesign of this system and provide motivation for thevarious design choices that it incorporates.Interface componentsA good interface design embodies a clear functionaldecomposition which in turn simplifies system implemen-tation and allows for independent development of differentcomponents.
The particular design we have arrived at hasso far proven to be quite useful, in that we have been ableto implement straightforwardly a number of differentrecognition systems with it, while maintaining itsmodularity.The design presented here decomposes a recognition sys-tem into what we believe are functionally independentunits, each corresponding to a necessary function in thespeech interface.
It should be noted that we have notcreated novel elements.
Each of these functions are im-plicit in all existing recognition systems but typically havenot been explicitly identified or recognized as separablecomponents of the interface.
The present decompositionprovides an explicit identification of these functions,thereby simplifying the exploration of issues that cor-respond to each component.Figure 1 shows the functional components of the spokenlanguage interface: the Attention Manager, theRecognition Engine, the Confirmation Manager and theTask Manager.
The following sections provide moredetailed escriptions of each component.The Attention Manager (AM)Humans are remazkably adept at attending to speech intheir environment.
Computer systems are remarkable inthe degree to which they lack this ability.
Spoken languagesystems need to approximate his ability in order to relieveusers of the burden of monitoring system input on theirown.
The current system isolates this function as aseparate module and permits its independent development.The signal processing component of the spoken languagesystem produces a constant stream of coded speech (in theform of vector codebook values).
The Attention Managersegments utterance-sized units from this stream and routesthese utterances to the recognizer.
The implementation fthe Attention Manager can span a range of complexity.
At225AttentionManagerRecognitionEngineConfirmation ManIger I~t  MTaan~kg e r \ [ ~rFigure 1: The Spoken language interfaceone extreme, the user explicitly controls the signal acquisi-tion process, by indicating to the system the start and theend of an utterance.
At a more complex level of function,the system determines these points on its own (throughautomatic end-point detection).
Ideally, the AttentionManager should be capable of determining whether theuser is addressing the computer (as opposed to anotheragent in the environment) using suitable cues in the speechstream or from the environment.The current version of the AM implements a selection ofattention modes, allowing individual users to select he onethey prefer or the one best suited to a particular activity.Visually, the AM offers the user a state indicator, in theform of a clickable button, and a sound level indicator, inthe form of a VU meter.
The following modes are sup-ported:* Push to Talk.
The user clicks the talk icon beforespeaking and clicks again when done speaking.
Thismode gives the user complete control over the inter-action and over delimiting an utterance.
The dis-advantages include the need to perform two separateacts for each utterance and the use of the mouse, in-creasing the user's cognitive load with actions that arenot application-related (the user may forget o chck atthe end, or even at the beginning).
* Push to Start.
The user clicks the talk icon whenready to speak, and the system decides when the ut-terance has ended, by performing endpoint detection.Only one act needs to be performed, but the user can-not include extended pauses into an utterance.
Theuser must also coordinate the click and the beginningof the utterance.?
Continuous Listening.
The system uses endpointdetection to delimit each utterance.
The advantage ishands-free operation; the disadvantage is lack of con-trol of the extent of an utterance and over the inclu-sion of extraneous speech.Recognition Engine (RE)The Recognition Engine transforms the coded utterancestream into an ASCII string corresponding to the decodingof the input.
In our present implementation, the RE func-tions as a dedicated server and allows multiple clients toshare the same recognition facilities.
Recognition imposesa high computational load and it is often impractical tohave this process reside on a computer on which severalapplicatious (themselves potentially requiring substantialresources) are active.
Ideally, the recognition engine wouldbe implemented as a specialized co-processor within thecomputer.In its current implementation, the RE maintains eparateknowledge bases for each speech application.
Control sig-nals communicated by the Attention Manager (which ob-rains this information from the Task Manager) allows theRecognition Engine to select the correct knowledge basefor each utterance.
The RE does not maintain any contextinformation of its own, treating each utterance as a separateevent.
This does not preclude, of course, the use of contex-tual constraint provided by a particular application, basedon its individual history.
In the current design, the intent isto communicate such information on an utterance by ut-terance basis.A critical attribute of a recognition engine is its ability todecode speech in real-time.
Real-time response (or ratherresponse that is within a 200-300 msec delay of the end ofan utterance) is necessary to maintain the rhythm of inter-action.
Slower response times force users into devotingresources to monitoring system availability instead of con-centrating on the task at hand \[4\].
We believe that "timesreal l ime" is no longer an appropriate metric for charac-terizing system performance.
Figure 2 displays a his-togram of "times real time" performance for a four-processor parallel implementation f the svmNx algorithm,calculated over a standard set of 150 Resource Manage-ment utterances, using a perplexity 20 grammar \[3\].
Onlythe search time component is shown.
Using the conven-tional method (calculating a mean), we might characterizesystem response as better than "real t ime", since the meansearch time is 0.68 times real time.
However, this wouldbe misleading, as the system actually responds lower thanreal time -15% of the time.
From the user's point of view,the recognizer introduces a delay at least fifteen percent ofthe time (in addition to other delays, due to signal acquisi-tion and processing).
Since delays have a clear impact onwhat users do\[1,5,6\] ,  we beheve that a "percentreal-time" measure is more relevant in characterizingspoken language system performance than a simple meanresponse time.
Our current recognizer is 85 percent real-time for the Resource Management task.226m 32"E 24"z 22"20=18=16=14-12"10"8"6"4"2"0 I i lO.O 0\[2 01.,4 0'.6 0'.8 '1.0 1.2 11.4 11.6 1.8Times real "limejibFor 4 processorsFigure 2: Histogram of system response time fora four-processor recognition system.Confwmat ion  Manager  (CM)The errorful nature of speech recognition compels the in-troduction of an additional component not typically foundin other interface technologies, the Confirmation Manager(CM).
This component allows the user to intercept or edita recognition before it is acted upon by the application.
Interms of human communication, the CM performs the errorrepair necessitated by breakdowns in the communicationcbannel (such as might be caused by a noisy telephone lineor a loud interruption).
It does not concern itself with theconsequences of errors due to some misunderstanding onthe part of the user (although it does offer an opportunityfor the immediate undoing of a just-spoken utterance).Minimally, the system can pass all utterances through with-out intervention, though at a cost in throughput \[2\].
Thesystem can also require the user to provide some ack-nowledgment (vocal or manual) of the correctness of aninput, though again at a cost.
In more complex implemen-tations, the system can allow for editing (either by voice orby keyboard) the input and for the generation of "undo"signals for the benefit of the application.
The latterfacilities are available in our current interface.A more sophisticated system would be able to, by model-ing the interaction and by integrating application-state in-formation, detect utterances that might be, with high prob-ability, incorrectly recognized and on that basis engage theuser in a clarification dialog.
Currently, we include aclarification dialog component as part of the Mapper withina particular application.
In the latter case, the dialog istriggered by inconsistencies in the results of (say) databasequeries and represents a different class of resolution.
Onthe other hand, if a system is capable of providing highrecognition accuracy, can respond rapidly (i.e., in real-time), and is essentially stateless, then simple repetitionuntil correct input is achieved can be a reasonable alter-native (see \[2\]).Task  Manager  (TM)In our previous experience, speech recognition systemshave been built as monolithic processes.
While this ap-proach is adequate for a computer that runs one or at most afew speech applications it is inefficient on a computer thatis meant to support a variety of speech-enabled applica-tions.
In this case it becomes more efficient o centralizespeech resources (all the more if they are not preemptableonce assigned to an application) and to allocate them toindividual applications.
The purpose of the Task Manageris to supervise, in the context of multiple voice-addressableapplications, the assignment of the speech channel to theproper application.
In our implementation, the actual ser-vices performed by the Task Manager also include themaintenance of context information and its communicationto the recognition engine.The Task Manager performs a control function comparableto that of the window manager in a window-oriented sys-tem.
In our design, the voice-capable computer systemactually has the possibility of two parallel input cbannels,vocal and manual.
Ideally, a single Manager would per-form this function, though in our current implementation,these are handled separately, theoretically allowing forparallel input to the computer, allowing the use to talk toone application while typing to another one.ApplicationsOur goal in providing an interface to individual applica-tions was to minimize the changes that need to be made inorder to incorporate speech into an application, while en-forcing a common approach to our system and accordinglya coherence between applications.
The diagram in Figure 3shows the components ofa voice-enabled application.Each application incorporates a frame-based parser,described elsewhere \[7\].
The frames produced by the par-ser are passed to a Mapper which translates each commandto the application into suitable method invocations (seenext section).
Two styles of interface are possible for ex-isting applications, either the Mapper can emulate an exist-ing interface, generating a stream of keyboard and mouseevents for each utterance that correspond to the equivalentinput for those modalities or it can access functions withinthe application directly.
A previous system \[5\] was im-plemented using the former strategy.
In the present case,we chose to have the Mapper access applicationfunctionality directly.
The availability of spoken languagedisposes the user to express requests in terms of goals andother abstractions, essentially what the advantage of an SLinterface should be: freeing the user from the need to ex-plicitly specify command sequences for the application andhiding this specification process within the natural lan-guage component of the system.
As a result he implemen-tation of an interface between speech and application at thismore abstract level can be done quite efficiently.In some cases, the Mapper encounters situations where auser request is either underspecified or contains am-227From Task ManagerApplicationVoiceInterfaceCommunicationParserMapperFigure 3: Components of the application inter-face.
The double flamed boxes are black boxes forthe programmer.biguifies.
Some of these situations can be dealt withthrough the invocation of mechanisms for, e.g., anaphoraresolution.
Others require the user to further specify theirintention.
To handle such cases, applications that need thisfacility can engage the user in a clarification dialog.
Cur-rently, the clarification procedure handles cases of am-biguity by informing the user of the situation then a~kingthem to interactively resolve the ambiguity (by choosingone of several alternatives).
More complex interactions arepossible within this framework, though we have not as yethad the need to consider them.
We anticipate the pos-sibility that clarification may need to be provided as anindependent service and not embedded in each individualapplication.Some implementation otesThe Carnegie Mellon Spoken Language Shell (CM-SLS)was intentionally designed to have easily modifiable com-ponents and to allow the incorporation of different applica-tions with minimum difficulty.
Some of the features in ourimplementation that make this possible include the follow-ing:?
The CM-SLS is implemented within an object-oriented paradigm that encourages modularity as wellas code reusability, thus making it easy to add newvoice driven applications into the system.?
The CM-SLS incorporates a fast speaker-independentcontinuous-speech multiple knowledge-base r cog-nition system.
General English models are used tospeed up task development, avoiding task-dependenttraining.perplexityapplication vocabulary (estimated)OMCalendarPIDVoice MailCalculatortotal36157367 (111+43)246 (111+43)588641554584454Table 1: OM task chaxacteristics a  of June 1990,PID and Voice Mail have 111 names and 43 nick-names in their vocabulary.?
The CM-SLS offers a high level user interface on aNeXT Machine for efficient end-user access to theapplications.
We provide an application fi'ameworkthat provides coherence across applications, allowsconciseness inside an application, offers an ap-propriate feedback, and presents a natural structuringof activities allowing a fast and effective access toapplications for both casual and expert users.?
The CM-SLS uses external tools to quickly build newapplications.
Tools include a case frame grammarcompiler, a case flame grammar parser, and a semi-automatic speech knowledge base (used by the RE)generator.The Office Manager (OM)To demonstrate our approach to speech interface design,we have implemented the Office Manager system, asystemwhich is meant o provide the user with voice access to anumber of common computer-based office applications.The Office Management domain has several interestingproperties that make it an ideal instrument for exploringissues in spoken language system design.
The critical at-tributes of this task domain are the following:?
It provides a range of interaction requirements, fromtight-loop (e.g., calculation) to open-ended (e.g.,database retrieval).?
It focuses on a realistic ta.~k domain that supportsmeaningful problem-solving activity (e.g., scheduling,information search).?
It's a domain in which it would be reasonable to ex-pect daily interaction through spoken language.
Sincethe tasks it encompasses are performed regularly, itcreates the opportunity to study spoken language in-teraction on an ongoing basis, under natural con-ditions.The Office Manager at present includes the applicationslisted in Table 1.
In addition to the applications them-selves, the OM understands a 36 word vocabulary, which isused to execute a variety of control functions, such ascreating tasks, switching between them, invoking help, etc.The current (June 1990) implementation f the system in-228eludes a database of addresses for the 111 official par-ticipants in the October 1989 Darpa Speech and NaturalLanguage Woxkshop.
This database is used by the VoiceMail and PID (Personal Information Directory) com-ponents of OM.
Our plan is to make available additionaldatabases to users in our environment (for example, a listof department members) and to pursue the development oftools for user-customizable databases.
Customization is oftwo types: the addition or modification of entries in exist-ing databases, and the creation of new, arbitrary databasesby the user.
Both forms of customization i troduce inter-esting problems for spoken-language systems: themodification of existing recognition knowledge bases (asmight be occasioned by the introduction of a new personname), and the creation (by a presumably naive user) of anaccess language for a new database.ConclusionThis paper has described a number of innovations in thedesign of spoken language interfaces.
We have advanced aparticnlar functional decomposition for the interface andhave argued that it identifies key areas in which advancesare needed.
We have proposed what we believe to be ameaningful metric for system response characteristics.
Wehave also briefly described the Office Management task,which we believe to be particularly suited for the study ofspoken language interface issues.Our future work includes the development of techniquesfor structuring recognition and parsing knowledge basesalong "object" lines to permit individual applications toinherit language characteristics from theft environment (theOM) and to encourage the modularization and reusabilityof language components.
The goal is to simplify theprocess of creating languages for particular applications byproviding the developer not only with standard interfacecomponents but also with standard language components.Meaningful study of spoken language interaction requiresthe use of a system that will be used on a daily basis andwhose utility will persist past the initial stages of play andexploration.
We believe that he Office Manager is such asystem.
Systems that do not have this persistence ofutilitywill ultimately have little to tell us about spoken com-munication with computers.AcknowledgmentsWe would like thank a number of people who have con-tributed to the work described in this paper, includingKathryn Baker, Eric Thayer, Bob Weide, Paul Arceneaux,and Alex Flranz.The research described in this paper was sponsored by theDefense Advanced Research Projects Agency (DOD), ArpaOrder No.
5167, monitored by SPAWAR under contractN00039-85-C-0163.
The views and conclusions containedin this document are those of the authors and should not beinterpreted as representing the official policies, either ex-pressed or implied, of the Defense Advanced ResearchProjects Agency or the US Goverament.References1.
Grossberg, M. and Wiesen, R.A. and Yntema, D.B.
"An experiment on problem solving with delayed computerresponses.".
IEEE Transactions on Systems, Man andCybernetics SMC-6, 3 (March 1976), 219-222.2.
Hauptmann, A.H. and Rudnicky, A.I.
A comparison ofspeech and typed input.
In Proceedings ofthe June 1990Darpa Workshop on Speech and Natural Language, Mor-gan Kaufu~ann~ San Mateo, 1990.3.
Lunati, J.-M. A parallel implementation f FBS,Results and Comments (September 1989).4.
A.I.
Rudnicky and J. L. Quirin.
Subjective reaction tosystem response delay: A pilot study (January 1990).5.
Rudnicky, A., Sakamoto M. A. and Polifroni, J.Evaluation of spoken language interaction.
In Proceedingsof the October Darpa Speech and Natural LanguageWorkshop, Morgan Kanfmann~ San Mateo, 1989, pp.150-159.6.
Rudnicky, A.
System response delay and user strategyselection in a spreadsheet task.
CHI'90 invited poster,April, 1990.7.
Ward, W. The CMU Air Travel Information Service:Understanding spontaneous speech.
In Proceedings oftheJune 1990 Darpa Workshop on Speech and NaturalLanguage, Morgan Kauffmann, San Mateo, 1990.229
