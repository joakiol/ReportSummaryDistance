Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 107?110,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Combined word alignmentsDan Tufi?, Radu Ion, Alexandru Ceau?u, Dan ?tef?nescuRomanian Academy Institute for Artificial Intelligence13, ?13 Septembrie?, 74311, Bucharest 5, Romania{tufis, radu, alceusu, danstef}@racai.roAbstractWe briefly describe a word alignment systemthat combines two different methods in bitextcorrespondences identification.
The first one isa hypotheses testing approach (Gale andChurch, 1991; Melamed, 2001; Tufi?
2002)while the second one is closer to a modelestimating approach (Brown et al, 1993; Ochand Ney, 2000).
We show that combining thetwo aligners the results are significantlyimproved as compared to each individualaligner.IntroductionIn (Tufi?, 2002) we described a translation equivalenceextraction program called TREQ the development ofwhich was twofold motivated: to help enriching thesynsets of the Romanian wordnet (Tufi?
et al 2004a)with new literals based on bilingual corpora evidenceand to check the interlingual alignment of our wordnetagainst the Princeton Wordnet.
The translationequivalence extractor has been also incorporated into aWSD system (Tufi?
et al, 2004b) part of a semanticweb annotation platform.
It also constituted thebackbone of our TREQ-AL word aligner whichsuccessfully participated in the previous HLT-NAACL2003 Shared Task1 on word alignment for Romanian-English parallel texts.
A detailed description ofTREQ&TREQ-AL is given in (Tufi?
et al 2003b) and itwill be very shortly overviewed.A quite different approach from our hypothesestesting implemented in the TREQ-AL aligner is takenby the model-estimating aligners, most of them relyingon the IBM models (1 to 5) described in the (Brown etal.
1993) seminal paper.
The first wide-spread andpublicly available implementation of the IBM modelswas the GIZA program, which itself was part of theSMT toolkit EGYPT (Al-Onaizan et al, 1999).
GIZAhas been superseded by its recent extension GIZA++(Och and Ney, 2000, 2003) publicly available2.
We usedthe translation probabilities generated by GIZA++ forimplementing a second aligner, MEBA, described in a1 http://www.cs.unt.edu/~rada/wpt/index.html#shared2 http://www.fjoch.com/GIZA++.2003-09-30.tar.gzlittle more details in a subsequent section.
Thealignments produced by MEBA were compared to theones produced by TREQ-AL.
We used for comparisonthe Gold Standard3 annotation from the HLT-NAACL2003 Shared Task.
In order to combine the two alignerswe had to check whether their accuracy was comparableand that when they are wrong the set of mistakes madeby one aligner is not a proper set of the errors made bythe second one.
The first check was performed by usingMcNamer?s test  (Dieterich, 1998) and for the secondwe used Brill &Wu test (Brill, Wu, 1998).
Both testsconfirmed that the conditions for combining wereensured so, we built the combiner.The Combined Word Aligner, COWAL, is awrapper of the two aligners (TREQ-AL and MEBA)ensuring the pre- and post-processing.
It iscomplemented by a graphical user interface that allowsfor the visualisation of the alignments (intermediary andthe final ones) as well as for their editing.
We shouldnote that the corrections made by the user are stored byCOWAL as positive and negative examples for worddependencies (in the monolingual context) andtranslation equivalencies (in the bilingual context).
Inthe current version the editorial logs are used by thehuman developers but we plan to further extendCOWAL for automatic learning from this extremelyvaluable kind of data.The bitext processingThe two base aligners and their combination use thesame format for the input data and provide thealignments in the same format.
The input format isobtained from two raw texts which represent reciprocaltranslations.
If not already sentence aligned, the twotexts are aligned.
In the shared task this step was notnecessary since both the training data and evaluationdata were provided in the sentence aligned format.The texts in each language are then tokenized withthe MULTEXT multilingual tokenizer4.
The tokenizer isa finite state automaton using language specific3 We noticed in the Gold Standard two sentences wherealignments were wrongly shifted by one position (due to anunprintable character) and we corrected them.4 http://aune.lpl.univ-aix.fr:16080/projects/multext/MtSeg/107resources.
It recognizes several compounds (phrasalverbs, idioms, dates) and split contrasted or cliticizedconstructions.
This tokenization considerably differsfrom the one prescribed by the Shared Task where atoken is any character string delimited by a blank or apunctuation sign (which itself is considered a token).Since our processing tools (especially the tokeniser)were built with a different segmentation strategy inmind, we generated the alignments based on our owntokenization and, at the end, we ?re-tokenised?
the textaccording to original evaluation data (and consequentlyre-index) all the linking pairs.
After tokenization, bothtexts are tagged and lemmatized.
We used in-houselanguage models and lemmatizers and the Brants?s TnTtagger5.
For both English and Romanian we usedMULTEXT-EAST6 compliant tagsets.
With differenttags, a tagset mapping table becomes an obligatoryexternal resource.
Although, more often than not, thetranslation equivalents have the same part-of speech,relying on such a restriction would seriously affect thealignment recall.
However, when the translationequivalents have different parts of speech, thisdifference is not arbitrary.
During the training phase weestimated bilingual POS affinities:{p(POSmRO| POSnEN)}and {p(POSnEN|POSmRO)}.
POS affinities were used asone of the information sources in dealing withcompetitive alignments.The next preprocessing step is represented by arather primitive form of sentence chunking in bothlanguages.
They roughly correspond to (non-recursive)noun phrases, adjectival phrases, prepositional phrasesand verb complexes (analytical realization of tense,aspect mood and diathesis and phrasal verbs).
The?chunks?
are recognized by a set of regular expressionsdefined over the tagsets.
Finally, the bitext is assembledas an XML document (XCES-Align-ana format), asused in the MULTEXT-EAST corpus, which is thestandard input for most of our tools, including COWALalignment platform.The three alignersTREQ-AL generates translation equivalence hypothesesfor the pairs of words (one for each language in theparallel corpus) which have been observed occurring inaligned sentences more than expected by chance.
Thehypotheses are filtered by a loglikelihood scorethreshold.
Several heuristics (string similarity-cognates,POS affinities and alignments locality7) are used in a5 http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf6 http://nl.ijs.si/ME/V2/7 The alignments locality heuristics exploits the observationmade by several researchers that adjacent words of a text inthe source language tend to align to adjacent words in thetarget language.
A more strict alignment locality constraintcompetitive linking manner (Melamed, 2001) to makethe final decision on the most likely translationequivalents.
Given that, initially, this program wasdesigned for extracting translation equivalents for thealignment of the Romanian wordnet to the Princetonwordnet, it deals only with one to one mappings.
Tocope with the many to many mappings (especially forfunctional words alignment), the earlier version of thetranslation equivalence extractor encoded some generalrules assumed to be valid over a large set of naturallanguages such as: auxiliaries and verbal particles(infinitive, subjunctive, aspectual and temporal) arerelated to the closest main verb, determiners (articles,pronominal adjectives, quantifiers) are related to theclosest nominal category (noun or pronoun).
Currentlythis part of the TREQ-AL code became redundantbecause the chunking module mentioned before doesthe same job in a more general and flexible way.MEBA is an iterative algorithm which uses thetranslation probabilities, distorsions and POS-affinitiesgenerated by GIZA++ and takes advantage of allpreprocessing phases mentioned in the previous section.In each step are aligned different categories of tokens(content words, named entities, functional words) indecreasing order of statistical evidence.
The score of alink is computed by a linear function of 7 parameters?scores: translation probability, POS affinity, stringsimilarity, alignments locality (both strict and weakerversions) distortions and the entropy of the translationequivalents.
For all these parameters, in each processingstep, we empirically set minimal thresholds and variousweights.
The tokens considered for the computingtranslation probabilities are the lemmas trailed by thegrammatical categories (eg.
plane_N, plane_Vplane_A).
This way we aimed at avoiding datasparseness and filtering noisy data.
For highlyinflectional languages (as Romanian is) the use oflemmas instead of word occurrences contributessignificantly to the data sparseness reduction.
Forlanguages with weak inflectional character (as Englishis) the POS trailing contributes especially to the filteringthe search space.
Each processing step is controlled byabove mentioned parameters, the weights and thresholdsof which vary from step to step (even the order of theprocessing steps is one of the possible parameters).The first alignment step builds only links with ahigh level of certainty (that is cognates, pairs of hightranslation probability and high POS affinity).
Thegrammatical categories which are considered in this stepare user controlled (usually nouns, adjectives or non-auxiliary verbs and which have the fewest competitivetranslations).
The next processing steps try to alignrequires that all alignment links starting from a chunk, in theone language end in a chunk in the other language.
Thisrestricted form of locality is relevant for related languages.108content words (open class categories) as confidently aspossible, following the alignments in previous steps asanchor points.
In all steps the candidates are consideredif and only if they meet the minimal thresholdrestrictions.
If the input bitext is chunked, the strictalignment locality heuristics is very effective todetermine the correct alignment even for unseen pairs ofwords (or for which the translation equivalenceprobability is below the considered threshold).
Whenthe pre-chunking of the parallel texts is not available,MEBA uses the weaker form of the locality heuristicsby analyzing the alignments already existing in awindow of N tokens centered on the focused token.
Thewindow size is variable, proportional to the sentencelength.
For all alignments in the window, an averagedisplacement is computed and, among the competingalignments, preference will be given to the links withdisplacement values closer to the average one.The functional words and punctuation are processedin the last step and their alignments are guided by thePOS-affinities and alignment locality heuristics.
If noneof the alignment clues or their combination (Tiedemann,2003) is strong enough, the functional words areautomatically aligned with the word(s) their governor isaligned to.
The governor is chunk-based defined: it isthe content word of a chunk (if there are more contentwords in a chunk, then the governor is the grammaticalhead).
If the chunking is not available, the closestcontent word is selected as the governor.
Proximity ischecked to the left or to the right according to thefrequencies of the POS-ngram containing the currentfunctional word.We should mention that the probabilities computedduring the training phase are not re-estimated for eachrun-time processing step.
At run-time only the weightsand thresholds change from step to step.COWAL, the combined aligner takes advantage of thealignments independently provided by TREQ-AL andMEBA.
The simplest combination method consists incomputing either the union (high recall, low precision),or the intersection (lower recall, higher precision) of theindependent alignments.
We evaluated both thesesimple methods of combination and found that the bestF-measure was provided by the union-basedcombination.
Although for the shared task we submittedthe union-based combined alignment (BaselineCOWAL, see Table 1), there are various ways toimprove it.
We discuss three cases where improvementis possible (C1, C2 and C3, see below) and which wereevaluated after the submission deadline.
The results ofthis (unofficial) evaluation are summarized in Table 1by the f-COWAL line.
These cases refer to competinglinks that appeared after the union of the independentalignments.
The conflicts resolution is based on the(weak) locality and distortion heuristics discussedbefore.
The currently identified competing links areonly those for which the following conditions apply:C1) if one aligner found for a word W a non-nullalignment and the other aligner generated for thesame word W a null link, then the baseline alignmentcontains an impossible situation: the token W isrecorded both as translated and not-translated in theother language.
The translation probabilities, POSaffinity and the relative displacement of the tokens inthe non-null candidates were the strongest decisioncriteria.
We found that in about 60% of the cases thenull alignments were mistaken.
So, for the time being,we simply eliminated the null competing alignments(this should be addressed in a more principled way bythe future version of the combiner).C2) long distant competing links; this case appearswhen one aligner found for the word Ws the link tothe target word Wtm, the other aligner found for Wsthe target Wtn, and the distance between Wtm andWtn, is more than 3 words (in a future version thismaximum distance will be a dynamic parameter,depending on the sentence length and the POS ofWs).C3) competing links to the same target(s) of a wordoccurring several times in the same sentence;consider, for example, the Romanian fragment:?
?la1 Neptun, la2 Orastie si la3 Afumati, ?which in English is translated by the next segment:?
?in Neptun, Orastie and Afumati?In spite of the gold standard considering that all threeoccurrences of the preposition ?la?
in Romanian (la1,la2 ,la3) are aligned to the same word in English (?in?
),the filtering, in this case, licensed only the alignment?la1 <-> in?.
We consider that this filtered alignmentis correct, since omitting ?la2?
and ?la3?
does not alterthe syntactic correctness of the Romanian text, andalso because the insertion in the English fragment ofthe preposition ?in?
before ?Orastie?
and before?Afumati?
wouldn?t alter the grammaticality of theEnglish fragment.
Since both repetitions andomissions are optional, we consider that only the firstoccurrence of the preposition (?la1?)
is translated inEnglish, while the others are omitted.Another possible improvement (not implemented yet)was revealed by observing that the final result containedseveral incomplete n-m (phrasal) alignments.
It is likelythat even an elementary n-gram analysis (both sides ofthe bitext) would bring valuable evidence for improvingthe phrasal alignments.Post-processingAs said in the second section, our tokenization wasdifferent from the tokenization in the training and testdata.
To comply with the evaluation protocol, we had tore-tokenize the aligned text and re-compute the indexes109of the links.
Some multi-word expressions recognizedby the tokenizer as one token, such as dates (25ianuarie, 2001), compound prepositions (de la, p?n?la), conjunctions (pentru ca, de c?nd, p?n?
c?nd) oradverbs (de jur ?mprejur, ?n fa?a) as well as the hyphenseparated nominal compounds (mass-media, prim-ministru) were split, their positions were re-indexed andthe initial one link of a split compound was replacedwith the set obtained by adding one link for eachconstituent of the compound to the target English word.The same hold for the other way around.
Therefore iftwo multiword expressions were initially found to betranslation equivalents (one alignment link) after thepost-processing number of  generated links becameN*M, where N represented the number of words in thefirst language compound and M the number of words inthe second language compound.Evaluation and conclusionsNeither TREQ-AL nor MEBA needs an a prioribilingual dictionary, as this will be automaticallyextracted by the TREQ or GIZA++.
We madeevaluation of the individual alignments in bothexperimental settings: without a startup bilinguallexicon and with an initial mid-sized bilingual lexicon.Surprisingly enough, we found that while theperformance of TREQ-AL increases a little bit (approx.1% increase of the F-measure) MEBA is doing betterwithout an additional lexicon.
So, in the evaluationbelow MEBA uses only the training data vocabulary.Aligner Precision Recall F-meas.AERTREQ-AL 81.71 60.57 69.57 30.43MEBA 82.85 60.41 69.87 30.13Baseline(union)COWAL70.84 76.67 73.64 26.36f-COWAL(H1+H2+H3)87.17 70.25 77.80 22.20Table 1.
Evaluation results against the official GSAfter the release of the official Gold Standard wenoticed and corrected some obvious errors and alsoremoved the controversial links of the type c) discussedin the previous section.
The evaluations against this new?Gold Standard?
showed, on average, 3.5% betterfigures (precision, recall, F-measure and AER) for theindividual aligners, while for the combined classifiers,the performance scores were about 4% better.MEBA is very sensitive to the values of theparameters which control its behavior.
Currently theyare set according to the developers?
intuition and afterthe analysis of the results from several trials.
Since thisactivity is pretty time consuming (human analysis plusre-training might take a couple of hours) we plan toextend MEBA with a supervised learning module,which would automatically determine the ?optimal?parameters (thresholds and weights) values.ReferencesAl-Onaizan, Y., Curin, J., Jahr, M., Knight K., Lafferty, J.,Melamed, D., Och, F. J., Purdy, D., Smith, N.A.,Yarowsky, D. (1999) : Statistical MachineTranslation, Final Report, JHU Workshop, 42 pagesBrill, E., and Wu, J.
(1998).
?Classifier Combination forImproved Lexical Disambiguation?
In Proceedings ofCOLING-ACL?98  Montreal, Canada, 191-195Brown, P. F., Della Pietra, S.A.,  Della Pietra, V. J.,Mercer, R. L.(1993) ?The mathematics of statisticalmachine translation: Parameter estimation?.Computational Linguistics, 19(2) pp.
263?311.Dietterich, T. G., (1998).
?Approximate Statistical Testsfor Comparing Supervised Classification LearningAlgorithms?.
Neural Computation, 10 (7) 1895-1924.Gale, W.A.
and Church, K.W.
(1991).
?Identifying wordcorrespondences in parallel texts?.
Proceedings of theFourth DARPA Workshop on Speech and NaturalLanguage.
Asilomar, CA, pp.
152?157.Melamed, D. (2001).
Empirical Methods for ExploitingParallel Texts.
Cambridge, MA: MIT Press.Och, F.J., Ney, H. (2003) "A Systematic Comparison ofVarious Statistical Alignment Models", Computa-tional Linguistics, 29(1), pp.
19-51Och, F.J., Ney, H.(2000) "Improved Statistical AlignmentModels".
Proceedings of the 38th ACL, Hongkong,pp.
440-447Tiedemann, J.
(2003) ?Combining clues for wordalignment?.
In Proceedings of the 10th EACL,Budapest, pp.
339?346Tufi?, D.(2002) ?A cheap and fast way to build usefultranslation lexicons?.
Proceedings of COLING2002,Taipei, pp.
1030-1036.Tufi?, D., Barbu, A.M., Ion R (2003).
: ?TREQ-AL: Aword-alignment system with limited languageresources?, Proceedings of the NAACL 2003Workshop on Building and Using Parallel Texts;Romanian-English Shared Task, Edmonton, pp.
36-39Tufi?, D., Ion, R., Ide, N.(2004a): Fine-Grained WordSense Disambiguation Based on Parallel Corpora,Word Alignment, Word Clustering and AlignedWordnets.
Proceedings of COLING2004, Geneva, pp.1312-1318Tufis, D., Barbu, E., Mititelu, V., Ion, R., Bozianu,L.
(2004b): ?The Romanian Wordnet?.
In RomanianJournal on Information Science and Technology, DanTufi?
(ed.)
Special Issue on BalkaNet, RomanianAcademy, 7(2-3), pp.
105-122.110
