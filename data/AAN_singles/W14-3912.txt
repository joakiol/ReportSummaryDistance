Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 102?106,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsThe IUCL+ System: Word-Level Language Identification via ExtendedMarkov ModelsLevi King, Eric Baucom, Timur Gilmanov, Sandra K?ubler, Daniel WhyattIndiana University{leviking,eabaucom,timugilm,skuebler,dwhyatt}@indiana.eduWolfgang MaierUniversita?t Du?sseldorfmaierw@hhu.dePaul RodriguesUniversity of Marylandprr@umd.eduAbstractWe describe the IUCL+ system for the sharedtask of the First Workshop on ComputationalApproaches to Code Switching (Solorio et al.,2014), in which participants were challengedto label each word in Twitter texts as a namedentity or one of two candidate languages.
Oursystem combines character n-gram probabili-ties, lexical probabilities, word label transitionprobabilities and existing named entity recog-nition tools within a Markovmodel frameworkthat weights these components and assigns alabel.
Our approach is language-independent,and we submitted results for all data sets(five test sets and three ?surprise?
sets, cov-ering four language pairs), earning the high-est accuracy score on the tweet level on twolanguage pairs (Mandarin-English, Arabic-dialects 1 & 2) and one of the surprise sets(Arabic-dialects).1 IntroductionThis shared task challenged participants to performword level analysis on short, potentially bilingual Twit-ter and blog texts covering four language pairs: Nepali-English, Spanish-English,Mandarin-English andMod-ern Standard Arabic-Arabic dialects.
Training setsranging from 1,000 to roughly 11,000 tweets were pro-vided for the language pairs, where the content of thetweets was tokenized and labeled with one of six la-bels.
The goal of the task is to accurately replicatethis annotation automatically on pre-tokenized texts.With an inventory of six labels, however, the task ismore than a simple binary classification task.
In gen-eral, the most common labels observed in the train-ing data are lang1 and lang2, with other (mainlycovering punctuation and emoticons) also common.Named entities (ne) are also frequent, and accountingfor them adds a significant complication to the task.Less common are mixed (to account for words thatmay e.g., apply L1 morphology to an L2 word), andambiguous (to cover a word that could exist in eitherlanguage, e.g., no in the Spanish-English data).Traditionally, language identification is performedon the document level, i.e., on longer segments oftext than what is available in tweets.
These methodsare based on variants of character n-grams.
Seminalwork in this area is by Beesley (1988) and Grefenstette(1995).
Lui and Baldwin (2014) showed that charactern-grams also perform on Twitter messages.
One of afew recent approaches working on individual words isby King et al.
(2014), who worked on historical data;see also work by Nguyen and Dogruz (2013) and Kingand Abney (2013).Our system is an adaptation of a Markov model,which integrates lexical, character n-gram, and la-bel transition probabilities (all trained on the provideddata) in addition to the output of pre-existing NERtools.
All the information sources are weighted in theMarkov model.One advantage of our approach is that it is language-independent.
We use the exact same architecture forall language pairs, and the only difference for the indi-vidual language pairs lies in a manual, non-exhaustivesearch for the best weights.
Our results show that theapproachworks well for the one language pair with dif-ferent writing systems (Mandarin-English) as well asfor the most complex language pair, the Arabic set.
Inthe latter data set, the major difficulty consists in theextreme skewing with an overwhelming dominance ofwords in Modern Standard Arabic.2 MethodOur system uses an extension of a Markov model toperform the task of word level language identification.The system consists of three main components, whichproduce named entity probabilities, emission probabil-ities and label transition probabilities.
The outputs ofthese three components are weighted and combined in-side the extended Markov model (eMM), where thebest tag sequence for a given tweet (or sentence) is de-termined via the Viterbi algorithm.In the following sections, we will describe thesecomponents in more detail.2.1 Named Entity RecognitionWe regard named entity recognition (NER) as a stand-alone task, independent of language identification.
Forthis reason, NER is performed first in our system.In order to classify named entities in the tweets, weemploy two external tools, Stanford-NER and Twit-terNLP.
Both systems are used in a black box approach,102without any attempt at optimization.
I.e., we use thedefault parameters where applicable.Stanford NER (Finkel et al., 2005) is a state-of-the-art named entity recognizer based on conditional ran-dom fields (CRF), which can easily be trained on cus-tom data.1For all of the four language pairs, we train aNER model on a modified version of the training datain which we have kept the label ?ne?
as our target la-bel, but replaced all others with the label ?O?.
Thus, wecreate a binary classification problem of distinguishingnamed entities from all other words.
This method isapplicable for all data sets.For the Arabic data, we additionally employ agazetteer, namely ANERgazet (Benajiba and Rosso,2008).2However, we do not use the three classes (per-son, location, organization) available in this resource.The second NER tool used in our system is the Twit-terNLP package.3This system was designed specifi-cally for Twitter data.
It deals with the particular dif-ficulties that Twitter-specific language (due to spelling,etc.)
poses to named entity recognition.
The system hasbeen shown to be very successful: Ritter et al.
(2011,table 6) achieve an improvement of 52% on segmen-tation F-score in comparison with Stanford NER onhand-annotated Twitter data, which is mainly due to aconsiderably increased recall.The drawback of using TwitterNLP for our task isthat it was developed for English, and adapting it toother languages would involve a major redesign andadaptation of the system.
For this reason, we decidedto use it exclusively on the language pairs that includeEnglish.
An inspection of the training data showed thatfor all language pairs involving English, a majority ofthe NEs are written in English and should thus be rec-ognizable by the system.TwitterNLP is an IOB tagger.
Since we do not dis-tinguish between the beginning and the rest of a namedentity, we change all corresponding labels to ?ne?
inthe output of the NER system.In testing mode, the NER tools both label each wordin a tweet as either ?O?
or ?ne?.
We combine the outputsuch that ?ne?
overrides ?O?
in case of any disagree-ments, and pass this information to the eMM.
This out-put is weighted with optimized weights unique to eachlanguage pair that were obtained through 10-fold crossvalidation, as discussed below.
Thus, the decisions ofthe NER systems is not final, but they rather provideevidence that can be overruled by other system compo-nents.2.2 Label Transition ModelsThe label transition probability component models lan-guage switches on the sequence of words.
It is also1See http://nlp.stanford.edu/software/CRF-NER.shtml.2As available from http://users.dsic.upv.es/grupos/nle/.3See https://github.com/aritter/twitter_nlp.trained on the provided training data.
In effect, thiscomponent consists of unigram, bigram, and trigramprobability models of the sequences of labels foundin the training data.
Our MM is second order, thusthe transition probabilities are linear interpolations ofthe uni-, bi-, and trigram label transition probabili-ties that were observed in the training data.
We addtwo beginning-of-sentence buffer labels and one end-of-sentence buffer label to assist in deriving the start-ing and ending probabilities of each label during thetraining.2.3 Emission ProbabilitiesThe emission probability component is comprised oftwo subcomponents: a lexical probability componentand a character n-gram probability component.
Bothare trained on the provided training data.Lexical probabilities: The lexical probability com-ponent consists of a dictionary for each label contain-ing the words found under that label and their rel-ative frequencies.
Each word type and its count oftokens are added to the total for each respective la-bel.
After training, the probability of a given labelemitting a word (i.e., P (word|label)) is derived fromthese counts.
To handle out-of-vocabulary words, weuse Chen-Goodman ?one-count?
smoothing, which ap-proximates the probabilities of unknownwords as com-pared to the occurrence of singletons (Chen and Good-man, 1996).Character n-gram probabilities: The character-based n-grammodel serves mostly as a back-off in casea word is out-of-vocabulary, in which case the lexi-cal probability may not be reliable.
However, it alsoprovides important information in the case of mixedwords, which may use morphology from one languageadded to a stem from the other one.
In this setting, un-igrams are not informative.
For this reason, we selectlonger n-grams, with n ranging between 2 and 5.Character n-gram probabilities are calculated as fol-lows: For each training set, the words in that trainingset are sorted into lists according to their labels.
Intraining models for each value of n, n ?
1 buffer char-acters are added to the beginning and end of each word.For example, in creating a trigram character modelfor the lang1 (English) words in the Nepali-Englishtraining set, we encounter the word star.
We first gen-erate the form $$star##, then derive the trigrams.
Thetrigrams from all training words are counted and sortedinto types, and the counts are converted to relative fre-quencies.Thus, using four values of n for a data setcontaining six labels, we obtain 24 character n-grammodels for that language pair.
Note that because thiscomponent operates on individual words, character n-grams never cross a word boundary.In testing mode, for each word and for each value ofn, the component generates a probability that the wordoccurred under each of the six labels.
These values103are passed to the eMM, which uses manually optimizedweights for each value of n to combine the four n-gramscores for each label into a single n-gram score for eachlabel.
In cases where an n-gram from the test wordwas not present in the training data, we use a primitivevariant of LaPlace smoothing, which returns a fixed,extremely low non-zero probability for that n-gram.2.4 The Extended Markov ModelOur approach is basically a trigram Markov model(MM), in which the observations are the words inthe tweet (or blog sentence) and the underlying statescorrespond to the sequence of codeswitching labels(lang1, lang2, ne, mixed, ambiguous,other).
The MM, as usual, also uses startingand ending probabilities (in our case, derived fromstandard training of the label transition model, dueto our beginning- and end-of-sentence buffer labels),label/state transition probabilities, and probabilitiesthat the state labels will emit particular observations.The only difference is that we modify the standardHMM emission probabilities.
We call this resultingMarkov model extended (eMM).First, for every possible state/label in the sequence,we linearly interpolate ?lexical (emission) probabil-ities?
Plex(the standard emission probabilities forHMMs) with character n-gram probabilities Pchar.That is, we choose 0 ?
?lex?
1 and 0 ?
?char?
1such that ?lex+ ?char= 1.
We use them to derivea new emission probability Pcombined= ?lex?
Plex+?char?Pchar.
This probability represents the likelihoodthat the given label in the hidden layer will emit the lex-ical observation, along with its corresponding charactern-gram sequence.Second, only for ne labels in the hidden layer, wemodify the probabilities that they will emit the ob-served word if that word has been judged by our NERmodule to be a named entity.
Since the NER compo-nent exhibits high precision but comparatively low re-call, we boost the Pcombined(label = ne|word) if theobservedword is judged to be a named entity, but we donot penalize the regular Pcombinedif not.
This boostingis accomplished via linear interpolation and another setof parameters, 0 ?
?ne?
1 and 0 ?
?combined?
1such that ?ne+ ?combined= 1.
Given a positive de-cision from the NER module, the new probability forthe ne label emitting the observed word is derived asPne+combined= ?ne?
0.80 + ?combined?
Pcombined,i.e., we simply interpolate the original probability witha high probability.
All lambda values, as well as theweights for the character n-gram probabilities, were setvia 10-fold cross-validation, discussed below.2.5 Cross Validation & OptimizationIn total, the system uses 11 weights, each of which isoptimized for each language pair.
In labeling namedentities, the output of the NER component is given oneweight and the named entity probabilities of the othersources (emission and label transition components) isgiven another weight, with these weights summing toone.
For the label transition component, the uni-, bi-and trigram scores receive weights that sum to one.Likewise, the emission probability component is com-prised of the lexical probability and the character n-gram probability, with weights that sum to one.
Thecharacter n-gram component is itself comprised of thebi-, tri-, four- and five-gram scores, again with weightsthat sum to one.For each language pair, these weights were opti-mized using a 10-fold cross validation script that splitsthe original training data into a training file and a testfile, runs the split files through the system and averagesthe output.
As time did not allow an exhaustive searchfor optimal weights in this multi-dimensional space, wenarrowed the space by first manually optimizing eachsubset of weights independently, then exploring com-binations of weights in the resulting neighborhood.3 Results3.1 Main ResultsThe results presented in this section are the official re-sults provided by the organizers.
The evaluation is splitinto two parts: a tweet level evaluation and a token levelevaluation.
On the tweet level, the evaluation concen-trates on the capability of systems to distinguish mono-lingual from multilingual tweets.
The token level eval-uation is concerned with the classification of individ-ual words into the different classes: lang1, lang2,ambiguous, mixed, ne, and other.Our results for the tweet level evaluation, in com-parison to the best or next-best performing system areshown in table 1.
They show that our system is ca-pable of discriminating monolingual from multilingualtweets with very high precision.
This resulted in thebest results in the evaluation with regard to accuracyfor Mandarin-English and for both Arabic-dialects set-tings.
We note that for the latter setting, reaching goodresults is exceedingly difficult without any Arabic re-sources.
This task is traditionally approached by us-ing a morphological analyzer, but we decided to usea knowledge poor approach.
This resulted in a ratherhigh accuracy but in low precision and recall, espe-cially for the first Arabic test set, which was extremelyskewed, with only 32 out of 2332 tweets displayingcodeswitching.Our results for the token level evaluation, in com-parison to the best performing system per language,are shown in table 2.
They show that our system sur-passed the baseline for both language pairs for whichthe organizers provided baselines.
In terms of accu-racy, our system is very close to the best performingsystem for the pairs Spanish-English andMandarin En-glish.
For the other language pairs, we partially sufferfrom a weak NER component.
This is especially obvi-ous for the Arabic dialect sets.
However, this is also aproblem that can be easily fixed by using a more com-104lang.
pair system Acc.
Recall Precision F-scoreNep.-Eng.
IUCL+ 91.2 95.6 94.9 95.2dcu-uvt 95.8 99.4 96.1 97.7Span.-Eng.
IUCL+ 83.8 51.4 87.7 64.8TAU 86.8 72.0 80.3 75.9Man.-Eng.
IUCL+ 82.4 94.3 85.0 89.4MSR-India 81.8 95.5 83.7 89.2Arab.
dia.
IUCL+ 97.4 12.5 11.1 11.8MSR-India 94.7 34.4 9.7 15.2Arab.
dia.
2 IUCL+ 76.6 24.9 27.1 26.0MSR-India 71.4 21.2 18.3 19.6Table 1: Tweet level results in comparison to the system with (next-)highest accuracy.lang1 lang2 mixed nelang.
pair system Acc.
R P F R P F R P F R P FNep.-Eng.
IUCL+ 75.2 85.1 89.1 87.1 68.9 97.6 80.8 1.7 100 3.3 55.1 48.7 51.7dcu-uvt 96.3 97.9 95.2 96.5 98.8 96.1 97.4 3.3 50.0 6.3 45.6 80.4 58.2base 70.0 57.1 76.5 65.4 92.3 62.8 74.7 0.0 100 0.0 0.0 100 0.0Span.-Eng.
IUCL+ 84.4 88.9 82.3 85.5 85.1 89.9 87.4 0.0 100 0.0 30.4 48.5 37.4TAU 85.8 90.0 83.0 86.4 86.9 91.4 89.1 0.0 100 0.0 31.3 54.1 39.6base 70.3 85.1 67.6 75.4 78.1 72.8 75.4 0.0 100 0.0 0.0 100 0.0Man.-Eng.
IUCL+ 89.5 98.3 97.8 98.1 83.9 66.6 74.2 0.0 100 0.0 70.1 50.3 58.6MSR-India 90.4 98.4 97.6 98.0 89.1 66.6 76.2 0.0 100 0.0 67.7 65.2 66.4Arab.
dia.
IUCL+ 78.8 96.1 81.6 88.2 34.8 8.9 14.2 ?
?
?
3.3 23.4 5.8CMU 91.0 92.2 97.0 94.6 57.4 4.9 9.0 ?
?
?
77.8 70.6 74.0Arab.
dia.
2 IUCL+ 51.9 90.7 43.8 59.0 47.7 78.3 59.3 0.0 0.0 0.0 8.5 28.6 13.1CMU 79.8 85.4 69.0 76.3 76.1 87.3 81.3 0.0 100 0.0 68.7 78.8 73.4Table 2: Token level results in comparison to the system with highest accuracy (results for ambiguous andother are not reported).lang1 lang2 nelang.
pair system Acc.
R P F R P F R P FNep.-Eng.
IUCL+ 80.5 86.1 78.8 82.3 97.6 80.9 88.5 29.9 80.9 43.7JustAnEagerStudent 86.5 91.3 80.2 85.4 93.6 91.1 92.3 39.4 83.3 53.5Span.-Eng.
IUCL+ 91.8 87.4 81.9 84.5 84.5 87.4 85.9 28.5 47.4 35.6dcu-uvt 94.4 87.9 80.5 84.0 84.1 86.7 85.4 22.4 55.2 31.9Arab.
dia.
IUCL+ 48.9 91.7 33.3 48.8 48.4 81.9 60.9 3.3 17.6 5.5CMU 77.5 87.6 55.5 68.0 75.6 89.8 82.1 52.3 73.8 61.2Table 3: Token level results for the out-of-domain data.petitive, language dependent system.
Another problemconstitutes the mixed cases, which cannot be reliablyannotated.3.2 Out-Of-Domain ResultsThe shared task organizers provided ?surprise?
data,from domains different from the training data.
Our re-sults on those data sets are shown in table 3.
For spacereasons, we concentrate on the token level results only.The results show that our system is very robust withregard to out-of-domain settings.
For Nepali-Englishand Spanish-English, we reach higher results than onthe original test sets, and for the Arabic dialects, the re-sults are only slightly lower.
These results need furtheranalysis for us to understand how our system performsin such situations.4 ConclusionsWe have presented the IUCL+ system for word levellanguage identification.
Our system is based on aMarkov model, which integrates different types of in-formation, including the named entity analyses, lexicaland character n-gram probabilities as well as transitionprobabilities.
One strength of the system is that it iscompletely language independent.
The results of theshared task have shown that the system generally pro-vides reliable results, and it is fairly robust in an out-of-domain setting.105ReferencesKenneth R. Beesley.
1988.
Language identifier: Acomputer program for automatic natural-languageidentification of on-line text.
In Proceedings of the29th Annual Conference of the American TranslatorsAssociation, volume 47, page 54.Yassine Benajiba and Paolo Rosso.
2008.
Arabicnamed entity recognition using conditional randomfields.
In Proceedings of Workshop on HLT & NLPwithin the Arabic World, LREC 2008, Marakech,Morroco.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meet-ing on Association for Computational Linguistics,pages 310?318.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 363?370.Gregory Grefenstette.
1995.
Comparing two languageidentification schemes.
In Proceedings of the ThirdInternational Conference on Statistical Analysis ofTextual Data (JADT), volume 2.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-languagedocuments usingweakly supervised methods.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1110?1119.
As-sociation for Computational Linguistics.Levi King, Sandra Ku?bler, and Wallace Hooper.
2014.Word-level language identification in The Chymistryof Isaac Newton.
Literary and Linguistic Comput-ing.Marco Lui and Timothy Baldwin.
2014.
Accurate lan-guage identification of Twitter messages.
In Pro-ceedings of the 5th Workshop on Language Analysisfor Social Media (LASM), pages 17?25, Gothenburg,Sweden.Dong Nguyen and A. Seza Dogruz.
2013.
Word levellanguage identification in online multilingual com-munication.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural LanguageProcess-ing, pages 857?862.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1524?1534, Edinburgh, Scotland,UK., July.
Association for Computational Linguis-tics.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Julia Hirshberg, Alison Chang, and Pas-cale Fung.
2014.
Overview for the first sharedtask on language identification in code-switcheddata.
In Proceedings of the First Workshop on Com-putational Approaches to Code-Switching.
EMNLP2014, Conference on Empirical Methods in NaturalLanguage Processing, Doha, Qatar.106
