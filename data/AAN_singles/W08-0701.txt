Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 1,Columbus, Ohio, USA June 2008. c?2008 Association for Computational LinguisticsInvited Talk:Phonological Models in Automatic Speech RecognitionKaren LivescuToyota Technological Institute at Chicago1427 E. 60th St., Chicago, IL 60637klivescu@tti-c.orgAbstractThe performance of automatic speech recognition systems varies widely across different contexts.
Verygood performance can be achieved on single-speaker, large-vocabulary dictation in a clean acoustic environ-ment, as well as on very small vocabulary tasks with much fewer constraints on the speakers and acousticconditions.
In other domains, speech recognition is still far from usable for real-world applications.
Onedomain that is still elusive is that of spontaneous conversational speech.
This type of speech poses a numberof challenges, such as the presence of disfluencies, a mix of speech and non-speech sounds such as laughter,and extreme variation in pronunciation.
In this talk, I will focus on the challenge of pronunciation variation.A number of analyses suggest that this variability is responsible for a large part of the drop in recognitionperformance between read (dictated) speech and conversational speech.I will describe efforts in the speech recognition community to characterize and model pronunciationvariation, both for conversational speech and in general.
The work can be roughly divided into several typesof approaches, including: augmentation of a phonetic pronunciation lexicon with phonological rules; the useof large (syllable- or word-sized) units instead of the more traditional phonetic ones; and the use of smallerunits, such as distinctive or articulatory features.
Of these, the first is the most thoroughly studied andalso the most disappointing: Despite successes in a few domains, it has been difficult to obtain significantrecognition improvements by including in the lexicon those phonetic pronunciations that appear to exist inthe data.
In part as a reaction to this, many have advocated the use of a ?null pronunciation model,?
i.e.
avery limited lexicon including only canonical pronunciations.
The assumption in this approach is that theobservation model?the distribution of the acoustics given phonetic units?will better model the ?noise?introduced by pronunciation variability.I will advocate an alternative view: that the phone unit may not be the most appropriate for modeling thelexicon.
When considering a variety of pronunciation phenomena, it becomes apparent that phonetic tran-scription often obscures some of the fundamental processes that are at play.
I will describe approaches usingboth larger and ?smaller?
units.
Larger units are typically syllables or words, and allow greater freedom tomodel the component states of each unit.
In the class of ?smaller?
unit models, ideas from articulatory andautosegmental phonology motivate multi-tier models in which different features (or groups of features) havesemi-independent behavior.
I will present a particular model in which articulatory features are representedas variables in a dynamic Bayesian network.Non-phonetic pronunciation models can involve significantly different model structures than those typi-cally used in speech recognition, and as a result they may also entail modifications to other components suchas the observation model and training algorithms.
At this point it is not clear what the ?winning?
approachwill be.
The success of a given approach may depend on the domain or on the amount and type of trainingdata available.
I will describe some of the current challenges and ongoing work, with a particular focus onthe role of phonological theories in statistical models of pronunciation (and vice versa?
).1
