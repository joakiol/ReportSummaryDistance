Coverage-based Evaluation of Parser GeneralizabilityTuomo Kakkonen and Erkki SutinenDepartment of Computer Science and StatisticsUniversity of JoensuuP.O.
Box 111, FI-80101 Joensuu, Finland{tuomo.kakkonen,erkki.sutinen}@cs.joensuu.fiAbstractWe have carried out a series of coverageevaluations of diverse types of parsers us-ing texts from several genres such as news-paper, religious, legal and biomedical texts.We compared the overall coverage of theevaluated parsers and analyzed the differ-ences by text genre.
The results indicatethat the coverage typically drops severalpercentage points when parsers are facedwith texts on genres other than newspapers.1 IntroductionThe fact that most of the parser evaluation re-sources employed consist of texts from a singlegenre constitutes a deficiency in most of the parserevaluations.
Evaluations are typically carried outon newspaper texts, i.e.
on section 23 of the PennTreebank (PTB) (Marcus et al, 1993).
A furthercomplication is that many parsing models aretrained on the same treebank.
Parsers thereforecome to be applied to texts from numerous othergenres untested.
The obvious question that con-fronts us in these circumstances is: How well will aparser that performs well on financial texts fromthe Wall Street Journal generalize to other texttypes?This present paper addresses parser evaluationfrom the perspective of coverage.
It is a part of aset of evaluations in which selected parsers areevaluated using five criteria: preciseness, coverage,robustness, efficiency and subtlety.
Parsing cover-age refers to the ability of a parser to produce ananalysis of sentences of naturally occurring free-text.
We used parsing coverage to assess the gen-eralizability of the grammars and parsing modelsand we looked for answers to the following re-search questions:?
What is the parsing coverage of the evalu-ated parsers??
How does the text genre affect the parsingcoverage?Previous work on evaluation methods and re-sources is discussed in Section 2.
Section 3 de-scribes the evaluation method and test settings.
InSection 4, we give the results of the experiments.Section 5 concludes with a description of remain-ing problems and directions for future research.2 Preliminaries2.1 Coverage EvaluationPrasad and Sarkar (2000) observe that the notion ofcoverage has the following two meanings in thecontext of parsing.
Grammatical coverage is theparser?s ability to handle different linguistic phe-nomena, and parsing coverage is a measure of thepercentage of naturally occurring free text in whicha parser can produce a full parse.
We divide pars-ing coverage further into genre coverage on differ-ent types of texts such as newspapers, religious,biomedicine and fiction.11The classification of texts in terms of domain, genre, registerand style is a rather controversial issue (see, for example, dis-cussion by Lee (2001)).
A detailed analysis of these issuesfalls outside of the scope of this paper.
We have thereforeadopted a simplified approach by indicating differences be-tween texts by using the word genres.
One may think of gen-res (in this sense) as indicating fundamental categorical differ-ences between texts that are revealed in sets of attributes suchas domain (e.g.
art, science, religion, government), medium703Parsing coverage can be measured as the per-centage of input sentences to which a parser is ableto assign a parse.
No annotated text is needed forperforming parsing coverage evaluations.
On onehand, it can be argued that coverage alone consti-tutes a rather weak measure of a parser?s perform-ance, and thus of its generalizability to diverse textgenres.
An obvious problem with measuring cov-erage alone is that a parser returning undetailedand flat analyses will easily get high coverage,whereas a parser that outputs detailed analyses willsuffer in covering all the input sentences.
More-over, preciseness and coverage can be seen as con-flicting requirements for a parser.
Increasing pre-ciseness of the grammar often causes its coverageto decrease; adding more constraints to the gram-mar causes some of the sentences to be rejectedeven they are acceptable to users of the language.Loosening the constraints allows more sentences tobe parsed, thus increasing the coverage, but at thesame time easily leads into overgeneration, prob-lems with disambiguation and decreased precise-ness.On the other hand, the points that we raisedabove indicate that there is a strong relationshipbetween coverage and preciseness.
The aim of syn-tactic parsers is to analyze whole sentences, notjust fragments (constituents/D links) precisely.
Theconnection between coverage and preciseness isclear in the case of sentence level evaluationsmeasures2: A sentence that cannot be fully ana-lyzed cannot have a complete match with the cor-rect structure in the evaluation resource.
Conse-quently, we argue that coverage can be used ameasure of generalizability; It sets the upper boundfor the performance on the sentence-level evalua-tion measures.
However, the evaluation should al-ways be accompanied with data on the precisenessof the parser and the level of detail in its output.2.2 Previous Coverage and Cross-genreEvaluationsRelatively little work has been done on the empiri-cal evaluation of parsers for text types other thannewspaper texts.
A key issue in available evalua-(e.g.
spoken, written), content (topic, theme) and type (narra-tive, argumentation, etc.
).2For example Yamada & Matsumoto (2003) uses completematch metric (the percentage of sentences whose unlabeled Dstructure is completely correct) to evaluate the sentence-levelpreciseness of D parsers.tion materials is the genre homogeneity.
Almost allthe available resources are based on a single genre(nearly always newspaper texts).
This makes it im-possible to extrapolate anything useful about thegeneralizability of the developed grammars andparsing models.To our knowledge, this experiment is the onlyone reported in the literature that compares thecoverage of a set of parsers for English.
The stud-ies that critically examine the genre dependencyhave come to the same unsurprising conclusionthat the text genre has an effect on the parser?s per-formance.
The genre dependency of parsers is anaccepted fact and has been described by, amongothers, Sekine (1997) and Gildea (2001).
For ex-ample, Clegg and Shepherd (2005) have under-taken experiments on biomedical data using theGENIA treebank.
Laakso (2005) reports experi-ments on the CHILDES corpus of transcribedspeech between parents and the children.
Mazzeiand Lombardo (2004) report cross-training ex-periments in Italian on newspaper and civil lawtexts.
They observed a dramatic drop of, mostcommonly, around 10-30 percentage points in theparsing coverage.2.3 Reasons for the Coverage DropGenre dependency is caused by several factors.One is that each text genre is characterized bygenre-specific words (Biber, 1993).
Another fea-ture of genre dependency is syntactic structure dis-tributions.
Baldwin et al (2004) have conductedone of the rare studies that offer an analysis of themain reasons for the diminished coverage.
Theyexperimented with an HPSG grammar that was acreated manually based on a corpus of data ex-tracted from informal genres such as conversationsabout schedules and e-mails about e-commerce.The grammar was used for parsing a random sam-ple of texts from several genres.
A diagnosis offailures to parse sentences with full lexical span3revealed the following causes for the errors: miss-ing lexical entries (40%), missing constructions(39%), preprocessor errors (4%), fragments (4%),parser failures (4%), and garbage strings (11%).They came to the conclusion that lexical expansionshould be the first step in the process of parser en-hancement.3Sentences that contained only words included in the lexicon.7043 Experiments3.1 Research ApproachIn order to investigate the effect of the text genreon the parsing results, we constructed a test corpusof more than 800,000 sentences and divided theminto six genres.
We parsed these texts by using fiveparsing systems.The design of our test settings and materials wasguided by our research questions (above).
We an-swered the first question by parsing vast documentcollections with several state-of-the-art parsingsystems and then measuring their parsing coverageon the data.
Because we had divided our purpose-built test set into genre-specific subsets, this al-lowed us to measure the effects of genre varianceand so provide an answer to the second researchquestion.
We also included two parsers that hadbeen developed in the 1990s to evaluate the extentto which progress has been made in parsing tech-nology in genre dependency and parsing coverage.3.2 Evaluation Metric and MeasuresThe most important decision in parsing coverageevaluation is how the distinction between a cov-ered and uncovered sentence is made.
This has tobe defined separately for each parser and the defi-nition depends on the type of output.
We imple-mented a set of Java tools to record the statisticsfrom the parsers?
outputs.
In addition to completelyfailed parses, we recorded information about in-complete analyses and the number of times theparsers crashed or terminated during parsing.3.3 MaterialsThe test set consisted of 826,485 sentences dividedinto six sub-corpora.
In order to cover several gen-res and to guarantee the diversity of the text types,we sourced a diversity of materials from severalcollections.
There are six sub-corpora in the mate-rial and each covers one of the following genres:newspaper, legislation, fiction, non-fiction, religionand biomedicine.Table 1 shows the sub-corpora and the figuresassociated with each corpus.
In total there were15,385,855 tokens.
The style of the newspapertexts led us to make an initial hypothesis that asimilar performance would probably be achievablewith non-fiction texts, and we suspected that thelegislative and fiction texts might be more difficultto parse because of the stylistic idiosyncrasies in-volved.
Biomedical texts also contained a consid-erable number of words that are probably notfound in the lexicons.
These two difficulties werecompounded in the religious texts, and the averagelength of the religion sub-corpus was far higherthan the average.Table 1.
The test sets.3.4 The ParsersWe included both dependency (D)- and phrasestructure (PS)-based systems in the experiment.The parsers use a Probabilistic Context-freeGrammar (PCFG), Combinatory CategorialGrammar (CCG), a semi-context sensitive gram-mar and a D-based grammar.Apple Pie Parser (APP) (v. 5.9, 4 April 1997) isa bottom-up probabilistic chart parser which findsthe analysis with the best score by means of best-first search algorithm (Sekine, 1998).
It uses asemi-context sensitive grammar obtained auto-matically from the PTB.
The parser outputs a PSanalysis consisting of 20 syntactic tags.
No word-level analysis is assigned.
We regard a sentence ashaving been covered if APP finds a single S non-terminal which dominates the whole sentence andif it does not contain any X tags which would indi-cate constituents of unrecognized category.C&C Parser (v. 0.96, 23 November 2006) isbased on a CCG.
It applies log-linear probabilistictagging and parsing models (Clark and Curran,2004).
Because the parser marks every output asGenre DescriptionNo.
ofsen-tences Avg.lengthLegislation Discussions of the Canadian Parliament 390,042 17.2Newspaper Texts from severalnewspapers 217,262 19.5Fiction Novels from the 20thand 21st century 97,156 15.9Non-fictionNon-fiction booksfrom the 20th and21st century61,911 21.9Religion The Bible, the Koran, the Book of Mormon 45,459 27.1Biomedi-cineAbstracts from bio-medical journals 14,655 21.6TOTAL 826,485 18.6705either parsed or failed, evaluation of failed parsesis straightforward.
Fragmented parses were de-tected from the grammatical relations (GR) output.Because GR representations can form cycles, ananalysis was not required to have a unique root.Instead, a parse was regarded as being incompleteif, after projecting each GR to a graph allowingcycles, more than one connected set (indicating afragmented analysis) was found.MINIPAR (unknown version, 1998) is a princi-ple-based parser applying a distributed chart algo-rithm and a D-style grammar (Lin, 1998).
The syn-tactic tagset comprises 27 grammatical relationtypes and word and phrase types are marked with20 tags.
A sentence is regarded as having beencovered by MINIPAR if a single root is found forit that is connected to all the words in the sentencethrough a path.
The root should in addition be as-signed with a phrase/sentence type marker.Stanford Parser (referred in the remainder ofthis text as SP) (v. 1.5.1, 30 May 2006) can useboth an unlexicalized and lexicalized PCFGs(Klein and Manning, 2003).
This parser uses aCYK search algorithm and can output both D andPS analyses (de Marneffe et al, 2006).
We ran theexperiment on the unlexicalized grammar and car-ried out the evaluation on the D output consistingof 48 D types.
We regard a sentence as havingbeen covered by SP in a way similar to that inMINIPAR: the sentence is covered if the D treereturned by the parser has a single root node inwhich there is a path to all the other nodes in thetree.StatCCG (Preliminary public release, 14 January2004) is a statistical parser for CCG that was de-veloped by Julia Hockenmaier (2003).
In contrastto C&C, this parser is based on a generative prob-abilistic model.
The lexical category set has around1,200 types, and there are four atomic types in thesyntactic description.
StatCCG marks every rele-vant sentence as ?failed?
or ?too long?
in its output.We were therefore able to calculate the failedparses directly from the system output.
We re-garded parses as being partially covered when nosentence level non-terminal was found.3.5 Test SettingsWe wanted to create similar and equal condi-tions for all parsers throughout the evaluation.Moreover, language processing applications thatinvolve parsing must incorporate practical limitson resource consumption.
4 Hence, we limited theuse of memory to the same value for all the parsersand experiments.
5 We selected 650 MB as the up-per limit.
It is a realistic setting for free workingmemory in a typical personal computer with 1 GBmemory.4 ResultsTable 2 summarizes the results of the experiments.The parsing coverage of the parsers for each of thesub-corpora is reported separately.
Total figuresare given for both parser and sub-corpus level.
InTable 3, the coverage figures are further brokendown to indicate the percentage of the analysesthat failed or were incomplete or those occasionson which the parser crashed or terminated duringthe process.The five parsers were able to cover, on average,88.8% of the sentences.
The coverage was, unsur-prisingly, highest on the newspaper genre.
Thelowest average coverage was achieved on the relig-ion genre.
The difficulties in parsing the religioustexts are attributable at least in part to the length ofthe sentences in the sub-corpus (on average 27.1words per sentence), which was the highest overall the genres.
Contrary to our expectation, thebiomedical genre, with its specialist terminology,was not the most difficult genre for the parsers.If one excludes the one-word sentences from thelegislation dataset, SP had the best coverage andbest generalizability over the text genres.
APP wasthe second best performer in this experiment, bothin coverage and generalizability.
While APP pro-duces shallow parses, this helps it to obtain a highcoverage.
Moreover, comparing the F-scores re-ported in the literature for the five parsers revealedthat the F-score (70.1) of this parser was more than10 percentage points lower than the score of thesecond worst parser MINIPAR.
Thus, it is obviousthat the high coverage in APP is achieved at thecost of preciseness and lack of detail in the output.4In addition, parsing in the order of hundreds of thousands ofsentences with five parsers takes thousands of hours of proces-sor time.
It was therefore necessary for us to limit the memoryconsumption in order to be able to run the experiments in par-allel.5Several methods were used for limiting the memory usagedepending on the parser.
For example, in the Java-based pars-ers, the limit was set on the size of the Java heap.706Table 2.
Comparison of the parsing results for each sub-corpus and parser.
?Average?
column gives theaverage of the coverage figures for the six genres weighted according to the number of sentences in eachgenre.
The column labeled ?Generalizability?
shows the drop of the coverage in the lowest-scoring genrecompared to the coverage in the newspaper genre.
*SP experienced a coverage drop of tens of percentage points in comparison to other genres on the Hansard data-set.
This was caused mainly by a single issue: the dataset contained a number of sentences that contained only a sin-gle word ?
sentences such as ?Nay.
?, ?Agreed.
?, ?No.?
and so on.
Because no root node is assigned to D analysis bySP, the parser did not return any analysis for such sentences.
These sentences were omitted from the evaluation.When the sentences were included, the coverage on legislation data was 59.5% and the average was 73.4%.Table 3.
Breakdown of the failures.
All the resultsare reported as a percentage of the total number ofsentences.
Column ?Incomplete?
reports the pro-portion of sentences that were parsed, but theanalysis was not full.
Column ?Failed?
shows thosecases in which the parser was not able to return aparse.
Column ?Terminated?
shows the proportionof the cases in which the parser crashed or termi-nated during the process of parsing a sentence.While StatCCG outperformed C&C parser by4.1 percentage points in average coverage, the twoCCG-based parsers achieved a similar generaliza-bility.
StatCCG was the most stable parser in theexperiment.
It did not crash or terminate once onthe test data.The only parser based on a manually-constructed grammar, MINIPAR, had the lowestcoverage and generalizability.
MINIPAR alsoproved to have stability problems.
While thisparser achieved an 88.0% coverage with the news-paper corpus, its performance dropped over 10percentage points with other corpora.
Its coveragewas only 34.4% with the religion genre.
The mostcommonly occurring type of problem with thisdata was a fragmented analysis occasioned by sen-tences beginning with an ?And?
or ?Or?
that wasnot connected to any other words in the parse tree.5 ConclusionThis paper describes our experiments in parsingdiverse text types with five parsers operating withfour different grammar formalisms.
To our knowl-edge, this experiment is the only large-scale com-parison of the coverage of a set of parsers for Eng-lish reported in the literature.
On average, the pars-ing coverage of the five parsers on newspaper textswas 94.4%.
The average dropped from 5.6 to 15.2percentage points on the other five text genres.
Thelowest average scores were achieved on the relig-ion test set.In comparison to MINIPAR, the results indicatethat the coverage of the newer parsers has im-proved.
The good performance of the APP maypartly be explained by a rather poor preciseness:the rate of just over 70% is much lower than that ofother parsers.
APP also produces a shallow analy-sis that enables it to achieve a high coverage.One observation that should be made relates tothe user friendliness and documentation of theparsing systems.
The parsing of a vast collection oftexts using several parsing systems was neithersimple nor straightforward.
To begin with, most ofthe parsers crashed at least once during the courseof the experiments.
The C&C parser, for example,terminates when it encounters a sentence with twospaces between words.
It would be far more con-Parser Newspaper Legislation Fiction Non-fiction Religion Biomedi-cine AverageGener-alizabilityAPP 99.8 98.9 97.5 96.4 93.1 98.9 98.5 6.7C&C 87.8 84.9 86.0 81.2 75.5 84.8 85.0 14.0MINIPAR 88.0 68.8 68.0 71.5 34.4 70.1 72.1 60.9SP* 99.8 99.5 98.0 98.3 98.9 98.5 99.2 1.8StatCCG 96.7 85.2 87.7 86.7 94.0 83.3 89.1 13.9Average 94.4 87.5 87.4 86.8 79.2 87.1 88.8 19.5Parser Incomplete Failed TerminatedAPP 1.5 0.0 0.001C&C 12.8 2.2 0.006MINIPAR 27.9 0.0 0.009SP 0.5 0.4 0.002StatCCG 9.6 1.4 0.000Average 10.5 0.8 0.004707venient for users if such sentences were automati-cally skipped or normalized.While another feature is that all the parsers havea set of parameters that can be adjusted, the ac-companying documentation about their effects is inmany cases insufficiently detailed.
From the NLPpractitioner?s point of view, the process of select-ing an appropriate parser for a given task is com-plicated by the fact that the output format of aparser is frequently described in insufficient detail.It would also be useful in many NLP applicationsif the parser were able to indicate whether or not itcould parse a sentence completely.
It would also beoptimal if a confidence score indicating the reli-ability of the returned analysis could be provided.The most obvious directions for work of thiskind would include other text genres, larger collec-tions of texts and more parsers.
One could alsopinpoint the most problematic types of sentencestructures by applying error-mining techniques tothe results of the experiments.ReferencesTimothy Baldwin, Emily M. Bender, Dan Flickinger,Ara Kim, and Stephan Oepen.
2004.
Road-testing theEnglish Resource Grammar over the British NationalCorpus.
In Proceedings of the 4th Language Re-sources and Evaluation Conference (LREC), Lisbon,Portugal.Douglas Biber.
1993.
Using Register-diversified Cor-pora for General Language Studies.
ComputationalLinguistics, 19(2):219?241.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and Log-Linear Models.
In Pro-ceedings of the 42nd ACL, Barcelona, Spain.Andrew B. Clegg and Adrian J. Shepherd.
2005.
Evalu-ating and Integrating Treebank Parsers on a Bio-medical Corpus.
In Proceedings of the Workshop onSoftware at the 43rd ACL, Ann Arbor, Michigan,USA.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the 5th LREC, Genoa, Italy.Daniel Gildea.
2001.
Corpus Variation and Parser Per-formance.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing,Pittsburgh, Pennsylvania, USA.Julia Hockenmaier.
2003.
Data and Models for Statisti-cal Parsing with Combinatory Categorial Grammar.PhD Dissertation, University of Edinburgh, UK.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of the 41stACL, Sapporo, Japan.Aarre Laakso.
On Parsing CHILDES.
2005.
In Proceed-ings of the Second Midwest Computational Linguis-tics Colloquium.
Columbus, Ohio, USA,David Lee.
2001.
Genres, Registers, Text Types, Do-mains, and Styles: Clarifying the Concepts and Navi-gating a Path through the BNC Jungle.
LanguageLearning & Technology, 5(3):37?72.Dekang Lin.
1998.
Dependency-Based Evaluation ofMINIPAR.
In Proceedings of the 1st LREC, Gra-nada, Spain.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Alessandro Mazzei and Vincenzo Lombardo.
2004.
AComparative Analysis of Extracted Grammars.
InProceedings of the 16th European Conference on Ar-tificial Intelligence, Valencia, Spain.Rashmi Prasad and Anoop Sarkar.
2000.
ComparingTest-suite Based Evaluation and Corpus-basedEvaluation of a Wide Coverage Grammar for Eng-lish.
In Proceedings of the Using Evaluation withinHLT Programs: Results and Trends Workshop at the2nd LREC, Athens, Greece.Geoffrey Sampson, editor.
1995.
English for the Com-puter: The Susanne Corpus and Analytic Scheme.Oxford University Press, Oxford, UK.Satoshi Sekine.
1997.
The Domain Dependence of Pars-ing.
In Proceedings of the 5th Conference on AppliedNatural Language Processing, Washington, DC,USA.Satoshi Sekine.
1998.
Corpus-based Parsing and Sub-language Studies.
PhD Thesis.
New York University,New York, USA.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal Dependency Analysis with Support Vector Ma-chines.
In Proceedings of the 8th International Work-shop on Parsing Technologies, Nancy, France.708
