1A quantitative evaluation of naturalistic models of language acquisition; theefficiency of the Triggering Learning Algorithm compared to a CategorialGrammar LearnerPaula ButteryNatural Language and Information Processing Group,Computer Laboratory, Cambridge University,15 JJ Thomson Avenue, Cambridge, CB3 0FD, UKpaula.buttery@cl.cam.ac.ukAbstractNaturalistic theories of language acquisition assumelearners to be endowed with some innate languageknowledge.
The purpose of this innate knowledgeis to facilitate language acquisition by constrain-ing a learner?s hypothesis space.
This paper dis-cusses a naturalistic learning system (a CategorialGrammar Learner (CGL)) that differs from previouslearners (such as the Triggering Learning Algorithm(TLA) (Gibson and Wexler, 1994)) by employing adynamic definition of the hypothesis-space whichis driven by the Bayesian Incremental ParameterSetting algorithm (Briscoe, 1999).
We comparethe efficiency of the TLA with the CGL when ac-quiring an independently and identically distributedEnglish-like language in noiseless conditions.
Weshow that when convergence to the target gram-mar occurs (which is not guaranteed), the expectednumber of steps to convergence for the TLA isshorter than that for the CGL initialized with uni-form priors.
However, the CGL converges morereliably than the TLA.
We discuss the trade-off ofefficiency against more reliable convergence to thetarget grammar.1 IntroductionA normal child acquires the language of her envi-ronment without any specific training.
Chomsky(1965) claims that, given the ?relatively slight ex-posure?
to examples and ?remarkable complexity?of language, it would be ?an extraordinary intellec-tual achievement?
for a child to acquire a languageif not specifically designed to do so.
His Argumentfrom the Poverty of the Stimulus suggests that if weknow X, and X is undetermined by learning expe-rience then X must be innate.
For an example con-sider structure dependency in language syntax:A question in English can be formed by invert-ing the auxiliary verb and subject noun-phrase: (1a)?Dinah was drinking a saucer of milk?
; (1b) ?wasDinah drinking a saucer of milk?
?Upon exposure to this example, a child could hy-pothesize infinitely many question-formation rules,such as: (i) swap the first and second words in thesentence; (ii) front the first auxiliary verb; (iii) frontwords beginning with w.The first two of these rules are refuted if the childencounters the following: (2a) ?the cat who wasgrinning at Alice was disappearing?
; (2b) ?was thecat who was grinning at Alice disappearing?
?If a child is to converge upon the correct hypoth-esis unaided she must be exposed to sufficient ex-amples so that all false hypotheses are refuted.
Un-fortunately such examples are not readily availablein child-directed speech; even the constructions inexamples (2a) and (2b) are rare (Legate, 1999).
Tocompensate for this lack of data Chomsky suggeststhat some principles of language are already avail-able in the child?s mind.
For example, if the childhad innately ?known?
that all grammar rules arestructurally-dependent upon syntax she would neverhave hypothesized rules (i) and (iii).
Thus, Chom-sky theorizes that a human mind contains a Univer-sal Grammar which defines a hypothesis-space of?legal?
grammars.1 This hypothesis-space must beboth large enough to contain grammar?s for all ofthe world?s languages and small enough to ensuresuccessful acquisition given the sparsity of data.Language acquisition is the process of searching thehypothesis-space for the grammar that most closelydescribes the language of the environment.
Withestimates of the number of living languages beingaround 6800 (Ethnologue, 2004) it is not sensible tomodel the hypothesis-space of grammars explicitly,rather it must be modeled parametrically.
Languageacquisition is then the process of setting these pa-rameters.
Chomsky (1981) suggested that param-eters should represent points of variation betweenlanguages, however the only requirement for pa-rameters is that they define the current hypothesis-space.1Discussion of structural dependence as evidence of the Ar-gument from the Poverty of Stimulus is illustrative, the sig-nificance being that innate knowledge in any form will placeconstraints on the hypothesis-space2The properties of the parameters used by thislearner (the CGL) are as follows: (1) Parameters arelexical; (2) Parameters are inheritance based; (3) Pa-rameter setting is statistical.1 - Lexical ParametersThe CGL employs parameter setting as a meansto acquire a lexicon; differing from other paramet-ric learners, (such as the Triggering Learning Al-gorithm (TLA) (Gibson and Wexler, 1994) and theStructural Triggers Learner (STL) (Fodor, 1998b),(Sakas and Fodor, 2001)) which acquire generalsyntactic information rather than the syntactic prop-erties associated with individual words.2In particular, a categorial grammar is acquired.The syntactic properties of a word are contained inits lexical entry in the form of a syntactic category.A word that may be used in multiple syntactic situ-ations (or sub-categorization frames) will have mul-tiple entries in the lexicon.Syntactic categories are constructed from a finiteset of primitive categories combined with two op-erators (/ and \) and are defined by their membersability to combine with other constituents; thus con-stituents may be thought of as either functions orarguments.The arguments of a functional constituent areshown to the right of the operators and the resultto the left.
The forward slash operator (/) indicatesthat the argument must appear to the right of thefunction and a backward slash (\) indicates that itmust appear on the left.
Consider the followingCFG structure which describes the properties of atransitive verb:s ?
np vpvp ?
tv nptv ?
gets, finds, ...Assume that there is a set of primitive categories{s,np}.
A vp must be in the category of func-tional constituents that takes a np from the left andreturns an s. This can be written s\np.
Likewisea tv takes an np from the right and returns a vp(whose type we already know).
A tv may be writ-ten (s\np)/np.Rules may be used to combine categories.
Weassume that our learner is innately endowed with therules of function application, function compositionand generalized weak permutation (Briscoe, 1999)(see figures 1 and 2).?
Forward Application (>)X/Y Y ?
X2The concept of lexical parameters and the lexical-linkingof parameters is to be attributed to Borer (1984).?
Backward Application (<)Y X\Y ?
X?
Forward Composition (> B)X/Y Y/Z ?
X/Z?
Backward Composition (< B)Y \X Z\Y ?
X\Z?
Generalized Weak Permutation (P )((X | Y1)... | Yn) ?
((X | Yn)... | Y1)where | is a variable over \ and /.Alicenpmay(s\np)/(s\np)eat(s\np)/np> B(s\np)/npthe cake??
?np>s\np<sFigure 1: Illustration of forward/backward applica-tion (>, <) and forward composition (> B)thenp/nrabbitnthat(n\n)/(s/np)shenpsaw(s\np)/npP(s/np)\np<(s/np)>n\n<n>npFigure 2: Illustration of generalized weak permuta-tion (P )The lexicon for a language will contain a finitesubset of all possible syntactic categories, the size ofwhich depends on the language.
Steedman (2000)suggests that for English the lexical functional cate-gories never need more than five arguments and thatthese are needed only in a limited number of casessuch as for the verb bet in the sentence I bet you fivepounds for England to win.The categorial grammar parameters of the CGLare concerned with defining the set of syntacticcategories present in the language of the environ-ment.
Converging on the correct set aids acquisitionby constraining the learner?s hypothesized syntacticcategories for an unknown word.
A parameter (with3value of either ACTIVE or INACTIVE) is associ-ated with every possible syntactic category to indi-cate whether the learner considers the category to bepart of the target grammar.Some previous parametric learners (TLA andSTL) have been primarily concerned with overallsyntactic phenomena rather than the syntactic prop-erties of individual words.
Movement parameters(such as the V 2 parameter of the TLA) may be cap-tured by the CGL using innate rules or multiple lex-ical entries.
For instance, Dutch and German wordorder is captured by assuming that verbs in theselanguages systematically have two categories, onedetermining main clause order and the other subor-dinate clause orders.2 - Inheritance Based ParametersThe complex syntactic categories of a categorialgrammar are a sub-categorization of simpler cate-gories; consequently categories may be arranged ina hierarchy with more complex categories inheritingfrom simpler ones.
Figure 3 shows a fragment of apossible hierarchy.
This hierarchical organization ofparameters provides the learner with several bene-fits: (1) The hierarchy can enforce an order on learn-ing; constraints may be imposed such that a parentparameter must be acquired before a child parame-ter (for example, in Figure 3, the learner must ac-quire intransitive verbs before transitive verbs maybe hypothesized).
(2) Parameter values may be in-herited as a method of acquisition.
(3) The parame-ters are stored efficiently.s - ACTIVE``````s/s s\np - ACTIVEXXXXX[s\np]/np - ACTIVE [s\np]/[s\np]Figure 3: Partial hierarchy of syntactic categories.Each category is associated with a parameter indi-cating either ACTIVE or INACTIVE status.3 - Statistical Parameter SettingThe learner uses a statistical method to track rela-tive frequencies of parameter-setting-utterances inthe input.3 We use the Bayesian Incremental Pa-rameter Setting (BIPS) algorithm (Briscoe, 1999)to set the categorial parameters.
Such an approachsets the parameters to the values that are most likelygiven all the accumulated evidence.
This represents3Other statistical parameter setting models include Yang?sVariational model (2002) and the Guessing STL (Fodor, 1998a)a compromise between two extremes: implementa-tions of the TLA are memoryless allowing a param-eter values to oscillate; some implementations of theSTL set a parameter once, for all time.Using the BIPS algorithm, evidence from an in-put utterance will either strengthen the current pa-rameter settings or weaken them.
Either way, thereis re-estimation of the probabilities associated withpossible parameter values.
Values are only assignedwhen sufficient evidence has been accumulated, i.e.once the associated probability reaches a thresholdvalue.
By employing this method, it becomes un-likely for parameters to switch between settings asthe consequence of an erroneous utterance.Another advantage of using a Bayesian approachis that we may set default parameter values by as-signing Bayesian priors; if a parameter?s defaultvalue is strongly biased against the accumulated ev-idence then it will be difficult to switch.
Also, we nolonger need to worry about ambiguity in parameter-setting-utterances (Clark, 1992) (Fodor, 1998b): theBayesian approach allows us to solve this problem?for free?
since indeterminacy just becomes anothercase of error due to misclassification of input data(Buttery and Briscoe, 2004).2 Overview of the Categorial GrammarLearnerThe learning system is composed of a three mod-ules: a semantics learning module, syntax learningmodule and memory module.
For each utteranceheard the learner receives an input stream of wordtokens paired with possible semantic hypotheses.For example, on hearing the utterance ?Dinah drinksmilk?
the learner may receive the pairing: ({dinah,drinks, milk}, drinks(dinah, milk)).2.1 The Semantic ModuleThe semantic module attempts to learn the mappingbetween word tokens and semantic symbols, build-ing a lexicon containing the meaning associatedwith each word sense.
This is achieved by analyz-ing each input utterance and its associated semantichypotheses using cross-situational techniques (fol-lowing Siskind (1996)).For a trivial example consider the utterances ?Al-ice laughs?
and ?Alice eats cookies?
; they mighthave word tokens paired with semantic expressionsas follows: ({alice, laughs}, laugh(alice)), ({alice,eats, cookies}, eat(alice, cookies)).From these two utterances it is possible to ascer-tain that the meaning associated with the word tokenalice must be alice since it is the only semantic ele-ment that is common to both utterances.42.2 The Syntactic ModuleThe learning system links the semantic module andsyntactic module by using a typing assumption: thesemantic arity of a word is usually the same as itsnumber of syntactic arguments.
For example, if it isknown that likes maps to like(x,y), then the typ-ing assumption suggests that its syntactic categorywill be in one of the following forms: a\b\c, a/b\c,a\b/c, a/b/c or more concisely a | b | c (where a, band c may be basic or complex syntactic categoriesthemselves).By employing the typing assumption the numberof arguments in a word?s syntactic category can behypothesized.
Thus, the objective of the syntacticmodule is to discover the arguments?
category typesand locations.The module attempts to create valid parse treesstarting from the syntactic information already as-sumed by the typing assumption (following But-tery (2003)).
A valid parse is one that adheresto the rules of the categorial grammar as well asthe constraints imposed by the current settings ofthe parameters.
If a valid parse can not be foundthe learner assumes the typing assumption to havefailed and backtracks to allow type raising.2.3 Memory ModuleThe memory module records the current state ofthe hypothesis-space.
The syntactic module refersto this information to place constraints upon whichsyntactic categories may be hypothesized.
Themodule consists of two hierarchies of parameterswhich may be set using the BIPS algorithm:Categorial Parameters determine whether a cat-egory is in use within the learner?s current modelof the input language.
An inheritance hierarchy ofall possible syntactic categories (for up to five argu-ments) is defined and a parameter associated witheach one (Villavicencio, 2002).
Every parameter(except those associated with primitive categoriessuch as S) is originally set to INACTIVE, i.e.
nocategories (except primitives) are known upon thecommencement of learning.
A categorial parametermay only be set to ACTIVE if its parent categoryis already active and there has been satisfactory ev-idence that the associated category is present in thelanguage of the environment.WordOrder Parameters determine the underly-ing order in which constituents occur.
They may beset to either FORWARD or BACKWARD depend-ing on whether the constituents involved are gen-erally located to the right or left.
An example isthe parameter that specifies the direction of the sub-ject of a verb: if the language of the environmentis English this parameter would be set to BACK-WARD since subjects generally appear to the left ofthe verb.
Evidence for the setting of word order pa-rameters is collected from word order statistics ofthe input language.3 The acquisition of an English-typelanguageThe English-like language of the three-parametersystem studied by Gibson and Wexler has theparameter settings and associated unembeddedsurface-strings as shown in Figure 4.
For this taskwe assume that the surface-strings of the English-like language are independent and identically dis-tributed in the input to the learner.Specifier Complement V20 (Left) 1 (Right) 0 (off )1.
Subj Verb2.
Subj Verb Obj3.
Subj Verb Obj Obj4.
Subj Aux Verb5.
Subj Aux Verb Obj6.
Subj Aux Verb Obj Obj7.
Adv Subj Verb8.
Adv Subj Verb Obj9.
Adv Subj Verb Obj Obj10.
Adv Subj Aux Verb11.
Adv Subj Aux Verb Obj12.
Adv Subj Aux Verb Obj ObjFigure 4: Parameter settings and surface-strings ofGibson and Wexler?s English-like Language.3.1 Efficiency of Trigger Learning AlgorithmFor the TLA to be successful it must converge tothe correct parameter settings of the English-likelanguage.
Berwick and Niyogi (1996) modeled theTLA as a Markov process (see Figure 5).Using this model it is possible to calculate theprobability of converging to the target from eachstarting grammar and the expected number of stepsbefore convergence.Probability of Convergence:Consider starting from Grammar 3, after the processfinishes looping it has a 3/5 probability of mov-ing to Grammar 4 (from which it will never con-verge) and a 2/5 probability of moving to Grammar7 (from which it will definitely converge), thereforethere is a 40% probability of converging to the targetgrammar when starting at Grammar 3.5Expected number of Steps to Convergence:Let Sn be the expected number of steps from staten to the target state.
For starting grammars 6, 7 and8, which definitely converge, we know:S6 = 1 +56S6 (1)S7 = 1 +23S7 +118S8 (2)S8 = 1 +112S6 +136S7 +89S8 (3)and for the times when we do converge from gram-mars 3 and 1 we can expect:S1 = 1 +35S1 (4)S3 = 1 +3133S3 (5)Figure 6 shows the probability of convergence andexpected number of steps to convergence for eachof the starting grammars.
The expected number ofsteps to convergence ranges from infinity (for start-ing grammars 2 and 4) down to 2.5 for Grammar1.
If the distribution over the starting grammars isuniform then the overall probability of convergingis the sum of the probabilities of converging fromeach state divided by the total number of states:1.00 + 1.00 + 1.00 + 1.00 + 0.40 + 0.668 = 0.63(6)and the expected number of steps given that youconverge is the weighted average of the number ofsteps from each possibly converging state:5.47 + 14.87 + 6 + 21.98 ?
0.4 + 2.5 ?
0.661.00 + 1.00 + 1.00 + 1.00 + 0.40 + 0.66 = 7.26(7)3.2 Efficiency of Categorial Grammar LearnerThe input data to the CGL would usually be an ut-terance annotated with a logical form; the only dataavailable here however, is surface-strings consist-ing of word types.
Hence, for the purpose of com-parison with the TLA the semantic module of ourlearner is by-passed; we assume that mappings tosemantic forms have previously been acquired andthat the subject and objects of surface-strings areknown.
For example, given surface-string 1 (SubjVerb) we assume the mapping Verb 7?
verb(x),which provides Verbwith a syntactic category of theform a|b by the typing assumption (where a, b areunknown syntactic categories and | is an operatorover \ and /); we also assume Subj to map to a prim-itive syntactic category SB, since it is the subject ofVerb.The criteria for success for the CGL when acquir-ing Gibson and Wexler?s English-like language is alexicon containing the following:4Adv S/S Aux [S\SB]/[S\SB]Obj OB Verb S\SBSubj SB [S\SB]/OB[[S\SB]/OB]/OBwhere S (sentence), SB (subject) and OB (ob-ject) are primitive categories which are innate to thelearner with SB and OB assumed to be derivablefrom the semantic module.During the learning process the CGL will haveconstructed a category hierarchy by setting appro-priate categorial parameters to true (see Figure 7).The learner will have also constructed a word-orderhierarchy (Figure 8), setting parameters to FOR-WARDor BACKWARD.
These hierarchies are usedduring the learning process to constrain hypothe-sized syntactic categories.
For this task the set-ting of the word-order parameters becomes trivialand their role in constraining hypotheses negligible;consequently, the rest of our argument will relate tocategorial parameters only.
For the purpose of thisgendir = /aaa!!
!subjdir = \ vargdir = /Figure 8: Word-order parameter settings required toparse Gibson and Wexler?s English-like language.analysis parameters are initialized with uniform pri-ors and are originally set INACTIVE.
Since the in-put is noiseless, the switching threshold is set suchthat parameters may be set ACTIVE upon the evi-dence from one surface-string.It is a requirement of the parameter setting de-vice that the parent-types of hypothesized syntaxcategories are ACTIVE before new parameters areset.
Thus, the learner is not allowed to hypoth-esize the syntactic category for a transitive verb[[S\SB]/OB] before it has learnt the category foran intransitive verb [S\SB]; this behaviour con-strains over-generation.
Additionally, it is usuallynot possible to derive a word?s full syntactic cate-gory (i.e.
without any remaining unknowns) unlessit is the only new word in the clause.As a consequence of these issues, the order inwhich the surface-strings appear to the learner af-4Note that the lexicon would usually contain orthographicentries for the words in the language rather than word type en-tries.6fects the speed of acquisition.
For instance, thelearner prefers to see the surface-string Subj Verbbefore Subj Verb Obj so that it can acquire themaximum information without wasting any strings.For the English-type language described by Gib-son and Wexler the learner can optimally acquirethe whole lexicon after seeing only 5 surface-strings(one string needed for each new complex syntacticcategory to be learnt).
However, the strings appearto the learner in a random order so it is necessary tocalculate the expected number of strings (or steps)before convergence.The learner must necessarily see the string SubjVerb before it can learn any other information.
With12 surface-strings the probability of seeing SubjVerb is 1/12 and the expected number of strings be-fore it is seen is 12.
The learner can now learn from3 surface-strings: Subj Verb Obj, Subj Aux Verb andAdv Subj Verb.
Figure 9 shows a Markov structureof the process.
From the model we can calculate theexpected number of steps to converge to be 24.53.4 ConclusionsThe TLA and CGL were compared for efficiency(expected number of steps to convergence) whenacquiring the English-type grammar of the three-parameter system studied by Gibson and Wexler.The expected number of steps for the TLA wasfound to be 7.26 but the algorithm only converged63% of the time.
The expected number of steps forthe CGL is 24.53 but the learner converges more re-liably; a trade off between efficiency and success.With noiseless input the CGL can only fail if thereis insufficient input strings or if Bayesian priors areheavily biased against the target.
Furthermore, theCGL can be made robust to noise by increasing theprobability threshold at which a parameter may beset ACTIVE; the TLA has no mechanism for copingwith noisy data.The CGL learns incrementally; the hypothesis-space from which it can select possible syntacticcategories expands dynamically and, as a conse-quence of the hierarchical structure of parameters,the speed of acquisition increases over time.
Forinstance, in the starting state there is only a 1/12probability of learning from surface-strings whereasin state k (when all but one category has been ac-quired) there is a 1/2 probability.
It is likely thatwith a more complex learning task the benefits ofthis incremental approach will outweigh the slowstarting costs.
Related work on the effects of incre-mental learning on STL performance (Sakas, 2000)draws similar conclusions.
Future work hopes tocompare the CGL with other parametric learners(such as the STL) in larger domains.ReferencesR Berwick and P Niyogi.
1996.
Learning from trig-gers.
Linguistic Inquiry, 27(4):605?622.H Borer.
1984.
Parametric Syntax: Case Studiesin Semitic and Romance Languages.
Foris, Dor-drecht.E Briscoe.
1999.
The acquisition of grammar in anevolving population of language agents.
MachineIntelligence, 16.P Buttery and T Briscoe.
2004.
The significance oferrors to parametric models of language acquisi-tion.
Technical Report SS-04-05, American As-sociation of Artificial Intelligence, March.P Buttery.
2003.
A computational model for firstlanguage acquisition.
In CLUK-6, Edinburgh.N Chomsky.
1965.
Aspects of the Theory of Syntax.MIT Press.N Chomsky.
1981.
Lectures on Government andBinding.
Foris Publications.R Clark.
1992.
The selection of syntactic knowl-edge.
Language Acquisition, 2(2):83?149.Ethnologue.
2004.
Languages of theworld, 14th edition.
SIL International.http://www.ethnologue.com/.J Fodor.
1998a.
Parsing to learn.
Journal of Psy-cholinguistic Research, 27(3):339?374.J Fodor.
1998b.
Unambiguous triggers.
LinguisticInquiry, 29(1):1?36.E Gibson and K Wexler.
1994.
Triggers.
LinguisticInquiry, 25(3):407?454.J Legate.
1999.
Was the argument that was madeempirical?
Ms, Massachusetts Institute of Tech-nology.W Sakas and J Fodor.
2001.
The structural triggerslearner.
In S Bertolo, editor, Language Acquisi-tion and Learnability, chapter 5.
Cambridge Uni-versity Press, Cambridge, UK.W Sakas.
2000.
Ambiguity and the ComputationalFeasibility of Syntax Acquisition.
Ph.D. thesis,City University of New York.J Siskind.
1996.
A computational study ofcross situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):39?91,Nov/Oct.M Steedman.
2000.
The Syntactic Process.
MITPress/Bradford Books.A Villavicencio.
2002.
The acquisition of aunification-based generalised categorial gram-mar.
Ph.D. thesis, University of Cambridge.C Yang.
2002.
Knowledge and Learning in NaturalLanguage.
Oxford University Press.7Figure 5: Gibson and Wexler?s TLA as a Markov structure.
Circles represent possible grammars (a config-uration of parameter settings).
The target grammar lies at the centre of the structure.
Arrows represent thepossible transitions between grammars.
Note that the TLA is constrained to only allow movement betweengrammars that differ by one parameter value.
The probability of moving between Grammar Gi and Gram-marGj is a measure of the number of target surface-strings that are inGj but not Gi normalized by the totalnumber of target surface-strings as well as the number of alternate grammars the learner can move to.
Forexample the probability of moving from Grammar 3 to Grammar 7 is 2/12 ?
1/3 = 1/18 since there are 2target surface-strings allowed by Grammar 7 that are not allowed by Grammar 3 out of a possible of 12 andthree grammars that differ from Grammar 3 by one parameter value.Initial Language Initial Grammar Prob.
of Converging Expected no.
of StepsVOS -V2 110 0.66 2.50VOS +V2 111 0.00 n/aOVS -V2 100 0.40 21.98OVS +V2 101 0.00 n/aSVO -V2 010 1.00 0.00SVO +V2 011 1.00 6.00SOV -V2 000 1.00 5.47SOV +V2 001 1.00 14.87Figure 6: Probability and expected number of steps to convergence from each starting grammar to anEnglish-like grammar (SVO -V2) when using the TLA.8top``````!!
!SB OB S``````S/S S\SBXXXXX[S\SB]/OB[[S\SB]/OB]/OB[S\SB]/[S\SB]Figure 7: Category hierarchy required to parse Gibson and Wexler?s English-like language.Figure 9: The CGL as a Markov structure.
The states represent the set of known syntactic cate-gories: state S - {}, state a - {S\SB}, state b - {S\SB, S/S}, state c - {S\SB, [S\SB]/OB},state d - {S\SB, [S\SB]/[S\SB]}, state e - {S\SB, S/S, [S\SB]/OB}, state f - {S\SB,[S\SB]/OB, [[S\SB]/OB]/OB}, state g - {S\SB, [S\SB]/[S\SB], S/S} state h - {S\SB,[S\SB]/[S\SB], [S\SB]/OB}, state i - {S\SB, S/S, [S\SB]/OB, [S\SB]/[S\SB]}, state j -{S\SB, S/S, [S\SB]/OB, [[S\SB]/OB]/OB}, state k - {S\SB, [S\SB]/OB, [[S\SB]/OB]/OB,[S\SB]/[S\SB]}, state l - {S\SB, [S\SB]/OB, [[S\SB]/OB]/OB, [S\SB]/[S\SB], S/S}.
