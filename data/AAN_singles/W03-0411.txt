Preposition Semantic Classification via PENN TREEBANK and FRAMENETTom O?HaraDepartment of Computer ScienceNew Mexico State UniversityLas Cruces, NM 88003tomohara@cs.nmsu.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260wiebe@cs.pitt.eduAbstractThis paper reports on experiments in clas-sifying the semantic role annotations as-signed to prepositional phrases in both thePENN TREEBANK and FRAMENET.
Inboth cases, experiments are done to seehow the prepositions can be classifiedgiven the dataset?s role inventory, usingstandard word-sense disambiguation fea-tures.
In addition to using traditional wordcollocations, the experiments incorporateclass-based collocations in the form ofWordNet hypernyms.
For Treebank, theword collocations achieve slightly betterperformance: 78.5% versus 77.4% whenseparate classifiers are used per preposi-tion.
When using a single classifier forall of the prepositions together, the com-bined approach yields a significant gain at85.8% accuracy versus 81.3% for word-only collocations.
For FrameNet, thecombined use of both collocation typesachieves better performance for the indi-vidual classifiers: 70.3% versus 68.5%.However, classification using a singleclassifier is not effective due to confusionamong the fine-grained roles.1 IntroductionEnglish prepositions convey important relations intext.
When used as verbal adjuncts, they are the prin-ciple means of conveying semantic roles for the sup-porting entities described by the predicate.
Preposi-tions are highly ambiguous.
A typical collegiate dic-tionary has dozens of senses for each of the commonprepositions.
These senses tend to be closely related,in contrast to the other parts of speech where theremight be a variety of distinct senses.Given the recent advances in word-sense disam-biguation, due in part to SENSEVAL (Edmonds andCotton, 2001), it would seem natural to apply thesame basic approach to handling the disambiguationof prepositions.
Of course, it is difficult to disam-biguate prepositions at the granularity present in col-legiate dictionaries, as illustrated later.
Nonetheless,in certain cases this is feasible.We provide results for disambiguating preposi-tions at two different levels of granularity.
Thecoarse granularity is more typical of earlier work incomputational linguistics, such as the role inventoryproposed by Fillmore (1968), including high-levelroles such as instrument and location.
Recently, sys-tems have incorporated fine-grained roles, often spe-cific to particular domains.
For example, in the CycKB there are close to 200 different types of seman-tic roles.
These range from high-level roles (e.g.,beneficiaries) through medium-level roles (e.g., ex-changes) to highly specialized roles (e.g., catalyst).1Preposition classification using two different se-mantic role inventories are investigated in this pa-per, taking advantage of large annotated corpora.After providing background to the work in Sec-tion 2, experiments over the semantic role anno-tations are discussed in Section 3.
The resultsover TREEBANK (Marcus et al, 1994) are coveredfirst.
Treebank include about a dozen high-levelroles similar to Fillmore?s.
Next, experiments us-ing the finer-grained semantic role annotations inFRAMENET version 0.75 (Fillmore et al, 2001) are1Part of the Cyc KB is freely available at www.opencyc.org.presented.
FrameNet includes over 140 roles, ap-proaching but not quite as specialized as Cyc?s in-ventory.
Section 4 follows with a comparison torelated work, emphasizing work in broad-coveragepreposition disambiguation.2 Background2.1 Semantic roles in the PENN TREEBANKThe second version of the Penn Treebank (Marcuset al, 1994) added additional clause usage informa-tion to the parse tree annotations that are popularfor natural language learning.
This includes a fewcase-style relation annotations, which prove usefulfor disambiguating prepositions.
For example, hereis a simple parse tree with the new annotation for-mat:(S (NP-TPC-5 This)(NP-SBJ every man)(VP contains(NP *T*-5)(PP-LOC within(NP him))))This shows that the prepositional phrase (PP) is pro-viding the location for the state described by the verbphrase.
Treating this as the preposition sense wouldyield the following annotation:This every man contains withinLOChimThe main semantic relations in TREEBANK arebeneficiary, direction, spatial extent, manner, loca-tion, purpose/reason, and temporal.
These tags canbe applied to any verb complement but normally oc-cur with clauses, adverbs, and prepositions.
Fre-quency counts for the prepositional phrase (PP) caserole annotations are shown in Table 1.The frequencies for the most frequent preposi-tions that have occurred in the prepositional phraseannotations are shown later in Table 7.
The tableis ordered by entropy, which measures the inherentambiguity in the classes as given by the annotations.Note that the Baseline column is the probability ofthe most frequent sense, which is a common esti-mate of the lower bound for classification experi-ments.2.2 Semantic roles in FRAMENETBerkeley?s FRAMENET (Fillmore et al, 2001)project provides the most recent large-scale anno-tation of semantic roles.
These are at a much finergranularity than those in TREEBANK, so they shouldprove quite useful for applications that learn detailedsemantics from corpora.
Table 2 shows the top se-mantic roles by frequency of annotation.
This il-lustrates that the semantic roles in Framenet can bequite specific, as in the roles cognizer, judge, andaddressee.
In all, there are over 140 roles annotatedwith over 117,000 tagged instances.FRAMENET annotations occur at the phrase levelinstead of the grammatical constituent level as inTREEBANK.
The cases that involve prepositionalphrases can be determined by the phrase-type at-tribute of the annotation.
For example, consider thefollowing annotation.
?S TPOS=?56879338??
?T TYPE=?sense2??
?/T?Itpnp hadvhd aat0 sharpaj0,pun pointedaj0 facenn1 andcjc?C FE=?BodP?
PT=?NP?
GF=?Ext?
?aat0 featheryaj0 tailnn1 thatcjt?/C?
?C TARGET=?y??
archedvvd?/C?
?C FE=?Path?
PT=?PP?
GF=?Comp?
?overavp?prp itsdps backnn1?/C?
.pun?/S?The constituent (C) tags identify the phrases thathave been annotated.
The target attribute indicatesthe predicating word for the overall frame.
Theframe element (FE) attribute indicates one of the se-mantic roles for the frame, and the phrase type (PT)attribute indicates the grammatical function of thephrase.
We isolate the prepositional phrase annota-tion and treat it as the sense of the preposition.
Thisyields the following annotation:It had a sharp, pointed face and a featherytail that arched overPathits back.The annotation frequencies for the most frequentprepositions are shown later in Table 8, again or-dered by entropy.
This illustrates that the role dis-tributions are more complicated, yielding higher en-tropy values on average.
In all, there are over 100prepositions with annotations, 65 with ten or moreinstances each.Tag Freq Descriptionpp-loc 17220 locativepp-tmp 10572 temporalpp-dir 5453 directionpp-mnr 1811 mannerpp-prp 1096 purpose/reasonpp-ext 280 spatial extentpp-bnf 44 beneficiaryTable 1: TREEBANK semantic roles for PP?s.
Tagis the label for the role in the annotations.
Freq isfrequency of the role occurrences.Tag Freq DescriptionSpkr 8310 speakerMsg 7103 messageSMov 6778 self-moverThm 6403 themeAgt 5887 agentGoal 5560 goalPath 5422 pathCog 4585 cognizerManr 4474 mannerSrc 3706 sourceCont 3662 contentExp 3567 experiencerEval 3108 evalueeJudge 3107 judgeTop 3074 topicOther 2531 undefinedCause 2306 causeAdd 2266 addresseeSrc-p 2179 perceptual sourcePhen 1969 phenomenonReas 1789 reasonArea 1328 areaDegr 1320 degreeBodP 1230 body partProt 1106 protagonistTable 2: Common FRAMENET semantic roles.
Thetop 25 of 141 roles are shown.3 Classification experimentsThe task of selecting the semantic roles for theprepositions can be framed as an instance of word-sense disambiguation (WSD), where the semanticroles serve as the senses for the prepositions.A straightforward approach for preposition dis-ambiguation would be to use standard WSD fea-tures, such as the parts-of-speech of surroundingwords and, more importantly, collocations (e.g., lex-ical associations).
Although this can be highly ac-curate, it will likely overfit the data and generalizepoorly.
To overcome these problems, a class-basedapproach is used for the collocations, with WordNethigh-level synsets as the source of the word classes.Therefore, in addition to using collocations in theform of other words, this uses collocations in theform of semantic categories.A supervised approach for word-sense disam-biguation is used following Bruce and Wiebe (1999).The results described here were obtained using thesettings in Figure 1.
These are similar to the set-tings used by O?Hara et al (2000) in the firstSENSEVAL competition, with the exception of thehypernym collocations.
This shows that for the hy-pernym associations, only those words that occurwithin 5 words of the target prepositions are con-sidered.2The main difference from that of a standard WSDapproach is that, during the determination of theclass-based collocations, each word token is re-placed by synset tokens for its hypernyms in Word-Net, several of which might occur more than once.This introduces noise due to ambiguity, but giventhe conditional-independence selection scheme, thepreference for hypernym synsets that occur for dif-ferent words will compensate somewhat.
O?Haraand Wiebe (2003) provide more details on the ex-traction of these hypernym collocations.
The fea-ture settings in Figure 1 are used in two differentconfigurations: word-based collocations alone, anda combination of word-based and hypernym-basedcollocations.
The combination generally produces2This window size was chosen after estimating that on aver-age the prepositional objects occur within 2.35+/?
1.26 wordsof the preposition and that the average attachment site is within3.0 +/?
2.98 words.
These figures were produced by ana-lyzing the parse trees for the semantic role annotations in thePENN TREEBANK.Features:POS?2 part-of-speech 2 words to leftPOS?1: part-of-speech 1 word to leftPOS+1: part-of-speech 1 word to rightPOS+2: part-of-speech 2 words to rightPrep preposition being classifiedWordColli: word collocation for role iHypernymColli: hypernym collocation for role iCollocation Context:Word: anywhere in the sentenceHypernym: within 5 words of target prepositionCollocation selection:Frequency: f(word) > 1CI threshold: p(c|coll)?p(c)p(c)>= 0.2Organization: per-class-binaryModel selection:overall classifier: Decision treeindividual classifiers: Naive Bayes10-fold cross-validationFigure 1: Feature settings used in the preposi-tion classification experiments.
CI refers to condi-tional independence; the per-class-binary organiza-tion uses a separate binary feature per role (Wiebe etal., 1998).the best results.
This exploits the specific clues pro-vided by the word collocations while generalizing tounseen cases via the hypernym collocations.3.1 PENN TREEBANKTo see how these conceptual associations are de-rived, consider the differences in the prior versusclass-based conditional probabilities for the seman-tic roles of the preposition ?at?
in TREEBANK.
Ta-ble 3 shows the global probabilities for the roles as-signed to ?at?.
Table 4 shows the conditional prob-Relation P(R) Examplelocative .732 workers at a factorytemporal .239 expired at midnight Tuesdaymanner .020 has grown at a sluggish pacedirection .006 CDs aimed at individual investorsTable 3: Prior probabilities of semantic relations for?at?
in TREEBANK.
P (R) is the relative frequency.Example usages are taken from the corpus.Category Relation P(R|C)ENTITY#1 locative 0.86ENTITY#1 temporal 0.12ENTITY#1 other 0.02ABSTRACTION#6 locative 0.51ABSTRACTION#6 temporal 0.46ABSTRACTION#6 other 0.03Table 4: Sample conditional probabilities of seman-tic relations for ?at?
in TREEBANK.
Category isWordNet synset defining the category.
P (R|C) isprobability of the relation given that the synset cate-gory occurs in the context.Relation P(R) Exampleaddressee .315 growled at the attendantother .092 chuckled heartily at this admissionphenomenon .086 gazed at him with disgustgoal .079 stationed a policeman at the gatecontent .051 angry at her stubbornnessTable 5: Prior probabilities of semantic relations for?at?
in FRAMENET for the top 5 of 40 applicableroles.Category Relation P(R|C)ENTITY#1 addressee 0.28ENTITY#1 goal 0.11ENTITY#1 phenomenon 0.10ENTITY#1 other 0.09ENTITY#1 content 0.03ABSTRACTION#6 addressee 0.22ABSTRACTION#6 other 0.14ABSTRACTION#6 goal 0.12ABSTRACTION#6 phenomenon 0.08ABSTRACTION#6 content 0.05Table 6: Sample conditional probabilities of seman-tic relations for ?at?
in FRAMENETabilities for these roles given that certain high-levelWordNet categories occur in the context.
These cat-egory probability estimates were derived by tabulat-ing the occurrences of the hypernym synsets for thewords occurring within a 5-word window of the tar-get preposition.
In a context with a concrete concept(ENTITY#1), the difference in the probability dis-tributions shows that the locative interpretation be-comes even more likely.
In contrast, in a contextwith an abstract concept (ABSTRACTION#6), thedifference in the probability distributions shows thatthe temporal interpretation becomes more likely.Therefore, these class-based lexical associations re-flect the intuitive use of the prepositions.The classification results for these prepositionsin the PENN TREEBANK show that this approach isvery effective.
Table 9 shows the results when allof the prepositions are classified together.
Unlikethe general case for WSD, the sense inventory isthe same for all the words here; therefore, a sin-gle classifier can be produced rather than individ-ual classifiers.
This has the advantage of allowingmore training data to be used in the derivation ofthe clues indicative of each semantic role.
Good ac-curacy is achieved when just using standard wordcollocations.
Table 9 also shows that significantimprovements are achieved using a combination ofboth types of collocations.
For the combined case,the accuracy is 86.1%, using Weka?s J48 classifier(Witten and Frank, 1999), which is an implementa-tion of Quinlan?s (1993) C4.5 decision tree learner.For comparison, Table 7 shows the results for indi-vidual classifiers created for each preposition (usingNaive Bayes).
In this case, the word-only colloca-tions perform slightly better: 78.5% versus 77.8%accuracy.3.2 FRAMENETIt is illustrative to compare the prior probabilities(i.e., P(R)) for FRAMENET to those seen earlierfor ?at?
in TREEBANK.
See Table 5 for the mostfrequent roles out of the 40 cases that were as-signed to it.
This highlights a difference betweenthe two sets of annotations.
The common tempo-ral role from TREEBANK is not directly representedin FRAMENET, and it is not subsumed by anotherspecific role.
Similarly, there is no direct role cor-responding to locative, but it is partly subsumed byDataset StatisticsInstances 26616Classes 7Entropy 1.917Baseline 0.480Experiment Accuracy STDEVWord Only 81.1 .996Combined 86.1 .491Table 9: Overall results for preposition disambigua-tion with TREEBANK semantic roles.
Instances isthe number of role annotations.
Classes is thenumber of distinct roles.
Entropy measures non-uniformity of the role distributions.
Baseline selectsthe most-frequent role.
The Word Only experimentjust uses word collocations, whereas Combined usesboth word and hypernym collocations.
Accuracy isaverage for percent correct over ten trials in crossvalidation.
STDEV is the standard deviation over thetrails.
The difference in the two experiments is sta-tistically significant at p < 0.01.Dataset StatisticsInstances 27300Classes 129Entropy 5.127Baseline 0.149Experiment Accuracy STDEVWord Only 49.0 0.90Combined 49.4 0.44Table 10: Overall results for preposition disam-biguation with FRAMENET semantic roles.
See Ta-ble 9 for the legend.Preposition Freq Entropy Baseline Word Only Combinedthrough 332 1.668 0.438 0.598 0.634as 224 1.647 0.399 0.820 0.879by 1043 1.551 0.501 0.867 0.860between 83 1.506 0.483 0.733 0.751of 30 1.325 0.567 0.800 0.814out 76 1.247 0.711 0.788 0.764for 1406 1.223 0.655 0.805 0.796on 1927 1.184 0.699 0.856 0.855throughout 61 0.998 0.525 0.603 0.584across 78 0.706 0.808 0.858 0.748from 1521 0.517 0.917 0.912 0.882Total 6781 1.233 0.609 0.785 0.778Table 7: Per-word results for preposition disambiguation with TREEBANK semantic roles.
Freq gives thefrequency for the prepositions.
Entropy measures non-uniformity of the role distributions.
The Baselineexperiment selects the most-frequent role.
The Word Only experiment just uses word collocations, whereasCombined uses both word and hypernym collocations.
Both columns show averages for percent correct overten trials.
Total averages the values of the individual experiments (except for Freq).Prep Freq Entropy Baseline Word Only Combinedbetween 286 3.258 0.490 0.325 0.537against 210 2.998 0.481 0.310 0.586under 125 2.977 0.385 0.448 0.440as 593 2.827 0.521 0.388 0.598over 620 2.802 0.505 0.408 0.526behind 144 2.400 0.520 0.340 0.473back 540 1.814 0.544 0.465 0.567around 489 1.813 0.596 0.607 0.560round 273 1.770 0.464 0.513 0.533into 844 1.747 0.722 0.759 0.754about 1359 1.720 0.682 0.706 0.778through 673 1.571 0.755 0.780 0.779up 488 1.462 0.736 0.736 0.713towards 308 1.324 0.758 0.786 0.740away 346 1.231 0.786 0.803 0.824like 219 1.136 0.777 0.694 0.803down 592 1.131 0.764 0.764 0.746across 544 1.128 0.824 0.820 0.827off 435 0.763 0.892 0.904 0.899along 469 0.538 0.912 0.932 0.915onto 107 0.393 0.926 0.944 0.939past 166 0.357 0.925 0.940 0.938Total 10432 1.684 0.657 0.685 0.703Table 8: Per-word results for preposition disambiguation with FRAMENET semantic roles.
See Table 7 forthe legend.goal.
This reflects the bias of FRAMENET towardsroles that are an integral part of the frame under con-sideration: location and time apply to all frames, sothese cases are not generally annotated.Table 9 shows the results of classification whenall of the prepositions are classified together.
Theoverall results are not that high due to the very largenumber of roles.
However, the combined colloca-tion approach still shows slight improvement (49.4%versus 49.0%).
Table 8 shows the results when us-ing individual classifiers.
This shows that the com-bined collocations produce better results: 70.3%versus 68.5%.
Unlike the case with Treebank, theperformance is below that of the individual classi-fiers.
This is due to the fine-grained nature of therole inventory.
When all the roles are considered to-gether, prepositions are prone to being misclassifiedwith roles that they might not have occurred with inthe training data, such as whenever other contextualclues are strong for that role.
This is not a problemwith Treebank given its small role inventory.4 Related workUntil recently, there has not been much work specif-ically on preposition classification, especially withrespect to general applicability in contrast to spe-cial purpose usages.
Halliday (1956) did some earlywork on this in the context of machine translation.Later work in that area addressed the classificationindirectly during translation.
In some cases, the is-sue is avoided by translating the preposition into acorresponding foreign function word without regardto the preposition?s underlying meaning (i.e., directtransfer).
Other times an internal representation ishelpful (Trujillo, 1992).
Taylor (1993) discussesgeneral strategies for preposition disambiguation us-ing a cognitive linguistics framework and illustratesthem for ?over?.
There has been quite a bit of workin this area but mainly for spatial prepositions (Jap-kowicz and Wiebe, 1991; Zelinsky-Wibbelt, 1993).There is currently more interest in this type ofclassification.
Litkowski (2002) presents manually-derived rules for disambiguating prepositions, inparticular for ?of?.
Srihari et al (2001) presentmanually-derived rules for disambiguating preposi-tions used in named entities.Gildea and Jurafsky (2002) classify seman-tic role assignments using all the annotations inFRAMENET, for example, covering all types of ver-bal arguments.
They use several features derivedfrom the output of a parser, such as the constituenttype of the phrase (e.g., NP) and the grammaticalfunction (e.g., subject).
They include lexical fea-tures for the headword of the phrase and the predi-cating word for the entire annotated frame.
They re-port an accuracy of 76.9% with a baseline of 40.6%over the FRAMENET semantic roles.
However, dueto the conditioning of the classification on the pred-icating word for the frame, the range of roles for aparticular classification is more limited than in ourcase.Blaheta and Charniak (2000) classify semanticrole assignments using all the annotations in TREE-BANK.
They use a few parser-derived features, suchas the constituent labels for nearby nodes and part-of-speech for parent and grandparent nodes.
Theyalso include lexical features for the head and al-ternative head (since prepositions are considered asthe head by their parser).
They report an accu-racy of 77.6% over the form/function tags from thePENN TREEBANK with a baseline of 37.8%,3 Theirtask is somewhat different, since they address all ad-juncts, not just prepositions, hence their lower base-line.
In addition, they include the nominal and ad-verbial roles, which are syntactic and presumablymore predictable than the others in this group.
Vanden Bosch and Bucholz (2002) also use the Tree-bank data to address the more general task of assign-ing function tags to arbitrary phrases.
For features,they use parts of speech, words, and morphologicalclues.
Chunking is done along with the tagging, butthey only present results for the evaluation of bothtasks taken together; their best approach achieves78.9% accuracy.5 ConclusionOur approach to classifying prepositions accordingto the PENN TREEBANK annotations is fairly accu-rate (78.5% individually and 86.1% together), whileretaining ability to generalize via class-based lexi-cal associations.
These annotations are suitable for3They target al of the TREEBANK function tags but giveperformance figures broken down by the groupings defined inthe Treebank tagging guidelines.
The baseline figure shownabove is their recall figure for the ?baseline 2?
performance.default classification of prepositions in case morefine-grained semantic role information cannot be de-termined.
For the fine-grained FRAMENET roles,the performance is less accurate (70.3% individu-ally and 49.4% together).
In both cases, the bestaccuracy is achieved using a combination of stan-dard word collocations along with class collocationsin the form of WordNet hypernyms.Future work will address cross-dataset experi-ments.
In particular, we will see whether the wordand hypernym associations learned over FrameNetcan be carried over into Treebank, given a mappingof the fine-grained FrameNet roles into the coarse-grained Treebank ones.
Such a mapping would besimilar to the one developed by Gildea and Jurafsky(2002).AcknowledgementsThe first author is supported by a generous GAANN fellowshipfrom the Department of Education.
Some of the work used com-puting resources at NMSU made possible through MII GrantsEIA-9810732 and EIA-0220590.ReferencesDon Blaheta and Eugene Charniak.
2000.
Assigningfunction tags to parsed text.
In Proc.
NAACL-00.Rebecca Bruce and Janyce Wiebe.
1999.
Decomposablemodeling in natural language processing.
Computa-tional Linguistics, 25 (2):195?208.A.
Van den Bosch and S. Buchholz.
2002.
Shallow pars-ing on the basis of words only: A case study.
In Pro-ceedings of the 40th Meeting of the Association forComputational Linguistics (ACL?02), pages 433?440.Philadelphia, PA, USA.P.
Edmonds and S. Cotton, editors.
2001.
Proceedings ofthe SENSEVAL 2 Workshop.
Association for Compu-tational Linguistics.Charles J. Fillmore, Charles Wooters, and Collin F.Baker.
2001.
Building a large lexical databank whichprovides deep semantics.
In Proceedings of the Pa-cific Asian Conference on Language, Information andComputation.
Hong Kong.C.
Fillmore.
1968.
The case for case.
In Emmon Bachand Rovert T. Harms, editors, Universals in LinguisticTheory.
Holt, Rinehart and Winston, New York.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.M.A.K.
Halliday.
1956.
The linguistic basis of amechanical thesaurus, and its application to Englishpreposition classification.
Mechanical Translation,3(2):81?88.Nathalie Japkowicz and Janyce Wiebe.
1991.
Translat-ing spatial prepositions using conceptual information.In Proc.
29th Annual Meeting of the Assoc.
for Com-putational Linguistics (ACL-91), pages 153?160.K.
C. Litkowski.
2002.
Digraph analysis of dictionarypreposition definitions.
In Proceedings of the Asso-ciation for Computational Linguistics Special InterestGroup on the Lexicon.
July 11, Philadelphia, PA.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.
InProc.
ARPA Human Language Technology Workshop.Tom O?Hara and Janyce Wiebe.
2003.
Classifying func-tional relations in Factotum viaWordNet hypernymas-sociations.
In Proc.
Fourth International Conferenceon Intelligent Text Processing and Computational Lin-guistics (CICLing-2003).TomO?Hara, JanyceWiebe, and Rebecca F. Bruce.
2000.Selecting decomposable models for word-sense dis-ambiguation: The GRLING-SDM system.
Computersand the Humanities, 34 (1-2):159?164.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, California.Rohini Srihari, Cheng Niu, and Wei Li.
2001.
A hybridapproach for named entity and sub-type tagging.
InProc.
6th Applied Natural Language Processing Con-ference.John R. Taylor.
1993.
Prepositions: patterns of polysem-ization and strategies of disambiguation.
In Zelinsky-Wibbelt (Zelinsky-Wibbelt, 1993).Arturo Trujillo.
1992.
Locations in the machine transla-tion of prepositional phrases.
In Proc.
TMI-92, pages13?20.Janyce Wiebe, Kenneth McKeever, and Rebecca Bruce.1998.
Mapping collocational properties into machinelearning features.
In Proc.
6th Workshop on VeryLarge Corpora (WVLC-98), pages 225?233,Montreal,Quebec, Canada.
Association for Computational Lin-guistics SIGDAT.Ian H.Witten and Eibe Frank.
1999.
DataMining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.Cornelia Zelinsky-Wibbelt, editor.
1993.
The Semanticsof Prepositions: From Mental Processing to NaturalLanguage Processing.
Mouton de Gruyter, Berlin.
