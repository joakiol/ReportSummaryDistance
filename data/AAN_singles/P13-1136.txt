Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384?1394,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Sentence Compression Based Framework to Query-FocusedMulti-Document SummarizationLu Wang1 Hema Raghavan2 Vittorio Castelli2 Radu Florian2 Claire Cardie11Department of Computer Science, Cornell University, Ithaca, NY 14853, USA{luwang, cardie}@cs.cornell.edu2IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA{hraghav, vittorio, raduf}@us.ibm.comAbstractWe consider the problem of using sentencecompression techniques to facilitate query-focused multi-document summarization.
Wepresent a sentence-compression-based frame-work for the task, and design a series oflearning-based compression models built onparse trees.
An innovative beam search de-coder is proposed to efficiently find highlyprobable compressions.
Under this frame-work, we show how to integrate various in-dicative metrics such as linguistic motivationand query relevance into the compression pro-cess by deriving a novel formulation of a com-pression scoring function.
Our best modelachieves statistically significant improvementover the state-of-the-art systems on severalmetrics (e.g.
8.0% and 5.4% improvements inROUGE-2 respectively) for the DUC 2006 and2007 summarization task.1 IntroductionThe explosion of the Internet clearly warrantsthe development of techniques for organizing andpresenting information to users in an effectiveway.
Query-focused multi-document summariza-tion (MDS) methods have been proposed as onesuch technique and have attracted significant at-tention in recent years.
The goal of query-focusedMDS is to synthesize a brief (often fixed-length)and well-organized summary from a set of topic-related documents that answer a complex ques-tion or address a topic statement.
The result-ing summaries, in turn, can support a number ofinformation analysis applications including open-ended question answering, recommender systems,and summarization of search engine results.
Asfurther evidence of its importance, the DocumentUnderstanding Conference (DUC) has used query-focused MDS as its main task since 2004 to fosternew research on automatic summarization in thecontext of users?
needs.To date, most top-performing systems formulti-document summarization?whether query-specific or not?remain largely extractive: theirsummaries are comprised exclusively of sen-tences selected directly from the documentsto be summarized (Erkan and Radev, 2004;Haghighi and Vanderwende, 2009; Celikyilmazand Hakkani-Tu?r, 2011).
Despite their simplicity,extractive approaches have some disadvantages.First, lengthy sentences that are partly relevantare either excluded from the summary or (if se-lected) can block the selection of other importantsentences, due to summary length constraints.In addition, when people write summaries, theytend to abstract the content and seldom useentire sentences taken verbatim from the originaldocuments.
In news articles, for example, mostsentences are lengthy and contain both potentiallyuseful information for a summary as well as un-necessary details that are better omitted.
Considerthe following DUC query as input for a MDSsystem:1 ?In what ways have stolen artworksbeen recovered?
How often are suspects arrestedor prosecuted for the thefts??
One manually gen-erated summary includes the following sentencebut removes the bracketed words in gray:A man suspected of stealing a million-dollar collectionof [hundreds of ancient] Nepalese and Tibetan art objects inNew York [11 years ago] was arrested [Thursday at his SouthLos Angeles home, where he had been hiding the antiquities,police said].In this example, the compressed sentence is rela-1From DUC 2005, query for topic d422g.1384tively more succinct and readable than the origi-nal (e.g.
in terms of Flesch-Kincaid Reading EaseScore (Kincaid et al, 1975)).
Likewise, removinginformation irrelevant to the query (e.g.
?11 yearsago?, ?police said?)
is crucial for query-focusedMDS.Sentence compression techniques (Knight andMarcu, 2000; Clarke and Lapata, 2008) are thestandard for producing a compact and grammat-ical version of a sentence while preserving rel-evance, and prior research (e.g.
Lin (2003)) hasdemonstrated their potential usefulness for genericdocument summarization.
Similarly, strides havebeen made to incorporate sentence compressioninto query-focused MDS systems (Zajic et al,2006).
Most attempts, however, fail to producebetter results than those of the best systems builton pure extraction-based approaches that use nosentence compression.In this paper we investigate the role of sentencecompression techniques for query-focused MDS.We extend existing work in the area first by inves-tigating the role of learning-based sentence com-pression techniques.
In addition, we design threetypes of approaches to sentence-compression?rule-based, sequence-based and tree-based?andexamine them within our compression-basedframework for query-specific MDS.
Our top-performing sentence compression algorithm in-corporates measures of query relevance, con-tent importance, redundancy and language qual-ity, among others.
Our tree-based methods rely ona scoring function that allows for easy and flexi-ble tailoring of sentence compression to the sum-marization task, ultimately resulting in significantimprovements for MDS, while at the same timeremaining competitive with existing methods interms of sentence compression, as discussed next.We evaluate the summarization models onthe standard Document Understanding Confer-ence (DUC) 2006 and 2007 corpora 2 for query-focused MDS and find that all of our compression-based summarization models achieve statisticallysignificantly better performance than the bestDUC 2006 systems.
Our best-performing sys-tem yields an 11.02 ROUGE-2 score (Lin andHovy, 2003), a 8.0% improvement over the bestreported score (10.2 (Davis et al, 2012)) on the2We believe that we can easily adapt our system for tasks(e.g.
TAC-08?s opinion summarization or TAC-09?s updatesummarization) or domains (e.g.
web pages or wikipediapages).
We reserve that for future work.DUC 2006 dataset, and an 13.49 ROUGE-2, a5.4% improvement over the best score in DUC2007 (12.8 (Davis et al, 2012)).
We also ob-serve substantial improvements over previous sys-tems w.r.t.
the manual Pyramid (Nenkova andPassonneau, 2004) evaluation measure (26.4 vs.22.9 (Jagarlamudi et al, 2006)); human annota-tors furthermore rate our system-generated sum-maries as having less redundancy and compara-ble quality w.r.t.
other linguistic quality metrics.With these results we believe we are the firstto successfully show that sentence compressioncan provide statistically significant improvementsover pure extraction-based approaches for query-focused MDS.2 Related WorkExisting research on query-focused multi-document summarization (MDS) largely relieson extractive approaches, where systems usuallytake as input a set of documents and selectthe top relevant sentences for inclusion in thefinal summary.
A wide range of methods havebeen employed for this task.
For unsupervisedmethods, sentence importance can be estimatedby calculating topic signature words (Lin andHovy, 2000; Conroy et al, 2006), combiningquery similarity and document centrality withina graph-based model (Otterbacher et al, 2005),or using a Bayesian model with sophisticatedinference (Daume?
and Marcu, 2006).
Davis etal.
(2012) first learn the term weights by LatentSemantic Analysis, and then greedily selectsentences that cover the maximum combinedweights.
Supervised approaches have mainlyfocused on applying discriminative learning forranking sentences (Fuentes et al, 2007).
Lin andBilmes (2011) use a class of carefully designedsubmodular functions to reward the diversity ofthe summaries and select sentences greedily.Our work is more related to the less studiedarea of sentence compression as applied to (sin-gle) document summarization.
Zajic et al (2006)tackle the query-focused MDS problem using acompress-first strategy: they develop heuristics togenerate multiple alternative compressions of allsentences in the original document; these then be-come the candidates for extraction.
This approach,however, does not outperform some extraction-based approaches.
A similar idea has been stud-ied for MDS (Lin, 2003; Gillick and Favre, 2009),1385but limited improvement is observed over extrac-tive baselines with simple compression rules.
Fi-nally, although learning-based compression meth-ods are promising (Martins and Smith, 2009;Berg-Kirkpatrick et al, 2011), it is unclear howwell they handle issues of redundancy.Our research is also inspired by probabilis-tic sentence-compression approaches, such as thenoisy-channel model (Knight and Marcu, 2000;Turner and Charniak, 2005), and its extension viasynchronous context-free grammars (SCFG) (Ahoand Ullman, 1969; Lewis and Stearns, 1968) forrobust probability estimation (Galley and McKe-own, 2007).
Rather than attempt to derive a newparse tree like Knight and Marcu (2000) and Gal-ley and McKeown (2007), we learn to safely re-move a set of constituents in our parse tree-basedcompression model while preserving grammati-cal structure and essential content.
Sentence-levelcompression has also been examined via a dis-criminative model McDonald (2006), and Clarkeand Lapata (2008) also incorporate discourse in-formation by using integer linear programming.3 The FrameworkWe now present our query-focused MDS frame-work consisting of three steps: Sentence Rank-ing, Sentence Compression and Post-processing.First, sentence ranking determines the importanceof each sentence given the query.
Then, a sen-tence compressor iteratively generates the mostlikely succinct versions of the ranked sentences,which are cumulatively added to the summary, un-til a length limit is reached.
Finally, the post-processing stage applies coreference resolutionand sentence reordering to build the summary.Sentence Ranking.
This stage aims to rank sen-tences in order of relevance to the query.
Un-surprisingly, ranking algorithms have been suc-cessfully applied to this task.
We experimentedwith two of them ?
Support Vector Regres-sion (SVR) (Mozer et al, 1997) and Lamb-daMART (Burges et al, 2007).
The formerhas been used previously for MDS (Ouyang etal., 2011).
LambdaMart on the other hand hasshown considerable success in information re-trieval tasks (Burges, 2010); we are the first toapply it to summarization.
For training, we use40 topics (i.e.
queries) from the DUC 2005 cor-pus (Dang, 2005) along with their manually gener-ated abstracts.
As in previous work (Shen and Li,Basic Featuresrelative/absolute positionis among the first 1/3/5 sentences?number of words (with/without stopwords)number of words more than 5/10 (with/without stopwords)Query-Relevant Featuresunigram/bigram/skip bigram (at most four words apart) overlapunigram/bigram TF/TF-IDF similaritymention overlapsubject/object/indirect object overlapsemantic role overlaprelation overlapQuery-Independent Featuresaverage/total unigram/bigram IDF/TF-IDFunigram/bigram TF/TF-IDF similarity with the centroid of the clusteraverage/sum of sumBasic/SumFocus (Toutanova et al, 2007)average/sum of mutual informationaverage/sum of number of topic signature words (Lin and Hovy, 2000)basic/improved sentence scorers from Conroy et al (2006)Content Featurescontains verb/web link/phone number?contains/portion of words between parenthesesTable 1: Sentence-level features for sentence ranking.2011; Ouyang et al, 2011), we use the ROUGE-2 score, which measures bigram overlap betweena sentence and the abstracts, as the objective forregression.While space limitations preclude a longer dis-cussion of the full feature set (ref.
Table 1), wedescribe next the query-relevant features used forsentence ranking as these are the most impor-tant for our summarization setting.
The goal ofthis feature subset is to determine the similaritybetween the query and each candidate sentence.When computing similarity, we remove stopwordsas well as the words ?discuss, describe, specify,explain, identify, include, involve, note?
that areadopted and extended from Conroy et al (2006).Then we conduct simple query expansion basedon the title of the topic and cross-document coref-erence resolution.
Specifically, we first add thewords from the topic title to the query.
And foreach mention in the query, we add other mentionswithin the set of documents that corefer with thismention.
Finally, we compute two versions of thefeatures?one based on the original query and an-other on the expanded one.
We also derive thesemantic role overlap and relation instance over-lap between the query and each sentence.
Cross-document coreference resolution, semantic role la-beling and relation extraction are accomplishedvia the methods described in Section 5.Sentence Compression.
As the main focus ofthis paper, we propose three types of compressionmethods, described in detail in Section 4 below.Post-processing.
Post-processing performscoreference resolution and sentence ordering.1386Basic Features Syntactic Tree Featuresfirst 1/3/5 tokens (toks)?
POS taglast 1/3/5 toks?
parent/grandparent labelfirst letter/all letters capitalized?
leftmost child of parent?is negation?
second leftmost child of parent?is stopword?
is headword?Dependency Tree Features in NP/VP/ADVP/ADJP chunk?dependency relation (dep rel) Semantic Featuresparent/grandparent dep rel is a predicate?is the root?
semantic role labelhas a depth larger than 3/5?Rule-Based FeaturesFor each rule in Table 2 , we construct a corresponding feature toindicate whether the token is identified by the rule.Table 3: Token-level features for sequence-based com-pression.We replace each pronoun with its referent unlessthey appear in the same sentence.
For sentenceordering, each compressed sentence is assignedto the most similar (tf-idf) query sentence.
Thena Chronological Ordering algorithm (Barzilay etal., 2002) sorts the sentences for each query basedfirst on the time stamp, and then the position inthe source document.4 Sentence CompressionSentence compression is typically formulated asthe problem of removing secondary informationfrom a sentence while maintaining its grammati-cality and semantic structure (Knight and Marcu,2000; McDonald, 2006; Galley and McKeown,2007; Clarke and Lapata, 2008).
We leave otherrewrite operations, such as paraphrasing and re-ordering, for future work.
Below we describethe sentence compression approaches developedin this research: RULE-BASED COMPRESSION,SEQUENCE-BASED COMPRESSION, and TREE-BASED COMPRESSION.4.1 Rule-based CompressionTurner and Charniak (2005) have shown that ap-plying hand-crafted rules for trimming sentencescan improve both content and linguistic qual-ity.
Our rule-based approach extends existingwork (Conroy et al, 2006; Toutanova et al, 2007)to create the linguistically-motivated compressionrules of Table 2.
To avoid ill-formed output, wedisallow compressions of more than 10 words byeach rule.4.2 Sequence-based CompressionAs in McDonald (2006) and Clarke and Lapata(2008), our sequence-based compression modelmakes a binary ?keep-or-delete?
decision for eachword in the sentence.
In contrast, however, weFigure 1: Diagram of tree-based compression.
Thenodes to be dropped are grayed out.
In this example,the root of the gray subtree (a ?PP?)
would be labeledREMOVE.
Its siblings and parent are labeled RETAINand PARTIAL, respectively.
The trimmed tree is real-ized as ?Malaria causes millions of deaths.
?view compression as a sequential tagging problemand make use of linear-chain Conditional Ran-dom Fields (CRFs) (Lafferty et al, 2001) to se-lect the most likely compression.
We representeach sentence as a sequence of tokens, X =x0x1 .
.
.
xn, and generate a sequence of labels,Y = y0y1 .
.
.
yn, that encode which tokens arekept, using a BIO label format: {B-RETAIN de-notes the beginning of a retained sequence, I-RETAIN indicates tokens ?inside?
the retained se-quence, O marks tokens to be removed}.The CRF model is built using the featuresshown in Table 3.
?Dependency Tree Features?encode the grammatical relations in which eachword is involved as a dependent.
For the ?Syntac-tic Tree?, ?Dependency Tree?
and ?Rule-Based?features, we also include features for the twowords that precede and the two that follow the cur-rent word.
Detailed descriptions of the trainingdata and experimental setup are in Section 5.During inference, we find the maximally likelysequence Y according to a CRF with parameter?
(Y = argmaxY ?
P (Y ?|X; ?
)), while simulta-neously enforcing the rules of Table 2 to reducethe hypothesis space and encourage grammaticalcompression.
To do this, we encode these rules asfeatures for each token, and whenever these fea-ture functions fire, we restrict the possible labelfor that token to ?O?.4.3 Tree-based CompressionOur tree-based compression methods are in linewith syntax-driven approaches (Galley and McK-eown, 2007), where operations are carried outon parse tree constituents.
Unlike previouswork (Knight and Marcu, 2000; Galley and McK-eown, 2007), we do not produce a new parse tree,1387Rule ExampleHeader [MOSCOW , October 19 ( Xinhua ) ?]
Russian federal troops Tuesday continued...Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was...Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said].Lead adverbials [Interestingly], while the Democrats tend to talk about...Noun appositives Wayne County Prosecutor [John O?Hara] wanted to send a message...Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], was elected in the presidential election...Adverbial clausal modifiers [Starting in 1998], California will require 2 per cent of a manufacturer...(Lead sentence) [Given the short time], car makers see electric vehicles as...Within Parentheses ...to Christian home schoolers in the early 1990s [(www.homecomputermarket.com)].Table 2: Linguistically-motivated rules for sentence compression.
The grayed-out words in brackets are removed.but focus on learning to identify the proper set ofconstituents to be removed.
In particular, when anode is dropped from the tree, all words it sub-sumes will be deleted from the sentence.Formally, given a parse tree T of the sentenceto be compressed and a tree traversal algorithm,T can be presented as a list of ordered constituentnodes, T = t0t1 .
.
.
tm.
Our objective is to find aset of labels, L = l0l1 .
.
.
lm, where li ?
{RETAIN,REMOVE, PARTIAL}.
RETAIN (RET) and RE-MOVE (REM) denote whether the node ti is re-tained or removed.
PARTIAL (PAR) means ti ispartly removed, i.e.
at least one child subtree of tiis dropped.Labels are identified, in order, according to thetree traversal algorithm.
Every node label needsto be compatible with the labeling history: givena node ti, and a set of labels l0 .
.
.
li?1 predictedfor nodes t0 .
.
.
ti?1, li =RET or li =REM is com-patible with the history when all children of ti arelabeled as RET or REM, respectively; li =PAR iscompatible when ti has at least two descendentstj and tk (j < i and k < i), one of which isRETained and the other, REMoved.
As such, theroot of the gray subtree in Figure 1 is labeled asREM; its left siblings as RET; its parent as PAR.As the space of possible compressions is expo-nential in the number of leaves in the parse tree,instead of looking for the globally optimal solu-tion, we use beam search to find a set of highlylikely compressions and employ a language modeltrained on a large corpus for evaluation.A Beam Search Decoder.
The beam search de-coder (see Algorithm 1) takes as input the sen-tence?s parse tree T = t0t1 .
.
.
tm, an order-ing O for traversing T (e.g.
postorder) as a se-quence of nodes in T , the set L of possiblenode labels, a scoring function S for evaluat-ing each sentence compression hypothesis, anda beam size N .
Specifically, O is a permuta-tion on the set {0, 1, .
.
.
,m}?each element anindex onto T .
Following O, T is re-ordered astO0tO1 .
.
.
tOm , and the decoder considers each or-dered constituent tOi in turn.
In iteration i, allexisting sentence compression hypotheses are ex-panded by one node, tOi , labeling it with all com-patible labels.
The new hypotheses (usually sub-sentences) are ranked by the scorer S and the topN are preserved to be extended in the next itera-tion.
See Figure 2 for an example.Input : parse tree T , ordering O = O0O1 .
.
.
Om,L ={RET, REM, PAR}, hypothesis scorer S,beam size NOutput: N best compressionsstack?
?
(empty set);foreach node tOi in T = tO0 .
.
.
tOm doif i == 0 (first node visited) thenforeach label lO0 in L donewHypothesis h?
?
[lO0 ];put h?
into Stack;endelsenewStack?
?
(empty set);foreach hypothesis h in stack doforeach label lOi in L doif lOi is compatible thennewHypothesis h?
?
h + [lOi ];put h?
into newStack;endendendstack?
newStack;endApply S to sort hypotheses in stack in descendingorder;Keep the N best hypotheses in stack;endAlgorithm 1: Beam search decoder.Our BASIC Tree-based Compression in-stantiates the beam search decoder withpostorder traversal and a hypothesis scorerthat takes a possible sentence compression?a sequence of nodes (e.g.
tO0 .
.
.
tOk ) andtheir labels (e.g.
lO0 .
.
.
lOk )?and returns?kj=1 logP (lOj |tOj ) (denoted later asScoreBasic).
The probability is estimated bya Maximum Entropy classifier (Berger et al,1388Figure 2: Example of beam search decoding.
Forpostorder traversal, the three nodes are visited in abottom-up order.
The associated compression hypothe-ses (boxed) are ranked based on the scores in parenthe-ses.
Beam scores for other nodes are omitted.Basic Features Syntactic Tree Featuresprojection falls w/in first 1/3/5 toks??
constituent labelprojection falls w/in last 1/3/5 toks??
parent left/right sibling labelsubsumes first 1/3/5 toks??
grandparent left/right sibling labelsubsumes last 1/3/5 toks??
is leftmost child of parent?number of words larger than 5/10??
is second leftmost child of parent?is leaf node??
is head node of parent?is root of parsing tree??
label of its head nodehas word with first letter capitalized?
has a depth greater than 3/5/10?has word with all letters capitalized?
Dependency Tree Featureshas negation?
dep rel of head node?has stopwords?
dep rel of parent?s head node?Semantic Features dep rel of grandparent?s head node?the head node has predicate?
contain root of dep tree?
?semantic roles of head node has a depth larger than 3/5?
?Rule-Based FeaturesFor each rule in Table 2 , we construct a corresponding feature to indicatewhether the token is identified by the rule.Table 4: Constituent-level features for tree-based com-pression.
?
or ?
denote features that are concatenatedwith every Syntactic Tree feature to compose a newone.1996) trained at the constituent level using thefeatures in Table 4.
We also apply the rules ofTable 2 during the decoding process.
Concretely,if the words subsumed by a node are identifiedby any rule, we only consider REM as the node?slabel.Given the N -best compressions from the de-coder, we evaluate the yield of the trimmed treesusing a language model trained on the Giga-word (Graff, 2003) corpus and return the compres-sion with the highest probability.
Thus, the de-coder is quite flexible ?
its learned scoring func-tion allows us to incorporate features salient forsentence compression while its language modelguarantees the linguistic quality of the compressedstring.
In the sections below we consider addi-tional improvements.4.3.1 Improving Beam SearchCONTEXT-aware search is based on the intu-ition that predictions on preceding context canbe leveraged to facilitate the prediction of thecurrent node.
For example, parent nodes withchildren that have all been removed (retained)should have a label of REM (RET).
In light ofthis, we encode these contextual predictions asadditional features of S, that is, ALL-CHILDREN-REMOVED/RETAINED, ANY-LEFTSIBLING-REMOVED/RETAINED/PARTLY REMOVED,LABEL-OF-LEFT-SIBLING/HEAD-NODE.HEAD-driven search modifies the BASIC pos-torder tree traversal by visiting the head node firstat each level, leaving other orders unchanged.
Ina nutshell, if the head node is dropped, then itsmodifiers need not be preserved.
We adopt thesame features as CONTEXT-aware search, but re-move those involving left siblings.
We also addone more feature: LABEL-OF-THE-HEAD-NODE-IT-MODIFIES.4.3.2 Task-Specific Sentence CompressionThe current scorer ScoreBasic is still fairly naivein that it focuses only on features of the sen-tence to be compressed.
However extra-sententialknowledge can also be important for query-focused MDS.
For example, information regard-ing relevance to the query might lead the de-coder to produce compressions better suited forthe summary.
Towards this goal, we constructa compression scoring function?the multi-scorer(MULTI)?that allows the incorporation of mul-tiple task-specific scorers.
Given a hypothesis atany stage of decoding, which yields a sequence ofwords W = w0w1...wj , we propose the followingcomponent scorers.Query Relevance.
Query information ought toguide the compressor to identify the relevant con-tent.
The query Q is expanded as described inSection 3.
Let |W ?
Q| denote the number ofunique overlapping words betweenW andQ, thenscoreq = |W ?Q|/|W |.Importance.
A query-independent impor-tance score is defined as the average Sum-Basic (Toutanova et al, 2007) value in W ,i.e.
scoreim =?ji=1 SumBasic(wi)/|W |.Language Model.
We let scorelm be the proba-bility of W computed by a language model.Cross-Sentence Redundancy.
To encourage di-versified content, we define a redundancy score todiscount replicated content: scorered = 1?
|W ?C|/|W |, whereC is the words already selected forthe summary.1389The multi-scorer is defined as a linearcombination of the component scorers: Let~?
= (?0, .
.
.
, ?4), 0 ?
?i ?
1, ???
?score =(scoreBasic, scoreq, scoreim, scorelm, scorered),S = scoremulti = ~?
?
???
?score (1)The parameters ~?
are tuned on a held-out tuningset by grid search.
We linearly normalize the scoreof each metric, where the minimum and maximumvalues are estimated from the tuning data.5 Experimental SetupWe evaluate our methods on the DUC 2005, 2006and 2007 datasets (Dang, 2005; Dang, 2006;Dang, 2007), each of which is a collection ofnewswire articles.
50 complex queries (topics) areprovided for DUC 2005 and 2006, 35 are collectedfor DUC 2007 main task.
Relevant documents foreach query are provided along with 4 to 9 humanMDS abstracts.
The task is to generate a summarywithin 250 words to address the query.
We splitDUC 2005 into two parts: 40 topics to train thesentence ranking models, and 10 for ranking algo-rithm selection and parameter tuning for the multi-scorer.
DUC 2006 and DUC 2007 are reserved asheld out test sets.Sentence Compression.
The datasetfrom Clarke and Lapata (2008) is used totrain the CRF and MaxEnt classifiers (Section 4).It includes 82 newswire articles with one manuallyproduced compression aligned to each sentence.Preprocessing.
Documents are processed by afull NLP pipeline, including token and sentencesegmentation, parsing, semantic role labeling,and an information extraction pipeline consist-ing of mention detection, NP coreference, cross-document resolution, and relation detection (Flo-rian et al, 2004; Luo et al, 2004; Luo and Zitouni,2005).Learning for Sentence Ranking and Compres-sion.
We use Weka (Hall et al, 2009) to train asupport vector regressor and experiment with var-ious rankers in RankLib (Dang, 2011)3.
As Lamb-daMART has an edge over other rankers on theheld-out dataset, we selected it to produce rankedsentences for further processing.
For sequence-based compression using CRFs, we employ Mal-let (McCallum, 2002) and integrate the Table 2rules during inference.
NLTK (Bird et al, 2009)3Default parameters are used.
If an algorithm needs a val-idation set, we use 10 out of 40 topics.MaxEnt classifiers are used for tree-based com-pression.
Beam size is fixed at 2000.4 Sen-tence compressions are evaluated by a 5-gram lan-guage model trained on Gigaword (Graff, 2003)by SRILM (Stolcke, 2002).6 ResultsThe results in Table 5 use the official ROUGE soft-ware with standard options5 and report ROUGE-2 (R-2) (measures bigram overlap) and ROUGE-SU4 (R-SU4) (measures unigram and skip-bigramseparated by up to four words).
We compare oursentence-compression-based methods to the bestperforming systems based on ROUGE in DUC2006 and 2007 (Jagarlamudi et al, 2006; Pingaliet al, 2007), system by Davis et al (2012) thatreport the best R-2 score on DUC 2006 and 2007thus far, and to the purely extractive methods ofSVR and LambdaMART.Our sentence-compression-based systems(marked with ?)
show statistically significantimprovements over pure extractive summarizationfor both R-2 and R-SU4 (paired t-test, p < 0.01).This means our systems can effectively removeredundancy within the summary through compres-sion.
Furthermore, our HEAD-driven beam searchmethod with MULTI-scorer beats all systems onDUC 20066 and all systems on DUC 2007 exceptthe best system in terms of R-2 (p < 0.01).
ItsR-SU4 score is also significantly (p < 0.01)better than extractive methods, rule-based andsequence-based compression methods on bothDUC 2006 and 2007.
Moreover, our systems withlearning-based compression have considerablecompression rates, indicating their capability toremove superfluous words as well as improvesummary quality.Human Evaluation.
The Pyramid (Nenkovaand Passonneau, 2004) evaluation was developedto manually assess how many relevant facts orSummarization Content Units (SCUs) are cap-tured by system summaries.
We ask a professionalannotator (who is not one of the authors, is highlyexperienced in annotating for various NLP tasks,and is fluent in English) to carry out a Pyramidevaluation on 10 randomly selected topics from4We looked at various beam sizes on the heldout data, andobserved that the performance peaks around this value.5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -fA -p 0.5 -t 0 -a -d6The system output from Davis et al (2012) is not avail-able, so significance tests are not conducted on it.1390DUC 2006 DUC 2007System C Rate R-2 R-SU4 C Rate R-2 R-SU4Best DUC system ?
9.56 15.53 ?
12.62 17.90Davis et al (2012) ?
10.2 15.2 ?
12.8 17.5SVR 100% 7.78 13.02 100% 9.53 14.69LambdaMART 100% 9.84 14.63 100% 12.34 15.62Rule-based 78.99% 10.62 ??
15.73 ?
78.11% 13.18?
18.15?Sequence 76.34% 10.49 ?
15.60 ?
77.20% 13.25?
18.23?Tree (BASIC + ScoreBasic) 70.48% 10.49 ?
15.86 ?
69.27% 13.00?
18.29?Tree (CONTEXT + ScoreBasic) 65.21% 10.55 ??
16.10 ?
63.44% 12.75 18.07?Tree (HEAD + ScoreBasic) 66.70% 10.66 ??
16.18 ?
65.05% 12.93 18.15?Tree (HEAD + MULTI) 70.20% 11.02 ??
16.25 ?
73.40% 13.49?
18.46?Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of wordspreserved.
R-2 (ROUGE-2) and R-SU4 (ROUGE-SU4) scores are multiplied by 100.
???
indicates that data isunavailable.
BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-drivensearch extensions respectively.
ScoreBasic and MULTI refer to the type of scorer used.
Statistically significantimprovements (p < 0.01) over the best system in DUC 06 and 07 are marked with ?.
?
indicates statisticalsignificance (p < 0.01) over extractive approaches (SVR or LambdaMART).
HEAD + MULTI outperforms all theother extract- and compression-based systems in R-2.System Pyr Gra Non-Red Ref Foc CohBest DUC system (ROUGE) 22.9?8.2 3.5?0.9 3.5?1.0 3.5?1.1 3.6?1.0 2.9?1.1Best DUC system (LQ) ?
4.0?0.8 4.2?0.7 3.8?0.7 3.6?0.9 3.4?0.9Our System 26.4?10.3 3.0?0.9 4.0?1.1 3.6?1.0 3.4?0.9 2.8?1.0Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al (2006) (Best DUC system(ROUGE)), and Lacatusu et al (2006) (Best DUC system (LQ)).
Our system can synthesize more relevant contentaccording to Pyramid (?100).
We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy(Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each ratedfrom 1 (very poor) to 5 (very good).
Our system has better non-redundancy than Jagarlamudi et al (2006) and iscomparable to Jagarlamudi et al (2006) and Lacatusu et al (2006) in other metrics except grammaticality.the DUC 2006 task with gold-standard SCU an-notation in abstracts.
The Pyramid score (see Ta-ble 6) is re-calculated for the system with bestROUGE scores in DUC 2006 (Jagarlamudi et al,2006) along with our system by the same annota-tor to make a meaningful comparison.We further evaluate the linguistic quality (LQ)of the summaries for the same 10 topics in ac-cordance with the measurement in Dang (2006).Four native speakers who are undergraduate stu-dents in computer science (none are authors) per-formed the task, We compare our system basedon HEAD-driven beam search with MULTI-scorerto the best systems in DUC 2006 achieving topROUGE scores (Jagarlamudi et al, 2006) (BestDUC system (ROUGE)) and top linguistic qualityscores (Lacatusu et al, 2006) (Best DUC system(LQ))7.
The average score and standard deviationfor each metric is displayed in Table 6.
Our sys-tem achieves a higher Pyramid score, an indicationthat it captures more of the salient facts.
We also7Lacatusu et al (2006) obtain the best scores in three lin-guistic quality metrics (i.e.
grammaticality, focus, structureand coherence), and overall responsiveness on DUC 2006.attain better non-redundancy than Jagarlamudi etal.
(2006), meaning that human raters perceiveless replicative content in our summaries.
Scoresfor other metrics are comparable to Jagarlamudiet al (2006) and Lacatusu et al (2006), whicheither uses minimal non-learning-based compres-sion rules or is a pure extractive system.
However,our compression system sometimes generates lessgrammatical sentences, and those are mostly dueto parsing errors.
For example, parsing a clausestarting with a past tense verb as an adverbialclausal modifier can lead to an ill-formed com-pression.
Those issues can be addressed by an-alyzing k-best parse trees and we leave it in thefuture work.
A sample summary from our multi-scorer based system is in Figure 3.Sentence Compression Evaluation.
Wealso evaluate sentence compression separatelyon (Clarke and Lapata, 2008), adopting the samepartitions as (Martins and Smith, 2009), i.e.
1, 188sentences for training and 441 for testing.
Ourcompression models are compared with HedgeTrimmer (Dorr et al, 2003), a discriminativemodel proposed by McDonald (2006) and a1391System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50McDonald (2006) 70.95% 0.77 0.78 0.77 0.55Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56Rule-based 87.65% 0.74 0.91 0.80 0.63Sequence 70.79% 0.77 0.80 0.76 0.58Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59Table 7: Sentence compression comparison.
The true c rate is 69.06% for the test set.
Tree-based approachesall use single-scorer.
Our context-aware and head-driven tree-based approaches outperform all the other systemssignificantly (p < 0.01) in precision (Uni-Prec) without sacrificing the recalls (i.e.
there is no statistically signifi-cant difference between our models and McDonald (2006) / M & S (2009) with p > 0.05).
Italicized numbers forunigram F1 (Uni-F1) are statistically indistinguishable (p > 0.05).
Our head-driven tree-based approach also pro-duces significantly better grammatical relations F1 scores (Rel-F1) than all the other systems except the rule-basedmethod (p < 0.01).Topic D0626H: How were the bombings of the US em-bassies in Kenya and Tanzania conducted?
What terror-ist groups and individuals were responsible?
How andwhere were the attacks planned?WASHINGTON, August 13 (Xinhua) ?
President BillClinton Thursday condemned terrorist bomb attacks atU.S.
embassies in Kenya and Tanzania and vowed to findthe bombers and bring them to justice.
Clinton met withhis top aides Wednesday in the White House to assess thesituation following the twin bombings at U.S. embassiesin Kenya and Tanzania, which have killed more than 250people and injured over 5,000, most of them Kenyans andTanzanians.
Local sources said the plan to bomb U.S. em-bassies in Kenya and Tanzania took three months to com-plete and bombers destined for Kenya were dispatchedthrough Somali and Rwanda.
FBI Director Louis Freeh,Attorney General Janet Reno and other senior U.S. gov-ernment officials will hold a news conference at 1 p.m.EDT (1700GMT) at FBI headquarters in Washington ?toannounce developments in the investigation of the bomb-ings of the U.S. embassies in Kenya and Tanzania,?
theFBI said in a statement.
...Figure 3: Part of the summary generated by the multi-scorer based summarizer for topic D0626H (DUC2006).
Grayed out words are removed.
Query-irrelevant phrases, such as temporal information orsource of the news, have been removed.dependency-tree based compressor (Martins andSmith, 2009)8.
We adopt the metrics in Martinsand Smith (2009) to measure the unigram-levelmacro precision, recall, and F1-measure withrespect to human annotated compression.
Inaddition, we also compute the F1 scores ofgrammatical relations which are annotated byRASP (Briscoe and Carroll, 2002) accordingto Clarke and Lapata (2008).In Table 7, our context-aware and head-driventree-based compression systems show statisticallysignificantly (p < 0.01) higher precisions (Uni-8Thanks to Andre?
F.T.
Martins for system outputs.Prec) than all the other systems, without decreas-ing the recalls (Uni-Rec) significantly (p > 0.05)based on a paired t-test.
Unigram F1 scores (Uni-F1) in italics indicate that the corresponding sys-tems are not statistically distinguishable (p >0.05).
For grammatical relation evaluation, ourhead-driven tree-based system obtains statisticallysignificantly (p < 0.01) better F1 score (Rel-F1than all the other systems except the rule-basedsystem).7 ConclusionWe have presented a framework for query-focusedmulti-document summarization based on sentencecompression.
We propose three types of com-pression approaches.
Our tree-based compres-sion method can easily incorporate measures ofquery relevance, content importance, redundancyand language quality into the compression pro-cess.
By testing on a standard dataset using theautomatic metric ROUGE, our models show sub-stantial improvement over pure extraction-basedmethods and state-of-the-art systems.
Our bestsystem also yields better results for human eval-uation based on Pyramid and achieves comparablelinguistic quality scores.AcknowledgmentsThis work was supported in part by National Sci-ence Foundation Grant IIS-0968450 and a giftfrom Boeing.
We thank Ding-Jung Han, Young-Suk Lee, Xiaoqiang Luo, Sameer Maskey, MyleOtt, Salim Roukos, Yiye Ruan, Ming Tan, ToddWard, Bowen Zhou, and the ACL reviewers forvaluable suggestions and advice on various as-pects of this work.1392ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1969.
Syntax directedtranslations and the pushdown assembler.
J. Comput.
Syst.Sci., 3(1):37?56.Regina Barzilay, Noemie Elhadad, and Kathleen R. McKe-own.
2002.
Inferring strategies for sentence ordering inmultidocument news summarization.
J. Artif.
Int.
Res.,17(1):35?55, August.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011.Jointly learning to extract and compress.
ACL ?11, pages481?490, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Comput.
Linguist.,22(1):39?71, March.Steven Bird, Ewan Klein, and Edward Loper.
2009.
NaturalLanguage Processing with Python.
O?Reilly Media.T.
Briscoe and J. Carroll.
2002.
Robust accurate statisticalannotation of general text.Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le.2007.
Learning to rank with nonsmooth cost functions.
InB.
Scho?lkopf, J. Platt, and T. Hoffman, editors, Advancesin Neural Information Processing Systems 19, pages 193?200.
MIT Press, Cambridge, MA.Christopher J. C. Burges.
2010.
From RankNet to Lamb-daRank to LambdaMART: An overview.
Technical report,Microsoft Research.Asli Celikyilmaz and Dilek Hakkani-Tu?r.
2011.
Discoveryof topically coherent sentences for extractive summariza-tion.
ACL ?11, pages 491?499, Stroudsburg, PA, USA.Association for Computational Linguistics.James Clarke and Mirella Lapata.
2008.
Global inferencefor sentence compression an integer linear programmingapproach.
J. Artif.
Int.
Res., 31(1):399?429, March.John M. Conroy, Judith D. Schlesinger, Dianne P. O?Leary,and Jade Goldstein, 2006.
Back to Basics: CLASSY 2006.U.S.
National Inst.
of Standards and Technology.Hoa T. Dang.
2005.
Overview of DUC 2005.
In DocumentUnderstanding Conference.Hoa Tran Dang.
2006.
Overview of DUC 2006.
InProc.
Document Understanding Workshop, page 10 pages.NIST.Hoa T. Dang.
2007.
Overview of DUC 2007.
In DocumentUnderstanding Conference.Van Dang.
2011.
RankLib.
Online.Hal Daume?, III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
ACL ?06, pages 305?312,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger.2012.
Occams - an optimal combinatorial covering algo-rithm for multi-document summarization.
In ICDM Work-shops, pages 454?463.Bonnie J Dorr, David Zajic, and Richard Schwartz.
2003.Hedge trimmer: a parse-and-trim approach to headlinegeneration.
In Proceedings of the HLT-NAACL 03 onText summarization workshop - Volume 5, HLT-NAACL-DUC ?03, pages 1 ?
8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics, Association for Computa-tional Linguistics.Gu?nes Erkan and Dragomir R. Radev.
2004.
Lexrank: graph-based lexical centrality as salience in text summarization.J.
Artif.
Int.
Res., 22(1):457?479, December.Radu Florian, Hany Hassan, Abraham Ittycheriah, HongyanJing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov,and Salim Roukos.
2004.
A statistical model for multilin-gual entity detection and tracking.
In HLT-NAACL, pages1?8.Maria Fuentes, Enrique Alfonseca, and Horacio Rodr??guez.2007.
Support vector machines for query-focused sum-marization trained and evaluated on pyramid data.
In Pro-ceedings of the 45th Annual Meeting of the ACL on In-teractive Poster and Demonstration Sessions, ACL ?07,pages 57?60, Stroudsburg, PA, USA.
Association forComputational Linguistics.Michel Galley and Kathleen McKeown.
2007.
LexicalizedMarkov grammars for sentence compression.
NAACL?07, pages 180?187, Rochester, New York, April.
Asso-ciation for Computational Linguistics.Dan Gillick and Benoit Favre.
2009.
A scalable global modelfor summarization.
In Proceedings of the Workshop on In-teger Linear Programming for Natural Langauge Process-ing, ILP ?09, pages 10?18, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.David Graff.
2003.
English Gigaword.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing content models for multi-document summarization.NAACL ?09, pages 362?370, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.
2009.The weka data mining software: an update.
SIGKDD Ex-plor.
Newsl., 11(1):10?18, November.Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma,2006.
Query Independent Sentence Scoring approach toDUC 2006.J.
Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, andBrad S. Chissom.
1975.
Derivation of New ReadabilityFormulas (Automated Readability Index, Fog Count andFlesch Reading Ease Formula) for Navy Enlisted Person-nel.
Technical report, February.Kevin Knight and Daniel Marcu.
2000.
Statistics-based sum-marization - step one: Sentence compression.
AAAI ?00,pages 703?710.
AAAI Press.Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-lor, 2006.
LCCs gistexter at duc 2006: Multi-strategymulti-document summarization.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
In1393Proceedings of the Eighteenth International Conferenceon Machine Learning, ICML ?01, pages 282?289, SanFrancisco, CA, USA.
Morgan Kaufmann Publishers Inc.P.
M. Lewis, II and R. E. Stearns.
1968.
Syntax-directedtransduction.
J. ACM, 15(3):465?488, July.Hui Lin and Jeff Bilmes.
2011.
A class of submodular func-tions for document summarization.
In Proceedings of the49th Annual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume 1,HLT ?11, pages 510?520, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2000.
The automated ac-quisition of topic signatures for text summarization.
InProceedings of the 18th conference on Computationallinguistics - Volume 1, COLING ?00, pages 495?501,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic eval-uation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Volume 1,pages 71?78.Chin-Yew Lin.
2003.
Improving summarization perfor-mance by sentence compression: a pilot study.
In Pro-ceedings of the sixth international workshop on Informa-tion retrieval with Asian languages - Volume 11, AsianIR?03, pages 1?8, Stroudsburg, PA, USA.
Association forComputational Linguistics.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingualcoreference resolution with syntactic features.
InHLT/EMNLP.Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm based onthe bell tree.
In ACL, pages 135?142.Andre?
F. T. Martins and Noah A. Smith.
2009.
Summariza-tion with a joint model for sentence extraction and com-pression.
In Proceedings of the Workshop on Integer Lin-ear Programming for Natural Langauge Processing, ILP?09, pages 1?9, Stroudsburg, PA, USA.
Association forComputational Linguistics.Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.Ryan McDonald.
2006.
Discriminative Sentence Compres-sion with Soft Syntactic Constraints.
In Proceedings ofthe 11th?EACL, Trento, Italy, April.Michael Mozer, Michael I. Jordan, and Thomas Petsche, ed-itors.
1997.
Advances in Neural Information ProcessingSystems 9, NIPS, Denver, CO, USA, December 2-5, 1996.MIT Press.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluatingcontent selection in summarization: The pyramid method.In Daniel Marcu Susan Dumais and Salim Roukos, edi-tors, HLT-NAACL 2004: Main Proceedings, pages 145?152, Boston, Massachusetts, USA, May 2 - May 7.
Asso-ciation for Computational Linguistics.Jahna Otterbacher, Gu?nes?
Erkan, and Dragomir R. Radev.2005.
Using random walks for question-focused sentenceretrieval.
In Proceedings of the conference on HumanLanguage Technology and Empirical Methods in NaturalLanguage Processing, HLT ?05, pages 915?922, Strouds-burg, PA, USA.
Association for Computational Linguis-tics.You Ouyang, Wenjie Li, Sujian Li, and Qin Lu.
2011.Applying regression models to query-focused multi-document summarization.
Inf.
Process.
Manage.,47(2):227?237, March.Prasad Pingali, Rahul K, and Vasudeva Varma, 2007.
IIITHyderabad at DUC 2007.
U.S. National Inst.
of Standardsand Technology.Chao Shen and Tao Li.
2011.
Learning to rank for query-focused multi-document summarization.
In Diane J.Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, andXindong Wu, editors, ICDM, pages 626?634.
IEEE.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of ICSLP, volume 2,pages 901?904, Denver, USA.Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vander-wende.
2007.
The PYTHY Summarization System: Mi-crosoft Research at DUC 2007.
In Proc.
of DUC.Jenine Turner and Eugene Charniak.
2005.
Supervised andunsupervised learning for sentence compression.
ACL?05, pages 290?297, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz.2006.
Sentence compression as a component of a multi-document summarization system.
Proceedings of the2006 Document Understanding Workshop, New York.1394
