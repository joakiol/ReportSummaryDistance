The Sentimental Factor: Improving Review Classification viaHuman-Provided InformationPhilip Beineke?and Trevor HastieDept.
of StatisticsStanford UniversityStanford, CA 94305Shivakumar VaithyanathanIBM Almaden Research Center650 Harry Rd.San Jose, CA 95120-6099AbstractSentiment classification is the task of labeling a re-view document according to the polarity of its pre-vailing opinion (favorable or unfavorable).
In ap-proaching this problem, a model builder often hasthree sources of information available: a small col-lection of labeled documents, a large collection ofunlabeled documents, and human understanding oflanguage.
Ideally, a learning method will utilize allthree sources.
To accomplish this goal, we general-ize an existing procedure that uses the latter two.We extend this procedure by re-interpreting itas a Naive Bayes model for document sentiment.Viewed as such, it can also be seen to extract apair of derived features that are linearly combinedto predict sentiment.
This perspective allows us toimprove upon previous methods, primarily throughtwo strategies: incorporating additional derived fea-tures into the model and, where possible, using la-beled data to estimate their relative influence.1 IntroductionText documents are available in ever-increasingnumbers, making automated techniques for infor-mation extraction increasingly useful.
Traditionally,most research effort has been directed towards ?ob-jective?
information, such as classification accord-ing to topic; however, interest is growing in produc-ing information about the opinions that a documentcontains; for instance, Morinaga et al (2002).
InMarch, 2004, the American Association for Artifi-cial Intelligence held a symposium in this area, en-titled ?Exploring Affect and Attitude in Text.
?One task in opinion extraction is to label a re-view document d according to its prevailing senti-ment s ?
{?1, 1} (unfavorable or favorable).
Sev-eral previous papers have addressed this problemby building models that rely exclusively upon la-beled documents, e.g.
Pang et al (2002), Daveet al (2003).
By learning models from labeleddata, one can apply familiar, powerful techniquesdirectly; however, in practice it may be difficult toobtain enough labeled reviews to learn model pa-rameters accurately.A contrasting approach (Turney, 2002) relies onlyupon documents whose labels are unknown.
Thismakes it possible to use a large underlying corpus ?in this case, the entire Internet as seen through theAltaVista search engine.
As a result, estimates formodel parameters are subject to a relatively smallamount of random variation.
The correspondingdrawback to such an approach is that its predictionsare not validated on actual documents.In machine learning, it has often been effec-tive to use labeled and unlabeled examples in tan-dem, e.g.
Nigam et al (2000).
Turney?s modelintroduces the further consideration of incorporat-ing human-provided knowledge about language.
Inthis paper we build models that utilize all threesources: labeled documents, unlabeled documents,and human-provided information.The basic concept behind Turney?s model is quitesimple.
The ?sentiment orientation?
(Hatzivas-siloglou and McKeown, 1997) of a pair of wordsis taken to be known.
These words serve as ?an-chors?
for positive and negative sentiment.
Wordsthat co-occur more frequently with one anchor thanthe other are themselves taken to be predictive ofsentiment.
As a result, information about a pair ofwords is generalized to many words, and then todocuments.In the following section, we relate this modelwith Naive Bayes classification, showing that Tur-ney?s classifier is a ?pseudo-supervised?
approach:it effectively generates a new corpus of labeled doc-uments, upon which it fits a Naive Bayes classifier.This insight allows the procedure to be representedas a probability model that is linear on the logisticscale, which in turn suggests generalizations that aredeveloped in subsequent sections.2 A Logistic Model for Sentiment2.1 Turney?s Sentiment ClassifierIn Turney?s model, the ?sentiment orientation?
?
ofword w is estimated as follows.??
(w) = logN(w,excellent)/NexcellentN(w,poor)/Npoor(1)Here, Na is the total number of sites on the Internetthat contain an occurrence of a ?
a feature that canbe a word type or a phrase.
N(w,a) is the number ofsites in which features w and a appear ?near?
eachother, i.e.
in the same passage of text, within a spanof ten words.
Both numbers are obtained from thehit count that results from a query of the AltaVistasearch engine.
The rationale for this estimate is thatwords that express similar sentiment often co-occur,while words that express conflicting sentiment co-occur more rarely.
Thus, a word that co-occurs morefrequently with excellent than poor is estimated tohave a positive sentiment orientation.To extrapolate from words to documents, the esti-mated sentiment s?
?
{?1, 1} of a review documentd is the sign of the average sentiment orientation ofits constituent features.1 To represent this estimateformally, we introduce the following notation: Wis a ?dictionary?
of features: (w1, .
.
.
, wp).
Eachfeature?s respective sentiment orientation is repre-sented as an entry in the vector ??
of length p:?
?j = ??
(wj) (2)Given a collection of n review documents, the i-theach di is also represented as a vector of length p,with dij equal to the number of times that feature wjoccurs in di.
The length of a document is its totalnumber of features, |di| =?pj=1 dij .Turney?s classifier for the i-th document?s senti-ment si can now be written:s?i = sign(?pj=1 ?
?jdij|di|)(3)Using a carefully chosen collection of features,this classifier produces correct results on 65.8% ofa collection of 120 movie reviews, where 60 arelabeled positive and 60 negative.
Although this isnot a particularly encouraging result, movie reviewstend to be a difficult domain.
Accuracy on senti-ment classification in other domains exceeds 80%(Turney, 2002).1Note that not all words or phrases need to be considered asfeatures.
In Turney (2002), features are selected according topart-of-speech labels.2.2 Naive Bayes ClassificationBayes?
Theorem provides a convenient frameworkfor predicting a binary response s ?
{?1, 1} from afeature vector x:Pr(s = 1|x) = Pr(x|s = 1)pi1?k?
{?1,1} Pr(x|s = k)pik(4)For a labeled sample of data (xi, si), i = 1, ..., n,a class?s marginal probability pik can be estimatedtrivially as the proportion of training samples be-longing to the class.
Thus the critical aspect of clas-sification by Bayes?
Theorem is to estimate the con-ditional distribution of x given s. Naive Bayes sim-plifies this problem by making a ?naive?
assump-tion: within a class, the different feature values aretaken to be independent of one another.Pr(x|s) =?jPr(xj|s) (5)As a result, the estimation problem is reduced tounivariate distributions.?
Naive Bayes for a Multinomial DistributionWe consider a ?bag of words?
model for a docu-ment that belongs to class k, where features are as-sumed to result from a sequence of |di| independentmultinomial draws with outcome probability vectorqk = (qk1, .
.
.
, qkp).Given a collection of documents with labels,(di, si), i = 1, .
.
.
, n, a natural estimate for qkj isthe fraction of all features in documents of class kthat equal wj:q?kj =?i:si=k dij?i:si=k |di|(6)In the two-class case, the logit transformationprovides a revealing representation of the class pos-terior probabilities of the Naive Bayes model.l?ogit(s|d) , log P?r(s = 1|d)P?r(s = ?1|d)(7)= log p?i1p?i?1+p?j=1dj logq?1jq?
?1j(8)= ?
?0 +p?j=1dj?
?j (9)where ?
?0 = logp?i1p?i?1(10)?
?j = logq?1jq?
?1j(11)Observe that the estimate for the logit in Equation9 has a simple structure: it is a linear function ofd.
Models that take this form are commonplace inclassification.2.3 Turney?s Classifier as Naive BayesAlthough Naive Bayes classification requires a la-beled corpus of documents, we show in this sec-tion that Turney?s approach corresponds to a NaiveBayes model.
The necessary documents and theircorresponding labels are built from the spans of textthat surround the anchor words excellent and poor.More formally, a labeled corpus may be producedby the following procedure:1.
For a particular anchor ak, locate all of the siteson the Internet where it occurs.2.
From all of the pages within a site, gather thefeatures that occur within ten words of an oc-currence of ak, with any particular feature in-cluded at most once.
This list comprises a new?document,?
representing that site.23.
Label this document +1 if ak = excellent, -1if ak = poor.When a Naive Bayes model is fit to the corpusdescribed above, it results in a vector ??
of lengthp, consisting of coefficient estimates for all fea-tures.
In Propositions 1 and 2 below, we show thatTurney?s estimates of sentiment orientation ??
areclosely related to ?
?, and that both estimates produceidentical classifiers.Proposition 1??
= C1??
(12)where C1 =Nexc./?i:si=1 |di|Npoor/?i:si=?1 |di|(13)Proof: Because a feature is restricted to at most oneoccurrence in a document,?i:si=kdij = N(w,ak) (14)Then from Equations 6 and 11:?
?j = logq?1jq?
?1j(15)= logN(w,exc.
)/?i:si=1 |di|N(w,poor)/?i:si=?1 |di|(16)= C1?
?j (17)22If both anchors occur on a site, then there will actually betwo documents, one for each sentimentProposition 2 Turney?s classifier is identical to aNaive Bayes classifier fit on this corpus, with pi1 =pi?1 = 0.5.Proof: A Naive Bayes classifier typically assigns anobservation to its most probable class.
This is equiv-alent to classifying according to the sign of the es-timated logit.
So for any document, we must showthat both the logit estimate and the average senti-ment orientation are identical in sign.When pi1 = 0.5, ?0 = 0.
Thus the estimated logitisl?ogit(s|d) =p?j=1?
?jdj (18)= C1p?j=1?
?jdj (19)This is a positive multiple of Turney?s classifier(Equation 3), so they clearly match in sign.
23 A More Versatile Model3.1 Desired ExtensionsBy understanding Turney?s model within a NaiveBayes framework, we are able to interpret its out-put as a probability model for document classes.
Inthe presence of labeled examples, this insight alsomakes it possible to estimate the intercept term ?0.Further, we are able to view this model as a mem-ber of a broad class: linear estimates for the logit.This understanding facilitates further extensions, inparticular, utilizing the following:1.
Labeled documents2.
More anchor wordsThe reason for using labeled documents isstraightforward; labels offer validation for any cho-sen model.
Using additional anchors is desirablein part because it is inexpensive to produce lists ofwords that are believed to reflect positive sentiment,perhaps by reference to a thesaurus.
In addition, asingle anchor may be at once too general and toospecific.An anchor may be too general in the sense thatmany common words have multiple meanings, andnot all of them reflect a chosen sentiment orien-tation.
For example, poor can refer to an objec-tive economic state that does not necessarily expressnegative sentiment.
As a result, a word such asincome appears 4.18 times as frequently with pooras excellent, even though it does not convey nega-tive sentiment.
Similarly, excellent has a technicalmeaning in antiquity trading, which causes it to ap-pear 3.34 times as frequently with furniture.An anchor may also be too specific, in the sensethat there are a variety of different ways to expresssentiment, and a single anchor may not capture themall.
So a word like pretentious carries a strongnegative sentiment but co-occurs only slightly morefrequently (1.23 times) with excellent than poor.Likewise, fascination generally reflects a positivesentiment, yet it appears slightly more frequently(1.06 times) with poor than excellent.3.2 Other Sources of Unlabeled DataThe use of additional anchors has a drawback interms of being resource-intensive.
A feature set maycontain many words and phrases, and each of themrequires a separate AltaVista query for every chosenanchor word.
In the case of 30,000 features and tenqueries per minute, downloads for a single anchorword require over two days of data collection.An alternative approach is to access a largecollection of documents directly.
Then all co-occurrences can be counted in a single pass.Although this approach dramatically reduces theamount of data available, it does offer several ad-vantages.?
Increased Query Options Search enginequeries of the form phrase NEAR anchormay not produce all of the desired co-occurrence counts.
For instance, one may wishto run queries that use stemmed words, hy-phenated words, or punctuation marks.
Onemay also wish to modify the definition ofNEAR, or to count individual co-occurrences,rather than counting sites that contain at leastone co-occurrence.?
Topic Matching Across the Internet as awhole, features may not exhibit the same cor-relation structure as they do within a specificdomain.
By restricting attention to documentswithin a domain, one may hope to avoid co-occurrences that are primarily relevant to othersubjects.?
Reproducibility On a fixed corpus, counts ofword occurrences produce consistent results.Due to the dynamic nature of the Internet,numbers may fluctuate.3.3 Co-Occurrences and Derived FeaturesThe Naive Bayes coefficient estimate ?
?j may itselfbe interpreted as an intercept term plus a linear com-bination of features of the form log N(wj ,ak).Num.
of Labeled Occurrences Correlation1 - 5 0.0226 - 10 0.08211 - 25 0.11326 - 50 0.18351 - 75 0.28376 - 100 0.316Figure 1: Correlation between Supervised and Un-supervised Coefficient Estimates?
?j = logN(j,exc.
)/?i:si=1 |di|N(j,pr.
)/?i:si=?1 |di|(20)= log C1 + log N(j,exc.)
?
log N(j,pr.
)(21)We generalize this estimate as follows: for a col-lection of K different anchor words, we consider ageneral linear combination of logged co-occurrencecounts.?
?j =K?k=1?k log N(wj ,ak) (22)In the special case of a Naive Bayes model, ?k =1 when the k-th anchor word ak conveys positivesentiment, ?1 when it conveys negative sentiment.Replacing the logit estimate in Equation 9 withan estimate of this form, the model becomes:l?ogit(s|d) = ?
?0 +p?j=1dj?
?j (23)= ?
?0 +p?j=1K?k=1dj?k log N(wj ,ak)(24)= ?0 +K?k=1?kp?j=1dj log N(wj ,ak)(25)(26)This model has only K + 1 parameters:?0, ?1, .
.
.
, ?K .
These can be learned straightfor-wardly from labeled documents by a method suchas logistic regression.Observe that a document receives a score for eachanchor word?pj=1 dj log N(wj ,ak).
Effectively, thepredictor variables in this model are no longercounts of the original features dj .
Rather, they are?2.0 ?1.5 ?1.0 ?0.5 0.0 0.5 1.0 1.5?3?2?101234Traditional Naive Bayes Coefs.TurneyNaiveBayesCoefs.Unsupervised vs.
Supervised CoefficientsFigure 2: Unsupervised versus Supervised Coeffi-cient Estimatesinner products between the entire feature vector dand the logged co-occurence vector N(w,ak).
In thisrespect, the vector of logged co-occurrences is usedto produce derived feature.4 Data Analysis4.1 Accuracy of Unsupervised CoefficientsBy means of a Perl script that uses the Lynxbrowser, Version 2.8.3rel.1, we download AltaVistahit counts for queries of the form ?target NEARanchor.?
The initial list of targets consists of44,321 word types extracted from the Pang cor-pus of 1400 labeled movie reviews.
After pre-processing, this number is reduced to 28,629.3In Figure 1, we compare estimates produced bytwo Naive Bayes procedures.
For each feature wj ,we estimate ?j by using Turney?s procedure, andby fitting a traditional Naive Bayes model to thelabeled documents.
The traditional estimates aresmoothed by assuming a Beta prior distribution thatis equivalent to having four previous observations ofwj in documents of each class.q?1jq?
?1j= C24 +?i:si=1 dij4 +?i:si=?1 dij(27)where C2 =4p +?i:si=1 |di|4p +?i:si=?1 |di|(28)Here, dij is used to indicate feature presence:dij ={1 if wj appears in di0 otherwise (29)3We eliminate extremely rare words by requiring each targetto co-occur at least once with each anchor.
In addition, certaintypes, such as words containing hyphens, apostrophes, or otherpunctuation marks, do not appear to produce valid counts, sothey are discarded.Positive Negativebest awfulbrilliant badexcellent patheticspectacular poorwonderful worstFigure 3: Selected Anchor WordsWe choose this fitting procedure among several can-didates because it performs well in classifying testdocuments.In Figure 1, each entry in the right-hand col-umn is the observed correlation between these twoestimates over a subset of features.
For featuresthat occur in five documents or fewer, the corre-lation is very weak (0.022).
This is not surpris-ing, as it is difficult to estimate a coefficient fromsuch a small number of labeled examples.
Corre-lations are stronger for more common features, butnever strong.
As a baseline for comparison, NaiveBayes coefficients can be estimated using a subsetof their labeled occurrences.
With two independentsets of 51-75 occurrences, Naive Bayes coefficientestimates had a correlation of 0.475.Figure 2 is a scatterplot of the same coefficientestimates for word types that appear in 51 to 100documents.
The great majority of features do nothave large coefficients, but even for the ones thatdo, there is not a tight correlation.4.2 Additional AnchorsWe wish to learn how our model performance de-pends on the choice and number of anchor words.Selecting from WordNet synonym lists (Fellbaum,1998), we choose five positive anchor words andfive negative (Figure 3).
This produces a total of25 different possible pairs for use in producing co-efficient estimates.Figure 4 shows the classification performanceof unsupervised procedures using the 1400 labeledPang documents as test data.
Coefficients ?
?j are es-timated as described in Equation 22.
Several differ-ent experimental conditions are applied.
The meth-ods labeled ?Count?
use the original un-normalizedcoefficients, while those labeled ?Norm.?
have beennormalized so that the number of co-occurrenceswith each anchor have identical variance.
Resultsare shown when rare words (with three or fewer oc-currences in the labeled corpus) are included andomitted.
The methods ?pair?
and ?10?
describewhether all ten anchor coefficients are used at once,or just the ones that correspond to a single pair ofMethod Feat.
Misclass.
St.DevCount Pair >3 39.6% 2.9%Norm.
Pair >3 38.4% 3.0%Count Pair all 37.4% 3.1%Norm.
Pair all 37.3% 3.0%Count 10 > 3 36.4% ?Norm.
10 > 3 35.4% ?Count 10 all 34.6% ?Norm.
10 all 34.1% ?Figure 4: Classification Error Rates for DifferentUnsupervised Approachesanchor words.
For anchor pairs, the mean erroracross all 25 pairs is reported, along with its stan-dard deviation.Patterns are consistent across the different condi-tions.
A relatively large improvement comes fromusing all ten anchor words.
Smaller benefits arisefrom including rare words and from normalizingmodel coefficients.Models that use the original pair of anchor words,excellent and poor, perform slightly better than theaverage pair.
Whereas mean performance rangesfrom 37.3% to 39.6%, misclassification rates forthis pair of anchors ranges from 37.4% to 38.1%.4.3 A Smaller Unlabeled CorpusAs described in Section 3.2, there are several rea-sons to explore the use of a smaller unlabeled cor-pus, rather than the entire Internet.
In our experi-ments, we use additional movie reviews as our doc-uments.
For this domain, Pang makes available27,886 reviews.4Because this corpus offers dramatically fewer in-stances of anchor words, we modify our estimationprocedure.
Rather than discarding words that rarelyco-occur with anchors, we use the same feature setas before and regularize estimates by the same pro-cedure used in the Naive Bayes procedure describedearlier.Using all features, and ten anchor words with nor-malized scores, test error is 35.0%.
This suggeststhat comparable results can be attained while re-ferring to a considerably smaller unlabeled corpus.Rather than requiring several days of downloads,the count of nearby co-occurrences was completedin under ten minutes.Because this procedure enables fast access tocounts, we explore the possibility of dramaticallyenlarging our collection of anchor words.
We col-4This corpus is freely available on the following website:http://www.cs.cornell.edu/people/pabo/movie-review-data/.100 200 300 400 500 6000.300.320.340.360.380.40Num.
of Labeled DocumentsClassif.
ErrorMisclassification versus Sample SizeFigure 5: Misclassification with Labeled Docu-ments.
The solid curve represents a latent fac-tor model with estimated coefficients.
The dashedcurve uses a Naive Bayes classifier.
The two hor-izontal lines represent unsupervised estimates; theupper one is for the original unsupervised classifier,and the lower is for the most successful unsuper-vised method.lect data for the complete set of WordNet syn-onyms for the words good, best, bad, boring, anddreadful.
This yields a total of 83 anchor words,35 positive and 48 negative.
When all of these an-chors are used in conjunction, test error increases to38.3%.
One possible difficulty in using this auto-mated procedure is that some synonyms for a worddo not carry the same sentiment orientation.
For in-stance, intense is listed as a synonym for bad, eventhough its presence in a movie review is a stronglypositive indication.54.4 Methods with SupervisionAs demonstrated in Section 3.3, each anchor wordak is associated with a coefficient ?k.
In unsu-pervised models, these coefficients are assumed tobe known.
However, when labeled documents areavailable, it may be advantageous to estimate them.Figure 5 compares the performance of a modelwith estimated coefficient vector ?, as opposed tounsupervised models and a traditional supervisedapproach.
When a moderate number of labeled doc-uments are available, it offers a noticeable improve-ment.The supervised method used for reference in thiscase is the Naive Bayes model that is described insection 4.1.
Naive Bayes classification is of partic-ular interest here because it converges faster to itsasymptotic optimum than do discriminative meth-ods (Ng, A. Y. and Jordan, M., 2002).
Further, with5In the labeled Pang corpus, intense appears in 38 positiverev ews and only 6 negative ones.a larger number of labeled documents, its perfor-mance on this corpus is comparable to that of Sup-port Vector Machines and Maximum Entropy mod-els (Pang et al, 2002).The coefficient vector ?
is estimated by regular-ized logistic regression.
This method has been usedin other text classification problems, as in Zhangand Yang (2003).
In our case, the regularization6is introduced in order to enforce the beliefs that:?1 ?
?2, if a1, a2 synonyms (30)?1 ?
?
?2, if a1, a2 antonyms (31)For further information on regularized model fitting,see for instance, Hastie et al (2001).5 ConclusionIn business settings, there is growing interest inlearning product reputations from the Internet.
Forsuch problems, it is often difficult or expensive toobtain labeled data.
As a result, a change in mod-eling strategies is needed, towards approaches thatrequire less supervision.
In this paper we pro-vide a framework for allowing human-provided in-formation to be combined with unlabeled docu-ments and labeled documents.
We have found thatthis framework enables improvements over existingtechniques, both in terms of the speed of model es-timation and in classification accuracy.
As a result,we believe that this is a promising new approach toproblems of practical importance.ReferencesKushal Dave, Steve Lawrence, and David M. Pen-nock.
2003.
Mining the peanut gallery: Opinionextraction and semantic classification of productreviews.C.
Fellbaum.
1998.
Wordnet an electronic lexicaldatabase.T.
Hastie, R. Tibshirani, and J. Friedman.
2001.The Elements of Statistical Learning: Data Min-ing, Inference, and Prediction.
Springer-Verlag.Vasileios Hatzivassiloglou and Kathleen R. McKe-own.
1997.
Predicting the semantic orientationof adjectives.
In Philip R. Cohen and WolfgangWahlster, editors, Proceedings of the Thirty-FifthAnnual Meeting of the Association for Computa-tional Linguistics and Eighth Conference of theEuropean Chapter of the Association for Com-putational Linguistics, pages 174?181, Somerset,New Jersey.
Association for Computational Lin-guistics.6By cross-validation, we choose the regularization term ?
=1.5/sqrt(n), where n is the number of labeled documents.Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,and Toshikazu Fukushima.
2002.
Mining prod-uct reputations on the web.Ng, A. Y. and Jordan, M. 2002.
On discriminativevs.
generative classifiers: A comparison of logis-tic regression and naive bayes.
Advances in Neu-ral Information Processing Systems, 14.Kamal Nigam, Andrew K. McCallum, SebastianThrun, and Tom M. Mitchell.
2000.
Text clas-sification from labeled and unlabeled documentsusing EM.
Machine Learning, 39(2/3):103?134.Bo Pang, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?
senti-ment classification using machine learningtechniques.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP).P.D.
Turney and M.L.
Littman.
2002.
Unsupervisedlearning of semantic orientation from a hundred-billion-word corpus.Peter Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervisedclassification of reviews.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics (ACL?02), pages 417?424, Philadelphia, Pennsylvania.
Association forComputational Linguistics.Janyce Wiebe.
2000.
Learning subjective adjec-tives from corpora.
In Proc.
17th National Con-ference on Artificial Intelligence (AAAI-2000),Austin, Texas.Jian Zhang and Yiming Yang.
2003.
?robustness ofregularized linear classification methods in textcategorization?.
In Proceedings of the 26th An-nual International ACM SIGIR Conference (SI-GIR 2003).
