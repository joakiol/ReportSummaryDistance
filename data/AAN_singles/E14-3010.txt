Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 85?95,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Graph-Based Approach to String RegenerationMatic HorvatComputer LaboratoryUniversity of Cambridge15 JJ Thomson Avenue, CB3 0FD, U.K.mh693@cam.ac.ukWilliam ByrneDepartment of EngineeringUniversity of CambridgeTrumpington Street, CB2 1PZ, U.K.wjb31@cam.ac.ukAbstractThe string regeneration problem is theproblem of generating a fluent sentencefrom a bag of words.
We explore the N-gram language model approach to stringregeneration.
The approach computes thehighest probability permutation of the in-put bag of words under an N-gram lan-guage model.
We describe a graph-basedapproach for finding the optimal permuta-tion.
The evaluation of the approach on anumber of datasets yielded promising re-sults, which were confirmed by conduct-ing a manual evaluation study.1 IntroductionThe string regeneration problem can be stated as:given a bag of words taken from a fluent grammat-ical sentence, recover the original sentence.
As itis often difficult to recover the exact original sen-tence based solely on a bag of words, the problemis relaxed to generating a fluent version of the orig-inal sentence (Zhang and Clark, 2011).The string regeneration problem can generallybe considered a difficult problem even for humans.Consider the following bag of words:{ Iraq, list, in, a, third, joins, the, ., of,Bush?s, of, critics, policy, senator, re-publican }and try to recover the original sentence or at leasta fluent grammatical sentence.
The original sen-tence was:a third republican senator joins the listof critics of Bush?s policy in Iraq.The purpose of investigating and developing ap-proaches to solving the string regeneration prob-lem is grammaticality and fluency improvementof machine generated text.
The output of sys-tems generating text, including SMT, abstract-liketext summarisation, question answering, and dia-logue systems, often lacks grammaticality and flu-ency (Knight, 2007; Soricut and Marcu, 2005).The string regeneration problem is used as anapplication-independent method of evaluating ap-proaches for improving grammaticality and flu-ency of such systems.The string regeneration can also be viewed asa natural language realization problem.
The basictask of all realization approaches is to take a mean-ing representation as input and generate human-readable output.
The approaches differ on howmuch information is required from the meaningrepresentation, ranging from semantically anno-tated dependency graphs to shallow syntactic de-pendency trees.
A simple bag of words can then beconsidered as the least constrained input providedto a natural language realization system.
The bagof words can be combined with partial constraintsto form a more realistic meaning representation.Wan et al.
(2009) proposed an algorithm forgrammaticality improvement based on depen-dency spanning trees and evaluated it on the stringregeneration task.
They compared its performanceagainst a baseline N-gram language model genera-tor.
They found that their approach performs betterwith regards to BLEU score.
The latter approachdoes well at a local level but nonetheless often pro-duces ungrammatical sentences.We argue that the authors have not fully ex-plored the N-gram language model approach tostring regeneration.
They used a Viterbi-like gen-erator with a 4-gram language model and beampruning to find approximate solutions.
Addition-ally, the 4-gram language model was trained ona relatively small dataset of around 20 millionwords.The N-gram language model approach finds thehighest probability permutation of the input bag85of words under an N-gram language model as thesolution to the string regeneration problem.
Inthis paper we describe a graph-based approach tocomputing the highest probability permutation ofa bag of words.
The graph-based approach modelsthe problem as a set of vertices containing wordsand a set of edges between the vertices, whosecost equals language model probabilities.
Findingthe permutation with the highest probability in thegraph formulation is equal to finding the shortesttour in the graph or, equally, solving the TravellingSalesman Problem (TSP).
Despite the TSP beingan NP-hard problem, state-of-the-art approachesexist to solving large problem instances.
An in-troduction to TSP and its variants discussed in thispaper can be found in Applegate et al.
(2006b).In contrast to the baseline N-gram approach byWan et al.
(2009), our approach finds optimal so-lutions.
We built several models based on 2-gram,3-gram, and 4-gram language models.
We exper-imentally evaluated the graph-based approach onseveral datasets.
The BLEU scores and exampleoutput indicated that our approach is successfulin constructing a fairly fluent version of the orig-inal sentence.
We confirmed the results of auto-matic evaluation by conducting a manual evalua-tion.
The human judges were asked to compare theoutputs of two systems and decide which is morefluent.
The results are statistically significant andconfirm the ranking of the systems obtained us-ing the BLEU scores.
Additionally, we exploredcomputing approximate solutions with time con-straints.
We found that approximate solutions sig-nificantly decrease the quality of the output com-pared to optimal ones.This paper describes work conducted in theMPhil thesis by Horvat (2013).2 Graph-Based Approach to StringRegenerationThe underlying idea of the approach discussed inthis paper is to use an N-gram language model tocompute the probabilities of permutations of a bagof words and pick the permutation with the highestprobability as our solution.The probability of a sequence of words under anN-gram language model is computed as:logP (wn1) =n?k=1logP (wk|wk?1k?N+1)(1)2.1 Naive ApproachA naive approach to finding the permutation withthe highest probability is to enumerate all permu-tations, compute their probabilities using Equa-tion 1, and choose the permutation with the highestprobability as the solution.The time complexity of the naive approach isO(n ?
n!)
as we are enumerating all permuta-tions of n words and multiplying n conditionalprobabilities for each permutation.
This meansthat the naive approach is not viable for sen-tences of even moderate length.
For example,there are 3,628,800 permutations of 10 words and355,687,428,096,000 of 17 words.2.2 Bigram Graph-Based ApproachIn this section we define the graph-based approachto finding the highest probability permutation andconsequently our solution to the string regenera-tion problem.
For a bag of words S we define aset of symbols X , X = S ?
{<s>,</s>}, whichcontains all the words in S (with indexes appendedto distinguish repeated words) and the start andend of sentence symbols.
For a bigram languagemodel, N = 2, we define a directed weightedgraph G = (V,E), with the set of vertices definedas V = {wi|wi?
X}.
Therefore, each symbolin X is represented by a single vertex.
Let the setof edges E be a set of ordered pairs of vertices(wi, wj), such that E = {(wi, wj)|wi, wj?
V }.The edge cost is then defined as:cij=????????
?0if wi= </s>and wj= <s>,?
logP (wj|wi) if wi6= wj,?
otherwise.
(2)The conditional log probabilities of the formlogP (wj|wi) are computed by a bigram languagemodel.
Consequently, finding the sentence permu-tation with the highest probability under the bi-gram language model equals finding the shortesttour in graph G or, equally, solving the Asymmet-ric Travelling Salesman Problem (ATSP).
A gen-eral example graph for a sentence of length 3 isshown in Figure 1a.The individual cases of the edge cost functionpresented in Equation 2 ensure that the solutiontour is a valid sentence permutation.
The nega-tion of log probabilities transforms the problem of86-logP(w1|<s>)- log P (</s> | w3)- log P ( w2| <s>)-logP(w1|w2)-logP(w2|w1)-logP(w3|w2)-logP(w2|w3)-logP(w3|w1)-logP(w1|w3)-logP(</s>|w1)-logP(w3|<s>)-logP(</s>|w2)0<s>w1w2w3</s>(a) A general graph.
The edge cost equals the negated bigramconditional log probability of the destination vertex given theorigin vertex.
Only edges with non-infinite edge cost areshown in the graph.
Finding the shortest tour in the graphequals finding the sentence permutation with the highest prob-ability.032300201222010120233<s>sHL]HGD\</s> WKH(b) An example graph for the bag of words { day, seize, the}.
The shortest tour is shown in bold and represents the wordsequence <s> seize the day </s> with the log probability of?10.98.
It is necessary to include the (</s> <s>) edge inorder to complete the tour.Figure 1: Graphs modelling a general (Figure 1a) and an example (Figure 1b) bag of words of size threeunder a bigram language model.finding the longest tour in graph G to the commonproblem of finding the shortest tour.Figure 1b shows a graph for the example bagof words { day, seize, the }.
The shortest tour isshown in bold and represents the word sequence<s> seize the day </s>.
The shortest tour equalsthe sentence permutation with the highest proba-bility under the bigram language model.The number of vertices and edges in the graphgrows with the size n of the bag of words S rep-resented by the graph G = (V,E).
The size ofthe set of vertices V in the graph is |V | = n + 2and the size of the set of edges E is |E| = |V |2=n2+ 4n+ 4.We can draw several conclusions about thegraph-based approach from its equality to the TSP.Firstly, we can observe that the problem of find-ing the highest probability permutation is an NP-hard problem.
Secondly, modelling the problemas a TSP still presents a large improvement onthe naive approach described in Section 2.1.
Thetime complexity of the naive approach for a bag ofwords of size n equalsO(n ?n!).
However, the al-gorithm for solving the TSP with the best-knownrunning time guarantee has the time complexity ofO(n22n) (Held and Karp, 1962; Applegate et al.,2006b).
Although the required time grows expo-nentially with the length of the sentence, it growssignificantly slower than with the factorial timecomplexity.
This is illustrated in Table 1.n 5 10 15n22n800 102,400 7,372,800n ?
n!
600 36,288,000 19,615,115,520,000Table 1: Illustration of problem size growth at in-creasing values of n for algorithms with time com-plexity of O(n22n) and O(n ?
n!
).Finally, by modelling the problem as a TSP weare able to take advantage of the extensive re-search into the TSP and choose between hundredsof algorithms for solving it.
Even though no al-gorithm with lower running time guarantee thanO(n22n) has been discovered since the dynamicprogramming algorithm described by Held andKarp (1962), many algorithms that have no guar-antees but perform significantly better with mostgraph instances have been developed since.
Thesize of the largest optimally solved instance hasincreased considerably over the years, reaching85,900 vertices in 2006 (Applegate et al., 2009).For a more complete overview of the history andcurrent state-of-the-art computational approachesto solving the TSP we refer the reader to Apple-gate et al.
(2006b).872.3 Higher N-gram Order Graph-BasedApproachHigher order N-gram language models use longercontext compared to bigram language modelswhen computing the conditional probability of thenext word.
This usually results in improved proba-bility estimates for sequences of words.
Therefore,to improve our initial approach using bigrams, weextend it to higher order N-grams.
We first ex-plain the intuition behind the approach and thencontinue with the formal definition.The higher N-gram Order Graph-Based Ap-proach can be modelled as a Generalized Asym-metric Travelling Salesman Problem (GATSP).GATSP is the directed (asymmetric) version ofthe Generalized Travelling Salesman Problem(GTSP).
GTSP generalizes the TSP by groupingcities into sets called districts.
GTSP can then bedescribed as finding the shortest tour of length s,visiting exactly one city in each of the s districts.In our formulation of the graph G, each vertex hasa word sequence associated with it and the districtsare defined by the first word in the sequence.
Thismeans that each word appears exactly once in thesolution to the GATSP, ensuring that the solutionis a valid permutation.
A general example graphwith districts for N = 3 and a bag of words ofsize 3 is shown in Figure 2.This is formally defined as follows.
For a bag ofwords S we define a set of symbols X as before.For a general N-gram language model, N > 2,and a set of n symbols X , |X| = n, we definean n-partite directed weighted graph G = (V,E),with the set of vertices defined as:V = {wi|wi[j] ?
X for 1 ?
j ?
N -1,wi[j] 6= wi[k] for 1 ?
j < k < N}(3)Each vertex therefore represents a sequence ofsymbols wi[1..N -1] of length N ?
1 from the setX , and the symbols occurring in the sequencedo not repeat themselves.
The set of vertices Vis partitioned into n disjoint independent subsets,Vi= {wj|wj?
V,wj[1] = i}, based on the firstword in the word sequence, wj[1].Let the set of edges E be a set of ordered pairsof vertices (wi, wj), such that:E = {(wi, wj)|wi?
Vk, wj?
Vl, k 6= l,wi[2..N -1] = wj[1..N -2],wi[1] 6= wj[N -1]}(4)w3<s>w3w1w3w2w3w3w3</s><s> <s><s> w1<s> w2<s> w3<s> </s>w1<s>w1w1w1w2w1w3w1</s>w2<s>w2w1w2w2w2w3w2</s>V!ZZZ</s><s></s>w1</s>w2</s> w3</s></s>V!Figure 2: A general example graph for a bag ofwords of size 3 using a trigram language model.The graph consists of s = 5 districts.
The verticesare assigned to a district based on the first word ofthe word sequence associated with the vertex.
Twovertices together form a word sequence of threewords.
The cost of the edge between them equalsthe conditional probability of the final word giventhe context of the first two words and is providedby the trigram language model.
Only edges withnon-infinite cost are shown in the graph.
Findingthe shortest tour of length s, visiting each districtexactly once, equals finding the sentence permuta-tion with the highest probability.88cij=??????????????????
?0 if wi[N -1] = </s> and wj[N -1] = <s>,?
logP (wj[N -1]|wi[1..N -1])if wi[k] ?
S, 2 ?
k ?
N -2and wi[1] 6= </s> and wj[N -1] 6= <s>,?
logP (wj[N -1]|wi[x..N -1])if x ?
2 and wi[x] = <s>and wi[x-1] = </s>?
otherwise.
(5)An edge therefore exists between two vertices ifthey are parts of two different subsets of V (havedifferent first word in the sequence), have a match-ing subsequence, and the words outside the match-ing subsequence do not repeat between the twovertices.
The edge cost is defined in Equation 5.The conditional log probabilities of the formlogP (wj[N -1]|wi[1..N -1]) are computed by anN-gram language model.
Consequently, findingthe highest probability permutation under an N-gram language model equals solving the Gener-alized Asymmetric Travelling Salesman Problem(GATSP) modelled by graph G.An important condition for an edge to exist be-tween two vertices is that the word subsequencesassociated with the vertices match.
If two verticesmatch, they form a word sequence of length N .The conditional log probability of the last wordin the sequence given the previous N ?
1 wordsequals the cost of the edge between the vertices.An additional condition for an edge to exist be-tween two vertices is that the words outside of therequired matching subsequence do not repeat be-tween the vertices.
For example, two sequences 12 3 4 and 2 3 4 1 match according to the conditiondescribed above, but outside the required match-ing subsequence (2 3 4), word 1 appears twicewhich produces an invalid permutation.The size of the vertex and edge set of the graphG = (V,E) grows with the size n of the bag ofwords S and the order N of the N -gram languagemodel.
The size of the set of vertices V in thegraph is |V | = (n+2)N?1for all values of N .
Thesize of the set of edges E (including the infinitecost edges between the full set of vertices) is |E| =|V |2= (n+ 2)2N?2.3 ImplementationThe graph-based approach represents the problemof finding the sentence permutation with the high-est probability as an instance of the TSP.
Using abigram language model, the problem equals solv-ing the Asymmetric TSP.
Using a higher order N-gram language model, the problem equals solvingthe Generalized Asymmetric TSP.Both variations of the TSP are not as widelystudied as the basic TSP and fewer algorithms ex-ist for solving them.
Transforming the variationsof TSP to basic TSP is a solved problem that en-ables us to use state-of-the-art algorithms for solv-ing large problem instances of the TSP.
We de-cided to use the Concorde TSP Solver (Applegateet al., 2006a), which is prominent for continuouslyincreasing the size of the largest optimally solvedTSP instance over the last two decades.Alternatively, a heuristic algorithm for solvingthe TSP can be used.
Heuristic algorithms do notguarantee finding the optimal solution, but attemptto find the best possible solution given a time con-straint.
We used LKH as the heuristic TSP solver,which is an effective implementation of the Lin-Kernighan heuristic (Helsgaun, 2000).
It currentlyholds the record for many large TSP instances withunknown optima.The use of a TSP solver makes it necessaryto transform the instances of ATSP and GATSPinto regular TSP instances.
We applied twograph transformations as necessary: (1) GATSPto ATSP transformation described by Dimitrijevi?cand?Sari?c (1997) and (2) ATSP to TSP transfor-mation described by Jonker and Volgenant (1983).These transformations allow application of thegeneral TSP solver, although they each double thenumber of vertices in the graph.Table 2 shows the total size of the vertex set af-ter applying the transformations.LM 5 10 15 202-gram 14 24 34 443-gram 196 576 1,156 1,9364-gram 1,372 6,912 19,652 42,592Table 2: The vertex set size after applyingthe transformations for several N-gram languagemodels at increasing sentence length.894 EvaluationWe evaluated three different versions of the graph-based approach based on 2-gram, 3-gram, and 4-gram language models.
We evaluated each versionof the system on three datasets of news sentencesby computing the dataset-wide BLEU scores.4.1 Language modelsFor the experimental evaluation of our graph-based approach we used 2-gram, 3-gram, and 4-gram language models with interpolated modi-fied Kneser-Ney smoothing (Chen and Goodman,1998).
They were built using the SRI LanguageModeling Toolkit (Stolcke, 2002) and KenLMLanguage Model Toolkit (Heafield, 2011).The language models were estimated on theEnglish Gigaword collection (V2, AFP andXIN parts) and the NIST OpenMT12 EvaluationDataset (target sides of parallel data for Ar-Engand Ch-Eng tasks).The total size of the corpus for estimating the lan-guage models was 1.16 billion words.4.2 Evaluation metricThe BLEU evaluation metric was developed byPapineni et al.
(2002) as an inexpensive andfast method of measuring incremental progress ofSMT systems.
BLEU measures closeness of acandidate translation to a reference translation us-ing N-gram precision.
Similarly, in the string re-generation problem we measure the closeness ofthe regenerated sentence to the original sentence.We used the case insensitive NIST BLEU scriptv13 against tokenized references to compute theBLEU scores.Espinosa et al.
(2010) have investigated the useof various automatic evaluation metrics to mea-sure the quality of NLG output.
They foundthat BLEU correlates moderately well with humanjudgements of fluency and that it is useful for eval-uation of NLG output, but should be used withcaution, especially when comparing different sys-tems.
As the string regeneration problem is a basicform of NLG, BLEU is an appropriate measure ofthe system?s performance with regards to fluencyof the output.
We provide examples of output andconduct a manual evaluation to confirm that theBLEU scores of individual systems reflect actualchanges in output quality.4.3 Automatic EvaluationWe evaluated the graph-based approach on threedatasets:MT08 The target side of the Ar-Eng newswirepart of the NIST OpenMT08.MT09 The target side of the Ar-Eng newswirepart of the NIST OpenMT09.SR11 The plain text news dataset of the SurfaceRealisation Task at GenChal?11.The MT08, MT09, and SR11 datasets contain 813,586, and 2398 sentences respectively.We have taken preprocessing steps to chop longsentences into manageable parts, which is a com-mon practice in translation.
Based on prelimi-nary experiments we decided to limit the maxi-mum length of the chopped sentence to 20 words.N-gram models cannot be used to model sentencesshorter than N words in this approach.
In or-der to make the models comparable we ignoredshort sentences containing 4 or fewer words.
Eachchopped sentence was regenerated separately andthe regenerated chopped sentences were concate-nated to form the original number of dataset sen-tences.
We expect that the preprocessing steps in-creased the reported BLEU scores to a certain de-gree.
However, all systems compared in the exper-imental evaluation were subject to the same condi-tions and their scores are therefore comparable.The graphs constructed under a 4-gram lan-guage model are too large to solve optimally inreasonable time (i.e.
under half an hour per sen-tence).
Because of this, we employ two ap-proaches to regenerate long sentences with the 4-gram language model: (1) Use the LKH heuristicalgorithm with a set time limit, and (2) back-off tothe trigram language model.
We refer the readerto Horvat (2013) for details.The BLEU scores for the four systems are re-ported in Table 3.
The 3-gram graph-based ap-proach performed considerably better than the 2-gram approach, increasing the BLEU score for10 BLEU points or more on all three datasets.The 4-gram approach augmented with a heuris-tic TSP solver performed significantly worse thanthe 3-gram approach on MT08 and MT09 datasets,while performing better on SR11 dataset.
Thereason for this difference is the different distri-bution of chopped sentence lengths between thethree datasets.
Around one fourth of all chopped90LM Solver MT08 MT09 SR112g opt 44.4 45.1 40.63g opt 57.9 58.0 50.24g opt +heur 44.8 42.6 51.74g +3g opt 59.1 59.5 51.8Table 3: BLEU scores for four versions of thegraph-based approach, based on 2-gram, 3-gram,and 4-gram language models.
We used the 4-gramapproach on sentences of up to length 18.
The re-maining sentences were computed using either aheuristic TSP solver (opt +heur) or by backing-offto a 3-gram approach (4g +3g).sentences in MT08 and MT09 datasets are longerthan 18 words.
On the other hand, less than 1% ofchopped sentences of the SR11 dataset are longerthan 18 words.
This means that significant partsof the MT08 and MT09 datasets were solved us-ing the heuristic approach, compared to a smallpart of the SR11 dataset.
Using a heuristic TSPsolver therefore clearly negatively affects the per-formance of the system.
The 4-gram approachbacking-off to the 3-gram approach achieved ahigher BLEU score than the 3-gram approach overall datasets.In Figure 3 we show examples of regeneratedsentences for three versions of the system.
Inthe first example, we can see the improvementsin the output fluency with better versions of thesystem.
The improvements are reflected by theBLEU scores.
The 4-gram output can be consid-ered completely fluent.
However, when comparedto the original sentence, its BLEU score is not 100,due to the fact that the number of people killed andpeople injured are switched.
In this regard, BLEUscore is harsh and not an ideal evaluation metricfor the task.
In the second example, the origi-nal sentence contains complicated wording whichis reflected in poor performance of all three ver-sions of the system, despite the high BLEU scoreof the 3-gram system.
In the final example, we canobserve the gradual improvement of fluency overthe three versions of the system.
This is reflectedby the BLEU score, which reaches 100.0 for the4-gram system, which produced an identical sen-tence to the original.4.4 Manual EvaluationWe manually evaluated three versions of thegraph-based approach: 2-gram, 3-gram, and 4-gram system using 3-gram as back-off.
We con-ducted a pairwise comparison of the three sys-tems: for each evaluation sentence, we comparedthe output of a pair of systems and asked whichoutput is more fluent.We used the crowdsourcing website Crowd-Flower1to gather fluency judgments.
Judges wereasked ?Please read both sentences and compare thefluency of sentence 1 and sentence 2.?
They weregiven three options: ?Sentence 1 is more fluent?,?Sentence 2 is more fluent?, ?Sentence 1 and Sen-tence 2 are indistinguishable in fluency?.
The or-der of presentation of the two systems was ran-domized for each sentence.100 sentences of length between 5 and 18 wordswere chosen randomly from the combined MT08and MT09 dataset.
We gathered 5 judgements foreach sentence of a single pairwise comparison oftwo systems.
Each pairwise comparison of twosystems is therefore based on 500 human judge-ments.The platform measures the reliability of judgesby randomly posing gold standard questions inbetween regular questions.
If any judge incor-rectly answered a number of gold standard ques-tions, their judgements were deemed unreliableand were not used in the final result set.
A thor-ough discussion of suitability and reliability ofcrowdsourcing for NLP and SMT tasks and re-lated ethical concerns can be found in: Snow etal.
(2008), Zaidan and Callison-Burch (2011), andFort et al.
(2011).The pairwise comparison results are shown inTable 4.
Each number represents the proportion ofthe human judgements that rated the output of therow system as better than the column system.
Theraw numbers of pairwise comparison judgementsin favor of each system are shown in Table 5.
Aone-sided sign test indicated that we can reject thenull hypothesis of the two systems being equal infavor of the alternative hypothesis of the first sys-tem being better than the second for all three sys-tem pairings: 3g and 2g, 4g and 2g, and 4g and 3g,p < 0.001 for all three comparisons.
The man-ual evaluation results therefore confirm the BLEUscore differences between the three graph-basedsystems.Interestingly, in automatic evaluation the differ-ence in BLEU scores between 2g and 3g systemswas much bigger (around 10 BLEU points) than1http://crowdflower.com/91Hypothesis BLEU1.
REF meanwhile , azim stated that 10 people were killed and 94 injured in yesterday ?s clashes .
(a) meanwhile , azim and 10 people were injured in clashes yesterday ?s stated that killed 94 .
21.4(b) azim , meanwhile stated that 94 people were killed and 10 injured in yesterday ?s clashes .
50.4(c) meanwhile , azim stated that 94 people were killed and 10 injured in yesterday ?s clashes .
66.32.
REF zinni indicated in this regard that president mubarak wants egypt to work with the west .
(a) egypt wants zinni in this regard to work with president mubarak indicated that the west .
24.9(b) zinni wants egypt to work with the west that president mubarak indicated in this regard .
63.4(c) work with zinni indicated that president mubarak wants the west to egypt in this regard .
30.63.
REF he stressed that this direction is taking place in all major cities of the world .
(a) he stressed that the world is taking place in this direction of all major cities .
33.9(b) he stressed that all major cities of the world is taking place in this direction .
58.0(c) he stressed that this direction is taking place in all major cities of the world .
100.0Figure 3: Output examples of three versions of the graph-based approach: (a) 2-gram, (b) 3-gram, and(c) 4-gram with 3-gram back-off.
The original sentence is given for each of the three examples.
SentenceBLEU scores are shown for each regenerated sentence.LM 2g 3g 4g2g - - -3g 65.4 - -4g 72.9 69.2 -Table 4: Manual evaluation results of pairwisecomparison between three versions of the system:2-gram, 3-gram, and the 4-gram system with 3-gram back-off.
The numbers represent the per-centage of judgements in favor of the row systemwhen paired with the column system.the difference between 3g and 4g systems (around1 BLEU point).
However, in manual evaluationthe difference between 3g and 4g systems is no-ticeably bigger (69.2%) than the difference be-tween 2g and 3g systems (65.4%).sys1 sys2 sys1 equal sys2 Total2g 3g 124 142 234 5002g 4g 102 124 274 5003g 4g 92 201 207 500Table 5: The raw numbers of pairwise compari-son judgements between the three systems.
Thecolumns give the number of judgements in favorof each of the three options.5 Related WorkThe basic task of all natural language realizationapproaches is to take a meaning representation asinput and generate human-readable output.
Theapproaches differ on how much information is re-quired from the meaning representation.
Deeprepresentation include dependency graphs anno-tated with semantic labels and other syntactic in-formation (Belz et al., 2011).
Shallow represen-tations include syntactic dependency trees anno-tated with POS tags and other syntactic informa-tion (Belz et al., 2011), IDL-expressions (Soricutand Marcu, 2005), and Abstract Meaning Repre-sentation (Langkilde and Knight, 1998).Soricut and Marcu (2005) consider NLG incontext of other popular natural language appli-cations, such as Machine Translation, Summa-rization, and Question Answering.
They viewthese as text-to-text applications that produce tex-tual output from textual input.
Because of this,many natural language applications need to in-clude some form of natural language generationto produce the output text.
However, the natu-ral language generation in these applications is of-ten handled in an application-specific way.
Theypropose to use IDL-expressions as an application-independent representation language for text-to-text NLG.
The IDL-expressions are created fromstrings using operators to combine them.
The au-thors evaluate their approach on the string regen-eration task and achieve moderate BLEU scores.Wan et al.
(2009) approach the string regener-ation problem using dependency spanning trees.Their approach is to search for the most proba-ble dependency tree containing each word in theinput or, equally, finding the optimal spanningtree.
Zhang and Clark (2011) propose a simi-lar approach using Combinatory Categorial Gram-mar (CCG) which imposes stronger category con-straints on the parse structure compared to de-pendency trees investigated by Wan et al.
(2009).They primarily focus on the search problem offinding an optimal parse tree among all possible92trees containing any choice and ordering of the in-put words.
The CCG approach achieved higherBLEU scores compared to the approach proposedby Wan et al.
(2009).
Zhang et al.
(2012) improvethe CCG approach by Zhang and Clark (2011) byincorporating an N-gram language model.
de Gis-pert et al.
(2014) present a similar N-gram lan-guage model approach to ours with a different de-coder that does not guarantee optimal results.
Intheir comparison with approach by Zhang et al.
(2012) they report gains of more than 20 BLEUpoints.The purpose of studying and building ap-proaches to solving the string regeneration prob-lem is to improve grammaticality and fluency ofmachine generated text.
An approach using a TSPreordering model by Visweswariah et al.
(2011)focused on the preordering task in SMT.
In thepreordering task the words of the source sentenceare reordered to reflect the word order expected inthe target sentence which helps improve the per-formance of the SMT system.6 Conclusions and Future WorkIn the paper we explored the N-gram languagemodel approach to the string regeneration prob-lem of recovering a fluent version of the originalsentence given a bag of words.
The N-gram lan-guage model approach computes the highest prob-ability permutation of the input bag of words un-der an N-gram language model.
We described agraph-based approach for finding the optimal per-mutation.
Finding the permutation with the high-est probability in the graph formulation is equal tofinding the shortest tour in the graph or, equally,solving the Travelling Salesman Problem.We evaluated the proposed approach on threedatasets.
The BLEU scores and example outputindicated that the graph-based approach is suc-cessful in constructing a fairly fluent version ofthe original sentence.
The 2-gram based approachperformed moderately well but was surpassed bythe 3-gram based approach.
The 4-gram based ap-proach offered an improvement on the 3-gram butis not of much practical use due to its long compu-tation times.
Approximate solutions computed us-ing a heuristic TSP solver significantly reduced thequality of the output and resulting BLEU score.We confirmed the results of automatic evaluationby conducting a manual evaluation.The BLEU scores of our approach and the ap-proach by Wan et al.
(2009) can?t be directly com-pared as we used different evaluation datasets andpreprocessing procedures.
Nonetheless, the differ-ence in BLEU scores is stark, our best system out-performing theirs by more than 20 BLEU points.The work presented in this paper can be ex-tended in a number of ways.
More extensivecomparison between optimal and approximate ap-proaches would help draw stronger conclusions re-garding the need for optimality.
A direct compar-ison between our N-gram language model basedapproach and approaches presented by Wan et al.
(2009), Zhang et al.
(2012), and others is neededto determine its performance relative to other ap-proaches.The graph-based approach itself can be ex-tended in a number of ways.
Emulating meth-ods from Statistical Machine Translation, the ap-proach could be extended to generate an N-bestlist of reorderings.
A different method could thenbe used to rerank the N-best list to choose the bestone.
The methods can range from rescoring theoutputs with a higher-order language model or adependency language model, to using discrimina-tive machine learning.
The approach could alsobe extended to handle additional constraints in theinput, such as phrases instead of words, by modi-fying the edge weights of the graph.Another interesting area of future research re-lating to the wider string regeneration problem isdetermining the human performance on the task.Based on a simple trial of trying to regenerate along sentence by hand, it is clear that human per-formance on the task would not equal 100 BLEUpoints.
It would therefore be interesting to deter-mine the human performance on the string regen-eration problem to provide a contrast and a pointof comparison to the performance of machine sys-tems.Finally, the string regeneration problem can beviewed as a constraint satisfaction approach wherethe constraints are minimal.
However, in manyinstances there is more information available re-garding the final output of a system, for examplesyntactic or semantic relationship between words.This information introduces additional constraintsto the simple bag of words that need to be includedin the output.
In future, we will explore methodsof generating from a set of constraints in a robustmanner to produce output that is fluent and gram-matical.93AcknowledgmentsWe thank Juan Pino for his help with languagemodeling and experiments.
We thank Ann Copes-take and the anonymous reviewers for their discus-sions and suggestions.Matic Horvat was supported by the Ad FuturaScholarship during his work on the MPhil thesis.ReferencesDavid L. Applegate, Robert E. Bixby, Va?sek Chv?atal,and William J. Cook.
2006a.
Concorde TSP Solver.David L. Applegate, Robert E. Bixby, Va?sek Chv?atal,and William J. Cook.
2006b.
The Traveling Sales-man Problem: A Computational Study.
PrincetonUniversity Press.David L. Applegate, Robert E. Bixby, Va?sek Chv?atal,William Cook, Daniel G. Espinoza, Marcos Goy-coolea, and Keld Helsgaun.
2009.
Certification ofan optimal TSP tour through 85,900 cities.
Opera-tions Research Letters, 37(1):11?15, January.Anja Belz, Michael White, Dominic Espinosa, EricKow, Deirdre Hogan, and Amanda Stent.
2011.
TheFirst Surface Realisation Shared Task : Overviewand Evaluation Results.
In Proceedings of the 13thEuropean Workshop on Natural Language Genera-tion (ENLG), volume 2, pages 217?226.Stanley F. Chen and Joshua T. Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical report, Harvard Univer-sity.Adri`a de Gispert, Marcus Tomalin, and William Byrne.2014.
Word Ordering with Phrase-Based Gram-mars.
In Proceedings of the European Chapter ofthe Association for Computational Linguistics, num-ber 2009.Vladimir Dimitrijevi?c and Zoran?Sari?c.
1997.
AnEfficient Transformation of the Generalized Trav-eling Salesman Problem into the Traveling Sales-man Problem on DIgraphs.
Information Sciences,102:105?110.Dominic Espinosa, Rajakrishnan Rajkumar, MichaelWhite, and Shoshana Berleant.
2010.
Further Meta-Evaluation of Broad-Coverage Surface Realization.Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages564?574.Kar?en Fort, Adda Gilles, and K. Bretonnel Cohen.2011.
Amazon Mechanical Turk: Gold Mine orCoal Mine?
Computational Linguistics, 37(2):413?420.Kenneth Heafield.
2011.
KenLM : Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,number 2009, pages 187?197.Michael Held and Richard M. Karp.
1962.
A Dy-namic Programming Approach to Sequencing Prob-lems.
Society for Industrial and Applied Mathemat-ics, 10(1):196?210.Keld Helsgaun.
2000.
An effective implementa-tion of the LinKernighan traveling salesman heuris-tic.
European Journal of Operational Research,126(1):106?130.Matic Horvat.
2013.
A Graph-Based Approach toString Regeneration.
Ph.D. thesis.Roy Jonker and Ton Volgenant.
1983.
Transform-ing Asymmetric into Symmetric Traveling SalesmanProblems.
Operations Research Letters, 2(4):161?163.Kevin Knight.
2007.
Automatic language transla-tion generation help needs badly.
In MT Summit XIWork- shop on Using Corpora for NLG: Keynote Ad-dress, pages 5?8.Irene Langkilde and Kevin Knight.
1998.
Generationthat Exploits Corpus-Based Statistical Knowledge.In Proceedings of the 17th international conferenceon Computational linguistics, pages 704?710.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU : a Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), number July,pages 311?318, Philadelphia.Rion Snow, Brendan O Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and Fast But is it Good?
Evaluating Non-Expert Annotations for NaturalLanguage Tasks.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, number October, pages 254?263.Radu Soricut and Daniel Marcu.
2005.
TowardsDeveloping Generation Algorithms for Text-to-TextApplications.
In Proceedings of the 43rd AnnualMeeting of the ACL, number June, pages 66?74, AnnArbor.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing, pages 901?904.Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A Word Reordering Model for Im-proved Machine Translation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 486?496.Stephen Wan, Mark Dras, Robert Dale, and CecileParis.
2009.
Improving Grammaticality in Statis-tical Sentence Generation : Introducing a Depen-dency Spanning Tree Algorithm with an ArgumentSatisfaction Model.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL, numberApril, pages 852?860.94Omar F Zaidan and Chris Callison-Burch.
2011.Crowdsourcing translation: Professional qualityfrom non-professionals.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1220?1229.Yue Zhang and Stephen Clark.
2011.
Syntax-Based Grammaticality Improvement using CCG andGuided Search.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 1147?1157.Yue Zhang, Graeme Blackwood, and Stephen Clark.2012.
Syntax-Based Word Ordering Incorporating aLarge-Scale Language Model.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 736?746, Avignon, France.95
