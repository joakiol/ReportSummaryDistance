Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 1?6,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsClassifier Combination Techniques Applied to Coreference ResolutionSmita Vemulapalli1, Xiaoqiang Luo2, John F. Pitrelli2 and Imed Zitouni21Center for Signal and Image Processing (CSIP) 2IBM T. J. Watson Research CenterSchool of ECE, Georgia Institute of Technology 1101 Kitchawan RoadAtlanta, GA 30332, USA Yorktown Heights, NY 10598, USAsmita@ece.gatech.edu {xiaoluo,pitrelli,izitouni}@us.ibm.comAbstractThis paper examines the applicability of clas-sifier combination approaches such as baggingand boosting for coreference resolution.
Tothe best of our knowledge, this is the first ef-fort that utilizes such techniques for corefer-ence resolution.
In this paper, we provide ex-perimental evidence which indicates that theaccuracy of the coreference engine can po-tentially be increased by use of bagging andboosting methods, without any additional fea-tures or training data.
We implement and eval-uate combination techniques at the mention,entity and document level, and also address is-sues like entity alignment, that are specific tocoreference resolution.1 IntroductionCoreference resolution is the task of partitioning aset of mentions (i.e.
person, organization and loca-tion) into entities.
A mention is an instance of textualreference to an object, which can be either named(e.g.
Barack Obama), nominal (e.g.
the president) orpronominal (e.g.
he, his, it).
An entity is an aggre-gate of all the mentions (of any level) which refer toone conceptual entity.
For example, in the followingsentence:John said Mary was his sister.there are four mentions: John, Mary, his, andsister.John and his belong to the one entity since theyrefer to the same person; Mary and sister bothrefer to another person entity.
Furthermore, Johnand Mary are named mentions, sister is a nomi-nal mention and his is a pronominal mention.In this paper, we present a potential approach forimproving the performance of coreference resolu-tion by using classifier combination techniques suchas bagging and boosting.
To the best of our knowl-edge, this is the first effort that utilizes classifiercombination for improving coreference resolution.Combination methods have been applied to manyproblems in natural-language processing (NLP).
Ex-amples include the ROVER system (Fiscus, 1997)for speech recognition, the Multi-Engine MachineTranslation (MEMT) system (Jayaraman and Lavie,2005), and part-of-speech tagging (Brill and Wu,1998; Halteren et al, 2001).
Most of these tech-niques have shown a considerable improvement overthe performance of a single classifier and, therefore,lead us to consider implementing such a multiple-classifier system for coreference resolution as well.Using classifier combination techniques one canpotentially achieve a classification accuracy that issuperior to that of the single best classifier.
Thisis based on the assumption that the errors made byeach of the classifiers are not identical, and there-fore if we intelligently combine multiple classifieroutputs, we may be able to correct some of these er-rors.The main contributions of this paper are:?
Demonstrating the potential for improvement inthe baseline ?
By implementing a system thatbehaves like an oracle, we have shown that theoutput of the combination of multiple classifiershas the potential to be significantly higher in ac-curacy than any of the individual classifiers.?
Adapting traditional bagging techniques ?
Mul-tiple classifiers, generated using bagging tech-niques, were combined using an entity-level sum1rule and mention-level majority voting.?
Implementing a document-level boosting algo-rithm ?
A boosting algorithm was implementedin which a coreference resolution classifier wasiteratively trained using a re-weighted trainingset, where the reweighting was done at the doc-ument level.?
Addressing the problem of entity alignment ?In order to apply combination techniques tomultiple classifiers, we need to address entity-alignment issues, explained later in this paper.The baseline coreference system we use is sim-ilar to the one described by Luo et al (Luo et al,2004).
In such a system, mentions are processedsequentially, and at each step, a mention is eitherlinked to one of existing entities, or used to create anew entity.
At the end of this process, each possiblepartition of the mentions corresponds to a unique se-quence of link or creation actions, each of which isscored by a statistical model.
The one with the high-est score is output as the final coreference result.2 Classifier Combination Techniques2.1 BaggingOne way to obtain multiple classifiers is via baggingor bootstrap aggregating (Breiman, 1996).
Theseclassifiers, obtained using randomly-sampled train-ing sets, may be combined to improve classification.We generated several classifiers by two tech-niques.
In the first technique, we randomly samplethe set of documents (training set) to generate a fewclassifiers.
In the second technique, we need to re-duce the feature set and this is not done in a randomfashion.
Instead, we use our understanding of the in-dividual features and also their relation to other fea-tures to decide which features may be dropped.2.2 OracleIn this paper, we refer to an oracle system whichuses knowledge of the truth.
Here, truth, called thegold standard henceforth, refers to mention detec-tion and coreference resolution done by a human foreach document.
It is possible that the gold standardmay have errors and is not perfect truth, but, as inmost NLP systems, it is considered the reference forevaluating computer-based coreference resolution.To understand the oracle, consider an example inwhich the outputs of two classifiers for the same in-put document are C1 and C2, as shown in Figure 1.C2-EPC2-EQC2-ERC2-ESC1-EAC1-EBC1-ECC1-EDG-E1G-E2G-E3G-E4C1-EAC2-ERC1-EDC2-ES0.72 0.661.0 0.850.880.780.75GoldGClassifier C2File X File X File X File XClassifier C1 Oracle OutputFigure 1: Working of the oracleThe number of entities in C1 and C2 may not be thesame and even in cases where they are, the numberof mentions in corresponding entities may not be thesame.
In fact, even finding the corresponding entityin the other classifier output or in the gold standardoutput G is not a trivial problem and requires us tobe able to align any two classifier outputs.The alignment between any two coreference la-belings, say C1 and G, for a document is the bestone-to-one map (Luo, 2005) between the entities ofC1 and G. To align the entities of C1 with those ofG, under the assumption that an entity in C1 maybe aligned with at most only one entity in G andvice versa, we need to generate a bipartite graphbetween the entities of C1 and G. Now the align-ment task is a maximum bipartite matching prob-lem.
This is solved by using the Kuhn-Munkres al-gorithm (Kuhn, 1955; Munkres, 1957).
The weightsof the edges of the graph are entity-level alignmentmeasures.
The metric we use is a relative mea-sure of the similarity between the two entities.
Tocompute the similarity metric ?
(Luo, 2005) for theentity pair (R,S), we use the formula shown inEquation 1, where (?)
represents the commonal-ity with attribute-weighted partial scores.
Attributesare things such as (ACE) entity type, subtype, entityclass, etc.?
(R,S) = 2 |R ?
S||R|+ |S| (1)The oracle output is a combination of the entitiesin C1 and C2 with the highest entity-pair alignmentmeasures with the entities in G.1 We can see in Fig-ure 1 that the entity G-E1 is aligned with entities C1-EA and C2-EP.
We pick the entity with the highestentity-pair alignment measure (highlighted in gray)which, in this case, is C1-EA.
This is repeated for1A mention may be repeated across multiple output entities,which is not an unwarranted advantage as the scorer insists onone-to-one entity alignment.
So if there are two entities con-taining mention A, at most one mention A is credited and theother will hurt the score.2F-E1F-E2F-E3F-E4C2-EPC2-EQC2-ERC2-ESC1-EAC1-EBC1-ECC1-ED0.72 0.61.0 0.850.88 0.780.75Full FFile X File X File XClassifier C1C2-ES F-E4C1-ED C2-EQ F-E3C1-EB  C2-ER F-E2C1-EA  C2-EP F-E1Entity-levelAlignment TableClassifier C2Figure 2: Entity alignment between classifier outputsevery entity in G. The oracle output can be seen inthe right-hand side of Figure 1.
This technique canbe scaled up to work for any number of classifiers.2.3 Preliminary Combination ApproachesImitating the oracle.
Making use of the existingframework of the oracle, we implement a combina-tion technique that imitates the oracle except that inthis case, we do not have the gold standard.
If wehave N classifiers Ci, i = 1 to N , then we replacethe gold standard by each of theN classifiers in suc-cession, to get N outputs Combi, i = 1 to N .The task of generating multiple classifier combi-nation outputs that have a higher accuracy than theoriginal classifiers is often considered to be easierthan the task of determining the best of these out-puts.
We used the formulas in Equations 2, 3 and 4to assign a score Si to each of the N combinationoutputs Combi, and then we pick the one with thehighest score.
The function Sc (which correspondsto the function ?
in Equation 1) gives the similaritybetween the entities in the pair (R,S).Si = 1N ?
1?j = 1 to Nj 6= iSc(Combi, Cj) (2)Si = Sc(Combi, Ci) (3)Si = 1N ?
1?j = 1 to Nj 6= iSc(Combi, Combj) (4)Entity-level sum-rule.
We implemented a basic sum-rule at the entity level, where we generate only onecombination classifier output by aligning the entitiesin the N classifiers and picking only one entity ateach level of alignment.
In the oracle, the referencefor entity-alignment was the gold standard.
Here,we use the baseline/full system (generated using theentire training and feature set) to do this.
The entity-level alignment is represented as a table in Figure 2.Let Ai, i = 1 to M be the aligned entities in onerow of the table in Figure 2.
Here, M ?
N ifA A1   A2   A3   A4  ?B B1   B2           B4  ?C C1   C2   C3   C4  ?D        D2   D3   D4  ?3010A{m1,m2,m6}B{ m3}C{ m4,m5}D{m7 }Entity-level Alignment TableMention m1Mention Count for m1 Output Majority Voting for mention m1Figure 3: Mention-level majority votingwe exclude the baseline from the combination andM ?
N + 1 if we include it.
To pick one entityout of these M entities, we use the traditional sumrule (Tulyakov et al, 2008), shown in Equation 5, tocompute the S(Ai) for each Ai and pick the entitywith the highest S(Ai) value.S(Ai) =?j = 1 to Nj 6= iSc(Ai, Aj) (5)2.4 Mention-level Majority VotingIn the previous techniques, entities are either pickedor rejected as a whole but never broken down fur-ther.
In the mention-level majority voting technique,we work at the mention level, so the entities createdafter combination may be different from the entitiesof all the classifiers that are being combined.In the entity-level alignment table (shown in Fig-ure 3), A, B, C and D refer to the entities in the base-line system and A1, A2, ..., D4 represent the enti-ties of the input classifiers that are aligned with eachof the baseline classifier entities.
Majority voting isdone by counting the number of times a mention isfound in a set of aligned entities.
So for every rowin the table, we have a mention count.
The row withthe highest mention count is assigned the mention inthe output.
This is repeated for each mention in thedocument.
In Figure 3, we are voting for the men-tion m1, which is found to have a voting count of 3(the majority vote) at the entity-level A and a countof 1 at the entity-level C, so the mention is assignedto the entity A.
It is important to note that some clas-sifier entities may not align with any baseline clas-sifier entity as we allow only a one-to-one mappingduring alignment.
Such entities will not be a part ofthe alignment table.
If this number is large, it mayhave a considerable effect on the combination.2.5 Document-level BoostingBoosting techniques (Schapire, 1999) combine mul-tiple classifiers, built iteratively and trained onre-weighted data, to improve classification accu-racy.
Since coreference resolution is done for awhole document, we can not split a document fur-3Testdocumentswithpercentile< P threshand F-measure< F threshbc bn cts nw un wl# Training documents : # Test documentsratio for every genre is maintainedTrain TestTrain TestTrain TestTrain TestTrain TestDocumentsto boostTraining SetShuffleBoosting of TrainingSetTraining SetFigure 4: Document-level boostingther.
So when we re-weight the training set, weare actually re-weighting the documents (hence thename document-level boosting).
Figure 4 shows anoverview of this technique.The decision of which documents to boost ismade using two thresholds: percentile thresholdPthresh and the F-measure threshold Fthresh.
Doc-uments in the test set that are in the lowest Pthreshpercentile and that have a document F-measure lessthan Fthresh will be boosted in the training set forthe next iteration.
We shuffle the training set to cre-ate some randomness and then divide it into groupsof training and test sets in a round-robin fashion suchthat a predetermined ratio of the number of trainingdocuments to the number of test documents is main-tained.
In Figure 4, the light gray regions refer totraining documents and the dark gray regions referto test documents.
Another important considerationis that it is difficult to achieve good coreference res-olution performance on documents of some genrescompared to others, even if they are boosted signif-icantly.
In an iterative process, it is likely that doc-uments of such genres will get repeatedly boosted.Also our training set has more documents of somegenres and fewer of others.
So we try to maintain, tosome extent, the ratio of documents from differentgenres in the training set while splitting this trainingset further into groups of training and test sets.3 EvaluationThis section describes the general setup used to con-duct the experiments and presents an evaluation ofthe combination techniques that were implemented.Experimental setup.
The coreference resolutionsystem used in our experiments makes use of a Max-imum Entropy model which has lexical, syntacti-cal, semantic and discourse features (Luo et al,Table 1: Statistics of ACE 2005 dataDataSet #Docs #Words #Mentions #EntitiesTraining 499 253771 46646 16102Test 100 45659 8178 2709Total 599 299430 54824 18811Table 2: Accuracy of generated and baseline classifiersClassifier Accuracy (%)C1 ?
C15 Average 77.52Highest 79.16Lowest 75.81C0 Baseline 78.532004).
Experiments are conducted on ACE 2005data (NIST, 2005), which consists of 599 documentsfrom rich and diversified sources.
We reserve thelast 16% documents of each source as the test set,and use the rest of the documents as the training set.The ACE 2005 data split is tabulated in Table 1.Bagging A total of 15 classifiers (C1 to C15) weregenerated, 12 of which were obtained by samplingthe training set and the remaining 3 by samplingthe feature set.
We also make use of the base-line classifier C0.
The accuracy of C0 to C15 hasbeen summarized in Table 2.
The agreement be-tween the classifiers?
output was found to be in therange of 93% to 95%.
In this paper, the metric usedto compute the accuracy of the coreference resolu-tion is the Constrained Entity-Alignment F-Measure(CEAF) (Luo, 2005) with the entity-pair similaritymeasure in Equation 1.Oracle.
To conduct the oracle experiment, we train1 to 15 classifiers and align their output to the goldstandard.
For all entities aligned with a gold entity,we pick the one with the highest score as the output.We measure the performance for varying number ofclassifiers, and the result is plotted in Figure 5.First, we observe a steady and significant increasein CEAF for every additional classifier, because ad-ditional classifiers can only improve the alignmentscore.
Second, we note that the oracle accuracy is87.58% for a single input classifier C1, i.e.
an abso-lute gain of 9% compared to C0.
This is because theavailability of gold entities makes it possible to re-move many false-alarm entities.
Finally, the oracleaccuracy when all 15 classifiers are used as input is94.59%, a 16.06% absolute improvement.This experiment helps us to understand the perfor-mance bound of combining multiple classifiers andthe contribution of every additional classifier.Preliminary combination approaches.
While theoracle results are encouraging, a natural question is475808590951000246810121416Accuracy (%)Numberof ClassifiersbaselineFigure 5: Oracle performance vs. number of classifiers7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56ClassifierC1ClassifierC2CombinationOutput Classifier  C37-10 7-17 7-18 7-19 7-27 7-30 15-22 20-33 20-68 37-567-10 7-17 7-27Legend:Type ImentionsType IImentionsType IIImentionsType IVmentions7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56BaselineC   7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66Figure 6: A real example showing the working ofmention-level majority votinghow much performance gain can be attained if thegold standard is not available.
To answer this ques-tion, we replace the gold standard with one of theclassifiers C1 to C15, and align the classifiers.
Thisis done in a round robin fashion as described in Sec-tion 2.3.
The best performance of this procedure is77.93%.
The sum-rule combination output had anaccuracy of 78.65% with a slightly different base-line of 78.81%.
These techniques do not yield a sta-tistically significant increase in CEAF but this is notsurprising as C1 to C15 are highly correlated.Mention-level majority voting.
This experiment isconducted to evaluate the mention-level majorityvoting technique.
The results are not statisticallybetter than the baseline, but they give us valuableinsight into the working of the combination tech-nique.
The example in Figure 6 shows a singleentity-alignment level for the baselineC0 and 3 clas-sifiers C1, C2, and C3 and the combination outputby mention-level majority voting.
The mentions aredenoted by the notation ?EntityID - MentionID?, forexample 7-10 is the mention with EntityID=7 andMentionID=10.
Here, we use the EntityID in thegold file.
The mentions with EntityID=7 are ?cor-rect?
i.e.
they belong in this entity, and the othersare ?wrong?
i.e.
they do not belong in this entity.The aligned mentions are of four types:?
Type I mentions ?
These mentions have a highestvoting count of 2 or more at the same entity-levelalignment and hence appear in the output.?
Type II mentions ?
These mentions have a high-est voting count of 1.
But they are present inmore than one input classifier and there is a tiebetween the mention counts at different entity-level alignments.
The rule to break the tie isthat mentions are included if they are also seenin the full system C0.
As can been seen, this rulebrings in correct mentions such as 7-61, 7-63,7-64, but it also admits 20-33,20-39 and 20-62.In the oracle, the gold standard helps to removeentities with false-alarm mentions, whereas thefull system output is noisy and it is not strongenough to reliably remove undesired mentions.?
Type III mentions ?
There is only one mention20-66 which is of this type.
It is selected in thecombination output since it is present in C2 andthe baseline C0, although it has been rejected asa false-alarm in C1 and C3.?
Type IV mentions ?
These false-alarm mentions(relative to C0) are rejected in the output.
As canbe seen, this correctly rejects mentions such as15-22 and 20-68, but it also rejects correct men-tions 7-18, 7-19 and 7-30.In summary, the current implementation of thistechnique has a limited ability to distinguish correctmentions from wrong ones due to the noisy natureof C0 which is used for alignment.
We also observethat mentions spread across different alignments of-ten have low-count and they are often tied in count.Therefore, it is important to set a minimum thresh-old for accepting these low-count majority votes andalso investigate better tie-breaking techniques.Document-level Boosting This experiment is con-ducted to evaluate the document-level boosting tech-nique.
Table 3 shows the results with the ratioof the number of training documents to the num-ber of test documents equal to 80:20, F-measurethreshold Fthresh = 74% and percentile thresholdPthresh = 25%.
The accuracy increases by 0.7%,relative to the baseline.
Due to computational com-plexity considerations, we used fixed values for theparameters.
Therefore, these values may be sub-optimal and may not correspond to the best possibleincrease in accuracy.4 Related WorkA large body of literature related to statistical meth-ods for coreference resolution is available (Ng andCardie, 2003; Yang et al, 2003; Ng, 2008; Poon and5Table 3: Results of document-level boostingIteration Accuracy (%)1 78.532 78.823 79.084 78.37Domingos, 2008; McCallum and Wellner, 2003).Poon and Domingos (Poon and Domingos, 2008)use an unsupervised technique based on joint infer-ence across mentions and Markov logic as a repre-sentation language for their system on both MUCand ACE data.
Ng (Ng, 2008) proposed a genera-tive model for unsupervised coreference resolutionthat views coreference as an EM clustering process.In this paper, we make use of a coreference enginesimilar to the one described by Luo et al (Luo et al,2004), where a Bell tree representation and a Maxi-mum entropy framework are used to provide a natu-rally incremental framework for coreference resolu-tion.
To the best of our knowledge, this is the first ef-fort that utilizes classifier combination techniques toimprove coreference resolution.
Combination tech-niques have earlier been applied to various applica-tions including machine translation (Jayaraman andLavie, 2005), part-of-speech tagging (Brill and Wu,1998) and base noun phrase identification (Sang etal., 2000).
However, the use of these techniques forcoreference resolution presents a unique set of chal-lenges, such as the issue of entity alignment betweenthe multiple classifier outputs.5 Conclusions and Future WorkIn this paper, we examined and evaluated the ap-plicability of bagging and boosting techniques tocoreference resolution.
We also provided empir-ical evidence that coreference resolution accuracycan potentially be improved by using multiple clas-sifiers.
In future, we plan to improve (1) the entity-alignment strategy, (2) the majority voting techniqueby setting a minimum threshold for the majority-vote and better tie-breaking, and (3) the boostingalgorithm to automatically optimize the parametersthat have been manually set in this paper.
Anotherpossible avenue for future work would be to testthese combination techniques with other coreferenceresolution systems.AcknowledgmentsThe authors would like to acknowledge Ganesh N.Ramaswamy for his guidance and support in con-ducting the research presented in this paper.ReferencesL.
Breiman.
1996.
Bagging predictors.
In MachineLearning.E.
Brill and J. Wu.
1998.
Classifier combination for im-proved lexical disambiguation.
In Proc.
of COLING.J.
Fiscus.
1997.
A post-processing system to yield re-duced word error rates: Recogniser output voting errorreduction (rover).
In Proc.
of ASRU.H.
V. Halteren et al 2001.
Improving accuracy inword class tagging through the combination of ma-chine learning systems.
Computational Linguistics,27.S.
Jayaraman and A. Lavie.
2005.
Multi-engine machinetranslation guided by explicit word matching.
In Proc.of ACL.H.
W. Kuhn.
1955.
The hungarian method for the assign-ment problem.
Naval Research Logistics Quarterly, 2.X.
Luo et al 2004.
A mention-synchronous coreferenceresolution algorithm based on the bell tree.
In Proc.
ofACL.X.
Luo.
2005.
On coreference resolution performancemetrics.
In Proc.
of EMNLP.A.
McCallum and B. Wellner.
2003.
Toward condi-tional models of identity uncertainty with applicationto proper noun coreference.
In Proc.
of IJCAI/IIWeb.J.
Munkres.
1957.
Algorithms for the assignment andtransportation problems.
Journal of the Society of In-dustrial and Applied Mathematics, 5(1).V.
Ng and C. Cardie.
2003.
Bootstrapping coreferenceclassifiers with multiple machine learning algorithms.In Proc.
of EMNLP.V.
Ng.
2008.
Unsupervised models for coreference reso-lution.
In Proc.
of EMNLP.NIST.
2005.
ACE?05 evaluation.
www.nist.gov/speech/tests/ace/ace05/index.html.H.
Poon and P. Domingos.
2008.
Joint unsupervisedcoreference resolution with Markov Logic.
In Proc.of EMNLP.E.
F. T. K. Sang et al 2000.
Applying system combi-nation to base noun phrase identification.
In Proc.
ofCOLING 2000.R.E.
Schapire.
1999.
A brief introduction to boosting.
InProc.
of IJCAI.S.
Tulyakov et al 2008. Review of classifier combi-nation methods.
In Machine Learning in DocumentAnalysis and Recognition.X.
Yang et al 2003.
Coreference resolution using com-petition learning approach.
In Proc.
of ACL.6
