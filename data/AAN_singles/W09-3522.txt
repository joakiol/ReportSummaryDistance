Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 100?103,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPTransliteration System using pair HMM with weighted FSTsPeter NabendeAlfa Informatica, CLCG,University of Groningen, Netherlandsp.nabende@rug.nlAbstractThis paper presents a transliteration systembased on pair Hidden Markov Model (pairHMM) training and Weighted Finite StateTransducer (WFST) techniques.
Parametersused by WFSTs for transliteration generationare learned from a pair HMM.
Parametersfrom pair-HMM training on English-Russiandata sets are found to give better transliterationquality than parameters trained for WFSTs forcorresponding structures.
Training a pairHMM on English vowel bigrams and standardbigrams for Cyrillic Romanization, and usinga few transformation rules on generated Rus-sian transliterations to test for context im-proves the system?s transliteration quality.1 IntroductionMachine transliteration is the automatic trans-formation of a word in a source language to aphonetically equivalent word in a target languagethat uses a different writing system.
Translitera-tion is important for various Natural LanguageProcessing (NLP) applications including: CrossLingual Information Retrieval (CLIR), and Ma-chine Translation (MT).
This paper introduces asystem that utilizes parameters learned for a pairHidden Markov Model (pair HMM) in a sharedtransliteration generation task1.
The pair HMMhas been used before (Mackay and Kondrak,2005; Wieling et al, 2007) for string similarityestimation, and is based on the notion of stringEdit Distance (ED).
String ED is defined here asthe total edit cost incurred in transforming asource language string (S) to a target languagestring (T) through a sequence of edit operations.The edit operations include: (M)atching an ele-ment in S with an element in T; (I)nserting anelement into T, and (D)eleting an element in S.1The generation task is part of the NEWS 2009 machinetransliteration shared task  (Li et al, 2009)Based on all representative symbols used foreach of the two languages, emission costs foreach of the edit operations and transition parame-ters can be estimated and used in measuring thesimilarity between two strings.
To generatetransliterations using pair HMM parameters,WFST (Graehl, 1997) techniques are adopted.Transliteration training is based mainly on theinitial orthographic representation and no explicitphonetic scheme is used.
Instead, transliterationquality is tested for different bigram combina-tions including all English vowel bigram combi-nations and n-gram combinations specified forCyrillic Romanization by the US Board on Geo-graphic Names and British Permanent Commit-tee on Geographic Names (BGN/PCGN).
How-ever, transliteration parameters can still be esti-mated for a pair HMM when a particular phonet-ic representation scheme is used.The quality of transliterations generated usingpair HMM parameters is evaluated against trans-literations generated from training WFSTs andtransliterations generated using a Phrase-basedStatistical Machine Translation (PBSMT) sys-tem.
Section 2 describes the components of thetransliteration system that uses pair HMM para-meters; section 3 gives the experimental set upand results associated with the transliterationsgenerated; and section 4 concludes the paper.2 Machine Transliteration SystemThe transliteration system comprises of a trainingand generation components (Figure 1).
In thetraining component, the Baum-Welch Expecta-tion Maximization (EM) algorithm (Baum et al,1970) is used to learn the parameters of a pairHMM.
In the generation component, WFSTtechniques (Graehl, 1997) model the learned pairHMM parameters for generating transliterations.2.1 Parameter Estimation for a pair-HMMA pair HMM has two output observations (Fig-ure 2) that are aligned through the hidden states,100Figure 1: Machine Transliteration systemFigure 2: pair-HMM alignment for converting anEnglish string ?Peter?
to a Russian string ?????
?unlike the classic HMMs that have only one ob-servation sequence.
The pair HMM structure dif-fers from that of WFSTs in that in WFSTs theinput and output symbols and associated weightsoccur on a transition arc while for the pair HMM,the input and output symbols and associated editcosts are encoded in a node.
Two main sets ofparameters are learned for the pair HMM: transi-tion parameters (?, ?, ?, ?M, ?DI) as shown in Fig-ure 3 for different state transitions; and emissionparameters in the (M)atch state and the other twogap states (D and I).
si in Figure 3 is the ith sym-bol in the source language string S while tj is thejth symbol in T.Figure 3: Pair Hidden Markov Model [Adapted fromMackay and Kondrak, 2005]Pair HMM Emission parameters are stored inmatrix form in three tables associated with theedit operations; transition parameters are alsostored in matrix form in a table.
The emissionparameters are ( )n m n m?
+ +  in total; n and mare the numbers of symbols in the pair HMMsource language alphabet (VS) and target lan-guage alphabet (VT) respectively.
The parametersof starting in a given edit operation state are de-rived from the parameters of transiting from thematch state (M) to either D or I or back to M.Although pair HMM training is evaluatedagainst WFST training, there is no major differ-ence in the training approach used in both cases;a forward-backward EM algorithm is used ineach case.
The main difference is in the struc-ture; for the pair-HMM, the state transition pa-rameter is also incorporated into the weight thatmeasures the level of relationship between theinput and output symbol when transformed to aWFST arc.2.2 Generating Transliterations in WFSTsA Weighted Finite State Transducer is a finiteautomaton whose state transitions are labeledwith input and output elements and weights thatexpress the level of relationship between the in-put and output elements.
Although the frame-work of WFSTs has mostly been applied inrepresenting various models for speech recogni-tion (Mohri et al, 2008) including HMMs,WFSTs have as well been used for transliteration(Knight and Graehl, 1998), and are the most suit-able for modeling pair HMM constraints for ge-nerating transliterations.
In the WFST frame-work, it is possible to specify various configura-tions associated with constraints inherent in aparticular model.
Figure 4 shows a WFST thatprecisely corresponds to the structure of the pairFigure 4: Finite State Transducer corresponding to thepair HMM.M start endDIsi:ee:tje:tje:tjsi:esi:ee:ee:ee:ee:ee:ee:esi:tjsi:tj si:tjP : ?Me : ?Mt : ?Me : _Dr : ?M Endsitjsitj?M endDI?DI?DI??M???
?1- ?- ?- ?DI1- ?- ?- ?DI1-2?- ?MPairs of correcttransliterationsTransliteration parame-ter estimation for pairHMMEstimatedparametersTransliteration gen-eration usingWeighted Finite StateTransducersSourcenameTargetname101HMM considering the constraints specified forthe pair HMM.
In Figure 4, e is an empty symbolwhile si and sj are as defined for the pair HMM inFigure 3.
Note that, in Figure 4, a start state isneeded to model pair HMM parameter con-straints for starting in any of the three edit states.However, it is possible to specify a WFST cor-responding to the pair HMM with no start state.Various WFST configurations that do not con-form to the bias corresponding to the pair HMMconstraints had low transliteration quality and forspace limitations, are not reported in this paper.2.3 Transformation RulesA look into the transliterations generated usingpair HMM parameters on English-Russian de-velopment data showed consistent mistranslitera-tions mainly due to lack of contextual modelingin the generated transliterations.
For example inall cases where the Russian character ?
?l?
pre-cedes the Russian soft sign ?
? '
?, the Russiansoft sign was missing, resulting into a loss oftransliteration accuracy.
Two examples of mi-stransliterations that do not include the Russiansoft sign ?
are: ???????
instead of ????????
?krefeld?, and ??????
instead of ????????bilbao?.
For such cases, simple transformationrules, such as ??????
were defined on the out-put transliterations in a post processing step.
25transformation rules were specified for some ofthe mistransliterations to test the effect of model-ing context.2.4 Transliteration using PSMT systemTransliterations generated using pair HMM pa-rameters and WFSTs are evaluated against thosegenerated from a state of the art Phrase-basedStatistical Machine Translation system calledMoses.
Moses has been used before for machinetransliteration (Matthews, 2007) and performedway better than a baseline system that was asso-ciated with finding the most frequent mappingsbetween source and target transliteration units inthe training data.
In the PBSMT system, bilin-gual phrase-tables are used and several compo-nents are combined in a log-linear model (trans-lation models, reverse translation model, wordand phrase penalties, language models, distortionparameters, etc.)
with weights optimized usingminimum error rate training.
For machine transli-teration: characters are aligned instead of words,phrases refer to character n-grams instead ofword n-grams, and language models are definedover character sequences instead of word se-quences.
A major advantage of the PBSMT sys-tem over the pair HMM and a WFST models isthat the phrase tables (character n-grams) cover alot of contextual dependencies found in the data.3 Experiments3.1 Data SetupThe data used is divided according to the expe-rimental runs that were specified for the NEWS2009 shared transliteration task (Li et al, 2009):a standard run and non-standard runs.
The stan-dard run involved using the transliteration systemdescribed above that uses pair HMM parameterscombined with transformation rules.
The Eng-lish-Russian datasets used here were provided forthe NEWS 2009 shared transliteration task (Ku-maran and Kellner, 2009): 5977 pairs of namesfor training, 943 pairs for development, and 1000for testing.
For the non-standard runs, an addi-tional English-Russian dataset extracted from theGeonames data dump was merged with theshared transliteration task data above to form10481 pairs for training and development.
For asecond set of experiments (Table 2), a differentset of test data (1000 pairs) extracted from theGeonames data dump was used.
For the systemused in the standard run, the training data waspreprocessed to include representation of bi-grams associated with Cyrillic Romanization andall English vowel bigram combinations.3.2 ResultsSix measures were used for evaluating systemtransliteration quality.
These include (Li et al,2009): Accuracy (ACC), Fuzziness in Top-1(Mean F Score), Mean Reciprocal Rank (MRR),Mean Average Precision for reference translite-rations (MAP_R), Mean Average Precision in 10best candidate transliterations (MAP_10), MeanAverage Precision for the system (MAP_sys).Table 1 shows the results obtained using only thedata sets provided for the shared transliterationtask.
The system used for the standard run is?phmm_rules?
described in section 2 to sub sec-tion 2.3.
?phmm_basic?
is the system in whichpair HMM parameters are used for transliterationgeneration but there is no representation for bi-grams as described for the system used in thestandard run.
Table 2 shows the results obtainedwhen additional data from Geonames data dumpwas used for training and development.
In Table2, ?WFST_basic?
and ?WFST_rules?
are sys-tems associated with training WFSTs for the?phmm_basic?
and ?phmm_rules?
systems102metricsmodelsACC Mean FScoreMRRphmm_basic 0.293 0.845 0.325Moses_PSMT   0.509 0.908 0.619phmm_rules 0.354 0.869 0.394metricsmodelsMAP_R MAP_10 MAP_sysphmm_basic 0.293 0.099 0.099Moses_PSMT 0.509 0.282 0.282phmm_rules 0.354 0.134 0.134Table 1 Results from data sets for shared transli-teration task.metricsmodelsACC Mean FScoreMRRphmm_basic 0.341 0.776 0.368phmm_rules 0.515 0.821 0.571WFST_basic 0.321 0.768 0.403WFST_rules 0.466 0.808 0.525Moses_PSMT 0.612 0.845 0.660metricsmodelsMAP_R MAP_10 MAP_sysphmm_basic 0.341 0.111 0.111phmm_rules 0.515 0.174 0.174WFST_basic 0.321 0.128 0.128WFST_rules 0.466 0.175 0.175Moses_PSMT 0.612 0.364 0.364Table 2 Results from additional Geonames datasets.respectively.
Moses_PSMT is the phrase-basedstatistical machine translation system.
The resultsin both tables show that the systems using pairHMM parameters perform relatively better thanthe systems trained on WFSTs but not better thanMoses.
The low transliteration quality in the pairHMM and WFST systems as compared to Mosescan be attributed to lack of modeling contextualdependencies unlike the case in PBSMT.4 ConclusionA Transliteration system using pair HMM para-meters has been presented.
Although its perfor-mance is better than that of systems based ononly WFSTs, its transliteration quality is lowerthan the PBSMT system.
On seeing that the pairHMM generated consistent mistransliterations,manual specification of a few contextual rulesresulted in improved performance.
As part offuture work, we expect a technique that automat-ically identifies the mistransliterations wouldlead to improved transliteration quality.
A moregeneral framework, in which we intend to inves-tigate contextual issues in addition to other fac-tors such as position in source and target stringsand edit operation memory in transliteration, isthat of Dynamic Bayesian Networks (DBNs).AcknowledgmentsFunds associated with this work are from a secondNPT Uganda project.
I also thank J?rg Tiedemann forhelping with experimental runs for the Moses PBSMTsystem.ReferencesA.
Kumaran and Tobias Kellner.
2007.
A GenericFramework for Machine Transliteration.
Pro-ceedings of the 30th SIGIR.David Matthews.
2007.
Machine Transliteration ofProper Names.
Master?s Thesis.
School of Infor-matics.
University of Edinburgh.Jonathan Graehl.
1997.
Carmel Finite-state Toolkit.http://www.isi.edu/licensed-sw/carmel/.Haizhou Li, A. Kumaran, Min Zhang, Vladimir Per-vouchine.
2009.
Whitepaper of NEWS 2009 Ma-chine Transliteration Shared Task.
Proceedings ofACL-IJCNLP 2009 Named Entities Workshop(NEWS 2009), Singapore.Kevin Knight and Jonathan Graehl.
1998.
MachineTransliteration.
Computational Linguistics, 24 (4):599-612, MIT Press Cambridge, MA, USA.Leonard E. Baum, Ted Petrie, George Soules, andNorman Weiss.
1970.
A Maximization TechniqueOccurring in the Statistical Analysis of Probabilis-tic Functions of Markov Chains.
The Annals ofMathematical Statistics, 41(1):164-171.Martijn Wieling, Therese Leinonen and John Ner-bonne.
2007.
Inducing Sound Segment Differencesusing Pair Hidden Markov Models.
In John Ner-bonne, Mark Ellison and Grzegorz Kondrak (eds.
)Computing Historical Phonology: 9th Meeting ofthe ACL Special Interest Group for ComputationalMorphology and Phonology Workshop, pp.
48-56,Prague.Mehryar Mohri, Fernando C.N.
Pereira, and MichaelRiley.
2008.
Speech Recognition with Weighte Fi-nite State Transducers.
In Larry Rabiner and FredJuang, editors, Handbook on Speech Processingand Speech Communication, Part E: Speech Rec-ognition.
Springer-Verlag, Heidelberg, Germany.Wesley Mackay and Grzegorz Kondrak.
2005.
Com-puting Word Similarity and Identifying Cognateswith Pair Hidden Markov Models.
Proceedings ofthe Ninth Conference on Computational NaturalLanguage Learning (CoNLL 2005), pp.
40-47,Ann-Arbor, Michigan.103
