Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1405?1415,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEntity-Centric Coreference Resolution with Model StackingKevin ClarkComputer Science DepartmentStanford Universitykevclark@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford Universitymanning@cs.stanford.eduAbstractMention pair models that predict whetheror not two mentions are coreferent havehistorically been very effective for coref-erence resolution, but do not make useof entity-level information.
However, weshow that the scores produced by suchmodels can be aggregated to define pow-erful entity-level features between clustersof mentions.
Using these features, wetrain an entity-centric coreference systemthat learns an effective policy for buildingup coreference chains incrementally.
Themention pair scores are also used to prunethe search space the system works in, al-lowing for efficient training with an exactloss function.
We evaluate our system onthe English portion of the 2012 CoNLLShared Task dataset and show that it im-proves over the current state of the art.1 IntroductionCoreference resolution, the task of identifyingmentions in a text that refer to the same real worldentity, is an important aspect of text understandingand has numerous applications.
Many approachesto coreference resolution learn a scoring functiondefined over mention pairs to guide the corefer-ence decisions (Soon et al, 2001; Ng and Cardie,2002; Bengtson and Roth, 2008).
However, suchsystems do not make use of entity-level informa-tion, i.e., features between clusters of mentions in-stead of pairs.Using entity-level information is valuable be-cause it allows early coreference decisions to in-form later ones.
For example, finding that Clin-ton and she corefer makes it more likely that Clin-ton corefers with Hillary Clinton than Bill Clin-ton due to gender agreement constraints.
Such in-formation has been incorporated successfully intoentity-centric coreference systems that build upcoreference clusters incrementally, using the in-formation from the partially completed corefer-ence chains produced so far to guide later deci-sions (Raghunathan et al, 2010; Stoyanov andEisner, 2012; Ma et al, 2014).However, defining useful features between clus-ters of mentions and learning an effective policyfor incrementally building up clusters can be chal-lenging, and many recent state-of-the-art systemswork entirely or almost entirely over pairs of men-tions (Fernandes et al, 2012; Durrett and Klein,2013; Chang et al, 2013).
In this paper we in-troduce a novel coreference system that combinesthe advantages of mention pair and entity-centricsystems with model stacking.
We first proposetwo mention pair models designed to capture dif-ferent linguistic phenomena in coreference resolu-tion.
We then describe how the probabilities pro-duced by these models can be used to generateexpressive features between clusters of mentions.Using these features, we train an entity-centric in-cremental coreference system.The entity-centric system builds up coreferencechains with agglomerative clustering: each men-tion starts in its own cluster and then pairs of clus-ters are merged each step.
We train an agent todetermine whether it is desirable to merge a par-ticular pair of clusters using an imitation learningalgorithm based on DAgger (Ross et al, 2011).Previous incremental coreference systems heuris-tically define which actions are beneficial for theagent to perform, but we instead propose a wayof assigning exact costs to actions based on coref-erence evaluation metrics, adding a concept ofthe severity of a mistake.
Furthermore, ratherthan considering all pairs of clusters as candidatemerges, we use the scores of the pairwise mod-els to reduce the search space, first by providingan ordering over which merges are considered andsecondly by discarding merges that are not likely1405to be good.
This greatly reduces the time it takesto run the agent, making learning computationallyfeasible.Imitation learning is challenging because it isa non-i.i.d.
learning problem; the distribution ofstates seen by the agent depends on the agent?s pa-rameters.
Model stacking offers a way of decom-posing the learning problem by training pairwisemodels with many parameters in a straightforwardsupervised learning setting and using their outputsfor training a much simpler model in the moredifficult imitation learning setting.
Furthermore,mention pair scores can produce powerful featuresfor training the agent because the scores indicatewhich mention pairs between the clusters in ques-tion are relevant; high scoring and low scoringpairs can indicate when a merge should be forcedor disallowed while other mention pairs may pro-vide little useful information.We run experiments on the English portion ofthe 2012 CoNLL Shared Task dataset.
The entity-centric clustering algorithm greatly outperformscommonly used heuristic methods for coordinat-ing pairwise scores to produce a coreference par-tition.
We also show that combining the scoresof different pairwise models designed to capturedifferent aspects coreference results in significantgains in accuracy.
Our final system gets a com-bined score of 63.02 on the dataset, substantiallyoutperforming other state of the art systems.2 Mention Pair ModelsMention pair models predict whether or not agiven pair of mentions belong in the same coref-erence cluster.
We incorporate two different men-tion pair models into our system.
However, otherpairwise models could easily be added; one advan-tage of our model stacking approach is that it cancombine different simple classifiers in a modularway.Our two models are designed to capture dif-ferent aspects of coreference.
The first one isbuilt to predict coreference for all of the candi-date antecedents of a mention.
This makes it use-ful for providing scores when the current mentionhas clear coreference links to many previous men-tions.
For example President Clinton might belinked to the president, Bill Clinton, and Mr. Pres-ident.However, mentions often only have one clearantecedent.
This is especially common in pronom-inal anaphora resolution, such as in the sentenceBill arrived, but nobody saw him.
The pronounhim is directly referring back to a previous part ofthe discourse, not some entity that other mentionsmay also refer to.
However, there still might becoreference links between him and previous men-tions in the text because of transitivity: any othermention about Bill would be coreferent with him.For such mentions, there may be very little ev-idence in the discourse to suggest a coreferencelink, so attempting to train a model to predict thesewill bear little fruit.
With this as motivation, wealso train a model to predict only one correct an-tecedent of the current mention.We found a classification model to be wellsuited for the first task and and a ranking model tobe well suited for the second one.
These two mod-els differ only in the training criteria used.
Bothmodels use a logistic classifier to assign a proba-bility to a mention m and candidate antecedent arepresenting the likelihood that the two mentionsare coreferent.
The candidate antecedent a maytake on the value NA indicating that m has no an-tecedent.
The probability of coreference takes thestandard logistic form:p?
(a,m) = (1 + e?T f(a,m))?1where f(a,m) is a vector of feature functions ona and m and ?
are the feature weights we wishto learn.
Let M denote the set of all mentionsin the training set, T (m) denote the set of true an-tecedents of a mentionm (i.e., mentions that occurbefore m in the text that are coreferent with m or{NA} if m has no antecedent), and F(m) denotethe set of false antecedents of m. We want to finda parameter vector ?
that assigns high probabili-ties to the candidate antecedents in T (m) and lowprobabilities to the ones in F(m).2.1 Classification ModelFor the classification model, we consider each pairof mentions independently with the goal of pre-dicting coreference correctly for as many of themas possible.
The model is trained by minimiz-ing negative conditional log likelihood augmentedwith L1 regularization:Lc(?c) = ?
?m?M(?t?T (m)log p?c(t,m)+?f?F(m)log(1?
p?c(f,m)))+ ?||?c||11406By summing over all candidate antecedents, theobjective encourages the model to produce goodprobabilities for all of them.2.2 Ranking ModelFor the ranking model, candidate antecedents for amention are considered simultaneously and com-pete with each other to be matched with the cur-rent mention.
This makes the model well suitedto the task of finding a single best antecedent fora mention.
A natural learning objective for sucha model would be a max-margin training crite-ria that encourages separation between the highestscoring true antecedent and highest scoring falseantecedent of the current mention.
However, wefound such models to be poor at producing scoresuseful for a downstream clustering model becausea max-margin objective encourages scores for trueantecedents to be high only relative to other can-didate antecedents.
It is much more beneficialto have mention pair scores that are comparableacross different mentions as well as different can-didate antecedents.
For this reason, we insteadtrain the model with an objective that maximizesthe conditional log likelihood of the highest scor-ing true and false antecedents under the logisticmodel:Lr(?r) = ?
?m?M(maxt?T (m)log p?r(t,m)+ minf?F(m)log(1?
p?r(f,m)))+ ?||?r||1For both models, we set ?
= 0.001 and opti-mize their objectives using AdaGrad (Duchi et al,2011).2.3 FeaturesOur mention pair models use a variety of commonfeatures for mention pair classification (for moredetails see (Bengtson and Roth, 2008; Stoyanov etal., 2010; Lee et al, 2011; Recasens et al, 2013)).These include?
Distance features, e.g., the distance betweenthe two mentions in sentences or number ofmentions.?
Syntactic features, e.g., number of embed-ded NPs under a mention, POS tags of thefirst, last, and head word.?
Semantic features, e.g., named entity type,speaker identification.?
Rule-based features, e.g., exact and partialstring matching.?
Lexical Features, e.g., the first, last, andhead word of the current mention.We also employ a feature conjunction scheme sim-ilar to the one described by Durrett and Klein(2013).3 Entity-Centric Coreference ModelMention pair scores alone are not enough to pro-duce a final set of coreference clusters becausethey do not enforce transitivity: if the pair of men-tions (a, b) and the pair of mentions (b, c) aredeemed coreferent by the model, there is no guar-antee that the model will also classify (a, c) ascoreferent.
Thus a second step is needed to co-ordinate the scores to produce a final coreferencepartition.
A widely used approach for this is best-first clustering (Ng and Cardie, 2002).
For eachmention, the best-first algorithm assigns the mostprobable preceding mention classified as corefer-ent with it as the antecedent.The primary weakness of this approach is that itonly relies on local information to make decisions,so it cannot consolidate information at the entitylevel.
As a result, coreference chains produced bysuch algorithms can exhibit low coherency.
Forexample, a cluster may consist of [Hillary Clin-ton, Clinton, he] because the coreference decisionbetween Hillary Clinton and Clinton is made in-dependently of the one between Clinton and he.To tackle this problem, we build an entity-centric model that operates between pairs of clus-ters instead of pairs of mentions, guided by scoresproduced by the pairwise models.
It builds upclusters of mentions believed to refer to the sameentity as it goes, relying on the partially formedclusters produced so far to make decisions.
Forexample, the system could reject linking [HillaryClinton] with [Clinton, he] because of the lowscore between the pair (Hillary Clinton, he).Our entity-centric ?agent?
builds up coreferencechains with agglomerative clustering.
It beginsin a start state where each mention is in a sepa-rate single-element cluster.
At each step, it ob-serves the current state s, which consists of all par-tially formed coreference clusters produced so far,and selects some action a which merges two exist-ing clusters.
The action will result in a new statewith new candidate actions and the process is re-peated.
The model is entity-centric in that it builds1407up clusters of mentions representing entities andmerges clusters if it predicts they are representingthe same one.3.1 Test-time InferenceThe agent assigns a score to each action a using alinear model with feature function fe and weightvector ?e: s?e(a) = ?Te fe(a).
A particular settingof ?e defines a policy pi that determines which ac-tion a = pi(s) the agent will take in state s. Thispolicy is to greedily take highest scoring candidateaction available from the current state.Rather than using all possible cluster merges asthe candidate set of actions the agent selects from,we use the scores produced by mention pair mod-els to reduce the search space.
First, we order allmention pairs in the document in descending or-der according to their pairwise scores.
This causesclustering to occur in an easy-first fashion, whereharder decisions are delayed until more informa-tion is available.
Secondly, we discard all men-tion pairs that score below a threshold t underthe assumption that the clusters containing thesepairs are unlikely to be coreferent.
In our experi-ments we were able able set t so that over 95% ofpairs were removed with no decrease in accuracy.Lastly, we iterate through this list of pairs in or-der.
For each pair, we make a binary decision onwhether or not the clusters containing these pairsshould be merged.
This formulates the agent?s taskso it only has two actions to chose from instead ofa number of actions proportional to the number ofclusters squared.
Algorithm 1 shows the full test-time procedure.3.2 LearningImitation Learning with DAggerWe face a sequential prediction problem where fu-ture observations (visited states) depend on previ-ous actions.
This is challenging because it violatesthe common i.i.d.
assumptions made in statisticallearning.
Imitation learning, where expert demon-strations of good behavior are used to teach theagent, has proven very useful in practice for thissort of problem (Argall et al, 2009).
We use imita-tion learning to set the parameters ?e of our agentby training it to classify whether a particular actionis the one an expert policy would take in the cur-rent state.
In particular, we use ?e as parametersfor a binary logistic classifier that predicts whichaction (merge or do not merge) matches the expertpolicy.Algorithm 1 Inference method: agglomerativeclusteringInput: Set of mentions in document M, pair-wise classifier with parameters ?c, agent withparameters ?e, cutoff threshold tOutput: Clustering CInitialize list of mention pairs P ?
[]for each pair (mi,mj) ?M2with i < j doif p?c(mi,mj) > t thenP .append((mi,mj))end ifend forSort P in descending order according to p?cInitialize C ?
initial clustering with each men-tion inM in its own clusterfor (mi,mj) ?
P doif C[mi] 6= C[mj]and s?e(C[mi], C[mj ]) > 0 thenDoMerge(C[mi], C[mj], C)end ifend forWe found the DAgger (Ross et al, 2011) imita-tion learning method (see Algorithm 2) to be effec-tive for this task.
DAgger is an iterative algorithmthat aggregates a datasetD consisting of states andthe actions performed by the expert policy in thosestates.
At each iteration, it first samples a trajec-tory of states visited by the current policy by run-ning the policy to completion from the start state.It then labels those states with the best action ac-cording to the expert policy, adds those labeled ex-amples to the dataset, and then trains a new clas-sifier over the dataset to get a new policy.
Whenproducing a trajectory to train on, the expert pol-icy is stochastically mixed with the current policy;with probability ?ithe expert?s action is chosen in-stead of the current policy?s.
We set ?
so it decaysexponentially as the iteration number increases.By sampling trajectories under the currentpolicy, DAgger exposes the system to states attrain time similar to the ones it will face at testtime.
In contrast, training the agent on the goldlabels alone would unrealistically teach it to makedecisions under the assumption that all previousdecisions were correct, potentially causing it toover-rely on information from past actions.
This isespecially problematic in coreference, where theerror rate is quite high.
Even when using DAgger,1408this problem could exist to a lesser degree if themodel heavily overfits to the training data.
How-ever, the agent has a small number of parametersthanks to our model stacking approach, reducingthe risk of this happening.Algorithm 2 Learning method: DAggerInput: initial policy p?i1, expert policy pi?Output: final policy p?iNInitialize D ?
?for i = 1 to N doLet pii= ?ipi?+ (1?
?i)p?iiSample a trajectory under the current policyusing piiGet dataset Di= (s, pi?
(s)) of states visitedby piiand actions given by the expertAggregate datasets: D ?
D ?DiTrain classifier p?ii+1on Dend forAssigning Costs to ActionsA key aspect of incrementally building corefer-ence clusters is that some local decisions are muchmore important than others.
For example, a mergebetween two large clusters influences the score farmore than a merge between two small ones.
Ad-ditionally, getting early decisions correct is crucialbecause later actions are dependent on early ones,causing errors to compound if mistakes are madeearly.
To capture this, we take an approach in-spired by the SEARN learning algorithm (Daum?eet al, 2009) and add costs to the actions in theaggregated dataset.
We then train the agent to docost-sensitive classification.
Using these costs, wesimply define the expert policy as the policy thattakes the action with the lowest cost at each step.We want our costs to represent how a partic-ular local decision will affect the final score ofthe coreference system.
Unfortunately, standardcoreference evaluation metrics do not decomposeover cluster merges.
Instead, we compute the lossof an action by ?rolling out?
the current policy tocompletion.
More concretely, let m be a function(such as a coreference evaluation metric) that as-signs scores to states; we are interested in reach-ing a final state for which m is high.
Suppose weare assigning costs to the set of actions A(s) thatcan be taken from some state s. For each actiona ?
A(s), we apply that action to s to get a newstate s?, run the current policy p?iifrom s?to com-pletion, and then compute the value of m on theresulting final state.
This gives exactly the finalscore the system would get if it made the action afrom state s and then continued under the currentpolicy.
Let fm(s, a) denote this value for a par-ticular metric, state, and action.
We assign eachaction the regret r associated with taking that ac-tion under the current policy as a cost:r(s, a) = maxa?
?A(s)fm(s, a?)?
fm(s, a)The ?rolling out?
procedure means we naivelyhave to visit O(t2) states each iteration instead oft, where t is the length of a trajectory.
However,the highly constrained action space described insection 3.1 combined with the use of memoizationallows the algorithm to still run efficiently.Improving Runtime with MemoizationDuring training, the agent will see many of thesame states and actions multiple times.
We can ex-ploit this with memoization, significantly improv-ing the algorithm?s runtime.
In particular, we storethe following values:?
Given a state s and action a, the value of thecost function, r(s, a).?
Given an action a, the score the model as-signs that action, s?e(a).?
Given an action a, the result of the featurefunction on that action, fe(a).The first two values depend on the current model,so the saved values must be cleared between iter-ations of training.
In experiments on the develop-ment set of the CoNLL 2012 corpus, these tableshad 76%, 94%, and 93% hit rates respectively af-ter 50 passes over the dataset.3.3 FeaturesOur agent uses features that are derived from thescores produced by the two mention pair mod-els.
Although these scores only operate on men-tion pairs, they are combined to capture cluster-level interactions by being aggregated in differ-ent ways over pairs of mentions from the clus-ters.
Mention pair scores can produce powerfulfeatures for training the agent because they showwhich mention pairs between the clusters in ques-tion are relevant, and often a small subset of themention pairs provide far more information thanthe rest.
For example, a strong negative pairwise1409Figure 1: Examples of features generated for a candidate cluster merge.
Weights on edges are theprobabilities of coreference produced by a mention pair model.link like Hillary Clinton and he should disallowa merge, while other mention pairs, such as twoinstances of the pronoun she far apart in the text,might provide very little information.
Using themention pair models for probabilities, we computethe following features over all pairs of mentionsbetween the clusters (i.e., each mention is in a dif-ferent cluster).?
The minimum and maximum probability ofcoreference.?
The average probability and average logprobability of coreference.?
The average probability and log proba-bility of coreference for a particular pairof grammatical types of mentions (eitherpronoun or non-pronoun).
For exam-ple, Avg-Prob non-pronoun pronoungives the average probability of coreferencewhen the candidate antecedent is not a pro-noun and the candidate anaphor is a pronoun.Note that the averaged features have a naturalprobabilistic interpretation; the average probabil-ity corresponds to the expected number of coref-erence links between the involved mention pairswhile the average log probability corresponds tothe probability that all mention pairs will have acoreference link.
All of these features are com-puted twice: once with the classification modeland once with the ranking model.We also compute the following features basedon other aspects of the current state:?
Whether a preceding mention pair in thelist of mention pairs has the same candidateanaphor as the current one.?
The index of the current mention pair in thelist divided by the size of the list, i.e., whatpercentage of the list have we seen so far.?
The number of mentions in the current docu-ment.?
The probability of the first-occurring men-tion in the second-occurring cluster not be-ing anaphoric (i.e., p?c(NA,m)).
This pre-vents producing clusters that, for example,start with a pronoun.Lastly, we take one feature conjunction witha boolean representing whether both clusters aresize 1.
In total, there are only 56 features af-ter the feature conjunction.
However, these fea-tures provide strong signal because they are di-rectly related to the probabilities of mentions be-ing coreferent.
In contrast, the pairwise modelsuse thousands of features (after feature conjunc-tions), including lexical features that are extremelysparse.
The pairwise models can easily exploitthis much bigger feature set because they oper-ate in a classic supervised learning setting.
Theentity-centric model, on the other hand, learns ina much more challenging non-i.i.d.
setting.
Modelstacking avoids the difficulty of directly trainingthe entity-centric model with a large set of weakfeatures by decomposing the task into first learn-ing to produce good pairwise scores and then us-ing those scores to generate a manageable set ofstrong features.3.4 Training DetailsBecause the entity-centric agent relies on the out-put of pairwise classifiers, they should not betrained on the same data.
Therefore we split the1410training set into two sections and use one for train-ing the pairwise models and the other for trainingthe agent.
When evaluating on the developmentset, we use 80% of the documents in the trainingset to train the mention pair models and the restto train the entity-centric model.
When evaluatingon the test set we use the whole training set forthe mention pair models and the development setfor the entity-centric model.
We also tried usingcross-validation instead of a single split, but foundthis did not improve performance, which we be-lieve to be because this trains the agent with dif-ferent pairwise models than the ones used at testtime.For our initial policy p?i1, we set the parame-ters of the agent so it operates with simple best-first clustering (initializing all feature weights to0 except for the maximum-score, anaphor-seen,and bias features).
For m, the performance met-ric determining the action costs, we use a linearcombination of the B3(Bagga and Baldwin, 1998)and MUC (Vilain et al, 1995) metrics, which areboth commonly used for evaluating coreferencesystems.
The other metric used in our evaluation,Entity-based CEAFE (CEAF?4) (Luo, 2005), wasnot used because it is expensive to compute.
Wefound weighting B3three times as much as MUCto be effective on the development set.4 Experiments and ResultsExperimental SetupWe apply our model to the English portion ofthe CoNLL 2012 Shared Task data (Pradhan etal., 2012), which is derived from the OntoNotescorpus (Hovy et al, 2006).
The data is split intoa training set of 2802 documents, developmentset of 343 documents, and a test set of 345documents.
We use the provided preprocessingfor parse trees, named entity tags, etc.
The modelsare evaluated using three of the most popularmetrics for coreference resolution: MUC, B3, andEntity-based CEAFE (CEAF?4).
We also includethe average F1score (CoNLL F1) of these threemetrics, as is commonly done in CoNLL SharedTasks.
We used the most recent version of theCoNLL scorer (version 8.01), which implementsthe original definitions of these metrics.Mention DetectionOur experiments were run using system-producedpredicted mentions.
We used the rule-basedMUC B3CEAF?4Avg.Classification, B.F. 72.00 60.01 55.63 62.55Ranking, B.F. 71.91 60.63 56.38 62.97Classification, E.C.
72.34 61.46 57.16 63.65Ranking, E.C.
72.37 61.34 57.13 63.61Both, E.C.
72.52 62.02 57.69 64.08Table 1: Metric scores on the development setfor the classification and ranking pairwise mod-els when using best-first clustering (B.F.) or theentity-centric model (E.C.
).mention detection algorithm from Raghunathanet al (2010), which first extracts pronouns andmaximal NP projections as candidate mentionsand then filters this set with rules that removespurious mentions such as numeric entities orpleonastic it pronouns.Comparison of ModelsWe compare the effectiveness of the entity-centricmodel with the commonly used best-first cluster-ing approach, which assigns mentions the high-est scoring previous mention as the antecedent.Unlike the entity-centric model, the best-first ap-proach only relies on local information to makedecisions.
We also compare the effectiveness ofthe ranking and classification pairwise models.Table 1 shows the results of these models on thedevelopment set.The entity-centric model outperforms best-first clustering for both mention pair models,demonstrating the utility of a learned, incrementalclustering algorithm.
The improvement is muchgreater for the classification pairwise model, caus-ing it to outperform the ranking model with theentity-centric clustering algorithm even thoughit performs significantly worse than the rankingmodel with best-first clustering.
This suggeststhat although the ranking model is better at findinga single correct antecedent for a mention, theclassification model is more useful for producingcluster-level features.
Incorporating probabilitiesfrom both pairwise models further improvedscores over using either model alone, indicatingthat the mention pair classifiers were successfulin learning scoring functions useful in differentcircumstances.Incorporating other Entity-Level FeaturesAlthough the entity-centric model has so far only1411MUC B3CEAF?4Avg.Scores Only 72.52 62.02 57.69 64.08+Agreement 72.59 61.98 57.58 64.05Table 2: Metric scores on the development set forthe entity-centric model with and without the ad-dition of entity-level agreement features.used features derived from the scores producedby mention pair models, other entity-level featurescould easily be added.
We experiment with this byadding four cluster-level agreement features basedon gender, number, animacy, and named entitytype.
Each of these features can take on three val-ues: ?same?
(e.g., both clusters have gender valuefeminine), ?compatible?
(e.g., one cluster has gen-der value feminine while the other has value un-known), or ?incompatible?
(one cluster has gendervalue feminine while the other has value mascu-line).
The cluster-level value for a particular fea-ture is the most common value among mentions inthat cluster (e.g., if a cluster has 2 masculine men-tions, 1 feminine mention, and 1 unknown men-tion) the value is considered masculine.
Table 2shows the results.Adding the additional features had no substan-tial impact on scores, suggesting that features de-rived from pairwise scores are sufficient for cap-turing this kind of entity-level information.
Adisagreement between clusters necessarily meansthere will be disagreements between some of theinvolved mentions, so features like the average andminimum probability between mention pairs willhave lower values when a disagreement is present.Final System PerformanceIn Table 3 we compare the results of our systemwith the following state-of-the-art approaches: theJOINT and INDEP models of the Berkeley sys-tem (Durrett and Klein, 2014) (the JOINT modeljointly does NER and entity linking along withcoreference); the Prune-and-Score system (Ma etal., 2014); the HOTCoref system (Bj?orkelund andKuhn, 2014); the CPL3M sytem (Chang et al,2013); and Fernandes et al We use the full entity-centric clustering algorithm drawing upon scoresfrom both pairwise models.
We do not make useof agreement features, as these did not increase ac-curacy and complicate the system.
Our final modelsubstantially outperforms the other systems on theCoNLL F1score.
The largest improvement is inthe B3metric, which is unsurprising because theentity-centric model primarily optimizes for thisduring training.
However, our model also achievesthe highest CEAF?4F1and second highest MUCF1scores among the other systems.5 Related WorkBoth mention pair (Soon et al, 2001; Ng andCardie, 2002; Bengtson and Roth, 2008; Stoyanovet al, 2010; Bj?orkelund and Farkas, 2012) andmention ranking models (Denis and Baldridge,2007b; Rahman and Ng, 2009) have been widelyused for coreference resolution, and there havebeen many proposed ways of post-processing thepairwise scores to make predictions.
Despitetheir simplicity, closest-first clustering (Soon etal., 2001) and best-first clustering (Ng and Cardie,2002) are arguably the most widely used of theseapproaches.
Other work uses global inferencewith integer linear programming to enforce tran-sitivity (Denis and Baldridge, 2007a; Finkel andManning, 2008), graph partitioning algorithms(McCallum and Wellner, 2005; Nicolae and Nico-lae, 2006), the Dempster-Shafer rule (Kehler,1997; Bean and Riloff, 2004), or correlationalclustering (McCallum and Wellner, 2003; Finleyand Joachims, 2005).
In contrast to these methods,our entity-centric model directly learns how to usepairwise scores to produce a coreference partitionthat scores highly according to an evaluation met-ric, and can use the outputs of more than one men-tion pair model.Recently, coreference models using latent an-tecedents have gained in popularity and achievedstate-of-the-art results (Fernandes et al, 2012;Durrett and Klein, 2013; Chang et al, 2013;Bj?orkelund and Kuhn, 2014).
These learn a scor-ing function over mention pairs, but are trained tomaximize a global objective function instead ofpairwise accuracy.
Unlike in our system, thesemethods typically consider one pair of mentionsat a time during inference.Several works have explored using non-localentity-level features in mention-entity models thatassign a single mention to a (partially completed)cluster (Luo et al, 2004; Yang et al, 2008; Rah-man and Ng, 2011).
Our system, however, buildsclusters incrementally through merge operations,and so can operate in an easy-first fashion.
Raghu-nathan et al (2010) take this approach with arule-based system that runs in multiple passes1412MUC B3CEAF?4CoNLLPrec.
Rec.
F1Prec.
Rec.
F1Prec.
Rec.
F1Avg.
F1Fernandes et al 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65Chang et al - - 69.48 - - 57.44 - - 53.07 60.00Bj?orkelund & Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63Ma et al 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56Durrett & Klein (INDEP.)
72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23Durrett & Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02Table 3: Comparison of this work with other state-of-the-art approaches on the test set.and Stoyanov and Eisner (2012) train a classi-fier to do this with a structured perceptron algo-rithm.
Entity-level information has also been suc-cessfully incorporated in coreference systems us-ing joint inference (McCallum and Wellner, 2003;Culotta et al, 2006; Poon and Domingos, 2008;Haghighi and Klein, 2010), but these approachesdo not directly learn parameters tuned so the sys-tem runs effectively at test time, while our imita-tion learning approach does.Imitation learning has been employed to traincoreference resolvers on trajectories of decisionssimilar to those that would be seen at test-time byDaum?e et al (2005) and Ma et al (2014).
Otherworks use structured perceptron models for thesame purpose (Stoyanov and Eisner, 2012; Fer-nandes et al, 2012; Bj?orkelund and Kuhn, 2014).These systems all heuristically determine whichactions are desirable for the system to perform.In contrast, our approach directly computes a costfor actions based on coreference evaluation met-rics.
This means our system directly learns whichactions lead to good clusterings instead of whichlook good locally according to a heuristic.
Fur-thermore, the costs provide our system a measureof the severity of a mistake, which we argue is verybeneficial for the coreference task.Our model stacking approach further distin-guishes this work by providing a new way of defin-ing cluster-level features.
The majority of usefulfeatures for coreference systems operate on pairsof mentions (in one of our experiments we showthe addition of classic entity-level features doesnot improve our system), but incremental corefer-ence systems must make decisions involving manymention pairs.
Other incremental coreference sys-tems either incorporate features from a single pair(Stoyanov and Eisner, 2012) or average featuresacross all pairs in the involved clusters (Ma etal., 2014).
Our system instead combines informa-tion from the involved mention pairs in a varietyof ways with with higher order features producedfrom the scores of mention pair models.6 ConclusionWe introduced a new approach to coreference res-olution that trains an entity-centric system usingthe scores produced by mention pair models asfeatures.
The brunt of task-specific learning oc-curs within the mention pair models, which aretrained in a straightforward supervised manner.Guided by the pairwise scores, our entity-centricagent then learns an effective procedure for build-ing up coreference clusters incrementally, usingprevious decisions to inform later ones.
The agentbenefits from using multiple mention pair mod-els designed to capture different aspects of coref-erence.
Experiments show that the agent, whichlearns how to coordinate mention pair scores, out-performs the commonly used best-first method.We evaluate our final system on the English por-tion of the CoNLL 2012 Shared Task and report asignificant improvement over the current state ofthe art.AcknowledgmentsWe thank the anonymous reviewers for theirthoughtful comments.
Stanford University grate-fully acknowledges the support of the Defense Ad-vanced Research Projects Agency (DARPA) DeepExploration and Filtering of Text (DEFT) Programunder Air Force Research Laboratory (AFRL)contract no.
FA8750-13-2-0040.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the view of the DARPA,AFRL, or the US government.1413ReferencesBrenna D Argall, Sonia Chernova, Manuela Veloso,and Brett Browning.
2009.
A survey of robot learn-ing from demonstration.
Robotics and AutonomousSystems, 57(5):469?483.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In The First Interna-tional Conference on Language Resources and Eval-uation Workshop on Linguistics Coreference, pages563?566.David L Bean and Ellen Riloff.
2004.
Unsupervisedlearning of contextual role knowledge for corefer-ence resolution.
In Human Language Technologyand North American Association for ComputationalLinguistics (HLT-NAACL), pages 297?304.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 294?303.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Conference on Computational Natu-ral Language Learning - Shared Task, pages 49?55.Anders Bj?orkelund and Jonas Kuhn.
2014.
Learn-ing structured perceptrons for coreference resolutionwith latent antecedents and non-local features.
InAssociation of Computational Linguistics (ACL).Kai-Wei Chang, Rajhans Samdani, and Dan Roth.2013.
A constrained latent variable model for coref-erence resolution.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 601?612.Aron Culotta, Michael Wick, Robert Hall, and An-drew McCallum.
2006.
First-order probabilisticmodels for coreference resolution.
In Human Lan-guage Technology and North American Associationfor Computational Linguistics (HLT-NAACL), pages81?88.Hal Daum?e III and Daniel Marcu.
2005.
A large-scale exploration of effective global features for ajoint entity detection and tracking model.
In Em-pirical Methods in Natural Language Processing(EMNLP), pages 97?104.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
MachineLearning, 75(3):297?325.Pascal Denis and Jason Baldridge.
2007a.
Joint de-termination of anaphoricity and coreference resolu-tion using integer programming.
In Human Lan-guage Technology and North American Associationfor Computational Linguistics (HLT-NAACL), pages236?243.Pascal Denis and Jason Baldridge.
2007b.
A rank-ing approach to pronoun resolution.
In InternationalJoint Conferences on Artificial Intelligence (IJCAI),pages 1588?1593.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159.Greg Durrett and Dan Klein.
2013.
Easy victoriesand uphill battles in coreference resolution.
In Em-pirical Methods in Natural Language Processing(EMNLP), pages 1971?1982.Greg Durrett and Dan Klein.
2014.
A joint modelfor entity analysis: Coreference, typing, and linking.Transactions of the Association for ComputationalLinguistics (TACL), 2:477?490.Eraldo Rezende Fernandes, C?
?cero Nogueira Dos San-tos, and Ruy Luiz Milidi?u.
2012.
Latent structureperceptron with feature induction for unrestrictedcoreference resolution.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Conference on Computa-tional Natural Language Learning - Shared Task,pages 41?48.Jenny Rose Finkel and Christopher D Manning.
2008.Enforcing transitivity in coreference resolution.
InAssociation for Computational Linguistics (ACL),Short Paper, pages 45?48.Thomas Finley and Thorsten Joachims.
2005.
Super-vised clustering with support vector machines.
InProceedings of the 22nd international conference onMachine learning, pages 217?224.Aria Haghighi and Dan Klein.
2010.
Coreferenceresolution in a modular, entity-centered model.
InHuman Language Technology and North AmericanAssociation for Computational Linguistics (HLT-NAACL), pages 385?393.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Human Language Technologyand North American Association for ComputationalLinguistics (HLT-NAACL), pages 57?60.Andrew Kehler.
1997.
Probabilistic coreference in in-formation extraction.
In Empirical Methods in Nat-ural Language Processing (EMNLP), pages 163?173.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Conference on Compu-tational Natural Language Learning: Shared Task,pages 28?34.1414Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the bell tree.
In Association for ComputationalLinguistics (ACL), page 135.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 25?32.Chao Ma, Janardhan Rao Doppa, J Walker Orr,Prashanth Mannem, Xiaoli Fern, Tom Dietterich,and Prasad Tadepalli.
2014.
Prune-and-score:Learning for greedy coreference resolution.
In Em-pirical Methods in Natural Language Processing(EMNLP).Andrew McCallum and Ben Wellner.
2003.
Towardconditional models of identity uncertainty with ap-plication to proper noun coreference.
In Proceed-ings of the IJCAI Workshop on Information Integra-tion on the Web.Andrew McCallum and Ben Wellner.
2005.
Condi-tional models of identity uncertainty with applica-tion to noun coreference.
In Advances in NeuralInformation Processing Systems (NIPS), pages 905?912.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Association of Computational Linguistics (ACL),pages 104?111.Cristina Nicolae and Gabriel Nicolae.
2006.
Bestcut:A graph algorithm for coreference resolution.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 275?283.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with markov logic.In Empirical Methods in Natural Language Process-ing (EMNLP), pages 650?659.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012 shared task: Modeling multilingual unre-stricted coreference in ontonotes.
In Proceedings ofthe Joint Conference on Empirical Methods in Natu-ral Language Processing and Conference on Com-putational Natural Language Learning - SharedTask, pages 1?40.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
Amulti-pass sieve for coreference resolution.
In Em-pirical Methods in Natural Language Processing(EMNLP), pages 492?501.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Empirical Methodsin Natural Language Processing (EMNLP), pages968?977.Altaf Rahman and Vincent Ng.
2011.
Narrowing themodeling gap: a cluster-ranking approach to coref-erence resolution.
Journal of Artificial IntelligenceResearch (JAIR), pages 469?521.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
InHuman Language Technology and North AmericanAssociation for Computational Linguistics (HLT-NAACL), pages 627?633.St?ephane Ross, Geoffrey J Gordon, and J Andrew Bag-nell.
2011.
A reduction of imitation learning andstructured prediction to no-regret online learning.In Artificial Intelligence and Statistics (AISTATS),pages 627?633.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Veselin Stoyanov and Jason Eisner.
2012.
Easy-firstcoreference resolution.
In COLING, pages 2519?2534.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.Reconcile: A coreference resolution research plat-form.
Computer Science Technical Report, CornellUniversity, Ithaca, NY.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the 6th conference on Message understand-ing, pages 45?52.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,Ting Liu, and Sheng Li.
2008.
An entity-mentionmodel for coreference resolution with inductivelogic programming.
In Association of Computa-tional Linguistics (ACL), pages 843?851.1415
