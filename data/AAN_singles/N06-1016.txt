Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 120?127,New York, June 2006. c?2006 Association for Computational LinguisticsAn Empirical Study of the Behavior of Active Learning for Word SenseDisambiguation1 Jinying Chen, 1 Andrew Schein, 1 Lyle Ungar, 2 Martha Palmer1 Department of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA, 19104{jinying,ais,ungar}@cis.upenn.edu2 Linguistic DepartmentUniversity of ColoradoBoulder, CO, 80309Martha.Palmer@colorado.eduAbstractThis paper shows that two uncertainty-based active learning methods, combinedwith a maximum entropy model, workwell on learning English verb senses.Data analysis on the learning process,based on both instance and feature levels,suggests that a careful treatment of featureextraction is important for the activelearning to be useful for WSD.
Theoverfitting phenomena that occurredduring the active learning process areidentified as classic overfitting in machinelearning based on the data analysis.1 IntroductionCorpus-based methods for word sensedisambiguation (WSD) have gained popularity inrecent years.
As evidenced by the SENSEVALexercises (http://www.senseval.org), machinelearning models supervised by sense-taggedtraining corpora tend to perform better on thelexical sample tasks than unsupervised methods.However, WSD tasks typically have very limitedamounts of training data due to the fact thatcreating large-scale high-quality sense-taggedcorpora is difficult and time-consuming.
Therefore,the lack of sufficient labeled training data hasbecome a major hurdle to improving theperformance of supervised WSD.A promising method for solving this problemcould be the use of active learning.
Researchersuse active learning methods to minimize thelabeling of examples by human annotators.
Adecrease in overall labeling occurs because activelearners (the machine learning models used inactive learning) pick more informative examplesfor the target word (a word whose senses need tobe learned) than those that would be pickedrandomly.
Active learning requires human labelingof the newly selected training data to ensure highquality.We focus here on pool-based active learningwhere there is an abundant supply of unlabeleddata, but where the labeling process is expensive.In NLP problems such as text classification (Lewisand Gale, 1994; McCallum and Nigam, 1998),statistical parsing (Tang et al, 2002), informationextraction (Thompson et al, 1999), and namedentity recognition (Shen et al, 2004), pool-basedactive learning has produced promising results.This paper presents our experiments in applyingtwo active learning methods, a min-margin basedmethod and a Shannon-entropy based one, to thetask of the disambiguation of English verb senses.The contribution of our work is not only indemonstrating that these methods work well for theactive learning of coarse-grained verb senses, butalso analyzing the behavior of the active learningprocess on two levels: the instance level and thefeature level.
The analysis suggests that a carefultreatment of feature design and feature generationis important for a successful application of activelearning to WSD.
We also accounted for theoverfitting phenomena that occurred in the learningprocess based on our data analysis.The rest of the paper is organized as follows.
InSection 2, we introduce two uncertainty samplingmethods used in our active learning experimentsand review related work in using active learningfor WSD.
We then present our active learningexperiments on coarse-grained English verb sensesin Section 3 and analyze the active learning120process in Section 4.
Section 5 presentsconclusions of our study.2 Active Learning AlgorithmsThe methods evaluated in this work fit into acommon framework described by Algorithm 1 (seeTable 1).
The key difference between alternativeactive learning methods is how they assess thevalue of labeling individual examples, i.e., themethods they use for ranking and selecting thecandidate examples for labeling.
The framework iswide open to the type of ranking rule employed.Usually, the ranking rule incorporates the modeltrained on the currently labeled data.
This is thereason for the requirement of a partial training setwhen the algorithm begins.Algorithm 1Require: initial training set, pool of unlabeled examplesRepeatSelect T random examples from poolRank T examples according to active learning rulePresent the top-ranked example to oracle for labelingAugment the training set with the new exampleUntil Training set reaches desirable sizeTable 1.
A Generalized Active Learning LoopIn our experiments we look at two variants ofthe uncertainty sampling heuristic: entropysampling and margin sampling.
Uncertaintysampling is a term invented by Lewis and Gale(Lewis and Gale, 1994) to describe a heuristicwhere a probabilistic classifier picks examples forwhich the model?s current predictions are leastcertain.
The intuitive justification for this approachis that regions where the model is uncertainindicate a decision boundary, and clarifying theposition of decision boundaries is the goal oflearning classifiers.
Schein (2005) demonstratesthe two methods run quickly and competefavorably against alternatives when combined withthe logistic regression classifier.2.1 Entropy SamplingA key question is how to measure uncertainty.Different methods of measuring uncertainty willlead to different variants of uncertainty sampling.We will look at two such measures.
As aconvenient notation we use q (a vector) torepresent the trained model?s predictions, with cqequal to the predicted probability of class c .
Onemethod is to pick the example whose predictionvector q displays the greatest Shannon entropy:?
?ccc qq log    (1)Such a rule means ranking candidate examplesin Algorithm 1 by Equation 1.2.2 Margin SamplingAn alternative method picks the example with thesmallest margin: the difference between the largesttwo values in the vector q (Abe and Mamitsuka,1998).
In other words, if c and 'c are the two mostlikely categories for example nx , the margin ismeasured as follows:)|'Pr()|Pr( nnn xcxcM ?=   (2)In this case Algorithm 1 would rank examplesby increasing values of margin, with the smallestvalue at the top of the ranking.Using either method of uncertainty sampling,the computational cost of picking an example fromT candidates is: O(TD) where D is the number ofmodel parameters.2.3 Related WorkTo our best knowledge, there have been very fewattempts to apply active learning to WSD in theliterature (Fujii and Inui, 1999; Chklovski andMihalcea, 2002; Dang, 2004).
Fujii and Inui (1999)developed an example sampling method for theirexample-based WSD system in the active learningof verb senses in a pool-based setting.
Unlike theuncertainty sampling methods (such as the twomethods we used), their method did not selectexamples for which the system had the minimalcertainty.
Rather, it selected the examples such thatafter training using those examples the systemwould be most certain about its predictions on therest of the unlabeled examples in the next iteration.This sample selection criterion was enforced bycalculating a training utility function.
The methodperformed well on the active learning of Japaneseverb senses.
However, the efficient computation ofthe training utility function relied on the nature ofthe example-based learning method, which madetheir example sampling method difficult to exportto other types of machine learning models.Open Mind Word Expert (Chklovski andMihalcea, 2002) was a real application of activelearning for WSD.
It collected sense-annotatedexamples from the general public through the Webto create the training data for the SENSEVAL-3lexical sample tasks.
The system used the121disagreement of two classifiers (which employeddifferent sets of features) on sense labels toevaluate the difficulty of the unlabeled examplesand ask the web users to tag the difficult examplesit selected.
There was no formal evaluation for thisactive learning system.Dang (2004) used an uncertainty samplingmethod to get additional training data for her WSDsystem.
At each iteration the system selected asmall set of examples for which it had the lowestconfidence and asked the human annotators to tagthese examples.
The experimental results on 5English verbs with fine-grained senses (fromWordNet 1.7) were a little surprising in that activelearning performed no better than randomsampling.
The proposed explanation was that thequality of the manually sense-tagged data waslimited by an inconsistent or unclear senseinventory for the fine-grained senses.3 Active Learning Experiments3.1 Experimental SettingWe experimented with the two uncertaintysampling methods on 5 English verbs that hadcoarse-grained senses (see Table 2), as describedbelow.
By using coarse-grained senses, we limitthe impact of noisy data due to unclear senseboundaries and therefore can get a clearerobservation of the effects of the active learningmethods themselves.verb # ofsen.baselineacc.
(%)Size of data foractive learningSize oftest dataAdd 3 91.4 400 100Do 7 76.9 500 200Feel 3 83.6 400 90See 7 59.7 500 200Work 9 68.3 400 150Table 2.
The number of senses, the baselineaccuracy, the number of instances used for activelearning and for held-out evaluation for each verbThe coarse-grained senses are produced bygrouping together the original WordNet sensesusing syntactic and semantic criteria (Palmer et al,2006).
Double-blind tagging is applied to 50instances of the target word.
If the ITA < 90%, thesense entry is revised by adding examples andexplanations of distinguishing criteria.Table 2 summarizes the statistics of the data.The baseline accuracy was computed by using the?most frequent sense?
heuristic to assign senselabels to verb instances (examples).
The data usedin active learning (Column 4 in Table 2) includetwo parts: an initial labeled training set and a poolof unlabeled training data.
We experimented withsizes 20, 50 and 100 for the initial training set.
Thepool of unlabeled data had actually been annotatedin advance, as in most pool-based active learningexperiments.
Each time an example was selectedfrom the pool by the active learner, its label wasreturned to the learner.
This simulates the processof asking human annotators to tag the selectedunlabeled example at each time.
The advantage ofusing such a simulation is that we can experimentwith different settings (different sizes of the initialtraining set and different sampling methods).The data sets used for active learning and forheld-out evaluation were randomly sampled from alarge data pool for each round of the activelearning experiment.
We ran ten rounds of theexperiments for each verb and averaged thelearning curves for the ten rounds.In the experiments, we used random sampling(picking up an unlabeled example randomly ateach time) as a lower bound.
Another control(ultimate-maxent) was the learner?s performanceon the test set when it was trained on a set oflabeled data that were randomly sampled from alarge data pool and equaled the amount of dataused in the whole active learning process (e.g., 400training data for the verb add).The machine learning model we used for activelearning was a regularized maximum entropy(MaxEnt) model (McCallum, 2002).
The featuresused for disambiguating the verb senses includedtopical, collocation, syntactic (e.g., the subject,object, and preposition phrases taken by a targetverb), and semantic (e.g., the WordNet synsets andhypernyms of the head nouns of a verb?s NParguments) features (Chen and Palmer, 2005).3.2 Experimental ResultsDue to space limits, Figure 1 only shows thelearning curves for 4 verbs do, feel, see, and work(size of the initial training set = 20).
The curve forthe verb add is similar to that for feel.
These curvesclearly show that the two uncertainty samplingmethods, the entropy-based (called entropy-maxentin the figure) and the margin-based (calledmin_margin-maxent), work very well for activelearning of the senses of these verbs.122Figure 1 Active learning for four verbsBoth methods outperformed the randomsampling method in that they reached the upper-bound accuracy earlier and had smoother learningcurves.
For the four verbs add, do, feel and see,their learning curves reached the upper bound atabout 200~300 iterations, which means 1/2 or 1/3of the annotation effort can be saved for theseverbs by using active learning, while still achievingthe same level of performance as supervised WSDwithout using active learning.
Given the large-scale annotation effort currently underway in theOntoNotes project (Hovy et al, 2006), this couldprovide considerable savings in annotation effortand speed up the process of providing sufficientdata for a large vocabulary.
The OntoNotes projecthas now provided coarse-grained entries for over350 verbs, with corresponding double?blindannotation and adjudication in progress.
As thisadjudicated data becomes available, we will beable to train our system accordingly.
Preliminaryresults for 22 of these coarse-grained verbs (withan average grouping polysemy of 4.5) give us anaverage accuracy of 86.3%.
This will also provideopportunities for more experiments with activelearning, where there are enough instances.
Activelearning could also be beneficial in porting thesesupervised taggers to new genres with differentsense distributions.We also experimented with different sizes ofthe initial training set (20, 50 and 100) and foundno significant differences in the performance atdifferent settings.
That means, for these 5 verbs,only 20 labeled training instances will be enoughto initiate an efficient active learning process.From Figure 1, we can see that the twouncertainty sampling methods generally performequally well except that for the verb do, the min-margin method is slightly better than the entropymethod at the beginning of active learning.
Thismay not be so surprising, considering that the twomethods are equal for two-class classification tasks(see Equations 1 and 2 for their definition) and theverbs used in our experiments have coarse-grainedsenses and often have only 2 or 3 major senses.An interesting phenomenon observed fromthese learning curves is that for the two verbs addand feel, the active learner reached the upperbound very soon (at about 100 iterations) and theneven breached the upper bound.
However, whenthe training set was extended, the learner?sperformance dropped and eventually returned to123the same level of the upper bound.
We discuss thephenomenon below.4 Analysis of the Learning ProcessIn addition to verifying the usefulness of activelearning for WSD, we are also interested in adeeper analysis of the learning process.
Forexample, why does the active learner?sperformance drop sometimes during the learningprocess?
What are the characteristics of beneficialfeatures that help to boost the learner?s accuracy?How do we account for the overfitting phenomenathat occurred during the active learning for theverbs add and feel?
We analyzed the effect of bothinstances and features throughout the course ofactive learning using min-margin-based sampling.4.1 Instance-level AnalysisIntuitively, if the learner?s performance drops aftera new example is added to the training set, it islikely that something has gone wrong with the newexample.
To find out such bad examples, wedefine a measure credit_inst for instance i as:?
?=+=?mrllnlAccAccliselm 111)(),(1   (3)where Accl and Accl+1 are the classificationaccuracies of the active learner at the lth and(l+1)th iterations.
n is the total number ofiterations of active learning and m is the number ofrounds of active learning (m=10 in our case).
),( lisel is 1 iff instance i is selected by the activelearner at the lth iteration and is 0 if otherwise.An example is a bad example if and only if itsatisfies the following conditions:a)  its credit_inst value is negativeb) it increases the learner?s performance, if itdoes, less often than it decreases theperformance in the 10 rounds.We ranked the bad examples by theircredit_inst values and their frequency ofdecreasing the learner?s performance in the 10rounds.
Table 3 shows the top five bad examplesfor feel and work.
There are several reasons whythe bad examples may hurt the learner?sperformance.
Column 3 of Table 3 proposesreasons for many of our bad examples.
Wecategorized these reasons into three major types.I.
The major senses of a target verb dependheavily on the semantic categories of its NParguments but WordNet sometimes fails to providethe appropriate semantic categories (features) forthe head nouns of these NP arguments.
Forexample, feel in the board apparently felt nopressure has Sense 1 (experience).
In Sense 1, feeltypically takes an animate subject.
However,board, the head word of the verb?s subject in theabove sentence has no animate meanings definedin WordNet.
Even worse, the major meaning ofboard, i.e., artifact, is typical for the subject of feelin Sense 2 (touch, grope).
Similar semantic typemismatches hold for the last four bad examples ofthe verb work in Table 3.II.
The contexts of the target verb are difficultfor our feature exaction module to analyze.
Forexample, the antecedent for the pronoun subjectthey in the first example of work in Table 3 shouldbe ringers, an agent subject that is typical forSense 1 (exert oneself in an activity).
However, thefeature exaction module found the wrongantecedent changes that is an unlikely fit for theintended verb sense.
In the fourth example for feel,the feature extraction module cannot handle theexpletive ?it?
(a dummy subject) in ?it was feltthat?, therefore, it cannot identify the typicalsyntactic pattern for Sense 3 (find, conclude), i.e.,subject+feel+relative clause.III.
Sometimes, deep semantic and discourseanalyses are needed to get the correct meaning ofthe target verb.
For example, in the third exampleof feel, ?
?, he or she feels age creeping up?, it isdifficult to tell whether the verb has Sense 1(experience) or Sense 3 (find) without anunderstanding of the meaning of the relative clauseand without looking at a broader discourse context.The syntactic pattern identified by our featureextraction module, subject+feel+relative clause,favors Sense 3 (find), which leads to an inaccurateinterpretation for this case.Recall that the motivation behind uncertaintysamplers is to find examples near decisionboundaries and use them to clarify the position ofthese boundaries.
Active learning often does findinformative examples, either ones from the lesscommon senses or ones close to the boundarybetween the different senses.
However, activelearning also identifies example sentences that aredifficult to analyze.
The failure of our featureextraction module, the lack of appropriate semanticcategories for certain NP arguments in WordNet,the lack of deep analysis (semantic and discourseanalysis) of the context of the target verb can all124Table 3 Data analysis of the top-ranked bad examples found for two verbsproduce misleading features.
Therefore, in order tomake active learning useful for its applications,both identifying difficult examples and gettinggood features for these examples are equallyimportant.
In other words, a careful treatment offeature design and feature generation is necessaryfor a successful application of active learning.There is a positive side to identifying such?bad?
examples; one can have human annotatorslook at the features generated from the sentences(as we did above), and use this to improve the dataor the classifier.
Note that this is exactly what wedid above: the identification of bad sentences wasautomatic, and they could then be reannotated orremoved from the training set or the featureextraction module needs to be refined to generateinformative features for these sentences.Not all sentences have obvious interpretations;hence the two question marks in Table 3.
Anexample can be bad for many reasons: conflictingfeatures (indicative of different senses), misleadingfeatures (indicative of non-intended senses), or justcontaining random features that are incorrectlyincorporated into the model.
We will return to thispoint in our discussion of the overfittingphenomena for active learning in Section 4.3.4.2 Feature-level AnalysisThe purpose of our feature-level analysis is toidentify informative features for verb senses.
Thelearning curve of the active learner may providesome clues.
The basic idea is, if the learner?sperformance increases after adding a new example,it is likely that the good example contains goodfeatures that contribute to the clarification of senseboundaries.
However, the feature-level analysis ismuch less straightforward than the instance-levelanalysis since we cannot simply say the featuresthat are active (present) in this good example areall good.
Rather, an example often contains bothgood and bad features, and many other featuresthat are somehow neutral or uninformative.
Theinteraction or balance between these featuresdetermines the final outcome.
On the other hand, astatistics based analysis may help us to findfeatures that tend to be good or bad.
For thisanalysis, we define a measure credit_feat forfeature i as:feel Proposed reasons for bad examples SensesSome days the coaches make you feel as though youare part of a large herd of animals .?
S1: experienceAnd , with no other offers on the table , the boardapparently felt no pressure to act on it.subject: board, no ?animate?
meaning inWordNetS1: experienceSometimes a burst of aggressiveness will sweep over aman -- or his wife -- because he or she feels agecreeping up.syntactic pattern: sbj+feel+relative clauseheaded by that, a typical pattern for Sense3 (find) rather than Sense 1 (experience)S1: experienceAt this stage it was felt I was perhaps more pertinent aschief.
executive .syntactic pattern: sbj+feel+relative clause,typical for Sense 3 (find) but has not beendetected by the feature exaction moduleS3: find, concludeI felt better Tuesday evening when I woke up.
?
S1: experienceWorkWhen their changes are completed, and after they haveworked up a sweat, ringers often ?
?subject: they, the feature exaction modulefound the wrong antecedent (changesrather than ringers) for theyS1: exert oneselfin an activityOthers grab books, records , photo albums , sofas andchairs , working frantically in the fear that anaftershock will jolt the house again .subject: others (means people here), nodefinition in WordNetS1: exert oneselfin an activitySecurity Pacific 's factoring business works withcompanies in the apparel, textile and food industries ?subject: business, no ?animate?
meaningin WordNetS1: exert oneselfin an activity?
; blacks could work there , but they had to leave atnight .subject: blacks, no ?animate?
meaning inWordNetS1: exert oneselfin an activity?
has been replaced by alginates (gelatin-like material) that work quickly and accurately and with leastdiscomfort to a child .subject: alginates, unknown by WordNet S2: perform,function, behave125?
?=+=?mr lllnl actAccAccliactivem 1111)(),(1         (4)where ),( liactive is 1 iff feature i is active in theexample selected by the active learner at the lthiteration and is 0 if otherwise.
actl is the totalnumber of active features in the example selectedat the lth iteration.
n and m have the samedefinition as in Equation 3.A feature is regarded as good if its credit_featvalue is positive.
We ranked the good features bytheir credit_feat values.
By looking at the top-ranked good features for the verb work (due tospace limitations, we omit the table data), weidentify two types of typically good features.The first type of good feature occurs frequentlyin the data and has a frequency distribution overthe senses similar to the data distribution over thesenses.
Such features include those denoting thatthe target verb takes a subject (subj), is not used ina passive mode (morph_normal), does not take adirect object (intransitive), occurs in present tense(word_work, pos_vb, word_works, pos_vbz), andsemantic features denoting an abstract subject(subjsyn_16993 1) or an entity subject (subjsyn_1742), etc.
We call such features backgroundfeatures.
They help the machine learning modellearn the appropriate sense distribution of the data.In other words, a learning model only using suchfeatures will be equal to the ?most frequent sense?heuristic used in WSD.Another type of good feature occurs lessfrequently and has a frequency distribution oversenses that mismatches with the sense distributionof the data.
Such features include those denotingthat the target verb takes an inanimate subject(subj_it), takes a particle out (prt_out), is followeddirectly by the word out (word+1_out), or occurs atthe end of the sentence.
Such features areindicative of less frequent verb senses  that stilloccur fairly frequently in the data.
For example,taking an inanimate subject (subj_it) is a strongclue for Sense 2 (perform, function, behave) of theverb work.
Occurring at the end of the sentence isalso indicative of Sense 2 since when work is usedin Sense 1 (exert oneself in an activity), it tends totake adjuncts to modify the activity as in He isworking hard to bring up his grade.1 Those features are from the WordNet.
The numbers areWordNet ids of synsets and hypernyms.There are some features that don?t fall into theabove two categories, such as the topical featuretp_know and the collocation feature pos-2_nn.There are no obvious reasons why they are goodfor the learning process, although it is possible thatthe combination of two or more such featurescould make a clear sense distinction.
However, thishypothesis cannot be verified by our currentstatistics-based analysis.
It is also worth noting thatour current feature analysis is post-experimental(i.e., based on the results).
In the future, we will tryautomatic feature selection methods that can beused in the training phase to select useful featuresand/or their combinations.We have similar results for the feature analysisof the other four verbs.4.3 Account for the Overfitting PhenomenaRecall that in the instance-level analysis in Section4.1, we found that some examples hurt the learningperformance during active learning but for noobvious reasons (the two examples marked by ?
inTable 3).
We found that these two examplesoccurred in the overfitting region for feel.
Bylooking at the bad examples (using the samedefinition for bad example as in Section 4.1) thatoccurred in the overfitting region for both feel andadd, we identified two major properties of theseexamples.
First, most of them occurred only onceas bad examples (19 out 23 for add and 40 out of63 for feel).
Second, many of the examples had noobvious reasons for their badness.Based on the above observations, we believethat the overfitting phenomena that occurred forthe two verbs during active learning is typical ofclassic overfitting, which is consistent with a"death by a thousand mosquito bites" of rare badfeatures, and consistent with there often being (tomix a metaphor) no "smoking gun" of a badfeature/instance that is added in, especially in theregion far away from the starting point of activelearning.5 ConclusionsWe have shown that active learning can lead tosubstantial reductions (often by half) in the numberof observations that need to be labeled to achieve agiven accuracy in word sense disambiguation,compared to labeling randomly selected instances.In a follow-up experiment, we also compared alarger number of different active learning methods.126The results suggest that for tasks like word sensedisambiguation where maximum entropy methodsare used as the base learning models, the minimummargin active criterion for active learning givessuperior results to more comprehensivecompetitors including bagging and two variants ofquery by committee (Schein, 2005).
By also takinginto account the high running efficiency of themin-margin method, it is a very promising activelearning method for WSD.We did an analysis on the learning process ontwo levels: instance-level and feature-level.
Theanalysis suggests that a careful treatment of featuredesign and feature generation is very important forthe active learner to take advantage of the difficultexamples it finds during the learning process.
Thefeature-level analysis identifies somecharacteristics of good features.
It is worth notingthat the good features identified are not particularlytied to active learning, and could also be obtainedby a more standard feature selection method ratherthan by looking at how the features providebenefits as they are added in.For a couple of the verbs examined, we foundthat active learning gives higher predictionaccuracy midway through the training than onegets after training on the entire corpus.
Analysissuggests that this is not due to bad examples beingadded to the training set.
It appears that the widelyused maximum entropy model with Gaussianpriors is overfitting: the model by including toomany features and thus fitting noise as well assignal.
Using different strengths of the Gaussianprior does not solve the problem.
If a very strongprior is used, then poorer accuracy is obtained.
Webelieve that using appropriate feature selectionwould cause the phenomenon to vanish.AcknowledgementsThis work was supported by National ScienceFoundation Grant NSF-0415923, Word SenseDisambiguation, the DTO-AQUAINT NBCHC-040036 grant under the University of Illinoissubcontract to University of Pennsylvania 2003-07911-01 and the GALE program of the DefenseAdvanced Research Projects Agency, Contract No.HR0011-06-C-0022.
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the authors and do notnecessarily reflect the views of the NationalScience Foundation, the DTO, or DARPA.ReferencesNaoki Abe and Hiroshi Mamitsuka.
1998.
Querylearning strategies using boosting and bagging.
InProc.
of ICML1998, pages 1?10.Jinying Chen and Martha Palmer.
2005.
TowardsRobust High Performance Word SenseDisambiguation of English Verbs Using RichLinguistic Features, In Proc.
of IJCNLP2005, Oct.,Jeju, Republic of Korea.Tim Chklovski and Rada Mihalcea, Building a SenseTagged Corpus with Open Mind Word Expert, inProceedings of the ACL 2002 Workshop on "WordSense Disambiguation: Recent Successes and FutureDirections", Philadelphia, July 2002.Hoa T. Dang.
2004.
Investigations into the role oflexical semantics in word sense disambiguation.
PhDThesis.
University of Pennsylvania.Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui,Hozumi Tanaka.
1998.
Selective sampling forexample-based word sense disambiguation,Computational Linguistics, v.24 n.4, p.573-597, Dec.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw and Ralph Weischedel.
OntoNotes: The90% Solution.
Accepted by HLT-NAACL06.
Shortpaper.David D. Lewis and William A. Gale.
1994.
Asequential algorithm for training text classifiers.
In W.Bruce Croft and Cornelis J. van Rijsbergen, editors,Proceedings of SIGIR-94, Dublin, IE.Andrew K. McCallum.
2002.
MALLET: A MachineLearning for Language Toolkit.
http://www.cs.umass.edu/~mccallum/mallet.Andew McCallum and Kamal Nigam.
1998.
EmployingEM in pool-based active learning for textclassification.
In Proc.
of ICML ?98.Martha Palmer, Hoa Trang Dang and ChristianeFellbaum.
(to appear, 2006).
Making fine-grained andcoarse-grained sense distinctions, both manually andautomatically.
Natural Language Engineering.Andrew I. Schein.
2005.
Active Learning for LogisticRegression.
Ph.D. Thesis.
Univ.
of Pennsylvania.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou and ChewLim Tan.
2004 Multi-criteria-based active learningfor named entity recognition, In Proc.
of ACL04,Barcelona, Spain.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.Active learning for statistical natural languageparsing.
In Proc.
of ACL 2002.Cynthia A. Thompson, Mary Elaine Califf, andRaymond J. Mooney.
1999.
Active learning fornatural language parsing and information extraction.In Proc.
of ICML-99.127
