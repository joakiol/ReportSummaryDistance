Proceedings of the 12th Conference of the European Chapter of the ACL, pages 264?272,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsTBL-Improved Non-Deterministic Segmentation and POS Tagging for aChinese ParserMartin Forst & Ji FangIntelligent Systems LaboratoryPalo Alto Research CenterPalo Alto, CA 94304, USA{mforst|fang}@parc.comAbstractAlthough a lot of progress has been maderecently in word segmentation and POStagging for Chinese, the output of cur-rent state-of-the-art systems is too inaccu-rate to allow for syntactic analysis basedon it.
We present an experiment in im-proving the output of an off-the-shelf mod-ule that performs segmentation and tag-ging, the tokenizer-tagger from BeijingUniversity (PKU).
Our approach is basedon transformation-based learning (TBL).Unlike in other TBL-based approaches tothe problem, however, both obligatory andoptional transformation rules are learned,so that the final system can output multi-ple segmentation and POS tagging anal-yses for a given input.
By allowing fora small amount of ambiguity in the out-put of the tokenizer-tagger, we achieve avery considerable improvement in accu-racy.
Compared to the PKU tokenizer-tagger, we improve segmentation F-scorefrom 94.18% to 96.74%, tagged wordF-score from 84.63% to 92.44%, seg-mented sentence accuracy from 47.15%to 65.06% and tagged sentence accuracyfrom 14.07% to 31.47%.1 IntroductionWord segmentation and tagging are the neces-sary initial steps for almost any language process-ing system, and Chinese parsers are no exception.However, automatic Chinese word segmentationand tagging has been recognized as a very difficulttask (Sproat and Emerson, 2003), for the follow-ing reasons:First, Chinese text provides few cues for wordboundaries (Xia, 2000; Wu, 2003) and part-of-speech (POS) information.
With the exception ofpunctuation marks, Chinese does not have worddelimiters such as the whitespace used in Englishtext, and unlike other languages without whites-paces such as Japanese, Chinese lacks morpholog-ical inflections that could provide cues for wordboundaries and POS information.
In fact, the lackof word boundary marks and morphological in-flection contributes not only to mistakes in ma-chine processing of Chinese; it has also been iden-tified as a factor for parsing miscues in Chinesechildren?s reading behavior (Chang et al, 1992).Second, in addition to the two problems de-scribed above, segmentation and tagging also suf-fer from the fact that the notion of a word isvery unclear in Chinese (Xu, 1997; Packard, 2000;Hsu, 2002).
While the word is an intuitive andsalient notion in English, it is by no means aclear notion in Chinese.
Instead, for historicalreasons, the intuitive and clear notion in Chineselanguage and culture is the character rather thanthe word.
Classical Chinese is in general mono-syllabic, with each syllable corresponding to anindependent morpheme that can be visually ren-dered with a written character.
In other words,characters did represent the basic syntactic unit inClassical Chinese, and thus became the sociolog-ically intuitive notion.
However, although collo-quial Chinese quickly evolved throughout Chinesehistory to be disyllabic or multi-syllabic, monosyl-labic Classical Chinese has been considered moreelegant and proper and was commonly used inwritten text until the early 20th century in China.Even in Modern Chinese written text, ClassicalChinese elements are not rare.
Consequently, evenif a morpheme represented by a character is no264longer used independently in Modern colloquialChinese, it might still appear to be a free mor-pheme in modern written text, because it containsClassical Chinese elements.
This fact leads to aphenomenon in which Chinese speakers have dif-ficulty differentiating whether a character repre-sents a bound or free morpheme, which in turnaffects their judgment regarding where the wordboundaries should be.
As pointed out by Hoosain(Hoosain, 1992), the varying knowledge of Classi-cal Chinese among native Chinese speakers in factaffects their judgments about what is or is not aword.
In summary, due to the influence of Classi-cal Chinese, the notion of a word and the bound-ary between a bound and free morpheme is veryunclear for Chinese speakers, which in turn leadsto a fuzzy perception of where word boundariesshould be.Consequently, automatic segmentation and tag-ging in Chinese faces a serious challenge fromprevalent ambiguities.
For example 1, the string?????
can be segmented as (1a) or (1b), de-pending on the context.
(1) a. ?
?
?yo?u y?`jianhave disagreementb.
??
?yo?uy?` jia`nhave the intention meetThe contrast shown in (2) illustrates that even astring that is not ambiguous in terms of segmenta-tion can still be ambiguous in terms of tagging.
(2) a.
?/a ?/nba?i hua?white flowerb.
?/d ?/vba?i hua?in vain spend?spend (money, time, energy etc.)
in vain?Even Chinese speakers cannot resolve such am-biguities without using further information froma bigger context, which suggests that resolvingsegmentation and tagging ambiguities probablyshould not be a task or goal at the word level.
In-stead, we should preserve such ambiguities in thislevel and leave them to be resolved in a later stage,when more information is available.1(1) and (2) are cited from (Fang and King, 2007)To summarize, the word as a notion and henceword boundaries are very unclear; segmentationand tagging are prevalently ambiguous in Chinese.These facts suggest that Chinese segmentation andpart-of-speech identification are probably inher-ently non-deterministic at the word level.
How-ever most of the current segmentation and/or tag-ging systems output a single result.While a deterministic approach to Chinese seg-mentation and POS tagging might be appropriateand necessary for certain tasks or applications, ithas been shown to suffer from a problem of lowaccuracy.
As pointed out by Yu (Yu et al, 2004),although the segmentation and tagging accuracyfor certain types of text can reach as high as 95%,the accuracy for open domain text is only slightlyhigher than 80%.
Furthermore, Chinese segmenta-tion (SIGHAN) bakeoff results also show that theperformance of the Chinese segmentation systemshas not improved a whole lot since 2003.
Thisfact also indicates that deterministic approachesto Chinese segmentation have hit a bottleneck interms of accuracy.The system for which we improved the outputof the Beijing tokenizer-tagger is a hand-craftedChinese grammar.
For such a system, as proba-bly for any parsing system that presupposes seg-mented (and tagged) input, the accuracy of thesegmentation and POS tagging analyses is criti-cal.
However, as described in detail in the fol-lowing section, even current state-of-art systemscannot provide satisfactory results for our ap-plication.
Based on the experiments presentedin section 3, we believe that a proper amountof non-deterministic results can significantly im-prove the Chinese segmentation and tagging accu-racy, which in turn improves the performance ofthe grammar.2 BackgroundThe improved tokenizer-tagger we developed ispart of a larger system, namely a deep Chinesegrammar (Fang and King, 2007).
The systemis hybrid in that it uses probability estimates forparse pruning (and it is planned to use trainedweights for parse ranking), but the ?core?
gram-mar is rule-based.
It is written within the frame-work of Lexical Functional Grammar (LFG) andimplemented on the XLE system (Crouch et al,2006; Maxwell and Kaplan, 1996).
The input toour system is a raw Chinese string such as (3).265(3)??
?
?
?xia?owa?ng zo?u le .XiaoWang leave ASP 2 .
?XiaoWang left.
?The output of the Chinese LFG consists of aConstituent Structure (c-structure) and a Func-tional Structure (f-structure) for each sentence.While c-structure represents phrasal structure andlinear word order, f-structure represents variousfunctional relations between parts of sentences.For example, (4) and (5) are the c-structure and f-structure that the grammar produces for (3).
Bothc-structure and f-structure information are carriedin syntactic rules in the grammar.
(4) c-structure of (3)(5) f-structure of (3)To parse a sentence, the Chinese LFG min-imally requires three components: a tokenizer-tagger, a lexicon, and syntactic rules.
Thetokenizer-tagger that is currently used in the gram-mar is developed by Beijing University (PKU)3and is incorporated as a library transducer (Crouchet al, 2006).Because the grammar?s syntactic rules are ap-plied based upon the results produced by thetokenizer-tagger, the performance of the latter is2ASP stands for aspect marker.3http://www.icl.pku.edu.cn/icl res/critical to overall quality of the system?s out-put.
However, even though PKU?s tokenizer-tagger is one of the state-of-art systems, its per-formance is not satisfactory for the Chinese LFG.This becomes clear from a small-scale evaluationin which the system was tested on a set of 101gold sentences chosen from the Chinese Treebank5 (CTB5) (Xue et al, 2002; Xue et al, 2005).These 101 sentences are 10-20 words long andall of them are chosen from Xinhua sources 4.Based on the deterministic segmentation and tag-ging results produced by PKU?s tokenizer-tagger,the Chinese LFG can only parse 80 out of the101 sentences.
Among the 80 sentences that areparsed, 66 received full parses and 14 receivedfragmented parses.
Among the 21 completelyfailed sentences, 20 sentences failed due to seg-mentation and tagging mistakes.This simple test shows that in order for thedeep Chinese grammar to be practically useful,the performance of the tokenizer-tagger must beimproved.
One way to improve the segmentationand tagging accuracy is to allow non-deterministicsegmentation and tagging for Chinese for the rea-sons stated in Section 1.
Therefore, our goalis to find a way to transform PKU?s tokenizer-tagger into a system that produces a proper amountof non-deterministic segmentation and tagging re-sults, one that can significantly improve the sys-tem?s accuracy without a substantial sacrifice interms of efficiency.
Our approach is described inthe following section.3 FST5 Rules for the Improvement ofSegmentation and Tagging OutputFor grammars of other languages implemented onthe XLE grammar development platform, the in-put is usually preprocessed by a cascade of gener-ally non-deterministic finite state transducers thatperform tokenization, morphological analysis etc.Since word segmentation and POS tagging aresuch hard problems in Chinese, this traditionalsetup is not an option for the Chinese grammar.However, finite state rules seem a quite natural ap-proach to improving in XLE the output of a sep-4The reason why only sentences from Xinhua sourceswere chosen is because the version of PKU?s tokenizer-taggerthat was integrated into the system was not designed to han-dle data from Hong Kong and Taiwan.5We use the abbreviation ?FST?
for ?finite-state trans-ducer?.
fst is used to refer to the finite-state tool called fst,which was developed by Beesley and Karttunen (2003).266arate segmentation and POS tagging module likePKU?s tokenizer-tagger.3.1 Hand-Crafted FST Rules for ConceptProvingAlthough the grammar developer had identifiedPKU?s tokenizer-tagger as the most suitable forthe preprocessing of Chinese raw text that isto be parsed with the Chinese LFG, she no-ticed in the process of development that (i) cer-tain segmentation and/or tagging decisions takenby the tokenizer-tagger systematically go counterher morphosyntactic judgment and that (ii) thetokenizer-tagger (as any software of its kind)makes mistakes.
She therefore decided to developa set of finite-state rules that transform the outputof the module; a set of mostly obligatory rewriterules adapts the POS-tagged word sequence to thegrammar?s standard, and another set of mostly op-tional rules tries to offer alternative segment andtag sequences for sequences that are frequentlyprocessed erroneously by PKU?s tokenizer-tagger.Given the absence of data segmented and taggedaccording to the standard the LFG grammar de-veloper desired, the technique of hand-craftingFST rules to postprocess the output of PKU?stokenizer-tagger worked surprisingly well.
Re-call that based on the deterministic segmentationand tagging results produced by PKU?s tokenizer-tagger, our system can only parse 80 out of the 101sentences, and among the 21 completely failedsentences, 20 sentences failed due to segmenta-tion and tagging mistakes.
In contrast, after theapplication of the hand-crafted FST rules for post-processing, 100 out of the 101 sentences can beparsed.
However, this approach involved a lotof manual development work (about 3-4 personmonths) and has reached a stage where it is dif-ficult to systematically work on further improve-ments.3.2 Machine-Learned FST RulesSince there are large amounts of training data thatare close to the segmentation and tagging standardthe grammar developer wants to use, the idea ofinducing FST rules rather than hand-crafting themcomes quite naturally.
The easiest way to do thisis to apply transformation-based learning (TBL) tothe combined problem of Chinese segmentationand POS tagging, since the cascade of transfor-mational rules learned in a TBL training run canstraightforwardly be translated into a cascade ofFST rules.3.2.1 Transformation-Based Learning and?-TBLTBL is a machine learning approach that has beenemployed to solve a number of problems in nat-ural language processing; most famously, it hasbeen used for part-of-speech tagging (Brill, 1995).TBL is a supervised learning approach, since it re-lies on gold-annotated training data.
In addition,it relies on a set of templates of transformationalrules; learning consists in finding a sequence of in-stantiations of these templates that minimizes thenumber of errors in a more or less naive base-lineoutput with respect to the gold-annotated trainingdata.The first attempts to employ TBL to solve theproblem of Chinese word segmentation go back toPalmer (1997) and Hockenmaier and Brew (1998).In more recent work, TBL was used for the adap-tion of the output of a statistical ?general pur-pose?
segmenter to standards that vary depend-ing on the application that requires sentence seg-mentation (Gao et al, 2004).
TBL approaches tothe combined problem of segmenting and POS-tagging Chinese sentences are reported in Florianand Ngai (2001) and Fung et al (2004).Several implementations of the TBL approachare freely available on the web, the most well-known being the so-called Brill tagger, fnTBL,which allows for multi-dimensional TBL, and?-TBL (Lager, 1999).
Among these, we chose?-TBL for our experiments because (like fnTBL)it is completely flexible as to whether a sampleis a word, a character or anything else and (un-like fnTBL) it allows for the induction of optionalrules.
Probably due to its flexibility, ?-TBL hasbeen used (albeit on a small scale for the most part)for tasks as diverse as POS tagging, map tasks, andmachine translation.3.2.2 Experiment Set-upWe started out with a corpus of thirty gold-segmented and -tagged daily editions of the Xin-hua Daily, which were provided by the Instituteof Computational Linguistics at Beijing Univer-sity.
Three daily editions, which comprise 5,054sentences with 129,377 words and 213,936 char-acters, were set aside for testing purposes; the re-maining 27 editions were used for training.
Withthe idea of learning both obligatory and optional267transformational rules in mind, we then split thetraining data into two roughly equally sized sub-sets.
All the data were broken into sentences us-ing a very simple method: The end of a para-graph was always considered a sentence bound-ary.
Within paragraphs, sentence-final punctua-tion marks such as periods (which are unambigu-ous in Chinese), question marks and exclamationmarks, potentially followed by a closing parenthe-sis, bracket or quote mark, were considered sen-tence boundaries.We then had to come up with a way of cast-ing the problem of combined segmentation andPOS tagging as a TBL problem.
Following a strat-egy widely used in Chinese word segmentation,we did this by regarding the problem as a charac-ter tagging problem.
However, since we intendedto learn rules that deal with segmentation andPOS tagging simultaneously, we could not adoptthe BIO-coding approach.6 Also, since the TBL-induced transformational rules were to be con-verted into FST rules, we had to keep our charactertagging scheme one-dimensional, unlike Florianand Ngai (2001), who used a multi-dimensionalTBL approach to solve the problem of combinedsegmentation and POS tagging.The character tagging scheme that we finallychose is illustrated in (6), where a. and b. show thecharacter tags that we used for the analyses in (1a)and (1b) respectively.
The scheme consists in tag-ging the last character of a word with the part-of-speech of the entire word; all non-final charactersare tagged with ?-?.
The main advantages of thischaracter tagging scheme are that it expresses bothword boundaries and parts-of-speech and that, atthe same time, it is always consistent; inconsisten-cies between BIO tags indicating word boundariesand part-of-speech tags, which Florian and Ngai(2001), for example, have to resolve, can simplynot arise.(6)?
?
?a.
v - nb.
- v vBoth of the training data subsets were taggedaccording to our character tagging scheme and6In this character tagging approach to word segmentation,characters are tagged as the beginning of a word (B), inside(or at the end) of a multi-character word (I) or a word of theirown (O).
Their are numerous variations of this approach.converted to the data format expected by ?-TBL.The first training data subset was used for learn-ing obligatory resegmentation and retagging rules.The corresponding rule templates, which definethe space of possible rules to be explored, aregiven in Figure 1.
The training parameters of?-TBL, which are an accuracy threshold and ascore threshold, were set to 0.75 and 5 respec-tively; this means that a potential rule was onlyretained if at least 75% of the samples to which itwould have applied were actually modified in thesense of the gold standard and not in some otherway and that the learning process was terminatedwhen no more rule could be found that applied toat least 5 samples in the first training data subset.With these training parameters, 3,319 obligatoryrules were learned by ?-TBL.Once the obligatory rules had been learned onthe first training data subset, they were applied tothe second training data subset.
Then, optionalrules were learned on this second training datasubset.
The rule templates used for optional rulesare very similar to the ones used for obligatoryrules; a few templates of optional rules are given inFigure 2.
The difference between obligatory rulesand optional rules is that the former replace onecharacter tag by another, whereas the latter addcharacter tags.
They hence introduce ambiguity,which is why we call them optional rules.
Like inthe learning of the obligatory rules, the accuracythreshold used was 0.75; the score theshold wasset to 7 because the training software seemed tohit a bug below that threshold.
753 optional ruleswere learned.
We did not experiment with the ad-justment of the training parameters on a separateheld-out set.Finally, the rule sets learned were converted intothe fst (Beesley and Karttunen, 2003) notation fortransformational rules, so that they could be testedand used in the FST cascade used for preprocess-ing the input of the Chinese LFG.
For evaluation,the converted rules were applied to our test data setof 5,054 sentences.
A few example rules learnedby ?-TBL with the set-up described above aregiven in Figure 3; we show them both in ?-TBLnotation and in fst notation.3.2.3 ResultsThe results achieved by PKU?s tokenizer-taggeron its own and in combination with the trans-formational rules learned in our experiments aregiven in Table 1.
We compare the output of PKU?s268tag:m> - <- wd:???
@[0] & wd:???
@[1] & "/" m WS @-> 0 || ?
_ ?
[ ( TAG )tag:q@[1,2,3,4] & {\+q=(-)}.
CHAR ]?
{0,3} "/" q WStag:r>n <- wd:???
@[-1] & wd:???@[0].
"/" r WS @-> "/" n TB || ?
( TAG ) ?
_tag:add nr <- tag:(-)@[0] & wd:???@[1].
[..] (@->) "/" n r TB || CHAR _ ?...
...Figure 3: Sample rules learned in our experiments in ?-TBL notation on the left and in fst notation onthe right8tag:A>B <- ch:C@[0].tag:A>B <- ch:C@[1].tag:A>B <- ch:C@[-1] & ch:D@[0].tag:A>B <- ch:C@[0] & ch:D@[1].tag:A>B <- ch:C@[1] & ch:D@[2].tag:A>B <- ch:C@[-2] & ch:D@[-1] &ch:E@[0].tag:A>B <- ch:C@[-1] & ch:D@[0] &ch:E@[1].tag:A>B <- ch:C@[0] & ch:D@[1] & ch:E@[2].tag:A>B <- ch:C@[1] & ch:D@[2] & ch:E@[3].tag:A>B <- tag:C@[-1].tag:A>B <- tag:C@[1].tag:A>B <- tag:C@[1] & tag:D@[2].tag:A>B <- tag:C@[-2] & tag:D@[-1].tag:A>B <- tag:C@[-1] & tag:D@[1].tag:A>B <- tag:C@[1] & tag:D@[2].tag:A>B <- tag:C@[1] & tag:D@[2] &tag:E@[3].tag:A>B <- tag:C@[-1] & ch:W@[0].tag:A>B <- tag:C@[1] & ch:W@[0].tag:A>B <- tag:C@[1] & tag:D@[2] &ch:W@[0].tag:A>B <- tag:C@[-2] & tag:D@[-1] &ch:W@[0].tag:A>B <- tag:C@[-1] & tag:D@[1] &ch:W@[0].tag:A>B <- tag:C@[1] & tag:D@[2] &ch:W@[0].tag:A>B <- tag:C@[1] & tag:D@[2] &tag:E@[3] & ch:W@[0].tag:A>B <- tag:C@[-1] & ch:W@[1].tag:A>B <- tag:C@[1] & ch:W@[1].tag:A>B <- tag:C@[1] & tag:D@[2] &ch:W@[1].tag:A>B <- tag:C@[-2] & tag:D@[-1] &ch:W@[1].tag:A>B <- tag:C@[-1] & ch:D@[0] &ch:E@[1].tag:A>B <- tag:C@[-1] & tag:D@[1] &ch:W@[1].tag:A>B <- tag:C@[1] & tag:D@[2] &ch:W@[1].tag:A>B <- tag:C@[1] & tag:D@[2] &tag:E@[3] & ch:W@[1].tag:A>B <- tag:C@[1,2,3,4] & {\+C=?-?
}.tag:A>B <- ch:C@[0] & tag:D@[1,2,3,4] &{\+D=?-?
}.tag:A>B <- tag:C@[-1] & ch:D@[0] &tag:E@[1,2,3,4] & {\+E=?-?
}.tag:A>B <- ch:C@[0] & ch:D@[1] &tag:E@[1,2,3,4] & {\+E=?-?
}.Figure 1: Templates of obligatory rules used in ourexperimentstag:add B <- tag:A@[0] & ch:C@[0].tag:add B <- tag:A@[0] & ch:C@[1].tag:add B <- tag:A@[0] & ch:C@[-1] &ch:D@[0]....Figure 2: Sample templates of optional rules usedin our experimentstokenizer-tagger run in the mode where it returnsonly the most probable tag for each word (PKUone tag), of PKU?s tokenizer-tagger run in themode where it returns all possible tags for a givenword (PKU all tags), of PKU?s tokenizer-taggerin one-tag mode augmented with the obligatorytransformational rules learned on the first part ofour training data (PKU one tag + deterministic ruleset), and of PKU?s tokenizer-tagger augmentedwith both the obligatory and optional rules learnedon the first and second parts of our training data re-spectively (PKU one tag + non-deterministic ruleset).
We give results in terms of character tag ac-curacy and ambiguity according to our charactertagging scheme.
Then we provide evaluation fig-ures for the word level.
Finally, we give results re-ferring to the sentence level in order to make clearhow serious a problem Chinese segmentation andPOS tagging still are for parsers, which obviouslyoperate at the sentence level.These results show that simply switching fromthe one-tag mode of PKU?s tokenizer-tagger to itsall-tags mode is not a solution.
First of all, sincethe tokenizer-tagger always produces only onesegmentation regardless of the mode it is used in,segmentation accuracy would stay completely un-affected by this change, which is particularly seri-ous because there is no way for the grammar to re-cover from segmentation errors and the tokenizer-tagger produces an entirely correct segmentationonly for 47.15% of the sentences.
Second, theimproved tagging accuracy would come at a veryheavy price in terms of ambiguity; the mediannumber of combined segmentation and POS tag-ging analyses per sentence would be 1,440.269In contrast, machine-learned transformationrules are an effective means to improve the out-put of PKU?s tokenizer-tagger.
Applying onlythe obligatory rules that were learned already im-proves segmented sentence accuracy from 47.15%to 63.14% and tagged sentence accuracy from14.07% to 27.21%, and this at no cost in termsof ambiguity.
Adding the optional rules that werelearned and hence making the rule set used forpost-processing the output of PKU?s tokenizer-tagger non-deterministic makes it possible to im-prove segmented sentence accuracy and taggedsentence accuracy further to 65.06% and 31.47%respectively, i.e.
tagged sentence accuracy is morethan doubled with respect to the baseline.
Whilethis last improvement does come at a price interms of ambiguity, the ambiguity resulting fromthe application of the non-deterministic rule set isvery low in comparison to the ambiguity of theoutput of PKU?s tokenizer-tagger in all-tags mode;the median number of analyses per sentences onlyincreases to 2.
Finally, it should be noted thatthe transformational rules provide entirely correctsegmentation and POS tagging analyses not onlyfor more sentences, but also for longer sentences.They increase the average length of a correctlysegmented sentence from 18.22 words to 21.94words and the average length of a correctly seg-mented and POS-tagged sentence from 9.58 wordsto 16.33 words.4 Comparison to related work andDiscussionComparing our results to other results in the liter-ature is not an easy task because segmentation andPOS tagging standards vary, and our test data havenot been used for a final evaluation before.
Nev-ertheless, there are of course systems that performword segmentation and POS tagging for Chineseand have been evaluated on data similar to our testdata.Published results also vary as to the evalua-tion measures used, in particular when it comesto combined word segmentation and POS tag-ging.
For word segmentation considered sepa-rately, the consensus is to use the (segmentation)F-score (SF).
The quality of systems that performboth segmentation and POS tagging is often ex-pressed in terms of (character) tag accuracy (TA),but this obviously depends on the character tag-ging scheme adopted.
An alternative measure isPOS tagging F-score (TF), which is the geomet-ric mean of precision and recall of correctly seg-mented and POS-tagged words.
Evaluation mea-sures for the sentence level have not been given inany publication that we are aware of, probably be-cause segmenters and POS taggers are rarely con-sidered as pre-processing modules for parsers, butalso because the figures for measures like sentenceaccuracy are strikingly low.For systems that perform only word segmenta-tion, we find the following results in the literature:(Gao et al, 2004), who use TBL to adapt a ?gen-eral purpose?
segmenter to varying standards, re-port an SF of 95.5% on PKU data and an SF of90.4% on CTB data.
(Tseng et al, 2005) achievean SF of 95.0%, 95.3% and 86.3% on PKU datafrom the Sighan Bakeoff 2005, PKU data fromthe Sighan Bakeoff 2003 and CTB data from theSighan Bakeoff 2003 respectively.
Finally, (Zhanget al, 2006) report an SF of 94.8% on PKU data.For systems that perform both word segmenta-tion and POS tagging, the following results werepublished: Florian and Ngai (2001) report an SFof 93.55% and a TA of 88.86% on CTB data.Ng and Low (2004) report an SF of 95.2% anda TA of 91.9% on CTB data.
Finally, Zhang andClark (2008) achieve an SF of 95.90% and a TFof 91.34% by 10-fold cross validation using CTBdata.Last but not least, there are parsers that oper-ate on characters rather than words and who per-form segmentation and POS tagging as part of theparsing process.
Among these, we would like tomention Luo (2003), who reports an SF 96.0%on Chinese Treebank (CTB) data, and (Fung etal., 2004), who achieve ?a word segmentation pre-cision/recall performance of 93/94%?.
Both theSF and the TF results achieved by our ?PKU onetag + non-deterministic rule set?
setup, whose out-put is slightly ambiguous, compare favorably withall the results mentioned, and even the resultsachieved by our ?PKU one tag + deterministic ruleset?
setup are competitive.5 Conclusions and Future WorkThe idea of carrying some ambiguity from oneprocessing step into the next in order not to prunegood solutions is not new.
E.g., Prins and van No-ord (2003) use a probabilistic part-of-speech tag-ger that keeps multiple tags in certain cases fora hand-crafted HPSG-inspired parser for Dutch,270PKU PKU PKU one tag + PKU one tag +one tag all tags det.
rule set non-det.
rule setCharacter tag accuracy (in %) 89.98 92.79 94.69 95.27Avg.
number of tags per char.
1.00 1.39 1.00 1.03Avg.
number of words per sent.
26.26 26.26 25.77 25.75Segmented word precision (in %) 93.00 93.00 96.18 96.46Segmented word recall (in %) 95.39 95.39 96.84 97.02Segmented word F-score (in %) 94.18 94.18 96.51 96.74Tagged word precision (in %) 83.57 87.87 91.27 92.17Tagged word recall (in %) 85.72 90.23 91.89 92.71Tagged word F-score (in %) 84.63 89.03 91.58 92.44Segmented sentence accuracy (in %) 47.15 47.15 63.14 65.06Avg.
nmb.
of words per correctly segm.
sent.
18.22 18.22 21.69 21.94Tagged sentence accuracy (in %) 14.07 21.09 27.21 31.47Avg.
number of analyses per sent.
1.00 4.61e18 1.00 12.84Median nmb.
of analyses per sent.
1 1,440 1 2Avg.
nmb.
of words per corr.
tagged sent.
9.58 13.20 15.11 16.33Table 1: Evaluation figures achieved by four different systems on the 5,054 sentences of our test setand Curran et al (2006) show the benefits of us-ing a multi-tagger rather than a single-tagger foran induced CCG for English.
However, to ourknowledge, this idea has not made its way intothe field of Chinese parsing so far.
Chinese pars-ing systems either pass on a single segmentationand POS tagging analysis to the parser proper orthey are character-based, i.e.
segmentation andtagging are part of the parsing process.
Althoughseveral treebank-induced character-based parsersfor Chinese have achieved promising results, thisapproach is impractical in the development of ahand-crafted deep grammar like the Chinese LFG.We therefore believe that the development of a?multi-tokenizer-tagger?
is the way to go for thissort of system (and all systems that can handle acertain amount of ambiguity that may or may notbe resolved at later processing stages).
Our resultsshow that we have made an important first step inthis direction.As to future work, we hope to resolve the prob-lem of not having a gold standard that is seg-mented and tagged exactly according to the guide-lines established by the Chinese LFG developerby semi-automatically applying the hand-craftedtransformational rules that were developed to thePKU gold standard.
We will then induce obliga-tory and optional FST rules from this ?grammar-compliant?
gold standard and hope that these willbe able to replace the hand-crafted transformationrules currently used in the grammar.
Finally, weplan to carry out more training runs; in particu-lar, we intend to experiment with lower accuracy(and score) thresholds for optional rules.
The ideais to find the optimal balance between ambigu-ity, which can probably be higher than with ourcurrent set of induced rules without affecting ef-ficiency too adversely, and accuracy, which stillneeds further improvement, as can easily be seenfrom the sentence accuracy figures.ReferencesKenneth R. Beesley and Lauri Karttunen.
2003.
Fi-nite State Morphology.
CSLI Publications, Stan-ford, CA.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.J.M Chang, D.L.
Hung, and O.J.L.
Tzeng.
1992.
Mis-cue analysis of chinese children?s reading behaviorat the entry level.
Journal of Chinese Linguistics,20(1).Dick Crouch, Mary Dalrymple, Ron Kaplan,Tracy Holloway King, John Maxwell, andPaula Newman.
2006.
XLE documentation.http://www2.parc.com/isl/groups/nltt/xle/doc/.James R. Curran, Stephen Clark, and David Vadas.2006.
Multi-Tagging for Lexicalized-GrammarParsing.
In In Proceedings of COLING/ACL-06,pages 697?704, Sydney, Australia.Ji Fang and Tracy Holloway King.
2007.
An lfg chi-nese grammar for machine use.
In Tracy Holloway271King and Emily M. Bender, editors, Proceedings ofthe GEAF 2007 Workshop.
CSLI Studies in Compu-tational Linguistics ONLINE.Radu Florian and Grace Ngai.
2001.
Multidimen-sional transformation-based learning.
In CoNLL?01: Proceedings of the 2001 workshop on Com-putational Natural Language Learning, pages 1?8,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-feng Chen.
2004.
A maximum-entropy Chineseparser augmented by transformation-based learning.ACM Transactions on Asian Language InformationProcessing (TALIP), 3(2):159?168.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive Chinese word segmentation.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Associ-ation for Computational Linguistics, page 462, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Julia Hockenmaier and Chris Brew.
1998.
Error-Driven Segmentation of Chinese.
InternationalJournal of the Chinese and Oriental Languages In-formation Processing Society, 8(1):69??84.R.
Hoosain.
1992.
Psychological reality of theword in chinese.
In H.-C. Chen and O.J.L.
Tzeng,editors, Language Processing in Chinese.
North-Holland and Elsevier, Amsterdam.Kylie Hsu.
2002.
Selected Issues in Mandarin ChineseWord Structure Analysis.
The Edwin Mellen Press,Lewiston, New York, USA.Torbjo?rn Lager.
1999.
The ?-TBL System: Logic Pro-gramming Tools for Transformation-Based Learn-ing.
In Proceedings of the Third International Work-shop on Computational Natural Language Learning(CoNLL?99), Bergen.Xiaoqiang Luo.
2003.
A Maximum Entropy ChineseCharacter-Based Parser.
In Michael Collins andMark Steedman, editors, Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, pages 192?199.John Maxwell and Ron Kaplan.
1996.
An efficientparser for LFG.
In Proceedings of the First LFGConference.
CSLI Publications.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?Word-Based or Character-Based?
.
In DekangLin and Dekai Wu, editors, Proceedings of EMNLP2004, pages 277?284, Barcelona, Spain, July.
Asso-ciation for Computational Linguistics.Jerome L. Packard.
2000.
The Morphology of Chinese.Cambridge University Press, Cambridge, UK.David D. Palmer.
1997.
A trainable rule-based algo-rithm for word segmentation.
In Proceedings of the35th annual meeting on Association for Computa-tional Linguistics, pages 321?328, Morristown, NJ,USA.
Association for Computational Linguistics.Robbert Prins and Gertjan van Noord.
2003.
Reinforc-ing parser preferences through tagging.
TraitementAutomatique des Langues, 44(3):121?139.Richard Sproat and Thomas Emerson.
2003.
The firstinternational chinese word segmentation bakeoff.
InProceedings of the Second SIGHAN Workshop onChinese Language Processing, pages 133?143.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
AConditional Random Field Word Segmenter forSIGHAN Bakeoff 2005.
In Proceedings of FourthSIGHAN Workshop on Chinese Language Process-ing.A.D.
Wu.
2003.
Customizable segmentation of mor-phologically derived words in chinese.
Interna-tional Journal of Computational Linguistics andChinese Language Processing, 8(1):1?28.Fei Xia.
2000.
The segmentation guidelines for thepenn chinese treebank (3.0).
Technical report, Uni-versity of Pennsylvania.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated Chinese cor-pus.
In Proceedings of the 19th.
International Con-ference on Computational Linguistics.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, pages 207?238.Tongqiang Xu?????.
1997.
On Language?????.
Dongbei Normal University Publishing,Changchun, China.Shiwen Yu????
?, Baobao Chang ????
?,and Weidong Zhan ?????.
2004.
An Intro-duction of Computational Linguistics ?????????.
Shangwu Yinshuguan Press, Beijing,China.Yue Zhang and Stephen Clark.
2008.
Joint Word Seg-mentation and POS Tagging Using a Single Percep-tron.
In Proceedings of ACL-08, Columbus, OH.Ruiqiang Zhang, Genichiro Kikui, and EiichiroSumita.
2006.
Subword-based tagging forconfidence-dependent Chinese word segmentation.In Proceedings of the COLING/ACL on Main con-ference poster sessions, pages 961?968, Morris-town, NJ, USA.
Association for Computational Lin-guistics.272
