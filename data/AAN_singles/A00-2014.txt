The Effectiveness of Corpus-Induced Dependency Grammars forPost-processing Speech*M.  P. Harper ,  C.  M .
Whi te ,  W.  Wang,  M.  T .
Johnson ,  and  R .
A.
He lzermanSchool of Electrical and Computer  Engineer ingPurdue UniversityWest Lafayette,  IN 47907-1285{ harper, robot, wang28, m johnson, helz} @ecn.purdue.
duAbst rac tThis paper investigates the impact of ConstraintDependency Grammars (CDG) on the accuracy ofan integrated speech recognition and CDG pars-ing system.
We compare a conventional CDG withCDGs that are induced from annotated sentencesand template-expanded s ntences.
The grammarsare evaluated on parsing speed, precision/coverage,and improvement of word and sentence accuracy ofthe integrated system.
Sentence-derived CDGs sig-nificantly improve recognition accuracy over the con-ventional CDG but are less general.
Expanding thesentences with templates provides us with a mech-anism for increasing the coverage of the grammarwith only minor reductions in recognition accuracy.1 BackgroundThe question of when and how to integrate languagemodels with speech recognition systems i gaining inimportance as recognition tasks investigated by thespeech community become increasingly more chal-lenging and as speech recognizers are used in hu-man/computer interfaces and dialog systems (Block,1997; Pieraccini and Levin, 1992; Schmid, 1994;Wright et al, 1994; Zue et al, 1991).
Many sys-tems tightly integrate N-gram stochastic languagemodels, with a power limited to a regular grammar,into the recognizer (Jeanrenaud et al, 1995; Ney etal., 1994; Placeway et al, 1993) to build more ac-curate speech recognizers.
However, in order to actbased on the spoken interaction with the user, thespeech signal must be mapped to an internal repre-sentation.
Obtaining a syntactic representation forthe spoken utterance has a high degree of utility formapping to a semantic representation.
Without astructural analysis of an input, it is difficult o guar-antee the correctness ofthe mapping from a sentenceto its interpretation (e.g., mathematical expressionsto internal calculations).
We believe that significantadditional improvement in accuracy can be gainedin specific domains by using a more complex lan-* This research was supported by grants from Intel, PurdueResearch Foundation, and National Science Foundation IRI97-04358, CDA 96-17388, and ~9980054-BCS.guage model that combines syntactic, semantic, anddomain knowledge.A language processing module that is more pow-erful than a regular grammar can be loosely, mod-erately, or tightly integrated with the spoken lan-guage system, and there are advantages and dis-advantages associated with each choice (Harper etal., 1994).
To tightly integrate a language modelwith the power of a context-free grammar with theacoustic module requires that the power of the twomodules be matched, making the integrated systemfairly intractable and difficult to train.
By separat-ing the language model from the acoustic model, itbecomes possible to use a more powerful anguagemodel without increasing computational costs or theamount of acoustic training data required by the rec-ognizer.
Furthermore, a loosely-integrated languagemodel can be developed independently of the speechrecognition component, which is clearly an advan-tage.
Decoupling the acoustic and language mod-els also adds flexibility: a wide variety of languagemodels can be tried with a single acoustic model.Systems that utilize a language model that operatesas a post-processor to a speech recognizer include(Block, 1997; Seneff, 1992; Zue et al, 1991).The goal of this research is to construct and ex-perimentally evaluate a prototype of a spoken lan-guage system that loosely integrates a speech recog-nition component with an NLP component that usessyntactic, semantic, and domain-specific knowledgeto more accurately select the sentence uttered by aspeaker.
First we describe the system we have built.Then we describe the mechanism used to rapidly de-velop a domain-specific grammar that improves ac-curacy of our speech recognizer.2 Our  Sys temWe have developed the prototype spoken languagesystem depicted in Figure 1 that integrates a speechrecognition component based on HMMs with a pow-erful grammar model based on Constraint Depen-dency Grammar (CDG).
The speech recognizer isimplemented as a multiple-mixture triphone HMMwith a simple integrated word co-occurrence gram-102mar (Ent, 1997; Young et al, 1997).
Mel-scale cep-stral coefficients, energy, and each of their their firstand second order differences are used as the under-lying feature vector for each speech frame.
Modeltraining is done using standard Baum-Welch Max-imum Likelihood parameter re-estimation on diag-onal covariance Gaussian Mixture Model (GMM)feature distributions.
The speech recognizer em-ploys a token-passing version of the Viterbi algo-rithm (Young et al, 1989) and pruning settings toproduce a pruned recognition lattice.
This prunedlattice contains the most likely alternative sentencesthat account for the sounds present in an utteranceas well as their probabilities.
Without any loss of in-formation, this lattice is then compressed into a wordgraph (Harper et al, 1999b; Johnson and Harper,1999), which acts as the interface between the rec-ognizer and the CDG parser.
The word graph algo-rithm begins with the recognition lattice and elim-inates identical subgraphs by iteratively combiningword nodes that have exactly the same precedingor following nodes (as well as edge probabilities),pushing excess probability to adjacent nodes when-ever possible.
The resulting word graph representsall possible word-level paths without eliminating oradding any paths or modifying their probabilities.Word graphs increase the bandwidth of useful acous-tic information passed from the HMM to the CDGparser compared to most current speech recognitionsystems.The CDG parser parses the word graph to identifythe best sentence consistent with both the acousticsof the utterance and its own additional knowledge.The loose coupling of the parser with the HMMallows us to construct a more powerful combinedsystem without increasing the amount of trainingdata for the HMM or the computational complex-ity of either of the component modules.
Our NLPcomponent is implemented using a CDG parser(Harper and Helzerman, 1995; Maruyama, 1990a;Maruyama, 1990b) because of its power and flexibil-ity, in particular:?
It supports the use of syntactic, semantic, anddomain-specific knowledge in a uniform frame-work.?
Our CDG parser supports efficient simultaneousparsing of alternative sentence hypotheses in aword graph (Harper and Helzerman, 1995; Helz-erman and Harper, 1996).
* Because CDG is a dependency grammar, it canbetter model free-order languages.
Hence, CDGcan be used in processing a wider variety of humanlanguages than other grammar paradigms.?
It is capable of representing and using context-dependent information unlike traditional gram-mar approaches, thus providing a finer degree ofcontrol over the syntactic analysis of a sentence.?
A CDG can be extracted irectly from sentencesannotated with dependency information (i.e., fea-ture and syntactic relationships).We hypothesize that the accuracy of the combinedHMM/CDG system should benefit from the abilityto create a grammar that covers the domain as pre-cisely as possible and that does not consider sen-tences that would not make sense given the domain.A corpus-based grammar is likely to have this degreeof control.
In the next section we describe how weconstruct a CDG from corpora.Figure 1: Block diagram of the loosely-coupled spo-ken language system.3 Learn ing  CDG Ru lesIn this section, we introduce CDG and then describehow CDG constraints can be learned from sentencesannotated with grammatical information.3.1 In t roduct ion  to  CDGConstraint Dependency Grammar (CDG), firstintroduced by Maruyama (Maruyama, 1990a;Maruyama, 1990b), uses constraints to determinethe grammatical dependencies for a sentence.
Theparsing algorithm is framed as a constraint satis-faction problem: the rules are the constraints andthe solutions are the parses.
A CDG is defined asa five-tuple, (2E, R, L, C, T), where ~ = {a l , .
.
.
,  c%}is a finite set of lexical categories (e.g., determiner),R = {r l , .
.
.
, rp}  is a finite set of uniquely namedroles or role ids (e.g., governor, needl, need2), L ={ l l , .
.
.
, lq}  is a finite set of labels (e.g., subject),C is a constraint formula, and T is a table thatspecifies allowable category-role-label combinations.A sentence s - WlW2W3.
.
.wn has length n andis an element of ~*.
For each word wi E ~ of asentence s, there are up to p different roles (withmost words needing only one or two (Harper et al,1999a)), yielding a maximum of n * p roles for theentire sentence.
A role is a variable that is assigneda role value, an element of the set L ?
(1, 2 , .
.
.
,  n}.Role values are denoted as l-m, where l E L andm E (1, 2 , .
.
.
,  n} is called the modifiee.
Maruyamaoriginally used a modifiee of NIL to indicate that arole value does not require a modifiee, but it is moreparsimonious to indicate that there is no dependentby setting the modifiee to the position of its word.Role values are assigned to roles to record the syn-tactic dependencies between words in the sentence.103The governor ole is assigned role values such thatthe modifiee of the word indicates the position of theword's governor or head (e.g., DET-3, when assignedto the governor ole of a determiner, indicates itsfunction and the position of its head).
Every wordin a sentence has a governor ole.
Need roles areused to ensure the requirements of a word are met.For example, an object is required by a verb thatsubcategorizes for one, unless it has passive voice.The required object is accounted for by requiringthe verb's need role to be assigned a role value witha modifiee that points at the object.
Words canhave more than one need role, depending on the lex-ical category of the word.
The table T indicates theroles that a word with a particular lexical categorymust support.A sentence s is said to be generated  by the gram-mar G if there exists an assignment A that maps arole value to each of the roles for s such that C issatisfied.
There may be more than one assignmentof role values to the roles of a sentence that satisfiesC, in which case there is ambiguity.
C is a first-order predicate calculus formula over all roles thatrequires that an assignment of role values to roles beconsistent with the formula; those role values incon-sistent with C can be eliminated.
A subformula P~of C is a predicate involving =, <, or >, or predi-cates joined by the logical connectives and, or, i f ,or not.
A subformula is a unary constraint if it con-tains only a single variable (by convention, we usezl) and a binary constraint if it contains two vari-ables (by convention zl and z2).
An example ofa unary and binary constraint appears in Figure 2.A CDG has an arity parameter a, which indicatesthe maximum number of variables in the subformu-las of C, and a degree parameter d, which is thenumber of roles in the grammar.
An arity of twosuffices to represent a grammar at least as power-ful as a context-free grammar (Maruyama, 1990a;Maruyama, 1990b).
In (Harper et al, 1999a), wedeveloped a way to write constraints concerning thecategory and feature values of a modifiee of a rolevalue (or role value pair).
These constraints looselycapture binary constraint information in unary con-straints (or beyond binary for binary constraints)and results in more efficient parsing.A u, liwy ?~nst\]llnt requiring that ?
role vmluo IlmlgnlKI to the vernot role of I determinerhave the label D~ lind ?
modlflee pointing to ?
lub4NIqtNl~t wocd*(if (and (= (category x 1) determiner)(= (rid x 1 ) G))(and (= (label x 1 ) DET)(> (rood x 1 ) (pos Xl))) )A binary oonatrllnt requiring that ?
role vlluewith the libel S Illlgned to ?
ne~dl role of oneword pOklt It Imother word whole governorrole I= mml~gnened ?
role veltm with the libel 8UBJand ?
rnodlflee that point?
beck at the flrat word.
(if (and (= (label x I ) S)(= (rid Xl) N1)(= (rnod Xl) (pos x2))(= (rid x2) G))(and (= (label x2) SUBJ)(= (rood x2) (pos xl))))Figure 2: A Unary and binary constraint for CDG.The white box in Figure 3 depicts a parse forthe sentence Clear the screen from the ResourceManagement corpus (Price et al, 1988) (the ARVand ARVP in the gray box will be discussed later),which is a corpus we will use to evaluate our speechprocessing system.
We have constructed a conven-tional CDG with around 1,500 unary and binaryconstraints (i.e., its arity is 2) that were designedto parse the sentences in the corpus.
This CDGcovers a wide variety of grammar constructs (includ-ing conjunctions and wh-movement) and has a fairlyrich semantics.
It uses 16 lexical categories, 4 roles(so its degree is 4), 24 labels, and 13 lexical fea-ture types (subcat,  agr, case, vtype (e.g., progres-sive), mood, gap, inverted, voice, behavior (e.g.,mass), type (e.g., interrogative, relative), semtype,takesdet ,  and conj type) .
The parse in Figure 3 isan assignment of role values to roles that is consis-tent with the unary and binary constraints.
A rolevalue, when assigned to a role, has access to not onlythe label and modifiee of its role value, but also therole name of the role to which it is assigned, informa-tion specific to the word (i.e., the word's position inthe sentence, its lexical category, and feature valuesfor each feature), and information about the lexicalclass and feature values of its modifiee.
Our unaryand binary constraints use this information to elim-inate ungrammatical ssignments.Parse for "Clear the screen"I 1 th2e-~ 3 Clear ~ n?a~=comlnont vtype=lnf =ulxa~t3= I behav=countNmt/~ram-G=root-1 G=de~3N2=S~3 N3=S-1{'~t l=det erm~ner, type1 ==definite, subcat l=count3s, ddl=G ?bell=de (< pOSXl) mod Xl) )J"~tl---determiner, typel=definite, subcat 1--cour~3s, ddl=G,~ ~,bell=det, (< (pos Xl) (rood Xl)), Cat2=noun, c~se2=common, I 1t b e.hav2=count, type2=none, semty~2=display, gl2=3s, J\]\] rid2=G, label2==obi, (< (rood x2) (POs x2)),(rood x2) (pos Xl)), (= (rood xl) (pos x2)) J =Figure 3: A CDG parse (see white box) is repre-sented by the assignment of role values to roles as-sociated with a word with a specific lexical categoryand one feature value per feature.
ARVs and ARVPs(see gray box) represent grammatical relations thatcan be extracted from a sentence's parse.3.2 Learn ing  CDG Const ra in tsThe grammaticality of a sentence in a language de-fined by a CDG was originally determined by apply-ing the constraints of the grammar to the possible104role value assignments.
If the set of all possible rolevalues assigned to the roles of a sentence of length nis denotedS1 =Y;.x RxPOSxLxMODx Ftx.. .
x Fk, where k is the number of feature types,Fi represents the set of feature values for that type,POS = {1, 2 , .
.
.
,  n} is the set of possible positions,MOD = {1, 2 , .
.
.
,  n} is the set of possible modi-flees, and n is sentence length (which can be anyarbitrary natural number), then unary constraintspartition $1 into grammatical and ungrammaticalrole values.
Similarly, binary constraints partitionthe set $2 = $1 x $1 = S~ into compatible and in-compatible pairs.
Building upon this concept of rolevalue partitioning, it is possible to construct anotherway of representing unary and binary constraintsbecause CDG constraints do not need to referencethe exact position of a word or a modifiee in thesentence to parse sentences (Harper and Helzerman,1995; Maruyama, 1990a; Maruyama, 1990b; Menzel,1994; Menzel, 1995).To represent he relative, rather than the abso-lute, position information for the role values in agrammatical sentence, it is only necessary to repre-sent the positional relations between the modifieesand the positions of the role values.
To support anarity of 2, these relations involve either equality orless-than relations over the modifiees and positionsof role values assigned to the roles zl and x2.
Sinceunary constraints operate over role values assignedto a single role, the only relative position relationsthat can be tested are between the role value's posi-tion (denoted as Pzl)  and its modifiee (denoted asMzl); one and only one of the following three re-lations must be true: (P~:I < Mzl), (Mzl < Pzl) ,or (Pzl = Mzl).
Since binary constraints operateover role values assigned to pairs of roles, zl and z2,the only possible relative position relations that canbe tested are between Pzl  and Mxt, P:e2 and Mx2,Pz l  and Mz~, Pz2 and Mxt, Pzt  and Px2, Mxl andMz2.
Note that each of the six has three positionalrelations (as in the case of unary relations on Pzland Mzt) such that one and only one of them issimultaneously true.The unary and binary positional relations providethe necessary mechanism to develop an alternativeview of the unary and binary constraints.
First, wedevelop the concept of an abstract role value (ARV),which is a finite characterization f all possible rolevalues using relative, rather than absolute, positionrelations.
Formally, an ARV for a particular gram-mar G = (~,, R, L, C, T, F t , .
.
.
,  Fk) is an element ofthe set: .dl = ExR?
L xFt  ?.
.
.xFkxUC,  where UCencodes the three possible positional relations be-tween Pxl and Mxl.
The gray box of Figure 3 showsan example of an ARV obtained from the parsed sen-tence.
Note that .At is a finite set representing thespace of all possible ARVs for the grammar1; hence,the set provides an alternative characterization ofthe unary constraints for the grammar, which canbe partitioned into positive (grammatical) and neg-ative (ungrammatical) ARVs.
During parsing, if arole value does not match one of the elements in thepositive ARV space, then it should be disallowed.Positive ARVs can be obtained directly from theparses of sentences: for each role value in a parse fora sentence, simply extract its category, feature, role,and label information, and then determine the po-sitional relation that holds between the role value'sposition and modifiee.Similarly the set of legal abstract role value pairs(ARVPs), A2 = \ ]ExRxLxFtx .
.
.xFkx~xRxLxF1 x .
.
.
x Fk x BC, where BC encodes the positionalrelations among Pxl,  Mxt, Px2, and Mx2, providesan alternative definition for the binary constraints 2.The gray box of Figure 3 shows an example of anARVP obtained from the parsed sentence.
PositiveARVPs can be obtained directly from the parses ofsentences.
For each pair of role values assigned todifferent roles, simply extract heir category, feature,role, and label information, and then determine thepositional relations that hold between the positionsand modifiees.An enumeration of the positive ARV/ARVPs canbe used to represent he CDG constraints, C, andARV/ARVPs are PAC-learnable from positive ex-amples, as can be shown using the techniques of(Natarajan, 1989; Valiant, 1984).
ARV/ARVP con-straints can be enforced by using a fast table lookupmethod to see if a role value (or role value pair) isallowed (rather than propagating thousands of con-straints), thus speeding up the parser.4 Eva luat ion  Us ing  the  Nava lResource  Management  DomainAn experiment was conducted to determine theplausibility and the benefits of extracting CDG con-straints from a domain-specific corpus of sentences.For our speech application, the ideal CDG should begeneral enough to cover sentences imilar to thosethat appear in the corpus while being restrictiveenough to eliminate sentences that are implausiblegiven the observed sentences.
Hence, we investigatewhether a grammar extracted from annotated sen-tences in a corpus achieves this precision of cover-age.
We also examine whether a learned grammarhas the ability to filter out incorrect sentence hy-potheses produced by the HMM component of oursystem in Figure 1.
To investigate these issues, wehave performed an experiment using the standard1,fit 1 can also include information about  the possible lexicalcategories and feature values of the modifiee of Xl.2.A2 can also include information about  the possible lexicalcategories and feature values of the modifiees of Xl and x2.105Resource Management (RM) (Price et al, 1988) andExtended Resource Management (RM2) ((DARPA),1990) corpora.
These mid-size speech corpora havea vocabulary of 991 words and contain utterances ofsentences derived from sentence templates based oninterviews with naval personnel familiar with navalresource management tasks.
They were chosen forseveral reasons: they are two existing speech corporafrom the same domain; their manageable sizes makethem a good platform for the development of tech-niques that require extensive xperimentation; andthe sentences have both syntactic variety and rea-sonably rich semantics.
RM contains 5,190 separateutterances (3,990 testing, 1,200 training) of 2,845distinct sentences (2,245 training, 600 testing).
Wehave extracted several types of CDGs from annota-tions of the RM sentences and tested their generalityusing the 7,396 sentences in RM2 (out of the 8,173)that are in the resource management domain but aredistinct from the RM sentences.
We compare theseCDGs to each other and to the conventional CDGdescribed previously.The corpus-based CDGs were created by extract-ing the allowable grammar elationships from theRM sentences that were annotated by language x-perts using the SENATOR annotation tool, a CGI(Common Gateway Interace) HTML script writtenin GNU C++ version 2.8.1 (White, 2000).
Wetested two major CDG variations: those derived di-rectly from the RM sentences (Sentence CDGs) andthose derived from simple template-expanded RMsentences (Template CDGs).
For example, "ListMIDPAC's deployments during (date)" is a sentencecontaining a date template which allows any daterepresentations.
For these experiments, we focusedon templates for dates, years, times, numbers, andlatitude and longitude coordinates.
Each templatename identifies a sub-grammar which was producedby annotating the appropriate strings.
We then an-notated sentences containing the template names asif they were regular sentences.
Approximately 25%of the 2,845 RM sentences were expanded with oneor more templates.Although annotating a corpus of sentences can bea labor intensive task, we used an iterative approachthat is based on parsing using grammars with vary-ing degrees of restrictiveness.
A grammar can bemade less restrictive by ignoring:* lexical information associated with a role value'smodifiee in the ARVPs,o feature information of two role values in an ARVPnot directly related based on their modifiee rela-tions,.
syntactic information provided by two role valuesthat are not directly related,?
specific feature information (e.g., semantics orsubcategorization).Initially, we bootstrapped the grammar by annotat-ing a 200 sentence subset of the RM corpus and ex-tracting a fairly general grammar from the annota-tions.
Then using increasingly restrictive grammarsat each iteration, we used the current grammar toidentify sentences that required annotation and ver-ified the parse information for sentences that suc-ceeded.
This iterative technique reduced the timerequired to build a CDG from about one year for theconventional CDG to around two months (White,2000).Several methods of extracting an ARV/ARVPgrammar from sentences or template-extended sen-tences were investigated.
The ARVPs are extracteddifferently for each method; whereas, the ARVsare extracted in the same manner egardless of themethod.
Recall that ARVs represent the set of ob-served role value assignments.
In our implementa-tion, each ARV includes: the label of the role value,the role to which the role value was assigned, thelexical category and feature values of the word con-taining the role, the relative position of the word andthe role value's modifiee, and the modifiee's lexicalcategory and feature values (modifiee constraints).We use modifiee constraints for ARVs regardless ofextraction method because their use does not changethe coverage of the extracted grammar and not usingthe information would significantly slow the parser(Harper et al, 1999a).
Because the ARVP space islarger than the ARV space, we investigate six varia-tions for extracting the pairs:1.
Ful l  Mod:  contains all grammar and featurevalue information for all pairs of role values fromannotated sentences, as well as modifiee con-straints.
For a role value pair in a sentence to beconsidered valid during parsing with this gram-mar, it must match an ARVP extracted from theannotated sentences.2.
Full: like Ful l  Mod except it does not imposemodifiee constraints on a pair of role values duringparsing.3.
Feature  Mod:  contains all grammar elationsbetween all pairs of role values, but it consid-ers feature and modifiee constraints only for pairsthat are directly related by a modifiee link.
Dur-ing parsing, if a role value pair is related by amodifiee link, then a corresponding ARVP withfull feature and modifiee information must appearin the grammar for it to be allowed.
If the pairis not directly related, then an ARVP must bestored for the grammar elations, ignoring featureand modifiee constraint information.4.
Feature :  like Feature  Mod except it does notimpose modifiee constraints on a pair of role val-ues during parsing.5.
D i rect  Mod:  stores only the grammar, feature,and modifiee information for those pairs of role106Table 1: Number of ARVs and ARVPs extracted foreach RM grammar.ARVP Sentence I Template PercentVariation CDG \[ CDG IncreaseFull Mod 270,034 408,912Full 165,480 200,792Feature ModFeatureDirect ModDirectARVs49,46836,55841,12428,2144,42456,75840,30847,00430,5544,64851.43%21.34%14.74%10.26%14.30%8.29%5.06%Table 2: Number of successfully parsed sentences inRM2 using the conventional CDG and CDGs derivedfrom sentences only or template-expanded s ntences.ARVP ~: Parsed with ~ Parsed withVariation Sentence CDG Template CDGFull ModFullFeature ModFeatureDirect ModDirectConventional3,735 (50.50%)4,509 (60.97%)5,365 (72.54%)5,772 (78.04%)5,464 (73.88%)5,931 (80.19%)7,144 (96.59%)4,461 (60.32%)5,316 (71.88%)5,927 (80.14%)6,208 (83.94%)5,979 (80.84%)6,275 (84.82%)not applicablevalues that are directly related by a modifiee link.During parsing, if a role value pair is related bysuch a link, then a corresponding ARVP must ap-pear in the grammar for it to be allowed.
Anypair of role values not related by a modifiee linkis allowed (an open-world assumption).6.
Di rect :  like D i rect  Mod except it does not im-pose modifiee constraints on a pair of role valuesduring parsing.Grammar sizes for these six grammars, extractedeither directly from the 2,845 sentences or from the2,845 sentences expanded with our sub-grammartemplates, appear in Table 1.
The largest gram-mars were derived using the Full Mod extrac-tion method, with a fairly dramatic growth result-ing from processing template-expanded sentences.The Feature  and D i rect  variations are more man-ageable in size, even those derived from template-expanded sentences.Size is not the only important consideration fora grammar.
Other important issues are grammargenerality and the impact of the grammar on theaccuracy of selecting the correct sentence from therecognition lattice of a spoken utterance.
Afterextracting the CDG grammars from the RM sen-tences and template-expanded sentences, we testedthe generality of the extracted grammars by usingeach grammar to parse the 7,396 RM2 sentences.See the results in Table 2.
The grammar with thegreatest generality was the conventional CDG forthe RM corpus; however, this grammar also hasthe unfortunate attribute of being quite ambigu-ous.
The most generalizable of extracted grammarsuses the D i rect  method on template-expanded s n-tences.
In all cases, the template-expanded sen-tence grammars gave better coverage than their cor-responding sentence-only grammars.We have also used the extracted grammars topost-process word graphs created by the word graphcompression algorithm of (Johnson and Harper,1999) for the test utterances in the RM corpus.
Aswas reported in (Johnson and Harper, 1999), theword-error rate of our HMM recognizer with an em-bedded word pair language model on the RM test setof 1200 utterances was 5.0%, the 1-best sentence ac-curacy was 72.1%, and the word graph coverage ac-curacy was 95.1%.
Also, the average uncompressedword graph size was 75.15 nodes, and our compres-sion algorithm resulted in a average word graph sizeof 28.62 word nodes.
When parsing the word graph,the probability associated with a word node can ei-ther represent its acoustic score or a combinationof its acoustic and stochastic grammar score.
Weuse the acoustic score because (Johnson and Harper,1999) showed that by using a word node's acousticscore alone when extracting the top sentence candi-date after parsing gave a 4% higher sentence accu-racy.For the parsing experiments, we processed the1,080 word graphs produced for the RM test setthat contained 50 or fewer word nodes after com-pression (out of 1,200 total) in order to efficientlycompare the 12 ARV/ARVP CDG grammars andthe conventional CDG (the larger word graphs re-quire significant ime and space to parse using theconventional CDG).
These 1,080 word graphs con-tain 24.95 word nodes on average with a standarddeviation (SD) of 10.80, and result in 1-best sen-tence accuracy was 75% before parsing.
The num-ber of role values prior to binary constraint propa-gation differ across the grammars with an average(and SD) for the conventional grammar of 504.99(442.00), for the sentence-only grammars of 133.37(119.48), and for the template-expanded grammarsof 157.87 (145.16).
Table 3 shows the word graphparsing speed and the path, node, and role value(RV) ambiguity after parsing; Table 4 shows thesentence accuracy and the accuracy and percent cor-rect for words.
Note that percent correct words iscalculated using N-D-S  and word accuracy using NN-D-S- I  where N is the number of words, D is Nthe number of deletions, S is the number of substi-tutions, and I is the number of insertions.The most selective RM sentence grammar, FullMod,  achieves the highest sentence accuracy, butat a cost of a greater average parsing time thanthe other RM sentence grammars.
Higher accu-107ARVP Variation Parse Time (sec.
)Full Mod 33.89 (41.12)Template Full Mod 41.85 (51.75)Full 29.73 (36.68)Template Full 36.80 (46.90)Feature Mod 11.46 (14.46)Template Feature Mod 13.80 (18.47)Feature 11.60 (14.97)Template Feature 14.24 (19.63)Direct Mod 13.93 (19.73)Template Direct Mod 17.28 (26.56)Direct 19.95 (36.89)Template Direct 28.02 (69.50)Coventional 83.48 (167.51)No.
Paths2.21 (1.74)2.78 (3.75)2.83 (2.92)3.40 (5.19)3.9 (5.97)4.22 (6.93)5.19 (8.36)6.86 (14.83)No.
Nodes10.59 (3.44)10.76 (3.64)10.87 (3.54)11.03 (3.74)11.20 (3.94)11.28 (4.06)11.72 (4.22)11.94 (4.52)4.25 (6.49) 11.46 (4.27)4.62 (8.61) 11.45 (4.28)808 (18.52) 12.81 (5.73)9.98 (25.52) 12.95 (5.95)51.33 (132.43) 17.14 (8.02)No.
RVs19.51 (8.32)19.93 (8.76)20.32 (8.86)20.77 (9.47)21.43 (10.49)21.81 (11.17)23.41 (12.72)24.47 (14.41)22.79 (13.44)22.95 (13.34)32.85 (34.65)33.36 (35.66)77.19 (76.26)Table 3: Average parse times (SD), number of paths (SD), number of nodes (SD), and number of role values(SD) remaining after parsing the 1,080 word graphs of 50 or fewer word nodes produced for the RM test setusing the 13 CDGs.ARVP Variation Sentence Accuracy ~o Correct Words Word AccuracyFull ModTemplate Full ModFullTemplate FullFeature ModTemplate Feature ModFeatureTemplate FeatureDirect ModTemplate Direct ModDirectTemplate DirectConventional91.94%91.57%91.57%91.20%90.56%90.19%90.28%89.91%90.46%90.09%89.91%89.44%81.20%98.55%98.50%98.49%98.45%98.38%98.34%98,35%98.29%98.37%98.32%98.30%98.25%97.11%98.19%98.14%98.11%98.05%97.95%97.90%97.91%97.85%97.91%97.86%97.82%97.75%96.10%Table 4: The sentence accuracy, percent correct words, and word accuracy from parsing 1,080 word graphsof 50 or fewer word nodes produced for the RM test set using the 13 CDGs.racy appears to be correlated with the ability of theconstraints to eliminate word nodes from the wordgraph during parsing.
The least restrictive sentencegrammar, D i rect ,  is less accurate than the othersentence grammars and offers an intermediate speedof parsing, most likely due to the increased ambigu-ity in the parsing space.
The fastest grammar wasthe Feature -Mod grammar, which also offers anintermediate l vel of accuracy.
Its size (even withtemplates), restrictiveness, and speed make it veryattractive.
The template versions of each grammarshowed a slight increase in average parse times (fromprocessing a larger number of role values) and aslight decrease in parsing accuracy.
The conven-tional grammar was the least competitive of thegrammars both in speed and in accuracy.5 Conclus ion and Future  D i rect ionsity to improve sentence accuracy of our speech sys-tem.
To achieve balance between precision and cov-erage of our corpus-induced grammars, we have ex-panded the RM sentences with templates for expres-sions like dates and times.
The grammars extractedfrom these expanded sentences gave increased RM2coverage without sacrificing even 1% of the sentenceaccuracy.
We are currently expanding the number oftemplates in our grammar in an attempt o obtainfull coverage of the RM2 corpus using only template-expanded RM sentences.
We have recently addedten semantic templates to the grammar and haveimproved the coverage by 9.19% without losing anysentence accuracy.
We are also developing a stochas-tic version of CDG that uses a statistical ARV, whichis similar to a supertag (Srinivas, 1996).ReferencesThe ability to extract ARV/ARVP grammars withvarying degrees of specificity provides us with theability to rapidly develop a grammar with the abil-H. U.
Block.
1997.
Language components in VERB-MOBIL.
In Proc.
of the Int.
Conf.
of Acoustics,Speech, and Signal Proc., pages 79-82.108Defense Advanced Research Projects Agency(DARPA).
1990.
Extended resource manage-ment: Continuous speech speaker-dependentcorpus (RM2).
CD-ROM.
NIST Speech Discs3-1.2 and 3-2.2.Entropic Cambridge Research Laboratory, Ltd.,1997.
HTK: Hidden Markov Model Toolkit V2.1.M.
P. Harper and R. A. Helzerman.
1995.
Exten-sions to constraint dependency parsing for spokenlanguage processing.
Computer Speech and Lan-guage, 9:187-234.M.
P. Harper, L. H. Jamieson, C. D. Mitchell,G.
Ying, S. Potisuk, P. N. Srinivasan, R. Chen,C.
B. Zoltowski, L. L. McPheters, B. Pellom,and R. A. Helzerman.
1994.
Integrating languagemodels with speech recognition.
In Proc.
of theAAAI Workshop on the Integration of NaturalLanguage and Speech Processing, pages 139-146.M.
P. Harper, S. A. Hockema, and C. M. White.1999a.
Enhanced constraint dependency grammarparsers.
In Proc.
of the IASTED Int.
Conf.
onArtificial Intelligence and Soft Computing.M.
P. Harper, M. T. Johnson, L. H. Jamieson, andC.
M. White.
1999b.
Interfacing a CDG parserwith an HMM word recognizer using word graphs.In Proc.
of the Int.
Conf.
of Acoustics, Speech, andSignal Proc.R.
A. Helzerman and M. P. Harper.
1996.
MUSECSP: An extension to the constraint satisfactionproblem.
Journal of Artificial Intelligence Re-search, 5:239-288.P.
Jeanrenaud, E. Eide, U. Chaudhari, J. Mc-Donough, K. Ng, M. Siu, and H. Gish.
1995.
Re-ducing word error rate on conversational speechfrom the Switchboard corpus.
In Proc.
of theInt.
Conf.
of Acoustics, Speech, and Signal Proc.,pages 53-56.M.
T. Johnson and M. P. Harper.
1999.
Near min-imal weighted word graphs for post-processingspeech.
In 1999 Int.
Workshop on AutomaticSpeech Recognition and Understanding.H.
Maruyama.
1990a.
Constraint DependencyGrammar and its weak generative capacity.
Com-puter Software.H.
Maruyama.
1990b.
Structural disambiguationwith constraint propagation.
In Proc.
of the An-nual Meeting of Association for ComputationalLinguistics, pages 31-38.W.
Menzel.
1994.
Parsing of spoken language un-der time constraints.
In 11th European Conf.
onArtificial Intelligence, pages 560-564.W.
Menzel.
1995.
Robust processing of natural an-guage.
In Proc.
of the 19th Annual German Conf.on Artificial Intelligence.B.
Natarajan.
1989.
On learning sets and functions.Machine Learning, 4(1).H.
Ney, U. Essen, and R. Kneser.
1994.
On struc-turing probabilistic dependences in stochastic lan-guage modelling.
Computer Speech and Language,8:1-38.R.
Pieraccini and E. Levin.
1992.
Stochastic repre-sentation of semantic structure for speech under-standing.
Speech Communication, 11:283-288.P.
Placeway, R. Schwartz, P. Fung, and L. Nguyen.1993.
The estimation of powerful anguage mod-els from small and large corpora.
In Proc.
of theInt.
Conf.
of Acoustics, Speech, and Signal Proc.,pages 33-36.P.
J.
Price, W. Fischer, J. Bernstein, and D. Pallett.1988.
A database for continuous peech recog-nition in a 1000-word omain.
In Proc.
of theInt.
Conf.
of Acoustics, Speech, and Signal Proc.,pages 651-654.L.
A. Schmid.
1994.
Parsing word graphs using a lin-guistic grammar and a statistical language model.In Proc.
of the Int.
Conf.
of Acoustics, Speech,and Signal Proc., pages 41-44.S.
Seneff.
1992.
TINA: A natural language systemfor spoken language applications.
American Jour-nal of Computational Linguistics, 18:61-86.B.
Srinivas.
1996.
'Almost parsing' technique forlanguage modeling.
In Proc.
of the Int.
Conf.
onSpoken Language Processing, pages 1173-1176.L.
G. Valiant.
1984.
A theory of the learnable.
Com-munications of the ACM, 27(11):1134-1142.C.
M. White.
2000.
Rapid Grammar Developmentand Parsing Using Constraint Dependency Gram-mars with Abstract Role Values.
Ph.D. thesis,Purdue University, School of Electrical and Com-puter Engineering, West Lafayette, IN.J.
H. Wright, G. J. F. Jones, and H. Lloyd-Thomas.1994.
Robust language model incorporating asubstring parser and extended N-grams.
In Proc.of the Int.
Conf.
of Acoustics, Speech, and SignalProc., pages 361-364.S.
J.
Young, N. H. Russell, and J. H. S. Thornton.1989.
Token passing : a simple conceptual modelfor connected speech recognition systems.
Tech-nical Report TR38, Cambridge University, Cam-bridge, England.S.
J.
Young, J. Odell, D. Ollason, V. Valtchev, andP.
Woodland, 1997.
The HTK Book.
EntropicCambridge Research Laboratory Ltd., 2.1 edition.V.
Zue, J.
Glass, D. Goodine, H. Leung, M. Phillips,J.
Polifroni, and S. Seneff.
1991.
Integration ofspeech recognition and natural anguage process-ing in the MIT Voyager system.
In Proe.
of theInt.
Conf.
of Acoustics, Speech, and Signal Proe.,pages 713-716.109
