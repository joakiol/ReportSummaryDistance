Supervised Grammar Induction using Training Data with LimitedConstituent Information *Rebecca  HwaDivision of Engineering and Applied SciencesHarvard UniversityCambridge, MA 02138 USArebecca@eecs.harvard.eduAbst ractCorpus-based grammar induction generally re-lies on hand-parsed training data to learn thestructure of the language.
Unfortunately, thecost of building large annotated corpora is pro-hibitively expensive.
This work aims to improvethe induction strategy when there are few labelsin the training data.
We show that the most in-formative linguistic constituents are the highernodes in the parse trees, typically denoting com-plex noun phrases and sentential clauses.
Theyaccount for only 20% of all constituents.
For in-ducing grammars from sparsely labeled trainingdata (e.g., only higher-level constituent labels),we propose an adaptation strategy, which pro-duces grammars that parse almost as well asgrammars induced from fully labeled corpora.Our results suggest hat for a partial parser toreplace human annotators, it must be able toautomatically extract higher-level constituentsrather than base noun phrases.1 In t roduct ionThe availability of large hand-parsed corporasuch as the Penn Treebank Project has madehigh-quality statistical parsers possible.
How-ever, the parsers risk becoming too tailored tothese labeled training data that they cannot re-liably process entences from an arbitrary do-main.
Thus, while a parser trained on the?
Wall Street Journal corpus can fairly accuratelyparse a new Wall Street Journal article, it maynot perform as well on a New Yorker article.To parse sentences from a new domain, onewould normally directly induce a new grammar* This material is based upon work supported by the Na-tional Science Foundation under Grant No.
IRI 9712068.We thank Stuart Shieber for his guidance, and LillianLee, Ric Crabbe, and the three anonymous reviewers fortheir comments on the paper.from that domain, in which the training pro-cess would require hand-parsed sentences fromthe new domain.
Because parsing a large cor-pus by hand is a labor-intensive task, it wouldbe beneficial to minimize the number of labelsneeded to induce the new grammar.We propose to adapt a grammar alreadytrained on an old domain to the new domain.Adaptation can exploit the structural similar-ity between the two domains o that fewer la-beled data might be needed to update the gram-mar to reflect he structure of the new domain.This paper presents a quantitative study com-paring direct induction and adaptation underdifferent training conditions.
Our goal is to un-derstand the effect of the amounts and typesof labeled data on the training process for bothinduction strategies.
For example, how muchtraining data need to be hand-labeled?
Mustthe parse trees for each sentence be fully spec-ified?
Are some linguistic constituents in theparse more informative than others?To answer these questions, we have performedexperiments that compare the parsing quali-ties of grammars induced under different rain-ing conditions using both adaptation and di-rect induction.
We vary the number of labeledbrackets and the linguistic lasses of the labeledbrackets.
The study is conducted on both a sim-ple Air Travel Information System (ATIS) cor-pus (Hemphill et al, 1990) and the more com-plex Wall Street Journal (WSJ) corpus (Marcuset al, 1993).Our results show that the training examplesdo not need to be fully parsed for either strat-egy, but adaptation produces better grammarsthan direct induction under the conditions ofminimally labeled training data.
For instance,the most informative brackets, which label con-stituents higher up in the parse trees, typically73identifying complex noun phrases and senten-tial clauses, account for only 17% of all con-stituents in ATIS and 21% in WSJ.
Trained onthis type of label, the adapted grammars parsebetter than the directly induced grammars andalmost as well as those trained on fully labeleddata.
Training on ATIS sentences labeled withhigher-level constituent brackets, a directly in-duced grammar parses test sentences with 66%accuracy, whereas an adapted grammar parseswith 91% accuracy, which is only 2% lower thanthe score of a grammar induced from fully la-beled training data.
Training on WSJ sentenceslabeled with higher-level constituent brackets,a directly induced grammar parses with 70%accuracy, whereas an adapted grammar parseswith 72% accuracy, which is 6% lower than thescore of a grammar induced from fully labeledtraining data.That the most informative brackets arehigher-level constituents and make up only one-fifth of all the labels in the corpus has two impli-cations.
First, it shows that there is potentialreduction of labor for the human annotators.Although the annotator still must process anentire sentence mentally, the task of identifyinghigher-level structures such as sentential c ausesand complex nouns should be less tedious thanto fully specify the complete parse tree for eachsentence.
Second, one might speculate the pos-sibilities of replacing human supervision alto-gether with a partial parser that locates con-stituent chunks within a sentence.
However,as our results indicate that the most informa-tive constituents are higher-level phrases, theparser would have to identify sentential clausesand complex noun phrases rather than low-levelbase noun phrases.2 Re la ted  Work  on  GrammarInduct ion?
Grammar induction is the process of inferringthe structure of a language by learning from ex-ample sentences drawn from the language.
Thedegree of difficulty in this task depends on threefactors.
First, it depends on the amount ofsupervision provided.
Charniak (1996), for in-stance, has shown that a grammar can be easilyconstructed when the examples are fully labeledparse trees.
On the other hand, if the examplesconsist of raw sentences with no extra struc-tural information, grammar induction is verydifficult, even theoretically impossible (Gold,1967).
One could take a greedy approach suchas the well-known Inside-Outside r -estimationalgorithm (Baker, 1979), which induces locallyoptimal grammars by iteratively improving theparameters of the grammar so that the entropyof the training data is minimized.
In practice,however, when trained on unmarked ata, thealgorithm tends to converge on poor grammarmodels.
For even a moderately complex domainsuch as the ATIS corpus, a grammar trainedon data with constituent bracketing informationproduces much better parses than one trainedon completely unmarked raw data (Pereira andSchabes, 1992).
Part of our work explores thein-between case, when only some constituent la-bels are available.
Section 3 defines the differenttypes of annotation we examine.Second, as supervision decreases, the learningprocess relies more on search.
The success ofthe induction depends on the initial parametersof the grammar because a local search strategymay converge to a local minimum.
For findinga good initial parameter set, Lari and Young(1990) suggested first estimating the probabili-ties with a set of regular grammar rules.
Theirexperiments, however, indicated that the mainbenefit from this type of pretraining is oneof run-time efficiency; the improvement in thequality of the induced grammar was minimal.Briscoe and Waegner (1992) argued that oneshould first hand-design the grammar to en-code some linguistic notions and then use the re-estimation procedure to fine-tune the parame-ters, substituting the cost of hand-labeled train-ing data with that of hand-coded grammar.
Ouridea of grammar adaptation can be seen as aform of initialization.
It attempts to seed thegrammar in a favorable search space by firsttraining it with data from an existing corpus.Section 4 discusses the induction strategies inmore detail.A third factor that affects the learning pro-cess is the complexity of the data.
In their studyof parsing the WSJ, Schabes et al (1993) haveshown that a grammar trained on the Inside-Outside re-estimation algorithm can performquite well on short simple sentences but faltersas the sentence length increases.
To take thisfactor into account, we perform our experiments74Categories Labeled SentenceHighPBaseNPBasePAllNP(I want (to take (the flight with at most one stop)))(I) want to take (the flight) with (at most one stop)(I) want to take (the flight) with (at most one) stop(I) want to take ((the flight) with (at most one stop))NotBaseP (I (want (to (take (the flight (with (at most one stop)))))))I AT IS  I WSJ17% 21%27% 29%32% 30%37% 43%68% 70%Table 1: The second column shows how the example sentence ((I) (want (to (take ((the flight)(with ((at most one) stop))))))) is labeled under each category.
The third and fourth columns listthe percentage break-down of brackets in each category for ATIS and WSJ respectively.on both a simple domain (ATIS) and a complexone (WSJ).
In Section 5, we describe the exper-iments and report the results.3 Training Data Annotat ionThe training sets are annotated in multipleways, falling into two categories.
First, we con-struct training sets annotated with random sub-sets of constituents consisting 0%, 25~0, 50%,75% and 100% of the brackets in the fully an-notated corpus.
Second, we construct sets train-ing in which only a certain type of constituent isannotated.
We study five linguistic categories.Table 1 summarizes the annotation differencesbetween the five classes and lists the percent-age of brackets in each class with respect tothe total number of constituents 1 for ATIS andWSJ.
In an AI1NP training set, all and onlythe noun phrases in the sentences are labeled.For the BaseNP class, we label only simplenoun phrases that contain no embedded nounphrases.
Similarly for a BaseP  set, all sim-ple phrases made up of only lexical items arelabeled.
Although there is a high intersectionbetween the set of BaseP labels and the set ofBaseNP labels, the two classes are not identical.A BaseNP may contain a BaseP.
For the exam-ple in Table 1, the phrase "at most one stop"is a BaseNP that contains a quantifier BaseP"at most one."
NotBaseP  is the complemento f  BaseP.
The majority of the constituents ina sentence belongs to this category, in which atleast one of the constituent's sub-constituents isnot a simple lexical item.
Finally, in a H ighPset, we label only complex phrases that decom-1 For computing the percentage ofbrackets, the outer-most bracket around the entire sentence and the brack-ets around singleton phrases (e.g., the pronoun "r' as aBaseNP) are excluded because they do not contribute tothe pruning of parses.pose into sub-phrases that may be either an-other HighP or a BaseP.
That is, a HighP con-stituent does not directly subsume any lexicalword.
A typical HighP is a sentential clause or acomplex noun phrase.
The example sentence inTable 1 contains 3 HighP constituents: a com-plex noun phrase made up of a BaseNP and aprepositional phrase; a sentential clause with anomitted subject NP; and the full sentence.4 Induct ion  S t ra teg iesTo induce a grammar from the sparsely brack-eted training data previously described, we usea variant of the Inside-Outside re-estimationalgorithm proposed by Pereira and Schabes(1992).
The inferred grammars are repre-sented in the Probabilistic Lexicalized Tree In-sertion Grammar (PLTIG) formalism (Schabesand Waters, 1993; Hwa, 1998a), which is lexical-ized and context-free quivalent.
We favor thePLTIG representation for two reasons.
First, itis amenable to the Inside-Outside r -estimationalgorithm (the equations calculating the insideand outside probabilities for PLTIGs can befound in Hwa (1998b)).
Second, its lexicalizedrepresentation makes the training process moreefficient than a traditional PCFG while main-taining comparable parsing qualities.Two training strategies are considered: di-rect induction, in which a grammar is inducedfrom scratch, learning from only the sparsely la-beled training data; and adaptation, a two-stagelearning process that first uses direct inductionto train the grammar on an existing fully la-beled corpus before retraining it on the new cor-pus.
During the retraining phase, the probabil-ities of the grammars are re-estimated based onthe new training data.
We expect he adaptivemethod to induce better grammars than directinduction when the new corpus is only partially75annotated because the adapted grammars havecollected better statistics from the fully labeleddata of another corpus.5 Exper iments  and  Resu l tsWe perform two experiments.
The first usesATIS as the corpus from which the differenttypes of partially labeled training sets are gener-ated.
Both induction strategies train from thesedata, but the adaptive strategy pretrains itsgrammars with fully labeled data drawn fromthe WSJ corpus.
The trained grammars arescored on their parsing abilities on unseen ATIStest sets.
We use the non-crossing bracket mea-surement as the parsing metric.
This experi-ment will show whether annotations ofa partic-ular linguistic category may be more useful fortraining grammars than others.
It will also in-dicate the comparative merits of the two induc-tion strategies trained on data annotated withthese linguistic categories.
However, pretrain-ing on the much more complex WSJ corpus maybe too much of an advantage for the adaptivestrategy.
Therefore, we reverse the roles of thecorpus in the second experiment.
The partiallylabeled data are from the WSJ corpus, and theadaptive strategy is pretrained on fully labeledATIS data.
In both cases, part-of-speech(POS)tags are used as the lexical items of the sen-tences.
Backing off to POS tags is necessarybecause the tags provide a considerable inter-section in the vocabulary sets of the two cor-pora.5.1 Experiment 1: Learning ATISThe easier learning task is to induce grammarsto parse ATIS sentences.
The ATIS corpus con-sists of 577 short sentences with simple struc-tures, and the vocabulary set is made up of 32?
POS tags, a subset of the 47 tags used for theWSJ.
Due to the limited size of this corpus, tensets of randomly partitioned train-test-held-outtriples are generated to ensure the statisticalsignificance of our results.
We use 80 sentencesfor testing, 90 sentences for held-out data, andthe rest for training.
Before proceeding withthe main discussion on training from the ATIS,we briefly describe the pretraining stage of theadaptive strategy.5.1.1 Pretraining with WSJThe idea behind the adaptive method is simplyto make use of any existing labeled data.
Wehope that pretraining the grammars on thesedata might place them in a better position tolearn from the new, sparsely labeled data.
Inthe pretraining stage for this experiment, agrammar is directly induced from 3600 fullylabeled WSJ sentences.
Without any furthertraining on ATIS data, this grammar achieves aparsing score of 87.3% on ATIS test sentences.The relatively high parsing score suggests thatpretraining with WSJ has successfully placedthe grammar in a good position to begin train-ing with the ATIS data.5.1.2 Part ia l ly  Superv ised Train ing onATISWe now return to the main focus of this experi-ment: learning from sparsely annotated ATIStraining data.
To verify whether some con-stituent classes are more informative than oth-ers, we could compare the parsing scores of thegrammars trained using different constituentclass labels.
But this evaluation method doesnot take into account hat the distribution ofthe constituent classes is not uniform.
To nor-malize for this inequity, we compare the parsingscores to a baseline that characterizes the rela-tionship between the performance of the trainedgrammar and the number of bracketed con-stituents in the training data.
To generate thebaseline, we create training data in which 0%,25%, 50%, 75%, and 100% of the constituentbrackets are randomly chosen to be included.One class of linguistic labels is better than an-other if its resulting parsing improvement overthe baseline is higher than that of the other.The test results of the grammars inducedfrom these different raining data are summa-rized in Figure 1.
Graph (a) plots the outcomeof using the direct induction strategy, and graph(b) plots the outcome of the adaptive strat-egy.
In each graph, the baseline of random con-stituent brackets is shown as a solid line.
Scoresof grammars trained from constituent type spe-cific data sets are plotted as labeled dots.
Thedotted horizontal line in graph (b) indicates theATIS parsing score of the grammar trained onWSJ alone.Comparing the five constituent types, we seethat the HighP class is the most informative7695ss8~ 6se~ 555ORand-75% Rand-1Rand-2S JNP NotBasePHi~iIPb I i , i I I20O 40O 6OO 80O 1000 1200 1400 1600Number of brackets in the ATIS ~ain~lg data(a)95<!7s? "
!
60 5'~ ssSO.
.
.
.
.
.
RIP-1X' hP Rand-25% = _ * Rand-Iig o A\]INP - NotBaseP......... ~ ..... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
WSJ ~W .......i 1 i i i i i200 4OO 6OO BOO 1000 1200 1400 1600Number of brackets in the ATIS training data(b)Figure 1: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as afunction of the number of brackets present in the training corpus.
There are 1595 brackets in thetraining corpus all together.for the adaptive strategy, resulting in a gram-mar that scored better than the baseline.
Thegrammars trained on the AllNP annotation per-formed as well as the baseline for both strate-gies.
Grammars trained under all the othertraining conditions cored below the baseline.Our results suggest hat while an ideal train-ing condition would include annotations ofbothhigher-level phrases and simple phrases, com-plex clauses are more informative.
This inter-pretation explains the large gap between theparsing scores of the directly induced grammarand the adapted grammar trained on the sameHighP data.
The directly induced grammarperformed poorly because it has never seen alabeled example of simple phrases.
In contrast,the adapted grammar was already exposed tolabeled WSJ simple phrases, so that it success-fully adapted to the new corpus from annotatedexamples of higher-level phrases.
On the otherhand, training the adapted grammar on anno-tated ATIS simple phrases is not successful eventhough it has seen examples of WSJ higher-level phrases.
This also explains why gram-mars trained on the conglomerate class Not-BaseP performed on the same level as thosetrained on the AllNP class.
Although the Not-BaseP set contains the most brackets, most ofthe brackets are irrelevant to the training pro-cess, as they are neither higher-level phrases norsimple phrases.Our experiment also indicates that inductionstrategies exhibit different learning characteris-tics under partially supervised training condi-tions.
A side by side comparison of Figure 1(a) and (b) shows that the adapted grammarsperform significantly better than the directlyinduced grammars as the level of supervisiondecreases.
This supports our hypothesis thatpretraining on a different corpus can place thegrammar in a good initial search space for learn-ing the new domain.
Unfortunately, a good ini-tial state does not obviate the need for super-vised training.
We see from Figure l(b) thatretraining with unlabeled ATIS sentences actu-ally lowers the grammar's parsing accuracy.5.2 Exper iment  2: Learn ing WSJIn the previous ection, we have seen that anno-tations of complex clauses are the most helpfulfor inducing ATIS-style grammars.
One of thegoals of this experiment is to verify whether theresult also holds for the WSJ corpus, which isstructurally very different from ATIS.
The WSJcorpus uses 47 POS tags, and its sentences arelonger and have more embedded clauses.As in the previous experiment, we constructtraining sets with annotations of different con-stituent ypes and of different numbers of ran-domly chosen labels.
Each training set consistsof 3600 sentences, and 1780 sentences are usedas held-out data.
The trained grammars aretested on a set of 2245 sentences.Figure 2 (a) and (b) summarize the outcomes7780"i 7s70 5i "55'5 50~ I";~ 4035' ' ' Ran~l-Rand-25%/ e...NP~,.pNo~,P65!eo~It"6i 50'Rand-TS~F~nd-50"/,~____-----R a n d - 2 5 % ~  Not~eP~ Ba~N~ImP'~-,oo~.~a-~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i i i i i i i i i 35 ?
i I i i i i a i i~0  1oooo 15ooo 200~0 25ooo 30~0 350c0 4oo0o 45ooo 5ooo I ocxJo 15ooo 2oooo 25ooo 300~0 3s0~0 40ooo 45c~0Numb4r of brackets in me WSJ uaining data number of brackets in the WSJ training data(a) (b)Figure 2: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as afunction of the number of brackets present in the training corpus.
There is a total of 46463 bracketsin the training corpus.of this experiment.
Many results of this sectionare similar to the ATIS experiment.
Higher-level phrases till provide the most information;the grammars trained on the HighP labels arethe only ones that scored as well as the baseline.Labels of simple phrases till seem the least in-formative; scores of grammars trained on BasePand BaseNP remained far below the baseline.Different from the previous experiment, how-ever, the AI1NP training sets do not seem toprovide as much information for this learningtask.
This may be due to the increase in thesentence complexity of the WSJ, which furtherde-emphasized the role of the simple phrases.Thus, grammars trained on AllNP labels havecomparable parsing scores to those trained onHighP labels.
Also, we do not see as big a gapbetween the scores of the two induction strate-gies in the HighP case because the adaptedgrammar's advantage of having seen annotatedATIS base nouns is reduced.
Nonetheless, theadapted grammars still perform 2% better thanthe directly induced grammars, and this im-provement is statistically significant.
2Furthermore, grammars trained on NotBasePdo not fall as far below the baseline and havehigher parsing scores than those trained onHighP and AllNP.
This suggests that for morecomplex domains, other linguistic constituents2A pair-wise t-test comparing the parsing scores ofthe ten test sets for the two strategies shows 99% confi-dence in the difference.such as verb phrases 3 become more informative.A second goal of this experiment is to test theadaptive strategy under more stringent condi-tions.
In the previous experiment, a WSJ-stylegrammar was retrained for the simpler ATIScorpus.
Now, we reverse the roles of the cor-pora to see whether the adaptive strategy stilloffers any advantage over direct induction.In the adaptive method's pretraining stage,a grammar is induced from 400 fully labeledATIS sentences.
Testing this ATIS-style gram-mar on the WSJ test set without further train-ing renders a parsing accuracy of 40%.
Thelow score suggests that fully labeled ATIS datadoes not teach the grammar as much aboutthe structure of WSJ.
Nonetheless, the adap-tive strategy proves to be beneficial for learningWSJ from sparsely labeled training sets.
Theadapted grammars out-perform the directly in-duced grammars when more than 50% of thebrackets are missing from the training data.The most significant difference is when thetraining data contains no label information atall.
The adapted grammar parses with 60.1%accuracy whereas the directly induced grammarparses with 49.8% accuracy.SV~e have not experimented with training sets con-taining only verb phrases labels (i.e., setting a pair ofbracket around the head verb and its modifiers).
Theyare a subset of the NotBaseP class.786 Conc lus ion  and  Future  WorkIn this study, we have shown that the structureof a grammar can be reliably learned withouthaving fully specified constituent informationin the training sentences and that the most in-formative constituents of a sentence are higher-level phrases, which make up only a small per-centage of the total number of constituents.Moreover, we observe that grammar adaptationworks particularly well with this type of sparsebut informative training data.
An adaptedgrammar consistently outperforms a directly in-duced grammar even when adapting from a sim-pler corpus to a more complex one.These results point us to three future di-rections.
First, that the labels for some con-stituents are more informative than others im-plies that sentences containing more of these in-formative constituents make better training ex-amples.
It may be beneficial to estimate theinformational content of potential training (un-marked) sentences.
The training set should onlyinclude sentences that are predicted to havehigh information values.
Filtering out unhelpfulsentences from the training set reduces unnec-essary work for the human annotators.
Second,although our experiments show that a sparselylabeled training set is more of an obstacle for thedirect induction approach than for the grammaradaptation approach, the direct induction strat-egy might also benefit from a two stage learningprocess imilar to that used for grammar adap-tation.
Instead of training on a different corpusin each stage, the grammar can be trained ona small but fully labeled portion of the corpusin its first stage and the sparsely labeled por-tion in the second stage.
Finally, higher-levelconstituents have proved to be the most infor-mative linguistic units.
To relieve humans fromlabeling any training data, we should considerusing partial parsers that can automatically de-tect complex nouns and sentential clauses.Re ferencesJ.K.
Baker.
1979.
Trainable grammars forspeech recognition.
In Proceedings of theSpring Conference of the Acoustical Society ofAmerica, pages 547-550, Boston, MA, June.E.J.
Briscoe and N. Waegner.
1992.
Robuststochastic parsing using the inside-outside al-gorithm.
In Proceedings of the AAAI Work-shop on Probabilistically-Based NLP Tech-niques, pages 39-53.E.
Charniak.
1996.
Tree-bank grammars.
InProceedings of the Thirteenth National Con-ference on Artificial Intelligence, pages 1031-1036.E.
Mark Gold.
1967.
Language identificationin the limit.
Information Control, 10(5):447-474.C.T.
Hemphill, J.J. Godfrey, and G.R.
Dod-dington.
1990.
The ATIS spoken languagesystems pilot corpus.
In DARPA Speech andNatural Language Workshop, Hidden Valley,Pennsylvania, June.
Morgan Kaufmann.R.
Hwa.
1998a.
An empirical evaluation ofprobabilistic lexicalized tree insertion gram-mars.
In Proceedings of COLING-A CL, vol-ume 1, pages 557-563.R.
Hwa.
1998b.
An empirical evaluation o fprobabilistic lexicalized tree insertion gram-mars.
Technical Report 06-98, Harvard Uni-versity.
Available as cmp-lg/9808001.K.
Lari and S.J.
Young.
1990.
The estima-tion of stochastic ontext-free grammars us-ing the inside-outside algorithm.
ComputerSpeech and Language, 4:35-56.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annontated corpus ofenglish: the penn treebank.
ComputationalLinguistics, 19(2):313-330.F.
Pereira and Y. Schabes.
1992.
Inside-Outside reestimation from partially bracketedcorpora.
In Proceedings of the 30th AnnualMeeting of the A CL, pages 128-135, Newark,Delaware.Y.
Schabes and R. Waters.
1993.
Stochasticlexicalized context-free grammar.
In Proceed-ings of the Third International Workshop onParsing Technologies, pages 257-266.Y.
Schabes, M. Roth, and R. Osborne.
1993.Parsing the Wall Street Journal with theInside-Outside algorithm.
In Proceedings ofthe Sixth Conference of the European Chap-ter of the ACL, pages 341-347.79
