Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 26?33,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsAnother look at indirect negative evidenceAlexander ClarkDepartment of Computer ScienceRoyal Holloway, University of Londonalexc@cs.rhul.ac.ukShalom LappinDepartment of PhilosophyKing?s College, Londonshalom.lappin@kcl.ac.ukAbstractIndirect negative evidence is clearly an im-portant way for learners to constrain over-generalisation, and yet a good learningtheoretic analysis has yet to be providedfor this, whether in a PAC or a proba-bilistic identification in the limit frame-work.
In this paper we suggest a theoreti-cal analysis of indirect negative evidencethat allows the presence of ungrammati-cal strings in the input and also accountsfor the relationship between grammatical-ity/acceptability and probability.
Givenindependently justified assumptions aboutlower bounds on the probabilities of gram-matical strings, we establish that a limitednumber of membership queries of somestrings can be probabilistically simulated.1 IntroductionFirst language acquisition has been studied for along time from a theoretical point of view, (Gold,1967; Niyogi and Berwick, 2000), but a consen-sus has not emerged as to the most appropriatemodel for learnability.
The two main competingcandidates, Gold-style identification in the limitand PAC-learning both have significant flaws.For most NLP researchers, these issues are sim-ply not problems: for all empirical purposes, oneis interested in modelling the distribution of exam-ples or the conditional distribution of labels givenexamples and the obvious solution ?
an  ?
?bound on some suitable loss function such as theKullback-Leibler Divergence ?
is sufficient (Horn-ing, 1969; Angluin, 1988a).
There may be somecomplexity issues involved with computing theseapproximations, but there is no debate about theappropriateness of the learning paradigm.However, such an approach is unappealing tolinguists for a number of reasons: it fails to drawa distinction between grammatical and ungram-matical sentences, and for many linguists the keydata are not the ?performance?
data but rather the?voice of competence?
as expressed in grammat-icality and acceptability judgments.
Many of themost interesting sentences for syntacticians arecomparatively rare and unusual and may occurwith negligible frequency in the data.We do not want to get into this debate here: inthis paper, we will assume that there is a categori-cal distinction between grammatical and ungram-matical sentences.
See (Schu?tze, 1996) for exten-sive discussion.Within this view learnability is technically quitedifficult to formalise in a realistic way.
Childrenclearly are provided with examples of the lan-guage ?
so-called positive data ?
but the statusof examples not in the language ?
negative data?
is one of the endless and rather circular de-bates in the language acquisition literature (Mar-cus, 1993).
Here we do not look at the role ofcorrections and other forms of negative data butwe focus on what has been called indirect nega-tive evidence (INE).
INE is the non-occurrence ofdata in the primary linguistic data; informally, ifthe child does not hear certain ungrammatical sen-tences, then by their absence the child can inferthat those strings are ungrammatical.Indirect negative evidence has long been recog-nised as an important source of information(Pinker, 1979).
However it has been surpris-ingly difficult to find an explicit learning theo-retic account of INE.
Indeed, in both the PACand IIL paradigms it can be shown, that underthe standard assumptions, INE cannot help thelearner.
Thus in many of these models, thereis a sharp and implausible distinction betweenlearning paradigms where the learner is providedsystematically with every negative example, andthose where the learner is denied any negative ev-idence at all.
Neither of these is very realistic.In this paper, we suggest a resolution for thisconflict, by re-examining the standard learnabilityassumptions.
We make three uncontroversial ob-26servations: first that the examples the child is pro-vided with are unlabelled, secondly that there area small proportion of ungrammatical sentences inthe input to the child, and thirdly that in spite ofthis, the child does in fact learn.We then draw a careful distinction betweenprobability and grammaticality and propose a re-striction on the class of distributions allowed totake account of the fact that children are exposedto some ungrammatical utterances.
We call thisthe Disjoint Distribution Assumption: the assump-tion that the classes of distributions for differentlanguages must be disjoint.
Based on this assump-tion, we argue that the learner can infer lowerbounds on the probabilities of grammatical strings,and that using these lower bounds allow a prob-abilistic approximation to membership queries ofsome strings.On this basis we conclude that the learner doeshave some limited access to indirect negative evi-dence, and we discuss some of the limitations onthis data and the implications for learnability.2 BackgroundThe most linguistically influential learnabilityparadigm is undoubtedly that of Gold (Gold,1967).
In this paradigm the learner is required toconverge to exactly the right answer after a finitetime.
In one variant of the paradigm the learneris provided with only positive examples, and mustlearn on every presentation of the language.
Un-der this paradigm no suprafinite class of languagesis learnable.
If alternatively the learner is pro-vided with a presentation of labelled examples,then pretty much anything is learnable, but clearlythis paradigm has little relevance to the course oflanguage acquisition.The major problem with the Gold positive dataparadigm is that the learner is required to learnunder every presentation; given the minimal con-straints on what counts as a presentation, this re-sults in a model which is unrealistically hard.
Inparticular, it is difficult for the learner to recoverfrom an overly general hypothesis; since it is hasonly positive examples, such a hypothesis willnever be directly contradicted.Indirect negative evidence is the claim that theabsence of sentences in the PLD can allow alearner to infer that those sentences are ungram-matical.
As (Chomsky, 1981, p. 9) says:A not unreasonable acquisition sys-tem can be devised with the opera-tive principle that if certain structuresor rules fail to be exemplified in rel-atively simple expressions, where theywould expect to be found, then a (pos-sibly marked) option is selected exclud-ing them in the grammar, so that a kindof ?negative evidence?
can be availableeven without corrections, adverse reac-tions etc.While this informal argument has been widelyaccepted, and is often appealed to, it has so far notbeen incorporated explicitly into a formal modelof learnability.
Thus there are no learning mod-els that we are aware of where positive learningresults have been achieved using indirect negativeevidence.
Instead positive learnability results havetypically used general probabilistic models of con-vergence without explicitly modelling grammati-cality.In what follows we will use the following no-tation.
?
is a finite alphabet, and ??
is theset of all finite strings over ?.
A (formal) lan-guage L is a subset of ??.
A distribution D over??
is a function pD from ??
to [0, 1] such that?w???
pD(w) = 1.
We will write D(??)
for theset of all distributions over ??.
The support of adistribution D is the set of strings with positiveprobability supp(D) = {w|pD(w) > 0}.3 Probabilistic learningThe solution is to recognise the probabilistic na-ture of how the samples are generated.
We canassume they are generated by some stochastic pro-cess.
On its own this says nothing ?
anything canbe modelled by a stochastic process.
To get learn-ability we will need to add some constraints.Suppose the child has seen thousands of timessentences of the type ?I am AP?, and ?He isAP?
where AP is an adjective phrase, but he hasnever heard anybody say ?He am AP?.
Intuitivelyit seems reasonable in this case to assume thatthe child can infer from this that sentences of theform?He am AP?
are ungrammatical.
Now, in thecase of the Gold paradigm, the child can makeno such inference.
No matter how many millionsor trillions of times he has heard other examples,the Gold paradigm does not allow any inferenceto be made from frequency.
The teacher, or en-vironment, is an adversary who might be deliber-ately withholding this data in order to confuse the27learner.
The learner has to ignore this information.However, in a more plausible learning environ-ment, the learner can reason as follows.
First, thenumber of times that the learner has observed sen-tences of the form ?He am AP?
is zero.
From this,the learner can infer that sentences of this type arerare: i.e.
that they are not very probable.
Similarlyfrom the high frequency of examples of the type ?Iam AP?
and so on in the observed data, the learnercan infer that the probability of these sentences ishigh.The second step is that the learner can con-clude from the difference in probability of thesetwo similar sets of sentences, that there must be adifference in grammaticality between ?He am AP?and ?He is AP?, and thus that sentences of the type?He am AP?
are ungrammatical.It is important to recognise that the inferenceproceeds in two steps:1. the first is the inference from low frequencyin the observed data to low probability and2.
the second is the inference from compara-tively low probability to ungrammaticality.Both of these steps need justification, but if theyare valid, then the learner can extract evidenceabout what is not in the language from stochasticevidence about what is in the language.
The firststep will be justified by some obvious and reason-able probabilistic assumptions about the presenta-tion of the data; the second step is more subtle andrequires some assumptions about the way the dis-tribution of examples relates to the language beinglearned.3.1 Stochastic assumptionsThe basic assumption we make is that the sam-ples are being generated randomly in some way;here we will make the standard assumption thateach sentence is generated independently fromthe same fixed distribution, the Independently andIdentically Distributed (IID) assumption.
Whilethis is a very standard assumption in statistics andprobability, it has been criticised as a modellingassumption for language acquisition (Chater andVita?nyi, 2007).Here we are interested in the acquisition of syn-tax.
We are therefore modelling the dependenciesbetween words and phrases in sentences, but as-suming that there are no dependencies betweendifferent sentences in discourse.
That is to say, weassume that the probability that a child hears a par-ticular sentence does not depend on the previouslyoccurring sentence.
Clearly, there are dependen-cies between sentences.
After questions, come an-swers; a polar interrogative is likely to be followedby a ?yes?
or a ?no?
; topics relate consecutivesentences semantically, and numerous other fac-tors cause inter-sentential relationships and regu-larities of various types.
Moreover, acceptabilitydoes depend a great deal on the immediate context.
?Where did who go??
is marginal in most con-texts; following ?Where did he go??
it is perfectlyacceptable.
Additionally, since there are multiplepeople generating Child Directed Speech (CDS),this also introduces dependencies: each personspeaks in a slightly different way; while a rela-tive is visiting, there will be a higher probabilityof certain utterances, and so on.
These correspondto a violation of the ?identically?
part of the IIDassumption: the distribution will change in time.The question is whether it is legitimate to ne-glect these issues in order to get some mathemat-ical insight: do these idealising assumptions criti-cally affect learnability?
All of the computationalwork that we are aware of makes these assump-tions, whether in a nativist paradigm, (Niyogi andBerwick, 2000; Sakas and Fodor, 2001; Yang,2002) or an empiricist one (Clark and Thollard,2004).
We do need to make some assumptions,otherwise even learning the class of observed nat-ural languages would be too hard.
The minimalassumptions if we wish to allow any learnabilityunder stochastic presentation are that the processgenerating the data is stationary and mixing.
Allwe need is for the law of large numbers to hold,and for there to be rapid convergence of the ob-served frequency to the expectation.
We can getthis easily with the IID assumption, or with a bitmore work using ergodic theory.
Thus in what fol-lows we will make the IID assumption; effectivelyusing it as a place-holder for some more realisticassumption, based on ergodic processes.
See forexample (Gamarnik, 2003) for a an extension ofPAC analysis in this direction.
The inference fromlow frequency to low probability follows from theminimal assumptions, specifically the IID, whichwe are making here.4 Probability and GrammaticalityWe now look at the second step in the probabilisticinference: how can the child go from low probabil-28ity to ungrammaticality?
More generally the ques-tion is what is the relation between probability andgrammaticality.
There are lots of factors that affectprobability other than grammaticality: length ofutterance, lexical frequency, semantic factors andreal world factors all can have an impact on prob-ability.Low probability on its own cannot imply un-grammaticality: if there are infinitely many gram-matical sentences then there cannot be a lowerbound on the probability: if all grammaticalsentences have probability at least  then therecould be at most 1/ grammatical sentences whichwould make the language finite.
A very longgrammatical sentence can have very low probabil-ity, lower than a short ungrammatical sentence, soa less naive approach is necessary: the key point isthat the probability must be comparatively low.Since we are learning from unlabelled data, theonly information that the child has comes fromfrom the distribution of examples, and so the dis-tribution must pick out the language precisely.
Tosee this more clearly, suppose that the learner hadaccess to an ?Oracle?
that would tell it the trueprobability of any string, and has no limit on howmany strings it sees.
A learner in this unrealisticmodel is clearly more powerful than any learnerthat just looks at a finite sample of the data.
If thislearner could not learn, then no real learner couldlearn on the basis of finite data.More precisely for any language L we will havea corresponding set of distributions D(L), and werequire the learner to learn under any of these dis-tributions.
What we require is that if we have twodistinct languages L and L?
then the two sets ofdistributionsD(L) andD(L?)
must be disjoint, i.e.have no elements in common.
If they did have adistribution D in common, then no learner couldtell the two languages apart as the information be-ing provided would be identical.
Of course, giventwo distinct languages L and L?, it is possible thatthey intersect, that is to say that there are stringsw in L?L?
; a natural language example would betwo related dialects of the same language such assome dialect of British English and some dialect ofAmerican; though the languages are distinct in for-mal terms, they are not disjoint, as there are sen-tences that are grammatical in both.
When we con-sider the sets of distributions that are allowed foreach language D(L) and D(L?
), we may find thatthere are elements D ?
D(L) and D?
?
D(L?
),whose supports overlap, or even whose supportsare identical, supp(D) = supp(D?
), and we maywell find that there are even some strings whoseprobabilities are identical; i.e.
there may be astring w such that pD(w) = pD?
(w) > 0.
Butwhat we do not allow is that we have a distributionD that is an element of both D(L) and D(L?).
Ifthere were such an element, then when the learnerwas provided with samples drawn from this dis-tribution, since the samples are unlabelled, thereis absolutely no way that the learner could workout whether the target was L or L?
; the distribu-tion would not determine the language.
Thereforethere must be a function from distributions to lan-guages.
We cannot have a single distribution thatcould be from two different languages.
Let?s callthis the disjoint distribution assumption (DDA):the assumption that the sets of distributions for dis-tinct languages are disjoint.Definition 1 The Disjoint Distribution Assump-tion: If L 6= L?
then D(L) ?
D(L?)
= ?.This assumption seems uncontroversial; indeedevery proposal for a formal probabilistic model oflanguage acquisition that we are aware of makesthis assumption implicitly.Now consider the convergence criterion: wewish to measure the error with respect to the distri-bution.
There are two error terms, correspondingto false positives and false negatives.
Suppose ourtarget language is T and our hypothesis is H .
De-fine PD(S) for some set S to be?w?S pD(s).e+ = PD(H \ T ) (1)e?
= PD(T \H) (2)We will require both of these error terms to con-verge to zero rapidly, and uniformly, as the amountof data the learner has increases.5 Modelling the DDAIf we accept this assumption, then we will requiresome constraints on the sets of distributions.
Thereare a number of ways to model this: the most ba-sic way is to assume that strings have probabilitygreater than zero if and only if the string is in thelanguage.
Formally, for all D in D(L)pD(w) > 0 ?
w ?
L (3)Here we clearly have a function from distribu-tions to languages: we just take the support of the29distribution to be the language: for all D in D(L),supp(D) = L. Under this assumption alone how-ever, indirect negative evidence will not be avail-able.That is because, in this situation, low probabil-ity does not imply ungrammaticality: only zeroprobability implies ungrammaticality.
The factthat we have never seen a sentence in a finite sam-ple of size n means that we can say that it is likelyto have probability less than about 1/n, but wecannot say that its probability is likely to be zero.Thus we can never conclude that a sentence is un-grammatical, if we make the assumption in Equa-tion 3, and assume that there are no other limita-tions on the set of distributions.
Since we haveto learn for any distribution, we must learn evenwhen the distribution is being picked adversari-ally.
Suppose we have never seen an occurrenceof a string; this could be because the probabilityhas been artificially lowered to some infinitesimalquantity by the adversary to mislead us.
Thus wegain nothing.
Since there is no non-trivial lowerbound on the probability of grammatical strings,effectively there is no difference between the re-quirement pD(w) > 0 ?
w ?
L and the weakercondition pD(w) > 0 ?
w ?
L.But this is not the only possibility: indeed, it isnot a very good model at all.
First, the assump-tion that ungrammatical strings have zero proba-bility is false.
Ungrammatical sentences, that isstrings w, such that w 6?
L, do occur in the en-vironment, albeit with low probability.
There areperformance errors, poetry and songs, other chil-dren with less than adult competence, foreignersand many other potential sources of ungrammat-ical sentences.
The orthodox view is that CDSis ?unswervingly well-formed?
(Newport et al,1977): this is a slight exaggeration as a quick lookat CHILDES (MacWhinney, 2000) will confirm.However, if we allow probabilities to be non-zerofor ungrammatical sentences, and put no other re-strictions on the distributions then the learner willfail on everything, since any distribution could befor any language.Secondly, the convergence criterion becomesvacuous.
As the probability of ungrammatical sen-tences is now zero, this means that PD(H \ T ) =e+ = 0, and thus the vacuous learner that alwaysreturns the hypothesis ??
will have zero error.
Thenormal way of dealing with this (Shvaytser, 1990)is to require the learner to hypothesize a subset ofthe target.
This is extremely undesirable, as it failsto account for the presence of over-generalisationerrors in the child ?
or any form of production ofungrammatical sentences.
On the basis of thesearguments, we can see that this naive approach isclearly inadequate.There are a number of other arguments why dis-tribution free approaches are inappropriate here,even though they are desirable in standard appli-cations of statistical estimation (Collins, 2005).First, the distribution of examples causally de-pends on the people who are uttering the exampleswho are native speakers of the language the learneris learning and use that knowledge to construct ut-terances.
Second, suppose that we are trying tolearn a class of languages that includes some in-finite regular language Lr.
For concreteness sup-pose it consists of {a?b?c?
}; any number of a?s fol-lowed by any number of b?s followed by any num-ber of c?s.
The learner must learn under any dis-tribution: in particular it will have to learn underthe distribution where every string except an in-finitesimally small amount has the number of ?a?sequal to the number of ?b?s, or under the distribu-tion where the number of occurrences of all threeletters must be equal, or any other arbitrary subsetof the target language.
The adversary can distortthe probabilities so that with probability close toone, at a fixed finite time, the learner will only seestrings from this subset.
In effect the learner hasto learn these arbitrary subsets, which could be ofmuch greater complexity than the language.Indeed researchers doing computational ormathematical modelling of language acquisitionoften find it convenient to restrict the distribu-tions in some way.
For example (Niyogi andBerwick, 2000), in some computational modellingof a parameter-setting model of language acquisi-tion sayIn the earlier section we assumedthat the data was uniformly distributed.. .
.
In particular we can choose a dis-tribution which will make the conver-gence time as large as we want.
Thusthe distribution-free convergence timefor the three parameter system is infi-nite.However, finding an alternative is not easy.There are no completely satisfactory ways of re-stricting the class of distributions, while maintain-ing the property that the support of the distribu-30tion is equal to the language.
(Clark and Thollard,2004) argue for limiting the class of distributionsto those defined by the probabilistic variants of thestandard Chomsky representations.
While this issufficient to achieve some interesting learning re-sults, the class of distributions seems too small,and is primarily motivated by the requirements ofthe learning algorithm, rather than an analysis ofthe learning situation.5.1 Other boundsRather than making the simplistic assumption thatthe support of the distribution must equal the lan-guage, we can instead make the more realistic as-sumption that every sentence, grammatical or un-grammatical, can in principle appear in the inputand have non zero probability.
In this case thenwe do not need to require the learner to produce ahypothesis that is a subset of the target, because ifthe learner overgeneralises, e+ will be non-zero.However, we clearly need to add some con-straints to enforce the DDA.
We can model this asa function from distributions to languages.
It is ob-vious that grammaticality is correlated with prob-ability in the sense that grammatical sentences are,broadly speaking, more likely than ungrammaticalsentences; a natural way of articulating this is tosay that that there must be a real valued thresholdfunction gD(w) such that if pD(w) > gD(w) thenw ?
L. Using this we define the set of allowabledistributions for a language L to be:D(L, g) = {D : pD(w) > gD(w) ?
w ?
L}(4)Clearly this will satisfy the DDA.
On its own thisis vacuous ?
we have just changed notation, butthis notation gives us a framework in which tocompare some alternatives.The original assumption that the support isequal to the languages in this framework then justhas the simple form gD(w) = 0.
The naive con-stant bound we rejected above would be to havethis threshold as a constant that depends neither onD nor on w i.e.
for all w , gD(w) =  > 0.
Bothof these bounds are clearly false, in the sense thatthey do not hold for natural distributions: the firstbecause there are ungrammatical sentences withnon-zero probability; the second because there aregrammatical sentences with arbitrarily low proba-bility.
But the bound here need not be a constant,and indeed it can depend both on the distributionD and the word w.5.2 Functional boundWe now look at variants of these bounds that pro-vide a more accurate picture of the set of distribu-tions that the child is exposed to.
Recall that whatwe are trying to do is to characterise a range of dis-tributions that is large enough to include those thatthe child will be exposed to.
A slightly more nu-anced way would be to have this as a very simplefunction ofw, that ignoresD, and is just a functionof length.
For example, we could have a simpleuniform exponential model:gD(w) = ?g?|w|g (5)This is in some sense an application of Harris?sidea of equiprobability (Harris, 1991):whatever else there is to be saidabout the form of language, a fun-damental task is to state the depar-tures from equiprobability in sound- andword-sequencesUsing this model, we do not assume that thelearner is provided with information about thethreshold g; rather the learner will have cer-tain, presumably domain general mechanisms thatcause it to discard anomalies, and pay attentionto significant deviations from equiprobability.
Wecan view the threshold g as defining a bound onequiprobability; the role of syntax is to charac-terise these deviations from the assumption that allsequences are in some sense equally likely.A more realistic model would depend also onD; for example once could define these thresholdsto depend on some simple observable properties ofthe distribution that could take account of lexicalprobabilities: more sophisticated versions of thisbound could be derived from a unigram model, ora class-based model (Pereira, 2000).Alternatively we could take account of the pre-fix and suffix probability of a string: for example,where for some ?
< 1: 1gD(w) = ?
maxuv=wpD(u??)pD(?
?v) (6)6 Using the lower boundPutting aside the specific proposal for the lowerbound g, and going back to the issue of indirect1A prefix is just an initial segment of a string and has nolinguistic and similarly for a suffix as the final segment.31negative evidence, we can see that the bound g isthe missing piece in the inference: if we observethat a string w has zero frequency in our data set,then we can conclude it has low probability, sayp; if p is less than g(w), then the string will beungrammatical; therefore the inference from lowprobability to ungrammaticality in this case willbe justified.The bound here is justified independently:given the indubitable fact that there is a non-zeroprobability of ungrammatical strings in the child?sinput, and the DDA, which again seems unassail-able, together with the fact that learners do learnsome languages, it is a logical necessity that thereis such a bound.
This bound then justifies indirectnegative evidence.It is important to realise how limited this neg-ative evidence is: it does not give the learner un-limited access to negative examples.
The learnercan only find out about sentences that would befrequent if they were grammatical; this may beenough to constrain overgeneralisation.The most straightforward way of formalisingthis indirect negative evidence is with membershipqueries (Valiant, 1984; Angluin, 1988b).
Mem-bership queries are a model of learning where thelearner, rather than merely passively receiving ex-amples, can query an oracle about whether an ex-ample is in the language or not.
In the model wepropose, the learner can approximate a member-ship query with high probability by seeing the fre-quency of an example with a high g in a large sam-ple.
If the frequency is low, often zero, in this sam-ple, then with high probability this example will beungrammatical.In particular given a functional bound, and somepolynomial thresholds on the probability, and us-ing Chernoff bounds we can simulate a polyno-mial number of membership queries, using largesamples of data.
Note that membership querieswere part of the original PAC model (Valiant,1984).
Thus we can precisely define a limitedform of indirect negative evidence.In particular given a bound g, we can test to seewhether a polynomial number of strings are un-grammatical by taking a large sample and examin-ing their frequency.The exact details here depend on the form ofgD(w); if the bound depends on D in some re-spect the learner will need to estimate some aspectof D to compute the bound.
This corresponds toworking out how probable the sentence would beif it were grammatical.
In the cases we have con-sidered here, given sufficient data, we can estimategD(w) with high probability to an accuracy of 1;call the estimate g?D(w).
We can also estimate theactual probability of the string with high probabil-ity again with accuracy 2: let us denote this es-timate by p?D(w).
If p?D(w) + 2 < g?D(w) ?
1,then we can conclude that pD(w) < gD(w) andtherefore that the sentence is ungrammatical.
Con-versely, the fact that a string has been observedonce does not necessarily mean that it is grammat-ical.
It only means that the probability is non-zero.For the learner to conclude that it is grammatical,s/he needs to have seen it enough times to con-clude that the probability is above threshold.
Thiswill be if p?D(w)?
2 > g?D(w) + 1Note that this may be slightly too weak andwe might want to have a separate lower boundfor grammaticality and upper bound for ungram-maticality.
Otherwise if the distribution is suchthat many strings are very close to the boundaryit will not be possible for the learner to determinewhether they are grammatical or not.We can thus define learnability with respect to abound g that defines a set of distributionsD(L,G).Thus this model differs from the PACmodel in tworespects: first the data is unlabelled, and secondlyis is not distribution free.Definition An algorithm A learns the class oflanguagesL if there is a polynomial p such that forevery language L ?
L, where n is the size of thesmallest representation of L, for all distributionsD ?
D(L, g) for all , ?
> 0, when the algorithmA is provided with at least p(n, ?1, ??1,?)
un-labelled examples drawn IID from D, it produceswith probability at least 1??
a hypothesis H suchthat the error PD(H \T ?T \H) <  and further-more it runs in time polynomial in the total size ofthe sample.7 DiscussionThe unrealistic assumptions of the Gold paradigmwere realised quite early on (Horning, 1969).
Itis possible to modify the Gold paradigm by in-corporating a probabilistic presentation in the dataand requiring the learner to learn with probabil-ity one.
Perhaps surprisingly this does not changeanything, if we put no constraints on the target dis-tribution (Angluin, 1988a).In particular given a presentation on which the32normal non-probabilistic learner fails, we can con-struct a distribution on which the probabilisticlearner will fail.
Thus allowing an adversary topick the distribution is just as bad as allowing anadversary to pick the presentation.
However, thedistribution free assumption with unlabelled datacannot account for the real variety of distributionsof CDS.
In this model we propose restrictions onthe class of distributions, motivated by the oc-currence of ungrammatical sentences.
This alsomeans that we do not require a separate bound forover-generalisation.
As a result, we conclude thatthere are limited amounts of negative evidence,and suggest that these can be formalised as a lim-ited number of membership queries, of strings thatwould occur infrequently if they were ungrammat-ical.To be clear, we are not claiming that this is a di-rect model of how children learn languages: ratherwe hope to get some insight into the fundamen-tal limitations of learning from unlabelled data byswitching to a more nuanced model.
Here we havenot presented any positive results using this model,but we observe that distribution dependent resultsfor learning regular languages and some contextfree languages could be naturally modified to learnin this framework.
We hope that the recognition ofthe validity of indirect negative evidence will di-rect attention away from the supposed problems ofcontrolling overgeneralisation and towards the realproblems: the computational complexity of infer-ring complex models.ReferencesD.
Angluin.
1988a.
Identifying languages fromstochastic examples.
Technical Report YALEU/DCS/RR-614, Yale University, Dept.
of ComputerScience, New Haven, CT.D.
Angluin.
1988b.
Queries and concept learning.Machine Learning, 2(4):319?342, April.N.
Chater and P. Vita?nyi.
2007.
?Ideal learning?
of nat-ural language: Positive results about learning frompositive evidence.
Journal of Mathematical Psy-chology, 51(3):135?163.N.
Chomsky.
1981.
Lectures on Government andBinding.Alexander Clark and Franck Thollard.
2004.
Par-tially distribution-free learning of regular languagesfrom positive samples.
In Proceedings of COLING,Geneva, Switzerland.M.
Collins.
2005.
Parameter estimation for statisticalparsing models: Theory and practice of distribution-free methods.
In Harry Bunt, John Carroll, andGiorgio Satta, editors, New Developments In Pars-ing Technology, chapter 2, pages 19?55.
Springer.D Gamarnik.
2003.
Extension of the PAC frameworkto finite and countable Markov chains.
IEEE Trans-actions on Information Theory, 49(1):338?345.E.
M. Gold.
1967.
Language identification in the limit.Information and control, 10(5):447 ?
474.Z.S.
Harris.
1991.
A Theory of Language and Informa-tion: A Mathematical Approach.
Clarendon Press.James Jay Horning.
1969.
A study of grammaticalinference.
Ph.D. thesis, Computer Science Depart-ment, Stanford University.B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum AssociatesInc, US.G.F.
Marcus.
1993.
Negative evidence in languageacquisition.
Cognition, 46(1):53?85.E.L.
Newport, H. Gleitman, and L.R.
Gleitman.
1977.Mother, I?d rather do it myself: Some effects andnon-effects of maternal speech style.
In Talkingto children: Language input and acquisition, pages109?149.
Cambridge University Press.Partha Niyogi and Robert C. Berwick.
2000.
Formalmodels for learning in the principle and parametersframework.
In Peter Broeder and Jaap Murre, ed-itors, Models of Language Acquisition, pages 225?243.
Oxford University Press.F.
Pereira.
2000.
Formal grammar and informationtheory: Together again?
In Philosophical Transac-tions of the Royal Society, pages 1239-1253.
RoyalSociety, London.Steven Pinker.
1979.
Formal models of languagelearning.
Cognition, 7:217?282.W.
Sakas and J.D.
Fodor.
2001.
The structural triggerslearner.
In Language Acquisition and Learnability,pages 172?233.
Cambridge University Press.Carson T. Schu?tze.
1996.
The Empirical Base of Lin-guistics.
University of Chicago Press.H.
Shvaytser.
1990.
A necessary condition for learn-ing from positive examples.
Machine Learning,5(1):101?113.L.
Valiant.
1984.
A theory of the learnable.
Communi-cations of the ACM, 27(11):1134 ?
1142.C.D.
Yang.
2002.
Knowledge and Learning in NaturalLanguage.
Oxford University Press, USA.33
