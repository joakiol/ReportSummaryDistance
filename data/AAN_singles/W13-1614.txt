Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 100?107,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsSentiment analysis on Italian tweetsValerio BasileUniversity of Groningenv.basile@rug.nlMalvina NissimUniversity of Bolognamalvina.nissim@unibo.itAbstractWe describe TWITA, the first corpus of Italiantweets, which is created via a completely au-tomatic procedure, portable to any other lan-guage.
We experiment with sentiment anal-ysis on two datasets from TWITA: a genericcollection and a topic-specific collection.
Theonly resource we use is a polarity lexicon,which we obtain by automatically matchingthree existing resources thereby creating thefirst polarity database for Italian.
We observethat albeit shallow, our simple system capturespolarity distinctions matching reasonably wellthe classification done by human judges, withdifferences in performance across polarity val-ues and on the two sets.1 IntroductionTwitter is an online service which lets subscriberspost short messages (?tweets?)
of up to 140 charac-ters about anything, from good-morning messagesto political stands.Such micro texts are a precious mine for graspingopinions of groups of people, possibly about a spe-cific topic or product.
This is even more so, sincetweets are associated to several kinds of meta-data,such as geographical coordinates of where the tweetwas sent from, the id of the sender, the time of theday ?
information that can be combined with textanalysis to yield an even more accurate picture ofwho says what, and where, and when.
The last yearshave seen an enormous increase in research on de-veloping opinion mining systems of various sortsapplying Natural Language Processing techniques.Systems range from simple lookups in polarity oraffection resources, i.e.
databases where a polarityscore (usually positive, negative, or neutral) is asso-ciated to terms, to more sophisticated models builtthrough supervised, unsupervised, and distant learn-ing involving various sets of features (Liu, 2012).Tweets are produced in many languages, but mostwork on sentiment analysis is done for English (evenindependently of Twitter).
This is also due to theavailability of tools and resources.
Developing sys-tems able to perform sentiment analysis for tweets ina new language requires at least a corpus of tweetsand a polarity lexicon, both of which, to the best ofour knowledge, do not exist yet for Italian.This paper offers three main contributions in thisrespect.
First, we present the first of corpus of tweetsfor Italian, built in such a way that makes it possi-ble to use the exact same strategy to build similarresources for other languages without any manualintervention (Section 2).
Second, we derive a polar-ity lexicon for Italian, organised by senses, also us-ing a fully automatic strategy which can replicatedto obtain such a resource for other languages (Sec-tion 3.1).
Third, we use the lexicon to automaticallyassign polarity to two subsets of the tweets in ourcorpus, and evaluate results against manually anno-tated data (Sections 3.2?3.4).2 Corpus creationWe collected one year worth of tweets, from Febru-ary 2012 to February 2013, using the Twitter fil-ter API1 and a language recognition strategy which1https://dev.twitter.com/docs/api/1/post/statuses/filter100we describe below.
The collection, named TWITA,consists of about 100 million tweets in Italian en-riched with several kinds of meta-information, suchas the time-stamp, geographic coordinates (when-ever present), and the username of the twitter.
Addi-tionally, we used off-the-shelf language processingtools to tokenise all tweets and tag them with part-of-speech information.2.1 Language detectionOne rather straightforward way of creating a corpusof language-specific tweets is to retrieve tweets viathe Twitter API which are matched with stronglylanguage-representative words.
Tjong Kim Sangand Bos (2012) compile their list of highly typ-ical Dutch terms manually to retrieve Dutch-onlytweets.
While we also use a list of strongly repre-sentative Italian words, we obtain such list automat-ically.
This has the advantage of making the proce-dure more objective and fully portable to any otherlanguage for which large reference corpora are avail-able.
Indeed, we relied on frequency information de-rived from ItWac, a large corpus of Italian (Baroni etal., 2009), and exploited Google n-grams to rule outcross-language homographs.
For boosting precision,we also used the publicly available language recog-nition software langid.py (Lui and Baldwin, 2012).The details of the procedure are given below:1. extract the 1.000 most frequent lemmas fromItWaC;2. extract tweets matched by the selected repre-sentative words and detect the language using afreely available software;23. filter out the terms in the original list whichhave high frequency in a conflicting language.Frequency is obtained from Google N-grams;4. use high frequency terms in the resultingcleaner list to search the Twitter API.The 20 top terms which were then used to matchItalian-only tweets are: vita Roma forza alla quantoamore Milano Italia fare grazie della anche peri-odo bene scuola dopo tutto ancora tutti fatto.
In the2Doing so, we identify other languages that share charac-ter sequences with Italian.
The large majority of tweets in thefirst search were identified as Portuguese, followed by English,Spanish and then Italian.extraction, we preserved metadata about user, time,and geographical coordinates whenever available.Both precision and recall of this method are hardto assess.
We cannot know how many tweets thatare in fact Italian we?re actually missing, but theamount of data we can in any case collect is so highthat the issue is not so relevant.3 Precision is moreimportant, but manual checking would be too time-consuming.
We inspected a subset of 1,000 tweetsand registered a precision of 99.7% (three very shorttweets were found to be in Spanish).
Consideringthat roughly 2.5% of the tweets also include the ge-ographical coordinates of the device used to send themessage, we assessed an approximate precision in-directly.
We plotted a one million tweets randomlychosen from our corpus and obtained the map shownin Figure 1 (the map is clipped to the Europe area forbetter identifiability).
We can see that Italy is clearlyoutlined, indicating that precision, though not quan-tifiable, is likely to be satisfactory.Figure 1: Map derived by plotting geo-coordinates oftweets obtained via our language-detection procedure.2.2 ProcessingThe collected tweets have then been enriched withtoken-level, POS-tags, and lemma information.Meta-information was excluded from processing.So for POS-tagging and lemmatisation we substi-tuted hashtags, mentions (strings of the form @user-3This is because we extract generic tweets.
Should one wantto extract topic-specific tweets, a more targeted list of charac-terising terms should be used.101name referring to a specific user) and URLs with ageneric label.
All the original information was re-inserted after processing.
The tweets were tokenisedwith the UCTO rule-based tokeniser4 and then POS-tagged using TreeTagger (Schmid, 1994) with theprovided Italian parameter file.
Finally, we used themorphological analyser morph-it!
(Zanchetta andBaroni, 2005) for lemmatisation.3 Sentiment AnalysisThe aim of sentiment analysis (or opinion mining) isdetecting someone?s attitude, whether positive, neu-tral, or negative, on the basis of some utterance ortext s/he has produced.
While a first step would bedetermining whether a statement is objective or sub-jective, and then only in the latter case identify itspolarity, it is often the case that only the second taskis performed, thereby also collapsing objective state-ments and a neutral attitude.In SemEval-2013?s shared task on ?SentimentAnalysis in Twitter?5 (in English tweets), which iscurrently underway, systems must detect (i) polar-ity of a given word in a tweet, and (ii) polarity ofthe whole tweet, in terms of positive, negative, orneutral.
This is also what we set to do for Italian.We actually focus on (ii) in the sense that we do notevaluate (i), but we use and combine each word?spolarity to obtain the tweet?s overall polarity.Several avenues have been explored for polar-ity detection.
The simplest route is detecting thepresence of specific words which are known to ex-press a positive, negative or neutral feeling.
Forexample, O?Connor et al(2010) use a lexicon-projection strategy yielding predictions which sig-nificantly correlate with polls regarding ratings ofObama.
While it is clear that deeper linguistic anal-ysis should be performed for better results (Pang andLee, 2008), accurate processing is rather hard ontexts such as tweets, which are short, rich in abbrevi-ations and intra-genre expressions, and often syntac-tically ill-formed.
Additionally, existing tools for thesyntactic analysis of Italian, such as the DeSR parser(Attardi et al 2009), might not be robust enough forprocessing such texts.Exploiting information coming from a polarity4http://ilk.uvt.nl/ucto/5www.cs.york.ac.uk/semeval-2013/task2/.lexicon, we developed a simple system which as-signs to a given tweet one of three possible values:positive, neutral or negative.
The only input to thesystem is the prior polarity coded in the lexicon perword sense.
We experiment with several ways ofcombining all the polarities obtained for each word(sense) in a given tweet.
Performance is evaluatedagainst manually annotated tweets.3.1 Polarity lexicon for ItalianMost polarity detection systems make use, in someway, of an affection lexicon, i.e.
a language-specificresource which assigns a negative or positive priorpolarity to terms.
Such resources have been built byhand or derived automatically (Wilson et al 2005;Wiebe and Mihalcea, 2006; Esuli and Sebastiani,2006; Taboada et al 2011, e.g.).
To our knowl-edge, there isn?t such a resource already availablefor Italian.
Besides hand-crafting, there have beenproposals for creating resources for new languagesin a semi-automatic fashion, using manually anno-tated sets of seeds (Pitel and Grefenstette, 2008),or exploiting twitter emoticons directly (Pak andParoubek, 2011).
Rather than creating a new po-larity lexicon from scratch, we exploit three exist-ing resources, namely MultiWordNet (Pianta et al2002), SentiWordNet (Esuli and Sebastiani, 2006;Baccianella et al 2010), and WordNet itself (Fell-baum, 1998) to obtain an annotated lexicon of sensesfor Italian.
Basically, we port the SentiWordNet an-notation to the Italian portion of MultiWordNet, andwe do so in a completely automatic fashion.Our starting point is SentiWordNet, a versionof WordNet where the independent values positive,negative, and objective are associated to 117,660synsets, each value in the zero-one interval.
Mul-tiWordNet is a resource which aligns Italian and En-glish synsets and can thus be used to transfer polar-ity information associated to English synsets in Sen-tiWordNet to Italian synsets.
One obstacle is thatwhile SentiWordNet refers to WordNet 3.0, Multi-WordNet?s alignment holds for WordNet 1.6, andsynset reference indexes are not plainly carried overfrom one version to the next.
We filled this gap usingan automatically produced mapping between synsetsof Wordnet versions 1.6 and 3.0 (Daud et al 2000),making it possible to obtain SentiWordNet annota-tion for the Italian synsets of MultiWordNet.
The102coverage of our resource is however rather low com-pared to the English version, and this is due to thealignment procedure which must exploit an earlierversion of the resource.
The number of synsets isless than one third of that of SentiWordNet.3.2 Polarity assignmentGiven a tweet, our system assigns a polarity score toeach of its tokens by matching them to the entries inSentiWordNet.
Only matches of the correct POS areallowed.
The polarity score of the complete tweet isgiven by the sum of the polarity scores of its tokens.Polarity is associated to synsets, and the sameterm can occur in more than one synset.
One optionwould be to perform word sense disambiguation andonly pick the polarity score associated with the in-tended sense.
However, the structure of tweets andthe tools available for Italian do not make this op-tion actually feasible, although we might investigateit in the future.
As a working solution, we computethe positive and negative scores for a term occurringin a tweet as the means of the positive and negativescores of all synsets to which the lemma belongs toin our lexical resource.
The resulting polarity scoreof a lemma is the difference between its positive andnegative scores.
Whenever a lemma is not found inthe database, it is given a polarity score of 0.One underlying assumption to this approach isthat the different senses of a given word have simi-lar sentiment scores.
However, because this assump-tion might not be true in all cases, we introduce theconcept of ?polypathy?, which is the characterisingfeature of a term exhibiting high variance of polarityscores across its synsets.
The polypathy of a lemmais calculated as the standard deviation of the polar-ity scores of the possible senses.
This informationcan be used to remove highly polypathic words fromthe computation of the polarity of a complete tweet,for instance by discarding the tokens with a poly-pathy higher than a certain threshold.
In particular,for the experiments described in this paper, a thresh-old of 0.5 has been empirically determined.
To givean idea, among the most polypathic words in Senti-WordNet we found weird (.62), stunning (.61), con-flicting (.56), terrific (.56).Taboada et al(2011) also use SentiWordNet forpolarity detection, either taking the first sense of aterm (the most frequent in WordNet) or taking theaverage across senses, as we also do ?
althoughwe also add the polypathy-aware strategy.
We can-not use the first-sense strategy because through thealignment procedure senses are not ranked accord-ing to frequency anymore.3.3 Gold standardFor evaluating the system performance we createdtwo gold standard sets, both annotated by three inde-pendent native-speakers, who were given very sim-ple and basic instructions and performed the anno-tation via a web-based interface.
The value to beassigned to each tweet is one out of positive, neu-tral, or negative.
As mentioned, the neutral valueincludes both objective statements as well as subjec-tive statements where the twitter?s position is neutralor equally positive and negative at the same time (seealso (Esuli and Sebastiani, 2007)).All data selected for annotation comes fromTWITA.
The first dataset consists of 1,000 ran-domly selected tweets.
The second dataset is topic-oriented, i.e.
we randomly extracted 1,000 tweetsfrom all those containing a given topic.
Topic-oriented, or target-dependent (Jiang et al 2011),classification involves detecting opinions about aspecific target rather than detecting the more gen-eral opinion expressed in a given tweet.
We identifya topic through a given hashtag, and in this experi-ment we chose the tag ?Grillo?, the leader of an Ital-ian political movement.
While in the first set the an-notators were asked to assign a polarity value to themessage of the tweet as a whole, in the second setthe value was to be assigned to the author?s opinionconcerning the hashtag, in this case Beppe Grillo.This is a relevant distinction, since it can happenthat the tweet is, say, very negative about someoneelse while being positive or neutral about Grillo atthe same time.
For example, the tweet in (1), ex-presses a negative opinion about Vendola, anotherItalian politician, but is remaining quite neutral to-wards Grillo, the target of the annotation exercise.
(1) #Vendola da` del #populista a #Grillo e` unabarzelletta o ancora non si e` accorto che il#comunismo e` basato sul populismo?Thus, in the topic-specific set we operate a moresubtle distinction when assigning polarity, some-103thing which should make the task simpler for a hu-man annotator while harder for a shallow system.As shown in Table 1, for both sets the annotatorsdetected more than half of the tweets as neutral, orthey were disagreeing ?
without absolute majority,a tweet is considered neutral; however these casesaccount for only 7.7% in the generic set and 6.9% inthe topic-specific set.Table 1: Distribution of the tags assigned by the absolutemajority of the ratersset positive negative neutralgeneric 94 301 605topic-specific 293 145 562Inter-annotator agreement was measured via Fleiss?Kappa across three annotators.
On the generic set,we found an agreement of Kappa = 0.321, whileon the topic-specific set we found Kappa = 0.397.This confirms our expectation that annotating topic-specific tweets is actually an easier task.
We mightalso consider using more sophisticated and fine-grained sentiment annotation schemes which haveproved to be highly reliable in the annotation of En-glish data (Su and Markert, 2008a).3.4 EvaluationWe ran our system on both datasets described in Sec-tion 3.3, using all possible variations of two parame-ters, namely all combinations of part-of-speech tagsand the application of the threshold scheme, as dis-cussed in Section 3.2.
We measure overall accuracyas well as precision, recall, and f-score per polar-ity value.
In Tables 2 and 3, we report best scores,and indicate in brackets the associated POS combi-nation.
For instance, in Table 2, we can read that therecall of 0.701 for positive polarity is obtained whenthe system is run without polypathy threshold andusing nouns, verbs, and adjectives (nva).We can draw several observations from these re-sults.
First, a fully automatic approach that lever-ages existing lexical resources performs better thana wild guess.
Performance is boosted when highlypolypathic words are filtered out.Second, while the system performs well at recog-nising especially neutral but also positive polarity,it is really bad at detecting negative polarity.
Es-pecially in the topic-specific set, the system assignsTable 2: Best results on the generic set.
In brackets POScombination: (n)oun, (v)erb, (a)djective, adve(r)b.without polypathy threshold, best accuracy: 0.505 (a)positive negative neutralbest precision 0.440 (r) 0.195 (v) 0.664 (nar)best recall 0.701 (nva) 0.532 (var) 0.669 (a)best F-score 0.485 (nvar) 0.262 (vr) 0.647 (a)with polypathy threshold, best accuracy: 0.554 (r)positive negative neutralbest precision 0.420 (r) 0.233 (v) 0.685 (nar)best recall 0.714 (nvar) 0.457 (var) 0.785 (r)best F-score 0.492 (nar) 0.296 (vr) 0.698 (r)Table 3: Best results on the topic-specific set.
In bracketsPOS combination: (n)oun, (v)erb, (a)djective, adve(r)b.without polypathy threshold, best accuracy: 0.487 (r)positive negative neutralbest precision 0.164 (a) 0.412 (a) 0.617 (nar)best recall 0.593 (nva) 0.150 (nr) 0.724 (a)best f-score 0.251 (nv) 0.213 (nr) 0.637 (a)with polypathy threshold, best accuracy: 0.514 (r)positive negative neutralbest precision 0.163 (nvar) 0.414 (a) 0.623 (nar)best recall 0.593 (nvar) 0.106 (nar) 0.829 (r)best f-score 0.256 (nvar) 0.166 (nar) 0.676 (r)too many positive labels in place of negative ones,causing at the same time positive?s precision andnegative?s recall to drop.
We believe there are twoexplanations for this.
The first one is the ?positive-bias?
of SentiWordNet, as observed by Taboada etal.
(2011), which causes limited performance in theidentification of negative polarity.
The second oneis that we do not use any syntactic clues, such as fordetecting negated statements.
Including some strat-egy for dealing with this should improve recognitionof negative opinions, too.Third, the lower performance on the topic-specificdataset confirms the intuition that this task is harder,mainly because we operate a more subtle distinc-tion when assigning a polarity label as we refer toone specific subject.
Deeper linguistic analysis, suchas dependency parsing, might help, as only certainwords would result as related to the intended targetwhile others wouldn?t.As far as parts of speech are concerned, thereis a tendency for adverbs to be good indicators to-wards overall accuracy, and best scores are usuallyobtained exploiting adjectives and/or adverbs.1044 Related workWe have already discussed some related work con-cerning corpus creation, the development of anaffection lexicon, and the use of such polarity-annotated resources for sentiment analysis (Sec-tion 3).
As for results, because this is the first experi-ment on detecting polarity in Italian tweets, compar-ing performance is not straightforward.
Most workon sentiment analysis in tweets is on English, and al-though there exist relatively complex systems basedon statistical models, just using information from apolarity resource is rather common.
Su and Markert(2008b) test SentiWordNet for assigning a subjec-tivity judgement to word senses on a gold standardcorpus, observing an accuracy of 75.3%.
Given thatSentiWordNet is the automatic expansion over a setof manually annotated seeds, at word-level, this canbe considered as an upper bound in sense subjectiv-ity detection.
Taboada et al(2011) offer a survey oflexicon-based methods which are evaluated on ad-jectives only, by measuring overall accuracy againsta manually annotated set of words.
Using Senti-WordNet in a lexicon-projection fashion yields anaccuracy of 61.47% under best settings.
These arehowever scores on single words rather than wholesentences or microtexts.Considering that we assign polarity to tweetsrather than single words, and that in the creation ofour resource via automatic alignment we lose morethan two thirds of the original synsets (see Sec-tion 3.1), our results are promising.
They are alsonot that distant from results reported by Agarwal etal.
(2011), whose best system, a combination of un-igrams and the best set of features, achieves an ac-curacy of 60.50% on a three-way classification likeours, evaluated against a manually annotated set ofEnglish tweets.
Best f-scores reported for positive,negative, and neutral are comprised between 59%and 62%.
Similar results are obtained by Pak andParoubek (2010), who train a classifier on automati-cally tagged data, and evaluate their model on about200 English tweets.
Best reported f-score on a three-way polarity assignment is just over 60%.5 Conclusions and future workWe have presented the first corpus of Italian tweetsobtained in a completely automatic fashion, the firstpolarity lexicon for Italian, and the first experimenton sentiment analysis on Italian tweets using thesetwo resources.
Both the corpus and the lexicon areas of now unique resources for Italian, and were pro-duced in a way which is completely portable to otherlanguages.
In compliance with licensing terms of thesources we have used, our resources are made avail-able for research purposes after reviewing.Simply projecting the affection lexicon, using twodifferent polarity scoring methods, we experimentedwith detecting a generic sentiment expressed in a mi-crotext, and detecting the twitter?s opinion on a spe-cific topic.
As expected, we found that topic-specificclassification is harder for an automatic system as itmust discern what is said about the topic itself andwhat is said more generally or about another entitymentioned in the text.Indeed, this contribution can be seen as a firststep towards polarity detection in Italian tweets.
Theinformation we obtain from SentiWordNet and theways we combine it could obviously be used as fea-ture in a learning setting.
Other sources of infor-mation, to be used in combination with our polarityscores or integrated in a statistical model, are the so-called noisy labels, namely strings (such as emoti-cons or specific hashtags (Go et al 2009; Davi-dov et al 2010)) that can be taken as positive ornegative polarity indicators as such.
Speriosu et al(2011) have shown that training a maximum entropyclassier using noisy labels as class predictors in thetraining set yields an improvement of about threepercentage points over a lexicon-based prediction.Another important issue to deal with is figurativelanguage.
During manual annotation we have en-countered many cases of irony or sarcasm, which is aphenomenon that must be obviously tackled.
Therehave been attempts at identifying it automatically inthe context of tweets (Gonza?lez-Iba?n?ez et al 2011),and we plan to explore this issue in future work.Finally, the co-presence of meta and linguisticinformation allows for a wide range of linguisticqueries and statistical analyses on the whole of thecorpus, also independently of sentiment informa-tion, of course.
For example, correlations betweenparts-of-speech and polarity have been found (Pakand Paroubek, 2010), and one could expect alsocorrelations with sentiment and time of the day, ormonth of the year, and so on.105AcknowledgmentsWe would like to thank Manuela, Marcella e Silviafor their help with annotation, and the reviewers fortheir useful comments.
All errors remain our own.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof twitter data.
In Proceedings of the Workshop onLanguages in Social Media, LSM ?11, pages 30?38,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, andJoseph Turian.
2009.
Accurate dependency parsingwith a stacked multilayer perceptron.
In Proceedingof Evalita 2009, LNCS.
Springer.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InNicoletta Calzolari et al editor, Proceedings of LREC2010.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Jordi Daud, Llus Padr, and German Rigau.
2000.
Map-ping wordnets using structural information.
In 38thAnnual Meeting of the Association for ComputationalLinguistics (ACL?2000)., Hong Kong.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 241?249, Stroudsburg, PA, USA.Association for Computational Linguistics.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinionmining.
In In Proceedings of the 5th Conference onLanguage Resources and Evaluation (LREC06, pages417?422.Andrea Esuli and Fabrizio Sebastiani.
2007.
Pagerank-ing wordnet synsets: An application to opinion min-ing.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages424?431, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Christiane Fellbaum, editor.
1998.
WordNet.
An Elec-tronic Lexical Database.
The MIT Press.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twittersentiment analysis using distant supervision.
http://cs.wmich.edu/?tllake/fileshare/TwitterDistantSupervision09.pdf.Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and NinaWacholder.
2011.
Identifying sarcasm in twitter: Acloser look.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 581?586, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent twitter sentiment clas-sification.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 151?160, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In ACL (Sys-tem Demonstrations), pages 25?30.
The Associationfor Computer Linguistics.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In William W. Cohen andSamuel Gosling, editors, Proceedings of the FourthInternational Conference on Weblogs and SocialMedia, ICWSM 2010, Washington, DC, USA, May23-26.
The AAAI Press.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Nicoletta Calzolari et al editor, Proceedings of theInternational Conference on Language Resources andEvaluation, LREC 2010, 17-23 May 2010, Valletta,Malta.
European Language Resources Association.Alexander Pak and Patrick Paroubek.
2011.
Twitter forsentiment analysis: When language resources are notavailable.
23rd International Workshop on Databaseand Expert Systems Applications, 0:111?115.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Emanuele Pianta, Luisa Bentivogli, and Christian Gi-rardi.
2002.
MultiWordNet: developing an alignedmultilingual database.
In Proceedings of the First In-ternational Conference on Global WordNet, pages 21?25.Guillaume Pitel and Gregory Grefenstette.
2008.
Semi-automatic building method for a multidimensional af-fect dictionary for a new language.
In Proceedings ofLREC 2008.106Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-son Baldridge.
2011.
Twitter polarity classificationwith label propagation over lexical links and the fol-lower graph.
In Proceedings of the First workshop onUnsupervised Learning in NLP, pages 53?63, Edin-burgh, Scotland, July.
Association for ComputationalLinguistics.Fangzhong Su and Katja Markert.
2008a.
Eliciting sub-jectivity and polarity judgements on word senses.
InProceedings of COLING 2008 Workshop on HumanJudgements in Computational Linguistics, Manch-ester, UK.Fangzhong Su and Katja Markert.
2008b.
From wordsto senses: A case study of subjectivity recognition.
InDonia Scott and Hans Uszkoreit, editors, Proceedingsof COLING 2008, Manchester, UK, pages 825?832.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-basedmethods for sentiment analysis.
Comput.
Linguist.,37(2):267?307, June.Erik Tjong Kim Sang and Johan Bos.
2012.
Predictingthe 2011 dutch senate election results with twitter.
InProceedings of the Workshop on Semantic Analysis inSocial Media, pages 53?60, Avignon, France, April.Association for Computational Linguistics.Janyce Wiebe and Rada Mihalcea.
2006.
Word senseand subjectivity.
In Nicoletta Calzolari, Claire Cardie,and Pierre Isabelle, editors, ACL.
The Association forComputer Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the Human Lan-guage Technology and Empirical Methods in NaturalLanguage Processing Conference, 6-8 October, Van-couver, British Columbia, Canada.Eros Zanchetta and Marco Baroni.
2005.
Morph-it!
afree corpus-based morphological resource for the ital-ian language.
In Proceedings of Corpus Linguistics2005.107
