Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294?303,Honolulu, October 2008. c?2008 Association for Computational LinguisticsUnderstanding the Value of Features for Coreference ResolutionEric Bengtson Dan RothDepartment of Computer ScienceUniversity of IllinoisUrbana, IL 61801{ebengt2,danr}@illinois.eduAbstractIn recent years there has been substantial workon the important problem of coreference res-olution, most of which has concentrated onthe development of new models and algo-rithmic techniques.
These works often showthat complex models improve over a weakpairwise baseline.
However, less attentionhas been given to the importance of selectingstrong features to support learning a corefer-ence model.This paper describes a rather simple pair-wise classification model for coreference res-olution, developed with a well-designed setof features.
We show that this produces astate-of-the-art system that outperforms sys-tems built with complex models.
We suggestthat our system can be used as a baseline forthe development of more complex models ?which may have less impact when a more ro-bust set of features is used.
The paper alsopresents an ablation study and discusses therelative contributions of various features.1 IntroductionCoreference resolution is the task of grouping all thementions of entities1 in a document into equivalenceclasses so that all the mentions in a given class referto the same discourse entity.
For example, given thesentence (where the head noun of each mention issubscripted)1We follow the ACE (NIST, 2004) terminology: A nounphrase referring to a discourse entity is called a mention, andan equivalence class is called an entity.An American1 official2 announced thatAmerican1 President3 Bill Clinton3 methis3 Russian4 counterpart5, VladimirPutin5, today.the task is to group the mentions so that those refer-ring to the same entity are placed together into anequivalence class.Many NLP tasks detect attributes, actions, andrelations between discourse entities.
In order todiscover all information about a given entity, tex-tual mentions of that entity must be grouped to-gether.
Thus coreference is an important prerequi-site to such tasks as textual entailment and informa-tion extraction, among others.Although coreference resolution has receivedmuch attention, that attention has not focused on therelative impact of high-quality features.
Thus, whilemany structural innovations in the modeling ap-proach have been made, those innovations have gen-erally been tested on systems with features whosestrength has not been established, and compared toweak pairwise baselines.
As a result, it is possiblethat some modeling innovations may have less im-pact or applicability when applied to a stronger base-line system.This paper introduces a rather simple but state-of-the-art system, which we intend to be used as astrong baseline to evaluate the impact of structuralinnovations.
To this end, we combine an effectivecoreference classification model with a strong set offeatures, and present an ablation study to show therelative impact of a variety of features.As we show, this combination of a pairwisemodel and strong features produces a 1.5 percent-294age point increase in B-Cubed F-Score over a com-plex model in the state-of-the-art system by Culottaet al (2007), although their system uses a complex,non-pairwise model, computing features over partialclusters of mentions.2 A Pairwise Coreference ModelGiven a document and a set of mentions, corefer-ence resolution is the task of grouping the mentionsinto equivalence classes, so that each equivalenceclass contains exactly those mentions that refer tothe same discourse entity.
The number of equiv-alence classes is not specified in advance, but isbounded by the number of mentions.In this paper, we view coreference resolution asa graph problem: Given a set of mentions and theircontext as nodes, generate a set of edges such thatany two mentions that belong in the same equiva-lence class are connected by some path in the graph.We construct this entity-mention graph by learningto decide for each mention which preceding men-tion, if any, belongs in the same equivalence class;this approach is commonly called the pairwise coref-erence model (Soon et al, 2001).
To decide whethertwo mentions should be linked in the graph, we learna pairwise coreference function pc that produces avalue indicating the probability that the two men-tions should be placed in the same equivalence class.The remainder of this section first discusses howthis function is used as part of a document-levelcoreference decision model and then describes howwe learn the pc function.2.1 Document-Level Decision ModelGiven a document d and a pairwise coreference scor-ing function pc that maps an ordered pair of men-tions to a value indicating the probability that theyare coreferential (see Section 2.2), we generate acoreference graph Gd according to the Best-Link de-cision model (Ng and Cardie, 2002b) as follows:For each mention m in document d, let Bm be theset of mentions appearing before m in d. Let a bethe highest scoring antecedent:a = argmaxb?Bm(pc(b,m)).If pc(a,m) is above a threshold chosen as describedin Section 4.4, we add the edge (a,m) to the coref-erence graph Gd.The resulting graph contains connected compo-nents, each representing one equivalence class, withall the mentions in the component referring to thesame entity.
This technique permits us to learn todetect some links between mentions while being ag-nostic about whether other mentions are linked, andyet via the transitive closure of all links we can stilldetermine the equivalence classes.We also require that no non-pronoun can referback to a pronoun: If m is not a pronoun, we donot consider pronouns as candidate antecedents.2.1.1 Related ModelsFor pairwise models, it is common to choose thebest antecedent for a given mention (thereby impos-ing the constraint that each mention has at most oneantecedent); however, the method of deciding whichis the best antecedent varies.Soon et al (2001) use the Closest-Link method:They select as an antecedent the closest preced-ing mention that is predicted coreferential by apairwise coreference module; this is equivalent tochoosing the closest mention whose pc value isabove a threshold.
Best-Link was shown to out-perform Closest-Link in an experiment by Ng andCardie (2002b).
Our model differs from that of Ngand Cardie in that we impose the constraint thatnon-pronouns cannot refer back to pronouns, and inthat we use as training examples all ordered pairs ofmentions, subject to the constraint above.Culotta et al (2007) introduced a model that pre-dicts whether a pair of equivalence classes should bemerged, using features computed over all the men-tions in both classes.
Since the number of possi-ble classes is exponential in the number of mentions,they use heuristics to select training examples.
Ourmethod does not require determining which equiva-lence classes should be considered as examples.2.2 Pairwise Coreference FunctionLearning the pairwise scoring function pc is a cru-cial issue for the pairwise coreference model.
Weapply machine learning techniques to learn from ex-amples a function pc that takes as input an orderedpair of mentions (a,m) such that a precedes m inthe document, and produces as output a value that is295interpreted as the conditional probability that m anda belong in the same equivalence class.2.2.1 Training Example SelectionThe ACE training data provides the equivalenceclasses for mentions.
However, for some pairs ofmentions from an equivalence class, there is little orno direct evidence in the text that the mentions arecoreferential.
Therefore, training pc on all pairs ofmentions within an equivalence class may not leadto a good predictor.
Thus, for each mention m weselect from m?s equivalence class the closest pre-ceding mention a and present the pair (a,m) as apositive training example, under the assumption thatthere is more direct evidence in the text for the ex-istence of this edge than for other edges.
This issimilar to the technique of Ng and Cardie (2002b).For each m, we generate negative examples (a,m)for all mentions a that precede m and are not in thesame equivalence class.
Note that in doing so wegenerate more negative examples than positive ones.Since we never apply pc to a pair where the firstmention is a pronoun and the second is not a pro-noun, we do not train on examples of this form.2.2.2 Learning Pairwise Coreference ScoringModelWe learn the pairwise coreference function usingan averaged perceptron learning algorithm (Freundand Schapire, 1998) ?
we use the regularized versionin Learning Based Java2 (Rizzolo and Roth, 2007).3 FeaturesThe performance of the document-level coreferencemodel depends on the quality of the pairwise coref-erence function pc.
Beyond the training paradigmdescribed earlier, the quality of pc depends on thefeatures used.We divide the features into categories, based ontheir function.
A full list of features and their cat-egories is given in Table 2.
In addition to theseboolean features, we also use the conjunctions of allpairs of features.32LBJ code is available at http://L2R.cs.uiuc.edu/?cogcomp/asoftware.php?skey=LBJ3The package of all features used is available athttp://L2R.cs.uiuc.edu/?cogcomp/asoftware.php?skey=LBJ#features.In the following description, the term head meansthe head noun phrase of a mention; the extent is thelargest noun phrase headed by the head noun phrase.3.1 Mention TypesThe type of a mention indicates whether it is a propernoun, a common noun, or a pronoun.
This feature,when conjoined with others, allows us to give dif-ferent weight to a feature depending on whether it isbeing applied to a proper name or a pronoun.
Forour experiments in Section 5, we use gold mentiontypes as is done by Culotta et al (2007) and Luo andZitouni (2005).Note that in the experiments described in Sec-tion 6 we predict the mention types as describedthere and do not use any gold data.
The mentiontype feature is used in all experiments.3.2 String Relation FeaturesString relation features indicate whether two stringsshare some property, such as one being the substringof another or both sharing a modifier word.
Featuresare listed in Table 1.
Modifiers are limited to thoseoccurring before the head.Feature DefinitionHead match headi == headjExtent match extenti == extentjSubstring headi substring of headjModifiers Match modi == (headj or modj)Alias acronym(headi) == headjor lastnamei == lastnamejTable 1: String Relation Features3.3 Semantic FeaturesAnother class of features captures the semantic re-lation between two words.
Specifically, we checkwhether gender or number match, or whether thementions are synonyms, antonyms, or hypernyms.We also check the relationship of modifiers thatshare a hypernym.
Descriptions of the methods forcomputing these features are described next.Gender Match We determine the gender (male,female, or neuter) of the two phrases, and reportwhether they match (true, false, or unknown).
For296Category Feature SourceMention Types Mention Type Pair Annotation and tokensString Relations Head Match TokensExtent Match TokensSubstring TokensModifiers Match TokensAlias Tokens and listsSemantic Gender Match WordNet and listsNumber Match WordNet and listsSynonyms WordNetAntonyms WordNetHypernyms WordNetBoth Speak ContextRelative Location Apposition Positions and contextRelative Pronoun Positions and tokensDistances PositionsLearned Anaphoricity LearnedName Modifiers Predicted Match LearnedAligned Modifiers Aligned Modifiers Relation WordNet and listsMemorization Last Words TokensPredicted Entity Types Entity Types Match Annotation and tokensEntity Type Pair WordNet and tokensTable 2: Features by Categorya proper name, gender is determined by the exis-tence of mr, ms, mrs, or the gender of the first name.If only a last name is found, the phrase is consid-ered to refer to a person.
If the name is found ina comprehensive list of cities or countries, or endswith an organization ending such as inc, then thegender is neuter.
In the case of a common nounphrase, the phrase is looked up in WordNet (Fell-baum, 1998), and it is assigned a gender according towhether male, female, person, artifact, location, orgroup (the last three correspond to neuter) is foundin the hypernym tree.
The gender of a pronoun islooked up in a table.Number Match Number is determined as fol-lows: Phrases starting with the words a, an, or thisare singular; those, these, or some indicate plural.Names not containing and are singular.
Commonnouns are checked against extensive lists of singularand plural nouns ?
words found in neither or bothlists have unknown number.
Finally, if the num-ber is unknown yet the two mentions have the samespelling, they are assumed to have the same number.WordNet Features We check whether any senseof one head noun phrase is a synonym, antonym, orhypernym of any sense of the other.
We also checkwhether any sense of the phrases share a hypernym,after dropping entity, abstraction, physical entity,object, whole, artifact, and group from the senses,since they are close to the root of the hypernym tree.Modifiers Match Determines whether the text be-fore the head of a mention matches the head or thetext before the head of the other mention.Both Mentions Speak True if both mentions ap-pear within two words of a verb meaning to say.
Be-ing in a window of size two is an approximation tobeing a syntactic subject of such a verb.
This featureis a proxy for having similar semantic types.3.4 Relative Location FeaturesAdditional evidence is derived from the relative lo-cation of the two mentions.
We thus measure dis-tance (quantized as multiple boolean features of the297form [distance ?
i]) for all i up to the distance andless than some maximum, using units of compatiblementions, and whether the mentions are in the samesentence.
We also detect apposition (mentions sepa-rated by a comma).
For details, see Table 3.Feature DefinitionDistance In same sentence# compatible mentionsApposition m1 ,m2 foundRelative Pronoun Apposition and m2 is PROTable 3: Location Features.
Compatible mentions arethose having the same gender and number.3.5 Learned FeaturesModifier Names If the mentions are both mod-ified by other proper names, use a basic corefer-ence classifier to determine whether the modifiersare coreferential.
This basic classifier is trainedusing Mention Types, String Relations, SemanticFeatures, Apposition, Relative Pronoun, and BothSpeak.
For each mention m, examples are generatedwith the closest antecedent a to form a positive ex-ample, and every mention between a and m to formnegative examples.Anaphoricity Ng and Cardie (2002a) and Denisand Baldridge (2007) show that when used effec-tively, explicitly predicting anaphoricity can be help-ful.
Thus, we learn a separate classifier to detectwhether a mention is anaphoric (that is, whether itis not the first mention in its equivalence class), anduse that classifier?s output as a feature for the coref-erence model.
Features for the anaphoricity classi-fier include the mention type, whether the mentionappears in a quotation, the text of the first word ofthe extent, the text of the first word after the head(if that word is part of the extent), whether there isa longer mention preceding this mention and havingthe same head text, whether any preceding mentionhas the same extent text, and whether any precedingmention has the same text from beginning of the ex-tent to end of the head.
Conjunctions of all pairs ofthese features are also used.
This classifier predictsanaphoricity with about 82% accuracy.3.6 Aligned ModifiersWe determine the relationship of any pair of modi-fiers that share a hypernym.
Each aligned pair mayhave one of the following relations: match, sub-string, synonyms, hypernyms, antonyms, or mis-match.
Mismatch is defined as none of the above.We restrict modifiers to single nouns and adjectivesoccurring before the head noun phrase.3.7 Memorization FeaturesWe allow our system to learn which pairs of nounstend to be used to mention the same entity.
For ex-ample, President and he often refer to Bush but sheand Prime Minister rarely do, if ever.
To enable thesystem to learn such patterns, we treat the presenceor absence of each pair of final head nouns, one fromeach mention of an example, as a feature.3.8 Predicted Entity TypeWe predict the entity type (person, organization,geo-political entity, location, facility, weapon, or ve-hicle) as follows: If a proper name, we check a list ofpersonal first names, and a short list of honorary ti-tles (e.g.
mr) to determine if the mention is a person.Otherwise we look in lists of personal last namesdrawn from US census data, and in lists of cities,states, countries, organizations, corporations, sportsteams, universities, political parties, and organiza-tion endings (e.g.
inc or corp).
If found in exactlyone list, we return the appropriate type.
We returnunknown if found in multiple lists because the listsare quite comprehensive and may have significantoverlap.For common nouns, we look at the hypernym treefor one of the following: person, political unit, loca-tion, organization, weapon, vehicle, industrial plant,and facility.
If any is found, we return the appropri-ate type.
If multiple are found, we sort as in theabove list.For personal pronouns, we recognize the entity asa person; otherwise we specify unknown.This computation is used as part of the followingtwo features.Entity Type Match This feature checks to seewhether the predicted entity types match.
The resultis true if the types are identical, false if they are dif-ferent, and unknown if at least one type is unknown.298Entity Type Conjunctions This feature indicatesthe presence of the pair of predicted entity types forthe two mentions, except that if either word is a pro-noun, the word token replaces the type in the pair.Since we do this replacement for entity types, wealso add a similar feature for mention types here.These features are boolean: For any given pair, afeature is active if that pair describes the example.3.9 Related WorkMany of our features are similar to those describedin Culotta et al (2007).
This includes MentionTypes, String Relation Features, Gender and Num-ber Match, WordNet Features, Alias, Apposition,Relative Pronoun, and Both Mentions Speak.
Theimplementations of those features may vary fromthose of other systems.
Anaphoricity has been pro-posed as a part of the model in several systems, in-cluding Ng and Cardie (2002a), but we are not awareof it being used as a feature for a learning algorithm.Distances have been used in e.g.
Luo et al (2004).However, we are not aware of any system using thenumber of compatible mentions as a distance.4 Experimental Study4.1 CorpusWe use the official ACE 2004 English trainingdata (NIST, 2004).
Much work has been done oncoreference in several languages, but for this workwe focus on English text.
We split the corpus intothree sets: Train, Dev, and Test.
Our test set containsthe same 107 documents as Culotta et al (2007).Our training set is a random 80% of the 336 doc-uments in their training set and our Dev set is theremaining 20%.For our ablation study, we further randomly splitour development set into two evenly sized parts,Dev-Tune and Dev-Eval.
For each experiment, weset the parameters of our algorithm to optimize B-Cubed F-Score using Dev-Tune, and use those pa-rameters to evaluate on the Dev-Eval data.4.2 PreprocessingFor the experiments in Section 5, following Culottaet al (2007), to make experiments more compara-ble across systems, we assume that perfect mentionboundaries and mention type labels are given.
Wedo not use any other gold annotated input at evalu-ation time.
In Section 6 experiments we do not useany gold annotated input and do not assume mentiontypes or boundaries are given.
In all experiments weautomatically split words and sentences using ourpreprocessing tools.44.3 Evaluation ScoresB-Cubed F-Score We evaluate over the com-monly used B-Cubed F-Score (Bagga and Baldwin,1998), which is a measure of the overlap of predictedclusters and true clusters.
It is computed as the har-monic mean of precision (P ),P =1N?d?D???m?d(cmpm)??
, (1)and recall (R),R =1N?d?D???m?d(cmtm)??
, (2)where cm is the number of mentions appearingboth in m?s predicted cluster and in m?s true clus-ter, pm is the size of the predicted cluster containingm, and tm is the size of m?s true cluster.
Finally, drepresents a document from the set D, and N is thetotal number of mentions in D.B-Cubed F-Score has the advantage of being ableto measure the impact of singleton entities, and ofgiving more weight to the splitting or merging oflarger entities.
It also gives equal weight to all typesof entities and mentions.
For these reasons, we re-port our results using B-Cubed F-Score.MUC F-Score We also provide results using theofficial MUC scoring algorithm (Vilain et al, 1995).The MUC F-score is also the harmonic mean ofprecision and recall.
However, the MUC precisioncounts precision errors by computing the minimumnumber of links that must be added to ensure that allmentions referring to a given entity are connectedin the graph.
Recall errors are the number of linksthat must be removed to ensure that no two men-tions referring to different entities are connected inthe graph.4The code is available at http://L2R.cs.uiuc.edu/?cogcomp/tools.php2994.4 Learning Algorithm DetailsWe train a regularized average perceptron using ex-amples selected as described in Section 2.2.1.
Thelearning rate is 0.1 and the regularization parameter(separator thickness) is 3.5.
At training time, we usea threshold of 0.0, but when evaluating, we select pa-rameters to optimize B-Cubed F-Score on a held-outdevelopment set.
We sample all even integer thresh-olds from -16 to 8.
We choose the number of roundsof training similarly, allowing any number from oneto twenty.5 ResultsPrecision Recall B3 FCulotta et al 86.7 73.2 79.3Current Work 88.3 74.5 80.8Table 4: Evaluation on unseen Test Data using B3 score.Shows that our system outperforms the advanced systemof Culotta et al The improvement is statistically signifi-cant at the p = 0.05 level according to a non-parametricbootstrapping percentile test.In Table 4, we compare our performance againsta system that is comparable to ours: Both use goldmention boundaries and types, evaluate using B-Cubed F-Score, and have the same training and testdata split.
Culotta et al (2007) is the best compara-ble system of which we are aware.Our results show that a pairwise model withstrong features outperforms a state-of-the-art systemwith a more complex model.MUC Score We evaluate the performance of oursystem using the official MUC score in Table 5.MUC Precision MUC Recall MUC F82.7 69.9 75.8Table 5: Evaluation of our system on unseen Test Datausing MUC score.5.1 Analysis of Feature ContributionsIn Table 6 we show the relative impact of variousfeatures.
We report data on Dev-Eval, to avoid thepossibility of overfitting by feature selection.
Theparameters of the algorithm are chosen to maximizethe BCubed F-Score on the Dev-Tune data.
Notethat since we report results on Dev-Eval, the resultsin Table 6 are not directly comparable with Culottaet al (2007).
For comparable results, see Table 4and the discussion above.Our ablation study shows the impact of variousclasses of features, indicating that almost all the fea-tures help, although some more than others.
It alsoillustrates that some features contribute more to pre-cision, others more to recall.
For example, alignedmodifiers contribute primarily to precision, whereasour learned features and our apposition features con-tribute to recall.
This information can be usefulwhen designing a coreference system in an applica-tion where recall is more important than precision,or vice versa.We examine the effect of some important features,selecting those that provide a substantial improve-ment in precision, recall, or both.
For each suchfeature we examine the rate of coreference amongstmention pairs for which the feature is active, com-pared with the overall rate of coreference.
We alsoshow examples on which the coreference systemsdiffer depending on the presence or absence of a fea-ture.Apposition This feature checks whether two men-tions are separated by only a comma, and it in-creases B-Cubed F-Score by about one percentagepoint.
We hypothesize that proper names and com-mon noun phrases link primarily through apposition,and that apposition is thus a significant feature forgood coreference resolution.When this feature is active 36% of the examplesare coreferential, whereas only 6% of all examplesare coreferential.
Looking at some examples oursystem begins to get right when apposition is added,we find the phraseIsrael?s Deputy Defense Minister,Ephraim Sneh.Upon adding apposition, our system begins to cor-rectly associate Israel?s Deputy Defense Ministerwith Ephraim Sneh.
Likewise in the phraseThe court president, Ronald Sutherland,the system correctly associates The court presidentwith Ronald Sutherland when they appear in an ap-positive relation in the text.
In addition, our system300Precision Recall B-Cubed FString Similarity 86.88 67.17 75.76+ Semantic Features 85.34 69.30 76.49+ Apposition 89.77 67.53 77.07+ Relative Pronoun 88.76 68.97 77.62+ Distances 89.62 71.93 79.81+ Learned Features 87.37 74.51 80.43+ Aligned Modifiers 88.70 74.30 80.86+ Memorization 86.57 75.59 80.71+ Predicted Entity Types 87.92 76.46 81.79Table 6: Contribution of Features as evaluated on a development set.
Bold results are significantly better than theprevious line at the p = 0.05 level according to a paired non-parametric bootstrapping percentile test.
These resultsshow the importance of Distance, Entity Type, and Apposition features.begins correctly associating relative pronouns suchas who with their referents in phrases likeSheikh Abbad, who died 500 years ago.although an explicit relative pronoun feature isadded only later.Although this feature may lead the system to linkcomma separated lists of entities due to misinter-pretation of the comma, for example Wyoming andwestern South Dakota in a list of locations, we be-lieve this can be avoided by refining the appositionfeature to ignore lists.Relative Pronoun Next we investigate the relativepronoun feature.
With this feature active, 93% ofexamples were positive, indicating the precision ofthis feature.
Looking to examples, we find who inthe official, who wished to remain anony-mousis properly linked, as is that innuclear warheads that can be fitted to mis-siles.Distances Our distance features measure separa-tion of two mentions in number of compatible men-tions (quantized), and whether the mentions are inthe same sentence.
Distance features are importantfor a system that makes links based on the best pair-wise coreference value rather than implicitly incor-porating distance by linking only the closest pairwhose score is above a threshold, as done by e.g.Soon et al (2001).Looking at examples, we find that adding dis-tances allows the system to associate the pronounit with this missile not separated by any mentions,rather than Tehran, which is separated from it bymany mentions.Predicted Entity Types Since no two mentionscan have different entity types (person, organization,geo-political entity, etc.)
and be coreferential, thisfeature has strong discriminative power.
When theentity types match, 13% of examples are positivecompared to only 6% of examples in general.
Qual-itatively, the entity type prediction correctly recog-nizes the Gulf region as a geo-political entity, andHe as a person, and thus prevents linking the two.Likewise, the system discerns Baghdad from am-bassador due to the entity type.
However, in somecases an identity type match can cause the system tobe overly confident in a bad match, as in the case ofa palestinian state identified with holy Jerusalem onthe basis of proximity and shared entity type.
Thistype of example may require some additional worldknowledge or deeper comprehension of the docu-ment.6 End-to-End CoreferenceThe ultimate goal for a coreference system is toprocess unannotated text.
We use the term end-to-end coreference for a system capable of determin-ing coreference on plain text.
We describe the chal-lenges associated with an end-to-end system, de-scribe our approach, and report results below.3016.1 ChallengesDeveloping an end-to-end system requires detectingand classifying mentions, which may degrade coref-erence results.
One challenge in detecting mentionsis that they are often heavily nested.
Additionally,there are issues with evaluating an end-to-end sys-tem against a gold standard corpus, resulting fromthe possibility of mismatches in mention boundaries,missing mentions, and additional mentions detected,along with the need to align detected mentions totheir counterparts in the annotated data.6.2 ApproachWe resolve coreference on unannotated text as fol-lows: First we detect mention heads following astate of the art chunking approach (Punyakanok andRoth, 2001) using standard features.
This results ina 90% F1 head detector.
Next, we detect the extentboundaries for each head using a learned classifier.This is followed by determining whether a mentionis a proper name, common noun phrase, prenominalmodifier, or pronoun using a learned mention typeclassifier that.
Finally, we apply our coreference al-gorithm described above.6.3 Evaluation and ResultsTo evaluate, we align the heads of the detected men-tions to the gold standard heads greedily based onnumber of overlapping words.
We choose not toimpute errors to the coreference system for men-tions that were not detected or for spuriously de-tected mentions (following Ji et al (2005) and oth-ers).
Although this evaluation is lenient, given thatthe mention detection component performs at over90% F1, we believe it provides a realistic measurefor the performance of the end-to-end system and fo-cuses the evaluation on the coreference component.The results of our end-to-end coreference system areshown in Table 7.Precision Recall B3 FEnd-to-End System 84.91 72.53 78.24Table 7: Coreference results using detected mentions onunseen Test Data.7 ConclusionWe described and evaluated a state-of-the-art coref-erence system based on a pairwise model and strongfeatures.
While previous work showed the impactof complex models on a weak pairwise baseline, theapplicability and impact of such models on a strongbaseline system such as ours remains uncertain.
Wealso studied and demonstrated the relative value ofvarious types of features, showing in particular theimportance of distance and apposition features, andshowing which features impact precision or recallmore.
Finally, we showed an end-to-end system ca-pable of determining coreference in a plain text doc-ument.AcknowledgmentsWe would like to thank Ming-Wei Chang, MichaelConnor, Alexandre Klementiev, Nick Rizzolo,Kevin Small, and the anonymous reviewers for theirinsightful comments.
This work is partly supportedby NSF grant SoD-HCER-0613885 and a grant fromBoeing.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In MUC7.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference reso-lution.
In HLT/NAACL, pages 81?88.P.
Denis and J. Baldridge.
2007.
Joint determinationof anaphoricity and coreference resolution using in-teger programming.
In HLT/NAACL, pages 236?243,Rochester, New York.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Y.
Freund and R. E. Schapire.
1998.
Large margin clas-sification using the Perceptron algorithm.
In COLT,pages 209?217.H.
Ji, D. Westbrook, and R. Grishman.
2005.
Us-ing semantic relations to refine coreference decisions.In EMNLP/HLT, pages 17?24, Vancouver, BritishColumbia, Canada.X.
Luo and I. Zitouni.
2005.
Multi-lingual coreferenceresolution with syntactic features.
In HLT/EMNLP,pages 660?667, Vancouver, British Columbia, Canada.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous corefer-ence resolution algorithm based on the bell tree.
InACL, page 135, Morristown, NJ, USA.302V.
Ng and C. Cardie.
2002a.
Identifying anaphoric andnon-anaphoric noun phrases to improve coreferenceresolution.
In COLING-2002.V.
Ng and C. Cardie.
2002b.
Improving machine learn-ing approaches to coreference resolution.
In ACL.NIST.
2004.
The ace evaluation plan.www.nist.gov/speech/tests/ace/index.htm.V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In The Conference onAdvances in Neural Information Processing Systems(NIPS), pages 995?1001.
MIT Press.N.
Rizzolo and D. Roth.
2007.
Modeling DiscriminativeGlobal Inference.
In Proceedings of the First Inter-national Conference on Semantic Computing (ICSC),pages 597?604, Irvine, California.W.
M. Soon, H. T. Ng, and C. Y. Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Computational Linguistics, 27(4):521?544.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In MUC6, pages 45?52.303
