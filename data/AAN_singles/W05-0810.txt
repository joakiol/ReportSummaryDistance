Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 75?78,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005NUKTI: English-Inuktitut Word Alignment System DescriptionPhilippe Langlais, Fabrizio Gotti, Guihong CaoRALIDe?partement d?Informatique et de Recherche Ope?rationnelleUniversite?
de Montre?alSuccursale Centre-VilleH3C 3J7 Montre?al, Canadahttp://rali.iro.umontreal.caAbstractMachine Translation (MT) as well asother bilingual applications stronglyrely on word alignment.
Efficient align-ment techniques have been proposedbut are mainly evaluated on pairs oflanguages where the notion of wordis mostly clear.
We concentrated oureffort on the English-Inuktitut wordalignment shared task and report ontwo approaches we implemented and acombination of both.1 IntroductionWord alignment is an important step in exploitingparallel corpora.
When efficient techniques havebeen proposed (Brown et al, 1993; Och and Ney,2003), they have been mostly evaluated on ?safe?pairs of languages where the notion of word israther clear.We devoted two weeks to the intriguing taskof aligning at the word level pairs of sentencesof English and Inuktitut.
We experimented withtwo different approaches.
For the first one, we re-lied on an in-house sentence alignment program(JAPA) where English and Inuktitut tokens wereconsidered as sentences.
The second approachwe propose takes advantage of associations com-puted between any English word and roughly anysubsequence of Inuktitut characters seen in thetraining corpus.
We also investigated the combi-nation of both approaches.2 JAPA: Word Alignment as a SentenceAlignment TaskTo adjust our systems, the organizers made avail-able to the participants a set of 25 pairs of sen-tences where words had been manually aligned.A fast inspection of this material reveals that inmost of the cases, the alignment produced aremonotonic and involve cepts of n adjacent En-glish words aligned to a single Inuktitut word.Many sentence alignment techniques stronglyrely on the monotonic nature of the inherent align-ment.
Therefore, we conducted a first experi-ment using an in-house sentence alignment pro-gram called JAPA that we developed within theframework of the Arcade evaluation campaign(Langlais et al, 1998).
The implementation de-tails of this aligner can be found in (Langlais,1997), but in a few words, JAPA aligns pairs ofsentences by first grossly aligning their words(making use of either cognate-like tokens, or aspecified bilingual dictionary).
A second passaligns the sentences in a way similar1 to the algo-rithm described by Gale and Church (1993), butwhere the search space is constrained to be closeto the one delimited by the word alignment.
Thistechnique happened to be among the most accu-rate of the ones tested during the Arcade exercise.To adapt JAPA to our needs, we only did twothings.
First, we considered single sentences asdocuments, and tokens as sentences (we definea token as a sequence of characters delimited by1In our case, the score we seek to globally maximize bydynamic programming is not only taking into account thelength criteria described in (Gale and Church, 1993) but alsoa cognate-based one similar to (Simard et al, 1992).751-1 0.406 4-1 0.092 4-2 0.0152-1 0.172 5-1 0.038 5-2 0.0113-1 0.123 7-1 0.027 3-2 0.011Table 1: The 9 most frequent English-Inuktitutpatterns observed on the development set.
A totalof 24 different patterns have been observed.white space).
Second, since in its default setting,JAPA only considers n-m sentence-alignment pat-terns with n,m ?
[0, 2], we provided it with a newpattern distribution we computed from the devel-opment corpus (see Table 1).
It is interesting tonote that although English and Inuktitut have verydifferent word systems, the length ratio (in char-acters) of the two sides of the TRAIN corpus is1.05.Each pair of documents (sentences) were thenaligned separately with JAPA.
1-n and n-1alignments identified by JAPA where output with-out further processing.
Since the word alignmentformat of the shared task do not account directlyfor n-m alignments (n,m > 1) we generated thecartesian product of the two sets of words for allthese n-m alignments produced by JAPA.The performance of this approach is reportedin Table 2.
Clearly, the precision is poor.
Thisis partly explained by the cartesian product we re-sorted to when n-m alignments were produced byJAPA.
We provide in section 4 a way of improvingupon this scenario.Prec.
Rec.
F-meas.
AER22.34 78.17 34.75 74.59Table 2: Performance of the JAPA alignment tech-nique on the DEV corpus.3 NUKTI: Word and SubstringAlignmentMartin et al (2003) documented a study in build-ing and using an English-Inuktitut bitext.
Theydescribed a sentence alignment technique tunedfor the specificity of the Inuktitut language, anddescribed as well a technique for acquiring cor-respondent pairs of English tokens and Inuktitutsubstrings.
The motivation behind their work wasto populate a glossary with reliable such pairs.We extended this line of work in order to achieveword alignment.3.1 Association ScoreAs Martin et al (2003) pointed out, the strong ag-glutinative nature of Inuktitut makes it necessaryto consider subunits of Inuktitut tokens.
This isreflected by the large proportion of token typesand hapax words observed on the Inuktitut sideof the training corpus, compared to the ratios ob-served on the English side (see table 3).Inutktitut % English %tokens 2 153 034 3 992 298types 417 407 19.4 27 127 0.68hapax 337 798 80.9 8 792 32.4Table 3: Ratios of token types and happax wordsin the TRAIN corpus.The main idea presented in (Martin et al, 2003)is to compute an association score between anyEnglish word seen in the training corpus and allthe Inuktitut substrings of those tokens that wereseen in the same region.
In our case, we com-puted a likelihood ratio score (Dunning, 1993) forall pairs of English tokens and Inuktitut substringsof length ranging from 3 to 10 characters.
A max-imum of 25 000 associations were kept for eachEnglish word (the top ranked ones).To reduce the computation load, we used a suf-fix tree structure and computed the associationscores only for the English words belonging to thetest corpus we had to align.
We also filtered outInuktitut substrings we observed less than threetimes in the training corpus.
Altogether, it takesabout one hour for a good desktop computer toproduce the association scores for one hundredEnglish words.We normalize the association scores such thatfor each English word e, we have a distribution oflikely Inuktitut substrings s:?s pllr(s|e) = 1.3.2 Word Alignment StrategyOur approach for aligning an Inuktitut sentenceof K tokens IK1 with an English sentence of Ntokens EN1 (where K ?
N )2 consists of finding2As a matter of fact, the number of Inuktitut words inthe test corpus is always less than or equal to the number ofEnglish tokens for any sentence pair.76K ?
1 cutting points ck?
[1,K?1] (ck ?
[1, N ?
1])on the English side.
A frontier ck delimits adja-cent English words Eckck?1+1 that are translation ofthe single Inuktitut word Ik.
With the conventionthat c0 = 0, cK = N and ck?1 < ck, we can for-mulate our alignment problem as seeking the bestword alignment A = A(IK1 |EN1 ) by maximizing:A = argmaxcK1K?k=1p(Ik|Eckck?1+1)?1 ?
p(dk)?2(1)where dk = ck?ck?1 is the number of Englishwords associated to Ik; p(dk) is the prior proba-bility that dk English words are aligned to a singleInuktitut word, which we computed directly fromTable 1; and ?1 and ?2 are two weighting coeffi-cients.We tried the following two approximations tocompute p(Ik|Eckck?1+1).
The second one led tobetter results.p(Ik|Eckck?1+1) '????
?maxckj=ck?1+1 p(Ik|Ej)or?ckj=ck?1+1p(Ik|Ej)We considered several ways of computing theprobability that an Inuktitut token I is the transla-tion of an English one E; the best one we foundbeing:p(I|E) '?s?I?pllr(s|E) + (1?
?
)pibm2(s|E)where the summation is carried over all sub-strings s of I of 3 characters or more.
pllr(s|E)is the normalized log-likelihood ratio score de-scribed above and pibm2(s|E) is the probabilityobtained from an IBM model 2 we trained afterthe Inuktitut side of the training corpus was seg-mented using a recursive procedure optimizing afrequency-based criterion.
?
is a weighting coef-ficient.We tried to directly embed a model trainedon whole (unsegmented) Inuktitut tokens, but no-ticed a degradation in performance (line 2 of Ta-ble 4).3.3 A Greedy Search StrategyDue to its combinatorial nature, the maximiza-tion of equation 1 was barely tractable.
There-fore we adopted a greedy strategy to reduce thesearch space.
We first computed a split of the En-glish sentence into K adjacent regions cK1 by vir-tually drawing a diagonal line we would observeif a character in one language was producing aconstant number of characters in the other one.An initial word alignment was then found by sim-ply tracking this diagonal at the word granularitylevel.Having this split in hand (line 1 of Table 4), wemove each cutting point around its initial valuestarting from the leftmost cutting point and goingrightward.
Once a locally optimal cutting pointhas been found (that is, maximizing the score ofequation 1), we proceed to the next one directlyto its right.3.4 ResultsWe report in Table 4 the performance of differentvariants we tried as measured on the developmentset.
We used these performances to select the bestconfiguration we eventually submitted.variant Prec.
Rec.
F-m. AERstart (diag) 51.7 53.66 52.66 49.54greedy (word) 61.6 63.94 62.75 35.93greedy (best) 63.5 65.92 64.69 34.21Table 4: Performance of several NUKTI align-ment techniques measured on the DEV corpus.It is interesting to note that the starting pointof the greedy search (line 1) does better than ourfirst approach.
However, moving from this ini-tial split clearly improves the performance (line3).
Among the greedy variants we tested, we no-ticed that putting much of the weight ?
on theIBM model 2 yielded the best results.
We also no-ticed that p(dk) in equation 1 did not help (?2 wasclose to zero).
A character-based model mighthave been more appropriate to the case.4 Combination of JAPA and NUKTIOne important weakness of our first approach liesin the cartesian product we generate when JAPAproduces a n-m (n,m > 1) alignment.
Thus,we tried a third approach: we apply NUKTI onany n-m alignment JAPA produces as if this ini-tial alignment were in fact two (small) sentencesto align, n- and m-word long respectively.
We can77therefore avoid the cartesian product and selectword alignments more discerningly.
As can beseen in Table 5, this combination improved overJAPA alone, while being worse than NUKTI alone.5 ResultsWe submitted 3 variants to the organizers.
Theperformances for each method are gathered in Ta-ble 5.
The order of merit of each approach wasconsistent with the performance we measured onthe DEV corpus, the best method being the NUKTIone.
Curiously, we did not try to propose any Surealignment but did receive a credit for it for two ofthe variants we submitted.variant T. Prec.
Rec.
F-m. AERJAPA P 26.17 74.49 38.73 71.27JAPA + S 9.62 67.58 16.84NUKTI P 51.34 53.60 52.44 46.64NUKTI S 12.24 86.01 21.43p 63.09 65.87 64.45 30.6Table 5: Performance of the 3 alignments we sub-mitted for the TEST corpus.
T. stands for the typeof alignment (Sure or Possible).6 DiscussionWe proposed two methods for aligning anEnglish-Inuktitut bitext at the word level and acombination of both.
The best of these meth-ods involves computing an association score be-tween English tokens and Inuktitut substrings.
Itrelies on a greedy algorithm we specifically de-vised for the task and which seeks a local opti-mum of a cumulative function of log-likelihoodratio scores.
This method obtained a precisionand a recall above 63% and 65% respectively.We believe this method could easily be im-proved.
First, it has some intrinsic limitations,as for instance, the fact that NUKTI only recog-nizes 1-n cepts and do not handle at all unalignedwords.
Indeed, our method is not even suited toaligning English sentences with fewer words thantheir respective Inuktitut counterpart.
Second, thegreedy search we devised is fairly aggressive andonly explores a tiny bit of the full search.
Last,the computation of the association scores is fairlytime-consuming.Our idea of redefining word alignment as a sen-tence alignment task did not work well; but at thesame time, we adapted poorly JAPA to this task.In particular, JAPA does not benefit here from allthe potential of the underlying cognate system be-cause of the scarcity of these cognates in verysmall sequences (words).If we had to work on this task again, we wouldconsider the use of a morphological analyzer.
Un-fortunately, it is only after the submission dead-line that we learned of the existence of such a toolfor Inuktitut3.AcknowledgementWe are grateful to Alexandre Patry who turnedthe JAPA aligner into a nicely written and efficientC++ program.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The Mathematics of StatisticalMachine Translation: Parameter Estimation.
Com-putational Linguistics, 19(2):263?311.T.
Dunning.
1993.
Accurate Methods for the Statis-tics of Surprise and Coincidence.
ComputationalLinguistics, 19(1).W.
A. Gale and K. W. Church.
1993.
A Program forAligning Sentences in Bilingual Corpora.
In Com-putational Linguistics, volume 19, pages 75?102.P.
Langlais, M. Simard, and J.
Ve?ronis.
1998.
Meth-ods and Practical Issues in Evaluating AlignmentTechniques.
In 36th annual meeting of the ACL,Montreal, Canada.P.
Langlais.
1997.
A System to Align Complex Bilin-gual Corpora.
QPSR 4, TMH, Stockholm, Sweden.J.
Martin, H. Johnson, B. Farley, and A. Maclach-lan.
2003.
Aligning and Using an English-InuktitutParallel Corpus.
In Building and using ParallelTexts: Data Driven Machine Translation and Be-yond, pages 115?118, Edmonton, Canada.F.J.
Och and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
Compu-tational Linguistics, 29:19?51.M.
Simard, G.F. Foster, and P. Isabelle.
1992.
UsingCognates to Align Sentences in Bilingual Corpora.In Conference on Theoretical and MethodologicalIssues in Machine Translation, pages 67?81.3See http://www.inuktitutcomputing.ca/Uqailaut/78
