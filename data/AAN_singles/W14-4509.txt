Proceedings of the AHA!
Workshop on Information Discovery in Text, pages 48?53,Dublin, Ireland, August 23 2014.Unsupervised Approach to Extracting Problem Phrases fromUser Reviews of ProductsElena Tutubalina1and Vladimir Ivanov1,2,31Kazan (Volga region) Federal University, Kazan, Russia2Institute of Informatics, Tatarstan Academy of Sciences, Kazan, Russia3National University of Science and Technology ?MISIS?, Moscow, Russia{tutubalinaev,nomemm}@gmail.comAbstractThis paper describes an approach to problem phrase extraction from texts that contain user expe-rience with products.
In contrast to other works, we propose a straightforward approach to prob-lem phrase extraction based on syntactic and semantic connections between a problem indicatorand mentions about the problem targets.
In this paper, we discuss (i) grammatical dependenciesbetween the target and the problem indicators and (ii) a number of domain-specific targets thatwere extracted using problem phrase structure and additional world knowledge.
The algorithmachieves an average F1-measure of 77%, evaluated on reviews about electronic and automobileproducts.1 IntroductionAutomatic analysis of reviews can increase information about product effectiveness.
This is especiallyimportant to a company if the information can be obtained with minimal costs.
Customers write reviewsregarding product issues that are too difficult to handle without technical support.In this paper, we present a study about connections between a product (the target of a problem phrase)and words (problem indicators), describing unexpected situations specific to products.
We define prob-lem indicators as words describing phrases that contain obvious links to a problem (e.g., problem, issue).We also define problem indicators as words that mention implicit problems (e.g., after, sometimes).
Theproblem indicator may be presented as an action verb with a negation expressing product failure.The task is to identify which noun phrases (NPs) referred to the problem target in the sentence.
Thetask is divided into two subtasks: (1) identify what phrase potentially contains information about aproblem and (2) find possible targets using the set of nouns for a given problem expression.
The firstsubtask, problem phrase identification, determines whether a given sentence contains problem phrases.The second subtask, target phrase extraction, identifies the targets of a given problem phrase.The problem indicators are significant for problem sentences where the device doesn?t work correctly.However, the presence of indicators in the sentence may have insufficient context to determine the prob-lem?s existence.
In examples 1 and 2, the object that receives the action isn?t defined (?I have not seenone?, ?something hasn?t been right?).1.
After looking for months, I have not seen one that I like at the local store and have expanded toother local stores.2.
Something hasn?t been right for sometimes now - we have advised the support staff several timesabout this issue.We present the dependency-based approach for extracting problem phrases and its target from userreviews of products.
We suppose that problem indicators have a syntax connection with the target ofa problem phrase.
Domain-specific targets are extracted by determining a domain category of relatedtarget words in WordNet.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/48The rest of the paper is organized as follows.
In section 2, we introduce related work in different areasof text classification of reviews and target detection.
Section 3 describes our approach and classifies de-pendency relations between problem indicators and targets of problem phrases.
We present experimentalresults in section 4.
Section 5 presents our conclusions and future extensions of this work.2 Related WorkSeveral different methods have been proposed in literature for the classification of product reviews indifferent areas of research.
The primary area of related works is sentiment analysis.
Turney (2002)presented an unsupervised learning algorithm for classifying reviews from a consumer platform on theWeb.
The author extracted phrases containing adjectives or adverbs from reviews of different domains.His algorithm achieved 74% accuracy for 410 reviews sampled from four different domains.
Popescuand Etzioni (2007) focused on entity-level classification using extraction rules on sentences with featuresor entities.
They proposed a method for opinion phrase extraction based on the semantic orientationof words in the context of product features.
Authors commonly focused on explicit product featuresfrom noun phrases.
They reported a recall of 89% and a precision of 86% for opinion phrase polaritydetermination with known product features and a precision of 79% and a recall of 76% for productfeature identification.
Hu et al.
(2004) extracted sentences that contained one or more product featuresand identified the polarity of the opinion sentences using the adjective set from WordNet.Another group of related works explores extracting information about subjectivity.
Wiebe and Wil-son (2002) recognized opinionated and evaluative (subjective) language in text.
According to them, asentence is subjective if it contains a significant expression of emotion, opinion, or an idea about whysomething has happened.
Wilson et al.
(2004) used dependency relations classifying the subjectivity ofdeeply nested clauses for their task of classifying the strength of the opinions.
Breck (2007) used semi-supervised sequence modeling by conditional random fields (CRF) for entity-level opinion expressions.The best F1-measure (70.65%) was achieved for identifying opinion expressions.There is much research on finding targets of objects using dependency relations (Quadir (2009); Lu(2010); Qiu et al.
(2011)).
Qadir (2009) identified opinion sentences that were specific to productfeatures.
The words forming the dependency relations were analyzed for frequent product feature.
Qadirtagged each sentence with only one product feature.
In contrast, we propose that problem phrases havemultiple targets.
Qiu et al.
(2011) extracted sentiment words and aspects by using syntactic relations.They described a propagation approach that achieved a recall of 83% and a precision of 88%.Problem detection and extraction of problem phrases from texts are less studied.
We used a clause-based approach to problem-phrase extraction from user reviews of products.
This method is based ondictionaries and rules and performed well compared to the simple baseline given by supervised machinelearning algorithms.
They achieved a recall of 77% and a precision of 74% for user reviews aboutelectronic products.
The current task of this research is identifying the targets of problem phrases toreduce classification errors.
Gupta (2013) studied the extraction of problems with AT&T products andservices from English Twitter messages.
The author used a supervised method to train a maximumentropy classifier.
Gupta reported the best performance F1-measure of 75% for identification of problemtarget.
In contrast, our method is based on grammatical domain-independent relations in a sentence.3 Target ExtractionIn this section, we describe our method for extracting problem phrases, related to targets, fromcustomer reviews.
PW is an initially empty set of the problem indicators, T is an empty targetset.
The common approach starts from an initially empty set of pairs (problem indicator, target),PTWs = (PW,T ), where PW = T = ?.
The approach is divided into three steps: problem-phrase identification, target extraction, and domain-specific target detection.
After three steps, the al-gorithm marks the sentence as a problem sentence if at least one pair (indicator, target) is extracted (i.e.PTWs = {(pw1, t1), (pw2, t2), ...}, pwi?
PW, ti?
T ).
We describe problem-phrase identificationin section 3.1.
Section 3.2 describes dependency relations for target extraction.
Section 3.3 explainsidentifying domain-specific targets using semantic knowledge from WordNet.493.1 Problem Phrase IdentificationA common appoach to problem-phrase extraction uses problem indicators (i.e., words, that indicate aproblem in a sentence).
We briefly describe the manually created ProblemWord dictionary, definedin Ivanov and Tutubalina (2014).
The ProblemWord dictionary includes problem indicators such asproblem, error, failure, malfunction, crash, break, reboot, have to replace, etc.
We collected synonymsfor these problem indicators.
The manually created dictionary consists of about 300 terms.Problem-phrase identification.
In this step, the algorithm looks for the problem indicators in thesentence.
The algorithm looks for verb phrases headed by an action verb with a negation or looks forproblem words from manually created dictionaries (without related negations).
As a result, the set PWis collected, which includes all problem indicators pwi?
PW , found in the sentence S.3.2 Dependency Relations for Target ExtractionThe method for target extraction uses syntax dependencies to determine connections between possibletargets and problem indicators.
The phrase describing the problem is a combination of the target andthe problem indicators.
The structure of a sentence with selected collapsed dependencies is shown inFigure 1.
We use the Stanford typed dependencies from the Stanford parser1.
For this sentence, the Stan-ford dependencies representation with the selected problem indicator is {dobj(use, firefox), nn(software,printer), prep with(use, software), vmod(software, added), pre to(added, computer)}.
These dependen-cies are written as relation type(head, dependent), where the head and the dependent have a dependencydirection associated with them in the sentence.For almost a year, I could not use???
?PWdobjprep withnsubjfirefox?
??
?Twith my printer software?
??
?Tnnpartmodadded?
??
?Sprep toto my computer?
??
?T.Figure 1: The syntactic structure of a sentence with collapsed dependencies.To identify connections between the problem indicator and the targets of problem phrase, we use directand indirect dependency relations between two words that are defined in (Qiu et.
al., 2011).
The intuitionbehind a choice of these types is that only syntactically and semantically rich mentions of targets areextracted, reducing noise in the extracted set of nouns.
The direct dependencies allow for recognizingsome relations directly between the target and the problem indicator.
Indirect dependency indicatesthat one word depends on the other word through some additional words, which we call successors.A successor word connects to a problem indicator and replaces a problem indicator in relation with atarget.
The connection of the successor with the problem indicator and the target can possibly indicatethe relation between the indicator and the target of the problem phrase through context.Sentences 3?4 show examples with direct and indirect dependency relations taken from the reviewsentences.
PW refers to a problem indicator, and T refers to a target of a problem phrase, S refers to asuccessor of a problem indicator.
In example 3, give is the action verb with the related negation, whichindicates the problem and has a direct connection with the target stars.
Example 4 contains problemswith the cord, and there is no problems with domain-specific products in example 3.3.
The onlyreason I didn?t give???
?PWdobjnsubjit five stars?
??
?Tis because I am unfamiliar with any of the bottles.4.They still don?t allow?
??
?PWxcompnsubjyou to unwind?
??
?Sdobjthe cord???
?Twhile it?s plugged in the cord would twist.1http://nlp.stanford.edu/software/lex-parser.shtml50Target extraction.
In second step, for each problem indicator pwi?
PW the approach extracts allrelated targets.
The targets are found in the dependencies representation of the sentence S. The targetis related to the problem word if there is direct or indirect dependency between the target word and theproblem indicator or the problem indicator?s successor, respectively, in the sentence representation.
Weadd to the set PWTs one pair(pwi, tj) for each target tj, i.e., PWTs = PWTs ?
{pair(pwi, tj)},T = T ?
{tj}.3.3 Domain-Specific Target Detection Using WordNet CategoriesDomain-specific targets can be extracted by using additional world knowledge.
Domain-specific targetsare objects that have important meanings in a particular domain.
The method uses WordNet for choosingdomain-specific targets.
WordNet organizes related words in synsets as synonym sets.
It extracts domaincategories from WordNet for selected problem targets in the sentence.
WordNet assigns multiple domainsemantic labels to terms in the domain.
The method extracts categories for each target and its hypernyms,hyponyms, and holonyms.
A hyponym is a lexical relation between a more general term and a moreparticular term (e.g., machine/computer).
A hypernym is a word that is more generic than a given entity(e.g., portable computer/laptop).
A holonym is a term that denotes a complete object whose part isdenoted by given word (e.g., computer/keyboard).Figure 2 shows the relations that classified targets into WordNet domains.
Each target from the prob-lem phrase is represented by a row in the first table.
Hyponym, hypernym and holonym relations aredrawn between rows in the tables.
Dashed arrows are represented type-of relations, and solid arrows arerepresented part-of relations between the target and the WordNet synset.
Solid lines are represented linksto the WordNet domain category.
In the example, the target TouchPad has an unknown category.keyboardlaptopinternetproductpowercar seatsTouchPadTargetelectronic computercomputer networkretail storeenergycarundefined termWordNetTermcomputingbusinessphysicsautounknownWordNetCategoryDomainFigure 2: The example of relating targets into WordNet categoriesDomain-specific target detection.
In the third step, the algorithm reduces pairs with non-domain-specific problem targets from the set PWTs using semantic knowledge.
The pair pair(pwi, tj) is re-duced if there is no term with a domain category in the hierarchy of WordNet synsets induced by thehypernym, hyponym, or holonym relations with target tj.4 Evaluation and DiscussionFor our experiments we collected 734 sentences from the HP website2.
We employed 953 sentencesfrom Amazon reviews3about automobile products.
Of the total sentences, 1,288 sentences (506 + 782from electronic and automobile domains) were classified as problem sentences, and 399 (228 + 171)sentences were labeled as part of the no-problem class.
Class labels for each sentence were obtained byusing the Amazon Mechanical Turk service.
Each sentence does not have any particular label for targetsand contains at least one problem indicator that the approach can find.
We propose that the problem2http://reviews.shop.hp.com3The dataset is available at https://snap.stanford.edu/data/web-Amazon.html51phrase with the problem indicator always has targets, but not necessarily domain-specific targets.
Forour performance metrics, we view that a text classification task is to identify whether a target is a domain-specific problem target.
We computed precision (P), recall (R), accuracy (Acc.)
and F1-mesure (F1).Type of targets ExamplesDomain-specific internet, printer, monitor, screen, laptop, processor, driverUndefined category Apple, HP printer, win7, printhead, adapter, rebootsOther targets marketing, stars, box, unit, letters, shipment, resultsTable 1: Problem targets related to WordNet categories.Table 1 gives examples of the problem targets that are extracted in connection with problem indicatorsand are related to the categories.
The total number of targets extracted from 506 problem sentencesabout electronic products were 258 domain-specific targets, 458 targets without a WordNet categoryand 138 other targets.
The approach collected 151 compound targets (e.g., auto configuration, networksettings), that were related to the undefined category.We used different methods to compare the performance of our approach, as follows:1.
We considered the targets extracted by direct dependencies (we did not use any lexical knowledge).2.
We considered all targets extracted by direct and indirect dependencies as domain-specific targets.3.
We considered only domain-specific targets extracted by direct and indirect dependencies.
We ex-tracted the targets, if the selected target was a term related to computing, electronic, or automobileterminology from WordNet.
We also considered the targets that don?t relate to any WordNet cate-gory and did not have a lexical meaning such as ?time?
or ?person?.Method nameElectronics CarsP R Acc.
F1 P R Acc.
F1Direct Dependencies .74 .48 .53 .58 .83 .67 .62 .74+ Indirect Dependencies .73 .90 .71 .81 .83 .92 .78 .87+ WordNet caterogies .74 .71 .62 .72 .84 .79 .70 .81Table 2: Performance metrics of the dependency-based approach.Performance metrics are calculated with this dataset and provided in able 2.
The average recall and theaverage precision of problem-phrase extraction related to domain-specific targets are 75% and 79%, re-spectively.
WordNet is limited and does not include many proper nouns (e.g., MacBook, Honda Odyssey)related to a particular domain.
Thus, domain-specific target detection using WordNet categories led to adecrease in the average recall (from 91% to 75%).
Another type of sentence, that decreases the averagerecall, is user sentences with a situation description, not just a report about a problem (?Something hasn?tbeen right for sometimes now?, for example).5 ConclusionIn this paper, we aim to identify problem phrases and to connect the proper targets of phrases withproblems or difficult situations.
Without using domain-specific knowledge about products, we focus ourattention on dependency-based syntactic information between the target and the problem indicators in thetext.
We use WordNet synsets with domain labels to reduce targets that aren?t related to a product domain.The average value of the F1-measure (about 84% for all targets and 77% for domain-specific targets) isbetter than the F1-measure in Ivanov and Tutubalina (2014).
Our research shows that dependencies andsyntactic information combine with each other to collect different parts of problem targets for targetphrase extraction.
For future work, we plan to extend the evaluation corpus with new domains to exploremore kinds of syntactic information between problem indicators and the dependent context.52ReferencesTurney P. D. 2002.
Thumbs up or thumbs down?
: semantic orientation applied to unsupervised classification ofreviews.
Association for Computational Linguistics, 417?424.Popescu A. M., Etzioni O.
2007.
Extracting product features and opinions from reviews.
//Natural languageprocessing and text mining, Springer, London, 9?28.Hu M., Liu B.
2004.
Mining and summarizing customer reviews.
Proceedings of the tenth ACM SIGKDDinternational conference on Knowledge discovery and data mining, 168?177.Wiebe J., Wilson T. 2002.
Learning to disambiguate potentially subjective expressions.
Proceedings of the 6thconference on Natural language learning-Volume 20, Association for Computational Linguistics, 1?7.Wilson T., Wiebe J., Hwa R. 2004.
Just how mad are you?
Finding strong and weak opinion clauses.
Aaai,761?769.Breck E., Choi Y., Cardie C. 2007.
Identifying Expressions of Opinion in Context.
IJCAI, 2683?2688.Narenda Gupta.
2013.
Extracting phrases describing problems with products and services from twitter messages.Conference on Intelligent Text Processing and Computational Linguistics, CICling2013.Ivanov V., Tutubalina E. 2014.
Clause-based approach to extracting problem phrases from user reviews ofproducts.
Proceeding of the 3d International Conference on Analysis of Images, Social Networks, and TextsAIST?14, Russia, 2014.Lu B.
2010.
Identifying opinion holders and targets with dependency parser in Chinese news texts.
Proceedingsof the NAACL HLT 2010 Student Research Workshop.
Association for Computational Linguistics, 46?51.Qadir A.
2009.
Detecting opinion sentences specific to product features in customer reviews using typed depen-dency relations.
Proceedings of the Workshop on Events in Emerging Text Types.
Association for Computa-tional Linguistics, 38?43.G.
Qiu, B. Liu, J. Bu, C. Chen 2011.
Opinion word expansion and target extraction through double propagation.Computational linguistics, 9?27.53
