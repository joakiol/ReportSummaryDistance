Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 181?186, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsKLUE-CORE: A regression model of semantic textual similarityPaul Greiner and Thomas Proisl and Stefan Evert and Besim KabashiFriedrich-Alexander-Universit?t Erlangen-N?rnbergDepartment Germanistik und KomparatistikProfessur f?r KorpuslinguistikBismarckstr.
691054 Erlangen, Germany{paul.greiner,thomas.proisl,stefan.evert,besim.kabashi}@fau.deAbstractThis paper describes our system entered for the*SEM 2013 shared task on Semantic TextualSimilarity (STS).
We focus on the core taskof predicting the semantic textual similarity ofsentence pairs.The current system utilizes machine learn-ing techniques trained on semantic similarityratings from the *SEM 2012 shared task; itachieved rank 20 out of 90 submissions from35 different teams.
Given the simple nature ofour approach, which uses only WordNet andunannotated corpus data as external resources,we consider this a remarkably good result, mak-ing the system an interesting tool for a widerange of practical applications.1 IntroductionThe *SEM 2013 shared task on Semantic TextualSimilarity (Agirre et al 2013) required participantsto implement a software system that is able to pre-dict the semantic textual similarity (STS) of sentencepairs.
Being able to reliably measure semantic simi-larity can be beneficial for many applications, e.g.
inthe domains of MT evaluation, information extrac-tion, question answering, and summarization.For the shared task, STS was measured on a scaleranging from 0 (indicating no similarity at all) to 5(semantic equivalence).
The system predictions wereevaluated against manually annotated data.2 Description of our approachOur system KLUE-CORE uses two approaches toestimate STS between pairs of sentences: a distri-butional bag-of-words model inspired by Sch?tze(1998), and a simple alignment model that links eachword in one sentence to the semantically most similarword in the other sentence.
For the alignment model,word similarities were obtained from WordNet (usinga range of state-of-the-art path-based similarity mea-sures) and from two distributional semantic models(DSM).All similarity scores obtained in this way werepassed to a ridge regression learner in order to obtaina final STS score.
The predictions for new sentencepairs were then transformed to the range [0,5], asrequired by the task definition.2.1 The training dataWe trained our system on manually annotated sen-tence pairs from the STS task at SemEval 2012(Agirre et al 2012).
Pooling the STS 2012 trainingand test data, we obtained 5 data sets from differ-ent domains, comprising a total of 5343 sentencepairs annotated with a semantic similarity score inthe range [0,5].
The data sets are paraphrase sen-tence pairs (MSRpar), sentence pairs from video de-scriptions (MSRvid), MT evaluation sentence pairs(MTnews and MTeuroparl), and glosses from twodifferent lexical semantic resources (OnWN).All sentence pairs were pre-processed with Tree-Tagger (Schmid, 1995)1 for part-of-speech annota-tion and lemmatization.1http://www.ims.uni-stuttgart.de/forschung/ressourcen/werkzeuge/treetagger.html1812.2 Similarity on word levelOur alignment model (Sec.
2.3.1) is based on similar-ity scores for pairs of words.
We obtained a total of11 different word similarity measures from WordNet(Miller et al 1990) and in a completely unsupervisedmanner from distributional semantic models.2.2.1 WordNetWe computed three state-of-the-art WordNet simi-larity measures, namely path similarity, Wu-Palmersimilarity and Leacock-Chodorow similarity (Budan-itsky and Hirst, 2006).
As usual, for each pair ofwords the synsets with the highest similarity scorewere selected.
For all three measures, we made use ofthe implementations provided as part of the NaturalLanguage ToolKit for Python (Bird et al 2009).2.2.2 Distributional semanticsWord similarity scores were also obtained from twoDSM: Distributional Memory (Baroni and Lenci,2010) and a model compiled from a version of theEnglish Wikipedia.2 For Distributional Memory, wechose the collapsed W ?W matricization, resultingin a 30686?30686 matrix that was further reducedto 300 latent dimensions using randomized SVD(Halko et al 2009).
For the Wikipedia DSM, weused a L2/R2 context window and mid-frequencyfeature terms, resulting in a 77598?30484 matrix.Co-occurrence frequency counts were weighted us-ing sparse log-likelihood association scores with asquare root transformation, and reduced to 300 latentdimensions with randomized SVD.
In both cases, tar-get terms are POS-disambiguated lemmas of contentwords, and the angle between vectors was used as adistance measure (equivalent to cosine similarity).For each DSM, we computed the following se-mantic distances: (i) angle: the angle between thetwo word vectors; (ii) fwdrank: the (logarithm ofthe) forward neighbour rank, i.e.
which rank the sec-ond word occupies among the nearest neighboursof the first word; (iii) bwdrank: the (logarithm ofthe) backward neighbour rank, i.e.
which rank thefirst word occupies among the nearest neighbours ofthe second word; (iv) rank: the (logarithm of the)arithmetic mean of forward and backward neighbour2For this purpose, we used the pre-processed and linguis-tically annotated Wackypedia corpus available from http://wacky.sslmit.unibo.it/.rank; (v) lowrank: the (logarithm of the) harmonicmean of forward and backward neighbour rank.A composite similarity score in the range [0,1]was obtained by linear regression on all five distancemeasures, using the WordSim-353 noun similarityratings (Finkelstein et al 2002) for parameter esti-mation.
This score is referred to as similarity below.Manual inspection showed that word pairs with simi-larity < 0.7 were completely unrelated in many cases,so we also included a ?strict?
version of similaritywith all lower scores set to 0.
We further includedrank and angle, which were linearly transformed tosimilarity values in the range [0,1].2.3 Similarity on sentence levelSimilarity scores for sentence pairs were obtained intwo different ways: with a simple alignment modelbased on the word similarity scores from Sec.
2.2(described in Sec.
2.3.1) and with a distributionalbag-of-words model (described in Sec.
2.3.2).2.3.1 Similarity by word alignmentThe sentence pairs were preprocessed in the follow-ing way: input words were transformed to lower-case; common stopwords were eliminated; and dupli-cate words within each sentence were deleted.
Forthe word similarity scores from Sec.
2.2.2, POS-disambiguated lemmas according to the TreeTaggerannotation were used.Every word of the first sentence in a given pairwas then compared with every word of the secondsentence, resulting in a matrix of similarity scoresfor each of the word similarity measures describedin Sec.
2.2.
Since we were not interested in an asym-metric notion of similarity, matrices were set up sothat the shorter sentence in a pair always correspondsto the rows of the matrix, transposing the similaritymatrix if necessary.
From each matrix, two similar-ity scores for the sentence pair were computed: thearithmetic mean of the row maxima (marked as shortin Tab.
4), and the artihmetic mean of the columnmaxima (marked as long in Tab.
4).This approach corresponds to a simple word align-ment model where each word in the shorter sentenceis aligned to the semantically most similar word inthe longer sentence (short), and vice versa (long).Note that multiple source words may be aligned tothe same target word, and target words can remain182unaligned without penalty.
Semantic similarities arethen averaged across all alignment pairs.In total, we obtained 22 sentence similarity scoresfrom this approach.2.3.2 Distributional similarityWe computed distributional similarity between thesentences in each pair directly using bag-of-wordscentroid vectors as suggested by Sch?tze (1998),based on the two word-level DSM introduced inSec.
2.2.2.For each sentence pair and DSM, we computed (i)the angle between the centroid vectors of the two sen-tences and (ii) a z-score relative to all other sentencesin the same data set of the training or test collection.Both values are measures of semantic distance, butare automatically transformed into similarity mea-sures by the regression learner (Sec.
2.4).For the z-scores, we computed the semantic dis-tance (i.e.
angle) between the first sentence of a givenpair and the second sentences of all word pairs in thesame data set.
The resulting list of angles was stan-dardized to z-scores, and the z-score correspondingto the second sentence from the given pair was usedas a measure of forward similarity between the firstand second sentence.
In the same way, a backwardz-score between the second and first sentence wasdetermined.
We used the average of the forward andbackward z-score as our second STS measure.The z-transformation was motivated by our obser-vation that there are substantial differences betweenthe individual data sets in the STS 2012 training andtest data.
For some data sets (MSRpar and MSRvid),sentences are often almost identical and even a single-word difference can result in low similarity ratings;for other data sets (e.g.
OnWN), similarity ratingsseem to be based on the general state of affairs de-scribed by the two sentences rather than their par-ticular wording of propositional content.
By usingother sentences in the same data set as a frame ofreference, corpus-based similarity scores can roughlybe calibrated to the respective notion of STS.In total, we obtained 4 sentence (dis)similarityscores from this approach.
Because of technical is-sues, only the z-score measures were used in thesubmitted system.
The experiments in Sec.
3 alsofocus on these z-scores.2.4 The regression modelThe 24 individual similarity scores described inSec.
2.3.1 and 2.3.2 were combined into a singleSTS prediction by supervised regression.We conducted experiments with various machinelearning algorithms implemented in the Python li-brary scikit-learn (Pedregosa et al 2011).
In partic-ular, we tested linear regression, regularized linearregression (ridge regression), Bayesian ridge regres-sion, support vector regression and regression trees.Our final system submitted to the shared task usesridge regression, a shrinkage method applied to linearregression that uses a least-squares regularization onthe regression coefficients (Hastie et al 2001, 59).Intuitively speaking, the regularization term discour-ages large value of the regression coefficients, whichmakes the learning technique less prone to overfit-ting quirks of the training data, especially with largenumbers of features.We tried to optimise our results by training the indi-vidual regressors for each test data set on appropriateportions of the training data.
For our task submis-sion, we used the following training data based oneducated guesses inspired by the very small amountof development data provied: for the headlines testset we trained on both glosses and statistical MTdata, for the OnWN and FNWN test sets we trainedon glosses only (OnWN), and for the SMT test setwe trained on statistical MT data only (MTnews andMTeuroparl).
We decided to omit the Microsoft Re-search Paraphrase Corpus (MSRpar and MSRvid)because we felt that the types of sentence pairs in thiscorpus were too different from the development data.For our submission, we used all 24 features de-scribed in Sec.
2.3 as input for the ridge regressionalgorithm.
Out of 90 submissions by 35 teams, oursystem ranked on place 20.33 ExperimentsIn this section, we describe some post-hoc experi-ments on the STS 2013 test data, which we performedin order to find out whether we made good decisionsregarding the machine learning method, training data,3This paper describes the run listed as KLUE-approach_2 inthe official results.
The run KLUE-approach_1 was produced bythe same system without the bag-of-words features (Sec.
2.3.2);it was only submitted as a safety backup.183similarity features, and other parameters.
Results ofour submitted system are typeset in italics, the bestresults in each column are typeset in bold font.3.1 Machine learning algorithmsTab.
1 gives an overview of the performance of vari-ous machine learning algorithms.
All regressors weretrained on the same combinations of data sets (seeSec.
2.4 above) using all available features, and eval-uated on the STS 2013 test data.
Overall, our choiceof ridge regression is justified.
Especially for theOnWN test set, however, support vector regressionis considerably better (it would have achieved rank11 instead of 17 on this test set).
If we had happenedto use the best learning algorithm for each test set,we would have achieved a mean score of 0.54768(putting our submission at rank 14 instead of 20).3.2 Regularization strengthWe also experimented with different regularizationstrengths, as determined by the parameter ?
of theridge regression algorithm (see Tab.
2).
Changing ?from its default value ?
= 1 does not seem to havea large impact on the performance of the regressor.Setting ?
= 2 for all test sets would have minimallyimproved the mean score (rank 19 instead of 20).Even choosing the optimal ?
for each test set wouldonly have resulted in a slightly improved mean scoreof 0.53811 (also putting our submission at rank 19).3.3 Composition of training dataAs described above, we suspected that using differentcombinations of the training data for different testsets might lead to better results.
The overview inTab.
3 confirms our expectations.
We did, however,fail to correctly guess the optimal combinations foreach test set.
We would have obtained the best re-sults by training on glosses (OnWN) for the headlinestest set (rank 35 instead of 40 in this category), bytraining on MSR data (MSRpar and MSRvid) for theOnWN (rank 11 instead of 17) and FNWN test sets(rank 9 instead of 10), and by combining glosses andmachine translation data (OnWN, MTnews MTeu-roparl) for the SMT test set (rank 30 instead of 33).Had we found the optimal training data for each testset, our system would have achieved a mean score of0.55021 (rank 11 instead of 20).3.4 FeaturesFor our submission, we used all the features de-scribed in Sec.
2.
Tab.
4 shows what results eachgroup of features would have achieved by itself (allruns use ridge regression, default ?
= 1 and the samecombinations of training data as in our submission).In Tab.
4, the line labelled wp500 shows the re-sults obtained using only word-alignment similarityscores (Sec.
2.3.1) based on the Wikipedia DSM(Sec.
2.2.2) as features.
The following two lines giveseparate results for the alignments from shorter tolonger sentence, i.e.
row maxima (wp500-short) andfrom longer to shorter sentence, i.e.
column maxima(wp500-long), respectively.
Below are correspondingresults for word alignments based on DistributionalMemory (dm, dm-short, dm-long) and WordNet simi-larity as described in Sec.
2.2.1 (WN, WN-short, WN-long).
The line labelled bow represents the two z-score similarities obtained from distributional bag-of-words models (Sec.
2.3.2); bow-wp500 (WikipediaDSM) and bow-dm (Distributional Memory) eachcorrespond to a single distributional feature.Combining all the available features indeed resultsin the highest mean score.
However, for OnWN andSMT a subset of the features would have led to betterresults.
Using only the bag-of-words scores wouldhave improved the results for the OnWN test set bya considerable margin (rank 8 instead of 17), usingonly the alignment scores based on WordNet wouldhave improved the results for the SMT test set (rank17 instead of 33).
If we had used the optimal subsetof features for each test set, the mean score wouldhave increased to 0.55556 (rank 9 instead of 20).4 ConclusionOur experiments show that it is essential for high-quality semantic textual similarity to adapt a corpus-based system carefully to each particular data set(choice of training data, feature engineering, tuningof machine learning algorithm).
Many of our edu-cated guesses for parameter settings turned out to befairly close to the optimal values, though there wouldhave been some room for improvement.Overall, our simple approach, which makes verylimited use of external resources, performs quite well?
achieving rank 20 out of 90 submissions ?
and willbe a useful tool for many real-world applications.184headlines OnWN FNWN SMT meanRidge Regression 0.65102 0.68693 0.41887 0.33599 0.53546Linear Regression 0.65184 0.68118 0.39707 0.32756 0.52966Bayesian Ridge 0.65164 0.68962 0.42344 0.33003 0.53474SVM SVR 0.52208 0.73330 0.40479 0.30810 0.49357Decision Tree 0.29320 0.50633 0.05022 0.17072 0.28510Table 1: Evaluation results for different machine learning algorithms?
headlines OnWN FNWN SMT mean1 0.65102 0.68693 0.41887 0.33599 0.535460.01 0.65184 0.68129 0.39773 0.32773 0.529800.1 0.65186 0.68224 0.40246 0.32900 0.530870.5 0.65161 0.68492 0.41346 0.33311 0.533740.9 0.65114 0.68660 0.41816 0.33560 0.535232 0.64941 0.68917 0.42290 0.33830 0.536595 0.64394 0.69197 0.42265 0.33669 0.53491Table 2: Evaluation results for different regularization strengths of the ridge regression learnerheadlines OnWN FNWN SMT meandef 0.65440 0.68693 0.41887 0.32694 0.53357smt 0.65322 0.62643 0.24895 0.33599 0.50684def+smt 0.65102 0.59665 0.24953 0.33867 0.49962msr 0.63633 0.73396 0.43073 0.33168 0.54185def+smt+msr 0.65008 0.65093 0.39636 0.28645 0.50777approach2 0.65102 0.68693 0.41887 0.33599 0.53546Table 3: Evaluation results for different training sets (?approach2?
refers to our shared task submission, cf.
Sec.
2.4)headlines OnWN FNWN SMT meanwp500 0.57099 0.59199 0.31740 0.31320 0.46899wp500-long 0.57837 0.59012 0.30909 0.30075 0.46614wp500-short 0.58271 0.58845 0.34205 0.29474 0.46794dm 0.42129 0.55945 0.21139 0.27426 0.38910dm-long 0.40709 0.56511 0.28993 0.23826 0.38037dm-short 0.44780 0.53555 0.28709 0.24484 0.38853WN 0.63654 0.65149 0.41025 0.35624 0.52783WN-long 0.62749 0.63828 0.39684 0.33399 0.51297WN-short 0.64986 0.66175 0.41441 0.33350 0.52759bow 0.52384 0.74046 0.31917 0.24611 0.46808bow-wp500 0.52726 0.73624 0.32797 0.24460 0.46841bow-dm 0.21908 0.66873 0.17096 0.20176 0.32138all 0.65102 0.68693 0.41887 0.33599 0.53546Table 4: Evaluation results for different sets of similarity scores as features (cf.
Sec.
3.4)185ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2012.
Semeval-2012 task 6:A pilot on semantic textual similarity.
In First JointConference on Lexical and Computational Semantics,pages 385?393.
Association for Computational Linguis-tics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional Memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?712.Steven Bird, Ewan Klein, and Edward Loper.
2009.Natural Language Processing with Python.
O?ReillyMedia, Sebastopol, CA.
Online version available athttp://www.nltk.org/book.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, EhudRivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2002.
Placing search in context: The concept revisited.ACM Transactions on Information Systems, 20(1):116?131.N.
Halko, P. G. Martinsson, and J.
A. Tropp.
2009.
Find-ing structure with randomness: Stochastic algorithmsfor constructing approximate matrix decompositions.Technical Report 2009-05, ACM, California Instituteof Technology, September.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.2001.
The Elements of Statistical Learning.
Data Min-ing, Inference, and Prediction.
Springer, New York,NY.George A. Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine J. Miller.
1990.
Introduc-tion to WordNet: An on-line lexical database.
Interna-tional Journal of Lexicography, 3(4):235?244.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In Proceedingsof the EACL SIGDAT-Workshop, pages 47?50, Dublin.Hinrich Sch?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.186
