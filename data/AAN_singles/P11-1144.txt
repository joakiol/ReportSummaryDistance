Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1435?1444,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemi-Supervised Frame-Semantic Parsing for Unknown PredicatesDipanjan Das and Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{dipanjan,nasmith}@cs.cmu.eduAbstractWe describe a new approach to disambiguat-ing semantic frames evoked by lexical predi-cates previously unseen in a lexicon or anno-tated data.
Our approach makes use of largeamounts of unlabeled data in a graph-basedsemi-supervised learning framework.
We con-struct a large graph where vertices correspondto potential predicates and use label propa-gation to learn possible semantic frames fornew ones.
The label-propagated graph is usedwithin a frame-semantic parser and, for un-known predicates, results in over 15% abso-lute improvement in frame identification ac-curacy and over 13% absolute improvementin full frame-semantic parsing F1 score on ablind test set, over a state-of-the-art supervisedbaseline.1 IntroductionFrame-semantic parsing aims to extract a shallow se-mantic structure from text, as shown in Figure 1.The FrameNet lexicon (Fillmore et al, 2003) isa rich linguistic resource containing expert knowl-edge about lexical and predicate-argument seman-tics.
The lexicon suggests an analysis based on thetheory of frame semantics (Fillmore, 1982).
Recentapproaches to frame-semantic parsing have broadlyfocused on the use of two statistical classifiers cor-responding to the aforementioned subtasks: the firstone to identify the most suitable semantic frame fora marked lexical predicate (target, henceforth) in asentence, and the second for performing semanticrole labeling (SRL) given the frame.The FrameNet lexicon, its exemplar sentencescontaining instantiations of semantic frames, andfull-text annotations provide supervision for learn-ing frame-semantic parsers.
Yet these annotationslack coverage, including only 9,300 annotated tar-get types.
Recent papers have tried to address thecoverage problem.
Johansson and Nugues (2007)used WordNet (Fellbaum, 1998) to expand the list oftargets that can evoke frames and trained classifiersto identify the best-suited frame for the newly cre-ated targets.
In past work, we described an approachwhere latent variables were used in a probabilisticmodel to predict frames for unseen targets (Das etal., 2010a).1 Relatedly, for the argument identifica-tion subtask, Matsubayashi et al (2009) proposeda technique for generalization of semantic roles toovercome data sparseness.
Unseen targets continueto present a major obstacle to domain-general se-mantic analysis.In this paper, we address the problem of idenfi-fying the semantic frames for targets unseen eitherin FrameNet (including the exemplar sentences) orthe collection of full-text annotations released alongwith the lexicon.
Using a standard model for the ar-gument identification stage (Das et al, 2010a), ourproposed method improves overall frame-semanticparsing, especially for unseen targets.
To better han-dle these unseen targets, we adopt a graph-basedsemi-supervised learning stategy (?4).
We constructa large graph over potential targets, most of which1Notwithstanding state-of-the-art results, that approach wasonly able to identify the correct frame for 1.9% of unseen tar-gets in the test data available at that time.
That system achievesabout 23% on the test set used in this paper.1435bell.nring.vthere be.venough.aLUNOISE_MAKERSSUFFICIENCYFrameEXISTENCECAUSE_TO_MAKE_NOISE.bellsN_mmore than six of the eightSound_makerEnabled_situationringtoringersItemenoughEntityAgentn'tarestillthereButFigure 1: An example sentence from the PropBank section of the full-text annotations released as part of FrameNet1.5.
Each row under the sentence correponds to a semantic frame and its set of corresponding arguments.
Thick linesindicate targets that evoke frames; thin solid/dotted lines with labels indicate arguments.
N m under ?bells?
is shortfor the Noise maker role of the NOISE MAKERS frame.are drawn from unannotated data, and a fractionof which come from seen FrameNet annotations.Next, we perform label propagation on the graph,which is initialized by frame distributions over theseen targets.
The resulting smoothed graph con-sists of posterior distributions over semantic framesfor each target in the graph, thus increasing cover-age.
These distributions are then evaluated withina frame-semantic parser (?5).
Considering unseentargets in test data (although few because the testdata is also drawn from the training domain), sig-nificant absolute improvements of 15.7% and 13.7%are observed for frame identification and full frame-semantic parsing, respectively, indicating improvedcoverage for hitherto unobserved predicates (?6).2 BackgroundBefore going into the details of our model, we pro-vide some background on two topics relevant tothis paper: frame-semantic parsing and graph-basedlearning applied to natural language tasks.2.1 Frame-semantic ParsingGildea and Jurafsky (2002) pioneered SRL, andsince then there has been much applied researchon predicate-argument semantics.
Early work onframe-semantic role labeling made use of the ex-emplar sentences in the FrameNet corpus, each ofwhich is annotated for a single frame and its argu-ments (Thompson et al, 2003; Fleischman et al,2003; Shi and Mihalcea, 2004; Erk and Pado?, 2006,inter alia).
Most of this work was done on an older,smaller version of FrameNet.
Recently, since the re-lease of full-text annotations in SemEval?07 (Bakeret al, 2007), there has been work on identifyingmultiple frames and their corresponding sets of ar-guments in a sentence.
The LTH system of Jo-hansson and Nugues (2007) performed the best inthe SemEval?07 shared task on frame-semantic pars-ing.
Our probabilistic frame-semantic parser out-performs LTH on that task and dataset (Das et al,2010a).
The current paper builds on those proba-bilistic models to improve coverage on unseen pred-icates.2Expert resources have limited coverage, andFrameNet is no exception.
Automatic induction ofsemantic resources has been a major effort in re-cent years (Snow et al, 2006; Ponzetto and Strube,2007, inter alia).
In the domain of frame semantics,previous work has sought to extend the coverageof FrameNet by exploiting resources like VerbNet,WordNet, or Wikipedia (Shi and Mihalcea, 2005;Giuglea and Moschitti, 2006; Pennacchiotti et al,2008; Tonelli and Giuliano, 2009), and projectingentries and annotations within and across languages(Boas, 2002; Fung and Chen, 2004; Pado?
and La-pata, 2005).
Although these approaches have in-creased coverage to various degrees, they rely onother lexicons and resources created by experts.Fu?rstenau and Lapata (2009) proposed the use of un-labeled data to improve coverage, but their work waslimited to verbs.
Bejan (2009) used self-training toimprove frame identification and reported improve-ments, but did not explicitly model unknown tar-gets.
In contrast, we use statistics gathered fromlarge volumes of unlabeled data to improve the cov-erage of a frame-semantic parser on several syntacticcategories, in a novel framework that makes use ofgraph-based semi-supervised learning.2SEMAFOR, the system presented by Das et al (2010a) ispublicly available at http://www.ark.cs.cmu.edu/SEMAFOR and has been extended in this work.14362.2 Graph-based Semi-Supervised LearningIn graph-based semi-supervised learning, one con-structs a graph whose vertices are labeled and unla-beled examples.
Weighted edges in the graph, con-necting pairs of examples/vertices, encode the de-gree to which they are expected to have the samelabel (Zhu et al, 2003).
Variants of label propaga-tion are used to transfer labels from the labeled to theunlabeled examples.
There are several instances ofthe use of graph-based methods for natural languagetasks.
Most relevant to our work an approach toword-sense disambiguation due to Niu et al (2005).Their formulation was transductive, so that the testdata was part of the constructed graph, and they didnot consider predicate-argument analysis.
In con-trast, we make use of the smoothed graph during in-ference in a probabilistic setting, in turn using it forthe full frame-semantic parsing task.
Recently, Sub-ramanya et al (2010) proposed the use of a graphover substructures of an underlying sequence model,and used a smoothed graph for domain adaptation ofpart-of-speech taggers.
Subramanya et al?s modelwas extended by Das and Petrov (2011) to inducepart-of-speech dictionaries for unsupervised learn-ing of taggers.
Our semi-supervised learning settingis similar to these two lines of work and, like them,we use the graph to arrive at better final structures, inan inductive setting (i.e., where a parametric modelis learned and then separately applied to test data,following most NLP research).3 Approach OverviewOur overall approach to handling unobserved targetsconsists of four distinct stages.
Before going into thedetails of each stage individually, we provide theiroverview here:Graph Construction: A graph consisting of ver-tices corresponding to targets is constructed us-ing a combination of frame similarity (for ob-served targets) and distributional similarity asedge weights.
This stage also determines afixed set of nearest neighbors for each vertexin the graph.Label Propagation: The observed targets (a smallsubset of the vertices) are initialized withempirical frame distributions extracted fromFrameNet annotations.
Label propagation re-sults in a distribution of frames for each vertexin the graph.Supervised Learning: Frame identification and ar-gument identification models are trained fol-lowing Das et al (2010a).
The graph is usedto define the set of candidate frames for unseentargets.Parsing: The frame identification model ofDas et al disambiguated among only thoseframes associated with a seen target in theannotated data.
For an unseen target, all framesin the FrameNet lexicon were considered (alarge number).
The current work replaces thatstrategy, considering only the top M frames inthe distribution produced by label propagation.This strategy results in large improvementsin frame identification for the unseen targetsand makes inference much faster.
Argumentidentification is done exactly like Das et al(2010a).4 Semi-Supervised LearningWe perform semi-supervised learning by construct-ing a graph of vertices representing a large numberof targets, and learn frame distributions for thosewhich were not observed in FrameNet annotations.4.1 Graph ConstructionWe construct a graph with targets as vertices.
Forus, each target corresponds to a lemmatized wordor phrase appended with a coarse POS tag, and itresembles the lexical units in the FrameNet lexicon.For example, two targets corresponding to the samelemma would look like boast.N and boast.V.
Here,the first target is a noun, while the second is a verb.An example multiword target is chemical weapon.N.We use two resources for graph construction.First, we take all the words and phrases present inthe dependency-based thesaurus constructed usingsyntactic cooccurrence statistics (Lin, 1998).3 Toconstruct this resource, a corpus containing 64 mil-lion words was parsed with a fast dependency parser(Lin, 1993; Lin, 1994), and syntactic contexts wereused to find similar lexical items for a given word3This resource is available at http://webdocs.cs.ualberta.ca/?lindek/Downloads/sim.tgz1437difference.Nsimilarity.Ndiscrepancy.Nresemble.Vdisparity.Nresemblance.Ninequality.Nvariant.Ndivergence.Npoverty.Nhomelessness.Nwealthy.Arich.Adeprivation.Ndestitution.Njoblessness.Nunemployment.N employment.Nunemployment rate.Npowerlessness.NUNEMPLOYMENT_RATE UNEMPLOYMENT_RATEUNEMPLOYMENT_RATEPOVERTY POVERTYPOVERTYSIMILARITY SIMILARITYSIMILARITYSIMILARITYSIMILARITYFigure 2: Excerpt from a graphover targets.
Green targets areobserved in the FrameNet data.Above/below them are shown themost frequently observed framethat these targets evoke.
The blacktargets are unobserved and labelpropagation produces a distributionover most likely frames that theycould evoke.or phrase.
Lin separately treated nouns, verbs andadjectives/adverbs and the thesaurus contains threeparts for each of these categories.
For each item inthe thesaurus, 200 nearest neighbors are listed with asymmetric similarity score between 0 and 1.
We pro-cessed this thesaurus in two ways: first, we lower-cased and lemmatized each word/phrase and mergedentries which shared the same lemma; second, weseparated the adjectives and adverbs into two listsfrom Lin?s original list by scanning a POS-taggedversion of the Gigaword corpus (Graff, 2003) andcategorizing each item into an adjective or an ad-verb depending on which category the item associ-ated with more often in the data.
The second stepwas necessary because FrameNet treats adjectivesand adverbs separately.
At the end of this processingstep, we were left with 61,702 units?approximatelysix times more than the targets found in FrameNetannotations?each labeled with one of 4 coarse tags.We considered only the top 20 most similar targetsfor each target, and noted Lin?s similarity betweentwo targets t and u, which we call simDL(t, u).The second component of graph constructioncomes from FrameNet itself.
We scanned the exem-plar sentences in FrameNet 1.54 and the training sec-tion of the full-text annotations that we use to trainthe probabilistic frame parser (see ?6.1), and gath-ered a distribution over frames for each target.
Fora pair of targets t and u, we measured the Euclideandistance5 between their frame distributions.
Thisdistance was next converted to a similarity score,namely, simFN (t, u) between 0 and 1 by subtract-ing each one from the maximum distance found in4http://framenet.icsi.berkeley.edu5This could have been replaced by an entropic distance metriclike KL- or JS-divergence, but we leave that exploration to fu-ture work.the whole data, followed by normalization.
LikesimDL(t, u), this score is symmetric.
This resultedin 9,263 targets, and again for each, we consideredthe 20 most similar targets.
Finally, the overall sim-ilarity between two given targets t and u was com-puted as:sim(t, u) = ?
?
simFN (t, u) + (1??)
?
simDL(t, u)Note that this score is symmetric because its twocomponents are symmetric.
The intuition behindtaking a linear combination of the two types of sim-ilarity functions is as follows.
We hope that distri-butionally similar targets would have the same se-mantic frames because ideally, lexical units evokingthe same set of frames appear in similar syntacticcontexts.
We would also like to involve the anno-tated data in graph construction so that it can elim-inate some noise in the automatically constructedthesaurus.6 Let K(t) denote the K most similar tar-gets to target t, under the score sim.
We link verticest and u in the graph with edge weight wtu, definedas:wtu ={sim(t, u) if t ?
K(u) or u ?
K(t)0 otherwise(1)The hyperparameters ?
and K are tuned by cross-validation (?6.3).4.2 Label PropagationFirst, we softly label those vertices of the con-structed graph for which frame distributions areavailable from the FrameNet data (the same distri-butions that are used to compute simFN ).
Thus, ini-tially, a small fraction of the vertices in the graph6In future work, one might consider learning a similarity metricfrom the annotated data, so as to exactly suit the frame identi-fication task.1438have soft frame labels on them.
Figure 2 shows anexcerpt from a constructed graph.
For simplicity,only the most probable frames under the empiricaldistribution for the observed targets are shown; weactually label each vertex with the full empirical dis-tribution over frames for the corresponding observedtarget in the data.
The dotted lines demarcate partsof the graph that associate with different frames.
La-bel propagation helps propagate the initial soft labelsthroughout the graph.
To this end, we use a vari-ant of the quadratic cost criterion of Bengio et al(2006), also used by Subramanya et al (2010) andDas and Petrov (2011).7Let V denote the set of all vertices in the graph,Vl ?
V be the set of known targets and F denote theset of all frames.
Let N (t) denote the set of neigh-bors of vertex t ?
V .
Let q = {q1, q2, .
.
.
, q|V |}be the set of frame distributions, one per vertex.
Foreach known target t ?
Vl, we have an initial framedistribution rt.
For every edge in the graph, weightsare defined as in Eq.
1.
We find q by solving:argminq?t?Vl?rt ?
qt?2+ ?
?t?V,u?N (t)wtu?qt ?
qu?2+ ?
?t?V ?qt ?1|F|?2s.t.
?t ?
V,?f?F qt(f) = 1?t ?
V, f ?
F , qt(f) ?
0(2)We use a squared loss to penalize various pairs ofdistributions over frames: ?a?b?2 =?f?F (a(f)?b(f))2.
The first term in Eq.
2 requires that, forknown targets, we stay close to the initial frame dis-tributions.
The second term is the graph smooth-ness regularizer, which encourages the distributionsof similar nodes (large wtu) to be similar.
The fi-nal term is a regularizer encouraging all distributionsto be uniform to the extent allowed by the first twoterms.
(If an unlabeled vertex does not have a pathto any labeled vertex, this term ensures that its con-verged marginal will be uniform over all frames.)
?and ?
are hyperparameters whose choice we discussin ?6.3.Note that Eq.
2 is convex in q.
While it is possibleto derive a closed form solution for this objective7Instead of a quadratic cost, an entropic distance measure couldhave been used, e.g., KL-divergence, considered by Subra-manya and Bilmes (2009).
We do not explore that directionin the current paper.function, it would require the inversion of a |V |?|V |matrix.
Hence, like Subramanya et al (2010), weemploy an iterative method with updates defined as:?t(f) ?
rt(f)1{t ?
Vl} (3)+ ?
?u?N (t)wtuq(m?1)u (f) +?|F|?t ?
1{t ?
Vl}+ ?
+ ?
?u?N (t)wtu (4)q(m)t (f) ?
?t(f)/?t (5)Here, 1{?}
is an indicator function.
The iterativeprocedure starts with a uniform distribution for eachq(0)t .
For all our experiments, we run 10 iterationsof the updates.
The final distribution of frames for atarget t is denoted by q?t .5 Learning and Inference forFrame-Semantic ParsingIn this section, we briefly review learning and infer-ence techniques used in the frame-semantic parser,which are largely similar to Das et al (2010a), ex-cept the handling of unknown targets.
Note that inall our experiments, we assume that the targets aremarked in a given sentence of which we want to ex-tract a frame-semantic analysis.
Therefore, unlikethe systems presented in SemEval?07, we do not de-fine a target identification module.5.1 Frame IdentificationFor a given sentence x with frame-evoking targetst, let ti denote the ith target (a word sequence).
Weseek a list f = ?f1, .
.
.
, fm?
of frames, one per tar-get.
LetL be the set of targets found in the FrameNetannotations.
Let Lf ?
L be the subset of these tar-gets annotated as evoking a particular frame f .The set of candidate frames Fi for ti is defined toinclude every frame f such that ti ?
Lf .
If ti 6?
L(in other words, ti is unseen), then Das et al (2010a)considered all frames F in FrameNet as candidates.Instead, in our work, we check whether ti ?
V ,where V are the vertices of the constructed graph,and set:Fi = {f : f ?M -best frames under q?ti} (6)The integer M is set using cross-validation (?6.3).If ti 6?
V , then all frames F are considered as Fi.1439The frame prediction rule uses a probabilistic modelover frames for a target:fi ?
argmaxf?Fi?`?Lfp(f, ` | ti,x) (7)Note that a latent variable ` ?
Lf is used, whichis marginalized out.
Broadly, lexical semantic re-lationships between the ?prototype?
variable ` (be-longing to the set of seen targets for a frame f ) andthe target ti are used as features for frame identifi-cation, but since ` is unobserved, it is summed outboth during inference and training.
A conditionallog-linear model is used to model this probability:for f ?
Fi and ` ?
Lf , p?
(f, ` | ti,x) =exp?>g(f, `, ti,x)?f ??Fi?`?
?Lf ?exp?>g(f ?, `?, ti,x)(8)where ?
are the model weights, and g is a vector-valued feature function.
This discriminative formu-lation is very flexible, allowing for a variety of (pos-sibly overlapping) features; e.g., a feature might re-late a frame f to a prototype `, represent a lexical-semantic relationship between ` and ti, or encodepart of the syntax of the sentence (Das et al, 2010b).Given some training data, which is of the form?
?x(j), t(j), f (j),A(j)?
?Nj=1 (where N is the numberof sentences in the data and A is the set of argu-ment in a sentence), we discriminatively train theframe identification model by maximizing the fol-lowing log-likelihood:8max?N?j=1mj?i=1log?`?Lf(j)ip?
(f(j)i , ` | t(j)i ,x(j)) (9)This non-convex objective function is locally op-timized using a distributed implementation of L-BFGS (Liu and Nocedal, 1989).95.2 Argument IdentificationGiven a sentence x = ?x1, .
.
.
, xn?, the set of tar-gets t = ?t1, .
.
.
, tm?, and a list of evoked frames8We found no benefit from using an L2 regularizer.9While training, in the partition function of the log-linearmodel, all frames F in FrameNet are summed up for a target tiinstead of only Fi (as in Eq.
8), to learn interactions betweenthe latent variables and different sentential contexts.f = ?f1, .
.
.
, fm?
corresponding to each target, ar-gument identification or SRL is the task of choos-ing which of each fi?s roles are filled, and by whichparts of x.
We directly adopt the model of Das etal.
(2010a) for the argument identification stage andbriefly describe it here.Let Rfi = {r1, .
.
.
, r|Rfi |} denote frame fi?sroles observed in FrameNet annotations.
A set S ofspans that are candidates for filling any role r ?
Rfiare identified in the sentence.
In principle, S couldcontain any subsequence of x, but we consider onlythe set of contiguous spans that (a) contain a sin-gle word or (b) comprise a valid subtree of a wordand all its descendants in a dependency parse.
Theempty span is also included in S, since some rolesare not explicitly filled.
During training, if an argu-ment is not a valid subtree of the dependency parse(this happens due to parse errors), we add its spanto S. Let Ai denote the mapping of roles in Rfi tospans in S. The model makes a prediction for eachAi(rk) (for all roles rk ?
Rfi):Ai(rk)?
argmaxs?S p(s | rk, fi, ti,x) (10)A conditional log-linear model over spans for eachrole of each evoked frame is defined as:p?
(Ai(rk) = s | fi, ti,x) = (11)exp?>h(s, rk, fi, ti,x)?s?
?S exp?>h(s?, rk, fi, ti,x)This model is trained by optimizing:max?N?j=1mj?i=1|Rf(j)i|?k=1log p?
(A(j)i (rk) | f(j)i , t(j)i ,x(j))This objective function is convex, and we globallyoptimize it using the distributed implementation ofL-BFGS.
We regularize by including ?
110??
?22 inthe objective (the strength is not tuned).
Na?
?ve pre-diction of roles using Equation 10 may result inoverlap among arguments filling different roles of aframe, since the argument identification model fillseach role independently of the others.
We wantto enforce the constraint that two roles of a sin-gle frame cannot be filled by overlapping spans.Hence, illegal overlap is disallowed using a 10,000-hypothesis beam search.1440UNKNOWN TARGETS ALL TARGETSModelExactMatchPartialMatchExactMatchPartialMatchSEMAFOR 23.08 46.62 82.97 90.51Self-training 18.88 42.67 82.45 90.19LinGraph 36.36 59.47 83.40 90.93FullGraph 39.86 62.35?
83.51 91.02?Table 1: Frame identification results in percentage accu-racy on 4,458 test targets.
Bold scores indicate significantimprovements relative to SEMAFOR and (?)
denotes sig-nificant improvements over LinGraph (p < 0.05).6 Experiments and ResultsBefore presenting our experiments and results, wewill describe the datasets used in our experiments,and the various baseline models considered.6.1 DataWe make use of the FrameNet 1.5 lexicon releasedin 2010.
This lexicon is a superset of previous ver-sions of FrameNet.
It contains 154,607 exemplarsentences with one marked target and frame-role an-notations.
78 documents with full-text annotationswith multiple frames per sentence were also released(a superset of the SemEval?07 dataset).
We ran-domly selected 55 of these documents for trainingand treated the 23 remaining ones as our test set.After scanning the exemplar sentences and the train-ing data, we arrived at a set of 877 frames, 1,068roles,10 and 9,263 targets.
Our training split ofthe full-text annotations contained 3,256 sentenceswith 19,582 frame annotatations with correspond-ing roles, while the test set contained 2,420 sen-tences with 4,458 annotations (the test set containedfewer annotated targets per sentence).
We also di-vide the 55 training documents into 5 parts for cross-validation (see ?6.3).
The raw sentences in all thetraining and test documents were preprocessed us-ing MXPOST (Ratnaparkhi, 1996) and the MST de-pendency parser (McDonald et al, 2005) followingDas et al (2010a).
In this work we assume theframe-evoking targets have been correctly identifiedin training and test data.10Note that the number of listed roles in the lexicon is nearly9,000, but their number in actual annotations is a lot fewer.6.2 BaselinesWe compare our model with three baselines.
Thefirst baseline is the purely supervised model of Daset al (2010a) trained on the training split of 55documents.
Note that this is the strongest baselineavailable for this task;11 we refer to this model as?SEMAFOR.
?The second baseline is a semi-supervised self-trained system, where we used SEMAFOR to label70,000 sentences from the Gigaword corpus withframe-semantic parses.
For finding targets in a rawsentence, we used a relaxed target identificationscheme, where we marked every target seen in thelexicon and all other words which were not prepo-sitions, particles, proper nouns, foreign words andWh-words as potential frame evoking units.
Thiswas done so as to find unseen targets and get frameannotations with SEMAFOR on them.
We appendedthese automatic annotations to the training data, re-sulting in 711,401 frame annotations, more than 36times the supervised data.
These data were next usedto train a frame identification model (?5.1).12 Thissetup is very similar to Bejan (2009) who used self-training to improve frame identification.
We refer tothis model as ?Self-training.
?The third baseline uses a graph constructed onlywith Lin?s thesaurus, without using supervised data.In other words, we followed the same scheme as in?4.1 but with the hyperparameter ?
= 0.
Next, la-bel propagation was run on this graph (and hyper-parameters tuned using cross validation).
The poste-rior distribution of frames over targets was next usedfor frame identification (Eq.
6-7), with SEMAFORas the trained model.
This model, which is very sim-ilar to our full model, is referred to as ?LinGraph.??FullGraph?
refers to our full system.6.3 Experimental SetupWe used five-fold cross-validation to tune the hy-perparameters ?, K, ?, and M in our model.
The11We do not compare our model with other systems, e.g.
theones submitted to SemEval?07 shared task, because SE-MAFOR outperforms them significantly (Das et al, 2010a)on the previous version of the data.
Moreover, we trained ourmodels on the new FrameNet 1.5 data, and training code forthe SemEval?07 systems was not readily available.12Note that we only self-train the frame identification model andnot the argument identification model, which is fixed through-out.1441UNKNOWN TARGETS ALL TARGETSModelExact Match Partial Match Exact Match Partial MatchP R F1 P R F1 P R F1 P R F1SEMAFOR 19.59 16.48 17.90 33.03 27.80 30.19 66.15 61.64 63.82 70.68 65.86 68.18Self-training 15.44 13.00 14.11 29.08 24.47 26.58 65.78 61.30 63.46 70.39 65.59 67.90LinGraph 29.74 24.88 27.09 44.08 36.88 40.16 66.43 61.89 64.08 70.97 66.13 68.46FullGraph 35.27?
28.84?
31.74?
48.81?
39.91?
43.92?
66.59?
62.01?
64.22?
71.11?
66.22?
68.58?Table 2: Full frame-semantic parsing precision, recall and F1 score on 2,420 test sentences.
Bold scores indicatesignificant improvements relative to SEMAFOR and (?)
denotes significant improvements over LinGraph (p < 0.05).uniform regularization hyperparameter ?
for graphconstruction was set to 10?6 and not tuned.
Foreach cross-validation split, four folds were used totrain a frame identification model, construct a graph,run label propagation and then the model was testedon the fifth fold.
This was done for all hyperpa-rameter settings, which were ?
?
{0.2, 0.5, 0.8},K ?
{5, 10, 15, 20}, ?
?
{0.01, 0.1, 0.3, 0.5, 1.0}and M ?
{2, 3, 5, 10}.
The joint setting which per-formed the best across five-folds was ?
= 0.2,K =10, ?
= 1.0,M = 2.
Similar tuning was also donefor the baseline LinGraph, where ?
was set to 0,and rest of the hyperparameters were tuned (the se-lected hyperparameters were K = 10, ?
= 0.1 andM = 2).
With the chosen set of hyperparameters,the test set was used to measure final performance.The standard evaluation script from the Se-mEval?07 task calculates precision, recall, and F1-score for frames and arguments; it also provides ascore that gives partial credit for hypothesizing aframe related to the correct one in the FrameNet lex-icon.
We present precision, recall, and F1-measuremicroaveraged across the test documents, reportlabels-only matching scores (spans must match ex-actly), and do not use named entity labels.
This eval-uation scheme follows Das et al (2010a).
Statisticalsignificance is measured using a reimplementationof Dan Bikel?s parsing evaluation comparator.136.4 ResultsTables 1 and 2 present results for frame identifica-tion and full frame-semantic parsing respectively.They also separately tabulate the results achievedfor unknown targets.
Our full model, denoted by?FullGraph,?
outperforms all the baselines for bothtasks.
Note that the Self-training model even falls13http://www.cis.upenn.edu/?dbikel/software.html#comparatorshort of the supervised baseline SEMAFOR, unlikewhat was observed by Bejan (2009) for the frameidentification task.
The model using a graph con-structed solely from the thesaurus (LinGraph) out-performs both the supervised and the self-trainingbaselines for all tasks, but falls short of the graphconstructed using the similarity metric that is a lin-ear combination of distributional similarity and su-pervised frame similarity.
This indicates that a graphconstructed with some knowledge of the superviseddata is more powerful.For unknown targets, the gains of our approachare impressive: 15.7% absolute accuracy improve-ment over SEMAFOR for frame identification, and13.7% absolute F1 improvement over SEMAFORfor full frame-semantic parsing (both significant).When all the test targets are considered, the gainsare still significant, resulting in 5.4% relative errorreduction over SEMAFOR for frame identification,and 1.3% relative error reduction over SEMAFORfor full-frame semantic parsing.Although these improvements may seem modest,this is because only 3.2% of the test set targets areunseen in training.
We expect that further gainswould be realized in different text domains, whereFrameNet coverage is presumably weaker than innews data.
A semi-supervised strategy like ours isattractive in such a setting, and future work mightexplore such an application.Our approach also makes decoding much faster.For the unknown component of the test set, SE-MAFOR takes a total 111 seconds to find the bestset of frames, while the FullGraph model takes only19 seconds to do so, thus bringing disambiguationtime down by a factor of nearly 6.
This is be-cause our model now disambiguates between onlyM = 2 frames instead of the full set of 877 framesin FrameNet.
For the full test set too, the speedup1442t = discrepancy.N t = contribution.N t = print.V t = mislead.Vf q?t (f) f q?t (f) f q?t (f) f q?t (f)?SIMILARITY 0.076 ?GIVING 0.167 ?TEXT CREATION 0.081 EXPERIENCER OBJ 0.152NATURAL FEATURES 0.066 MONEY 0.046 SENDING 0.054 ?PREVARICATION 0.130PREVARICATION 0.012 COMMITMENT 0.046 DISPERSAL 0.054 MANIPULATE INTO DOING 0.046QUARRELING 0.007 ASSISTANCE 0.040 READING 0.042 COMPLIANCE 0.041DUPLICATION 0.007 EARNINGS AND LOSSES 0.024 STATEMENT 0.028 EVIDENCE 0.038Table 3: Top 5 frames according to the graph posterior distribution q?t (f) for four targets: discrepancy.N, contri-bution.N, print.V and mislead.V.
None of these targets were present in the supervised FrameNet data.
?
marks thecorrect frame, according to the test data.
EXPERIENCER OBJ is described in FrameNet as ?Some phenomenon (theStimulus) provokes a particular emotion in an Experiencer.
?is noticeable, as SEMAFOR takes 131 seconds forframe identification, while the FullGraph model onlytakes 39 seconds.6.5 DiscussionThe following is an example from our test set show-ing SEMAFOR?s output (for one target):REASONDiscrepanciesdiscrepancy.Nbetween North Korean de-clarations and IAEA inspection findingsActionindicate that North Korea might have re-processed enough plutonium for one ortwo nuclear weapons.Note that the model identifies an incorrect frameREASON for the target discrepancy.N, in turn identi-fying the wrong semantic role Action for the under-lined argument.
On the other hand, the FullGraphmodel exactly identifies the right semantic frame,SIMILARITY, as well as the correct role, Entities.
Thisimprovement can be easily explained.
The excerptfrom our constructed graph in Figure 2 shows thesame target discrepancy.N in black, conveying thatit did not belong to the supervised data.
However,it is connected to the target difference.N drawn fromannotated data, which evokes the frame SIMILARITY.Thus, after label propagation, we expect the frameSIMILARITY to receive high probability for the targetdiscrepancy.N.Table 3 shows the top 5 frames that are assignedthe highest posterior probabilities in the distribu-tion q?t for four hand-selected test targets absent insupervised data, including discrepancy.N.
For allof them, the FullGraph model identifies the correctframes for all four words in the test data by rank-ing these frames in the top M = 2.
LinGraphalso gets all four correct, Self-training only getsprint.V/TEXT CREATION, and SEMAFOR gets none.Across unknown targets, on average the M = 2most common frames in the posterior distributionq?t found by FullGraph have q(?
)t (f) =7877 , orseven times the average across all frames.
This sug-gests that the graph propagation method is confi-dent only in predicting the top few frames out ofthe whole possible set.
Moreover, the automaticallyselected number of frames to extract per unknowntarget, M = 2, suggests that only a few meaningfulframes were assigned to unknown predicates.
Thismatches the nature of FrameNet data, where the av-erage frame ambiguity for a target type is 1.20.7 ConclusionWe have presented a semi-supervised strategy toimprove the coverage of a frame-semantic pars-ing model.
We showed that graph-based labelpropagation and resulting smoothed frame distri-butions over unseen targets significantly improvedthe coverage of a state-of-the-art semantic framedisambiguation model to previously unseen pred-icates, also improving the quality of full frame-semantic parses.
The improved parser is available athttp://www.ark.cs.cmu.edu/SEMAFOR.AcknowledgmentsWe are grateful to Amarnag Subramanya for helpful dis-cussions.
We also thank Slav Petrov, Nathan Schneider,and the three anonymous reviewers for valuable com-ments.
This research was supported by NSF grants IIS-0844507, IIS-0915187 and TeraGrid resources providedby the Pittsburgh Supercomputing Center under NSFgrant number TG-DBS110003.1443ReferencesC.
Baker, M. Ellsworth, and K. Erk.
2007.
SemEval-2007 Task 19: frame semantic structure extraction.
InProc.
of SemEval.C.
A. Bejan.
2009.
Learning Event Structures From Text.Ph.D.
thesis, The University of Texas at Dallas.Y.
Bengio, O. Delalleau, and N. Le Roux.
2006.
La-bel propagation and quadratic criterion.
In Semi-Supervised Learning.
MIT Press.H.
C. Boas.
2002.
Bilingual FrameNet dictionaries formachine translation.
In Proc.
of LREC.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In Proc.
of ACL-HLT.D.
Das, N. Schneider, D. Chen, and N. A. Smith.
2010a.Probabilistic frame-semantic parsing.
In Proc.
ofNAACL-HLT.D.
Das, N. Schneider, D. Chen, and N. A. Smith.2010b.
SEMAFOR 1.0: A probabilistic frame-semantic parser.
Technical Report CMU-LTI-10-001,Carnegie Mellon University.K.
Erk and S. Pado?.
2006.
Shalmaneser - a toolchain forshallow semantic parsing.
In Proc.
of LREC.C.
Fellbaum, editor.
1998.
WordNet: an electronic lexi-cal database.
MIT Press, Cambridge, MA.C.
J. Fillmore, C. R. Johnson, and M. R.L.
Petruck.
2003.Background to FrameNet.
International Journal ofLexicography, 16(3).C.
J. Fillmore.
1982.
Frame semantics.
In Linguistics inthe Morning Calm, pages 111?137.
Hanshin Publish-ing Co., Seoul, South Korea.M.
Fleischman, N. Kwon, and E. Hovy.
2003.
Maximumentropy models for FrameNet classification.
In Proc.of EMNLP.P.
Fung and B. Chen.
2004.
BiFrameNet: bilin-gual frame semantics resource construction by cross-lingual induction.
In Proc.
of COLING.H.
Fu?rstenau and M. Lapata.
2009.
Semi-supervised se-mantic role labeling.
In Proc.
of EACL.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3).A.-M. Giuglea and A. Moschitti.
2006.
Shallow se-mantic parsing based on FrameNet, VerbNet and Prop-Bank.
In Proc.
of ECAI 2006.D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium.R.
Johansson and P. Nugues.
2007.
LTH: semantic struc-ture extraction using nonprojective dependency trees.In Proc.
of SemEval.D.
Lin.
1993.
Principle-based parsing without overgen-eration.
In Proc.
of ACL.D.
Lin.
1994.
Principar?an efficient, broadcoverage,principle-based parser.
In Proc.
of COLING.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proc.
of COLING-ACL.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Programming, 45(3).Y.
Matsubayashi, N. Okazaki, and J. Tsujii.
2009.
Acomparative study on generalization of semantic rolesin FrameNet.
In Proc.
of ACL-IJCNLP.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.of ACL.Z.-Y.
Niu, D.-H. Ji, and C. L. Tan.
2005.
Word sensedisambiguation using label propagation based semi-supervised learning.
In Proc.
of ACL.S.
Pado?
and M. Lapata.
2005.
Cross-linguistic projec-tion of role-semantic information.
In Proc.
of HLT-EMNLP.M.
Pennacchiotti, D. De Cao, R. Basili, D. Croce, andM.
Roth.
2008.
Automatic induction of FrameNetlexical units.
In Proc.
of EMNLP.S.
P. Ponzetto and M. Strube.
2007.
Deriving a largescale taxonomy from wikipedia.
In Proc.
of AAAI.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
of EMNLP.L.
Shi and R. Mihalcea.
2004.
An algorithm for opentext semantic parsing.
In Proc.
of Workshop on RobustMethods in Analysis of Natural Language Data.L.
Shi and R. Mihalcea.
2005.
Putting pieces together:combining FrameNet, VerbNet and WordNet for ro-bust semantic parsing.
In Computational Linguis-tics and Intelligent Text Processing: Proc.
of CICLing2005.
Springer-Verlag.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InProc.
of COLING-ACL.A.
Subramanya and J.
A. Bilmes.
2009.
Entropic graphregularization in non-parametric semi-supervised clas-sification.
In Proc.
of NIPS.A.
Subramanya, S. Petrov, and F. Pereira.
2010.
EfficientGraph-based Semi-Supervised Learning of StructuredTagging Models.
In Proc.
of EMNLP.C.
A. Thompson, R. Levy, and C. D. Manning.
2003.
Agenerative model for semantic role labeling.
In Proc.of ECML.S.
Tonelli and C. Giuliano.
2009.
Wikipedia as frameinformation repository.
In Proc.
of EMNLP.X.
Zhu, Z. Ghahramani, and J. D. Lafferty.
2003.
Semi-supervised learning using gaussian fields and har-monic functions.
In Proc.
of ICML.1444
