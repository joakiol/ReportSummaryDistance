Proceedings of the 12th Conference of the European Chapter of the ACL, pages 372?379,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsImproving Mid-Range Reordering using Templates of FactorsHieu HoangSchool of InformaticsUniversity of Edinburghh.hoang@sms.ed.ac.ukPhilipp KoehnSchool of InformaticsUniversity of Edinburghpkoehn@inf.ed.ac.ukAbstractWe extend the factored translation model(Koehn and Hoang, 2007) to allow trans-lations of longer phrases composed of fac-tors such as POS and morphological tagsto act as templates for the selection and re-ordering of surface phrase translation.
Wealso reintroduce the use of alignment in-formation within the decoder, which formsan integral part of decoding in the Align-ment Template System (Och, 2002), intophrase-based decoding.Results show an increase in transla-tion performance of up to 1.0% BLEUfor out-of-domain French?English transla-tion.
We also show how this method com-pares and relates to lexicalized reordering.1 IntroductionOne of the major issues in statistical machinetranslation is reordering due to systematic word-ordering differences between languages.
Often re-ordering is best explained by linguistic categories,such as part-of-speech tags.
In fact, prior workhas examined the use of part-of-speech tags inpre-reordering schemes, Tomas and Casacuberta(2003).Re-ordering can also be viewed as composingof a number of related problems which can be ex-plained or solved by a variety of linguistic phe-nomena.
Firstly, differences between phrase or-dering account for much of the long-range re-ordering.
Syntax-based and hierarchical modelssuch as (Chiang, 2005) attempts to address thisproblem.
Shorter range re-ordering, such as intra-phrasal word re-ordering, can often be predictedfrom the underlying property of the words andits context, the most obvious property being POStags.In this paper, we tackle the issue of shorter-range re-ordering in phrase-based decoding bypresenting an extension of the factored transla-tion which directly models the translation of non-surface factors such as POS tags.
We shall call thisextension the factored template model.
We use thefact that factors such as POS-tags are less sparsethan surface words to obtain longer phrase trans-lations.
These translations are used to inform there-ordering of surface phrases.Despite the ability of phrase-based systems touse multi-word phrases, the majority of phrasesused during decoding are one word phrases, whichwe will show in later sections.
Using word trans-lations negates the implicit capability of phrasesto re-order words.
We show that the proposedextension increases the number of multi-wordphrases used during decoding, capturing the im-plicit ordering with the phrase translation, lead-ing to overall better sentence translation.
Inour tests, we obtained 1.0% increase in absolutefor French-English translation, and 0.8% increasefor German-English translation, trained on NewsCommentary corpora 1.We will begin by recounting the phrase-basedand factored model in Section 2 and describe thelanguage model and lexicalized re-ordering modeland the advantages and disadvantages of usingthese models to influence re-ordering.
The pro-posed model is described in Section 4.2 BackgroundLet us first provide some background on phrase-based and factored translation, as well as the useof part-of-speech tags in reordering.2.1 Phrase-Based ModelsPhrase-based statistical machine translation hasemerged as the dominant paradigm in machinetranslation research.
We model the translation ofa given source language sentence s into a targetlanguage sentence t with a probability distributionp(t|s).
The goal of translation is to find the besttranslation according to the modeltBEST = argmaxt p(t|s) (1)The argmax function defines the search objec-tive of the decoder.
We estimate p(t|s) by decom-1http://www.statmt.org/wmt07/shared-task.html372posing it into component modelsp(t|s) =1Z?mh?m(t, s)?m (2)where h?m(t, s) is the feature function for compo-nent m and ?m is the weight given to componentm.
Z is a normalization factor which is ignored inpractice.
Components are translation model scor-ing functions, language model, reordering modelsand other features.The problem is typically presented in log-space,which simplifies computations, but otherwise doesnot change the problem due to the monotonicity ofthe log function (hm = log h?m)log p(t|s) =?m?m hm(t, s) (3)Phrase-based models (Koehn et al, 2003) arelimited to the mapping of small contiguous chunksof text.
In these models, the source sentence s issegmented into a number of phrases s?k, which aretranslated one-to-one into target phrases t?k.
Thetranslation feature functions hTM(t, s) are com-puted as sum of phrase translation feature func-tions h?TM(t?k, s?k):hTM(t, s) =?kh?TM(t?k, s?k) (4)where t?k and s?k are the phrases that make up thetarget and source sentence.
Note that typicallymultiple feature functions for one translation tableare used (such as forward and backward probabil-ities and lexical backoff).2.2 Reordering in Phrase ModelsPhrase-based systems implicitly perform short-range reordering by translating multi-wordphrases where the component words may bereordered relative to each other.
However, multi-word phrases have to have been seen and learntfrom the training corpus.
This works better whenthe parallel corpus is large and the training corpusand input are from the same domain.
Otherwise,the ability to apply multi-word phrases is lesseneddue to data sparsity, and therefore most usedphrases are only 1 or 2 words long.A popular model for phrasal reordering is lexi-calized reordering (Tillmann, 2004) which intro-duces a probability distribution for each phrasepair that indicates the likelihood of being trans-lated monotone, swapped, or placed discontinu-ous to its previous phrase.
However, whether aphrase is reordered may depend on its neighboringphrases, which this model does not take into ac-count.
For example, the French phrase noir wouldbe reordered if preceded by a noun when translat-ing into English, as in as in chat noir, but would re-main in the same relative position when precededby a conjunction such as rouge et noir.The use of language models on the decodingoutput also has a significant effect on reorder-ing by preferring hypotheses which are more flu-ent.
However, there are a number of disadvantageswith this low-order Markov model over consecu-tive surface words.
Firstly, the model has no infor-mation about the source and may prefer orderingsof target words that are unlikely given the source.Secondly, data sparsity may be a problem, evenif language models are trained on a large amountof monolingual data which is easier to obtain thanparallel data.
When the test set is out-of-domainor rare words are involved, it is likely that the lan-guage model backs off to lower order n-grams,thus further reducing the context window.2.3 POS-Based ReorderingThis paper will look at the use of POS tags to con-dition reordering of phrases which are closely po-sitioned in the source and target, such as intra-clausal reordering, however, we do not explicitsegment along clausal boundaries.
By mid-rangereordering we mean a maximum distortion ofabout 5 or 6 words.The phrase-based translation model is gener-ally believed to perform short-range reorderingadequately.
It outperforms more complex mod-els such as hierarchical translation when the mostof the reordering in a particular language pair isreasonably short (Anonymous, 2008), as is thecase with Arabic?English.
However, phrase-basedmodels can fail to reorder words or phrases whichwould seem obvious if it had access to the POStags of the individual words.
For example, a trans-lation from French to English will usually cor-rectly reorder the French phrase with POS tagsNOUN ADJECTIVE if the surface forms exists inthe phrase table or language model, e.g.,Union Europe?enne ?
European UnionHowever, phrase-based models may not reordereven these small two-word phrases if the phraseis not in the training data or involves rare words.This situation worsens for longer phrases wherethe likelihood of the phrase being previously un-373seen is higher.
The following example has a sourcePOS pattern NOUN ADJECTIVE CONJUNCTIONADJECTIVE but is incorrectly ordered as the sur-face phrase does not occur in training,difficulte?s e?conomiques et socials?
economic and social difficultiesHowever, even if the training data does not con-tain this particular phrase, it contains many similarphrases with the same underlying POS tags.
Forexample, the correct translation of the correspond-ing POS tags of the above translationNOUN ADJ CONJ ADJ?
ADJ CONJ ADJ NOUNis typically observed many times in the trainingcorpus.The alignment information in the training cor-pus shows exactly how the individual words in thisphrase should be distorted, along with the POStag of the target words.
The challenge addressedby this paper is to integrate POS tag phrase trans-lations and alignment information into a phrase-based decoder in order to improve reordering.2.4 Factor Model DecompositionFactored translation models (Koehn and Hoang,2007) extend the phrase-based model by inte-grating word level factors into the decoding pro-cess.
Words are represented by vectors of fac-tors, not simple tokens.
Factors are user-definableand do not have any specific meaning within themodel.
Typically, factors are obtained from lin-guistic tools such as taggers and parsers.The factored decoding process can be decom-posed into multiple steps to fully translate the in-put.
Formally, this decomposes Equation 4 furtherinto sub-component models (also called transla-tion steps)h?TM(t?, s?)
=?ih?iTM(t?, s?)
(5)with an translation feature function h?iTM for eachtranslation step for each factor (or sets of factors).There may be also generation models which createtarget factors from other target factors but we ex-clude this in our presentation for the sake of clar-ity.Decomposition is a convenient and flexiblemethod for integrating word level factors intophrase-based decoding, allowing source and tar-get sentences to be augmented with factors, whileat the same time controlling data sparsity.
How-ever, decomposition also implies certain indepen-dence assumptions which may not be justified.Various internal experiments show that decompo-sition may decrease performance and that betterresults can often be achieved by simply translat-ing all factors jointly.
While we can gain benefitfrom adding factor information into phrase-baseddecoding, our experience also shows the short-comings of decomposing phrase translation.3 Related WorkEfforts have been made to integrate syntactic in-formation into the decoding process to improve re-ordering.Collins et al (2005) reorder the source sentenceusing a sequence of six manually-crafted rules,given the syntactic parse tree of the source sen-tence.
While the transformation rules are specificto the German parser that was used, they couldbe adapted to other languages and parsers.
Xiaand McCord (2004) automatically create rewriterules which reorder the source sentence.
Zhangand Zens (2007) take a slightly different approachby using chunk level tags to reorder the sourcesentence, creating a confusion network to repre-sent the possible reorderings of the source sen-tence.
All these approaches seek to improve re-ordering by making the ordering of the source sen-tence similar to the target sentence.Costa-jussa` and Fonollosa (2006) use a twostage process to reorder translation in an n-grambased decoder.
The first stage uses word classes ofsource words to reorder the source sentence intoa string of word classes which can be translatedmonotonically to the target sentences in the sec-ond stage.The Alignment Template System (Och, 2002)performs reordering by translating word classeswith their corresponding alignment information,then translates each surface word to be consis-tent with the alignment.
Tomas and Casacuberta(2003) extend ATS by using POS tags instead ofautomatically induced word classes.Note the limitation of the existing work of POS-driven reordering in phrase-based models: the re-ordering model is separated from the translationmodel and the two steps are pipelined, with pass-ing the 1-best reordering or at most a lattice to thetranslation stage.
The ATS models do provide anintegrated approach, but their lexical translation is374limited to the word level.In contrast to prior work, we present a inte-grated approach that allows POS-based reorderingand phrase translation.
It is also open to the use ofany other factors, such as driving reordering withautomatic word classes.Our proposed solution is similar to structuraltemplates described in Phillips (2007) which wasapplied to an example-based MT system.4 Translation Using Templates of FactorsA major motivation for the introduction of fac-tors into machine translation is to generalizephrase translation over longer segments using lesssparse factors than is possible with surface forms.
(Koehn and Hoang, 2007) describes various strate-gies for the decomposition of the decoding intomultiple translation models using the Moses de-coder.
We shall focus on POS-tags as an exampleof a less-sparsed factor.Decomposing the translation by separately de-coding the POS tags and surface forms is be theobvious option, which also has a probabilistic in-terpretation.
However, this combined factors intotarget words which don?t exist naturally and bringdown translation quality.
Therefore, the decodingis constrained by decomposing into two transla-tion models; a model with POS-tag phrase pairsonly and one which jointly translates POS-tagsand surface forms.
This can be expressed usingfeature-functionsh?TM(t?, s?)
= h?posTM (t?, s?
)h?surfaceTM (t?, s?)
(6)Source segment must be decoded by both trans-lation models but only phrase pairs where the over-lapping factors are the same are used.
As an ad-ditional constraint, the alignment information isretained in the translation model from the train-ing data for every phrase pair, and both translationmodels must produce consistent alignments.
Thisis expressed formally in Equation 7 to 9.An alignment is a relationship which maps asource word at position i to a target word at po-sition j:a : i?
j (7)Each word at each position can be aligned tomultiple words, therefore, we alter the alignmentrelation to express this explicitly:a : i?
j (8)where J is the set of positions, jJ , that I isaligned to in the other language.
Phrase pairsfor each translation model are used only if theycan satisfy condition 9 for each position of everysource word covered.
?a, b  T ?p : JpaJpb 6= ?
(9)where Jpa is the alignment information for trans-lation model, a, at word position, p and T is the setof translation models.4.1 TrainingThe training procedure is identical to the fac-tored phrase-based training described in (Koehnand Hoang, 2007).
The phrase model retains theword alignment information found during train-ing.
Where multiple alignment exists in the train-ing data for a particular phrase pair, the most fre-quent is used, in a similar manner to the calcula-tion of the lexicalized probabilities.Words positions which remain unaligned are ar-tificially aligned to every word in the other lan-guage in the phrase translation during decoding toallow the decoder to cover the position.4.2 DecodingThe beam search decoding algorithm is unchangedfrom traditional phrase-based and factored decod-ing.
However, the creation of translation options isextended to include the use of factored templates.Translation options are the intermediate represen-tation between the phrase pairs from the transla-tion models and the hypotheses in the stack de-coder which cover specific source spans of a sen-tence and are applied to hypotheses to create newhypotheses.In phrase-based decoding, a translation optionstrictly contains one phrase pair.
In factored de-coding, strictly one phrase pair from each trans-lation model is used to create a translation op-tions.
This is possible only when the segmenta-tion is identical for both source and target span ofeach phrase pair in each translation model.
How-ever, this constraint limits the ability to use longPOS-tag phrase pairs in conjunction with shortersurface phrase pairs.The factored template approach extend factoreddecoding by constructing translation options froma single phrase pair from the POS-tag translationmodel, but allowing multiple phrase pairs from375other translation models.
A simplified stack de-coder is used to compose phrases from the othertranslation models.
This so called intra-phrase de-coder is constrained to creating phrases which ad-heres to the constraint described in Section 4.
Theintra-phrase decoder uses the same feature func-tions as the main beam decoder but uses a largerstack size due to the difficulty of creating com-pleted phrases which satisfy the constraint.
Everysource position must be covered by every transla-tion model.The intra-phrase decoder is used for each con-tiguous span in the input sentence to producetranslation options which are then applied as usualby the main decoder.5 ExperimentsWe performed our experiments on the news com-mentary corpus2 which contains 60,000 parallelsentences for German?English and 43,000 sen-tences for French?English.
Tuning was done ona 2000 sentence subset of the Europarl corpus(Koehn, 2005) and tested on a 2000 sentence Eu-roparl subset for out-of-domain, and a 1064 newscommentary sentences for in-domain.The training corpus is aligned using Giza++(Och and Ney, 2003).
To create POS tag trans-lation models, the surface forms on both sourceand target language training data are replaced withPOS tags before phrases are extracted.
The taggersused were the Brill Tagger (Brill, 1995) for En-glish, the Treetagger for French (Schmid, 1994),and the LoPar Tagger (Schmidt and Schulte imWalde, 2000) for German.
The training script sup-plied with the Moses toolkit (Koehn et al, 2007)was used, extended to enable alignment informa-tion of each phrase pair.
The vanilla Moses MERTtuning script was used throughout.Results are also presented for models trained onthe larger Europarl corpora3.5.1 German?EnglishWe use as a baseline the traditional, non-factoredphrase model which obtained a BLEU score of14.6% on the out-of-domain test set and 18.2% onthe in-domain test set (see Table 1, line 1).POS tags for both source and target languageswere augmented to the training corpus and used inthe decoding and an additional trigram language2http://www.statmt.org/wmt07/shared-task.html3http://www.statmt.org/europarl/# Model out-domain in-domain1 Unfactored 14.6 18.22 Joint factors 15.0 18.83 Factored template 15.3 18.8Table 1: German?English results, in %BLEU# Model out-domain in-domain1 Unfactored 19.6 23.12 Joint factors 19.8 23.03 Factored template 20.6 24.1Table 2: French?English resultsmodel was used on the target POS tags.
Thisincreased translation performance (line 2).
Thismodel has the same input and output factors, andthe same language models, as the factored modelwe will present shortly and it therefore offers afairer comparison of the factored template modelthan the non-factored baseline.The factored template model (line 3) outper-forms the baseline on both sets and the joint factormodel on the out-of-domain set.However, we believe the language pairGerman?English is not particularly suited forthe factored template approach as many of theshort-range ordering properties of German andEnglish are similar.
For example, ADJECTIVENOUN phrases are ordered the same in bothlanguages.5.2 French?EnglishRepeating the same experiments for French?English produces bigger gains for the factoredtemplate model.
See Table 4 for details.
Usingthe factored template model produces the best re-sult, with gains of 1.0 %BLEU over the unfactoredbaseline on both test sets.
It also outperforms thejoint factor model.5.3 Maximum Size of TemplatesTypical phrase-based model implementation use amaximum phrase length of 7 but such long phrasesare rarely used.
Long templates over POS may bemore valuable.
The factored template models wereretrained with increased maximum phrase lengthbut this made no difference or negatively impactedtranslation performance, Figure 1.However, using larger phrase lengths over 5words does not increase translation performance,376Figure 1: Varying max phrase lengthas had been expected.
Translation is largely un-affected until the maximum phrase length reaches10 when performance drops dramatically.
This re-sults suggested that the model is limited to mid-range reordering.6 Lexicalized Reordering ModelsThere has been considerable effort to improve re-ordering in phrase-based systems.
One of the mostwell known is the lexicalized reordering model(Tillmann, 2004).The model uses the same word alignment that isused for phrase table construction to calculate theprobability that a phrase is reordered, relative tothe previous and next source phrase.6.1 SmoothingTillmann (2004) proposes a block orientationmodel, where phrase translation and reorderingorientation is predicted by the same probabilitydistribution p(o, s?|t?).
The variant of this imple-mented in Moses uses a separate phrase translationmodel p(s?|t?)
and lexicalized reordering modelp(o|s?, t?
)The parameters for the lexicalized reorderingmodel are calculated using maximum likelihoodwith a smoothing value ?p(o|s?, t?)
=count(o, s?, t?)
+ ??o?
(count(o, s?, t?)
+ ?
)(10)where the predicted orientation o is either mono-tonic, swap or discontinuous.The effect of smoothing lexical reordering ta-bles on translation is negligible for both surfaceforms and POS tags, except when smoothing isdisabled (?=0).
Then, performance decreasesmarkedly, see Figure 2 for details.
Note that theFigure 2: Effect of smoothing on lexicalized re-ordering# Model out-domain in-domain1 Unfactored 19.6 23.11a + word LR 20.2 24.02 Joint factors 19.8 23.02a + POS LR 20.1 24.02b + POS LR + word LR 20.3 24.13 Factored template 20.6 24.13a + POS LR 20.6 24.3Table 3: Extending the models with lexicalized re-ordering (LR)un-smoothed setting is closer to the block orienta-tion model by Tillmann (2004).6.2 Factors and Lexicalized ReorderingThe model can easily be extended to take advan-tage of the factored approach available in Moses.In addition to the lexicalized reordering modeltrained on surface forms (see line 1a in Table 3),we also conducted various experiments with thelexicalized reordering model for comparison.In the joint factored model, we have both sur-face forms and POS tags available to train the lex-icalized reordering models on.
The lexicalized re-ordering model can be trained on the surface form,the POS tags, jointly on both factors, or indepen-dent models can be trained on each factor.
It canbe seen from Table 3 that generalizing the reorder-ing model on POS tags (line 2a) improves perfor-mance, compared to the non-lexicalized reorder-ing model (line 2).
However, this performancedoes not improve over the lexicalized reorderingmodel on surface forms (line 1a).
The surface andPOS tag models complement each other to give anoverall better BLEU score (line 2b).In the factored template model, we add a POS-377based lexicalized reordering model on the level ofthe templates (line 3a).
This gives overall the bestperformance.
However, the use of lexicalized re-ordering models in the factored template modelonly shows improvements in the in-domain testset.Lexicalized reordering model on POS tags infactored models underperforms factored templatemodel as the latter includes a larger context of thesource and target POS tag sequence, while the for-mer is limited to the extent of the surface wordphrase.7 AnalysisA simple POS sequence that phrase-based systemsoften fail to reorder is the French?EnglishNOUN ADJ ?
ADJ NOUNWe analyzed a random sample of such phrasesfrom the out-of-domain corpus.
The baselinesystem correctly reorders 58% of translations.Adding a lexicalized reordering model or the fac-tored template significantly improves the reorder-ing to above 70% (Figure 3).Figure 3: Percentage of correctly ordered NOUNADJ phrases (100 samples)A more challenging phrase to translate, such asNOUN ADJ CONJ ADJ ?
ADJ CONJ ADJ NOUNwas judge in the same way and the results show thevariance between the lexicalized reordering andfactored template model (Figure 4).The factored template model successfully usesPOS tag templates to enable longer phrases tobe used in decoding.
It can be seen from Fig-ure 5, that the majority of input sentence is de-coded word-by-word even in a phrase-based sys-tem.
However, the factored template configura-Figure 4: Percentage of correctly ordered NOUNADJ CONJ ADJ phrases (69 samples)Figure 5: Length of source segmentation when de-coding out-of-domain test settion contains more longer phrases which enhancesmid-range reordering.8 Larger training corporaIt is informative to compare the relative per-formance of the factored template model whentrained with more data.
We therefore used the Eu-roparl corpora to train and tuning the models forFrench to English translation.
The BLEU scoresare shown below, showing no significant advan-tage to adding POS tags or using the factored tem-plate model.
This result is similar to many otherswhich have shown that the large amounts of addi-tional data negates the improvements from bettermodels.# Model out-domain in-domain1 Unfactored 31.8 32.22 Joint factors 31.6 32.03 Factored template 31.7 32.2Table 4: French?English results, trained on Eu-roparl corpus3789 ConclusionWe have shown the limitations of the current fac-tored decoding model which restrict the use oflong phrase translations of less-sparsed factors.This negates the effectiveness of decomposingthe translation process, dragging down translationquality.An extension to the factored model was imple-mented which showed that using POS tag transla-tions to create templates for surface word trans-lations can create longer phrase translation andlead to higher performance, dependent on lan-guage pair.For French?English translation, we obtained a1.0% BLEU increase on the out-of-domain and in-domain test sets, over the non-factored baseline.The increase was also 0.4%/0.3% when using alexicalized reordering model in both cases.In future work, we would like to apply the fac-tored template model to reorder longer phrases.We believe that this approach has the potential forlonger range reordering which has not yet been re-alized in this paper.
It also has some similarity toexample-based machine translation (Nagao, 1984)which we would like to draw experience from.We would also be interested in applying this toother language pairs and using factor types otherthan POS tags, such as syntactic chunk labels orautomatically clustered word classes.AcknowledgmentsThis work was supported by the EuroMa-trix project funded by the European Com-mission (6th Framework Programme) andmade use of the resources provided bythe Edinburgh Compute and Data Facility(http://www.ecdf.ed.ac.uk/).
TheECDF is partially supported by the eDIKTinitiative (http://www.edikt.org.uk/).ReferencesAnonymous (2008).
Understanding reordering in statisticalmachine translation.
In (submitted for publucation).Brill, E. (1995).
Transformation-based error-driven learningand natural language processing: A case study in part ofspeech tagging.
Computational Linguistics, 21(4).Chiang, D. (2005).
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 263?270, Ann Arbor, Michigan.Association for Computational Linguistics.Collins, M., Koehn, P., and Kucerova, I.
(2005).
Clauserestructuring for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05), pages 531?540,Ann Arbor, Michigan.
Association for Computational Lin-guistics.Costa-jussa`, M. R. and Fonollosa, J.
A. R. (2006).
Statisti-cal machine reordering.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural Language Pro-cessing, pages 70?76, Sydney, Australia.
Association forComputational Linguistics.Koehn, P. (2005).
Europarl: A parallel corpus for statisticalmachine translation.
In Proceedings of the Tenth MachineTranslation Summit (MT Summit X), Phuket, Thailand.Koehn, P. and Hoang, H. (2007).
Factored translation models.In Proceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL),pages 868?876.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,E.
(2007).
Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and Poster Ses-sions, pages 177?180, Prague, Czech Republic.
Associa-tion for Computational Linguistics.Koehn, P., Och, F. J., and Marcu, D. (2003).
Statistical phrasebased translation.
In Proceedings of the Joint Conferenceon Human Language Technologies and the Annual Meet-ing of the North American Chapter of the Association ofComputational Linguistics (HLT-NAACL).Nagao, M. (1984).
A framework of a mechanical translationbetween japanese and english by analogy principle.
InProceedings of Artificial and Human Intelligence.Och, F. J.
(2002).
Statistical Machine Translation: FromSingle-Word Models to Alignment Templates.
PhD thesis,RWTH Aachen, Germany.Och, F. J. and Ney, H. (2003).
A systematic comparison ofvarious statistical alignment models.
Computational Lin-guistics, 29(1):19?52.Phillips, A.
B.
(2007).
Sub-phrasal matching and struc-tural templates in example-based mt.
In Theoretical andMethodological Issues in Machine Translation, Prague,Czech Republic.Schmid, H. (1994).
Probabilistic part-of-speech tagger usingdecision trees.
In International Conference on New meth-ods in Language Processing.Schmidt, H. and Schulte im Walde, S. (2000).
RobustGerman noun chunking with a probabilistic context-freegrammar.
In Proceedings of the International Conferenceon Computational Linguistics (COLING).Tillmann, C. (2004).
A unigram orientation model for statis-tical machine translation.
In Proceedings of the Joint Con-ference on Human Language Technologies and the AnnualMeeting of the North American Chapter of the Associationof Computational Linguistics (HLT-NAACL).Tomas, J. and Casacuberta, F. (2003).
Combining phrase-based and template-based alignment models in statisticaltranslation.
In IbPRIA.Xia, F. and McCord, M. (2004).
Improving a statisticalMT system with automatically learned rewrite patterns.In Proceedings of Coling 2004, pages 508?514, Geneva,Switzerland.
COLING.Zhang, Y. and Zens, R. (2007).
Improved chunk-level re-ordering for statistical machine translation.
In Interna-tional Workshop on Spoken Language Translation.379
