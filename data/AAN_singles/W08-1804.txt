Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 26?33Manchester, UK.
August 2008Passage Retrieval for Question Answering using Sliding WindowsMahboob Alam KhalidISLA, University of Amsterdammahboob@science.uva.nlSuzan VerberneRadboud University Nijmegens.verberne@let.ru.nlAbstractThe information retrieval (IR) commu-nity has investigated many different tech-niques to retrieve passages from large col-lections of documents for question answer-ing (QA).
In this paper, we specifically ex-amine and quantitatively compare the im-pact of passage retrieval for QA using slid-ing windows and disjoint windows.
Weconsider two different data sets, the TREC2002?2003 QA data set, and 93 why-questions against INEX Wikipedia.
Wediscovered that, compared to disjoint win-dows, using sliding windows results in im-proved performance of TREC-QA in termsof TDRR, and in improved performance ofwhy-QA in terms of success@n and MRR.1 IntroductionIn question answering (QA), text passages are animportant intermediary between full documentsand exact answers.
They form a very natural unitof response for QA systems (Tellex et al, 2003)and it is known from user studies that users pre-fer answers to be embedded in paragraph-sizedchunks (Lin et al, 2003) because they can providethe context of an answer.
Therefore, almost allstate-of-the-art QA systems implement some tech-nique for extracting paragraph-sized fragments oftext from a large corpus.Most QA systems have a pipeline architectureconsisting of at least three components: ques-tion analysis, document/passage retrieval, and an-swer extraction (Hirschman and Gaizauskas, 2001;c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Voorhees, 2001).
The quality of a QA sys-tem heavily depends on the effectiveness of theintegrated retrieval system (second step of thepipeline): if a retrieval system fails to find any rel-evant documents for a question, further processingsteps to extract an answer will inevitably fail too(Monz, 2003).
This motivates the need to studypassage retrieval for QA.There are two common approaches to retriev-ing passages from a corpus: one is to index eachpassage as separate document and retrieve them assuch.
The other option is to first retrieve relevantdocuments for a given question and then retrievepassages from the retrieved documents.
The pas-sages themselves can vary in size and degree ofoverlap.
Their size can be fixed as a number ofwords or characters, or varying with the semanticcontent (Hearst and Plaunt, 1993) or the structureof the text (Callan, 1994).
The overlap betweentwo adjacent passages can be either zero, in whichcase we speak of disjoint passages, or the passagesmay be overlapping, which we refer to as slidingpassages.In this paper, we compare the effectiveness ofseveral passage retrieval techniques with respect totheir usefulness for QA.
Our main interest is thecontribution of sliding passages as apposed to dis-joint passages, and we will experiment with a num-ber of retrieval models.
We evaluate the retrievalapproaches on two different QA tasks: (1) factoid-QA, as defined by the test collection provided byTREC (Voorhees, 2002; Voorhees, 2003), and (2)a relatively new problem in the QA field: that ofanswering why-questions (why-QA).The remainder of the paper is organized as fol-lows.
In the next section, we describe related workon passage retrieval for QA and we motivate whatthe main contribution of the current paper is.
In26section 3 we describe our general set-up for pas-sage retrieval in both QA tasks that we consider.
Insection 4, we present the results of the experimentson TREC-QA data, and in section 5 we present ourresults on why-QA.
Section 6 gives an overall con-clusion.2 Related workThe use of passage retrieval for QA has been stud-ied before.
For example, (Tellex et al, 2003)performed a quantitative evaluation of passage re-trieval algorithms for QA.
They compared differ-ent passage retrieval algorithms in the context oftheir QA system.
Their system first returns aranked list of 200 documents and then applies dif-ferent passage retrieval algorithms to the retrieveddocuments.
They find that the performance of pas-sage retrieval depends on the performance of thepre-applied document retrieval step, and thereforethey suggest that document and passage retrievaltechnology should be developed independently.A similar message is conveyed by (Roberts andGaizauskas, 2004).
They investigate different ap-proaches to passage retrieval for QA.
They iden-tify each paragraph as a seperate passage.
Theyfind that the optimal approach is to allow multiplepassages per document to be returned and to scorepassages independently of their source document.
(Tiedemann, 2007) studies the impact of doc-ument segmentation approaches on the retrievalperformance of IR for Dutch QA.
He finds thatsegmentation based on document structure suchas the use of paragraph markup (discourse-basedsegmentation) works well with standard informa-tion retrieval techniques.
He tests various othertechniques for document segmentation and variouspassage sizes.
In his experimental setting, largertext units (such as documents) produce better per-formance in passage retrieval.
Tiedemann com-pares different sizes of discourse-based segmen-tation: sentences, paragraphs and documents.
Hefinds that larger text units result in a large searchspace for subsequent QA modules and hence re-duce the overall performance of the QA system.That is why we do not conduct experiments withdifferent passage sizes in this paper: it is difficultto measure the outcome of such experiments in-dependently of the specific answer extraction sys-tem.
We adopt Tiedemann?s best strategy of docu-ment segmentation strategy, i.e., paragraph-based,but with equally sized passages instead.3 General experiment set-upThe main purpose of our experiments is to studythe contribution of sliding windows as apposed todisjoint windows in the context of QA.
Therefore,in our experiment setup, we have kept fixed theother segmentation variables, passage size and de-gree of overlap.
We set out to examine two differ-ent strategies of document segmentation (disjointand sliding passages) with a number of retrievalmodels for two different QA tasks: TREC factoid-QA and why-QA.3.1 Retrieval modelsWe use the Lemur retrieval engine1 for passage re-trieval because it provides a flexible support fordifferent types of retrieval models including vec-tor space models and language models.
In thispaper we have selected two vector space mod-els: TFIDF and Okapi BM25 (Robertson andWalker, 1999), and one language model based onKullback-Leibler (KL) divergence (Lafferty andZhai, 2001).The TFIDF weighting scheme is often used ininformation retrieval.
There are several variationsof the TFIDF weighting scheme that can effect theperformance significantly.
The Lemur toolkit pro-vides a variant of the TFIDF model based on theOkapi TF formula (Robertson et al, 1995).Lemur also provides the implementation of theoriginal Okapi BM25 model, and we have usedthis model with default values of 1.2 for k1, 0.75for b and 7 for k3 as suggested by (Robertsonand Walker, 1999).
The KL-divergence retrievalmodel, which implements the cross entropy of thequery model with respect to the document model,is a standard metric for comparing distributions,which has proven to work well in IR experimentsin the past.
To address the data sparseness prob-lem during model estimation, we use the Dirichletsmoothing method (Zhai and Lafferty, 2004) withdefault parameter values provided in the Lemurtoolkit.Currently, however, the Lemur 2 does not sup-port direct passage retrieval.
For these experi-ments, therefore, we first need to segment docu-ments into passages before indexing them into the1Lemur toolkit: http://www.lemurproject.org2Lemur and Indri are different search engines.
Indri pro-vides the #passage operator, but it doesn?t consider para-graph boundaries or sentence boundaries for constructing pas-sages.27Lemur retrieval engine.
Our segmenting strategyis explained below.3.2 Passage identificationFor our experiments, we take into account twodifferent corpora: AQUAINT and the WikipediaXML corpus as used in INEX (Denoyer and Gal-linari, 2006).
The AQUAINT corpus consists ofnews articles from the Associated Press, New YorkTimes, and Xinhua News Agency (English ver-sion) from 1996 to 2000.
The Wikipedia XMLcollection consists of 659,388 articles as they oc-cured in the online Wikipedia in the summer of2006.
As we have discussed in Section 2, (Tiede-mann, 2007) discovered that discourse-based seg-mentation into paragraphs works well with stan-dard information retrieval techniques.
They alsoobserve that larger retrieval units produce better re-sults for passage retrieval, since larger units havehigher chance to cover the required information.Therefore, we decide to segment each documentinto similar sized passages while taking into ac-count complete paragraphs only.For document segmentation, our method firstdetects sentences in the text using punctuationmarks as separators, and then paragraphs usingempty lines as separators.
Sentence boundariesare necessary because we aim at retrieving pas-sages that do not contain any broken sentences.The required passages are identified by aligningover paragraph boundaries (merging paragraphsinto units until they have the required length ,i.e.500 characters).
The disjoint passages do not shareany content with each other, and the sliding pas-sages slide with the difference of one paragraphboundary, i.e., we start forming a new passagefrom beginning of each paragraph of the docu-ment.
If paragraph boundaries are not detected,then these sliding passages are half-overlappedwith each other.For the Wikipedia XML corpus, we have foundthat documents have already been annotated with<p> elements.
Thus we consider these elemensas paragraph boundaries instead of empty lines aswe did for the AQAINT corpus.
We observe thatsome textual parts of the documents are not cov-ered by the XML paragraph boundaries.
Thereforewe have extended the existing paragraph bound-aries such that the missing text fragments becomepart of the paragraphs.We split both corpora into disjoint and slid-ing windows as we have discussed above.
Aftersplitting the 1.03M documents of the AQUAINT-1 collection we have 14.2M sliding passages, and4.82M disjoint passages.
And similarly we got4.1M sliding passages and 2M disjoint passagesfrom the Wikipedia XML collection of 659,388documents.3.3 Evaluation metricsFor our experiments, we use the following metricsfor evaluation:Mean reciprocal rank (MRR) at n is the mean(calculated over all questions) of the recipro-cal rank (which is 1 divided by the rank or-dinal) of the highest ranked relevant (i.e.
an-swer bearing) passage.
RR is zero for a ques-tion if no relevant passage is returned by thesystem at limit n.Success at n for a question is 1 if the answerto this question is found in top n passagesfetched up by our system.
Success@n is av-eraged over all questions.Total document reciprocal rank (TDRR)(Bilotti et al, 2004) is the sum of all recipro-cal ranks of all answer bearing passages perquestion (averaged over all questions).
Thevalue of TDRR is maximum if all retrievedpassages are relevant.
TDRR is an extensionof MRR that favors a system that ranks morethat one relevant passage higher than allnon-relevant passages.
This way, TDRRextends MRR with a notion of recall.When we compare retrieval performance of tworetrieval settings (such as the use of disjoint versussliding windows), then we obtain a list of pairedscores.
That?s why we use the Wilcoxon signed-rank test to show the statistical significance of theimprovements.In summary, we experiment with three retrievalmodels in Lemur: TFIDF, Okapi, and a languagemodel based on the Kullback-Leibler divergence.For each of these retrieval models, we evaluate theuse of both sliding and disjoint passages.
Thismakes a total of six retrieval settings.4 Evaluating passage retrieval forTREC-QAAs test collection for factoid QA, we use a standardset of 822 question/answer pairs from the TREC28QA tasks of 2002-2003.
For evaluation of thepassage retrieval approaches that we consider, wecompute strict scores as defined by (Tellex et al,2003).
Strict scoring means that a retrieved pas-sage is considered relevant if the passage not onlymatches one of the answer patterns provided byNIST, but its associated document is also listed asone of the relevant documents assessed by NIST.
(Bilotti et al, 2004) have reviewed 109 factoidquestions of the TREC-2002 task and they haveextended the existing set of relevant documents byadding more relevant documents.
We have also in-cluded this extended list of relevant documents forthese questions in our experiment setup.We evaluate the impact of disjoint and slidingwindows on passage retrieval for QA using threedifferent retrieval models, using the MRR@n, Suc-cess@n and TDRR@n metrics as described in sec-tion 3.3.
Table 1 shows the evaluation results (bestscores for each measure in bold face).
The ex-periment results show that language model basedon Kullback-Leibler divergence shows better per-formance than two vector space models for bothtypes of windows retrieval according to MRR, suc-cess@n and TDRR evaluation metrics.4.1 DiscussionIn a pipeline QA system, the answer extractionmodule depends on the performance of passage re-trieval.
If more answer bearing passages are pro-vided in the stream, then there is a high chanceof selecting the correct answer from the streamin later stages of QA.
(Roberts and Gaizauskas,2004) have also discussed the importance of thisaspect of passage retrieval for QA.
They have mea-sured the answer redundancy of a retrieval systemwhich measures how many answer bearing pas-sages are returned per question at limit n. (Tiede-mann, 2007) have also used this metric and arguethat high redundancy is desired to make it easierfor the answer extraction module to spot possibleanswers.
We consider TDRR as the most impor-tant measure for the passage retrieval task sinceit does not only measure the redundancy of a re-trieval system but also measures how much im-provement there is in returning the relevant pas-sages at top ranks.According to TDRR@n in table 1, retrieval ofsliding windows outperforms retrieval of disjointwindows at all limits of n for all retrieval mod-els.
For n = 100, the improvement is significantat p = 0.01 level.
This high value of TDRR@nsuggests that segmenting the documents into slid-ing windows is a better choice in order to return asmany relevant passages as possible at top ranks.If we consider Success@n as evaluation mea-sure instead of TDRR, retrieval of disjoint win-dows outperforms retrieval of sliding windows.We think that one of the reasons for this behaviouris that since sliding windows overlap with theirneighbours, they are more pair-wise similar thandisjoint windows.
Therefore, it is possible that forsome non-answered questions many irrelevant pas-sages are returned at top ranks and that relevantpassages are surpressed down.5 Evaluating passage retrieval forwhy-QAIn the previous section, we showed that for TRECdata, the choice of the retrieval model and the typeof windows to be retrieved influence on the re-trieval performance.
We found that for the TRECdata, a language modeling approach (based onKullback-Leibler divergence) on sliding windowsgives the best results in terms of TDRR.
In thissection, we aim to find out what the optimal pas-sage retrieval approach is for a very different typeof QA, namely why-QA.5.1 Background of why-QA systemdevelopmentIn (Verberne et al, 2008), we present an approachfor why-QA that is based on paragraph retrievalfrom the INEX Wikipedia corpus (Denoyer andGallinari, 2006).
Our system for why-QA con-sists of two modules: a passage retrieval mod-ule and a re-ranking module.
In earlier retrievalexperiments, we used the Wumpus retrieval sys-tem (Buttcher, 2007), and we defined passagessimply by the XML paragraph markup <p>.
Pas-sage ranking in Wumpus is done by the QAP pas-sage scoring algorithm (Buttcher et al, 2004).The second module of our why-system is a re-ranking step that uses syntactic features of thequestion and the retrieved answers for adapting thescores of the answers and changing the ranking or-der.
The weights of the re-ranking features havebeen optimized by training on our question answerdata in five folds3 using a genetic algorithm.
Welet Wumpus retrieve and rank 150 paragraphs per3In five turns, we tune the feature weights on four of thefive folds and evaluate them on the fifth29Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows(SW).
??
indicates a significant improvements of sliding windows over disjoint windows at the p = 0.01level.MRR Success@n TDRRn retrieval model DW SW DW SW DW SW10 TFIDF 0.327 0.326 51.8% 50.1% 0.465 0.637Okapi 0.322 0.328 51.9% 51.2% 0.459 0.649KL 0.355 0.345 55.7% 51.3% 0.518 0.710100 TFIDF 0.336 0.386 54.1% 53.3% 0.517 0.819?
?Okapi 0.333 0.339 77.0% 76.2% 0.535 0.835?
?KL 0.363 0.353 77.1% 75.2% 0.525 0.902??question.
This number of 150 answers was chosenas a trade-off between covering as many as possi-ble of the relevant answers retrieved by Wumpus,and the system load that was needed for automaticsyntactic analysis of all answers in the second (re-ranking) module of the system.
For evaluation ofthe results, we performed manual assessment ofall answers retrieved, starting at the highest-rankedanswer and ending as soon as we encountered arelevant answer4.The results for our original why-system are inTable 2.
We show the results in terms of suc-cess@n and MRR@n. As opposed to the evalua-tion of TREC-QA, we do not consider TDRR asevaluation measure for experiments on why-QA.This is because in why-QA, we are only interestedin the top-ranked answer-bearing passage.
For cal-culating TDRR, assessment of all 150 retrieved an-swers would be necessary.Table 2 shows that success@150 for the retrievalmodule (Wumpus/QAP) is 73.1%.
This means thatfor 26.9% of the questions, no relevant answer isretrieved in the first module.
Re-ranking the an-swers cannot increase MRR for these questions,since none of the 150 answers in the result listis relevant.
We consider a success@150 score of73.1% to be quite low.
We aim to improve theperformance of our system by optimizing its firstmodule, passage retrieval.We experiment with a number of passage re-trieval approaches in order to reach better retrievalin the first module of our system.
We aim to findout which type of retrieval model and what win-dow type (disjoint or sliding) gives optimal resultsfor retrieving passages relevant to why-questions.If the retrieval performance indeed goes up, we4We didn?t need to assess the tail since we were only in-terested in the highest-ranked relevant answer for calculatingMRR and success@nwill apply our re-ranking module to the newlyretrieved data to see what overall system perfor-mance we can reach with the new retrieval ap-proach.5.2 Data and evaluation setupFor development and testing purposes, we use theWebclopedia question set by (Hovy et al, 2002).This set contains questions that were asked to theonline QA system answers.com.
805 of thesequestions are why-questions.
We manually in-spect a sample of 400 of the Webclopedia why-questions.
Of these, 93 have an answer in theWikipedia XML corpus (see section 3).
Manualextraction of one correct answer for each of thesequestions results in a set of 93 why-questions andtheir reference answer.In order to be able to do fast evaluation of thedifferent evaluation settings, we manually createan answer pattern for each of the questions in ourset.
These answer patterns are based on a set of 93reference answers (one answer per question) thatwe have manually extracted from the Wikipediacorpus.
An answer pattern is a regular expres-sion that defines which of the retrieved passagesare considered a relevant answer to the input ques-tion.As opposed to the answer patterns provided byNIST for the evaluation of factoid QA (see sec-tion 4), our answer patterns for why-questions arerelatively strict.
A why-answer can be formulatedin many different ways with different words, whichmay not all be in the answer pattern.
For a factoidquestion such as ?When was John Lennon born?
?,the answer is only one phrase, and the answerpattern is short and unambiguous, i.e.
/1940/.However, if we consider the why-question ?Whyare some organ transplants unsuccessful?
?, theanswer pattern cannot be stated in one phrase.
For30Table 2: Results for the original why-QA pipeline systemsuccess@10 success@150 MRR@150Wumpus/QAP Retrieval 43.0% 73.1% 0.260+ Re-ranking module 54.8% 73.1% 0.380this example, we created the following answerpattern based on the pre-extracted referenceanswer5: /.
*immune system.*foreigntissues.*destroy.*/.
It is however pos-sible that a relevant answer is formulated in away that does not match this regular expression.Thus, the use of answer patterns for the evaluationof why-QA leads to conservative results: somerelevant answers may be missed in the evaluationprocedure.After applying the answer patterns, we count thequestions that have at least one relevant answerin the top 10 and the top 150 of the results (suc-cess@10, success@150).
For the highest rankedrelevant answer per question, we determine the re-ciprocal rank (RR).
If there is no correct answerretrieved by the system at n = 150, the RR is 0.Over all questions, we calculate the MRR@150.5.3 Passage retrieval resultsWe segment and index the Wikipedia corpus as de-scribed in section 3 and run all six retrieval set-tings on our set of 93 why-questions.
For consis-tent evaluation, we applied the answer patterns thatwe created to the newly retrieved Lemur data aswell as to the original Wumpus output.The retrieval results for all settings are in Table3.
We show both success@10 and success@150,and MRR@150 for each setting.
Success@150 isimportant if we consider the current results as inputfor the re-ranking module.
As explained before,re-ranking can only be successful if at least one rel-evant answer is retrieved by the retrieval module.For each measure (s@10, s@150 and MRR@150),the score of the highest-scoring setting is printed inbold face.As expected, the evaluation of the Wumpus datawith the use of answer patterns gives somewhatlower scores than evaluation based on manual as-sessment of all answers (table 2).
This confirmsour idea that the use of answer patterns for why-QA leads to conservative results.
Thus we can5The pre-extracted reference answer is: ?This is becausea normal healthy human immune system can distinguish for-eign tissues and attempts to destroy them, just as it attemptsto destroy infective organisms such as bacteria and viruses.
?state that the Lemur scores shown in table 3 arenot overestimated and therefore reliable.Since we are using the output of the passage re-trieval module as input for our re-ranking mod-ule, we are mainly interested in the scores forsuccess@150.
For the four retrieval models, wesee that TFIDF seems to score somewhat betteron retrieving sliding windows in terms of suc-cess@150 than Okapi and the Kullback-Leiblerlanguage model.
On the other hand, Kullback-Leibler and QAP seem to perform better on retriev-ing disjoint windows.
However, these differencesare not significant at the p = 0.01 level.
For thedifferences between disjoint and sliding windowsfor all retrieval models together, we see that re-trieval of sliding windows gives significantly bet-ter results than disjoint windows in terms of suc-cess@150 (p < 0.001).5.4 The influence of passage retrieval on ourpipeline systemAs described in section 5.1, our system is apipeline: after passage retrieval, we apply a re-ranking module that uses syntactic information forre-scoring the results from the retrieval module.
Asinput for our re-ranking module we use the out-put of the retrieval setting with the highest suc-cess@150 score: Lemur/TFIDF on sliding win-dows.
For 81.7% of the questions in our set,Lemur/TFIDF retrieved an answer in the top-150.This means that the maximum success@10 scorethat we can obtain by re-ranking is 81.7%.For weighting the feature values, we re-use theweights that we had earlier found from training onour set of 93 questions and the 150 answers thatwere retrieved by Wumpus.
We again take intoaccount five-fold cross validation for evaluation.For a detailed description of our re-ranking mod-ule and the syntactic features that we exploit, werefer to (Verberne et al, 2008).The results from re-ranking are in Table 4.In the table, four system versions are compared:(1) the original Wumpus/QAP module, (2) theoriginal why-pipeline system: Wumpus/QAP withre-ranking, (3) TFIDF-sliding and (4) the new31Table 3: Results for passage retrieval on why-questions against Wikipedia using disjoint windows (DW)and sliding windows (SW)Success@10 Success@150 MRR@150Retrieval model DW SW DW SW DW SWBaseline: Wumpus/QAP 40.9% 72.0% 0.229Lemur/TFIDF 43.0% 45.2% 71.1% 81.7% 0.247 0.338Lemur/Okapi 41.9% 44.1% 67.7% 79.6% 0.243 0.320Lemur/KL 48.9% 50.0% 72.8% 77.2% 0.263 0.324pipeline system: TFIDF-sliding with re-ranking.We again show MRR, success@10 and suc-cess@150.
For each measure, the score of thehighest-scoring setting is printed in bold face.After applying our re-ranking module (right bot-tom setting), we find a significant improvementover bare TFIDF (left bottom setting).
In termsof MRR, we also see an improvement over the re-sults that we had obtained by re-ranking the Wum-pus/QAP output (right top setting).
However, suc-cess@10 does not show significant improvement.The improvement that the re-ranking module givesis smaller for the TFIDF retrieval results (MRRgoes from 0.338 to 0.359) than for the QAP results(MRR increases from 0.260 to 0.328).
We suspectthat this may be due to the fact that we used featureweights for re-ranking that we had earlier obtainedfrom training on the Wumpus/QAP data (see sec-tion 5.4).
It would be better to re-train our featureweights on the Lemur data.
Probably, re-rankingcan then make a bigger contribution than it doesnow for the Lemur data.6 Overall conclusionIn this paper we have investigated the contribu-tion of sliding windows as apposed to disjoint win-dows with different retrieval modules for two dif-ferent QA tasks: the TREC-QA 2002?2003 taskand why-QA.For the TREC factoid-QA task, we have foundthat retrieval of sliding windows outperfoms re-trieval of disjoint windows in returning as manyrelevant passages as possible on top ranks (accord-ing to the TDRR metric).
The experimental resultsshow that a language model based on Kullback-Leibler divergence gives better performance thantwo vector space models for both types of win-dows retrieval according to MRR, success@n andTDRR evaluation metrics.
We found that thenumber of answered questions (success@n) wasslightly lower when we used sliding windows forpassage retrieval than disjoint windows, but wethink one of the reasons is that sliding windowsare more homogeneous than disjoint windows, andtherefore for some questions more irrelevant pas-sages are returned at top ranks and relevant pas-sages are surpressed down.For the task of retrieving answers to why-questions from Wikipedia data, we found that thebest retrieval model is TFIDF, and sliding win-dows give significantly better results than disjointwindows.
We also found better performance forour complete why-pipeline system after applyingour existing re-ranking module to the passages re-trieved with TFIDF-sliding.In general, we find that for QA, sliding win-dows give better results than disjoint windows inthe passage retrieval step.
The best scoring re-trieval model depends on the task under consid-eration, because the nature of the documents andquestion sets differ.
This shows that for each spe-cific QA task, different retrieval models should beconsidered.In the future, we aim to boost passage retrievalfor QA even more by applying query expansiontechniques that are specific to the QA tasks thatwe consider, i.e.
TREC factoid-QA and why-QA.ReferencesBilotti, M.W., B. Katz, and J. Lin.
2004.
What worksbetter for question answering: Stemming or morpho-logical query expansion.
In Proceedings of the SI-GIR 2004 Workshop IR4QA: Information Retrievalfor Question Answering, July.Buttcher, S., C.L.A.
Clarke, and G.V.
Cormack.
2004.Domain-specific synonym expansion and validationfor biomedical information retrieval (multitext ex-periments for trec 2004).Buttcher, S. 2007.
The wumpus search engine.http://www.wumpus-search.org/.Callan, James P. 1994.
Passage-level evidence in doc-ument retrieval.
In SIGIR, pages 302?310.32Table 4: Results for the why-QA pipeline system for best-scoring passage retrieval setting comparedagainst the Wumpus baseline, for both bare retrieval and the complete system with re-rankingSuccess@10 Success@150 MRRRetrieval model Bare +Re-rank Bare +Re-rank Bare +Re-rankBaseline: Wumpus/QAP-disjoint 43.0% 54.8% 73.1% 73.1% 0.260 0.328Lemur/TFIDF-sliding 45.2% 55.9% 81.7% 81.7% 0.338 0.359Denoyer, L. and P. Gallinari.
2006.
The WikipediaXML corpus.
ACM SIGIR Forum, 40(1):64?69.Hearst, Marti A. and Christian Plaunt.
1993.
Subtopicstructuring for full-length document access.
InACM-SIGIR, 1993, pages 59?68.Hirschman, L. and R. Gaizauskas.
2001.
Natural lan-guage question answering: the view from here.
Nat.Lang.
Eng., pages 275?300.Hovy, E.H., U. Hermjakob, and D. Ravichandran.2002.
A question/answer typology with surface textpatterns.
In Proceedings of the Human LanguageTechnology conference (HLT), San Diego, CA.Lafferty, J. and C. Zhai.
2001.
Document languagemodels, query models, and risk minimization for in-formation retrieval.
In In Proceedings of SIGIR?01,pages 111?119.Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh,B.
Katz, and D.R.
Karger.
2003.
The role of con-text in question answering systems.
Conference onHuman Factors in Computing Systems, pages 1006?1007.Monz, Christof.
2003.
Document retrieval in the con-text of question answering.
In ECIR, pages 571?579.Roberts, I. and R. Gaizauskas.
2004.
Evaluating pas-sage retrieval approaches for question answering.
InIn Proceedings of ECIR , 2004.Robertson, Stephen E. and Steve Walker.
1999.Okapi/keenbow at trec-8.
In Text Retrieval Confer-ence.Robertson, Stephen E., Steve Walker, MichelineHancock-Beaulieu, and Gatford M. 1995.
Okapi attrec-3.
In Text Retrieval Conference, pages 109?26.Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.2003.
Quantitative evaluation of passage retrieval al-gorithms for question answering.
In In SIGIR con-ference on Research and development in informaionretrieval, 2003, pages 41?47.Tiedemann, Jo?rg.
2007.
Comparing document seg-mentation strategies for passage retrieval in questionanswering.
In Proceedings of RANLP 07, Borovets,Bulgaria.Verberne, Suzan, Lou Boves, Nelleke Oostdijk, andPeter-Arno Coppen.
2008.
Using Syntactic Infor-mation for Improving Why-Question Answering.
InProceedings of The 22nd International Conferenceon Computational Linguistics (COLING 2008).Voorhees, Ellen.
2001.
Overview of trec 2001 questionanswering track.
In In Proceedings of TREC.Voorhees, Ellen.
2002.
Overview of trec 2002 questionanswering track.
In In Proceedings of TREC.Voorhees, Ellen.
2003.
Overview of trec 2003 questionanswering track.
In In Proceedings of TREC.Zhai, ChengXiang and John D. Lafferty.
2004.
A studyof smoothing methods for language models appliedto information retrieval.
ACM Trans.
Inf.
Syst., pages179?214.33
