Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 438?442,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsExploiting Linguistic Features for Sentence CompletionAubrie M. WoodsCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USAamwoods@cmu.eduAbstractThis paper presents a novel approach toautomated sentence completion based onpointwise mutual information (PMI).
Fea-ture sets are created by fusing the varioustypes of input provided to other classesof language models, ultimately allowingmultiple sources of both local and dis-tant information to be considered.
Fur-thermore, it is shown that additional preci-sion gains may be achieved by incorporat-ing feature sets of higher-order n-grams.Experimental results demonstrate that thePMI model outperforms all prior modelsand establishes a new state-of-the-art re-sult on the Microsoft Research SentenceCompletion Challenge.1 IntroductionSkilled reading is a complex cognitive process thatrequires constant interpretation and evaluation ofwritten content.
To develop a coherent picture,one must reason from the material encounteredto construct a mental representation of meaning.As new information becomes available, this repre-sentation is continually refined to produce a glob-ally consistent understanding.
Sentence comple-tion questions, such as those previously featuredon the Scholastic Aptitude Test (SAT), were de-signed to assess this type of verbal reasoning abil-ity.
Specifically, given a sentence containing 1-2blanks, the test taker was asked to select the cor-rect answer choice(s) from the provided list of op-tions (College Board, 2014).
A sample sentencecompletion question is illustrated in Figure 1.To date, relatively few publications have fo-cused on automatic methods for solving sentencecompletion questions.
This scarcity is likely at-tributable to the difficult nature of the task, whichCertain clear patterns in the metamorphosisof a butterfly indicate that the process is??-.
(A) systematic(B) voluntary(C) spontaneous(D) experimental(E) clinicalFigure 1: An example sentence completion ques-tion (The Princeton Review, 2007).occasionally involves logical reasoning in additionto both general and semantic knowledge (Zweiget al, 2012b).
Fundamentally, text completion isa challenging semantic modeling problem, and so-lutions require models that can evaluate the globalcoherence of sentences (Gubbins and Vlachos,2013).
Thus, in many ways, text completion epito-mizes the goals of natural language understanding,as superficial encodings of meaning will be insuf-ficient to determine which responses are accurate.In this paper, a model based on pointwise mu-tual information (PMI) is proposed to measure thedegree of association between answer options andother sentence tokens.
The PMI model considersmultiple sources of information present in a sen-tence prior to selecting the most likely alternative.The remainder of this report is organized as fol-lows.
Section 2 describes the high-level character-istics of existing models designed to perform auto-mated sentence completion.
This prior work pro-vides direct motivation for the PMI model, intro-duced in Section 3.
In Section 4, the model?s per-formance on the Microsoft Research (MSR) Sen-tence Completion Challenge and a data set com-prised of SAT questions are juxtaposed.
Finally,Section 5 offers concluding remarks on this topic.4382 BackgroundPrevious research expounds on various architec-tures and techniques applied to sentence comple-tion.
Below, models are roughly categorized onthe basis of complexity and type of input analyzed.2.1 N-gram ModelsAdvantages of n-gram models include their abil-ity to estimate the likelihood of particular tokensequences and automatically encode word order-ing.
While relatively simple and efficient to trainon large, unlabeled text corpora, n-gram modelsare nonetheless limited by their dependence on lo-cal context.
In fact, such models are likely to over-value sentences that are locally coherent, yet im-probable due to distant semantic dependencies.2.2 Dependency ModelsDependency models circumvent the sequentialitylimitation of n-gram models by representing eachword as a node in a multi-child dependency tree.Unlabeled dependency language models assumethat each word is (1) conditionally independent ofthe words outside its ancestor sequence, and (2)generated independently from the grammatical re-lations.
To account for valuable information ig-nored by this model, e.g., two sentences that dif-fer only in a reordering between a verb and its ar-guments, the labeled dependency language modelinstead treats each word as conditionally indepen-dent of the words and labels outside its ancestorpath (Gubbins and Vlachos, 2013).In addition to offering performance superior ton-gram models, advantages of this representationinclude relative ease of training and estimation, aswell as the ability to leverage standard smoothingmethods.
However, the models?
reliance on out-put from automatic dependency extraction meth-ods and vulnerability to data sparsity detract fromtheir real-world practicality.2.3 Continuous Space ModelsNeural networks mitigate issues with data sparsityby learning distributed representations of words,which have been shown to excel at preserving lin-ear regularities among tokens.
Despite drawbacksthat include functional opacity, propensity towardoverfitting, and elevated computational demands,neural language models are capable of outper-forming n-gram and dependency models (Gub-bins and Vlachos, 2013; Mikolov et al, 2013;Mnih and Kavukcuoglu, 2013).Log-linear model architectures have been pro-posed to address the computational cost associatedwith neural networks (Mikolov et al, 2013; Mnihand Kavukcuoglu, 2013).
The continuous bag-of-words model attempts to predict the current wordusing n future and n historical words as context.In contrast, the continuous skip-gram model usesthe current word as input to predict surroundingwords.
Utilizing an ensemble architecture com-prised of the skip-gram model and recurrent neu-ral networks, Mikolov et al (2013) achieved priorstate-of-the-art performance on the MSR SentenceCompletion Challenge.3 PMI ModelThis section describes an approach to sentencecompletion based on pointwise mutual informa-tion.
The PMI model was designed to account forboth local and distant sources of information whenevaluating overall sentence coherence.Pointwise mutual information is aninformation-theoretic measure used to dis-cover collocations (Church and Hanks, 1990;Turney and Pantel, 2010).
Informally, PMIrepresents the association between two words, iand j, by comparing the probability of observingthem in the same context with the probabilities ofobserving each independently.The first step toward applying PMI to the sen-tence completion task involved constructing aword-context frequency matrix from the train-ing corpus.
The context was specified to in-clude all words appearing in a single sentence,which is consistent with the hypothesis that itis necessary to examine word co-occurrences atthe sentence level to achieve appropriate granu-larity.
During development/test set processing, allwords were converted to lowercase and stop wordswere removed based on their part-of-speech tags(Toutanova et al, 2003).
To determine whether aparticular part-of-speech tag type did, in fact, sig-nal the presence of uninformative words, tokensassigned a hypothetically irrelevant tag were re-moved if their omission positively affected perfor-mance on the development portion of the MSRdata set.
This non-traditional approach, selectedto increase specificity and eliminate dependenceon a non-universal stop word list, led to the re-moval of determiners, coordinating conjunctions,439Figure 2: The dependency parse tree for Question 17 in the MSR data set.
Words that share a grammati-cal relationship with the missing word rising are underscored.
Following stop word removal, the featureset for this question is [darkness, was, hidden].pronouns, and proper nouns.1Next, feature setswere defined to capture the various sources of in-formation available in a sentence.
While featureset number and type is configurable, compositionvaries, as sets are dynamically generated for eachsentence at run time.
Enumerated below are thethree feature sets utilized by the PMI model.1.
Reduced Context.
This feature set con-sists of words that remain following the pre-processing steps described above.2.
Dependencies.
Sentence words that sharea semantic dependency with the candidateword(s) are included in this set (Chen andManning, 2014).
Absent from the set ofdependencies are words removed during thepre-processing phase.
Figure 2 depicts an ex-ample dependency parse tree along with fea-tures provided to the PMI model.3.
Keywords.
Providing the model with a col-lection of salient tokens effectively increasesthe tokens?
associated weights.
An analo-gous approach to the one described for stopword identification was applied to discoverthat common nouns consistently hold greatersignificance than other words assigned hypo-thetically informative part-of-speech tags.Let X represent a word-context matrix with nrows and m columns.
Row xi:corresponds to wordi and column x:jrefers to context j.
The term x(i,j)indicates how many times word i occurs in contextj.
Applying PMI to X results in the n x m matrix Y,where term y(i,j) is defined by (1).
To avoid overlypenalizing words that are unrelated to the context,1Perhaps counterintuitively, most proper nouns are unin-formative for sentence completion, since they refer to specificnamed entities (e.g.
people, locations, organizations, etc.
).the positive variant of PMI is considered, in whichnegative scores are replaced with zero (4).P (i, j) =x(i, j)?ni=1?mj=1x(i, j)(1)P (i?)
=?mj=1x(i, j)?ni=1?mj=1x(i, j)(2)P (?j) =?ni=1x(i, j)?ni=1?mj=1x(i, j)(3)pmi(i, j) = max{0, log(P (i, j)P (i?
)P (?j))}(4)In addition, the discounting factor described byPantel and Lin (2002) is applied to reduce bias to-ward infrequent words (7).mincontext = min(n?k=1x(k, j),m?k=1x(i, k))(5)?
(i, j) =x(i, j)x(i, j) + 1?mincontextmincontext+ 1(6)dpmi(i, j) = pmi(i, j) ?
?
(i, j) (7)similarity(i, S) =?j?Sdpmi(i, j) ?
?
(8)The PMI model evaluates each possible re-sponse to a sentence completion question by sub-stituting each candidate answer, i, in place of theblank and scoring the option according to (8).This equation measures the semantic similarity be-tween each candidate answer and all other wordsin the sentence, S. Prior to being summed, individ-ual PMI values associated with a particular word i440and context word j are multiplied by ?, which re-flects the number of feature sets containing j. Ulti-mately, the candidate option with the highest sim-ilarity score is selected as the most likely answer.Using the procedure described above, addi-tional feature sets of bigrams and trigrams werecreated and subsequently incorporated into thesemantic similarity assessment.
This extendedmodel accounts for both word- and phrase-level information by considering windowed co-occurrence statistics.4 Experimental Evaluation4.1 Data SetsSince its introduction, the Microsoft ResearchSentence Completion Challenge (Zweig andBurges, 2012a) has become a commonly usedbenchmark for evaluating semantic models.
Thedata is comprised of material from nineteenth-century novels featured on Project Gutenberg.Each of the 1,040 test sentences contains a singleblank that must be filled with one of five candidatewords.
Associated candidates consist of the cor-rect word and decoys with similar distributionalstatistics.To further validate the proposed method, 285sentence completion problems were collectedfrom SAT practice examinations given from 2000-2014 (College Board, 2014).
While the MSRdata set includes a list of specified training texts,there is no comparable material for SAT ques-tions.
Therefore, the requisite word-context ma-trices were constructed by computing token co-occurrence frequencies from the New York Timesportion of the English Gigaword corpus (Parker etal., 2009).4.2 ResultsThe overall accuracy achieved on the MSR andSAT data sets reveals that the PMI model is ableto outperform prior models applied to sentencecompletion.
Table 1 provides a comparison of theaccuracy values attained by various architectures,while Table 2 summarizes the PMI model?s per-formance given feature sets of context words, de-pendencies, and keywords.
Recall that the n-gramvariant reflects how features are partitioned.It appears that while introducing phrase-levelinformation obtained from higher-order n-gramsleads to gains in precision on the MSR data set,the same cannot be stated for the set of SAT ques-Language Model MSRRandom chance 20.00N-gram [Zweig (2012b)] 39.00Skip-gram [Mikolov (2013)] 48.00LSA [Zweig (2012b)] 49.00Labeled Dependency [Gubbins (2013)] 50.00Dependency RNN [Mirowski (2015)] 53.50RNNs [Mikolov (2013)] 55.40Log-bilinear [Mnih (2013)] 55.50Skip-gram + RNNs [Mikolov (2013)] 58.90PMI 61.44Table 1: Best performance of various models onthe MSR Sentence Completion Challenge.
Valuesreflect overall accuracy (%).Features MSR SATUnigrams 58.46 58.95Unigrams + Bigrams 60.87 58.95Unigrams + Bigrams + Trigrams 61.44 58.95Table 2: PMI model performance improvements(% accurate) from incorporating feature sets ofhigher-order n-grams.tions.
The most probable explanation for thisis twofold.
First, informative context words aremuch less likely to occur within 2-3 tokens ofthe target word.
Second, missing words, whichare selected to test knowledge of vocabulary, arerarely found in the training corpus.
Bigrams andtrigrams containing these infrequent terms are ex-tremely uncommon.
Regardless of sentence struc-ture, the sparsity associated with higher-order n-grams guarantees diminishing returns for largervalues of n. When deciding whether or not to in-corporate this information, it is also important toconsider the significant trade-off with respect toinformation storage requirements.5 ConclusionThis paper described a novel approach to answer-ing sentence completion questions based on point-wise mutual information.
To capture unique in-formation stemming from multiple sources, sev-eral features sets were defined to encode both lo-cal and distant sentence tokens.
It was shown thatwhile precision gains can be achieved by augment-ing these feature sets with higher-order n-grams, asignificant cost is incurred as a result of the in-creased data storage requirements.
Finally, the su-periority of the PMI model is demonstrated by itsperformance on the Microsoft Research SentenceCompletion Challenge, during which a new state-of-the-art result was established.441ReferencesDanqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 740?750.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.The College Board.
2014.
Sat reading practicequestions: Sentence completion.
Retrieved fromhttps://sat.collegeboard.org/practice/sat-practice-questions-reading/sentence-completion.Joseph Gubbins and Andreas Vlachos.
2013.
De-pendency language models for sentence completion.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1405?1410.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Workshop Proceedings ofthe International Conference on Learning Represen-tations.Piotr Mirowski and Andreas Vlachos.
2015.
De-pendency recurrent neural language models for sen-tence completion.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing, pages 511?517.
Association for Computational Linguistics.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In Advances in Neural Information Pro-cessing Systems 26, pages 2265?2273.
Curran Asso-ciates, Inc.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of the EighthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 613?619.Association for Computing Machinery.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English gigaword fourthedition ldc2009t13.The Princeton Review.
2007.
11 Practice Tests for theSAT and PSAT, 2008 Edition.
Random House, Inc.,New York City, NY.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the Human Language Technol-ogy Conference of the North American Chapter ofthe Association for Computational Linguistics, vol-ume 1, pages 252?259.
Association for Computa-tional Linguistics.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Geoffrey Zweig and Christopher J.C. Burges.
2012a.A challenge set for advancing language modeling.In Proceedings of the NAACL-HLT 2012 Workshop:Will We Ever Really Replace the N-gram Model?
Onthe Future of Language Modeling for HLT, pages29?36.
Association for Computational Linguistics.Geoffrey Zweig, John C. Platt, Christopher Meek,Christopher J.C. Burges, Ainur Yessenalina, andQiang Liu.
2012b.
Computational approaches tosentence completion.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics, volume 1, pages 601?610.
Associationfor Computational Linguistics.442
