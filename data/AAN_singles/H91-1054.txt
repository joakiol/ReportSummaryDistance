A Study on Speaker-Adaptive Speech RecognitionX.D.
HuangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTSpeaker-independent sys em is desirable in many applicationswhere speaker-specific data do not exist.
However, if speaker-dependent data are available, the system could be adapted tothe specific speaker such that the error rate could be signifi-cantly reduced.
In this paper, DARPA Resource Managementtask is used as the domain to investigate the performanceof speaker-adaptive speech recognition.
Since adaptation isbased on speaker-independent sys ems with only limited adap-tation data, a good adaptation algorithm should be consistentwith the speaker-independent parameter stimation criterion,and adapt hose parameters that are less sensitive to the limitedtraining data.
Two parameter sets, the codebook mean vectorand the output distribution, are regarded to be most important.They are modified in the framework of maximum likelihoodestimation criterion according to the characteristics of eachspeaker.
In order to reliably estimate those parameters, outputdistributions are shared with each other if they exhibit certainacoustic similarity.
In addition to modify these parameters,speaker normalization with neural networks is also studied inthe hope that acoustic data normalization will not only rapidlyadapt he system but also enhance the robustness of speaker-independent speech recognition.
Preliminary results indicatethat speaker differences can be well minimized.
In compar-ison with speaker-independent speech recognition, the errorrate has been reduced from 4.3% to 3.1% by only using pa-rameter adaptation techniques, with 40 adaptation sentencesfor each speaker.
When the number of speaker adaptationsentences i comparable to that of speaker-dependent train-ing, speaker-adaptive recognition works better than the bestspeaker-dependent recognition results on the same test set,which indicates the robustness of speaker-adaptive speechrecognition.1 INTRODUCTIONSpeaker-independent speech recognition systems could pro-vide users with a ready-to-use ystem \[1, 2, 3, 4\].
There is noneed to collect speaker-specific data to train the system, butcollect data from a variety of speakers to reliably model manydifferent speakers.
Speaker-independent systems are defi-nitely desirable in many applications where speaker-specificdata do not exist.
On the other hand, if speaker-dependentdata are available, the system could be adapted to a specificspeaker to further reduce the error rate.
The problem ofspeaker-dependent systems is that for large-vocabulary con-tinuous peech recognition, half an hour of speech from thespecific speaker is generally needed to reliably estimate sys-tem parameters.
The problem of speaker-independent sys emsis that the error rate of speaker-independent speech recogni-tion systems is generally two to three times higher than thatof speaker-dependent speech recognition systems \[2, 3\].
Alogical compromise for a practical system is to start with aspeaker-independent system, and then adapt the system toeach individual user.Since adaptation is based on the speaker-independent sys-tem with only limited adaptation data, a good adaptation al-gorithm should be consistent with speaker-independent pa-rameter estimation criterion, and adapt hose parameters thatare less sensitive to the limited training data.
Two parametersets, the codebook mean vector and the output distribution, aremodified in the framework of maximum likelihood estimationcriterion according to the characteristics of each speaker.
Inaddition to modify those parameters, peaker normalizationusing neural networks is also studied in the hope that acousticdata normalization will not only rapidly adapt he system butalso enhance the robustness of speaker-independent speechrecognition.The codebook mean vector can represent the essential char-acteristics of different speakers, and can be rapidly estimatedwith only limited training data \[5, 6, 7\].
Because of this, itis considered to be the most important parameter set.
Thesemi-continuous hidden Markov model (SCHMM) \[8\] is agood tool to modify the codebook for each speaker.
With ro-bust speaker-independent models, the codebook is modifiedaccording to the SCHMM structure such that the SCHMMlikelihood can be maximized for the given speaker.
Thisestimation procedure considers both phonetic and acousticinformation.
Another important parameter set is the outputdistribution (weighting coefficients) of the SCHMM.
Sincethere are too many parameters in the output distributions,direct use of the SCHMM would not lead to any improve-ment.
The speaker-dependent ou put distributions are thusshared (by clustering) with each other if they exhibit cer-tain acoustic similarity.
Analogous to Bayesian learning \[9\],speaker-independent estimates can then bc interpolated withthe clustered speaker-dependent output distribution.In addition to modify codebook and output distribution pa-rameters, speaker normalization techniques are also studiedin the hope that speaker normalization can not only adapt hesystem rapidly but also enhance the robustness of speaker-independent speech recognition \[10\].
Normalization of cep-strum has also achieved many successful results in environ-ment adaptation \[11\].
The normalization techniques proposedhere involve cepstrum transformation f any target speaker to278the reference speaker.
For each cepstrum vector A2, the nor-realization function Jr(,12) is defined such that the SCHMMprobability Pr(Jr(A;)\[.Ad) can be maximized, where .A4 canbe either speaker-independent, or speaker-dependent models;and .T'(A2) can be either a simple function like.,4,12 +/3, or anycomplicated nonlinear function.
Thus, a speaker-dependentfunction Jr(A2) can be used to normalize the voice of anytarget speaker to a chosen reference speaker, or a speaker-independent function Jr(h2) can be built to reduce speakerdifferences before speaker-independent training is involvedsuch that the speaker-independent models are more accurate.In this paper, DARPA Resource Management task is used asthe domain to investigate he performance of speaker-adaptivespeech recognition.
An improved speaker-independentspeech recognition system, SPHINX \[12\], is used as the base-line system here.
The error ate for the RM2 test set, consistingof two male (JLS and LPN) and two female (BJW and JRM)speakers with 120 sentences for each, is 4.3%.
This resultis based on June 1990 system \[13\].
Recent results using theshared SCHMM is not included, which led to additional 15 %error reduction \[12\].Proposed techniques have been evaluated with the RM2test set.
With 40 adaptation sentences (randomly extractedfrom training set with triphone coverage around 20%) foreach speaker, the parameter adaptation algorithms reducedthe error rate to 3.1%.
In comparison with the best speaker-independent result on the same test set, the error ate is reducedby more than 25% As the proposed algorithm can be usedto incrementally adapt the speaker-independent system, theadaptation sentences i incrementally increased to 300-600.With only 300 adaptation sentences, the error rate is lowerthan that of the best speaker-dependent system on the sametest set (trained with 600 sentences).
For speaker normaliza-tion, two experiments were carried out.
In the first experiment,two transformation matrix .,4 and/3 are defined such that thespeaker-independent SCHMM probability Pr(.Ah2 +/31.Ad)is maximized.
The error ate for the same test set with speaker-independent models is 3.9%.
This indicates that the lineartransformation is insufficient to bridge the difference amongspeakers.
Because of this, the multi-layer perceptron (MLP)with the back-propagation algorithm \[14, 15\] is employed forcepstrum transformation.
When the speaker-dependent modelis used, the recognition error ate for other speakers i 41.9%,which indicates vast differences of different speakers.
How-ever, when 40 speaker-dependent training sentences are usedto build the MLP, the error rate is reduced to 6.8%, whichdemonstrated the ability of MLP-based speaker normaliza-tion.The paper is organized as follows.
In Section 2, the base-line system for this study is described.
Section 3 describesthe techniques used for speaker-adaptive speech recognition,which consists of codebook adaptation, output distributionadaptation, and cepstrum normalization.2 BASEL INE SYSTEMLarge-vocabulary speaker-independent continuous speechrecognition has made significant progress during the pastyears \[1, 2, 3, 4\].
Sphinx, a state-of-the-art speaker-independent speech recognition system developed at CMU\[1\], has achieved high word recognition accuracy with theintroduction and usage of the following techniques: (1) mul-tiple VQ codebooks.
In order to incorporate the multipleknowledge sources and minimize VQ errors, multiple vectorquantized codebooks incorporating LPC cepstrum, differen-tial cepstrum, second order differential cepstrum, and log-power parameters were used \[13\]; (2) generalized triphonemodels.
Triphones have been successfully used by \[16, 17\].However, many contexts are quite similar, and can be com-bined.
Clustering contexts leads to fewer, and thus moretrainable, models \[18\]; (3) function-word-dependent phonemodels.
These models were used to model phones in functionwords, which are typically short, poorly-articulated wordssuch as the, a, in, and; (4) between-word coarticulation mod-eling.
The concept of triphone modeling was extended tothe word boundary, which leads to between-word triphonemodels \[19\]; (5) semi-continuous models.
SCHMMs mutu-ally optimize the VQ codebook and HMM parameters under aunified probabilistic framework \[20\], which greatly enhancesthe robustness in comparison with the discrete HMM \[12\];(6) speaker-clustered models.
Another advantage to use theSCHMM is that it requires less training data in comparisonwith the discrete HMM.
Therefore, speaker-clustered models(male/female in this study) were employed to improve therecognition accuracy \[ 12\].The above system was evaluated on the June 90 (RM2) testset, which consists of 480 sentences spoken by four speak-ers.
The evaluation results are shown in Table 1.
This willbe referred as the baseline system in comparison with bothspeaker-dependent andspeaker-adaptive systems.
Recent re-suits using the shared istribution modeling have not yet in-cluded, which led to additional 15% error reduction \[12\].Speaker 3990 Training SentWord-Pair Grammar Error RateBJW 3.1%JLS 4.8%JRM 5.8%LPN 3.6%4.3% AverageTable 1: Speaker-independent r sults with RM2 test set.The same technology was extended for speaker-dependentspeech recognition with 600/2400 training sentences for eachspeaker \[21\].
The SCHMM parameters and VQ code-book were estimated jointly starting with speaker-independentmodels.
Results are listed in Table 2.
The error rate of thespeaker-dependent system can be reduced by three times incomparison with the speaker-independent sys em, albeit thiscomparison is not fair since the speaker-independent sys em istrained with 3990 sentences from about 100 speakers.
How-ever, these results clearly indicate the importance of speaker-dependent training data, and effects of speaker variability inthe speaker-independent system.
If speaker-dependent dataor speaker-normalization techniques are available, the errorrate may be significantly reduced.279Speaker 600 Training Sent 2400 Training sentError Rate Error RateBJW 1.6% 1.0%JLS 4.4% 2.7%JRM 2.3% 1.5%LPN 2.1% 0.4%Average 2.6 % 1.4%Table 2: Speaker-dependent results with RM2 test set.from each speaker are listed in Table 3.
Detailed results for40 adaptive sentences are listed in Table 4.Systems Word Pair Grammar ErrorWithout adapt 4.3 %5 adapt-sent 3.8%40 adapt-sent 3.6%i 50 adapt-sent 3.5 %Table 3: Adaptation results with the SCHMM.3 SPEAKER-ADAPT IVE  SYSTEMLast section clearly demonstrated the importance of speaker-dependent data, and requirements of speaker normalizationmechanism for speaker-independent sys em design.
This sec-tion will describe several techniques to adapt the speaker-independent system so that an initially speaker-independentsystem can be rapidly improved as a speaker uses the system.Speaker normalization techniques that may have a significantimpact on both speaker-adaptive and speaker-independentspeech recognition are also examined.3.1 Codebook adaptationThe SCHMM has been proposed to extend the discrete HMMby replacing discrete output probability distributions with acombination of the original discrete output probability dis-tributions and continuous pdf of a codebook \[8, 20\], In com-parison with the conventional codebook adaptation techniques\[5,6, 7\], the SCHMM can jointly reestimate both the codebookand HMM parameters in order to achieve an optimal code-book/model combination according to the maximum likeli-hood criterion.
The SCHMM can thus be readily appliedto speaker-adaptive speech recognition by reestimating thecodebook.With robust speaker-independent models, the codebook ismodified according to the SCHMM structure such that theSCHMM likelihood can be maximized for a given speaker.Here, both phonetic and acoustic information are consideredin the codebook mapping procedure since Pr(XI.A4), theprobability of acoustic observations ?d given the model .A/l,is directly maximized.
To elaborate, the posterior probabil-ity Ai (t) is first computed based on the speaker-independentmodel \[20\].
Ai (t) measures the similarity that acoustic vectorat time t will be quantized with codeword i.
The ith meanvector #i of the codebook can then be computed withIn this study, the SCHMM is used to reestimate he meanvector only.
Three iterations are carried out for each speaker.The error rates with 5 to 40 adaptive sentences from eachspeaker are 3.8% and 3.6%, respectively.
In comparison withthe speaker-independent model, the error ate of adaptive sys-tems is reduced by about 15% with only 40 sentences fromeach speaker.
Further increase in the number of adaptive sen-tences did not lead to any significant improvement.
Speaker-adaptive recognition results with 5 to 150 adaptive sentencesSpeakers Word Pair Grammar ErrorBJW 2.4%JLS 5.0%JRM 4.5%LPN 2.4%Average 3.6%Table 4: Detailed results using the SCHMM for each speaker.In fact, both the mean and variance vector can be adaptediteratively.
However, the variances cannot be reliably esti-mated with limited adaptive data.
Because of this, estimatesare interpolated with speaker-independent estimates analo-gous to Bayesian adaptation \[9, 22\].
However, in compari-son with iterative SCHMM codebook reestimation, there isno significant error eduction by combining interpolation i tothe codebook mapping procedure.
It is sufficient by just usingvery few samples to reestimate he mean vector.3.2 Output distribution adaptationSeveral output-distribution adaptation techniques, includingcooccurence mapping \[23, 24\], deleted interpolation \[25, 20\],and state-level-distribution clustering, are examined.
Allthese studies are based on SCHMM-adapted codebook as dis-cussed above.In cooccurence mapping, the cooccurence matrix, the prob-ability of codewords of the target speaker given the codewordof speaker-independent models, is first computed \[24\].
Theoutput distribution of the speaker-independent models is thenprojected according to the cooccurence matrix, there is noimprovement with cooccurence mapping.
This is probablybecause that cooccurence smoothing only plays the role ofsmoothing, which is not directly relatect to maximum likeli-hood estimation.A better adaptation technique should be consistent withthe criterion used in the speech recognition system.
As thetotal number of distribution parameters i much larger thanthe codebook parameters, direct reestimation based on theSCHMM will not lead to any improvement.
To alleviate theparameter problem, the similarity between output distribu-tions of different phonetic models is measured.
If two dis-tributions are similar, they are grouped into the same clusterin a similar manner as the generalized triphone \[23\].
Sinceclustering is carried out at the state-level, it is more flexible280and more reliable in comparison with model-level c ustering.Given two distributions, bi(Oh) and bj (Oh), the similaritybetween hi(Ok) and bj (Ok) is measured byd(bi, bj) = (\[Ik bi(Ok)C'(Ok))(H~ b.i(Ok) cA?"))
(2)(lq~ b~+j ( O~ )C,+~( o~) )where Ci(Ok) is the count of codeword k in distributioni, bi+j (Ok) is the merged istribution by adding bi(Ok) andbj (O k ).
Equation 2measures the ratio between the probabilitythat the individual distributions generated the training dataand the probability that the merged istribution generated thetraining data in the similar manner as the generalized triphone.Number ofClusters Word-PairE~or Rate300 3.2%500 3.1%900 3.3%3.3% 15002100 3.4%Table 5: Adaptation results with different clusters.Speakers Word Pair Error RateBJW 2.1%...... JLS 4.6%JRM 3.5%LPN 2.4%Average 3.1%Table 6: Detailed results using 500 clusters for each speaker.Based on the similarity measure given in Equation 2, theBaum-Welch reestimation can be directly used to estimatethe clustered istribution, which is consistent with the crite-rion used in our speaker-independent sys em.
With speaker-dependent clustered distributions, the original speaker-independent models are interpolated.
The interpolationweights can be either estimated using deleted interpolation orby mixing speaker-independent and speaker-dependent countsaccording to a pre-determined ratio that depends on the num-ber of speaker-dependent da a.
Due to limited amount ofadaptive data, the latter approach is more suitable to the for-mer.
It is also found that this procedure is more effectivewhen the interpolation is performed irectly on the raw data(counts), rather than on estimates of probability distributionsderived from the counts.
Let Cg -dep and C~ -indep representspeaker-dependent andspeaker-independent counts for distri-bution i, Afi denote the number of speaker-dependent data fordistribution i.
Final interpolated counts are computed withCi,~t~rpot~te~ -- (7.~--indep + log( 1 +Afi) * C~-dep (3)from which interpolated counts are interpolated withcontext-independent models and uniform distributions withdeleted interpolation.
Varying the number of clustered is-tributions from 300 to 2100, speaker-adaptive recognition re-suits are shown in Table 5.
Just as in generalized triphone\[23\], the number of clustered istributions depends on theavailable adaptive data.
From Table 5, it can be seen thatwhen 40 sentences are used, the optimal number of clustereddistributions i 500.
The error rate is reduced from 3.6%(without distribution adaptation) to3.1%.
Detailed results foreach speaker is shown in Table 6.
In comparison with thespeaker-independent sys em, the error reduction is more than25%.The proposed algorithm can also be employed to incre-mentally adapt he voice of each speaker.
Results are shownin Table 7.
When 300 to 600 adaptive sentences are used,the error rate becomes lower than that of the best speaker-dependent systems.
Here, clustered istributions are not usedbecause of available adaptation data.
With 300-600 adaptivesentences, the error rate is reduced to 2.5-2.4%, which is bet-ter than the best speaker-dependent system trained with 600sentences.
This indicates peaker-adaptive speech recogni-tion is quite robust since information provided by speaker-independent models is available.Incremental Sent Word-Pair Error Rate1 4.iCYo40 3.6%200 3.0%300 2.5%600 2.4%Table 7: Incremental daptation results.3.3 Speaker normalizationSpeaker normalization may have a significant impact on bothspeaker-adaptive and speaker-independent speech recogni-tion.
Normalization techniques proposed here involve eep-strum transformation of a target speaker to the referencespeaker.
For each cepstrum vector ,?, the transformationfunction F(?
() is defined such that the SCHMM probabil-ity P r (T (?
( ) \ ]M)  can be maximized, where .h4 can be ei-ther speaker-independent or speaker-dependent models; andf(Af) can be either a simple function as A~Y + B or any com-plicated nonlinear function.
Thus, a speaker-dependent fu c-tion ~(,?)
can be used to normalize the voice of any targetspeaker to a chosen reference speaker for speaker-adaptivespeech recognition.
Furthermore, a speaker-independentfunction .T0-V ) can also be built to reduce the difference ofspeakers before speaker-independent HMM training is appliedsuch that he resulting speaker-independent models have sharpdistributions.In the first experiment, two transformation matrix A and/3are defined such that he speaker-independent SCHMM proba-bility Pr(.AX + Bl.
?vl ) is maximized.
The mapping structureused here can be regarded as a one-layer perceptron, where theSCHMM probability is used as the objective function.
Basedon the speaker-independent model, the error ate for the sametest set is reduced from 4.3% to 3.9%.
This indicates that the281linear transformation used here may be insufficient to bridgethe difference between speakers.As multi-layer perceptrons (MLP) can be used to approx-imate any nonlinear function, the fully-connected MLP asshown in Figure 1 is employed for speaker normalization.Such a network can be well trained with the back-propagationalgorithm.
The input of the nonlinear mapping network con-sists of three frames (3x13) from the target speaker.
Theoutput of the network is a normalized cepstrum frame, whichis made to approximate the frame of the desired referencespeaker.
The objective function for network learning is tominimize the distortion (mean squared error) between thenetwork output and the desired reference speaker frame.
Thenetwork has two hidden layers, each of which has 20 hiddenunits.
Each hidden unit is associated with a sigmoid function.For simplicity, the objective function used here has not beenunified with the SCHMM.
However, the extension should bestraightforward.To provide learning examples for the network, a DTWalgorithm \[26\] is used to warp the target data to the refer-ence data.
Optimal alignment pairs are used to supervisenetwork learning.
For the given input frames, the desiredoutput frame for network learning is the one paired by themiddle input frame in DTW alignment.
Since the goal hereis to transform the target speaker to the reference speaker, thesigmoid function is not used for the output layer.
Multipleinput frames feeded to the network not only alleviate possibleinaccuracy of DTW alignment but also incorporate dynamicinformation i the learning procedure.
As nonlinear networkmay be less well trained, full connections between input unitsand output units are added.
This has an effect of interpola-tion between the nonlinear network output and the originalspeech frames.
This interpolation helps generalization ca-pability of the nonlinear network significantly.
To minimizethe objective function, both nonlinear connection weights anddirect linear connection weights are simultaneously adjustedwith the back-propagation algorithm.
Experimental experi-ence indicates that 200 to 250 epochs are required to achieveacceptable distortion.speaker normalization.
Speaker-dependent models (2400training sentences) are used instead of speaker-independentmodels.
When the reference speaker is randomly selected asLPN, the average recognition error rate for the other threespeakers is 41.9% as shown in Table 8.
When 40 text-Speakers Word-Pair Error Word-Pair ErrorWithout Normalization With NomaalizationJLS 8.5% 6.8%BJW 62.1% 4.2%JRM 55.3% 9,5%Average 6.8% age 41.9%Table 8: Speaker normalization error rates.dependent training sentences are used to build the speakernormalization network, the average rror rate is reduced to6.8%.
Note that neither codebook nor output distribution hasbeen adapted yet in this experiment.
The error rate has al-ready been reduced by 80%.
It is also interesting tonote thatfor female speakers QRM and BJW), speaker normalizationdramatically reduces the error rate.
Although the error rateof 6.8% is worse than that of the speaker-independent sys em(4.5%) for the same test set, this nevertheless demonstratedthe ability of MLP-based speaker normalization.4 D ISCUSSION AND CONCLUSIONSBy using parameter adaptation techniques only, the errorrate can be reduced from 4.3% to 3.1% with 40 adap-tation sentences for each speaker.
While the number ofspeaker adaptation sentences i  comparable tothat of speaker-dependent training, speaker-adaptive recognition works betterthan speaker-dependent recognition, which indicates the ro-bustness of the proposed speaker-adaptive sp ech recognition.For speaker normalization, the error rate is reduced from41.9% to 6.8% for cross speaker recognition with a speaker-dependent model.
Here again, 40 training sentences are used.~ .
.q13 output unitsEE39 input units1Figure 1: Speaker Net; 39 input units corresponding to 3input frames, 13 output units corresponding to the normalizedoutput frameSince the study here is to investigate the capability of\[ Signal Processing (Cepstrum)Speaker Normalization Net1HMM Training/Recognitior initionFigure 2: Speaker-independent speech recognition withspeaker normalization network282to build the MLP-based nonlinear transformation function.The 80% error reduction demonstrated the ability of MLP-based speaker normalization.
Due to the success of speakernormalization networks, a speaker-independent MLP-basednetwork is being used as part of the front-end of the speaker-independent speech recognition system as shown in Figure 2.The network is built to reduce the difference of speakers be-fore speaker-independent HMM training is involved such thatspeaker-independent models will have sharper distributions(better discrimination capability) in comparison with the con-ventional training procedure.
Use of such normalization net-works for speaker-independent speech recognition as well asunification of the SCHMM and MLP speaker normalizationis currently in progress.5 ACKNOWLEDGEMENTSThe author would like to express his gratitude to members ofCMU speech group for their help; in particular, to ProfessorRaj Reddy and Dr. Kai-Fu Lee for their support.References\[1\] Lee, K., Hon, H., and Reddy, R. An Overview of the SPHINXSpeech Recognition System.
IEEE Trans.
on ASSP, January1990, pp.
599-609.\[2\] Paul, D. The Lincoln Robust Continuous Speech Recognizer.in: ICASSP.
1989, pp.
449- 452.\[3\] Kubala, F. and Schwartz, R. A New Paradigm for Speaker-Independent Training and Speaker Adaptation.
in: DARPASpeech and Language Workshop.
Morgan Kaufmann Pub-lishers, San Mateo, CA, 1990.\[4\] Lee, C., Giachin, E., Rabiner, R., L. P., and Rosenberg, A.Improved Acoustic Modeling for Continuous Speech Recogni-tion.
in: DARPA Speech and Language Workshop.
MorganKaufmann Publishers, San Mateo, CA, 1990.\[5\] Shikano, K., Lee, K., and Reddy, D. R. Speaker Adaptationthrough Vector Quantization.
i : ICASSP.
1986.\[6\] Nishimura, M. and Sugawara, K. Speaker Adaptation Methodfor HMM-Based Speech Recognition.
in: ICASSP.
1988,pp.
207-211.\[7\] Nakamura, S.and Shikano, K. Speaker Adaptation Applied toHMM and NeuraI Networks.
in: ICASSP.
1989.\[8\] Huang, X. and Jack, M. Semi-Continuous Hidden MarkovModels for SpeechSignals.
Computer Speech and Language,vol.
3 (1989), pp.
239-252.\[9\] Brown, P. F., Lee, C.-H., and Spohr, J. C. BayesianAdaptationin Speech Recognition.
i : ICASSP.
1983, pp.
761-764.\[10\] Kubala, F., Schwartz, R., and Barry, C. Speaker AdaptationUsing Multiple Reference Speakers.
in: DARPA Speech andLanguage Workshop.
Morgan Kaufmann Publishers, SanMateo, CA, 1989.\[11\] Acero, A. and Stern, R. Environmental Robustness in Auto-matic Speech Recognition.
in: ICASSP.
1990, pp.
849-852.\[12\] Huang, X., Lee, K., Hon, H., and Hwang, M. lmprovedAcous-tic Modeling for th e SPItlNX Speech Recognition System.
in:ICASSP.
1991.\[13\] Huang, X., Alleva, F., Hayamizu, S., Hon, H., Hwang, M.,and Lee, K. Improved Hidden Markov Modeling for Speaker-Independent Continuous Speech Recognition.
in: DARPASpeech and Language Workshop.
Morgan Kaufmann Pub-lishers, San Mateo, CA, 1990, pp.
327-331.283\[14\] Rumelhart, D., Hinton, G., and Williams, R. LearninglnternalRepresentation by Error Propagation.
in: Learning Inter-nal Representation by Error Propagation, by D. Rumelhart,G.
Hinton, and R. Williams, edited by D. Rumelhart and J.McClelland.
M1T Press, Cambridge, MA, 1986.\[15\] Lippmann, R. NeuraINetsfor Computing.
in: ICASSP.
1988,pp.
1-6.\[16\] Schwartz, R., Chow, Y., Kimball, O., Roucos, S., Krasner, M.,and Makhoul, J. Context-Dependent Modeling for Acoustic-Phonetic Recognition of Continuous Speech.
in: ICASSP.1985, pp.
1205-1208.\[17\] Paul, D. and Martin, E. Speaker Stress-Resistant ContinuousSpeech Recognition.
i : ICASSP.
1988.\[18\] Lee, K. Context-Dependent Phonetic Hidden Markov Modelsfor Continuous Speech Recognition.
IEEE Trans.
on ASSP,April 1990.\[19\] Hwang, M., Hon, H., and Lee, K. Between-Word Coarticula-tion Modeling for Continuous Speech Recognition.
TechnicalReport, Carnegie Mellon University, April 1989.\[20\] Huang, X., Ariki, Y., and Jack, M. Hidden Markov Models forSpeech Recognition.
Edinburgh University Press, Edinburgh,U.K., 1990.\[21\] Huang, X. and Lee, K. On Speaker-Independent, Speaker-Dependent, and Speaker-Adaptive Speech Recognition.
in:ICASSP.
1991.\[22\] Stem, R. M. and Lasry, M. J.
Dynamic Speaker Adaptationfor Isolated Letter Recognition Using MAP Estimation.
in:ICASSP.
1983, pp.
734--737.\[23\] Lee, K. Automatic Speech Recognition: The Develop-ment of the SPHINX System.
Kluwer Academic Publishers,Boston, 1989.\[24\] Fang, M., Kubala, F., and Schwartz, R. Improved SpeakerAdaptation Using Text Dependent Mappings.
in: ICASSP.1988.\[25\] Jelinek, F. and Mercer, R. Interpolated Estimation of MarkovSource Parameters f om Sparse Data.
in: Pattern Recogni-tion in Practice, edited by E. Gelsema nd L. Kanal.
North-Holland Publishing Company, Amsterdam, the Netherlands,1980, pp.
381-397.\[26\] Sakoe, H. and Chiba, S. Dynamic Programming AlgorithmOptimization for Spoken Word Recognition.
IEEE Trans.
onASSP, vol.
ASSP-26 (1978), pp.
43-49.
