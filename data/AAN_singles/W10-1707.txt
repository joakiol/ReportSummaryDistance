Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 72?76,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe University of Maryland Statistical Machine Translation System forthe Fifth Workshop on Machine TranslationVladimir Eidelman?, Chris Dyer?
?, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing?Department of LinguisticsUniversity of Maryland, College Park{vlad,redpony,resnik}@umiacs.umd.eduAbstractThis paper describes the system we devel-oped to improve German-English transla-tion of News text for the shared task ofthe Fifth Workshop on Statistical MachineTranslation.
Working within cdec, anopen source modular framework for ma-chine translation, we explore the benefitsof several modifications to our hierarchicalphrase-based model, including segmenta-tion lattices, minimum Bayes Risk de-coding, grammar extraction methods, andvarying language models.
Furthermore,we analyze decoder speed and memoryperformance across our set of models andshow there is an important trade-off thatneeds to be made.1 IntroductionFor the shared translation task of the Fifth Work-shop on Machine Translation (WMT10), we par-ticipated in German to English translation underthe constraint setting.
We were especially inter-ested in translating from German due to set ofchallenges it poses for translation.
Namely, Ger-man possesses a rich inflectional morphology, pro-ductive compounding, and significant word re-ordering with respect to English.
Therefore, wedirected our system design and experimentationtoward addressing these complications and mini-mizing their negative impact on translation qual-ity.The rest of this paper is structured as follows.After a brief description of the baseline systemin Section 2, we detail the steps taken to improveupon it in Section 3, followed by experimental re-sults and analysis of decoder performance metrics.2 Baseline systemAs our baseline system, we employ a hierarchicalphrase-based translation model, which is formallybased on the notion of a synchronous context-freegrammar (SCFG) (Chiang, 2007).
These gram-mars contain pairs of CFG rules with aligned non-terminals, and by introducing these nonterminalsinto the grammar, such a system is able to uti-lize both word and phrase level reordering to cap-ture the hierarchical structure of language.
SCFGtranslation models have been shown to be wellsuited for German-English translation, as they areable to both exploit lexical information for and ef-ficiently compute all possible reorderings using aCKY-based decoder (Dyer et al, 2009).Our system is implemented within cdec, an ef-ficient and modular open source framework foraligning, training, and decoding with a num-ber of different translation models, includingSCFGs (Dyer et al, 2010).1 cdec?s modularframework facilitates seamless integration of atranslation model with different language models,pruning strategies and inference algorithms.
Asinput, cdec expects a string, lattice, or context-freeforest, and uses it to generate a hypergraph repre-sentation, which represents the full translation for-est without any pruning.
The forest can now berescored, by intersecting it with a language modelfor instance, to obtain output translations.
Theabove capabilities of cdec allow us to perform theexperiments described below, which would other-wise be quite cumbersome to carry out in anothersystem.The set of features used in our model were therule translation relative frequency P (e|f), a targetn-gram language model P (e), a ?pass-through?penalty when passing a source language wordto the target side without translating it, lexicaltranslation probabilities Plex(e|f) and Plex(f |e),1http://cdec-decoder.org72a count of the number of times that arity-0,1, or 2SCFG rules were used, a count of the total num-ber of rules used, a source word penalty, a targetword penalty, the segmentation model cost, and acount of the number of times the glue rule is used.The number of non-terminals allowed in a syn-chronous grammar rule was restricted to two, andthe non-terminal span limit was 12 for non-gluegrammars.
The hierarchical phrase-base transla-tion grammar was extracted using a suffix arrayrule extractor (Lopez, 2007).2.1 Data preparationIn order to extract the translation grammar nec-essary for our model, we used the provided Eu-roparl and News Commentary parallel trainingdata.
The lowercased and tokenized training datawas then filtered for length and aligned using theGIZA++ implementation of IBM Model 4 (Ochand Ney, 2003) to obtain one-to-many alignmentsin both directions and symmetrized by combiningboth into a single alignment using the grow-diag-final-and method (Koehn et al, 2003).
We con-structed a 5-gram language model using the SRIlanguage modeling toolkit (Stolcke, 2002) fromthe provided English monolingual training dataand the non-Europarl portions of the parallel datawith modified Kneser-Ney smoothing (Chen andGoodman, 1996).
Since the beginnings and endsof sentences often display unique characteristicsthat are not easily captured within the context ofthe model, and have previously been demonstratedto significantly improve performance (Dyer et al,2009), we explicitly annotate beginning and endof sentence markers as part of our translationprocess.
We used the 2525 sentences in news-test2009 as our dev set on which we tuned the fea-ture weights, and report results on the 2489 sen-tences of the news-test2010 test set.2.2 Viterbi envelope semiring trainingTo optimize the feature weights for our model,we use Viterbi envelope semiring training (VEST),which is an implementation of the minimum er-ror rate training (MERT) algorithm (Dyer et al,2010; Och, 2003) for training with an arbitraryloss function.
VEST reinterprets MERT withina semiring framework, which is a useful mathe-matical abstraction for defining two general oper-ations, addition (?)
and multiplication (?)
overa set of values.
Formally, a semiring is a 5-tuple(K,?,?, 0, 1), where addition must be commu-nicative and associative, multiplication must be as-sociative and must distribute over addition, and anidentity element exists for both.
For VEST, hav-ing K be the set of line segments, ?
be the unionof them, and?
be Minkowski addition of the linesrepresented as points in the dual plane, allows usto compute the necessary MERT line search withthe INSIDE algorithm.2 The error function we useis BLEU (Papineni et al, 2002), and the decoder isconfigured to use cube pruning (Huang and Chi-ang, 2007) with a limit of 100 candidates at eachnode.
During decoding of the test set, we raisethe cube pruning limit to 1000 candidates at eachnode.2.3 Compound segmentation latticesTo deal with the aforementioned problem in Ger-man of productive compounding, where wordsare formed by the concatenation of several mor-phemes and the orthography does not delineate themorpheme boundaries, we utilize word segmen-tation lattices.
These lattices serve to encode al-ternative ways of segmenting compound words,and as such, when presented as the input to thesystem allow the decoder to automatically choosewhich segmentation is best for translation, leadingto markedly improved results (Dyer, 2009).In order to construct diverse and accurate seg-mentation lattices, we built a maximum entropymodel of compound word splitting which makesuse of a small number of dense features, suchas frequency of hypothesized morphemes as sep-arate units in a monolingual corpus, number ofpredicted morphemes, and number of letters ina predicted morpheme.
The feature weights aretuned to maximize conditional log-likelihood us-ing a small amount of manually created referencelattices which encode linguistically plausible seg-mentations for a selected set of compound words.3To create lattices for the dev and test sets, a lat-tice consisting of all possible segmentations forevery word consisting of more than 6 letters wascreated, and the paths were weighted by the pos-terior probability assigned by the segmentationmodel.
Then, max-marginals were computed us-ing the forward-backward algorithm and used toprune out paths that were greater than a factor of2.3 from the best path, as recommended by Dyer2This algorithm is equivalent to the hypergraph MERT al-gorithm described by Kumar et al (2009).3The reference segmentation lattices used for training areavailable in the cdec distribution.73(2009).4 To create the translation model for latticeinput, we segmented the training data using the1-best segmentation predicted by the segmenta-tion model, and word aligned this with the Englishside.
This version of the parallel corpus was con-catenated with the original training parallel cor-pus.3 Experimental variationThis section describes the experiments we per-formed in attempting to assess the challengesposed by current methods and our exploration ofnew ones.3.1 Bloom filter language modelLanguage models play a crucial role in transla-tion performance, both in terms of quality, and interms of practical aspects such as decoder memoryusage and speed.
Unfortunately, these two con-cerns tend to trade-off one another, as increasingto a higher-order more complex language modelimproves performance, but comes at the cost ofincreased size and difficulty in deployment.
Ide-ally, the language model will be loaded into mem-ory locally by the decoder, but given memory con-straints, it is entirely possible that the only optionis to resort to a remote language model server thatneeds to be queried, thus introducing significantdecoding speed delays.One possible alternative is a randomized lan-guage model (RandLM) (Talbot and Osborne,2007).
Using Bloom filters, which are a ran-domized data structure for set representation, wecan construct language models which signifi-cantly decrease space requirements, thus becom-ing amenable to being stored locally in memory,while only introducing a quantifiable number offalse positives.
In order to assess what the im-pact on translation quality would be, we traineda system identical to the one described above, ex-cept using a RandLM.
Conveniently, it is possi-ble to construct a RandLM directly from an exist-ing SRILM, which is the route we followed in us-ing the SRILM described in Section 2.1 to createour RandLM.5 Table 1 shows the comparison ofSRILM and RandLM with respect to performanceon BLEU and TER (Snover et al, 2006) on the testset.4While normally the forward-backward algorithm com-putes sum-marginals, by changing the addition operator tomax, we can obtain max-marginals.5Default settings were used for constructing the RandLM.Language Model BLEU TERRandLM 22.4 69.1SRILM 23.1 68.0Table 1: Impact of language model on translation3.2 Minimum Bayes risk decodingDuring minimum error rate training, the decoderemploys a maximum derivation decision rule.However, upon exploration of alternative strate-gies, we have found benefits to using a mini-mum risk decision rule (Kumar and Byrne, 2004),wherein we want the translation E of the input Fthat has the least expected loss, again as measuredby some loss function L:E?
= argminE?EP (E|F )[L(E,E?
)]= argminE?
?EP (E|F )L(E,E?
)Using our system, we generate a unique 500-best list of translations to approximate the poste-rior distribution P (E|F ) and the set of possibletranslations.
Assuming H(E,F ) is the weight ofthe decoder?s current path, this can be written as:P (E|F ) ?
exp?H(E,F )where ?
is a free parameter which depends onthe models feature functions and weights as wellas pruning method employed, and thus needs tobe separately empirically optimized on a held outdevelopment set.
For this submission, we used?
= 0.5 and BLEU as the loss function.
Table 2shows the results on the test set for MBR decod-ing.Language Model Decoder BLEU TERRandLMMax-D 22.4 69.1MBR 22.7 68.8SRILMMax-D 23.1 68.0MBR 23.4 67.7Table 2: Comparison of maximum derivation ver-sus MBR decoding3.3 Grammar extractionAlthough the grammars employed in a SCFGmodel allow increased expressivity and translationquality, they do so at the cost of having a large74Language Model Grammar Decoder Memory (GB) Decoder time (Sec/Sentence)Local SRILM corpus 14.293 ?
1.228 5.254 ?
3.768Local SRILM sentence 10.964 ?
.964 5.517 ?
3.884Remote SRILM corpus 3.771 ?
.235 15.252 ?
10.878Remote SRILM sentence .443 ?
.235 14.751 ?
10.370RandLM corpus 7.901 ?
.721 9.398 ?
6.965RandLM sentence 4.612 ?
.699 9.561 ?
7.149Table 3: Decoding memory and speed requirements for language model and grammar extraction varia-tionsnumber of rules, thus efficiently storing and ac-cessing grammar rules can become a major prob-lem.
Since a grammar consists of the set of rulesextracted from a parallel corpus containing tens ofmillions of words, the resulting number of rulescan be in the millions.
Besides storing the wholegrammar locally in memory, other approacheshave been developed, such as suffix arrays, whichlookup and extract rules on the fly from the phrasetable (Lopez, 2007).
Thus, the memory require-ments for decoding have either been for the gram-mar, when extracted beforehand, or the corpus, forsuffix arrays.
In cdec, however, loading grammarsfor single sentences from a disk is very fast relativeto decoding time, thus we explore the additionalpossibility of having sentence-specific grammarsextracted and loaded on an as-needed basis by thedecoder.
This strategy is shown to massively re-duce the memory footprint of the decoder, whilehaving no observable impact on decoding speed,introducing the possibility of more computationalresources for translation.
Thus, in addition to thelarge corpus grammar extracted in Section 2.1,we extract sentence-specific grammars for each ofthe test sentences.
We measure the performanceacross using both grammar extraction mechanismsand the three different language model configu-rations: local SRILM, remote SRILM, and Ran-dLM.As Table 3 shows, there is a marked trade-off between memory usage and decoding speed.Using a local SRILM regardless of grammar in-creases decoding speed by a factor of 3 comparedto the remote SRILM, and approximately a fac-tor of 2 against the RandLM.
However, this speedcomes at the cost of its memory footprint.
With acorpus grammar, the memory footprint of the lo-cal SRILM is twice as large as the RandLM, andalmost 4 times as large as the remote SRILM.
Us-ing sentence-specific grammars, the difference be-comes increasingly glaring, as the remote SRILMmemory footprint drops to ?450MB, a factor ofnearly 24 compared to the local SRILM and a fac-tor of 10 compared to the process size with theRandLM.
Thus, using the remote SRILM reducesthe memory footprint substantially but at the costof significantly slower decoding speed, and con-versely, using the local SRILM produces increaseddecoder speed but introduces a substantial mem-ory overhead.
The RandLM provides a medianbetween the two extremes: reduced memory and(relatively) fast decoding at the price of somewhatdecreased translation quality.
Since we are usinga relatively large beam of 1000 candidates for de-coding, the time presented in Table 3 does not rep-resent an accurate basis for comparison of cdec toother decoders, which should be done using theresults presented in Dyer et al (2010).We also tried one other grammar extractionconfiguration, which was with so-called ?loose?phrase extraction heuristics, which permit un-aligned words at the edges of phrases (Ayan andDorr, 2006).
When decoded using the SRILM andMBR, this achieved the best performance for oursystem, with a BLEU score of 23.6 and TER of67.7.4 ConclusionWe presented the University of Maryland hier-archical phrase-based system for the WMT2010shared translation task.
Using cdec, we experi-mented with a number of methods that are shownabove to lead to improved German-to-Englishtranslation quality over our baseline according toBLEU and TER evaluation.
These include methodsto directly address German morphological com-plexity, such as appropriate feature functions, seg-mentation lattices, and a model for automaticallyconstructing the lattices, as well as alternative de-coding strategies, such asMBR.We also presented75several language model configuration alternatives,as well as grammar extraction methods, and em-phasized the trade-off that must be made betweendecoding time, memory overhead, and translationquality in current statistical machine translationsystems.5 AcknowledgmentsThe authors gratefully acknowledge partial sup-port from the GALE program of the Defense Ad-vanced Research Projects Agency, Contract No.HR0011-06-2-001 and NSF award IIS0838801.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the view of thesponsors.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
Goingbeyond AER: An extensive analysis of word align-ments and their impact on MT.
In Proceedingsof the Joint Conference of the International Com-mittee on Computational Linguistics and the As-sociation for Computational Linguistics (COLING-ACL?2006), pages 9?16, Sydney.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th Annual Meet-ing of the Association for Computational Linguis-tics, pages 310?318.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
In Computational Linguistics, volume 33(2),pages 201?228.Chris Dyer, Hendra Setiawan, Yuval Marton, andP.
Resnik.
2009.
The University of Maryland sta-tistical machine translation system for the FourthWorkshop on Machine Translation.
In Proceedingsof the EACL-2009 Workshop on Statistical MachineTranslation.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL System Demonstrations.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for mt.
In Proceedingsof NAACL-HLT.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages144?151.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In HLT-NAACL 2004: Main Proceedings.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error ratetraining and minimum bayes-risk decoding for trans-lation hypergraphs and lattices.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP,pages 163?171.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of EMNLP,pages 976?985.Franz Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.In Computational Linguistics, volume 29(21), pages19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In In Proceedings of Association for MachineTranslation in the Americas, pages 223?231.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Intl.
Conf.
on SpokenLanguage Processing.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, June.76
