Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534?542,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPThe role of named entities in Web People SearchJavier ArtilesUNED NLP & IR groupMadrid, Spainjavart@bec.uned.esEnrique Amig?oUNED NLP & IR groupMadrid, Spainenrique@lsi.uned.esJulio GonzaloUNED NLP & IR groupMadrid, Spainjulio@lsi.uned.esAbstractThe ambiguity of person names in the Webhas become a new area of interest for NLPresearchers.
This challenging problem hasbeen formulated as the task of clusteringWeb search results (returned in responseto a person name query) according to theindividual they mention.
In this paper wecompare the coverage, reliability and in-dependence of a number of features thatare potential information sources for thisclustering task, paying special attention tothe role of named entities in the texts tobe clustered.
Although named entities areused in most approaches, our results showthat, independently of the Machine Learn-ing or Clustering algorithm used, namedentity recognition and classification per seonly make a small contribution to solve theproblem.1 IntroductionSearching the Web for names of people is a highlyambiguous task, because a single name tends tobe shared by many people.
This ambiguity hasrecently become an active research topic and, si-multaneously, in a relevant application domain forweb search services: Zoominfo.com, Spock.com,123people.com are examples of sites which per-form web people search, although with limiteddisambiguation capabilities.A study of the query log of the AllTheWeb andAltavista search sites gives an idea of the relevanceof the people search task: 11-17% of the querieswere composed of a person name with additionalterms and 4% were identified as person names(Spink et al, 2004).
According to the data avail-able from 1990 U.S. Census Bureau, only 90,000different names are shared by 100 million people(Artiles et al, 2005).
As the amount of informa-tion in the WWW grows, more of these people arementioned in different web pages.
Therefore, aquery for a common name in the Web will usuallyproduce a list of results where different people arementioned.This situation leaves to the user the task of find-ing the pages relevant to the particular person heis interested in.
The user might refine the originalquery with additional terms, but this risks exclud-ing relevant documents in the process.
In somecases, the existence of a predominant person (suchas a celebrity or a historical figure) makes it likelyto dominate the ranking of search results, compli-cating the task of finding information about otherpeople sharing her name.
The Web People Searchtask, as defined in the first WePS evaluation cam-paign (Artiles et al, 2007), consists of groupingsearch results for a given name according to thedifferent people that share it.Our goal in this paper is to study which doc-ument features can contribute to this task, and inparticular to find out which is the role that can beplayed by named entities (NEs): (i) How reliableis NEs overlap between documents as a source ofevidence to cluster pages?
(ii) How much recalldoes it provide?
(iii) How unique is this signal?(i.e.
is it redundant with other sources of informa-tion such as n-gram overlap?
); and (iv) How sen-sitive is this signal to the peculiarities of a givenNE recognition system, such as the granularity ofits NE classification and the quality of its results?Our aim is to reach conclusions which are arenot tied to a particular choice of Clustering or Ma-chine Learning algorithms.
We have taken two de-cisions in this direction: first, we have focused onthe problem of deciding whether two web pagesrefer to the same individual or not (page corefer-ence task).
This is the kind of relatedness measurethat most clustering algorithms use, but in this waywe can factor out the algorithm and its parametersettings.
Second, we have developed a measure,Maximal Pairwise Accuracy (PWA) which, given534an information source for the problem, estimatesan upper bound for the performance of any Ma-chine Learning algorithm using this information.We have used PWA as the basic metric to study therole of different document features in solving thecoreference problem, and then we have checkedthe predictive power of PWA with a Decision Treealgorithm.The remainder of the paper is organised as fol-lows.
First, we examine the previous work in Sec-tion 2.
Then we describe the our experimental set-tings (datasets and features we have used) in Sec-tion 3 and our empirical study in Section 4.
Thepaper ends with some conclusions in Section 5.2 Previous workIn this section we will discuss (i) the state of theart in Web People Search in general, focusing onwhich features are used to solve the problem; and(ii) lessons learnt from the WePS evaluation cam-paign where most approaches to the problem havebeen tested and compared.The disambiguation of person names in Webresults is usually compared to two other Natu-ral Language Processing tasks: Word Sense Dis-ambiguation (WSD) (Agirre and Edmonds, 2006)and Cross-document Coreference (CDC) (Baggaand Baldwin, 1998).
Most of early research workon person name ambiguity focuses on the CDCproblem or uses methods found in the WSD litera-ture.
It is only recently that the web name ambigu-ity has been approached as a separate problem anddefined as an NLP task - Web People Search - onits own (Artiles et al, 2005; Artiles et al, 2007).Therefore, it is useful to point out some crucialdifferences between WSD, CRC and WePS:?
WSD typically concentrates in the disam-biguation of common words (nouns, verbs,adjectives) for which a relatively small num-ber of senses exist, compared to the hun-dreds or thousands of people that can sharethe same name.?
WSD can rely on dictionaries to define thenumber of possible senses for a word.
In thecase of name ambiguity no such dictionaryis available, even though in theory there is anexact number of people that can be accountedas sharing the same name.?
The objective of CDC is to reconstruct thecoreference chain for every mention of a per-son.
In Web person name disambiguation itsuffices to group the documents that containat least one mention to the same person.Before the first WePS evaluation campaign in2007 (Artiles et al, 2007), research on the topicwas not based on a consistent task definition, andit lacked a standard manually annotated testbed.In the WePS task, systems were given the top websearch results produced by a person name query.The expected output was a clustering of these re-sults, where each cluster should contain all andonly those documents referring to the same indi-vidual.2.1 Features for Web People SearchMany different features have been used to repre-sent documents where an ambiguous name is men-tioned.
The most basic is a Bag of Words (BoW)representation of the document text.
Within-document coreference resolution has been appliedto produce summaries of text surrounding occur-rences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).
Nevertheless, the fulldocument text is present in most systems, some-times as the only feature (Sugiyama and Okumura,2007) and sometimes in combination with others -see for instance (Chen and Martin, 2007; Popescuand Magnini, 2007)-.
Other representations usethe link structure (Malin, 2005) or generate graphrepresentations of the extracted features (Kalash-nikov et al, 2007).Some researchers (Cucerzan, 2007; Nguyen andCao, 2008) have explored the use of Wikipediainformation to improve the disambiguation pro-cess.
Wikipedia provides candidate entities thatare linked to specific mentions in a text.
The obvi-ous limitation of this approach is that only celebri-ties and historical figures can be identified in thisway.
These approaches are yet to be applied to thespecific task of grouping search results.Biographical features are strongly related toNEs and have also been proposed for this taskdue to its high precision.
Mann (2003) extractedthese features using lexical patterns to group pagesabout the same person.
Al-Kamha (2004) used asimpler approach, based on hand coded features(e.g.
email, zip codes, addresses, etc).
In Wan(2005), biographical information (person name, ti-tle, organisation, email address and phone num-ber) improves the clustering results when com-bined with lexical features (words from the doc-535ument) and NE (person, location, organisation).The most used feature for the Web PeopleSearch task, however, are NEs.
Ravin (1999) in-troduced a rule-based approach that tackles bothvariation and ambiguity analysing the structure ofnames.
In most recent research, NEs (person, lo-cation and organisations) are extracted from thetext and used as a source of evidence to calculatethe similarity between documents -see for instance(Blume, 2005; Chen and Martin, 2007; Popescuand Magnini, 2007; Kalashnikov et al, 2007)-.
For instance, Blume (2005) uses NEs coocur-ring with the ambiguous mentions of a name as akey feature for the disambiguation process.
Sag-gion (2008) compared the performace of NEs ver-sus BoW features.
In his experiments a only arepresentation based on Organisation NEs outper-formed the word based approach.
Furthermore,this result is highly dependent on the choice ofmetric weighting (NEs achieve high precision atthe cost of a low recall and viceversa for BoW).In summary, the most common document repre-sentations for the problem include BoW and NEs,and in some cases biographical features extractedfrom the text.2.2 Named entities in the WePS campaignAmong the 16 teams that submitted results for thefirst WePS campaign, 10 of them1used NEs intheir document representation.
This makes NEsthe second most common type of feature; onlythe BoW feature was more popular.
Other fea-tures used by the systems include noun phrases(Chen and Martin, 2007), word n-grams (Popescuand Magnini, 2007), emails and URLs (del Valle-Agudo et al, 2007), etc.
In 2009, the secondWePS campaign showed similar trends regardingthe use of NE features (Artiles et al, 2009).Due to the complexity of systems, the resultsof the WePS evaluation do not provide a directanswer regarding the advantages of using NEsover other computationally lighter features such asBoW or word n-grams.
But the WePS campaignsdid provide a useful, standardised resource to per-form the type of studies that were not possible be-fore.
In the next Section we describe this datasetand how it has been adapted for our purposes.1By team ID: CU-COMSEM, IRST-BP, PSNUS, SHEF,FICO, UNN, AUG, JHU1, DFKI2, UC3M133 Experimental settings3.1 DataWe have used the testbeds from WePS-1 (Artiles etal., 2007)2and WePS-2 (Artiles et al, 2009) eval-uation campaigns3.Each WePS dataset consists of 30 test cases: arandom sample of 10 names from the US Cen-sus, 10 names from Wikipedia, and 10 names fromProgramme Committees in the Computer Sciencedomain (ACL and ECDL).
Each test case consistsof, at most, 100 web pages from the top searchresults of a web search engine, using a (quoted)person name as query.For each test case, annotators were asked to or-ganise the web pages in groups where all docu-ments refer to the same person.
In cases wherea web page refers to more than one person us-ing the same ambiguous name (e.g.
a web pagewith search results from Amazon), the documentis assigned to as many groups as necessary.
Doc-uments were discarded when they did not containany useful information about the person being re-ferred.Both the WePS-1 and WePS-2 testbeds havebeen used to evaluate clustering systems by WePStask participants, and are now the standard testbedto test Web People Search systems.3.2 FeaturesThe evaluated features can be grouped in fourmain groups: token-based, n-grams, phrases andNEs.
Wherever possible, we have generated lo-cal versions of these features that only considerthe sentences of the text that mention the ambigu-ous person name4.
Token-based features consid-ered include document full text tokens, lemmas(using the OAK analyser, see below), title, snip-pet (returned in the list of search results) and URL(tokenised using non alphanumeric characters asboundaries) tokens.
English stopwords were re-moved, including Web specific stopwords, as fileand domain extensions, etc.We generated word n-grams of length 2 to 5,2The WePS-1 corpus includes data from the Web03testbed (Mann, 2006) which follows similar annotationguidelines, although the number of document per ambiguousname is more variable.3Both corpora are available from the WePS websitehttp://nlp.uned.es/weps4A very sparse feature might never occur in a sentencewith the person name.
In that cases there is no local versionof the feature.536using the sentences found in the document text.Punctuation tokens (commas, dots, etc) were gen-eralised as the same token.
N-grams were dis-carded when they were composed only of stop-words or when they did not contain at least onetoken formed by alphanumeric characters (e.g.
n-grams like ?at the?
or ?# @?).
Noun phrases (us-ing OAK analyser) were detected in the documentand filtered in a similar way.Named entities were extracted using two dif-ferent tools: the Stanford NE Recogniser and theOAK System5.Stanford NE Recogniser6is a high-performanceNamed Entity Recognition (NER) system basedon Machine Learning.
It provides a general im-plementation of linear chain Conditional Ran-dom Field sequence models and includes a modeltrained on data from CoNLL, MUC6, MUC7, andACE newswire.
Three types of entities were ex-tracted: person, location and organisation.OAK7is a rule based English analyser that in-cludes many functionalities (POS tagger, stemmer,chunker, Named Entity (NE) tagger, dependencyanalyser, parser, etc).
It provides a fine grainedNE recognition covering 100 different NE types(Sekine, 2008).
Given the sparseness of most ofthese fine-grained NE types, we have merged themin coarser groups: event, facility, location, person,organisation, product, periodx, timex and numex.We have also used the results of a baselineNE recognition for comparison purposes.
Thismethod detects sequences of two or more upper-cased tokens in the text, and discards those that arefound lowercased in the same document or that arecomposed solely of stopwords.Other features are: emails, outgoing links foundin the web pages and two boolean flags that in-dicate whether a pair of documents is linked orbelongs to the same domain.
Because of theirlow impact in the results these features haven?t re-ceived an individual analysis, but they are includedin the ?all features?
combination in Figure 7.5From the output of both systems we have discarded per-son NEs made of only one token (these are often first namesthat significantly deteriorate the quality of the comparison be-tween documents).6http://nlp.stanford.edu/software/CRF-NER.shtml7http://nlp.cs.nyu.edu/oak .
OAK was also used to detectnoun phrases and extract lemmas from the text.4 Experiments and results4.1 Reformulating WePS as a classificationtaskAs our goal is to study the impact of different fea-tures (information sources) in the task, a directevaluation in terms of clustering has serious disad-vantages.
Given the output of a clustering systemit is not straightforward to assess why a documenthas been assigned to a particular cluster.
There areat least three different factors: the document sim-ilarity function, the clustering algorithm and itsparameter settings.
Features are part of the doc-ument similarity function, but its performance inthe clustering task depends on the other factors aswell.
This makes it difficult to perform error anal-ysis in terms of the features used to represent thedocuments.Therefore we have decided to transform theclustering problem into a classification problem:deciding whether two documents refer to the sameperson.
Each pair of documents in a name datasetis considered a classification instance.
Instancesare labelled as coreferent (if they share the samecluster in the gold standard) or non coreferent (ifthey do not share the same cluster).
Then wecan evaluate the performance of each feature sep-arately by measuring its ability to rank coreferentpairs higher and non coreferent pairs lower.
In thecase of feature combinations we can study them bytraining a classifier or using the maximal pairwiseaccuracy methods (explained in Section 4.3).Each instance (pair of documents) is repre-sented by the similarity scores obtained using dif-ferent features and similarity metrics.
We havecalculated for each feature three similarity met-rics: Dice?s coefficient, cosine (using standardtf.idf weighting) and a measure that simply countsthe size of the intersection set for a given featurebetween both documents.
After testing these met-rics we found that Dice provides the best resultsacross different feature types.
Differences be-tween Dice and cosine were consistent, althoughthey were not especially large.
A possible expla-nation is that Dice does not take into account theredundancy of an n-gram or NE in the document,and the cosine distance does.
This can be a cru-cial factor, for instance, in the document retrievalby topic; but it doesn?t seem to be the case whendealing with name ambiguity.The resulting classification testbed consists of293,914 instances with the distribution shown in537Table 1, where each instance is represented by 69features.true false totalWePS1 61,290 122,437 183,727WePS2 54,641 55,546 110,187WePS1+WePS2 115,931 177,983 293,914Table 1: Distribution of classification instances4.2 Analysis of individual featuresThere are two main aspects related with the use-fulness of a feature for WePS task.
The first one isits performance.
That is, to what extent the simi-larity between two documents according to a fea-ture implies that both mention the same person.The second aspect is to what extent a feature is or-thogonal or redundant with respect to the standardtoken based similarity.4.2.1 Feature performanceAccording to the transformation of WePS cluster-ing problem into a classification task (describedin Section 4.1), we follow the next steps to studythe performance of individual features.
First, wecompute the Dice coefficient similarity over eachfeature for all document pairs.
Then we rank thedocument pair instances according to these simi-larities.
A good feature should rank positive in-stances on top.
If the number of coreferent pairsin the top n pairs is tnand the total number ofcoreferent pairs is t, then P =tnnand R =tnt.
Weplot the obtained precision/recall curves in Figures1, 2, 3 and 4.From the figures we can draw the followingconclusions:First, considering subsets of tokens or lemma-tised tokens does not outperform the basic tokendistance (figure 1 compares token-based features).We see that only local and snippet tokens performslightly better at low recall values, but do not gobeyond recall 0.3.Second, shallow parsing or n-grams longer than2 do not seem to be effective, but using bi-gramsimproves the results in comparison with tokens.Figure 2 compares n-grams of different sizes withnoun phrases and tokens.
Overall, noun phraseshave a poor performance, and bi-grams give thebest results up to recall 0.7.
Four-grams giveslightly better precision but only reach 0.3 recall,and three-grams do not give better precision thanbi-grams.Figure 1: Precision/Recall curve of token-basedfeaturesFigure 2: Precision/Recall curve of word n-gramsThird, individual types of NEs do not improveover tokens.
Figure 3 and Figure 4 display theresults obtained by the Stanford and OAK NERtools respectively.
In the best case, Stanford per-son and organisation named entities obtain resultsthat match the tokens feature, but only at lowerlevels of recall.Finally, using different NER systems clearlyleads to different results.
Surprisingly, the base-line NE system yields better results in a one toone comparison, although it must be noted thatthis baseline agglomerates different types of en-538Figure 3: Precision/Recall curve of NEs obtainedwith the Stanford NER toolFigure 4: Precision/Recall curve of NEs obtainedwith the OAK NER tooltities that are separated in the case of Stanford andOAK, and this has a direct impact on its recall.The OAK results are below the tokens and NEbaseline, possibly due to the sparseness of its veryfine grained features.
In NE types, cases such asperson and organisation results are still lower thanobtained with Stanford.4.2.2 RedundancyIn addition to performance, named entities (as wellas other features) are potentially useful for the taskonly if they provide information that complements(i.e.
that does not substantially overlap) the basictoken based metric.
To estimate this redundancy,let us consider all document tuples of size four <a, b, c, d >.
In 99% of the cases, token similarity isdifferent for < a, b > than for < c, d >.
We takecombinations such that < a, b > are more similarto each other than < c, d > according to tokens.That is:simtoken(a, b) > simtoken(c, d)Then for any other feature similaritysimx(a, b), we will talk about redundant sampleswhen simx(a, b) > simx(c, d), non redundantsamples when simx(a, b) < simx(c, d), andnon informative samples when simx(a, b) =simx(c, d).
If all samples are redundant ornon informative, then simxdoes not provideadditional information for the classification task.Figure 5 shows the proportion of redundant, nonredundant and non informative samples for sev-eral similarity criteria, as compared to token-basedsimilarity.
In most cases NE based similaritiesgive little additional information: the baseline NErecogniser, which has the largest independent con-tribution, gives additional information in less than20% of cases.In summary, analysing individual features, theNEs do not outperform BoW in terms of the clas-sification task.
In addition, NEs tend to be re-dundant regarding BoW.
However, if we are ableto combine optimally the contributions of the dif-ferent features, the BoW approach could be im-proved.
We address this issue in the next section.Figure 5: Independence of similarity criteria withrespect to the token based feature5394.3 Analysis of feature combinationsUp to now we have analysed the usefulness of in-dividual features for the WePS Task.
However,this begs to ask to what extent the NE features cancontribute to the task when they are combined to-gether and with token and n-gram based features.First, we use each feature combinations as the in-put for a Machine Learning algorithm.
In particu-lar, we use a Decision Tree algorithm and WePS-1data for training and WePS-2 data for testing.
TheDecision Tree algorithm was chosen because wehave a small set of features to train (similarity met-rics) and some of these features output Booleanvalues.Results obtained with this setup, however, canbe dependent on the choice of the ML approach.To overcome this problem, in addition to the re-sults of a Decision Tree Machine Learning algo-rithm, we introduce a Maximal Pairwise Accuracy(MPA) measure that provides an upper bound forany machine learning algorithm using a featurecombination.We can estimate the performance of an individ-ual similarity feature x in terms of accuracy.
Itis considered a correct answer when the similarityx(a, a?)
between two pages referring to the sameperson is higher than the similarity x(b, c) betweentwo pages referring to different people.
Let uscall this estimation Pairwise Accuracy.
In termsof probability it can be defined as:PWA = Prob(x(a, a?)
> x(c, d))PWA is defined over a single feature (similar-ity metric).
When considering more than one sim-ilarity measure, the results depend on how mea-sures are weighted.
In that case we assume thatthe best possible weighting is applied.
When com-bining a set of features X = {x1.
.
.
xn}, a per-fect Machine Learning algorithm would learn toalways ?listen?
to the features giving correct in-formation and ignore the features giving erroneousinformation.
In other words, if at least one featuregives correct information, then the perfect algo-rithm would produce a correct output.
This is whatwe call the Maximal Pairwise Accuracy estimationof an upper bound for any ML system using the setof features X:MaxPWA(X) =Prob(?x ?
X.x(a, a?)
> x(c, d))Figure 6: Estimated PWA upper bound versus thereal PWA of decision trees trained with featurecombinationsFigure 7: Maximal Pairwise Accuracy vs. resultsof a Decision TreeThe upper bound (MaxPWA) of feature combi-nations happens to be highly correlated with thePWA obtained by the Decision Tree algorithm (us-ing its confidence values as a similarity metric).Figure 6 shows this correlation for several featurescombinations.
This is an indication that the Deci-sion Tree is effectively using the information in thefeature set.Figure 7 shows the PWA upper bound estima-tion and the actual PWA performance of a Deci-sion Tree ML algorithm for three combinations:(i) all features; (ii) non linguistic features, i.e.,features which can be extracted without naturallanguage processing machinery: tokens, url, title,snippet, local tokens, n-grams and local n-grams;and (iii) just tokens.
The results show that accord-ing to both the Decision Tree results and the upper-bound (MaxPWA), adding new features to tokensimproves the classification.
However, taking non-linguistic features obtains similar results than tak-ing all features.
Our conclusion is that NE featuresare useful for the task, but do not seem to offer a540competitive advantage when compared with non-linguistic features, and are more computationallyexpensive.
Note that we are using NE features in adirect way: our results do not exclude the possibil-ity of effectively exploiting NEs in more sophisti-cated ways, such as, for instance, exploiting theunderlying social network relationships betweenNEs in the texts.4.3.1 Results on the clustering taskIn order to validate our results, we have testedwhether the classifiers learned with our featuresets lead to competitive systems for the full clus-tering task.
In order to do so, we use the output ofthe classifiers as similarity metrics for a particu-lar clustering algorithm, using WePS-1 to train theclassifiers and WePS-2 for testing.We have used a Hierarchical AgglomerativeClustering algorithm (HAC) with single linkage,using the classifier?s confidence value in the nega-tive answer for each instance as a distance metric8between document pairs.
HAC is the algorithmused by some of the best performing systems in theWePS-2 evaluation.
The distance threshold wastrained using the WePS-1 data.
We report resultswith the official WePS-2 evaluation metrics: ex-tended B-Cubed Precision and Recall (Amig?o etal., 2008).Two Decision Tree models were evaluated: (i)ML-ALL is a model trained using all the availablefeatures (which obtains 0.76 accuracy in the clas-sification task) (ii) ML-NON LING was trainedwith all the features except for OAK and StanfordNEs, noun phrases, lemmas and gazetteer features(which obtains 0.75 accuracy in the classificationtask).
These are the same classifiers considered inFigure 7.Table 2 shows the results obtained in the clus-tering task by the two DT models, together withthe four top scoring WePS-2 systems and the av-erage values for all WePS-2 systems.
We foundthat a ML based clustering using only non linguis-tic information slightly outperforms the best par-ticipant in WePS-2.
Surprisingly, adding linguis-tic information (NEs, noun phrases, etc.)
has asmall negative impact on the results (0.81 versus0.83), although the classifier with linguistic infor-mation was a bit better than the non-linguistic one.This seems to be another indication that the use of8The DT classifier output consists of two confidence val-ues, one for the positive and one for the negative answer, thatadd up to 1.0 .noun phrases and other linguistic features to im-prove the task is non-obvious to say the least.B-Cubedrun F-?
=0.5Pre.
Rec.ML-NON LING .83 .91 .77S-1 .82 .87 .79ML- ALL .81 .89 .76S-2 .81 .85 .80S-3 .81 .93 .73S-4 .72 .82 .66WePS-2 systems aver.
.61 .74 .63Table 2: Evaluation on the WePS-2 clustering task5 ConclusionsWe have presented an empirical study that tries todetermine the potential role of several sources ofinformation to solve the Web People Search clus-tering problem, with a particular focus on studyingthe role of named entities in the task.To abstract the study from the particular choiceof a clustering algorithm and a parameter set-ting, we have reformulated the problem as a co-reference classification task: deciding whethertwo pages refer to the same person or not.
Wehave also proposed the Maximal Pairwise Accu-racy estimation that establish an upper bound forthe results obtained by any Machine Learning al-gorithm using a particular set of features.Our results indicate that (i) NEs do not provide asubstantial competitive advantage in the clusteringprocess when compared to a rich combination ofsimpler features that do not require linguistic pro-cessing (local, global and snippet tokens, n-grams,etc.
); (ii) results are sensitive to the NER systemused: when using all NE features for training, thericher number of features provided by OAK seemsto have an advantage over the simpler types inStanford NER and the baseline NER system.This is not exactly a prescription against the useof NEs for Web People Search, because linguisticknowledge can be useful for other aspects of theproblem, such as visualisation of results and de-scription of the persons/clusters obtained: for ex-ample, from a user point of view a network of theconnections of a person with other persons and or-ganisations (which can only be done with NER)can be part of a person?s profile and may help asa summary of the cluster contents.
But from theperspective of the clustering problem per se, a di-rect use of NEs and other linguistic features doesnot seem to pay off.541AcknowledgmentsThis work has been partially supported by theRegional Government of Madrid, project MAVIRS0505-TIC0267.ReferencesEneko Agirre and Philip Edmonds, editors.
2006.Word Sense Disambiguation: Algorithms and Appli-cations.
Springer.Reema Al-Kamha and David W. Embley.
2004.Grouping search-engine returned citations forperson-name queries.
In WIDM ?04: Proceedings ofthe 6th annual ACM international workshop on Webinformation and data management.
ACM Press.Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-lisa Verdejo.
2008.
A comparison of extrinsicclustering evaluation metrics based on formal con-straints.
Information Retrieval.Javier Artiles, Julio Gonzalo, and Felisa Verdejo.
2005.A testbed for people searching strategies in thewww.
In SIGIR.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.2007.
The semeval-2007 weps evaluation: Estab-lishing a benchmark for the web people search task.In Proceedings of the Fourth International Work-shop on Semantic Evaluations (SemEval-2007).ACL.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.2009.
Weps 2 evaluation campaign: overview ofthe web people search clustering task.
In WePS 2Evaluation Workshop.
WWW Conference 2009.Amit Bagga and Breck Baldwin.
1998.
Entity-based cross-document coreferencing using the vec-tor space model.
In Proceedings of the 17th inter-national conference on Computational linguistics.ACL.Matthias Blume.
2005.
Automatic entity disambigua-tion: Benefits to ner, relation extraction, link anal-ysis, and inference.
In International Conference onIntelligence Analysis.Ying Chen and James H. Martin.
2007.
Cu-comsem:Exploring rich features for unsupervised web per-sonal name disambiguation.
In Proceedings of theFourth International Workshop on Semantic Evalu-ations.
ACL.Silviu Cucerzan.
2007.
Large scale named entitydisambiguation based on wikipedia data.
In TheEMNLP-CoNLL-2007.David del Valle-Agudo, C?esar de Pablo-S?anchez, andMar?
?a Teresa Vicente-D??ez.
2007.
Uc3m-13: Dis-ambiguation of person names based on the compo-sition of simple bags of typed terms.
In Proceedingsof the Fourth International Workshop on SemanticEvaluations.
ACL.Chung Heong Gooi and James Allan.
2004.
Cross-document coreference on a large scale corpus.
InHLT-NAACL.Dmitri V. Kalashnikov, Stella Chen, Rabia Nuray,Sharad Mehrotra, and Naveen Ashish.
2007.
Dis-ambiguation algorithm for people search on the web.In Proc.
of IEEE International Conference on DataEngineering (IEEE ICDE).Bradley Malin.
2005.
Unsupervised name disam-biguation via social network similarity.
In Workshopon Link Analysis, Counterterrorism, and Security.Gideon S. Mann and David Yarowsky.
2003.
Unsuper-vised personal name disambiguation.
In Proceed-ings of the seventh conference on Natural LanguageLearning (CoNLL) at HLT-NAACL 2003.
ACL.Gideon S. Mann.
2006.
Multi-Document StatisticalFact Extraction and Fusion.
Ph.D. thesis, JohnsHopkins University.Hien T. Nguyen and Tru H. Cao, 2008.
Named En-tity Disambiguation: A Hybrid Statistical and Rule-Based Incremental Approach.
Springer.Octavian Popescu and Bernardo Magnini.
2007.
Irst-bp: Web people search using name entities.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations.
ACL.Y.
Ravin and Z. Kazi.
1999.
Is hillary rodham clintonthe president?
disambiguating names across docu-ments.
In Proceedings of the ACL ?99 Workshopon Coreference and its Applications Association forComputational Linguistics.Horacio Saggion.
2008.
Experiments on semantic-based clustering for cross-document coreference.
InInternational Joint Conference on Natural languageProcessing.Satoshi Sekine.
2008.
Extended named entity on-tology with attribute information.
In Proceedingsof the Sixth International Language Resources andEvaluation (LREC?08).Amanda Spink, Bernard Jansen, and Jan Pedersen.2004.
Searching for people on web search engines.Journal of Documentation, 60:266 ?
278.Kazunari Sugiyama and Manabu Okumura.
2007.Titpi: Web people search task using semi-supervisedclustering approach.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations.ACL.Xiaojun Wan, Jianfeng Gao, Mu Li, and BinggongDing.
2005.
Person resolution in person search re-sults: Webhawk.
In CIKM ?05: Proceedings of the14th ACM international conference on Informationand knowledge management.
ACM Press.542
