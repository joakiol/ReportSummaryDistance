Genera l i zed  Le f t -Corner  Pars ingMark-Jan Nederhof *University of Nijmegen, Department of Computer ScienceToernooiveld, 65125 ED Nijmegen, The Netherlandsmarkjan@cs.kun.nlAbst rac tWe show how techniques known from gen-erMized LR parsing can be applied to left-corner parsing.
The ~esulting parsing algo-rithm for context-free grammars has someadvantages over generalized LR parsing:the sizes and generation times of the parsersare smaller, the produced output is morecompact, and the basic parsing techniquecan more easily be adapted to arbitrarycontext-free grammars.The algorithm can be seen as an optimiza-tion of algorithms known from existing lit-erature.
A strong advantage ofour presen-tation is that it makes explicit the role ofleft-corner parsing in these algorithms.Keywords: Generalized LR parsing, left-corner parsing, chart parsing, hidden leftrecursion.1 IntroductionGeneralized LR parsing was first described byTomita \[Tomita, 1986; Tomita, 1987\].
It has beenregarded as the most efficient parsing technique forcontext-free grammars.
The technique has beenadapted to other formalisms than context-free gram-mars in \[Tomita, 1988\].A useful property of generalized LR parsing(henceforth abbreviated to GLR parsing) is that in-put is parsed in polynomial time.
To be exact, if thelength of the right side of the longest rule is p, andif the length of the input is n, then the time com-plexity is O(nP+l).
Theoretically, this may be worse*Supported by the Dutch Organization for ScientificResearch (NWO), under grant 00-62-518than the time complexity of Earley's algorithm \[Ear-ley, 1970\], which is O(n3).
For practical cases innatural anguage processing however, GLR parsingseems to give the best results.The polynomial time complexity is established byusing a graph-structured stack, which is a generaliza-tion of the notion of parse stack, in which pointers areused to connect stack elements.
If nondeterminismoccurs, then the search paths are investigated simul-taneously, where the initial part of the parse stackwhich is common to all search paths is representedonly once.
If two search paths share the state ofthe top elements of their imaginary individual parsestacks, then the top element isrepresented only once,so that any computation which thereupon pushes el-ements onto the stack is performed only once.Another useful property of GLR parsing is thatthe output is a concise representation f all possi-ble parses, the so called parse forest, which can beseen as a generalization f the notion of parse tree.
(By some authors, parse forests are more specificallycalled shared, shared-packed, or packed shared (parse)forests.)
The parse forests produced by the Mgorithmcan be represented using O(n p+I) space.
Efficientdecoration of parse forests with attribute values hasbeen investigated in \[Dekkers et al, 1992\].There are however some drawbacks to GLR pars-ing.
In order of decreasing importance, these are:?
The parsing technique is based on the use of LRtables, which may be very large for grammarsdescribing natural anguages.
1 Related to thisis the large amount of time needed to constructl\[Purdom, 1974\] argues that grammars for program-ruing languages require LR tables which have a size whichis about linear in the size of the grammar.
It is gener-ally considered doubtful that similar observations can bemade for grammars for natural languages.305a parser.
Incremental construction of parsersmay in some cases alleviate this problem \[Rek-ers, 1992\].?
The parse forests produced by the algorithm arenot as compact as they might be.
This is be-cause packing of subtrees is guided by the merg-ing of search paths due to equal LR states, in-stead of by the equality of the derived nonter-minals.
The solution presented in \[Rekers, 1992\]implies much computational overhead.?
Adapting the technique to arbitrary grammarsrequires the generalization to cyclic graph-structured stacks \[Nozohoor-Farshi, 1991\],which may complicate the implementation.?
A minor disadvantage is that the theoreticaltime complexity worsens if p becomes larger.The solution given in \[Kipps, 1991\] to obtaina variant of the parsing technique which hasa fixed time complexity of O(n3), independentof p, implies an overhead in computation costswhich worsens instead of improves the time com-plexity in practical cases.These disadvantages of generalized LR parsingare mainly consequences of the LR parsing tech-nique, more than consequences of the use of graph-structured stacks and parse forests.Lang \[Lang, 1974; Lang, 1988c\] gives a general con-struction of deterministic parsing algorithms fromnondeterministic push-down automata.
The pro-duced data structures have a strong similarity toparse forests, as argued in \[Billot and Lang, 1989;Lang, 1991\].
The general idea of Lang has beenapplied to other formalisms than context-free gram-mars in \[Lang, 1988a; Lang, 1988b; Lang, 1988d\].The idea of a graph-structured stack, however,does not immediately follow from Lang's construc-tion.
Instead, Lang uses the abstract notion of atable to store information, without rying to find thebest implementation for this table.
2One of the parsing techniques which can withsome minor difficulties be derived from the con-struction of Lang is generalized left-corner parsing(henceforth abbreviated to GLC parsing).
3 Thestarting-point is left-corner parsing, which was firstformally defined in \[Rosenkrantz and Lewis II, 1970\].Generalized left-corner parsing, albeit under a dif-ferent name, has first been investigated in \[Pratt,2\[Sikkel, 1990\] argues that the way in which the ta-ble is implemented (using a two-dimensional matrix asin case of Earley's algorithm or using a graph-structuredstack) is only of secondary importance to the global be-haviour of the parsing algorithm.3The term "generalized left-corner parsing" has beenused before in \[Demers, 1977\] for a different parsing tech-nique.
Demers generalizes "left corner of a right side" tobe a prefix of a right side which does not necessarily con-sist of one member, whereas we generalize LG parsingwith zero lookahead to grammars which are not LC(0).1975\].
(See also \[Tanaka et al, 1979; Bear, 1983;Sikkel and Op den ikker,  1992\].)
In \[Shann, 1991\]it was shown that the parsing technique can be a se-rious rival to generalized LR parsing with regard tothe time complexities.
(Other papers discussing thetime complexity of GLC parsing are \[Slocum, 1981;Wir~n, 1987\].
)A functional variant of GLC parsing for defi-nite clause grammars has been discussed in \[Mat-sumoto and Sugimura, 1987\].
This algorithm doesnot achieve a polynomial time complexity however,because no "packing" takes place.A variant of Earley's algorithm discussed in \[Leiss,1990\] also is very similar to GLC parsing althoughthe top-down nature of Earley's algorithm is pre-served.GLC parsin~ has been rediscovered a number oftimes (e.g.
in \[Leermakers, 1989; Leermakers, 1992\],\[Schabes, 1991\], and \[Perlin, 1991\]), but without anymention of the connection with LC parsing, whichmade the presentations unnecessarily difficult to un-derstand.
This also prevented iscovery of a numberof optimizations which are obvious from the view-point of left-corner parsing.In this paper we reinvestigate GLC parsing incombination with graph-structured stacks and parseforests.
It is shown that this parsing technique is notsubject o the four disadvantages of the algorithm ofTomita.The structure of this paper is as follows.
In Sec-tion 2 we explain nondeterministic LC parsing.
Thisparsing algorithm is the starting-point of Section 3,which shows how a deterministic algorithm can bedefined which uses a graph-structured stack and pro-duces parse forests.
Section 4 discusses how this gen-eralized LC parsing algorithm can be adapted to ar-bitrary context-free grammars.How the algorithm can be improved to operatein cubic time is shown in Section 5.
The improvedalgorithm produces parse forests in a non-standardrepresentation, which requires only cubic space.
Onemore class of optimizations i  discussed in Section 6.Preliminary results with an implementation f ouralgorithm are discussed in Section 7.2 Left-corner parsingBefore we define LC parsing, we first define somenotions trongly connected with this kind of parsing.We define a spine to be a path in a parse treewhich begins at some node which is not the first sonof its father (or which does not have a father), thenproceeds downwards every time taking the leftmostson, and finally ends in a leaf.We define the relation / between nonterminalssuch that B / A if and only if there is a rule A --* B a,where a denotes ome sequence of grammar symbols.The transitive and reflexive closure of / is denotedby L*, which is called the left-corner elation.
Infor-mally, we have that B /*  A if and only if it is possible306to have a spine in some parse tree in which B occursbelow A (or 13 = A).
We pronounce B Z* A as "B isa left corner of A".We define the set GOAL to be the set consistingof S, the start symbol, and of all nonterminals Awhich occur in a rule of the form B--* t~ A fl whereis not e (the empty sequence of grammar symbols).Informally, a nonterminal is in GOAL if and only ifit may occur at the first node of some spine.We explain LC parsing by means of the smallcontext-free grammar below.
No claims are madeabout the linguistic relevance of this grammar.
Notethat we have transformed lexieal ambiguity intogrammatical mbiguity by introducing the nonter-minals VorN and VorP.S --, NPVPS - *SPPNP --~ "time"NP -~ "an ....  arrow"NP -~ NP NPNP -~ VorNVP --~ VorNVP - *  VorP NPPP --* VorP NPVorN -*  "flies"VorP --~ "like"The algorithm reads the input from left to right.The elements on the parse stack are either nonter-minals (the goal elements) or items (the item ele-ments).
Items consist of a rule in which a dot hasbeen inserted somewhere in the right side to separatethe members which have been recognized from thosewhich have not.Initially, the parse stack consists only of the startsymbol, which is the first goal, as indicate in Fig-ure 1.
The indicated parse corresponds with one ofthe two possible readings of "time flies like an arrow"according to the grammar above.We define a nondeterministic LC parser by theparsing steps which are possible according to the fol-lowing clauses:la.
If the element on top of the stack is the nonter-minal A and if the first symbol of the remaininginput is t, then we may remove t from the inputand push an item \[B --~ t ?
~\] onto the stack,provided B /* A.lb.
If the element on top of the stack is the non-terminal A, then we may push an item \[B --~ .\]onto the stack, provided B /* A.
(The item \[B--* .\] is derived from an epsilon rule B ---, c.)2.
If the element on top of the stack is the item\[A ~ c~ .
t /~\] and if the first symbol of theremaining input is t, then we may remove t fromthe input and replace the item by the item \[A---+ Ott o ill.3.
If the top-most wo elements on the stack are B\[A ~ ~ .\], then we may replace the item by anitem of the form \[C --* A ?
ill, provided C Z* B.4.
If the top-most hree elements on the stack are\[a -~ f t .
A 7\] A \[A -~ ~ 4, then we may replacethese three elements by the item \[B --* fl A ?
7\]-5.
If a step according to one of the previous clausesends with an item \[A ~ t~ ?
B ~ on top of thestack, where B is a nonterminal, then we subse-quently push B onto the stack.6.
If the stack consists only of the two elements S\[S--* a .\] and if the input has been completelyread, then we may successfully terminate theparsing process.Note that only nonterminals from GOAL will oc-cur as separate lements on the stack.The nondeterministie LC parsing algorithm de-fined above uses one symbol of lookahead in case ofterminal left corners.
The algorithm is therefore de-terministic for the LC(0) grammars, according to thedefinition of LC(k) grammars in \[Soisalon-Soininenand Ukkonen, 1979\].
(This definition is incompati-ble with that of \[Rosenkrantz and Lewis II, 1970\].
)The exact formulation of the algorithm above ischosen to simplify the treatment of generalized LCparsing in the next section.
The strict separation be-tween goal elements and item elements has also beenachieved in \[Perlin, 1991\], as opposed to \[Schabes,1991\].3 Genera l i z ing  le f t - corner  pars ingThe construction of Lang can be used to form deter-ministic table-driver parsing algorithms from non-deterministic push-down automata.
Because left-corner parsers are also push-down automata, Lang'sconstruction can also be applied to formulate a de-terministic parsing algorithm based on LC parsing.The parsing algorithm we propose in this pa-per does however not follow straightforwardly fromLang's construction.
If we applied the constructiondirectly, then not as much sharing would be providedas we would like.
This is caused by the fact thatsharing of computation of different search paths isinterrupted if different elements occur on top of thestack (or just beneath the top if elements below thetop are investigated).To explain this more carefully we focus on Clause 3of the nondeterministic LC parser.
Assume the fol-lowing situation.
Two different search paths haveat the same time the same item element \[A --* a o\]on top of the stack.
The goal elements (say B' andB" ) below that item element are different however inboth search paths.This means that the step which replaces \[A ---* a o\]by \[C ---, A ?/~\], which is done for both search paths(provided both C /*  B' and C /*  B"), is done sepa-rately because B' and B" differ.
This is unfortunate307Step Parse stack Input readS1 S \[NP ~ "time" .\]2 S\[NP -~ NP.
NP\]NP3 S \[NP --~ NP.
NP\] NP \[VorN -* "flies' .\]4 S \[NP-~ NP.
NP\] NP \[NP -* VorN .\]5 S\[NP -~NPNP.
\ ]6 S \ [S~NP.VP\ ]VP7 S \ [S~NP.VP\ ]VP \ [VorP~ like" .\]8 S \[S --* NP.
VP\] VP \[VP -* VorP.
NP\] NP9 S \[S ~ NP.
VP\] VP \[VP ~ VorP.
NP\] NP \[NPi0 S \[S ~ NP.
VP\] VP \[VP -* VorP.
NP\] NP \[NP -~i i  S \[S -~ NP.
VP\] VP \[VP -~ VorP NP .\]12 S\[S -~NPVP.
\ ]13"an" ?
"arrow"\]"an" "arrow" ,\]"time""flies""like""arl""arrow"Figure 1: One possible sequence of parsing steps while reading "time flies like an arrow"because sharing of computation i this case is desir-able both for efficiency reasons but also because itwould simplify the construction of a most-compactparse forest.Related to the fact that we propose to implementthe parse table by means of a graph-structured stack,our solution to this problem lies in the introduc-tion of goal elements consisting of sets of nontermi-nals from GOAL, instead of single nonterminals fromGOAL.As an example, Figure 2 shows the state of thegraph-structured stack for the situation just afterreading "time flies".
Note that this state representsthe states of two different search paths of a nonde-terministic LC parser after reading "time flies", oneof which is the state after Step 3 in Figure 1.We see that the goals NP and VP are merged inone goal element so that there is only one edge fromthe item element labelled with \[VorN ~ "flies" ?\] tothose goals.Merging goals in one stack element is of courseonly useful if those goals have at least one left cornerin common.
For the simplicity of the algorithm, weeven allow merging of two goals in one goal elementif these goals have anything to do with each otherwith respect o the left-corner relation /*.Formally, we define an equivalence relation ~ onnonterminals, which is the reflexive, transitive, andsymmetric losure of L. An equivalence class of thisrelation which includes nonterminal A will be de-noted by \[A\].
Each goal element will now consistof a subset of some equivalence class of ~.In the running example, the goal elements con-sist of subsets of {S, NP,VP, PP}, which is the onlyequivalence class in this example.Figures 3 and 4 give the complete generalized LCparsing algorithm.
At this stage we do not want tocomplicate the algorithm by allowing epsilon rules inthe grammar.
Consequently, Clause lb of the non-deterministic LC parser will have no correspondingpiece of code in the GLC parsing algorithm.
Forthe other clauses, we will indicate where they canbe retraced in the new algorithm.
In Section 4 weexplain how our algorithm can be extended so thatalso grammars with epsilon rules can be handled.The nodes and arrows in the parse forest are con-structed by means of two functions:MAKE_NODE (X) constructs a node with label X,which is a terminal or nonterminal.
It returns (theaddress of) that node.A node is associated with a number of lists of sons,which are other nodes in the forest.
Each list rep-resents an alternative derivation of the nonterminalwith which the node is labelled.
Initially, a node isassociated with an empty collection of lists of sons.ADD_SUBNODE (m, 1) adds a list of sons I to thenode m.In the algorithm, an item element el labelled with\[A --* Xx .
.
.
X,n ?
.a\] is associated with a list ofnodes deriving X1 .
.
.
.
.
Xm.
This list is accessed bySONS (el).
A list consisting of exactly one node m isdenoted by <m>,  and list concatenation is denotedby the operator +.A goal element g contains for every nonterminal Asuch that A L* P for some P in g a value NODE (g,A), which is the node representing some derivationof A found at the current input position, providedsuch a derivation exists, and NODE (9, A) is NILotherwise.In the graph-structured stack there may be an edgefrom an item element o a unique goal element, andfrom a goal in a goal element o a number of itemelements.
For item element el, SUCCESSOR (el)yields the unique goal element o which there is anedge from el.
For goal element g and goal P in g,SUCCESSORS (g, P) yields the zero or more itemelements to which there is an edge from P in g.The global variables used by the algorithm are the308N -*  NP .
NP I, I ~ =S ~ NP .VP  l*I "flies" I I VorN  ~Figure 2: The graph-structured stack after reading "time flies"following.a0 al .
.
.
an The symbols in the input string.i The current input position.r The root of the parse forest.
It has the value NILat the end of the algorithm if no parse has beenfound.r and Fnezt The sets of goal elements containinggoals to be fulfilled from the current and nextinput position on, respectively.I and Inezt The sets of item elements labelled with\[A ~ a ?
t ~ such that a shift may be performedthrough t at the current and next input position,respectively.F The set of pairs (g, A) such that a derivation fromA has been found for g at the current input po-sition.
In other words, F is the set of all pairs(g, A) such that NODE (g, A) ~ NIL.The graph-structured stack (which is initiallyempty) and the rules of the grammar are implicitglobal data structures.In a straightforward implementation, the relation/* is recorded by means of one large s' x s booleanmatrix, where s is the number of nonterminals in thegrammar, and s' is the number of elements in GOAL.We can do better however by using the fact that AZ* B is never true if A 7~ B.
We propose the storageof Z* for every equivalence class of ,,, separately, i.e.we store one t' x t boolean matrix for every class of,,, with t members, t ~ of which are in GOAL.We furthermore need a list of all rules A --* X afor each terminal and nonterminal X.
A small op-timization of top-town filtering (see also Section 6)can be achieved by grouping the rules in these listsaccording to the left sides A.Note that the storage of the relation Z* is the mainobstacle to a linear-sized parser.The time needed to generate a parser is determinedby the time needed to compute Z* and the classes of-~, which is quadratic in the size of the grammar.4 Adapt ing  the  a lgor i thm fora rb i t ra ry  context - f ree  grammarsThe generalized LC parsing algorithm from the pre-vious section is only specified for grammars withoutepsilon rules.
Allowing epsilon rules would not onlycomplicate the algorithm but would for some gram-mars also introduce the danger of non-termination fthe parsing process.There are two sources of non-termination for non-deterministic LC and LR parsing: cyelicity and hid-den left-recursion.
A grammar is said to be cyclicif there is some derivation of the form A ---+ A. Agrammar is said to be hidden left-recursive if A --*B a, B -+* e, and c~ --+* A ~, for some A, B, a,and ~.
Hidden left recursion is a special case of leftrecursion where the fact is "hidden" by an empty-generating nonterminal.
(A nonterminal is said tobe nonfalse if it generates the empty string.
)Both sources of non-termination have been studiedextensively in \[Nederhof and Koster, 1993; Nederhofand Sarbo, 1993\].An obvious way to avoid non-termination for non-deterministic LC parsers in case of hidden left-recursive grammars is the following.
We general-ize the relation i so that B L A if and only ifthere is a rule A -~ p B fl, where p is a (possiblyempty) sequence of grammar symbols such that /~--* e. Clause lb is eliminated and to compensatethis, Clauses la and 3 are modified so that they takeinto account prefixes of right sides which generatethe empty string:la.
If the element on top of the stack is the nonter-minal A and if the first symbol of the remaininginput is t, then we may remove t from the inputand push an item \[B -* p t ?
a\] onto the stack,provided B Z* A and p -+* e.3.
If the top-most wo elements on the stack are B\[A --+ a .\], then we may replace the item by anitem of the form \[C --+ p A ?
fl\], provided C L*B and # -+* e.These clauses now allow for nonfalse members atthe beginning of right sides.
To allow for other non-false members we need an extra seventh clause: 47.
If the element on top of the stack is the item \[A--+ t~ ?
B fl\], then we may replace this item bythe item \[A --+ a B ?
fl\], provided B --+* e.The same idea can be used in a straightforwardway to make generalized LC parsing suitable for4Actually, an eighth clause is necessary to handle thespecial case where S, the start symbol, is nonfalse, andthe input is empty.
We omit this clause for the sake ofclarity.309PARSE:?
r ?= NIL?
Create goal element g consisting of S, the start symbol?
r = {g}?
I ?=0oF~O?
for  i ?= 0 to  n do PARSE_WORD?
re turn  r, as the root of the parse forestPARSE_WORD:?
rnezt  ?= 0?
Inezt ?= O?
for  all pairs (g, A) E F doo NODE (g, A) ?= NILoF?=O?
t ~= MAKE_NODE (eq)?
FIND_CORNERS (t)?
SHIFT (t)?
F ?= Fnezt?
I ?= IneztFIND_CORNERS (t): /*  cf.
Clause la of the nondeterministic LC parser */?
for  all goal elements g in F containing oals in class \[B\] doo for  al l  rules A --* ai a such that A E \[B\] do?
i f  A Z* P for some goal P in g /* top-down filtering */theno MAKE_ITEM_ELEM (\[A ---, ai ?
c~\], </>,  g)SHIFT (Q: 1" cf.
Clause 2 "1?
for  al l  item elements el in I labelled with \[A --* a ?
ai fl\] doo MAKE_ITEM_ELEM (\[A ~ ~ ai ?
fl\], SONS (el) + <l>,  SUCCESSOR (el))MAKE_ITEM_ELEM (\[A ~ a .
~\], l, g):?
Create item element el labelled with \[A --* ~ ?
fl\]?
SONS (el) .
?= 1?
Create an edge from el to g?
iff~ = etheno REDUCE (el)elself/~ = t7, where t is a terminaltheno Inezt ~ Inezt U {el}elsel f  ~ = B7, where B is a nonterminaltheno MAKE_GOAL (B, el)/* cf.
Clause 5 */MAKE_GOAL (A, el):?
i f  there is a goal element g in Fnext containing oals in class \[A\]theno Add goal A to g (provided it is not already there)elseo Create goal element g consisting of Ao Add g to Fnezt?
Create an edge from A in g to elFigure 3: The generalized LC parsing algorithm310REDUCE (el):?
Assume the label of el is \[A ~ a .\]?
Assume SUCCESSOR (el) is g?
i f  NODE (g, A) = NILtheno m ?
: MAKE_NODE (A)o NODE (g, A)?= mo F~FU{(g ,A)}o for all rules B --* A/3 do /*  cf.
Clause 3 */?
i f  B / *  P for some goal P in g /* top-down filtering */theno MAKE_ITEM_ELEM (\[B ---* A o ~,  <m>, g)o i f  A is a goal in gthen?
i f  SUCCESSORS (g, A) # 0theno for all el' E SUCCESSORS (g, A) labelled with \[B --, ~ ?
A 7\] do /* cf.
Clause 4 */?
MAKE_ITEM_ELEM (IS --~/3 A ?
7\], SONS (el') + <m>, SUCCESSOR (el'))elseif  i = n /*  cf.
Clause 6 */theno rC=m?
ADD_SUBNODE (NODE (g, A), SONS (el))Figure 4: The generalized LC parsing algorithm (continued)hidden left-recursive grammars, similar to the waythis is handled in \[Schabes, 1991\] and \[Leermakers,1992\].
The only technical problem is that, in or-der to be able to construct a complete parse for-est, we need precomputed subforests which derivethe empty string in every way from nonfalse nonter-minals.
This precomputation consists of performingm A ?= MAKE_NODE (A) for each nonfalse nonter-minal A, (where m A are specific variables, one foreach nonterminal A) and subsequently performingADD_SUBNODE (mA, <ms1, .
.
.
, mBk> ) for eachrule A -~ B1 .
.
.
Bk consisting only of nonfalse non-terminals.
The variables m A now contain pointersto the required subforests.GLC parsing is guaranteed to terminate also forcyclic grammars, in which case the infinite amountof parses is reflected by cyclic forests, which are alsodiscussed in \[Nozohoor-Farshi, 1991\].5 Pars ing  in  cub ic  t imeThe size of parse forests, even of those which areoptimally dense, can be more than cubic in the lengthof the input.
More precisely, the number of nodes ina parse forest is O(nP+l), where p is the length ofthe right side of the longest rule.Using the normal representation of parse forestsdoes therefore not allow cubic parsing algorithmsfor arbitrary grammars.
There is however a kindof shorthand for parse forests which allows a repre-sentation which only requires cubic space.For example, suppose that of some rule A --* c~fl, the prefix a of the right side derives the samepart of the input in more than one way, then thesederivations may be combined in a new kind of packednode.
Instead of the multiple derivations from a, thispacked node is then combined with the derivationsfrom j3 deriving subsequent input.
We call packing ofderivations from prefixes of right sides subpacl?ing todistinguish this from normal packing of derivationsfrom one nonterminal.Subpacking has been discussed in \[Billet and Lang,1989: Leiss, 1990; Leermakers, 1991\]; see also \[Sheil,1976\].Connected with cubic representation of parseforests is cubic parsing.
The GLC parsing algorithmin Section 3 has a time complexity of O(nP+l).
Thealgorithm can be easily changed so that, with a lit-tle amount of overhead, the time complexit~ is re-3 duced to O(n ), similar to the algorithms in \[Perlin,1991\] and \[Leermakers, 1992\], and the algorithm pro-duces parse forests with subpacking, which requireonly O(n 3) space for storage.We consider how this can be accomplished.
Firstwe define the underlying rule of an item element la-belled with \[A --~ a ?
fl\] to be the rule A --* a ft. Nowsuppose that two item elements ell and el2 with thesame underlying rule, with the dot at the same posi-tion and with the same successor are created at thesame input position, then we may perform subpack-ing for the prefix of the right side before the dot.From then on, we only need one of the item elementsell and el2 for continuing the parsing process.Whether two item elements have one and the samegoal element as successors cannot be efficiently veri-311fled.
Therefore we propose to introduce a new kindof stack element which takes over the role of all for-mer item elements whose successors are one and thesame goal element and which have the same under-lying rule.We leave the details to the imagination of thereader.6 Optimization of top-down filteringOne of the most time-costly activities of general-ized LC parsing is the check whether for a goal el-ement g and a nonterminal A there is some goal Pin g such that A Z* P. This check, which is some-times called top-down filtering, occurs in the routinesFIND_CORNERS and REDUCE.
We propose someoptimizations to reduce the number of goals P in gfor which A Z* P has to be checked.The most straightforward optimization consists ofannotating every edge from an item element labelledwith \[A ~ a ?/~\] to a goal element g with the sub-set of goals in g which does not include those goalsP for which A L* P has already been found to befalse.
This is the set of goals in g which are actu-ally useful in top-down filtering when a new itemelement labelled with \[B ---* A .
7\] is created uring aREDUCE (see the piece of code in REDUCE corre-sponding with Clause 3 of the nondeterministic LCparser).
The idea is that if A L* P does not holdfor goal P in g, then neither does B Z* P if A L B.This optimization can be realized very easily if setsof goals are implemented as lists.A second optimization is useful if / is such thatthere are many nonterminals A such that there isonly one B with A ?
B.
In case we have such a non-terminal A which is not a goal, then no top-downfiltering needs to be performed when a new item el-ement labelled with \[B --* A ?
a\] is created during aREDUCE.
This can be explained by the fact that iffor some goal P we have A Z* P, and ifA ?
P, and ifthere is only one B such that A / B, then we alreadyknow that B z* p.There are many more of these optimizations butnot all of these give better performance in all cases.It depends heavily on the properties of / whetherthe gain in time while performing the actual top-down filtering (i.e.
performing the tests A /* P forsome P in a particular subset of the goals in a goalelement g) outweighs the time needed to set up ex-tra administration for the purpose of reducing thosesubsets of the goals.7 Preliminary resultsOnly recently the author has implemented a GLCparser.
The algorithm as presented in this paper hasbeen implemented almost literally, with the treat-ment of epsilon rules as suggested in Section 4.
Asmall adaptation has been made to deal with termi-nals of different lengths.Also recently, some members of our departmenthave completed the implementation f a GLR parser.Because both systems have been implemented us-ing different programming languages, fair compari-son of the two systems is difficult.
Specific problemswhich occurred concerning the efficient calculation ofLR tables and the correct reatment of epsilon rulesfor GLR parsing suggest hat GLR parsing requiresmore effort to implement than GLC parsing.Preliminary tests show that the division of nonter-minals into equivalence classes yields disappointingresults.
In all tested cases, one large class containedmost of the nonterminals.The first optimization discussed in Section 6proved to be very useful.
The number of goals whichhad to be considered could in some cases be reducedto one fifth.ConclusionsWe have discussed a parsing algorithm for context-free grammars called generalized LC parsing.
Thisparsing algorithm has the following advantages overgeneralized LR parsing (in order of decreasing im-portance).?
The size of a parser is much smaller; if we neglectthe storage of the relation / ' ,  the size is evenlinear in the size of the grammar.
Related tothis, only a little amount of time is needed togenerate a parser.?
The generated parse forests are as compact aspossible.?
Cyclic and hidden left-recursive grammars canbe handled more easily and more efficiently (Sec-tion 4).?
As Section 5 shows, GLC parsing can more eas-ily be made to run in cubic time for arbitrarycontext-free grammars.
Furthermore, this canbe done without much loss of efficiency in prac-tical cases.Because LR parsing is a more refined form of pars-ing than LC parsing, generalized LR parsing mayat least for some grammars be more efficient thangeneralized LC parsing.
5 However, we feel that thisdoes not outweigh the disadvantages of the large sizesand generation times of LR parsers in general, whichrenders GLR parsing unfeasible in some natural an-guage applications.GLC parsing does not suffer from these defects.We therefore propose this parsing algorithm as a rea-sonable alternative to GLR parsing.
Because of thesmall generation time of GLC parsers, we expect hiskind of parsing to be particularly appropriate dur-ing the development of grammars, when grammarsSThe ratio between the time complexities of GLCparsing and GLR parsing is smaller than some constant,which is dependent on the grammar.312change often and consequently new parsers have tobe generated many times.As we have shown in this paper, the implementa-tion of GLC parsing using a graph-structured stackallows many optimizatious.
These optimizatiouswould be less straightforward and possibly less ef-fective if a two-dimensional matrix was used for theimplementation f the parse table.
Furthermore, ma-trices require a large amount of space, especially forlong input, causing overhead for initialization (atleast if no optimizations are used).In contrast, the time and space requirements ofGLC parsing using a graph-structured stack are onlya negligible quantity above that of nondeterministicLC parsing if no nondeterminism occurs (e.g.
if thegrammar is LC(O)).
Only in the worst-case does agraph-structured stack require the same amount ofspace as a matrix.In this paper we have not considered GLC parsingwith more lookahead than one symbol for terminalleft corners.
The reason for this is that we feel thatone of the main advantages ofour parsing algorithmover GLIt parsing is the small sizes of the parsers.Adding more lookahead requires larger tables andmay therefore reduce the advantage of generalizedLC parsing over its Lit counterpart.On the other hand, the phenomenon reported in\[Billot and Lang, 1989\] and \[Lankhorst, 1991\] thatthe time complexity of GLIt parsing sometimes wors-ens if more lookahead isused, does possibly not applyto GLC parsing.
For GLIt parsing, more lookaheadmay result in more Lit states, which may result inless sharing of computation.
For GLC parsing thereis however no relation between the amount of looka-head and the amount of sharing of computation.Therefore, ajudicious use of extra lookahead mayon the whole be advantageous to the usefulness ofGLC parsing.AcknowledgementsThe author is greatly indebted to Klaas Sikkel, JanosSarbo, Franc Grootjen, and Kees Koster, for manyfruitful discussions.
Valuable correspondence withRend Leermakers, Jan Itekers, Masaru Tomita, andDick Grune is gratefully acknowledged.References\[Bear, 1983\] J.
Bear.
A breadth-first parsing model.In Proc.
of the Eighth International Joint Con-ference on Artificial Intelligence, volume 2, pages696-698, Karlsruhe, West Germany, August 1983.\[Billot and Lang, 1989\] S. Billot and B. Lang.
Thestructure of shared forests in ambiguous parsing.In 27th Annual Meeting of the ACL \[1\], pages 143-151.\[Dekkers el al., 1992\] C. Dekkers, M.J. Nederhof,and J.J. Sarbo.
Coping with ambiguity in dee-orated parse forests.
In Coping with LinguisticAmbiguity in Typed Feature Formalisms, Proceed-ings of a Workshop held at ECAI 92, pages 11-19,Vienna, Austria, August 1992.\[Demers, 1977\] A.J.
Demers.
Generalized left cor-ner parsing.
In Conference Record of the FourthACM Symposium on Principles of ProgrammingLanguages, pages 170-182, Los Angeles, Califor-nia, January 1977.\[Earley, 1970\] J. Earley.
An efficient context-freeparsing algorithm.
Communications of the A CM,13(2):94-102, February 1970.\[Kipps, 1991\] J.it.
Kipps.
GLIt parsing in timeO(na).
In \[Tomita, 1991\], chapter 4, pages 43-59.\[Lang, 1974\] B. Lang.
Deterministic techniquesfor efficient non-deterministic parsers.
In Au-tomata, Languages and Programming, ~nd Col-loquium, Lecture Notes in Computer Science,volume 14, pages 255-269, Saarbriicken, 1974.Springer-Verlag.\[Lang, 1988a\] B. Lang.
Complete evaluation ofHorn clauses: An automata theoretic approach.Rapport de Recherche 913, Iustitut National deRecherche n Informatique t en Automatique,I~cquencourt, France, November 1988.\[Lang, 1988b\] B. Lang.
Datalog automata.
In Proc.of the Third International Conference on Dataand Knowledge Bases: Improving Usability andResponsiveness, pages 389-401, Jerusalem, June1988.\[Lang, 1988c\] B. Lang.
Parsing incomplete sen-tences.
In Proc.
of the l~h International Con-ference on Computational Linguistics, volume 1,pages 365-371, Budapest, August 1988.\[Lang, 1988d\] B. Lang.
The systematic constructionof Earley parsers: Application to the production ofO(n 6) Earley parsers for tree adjoining rammars.Unpublished paper, December 1988.\[Lang, 1991\] B. Lang.
Towards a uniform for-mal framework for parsing.
In M. Tomita, edi-tor, Current Issues in Parsing Technology, chap-ter 11, pages 153-171.
Kluwer Academic Publish-ers, 1991.\[Lankhorst, 1991\] M. Lankhorst.
An empirical com-parison of generalized Lit tables.
In It.
Heemels,A.
Nijholt, and K. Sikkel, editors, Tomita's Al-gorithm: Extensions and Applications, Proc.
ofthe first Twente Workshop on Language Technol-ogy, pages 87-93.
University of Twente, September1991.
Memoranda Informatica 91-68.\[Leermakers, 1989\] It.
Leermakers.
How to cover agrammar.
In 27th Annual Meeting of the ACL \[1\],pages 135-142.\[Leermakers, 1991\] It.
Leermakers.
Non-deterministic recursive ascent parsing.
In Fifth313Conference of the European Chapter of the Asso-ciation for Computational Linguistics, Proceedingsof the Conference, pages 63-68, Berlin, Germany,April 1991.\[Leermakers, 1992\] R. Leermakers.
A recursive as-cent Earley parser.
Information Processing Let-ters, 41(2):87-91, February 1992.\[Leiss, 1990\] H. Leiss.
On Kilbury's modification ofEarley's algorithm.
ACM Transactions on Pro-gramming Languages and Systems, 12(4):610-640,October 1990.\[Matsumoto and Sugimura, 1987\]Y. Matsumoto and R. Sugimura.
A parsing systembased on logic programming.
In Proc.
of the TenthInternational Joint Conference on Artificial Intel-ligence, volume 2, pages 671-674, Milan, August1987.\[Nederhof, 1992\] M.J. Nederhof.
Generalized left-corner parsing.
Technical report no.
92-21, Uni-versity of Nijmegen, Department ofComputer Sci-ence, August 1992.\[Nederhof and Koster, 1993\]M.J. Nederhof and C.H.A.
Koster.
Top-down pars-ing of left-recursive grammars.
Technical report,University of Nijmegen, Department of ComputerScience, 1993. forthcoming.\[Nederhof and Sarbo, 1993\] M.J. Nederhof and J.J.Sarbo.
Increasing the applicability of LR parsing.Submitted for publication, 1993.\[Nozohoor-Farshi, 1991\] R. Nozohoor-Farshi.
GLRparsing for e-grammars.
In \[Tomita, 1991\], chap-ter 5, pages 61-75.\[Perlin, 1991\] M. Perlin.
LR recursive transition et-works for Earley and Tomita parsing.
In 29th An-nual Meeting of the ACL \[2\], pages 98-105.\[Pratt, 1975\] V.R.
Pratt.
LINGOL - A progress re-port.
In Advance Papers of the Fourth Interna-tional Joint Conference on Artificial Intelligence,pages 422-428, Tbilisi, Georgia, USSR, September1975.\[Purdom, 1974\] P. Purdom.
The size of LALR (1)parsers.
BIT, 14:326-337, 1974.\[Rekers, 1992\] J. Rekers.
Parser Generation for In-teractive Environments.
PhD thesis, University ofAmsterdam, 1992.\[Rosenkrantz and Lewis II, 1970\] D.J.
Rosenkrantzand P.M. Lewis II.
Deterministic left corner pars-ing.
In 1EEE Conference Record of the 11th An-nual Symposium on Switching and Automata The-ory, pages 139-152, 1970.\[Schabes, 1991\] Y. Schabes.
Polynomial time andspace shift-reduce parsing of arbitrary context-freegrammars.
In 29th Annual Meeting of the ACL \[2\],pages 106-113.\[Shann, 1991\] P. Shann.
Experiments with GLR andchart parsing.
In \[Tomita, 1991\], chapter 2, pages17-34.\[Sheil, 1976\] B.A.
Sheil.
Observations on context-free parsing.
Statistical Methods in Linguistics,1976, pages 71-109.\[Sikkel, 1990\] K. Sikkel.
Cross-fertilization of Ear-ley and Tomita.
Memoranda informatica 90-69,University of Twente, November 1990.\[Sikkel and Op den Akker, 1992\]K. Sikkel and R. op den Akker.
Head-corner chartparsing.
In Computing Science in the Netherlands,Utrecht, November 1992.\[Slocum, 1981\] J. Slocum.
A practical comparison ofparsing strategies.
In 19th Annual Meeting of theAssociation for Computational Linguistics, Pro-ceedings of the Conference, pages 1-6, Stanford,California, June-July 1981.\[Soisalon-Soininen and Ukkonen, 1979\] E. Soisalon-Soininen and E. Ukkonen.
A method for trans-forming grammars into LL(k) form.
Acta lnfor-matica, 12:339-369, 1979.\[Tanaka et al, 1979\] H. Tanaka, T. Sato, and F. Mo-toyoshi.
Predictive control parser: Extended LIN-GOL.
In Proc.
of the Sixth International JointConference on Artificial Intelligence, volume 2,pages 868-870, Tokyo, August 1979.\[Tomita, 1986\] M. Tomita.
Efficient Parsing forNatural Language.
Kluwer Academic Publishers,1986.\[Tomita, 1987\] M. Tomita.
An efficient augmented-context-free parsing algorithm.
ComputationalLinguistics, 13:31-46, 1987.\[Tomita, 1988\] M. Tomita.
Graph-structured stackand natural language parsing.
In 26th AnnualMeeting of the Association for Computational Lin-guistics, Proceedings of the Conference, pages 249-257, Buffalo, New York, June 1988.\[Tomita, 1991\] M. Tomita, editor.
Generalized LRParsing.
Kluwer Academic Publishers, 1991.\[Wir~n, 1987\] Mats Wir~n.
A comparison of rule-invocation strategies in context-free chart pars-ing.
In Third Conference of the European Chap-ter of the Association for Computational Linguis-tics, Proceedings of the Conference, pages 226-233,Copenhagen, Denmark, April 1987.\[1\] 27th Annual Meeting of the Association for Com-putational Linguistics, Proceedings of the Confer-ence, Vancouver, British Columbia, June 1989.\[2\] egth Annual Meeting of the Association for Com-putational Linguistics, Proceedings of the Confer-ence, Berkeley, California, June 1991.314
