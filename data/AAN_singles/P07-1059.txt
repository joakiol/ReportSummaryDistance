Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464?471,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsStatistical Machine Translation for Query Expansion in Answer RetrievalStefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi LiuGoogle Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043{riezler|avasserm|ioannis|vibhu|yliu}@google.comAbstractWe present an approach to query expan-sion in answer retrieval that uses Statisti-cal Machine Translation (SMT) techniquesto bridge the lexical gap between ques-tions and answers.
SMT-based query ex-pansion is done by i) using a full-sentenceparaphraser to introduce synonyms in con-text of the entire query, and ii) by trans-lating query terms into answer terms us-ing a full-sentence SMT model trained onquestion-answer pairs.
We evaluate theseglobal, context-aware query expansion tech-niques on tfidf retrieval from 10 millionquestion-answer pairs extracted from FAQpages.
Experimental results show that SMT-based expansion improves retrieval perfor-mance over local expansion and over re-trieval without expansion.1 IntroductionOne of the fundamental problems in Question An-swering (QA) has been recognized to be the ?lexi-cal chasm?
(Berger et al, 2000) between questionstrings and answer strings.
This problem is mani-fested in a mismatch between question and answervocabularies, and is aggravated by the inherent am-biguity of natural language.
Several approaches havebeen presented that apply natural language process-ing technology to close this gap.
For example, syn-tactic information has been deployed to reformu-late questions (Hermjakob et al, 2002) or to re-place questions by syntactically similar ones (Linand Pantel, 2001); lexical ontologies such as Word-net1 have been used to find synonyms for questionwords (Burke et al, 1997; Hovy et al, 2000; Prageret al, 2001; Harabagiu et al, 2001), and statisti-cal machine translation (SMT) models trained onquestion-answer pairs have been used to rank can-didate answers according to their translation prob-abilities (Berger et al, 2000; Echihabi and Marcu,2003; Soricut and Brill, 2006).
Information retrieval(IR) is faced by a similar fundamental problem of?term mismatch?
between queries and documents.A standard IR solution, query expansion, attempts toincrease the chances of matching words in relevantdocuments by adding terms with similar statisticalproperties to those in the original query (Voorhees,1994; Qiu and Frei, 1993; Xu and Croft, 1996).In this paper we will concentrate on the task ofanswer retrieval from FAQ pages, i.e., an IR prob-lem where user queries are matched against docu-ments consisting of question-answer pairs found inFAQ pages.
Equivalently, this is a QA problem thatconcentrates on finding answers given FAQ docu-ments that are known to contain the answers.
Ourapproach to close the lexical gap in this setting at-tempts to marry QA and IR technology by deploy-ing SMT methods for query expansion in answerretrieval.
We present two approaches to SMT-basedquery expansion, both of which are implemented inthe framework of phrase-based SMT (Och and Ney,2004; Koehn et al, 2003).Our first query expansion model trains an end-to-end phrase-based SMT model on 10 millionquestion-answer pairs extracted from FAQ pages.1http://wordnet.princeton.edu464The goal of this system is to learn lexical correla-tions between words and phrases in questions andanswers, for example by allowing for multiple un-aligned words in automatic word alignment, and dis-regarding issues such as word order.
The ability totranslate phrases instead of words and the use of alarge language model serve as rich context to makeprecise decisions in the case of ambiguous transla-tions.
Query expansion is performed by adding con-tent words that have not been seen in the originalquery from the n-best translations of the query.Our second query expansion model is based onthe use of SMT technology for full-sentence para-phrasing.
A phrase table of paraphrases is extractedfrom bilingual phrase tables (Bannard and Callison-Burch, 2005), and paraphrasing quality is improvedby additional discriminative training on manuallycreated paraphrases.
This approach utilizes largebilingual phrase tables as information source to ex-tract a table of para-phrases.
Synonyms for queryexpansion are read off from the n-best paraphrasesof full queries instead of from paraphrases of sep-arate words or phrases.
This allows the model totake advantage of the rich context of a large n-gramlanguage model when adding terms from the n-bestparaphrases to the original query.In our experimental evaluation we deploy adatabase of question-answer pairs extracted fromFAQ pages for both training a question-answertranslation model, and for a comparative evalua-tion of different systems on the task of answer re-trieval.
Retrieval is based on the tfidf frameworkof Jijkoun and de Rijke (2005), and query expan-sion is done straightforwardly by adding expansionterms to the query for a second retrieval cycle.
Wecompare our global, context-aware query expansiontechniques with Jijkoun and de Rijke?s (2005) tfidfmodel for answer retrieval and a local query expan-sion technique (Xu and Croft, 1996).
Experimen-tal results show a significant improvement of SMT-based query expansion over both baselines.2 Related WorkQA has approached the problem of the lexical gapby various techniques for question reformulation,including rule-based syntactic and semantic refor-mulation patterns (Hermjakob et al, 2002), refor-mulations based on shared dependency parses (Linand Pantel, 2001), or various uses of the Word-Net ontology to close the lexical gap word-by-word(Hovy et al, 2000; Prager et al, 2001; Harabagiuet al, 2001).
Another use of natural language pro-cessing has been the deployment of SMT models onquestion-answer pairs for (re)ranking candidate an-swers which were either assumed to be containedin FAQ pages (Berger et al, 2000) or retrieved bybaseline systems (Echihabi and Marcu, 2003; Sori-cut and Brill, 2006).IR has approached the term mismatch problem byvarious approaches to query expansion (Voorhees,1994; Qiu and Frei, 1993; Xu and Croft, 1996).Inconclusive results have been reported for tech-niques that expand query terms separately by addingstrongly related terms from an external thesaurussuch as WordNet (Voorhees, 1994).
Significantimprovements in retrieval performance could beachieved by global expansion techniques that com-pute corpus-wide statistics and take the entire query,or query concept (Qiu and Frei, 1993), into account,or by local expansion techniques that select expan-sion terms from the top ranked documents retrievedby the original query (Xu and Croft, 1996).A similar picture emerges for query expansionin QA: Mixed results have been reported for word-by-word expansion based on WordNet (Burke etal., 1997; Hovy et al, 2000; Prager et al, 2001;Harabagiu et al, 2001).
Considerable improvementshave been reported for the use of the local contextanalysis model of Xu and Croft (1996) in the QAsystem of Ittycheriah et al (2001), or for the sys-tems of Agichtein et al (2004) or Harabagiu andLacatusu (2004) that use FAQ data to learn how toexpand query terms by answer terms.The SMT-based approaches presented in this pa-per can be seen as global query expansion tech-niques in that our question-answer translation modeluses the whole question-answer corpus as informa-tion source, and our approach to paraphrasing de-ploys large amounts of bilingual phrases as high-coverage information source for synonym finding.Furthermore, both approaches take the entire querycontext into account when proposing to add newterms to the original query.
The approaches thatare closest to our models are the SMT approach ofRadev et al (2001) and the paraphrasing approach465web pages FAQ pages QA pairscount 4 billion 795,483 10,568,160Table 1: Corpus statistics of QA pair dataof Duboue and Chu-Carroll (2006).
None of theseapproaches defines the problem of the lexical gapas a query expansion problem, and both approachesuse much simpler SMT models than our systems,e.g., Radev et al (2001) neglect to use a languagemodel to aid disambiguation of translation choices,and Duboue and Chu-Carroll (2006) use SMT asblack box altogether.In sum, our approach differs from previous workin QA and IR in the use SMT technology for queryexpansion, and should be applicable in both areaseven though experimental results are only given forthe restricted domain of retrieval from FAQ pages.3 Question-Answer Pairs from FAQ PagesLarge-scale collection of question-answer pairs hasbeen hampered in previous work by the small sizesof publicly available FAQ collections or by restrictedaccess to retrieval results via public APIs of searchengines.
Jijkoun and de Rijke (2005) neverthelessmanaged to extract around 300,000 FAQ pagesand 2.8 million question-answer pairs by repeatedlyquerying search engines with ?intitle:faq?and ?inurl:faq?.
Soricut and Brill (2006) coulddeploy a proprietary URL collection of 1 billionURLs to extract 2.3 million FAQ pages contain-ing the uncased string ?faq?
in the url string.
Theextraction of question-answer pairs amounted to adatabase of 1 million pairs in their experiment.However, inspection of the publicly available Web-FAQ collection provided by Jijkoun and de Rijke2showed a great amount of noise in the retrievedFAQ pages and question-answer pairs, and yet theindexed question-answer pairs showed a serious re-call problem in that no answer could be retrieved formany well-formed queries.
For our experiment, wedecided to prefer precision over recall and to attempta precision-oriented FAQ and question-answer pairextraction that benefits the training of question-answer translation models.2http://ilps.science.uva.nl/Resources/WazDah/As shown in Table 1, the FAQ pages used in ourexperiment were extracted from a 4 billion pagesubset of the web using the queries ?inurl:faq?and ?inurl:faqs?
to match the tokens ?faq?
or?faqs?
in the urls.
This extraction resulted in 2.6million web pages (0.07% of the crawl).
Since notall those pages are actually FAQs, we manually la-beled 1,000 of those pages to train an online passive-aggressive classificier (Crammer et al, 2006) in a10-fold cross validation setup.
Training was doneusing 20 feature functions on occurrences questionmarks and key words in different fields of webpages, and resulted in an F1 score of around 90%for FAQ classification.
Application of the classifierto the extracted web pages resulted in a classificationof 795,483 pages as FAQ pages.The extraction of question-answer pairs from thisdatabase of FAQ pages was performed again in aprecision-oriented manner.
The goal of this stepwas to extract url, title, question, and answers fieldsfrom the question-answer pairs in FAQ pages.
Thiswas achieved by using feature functions on punc-tuations, HTML tags (e.g., <p>, <BR>), listingmarkers (e.g., Q:, (1)), and lexical cues (e.g.,What, How), and an algorithm similar to Joachims(2003) to propagate initial labels across similar textpieces.
The result of this extraction step is a databaseof about 10 million question answer pairs (13.3pairs per FAQ page).
A manual evaluation of 100documents, containing 1,303 question-answer pairs,achieved a precision of 98% and a recall of 82% forextracting question-answer pairs.4 SMT-Based Query ExpansionOur SMT-based query expansion techniques arebased on a recent implementation of the phrase-based SMT framework (Koehn et al, 2003; Och andNey, 2004).
The probability of translating a foreignsentence f into English e is defined in the noisy chan-nel model asargmaxep(e|f) = argmaxep(f|e)p(e) (1)This allows for a separation of a language modelp(e), and a translation model p(f|e).
Translationprobabilities are calculated from relative frequenciesof phrases, which are extracted via various heuris-tics as larger blocks of aligned words from best word466alignments.
Word alignments are estimated by mod-els similar to Brown et al (1993).
For a sequence ofI phrases, the translation probability in equation (1)can be decomposed intop(f Ii |eIi ) =I?i=1p(fi|ei) (2)Recent SMT models have shown significant im-provements in translation quality by improved mod-eling of local word order and idiomatic expressionsthrough the use of phrases, and by the deploymentof large n-gram language models to model fluencyand lexical choice.4.1 Question-Answer TranslationOur first approach to query expansion treats thequestions and answers in the question-answer cor-pus as two distinct languages.
That is, the 10 millionquestion-answer pairs extracted from FAQ pages arefed as parallel training data into an SMT trainingpipeline.
This training procedure includes variousstandard procedures such as preprocessing, sentenceand chunk alignment, word alignment, and phraseextraction.
The goal of question-answer translationis to learn associations between question words andsynonymous answer words, rather than the trans-lation of questions into fluent answers.
Thus wedid not conduct discriminative training of featureweights for translation probabilities or languagemodel probabilities, but we held out 4,000 question-answer pairs for manual development and testing ofthe system.
For example, the system was adjustedto account for the difference in sentence length be-tween questions and answers by setting the null-word probability parameter in word alignment to0.9.
This allowed us to concentrate the word align-ments to a small number of key words.
Furthermore,extraction of phrases was based on the intersectionof alignments from both translation directions, thusfavoring precision over recall also in phrase align-ment.Table 2 shows unique translations of the query?how to live with cat allergies?
on the phrase-level,with corresponding source and target phrases shownin brackets.
Expansion terms are taken from phraseterms that have not been seen in the original query,and are highlighted in bold face.4.2 SMT-Based ParaphrasingOur SMT-based paraphrasing system is based on theapproach presented in Bannard and Callison-Burch(2005).
The central idea in this approach is to iden-tify paraphrases or synonyms at the phrase level bypivoting on another language.
For example, givena table of Chinese-to-English phrase translations,phrasal synonyms in the target language are definedas those English phrases that are aligned to the sameChinese source phrases.
Translation probabilities forextracted para-phrases can be inferred from bilin-gual translation probabilities as follows: Given anEnglish para-phrase pair (trg, syn), the probabilityp(syn|trg) that trg translates into syn is definedas the joint probability that the English phrase trgtranslates into the foreign phrase src, and that theforeign phrase src translates into the English phrasesyn.
Under an independence assumption of thosetwo events, this probability and the reverse transla-tion direction p(trg|syn) can be defined as follows:p(syn|trg) = maxsrcp(src|trg)p(syn|src) (3)p(trg|syn) = maxsrcp(src|syn)p(trg|src)Since the same para-phrase pair can be obtainedby pivoting on multiple foreign language phrases, asummation or maximization over foreign languagephrases is necessary.
In order not to put too muchprobability mass onto para-phrase translations thatcan be obtained from multiple foreign languagephrases, we maximize instead of summing over src.In our experiments, we employed equation (3)to infer for each para-phrase pair translation modelprobabilities p?
(syn|trg) and p??
(trg|syn) fromrelative frequencies of phrases in bilingual tables.In contrast to Bannard and Callison-Burch (2005),we applied the same inference step to infer alsolexical translation probabilities pw(syn|trg) andpw?
(trg|syn) as defined in Koehn et al (2003) forpara-phrases.
Furthermore, we deployed features forthe number of words lw, number of phrases c?
, areordering score pd , and a score for a 6-gram lan-guage model pLM trained on English web data.
Thefinal model combines these features in a log-linearmodel that defines the probability of paraphrasing afull sentence, consisting of a sequence of I phrases467qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies)(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy)(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, food)(how, how) (to, to) (live, live) (with, with) (cat, cats) (allergies, allergies)paraphrasing (how, how) (to live, to live) (with cat, with cat) (allergies, allergy)(how, ways) (to live, to live) (with cat, with cat) (allergies, allergies)(how, how) (to live with, to live with) (cat, feline) (allergies, allergies)(how to, how to) (live, living) (with cat, with cat) (allergies, allergies)(how to, how to) (live, life) (with cat, with cat) (allergies, allergies)(how, way) (to live, to live) (with cat, with cat) (allergies, allergies)(how, how) (to live, to live) (with cat, with cat) (allergies, allergens)(how, how) (to live, to live) (with cat, with cat) (allergies, allergen)Table 2: Unique n-best phrase-level translations of query ?how to live with cat allergies?.as follows:p(synI1|trgI1) = (I?i=1p?(syni|trgi)??
(4)?
p??(trgi|syni)????
pw(syni|trgi)?w?
pw?(trgi|syni)?w??
pd(syni, trgi)?d)?
lw(synI1)?l?
c?(synI1)?c?
pLM (synI1)?LMFor estimation of the feature weights ~?
definedin equation (4) we employed minimum error rate(MER) training under the BLEU measure (Och,2003).
Training data for MER training were takenfrom multiple manual English translations of Chi-nese sources from the NIST 2006 evaluation data.The first of four reference translations for each Chi-nese sentence was taken as source paraphrase, therest as reference paraphrases.
Discriminative train-ing was conducted on 1,820 sentences; final evalua-tion on 2,390 sentences.
A baseline paraphrase tableconsisting of 33 million English para-phrase pairswas extracted from 1 billion phrase pairs from threedifferent languages, at a cutoff of para-phrase prob-abilities of 0.0025.Query expansion is done by adding terms intro-duced in n-best paraphrases of the query.
Table 2shows example paraphrases for the query ?how tolive with cat allergies?
with newly introduced termshighlighted in bold face.5 Experimental EvaluationOur baseline answer retrieval system is modeled af-ter the tfidf retrieval model of Jijkoun and de Ri-jke (2005).
Their model calculates a linear com-bination of vector similarity scores between theuser query and several fields in the question-answerpair.
We used the cosine similarity metric withlogarithmically weighted term and document fre-quency weights in order to reproduce the Lucene3model used in Jijkoun and de Rijke (2005).
Forindexing of fields, we adopted the settings thatwere reported to be optimal in Jijkoun and deRijke (2005).
These settings comprise the use of8 question-answer pair fields, and a weight vec-tor ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3?
for fields or-dered as follows: (1) full FAQ document text, (2)question text, (3) answer text, (4) title text, (5)-(8)each of the above without stopwords.
The secondfield thus takes takes wh-words, which would typ-ically be filtered out, into account.
All other fieldsare matched without stopwords, with higher weightassigned to document and question than to answerand title fields.
We did not use phrase-matching orstemming in our experiments, similar to Jijkoun andde Rijke (2005), who could not find positive effectsfor these features in their experiments.Expansion terms are taken from those termsin the n-best translations of the query that havenot been seen in the original query string.
Forparaphrasing-based query expansion, a 50-best listof paraphrases of the original query was used.For the noisier question-answer translation, expan-sion terms and phrases were extracted from a 10-3http://lucene.apache.org468S2@10 S2@20 S1,2@10 S1,2@20baseline tfidf 27 35 58 65local expansion 30 (+ 11.1) 40 (+ 14.2) 57 (- 1) 63 (- 3)SMT-based expansion 38 (+ 40.7) 43 (+ 22.8) 58 65Table 3: Success rate at 10 or 20 results for retrieval of adequate (2) or material (1) answers; relative changein brackets.best list of query translations.
Terms taken fromquery paraphrases were matched with the same fieldweight vector ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3?
asabove.
Terms taken from question-answer trans-lation were matched with the weight vector?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3?, preferring an-swer fields over question fields.
After stopwordremoval, the average number of expansion termsproduced was 7.8 for paraphrasing, and 3.1 forquestion-answer translation.The local expansion technique used in our exper-iments follows Xu and Croft (1996) in taking ex-pansion terms from the top n answers that were re-trieved by the baseline tfidf system, and by incorpo-rating cooccurrence information with query terms.This is done by calculating term frequencies for ex-pansion terms by summing up the tfidf weights ofthe answers in which they occur, thus giving higherweight to terms that occur in answers that receivea higher similarity score to the original query.
Inour experiments, expansion terms are ranked accord-ing to this modified tfidf calculation over the top 20answers retrieved by the baseline retrieval run, andmatched a second time with the field weight vector?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3?
that prefers an-swer fields over question fields.
After stopword re-moval, the average number of expansion terms pro-duced by the local expansion technique was 9.25.The test queries we used for retrieval are takenfrom query logs of the MetaCrawler search en-gine4 and were provided to us by Valentin Jijk-oun.
In order to maximize recall for the comparativeevaluation of systems, we selected 60 queries thatwere well-formed natural language questions with-out metacharacters and spelling errors.
However, forone third of these well-formed queries none of thefive compared systems could retrieve an answer.
Ex-amples are ?how do you make a cornhusk doll?,4http://www.metacrawler.com?what is the idea of materialization?, or ?what does8x certified mean?, pointing to a severe recall prob-lem of the question-answer database.Evaluation was performed by manual labeling oftop 20 answers retrieved for each of 60 queries foreach system by two independent judges.
For the sakeof consistency, we chose not to use the assessmentsprovided by Jijkoun and de Rijke.
Instead, the judgeswere asked to find agreement on the examples onwhich they disagreed after each evaluation round.The ratings together with the question-answer pairid were stored and merged into the retrieval resultsfor the next system evaluation.
In this way consis-tency across system evaluations could be ensured,and the effort of manual labeling could be substan-tially reduced.
The quality of retrieval results wasassessed according to Jijkoun and de Rijke?s (2005)three point scale:?
adequate (2): answer is contained?
material (1): no exact answer, but important in-formation given?
unsatisfactory (0): user?s information need isnot addressedThe evaluation measure used in Jijkoun and deRijke (2005) is the success rate at 10 or 20 an-swers, i.e., S2@n is the percentage of queries withat least one adequate answer in the top n retrievedquestion-answer pairs, and S1,2@n is the percentageof queries with at least one adequate or material an-swer in the top n results.
This evaluation measure ac-counts for improvements in coverage, i.e., it rewardscases where answers are found for queries that didnot have an adequate or material answer before.
Incontrast, the mean reciprocal rank (MRR) measurestandardly used in QA can have the effect of prefer-ring systems that find answers only for a small setof queries, but rank them higher than systems with469(1) query: how to live with cat allergieslocal expansion (-): allergens allergic infections filter plasmacluster rhinitis introduction effective replacementqa-translation (+): allergy cats pet foodparaphrasing (+): way allergens life allergy feline ways living allergen(2) query: how to design model rocketslocal expansion (-): models represented orientation drawings analysis element environment different structureqa-translation (+): models rocketparaphrasing (+): missiles missile rocket grenades arrow designing prototype models ways paradigm(3) query: what is dna hybridizationlocal expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biologicalgenetic moleculeqa-translation (+): slides clone cdna sitting sequencesparaphrasing (+): hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing(4) query: how to enhance competitiveness of indian industrieslocal expansion (+): resources production quality processing established investment development facilities institutionalqa-translation (+): increase industryparaphrasing (+): promote raise improve increase industry strengthen(5) query: how to induce labourlocal expansion (-): experience induction practice imagination concentration information consciousness different meditationrelaxationqa-translation (-): birth industrial induced inducesparaphrasing (-): way workers inducing employment ways labor working child work job action unionsTable 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0)retrieval performance compared to retrieval without expansion.higher coverage.
This makes MRR less adequate forthe low-recall setup of FAQ retrieval.Table 3 shows success rates at 10 and 20 retrievedquestion-answer pairs for five different systems.
Theresults for the baseline tfidf system, following Jijk-oun and de Rijke (2005), are shown in row 2.
Row3 presents results for our variant of local expansionby pseudo-relevance feedback (Xu and Croft, 1996).Results for SMT-based expansion are given in row 4.A comparison of success rates for retrieving at leastone adequate answer in the top 10 results shows rel-ative improvements over the baseline of 11.1% forlocal query expansion, and of 40.7% for combinedSMT-based expansion.
Success rates at top 20 re-sults show similar relative improvements of 14.2%for local query expansion, and of 22.8% for com-bined SMT-based expansion.
On the easier task ofretrieving a material or adequate answer, successrates drop by a small amount for local expansion,and stay unchanged for SMT-based expansion.These results can be explained by inspecting a fewsample query expansions.
Examples (1)-(3) in Ta-ble 4 illustrate cases where SMT-based query expan-sion improves results over baseline performance, butlocal expansion decreases performance by introduc-ing irrelevant terms.
In (4) retrieval performance isimproved over the baseline for both expansion tech-niques.
In (5) both local and SMT-based expansionintroduce terms that decrease retrieval performancecompared to retrieval without expansion.6 ConclusionWe presented two techniques for query expansion inanswer retrieval that are based on SMT technology.Our method for question-answer translation uses alarge corpus of question-answer pairs extracted fromFAQ pages to learn a translation model from ques-tions to answers.
SMT-based paraphrasing utilizeslarge amounts of bilingual data as a new informa-tion source to extract phrase-level synonyms.
BothSMT-based techniques take the entire query contextinto account when adding new terms to the orig-inal query.
In an experimental comparison with abaseline tfidf approach and a local query expansiontechnique on the task of answer retrieval from FAQpages, we showed a significant improvement of bothSMT-based query expansion over both baselines.Despite the small-scale nature of our current ex-perimental results, we hope to apply the presentedtechniques to general web retrieval in future work.Another task for future work is to scale up the ex-traction of question-answer pair data in order toprovide an improved resource for question-answertranslation.470ReferencesEugene Agichtein, Steve Lawrence, and Luis Gravano.2004.
Learning to find answers to questions onthe web.
ACM Transactions on Internet Technology,4(2):129?162.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of (ACL?05), Ann Arbor, MI.Adam L. Berger, Rich Caruana, David Cohn, Dayne Fre-itag, and Vibhu Mittal.
2000.
Bridging the lexicalchasm: Statistical approaches to answer-finding.
InProceedings of SIGIR?00, Athens, Greece.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311.Robin B. Burke, Kristian J. Hammond, and Vladimir A.Kulyukin.
1997.
Question answering fromfrequently-asked question files: Experiences with theFAQ finder system.
AI Magazine, 18(2):57?66.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yo-ram Singer.
2006.
Online passive-agressive algorithms.
Machine Learning, 7:551?585.Pablo Ariel Duboue and Jennifer Chu-Carroll.
2006.
An-swering the question you wish they had asked: The im-pact of paraphrasing for question answering.
In Pro-ceedings of (HLT-NAACL?06), New York, NY.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of (ACL?03), Sapporo, Japan.Sanda Harabagiu and Finley Lacatusu.
2004.
Strategiesfor advanced question answering.
In Proceedings ofthe HLT-NAACL?04 Workshop on Pragmatics of Ques-tion Answering, Boston, MA.Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, RadaMihalcea, Mihai Surdeanu, Ra?zvan Bunescu, RoxanaG?
?rju, Vasile Rus, and Paul Mora?rescu.
2001.
Therole of lexico-semantic feedback in open-domain tex-tual question-answering.
In Proceedings of (ACL?01),Toulouse, France.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformulationresource and web exploitation for question answering.In Proceedings of TREC-11, Gaithersburg, MD.Eduard Hovy, Laurie Gerber, Ulf Hermjakob, MichaelJunk, and Chin-Yew Lin.
2000.
Question answeringin webclopedia.
In Proceedings of TREC 9, Gaithers-burg, MD.Abraham Ittycheriah, Martin Franz, and Salim Roukos.2001.
IBM?s statistical question answering system.
InProceedings of TREC 10, Gaithersburg, MD.Valentin Jijkoun and Maarten de Rijke.
2005.
Retrievinganswers from frequently asked questions pages on theweb.
In Proceedings of the Tenth ACM Conference onInformation and Knowledge Management (CIKM?05),Bremen, Germany.Thorsten Joachims.
2003.
Transductive learningvia spectral graph partitioning.
In Proceedings ofICML?03, Washington, DC.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of (HLT-NAACL?03), Edmonton, Cananda.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Journal of NaturalLanguage Engineering, 7(3):343?360.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings of(HLT-NAACL?03), Edmonton, Cananda.John Prager, Jennifer Chu-Carroll, and Krysztof Czuba.2001.
Use of wordnet hypernyms for answering what-is questions.
In Proceedings of TREC 10, Gaithers-burg, MD.Yonggang Qiu and H. P. Frei.
1993.
Concept based queryexpansion.
In Proceedings of SIGIR?93, Pittsburgh,PA.Dragomir R. Radev, Hong Qi, Zhiping Zheng, SashaBlair-Goldensohn, Zhu Zhang, Weigo Fan, and JohnPrager.
2001.
Mining the web for answers to natu-ral language questions.
In Proceedings of (CIKM?01),Atlanta, GA.Radu Soricut and Eric Brill.
2006.
Automatic questionanswering using the web: Beyond the factoid.
Journalof Information Retrieval - Special Issue on Web Infor-mation Retrieval, 9:191?206.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of SI-GIR?94, Dublin, Ireland.Jinxi Xu and W. Bruce Croft.
1996.
Query expansionusing local and global document analysis.
In Proceed-ings of SIGIR?96, Zurich, Switzerland.471
