Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46?54,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguisticsn-Best Parsing Revisited?Matthias Bu?chse and Daniel Geisler and Torsten Stu?ber and Heiko VoglerFaculty of Computer ScienceTechnische Universita?t Dresden01062 Dresden{buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.deAbstractWe derive and implement an algorithmsimilar to (Huang and Chiang, 2005) forfinding the n best derivations in a weightedhypergraph.
We prove the correctness andtermination of the algorithm and we showexperimental results concerning its run-time.
Our work is different from the afore-mentioned one in the following respects:we consider labeled hypergraphs, allowingfor tree-based language models (Malettiand Satta, 2009); we specifically handlethe case of cyclic hypergraphs; we admitstructured weight domains, allowing formultiple features to be processed; we usethe paradigm of functional programmingtogether with lazy evaluation, achievingconcise algorithmic descriptions.1 IntroductionIn statistical natural language processing, proba-bilistic models play an important role which canbe used to assign to some input sentence a set ofanalyses, each carrying a probability.
For instance,an analysis can be a parse tree or a possible trans-lation.
Due to the ambiguity of natural language,the number of analyses for one input sentence canbe very large.
Some models even assign an infinitenumber of analyses to an input sentence.In many cases however, the set of analyses canin fact be represented in a finite and compact way.While such a representation is space-efficient, itmay be incompatible with subsequent operations.In these cases a finite subset is used as an approx-imation, consisting of n best analyses, i. e. n anal-yses with highest probability.
For example, thisapproach has the following two applications.
(1) Reranking: when log-linear models (Ochand Ney, 2002) are employed, some features may?
This research was financially supported by DFG VO1101/5-1.not permit an efficient evaluation during the com-putation of the analyses.
These features are com-puted using individual analyses from said approx-imation, leading to a reranking amongst them.
(2) Spurious ambiguity: many models produceanalyses which may be too fine-grained for furtherprocessing (Li et al, 2009).
As an example, con-sider context-free grammars, where several left-most derivations may exist for the same terminalstring.
The weight of the terminal string is ob-tained by summing over these derivations.
Then best leftmost derivations may be used to approx-imate this sum.In this paper, we consider the case where thefinite, compact representation has the form of aweighted hypergraph (with labeled hyperedges)and the analyses are derivations of the hypergraph.This covers many parsing applications (Klein andManning, 2001), including weighted deductivesystems (Goodman, 1999; Nederhof, 2003), andalso applications in machine translation (May andKnight, 2006).In the nomenclature of (Huang and Chiang,2005), which we adopt here, a derivation of a hy-pergraph is a tree which is obtained in the follow-ing way.
Starting from some node, an ingoing hy-peredge is picked and recorded as the label of theroot of the tree.
Then, for the subtrees, one con-tinues with the source nodes of said hyperedge inthe same way.
In other words, a derivation can beunderstood as an unfolding of the hypergraph.The n-best-derivations problem then amountsto finding n derivations which are best with re-spect to the weights induced by the weighted hy-pergraph.1 Among others, weighted hypergraphswith labeled hyperedges subsume the followingtwo concepts.
(I) probabilistic context-free grammars (pcfgs).1Note that this problem is different from the n-best-hyperpaths problem described by Nielsen et al (2005), asalready argued in (Huang and Chiang, 2005, Section 2).46In this case, nodes correspond to nonterminals,hyperedges are labeled with productions, and thederivations are exactly the abstract syntax trees(ASTs) of the grammar (which are closely relatedthe parse trees).
Note that, unless the pcfg is un-ambiguous, a given word may have several cor-responding ASTs, and its weight is obtained bysumming over the weights of the ASTs.
Hence,the n best derivations need not coincide with then best words (cf.
application (2) above).
(II) weighted tree automata (wta) (Alexandrakisand Bozapalidis, 1987; Berstel and Reutenauer,1982; ?Esik and Kuich, 2003; Fu?lo?p and Vogler,2009).
These automata serve both as a tree-basedlanguage model and as a data structure for theparse forests obtained from that language modelby applying the Bar-Hillel construction (Malettiand Satta, 2009).
It is well known that context-freegrammars and tree automata are weakly equiv-alent (Thatcher, 1967; ?Esik and Kuich, 2003).However, unlike the former formalism, the latterone has the ability to model non-local dependen-cies in parse trees.In the case of wta, nodes correspond to states,hyperedges are labeled with input symbols, andthe derivations are exactly the runs of the automa-ton.
Since, due to ambiguity, a given tree mayhave several accepting runs, the n best derivationsneed not coincide with the n best trees.
As forthe pcfgs, this is an example of spurious ambigu-ity, which can be tackled as indicated by appli-cation (2) above.
Alternatively, one can attemptto find an equivalent deterministic wta (May andKnight, 2006; Bu?chse et al, 2009).Next, we briefly discuss four known algorithmswhich solve the n-best-derivations problem orsubproblems thereof.?
The Viterbi algorithm solves the 1-best-derivation problem for acyclic hypergraphs.
It isbased on a topological sort of the hypergraph.?
Knuth (1977) generalizes Dijkstra?s algorithm(for finding the single-source shortest paths in agraph) to hypergraphs, thus solving the case n = 1even if the hypergraph contains cycles.
Knuth as-sumes the weights to be real numbers, and he re-quires weight functions to be monotone and supe-rior in order to guarantee that a best derivation ex-ists.
(The superiority property corresponds to Di-jkstra?s requirement that edge weights?or, moregenerally, cycle weights?are nonnegative.)?
Huang and Chiang (2005) show that the n-best-derivations problem can be solved efficientlyby first solving the 1-best-derivation problem andthen extending that solution in a lazy manner.Huang and Chiang assume weighted unlabeled hy-pergraphs with weights computed in the reals, andthey require the weight functions to be monotone.Moreover they assume that the 1-best-derivation problem be solved using the Viterbialgorithm, which implies that the hypergraph mustbe acyclic.
However they conjecture that theirsecond phase also works for cyclic hypergraphs.?
Pauls and Klein (2009) propose a variationof the algorithm of Huang and Chiang (2005) inwhich the 1-best-derivation problem is computedvia an A?-based exploration of the 1-best charts.In this paper, we also present an algorithmfor solving the n-best-derivations problem.
Ulti-mately it uses the same algorithmic ideas as theone of Huang and Chiang (2005); however, it isdifferent in the following sense:1. we consider labeled hypergraphs, allowingfor wta to be used in parsing;2. we specifically handle the case of cyclichypergraphs, thus supporting the conjecture ofHuang and Chiang; for this we impose on theweight functions the same requirements as Knuthand use his algorithm;3. by using the concept of linear pre-orders (andnot only linear orders on the set of reals) our ap-proach can handle structured weights such as vec-tors over frequencies, probabilities, and reals;4. we present our algorithm in the frameworkof functional programming (and not in that of im-perative programming); this framework allows todecribe algorithms in a more abstract and concise,yet natural way;5. due to the lazy evaluation paradigm oftenfound in functional programming, we obtain thelaziness on which the algorithm of Huang and Chi-ang (2005) is based for free;6. exploiting the abstract level of description(see point 4) we are able to prove the correctnessand termination of our algorithm.At the end of this paper, we will discuss experi-ments which have been performed with an imple-mentation of our algorithm in the functional pro-gramming language HASKELL.2 The n-best-derivations problemIn this section, we state the n-best-derivationsproblem formally, and we give a comprehensive47example.
First, we introduce some basic notions.Trees and hypergraphs The definition ofranked trees commonly used in formal tree lan-guage theory will serve us as the basis for definingderivations.A ranked alphabet is a finite set ?
(of symbols)where every symbol carries a rank (a nonnegativeinteger).
By ?
(k) we denote the set of those sym-bols having rank k. The set of trees over ?, de-noted by T?
, is the smallest set T such that forevery k ?
N, ?
?
?
(k), and ?1, .
.
.
, ?k ?
T ,also ?
(?1, .
.
.
, ?k) ?
T ;2 for ?
?
?
(0) we ab-breviate ?
() by ?.
For every k ?
N, ?
??
(k) and subsets T1, .
.
.
, Tk ?
T?
we definethe top-concatenation (with ?)
?
(T1, .
.
.
, Tk) ={?
(?1, .
.
.
, ?k) | ?1 ?
T1, .
.
.
, ?k ?
Tk}.A ?-hypergraph is a pair H = (V,E) whereV is a finite set (of vertices or nodes) and E ?V ???
?V is a finite set (of hyperedges) such thatfor every (v1 .
.
.
vk, ?, v) ?
E we have that ?
??
(k).3 We interpret E as a ranked alphabet wherethe rank of each edge is carried over from its labelin ?.
The family (Hv | v ?
V ) of derivations of His the smallest family (Pv | v ?
V ) of subsetsof TE such that e(Pv1 , .
.
.
, Pvk) ?
Pv for everye = (v1 .
.
.
vk, ?, v) ?
E.A ?-hypergraph (V,E) is cyclic if thereare hyperedges (v11 .
.
.
v1k1 , ?1, v1), .
.
.
,(vl1 .
.
.
vlkl , ?l, vl) ?
E such that vj?1 occursin vj1 .
.
.
vjkj for every j ?
{2, .
.
.
, l} and vl occursin v11 .
.
.
v1k1 .
It is called acyclic if it is not cyclic.Example 1 Consider the ranked alphabet ?
=?(0)??(1)??
(2) with ?
(0) = {?, ?
}, ?
(1) = {?
},and ?
(2) = {?
}, and the ?-hypergraph H =(V,E) where?
V = {0, 1} and?
E = {(?, ?, 1), (?, ?, 1), (1, ?, 1), (11, ?, 0),(1, ?, 0)}.A graphical representation of this hypergraph isshown in Fig.
1.
Note that this hypergraph is cyclicbecause of the edge (1, ?, 1).We indicate the derivations of H , assuming thate1, .
.
.
, e5 are the edges in E in the order givenabove:2The term ?
(?1, .
.
.
, ?k) is usually understood as a stringcomposed of the symbol ?, an opening parenthesis, thestring ?1, a comma, and so on.3The hypergraphs defined here are essentially nondeter-ministic tree automata, where V is the set of states and E isthe set of transitions.?
?0 1 ??
?e5e3e4e1e2Figure 1: Hypergraph of Example 1.?
H1 = {e1, e2, e3(e1), e3(e2), e3(e3(e1)), .
.
.
}and?
H0 = e4(H1,H1) ?
e5(H1) where, e. g.,e4(H1,H1) is the top-concatenation of H1,H1 with e4, and thuse4(H1,H1) = {e4(e1, e1), e4(e1, e2),e4(e1, e3(e1)), e4(e3(e1), e1), .
.
. }
.Next we give an example of ambiguity in hyper-graphs with labeled hyperedges.
Suppose that Econtains an additional hyperedge e6 = (0, ?, 0).Then H0 would contain the derivations e6(e5(e1))and e5(e3(e1)), which describe the same ?-tree,viz.
?(?(?))
(obtained by the node-wise projec-tion to the second component).
In the sequel, let H = (V,E) be a ?-hypergraph.Ordering Usually an ordering is induced on theset of derivations by means of probabilities or,more generally, weights.
In the following, we willabstract from the weights by using a binary rela-tion - directly on derivations, where we will in-terpret the fact ?1 - ?2 as ?
?1 is better than orequal to ?2?.Example 2 (Ex.
1 contd.)
First we show how anordering is induced on derivations by means ofweights.
To this end, we associate an operationover the set R of reals with every hyperedge (re-specting its arity) by means of a mapping ?:?
(e1)() = 4 ?
(e2)() = 3?
(e3)(x1) = x1 + 1 ?
(e4)(x1, x2) = x1 + x2?
(e5)(x1) = x1 + 0.5The weight h(?)
of a tree ?
?
TE is obtained byinterpreting the symbols at each node using ?, e. g.h(e3(e2)) = ?(e3)(?
(e2)()) = ?
(e2)() + 1 = 4.Then the natural order ?
on R induces the bi-nary relation - over TE as follows: for every?1, ?2 ?
TE we let ?1 - ?2 iff h(?1) ?
h(?2),meaning that trees with smaller weights are con-sidered better.
(This is, e. g., the case when calcu-lating probabilites in the image of ?
log x.)
Note48that we could just as well have defined - with theinverted order.Since addition is commutative, we obtainfor every ?1, ?2 ?
TE that h(e4(?1, ?2)) =h(e4(?2, ?1)) and thus e4(?1, ?2) - e4(?2, ?1) andvice versa.
Thus, for two different trees (e4(?1, ?2)and e4(?2, ?1)) having the same weight, - shouldnot prefer any of them.
That is, - need not beantisymmetric.As another example, the mapping ?
could as-sign to each symbol an operation over real-valuedvectors, where each component represents onefeature of a log-linear model such as frequencies,probabilities, reals, etc.
Then the ordering couldbe defined by means of a linear combination of thefeature weights.
We use the concept of a linear pre-order to cap-ture the orderings which are obtained this way.Let S be a set.
A pre-order (on S) is a binaryrelation - ?
S ?
S such that (i) s - s for ev-ery s ?
S (reflexivity) and (ii) s1 - s2 and s2 - s3implies s1 - s3 for every s1, s2, s3 ?
S (transi-tivity).
A pre-order - is called linear if s1 - s2or s2 - s1 for every s1, s2 ?
S. For instance, thebinary relation - on TE as defined in Ex.
2 is alinear pre-order.We will restrict our considerations to a classof linear pre-orders which admit efficient algo-rithms.
For this, we will always assume a lin-ear pre-order - with the following two properties(cf.
Knuth (1977)).4SP (subtree property) For every e(?1, .
.
.
, ?k) ?TE and i ?
{1, .
.
.
, k} we have ?i -e(?1, .
.
.
, ?k).5CP (compatibility) For every pair e(?1, .
.
.
, ?k),e(?
?1, .
.
.
, ?
?k) ?
TE with ?1 - ?
?1, .
.
.
,?k - ?
?k we have that e(?1, .
.
.
, ?k) -e(?
?1, .
.
.
, ?
?k).It is easy to verify that the linear pre-order - ofEx.
2 has the aforementioned properties.In the sequel, let- be a linear pre-orderon TE fulfilling SP and CP.4Originally, these properties were called ?superiority?
and?monotonicity?
because they were viewed as properties ofthe weight functions.
We use the terms ?subtree property?and ?compatibility?
respectively, because we view them asproperties of the linear pre-order.5This strong property is used here for simplicity.
It suf-fices to require that for every v ?
V and pair ?, ??
?
Hv wehave ?
- ??
if ?
is a subtree of ?
?.Before we state the n-best-derivations problemformally, we define the operation minn, whichmaps every subset T of TE to the set of all se-quences of n best elements of T .
To this end, letT ?
TE and n ?
|T |.
We define minn(T ) to bethe set of all sequences (?1, .
.
.
, ?n) ?
T n of pair-wise distinct elements such that ?1 - .
.
.
- ?n andfor every ?
?
T \ {?1, .
.
.
, ?k} we have ?n - ?.For every n > |T | we set minn(T ) = min|T |(T ).In addition, we set min?n(T ) =?ni=0 mini(T ).n-best-derivations problem The n-best-derivations problem amounts to the following.Given a ?-hypergraph H = (V,E), a vertex v ?V , and a linear pre-order - on TE fulfillingSP and CP,compute an element of minn(Hv).3 Functional ProgrammingWe will describe our main algorithm as a func-tional program.
In essence, such a program is asystem of (recursive) equations that defines sev-eral functions (as shown in Fig.
2).
As a conse-quence the main computational paradigm for eval-uating the application (f a) of a function f to anargument a is to choose an appropriate definingequation f x = r and then evaluate (f a) to r?which is obtained from r by substituting every oc-currence of x by a.We assume a lazy (and in particular, call-by-need) evaluation strategy, as in the functional pro-gramming language HASKELL.
Roughly speak-ing, this amounts to evaluating the arguments ofa function only as needed to evaluate the its body(i. e. for branching).
If an argument occurs multi-ple times in the body, it is evaluated only once.We use HASKELL notation and functions fordealing with lists, i. e. we denote the empty list by[] and list construction by x:xs (where an ele-ment x is prepended to a list xs), and we use thefunctions head (line 01), tail (line 02), and take(lines 03 and 04), which return the first element ina list, a list without its first element, and a prefixof a list, respectively.In fact, the functions shown in Fig.
2 will beused in our main algorithm (cf.
Fig.
4).
Thus,we explain the functions merge (lines 05?07) ande(l1, .
.
.
,lk) (lines 08?10) a bit more in detail.The merge function takes a set L of pairwisedisjoint lists of derivations, each one in ascend-ing order with respect to -, and merges them into49-- standard Haskell functions: list deconstructors, take operation01 head (x:xs) = x02 tail (x:xs) = xs03 take n xs = [] if n == 0 or xs == []04 take n xs = (head xs):take (n-1) (tail xs)-- merge operation (lists in L should be disjoint)05 merge L = [] if L \ {[]} = ?06 merge L = m:merge ({tail l | l ?
L, l != [], head l == m} ?
{l | l ?
L, l != [], head l != m})07 where m = min{head l | l ?
L, l != []}-- top concatenation08 e(l1, .
.
.
,lk) = [] if li == [] for some i ?
{1, .
.
.
, k}09 e(l1, .
.
.
,lk) = e(head l1, .
.
.
, head lk):merge {e(li1, .
.
.
,lik) | i ?
{1, .
.
.
, k}}10 where lij =????
?lj if j < itail lj if j = i[head lj] if j > iFigure 2: Some useful functions specified in a functional programming style.one list with the same property (as known from themerge sort algorithm).Note that the minimum used in line 07 is basedon the linear pre-order -.
For this reason, itneed not be uniquely determined.
However, in animplementation this function is deterministic, de-pending on the the data structures.The function e(l1, .
.
.
,lk) implements the top-concatenation with e on lists of derivations.
It isdefined for every e = (v1 .
.
.
vk, ?, v) ?
E andtakes lists l1, .
.
.
, lk of derivations, each in as-cending order as for merge.
The resulting list isalso in ascending order.4 AlgorithmIn this section, we develop our algorithm for solv-ing the n-best-derivations problem.
We begin bymotivating our general approach, which amountsto solving the 1-best-derivation problem first andthen extending that solution to a solution of the n-best-derivations problem.It can be shown that for every m ?
n, theset minn(Hv) is equal to the set of all prefixes oflength n of elements of minm(Hv).
Accordingto this observation, we will develop a function pmapping every v ?
V to a (possibly infinite) listsuch that the prefix of length n is in minn(Hv)for every n. Then, by virtue of lazy evaluation, asolution to the n-best-derivations problem can beobtained by evaluating the termtake n (p v)where take is specified in lines 03?04 of Fig.
2.Thus, what is left to be done is to specify p appro-priately.4.1 A provisional specification of pConsider the following provisional specificationof p:p v = merge {e(p v1, .
.
.
,p vk) |e = (v1 .
.
.
vk, ?, v) ?
E} (?
)where the functions merge and e(l1, .
.
.
,lk) arespecified in lines 05?07 and lines 08?10 of Fig.
2,respectively.
This specification models exactly thetrivial equationHv =?e=(v1...vk ,?,v)?Ee(Hv1 , .
.
.
,Hvk)for every v ?
V , where the union and the top-concatenation have been implemented for lists viathe functions merge and e(l1, .
.
.
,lk).This specification is adequate if H is acyclic.For cyclic hypergraphs however, it can not evensolve the 1-best-derivation problem.
To illustratethis, we consider the hypergraph of Ex.
2 and cal-50culate6take 1 (p 1)= (head (p 1)):take 0 (tail (p 1)) (04)= head (p 1) (03)= head (merge {e1(), e2(), e3(p 1)}) (?
)= min{head e1(), head e2(), head e3(p 1)}(01, 06, 07)= min{head e1(), head e2(), e3(head (p 1))}.
(09)Note that the infinite regress occurs because thecomputation of the head element head (p 1) de-pends on itself.
This leads us to the idea of?pulling?
this head element (which is the solu-tion to the 1-best-derivation problem) ?out?
of themerge in (?).
Applying this idea to our particularexample, we reach the following equation for p 1:p 1 = e2: merge {e1(), e3(p 1)}because e2 is the best derivation in H1.
Then, inorder to evaluate merge we have to computemin{head e1(), head e3(p 1)}= min{e1, e3(head (p 1))}= min{e1, e3(e2)}.Since h(e1) = h(e3(e2)) = 4, we can choose anyof them, say e1, and continue:e2: merge {e1(), e3(p 1)}= e2: e1: merge {tail e1(), e3(p 1)}= e2: e1: e3(e2): merge {tail e3(p 1)}= ...Generalizing this example, the function p couldbe specified as follows:p 1 = (b 1) : merge {exp} (??
)where b 1 evaluates the 1-best derivation in H1and exp ?somehow?
calculates the next bestderivations.
In the following subsection, we elabo-rate this approach.
First, we develop an algorithmfor solving the 1-best-derivation problem.4.2 Solving the 1-best-derivation problemUsing SP and CP, it can be shown that for ev-ery v ?
V such that Hv 6= ?
there is a mini-mal derivation in Hv which does not contain anysubderivation in Hv (apart from itself).
In otherwords, it is not necessary to consider cycles whensolving the 1-best-derivation problem.6Please note that e1() is an application of the function inlines 08?10 of Fig.
2 while e1 is a derivation.We can exploit this knowledge in a program bykeeping a set U of visited nodes, taking care not toconsider edges which lead us back to those nodes.Consider the following function:b v U = min{e(b v1 U?, .
.
.
, b vk U?)
|e = (v1 .
.
.
vk, ?, v) ?
E,{v1, .
.
.
, vk} ?
U?
= ?
}where U?
= U ?
{v}The argument U is the set of visited nodes.
Theterm b v ?
evaluates to a minimal element of Hv,or to min ?
if Hv = ?.
The problem of this divide-and-conquer (or top-down) approach is that man-aging a separate set U for every recursive call in-curs a big overhead in the computation.This overhead can be avoided by using adynamic programming (or bottom-up) approachwhere each node is visited only once, and nodesare visited in the order of their respective bestderivations.To be more precise, we maintain a family (Pv |v ?
V ) of already found best derivations (wherePv ?
min?1(Hv) and initially empty) and a set Cof candidate derivations, where candidates for allvertices are considered at the same time.
In eachiteration, a minimal candidate with respect to - isselected.
This candidate is then declared the bestderivation of its respective node.The following lemma shows that the bottom-upapproach is sound.Lemma 3 Let (Pv | v ?
V ) be a family such thatPv ?
min?1(Hv).
We defineC = ?e=(v1...vk ,?,v)?E,Pv=?e(Pv1 , .
.
.
, Pvk) .Then (i) for every ?
?
?v?V,Pv=?
Hv there is a??
?
C such that ??
 ?, and (ii) for every v ?
Vand ?
?
C ?
Hv the following implication holds:if ?
?
??
for every ??
?
C , then ?
?
min1(Hv).An algorithm based on this lemma is shown inFig.
3.
Its key function iter uses the notion of ac-cumulating parameters.
The parameter q is a map-ping corresponding to the family (Pv | v ?
V ) ofthe lemma, i. e., q v = Pv; the parameter c is aset corresponding to C .
We begin in line 01 withthe function q0 mapping every vertex to the emptylist.
According to the lemma, the candidates thenconsist of the nullary edges.As long as there are candidates left (line 04),in a recursive call of iter the parameter q is up-dated with the newly found pair (v, [?])
of ver-tex v and (list of) best derivation ?
(expressed by51Require ?-hypergraph H = (V,E), linear pre-order - fulfilling SP and CP.Ensure b v ?
min1(Hv) for every v ?
Vsuch that if b v == [e(?1, .
.
.
, ?k)] for somee = (v1 .
.
.
vk, ?, v) ?
E, then b vi == [?i] forevery i ?
{1, .
.
.
, k}.01 b = iter q0 {(?, ?, v) ?
E | ?
?
?
(0)}02 q0 v = []03 iter q ?
= q04 iter q c = iter (q//(v,[?]))
c?05 where06 ?
= min c and ?
?
Hv07 c?
=?e=(v1...vk,?,v)?Eq v == []e(q v1, .
.
.
,q vk)Figure 3: Algorithm solving the 1-best-derivationproblem.q//(v,[?]))
and the candidate set is recomputedaccordingly.
When the candidate set is exhausted(line 03), then q is returned.Correctness and completeness of the algorithmfollow from Statements (ii) and (i) of Lemma 3,respectively.
Now we show termination.
In everyiteration a new next best derivation is determinedand the candidate set is recomputed.
This set onlycontains candidates for vertices v ?
V such thatq v == [].
Hence, after at most |V | iterationsthe candidates must be depleted, and the algorithmterminates.We note that the algorithm is very similar to thatof Knuth (1977).
However, in contrast to the latter,(i) it admits Hv = ?
for some v ?
V and (ii) itcomputes some minimal derivation instead of theweight of some minimal derivation.Runtime According to the literature, the run-time of Knuth?s algorithm is in O(|E| ?
log|V |)(Knuth, 1977).
This statement relies on a numberof optimizations which are beyond our scope.
Wejust sketch two optimizations: (i) the candidate setcan be implemented in a way which admits ob-taining its minimum in O(log|C|), and (ii) for thecomputation of candidates, each edge needs to beconsidered only once during the whole run of thealgorithm.4.3 Solving the n-best-derivations problemBeing able to solve the 1-best-derivation problem,we can now refine our specification of p. The re-fined algorithm is given in Fig.
4; for the func-tions not given there, please refer to Fig.
3 (func-tion b) and to Fig.
2 (functions merge, tail, andthe top-concatenation).
In particular, line 02 ofFig.
4 shows the general way of ?pulling out?
thehead element as it was indicated in Section 4.1 viaan example.
We also remark that the definition ofthe top-concatenation (lines 08?09 of Fig.
2) cor-responds to the way in which multk was sped upin Fig.
4 of (Huang and Chiang, 2005).Theorem 4 The algorithm in Fig.
4 is correct withrespect to its require/ensure specification and itterminates for every input.PROOF (SKETCH).
We indicate how induction on ncan be used for the proof.
If n = 0, then the statementis trivially true.
Let n > 0.
If b v == [], then thestatement is trivially true as well.
Now we consider theconverse case.
To this end, we use the following threeauxiliary statements.
(1) take n (merge {l1, .
.
.
,lk}) =take n (merge {take n l1, .
.
.
,take n lk}),(2) take n e(l1, .
.
.
,lk) =take n e(take n l1, .
.
.
,take n lk),(3) take n (tail l) = tail (take (n+1) l).Using these statements, line 04 of Fig.
2, and line 02of Fig.
4, we are able to ?pull?
the take of take n (pv) ?into?
the right-hand side of p v, ultimately yield-ing terms of the form take n (p vj) in the first lineof the merge application and take (n-1) (p v?j) inthe second one.Then we can show the following statement by induc-tion on m (note that the n is still fixed from the outerinduction): for every m ?
N we have that if the treein b v has at most height m, then take n (p v) ?minn(Hv).
To this end, we use the following two aux-iliary statements.
(4) For every sequence of pairwise disjoint sub-sets P1, .
.
.
, Pk ?
?v?V Hv, sequence of nat-ural numbers n1, .
.
.
, nk ?
N, and lists l1 ?minn1(P1), .
.
.
, lk ?
minnk(Pk) such thatnj ?
n for every j ?
{1, .
.
.
, k} we have thattake n (merge {l1, .
.
.
, lk}) ?
minn(P1?.
.
.?Pk).
(5) For every edge e = (v1 .
.
.
vk, ?, v) ?
E, subsetsP1, .
.
.
, Pk ?
?v?V Hv , and lists l1 ?
minn(P1), .
.
.
,lk ?
minn(Pk) we have that take n e(l1, .
.
.
, lk) ?minn(e(P1, .
.
.
, Pk)).Using these statements, it remains to show that{e(?1, .
.
.
, ?k)} ?
minn?1((e(Hv1 , .
.
.
, Hvk) \{e(?1, .
.
.
, ?k)}) ??e?
6=e e?
(Hv?1 , .
.
.
, Hv?k))?minn(Hv) where b v = [e(?1, .
.
.
, ?k)] and ?denotes language concatenation.
This can be shown byusing the definition of minn.Termination of the algorithm now follows from thefact that every finite prefix of p v is well defined.
52Require ?-hypergraph H = (V,E), linear pre-order - fulfilling SP and CP.Ensure(take n (p v))?
minn(Hv) for every v ?
V and n ?
N.01 p v = [] if b v == []02 p v = e(?1, .
.
.
, ?k):merge ({tail e(p v1, .
.
.
, p vk) | e = (v1 .
.
.
vk, ?, v) ?
E} ?{e?
(p v?1, .
.
.
, p v?k) | e?
= (v?1 .
.
.
v?k, ?
?, v) ?
E, e?
6= e})if b v == [e(?1, .
.
.
, ?k)]Figure 4: Algorithm solving the n-best-derivations problem.4.4 Implementation, Complexity, andExperimentsWe have implemented the algorithm (consistingof Figs.
3 and 4 and the auxiliary functions ofFig.
2) in HASKELL.
The implementation israther straightforward except for the followingthree points.
(1) Weights: we assume that - is defined bymeans of weights (cf.
Ex.
2), and that comparingthese weights is in O(1) (which often holds be-cause of limited precision).
Hence, we store witheach derivation its weight so that comparison ac-cording to - is in O(1) as well.
(2) Memoization: we use a memoization tech-nique to ensure that no derivation occurring in p vis computed twice.
(3) Merge: the merge operation deserves someconsideration because it is used in a nested fash-ion, yielding trees of merge applications.
Thisleads to an undesirable runtime complexity be-cause these trees need not be balanced.
Thus, in-stead of actually computing the merge in p and inthe top-concatenation, we just return a data struc-ture describing what should be merged.
That datastructure consists of a best element and a list oflists of derivations to be merged (cf.
lines 06 and09 in Fig.
2).
We use a higher-order function tomanage these data structures on a heap, perform-ing the merge in a nonnested way.Runtime Here we consider the n-best part of thealgorithm, i. e. we assume the computation of themapping b to take constant time.
Note howeverthat due to memoization, b is only computed once.Then the runtime complexity of our implementa-tion is in O(|E|+ |V | ?n ?
log(|E|+n)).
This canbe seen as follows.By line 02 in Fig.
4, the initial heaps in thehigher-order merge described under (3) have a to-tal of |E| elements.
Building these heaps is thusin O(|E|).
By line 09 in Fig.
2, each newly foundderivation spawns at most as many new candidatesn total time [s] time for n-best part [s]1 8.713 ?25 000 10.832 2.11950 000 12.815 4.102100 000 16.542 7.739200 000 24.216 15.503Table 1: Experimental resultson the heap as the maximum rank in ?.
We assumethis to be constant.
Moreover, at most n deriva-tions are computed for each node, that is, at most|V |?n in total.
Hence, the size of the heap of a nodeis in O(|E|+n).
For each derivation we compute,we have to pop the minimal element off the heap(cf.
line 07 in Fig.
2), which is in O(log(|E|+n)),and we have to compute the union of the remainingheap with the newly spawned candidates, whichhas the same complexity.We give another estimate for the total numberof derivations computed by the algorithm, whichis based on the following observation.
When pop-ping a new derivation ?
off the heap, new next bestcandidates are computed.
This involves comput-ing at most as many new derivations as the numberof nodes of ?, because for each hyperedge occur-ring in ?
we have to consider the next best alter-native.
Since we pop off at most n elements fromthe heap belonging to the target node, we arrive atthe estimate d ?n, where d is the size of the biggestderivation of said node.A slight improvement of the runtime complex-ity can be obtained by restricting the heap size ton best elements, as argued by Huang and Chiang(2005).
This way, they are able to obtain the com-plexity O(|E| + d ?
n ?
log n).We have conducted experiments on an IntelCore Duo 1200 MHz with 2 GB of RAM usinga cyclic hypergraph containing 671 vertices and12136 edges.
The results are shown in Table 1.This table indicates that the runtime of the n-bestpart is roughly linear in n.53ReferencesAthanasios Alexandrakis and Symeon Bozapalidis.1987.
Weighted grammars and Kleene?s theorem.Inform.
Process.
Lett., 24(1):1?4.Jean Berstel and Christophe Reutenauer.
1982.
Recog-nizable formal power series on trees.
Theoret.
Com-put.
Sci., 18(2):115?148.Matthias Bu?chse, Jonathan May, and Heiko Vogler.2009.
Determinization of weighted tree automatausing factorizations.
Talk presented at FSMNLP 09in Pretoria, South Africa.Zolta?n ?Esik and Werner Kuich.
2003.
Formal tree se-ries.
J. Autom.
Lang.
Comb., 8(2):219?285.Zolta?n Fu?lo?p and Heiko Vogler.
2009.
Weighted treeautomata and tree transducers.
In Manfred Droste,Werner Kuich, and Heiko Vogler, editors, Handbookof Weighted Automata, chapter 9.
Springer.Joshua Goodman.
1999.
Semiring parsing.
Comp.Ling., 25(4):573?605.Liang Huang and David Chiang.
2005.
Better k-best parsing.
In Parsing ?05: Proceedings of theNinth International Workshop on Parsing Technol-ogy, pages 53?64.
ACL.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proceedings of IWPT, pages123?134.Donald E. Knuth.
1977.
A Generalization of Dijkstra?sAlgorithm.
Inform.
Process.
Lett., 6(1):1?5, Febru-ary.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proc.
ACL-IJCNLP ?09, pages 593?601.ACL.Andreas Maletti and Giorgio Satta.
2009.
Parsing al-gorithms based on tree automata.
In Proc.
11th Int.Conf.
Parsing Technologies, pages 1?12.
ACL.Jonathan May and Kevin Knight.
2006.
A better n-bestlist: practical determinization of weighted finite treeautomata.
In Proc.
HLT, pages 351?358.
ACL.Mark-Jan Nederhof.
2003.
Weighted deductive pars-ing and Knuth?s algorithm.
Comp.
Ling., 29(1):135?143.Lars Relund Nielsen, Kim Allan Andersen, andDaniele Pretolani.
2005.
Finding the k shortest hy-perpaths.
Comput.
Oper.
Res., 32(6):1477?1497.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In ACL, pages 295?302.Adam Pauls and Dan Klein.
2009. k-best a* parsing.In Proc.
ACL-IJCNLP ?09, pages 958?966, Morris-town, NJ, USA.
ACL.J.
W. Thatcher.
1967.
Characterizing derivation treesof context-free grammars through a generalizationof finite automata theory.
J. Comput.
Syst.
Sci.,1(4):317?322.54
