Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 1?8,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsActive Learning for Anaphora ResolutionCaroline GasperinComputer Laboratory, University of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKcvg20@cl.cam.ac.ukAbstractIn this paper we present our experiments withactive learning to improve the performanceof our probabilistic anaphora resolution sys-tem.
We have adopted entropy-based uncer-tainty measures to select new instances to beadded to our training data.
The actively se-lected instances, however, were not more suc-cessful in improving the performance of thesystem than the same amount of randomly se-lected instances.
The uncertainty measureswe used behave differently from each otherwhen selecting new instances, but none ofthem achieved remarkable performance.
Fur-ther studies on active sample selection foranaphora resolution are necessary.1 IntroductionAnaphora is the relation between two linguistic ex-pressions in the discourse where the reader is re-ferred back to the first of them when reading the sec-ond later in the text.
Anaphora resolution can be un-derstood as the process of identifying an anaphoricrelation between two expressions in the text and con-sequently linking the two of them, one being theanaphor and the other being the antecedent.
Man-ually annotating corpora with anaphoric links in or-der to use it as training or test data for a corpus-basedanaphora resolution system is a particulary difficultand time consuming task, given the complex natureof the phenomenon.We have developed a probabilistic model for res-olution of non-pronominal anaphora and aim to im-prove its performance by acquiring incrementallyand selectively more training data using active learn-ing.
We have adopted an uncertainty-based activelearning approach in order to do that, and it uses ourprobabilistic model as the base classifier.The uncertainty-based approach has been appliedto, for instance, named-entity recognition by Shen etal.
(2004) who report at least 80% reduction in an-notation costs, parsing by Tang et al (2002) who re-ports 67% savings, and parse selection by Baldridgeand Osborne (2003) who report 60% savings.
Weare not aware of any work that has applied activelearning to anaphora resolution.For calculating the uncertainty of an anaphora res-olution model, we feel the need to combine the in-formation about the confindence of the model forthe classification of each antecedent candidate as-sociated to a given anaphor.
We have tested threeentropy-based uncertainty measures in order to se-lect the instances to be added to the training data.Our training corpus is composed of five full-length scientific articles from the biomedical do-main.
We have used this corpus to simulate activelearning: we have divided our training data into twoparts, one for the initial training and the other for ac-tive learning (simulating unlabelled data), and havecompared the classifier performance when trainedon a sample selected by active learning to its per-formance when trained on the same amount of ran-domly selected instances.In the next section we describe our probabilisticmodel for anaphora resolution.
In Section 3 we de-tail our training corpus.
In Section ??
we describethe strategy we have adopted to select the samplesto take part in the active learning, and in Section 51we describe our experiments.2 Anaphora resolution modelWe have inplemented a probabilistic model foranaphora resolution in the biomedical domain(Gasperin and Briscoe, 2008).
This model aims toresolve both coreferent and associative (also calledbridging (Poesio and Vieira, 1998)) cases of non-pronominal anaphora.
Table 1 shows examples ofthese types of anaphoric relations.
Coreferent arethe cases in which the anaphor and the antecedentrefer to the same entity in the world, while associa-tive cases are the ones in which the anaphor and an-tecedent refer to different but somehow related en-tities.
We only take into account noun phrases re-ferring to biomedical entities, since this was the fo-cus of our resolution model.
We consider two typesof associative relations: biotype relations, whichare anaphoric associative relations between nounphrases that share specific ontological relations inthe biomedical domain; and set-member relations,in which the noun phrases share a set-membershiprelation.
It is frequent however that some nounphrases do not have an antecedent, these are con-sidered discourse-new cases, which we also aim toidentify.The probabilistic model results from a simple de-composition process applied to a conditional proba-bility equation that involves several parameters (fea-tures).
It is inspired by Ge et al?s (1998) probabilis-tic model for pronoun resolution.
The decomposi-tion makes use of Bayes?
theorem and independenceassumptions, and aims to decrease the impact of datasparseness on the model, so that even small train-ing corpora can be viable.
The decomposed modelcan be thought of as a more sophisticated versionof the naive-Bayes algorithm, since we consider thedependence among some of the features instead offull independence as in naive Bayes.
Probabilisticmodels can return a confidence measure (probabil-ity) for each decision they make, which allow us toadopt techniques such as active learning for furtherprocessing.Our model seeks to classify the relation betweenan anaphoric expression and an antecedent candi-date as coreferent, biotype, set-member or neither.It computes the probability of each pair of anaphorand candidate for each class.
The candidate with thehighest overall probability for each class is selectedas the antecedent for that class, or no antecedent isselected if the probability of no relation overcomesthe positive probabilities; in this case, the expressionis considered to be new to the discourse.We have chosen 11 features to describe theanaphoric relations between an antecedent candi-date a and an anaphor A.
The features are pre-sented in Table 2.
Most features are domain-independent, while one, gpa,A, is specific for thebiomedical domain.
Our feature set covers the basicaspects that influence anaphoric relations: the formof the anaphor?s NP, string matching, semantic classmatching, number agreement, and distance.Given these features, we compute the probabilityP of an specific class of anaphoric relation C be-tween a (antecedent candidate) andA (anaphor).
Foreach pair of a given anaphor and an antecedent can-didate we compute P for C=?coreferent?, C=?biotype?,and C=?set-member?.
We also compute C=?none?, thatrepresents the probability of no relation between theNPs.
P can be defined as follows:P (C = ?class?|fA, fa, hma,A, hmma,A,mma,A,numa,A, sra, bma,A, gpa,A, da,A, dma,A)If we were to use P as above we would suffer con-siderably data sparseness.
In order to reduce that, wedecompose the probability P and assume indepen-dence among some of the features in order to handlethe sparseness of the training data.
For more detailon the decomposition process refer to (Gasperin andBriscoe, 2008).Applying Bayes?
rule and selectively applying thechain rule to the above equation, as well as assum-ing independece among some features, we reach thefollowing equation:P (C|fA, fa, hm, hmm,mm,num, sr, bm, gp, d, dm) =P (C) P (fA|C) P (fa|C, fA) P (d, dm|C, fA, fa)P (sr|C, d, dm) P (bm, gp|C) P (num|C, fA, fa)P (hm, hmm,mm|C, fA, fa, bm)P (fA) P (fa|fA) P (d, dm|fA, fa)P (sr|d, dm) P (bm, gp) P (num|fA, fa)P (hm, hmm,mm|fA, fa, bm)(1)2C ?The expression of reaper has been shown ... the gene encodes ...B ?Drosophila gene Bok interacts with ... expression of Bok protein promotes apoptosis ...?S ?...
ced-4 and ced-9 ... the genes ...??...
the mammalian anti-apoptotic protein Bcl-2 ...Bcl-2 family ...?Table 1: Examples of coreferent (C), associative biotype (B) and associative set-member (S) anaphoric relationsFeature Possible valuesfA Form of noun phrase of the anaphor A: ?pn?, ?defnp?, ?demnp?, ?indefnp?, ?quantnp?, or ?np?.fa Form of noun phrase of the antecedent candidate a: same values as for fA.hma,A Head-noun matching: ?yes?
if the anaphor?s and the candidate?s head nouns match, ?no?
otherwise.hmma,A Head-modifier matching: ?yes?
if the anaphor?s head noun matches any of the candidate?s pre-modifiers, or vice-versa, ?no?
otherwise.mma,A Modifier matching: ?yes?
if anaphor and candidate have at least one head modifier in common, ?no?otherwise.numa,A Number agreement: ?yes?
if anaphor and candidate agree in number, ?no?
otherwise.sra,A Syntactic relation between anaphor and candidate: ?none?, ?apposition?, ?subj-obj?, ?pp?, and fewothers.bma,A Biotype matching: ?yes?
if anaphor?s and candidate?s biotype (semantic class) match, ?no?
otherwise.gpa,A is biotype gene or product?
?yes?
if the anaphor biotype or candidate biotype is gene or product, ?no?otherwise.
This feature is mainly to distinguish which pairs can hold biotype relations.da,A Distance in sentences between the anaphor and the candidate.dma,A Distance in number of entities (markables) between the anaphor and the candidate.Table 2: Feature setThis equation is the basis of our resolution model.We collect the statistics to train this model from acorpus annotated with anaphoric relations that wehave created.
The corpus is described in the nextsection.3 Our corpusThere are very few biomedical corpora annotatedwith anaphora information, and all of them are builtfrom paper abstracts (Cohen et al, 2005), instead offull papers.
As anaphora is a phenomenon that de-velops through a text, we believe that short abstractsare not the best source to work with and decided toconcentrate on full papers.In order to collect the statistics to train our model,we have manually annotated anaphoric relationsbetween biomedical entities in 5 full-text articles(approx.
33,300 words)1, which are part of theDrosophila molecular biology literature.
The corpusand annotation process are described in (Gasperin etal., 2007).
To the best of our knowledge, this corpus1Corpus available via the FlySlip project websitehttp://www.wiki.cl.cam.ac.uk/rowiki/NaturalLanguage/FlySlipis the first corpus of biomedical full-text articles tobe annotated with anaphora information.Before annotating anaphora, we have prepro-cessed the articles in order to (1) tag gene names,(2) identify all NPs, and (3) classify the NPs accord-ing to their domain type, which we call biotype.
Totag all gene names in the corpus, we have appliedthe gene name recogniser developed by Vlachos etal.
(2006).
To identify all NPs, their subconstituents(head, modifiers, determiner) and broader pre- andpost-modification patterns, we have used the RASPparser (Briscoe et al, 2006).
To classify the NPs ac-cording to their type in biomedical terms, we haveadopted the Sequence Ontology (SO)2 (Eilbeck andLewis, 2004).
SO is a fine-grained ontology, whichcontains the names of practically all entities that par-ticipate in genomic sequences, besides the relationsamong these entities (e.g.
is-a, part-of, derived-fromrelations).
We derived from SO seven biotypes tobe used to classify the entities in the text, namely:?gene?, ?gene product?, ?part of gene?, ?part ofproduct?, ?gene variant?, ?gene subtype?, and ?gene2http://www.sequenceontology.org/3Class Relationscoreferent 1678biotype 274set-member 543discourse new 436Total 3048none 873,731Table 3: Training instances, according to anaphoric classsupertype?.
We also created the biotype ?other-bio?to be associated with noun phrases that contain agene name (identified by the gene name recogniser)but whose head noun does not fit any of the otherbiotypes.
All NPs were tagged with their biotypes,and NPs for which no biotypes were found were ex-cluded.The gene-name tags, NP boundaries and biotypesresulting from the preprocessing phase were revisedand corrected by hand before the anaphoric relationswere annotated.For each biotyped NP we annotated its closestcoreferent antecedent (if found) and its closest as-sociative antecedent (if found), from one of the as-sociative classes.
From our annotation, we can infercoreference chains by merging the coreferent linksbetween mentions of a same entity.The annotated relations, and the features derivedfrom them, are used as training data for the proba-bilistic model above.
A special characteristic of dataannotated with anaphora information is the over-whelming amount of negative instances, which re-sult from the absence of an anaphoric relation be-tween a NP that precedes an anaphoric expressionand was not marked as its antecedent (nor marked aspart of the same coreference chain of its antecedent).The negative instances outnumber considerably thenumber of positive instances (annotated cases).
Ta-ble 3 presents the distribution of the cases among theclasses of anaphoric relations.To balance the ratio between positive and nega-tive training samples, we have clustered the negativesamples and kept only a portion of each cluster,proportional to its size.
All negative samples thathave the same values for all features are groupedtogether (consequently, a cluster is formed by a setof identical samples) and only one-tenth of eachcluster members is kept, resulting in 85,314 negativesamples.
This way, small clusters (with less than10 members), which are likely to represent noisysamples (similar to positive ones), are eliminated,and bigger clusters are shrunk; however the shape ofthe distribution of the negative samples is preserved.For example, our biggest cluster (feature valuesare: fA=?pn?, fa=?pn?, hm=?no?, hmm=?no?,mm=?no?, bm=?yes?, gp=?yes?, num=?yes?,sr=?none?, d=?16<?, dm=?50<?)
with 33,998instances is reduced to 3,399 ?
still considerablymore numerous than any positive sample.
Otherworks have used a different strategy to reduce theimbalance between positive and negative samples(Soon et al, 2001; Ng and Cardie, 2002; Strubeet al, 2002), where only samples composed by anegative antecedent that is closer than the annotatedone are considered.
Our strategy is more flexibleand is able to the reduce further the number of neg-ative samples.
The higher the number of negativesamples, the higher the precision of the resolution,but the lower the recall.4 Active learningWhen trained using all our annotated corpus on a 10-fold cross-validation setting our anaphora resolutionmodel, presented above, reached the results shownin Table 43.We would like to improve this results without hav-ing to annotate too much more data, therefore wedecided to experiment with active learning.
We de-fined three entropy-based measures to calculate theuncertanty of our model for each decidion is makes.3?Perfect?
scores shows the result of a strict evaluation,where we consider as correct all pairs that match exactly anantecedent-anaphor pair in the annotated data.
On the otherhand, column ?Relaxed?
treats as correct also the pairs wherethe assigned antecedent is not the exact match in the annotateddata but is coreferent with it.Perfect RelaxedClass P R F P R Fcoreferent 56.3 54.7 55.5 69.4 67.4 68.3biotype 28.5 35.0 31.4 31.2 37.9 34.2set-member 35.4 38.2 36.7 38.5 41.5 40.0discourse new 44.3 53.4 48.4 44.3 53.4 48.4Table 4: Performance of the probabilistic model44.1 Uncertainty measuresIn order to measure how confident our model isabout the class it assigns to each candidate, and con-sequently the one it chooses as the antecedent of ananaphor, we experiment with the following entropy-based measures.We first compute what we call the ?local en-tropy?
among the probabilities for each class?P(C=?coreferent?
), P(C=?biotype?
), P(C=?set-member?)
and P(C=?none?
)?for a given pairanaphor(A)-candidate(a), which is defined asLE(A, a) = ?
?CP (C)log2P (C) (2)where P (C) represents Equation 1 above, that is, theprobability assigned to the anaphor-candidate rela-tion by our probabilistic model for a particular class.The more similar the probabilities are, the more un-certain the model is about the relation, so the higherthe local entropy.
This measure is similar to othersused in previous work for different problems.We also compute the ?global entropy?
of thedistribution of candidates across classes for eachanaphor.
The global entropy aims to combine theuncertainty information from all antecedent candi-dates for a given anaphor (instead of consideringonly a single candidate-anaphor pair as for LE).
Thehigher the global entropy, the higher the uncertaintyof the model about the antecedent for an anaphor.The global entropy combines the local entropies forall antecedent candidates of a given anaphor.
Wepropose two versions of the global entropy measure.The first is simply a sum of the local entropies of allcandidates available for a given anaphor, it is definedasGE1(A) =?aLE(A, a) (3)The second version averages the local entropiesacross all candidates, it is defined asGE2(A) =?a LE(A, a)|a| (4)where |a| corresponds to the number of candidatesavailable for a given anaphor.We consider that in general the further away acandidate is from the anaphor, the lower the localentropy of the pair is (given that when distance in-creases, the probability of the candidate not beingthe antecedent, P(C=?none?
), also increases), andconsequently the less it contributes to the global en-tropy.
This is the intuition behind GE1(A).However, in some cases, mainly when theanaphor is a proper name, there can be several can-didates at a long distance from the anaphor that stillget a reasonable probability assigned to them dueto positive string matching.
Therefore we decided toexperiment with averaging the sum of the local prob-abilities by the number of candidates, so GE2(A).5 ExperimentsInitially, our training data was divided in 10-foldsfor cross-validation evaluation of our probabilis-tic model for anaphora resolution.
For the activelearning experiments we kept the same folds, us-ing one for the initial training, eight for the ac-tive learning phase, and the remaining one for test-ing.
We have experimented with 10 different initial-training/active-learning/testing splits, selected ran-domly from all combinations of the 10 folds, andthe results in this section correspond to the averageof the results from the different data splits.
A foldcontains the positive and negative samples derivedfrom about 270 anaphors, it contains about 7000candidate-anaphor pairs (an average of about 26 an-tecedent candidates per anaphor).
The anaphors thatare part of each fold were randomly selected.The purpose of our experiments is to checkwhether the samples selected by using the entropy-based measures described above, when added to ourtraining data, can improve the performance of themodel more than in the case of adding the sameamount of randomly selected samples.
For that,we computed (1) the performance of our model us-ing one fold of training data, (2) the performanceof the model over 10 iterations of active learningusing each of the uncertainty measures above, and(3) the performance of the model over 10 iterationsadding the same amount of randomly selected in-stances as for active learning.
At each active learn-ing iteration, when using LE(A, a) we selectedthe 1500 candidate-anaphor pairs for which uncer-tainty was the highest, and when using GE1(A) andGE2(A) we selected the 50 anaphors for which the5model was most uncertain and generated the posi-tive and negative instances that were associated tothe anaphors.We expected (2), entropy-based sample selection,to achieve better performance than (3), random sam-ple selection, however this has not happened.
Thegraphs in Figure 1 compare the precision, recall andF-measure scores for (2) and (3) along the 10 it-erations for each class of anaphoric relation.
Thelines corresponding to random sampling plot the re-sults of the experiments done in the same way as forGE1(A) and GE2(A), that is, where 50 anaphorsare selected at each iteration, although we also testedrandom sampling in the LE(A, a) fashion, selecting1500 candidate-anaphor pairs.We observe that none of the uncertainty measuresthat we tested have performed consistently betterthan random sampling.
LE(A, a) presents the mostdramatic results, it worsens the general performanceof the model for all classes, although it causes aconsiderable increase in precision for coreferent andset-member cases.
GE1(A) and GE2(A) have aless clear pattern, but it is possible to notice thatGE1(A) tends to bring improvements in precisionwhile GE2(A) causes the opposite, improvementsin recall and drops in precision.6 DiscussionWhen looking at the instances selected by each ac-tive learning strategy, we observe the following.LE(A, a), which considers anaphor-candidate pairs,selects mostly negative instances, given the fact thatthese are highly frequent.
This can explain the in-crease in precision and drop in recall for the posi-tive cases (observed for coreferent and set-member,the most frequent positive classes), since that is ex-pected with the increase of negative instances.GE1(A) and GE2(A) select a proportional num-ber of positive and negative instances, since thesemeasures consider an anaphor and all possible an-tecedent candidates, generating all instances that de-rive from each selected anaphor (usually one or twopositive intances and several negative ones).
How-ever, we can observe some differences between theimpact of using GE1(A) and GE2(A) to select in-stances.
We observe that about 70% of the sam-ples selected by GE1(A) were proper names, whilethe distribution of NP types among the samples se-lected by GE2(A) is similar to the original distri-bution in the data.
This confirms the problem weexpected to have with GE1(A), since exact matchesof proper names that occur at a considerable distancefrom the anaphor still get a higher probability as-signed to them, which does not happen so often withother types of NPs.
On the other hand, the correctantecedent of about 30% of GE2(A)-selected sam-ples were in the same sentence as the anaphor, whilethe same occurs with only 8% of GE1(A)-selectedsamples.
GE2(A) behaviour in this case is counterintuitive, since antecedents in the same sentenceshould be found by the model with lower uncertaintythan antecedents further away from the anaphor.
An-other counter intuitive behaviour of GE2(A) is thatonly 3% of the selected anaphors have no stringmatching with their antecedents (33% have no head-noun matching), while these cases correspond toabout 30% of samples selected by GE1(A) (62%of samples have no head-noun matching).
We ex-pected samples involving no string matching to beselected because they are usually the ones the modelis mostly uncertain about.Despite the different behaviour among the mea-sures none was successful in improving the perfor-mance of the model in relation to the performanceof random sampling.While entropy-based measures for sample selec-tion seem the obvious option given that we use aprobabilistic model, they did not give positive re-sults in our case.
A future study of different ways tocombine the local entropies is necessary, as well asthe study of other non-entropy-based measures forsample selection.The main difference between our application ofactive learning to anaphora resolution and previoussuccessful applications of active learning to othertasks is the amount of probabilities involved in thecalculation of the uncertainty of the model.
We be-lieve this is the reason why our active learning ex-periments were not succesfull.
While, for example,name entity recognition involves a binary decision,and parse selection involves a few parsing options,in our case there are several antecedent candidatesto be considered.
For anaphora resolution, when us-ing a pairwise resolution model, it is necessary tocombine the predictions for one candidate-anaphor600.10.20.30.40.50.60.70.8109876543210PrecisionIterationsBiotypeGE2GE1RandLE00.10.20.30.40.50.60.70.8109876543210PrecisionIterationsCoreferentGE2GE1RandLE00.10.20.30.40.50.60.70.8109876543210PrecisionIterationsSet-memberGE2GE1RandLE(a) Precision00.10.20.30.40.50.60.70.8109876543210RecallIterationsBiotypeGE2GE1RandLE00.10.20.30.40.50.60.70.8109876543210RecallIterationsCoreferentGE2GE1RandLE00.10.20.30.40.50.60.70.8109876543210RecallIterationsSet-memberGE2GE1RandLE(b) Recall00.10.20.30.40.50.60.70.8109876543210F-MeasureIterationsBiotypeGE2GE1RandLE00.10.20.30.40.50.60.70.8109876543210F-MeasureIterationsCoreferentGE2GE1RandLE00.10.20.30.40.50.60.70.8109876543210F-MeasureIterationsSet-memberGE2GE1RandLE(c) F-measureFigure 1: Graphs of the performance of active learning using LE(A, a), GE1(A), GE2(A) and random sampling.7pair to the others in order to predict the global un-certainty of the model.AcknowledgmentsThis work was part of the BBSRC-funded FlySlipproject.
Caroline Gasperin was funded by a CAPESaward from the Brazilian government.
Thanks toTed Briscoe for comments on this manuscript.ReferencesJason Baldridge and Miles Osborne.
2003.
Active learn-ing for hpsg parse selection.
In Walter Daelemans andMiles Osborne, editors, Proceedings of CoNLL-2003,pages 17?24.
Edmonton, Canada.Edward J. Briscoe, John Carroll, and Rebecca Watson.2006.
The second release of the RASP system.
InProceedings of ACL-COLING 06, Sydney, Australia.K.
Bretonnel Cohen, Lynne Fox, Philip Ogren, andLawrence Hunter.
2005.
Corpus design for biomed-ical natural language processsing.
In Proceedings ofthe ACL-ISMB Workshop on Linking Biological Liter-ature, Ontologies and Databases, Detroit.Karen Eilbeck and Suzanna E. Lewis.
2004.
Sequenceontology annotation guide.
Comparative and Func-tional Genomics, 5:642?647.Caroline Gasperin and Ted Briscoe.
2008.
Statisticalanaphora resolution in biomedical texts.
In Proceed-ings of COLING 2008, Manchester, UK.Caroline Gasperin, Nikiforos Karamanis, and Ruth Seal.2007.
Annotation of anaphoric relations in biomedi-cal full-text articles using a domain-relevant scheme.In Proceedings of DAARC 2007, pages 19?24, Lagos,Portugal.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora -COLING-ACL?98, Montreal, Canada.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of ACL 2002, Philadelphia.Massimo Poesio and Renata Vieira.
1998.
A corpus-based investigation of definite description use.
Com-putational Linguistics, 24(2):183?216.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-criteria-based active learningfor named entity recognition.
In Proceedings of ACL2004, Barcelona.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Michael Strube, Stefan Rapp, and ChristophMller.
2002.The influence of minimum edit distance on refer-ence resolution.
In Proceedings of the EMNLP 2002,Philadelphia.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.Active learning for statistical natural language pars-ing.
In Proceedings of ACL 2002, pages 120?127,Philadelphia, Pennsylvania.
Association for Computa-tional Linguistics.Andreas Vlachos and Caroline Gasperin.
2006.
Boot-strapping and evaluating named entity recognition inthe biomedical domain.
In Proceedings of BioNLP atHLT-NAACL 2006, pages 138?145, New York.8
