Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 303?313,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSign constraints on feature weights improve a joint model of wordsegmentation and phonologyMark JohnsonMacquarie UniversitySydney, AustraliaMark Johnson@MQ.edu.auJoe PaterUniversity of Massachusetts, AmherstAmherst, MA, USApater@linguist.umass.eduRobert StaubsUniversity of Massachusetts, AmherstAmherst, MA, USArstaubs@linguist.umass.eduEmmanuel Dupoux?Ecole des Hautes Etudesen Sciences Sociales, ENS, CNRS,Paris, Franceemmanuel.dupoux@gmail.comAbstractThis paper describes a joint model of wordsegmentation and phonological alternations,which takes unsegmented utterances as inputand infers word segmentations and underlyingphonological representations.
The model is aMaximum Entropy or log-linear model, whichcan express a probabilistic version of Opti-mality Theory (OT; Prince and Smolensky(2004)), a standard phonological framework.The features in our model are inspired by OT?sMarkedness and Faithfulness constraints.
Fol-lowing the OT principle that such features in-dicate ?violations?, we require their weightsto be non-positive.
We apply our model to amodified version of the Buckeye corpus (Pittet al, 2007) in which the only phonologicalalternations are deletions of word-final /d/ and/t/ segments.
The model sets a new state-of-the-art for this corpus for word segmentation,identification of underlying forms, and identi-fication of /d/ and /t/ deletions.
We also showthat the OT-inspired sign constraints on fea-ture weights are crucial for accurate identifi-cation of deleted /d/s; without them our modelposits approximately 10 times more deletedunderlying /d/s than appear in the manuallyannotated data.1 IntroductionThis paper unifies two different strands of researchon word segmentation and phonological rule induc-tion.
The word segmentation task is the task ofsegmenting utterances represented as sequences ofphones into sequences of words.
This is an ideal-isation of the lexicon induction problem, since theresulting words are phonological forms for lexicalentries.In its simplest form, the data for a word segmen-tation task is obtained by looking up the words ofan orthographic transcript (of, say, child-directedspeech) in a pronouncing dictionary and concate-nating the results.
However, this formulation sig-nificantly oversimplifies the problem because it as-sumes that each token of a word type is pronouncedidentically in the form specified by the pronouncingdictionary (usually its citation form).
In reality thereis usually a significant amount of pronunciation vari-ation from token to token.The Buckeye corpus, on which we base our exper-iments here, contains manually-annotated surfacephonetic representations of each word as well as thecorresponding underlying form (Pitt et al, 2007).For example, a token of the word ?lived?
has theunderlying form /l.ih.v.d/ and could have the sur-face form [l.ah.v] (we follow standard phonologicalconvention by writing underlying forms with slashesand surface forms with square brackets, and use theBuckeye transcription format).There is a large body of work in the phonolog-ical literature on inferring phonological rules map-ping underlying forms to their surface realisations.While most of this work assumes that the underly-ing forms are available to the inference procedure,there is work that induces underlying forms as wellas the phonological processes that map them to sur-303face forms (Eisenstat, 2009; Pater et al, 2012).We present a model that takes a corpus of unseg-mented surface representations of sentences and in-fers a word segmentation and underlying forms foreach hypothesised word.
We test this model on dataderived from the Buckeye corpus where the onlyphonological variation consists of word-final /d/ and/t/ deletions, and show that it outperforms a state-of-the-art model that only handles word-final /t/ dele-tions.Our model is a MaxEnt or log-linear model,which means that it is formally equivalent to a Har-monic Grammar, which is a continuous version ofOptimality Theory (OT) (Smolensky and Legen-dre, 2005).
We use features inspired by OT, andshow that sign constraints on feature weights re-sult in models that recover underlying /d/s signif-icantly more accurately than models that don?t in-clude such contraints.
We present results suggest-ing that these constraints simplify the search prob-lem that the learner faces.The rest of this paper is structured as follows.The next section describes related work, includingprevious work that this paper builds on.
Section 3describes our model, while section 4 explains howwe prepared the data, presents our experimental re-sults and investigates the effects of design choices onmodel performance.
Section 5 concludes the paperand discusses possible future directions.2 Background and related workThe word segmentation task is the task of segment-ing utterances represented as sequences of phonesinto sequences of words.
Elman (1990) introducedthe word segmentation task as a simplified form oflexical acquisition, and Brent and Cartwright (1996)and Brent (1999) introduced the unigram model ofword segmentation, which forms the basis of themodel used here.
Goldwater et al (2009) describeda non-parametric Bayesian model of word segmen-tation, and highlighted the importance of contex-tual dependencies.
Johnson (2008) and Johnsonand Goldwater (2009) showed that word segmen-tation accuracy improves when phonotactic con-straints on word shapes are incorporated into themodel.
That model has been extended to also exploitstress cues (B?orschinger and Johnson, 2014), the?topics?
present in the non-linguistic context (John-son et al, 2010) and the special properties of func-tion words (Johnson et al, 2014).Liang and Klein (2009) proposed a simple un-igram model of word segmentation much like theoriginal Brent unigram model, and introduced a?word length penalty?
to avoid under-segmentationthat we also use here.
(As Liang et alnote, withoutthis the maximum likelihood solution is not to seg-ment utterances at all, but to analyse each utteranceas a single word).
Berg-Kirkpatrick et al (2010) ex-tended this model by defining the unigram distribu-tion with a MaxEnt model.
The MaxEnt featurescan capture phonotactic generalisations about possi-ble word shapes, and their model achieves a state-of-the-art word segmentation f-score.The phonological learning task is to learn thephonological mapping from underlying forms to sur-face forms.
Johnson (1984) and Johnson (1992)describe a search procedure for identifying under-lying forms and the phonological rules that mapthem to surface forms given surface forms organ-ised into inflectional paradigms.
Goldwater andJohnson (2003) and Goldwater and Johnson (2004)showed how Harmonic Grammar phonological con-straint weights (Smolensky and Legendre, 2005) canbe learnt using a Maximum Entropy parameter esti-mation procedure given data consisting of underly-ing and surface word form pairs.
There is now asignificant body of work using Maximum Entropytechniques to learn phonological constraint weights(see esp.
Hayes and Wilson (2008), as well as thereview in Coetzee and Pater (2011)).Recently there has been work attempting to inte-grate these two approaches.
The word segmentationwork generally ignores pronunciation variation byassuming that the input to the learner consists of se-quences of citation forms of words, which is highlyunrealistic.
The phonology learning work has gen-erally assumed that the learner has access to the un-derlying forms of words, which is also unrealistic.In the word segmentation area, Elsner et al (2012)and Elsner et al (2013) generalise the Goldwater bi-gram model by assuming that the bigram model gen-erates underlying forms, which a finite state trans-ducer maps to surface forms.
While this is an ex-tremely general model, inference in such a modelis very challenging, and they restrict attention to304transducers where the underlying to surface map-ping consists of simple substitutions, so their modelcannot handle the deletion phenomena studied here.B?orschinger et al (2013) also generalise the Gold-water bigram model by including an underlying-to-surface mapping, but their mapping only allowsword-final underlying /t/ to be deleted, which en-ables them to use a straight-forward generalisationof Goldwater?s Gibbs sampling inference procedure.In phonology, Eisenstat (2009) and Pater et al(2012) showed how to generalise a MaxEnt modelso it also learns underlying forms as well as MaxEntphonological constraint weights given surface formsin paradigm format.
The vast sociolinguistic liter-ature on /t/-/d/-deletion is surveyed in Coetzee andPater (2011), together with prior OT and MaxEntanalyses of the phenomena.2.1 The Berg-Kirkpatrick et al modelThis section contains a more technical descriptionof the Berg-Kirkpatrick et al (2010) MaxEnt uni-gram model of word segmentation, which our modeldirectly builds on.
Our model integrates the Max-Ent unigram word segmentation model of Berg-Kirkpatrick et al with the MaxEnt phonology mod-els developed by Goldwater and Johnson (2003)and Goldwater and Johnson (2004).
Because bothkinds of models are MaxEnt models, this integra-tion is fairly easy, and the inference procedure re-quires optimisation of a fairly straight-forward ob-jective function.
We use a customised version ofthe OWLQN-LBFGS procedure (Andrew and Gao,2007) that allows us to impose sign constraints onindividual feature weights.As is standard in the word-segmentation liter-ature, the model?s input is a sequence of utter-ances D = (w1, .
.
.
, wn), where each utterancewi= (wi,1, .
.
.
, wi,mi) is a sequence of (surface)phones.
The Berg-Kirkpatrick et almodel is a uni-gram model, so it defines a probability distributionover possible words s, where s is also a sequence ofphones.
The probability of an utterance w is the sumof the probability of all word sequences that gener-ate it:P(w | ?)
=?s1...s`s.t.s1...s`=w`?j=1P(sj| ?
)Berg-Kirkpatrick et als model of word probabili-ties P(s | ?)
is a MaxEnt model with parameters ?,where the features f(s) of surface form s are chosento encourage the model to generalise appropriatelyover word shapes.
While they don?t describe theirfeatures in complete detail, they include features foreach word s, features for the prefix and suffix of sand features for the CV skeleton of the prefix andsuffix of s.In more detail, P(s | ?)
is a MaxEnt model asfollows:P(s | ?)
=1Zexp(?
?
f(s)), where:Z =?s??Sexp(?
?
f(s?
))The set of possible surface word forms S is theset of substrings (i.e., sequences of phones) occuringin the training data D that are shorter than a user-specified length bound.
We follow Berg-Kirkpatrickin imposing a length bound on possible words; forthe Brent corpus the maximum word length is 10phones, while for the Buckeye corpus the maximumword length is 15 phones (reflecting the fact thatwords are longer in this adult-directed corpus).While restricting the set of possible word formsS to the substrings appearing in D is reasonablefor a simple multinomial model like the one inLiang and Klein (2009), it?s interesting that this pro-duces good results with a MaxEnt model like Berg-Kirkpatrick et als, since one might expect such amodel would have to learn generalisations about im-possible word shapes in order to perform well.
Be-cause S only contains a small fraction of the possi-ble phone strings, one might worry that the modelwould not see enough ?impossible words?
to learnto distinguish possible words from impossible ones,but the model?s good performance suggests this isnot the case.11The non-parametric Bayesian approach of Goldwater et al(2009) and Johnson (2008) can be viewed as setting S to the setof all possible phone strings (i.e., a possible word can be anystring of phones, whether or not it appears in D).
The successof Berg-Kirkpatrick et als approach suggests that these non-parametric methods might not be necessary here, i.e., the set ofsubstrings actually occuring in D is ?large enough?
to enablethe model to learn ?implicit negative evidence?
generalisationsabout impossible word shapes.305Berg-Kirkpatrick et alfollow Liang et alin us-ing maximum likelihood estimation to estimate theirmodel?s parameters (Berg-Kirkpatrick et alactuallyuse L2-regularised maximum likelihood estimates).As Liang et alnote, it?s easy to show that the maxi-mum likelihood segmentation leaves each utteranceunsegmented, i.e., each utterance is analysed as asingle word.
To avoid this, Berg-Kirkpatrick et alfollow Liang et alby multiplying the word probabil-ities by a word length penalty term.
Thus the likeli-hood LDthey actually maximise is as shown below:LD(?)
=n?i=1P(wi| ?
)P(w | ?)
=?s1...s`s.t.s1...s`=w`?j=1P(sj| ?)
exp(?|si|d)where d is a constant chosen to optimise segmenta-tion performance.
This means that the model is defi-cient, i.e.,?s?SP(s | ?)
< 1.
(Because our modeluses a word length penalty in the same way, it too isdeficient).As Figure 1 shows, performance is very sensitiveto the word length penalty parameter d: the bestword segmentation on the Brent corpus is obtainedwhen d ?
1.6, while the best segmentation on theBuckeye corpus is obtained when d ?
1.5.
As far aswe know there is no principled way to set d in an un-supervised fashion, so this sensitivity to d is perhapsthe greatest weakness of this kind of model.Even so, it?s interesting that a unigram modelwithout the kind of inter-word dependencies thatGoldwater et al (2009) argues for can do so well.It?s possible that the improvement that Goldwateret alfound with the bigram model is because mod-elling individual bigram dependencies splits the datain a way that reduces overlearning (B?orschinger etal., 2012).3 A MaxEnt unigram model of wordsegmentation and word-final /d/ and /t/deletionThis section explains how we extend the Berg-Kirkpatrick et al (2010) model to handle a set P ofphonological processes, where a phonological pro-cess p ?
P is a partial, non-deterministic function0.30.40.50.60.70.80.91.4 1.5 1.6 1.7Word length penaltySurface tokenf-scoreDataBrentBuckeyeFigure 1: Sensitivity of surface token f-score to wordlength penalty factor d for the Brent and Buckeye cor-pora on data with no /d/ or /t/ deletions.
Performance issensitive to the value of the word length penalty d, andthe optimal value of d depends on the corpus.mapping underlying forms to surface forms.
For ex-ample, word-final /t/ deletion is the function map-ping underlying underlying forms ending in /t/ tosurface forms lacking that final segment.Our model is also a unigram model, but it definesa distribution over pairs (s, u) of surface/underlyingform pairs, where s is a surface form and u is an un-derlying form.
Below we allow this distribution tocondition on phonological properties of the neigh-bouring surface forms.The set X of possible (s, u) surface/underlyingform pairs is defined as follows.
For each surfaceform s ?
S (the set of length-bounded phone sub-strings of the data D), (s, s) ?
X .
In addition, ifu ?
S and some phonological alternation p ?
Pmaps u to a surface form s ?
p(u) ?
S , then(s, u) ?
X .
That is, we require that potential under-lying forms appear as surface substrings somewherein the data D (which means this model cannot han-dle e.g., absolute neutralisation).In the experiments below, we let P be phono-logical processes that delete word-final /d/ and/t/ phonemes.
Given the Buckeye data, ([l.ih.v],/l.ih.v/), ([l.ih.v], /l.ih.v.d/) and ([l.ih.v], /l.ih.v.t/) areall members ofX (i.e., candidate (s, u) pairs), corre-sponding to ?live?, ?lived?
and the non-word ?livet?respectively, where the latter two surface forms aregenerated by final /d/ and /t/ deletion respectively.Word-final /d/ and /t/ deletion depends on var-ious aspects of the phonological context, such as306whether the following word begins with a conso-nant or a vowel.
Our model handles this depen-dency by learning a conditional model over sur-face/underlying form pairs (s, u) ?
X that dependson the phonological context c:P(s, u | c, ?)
=1Zcexp(?
?
f(s, u, c)), where:Zc=?(s,u)?Xexp(?
?
f(s, u, c))In our experiments below, the set of possible con-texts is C = {C,V,#}, encoding whether the fol-lowing word begins with a consonant, a vowel oris the end of the utterance respectively.
We leavefor future research the exploration of other sorts ofcontextual conditioning.
Note that the set X is thesame for all contexts c; we show below that restrict-ing attention to just those surface/underlying pairsappearing in the context c degrades the model?s per-formance.
In other words, the model benefits fromthe implicit negative evidence provided by underly-ing/surface pairs that do not occur in a given context.We define the probability of a surface form s ?
Sin a context c ?
C by marginalising out the underly-ing form:P(s | c, ?)
=?u:(s,u)?XP(s, u | c, ?
)We optimise a penalised log likelihood QD(?
),with the word length penalty term d applied to theunderlying form u.Q(s | c, ?)
=?u:(s,u)?XP(s, u | c, ?)
exp(?|u|d)Q(w | ?)
=?s1...s`s.t.s1...s`=w`?j=1Q(sj| c, ?)QD(?)
=n?i=1logQ(wi| ?)?
?
||?||1We are somewhat cavalier about the conditionalcontexts c here: in our model below the context cfor a word is determined by the following word, soone can view our model as a generative model thatgenerates the words in an utterance from right to left.Because our model is a MaxEnt model, we haveconsiderable freedom in the choice of features, andas Berg-Kirkpatrick et al (2010) emphasise, thechoice of features directly determines the kinds ofgeneralisations the model can learn.
The featuresf(s, u, c) of a surface form s, underlying form uand context c we use here are inspired by OT.
Wedescribe our features using an example where s =[l.ih.v], u = /l.ih.v.t/ and c = C (i.e., the word isfollowed by a consonant).Underlying form lexical features: A feature foreach underlying form u.
In our example, thefeature is <U l ih v t>.
These features en-able the model to learn language-specific lex-ical entries.
There are 4,803,734 underlyingform lexical features (one for each possiblesubstring in the training data).Surface markedness features: The length ofthe surface string (<#L 3>), the number ofvowels (<#V 1>) (this is a rough indicationof the number of syllables), the surface suf-fix (<Suffix v>), the surface prefix andsuffix CV shape (<CVPrefix CV> and<CVSuffix VC>), and suffix+contextCV shape (<CVContext _C> and<CVContext C _C>).
There are 108surface markedness features.Faithfulness features: A feature for each diver-gence between underlying and surface forms(in this case, <*F t>).
There are two faith-fulness features.We used L1regularisation here, rather than theL2regularisation used by Berg-Kirkpatrick et al(2010), in the hope that its sparsity-inducing ?fea-ture selection?
capabilities would enable it to ?learn?lexical entries for the language, as well as preciselywhich markedness features are required to accountfor the data.
However, we found that the choiceof L1versus L2regression makes little difference,and the model is insensitive to the value of the reg-ulariser constant ?
(we set to ?
= 1 in the experi-ments below).We developed a specially modified version of theLBFGS-OWLQN optimisation procedure for opti-mising L1-regularised loss functions (Andrew and307Gao, 2007) that allows us to constrain certain featureweights ?kto have a particular sign.
This is a naturalextension of the LBFGS-OWLQN procedure sinceit performs orthant-constrained line searches in anycase.
We describe experiments below where we re-quire the feature weights for the markedness andfaithfulness features to be non-positive, and wherethe underlying lexical form features are required tobe non-negative.
The requirement that the lexicalform features are positive, combined with the spar-sity induced by the L1regulariser, was intendedto force the model to learn an explicit lexicon en-coded by the underlying form features with positiveweights (although our results below suggest that itdid not in fact do this).The inspiration for the requirement that marked-ness and faithfulness features are non-positivecomes from OT, which claims that the presenceof such features can only reduce the ?harmony?,i.e., the well-formedness, of an (s, u) pair.
Ver-sions of Harmonic Grammar that aim to produce OT-like behavior with weighted constraints often boundweights at zero (see e.g.
Pater (2009)).
The resultsbelow are the first to show that these constraints mat-ter for word segmentation.4 Experimental resultsThis section describes the experiments we per-formed to evaluate the model just described.
Wefirst describe how we prepared the data on which themodel is trained and evaluated, and then we describethe performance of that model.
Finally we performan analysis of how the model?s performance variesas parameters of the model are changed.We ran this model on data extracted from theBuckeye corpus of conversational speech (Pitt et al,2007) which was modified so the only alternations itcontained are final /d/ and /t/ deletions.
The Buck-eye corpus gives a surface realisation and an un-derlying form for each word token, and followingB?orschinger et al (2013), we prepared the data asfollows.
We used the Buckeye underlying forms asour underlying forms.
Our surface forms were alsoidentical to the Buckeye underlying forms, exceptwhen the underlying form ends in either a /d/ or a/t/.
In this case, if the Buckeye surface form does notend in an allophonic variant of that segment, thenour surface form consists of the Buckeye underly-ing form with that final segment deleted.
Thus theonly phonological variation in our data are deletionsof word-final /d/ and /t/ appearing in the Buckeyecorpus, otherwise our surface forms are identical toBuckeye underlying forms.For example, consider a token whose Buckeyeunderlying form is /l.ih.v.d/ ?lived?.
If the Buck-eye surface form is [l.ah.v] then our surface formwould be [l.ih.v], while if the Buckeye surface formis [l.ah.v.d] then our surface form would be [l.ih.v.d].We now present some descriptive statistics onour data.
The data contains 48,796 sentences and890,597 segments.
The longest sentence has 187segments.
The ?gold?
data has the following prop-erties.
There are 236,996 word boundaries, 285,792word tokens, and 9,353 underlying word types.
Thelongest word has 17 segments.
Of the 41,186 /d/sand 73,392 /t/s in the underlying forms, 24,524 /d/sand 40,720 /t/s are word final, and of these 13,457/d/s and 11,727 /t/s are deleted (i.e., do not appearon the surface).Our model considers all possible substrings oflength 15 or less as a possible surface form of aword, yielding 4,803,734 possible word types and5,292,040 possible surface/underlying word typepairs.
Taking the 3 contexts derived from thefollowing word into account, there are 4,969,718possible word+context types.
When all possiblesurface/underlying pairs are considered in all pos-sible contexts there are 15,876,120 possible sur-face/underlying/context triples.Table 1 summarises the major experimental re-sults for this model, and compares them to the re-sults of B?orschinger et al (2013).
Note that theirmodel only recovers word-final /t/ deletions and wasrun on data without word-final /d/ deletions, so it issolving a simpler problem than the one studied here.Even so, our model achieves higher overall accura-cies.We also conducted experiments on several of thedesign choices in our model.
Figure 2 shows theeffect of the sign constraints on feature weights dis-cussed above.
This plot shows that the contraints onthe weights of markedness and faithfulness featuresseems essential for good word segmentation perfor-mance.
Interestingly, we found that the weight con-straints make very little difference if the data does308B?orschinger et al 2013 Our modelSurface token f-score 0.72 0.76 (0.01)Underlying type f-score ?
0.37 (0.02)Deleted /t/ f-score 0.56 0.58 (0.03)Deleted /d/ f-score ?
0.62 (0.19)Table 1: Results summary for our model compared to thatof the B?orschinger et al (2013) model.
Surface token f-score is the standard token f-score, while underlying typeor ?lexicon?
f-score measures the accuracy with whichthe underlying word types are recovered.
Deleted /t/ and/d/ f-scores measure the accuracy with which the modelrecovers segments that don?t appear in the surface.
Theseresults are averaged over 40 runs (standard deviations inparentheses) with the word length penalty d = 1.525 ap-plied to underlying forms; standard deviations are givenin parentheses.0.00.20.40.60.81.4 1.5 1.6 1.7Word length penaltySurface tokenf-score Signconstraintson weightsNoneOTLexicalOT+LexicalFigure 2: The effect of constraints on feature weights onsurface token f-score.
?OT?
indicates that the markednessand faithfulness features are required to be non-positive,while ?Lexical?
indicates that the underlying lexical fea-tures are required to be non-negative.05000100001500020000 40000Number of deleted /d/Number of deleted/t/ Signconstraintson weightsNoneOTLexicalOT+LexicalFigure 3: The effect of constraints feature weights on thenumber of deleted underlying /d/ and /t/ segments positedby the model (d = 1.525).
The red diamond indicates the13,457 deleted underlying /d/ and 11,727 deleted under-lying /t/ in the ?gold?
data.380000039000004000000410000042000004000 6000 8000 10000Number of non-zero feature weightsRegularised negative log-likelihoodSignconstraintson weightsNoneOTLexicalOT+LexicalFigure 4: The regularised log-likelihood as a function ofthe number of non-zero weights for different constraintson feature weights (d = 1.525).3092000040000600004000 6000 8000 10000Number of non-zero feature weightsNumber of underlying types Signconstraintson weightsNoneOTLexicalOT+LexicalFigure 5: The number of underlying types proposedby the model as a function of the number of non-zeroweights, for different constraints on feature weights (d =1.525).
There are 9,353 underlying types in the ?gold?data.0.00.20.40.61.4 1.5 1.6 1.7Word length penaltyDeleted segment f-scoreAll pairsin all contextsFALSETRUEFigure 6: F-score for deleted /d/ and /t/ recovery as afunction of word length penalty d and whether all sur-face/underlying pairs X are included in all contexts C(d = 1.525).not any /t/ or /d/ deletions (i.e., the case that Berg-Kirkpatrick et al (2010) studied).Investigating this further, we found that theweight constraints on the markedness and faithful-ness features has a dramatic effect on the recov-ery of underlying segments, particularly underlying/d/s.
Figure 3 shows that with these constraints themodel recovers approximately the correct numberof deleted underlying segments, while without thisconstraint the model posits far too many underlying/d/s.
Figure 4 shows that these constraints help themodel find higher regularised likelihood sets of fea-ture weights with fewer non-zero feature weights.We examined how the number of non-zero fea-ture weights (most of which are for underlying typefeatures) relate to the number of underlying typesposited by the model.
Figure 5 shows that theweight constraints on markedness and faithfulnessconstraints have great impact on the number of non-zero feature weights and on the number of underly-ing forms the model posits.
In all cases, the modelrecovers far more underlying forms than it finds non-zero weights.The lexicon weight constraints have much lessimpact than the OT weight constraints.
As Figure 3shows, without the OT weight constraints the mod-els posit too many deleted /d/ and essentially nodeleted /t/.
Figure 4 shows that OT weight con-straints enable the model to find higher likelihoodsolutions, i.e., the OT weight constraints help search.Inspired by a reviewer?s comments, we studied type-token ratios and the number of boundaries our mod-els posit.
We found that the models without OTweight constraints posit far too few word boundariescompared to the gold data, so the number of surfacetokens is too low, so the words are too long, and thenumber of underlying types is too high.
This is con-sistent with Figures 4?5.We also examined whether it is necessary to con-sider all surface/underlying pairs X in each contextC, or whether it is possible to restrict attention tothe much smaller sets Xcthat occur in each c ?
C(this dramatically reduces the amount of memory re-quired and speeds the computation).
Figure 6 showsthat working with the smaller, context-specific setsdramatically decreases the model?s ability to recoverdeleted segments.5 Conclusions and future workThe MaxEnt unigram model of word segmentationdeveloped by Berg-Kirkpatrick et al (2010) inte-grates straight-forwardly with the MaxEnt phonol-ogy models of Goldwater and Johnson (2003) to pro-duce a MaxEnt model that jointly models word seg-mentation and the mapping from underlying to sur-face forms.We tested our model on data derived from themanually-annotated Buckeye corpus of conversa-tional speech (Pitt et al, 2007) in which the onlyphonological alternations are deletions of word-final/d/ and /t/ segments.
We demonstrated that ourmodel improves on the state-of-the-art for word seg-310mentation, recovery of underlying forms and recov-ery of deleted segments for this corpus.Our model is a MaxEnt or log-linear un-igram model over the set of possible sur-face/underlying form pairs.
Inspired by the workof Berg-Kirkpatrick et al (2010), the set of sur-face/underlying form pairs our model calculates thepartition function over is restricted to those actuallyappearing in the training data, and doesn?t includeall logically possible pairs.
We found that even withthis restriction, the model produces good results.Because our model is a Maximum Entropy or log-linear model, it is formally an instance of a Har-monic Grammar (Smolensky and Legendre, 2005),so we investigated features inspired by OT, whichis a discretised version of Harmonic Grammar thathas been extensively developed in the linguistics lit-erature.
The features our model uses consist of un-derlying form features (one for each possible under-lying form), together with markedness and faithful-ness phonological features inspired by OT phono-logical analyses.
According to OT, these marked-ness and faithfulness features should always havenegative weights (i.e., when such a feature ?fires?, itshould always make the analysis less probable).
Wefound that constraining feature weights in this waydramatically improves the model?s accuracy, appar-ently helping to find higher likelihood solutions.Looking forwards, a major drawback of the Max-Ent approaches to word segmentation are theirsensitivity to the word length penalty parameter,which this model shares with the models of Berg-Kirkpatrick et al (2010) and (Liang and Klein,2009) on which it is based.
It would be very de-sirable to have a principled way to set this parameterin an unsupervised manner.Because our goal was to explore the MaxEnt ap-proach to joint segmenation and alternation, we de-liberately used a minimal feature set here.
As thereviewers pointed out, we did not include any mor-phological features, which could have a major im-pact on the model.
Investigating the impact of richerfeature sets, including a combination of phonotacticand morphological features, would be an excellenttopic for future work.It would be interesting to extend this approachto a wider range of phonological processes in ad-dition to the word-final /t/ and /d/ deletion studiedhere.
Because this model enumerates the possiblesurface/underlying/context triples before beginningto search for potential surface and underlying words,its memory requirements would grow dramaticallyif the set of possible surface/underlying alternationswere increased.
(The fact that we only consideredword final /d/ and /t/ deletions means that there areonly three possible underlying word forms for eachsurface word forms).
Perhaps there is a way of iden-tifying potential underlying forms that avoids enu-merating them.
For example, it might be possibleto sample possible underlying word forms duringthe learning process rather than enumerating themahead of time, perhaps by adapting non-parametricBayesian approaches (Goldwater et al, 2009; John-son and Goldwater, 2009; B?orschinger et al, 2013).AcknowledgmentsThis research was supported under the Aus-tralian Research Council?s Discovery Projects fund-ing scheme (project numbers DP110102506 andDP110102593), by the Mairie de Paris, the fon-dation Pierre Gilles de Gennes, the?Ecole desHautes Etudes en Sciences Sociales, the?Ecole Nor-male Sup?erieure, the Region Ile de France, by theUS National Science Foundation under Grant No.S121000000211 to the third author and Grant BCS-424077 to the University of Massachusetts, and bygrants from the European Research Council (ERC-2011-AdG-295810 BOOTPHON) and the AgenceNationale pour la Recherche (ANR-10-LABX-0087IEC, ANR-10-IDEX-0001-02 PSL*).
We?d also liketo thank the three anonymous reviewers for helpfulcomments and suggestions.ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of l1-regularized log-linear models.
In Proceed-ings of the 24th International Conference on MachineLearning, ICML ?07, pages 33?40, New York, NewYork.
ACM.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590.
Association forComputational Linguistics.311Benjamin B?orschinger and Mark Johnson.
2014.
Explor-ing the role of stress in Bayesian word segmentationusing adaptor grammars.
Transactions of the Associa-tion for Computational Linguistics, 2(1):93?104.Benjamin B?orschinger, Katherine Demuth, and MarkJohnson.
2012.
Studying the effect of input size forBayesian word segmentation on the Providence cor-pus.
In Proceedings of the 24th International Con-ference on Computational Linguistics (Coling 2012),pages 325?340, Mumbai, India.
Coling 2012 Organiz-ing Committee.Benjamin B?orschinger, Mark Johnson, and Katherine De-muth.
2013.
A joint model of word segmentationand phonological variation for English word-final /t/-deletion.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1508?1516, Sofia, Bul-garia.
Association for Computational Linguistics.M.
Brent and T. Cartwright.
1996.
Distributional reg-ularity and phonotactic constraints are useful for seg-mentation.
Cognition, 61:93?125.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.
Ma-chine Learning, 34:71?105.Andries Coetzee and Joe Pater.
2011.
The place of vari-ation in phonological theory.
In John Goldsmith, Ja-son Riggle, and Alan Yu, editors, The Handbook ofPhonological Theory, pages 401?431.
Blackwell, 2ndedition.Sarah Eisenstat.
2009.
Learning underlying forms withMaxEnt.
Master?s thesis, Brown University.Jeffrey Elman.
1990.
Finding structure in time.
Cogni-tive Science, 14:197?211.Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.2012.
Bootstrapping a unified model of lexical andphonetic acquisition.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics, pages 184?193, Jeju Island, Korea.
Asso-ciation for Computational Linguistics.Micha Elsner, Sharon Goldwater, Naomi Feldman, andFrank Wood.
2013.
A joint learning model of wordsegmentation, lexical acquisition, and phonetic vari-ability.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 42?54, Seattle, Washington, USA, October.
As-sociation for Computational Linguistics.Sharon Goldwater and Mark Johnson.
2003.
Learn-ing OT constraint rankings using a Maximum Entropymodel.
In J. Spenader, A. Eriksson, and Osten Dahl,editors, Proceedings of the Stockholm Workshop onVariation within Optimality Theory, pages 111?120,Stockholm.
Stockholm University.Sharon Goldwater and Mark Johnson.
2004.
Priorsin Bayesian learning of phonological rules.
In Pro-ceedings of the Seventh Meeting Meeting of the ACLSpecial Interest Group on Computational Phonology:SIGPHON 2004.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?54.Bruce Hayes and Colin Wilson.
2008.
A Maximum En-tropy model of phonotactics and phonotactic learning.Linguistic Inquiry, 39(3):379?440.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric Bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 317?325, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Mark Johnson, Katherine Demuth, Michael Frank, andBevan Jones.
2010.
Synergies in learning wordsand their referents.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors,Advances in Neural Information Processing Systems23, pages 1018?1026.Mark Johnson, Anne Christophe, Emmanuel Dupoux,and Katherine Demuth.
2014.
Modelling functionwords improves unsupervised word segmentation.
InProceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 282?292.Association for Computational Linguistics, June.Mark Johnson.
1984.
A discovery procedure for certainphonological rules.
In 10th International Conferenceon Computational Linguistics and 22nd Annual Meet-ing of the Association for Computational Linguistics.Mark Johnson.
1992.
Identifying a rule?s context fromdata.
In The Proceedings of the 11th West Coast Con-ference on Formal Linguistics, pages 289?297, Stan-ford, CA.
Stanford Linguistics Association.Mark Johnson.
2008.
Using Adaptor Grammars to iden-tify synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics, pages 398?406, Columbus, Ohio.
Association forComputational Linguistics.Percy Liang and Dan Klein.
2009.
Online EM for un-supervised models.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 611?619, Boulder,Colorado, June.
Association for Computational Lin-guistics.312Joe Pater, Robert Staubs, Karen Jesney, and Brian Smith.2012.
Learning probabilities over underlying repre-sentations.
In Proceedings of the Twelfth Meeting ofthe ACL-SIGMORPHON: Computational Research inPhonetics, Phonology, and Morphology, pages 62?71.Joe Pater.
2009.
Weighted constraints in generative lin-guistics.
Cognitive Science, 33:999?1035.Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-ling, William Raymond, Elizabeth Hume, and EricFosler-Lussier.
2007.
Buckeye corpus of conversa-tional speech.Alan Prince and Paul Smolensky.
2004.
Optimality The-ory: Constraint Interaction in Generative Grammar.Blackwell.Paul Smolensky and G?eraldine Legendre.
2005.
TheHarmonic Mind: From Neural Computation ToOptimality-Theoretic Grammar.
The MIT Press.313
