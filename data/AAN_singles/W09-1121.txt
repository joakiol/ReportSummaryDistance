Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 165?173,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsThe NVI Clustering Evaluation MeasureRoi ReichartICNCHebrew University of Jerusalemroiri@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractClustering is crucial for many NLP tasks andapplications.
However, evaluating the resultsof a clustering algorithm is hard.
In this paperwe focus on the evaluation setting in which agold standard solution is available.
We discusstwo existing information theory based mea-sures, V and VI, and show that they are bothhard to use when comparing the performanceof different algorithms and different datasets.The V measure favors solutions having a largenumber of clusters, while the range of scoresgiven by VI depends on the size of the dataset.We present a new measure, NVI, which nor-malizes VI to address the latter problem.
Wedemonstrate the superiority of NVI in a largeexperiment involving an important NLP appli-cation, grammar induction, using real corpusdata in English, German and Chinese.1 IntroductionClustering is a major technique in machine learn-ing and its application areas.
It lies at the heartof unsupervised learning, which has great potentialadvantages over supervised learning.
This is es-pecially true for NLP, due to the high efforts andcosts incurred by the human annotations required fortraining supervised algorithms.
Recent NLP prob-lems addressed by clustering include POS induction(Clark, 2003; Goldwater and Griffiths, 2007), wordsense disambiguation (Shin and Choi, 2004), seman-tic role labeling (Baldewein et al, 2004), pitch ac-cent type disambiguation (Levow, 2006) and gram-mar induction (Klein, 2005).Evaluation of clustering results is a challengingtask.
In this paper we address the external measuressetting, where a correct assignment of elements toclasses is available and is used for evaluating thequality of another assignment of the elements intoclusters.
Many NLP works have used external clus-tering evaluation measures (see Section 2).Recently, two measures have been proposed thatavoid many of the weaknesses of previous measuresand exhibit several attractive properties (see Sec-tions 2 and 3): the VI measure (Meila, 2007) andthe V measure (Rosenberg and Hirschberg, 2007).However, each of these has a serious drawback.
Thepossible values of VI lie in [0, 2log N ], where N isthe size of the clustered dataset.
Hence it has lim-ited use when comparing performance on differentdatasets.
V measure values lie in [0, 1] regardless ofthe dataset, but the measure strongly favors a cluster-ing having many small clusters.
In addition, V doesnot have many of the attractive properties of VI.This paper has two contributions.
First, we pro-pose the NVI measure, a normalization of VI whichguarantees that the score of clusterings that VI con-siders good lies in [0,1], regardless of dataset size.Most of VI?s attractive properties are retained byNVI.Second, we compare the behavior of V, VI andNVI in various situations to the desired behavior andto each other.
In particular, we show that V giveshigh scores to clusterings with a large number ofclusters even when they are of low quality.
Wedemonstrate this both in a synthetic example (Sec-tion 5) and in the evaluation (in three languages) ofa difficult NLP problem, labeled parse tree induc-165tion (Section 6).
We show that in both cases, NVIconstitutes a better clustering evaluation measure.2 Previous Evaluation MeasuresA large number of clustering quality measures havebeen proposed.
Here we briefly survey the threemain types, mapping based measures, counting pairsmeasures and information theory based measures.We first review some terminology (Meila, 2007;Rosenberg and Hirschberg, 2007).
In a homoge-neous clustering, every cluster contains only ele-ments from a single class.
In a complete cluster-ing, all elements of each class are assigned to thesame cluster.
The perfect solution is the fully ho-mogeneous and complete clustering.
We will illus-trate the behavior of some measures using three ex-treme cases: the single cluster case, in which alldata elements are put in the same single cluster; thesingletons case, in which each data element is putin a cluster of its own; and the no knowledge case,in which the class distribution within each clusteris identical to the class distribution in the entiredataset.
If the single cluster solution is not the per-fect one, the no knowledge solution is the worst pos-sible solution.
Throughout the paper, the number ofdata elements to be clustered is denoted by N.Mapping based measures are based on a post-processing step in which each cluster is mapped to aclass.
Among these are: L (Larsen, 1999), D (VanDongen, 2000), misclassification index (MI) (Zenget al, 2002), H (Meila, 2001), clustering F-measure(Fung et al, 2003) and micro-averaged precisionand recall (Dhillon et al, 2003).
As noted in (Rosen-berg and Hirschberg, 2007), these measures evalu-ate not only the quality of the proposed clusteringbut also of the mapping scheme.
Different mappingschemes can lead to different quality scores for thesame clustering.
Moreover, even when the mappingscheme is fixed, it can lead to not evaluating the en-tire membership of a cluster and not evaluating everycluster (Meila, 2007).Counting pairs measures are based on a com-binatorial approach which examines the number ofpairs of data elements that are clustered similarly inthe reference and proposed clustering.
Among theseare Rand Index (Rand, 1971), Adjusted Rand In-dex (Hubert and Arabie, 1985), ?
statistic (Hubertand Schultz, 1976), Jaccard (Milligan et al, 1983),Fowlkes-Mallows (Fowlkes and Mallows, 1983) andMirkin (Mirkin, 1996).Meila (2007) described a number of problemswith such measures.
The most acute one is that theirvalues are unbounded, making it hard to interprettheir results.
The problem can be solved by transfor-mations adjusting their values to lie in [0, 1], but theadjusted measures suffer from severe distributionalproblems, again limiting their usability in practice.Information-theoretic (IT) based measures arethose addressed in this work.
The measures in thisfamily suffer neither from the problems associatedwith mappings, since they evaluate the entire mem-bership of each cluster and not just a mapped por-tion, nor from the distributional problems of thecounting pairs measures.Zhao and Karypis (2001) define Purity and En-tropy as follows:Purity = ?kr=1 1Nmaxi(nir)Entropy = ?kr=1 nrN (?
1logq?qi=1nirnr log(nirnr ))where q is the number of classes, k the number ofclusters, nr cluster r?s size, and nir is the number ofelements in class i assigned to cluster r.Both measures are good measures for homogene-ity (Purity increases and Entropy decreases whenhomogeneity increases).
However, they do not eval-uate completeness at all.
The singletons solution isthus considered optimal even if in fact it is of verylow quality.Dom (2001) proposed the Q measure, the sum ofa homogeneity term H(C|K) and a model cost termcalculated using a coding theory argument:Q(C,K) = H(C|K) + 1N?|k|k=1 log(h(k)+|C|?1|C|?1)where C are the correct classes, K are the inducedclusters and h(k) is the number of elements in clus-ter k. Dom also presented a normalized version ofthe Q measure (called Q2) whose range is (0, 1] andgives higher scores to clusterings that are preferable.As noted by (Rosenberg and Hirschberg, 2007), theQ measure does not explicitly address the complete-ness of the suggested clustering.
Due to the costterm, if two clusterings have the same H(C|K)value, the model prefers the one with the lower num-ber of clusters, but the trade-off between homogene-ity and completeness is not explicitly addressed.In the next section we describe the V and VI mea-166sures, which are IT measures that explicitly assessboth the homogeneity and completeness of the clus-tering solution.BCubed (Bagga and Baldwin, 1998) is an attrac-tive measure that addresses both completeness andhomogeneity.
It does not explicitly use IT conceptsand avoids mapping.
In this paper we focus on Vand VI; a detailed comparison with BCubed is out ofour scope here and will be done in future work.Several recent NLP papers used clustering tech-niques and evaluation measures.
Examples include(Finkel and Manning, 2008), using VI, Rand in-dex and clustering F-score for evaluating corefer-ence resolution; (Headden et al, 2008), using VI, V,greedy 1-to-1 and many-to-1 mapping for evaluatingunsupervised POS induction; (Walker and Ringger,2008), using clustering F-score, the adjusted Randindex, V, VI and Q2 for document clustering; and(Reichart and Rappoport, 2008), using greedy 1-to-1 and many-to-1 mappings for evaluating labeledparse tree induction.Schulte im Walde (2003) used clustering to in-duce semantic verb classes and extensively dis-cussed non-IT based clustering evaluation measures.Pfitzner et al (2008) presented a comparison of clus-tering evaluation measures (IT based and others).While their analysis is extensive, their experimentswere confined to artificial data.
In this work, weexperiment with a complex NLP application usinglarge real datasets.3 The V and VI MeasuresThe V (Rosenberg and Hirschberg, 2007) and VI(Meila, 2007) measures are IT based measures.
Inthis section we give a detailed description of thesemeasures and analyze their properties.Notations.
The partition of the N data elementsinto classes is denoted by C = {c1, .
.
.
, c|C|}.The clustering solution is denoted by K ={k1, .
.
.
, k|K|}.
A = {aij} is a |C| ?
|K| contin-gency matrix such that aij is the number of data ele-ments that are members of class ci and are assignedby the algorithm to cluster kj .As other IT measures, V and VI assume that theelements in the dataset are taken from a known dis-tribution (both assume the uniform distribution), andthus the classes and clusters can be treated as ran-dom variables.
When assuming the uniform distri-bution, the probability of an event (a class or a clus-ter) is its relative size, so p(c) = ?|K|k=1 ackN andp(k) = ?|C|c=1 ackN .
Under this assumption we cantalk about the entropies H(C) and H(K) and theconditional entropies H(C|K) and H(K|C):H(C) = ?
?|C|c=1P|K|k=1 ackN logP|K|k=1 ackNH(K) = ?
?|K|k=1P|C|c=1 ackN logP|C|c=1 ackNH(C|K) = ?
?|K|k=1?|C|c=1ackN logackP|C|c=1 ackH(K|C) = ?
?|K|k=1?|C|c=1ackN logackP|K|k=1 ackIn Section 2 we defined the concepts of homo-geneity and completeness.
In order to satisfy the ho-mogeneity criterion, each cluster must be containedin a certain class.
This results in the minimizationof the conditional entropy of the classes given theclusters, H(C|K) = 0.
In the least homogeneoussolution, the conditional entropy is maximized, andH(C|K) = H(C).
Similarly, in order to satisfy thecompleteness criterion, each class must be containedin a certain cluster, which results in the minimiza-tion of the conditional entropy of the clusters giventhe classes, H(K|C) = 0.
In the least completesolution, the conditional entropy is maximized, andH(K|C) = H(K).The VI measure.
Variation of information (VI) isdefined as follows:V I(C,K) = H(C|K) + H(K|C).In the least homogeneous (complete) clustering, thevalues of H(C|K) (H(K|C)) are maximal.
Asa clustering solution becomes more homogeneous(complete), the values of H(C|K) (H(K|C)) de-crease to zero.
Consequently, lower VI values im-ply better clustering solutions.
In the perfect so-lution, both H(C|K) = 0 and H(K|C) = 0 andthus V I = 0.
For the least homogeneous and com-plete clustering solution, where knowing the clustertells nothing about the class and vise versa, V I =H(C) + H(K).As a result, the range of values that VI takes isdataset dependent, and the numbers themselves tell167us nothing about the quality of the clustering solu-tion (apart from a score of 0, which is given to thebest possible solution).A bound for VI values is a function of the maxi-mum number of clusters in C or K, denoted by k?.This is obtained when each cluster contains a sin-gle element, and k?
= N .
Thus, V I ?
[0, 2logN ].Consequently, the range of VI values is dataset de-pendent and unbounded when datasets change.
Thismeans that it is hard to use VI to compare the perfor-mance of a clustering algorithm across datasets.An apparent simple solution to this problemwould be to normalize VI by 2logk?
or 2logN , sothat its values would lie in [0, 1].
We discuss this atthe end of the next section.VI has two useful properties.
First, it satis-fies the metric axioms, that is: V I(C,K) ?0, V I(C,K) = V I(K,C), V I(C1, C2) +V I(C2, C3) ?
V I(C1, C3).
This gives an intuitiveunderstanding of the relation between VI values.Second, it is convexly additive.
Thismeans that if K is obtained from C bysplitting Cj into clusters K1j , .
.
.
,Kmj ,H?
(Kj) = ?
?mi=1 P (Kij |Cj)logP (Kij |Cj),then V I(C,K) = P (Cj)H?(Kj).
This propertyguarantees that all changes to VI are local; theimpact of splitting or merging clusters is limitedonly to those clusters involved, and its size isrelative to the size of these clusters.The V measure.
The V measure uses homogeneity(h) and completeness (c) terms as follows:h ={1 H(C) = 01?
H(C|K)H(C) H(C) 6= 0c ={1 H(K) = 01?
H(K|C)H(K) H(K) 6= 0V = 2hch + cIn the least homogeneous clustering, H(C|K) ismaximal, at H(C|K) = H(C).
In this case hreaches its minimum value, which is 0.
As homo-geneity increases H(C|K) values decrease.
For themost homogeneous clustering, H(C|K) = 0 andh = 1.
The same considerations hold for c, whichranges between 0 (for the least complete clustering)and 1 (for a complete clustering).
Since V is de-fined to be the harmonic mean of h and c, V valueslie in [0, 1].
Consequently, it can be used to com-pare the performance of clustering algorithms acrossdatasets.
Higher V values imply better clusterings.Unlike VI, V does not satisfy the metric axiomsand is not convexly additive.
The range of values itcan get does not depend on dataset size.Extreme cases for the two measures.
In thesingle cluster solution H(C|K) = H(C) andH(K|C) = 0, and thus V = 0 (the worst possi-ble score) and V I = H(C).
If there is indeed onlya single class, then V I = 0, the best possible score,which is the correct behavior.
VI behaves better thanV here.The singletons solution is a fully homogeneousclustering in which H(C|K) = 0.
The score of eachmeasure depends on the completeness of the solu-tion.
The completeness of a singletons clustering in-creases with the number of classes.
In the extremecase where every element is assigned to a uniqueclass (|C| = |K| = N ) singletons is also complete,H(K|C) = 0, and V (C,K) = 1, V I(C,K) = 0.Both measures exhibit the correct behavior.If there are classes that contain many elements,singletons is far from being complete and should betreated as a low quality solution.
Again, in the sin-gletons solution V I = H(K|C).
Suppose that thenumber of clusters is fixed.
When the number ofclasses increases, this value decreases, which is whatwe want.
When the number of classes decreases, thescore increases, which is again the correct behav-ior.
In Section 5 we show that this desired behaviorshown by VI is not shown by V.Both measures treat the no knowledge solution asthe worst one possible: V = 0, and V I = H(C) +H(K).4 Normalized Variation of InformationIn this section we define NVI, a normalization ofVI.
NVI is N -independent and its values for clus-terings considered as good by VI lie in [0, 1].
Hence,NVI can be used to compare clustering performanceacross datasets.
We show that NVI keeps the convexadditivity property of VI but not its metric axioms.168Definition.
We define NVI to be:NV I(C,K) ={H(C|K)+H(K|C)H(C) H(C) 6= 0H(K) H(C) = 0We define NVI to be H(K) when H(C) = 0 to sat-isfy the requirements that NVI values decrease as Cand K become more similar and that NVI would be0 when they are identical1.Range and extreme cases.
Like VI, NVI decreasesas the clustering becomes more complete and morehomogeneous.
For the perfect solution, NV I = 0.In both the single cluster and the no knowledge so-lutions, H(C|K) = H(C).
Thus, in the former caseNV I = 1, and in the latter NV I = 1 + H(K)HC ?
1.For the singletons clustering case, NV I =H(K|C)H(C) .
Suppose that the number of clusters isfixed.
When the number of classes increases, thenumerator decreases and the denominator increases,and hence the score decreases.
In other words, as thereal solution gets closer to the singletons solution,the score decreases, which is the correct behavior.When the number of classes decreases, the score in-creases, which is again the correct behavior.For any pair of clusterings K1 and K2,V I(C,K1) > V I(C,K2) iff NV I(C,K1) >V I(C,K2).
This implies that only clustering solu-tions whose VI scores are better (i.e., numericallylower) than the score of the single cluster solutionwill be scored lower than 1 by NVI.Note that NVI is meant to be used when there isa ?correct?
reference solution.
In this case H(C) isconstant, so the property above holds.
In this sense,VI is more general, allowing us to compare any threeclustering solutions even when we do not have a cor-rect reference one.To summarize:1.
All clusterings considered by VI to be of highquality (i.e., better than the single cluster solu-tion) are scored by NVI in the range of [0, 1].2.
All clusterings considered by VI to be of lowerquality than the single cluster solution arescored higher than 1 by NVI.1H(C) = 0 iff C consists of a single class, and thereforeH(C) = H(K) = 0 iff C (K) consists of a single class (clus-ter).3.
The ordering of scores between solutions givenby VI is preserved by NVI.4.
The behavior of NVI on the extreme cases is thedesired one.Useful properties.
In Section 3 we saw that VI hastwo useful properties, satisfying the metric axiomsand being convexly additive.
NVI is not symmetricsince the term in its denominator is H(C), the en-tropy of the correct class assignment.
Thus, it doesnot satisfy the metric axioms.
Being convexly addi-tive, however, is preserved.
In the class splitting sce-nario (see convex additivity definition in Section 3)it holds that NV I(C,K) = P (Cj)H?
(Kj)H(C) .
That is,like for VI, the impact of splitting or merging a clus-ter on NVI is limited only to those clusters involved,and its size is relative to the size of these clusters.Meila (2007) derived various interesting propertiesof VI from the convex additivity property.
Theseproperties generally hold for NVI as well.H(K) normalization.
Normalizing by H(C)takes into consideration the complexity of the cor-rect clustering.
Another normalization option wouldbe to normalize by H(K), which represents the in-duced clustering complexity.
This normalizationdoes not guarantee that the scores of the ?good?
clus-terings lie in a data-independent range.Let us define NVIK(C,K) to be V I(C,K)H(K) ifH(K) > 0 and H(C) if H(K) = 0.
Recall thatin order for NVIK to be 0 iff C and K are identi-cal, we must require that NV IK = H(C) whenH(K) = 0.
In the no knowledge case, NV IK =H(C)+H(K)H(K) = H(C)H(K) + 1 > 1.
In the single clustersolution, however, NV IK = H(C) (since in thiscase H(K) = 0) which ranges in [0, logN ].
This isa serious drawback of NVIK.
In Section 6 we empir-ically show an additional drawback of NVIK.logN normalization.
Another possible normal-ization of VI is by 2logN (or 2logk?
), which is anupper bound on VI values.
However, this results inthe values of the measure being dependent on datasetsize, so results on datasets with different sizes againcannot be compared.
For example, take any C andK and split each element into two.
All entropy val-ues, and the quality of the solution, are preserved,but the scores given to the two K?s (before and after1697 1 1 1 0 0 0 0 0 00 7 1 1 1 0 0 0 0 00 0 7 1 1 1 0 0 0 00 0 0 7 1 1 1 0 0 00 0 0 0 7 1 1 1 0 00 0 0 0 0 7 1 1 1 00 0 0 0 0 0 7 1 1 11 0 0 0 0 0 0 7 1 11 1 0 0 0 0 0 0 7 11 1 1 0 0 0 0 0 0 7V VI NVI NVIKSingletons 0.667 2.303 1 0.5Solution R 0.587 1.88 0.81 0.81Table 1: The clustering matrix of solution R (top), andthe scores given to it and to the singletons solution by thefour measures (bottom).
Although solution R is superior,the score given by V to the singletons solution is muchhigher.
NVI exhibits the most preferable behavior (recallthat higher V values are better, as opposed to the otherthree measures).the split) by such a normalized VI would be differ-ent.
Since H(C) is preserved, the scores given byNVI to the two K?s are identical.5 Problematic V Behavior ExampleIn this section we provide a synthetic example thatdemonstrates an undesireable behavior of V (andNVIK) not manifested by VI and NVI.
Specifically,V favors solutions with a large number of clusters,giving them higher scores than to solutions that areevidently superior.
In addition, the score given to thesingletons solution is high in absolute terms.To present the example, we use the matrix repre-sentation A of a clustering solution defined in Sec-tion 3.
The entries in row i sum to the number ofelements in class i, while those in column j sum tothe number of elements in cluster j.Suppose that we have 100 elements assigned to 10classes such that there are 10 elements in each class.We consider two clustering solutions: the singletonssolution, and solution R whose matrix is shown inTable 1 (top).
Like the real solution, solution R alsohas 10 clusters each having 10 elements.
SolutionR is not very far from the correct solution, sinceeach cluster has 7 elements of the same class, andthe three other elements in a cluster are taken froma different class each and can be viewed as ?noise?.Solution R is thus much better than the singletonssolution.
In order not to rely on our own opinion,we have performed a simple human judgment ex-periment with 30 subjects (university graduates indifferent fields), all of whom preferred solution R2.The scores given by V, VI, NVI and NVIK to thetwo solutions are shown in Table 1 (bottom).
Vscores solution R as being worse than the single-tons solution, and gives the latter a number that?srelatively high in absolute terms (0.667).
VI ex-hibits qualitatively correct behavior, but the num-bers it uses are hard to interpret since they are N-dependent.
NVI scores solution R as being betterthan singletons, and its score is less than 1, indicat-ing that it might be a good solution.6 Grammar Induction ExperimentIn this section we analyse the behavior of V, VI,NVI and NVIK using a highly non-trivial NLP ap-plication with large real datasets, the unsupervisedlabeled parse tree induction (LTI) algorithm of (Re-ichart and Rappoport, 2008).
We focus on the label-ing that the algorithm finds for parsing constituents,which is a clustering of constituents.Summary of result.
We show that V gives aboutthe same score to a labeling that uses thousands oflabels and to labelings in which the number of la-bels (dozens) is identical or smaller than the numberof labels in the reference evaluation set (an anno-tated corpus).
Contrary to V, both NVI and VI givemuch better scores to the solutions having a smallernumber of labels.It could be argued that the total number of ?real?labels in the data is indeed large (e.g., because everyverb exhibits its own syntactic patterns) and that asmall number of labels is just an arbitrary decision ofthe corpus annotators.
However, most linguistic the-ories agree that there is a prototypical level of gen-eralization that uses concepts such as Noun Phraseand Verb Phrase, a level which consists of at mostdozens of labels and is strongly manifested by reallanguage data.
Under these accepted assumptions,the scoring behavior of V is unreasonable.2We must rely on people?s expectations, since the wholepoint in this area is that clustering quality cannot be formalizedin an objective, application-independent way.170MDL+SC (T labels) MDL+SC (P labels) MDL labelsCorpus L = 1 < 10 < 102 ?
102 L = 1 < 10 < 102 ?
102 L = 1 < 10 < 102 ?
102WSJ10 26 0 0 3 23 8 0 0 0 8 2916 2282 2774 2864 52NEGRA10 22 0 2 12 10 6 0 0 1 5 1202 902 1114 1191 11CTB10 24 1 4 11 13 9 1 2 4 5 1050 816 993 1044 6Table 2: The number of elements (constituents) covered by the clusters (labels) produced by the MDL+SC (T or Plabels) and MDL clusterings.
L is the total number of labels.
Shown are the number of clusters having one element,less than 10 elements, less than 100 elements, and more than 100 elements.
It is evident that MDL induces a sparseclustering with many clusters that annotate very few constituents.V VI NVI NVIKCorpus MDL T P MDL T P MDL T P MDL T PWSJ10 0.4 0.44 0.41 3.83 2.32 1.9 2.21 1.34 1.1 0.81 0.86 1.2NEGRA10 0.47 0.5 0.5 2.56 1.8 1.4 1.51 1.1 0.83 0.76 0.96 1.1CTB10 0.42 0.42 0.45 3 2.22 1.85 1.72 1.26 1.1 0.87 1.1 1.25Table 3: V, VI, NVI and NVIK values for MDL and MDL+SC with T or P labels.
V gives the three clusteringsvery similar scores.
NVIK prefers MDL labeling.
NVI and VI both show the expected qualitative behavior, favoringMDL+SC clustering with P labels.
The most preferable scores are those of NVI, whose numbers are also the easiestto interpret.The experiment.
The LTI algorithm has threestages: bracketing, initial labeling, and label clus-tering.
Bracketing is done from raw text usingthe unsupervised incremental parser of (Seginer,2007).
Initial labeling is done using the BMM model(Borensztajn and Zuidema, 2007), which aims atminimizing the grammar description length (MDL).Finally, labels are clustered to a desired number oflabels using the k-means algorithm with syntacticfeatures extracted from the initially labeled trees.We refer to this stage as MDL+SC (for ?syntacticclustering?).
Using a mapping-based evaluation withtwo different mapping functions, the LTI algorithmwas shown to outperform previous work on unsu-pervised labeled parse tree induction.The MDL clustering step induces several thou-sand labels for corpora of several tens of thousandsof constituents.
The role of the SC step is to gen-eralize these labels using syntactic features.
Thereare two versions of the SC step.
In one, the num-ber of clusters is identical to the number of labelsin the gold standard annotation of the experimentalcorpus.
This set of labels is called T (for target)labels.
In the other SC version, the number of la-bels is the minimum number of labels required toannotate more than 95% of the constituents in thegold standard annotation of the corpus.
This set oflabels is called P (for prominent) labels.
Since con-stituent labels follow the Zipfian distribution, P ismuch smaller than T .In this paper we run the LTI algorithm and evalu-ate its labeling quality using V, VI, NVI and NVIK.We compare the quality of the clustering induced bythe first clustering step alone (the MDL clustering)to the quality of the clustering induced by the fullalgorithm (i.e., first applying MDL and then clus-tering its output using the SC algorithm for T or Plabels)3.We follow the experimental setup in (Reichartand Rappoport, 2008), running the algorithm on En-glish, German and Chinese corpora: the WSJ PennTreebank (English), the Negra corpus (Brants, 1997)(German), and version 5.0 of the Chinese Penn Tree-bank (Xue et al, 2002).
In each corpus, we usedthe sentences of length at most 10,4 numbering 7422(WSJ10), 7542 (NEGRA10) and 4626 (CTB10).The characteristics of the induced clusterings areshown in Table 25.
The table demonstrates thefact that MDL labeling, while perhaps capturing the3Note that our evaluation here has nothing to do with theevaluation done in (Reichart and Rappoport, 2008), which pro-vided a comparison of the full grammar induction results be-tween different algorithms, using mapping-based measures.
Weevaluate the labeling stages alone.4Excluding punctuation and null elements, according to thescheme of (Klein, 2005).5The number of MDL labels in the table differs from theirnumbers, since we report the number of unique MDL labelsused for annotating correct constituents in the parser?s output,while they report the number of unique labels used for annotat-ing all constituents in the parser?s output.171salient level of generalization of the data in its lead-ing clusters, is extremely noisy.
For WSJ10, for ex-ample, 2282 of the 2916 unique labels annotate onlyone constituent, and 2774 labels label less than 10constituents.
These 2774 labels annotate 14.4% ofcompared constituents, and the 2864 labels that an-notate less than 100 constituents each, cover 30.7%of the compared constituents (these percentages arenot shown in the table).
In other words, MDL is nota solution in which almost all of the mass is concen-trated in the few leading clusters; its tail occupies alarge percentage of its mass.MDL patterns for NEGRA10 and CTB10 are verysimilar.
For MDL+SC with T or P labels, mostof the induced labels annotate 100 constituents ormore.
We thus expect MDL+SC to provide betterclustering than MDL; a good clustering evaluationmeasure should reflect this expectation.Table 3 shows V, VI, NVI and NVIK scores forMDL and MDL+SC (with T or P labels).
For allthree corpora, V values are almost identical for theMDL and the MDL+SC schemes.
This is in con-trast to VI and NVI values that strongly prefer theMDL+SC clusterings, fitting our expectations (re-call that for these measures, the lower the score, thebetter the clustering).
Moreover, VI and NVI pre-fer MDL+SC with P labels, which again accordswith our expectations, since P labels were definedas those that are more salient in the data (see above).The patterns of NVI and VI are identical, sinceNV I = V IH(C) and H(C) is independent of theinduced clustering.
However, the numbers givenby NVI are easier to interpret than those given byVI.
The latter are basically meaningless, convey-ing nothing about clustering quality.
The former arequite close to 1, telling us that clustering quality isnot that good but not horrible either.
This makessense, because the overall quality of the labeling in-duction algorithm is indeed not that high: using one-to-one mapping (the more forgiving mapping), theaccuracy of the labels induced by MDL+SC is only45?72% (Reichart and Rappoport, 2008).NVIK, the normalization of VI with H(K), isworse even than V. This measure (which also giveslower scores to better clusterings) prefers the MDLover MDL+SC labels.
This is a further justificationof our decision to define NVI by normalizing VI byH(C) rather than by H(K).Corpus H(C) H(K)MDL T PWSJ10 1.73 4.72 2.7 1.58NEGRA10 1.69 3.36 1.87 1.29CTB10 1.76 3.45 2.1 1.48Table 4: Class (H(C)) and cluster (H(K)) entropy forMDL and MDL+SC with T or P labels.
H(C) is clusterindependent.
H(K) increases with the number of clus-ters.Table 4 shows the H(C) and H(K) values in theexperiment.
While H(C) is independent of the in-duced clustering and is thus constant for a givenannotated corpus, H(K) monotonically increaseswith the number of induced clusters.
Since bothNVIK and the completeness term of V are normalizedby H(K), these measures prefer clusterings with alarge number of clusters even when many of theseclusters provide useless information.7 ConclusionUnsupervised clustering evaluation is important forvarious NLP tasks and applications.
Recently, theimportance of the completeness and homogeneity asevaluation criteria for such clusterings has been rec-ognized.
In this paper we addressed the two mea-sures that address these criteria: VI (Meila, 2007)and V (Rosenberg and Hirschberg, 2007).While VI has many useful properties, the range ofvalues it can take is dataset dependent, which makesit unsuitable for comparing clusterings of differentdatasets.
This imposes a serious restriction on themeasure usage.
We presented NVI, a normalized ver-sion of VI, which does not have this restriction andstill retains some of its useful properties.Using experiments with both synthetic data anda complex NLP application, we showed that the Vmeasure prefers clusterings with many clusters evenwhen these are clearly of low quality.
VI and NVI donot exhibit such behavior, and the numbers given byNVI are easier to interpret than those given by VI.In future work we intend to explore more of theproperties of NVI and use it in other real NLP appli-cations.ReferencesAmit Bagga and Breck Baldwin, 1998.
Entity-basedcross-document coreferencing using the vector space172model.
ACL 98.Ulrike Baldewein, Katrin Erk, Sebastian Pado, and DetlefPrescher 2004.
Semantic role labeling with similaritybased generalization using EM?based clustering.
Sen-seval ?04.Thorsten Brants, 1997.
The NEGRA export format.CLAUS Report, Saarland University.Gideon Borensztajn and Willem Zuidema, 2007.Bayesian model merging for unsupervised constituentlabeling and grammar induction.
Technical Report,ILLC.
http: //staff.science.uva.nl/?gideon/Alexander Clark, 2003.
Combining distributional andmorphological information for part of speech induc-tion.
EACL ?03.I.
S. Dhillon, S. Mallela, and D. S. Modha, 2003.
Infor-mation theoretic co-clustering.
KDD ?03.Byron E. Dom, 2001.
An information-theoretic externalcluster validity measure .
Journal of American statis-tical Association,78:553?569.Jenny Rose Finkel and Christopher D. Manning, 2008.Enforcing transitivity in coreference resolution.
ACL?08.E.B Fowlkes and C.L.
Mallows, 1983.
A method forcomparing two hierarchical clusterings.
Journal ofAmerican statistical Association,78:553?569.Benjamin C. M. Fung, Ke Wang, and Martin Ester, 2003.Hierarchical document clustering using frequent item-sets.
SIAM International Conference on Data Mining?03.Sharon Goldwater and Thomas L. Griffiths, 2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
ACL ?07.William P. Headden, David McClosky, and Eugene Char-niak, 2008.
Evaluating unsupervised part-of-speechtagging for grammar induction.
COLING ?08.L.
Hubert and P. Arabie, 1985.
Comparing partitions.Journal of Classification, 2:193?218.L.
Hubert and J. Schultz, 1976.
Quadratic assignmentas a general data analysis strategy.
British Journalof Mathematical and Statistical Psychology, 29:190?241.Dan Klein, 2005.
The unsupervised learning of naturallanguage structure.
Ph.D. thesis, Stanford University.Bjornar Larsen and Chinatsu Aone, 1999.
Fast and effec-tive text mining using linear-time document clustering.KDD ?99.Gina-Anne Levow, 2006.
Unsupervised and semi-supervised learning of tone and pitch accent.
HLT-NAACL ?06.Marina Meila and David Heckerman, 2001.
An exper-imental comparison of model-based clustering meth-ods.
Machine Learning, 42(1/2):9-29.Marina Meila, 2007.
Comparing clustering ?
an infor-mation based distance.
Journal of Multivariate Analy-sis, 98:873?895.C.W Milligan, S.C Soon and L.M Sokol, 1983.
Theeffect of cluster size, dimensionality and the numberof clusters on recovery of true cluster structure.
IEEEtransactions on Pattern Analysis and Machine Intelli-gence, 5:40?47.Boris G. Mirkin, 1996.
Mathematical classification andclustering.
Kluwer Academic Press.Darius M. Pfitzner, Richard E. Leibbrandt and DavidM.W Powers, 2008.
Characterization and evaluationof similarity measures for pairs of clusterings.
Knowl-edge and Information Systems: An International Jour-nal, DOI 10.1007/s10115-008-0150-6.William Rand, 1971.
Objective criteria for the evalua-tion of clustering methods.
Journal of the AmericanStatstical Association, 66(336):846?850.Roi Reichart and Ari Rappoport, 2008.
Unsupervised in-duction of labeled parse trees by clustering with syn-tactic features.
COLING ?08.Andrew Rosenberg and Julia Hirschberg, 2007.
V?Measure: a conditional entropy?based external clusterevaluation measure.
EMNLP ?07.Sabine Schulte im Walde, 2003.
Experiments on theautomatic induction of German semantic verb classes.Ph.D.
thesis, Universitat Stuttgart.Yoav Seginer, 2007.
Fast unsupervised incremental pars-ing.
ACL 07.Sa-Im Shin and Key-Sun Choi, 2004.
Automatic wordsense clustering using collocation for sense adaptation.The Second Global WordNet Conference.Stijn van Dongen, 2000.
Performance criteria for graphclustering and markov cluster experiments.
Technicalreport CWI, AmsterdamDaniel D. Walker and Eric K. Ringger, 2008.
Model-based document clustering with a collapsed Gibbssampler.
KDD ?08.Nianwen Xue, Fu-Dong Chiou and Martha Palmer, 2002.Building a large?scale annotated Chinese corpus.
ACL?02.Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, andGuang R. Gao, 2002.
An adaptive meta-clusteringapproach: combining the information from differentclustering results.
IEEE Computer Society Bioinfor-matics Conference (CSB ?02) .Ying Zhao and George Karypis, 2001.
Criterion func-tions for document clustering: experiments and analy-sis.
Technical Report TR 01-40, Department of Com-puter Science, University of Minnesota.173
