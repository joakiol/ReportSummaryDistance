Pat tern -Based  Context -F ree  Grammars  for Mach ine  Trans la t ionKoichi TakedaTokyo Research Laboratory,  IBM Research1623-14 Sh imotsuruma,  Yamato,  Kanagawa 242, JapanPhone: 81-462-73-4569, 81-462-73-7413 (FAX)takeda@trl, vnet.
ibm.
comAbstractThis paper proposes the use of "pattern-based" context-free grammars as a basisfor building machine translation (MT) sys-tems, which are now being adopted as per-sonal tools by a broad range of users inthe cyberspace society.
We discuss ma-jor requirements for such tools, includingeasy customization for diverse domains,the efficiency of the translation algorithm,and scalability (incremental improvementin translation quality through user interac-tion), and describe how our approach meetsthese requirements.1 IntroductionWith the explosive growth of the World-Wide Web(WWW) as information source, it has become rou-tine for Internet users to access textual data writtenin foreign languages.
In Japan, for example, a dozenor so inexpensive MT tools have recently been puton the market o help PC users understand Englishtext in WWW home pages.
The MT techniques em-ployed in the tools, however, are fairly conventional.For reasons of affordability, their designers appearto have made no attempt to tackle the well-knownproblems in MT, such as how to ensure the learnabil-ity of correct ranslations and facilitate customiza-tion.
As a result, users are forced to see the samekinds of translation errors over and over again, ex-cept they in cases where they involve merely addinga missing word or compound to a user dictionary, orspecifying one of several word-to-word translationsas a correct choice.There are several alternative approaches thatmight eventually liberate us from this limitation onthe usability of MT systems:Unification-based grammar for-malisms and lexical-semantics formalisms (see LFG(Kaplan and Bresnan, 1982), HPSG (Pollard andSag, 1987), and Generative Lexicon (Pustejovsky,1991), for example) have been proposed to facili-tate computationally precise description of natural-language syntax and semantics.
It is possible that,with the descriptive power of these grammars andlexicons, individual usages of words and phrases maybe defined specifically enough to give correct rans-lations.
Practical implementation f MT systemsbased on these formalisms, on the other hand, wouldnot be possible without much more efficient parsingand disambiguation algorithms for these formalismsand a method for building a lexicon that is easy evenfor novices to use.Corpus-based or example-based MT (Sato andNagao, 1990; Sumita and Iida, 1991) and statisti-cal MT (Brown et al, 1993) systems provide theeasiest customizability, since users have only to sup-ply a collection of source and target sentence pairs(a bilingual corpus).
Two open questions, however,have yet to be satisfactorily answered before we canconfidently build commercial MT systems based onthese approaches:?
Can the system be used for various domainswithout showing severe degradation of transla-tion accuracy??
What is the minimum number of examples (ortraining data) required to achieve reasonableMT quality for a new domain?TAG-based MT (Abeill~, Schabes, and Joshi,1990) 1 and pattern-based translation (Maruyama,1993) share many important properties for successfulimplementation in practical MT systems, namely:?
The existence of a polynomial-time parsing al-gorithm?
A capability for describing a larger domain oflocality (Schabes, Abeill~, and Joshi, 1988)?
Synchronization (Shieber and Schabes, 1990) ofthe source and target language structuresReaders should note, however, that the pars-1 See LTAG (Schabes, AbeiU~, and Joshi, 1988) (Lex-icalized TAG) and STAG (Shieber and Schabes, 1990)(Synchronized TAG) for each member of the TAG (TreeAdjoining Grammar) family.144ing algorithm for TAGs has O(IGIn6) 2 worst casetime complexity (Vijay-Shanker, 1987), and thatthe "patterns" in Maruyama's approach are merelycontext-free grammar (CFG) rules.
Thus, it hasbeen a challenge to find a framework in which wecan enjoy both a grammar formalism with betterdescriptive power than CFG and more efficient pars-ing/generation algorithms than those of TAGs.
3In this paper, we will show that there exists aclass of "pattern-based" grammars that is weaklyequivalent o CFG (thus allowing the CFG parsingalgorithms to be used for our grammars), but thatit facilitates description of the domain of locality.Furthermore, we will show that our framework canbe extended to incorporate xample-based MT anda powerful earning mechanism.2 Pat tern -Based  Context -F reeGrammarsPattern-based context-free grammars (PCFG) con-sists of a set of translation patterns.
A pattern is apair of CFG rules, and zero or more syntactic headand link constraints for nonterminal symbols.
Forexample, the English-French translation pattern 4NP:I  miss:V:2 NP:3 ---* S:2S:2 ~-- NP:3 manquer:V:2 h NP:Iessentially describes a synchronized 5 pair consistingof a left-hand-side English CFG rule (called a sourcerule)NP V NP --~ Sand a French CFG rule (called a target rule)S ~ NP V h NPaccompanied by the following constraints.1.
Head const ra in ts :  The nonterminal symbol Vin the source rule must have the verb miss as asyntactic head.
The symbol V in the target rulemust have the verb manquer as a syntactic head.The head of symbol S in the source (target) ruleis identical to the head of symbol V in the source(target) rule as they are co-indexed.2.
L ink  const ra in ts :  Nonterminal symbols insource and target CFG rules are linked if they2Where \]G\] stands for the size of grammar G, and nis the length of an input string.3Lexicalized CFG, or Tree Insertion Grammar (TIG)(Schabes and Waters, 1995), has been recently intro-duced to achieve such efficiency and lexicalization.4and its inflectional variants - -  we will discuss inflec-tions and agreement issues later.5The meaning of the word "synchronized" here is ex-actly the same as in STAG (Shieber and Schabes, 1990).See also bilingual signs (Tsujii and Fujita, 1991) for adiscussion of the importance of combining the appropri-ate domain of locality and synchronization.are given the same index ":i".
Linked nonter-minal must be derived from a sequence of syn-chronized pairs.
Thus, the first NP (NP:I) inthe source rule corresponds to the second NP(NP:I) in the target rule, the Vs in both rulescorrespond to each other, and the second NP(NP:3) in the source rule corresponds to the firstNP (NP:3) in the target rule.The source and target rules are called CFG skele-ton of the pattern.
The notion of a syntactic headis similar to that used in unification grammars, al-though the heads in our patterns are simply encodedas character strings rather than as complex featurestructures.
A head is typically introduced 6 in preter-minal rules such asleave ---* V V *-- partirwhere two verbs, "leave" and "partir," are associatedwith the heads of the nonterminal symbol V. This isequivalently expressed asleave:l --~ V:I V:I ~ partir: lwhich is physically implemented as an entry of anEnglish-French lexicon.A set T of translation patterns is said to acceptan input s iff there is a derivation sequence Q for susing the source CFG skeletons of T, and every headconstraint associated with the CFG skeletons in Q issatisfied.
Similarly, T is said to translate s iff thereis a synchronized erivation sequence Q for s suchthat T accepts , and every head and link constraintassociated with the source and target CFG skeletonsin Q is satisfied.
The derivation Q then produces atranslation t as the resulting sequence of terminalsymbols included in the target CFG skeletons in Q.Translation of an input string s essentially consistsof the following three steps:1.
Parsing s by using the source CFG skeletons2.
Propagating link constraints from source to tar-get CFG skeletons to build a target CFG deriva-tion sequence3.
Generating t from the target CFG derivationsequenceThe third step is a trivial procedure when the targetCFG derivation is obtained.Theorem 1 Let T be a PCFG.
Then, there existsa CFG GT such that for two languages L(T) andL(GT) accepted by T and GT, respectively, L(T) =L(GT) holds.
That is, T accepts a sentence s iff GTaccepts .Proof :  We can construct a CFG GT as follows:1.
GT has the same set of terminal symbols as T.6A nonterminal symbol X in a source or target CFGrule X --* X1 ... Xk can only be constrained to have oneof the heads in the RHS X1 ... X~.
Thus, monotonicityof head constraints holds throughout the parsing process.1452.
For each nonterminal symbol X in T, GT in-eludes a set of nonterminal symbols {X~ \]w iseither a terminal symbol in T or a special sym-bol e}.3.
For each preterminal ruleX:i --+ wl: l  w2:2 .
.
.
wk:k (1 < i < k),GT includes zXwi  --~ wl w2 .. .
wk (1 < i < k).If X is not co-indexed with any of wl, GT in-cludesXe ~Wl  w2 .
.
.
Wk.4.
For each source CFG rule with head constraints(hi, h2, .
.
.
,  hk) and indexes (il, i2 , .
.
.
,  ik),Y :ij ---* hl :Xl :il .
.
.
hk :Xk :ik (1 <_ j <k),GT includesYh j  ---* Xh l  Xh2 .. .
Xhk.If Y is not co-indexed with any of its children,we haveY~ --* Xh~ Xh2 .. .
Xhk.I f  Xj has no head constraint in the above rule,GT includes a set of (N + 1) rules, where Xhjabove is replaced with Xw for every terminalsymbol w and Xe (Yhj  will also be replaced ifit is co-indexed with Xj).sNow, L(T) C_ L(GT) is obvious, since GT can simu-late the derivation sequence in T with correspondingrules in GT.
L(GT) C L(T) can be proven, withmathematical  induction, from the fact that everyvalid derivation sequence of GT satisfies head con-straints of corresponding rules in T.\[3P ropos i t ion  1 Let a CFG G be a set of source CFGskeletons in T. Then, L(T) C n(c ) .Since a valid derivation sequence in T is always avalid derivation sequence in G, the proof is immedi-ate.
Similarly, we havePropos i t ion  2 Let a CFG H be a subset of sourceCFG skeletons in T such that a source CFG skeletonk is in H iffk has no head constraints associated withit.
Then, L(H) C L(T).THead constraints ate trivially satisfied or violated inpreterminal rules.
Hence, we assume, without loss ofgenerality, that no head constraint is given in pretetmi-nal rules.
We also assume that "X ---* w" implies "X:Iw:l".STherefore, a single rule in T can be mapped to asmany as (N + 1) k rules in GT, where N is the number ofterminal symbols in T. GT could be exponentially argerthan T.Two CFGs G and H define the range of CFL L(T).These two CFGs can be used to measure the "de-fault" translation quality, since idioms and colloca-tional phrases are typically translated by patternswith head constraints.Theorem 2 Let a CFG G be a set of source CFGskeletons in T. Then, L(T) C L(G) is undecidable.Proof"  The decision problem, L(T) C L(G), oftwo CFLs such that L(T) C L(G) is solvable iffL(T) = L(G) is solvable.
This includes a known un-decidable problem, L(T) = E*?, since we can choosea grammar U with L(U) = E*, nullify the entire setof rules in U by defining T to be a vacuous set {S:Ia:Sb:l, Sb:l --+ b:Su:l} U U (Sv and S are startsymbols in U and T, respectively), and, finally, letT further include an arbitrary CFG F. L(G) = E*is obvious, since G has {S --* Sb, Sb --* Sv} U U.Now, we have L(G) = L(T) iff L(F)  = E*.\[3Theorem 2 shows that the syntactic coverage ofT is, in general, only computable by T itself, eventhough T is merely a CFL.
This may pose a seriousproblem when a grammar writer wishes to know ifthere is a specific expression that is only acceptableby using at least one pattern with head constraints,for which the answer is "no" iff L(G) = L(T).
Oneway to trivialize this problem is to let T include apattern with a pair of pure CFG rules for every pat-tern with head constraints, which guarantees thatL(H) = L(T) = L(G).
In this case, we know thatthe coverage of "default" patterns is always identi-cal to L(T).Although our "patterns" have no more theoreti-cal descriptive power than CFG, they can provideconsiderably better descriptions of the domain of lo-cality than ordinary CFG rules.
For example,be:V:l year:NP:2 old ---* VP: IVP: I  *- avoir:V:l an:NP:2can handle such NP pairs as "one year" and "un an,"and "more than two years" and "plus que deux ans,"which would have to be covered by a large numberof plain CFG rules.
TAGs, on the other hand, areknown to be "mildly context-sensitive" grammars,and they can capture a broader range of syntacticdependencies, such as cross-serial dependencies.
Thecomputational complexity of parsing for TAGs, how-ever, is O(IGIn6), which is far greater than that ofCFG parsing.
Moreover, defining a new STAG ruleis not as easy for the users as just adding an entryinto a dictionary, because ach STAG rule has to bespecified as a pair of tree structures.
Our patterns,on the other hand, concentrate on specifying linearordering of source and target constituents, and canbe written by the users as easily as 99By sacrificing linguistic accuracy for the descriptionof syntactic structures.146to leave * -- de quitter *to be year:* old = d'avoir an:*Here, the wildcard "*" stands for an NP by default.The preposition "to" and "de" are used to specifythat the patterns are for VP pairs, and "to be" isused to show that the phrase is the BE-verb and itscomplement.
A wildcard can be constrained with ahead, as in "house:*" and "maison:*".
The internalrepresentations of these patterns are as follows:leave:V:l NP:2 ~ VP:IVP:I ~-- quitter:V:l NP:2be:V:l year:NP:2 old --+ VP:IVP:I ~ avoir:V:l an:NP:2These patterns can be associated with an explicitnonterminal symbol such as "V:*" or "ADJP:*" inaddition to head constraints (e.g., "leave:V:*').
Bydefining a few such notations, these patterns canbe successfully converted into the formal represen-tations defined in this section.
Many of the diver-gences (Doff, 1993) in source and target languageexpressions are fairly collocational, and can be ap-propriately handled by using our patterns.
Notethe simplicity that results from using a notation inwhich users only have to specify the surface orderingof words and phrases.
More powerful grammar for-malisms would generally require either a structuraldescription or complex feature structures.3 The  Trans la t ion  A lgor i thmThe parsing algorithm for translation patterns canbe any of known CFG parsing algorithms includ-ing CKY and Earley algorithms 1?
At this stage,head and link constraints are ignored.
It is easyto show that the number of target charts for a sin-gle source chart increases exponentially if we buildtarget charts simultaneously with source charts.
Forexample, the two patternsA:I B:2 ~ B:2 B:2 ~-- A:I B:2, andA:I B:2 --~ B:2 A:I ~- B:2 A:Iwill generate the following 2 n synchronized pairs ofcharts for the sequence of (n+l)  nonterminal sym-bols AAA.
.
.AB ,  for which no effective packing ofthe target charts is possible.
(A (A .
.
.
(A B))) with (A (A .
.
.
(A B)))(A (A .
.
.
(A B))) with ((A .. .
(A B)) A)iA (A .
.
.
(A S))) with (((B A) A ) .
.
.
A)Our strategy is thus to find a candidate set ofsource charts in polynomial time.
We thereforeapply heuristic measurements to identify the mostpromising patterns for generating translations.
In1?Our prototype implementation was based on theEarley algorithm, since this does not require lexicaliza-tion of CFG rules.this sense, the entire translation algorithm is notguaranteed to run in polynomial time.
Practically, atimeout mechanism and a process for recovery fromunsuccessful translation (e.g., applying the idea offitted parse (Jensen and Heidorn, 1983) to targetCFG rules) should be incorporated into the transla-tion algorithm.Some restrictions on patterns must be imposedto avoid infinitely many ambiguities and arbitrarilylong translations.
The following patterns are there-fore not allowed:1.
A - - *XY~- -B2.
A + X Y ~-C1 .
.
.B .
.
.C~if there is a cycle of synchronized erivation suchthatA--+ X .
.
.
- -~  A andB (or C l .
.
.B .
.
.Ck)  --* Y .
.
.
-+  B,where A, B, X, and Y are nonterminal symbols withor without head and link constraints, and C's areeither terminal or nonterminal symbols.The basic strategy for choosing a candidatederivation sequence from ambiguous parses is asfollows.
11 A simplified view of the Earley algorithm(Earley, 1970) consists of three major components,predict(i), complete(i), and scan(i), which are calledat each position i = 0, 1 , .
.
.
,  n in an input string I =s ls2.
.
.sn.
Predict(i) returns a set of currently ap-plicable CFG rules at position i.
Complete(i) com-bines inactive charts ending at i with active chartsthat look for the inactive charts at position i to pro-duce a new collection of active and inactive charts.Scan(i) tries to combine inactive charts with thesymbol si+l at position i.
Complete(n) gives theset of possible parses for the input I.Now, for every inactive chart associated with anonterminal symbol X for a span of (i~) (1 ~ i, j <_n), there exists a set P of patterns with the sourceCFG skeleton, .
.
.
--* X.
We can define the fol-lowing ordering of patterns in P; this gives patternswith which we can use head and link constraints forbuilding target charts and translations.
These can-didate patterns can be arranged and associated withthe chart in the complete() procedure.1.
Prefer a pattern p with a source CFG skeletonX --~ X1.
.
.X~ over any other pattern q withthe same source CFG skeleton X --~ X1 .
. '
Xk,such that p has a head constraint h:Xi if q hash:Xi (i = 1, .
.
.
,k ) .
The pattern p is said tobe more specific than q.
For example, p =11 This strategy is similar to that of transfer-driven MT(TDMT) (Furuse and Iida, 1994).
TDMT, however, isbased on a combination of declarative/procedural knowl-edge sources for MT, and no clear computational prop-erties have been investigated.147"leave:V:1 house:NP --+ VP:I" is preferred toq = "leave:V:l NP --* VP:I".2.
Prefer a pattern p with a source CFG skeletonto any pattern q that has fewer terminal sym-bols in the source CFG skeleton than p. Forexample, prefer "take:V:l a walk" to "take:V:lNP" if these patterns give the VP charts withthe same span.3.
Prefer a pattern p which does not violate anyhead constraint over those which violate a headconstraint.4.
Prefer the shortest derivation sequence for eachinput substring.
A pattern for a larger domainof locality tends to give a shorter derivation se-quence.These preferences can be expressed as numericvalues (cost) for patterns.
12 Thus, our strategy fa-vors lexicalized (or head constrained) and colloca-tional patterns, which is exactly what we are go-ing to achieve with pattern-based MT.
Selection ofpatterns in the derivation sequence accompanies theconstruction of a target chart.
Link constraints arepropagated from source to target derivation trees.This is basically a bottom-up rocedure.Since the number M of distinct pairs (X,w), for anonterminal symbol X and a subsequence w of inputstring s, is bounded by Kn 2, we can compute the m-best choice of pattern candidates for every inactivechart in time O(ITIKn 3) as claimed by Maruyama(Maruyama, 1993), and Schabes and Waters (Sch-abes and Waters, 1995).
Here, K is the number ofdistinct nonterminal symbols in T, and n is the sizeof the input string.
Note that the head constraintsassociated with the source CFG rules can be incor-porated in the parsing algorithm, since the numberof triples (X,w,h), where h is a head of X, is boundedby Kn 3.
We can modify the predict(), complete(),and scan() procedures to run in O(\[T\[Kn 4) whilechecking the source head constraints.
Constructionof the target charts, if possible, on the basis of the mbest candidate patterns for each source chart takesO(Kn~m) time.
Here, m can be larger than 2 n if wegenerate very possible translation.The reader should note critical differences betweenlexicalized grammar rules (in the sense of LTAG andTIG) and translation patterns when they are usedfor MT.Firstly, a pattern is not necessarily lexicalized.
Aneconomical way of organizing translation patternsis to include non-lexicalized patterns as "default"translation rules.12A similar preference can be defined for the tar-get part of each pattern, but we found many counter-examples, where the number of nontermina\] symbolsshows no specificity of the patterns, in the target partof English-to-Japanese translation patterns.
Therefore,only the head constraint violation in the target part isaccounted for in our prototype.Secondly, lexicalization might increase the size ofSTAG grammars (in particular, compositional gram-mar rules such as ADJP NP --* NP) considerablywhen a large number of phrasal variations (adjec-tives, verbs in present participle form, various nu-meric expressions, and so on) multiplied by the num-ber of their translations, are associated with theADJP part.
The notion of structure sharing (Vijay-Shanker and Schabes, 1992) may have to be ex-tended from lexical to phrasal structures, as well asfrom monolingual to bilingual structures.Thirdly, a translation pattern can omit the treestructure of a collocation, and leave it as just a se-quence of terminal symbols.
The simplicity of thishelps users to add patterns easily, although precisedescription of syntactic dependencies is lost.4 Features  and  AgreementsTranslation patterns can be enhanced with unifica-tion and feature structures to give patterns addi-tional power for describing ender, number, agree-ment, and so on.
Since the descriptive power ofunification-based grammars is considerably greaterthan that of CFG (Berwick, 1982), feature struc-tures have to be restricted to maintain the efficiencyof parsing and generation algorithms.
Shieber andSchabes briefly discuss the issue (Shieber and Sch-abes, 1990).
We can also extend translation patternsas follows:Each nonterminal node in a pattern can beassociated with a fixed-length vector of bi-nary features.This will enable us to specify such syntactic de-pendencies as agreement and subcategorization inpatterns.
Unification of binary features, however,is much simpler: unification of a feature-value pairsucceeds only when the pair is either (0,0) or (1,1/.Since the feature vector has a fixed length, unifica-tion of two feature vectors is performed in a constanttime.
For example, the patterns 13V: I :+TRANS NP:2 --* VP:I  VP:IV: I :+TRANS NP:2V:I :+INTRANS --+ VP:I  VP:I  ~-V:I :+INTRANSare unifiable with transitive and intransitive verbs,respectively.
We can also distinguish local and headfeatures, as postulated in HPSG.
Simplified versionof verb subcategorization is then encoded asVP:I :+TRANS-OBJ NP:2 --* VP: I :+OBJVP: I :+OBJ ~-VP: I :+TRANS-OBJ  NP:2where "-OBJ" is a local feature for head VPs inLIISs, while "+OBJ" is a local feature for VPs in13Again, these patterns can be mapped to a weaklyequivalent set of CFG rules.
See GPSG (Gazdar, Pul-lum, and Sag, 1985) for more details.148the RHSs.
Unification of a local feature with +OBJsucceeds ince it is not bound.Agreement on subjects (nominative NPs) andfinite-form verbs (VPs, excluding the BE verb) isdisjunctively specified asNP : 1 : +NOMI+3RD+SG VP : 2 : +FIN+3SGNP : 1 : +NOMI+3RD+PL VP : 2 : +FIN-3SGNP : 1 : +NOMI-3RD VP : 2 : +FIN-3SGNP : 1 : +NOMI VP : 2 : +FIN+PASTwhich is collectively expressed asNP : 1 : *AGRS VP : 2 : *AGRVHere, *AGRS and *AGRV are a pair of aggregateunification specifiers that succeeds only when oneof the above combinations of the feature values isunifiable.Another way to extend our grammar formalism isto associate weights with patterns.
It is then possi-ble to rank the matching patterns according to a lin-ear ordering of the weights rather than the pairwisepartial ordering of patterns described in the previ-ous section.
In our prototype system, each patternhas its original weight, and according to the prefer-ence measurement described in the previous section,a penalty is added to the weight to give the effectiveweight of the pattern in a particular context.
Pat-terns with the least weight are to be chosen as themost preferred patterns.Numeric weights for patterns are extremely use-ful as means of assigning higher priorities uniformlyto user-defined patterns.
Statistical training of pat-terns can also be incorporated to calculate suchweights systematically (Fujisaki et al, 1989).Figure I shows a sample translation of the input"He knows me well," using the following patterns.NP : I : *AGRS VP : I : *AGRS ~ S:IS:I ~- NP : I : *AGRS VP : I : *AGRS ... (a)VP:I ADVP:2  ~ VP:IVP:I ~ VP:I ADVP:2  ... (b)know:VP: l :+OBJ  well --+ VP:IVP:I ~-- connaitre:VP:h+OBJ bien ... (c)V:I NP:2 --~ VP : I :+OBJVP : I :+OBJ  *-- V:I NP:2:-PRO ... (d)V:I NP:2 --+ VP : I :+OBJVP : I :+OBJ  ~ NP:2 :+PRO V:I ... (e)To simplify the example, let us assume that wehave the following preterminal rules:he --~ NP:+PRO+NOMI+3RD+SGNP:+PRO+NOMI+3RD+SG ~ il ... (f)me --+ NP:+PRO+CAUS+SG-3RDNP:+PRO+CAUS+SG-3RD ,--- me ... (g)knows --+ V :+F IN+3SGV:+F IN+3SG ,-- salt ... (h)knows --~ V :+F IN+3SGV:+F IN+3SG ~-- connait ... (i)Input: He knows me wellPhase 1: Source Analys is\[0 i\] He ---> (f) NP(ac t ive  arc \[0 1\] (a) NP.VP)\[1 23 knows ---> (h) V, (i) V(active arcs \[I 2\] (d) V.NP,\[1 2\] (e) V.NP)\[2 3\] me ---> (g) NP(inactive arcs \[I 3\] (d) V NP,\[i 3\] (e) V NP)\[I 3\] knows me ---> (d), (e) VP(inactive arc \[0 3\] (a) NP VP,active arcs \[I 3\] (b) VP.well,\[i 3\] (c) VP.ADVP)\[0 3\] He knows me ---> (a) S\[3 4\] well ---> (j) ADVP, (k) ADVP(inactive arcs \[I 4\] (b) VP ADVP,\[i 4\] (c) VP ADVP)\[i 4\] knows me well ---> (b), (c) VP(inactive arc \[0 4\] (a) NP VP)\[0 4\] He knows me well ---> (a) SPhase 2: Constraint Checking\[0 I \ ]  He - - ->  ( f )  NP\[1 2\] knows - - ->  ( i )  V, ( j )  V\[2 3\] me ---> (g) NP\[I 3\] knows me ---> (e) VP(pattern (d) fails)\[0 3\] He knows me ---> (a) S\[3 4\] well ---> (i) ADVP, (j) ADVP\[i 4\] knows me well ---> (b), (c) VP(preference order ing  (c ) ,  (b))\[0 4\] He knows me wel l  - - ->  (a) SPhase 3: Target  Generat ion\[0 4\] He knows me wel l  - - ->  (a) S\[0 1\] He ---> il\[I 4\] knows me well ---> (c) VPwell ---> bien\[I 3\] knows me ---> (e) VP\[1 2\] knows ---> connait(h) violates a head constraint\[2 3\] me ---> meTranslation: il me connait bienFigure 1: Sample Translationwell --* ADVP ADVP ~-- bien ... (j)well --~ ADVP ADVP ~-- beaucoup ... (k)In the above example, the Earley-based algorithmwith source CFG rules is used in Phase 1.
In Phase2, head and link constraints are examined, and unifi-cation of feature structures i performed by using thecharts obtained in Phase 1.
Candidate patterns areordered by their weights and preferences.
Finally,in Phase 3, the target charts are built to generatetranslations based on the selected patterns.5 In tegrat ion  o f  B i l ingua l  CorporaIntegration of translation patterns with translationexamples, or bilingual corpora, is the most impor-tant extension of our framework.
There is no dis-149crete line between patterns and bilingual corpora.Rather, we can view them together as a uniformset of translation pairs with varying degrees of lex-icalization.
Sentence pairs in the corpora, however,should not be just added as patterns, since they areoften redundant, and such additions contribute toneither acquisition or refinement of non-sententialpatterns.Therefore, we have been testing the integrationmethod with the following steps.
Let T be a set oftranslation patterns, B be a bilingual corpus, and(s,t) be a pair of source and target sentences.1.
\ [Correct  Trans lat ion\ ]  I fT  can translate s intot, do nothing.2.
\ [Compet i t i ve  S i tuat ion\ ]  If T can translate sinto t' (t ~ t~), do the following:(a) \[Lexical izat ion\]  If there is a paired deriva-tion sequence Q of (s,t) in T, create a newpattern p' for a pattern p used in Q suchthat every nonterminal symbol X in p withno head constraint is associated with h:Xin q, where the head h is instantiated in Xof p. Add p~ to T if it is not already there.Repeat the addition of such patterns, andassign low weights to them until the refinedsequence Q becomes the most likely trans-lation of s. For example, addleave:VP: 1:+OBJconsiderably:ADVP:2 -* VP:IVP:I *- laisser:VP:l:+OBJ con-sid@rablement:ADVP:2if the existing VP ADVP pattern does notgive a correct translation.
(b) \ [Add i t ion  of  New Pat terns \ ]  If there isno such paired derivation sequence, addspecific patterns, if possible, for idioms andcollocations that are missing in T, or addthe pair (s,t) to T as a translation pattern.For example, addleave:VP:l:+OBJ behind --* VP:IVP:I *-- laisser:VP:l:+OBJif the phrase "leave it behind" is not cor-rectly translated.3.
\ [Trans lat ion  Fai lure\] If T cannot translate sat all, add the pair (s,t) to T as a translationpattern.The grammar acquisition scheme described abovehas not yet been automated, but has been manuallysimulated for a set of 770 English-Japanese simplesentence pairs designed for use in MT system eval-uation, which is available from JEIDA (the JapanElectronic Industry Development Association) ((theJapan Electronic Industry Development Associa-tion), 1995), including:#100: Any question will be welcomed.~200: He kept calm in the face of greatdanger.#300: He is what is called "the man in thenews".~400: Japan registered a trade deficit of$101 million, reflecting the country's eco-nomic sluggishness, according to govern-ment figures.#500: I also went to the beach 2 weeksearlier.At an early stage of grammar acquisition, \ [Add i t ionof  New Pat terns \ ]  was primarily used to enrichthe set T of patterns, and many sentences were un-ambiguously and correctly translated.
At a laterstage, however, JEIDA sentences usually gave sev-eral translations, and \[Lexical izat ion\]  with care-ful assignment of weights was the most critical task.Although these sentences are intended to test a sys-tem's ability to translate one basic linguistic phe-nomenon in each simple sentence, the result wasstrong evidence for our claim.
Over 90% of JEIDAsentences were correctly translated.
Among the fail-ures were:~95: I see some stamps on the desk .#171: He is for the suggestion, but I 'magainst it.~244: She made him an excellent wife.#660: He painted the walls and the floorwhite.Some (prepositional nd sentential) attachment am-biguities needs to be resolved on the basis of seman-tic information, and scoping of coordinated struc-tures would have to be determined by using not onlycollocational patterns but also some measures of bal-ance and similarities among constituents.6 Conc lus ions  and  Future  WorkSome assumptions about patterns should be re-examined when we extend the definition of patterns.The notion of head constraints may have to be ex-tended into one of a set membership constraint if weneed to handle coordinated structures (Kaplan andMaxwell III, 1988).
Some light-verb phrases cannotbe correctly translated without "exchanging" severalfeature values between the verb and its object.
Asimilar problem has been found in be-verb phrases.Grammar acquisition and corpus integration arefundamental issues, but automation of these pro-cesses (Watanabe, 1993) is still not complete.
Devel-opment of an efficient ranslation algorithm, not justan efficient parsing algorithm, will make a significantcontribution to research on synchronized grammars,including STAGs and our PCFGs.AcknowledgmentsHideo Watanabe designed and implemented a pro-totype MT system for pattern-based CFGs, whileShiho Ogino developed a Japanese generator of the150prototype.
Their technical discussions and sugges-tions greatly helped me shape the idea of pattern-based CFGs.
I would also like to thank TaijiroTsutsumi, Masayuki Morohashi, Hiroshi Nomiyama,Tetsuya Nasukawa, and Naohiko Uramoto for theirvaluable comments.
Michael McDonald, as usual,helped me write the final version.Re ferencesAbeill@, A., Y. Schabes, and A. K. Joshi.
1990.
"Using Lexicalized Tags for Machine Translation".In Proc.
of the 13th International Conference onComputational Linguistics, volume 3, pages 1-6,Aug.Berwick, R .C .
1982.
"Computational Complex-ity and Lexical-Functional Grammar".
AmericanJournal of Computational Linguistics, pages 97-109, July-Dec.Brown, P. F., S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
"The Mathematics ofStatistical Machine Translation: Parametric Es-timation".
Computational Linguistics, 19(2):263-311, June.Dorr, B. J.
1993.
"Machine Translation: A Viewfrom the Lexicon".
The MIT Press, Cambridge,Mass.Earley, J.
1970.
"An Efficient Context-free Pars-ing Algorithm".
Communications of the ACM,6(8):94-102, February.Fujisaki, T., F. Jelinek, J. Cocke, E. Black, andT.
Nishino.
1989.
"A Probabilistie ParsingMethod for Sentence Disambiguation".
In Proc.of the International Workshop on Parsing Tech-nologies, pages 85-94, Pittsburgh, Aug.Furuse, O. and H. Iida.
1994.
"Cooperation be-tween Transfer and Analysis in Example-BasedFramework".
In Proc.
of the 15th InternationalConference on Computational Linguistics, pages645-651, Aug.Gazdar, G., G. K. Pullum, and I.
A.
Sag.
1985.
"Generalized Phrase Structure Grammar".
Har-vard University Press, Cambridge, Mass.Jensen, K. and G. E. Heidorn.
1983.
"The Fit-ted Parse: 100% Parsing Capability in a SyntacticGrammar of English".
In Proc.
of the 1st Confer-ence on Applied NLP, pages 93-98.Kaplan, R. and J. Bresnan.
1982.
"Lexical-Functional Grammar: A Formal System forGeneralized Grammatical Representation".
InJ.
Bresnan, editor, "Mental Representation ofGrammatical Relations".
MIT Press, Cambridge,Mass., pages 173-281.Kaplan, R. M. and J. T. Maxwell III.
1988.
"Constituent Coordination in Lexical-FunctionalGrammar".
In Proc.
of the 12th InternationalConference on Computational Linguistics, pages303-305, Aug.Maruyama, H. 1993.
"Pattern-Based Translation:Context-Free Transducer and Its Applications toPractical NLP".
In Proc.
of Natural Language Pa-cific Rim Symposium (NLPRS' 93), pages 232-237, Dec.Pollard, C. and I.
A.
Sag.
1987.
"An Information-Based Syntax and Semantics, Vol.1 Fundamen-tals".
CSLI Lecture Notes, Number 13.Pustejovsky, J.
1991.
"The Generative Lexi-con".
Computational Linguistics, 17(4):409-441,December.Sato, S. and M. Nagao.
1990.
"Toward Memory-based Translation".
In Proc.
of the 13th Interna-tional Conference on Computational Linguistics,pages 247-252, Helsinki, Aug.Schabes, Y., A. Abeill~, and A. K. Joshi.
1988.
"Parsing Algorithm with 'lexicalized' grammars:Application to tree adjoining rammars".
In Proc.of the 12th International Conference on Compu-tational Linguistics, pages 578-583, Aug.Schabes, Y. and R. C. Waters.
1995.
"Tree In-sertion Grammar: A Cubic-Time, Parsable For-malism that Lexicalizes Context-Free Grammarwithout Changing the Trees Produced".
Compu-tational Linguistics, 21(4):479-513, Dec.Shieber, S. M. and Y. Schabes.
1990.
"SynchronousTree-Adjoining Grammars".
In Proc.
of the 13thInternational Conference on Computational Lin-guistics, pages 253-258, August.Sumita, E. and H. Iida.
1991.
"Experiments andProspects of Example-Based Machine Transla-tion".
In Proc.
of the 29th Annual Meeting of theAssociation for Computational Linguistics, pages185-192, Berkeley, June.JEIDA (the Japan Electronic Industry Develop-ment Association).
1995.
"Evaluation Standardsfor Machine Translation Systems (in Japanese)".95-COMP-17, Tokyo.Tsujii, J. and K. Fujita.
1991.
"Lexical Transferbased on Bilingual Signs".
In Proc.
of the 5thEuropean ACL Conference.Vijay-Shanker, K. 1987.
"A Study of Tree Ad-joining Grammars".
Ph.D. thesis, Department ofComputer and Information Science, University ofPennsylvania.Vijay-Shanker, K. and Y. Schabes.
1992.
"Struc-ture Sharing in Lexicalized Tree-Adjoining Gram-mars".
In Proc.
of the 14th International Con-ference on Computational Linguistics, pages 205-211, Aug.Watanabe, H. 1993.
"A Method for Extract-ing Translation Patterns from Translation Exam-ples".
In Proc.
of 5th Intl.
Conf.
on Theoreticaland Methodological Issues in Machine Translationof Natural Languages, pages 292-301, July.151
