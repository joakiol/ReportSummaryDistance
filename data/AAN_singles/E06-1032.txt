Re-evaluating the Role of BLEU in Machine Translation ResearchChris Callison-Burch Miles Osborne Philipp KoehnSchool on InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWcallison-burch@ed.ac.ukAbstractWe argue that the machine translationcommunity is overly reliant on the Bleumachine translation evaluation metric.
Weshow that an improved Bleu score is nei-ther necessary nor sufficient for achievingan actual improvement in translation qual-ity, and give two significant counterex-amples to Bleu?s correlation with humanjudgments of quality.
This offers new po-tential for research which was previouslydeemed unpromising by an inability to im-prove upon Bleu scores.1 IntroductionOver the past five years progress in machine trans-lation, and to a lesser extent progress in naturallanguage generation tasks such as summarization,has been driven by optimizing against n-gram-based evaluation metrics such as Bleu (Papineniet al, 2002).
The statistical machine translationcommunity relies on the Bleu metric for the pur-poses of evaluating incremental system changesand optimizing systems through minimum er-ror rate training (Och, 2003).
Conference pa-pers routinely claim improvements in translationquality by reporting improved Bleu scores, whileneglecting to show any actual example transla-tions.
Workshops commonly compare systems us-ing Bleu scores, often without confirming theserankings through manual evaluation.
All theseuses of Bleu are predicated on the assumption thatit correlates with human judgments of translationquality, which has been shown to hold in manycases (Doddington, 2002; Coughlin, 2003).However, there is a question as to whether min-imizing the error rate with respect to Bleu does in-deed guarantee genuine translation improvements.If Bleu?s correlation with human judgments hasbeen overestimated, then the field needs to ask it-self whether it should continue to be driven byBleu to the extent that it currently is.
In thispaper we give a number of counterexamples forBleu?s correlation with human judgments.
Weshow that under some circumstances an improve-ment in Bleu is not sufficient to reflect a genuineimprovement in translation quality, and in othercircumstances that it is not necessary to improveBleu in order to achieve a noticeable improvementin translation quality.We argue that Bleu is insufficient by showingthat Bleu admits a huge amount of variation foridentically scored hypotheses.
Typically there aremillions of variations on a hypothesis translationthat receive the same Bleu score.
Because not allthese variations are equally grammatically or se-mantically plausible there are translations whichhave the same Bleu score but a worse human eval-uation.
We further illustrate that in practice ahigher Bleu score is not necessarily indicative ofbetter translation quality by giving two substantialexamples of Bleu vastly underestimating the trans-lation quality of systems.
Finally, we discuss ap-propriate uses for Bleu and suggest that for someresearch projects it may be preferable to use a fo-cused, manual evaluation instead.2 BLEU DetailedThe rationale behind the development of Bleu (Pa-pineni et al, 2002) is that human evaluation of ma-chine translation can be time consuming and ex-pensive.
An automatic evaluation metric, on theother hand, can be used for frequent tasks likemonitoring incremental system changes during de-velopment, which are seemingly infeasible in amanual evaluation setting.The way that Bleu and other automatic evalu-ation metrics work is to compare the output of amachine translation system against reference hu-man translations.
Machine translation evaluationmetrics differ from other metrics that use a refer-ence, like the word error rate metric that is used249Orejuela appeared calm as he was led to theAmerican plane which will take him to Mi-ami, Florida.Orejuela appeared calm while being escortedto the plane that would take him to Miami,Florida.Orejuela appeared calm as he was being ledto the American plane that was to carry himto Miami in Florida.Orejuela seemed quite calm as he was beingled to the American plane that would takehim to Miami in Florida.Appeared calm when he was taken tothe American plane, which will to Miami,Florida.Table 1: A set of four reference translations, anda hypothesis translation from the 2005 NIST MTEvaluationin speech recognition, because translations have adegree of variation in terms of word choice and interms of variant ordering of some phrases.Bleu attempts to capture allowable variation inword choice through the use of multiple referencetranslations (as proposed in Thompson (1991)).In order to overcome the problem of variation inphrase order, Bleu uses modified n-gram precisioninstead of WER?s more strict string edit distance.Bleu?s n-gram precision is modified to elimi-nate repetitions that occur across sentences.
Forexample, even though the bigram ?to Miami?
isrepeated across all four reference translations inTable 1, it is counted only once in a hypothesistranslation.
Table 2 shows the n-gram sets createdfrom the reference translations.Papineni et al (2002) calculate their modifiedprecision score, pn, for each n-gram length bysumming over the matches for every hypothesissentence S in the complete corpus C as:pn =?S?C?ngram?S Countmatched(ngram)?S?C?ngram?S Count(ngram)Counting punctuation marks as separate tokens,the hypothesis translation given in Table 1 has 15unigram matches, 10 bigram matches, 5 trigrammatches (these are shown in bold in Table 2), andthree 4-gram matches (not shown).
The hypoth-esis translation contains a total of 18 unigrams,17 bigrams, 16 trigrams, and 15 4-grams.
If thecomplete corpus consisted of this single sentence1-grams: American, Florida, Miami, Orejuela, ap-peared, as, being, calm, carry, escorted, he, him, in, led,plane, quite, seemed, take, that, the, to, to, to, was , was,which, while, will, would, ,, .2-grams: American plane, Florida ., Miami ,, Miamiin, Orejuela appeared, Orejuela seemed, appeared calm,as he, being escorted, being led, calm as, calm while, carryhim, escorted to, he was, him to, in Florida, led to, planethat, plane which, quite calm, seemed quite, take him, thatwas, that would, the American, the plane, to Miami, tocarry, to the, was being, was led, was to, which will, whilebeing, will take, would take, , Florida3-grams: American plane that, American plane which,Miami , Florida, Miami in Florida, Orejuela appearedcalm, Orejuela seemed quite, appeared calm as, appearedcalmwhile, as he was, being escorted to, being led to, calmas he, calm while being, carry him to, escorted to the, hewas being, he was led, him to Miami, in Florida ., led tothe, plane that was, plane that would, plane which will,quite calm as, seemed quite calm, take him to, that was to,that would take, the American plane, the plane that, toMiami ,, to Miami in, to carry him, to the American, tothe plane, was being led, was led to, was to carry, whichwill take, while being escorted, will take him, would takehim, , Florida .Table 2: The n-grams extracted from the refer-ence translations, with matches from the hypoth-esis translation in boldthen the modified precisions would be p1 = .83,p2 = .59, p3 = .31, and p4 = .2.
Each pn is com-bined and can be weighted by specifying a weightwn.
In practice each pn is generally assigned anequal weight.Because Bleu is precision based, and becauserecall is difficult to formulate over multiple refer-ence translations, a brevity penalty is introduced tocompensate for the possibility of proposing high-precision hypothesis translations which are tooshort.
The brevity penalty is calculated as:BP ={1 if c > re1?r/c if c ?
rwhere c is the length of the corpus of hypothesistranslations, and r is the effective reference corpuslength.1Thus, the Bleu score is calculated asBleu = BP ?
exp(N?n=1wn logpn)A Bleu score can range from 0 to 1, wherehigher scores indicate closer matches to the ref-erence translations, and where a score of 1 is as-signed to a hypothesis translation which exactly1The effective reference corpus length is calculated as thesum of the single reference translation from each set which isclosest to the hypothesis translation.250matches one of the reference translations.
A scoreof 1 is also assigned to a hypothesis translationwhich has matches for all its n-grams (up to themaximum n measured by Bleu) in the clipped ref-erence n-grams, and which has no brevity penalty.The primary reason that Bleu is viewed as a use-ful stand-in for manual evaluation is that it hasbeen shown to correlate with human judgments oftranslation quality.
Papineni et al (2002) showedthat Bleu correlated with human judgments inits rankings of five Chinese-to-English machinetranslation systems, and in its ability to distinguishbetween human and machine translations.
Bleu?scorrelation with human judgments has been fur-ther tested in the annual NIST Machine Transla-tion Evaluation exercise wherein Bleu?s rankingsof Arabic-to-English and Chinese-to-English sys-tems is verified by manual evaluation.In the next section we discuss theoretical rea-sons why Bleu may not always correlate with hu-man judgments.3 Variations Allowed By BLEUWhile Bleu attempts to capture allowable variationin translation, it goes much further than it should.In order to allow some amount of variant order inphrases, Bleu places no explicit constraints on theorder that matching n-grams occur in.
To allowvariation in word choice in translation Bleu usesmultiple reference translations, but puts very fewconstraints on how n-gram matches can be drawnfrom the multiple reference translations.
BecauseBleu is underconstrained in these ways, it allows atremendous amount of variation ?
far beyond whatcould reasonably be considered acceptable varia-tion in translation.In this section we examine various permutationsand substitutions allowed by Bleu.
We show thatfor an average hypothesis translation there are mil-lions of possible variants that would each receivea similar Bleu score.
We argue that because thenumber of translations that score the same is solarge, it is unlikely that all of them will be judgedto be identical in quality by human annotators.This means that it is possible to have items whichreceive identical Bleu scores but are judged by hu-mans to be worse.
It is also therefore possible tohave a higher Bleu score without any genuine im-provement in translation quality.
In Sections 3.1and 3.2 we examine ways of synthetically produc-ing such variant translations.3.1 Permuting phrasesOne way in which variation can be introduced isby permuting phrases within a hypothesis trans-lation.
A simple way of estimating a lower boundon the number of ways that phrases in a hypothesistranslation can be reordered is to examine bigrammismatches.
Phrases that are bracketed by thesebigram mismatch sites can be freely permuted be-cause reordering a hypothesis translation at thesepoints will not reduce the number of matching n-grams and thus will not reduce the overall Bleuscore.Here we denote bigram mismatches for the hy-pothesis translation given in Table 1 with verticalbars:Appeared calm | when | he was | taken |to the American plane | , | which will |to Miami , Florida .We can randomly produce other hypothesis trans-lations that have the same Bleu score but are rad-ically different from each other.
Because Bleuonly takes order into account through rewardingmatches of higher order n-grams, a hypothesissentence may be freely permuted around thesebigram mismatch sites and without reducing theBleu score.
Thus:which will | he was | , | when | taken |Appeared calm | to the American plane| to Miami , Florida .receives an identical score to the hypothesis trans-lation in Table 1.If b is the number of bigram matches in a hy-pothesis translation, and k is its length, then thereare(k ?
b)!
(1)possible ways to generate similarly scored itemsusing only the words in the hypothesis transla-tion.2 Thus for the example hypothesis transla-tion there are at least 40,320 different ways of per-muting the sentence and receiving a similar Bleuscore.
The number of permutations varies withrespect to sentence length and number of bigrammismatches.
Therefore as a hypothesis translationapproaches being an identical match to one of thereference translations, the amount of variance de-creases significantly.
So, as translations improve2Note that in some cases randomly permuting the sen-tence in this way may actually result in a greater number ofn-gram matches; however, one would not expect random per-mutation to increase the human evaluation.2510204060801001201  1e+10  1e+20  1e+30  1e+40  1e+50  1e+60  1e+70  1e+80SentenceLengthNumber of PermutationsFigure 1: Scatterplot of the length of each trans-lation against its number of possible permutationsdue to bigram mismatches for an entry in the 2005NIST MT Evalspurious variation goes down.
However, at today?slevels the amount of variation that Bleu admits isunacceptably high.
Figure 1 gives a scatterplotof each of the hypothesis translations produced bythe second best Bleu system from the 2005 NISTMT Evaluation.
The number of possible permuta-tions for some translations is greater than 1073.3.2 Drawing different items from thereference setIn addition to the factorial number of ways thatsimilarly scored Bleu items can be generatedby permuting phrases around bigram mismatchpoints, additional variation may be synthesizedby drawing different items from the reference n-grams.
For example, since the hypothesis trans-lation from Table 1 has a length of 18 with 15unigram matches, 10 bigram matches, 5 trigrammatches, and three 4-gram matches, we can arti-ficially construct an identically scored hypothesisby drawing an identical number of matching n-grams from the reference translations.
Thereforethe far less plausible:was being led to the | calm as he was |would take | carry him | seemed quite |when | takenwould receive the same Bleu score as the hypoth-esis translation from Table 1, even though humanjudges would assign it a much lower score.This problem is made worse by the fact thatBleu equally weights all items in the referencesentences (Babych and Hartley, 2004).
There-fore omitting content-bearing lexical items doesnot carry a greater penalty than omitting functionwords.The problem is further exacerbated by Bleu nothaving any facilities for matching synonyms orlexical variants.
Therefore words in the hypothesisthat did not appear in the references (such as whenand taken in the hypothesis from Table 1) can besubstituted with arbitrary words because they donot contribute towards the Bleu score.
Under Bleu,we could just as validly use the words black andhelicopters as we could when and taken.The lack of recall combined with naive tokenidentity means that there can be overlap betweensimilar items in the multiple reference transla-tions.
For example we can produce a translationwhich contains both the words carry and take eventhough they arise from the same source word.
Thechance of problems of this sort being introducedincreases as we add more reference translations.3.3 Implication: BLEU cannot guaranteecorrelation with human judgmentsBleu?s inability to distinguish between randomlygenerated variations in translation hints that it maynot correlate with human judgments of translationquality in some cases.
As the number of identi-cally scored variants goes up, the likelihood thatthey would all be judged equally plausible goesdown.
This is a theoretical point, and while thevariants are artificially constructed, it does high-light the fact that Bleu is quite a crude measure-ment of translation quality.A number of prominent factors contribute toBleu?s crudeness:?
Synonyms and paraphrases are only handledif they are in the set of multiple referencetranslations.?
The scores for words are equally weightedso missing out on content-bearing materialbrings no additional penalty.?
The brevity penalty is a stop-gap measure tocompensate for the fairly serious problem ofnot being able to calculate recall.Each of these failures contributes to an increasedamount of inappropriately indistinguishable trans-lations in the analysis presented above.Given that Bleu can theoretically assign equalscoring to translations of obvious different qual-ity, it is logical that a higher Bleu score may not252FluencyHow do you judge the fluency of this translation?5 = Flawless English4 = Good English3 = Non-native English2 = Disfluent English1 = IncomprehensibleAdequacyHow much of the meaning expressed in the refer-ence translation is also expressed in the hypothesistranslation?5 = All4 = Most3 = Much2 = Little1 = NoneTable 3: The scales for manually assigned ade-quacy and fluency scoresnecessarily be indicative of a genuine improve-ment in translation quality.
This begs the questionas to whether this is only a theoretical concern orwhether Bleu?s inadequacies can come into playin practice.
In the next section we give two signif-icant examples that show that Bleu can indeed failto correlate with human judgments in practice.4 Failures in Practice: the 2005 NISTMT Eval, and Systran v. SMTThe NIST Machine Translation Evaluation exer-cise has run annually for the past five years aspart of DARPA?s TIDES program.
The quality ofChinese-to-English and Arabic-to-English transla-tion systems is evaluated both by using Bleu scoreand by conducting a manual evaluation.
As such,the NIST MT Eval provides an excellent sourceof data that allows Bleu?s correlation with hu-man judgments to be verified.
Last year?s eval-uation exercise (Lee and Przybocki, 2005) wasstartling in that Bleu?s rankings of the Arabic-English translation systems failed to fully corre-spond to the manual evaluation.
In particular, theentry that was ranked 1st in the human evaluationwas ranked 6th by Bleu.
In this section we exam-ine Bleu?s failure to correctly rank this entry.The manual evaluation conducted for the NISTMT Eval is done by English speakers without ref-erence to the original Arabic or Chinese docu-ments.
Two judges assigned each sentence inIran has already stated that Kharazi?s state-ments to the conference because of the Jor-danian King Abdullah II in which he stoodaccused Iran of interfering in Iraqi affairs.n-gram matches: 27 unigrams, 20 bigrams,15 trigrams, and ten 4-gramshuman scores: Adequacy:3,2 Fluency:3,2Iran already announced that Kharrazi will notattend the conference because of the state-ments made by the Jordanian Monarch Ab-dullah II who has accused Iran of interferingin Iraqi affairs.n-gram matches: 24 unigrams, 19 bigrams,15 trigrams, and 12 4-gramshuman scores: Adequacy:5,4 Fluency:5,4Reference: Iran had already announcedKharazi would boycott the conference afterJordan?s King Abdullah II accused Iran ofmeddling in Iraq?s affairs.Table 4: Two hypothesis translations with similarBleu scores but different human scores, and one offour reference translationsthe hypothesis translations a subjective 1?5 scorealong two axes: adequacy and fluency (LDC,2005).
Table 3 gives the interpretations of thescores.
When first evaluating fluency, the judgesare shown only the hypothesis translation.
Theyare then shown a reference translation and areasked to judge the adequacy of the hypothesis sen-tences.Table 4 gives a comparison between the outputof the system that was ranked 2nd by Bleu3 (top)and of the entry that was ranked 6th in Bleu but1st in the human evaluation (bottom).
The exam-ple is interesting because the number of match-ing n-grams for the two hypothesis translationsis roughly similar but the human scores are quitedifferent.
The first hypothesis is less adequatebecause it fails to indicated that Kharazi is boy-cotting the conference, and because it inserts theword stood before accused which makes the Ab-dullah?s actions less clear.
The second hypothe-sis contains all of the information of the reference,but uses some synonyms and paraphrases whichwould not picked up on by Bleu: will not attendfor would boycott and interfering for meddling.3The output of the system that was ranked 1st by Bleu isnot publicly available.25322.533.540.38  0.4  0.42  0.44  0.46  0.48  0.5  0.52HumanScoreBleu ScoreAdequacyCorrelationFigure 2: Bleu scores plotted against human judg-ments of adequacy, with R2 = 0.14 when the out-lier entry is includedFigures 2 and 3 plot the average human scorefor each of the seven NIST entries against itsBleu score.
It is notable that one entry receiveda much higher human score than would be antici-pated from its low Bleu score.
The offending en-try was unusual in that it was not fully automaticmachine translation; instead the entry was aidedby monolingual English speakers selecting amongalternative automatic translations of phrases in theArabic source sentences and post-editing the result(Callison-Burch, 2005).
The remaining six entrieswere all fully automatic machine translation sys-tems; in fact, they were all phrase-based statisticalmachine translation system that had been trainedon the same parallel corpus and most used Bleu-based minimum error rate training (Och, 2003) tooptimize the weights of their log linear models?feature functions (Och and Ney, 2002).This opens the possibility that in order for Bleuto be valid only sufficiently similar systems shouldbe compared with one another.
For instance, whenmeasuring correlation using Pearson?s we get avery low correlation of R2 = 0.14 when the out-lier in Figure 2 is included, but a strong R2 = 0.87when it is excluded.
Similarly Figure 3 goes fromR2 = 0.002 to a much stronger R2 = 0.742.Systems which explore different areas of transla-tion space may produce output which has differ-ing characteristics, and might end up in differentregions of the human scores / Bleu score graph.We investigated this by performing a manualevaluation comparing the output of two statisti-cal machine translation systems with a rule-basedmachine translation, and seeing whether Bleu cor-22.533.540.38  0.4  0.42  0.44  0.46  0.48  0.5  0.52HumanScoreBleu ScoreFluencyCorrelationFigure 3: Bleu scores plotted against human judg-ments of fluency, with R2 = 0.002 when the out-lier entry is includedrectly ranked the systems.
We used Systran for therule-based system, and used the French-Englishportion of the Europarl corpus (Koehn, 2005) totrain the SMT systems and to evaluate all threesystems.
We built the first phrase-based SMT sys-tem with the complete set of Europarl data (14-15 million words per language), and optimized itsfeature functions using minimum error rate train-ing in the standard way (Koehn, 2004).
We eval-uated it and the Systran system with Bleu usinga set of 2,000 held out sentence pairs, using thesame normalization and tokenization schemes onboth systems?
output.
We then built a number ofSMT systems with various portions of the trainingcorpus, and selected one that was trained with 164of the data, which had a Bleu score that was closeto, but still higher than that for the rule-based sys-tem.We then performed a manual evaluation wherewe had three judges assign fluency and adequacyratings for the English translations of 300 Frenchsentences for each of the three systems.
Thesescores are plotted against the systems?
Bleu scoresin Figure 4.
The graph shows that the Bleu scorefor the rule-based system (Systran) vastly under-estimates its actual quality.
This serves as anothersignificant counter-example to Bleu?s correlationwith human judgments of translation quality, andfurther increases the concern that Bleu may not beappropriate for comparing systems which employdifferent translation strategies.25422.533.544.50.18  0.2  0.22  0.24  0.26  0.28  0.3HumanScoreBleu ScoreAdequacyFluencySMT System 1SMT System 2Rule-based System(Systran)Figure 4: Bleu scores plotted against humanjudgments of fluency and adequacy, showing thatBleu vastly underestimates the quality of a non-statistical system5 Related WorkA number of projects in the past have looked intoways of extending and improving the Bleu met-ric.
Doddington (2002) suggested changing Bleu?sweighted geometric average of n-gram matches toan arithmetic average, and calculating the brevitypenalty in a slightly different manner.
Hovy andRavichandra (2003) suggested increasing Bleu?ssensitivity to inappropriate phrase movement bymatching part-of-speech tag sequences against ref-erence translations in addition to Bleu?s n-grammatches.
Babych and Hartley (2004) extend Bleuby adding frequency weighting to lexical itemsthrough TF/IDF as a way of placing greater em-phasis on content-bearing words and phrases.Two alternative automatic translation evaluationmetrics do a much better job at incorporating re-call than Bleu does.
Melamed et al (2003) for-mulate a metric which measures translation accu-racy in terms of precision and recall directly ratherthan precision and a brevity penalty.
Banerjee andLavie (2005) introduce the Meteor metric, whichalso incorporates recall on the unigram level andfurther provides facilities incorporating stemming,and WordNet synonyms as a more flexible match.Lin and Hovy (2003) as well as Soricut and Brill(2004) present ways of extending the notion of n-gram co-occurrence statistics over multiple refer-ences, such as those used in Bleu, to other naturallanguage generation tasks such as summarization.Both these approaches potentially suffer from thesame weaknesses that Bleu has in machine trans-lation evaluation.Coughlin (2003) performs a large-scale inves-tigation of Bleu?s correlation with human judg-ments, and finds one example that fails to corre-late.
Her future work section suggests that shehas preliminary evidence that statistical machinetranslation systems receive a higher Bleu scorethan their non-n-gram-based counterparts.6 ConclusionsIn this paper we have shown theoretical and prac-tical evidence that Bleu may not correlate with hu-man judgment to the degree that it is currently be-lieved to do.
We have shown that Bleu?s rathercoarse model of allowable variation in translationcan mean that an improved Bleu score is not suffi-cient to reflect a genuine improvement in transla-tion quality.
We have further shown that it is notnecessary to receive a higher Bleu score in orderto be judged to have better translation quality byhuman subjects, as illustrated in the 2005 NISTMachine Translation Evaluation and our experi-ment manually evaluating Systran and SMT trans-lations.What conclusions can we draw from this?Should we give up on using Bleu entirely?
Wethink that the advantages of Bleu are still verystrong; automatic evaluation metrics are inexpen-sive, and do allow many tasks to be performedthat would otherwise be impossible.
The impor-tant thing therefore is to recognize which uses ofBleu are appropriate and which uses are not.Appropriate uses for Bleu include trackingbroad, incremental changes to a single system,comparing systems which employ similar trans-lation strategies (such as comparing phrase-basedstatistical machine translation systems with otherphrase-based statistical machine translation sys-tems), and using Bleu as an objective function tooptimize the values of parameters such as featureweights in log linear translation models, until abetter metric has been proposed.Inappropriate uses for Bleu include comparingsystems which employ radically different strate-gies (especially comparing phrase-based statisticalmachine translation systems against systems thatdo not employ similar n-gram-based approaches),trying to detect improvements for aspects of trans-lation that are not modeled well by Bleu, andmonitoring improvements that occur infrequentlywithin a test corpus.These comments do not apply solely to Bleu.255Meteor (Banerjee and Lavie, 2005), Precision andRecall (Melamed et al, 2003), and other such au-tomatic metrics may also be affected to a greateror lesser degree because they are all quite roughmeasures of translation similarity, and have inex-act models of allowable variation in translation.Finally, that the fact that Bleu?s correlation withhuman judgments has been drawn into questionmay warrant a re-examination of past work whichfailed to show improvements in Bleu.
For ex-ample, work which failed to detect improvementsin translation quality with the integration of wordsense disambiguation (Carpuat and Wu, 2005), orwork which attempted to integrate syntactic infor-mation but which failed to improve Bleu (Char-niak et al, 2003; Och et al, 2004) may deserve asecond look with a more targeted manual evalua-tion.AcknowledgmentsThe authors are grateful to Amittai Axelrod,Frank Keller, Beata Kouchnir, Jean Senellart, andMatthew Stone for their feedback on drafts of thispaper, and to Systran for providing translations ofthe Europarl test set.ReferencesBogdan Babych and Anthony Hartley.
2004.
Extend-ing the Bleu MT evaluation method with frequencyweightings.
In Proceedings of ACL.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, Ann Arbor, Michigan.Chris Callison-Burch.
2005.
Linear B system descrip-tion for the 2005 NIST MT evaluation exercise.
InProceedings of the NIST 2005 Machine TranslationEvaluation Workshop.Marine Carpuat and Dekai Wu.
2005.
Word sense dis-ambiguation vs. statistical machine translation.
InProceedings of ACL.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for machinetranslation.
In Proceedings of MT Summit IX.Deborah Coughlin.
2003.
Correlating automated andhuman assessments of machine translation quality.In Proceedings of MT Summit IX.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Human Language Technol-ogy: Notebook Proceedings, pages 128?132, SanDiego.Eduard Hovy and Deepak Ravichandra.
2003.
Holyand unholy grails.
Panel Discussion at MT SummitIX.Philipp Koehn.
2004.
Pharaoh: A beam search de-coder for phrase-based statistical machine transla-tion models.
In Proceedings of AMTA.Philipp Koehn.
2005.
A parallel corpus for statisticalmachine translation.
In Proceedings of MT-Summit.LDC.
2005.
Linguistic data annotation specification:Assessment of fluency and adequacy in translations.Revision 1.5.Audrey Lee and Mark Przybocki.
2005.
NIST 2005machine translation evaluation official results.
Of-ficial release of automatic evaluation scores for allsubmissions, August.Chin-Yew Lin and Ed Hovy.
2003.
Automatic eval-uation of summaries using n-gram co-occurrencestatistics.
In Proceedings of HLT-NAACL.Dan Melamed, Ryan Green, and Jospeh P. Turian.2003.
Precision and recall of machine translation.In Proceedings of HLT/NAACL.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.A smorgasbord of features for statistical machinetranslation.
In Proceedings of NAACL-04, Boston.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof ACL, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof ACL.Radu Soricut and Eric Brill.
2004.
A unified frame-work for automatic evaluation using n-gram co-occurrence statistics.
In Proceedings of ACL.Henry Thompson.
1991.
Automatic evaluation oftranslation quality: Outline of methodology and re-port on pilot experiment.
In (ISSCO) Proceedingsof the Evaluators Forum, pages 215?223, Geneva,Switzerland.256
