The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33?43,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsModeling coherence in ESOL learner textsHelen YannakoudakisComputer LaboratoryUniversity of CambridgeUnited KingdomHelen.Yannakoudakis@cl.cam.ac.ukTed BriscoeComputer LaboratoryUniversity of CambridgeUnited KingdomTed.Briscoe@cl.cam.ac.ukAbstractTo date, few attempts have been made to de-velop new methods and validate existing onesfor automatic evaluation of discourse coher-ence in the noisy domain of learner texts.We present the first systematic analysis ofseveral methods for assessing coherence un-der the framework of automated assessment(AA) of learner free-text responses.
We ex-amine the predictive power of different coher-ence models by measuring the effect on per-formance when combined with an AA systemthat achieves competitive results, but does notuse discourse coherence features, which arealso strong indicators of a learner?s level of at-tainment.
Additionally, we identify new tech-niques that outperform previously developedones and improve on the best published resultfor AA on a publically-available dataset of En-glish learner free-text examination scripts.1 IntroductionAutomated assessment (hereafter AA) systems ofEnglish learner text assign grades based on textualfeatures which attempt to balance evidence of writ-ing competence against evidence of performance er-rors.
Previous work has mostly treated AA as asupervised text classification or regression task.
Anumber of techniques have been investigated, in-cluding cosine similarity of feature vectors (Attaliand Burstein, 2006), often combined with dimen-sionality reduction techniques such as Latent Se-mantic Analysis (LSA) (Landauer et al, 2003), andgenerative machine learning models (Rudner andLiang, 2002) as well as discriminative ones (Yan-nakoudakis et al, 2011).
As multiple factors influ-ence the linguistic quality of texts, such systems ex-ploit features that correspond to different propertiesof texts, such as grammar, style, vocabulary usage,topic similarity, and discourse coherence and cohe-sion.Cohesion refers to the use of explicit linguisticcohesive devices (e.g., anaphora, lexical semanticrelatedness, discourse markers, etc.)
within a textthat can signal primarily suprasentential discourserelations between textual units (Halliday and Hasan,1976).
Cohesion is not the only mechanism of dis-course coherence, which may also be inferred frommeaning without presence of explicit linguistic cues.Coherence can be assessed locally in terms of tran-sitions between adjacent clauses, parentheticals, andother textual units capable of standing in discourserelations, or more globally in terms of the overalltopical coherence of text passages.There is a large body of work that has investi-gated a number of different coherence models onnews texts (e.g., Lin et al (2011), Elsner and Char-niak (2008), and Soricut and Marcu (2006)).
Re-cently, Pitler et al (2010) presented a detailed surveyof current techniques in coherence analysis of ex-tractive summaries.
To date, however, few attemptshave been made to develop new methods and vali-date existing ones for automatic evaluation of dis-course coherence and cohesion in the noisy domainof learner texts, where spelling and grammatical er-rors are common.Coherence quality is typically present in markingcriteria for evaluating learner texts, and it is iden-33tified by examiners as a determinant of the overallscore.
Thus we expect that adding a coherence met-ric to the feature set of an AA system would betterreflect the evaluation performed by examiners andimprove performance.
The goal of the experimentspresented in this paper is to measure the effect anumber of (previously-developed and new) coher-ence models have on performance when combinedwith an AA system that achieves competitive results,but does not use discourse coherence features.Our contribution is threefold: 1) we present thefirst systematic analysis of several methods for as-sessing discourse coherence in the framework ofAA of learner free-text responses, 2) we identifynew discourse features that serve as proxies for thelevel of (in)coherence in texts and outperform pre-viously developed techniques, and 3) we improvethe best results reported by Yannakoudakis et al(2011) on the publically available ?English as a Sec-ond or Other Language?
(ESOL) corpus of learnertexts (to date, this is the only public-domain corpusthat contains grades).
Finally, we explore the utilityof our best model for assessing the incoherent ?out-lier?
texts used in Yannakoudakis et al (2011).2 Experimental Design & BackgroundWe examine the predictive power of a number ofdifferent coherence models by measuring the effecton performance when combined with an AA systemthat achieves state-of-the-art results, but does notuse discourse coherence features.
Specifically, wedescribe a number of different experiments improv-ing on the AA system presented in Yannakoudakiset al (2011); AA is treated as a rank preferencesupervised learning problem and ranking SupportVector Machines (SVMs) (Joachims, 2002) are usedto explicitly model the grade relationships betweenscripts.
This system uses a number of different lin-guistic features that achieve good performance onthe AA task.
However, these features only focus onlexical and grammatical properties, as well as errorswithin individual sentences, ignoring discourse co-herence, which is also present in marking criteria forevaluating learner texts, as well as a strong indicatorof a writer?s understanding of a language.Also, in Yannakoudakis et al (2011), experimentsare presented that test the validity of the systemusing a number of automatically-created ?outlier?texts.
The results showed that the model is vulner-able to input where individually high-scoring sen-tences are randomly ordered within a text.
Failing toidentify such pathological cases makes AA systemsvulnerable to subversion by writers who understandsomething of its workings, thus posing a threat totheir validity.
For example, an examinee might learnby rote a set of well-formed sentences and repro-duce these in an exam in the knowledge that an AAsystem is not checking for prompt relevance or co-herence1.3 Dataset & Experimental SetupWe use the First Certificate in English (FCE) ESOLexamination scripts2 (upper-intermediate level as-sessment) described in detail in Yannakoudakis et al(2011), extracted from the Cambridge Learner Cor-pus3 (CLC).
The dataset consists of 1,238 texts be-tween 200 and 400 words produced by 1,238 distinctlearners in response to two different prompts.
Anoverall mark has been assigned in the range 1?40.For all experiments, we use a series of 5-foldcross-validation runs on 1,141 texts from the exami-nation year 2000 to evaluate performance as well asgeneralization of numerous models.
Moreover, weidentify the best model on year 2000 and we also testit on 97 texts from the examination year 2001, previ-ously used in Yannakoudakis et al (2011) to reportthe best published results.
Validating the results ona different examination year tests generalization tosome prompts not used in 2000, and also allows us totest correlation between examiners and the AA sys-tem.
Again, we treat AA as a rank preference learn-ing problem and use SVMs, utilizing the SVMlightpackage (Joachims, 2002), to facilitate comparisonwith Yannakoudakis et al (2011).4 Discourse CoherenceWe focus on the development and evaluation of (au-tomated) methods for assessing coherence in learner1Powers et al (2002) report the results of a related exper-iment with the AA system e-Rater, in which experts tried tosubvert the system by submitting essays they believed would beinaccurately scored.2http://ilexir.co.uk/applications/clc-fce-dataset/3http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/34texts under the framework of AA.
Most of the meth-ods we investigate require syntactic analysis.
As inYannakoudakis et al (2011), we analyze all texts us-ing the RASP toolkit (Briscoe et al, 2006)4.4.1 ?Superficial?
ProxiesIn this section we introduce diverse classes of ?su-perficial?
cohesive features that serve as proxies forcoherence.
Surface text properties have been as-sessed in the framework of automatic summary eval-uation (Pitler et al, 2010), and have been shown tosignificantly correlate with the fluency of machine-translated sentences (Chae and Nenkova, 2009).4.1.1 Part-of-Speech (POS) DistributionThe AA system described in Yannakoudakis etal.
(2011) exploited features based on POS tag se-quences, but did not consider the distribution of POStypes across grades.
In coherent texts, constituentclauses and sentences are related and depend on eachother for their interpretation.
Anaphors such as pro-nouns link the current sentence to those where theentities were previously mentioned.
Pronouns canbe directly related to (lack of) coherence and makeintuitive sense as cohesive devices.
We compute thenumber of pronouns in a text and use it as a shallowfeature for capturing coherence.4.1.2 Discourse ConnectivesDiscourse connectives (such as but or because) re-late propositions expressed by different clauses orsentences.
The presence of such items in a textshould be indicative of (better) coherence.
We thuscompute a number of shallow cohesive features asproxies for coherence, based on fixed lists of wordsbelonging to the following categories: (a) Addition(e.g., additionally), (b) Comparison (e.g., likewise),(c) Contrast (e.g., whereas) and (d) Conclusion (e.g.,therefore), and use the frequencies of these four cat-egories as features.4.1.3 Word LengthThe previous AA system treated script length asa normalizing feature, but otherwise avoided such?superficial?
proxies of text quality.
However, manycohesive words are longer than average, especiallyfor the closed-class functional component of English4http://ilexir.co.uk/applications/rasp/vocabulary.
We thus assess the minimum, maximumand average word length as a superficial proxy forcoherence.4.2 Semantic SimilarityWe explore the utility of inter-sentential featuretypes for assessing discourse coherence.
Among thefeatures used in Yannakoudakis et al (2011), noneexplicitly captures coherence and none models inter-sentential relationships.
Incremental Semantic anal-ysis (ISA) (Baroni et al, 2007) is a word-level dis-tributional model that induces a semantic space frominput texts.
ISA is a fully-incremental variation ofRandom Indexing (RI) (Sahlgren, 2005), which canefficiently capture second-order effects in commonwith other dimensionality-reduction methods basedon singular value decomposition, but does not relyon stoplists or global statistics for weighting pur-poses.Utilizing the S-Space package (Jurgens andStevens, 2010), we trained an ISA model5 using asubset of ukWaC (Ferraresi et al, 2008), a large cor-pus of English containing more than 2 billion tokens.We used the POS tagger lexicon provided with theRASP system to discard documents whose propor-tion of valid English words to total words is less than0.4; 78,000 documents were extracted in total andwere then preprocessed replacing URLs, email ad-dresses, IP addresses, numbers and emoticons withspecial markers.
To measure local coherence we de-fine the similarity between two sentences si and si+1as the maximum cosine similarity between the his-tory vectors of the words they contain.
The overallcoherence of a text T is then measured by taking themean of all sentence-pair scores:coherence(T ) =?n?1i=1 maxk,j sim(ski , sji+1)n?
1(1)where sim(ski , sji+1) is the cosine similarity betweenthe history vectors of the kth word in si and thejth word in si+1, and n is the total number ofsentences6.
We investigate the efficacy of ISA byadding this coherence score, as well as the maximum5The parameters of our ISA model are fairly standard: 1800dimensions, a context window of 3 words, impact rate i =0.0003 and decay rate km = 50.6We exclude articles, conjunctions, prepositions and auxil-iary verbs from the calculation of sentence similarity.35sim value found over the entire text, to the vectorsof features associated with a text.
The hypothesisis that the degree of semantic relatedness betweenadjoining sentences serves as a proxy for local dis-course coherence; that is, coherent text units containsemantically-related words.Higgins et al (2004) and Higgins and Burstein(2007) use RI to determine the semantic similaritybetween sentences of same/different discourse seg-ments (e.g., from the essay thesis and conclusion, orbetween sentences and the essay prompt), and assessthe percentage of sentences that are correctly clas-sified as related or unrelated.
The main differencesfrom our approach are that we assess the utility of se-mantic space models for predicting the overall gradefor a text, in contrast to binary classification at thesentence-level, and we use ISA rather than RI7.4.3 Entity-based CoherenceThe entity-based coherence model, proposed byBarzilay and Lapata (2008), is one of the most pop-ular statistical models of inter-sentential coherence,and learns coherence properties similar to those em-ployed by Centering Theory (Grosz et al, 1995).Local coherence is modeled on the basis of se-quences of entity mentions that are labeled withtheir syntactic roles (e.g., subject, object).
We con-struct the entity grids using the Brown CoherenceToolkit8,9 (Elsner and Charniak, 2011b), and use asfeatures the probabilities of different entity transi-tion types, defined in terms of their role in adja-cent sentences10.
Burstein et al (2010) show howthe entity-grid can be used to discriminate high-coherence from low-coherence learner texts.
Themain difference with our approach is that we eval-uate the entity-grid model in the context of AA textgrading, rather than binary classification.7We also used RI in addition to ISA, and found that it didnot yield significantly different results.
In particular, we traineda RI model with 2,000 dimensions and a context window of 3on the same ukWaC data.
Below we only report results for thefully-incremental ISA model.8https://bitbucket.org/melsner/browncoherence9The tool does not perform full coreference resolution; in-stead, coreference is approximated by linking entities that sharea head noun.10We represent entities with specified roles (Subject, Object,Neither, Absent), use transition probabilities of length 2, 3 and4, and a salience option of 2.4.4 Pronoun Coreference ModelPronominal anaphora is another important aspectof coherence.
Charniak and Elsner (2009) presentan unsupervised generative model of pronominalanaphora for coherence modeling.
In their imple-mentation, they model each pronoun as generated byan antecedent somewhere in the previous two sen-tences.
If a ?good?
antecedent is found, the probabil-ity of a pronoun will be high; otherwise, the proba-bility will be low.
The overall probability of a textis then calculated as the probability of the result-ing sequence of pronoun assignments.
In our ex-periments, we use the pre-trained model distributedby Charniak and Elsner (2009) for news text to esti-mate the probability of a text and include it as a fea-ture.
However, this model is trained on high-qualitytexts, so performance may deteriorate when appliedto learner texts.
It is not obvious how to train sucha model on learner texts and we leave this for futureresearch.4.5 Discourse-new ModelElsner and Charniak (2008) apply a discourse-newclassifier to model coherence.
Their classifier dis-tinguishes NPs whose referents have not been pre-viously mentioned in the discourse from those thathave been already introduced, using a number ofsyntactic and lexical features.
To model coher-ence, they assign each NP in a text a label Lnp ?
{new, old}11, and calculate the probability of a textas ?np:NPsP (Lnp|np).
Again, we use the pre-trained model distributed by Charniak and Elsner(2009) for news text to find the probability of a textfollowing Elsner and Charniak (2008) and include itas a feature.4.6 IBM Coherence ModelSoricut and Marcu (2006) adapted the IBM model1 (Brown et al, 1994) used in machine translation(MT) to model local discourse coherence.
The intu-ition behind the IBM model in MT is that the use ofcertain words in a source language is likely to trig-ger the use of certain words in a target language.Instead, they hypothesized that the use of certainwords in a sentence tends to trigger the use of cer-tain words in an adjoining sentence.
In contrast to11NPs with the same head are considered to be coreferent.36semantic space models such as ISA or RI (discussedabove), this method models the intuition that localcoherence is signaled by the identification of wordco-occurrence patterns across adjacent sentences.We compute two features introduced by Soricutand Marcu (2006): the forward likelihood and thebackward likelihood.
The first refers to the likeli-hood of observing the words in sentence si+1 condi-tioned on si, and the latter to the likelihood of ob-serving the words in si conditioned on si+1.
Weextract 3 million adjacent sentences from ukWaC12,and use the GIZA++ (Och and Ney, 2000) imple-mentation of IBM model 1 to obtain the probabili-ties of recurring patterns.
The forward and backwardprobabilities are calculated over the entire text, andtheir values are used as features in our feature vec-tors13.
We further extend the above model and incor-porate syntactic aspects of text coherence by train-ing on POS tags instead of lexical items.
We try tomodel the intuition that local coherence is signaledby the identification of POS co-occurrence patternsacross adjacent sentences, where the use of certainPOS tags in a sentence tends to trigger the use ofother POS tags in an adjacent sentence.
We analyze3 million adjacent sentences using the RASP POStagger and train the same IBM model to obtain theprobabilities of recurring POS patterns.4.7 Lemma/POS Cosine SimilarityA simple method of incorporating (syntactic) as-pects of text coherence is to use cosine similaritybetween vectors of lemma and/or POS-tag counts inadjacent sentences.
We experiment with both: eachsentence is represented by a vector whose dimen-sion depends on the total number of lemmas/POS-types.
The sentence vectors are weighted usinglemma/POS frequency, and the cosine similarity be-tween adjacent sentences is calculated.
The coher-ence of a text T is then calculated as the averagevalue of cosine similarity over the entire text14:coherence(T ) =?n?1i=1 sim(si, si+1)n?
1(2)12We use the same subset of documents as the ones used totrain our ISA model in Section 4.2.13Pitler et al (2010) have also investigated the IBM model tomeasure text quality in automatically-generated texts.14Pitler et al (2010) use POS cosine similarity to measurecontinuity in automatically-generated texts.4.8 Locally-Weighted Bag-of-WordsThe popular bag-of-words (BOW) assumption rep-resents a text as a histogram of word occurrences.While computationally efficient, such a represen-tation is unable to maintain any sequential infor-mation.
The locally-weighted bag-of-words (LOW-BOW) framework, introduced by Lebanon et al(2007), is a sequentially-sensitive alternative toBOW.
In BOW, we represent a text as a histogramover the vocabulary used to generate that text.
InLOWBOW, a text is represented by a set of lo-cal histograms computed across the whole text, butsmoothed by kernels centered on different locations.More specifically, a smoothed characterizationof the local histogram is obtained by integrating alength-normalized document with respect to a non-uniform measure that is concentrated around a par-ticular location ?
?
[0, 1].
In accordance with thestatistical literature on non-parametric smoothing,we refer to such a measure as a smoothing kernel.The kernel parameters ?
and ?
specify the local his-togram?s position in the text (i.e., where it is cen-tered) and its scale (i.e., to what extent it is smoothedover the surrounding region) respectively.
In con-trast to BOW or n-grams, which keep track of fre-quently occurring patterns independent of their po-sitions, this representation is able to robustly capturemedium and long range sequential trends in a text bykeeping track of changes in the histograms from itsbeginning to end.Geometrically, LOWBOW uses local smoothingto embed texts as smooth curves in the multinomialsimplex.
These curves summarize the progressionof semantic and/or statistical trends through the text.By varying the amount of smoothing we obtain afamily of sequential representations possessing dif-ferent sequential resolutions or scales.
Low resolu-tion representations capture topic trends and shiftswhile ignoring finer details.
High resolution repre-sentations capture fine sequential details but make itdifficult to grasp the general trends within the text15.Since coherence involves both cohesive lexicaldevices and sequential progression within a text, webelieve that LOWBOW can be used to assess the se-quential content and the global structure and coher-15For more details regarding LOWBOW and its geometricproperties see Lebanon et al (2007).37ence of texts.
We use a publically-available LOW-BOW implementation16 to create local histogramsover word unigrams.
For the LOWBOW kernelsmoothing function (see above), we use the Gaus-sian probability density function restricted to [0, 1]and re-normalized, and a smoothing ?
value of 0.02.Additionally, we consider a total number of 9 localhistograms (discourse segments).
We further extendthe above model and incorporate syntactic aspects oftext coherence by using local histograms over POSunigrams.
This representation is able to capture se-quential trends abstracted into POS tags.
We tryto model the hypothesis that coherence is signaledby sequential, mostly inter-sentential progression ofPOS types.Since each text is represented by a set of localhistrograms/vectors, and standard SVM kernels can-not work with such input spaces, we use instead akernel defined over sets of vectors: the diffusionkernel (Lafferty and Lebanon, 2005) compares lo-cal histograms in a one-to-one fashion (i.e., his-tograms at the same locations are compared to eachother), and has proven to be useful for related tasks(Lebanon et al, 2007; Escalante et al, 2011).
To thebest of our knowledge, LOWBOW representationshave not been investigated for coherence evaluation(under the AA framework).
So far, they have beenapplied to discourse segmentation (AMIDA, 2007),text categorization (Lebanon et al, 2007), and au-thorship attribution (Escalante et al, 2011).5 EvaluationWe examine the predictive power of each of the co-herence models/features described in Section 4 bymeasuring the effect on performance when com-bined with an AA system that achieves state-of-the-art results on the FCE dataset, but does not use dis-course coherence features.
In particular, we use thesystem described in Yannakoudakis et al (2011) asour baseline AA system.
Discourse coherence is astrong indicator of thorough knowledge of a secondlanguage and thus we expect coherence features tofurther improve performance of AA systems.We evaluate the grade predictions of our mod-els against the gold standard grades in the datasetusing Pearson?s product-moment correlation coeffi-16http://goo.gl/yQ0Q0cient (r) and Spearman?s rank correlation coefficient(?)
as is standard in AA research (Briscoe et al,2010).
Table 1 gives results obtained by augmentingthe baseline model with each of the coherence fea-tures described above.
In each of these experiments,we perform 5-fold cross-validation17 using all 1,141texts from the exam year 2000 (see Section 3).Most of the resulting models have minimal ef-fect on performance18.
However, word length, ISA,LOWBOWlex, and the IBM modelPOSf derived mod-els all improve performance, while larger differ-ences are observed in r. The highest performance?
0.675 and 0.678 ?
is obtained with ISA, while thesecond best feature is word length.
The entity-grid,the pronoun model and the discourse-new model donot improve on the baseline.
Although these mod-els have been successfully used as components instate-of-the-art systems for discriminating coherentfrom incoherent news documents (Elsner and Char-niak, 2011b), and the entity-grid model has alsobeen successfully applied to learner text (Bursteinet al, 2010), they seem to have minimal impacton performance, while the discourse-new model de-creases ?
by?0.01.
On the other hand, LOWBOWlexand LOWBOWPOS give an increase in performance,which confirms our hypothesis that local histogramsare useful.
Also, the former seems to performslightly better than the latter.Our adapted version of the IBM model ?
IBMmodelPOS ?
performs better than its lexicalized ver-sion, which does not have an impact on perfor-mance, while larger differences are observed in r.Additionally, the increase in performance is largerthan the one obtained with the entity-grid, pro-noun or discourse-new model.
The forward ver-sion of IBM modelPOS seems to perform slightlybetter than the backward one, while the results arecomparable to LOWBOWPOS and outperformed byLOWBOWlex.
The rest of the models do not performas well; the number of pronouns or discourse con-nectives gives low results, while lemma and POS co-sine similarity between adjacent sentences are also17We compute mean values of correlation coefficients by firstapplying the r-to-Z Fisher transformation, and then using theFisher weighted mean correlation coefficient (Faller, 1981).18Significance tests in averaged correlations are omitted asvariable estimates are produced, whose variance is hard to beestimated unbiasedly.38r ?0 Baseline 0.651 0.6701 POS distr.
0.653 0.6702 Disc.
connectives 0.648 0.6683 Word length 0.667 0.6764 ISA 0.675 0.6785 EGrid 0.650 0.6686 Pronoun 0.650 0.6687 Disc-new 0.646 0.6628 LOWBOWlex 0.663 0.6779 LOWBOWPOS 0.659 0.67410 IBM modellexf 0.649 0.66811 IBM modellexb 0.649 0.66712 IBM modelPOSf 0.661 0.67213 IBM modelPOSb 0.658 0.66914 Lemma cosine 0.651 0.66715 POS cosine 0.650 0.66516 5+6+7+10+11 0.648 0.66517 All 0.677 0.671Table 1: 5-fold cross-validation performance on textsfrom year 2000 when adding different coherence featureson top of the baseline AA system.among the weakest predictors.Elsner and Charniak (2011b) have shown thatcombining the entity-grid with the pronoun,discourse-new and lexicalized IBM models givesstate-of-the-art results for discriminating news docu-ments and their random permutations.
We also com-bine these models and assess their performance un-der the AA framework.
Row 16 of Table 1 showsthat the combination does not give an improvementover the individual models.
Moreover, combiningall feature classes together in row 17 does not yieldhigher results than those obtained with ISA, while ?is no better than the baseline.In the following experiments, we evaluate the bestmodel identified on year 2000 on a set of 97 textsfrom the exam year 2001, previously used in Yan-nakoudakis et al (2011) to report results of the fi-nal best system.
Validating the model on a differentexam year also shows us the extent to which it gen-eralizes between years.
Table 2 presents the results.The published correlations on this dataset are 0.741and 0.773 r and ?
respectively.
Adding ISA on topof the previous system significantly improves19 the19Calculated using one-tailed tests for the difference betweenr ?Baseline 0.741 0.773ISA 0.749 0.790?Table 2: Performance on the exam scripts drawn from theexamination year 2001. ?
indicates a significant improve-ment at ?
= 0.05.published results on the 2001 texts, getting closer tothe upper-bound.
The upper-bound on this dataset20is 0.796 and 0.792 r and ?
respectively, calculatedby taking the average correlation between the FCEgrades and the ones provided by 4 senior ESOL ex-aminers21.
Table 3 also presents the average corre-lation between our extended AA system?s predictedgrades and the 4 examiners?
grades, in addition tothe original FCE grades from the dataset.
Again,our extended model improves over the baseline.Finally, we explore the utility of our best modelfor assessing the publically available ?outlier?
textsused in Yannakoudakis et al (2011).
The previousAA system is unable to downgrade appropriately?outlier?
scripts containing individually high-scoringsentences with poor overall coherence, created byrandomly ordering a set of highly-marked texts.
Totest our best system, we train an SVM rank prefer-ence model with the ISA-derived coherence feature,which can explicitly capture such sequential trends.A generic model for flagging putative ?outlier?
texts?
whose predicted score is lower than a predefinedthreshold ?
for manual checking might be used asthe first stage of a deployed AA system.
The ISAmodel improves r and ?
by 0.320 and 0.463 respec-tively for predicting a score on this type of ?outlier?texts and their original version (Table 4).6 Analysis & DiscussionIn the previous section, we evaluated various co-hesion and coherence features on learner data, andfound different patterns of performance compared tothose previously reported on news texts (see Section7 for more details).
Although most of the models ex-amined gave a minimal effect on AA performance,ISA, LOWBOWlex, IBM modelPOSf and word lengthdependent correlations (Williams, 1959; Steiger, 1980).20See Yannakoudakis et al (2011) for details.21The examiners?
scores are also distributed with the FCEdataset.39r ?Baseline 0.723 0.721ISA 0.727 0.736Table 3: Average correlation between the AA model, theFCE dataset grades, and 4 examiners on the exam scriptsfrom year 2000.r ?Baseline 0.08 0.163ISA 0.400 0.626Table 4: Performance of the ISA AA model on outliers.gave a clear improvement in correlation, with largerdifferences in r. Our results indicate that coherencemetrics further improve the performance of a com-petitive AA system.
More specifically, we found theISA-derived feature to be the most effective contrib-utor to the prediction of text quality.
This suggeststhat incoherence in FCE texts might be due to topicdiscontinuities.
Also, the improvement obtained byLOWBOW suggests that patterns of sequential pro-gression within a text can be useful: coherent textsappear to use similar token distributions at similarlocations across different documents.The word length feature was successfully used asa proxy for coherence, perhaps because many cohe-sive words are longer than average.
However, sucha feature can also capture further aspects of texts,such as lexical complexity, so further investigationis needed to identify the extent to which it measuresdifferent properties.
On the other hand, the minimaleffect of the entity-grid, pronoun and discourse-newmodel suggests that infelicitous use of pronominalforms or sequences of entities may not be an issuein FCE texts.
Preliminary investigation of the scriptsshowed that learners tend to repeat the same entitynames or descriptions rather than use pronouns orshorter descriptions.A possible explanation for the difference in per-formance between the lexicalized and POS IBMmodel is that the latter abstracts away from lexi-cal information and thus avoids misspellings andreduces sparsity.
Also, our discourse connectiveclasses do not seem to have a predictive power.
Thismay be because our manually-built word lists do nothave sufficient coverage.7 Previous WorkComparatively few metrics have been investigatedfor evaluating coherence in (ESOL) learner texts.Miltsakaki and Kukich (2004) employ e-Rater (At-tali and Burstein, 2006), an essay scoring system,and show that Centering Theory?s Rough-Shift tran-sitions (Grosz et al, 1995) contribute significantly tothe assessment of learner texts.
Higgins et al (2004)and Higgins and Burstein (2007) use RI to deter-mine the semantic similarity between sentences ofsame/different discourse segments.
Their model isbased on a number of different semantic similarityscores and assesses the percentage of sentences thatare correctly classified as (un)related.
Among theirresults, they found that it is hard to beat the baseline(as 98.1% of the sentences were annotated as ?highlyrelated?)
and identify sentences which are not relatedto other ones in the same discourse segment.
Wedemonstrate that the related fully-incremental ISAmodel can be used to improve AA grading accuracyon the FCE dataset, as opposed to classifying the(non-)relatedness of sentences.Burstein et al (2010) show how the entity-gridcan be used to discriminate high-coherence fromlow-coherence learner texts.
They augment thismodel with additional features related to writingquality and word usage, and show a positive effectin performance for automated coherence predictionof student essays of different populations.
On theFCE dataset used here, entity-grids do not improveAA grading accuracy.
This may be because the textsare shorter or because grading is a more difficult taskthan binary classification.
Application of their aug-mented entity-grid model to FCE texts would be aninteresting avenue for future research.Foltz et al (1998) examine local coherence intextbooks and articles using Latent Semantic Anal-ysis (LSA) (Landauer et al, 2003).
They assess se-mantic relatedness using vector-based similarity be-tween adjacent sentences.
They argue that LSA maybe more appropriate for comparing the relative qual-ity of texts; for determining the overall text coher-ence it may be difficult to set a criterion for the co-herence value since it depends on a variety of dif-ferent factors, such as the size of the text units to becompared.
Nevertheless, our results show that ISA,a similar distributional semantic model with dimen-40sionality reduction, improves FCE grading accuracy.Barzilay and Lee (2004) implement lexicalizedcontent models that represent global text proper-ties on news articles and narratives using HiddenMarkov Models (HMMs).
In the HMM, states cor-respond to distinct topics, and transitions betweenstates represent the probability of moving from onetopic to another.
This approach has the advantageof capturing the order in which different topics ap-pear in texts; however, the HMMs are highly domainspecific and would probably need retraining for eachdistinct essay prompt.Soricut and Marcu (2006) use a log-linear modelthat combines local and global models of coher-ence and show that it outperforms each of the in-dividual ones on news articles and accident reports.Their global model is based on the document con-tent model proposed by Barzilay and Lee (2004).Their local model of discourse coherence is basedon the entity-grid (Barzilay and Lapata, 2008), aswell as on the lexicalized IBM model (see Section4.6 above); we have experimented with both, andshowed that they have a minimal effect on gradingperformance with the FCE dataset.Elsner and Charniak (2008;2011a) apply adiscourse-new classifier and a pronoun coreferencesystem to model coherence (see Section 4) on dia-logue and news texts.
They found that combiningthese models with the entity-grid achieves state-of-the-art performance.
We found that such a combina-tion, as well as the individual models do not performwell for grading the FCE texts.Recently, Elsner and Charniak (2011a) proposed avariation of the entity-grid intended to integrate top-ical information.
They use Latent Dirichlet Alloca-tion (Blei et al, 2003) to learn topic-to-word distri-butions, and model coherence by generalizing the bi-nary history features of the entity-grid and comput-ing a real-valued feature which represents the simi-larity between an entity and the subject(s) of the pre-vious sentence.
Also, Lin et al (2011) proposed amodel that assesses the coherence of a text based ondiscourse relation transitions.
The underlying ideais that coherent texts exhibit measurable preferencesfor specific intra- and inter-discourse relation order-ing.
They found their model to be complementary tothe entity-grid, as it encodes the notion of preferen-tial ordering of discourse relations, and thus tackleslocal coherence from a different perspective.
Apply-ing the above models to AA on learner texts wouldalso be an interesting avenue for future work.8 ConclusionWe presented the first systematic analysis of a widevariety of models for assessing discourse coherenceon learner data, and evaluated their individual per-formance as well as their combinations for the AAgrading task.
We adapted the LOWBOW model forassessing sequential content in texts, and showedevidence supporting our hypothesis that local his-tograms are useful.
We also successfully adaptedISA, an efficient and incremental variant distribu-tional semantic model, to this task.
ISA, LOWBOW,the POS IBM model and word length are the best in-dividual features for assessing coherence.A significant improvement over the AA systempresented in Yannakoudakis et al (2011) and thebest published result on the FCE dataset was ob-tained by augmenting the system with an ISA-basedlocal coherence feature.
However, it is quite likelythat further experimentation with LOWBOW fea-tures, given the large range of possible parametersettings, would yield better results too.We also explored the robustness of the ISA modelof local coherence on ?outlier?
texts and achievedmuch better correlations with the examiner?s gradesfor these texts in the FCE dataset.
This should facil-itate development of an automated system to detectessays consisting of high-quality but incoherent se-quences of sentences.All our results are specific to ESOL FCE texts andmay not generalize to other genres or ESOL attain-ment levels.
Future work should also investigate awider range of (learner) texts and further coherencemodels, such as that of Elsner and Charniak (2011a)and Lin et al (2011).AcknowledgmentsWe are grateful to Cambridge ESOL, a divisionof Cambridge Assessment, for supporting this re-search.
We would like to thank Marek Rei and ?is-tein Andersen for their valuable comments and sug-gestions, Yi Mao for giving us access to her code,as well as the anonymous reviewers for their usefulfeedback.41ReferencesAMIDA.
2007.
Augmented multi-party interactionwith distance access.
Available from www.
amidapro-ject.org/, AMIDA Report.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v.2.
Journal of Technology, Learn-ing, and Assessment, 4(3):1?30.Marco Baroni, Alessandro Lenci, and Luca Onnis.
2007.ISA meets Lara: An incremental word space model forcognitively plausible simulations of semantic learning.In Proceedings of the Association for ComputationalLinguistics.Regina Barzilay and Mirella Lapata.
2008.
ModelingLocal Coherence: An Entity-Based Approach.
Com-putational Linguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the COLING/ACL, volume 6.Ted Briscoe, Ben Medlock, and ?istein Andersen.
2010.Automated assessment of ESOL free text examina-tions.
Technical Report UCAM-CL-TR-790, Univer-sity of Cambridge, Computer Laboratory, November.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1994.
The mathe-matic of statistical machine translation: Parameter es-timation.
Computational linguistics, 19(2):263?311.Jill Burstein, Joel Tetreault, and Slava Andreyev.
2010.Using entity-based features to model coherence in stu-dent essays.
In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 681?684.Jieun Chae and Ani Nenkova.
2009.
Predicting the flu-ency of text with shallow structural features: case stud-ies of machine translation and human-written text.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 139?147.Eugene Charniak and Micha Elsner.
2009.
EM works forpronoun anaphora resolution.
In Proceedings of the12th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 148?156.Micha Elsner and Eugene Charniak.
2008.
Coreference-inspired coherence modeling.
In Proceedings of the46th Annual Meeting of the Association for Computa-tional Linguistics on Human Language Technologies,pages 41?44.Micha Elsner and Eugene Charniak.
2011a.
Disentan-gling chat with local coherence models.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1179?1189.Micha Elsner and Eugene Charniak.
2011b.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 125?129.Hugo J. Escalante, Thamar Solorio, and Manuel Montes-y Go?mez.
2011.
Local Histograms of Character N-grams for Authorship Attribution.
In Proceedings ofthe 49th Annual Meeting on Association for Computa-tional Linguistics, pages 288?298.Alan J. Faller.
1981.
An Average Correlation Coeffi-cient.
Journal of Applied Meteorology.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of English.In S. Evert, A. Kilgarriff, and S. Sharoff, editors, Pro-ceedings of the 4th Web as Corpus Workshop.Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-dauer.
1998.
The measurement of textual coherencewith latent semantic analysis.
Discourse processes,25(2):285?308.Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational linguistics,21(2):203?225.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English .
Longman Pub Group.Derrick Higgins and Jill Burstein.
2007.
Sentence sim-ilarity measures for essay coherence.
In Proceedingsof the 7th International Workshop on ComputationalSemantics, pages 1?12.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects of co-herence in student essays.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the ACMConference on Knowledge Discovery and Data Min-ing, pages 133?142.David Jurgens and Keith Stevens.
2010.
The S-Spacepackage: an open source package for word space mod-els.
In Proceedings of the Association for Computa-tional Linguistics 2010 System Demonstrations, pages30?35.42John Lafferty and Guy Lebanon.
2005.
Diffusion kernelson statistical manifolds.
Journal of Machine LearningResearch, 6:129?163.Thomas K. Landauer, Darrell Laham, and Peter W. Foltz.2003.
Automated scoring and annotation of essayswith the Intelligent Essay Assessor.
In M.D.
Shermisand J.C. Burstein, editors, Automated essay scoring: Across-disciplinary perspective, pages 87?112.Guy Lebanon, Yi Mao, and Joshua Dillon.
2007.
Thelocally weighted bag-of-words framework for docu-ment representation.
Journal of Machine Learning Re-search, 8(10):2405?2441.Ziheng Lin, Hwee T. Ng, and Min-Yen Kan. 2011.
Auto-matically Evaluating Text Coherence Using DiscourseRelations.
In Proceedings of the 49th Annual Meetingon Association for Computational Linguistics.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring systems.Natural Language Engineering, 10(01):25?55.Franz J. Och and Hermann Ney.
2000.
Improved statisti-cal alignment models.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics, pages 440?447.Emily Pitler, Annie Louis, and Ani Nenkova.
2010.Automatic evaluation of linguistic quality in multi-document summarization.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 544?554.Donald E. Powers, Jill C. Burstein, Martin Chodorow,Mary E. Fowles, and Karen Kukich.
2002.
Stump-ing e-rater: challenging the validity of automated essayscoring.
Computers in Human Behavior, 18(2):103?134.Lawrence M. Rudner and Tahung Liang.
2002.
Auto-mated essay scoring using Bayes?
theorem.
The Jour-nal of Technology, Learning and Assessment, 1(2):3?21.Magnus Sahlgren.
2005.
An introduction to random in-dexing.
In Methods and Applications of Semantic In-dexing Workshop at the 7th International Conferenceon Terminology and Knowledge Engineering, pages 1?9.
Citeseer.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of the COLING/ACL on Main conferenceposter sessions, pages 803?810.James H. Steiger.
1980.
Tests for comparing ele-ments of a correlation matrix.
Psychological Bulletin,87(2):245?251.Evan J. Williams.
1959.
The Comparison of RegressionVariables.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 21(2):396?399.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for AutomaticallyGrading ESOL Texts.
In The 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies.43
