Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 52?58,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsBuilding basic vocabulary across 40 languagesJudit A?cs Katalin PajkossyHAS Computer and Automation Research InstituteH-1111 Kende u 13-17, Budapest{judit.acs,pajkossy,kornai}@sztaki.mta.huAndra?s KornaiAbstractThe paper explores the options for build-ing bilingual dictionaries by automatedmethods.
We define the notion ?ba-sic vocabulary?
and investigate how wellthe conceptual units that make up thislanguage-independent vocabulary are cov-ered by language-specific bindings in 40languages.IntroductionGlobalization increasingly brings languages incontact.
At the time of the pioneering IBM workon the Hansard corpus (Brown et al 1990), onlytwo decades ago, there was no need for a Basque-Chinese dictionary, but today there is (Saralegi etal., 2012).
While the methods for building dic-tionaries from parallel corpora are now mature(Melamed, 2000), there is a dearth of bilingual oreven monolingual material (Zse?der et al 2012),hence the increased interest in comparable cor-pora.Once we find bilingual speakers capable of car-rying out a manual evaluation of representativesamples, it is relatively easy to measure the pre-cision of a dictionary built by automatic meth-ods.
But measuring recall remains a challenge, forif there existed a high quality machine-readabledictionary (MRD) to measure against, building anew one would largely be pointless, except per-haps as a means of engineering around copyrightrestrictions.
We could measure recall against Wik-tionary, but of course this is a moving target, andmore importantly, the coverage across languagepairs is extremely uneven.What we need is a standardized vocabulary re-source that is equally applicable to all languagepairs.
In this paper we describe our work towardcreating such a resource by extending the 4langconceptual dictionary (Kornai and Makrai, 2013)to the top 40 languages (by Wikipedia size) usinga variety of methods.
Since some of the resourcesstudied here are not available for the initial list of40 languages, we extended the original list to 50languages so as to guarantee at least 40 languagesfor every method.
Throughout the paper, resultsare provided for all 50 languages, indicating miss-ing data as needed.Section 1 outlines the approach taken towarddefining the basic vocabulary and translationalequivalence.
Section 2 describes how Wiktionaryitself measures up against the 4lang resourcedirectly and after triangulation across languagepairs.
Section 2.3 and Section 2.4 deals with ex-traction from multiply parallel and near-parallelcorpora, and Section 3 offers some conclusions.1 Basic vocabularyThe idea that there is a basic vocabulary composedof a few hundred or at most a few thousand ele-ments has a long history going back to the Renais-sance ?
for a summary, see Eco (1995).
The firstmodern efforts in this direction are Thorndike?s(1921) Word Book, based entirely on frequencycounts (combining TF and DF measures), andOgden?s (1944) Basic English, based primarilyon considerations of definability.
Both had last-ing impact, with Thorndike?s approach formingthe basis of much subsequent work on readabil-ity (Klare 1974, Kanungo and Orr 2009) andOgden?s forming the basis of the Simple En-glish Wikipedia1.
An important landmark is theSwadesh (1950) list, which puts special emphasison cross-linguistic definability, as its primary goalis to support glottochronological studies.Until the advent of large MRDs, the frequency-based method was much easier to follow, andThorndike himself has extended his original list often thousand words to twenty thousand (Thorndike1http://simple.wikipedia.org521931) and thirty thousand (Thorndike and Lorge1944).
For a recent example see Davies and Gard-ner (2010), for a historical survey see McArthur(1998).
The main problem with this approach isthe lack of clear boundaries both at the top of thelist, where function words dominate, and at thebottom, where it seems quite arbitrary to cut thelist off after the top three hundred words (Diedrich1938), the top thousand, as is common in foreignlanguage learning, or the top five thousand, es-pecially as the frequency curves are generally ingood agreement with Zipf?s law and thus show noobvious inflection point.
The problem at the topis perhaps more significant, since any frequency-based listing will start with the function wordsof the language, characterizing more its grammarthan its vocabulary.
For this reason, the list ishighly varied across languages, and what is a word(free form) in one language, like English the, oftenends up as an affix (bound form) in another, likethe Romanian suffix -ul.
By choosing a frequency-based approach, we inevitably put the emphasis oncomparing grammars and morphologies, insteadof comparing vocabularies.The definitional method is based on the assump-tion that dictionaries will attempt to define themore complex words by simpler ones.
Therefore,starting with any word list L, the list D(L) ob-tained by collecting the words appearing on theright-hand side of the dictionary definitions willbe simpler, the list D(D(L)) obtained by repeat-ing the method will be yet simpler, and so on, un-til we arrive at an irreducible list of basic wordsthat can no longer be further simplified.
Mod-ern MRDs, starting with the Longman Dictionaryof Contemporary English (LDOCE), generally en-force a strict list of words and word senses that canappear in definitions, which guarantees that the ba-sic list will be a subset of this defining vocabulary.This method, while still open to charges of arbi-trariness at the high end, in regards to the separa-tion of function words from basic words, creates abright line at the low end: no word, no matter howfrequent, needs to be included as long as it is notnecessary for defining other words.In creating the 4lang conceptual dictionary(Kornai and Makrai, 2013), we took advantageof the fact that the definitional method is robustin terms of choosing the seed list L, and built aseed of approximately 3,500 entries composed ofthe Longman Defining Vocabulary (2,200 entries),the most frequent 2,000 words according to theGoogle unigram count (Brants and Franz 2006)and the BNC, as well as the most frequent 2,000words from Polish (Hala?csy et al004) and Hun-garian (Kornai et al006).
Since Latin is one ofthe four languages supported by 4lang (the otherthree being English, Polish, and Hungarian), weadded the classic Diederich (1938) list and Whit-ney?s (1885) Roots.The basic list emerging from the iteration has1104 elements (including two bound morphemesbut excluding technical terms of the formal seman-tic model that have no obvious surface reflex).
Wewill refer to this as the basic or uroboros set as ithas the property that each of its members can bedefined in terms of the others, and we reserve thename 4lang for the larger set of 3,345 elementsfrom which it was obtained.
Since 4lang wordscan be defined using only the uroboros vocabu-lary, and every word in the Longman Dictionaryof Contemporary English can be defined using the4lang vocabulary (since this is a superset of LDV),we have full confidence that every sense of everynon-technical word can be defined by the uroborosvocabulary.
In fact, the Simple English Wikipediais an attempt to do this (Yasseri et al 2012) basedon Ogden?s Basic English, which overlaps with theuroboros set very significantly (Dice 0.527).The lexicographic principles underlying 4langhave been discussed elsewhere (Kornai, 2012;Kornai and Makrai, 2013), here we just summa-rize the most salient points.
First, the system isintended to capture everyday vocabulary.
Oncethe boundaries of natural language are crossed,and goats are defined by their set of genes (ratherthan an old-fashioned taxonomic description in-volving cloven hooves and the like), or derivativeis defined as lim?
?0(f(x + ?)
?
f(x))/?, theuroboros vocabulary loses its grip.
But for thenon-technical vocabulary, and even the part of thetechnical vocabulary that rests on natural language(e.g.
legal definitions or the definitions in philos-ophy and discursive prose in general), coverageof the uroboros set promises a strategy of grad-ually extending the vocabulary from the simpleto the more complex.
Thus, to define Jupiter as?the largest planet of the Sun?, we need to defineplanet, but not large as this item is already listed inthe uroboros set.
Since planet is defined ?as a largebody in space that moves around a star?, by substi-tution we will obtain for Jupiter the definition ?the53largest body in space that moves around the Sun?where all the key items large, body, space, move,around are part of the uroboros set.
Proper nounslike Sun are discussed further in (Kornai, 2010),but we note here that they constitute a very smallproportion (less than 6%) of the basic vocabulary.Second, the ultimate definitions of the uroboroselements are given in the formal language of ma-chines (Eilenberg, 1974), and at that level the En-glish words serve only a mnemonic purpose, andcould in principle be replaced by any arbitrarynames or even numbers.
Because this would makedebugging next to impossible, as in purposely ob-fuscated code, we resort to using English print-names for each concept, but it is important to keepin mind that these are only weakly reflective ofthe English word.
For example, the system reliesheavily on an element has that indicates the pos-sessive relation both in the direct sense, as in theSun?s planet, the planet of the Sun and in its moreindirect uses, as in John?s favorite actress wherethere is no question of John being in possessionof the actress.
In other languages, has will gen-erally be translated by morphemes (often boundmorphemes) indicating possession, but there is noattempt to cross-link all relevant uses.
The el-ement has will appear in the definition of Latinmeus and noster alike, but of course there is noclaim that English has underlies the Latin senses.If we know how to express the basic vocabularyelements in a given language, which is the task weconcentrate on here, and how to combine the ex-pressions in that language, we are capable of defin-ing all remaining words of the language.In general, matching up function words cross-linguistically is an extremely hard task, especiallyas they are often expressed by inflectional mor-phology and our workflow, which includes stem-ming, just strips off the relevant elements.
Evenacross languages where morphological analysis isa solved task, it will take a great deal of man-ual work to establish some form of translationalequivalence, and we consider the issue out ofscope here.
But for content words, the use oflanguage-independent concepts simplifies mattersa great deal: instead of finding(402)translationpairs for the 3,384 concepts that already have man-ual bindings in four languages (currently, Latinand Polish are only 90% complete), our goal isonly to find reasonable printnames for the 1,104basic concepts in all 40 languages.
Translationpairs are only obtained indirectly, through the con-ceptual pivot, and thus do not amount to fully validbilingual translation pairs.
For example, he-goatin one language may just get mapped to the con-cept goat, and if billy-goat is present in anotherlanguage, the strict translational equivalence be-tween the gendered forms will be lost becauseof the poverty of the pivot.
Nevertheless, roughequivalence at the conceptual level is already auseful notion, especially for filtering out candidatepairs produced by more standard bilingual dictio-nary building methods, to which we now turn.2 WiktionaryWiktionary is a crowdsourced dictionary withmany language editions that aim at eventuallydefining ?all words?.
Although Wiktionary is pri-marily for human audience, since editors are ex-pected to follow fairly strict formatting standards,we can automate the data extraction to a cer-tain degree.
While not a computational linguistictask par excellence, undoing the MediaWiki for-mat, identifying the templates and simply detect-ing the translation pairs requires a great deal ofscripting.
Some Wiktionaries, among others theBulgarian, Chinese, Danish, German, Hungarian,Korean, and Russian, are formatted so heteroge-neously that automated extraction of translationpairs is very hard, and our results could be furtherimproved.Table 1 summarizes the coverage of Wiktionaryon the basic vocabulary from the perspective oftranslation pairs with one manual member, En-glish, Hungarian, Latin, and Polish respectively.The last column represents the overall coveragecombining all four languages.
As can be seen,the better resourced languages fare better in Wik-tionary as well, with the most translations foundusing English as the source language (64.9% onthe smaller basic set, and 64% on the larger 4langvocabulary), Polish and Hungarian faring aboutTable 1: 4lang coverage of Wiktionary data.Based onen hu la pl all4lang 59.43 22.09 7.9 19.6 64.01uroboros 60.29 22.88 9.11 21.09 64.9154equally well, although the Polish list of 4lang hasmore missing bindings, and the least resourcedLatin faring the worst.Another measure of coverage is obtained byseeing how many language bindings are found onthe average for each concept: 65% on 4lang and64% for the basic set (32 out of the 50 languagesconsidered here).2.1 TriangulatingNext we used a simple triangulation method toexpand the collection of translation pairs, whichadded new translation pairs if they had been linkedwith the same word in a third language.
An ex-ample, the English:Romanian pair guild:breasla?,obtained through a Hungarian pivot, is shown inFigure 1.hu:ce?hen:guild ro:breasla?Figure 1: The non-dashed edge represents transla-tion pairs extracted directly from the Wiktionaries.The pair guild?breasla?
were found via triangulat-ing.While direct translation pairs come from themanually built Wiktionaries and can be consid-ered gold (not entirely without reservations, butclearly over 90% correct in most language pairswe could manually spot-check), indirect pairsmust be viewed with considerable suspicion, asmultiple word senses bring in false positives quiteoften.
Using 3,317,861 pairs extracted from 40Wiktionaries, we obtained a total of 126,895,236indirect pairs, but in the following table we con-sider only those that were obtained through at leasttwo different third-language pivots with the pairsoriginating from different Wiktionaries, and dis-carded the vast majority, leaving 5,720,355 pairsthat have double confirmation.
Manual checkingproved that the quality of these pairs is compara-ble to that of the original data (see Table 7).
Asimilar method, within one dictionary rather thanTable 2: 4lang coverage of triangulating.Based onen hu la pl all4lang 76.09 64.91 43.25 53.74 85.81basic 77.81 64.74 48.07 58.55 86.97Table 3: 4lang coverage of Wiktionary data andtriangulating.Based onen hu la pl all4lang 80.77 65.69 43.63 54.30 86.80basic 82.07 65.47 48.41 59.13 87.81across several, was used in (Saralegi et al 2012)to remove triangulation noise.
Since recall wouldbe considerably improved by some less aggres-sive filtering method, in the future we will alsoconsider improving the similarity scores of ourcorpus-based methods using the single triangleswe now discard.Triangulating by itself improves coverage from65% to 85.8% (4lang) and from 64% to 87% (ba-sic), see Table 2.
Table 3 shows the combined cov-erage which is not much different from Table 2 butconsidering that the triangulating used the Wik-tionary data as input, we expected a very large in-tersection (it turned out to be more than 40% ofthe pairs acquired through triangulating).
The av-erage number of language bindings also improvessignificantly, to 43.5/50 (4lang) and 44/50 (basic).2.2 Wikipedia titlesAnother crowdsourced method that promises greatprecision is comparing Wikipedia article titlesacross languages: we extracted over 187m poten-tial translation pairs this way.
Yet the raw data isquite noisy, for example French chambre points toEnglish Ca?mara, an article devoted to the fact that?Ca?mara (meaning ?chamber?)
is a common sur-name in the Portuguese language?
rather than tosome article on bedroom, room, or chamber.
Wefiltered this data in several ways.
First, we dis-carded all pairs that contain words that appear fiveor fewer times in the frequency count generatedfrom the language in question.
This reduced the55Table 4: 4lang coverage of Wikipedia interwikilinks (langlinks).Based onen hu la pl all4lang 21.51 14.4 9.54 12.26 31.74basic 20.7 13.0 10.22 13.43 31.32number of pairs to 15m.
Most of these, unfortu-nately, are string-identical across languages, leav-ing us with a total of 6.15m nontrivial translationpairs.
A large portion of these are named entitiesthat do not always add meaningfully to a bilingualdictionary.The average number of language bindings is16.5 and 12.6 respectively.
The combined resultsimprove slightly as shown in Table 8.2.3 Parallel textsUsing the Bible as a parallel text in dictionarybuilding has a long tradition (Resnik et al 1999).Somewhat surprisingly in the age of parallelcorpora, the only secular text available in allour languages is the Universal Declaration ofHuman Rights, which is simply too short to addmeaningfully to the coverage obtained on theBible.
In addition to downloading the collectionat http://homepages.inf.ed.ac.uk/s0787820/bible,we used http://www.jw.org (for Dutch, Ar-menian and Korean), www.gospelgo.com (forCatalan, Kazakh, Macedonian, Malay andPersian), http://www.biblegateway.com (forCzech), http://biblehub.com (for English) andhttp://www.mek.oszk.hu (for Hungarian).
To theextent feasible we tried to use modern Bibletranslations, resorting to more traditional trans-lations only where we could not identify a morecontemporary version.The average number of languages with transla-tions found is 19 (basic) and 17.8 (4lang).
TheseTable 5: 4lang coverage of the Bible data.Based onen hu la pl all4lang 19.64 15.17 13.78 14.13 35.49basic 21.47 17.12 15.67 15.78 38.13numbers are considerably weaker than the crowd-sourced results, suggesting that the dearth of mul-tiply parallel texts, even in the best resourcedgroup of 40 languages, needs to be addressed.2.4 Comparable textsComparable corpora were built from Wikipedia ar-ticles in the following manner.
For each languagepair, we considered those articles that mutuallylinked each other, and took the first 50 words, ex-cluding the title itself.
Article pairs whose lengthdiffered drastically (more than a factor of five)were discarded.Table 6: 4lang coverage of the dictionary extractedfrom Wikipedia as comparable corpora.Based onen hu la pl all4lang 5.58 5.66 4.30 4.96 16.00basic 5.70 5.86 4.93 5.39 16.77The 4lang coverage based solely on the trans-lations acquired from comparable corpora is pre-sented in Table 6.
The average number of lan-guages with translations found is 8 (basic) and 8.4(4lang).2.5 EvaluationWe used manual evaluation for a small subset oflanguage pairs.
Human annotators received a sam-ple of 100 translation candidate-per-method.
Thesamples were selected from translations that werefound by only one method, as we suspect thattranslations found by several methods are morelikely to be correct.
Using this strict data selectionTable 7: Manual evaluation of extracted pairs thatdo not appear in more than one dictionary.Wikt Tri Title Par Compcs-hu 82 81 95 41 40de-hu 92 87 96 46 68fr-hu 76 80 89 43 54fr-it 79 79 92 43 36hu-en 87 75 92 28 63hu-it 94 93 93 35 61hu-ko 87 85 99 N/A N/Aavg 85.3 82.9 93.7 39.3 53.756criterion we evaluated the added quality of eachmethod.
Results are presented in Table 7.
It isclear that set next to the crowdsourced methods,dictionary extraction from either parallel or com-parable corpora cannot add new translations withhigh precision.
When high quality input data isavailable, triangulating appears to be a powerfulyet simple method.3 Conclusions and future workThe major lesson emerging from this work is thatcurrently, crowdsourced methods are considerablymore powerful than the parallel and comparablecorpora-based methods that we started with.
Thereason is simply the lack of sufficiently large par-allel and near-parallel data sets, even among themost commonly taught languages.
If one is actu-ally interested in creating a resource, even a smallresource such as our basic vocabulary set, withbindings for all 40 languages, one needs to engagethe crowd.Table 8: Summary of the increase in 4lang cover-age achieved by each method.
Wikt: Wiktionary,Tri: triangulating, WPT: Wikipedia titles, Par: theBible as parallel corpora, WPC: Wikipedia articlesas comparable corporaSrc SetBased onen hu la pl allWikt4lang 59.43 22.09 7.90 19.6 64.01basic 60.29 22.88 9.11 21.09 64.91Tri4lang 80.77 65.69 43.63 54.3 86.8basic 82.07 65.47 48.41 59.13 87.81WPT4lang 81.39 66.27 44.2 54.66 87.39basic 82.51 65.86 48.89 59.53 88.17Par4lang 82.22 67.35 45.99 55.4 88.22basic 83.27 67.04 50.62 60.25 88.91WPC4lang 81.56 66.49 44.42 54.77 87.58basic 82.66 66.06 49.14 59.62 88.33The resulting 40lang resource, cur-rently about 88% complete, is availablefor download at http://hlt.sztaki.hu.
TheWiktionary extraction tool is available athttps://github.com/juditacs/wikt2dict.
40lang,while not 100% complete and verified, canalready serve as an important addition to existingMRDs in several applications.
In comparingcorpora the extent vocabulary is shared acrossthem is a critical measure, yet the task is nottrivial even when these corpora are taken from thesame language.
We need to compare vocabulariesat the conceptual level, and checking the shared40lang content between two texts is a good firstcut.
Automated dictionary building itself canbenefit from the resource, since both alignersand dictionary extractors benefit from knowntranslation pairs.AcknowledgmentsThe results presented here have improved sinceA?cs (2013).
A?cs did the work on Wik-tionary and Wikipedia titles, Pajkossy on par-allel corpora, Kornai supplied the theory andadvised.
The statistics were created by A?cs.We thank Attila Zse?der, whose HunDict system(see https://github.com/zseder/hundict) was usedon the (near)parallel data, for his constant sup-port at every stage of the work.
We also thankour annotators: Kla?ra Szalay, E?va Novai, Ange-lika Sa?ndor, and Ga?bor Recski.The research reported in the paper was con-ducted with the support of the EFNILEX projecthttp://efnilex.efnil.org of the European Feder-ation of National Institutions for Languagehttp://www.efnil.org, and OTKA grant #82333.ReferencesJudit A?cs.
2013.
Intelligent multilingual dictionarybuilding.
MSc Thesis, Budapest University of Tech-nology and Economics.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram version 1.Peter Brown, John Cocke, Stephen Della Pietra, Vin-cent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.A statistical approach to machine translation.
Com-putational Linguistics, 16:79?85.M.
Davies and D. Gardner.
2010.
A Frequency Dic-tionary of Contemporary American English: WordSketches, Collocates, and Thematic Lists.
Rout-ledge Frequency Dictionaries Series.
Routledge.Paul Bernard Diederich.
1939.
The frequency of Latinwords and their endings.
The University of Chicagopress.Umberto Eco.
1995.
The Search for the Perfect Lan-guage.
Blackwell, Oxford.Samuel Eilenberg.
1974.
Automata, Languages, andMachines, volume A.
Academic Press.Pe?ter Hala?csy, Andra?s Kornai, La?szlo?
Ne?meth, Andra?sRung, Istva?n Szakada?t, and Viktor Tro?n.
2004.
Cre-ating open language resources for Hungarian.
In57Proceedings of the 4th international conference onLanguage Resources and Evaluation (LREC2004),pages 203?210.Tapas Kanungo and David Orr.
2009.
Predicting thereadability of short web summaries.
In 2nd ACMInt.
Conf.
on Web Search and Data Mining.George R. Klare.
1974.
Assessing readability.
Read-ing Research Quarterly, 10(1):62?102.Andra?s Kornai and Ma?rton Makrai.
2013.
A 4langfogalmi szo?ta?r [the 4lang concept dictionary].
InA.
Tana?cs and V. Vincze, editors, IX.
MagyarSza?mito?ge?pes Nyelve?szeti Konferencia [Ninth Con-ference on Hungarian Computational Linguistics],pages 62?70.A.
Kornai, P. Hala?csy, V. Nagy, Cs.
Oravecz, V. Tro?n,and D. Varga.
2006.
Web-based frequency dictio-naries for medium density languages.
In A. Kilgar-iff and M. Baroni, editors, Proc.
2nd Web as CorpusWkshp (EACL 2006 WS01), pages 1?8.Andra?s Kornai.
2010.
The algebra of lexical seman-tics.
In Christian Ebert, Gerhard Ja?ger, and JensMichaelis, editors, Proceedings of the 11th Mathe-matics of Language Workshop, LNAI 6149, pages174?199.
Springer.Andra?s Kornai.
2012.
Eliminating ditransitives.
InPh.
de Groote and M-J Nederhof, editors, Revisedand Selected Papers from the 15th and 16th FormalGrammar Conferences, LNCS 7395, pages 243?261.
Springer.Tom McArthur.
1998.
Living Words: Language, Lex-icography, and the Knowledge Revolution.
ExeterLanguage and Lexicography Series.
University ofExeter Press.I Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.C.K.
Ogden.
1944.
Basic English: A General Intro-duction with Rules and Grammar.
Psyche minia-tures: General Series.
Kegan Paul, Trench, Trubner.Philip Resnik, Mari Broman Olsen, and Mona Diab.1999.
The bible as a parallel corpus: Annotatingthe ?Book of 2000 Tongues?.
Computers and theHumanities, 33(1-2):129?153.Xabier Saralegi, Iker Manterola, and In?aki San Vicente.2012.
Building a basque-chinese dictionary by us-ing english as pivot.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Thierry Declerck,Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey, may.
European Language ResourcesAssociation (ELRA).Morris Swadesh.
1950.
Salish internal relation-ships.
International Journal of American Linguis-tics, pages 157?167.Edward L. Thorndike and Irving Lorge.
1944.
Theteacher?s word book of 30,000 words.
Teachers Col-lege Bureau of Publications.Edward L. Thorndike.
1921.
The teacher?s word book.New York Teachers College, Columbia University.E.L.
Thorndike.
1931.
A teacher?s word book.
NewYork Teachers College, Columbia University.William Dwight Whitney.
1885.
The roots of the San-skrit language.
Transactions of the American Philo-logical Association (1869-1896), 16:5?29.Taha Yasseri, Andra?s Kornai, and Ja?nos Kerte?sz.
2012.A practical approach to language complexity: awikipedia case study.
PLoS ONE, 7(11):e48386.doi:10.1371/journal.pone.0048386.Attila Zse?der, Ga?bor Recski, Da?niel Varga, and Andra?sKornai.
2012.
Rapid creation of large-scale corporaand frequency dictionaries.
In Proceedings to LREC2012.58
