Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 393?400,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning Event Durations from Event DescriptionsFeng Pan, Rutu Mulkar, and Jerry R. HobbsInformation Sciences Institute (ISI), University of Southern California4676 Admiralty Way, Marina del Rey, CA 90292, USA{pan, rutu, hobbs}@isi.eduAbstractWe have constructed a corpus of news ar-ticles in which events are annotated forestimated bounds on their duration.
Herewe describe a method for measuring in-ter-annotator agreement for these eventduration distributions.
We then show thatmachine learning techniques applied tothis data yield coarse-grained event dura-tion information, considerably outper-forming a baseline and approaching hu-man performance.1 IntroductionConsider the sentence from a news article:George W. Bush met with Vladimir Putin inMoscow.How long was the meeting?
Our first reactionto this question might be that we have no idea.But in fact we do have an idea.
We know themeeting was longer than 10 seconds and lessthan a year.
How much tighter can we get thebounds to be?
Most people would say the meet-ing lasted between an hour and three days.There is much temporal information in textthat has hitherto been largely unexploited, en-coded in the descriptions of events and relyingon our knowledge of the range of usual durationsof types of events.
This paper describes one partof an exploration into how this information canbe captured automatically.
Specifically, we havedeveloped annotation guidelines to minimize dis-crepant judgments and annotated 58 articles,comprising 2288 events; we have developed amethod for measuring inter-annotator agreementwhen the judgments are intervals on a scale; andwe have shown that machine learning techniquesapplied to the annotated data considerably out-perform a baseline and approach human per-formance.This research is potentially very important inapplications in which the time course of events isto be extracted from news.
For example, whethertwo events overlap or are in sequence often de-pends very much on their durations.
If a warstarted yesterday, we can be pretty sure it is stillgoing on today.
If a hurricane started last year,we can be sure it is over by now.The corpus that we have annotated currentlycontains all the 48 non-Wall-Street-Journal (non-WSJ) news articles (a total of 2132 event in-stances), as well as 10 WSJ articles (156 eventinstances), from the TimeBank corpus annotatedin TimeML (Pustejovky et al, 2003).
The non-WSJ articles (mainly political and disaster news)include both print and broadcast news that arefrom a variety of news sources, such as ABC,AP, and VOA.In the corpus, every event to be annotated wasalready identified in TimeBank.
Annotatorswere instructed to provide lower and upperbounds on the duration of the event, encompass-ing 80% of the possibilities, excluding anoma-lous cases, and taking the entire context of thearticle into account.
For example, here is thegraphical output of the annotations (3 annotators)for the ?finished?
event (underlined) in the sen-tenceAfter the victim, Linda Sanders, 35, had fin-ished her cleaning and was waiting for herclothes to dry,...393This graph shows that the first annotator be-lieves that the event lasts for minutes whereas thesecond annotator believes it could only last forseveral seconds.
The third annotates the event torange from a few seconds to a few minutes.
Alogarithmic scale is used for the output becauseof the intuition that the difference between 1 sec-ond and 20 seconds is significant, while the dif-ference between 1 year 1 second and 1 year 20seconds is negligible.A preliminary exercise in annotation revealedabout a dozen classes of systematic discrepanciesamong annotators?
judgments.
We thus devel-oped guidelines to make annotators aware ofthese cases and to guide them in making thejudgments.
For example, many occurrences ofverbs and other event descriptors refer to multi-ple events, especially but not exclusively if thesubject or object of the verb is plural.
In ?Iraqhas destroyed its long-range missiles?, there isthe time it takes to destroy one missile and theduration of the interval in which all the individ-ual events are situated ?
the time it takes to de-stroy all its missiles.
Initially, there were widediscrepancies because some annotators wouldannotate one value, others the other.
Annotatorsare now instructed to make judgments on bothvalues in this case.
The use of the annotationguidelines resulted in about 10% improvement ininter-annotator agreement (Pan et al, 2006),measured as described in Section 2.There is a residual of gross discrepancies inannotators?
judgments that result from differ-ences of opinion, for example, about how long agovernment policy is typically in effect.
But thenumber of these discrepancies was surprisinglysmall.The method and guidelines for annotation aredescribed in much greater detail in (Pan et al,2006).
In the current paper, we focus on howinter-annotator agreement is measured, in Sec-tion 2, and in Sections 3-5 on the machine learn-ing experiments.
Because the annotated corpusis still fairly small, we cannot hope to learn tomake fine-grained judgments of event durationsthat are currently annotated in the corpus, but aswe demonstrate, it is possible to learn usefulcoarse-grained judgments.Although there has been much work on tem-poral anchoring and event ordering in text(Hitzeman et al, 1995; Mani and Wilson, 2000;Filatova and Hovy, 2001; Boguraev and Ando,2005), to our knowledge, there has been no seri-ous published empirical effort to model and learnvague and implicit duration information in natu-ral language, such as the typical durations ofevents, and to perform reasoning over this infor-mation.
(Cyc apparently has some fuzzy durationinformation, although it is not generally avail-able; Rieger (1974) discusses the issue for lessthan a page; there has been work in fuzzy logicon representing and reasoning with imprecisedurations (Godo and Vila, 1995; Fortemps,1997), but these make no attempt to collect hu-man judgments on such durations or learn to ex-tract them automatically from texts.
)2 Inter-Annotator AgreementAlthough the graphical output of the annotationsenables us to visualize quickly the level of agree-ment among different annotators for each event,a quantitative measurement of the agreement isneeded.The kappa statistic (Krippendorff, 1980; Car-letta, 1996) has become the de facto standard toassess inter-annotator agreement.
It is computedas:)(1)()(EPEPAP?
?=?P(A) is the observed agreement among the an-notators, and P(E) is the expected agreement,which is the probability that the annotators agreeby chance.In order to compute the kappa statistic for ourtask, we have to compute P(A) and P(E), butthose computations are not straightforward.P(A): What should count as agreement amongannotators for our task?P(E): What is the probability that the annota-tors agree by chance for our task?2.1 What Should Count as Agreement?Determining what should count as agreement isnot only important for assessing inter-annotatoragreement, but is also crucial for later evaluationof machine learning experiments.
For example,for a given event with a known gold standardduration range from 1 hour to 4 hours, if a ma-chine learning program outputs a duration of 3hours to 5 hours, how should we evaluate thisresult?In the literature on the kappa statistic, most au-thors address only category data; some can han-dle more general data, such as data in intervalscales or ratio scales.
However, none of the tech-niques directly apply to our data, which areranges of durations from a lower bound to anupper bound.394Figure 1: Overlap of Judgments of [10 minutes,30 minutes] and [10 minutes, 2 hours].In fact, what coders were instructed to anno-tate for a given event is not just a range, but aduration distribution for the event, where thearea between the lower bound and the upperbound covers about 80% of the entire distributionarea.
Since it?s natural to assume the most likelyduration for such distribution is its mean (aver-age) duration, and the distribution flattens outtoward the upper and lower bounds, we use thenormal or Gaussian distribution to model ourduration distributions.
If the area between lowerand upper bounds covers 80% of the entire dis-tribution area, the bounds are each 1.28 standarddeviations from the mean.Figure 1 shows the overlap in distributions forjudgments of [10 minutes, 30 minutes] and [10minutes, 2 hours], and the overlap or agreementis 0.508706.2.2 Expected AgreementWhat is the probability that the annotators agreeby chance for our task?
The first quick responseto this question may be 0, if we consider all thepossible durations from 1 second to 1000 yearsor even positive infinity.However, not all the durations are equally pos-sible.
As in (Krippendorff, 1980), we assumethere exists one global distribution for our task(i.e., the duration ranges for all the events), and?chance?
annotations would be consistent withthis distribution.
Thus, the baseline will be anannotator who knows the global distribution andannotates in accordance with it, but does not readthe specific article being annotated.
Therefore,we must compute the global distribution of thedurations, in particular, of their means and theirwidths.
This will be of interest not only in deter-mining expected agreement, but also in terms of-5 0 5 10 15 20 25 30020406080100120140160180Means of Annotated DurationsNumber of AnnotatedDurationsFigure 2: Distribution of Means of AnnotatedDurations.what it says about the genre of news articles andabout fuzzy judgments in general.We first compute the distribution of the meansof all the annotated durations.
Its histogram isshown in Figure 2, where the horizontal axisrepresents the mean values in the natural loga-rithmic scale and the vertical axis represents thenumber of annotated durations with that mean.There are two peaks in this distribution.
One isfrom 5 to 7 in the natural logarithmic scale,which corresponds to about 1.5 minutes to 30minutes.
The other is from 14 to 17 in the naturallogarithmic scale, which corresponds to about 8days to 6 months.
One could speculate that thisbimodal distribution is because daily newspapersreport short events that happened the day beforeand place them in the context of larger trends.We also compute the distribution of the widths(i.e., Xupper ?
Xlower) of all the annotated durations,and its histogram is shown in Figure 3, where thehorizontal axis represents the width in the naturallogarithmic scale and the vertical axis representsthe number of annotated durations with thatwidth.
Note that it peaks at about a half order ofmagnitude (Hobbs and Kreinovich, 2001).Since the global distribution is determined bythe above mean and width distributions, we canthen compute the expected agreement, i.e., theprobability that the annotators agree by chance,where the chance is actually based on this globaldistribution.Two different methods were used to computethe expected agreement (baseline), both yieldingnearly equal results.
These are described in detailin (Pan et al, 2006).
For both, P(E) is about 0.15.395-5 0 5 10 15 20 25050100150200250300350400Widths of Annotated DurationsNumberofAnnotatedDurationsFigure 3: Distribution of Widths of AnnotatedDurations.3 FeaturesIn this section, we describe the lexical, syntactic,and semantic features that we considered inlearning event durations.3.1 Local ContextFor a given event, the local context features in-clude a window of n tokens to its left and n to-kens to its right, as well as the event itself, for n= {0, 1, 2, 3}.
The best n determined via crossvalidation turned out to be 0, i.e., the event itselfwith no local context.
But we also present resultsfor n = 2 in Section 4.3 to evaluate the utility oflocal context.A token can be a word or a punctuation mark.Punctuation marks are not removed, because theycan be indicative features for learning event du-rations.
For example, the quotation mark is agood indication of quoted reporting events, andthe duration of such events most likely lasts forseconds or minutes, depending on the length ofthe quoted content.
However, there are also caseswhere quotation marks are used for other pur-poses, such as emphasis of quoted words andtitles of artistic works.For each token in the local context, includingthe event itself, three features are included: theoriginal form of the token, its lemma (or rootform), and its part-of-speech (POS) tag.
Thelemma of the token is extracted from parse treesgenerated by the CONTEX parser (Hermjakoband Mooney, 1997) which includes rich contextinformation in parse trees, and the Brill tagger(Brill, 1992) is used for POS tagging.The context window doesn?t cross the bounda-ries of sentences.
When there are not enough to-kens on either side of the event within the win-dow, ?NULL?
is used for the feature values.Features Original Lemma POSEvent signed sign VBD1token-after the the DT2token-after plan plan NN1token-before Friday Friday NNP2token-before on on INTable 1: Local context features for the ?signed?event in sentence (1) with n = 2.The local context features extracted for the?signed?
event in sentence (1) is shown in Table1 (with a window size n = 2).
The feature vectoris [signed, sign, VBD, the, the, DT, plan, plan,NN, Friday, Friday, NNP, on, on, IN].
(1) The two presidents on Friday signed theplan.3.2 Syntactic RelationsThe information in the event?s syntactic envi-ronment is very important in deciding the dura-tions of events.
For example, there is a differencein the durations of the ?watch?
events in thephrases ?watch a movie?
and ?watch a bird fly?.For a given event, both the head of its subjectand the head of its object are extracted from theparse trees generated by the CONTEX parser.Similarly to the local context features, for boththe subject head and the object head, their origi-nal form, lemma, and POS tags are extracted asfeatures.
When there is no subject or object foran event, ?NULL?
is used for the feature values.For the ?signed?
event in sentence (1), thehead of its subject is ?presidents?
and the head ofits object is ?plan?.
The extracted syntactic rela-tion features are shown in Table 2, and the fea-ture vector is [presidents, president, NNS, plan,plan, NN].3.3 WordNet HypernymsEvents with the same hypernyms may have simi-lar durations.
For example, events ?ask?
and?talk?
both have a direct WordNet (Miller, 1990)hypernym of ?communicate?, and most of thetime they do have very similar durations in thecorpus.However, closely related events don?t alwayshave the same direct hypernyms.
For example,?see?
has a direct hypernym of ?perceive?,whereas ?observe?
needs two steps up throughthe hypernym hierarchy before reaching ?per-ceive?.
Such correlation between events may belost if only the direct hypernyms of the words areextracted.396Features Original Lemma POSSubject presidents president NNSObject plan plan NNTable 2: Syntactic relation features for the?signed?
event in sentence (1).Feature 1-hyper 2-hyper 3-hyperEvent write communicate interactSubject corporate executive executiveadminis-tratorObject idea content cognitionTable 3: WordNet hypernym features for theevent (?signed?
), its subject (?presidents?
), andits object (?plan?)
in sentence (1).It is useful to extract the hypernyms not onlyfor the event itself, but also for the subject andobject of the event.
For example, events relatedto a group of people or an organization usuallylast longer than those involving individuals, andthe hypernyms can help distinguish such con-cepts.
For example, ?society?
has a ?group?
hy-pernym (2 steps up in the hierarchy), and?school?
has an ?organization?
hypernym (3steps up).
The direct hypernyms of nouns arealways not general enough for such purpose, buta hypernym at too high a level can be too generalto be useful.
For our learning experiments, weextract the first 3 levels of hypernyms fromWordNet.Hypernyms are only extracted for the eventsand their subjects and objects, not for the localcontext words.
For each level of hypernyms inthe hierarchy, it?s possible to have more than onehypernym, for example, ?see?
has two direct hy-pernyms, ?perceive?
and ?comprehend?.
For agiven word, it may also have more than onesense in WordNet.
In such cases, as in (Gildeaand Jurafsky, 2002), we only take the first senseof the word and the first hypernym listed for eachlevel of the hierarchy.
A word disambiguationmodule might improve the learning performance.But since the features we need are the hypernyms,not the word sense itself, even if the first wordsense is not the correct one, its hypernyms canstill be good enough in many cases.
For example,in one news article, the word ?controller?
refersto an air traffic controller, which corresponds tothe second sense in WordNet, but its first sense(business controller) has the same hypernym of?person?
(3 levels up) as the second sense (directhypernym).
Since we take the first 3 levels ofhypernyms, the correct hypernym is still ex-tracted.P(A) P(E) Kappa0.528 0.740 0.8770.500 0.755Table 4: Inter-Annotator Agreement for BinaryEvent Durations.When there are less than 3 levels of hy-pernyms for a given word, its hypernym on theprevious level is used.
When there is no hy-pernym for a given word (e.g., ?go?
), the worditself will be used as its hypernyms.
SinceWordNet only provides hypernyms for nounsand verbs, ?NULL?
is used for the feature valuesfor a word that is not a noun or a verb.For the ?signed?
event in sentence (1), the ex-tracted WordNet hypernym features for the event(?signed?
), its subject (?presidents?
), and its ob-ject (?plan?)
are shown in Table 3, and the fea-ture vector is [write, communicate, interact, cor-porate_executive, executive, administrator, idea,content, cognition].4 ExperimentsThe distribution of the means of the annotateddurations in Figure 2 is bimodal, dividing theevents into those that take less than a day andthose that take more than a day.
Thus, in our firstmachine learning experiment, we have tried tolearn this coarse-grained event duration informa-tion as a binary classification task.4.1 Inter-Annotator Agreement, Baseline,and Upper BoundBefore evaluating the performance of differentlearning algorithms, the inter-annotator agree-ment, the baseline and the upper bound for thelearning task are assessed first.Table 4 shows the inter-annotator agreementresults among 3 annotators for binary event dura-tions.
The experiments were conducted on thesame data sets as in (Pan et al, 2006).
Twokappa values are reported with different ways ofmeasuring expected agreement (P(E)), i.e.,whether or not the annotators have prior knowl-edge of the global distribution of the task.The human agreement before reading theguidelines (0.877) is a good estimate of the upperbound performance for this binary classificationtask.
The baseline for the learning task is alwaystaking the most probable class.
Since 59.0% ofthe total data is ?long?
events, the baseline per-formance is 59.0%.397Class Algor.
Prec.
Recall F-ScoreSVM 0.707 0.606 0.653NB 0.567 0.768 0.652 ShortC4.5 0.571 0.600 0.585SVM 0.793 0.857 0.823NB 0.834 0.665 0.740LongC4.5 0.765 0.743 0.754Table 5: Test Performance of Three Algorithms.4.2 DataThe original annotated data can be straightfor-wardly transformed for this binary classificationtask.
For each event annotation, the most likely(mean) duration is calculated first by averaging(the logs of) its lower and upper bound durations.If its most likely (mean) duration is less than aday (about 11.4 in the natural logarithmic scale),it is assigned to the ?short?
event class, otherwiseit is assigned to the ?long?
event class.
(Note thatthese labels are strictly a convenience and not ananalysis of the meanings of ?short?
and ?long?.
)We divide the total annotated non-WSJ data(2132 event instances) into two data sets: a train-ing data set with 1705 event instances (about80% of the total non-WSJ data) and a held-outtest data set with 427 event instances (about 20%of the total non-WSJ data).
The WSJ data (156event instances) is kept for further test purposes(see Section 4.4).4.3 Experimental Results (non-WSJ)Learning Algorithms.
Three supervised learn-ing algorithms were evaluated for our binaryclassification task, namely, Support Vector Ma-chines (SVM) (Vapnik, 1995), Na?ve Bayes(NB) (Duda and Hart, 1973), and Decision TreesC4.5 (Quinlan, 1993).
The Weka (Witten andFrank, 2005) machine learning package was usedfor the implementation of these learning algo-rithms.
Linear kernel is used for SVM in our ex-periments.Each event instance has a total of 18 featurevalues, as described in Section 3, for the eventonly condition, and 30 feature values for the lo-cal context condition when n = 2.
For SVM andC4.5, all features are converted into binary fea-tures (6665 and 12502 features).Results.
10-fold cross validation was used totrain the learning models, which were then testedon the unseen held-out test set, and the perform-ance (including the precision, recall, and F-score11 F-score is computed as the harmonic mean of the preci-sion and recall: F = (2*Prec*Rec)/(Prec+Rec).Algorithm PrecisionBaseline 59.0%C4.5 69.1%NB 70.3%SVM 76.6%Human Agreement 87.7%Table 6: Overall Test Precision on non-WSJData.for each class) of the three learning algorithms isshown in Table 5.
The significant measure isoverall precision, and this is shown for the threealgorithms in Table 6, together with human a-greement (the upper bound of the learning task)and the baseline.We can see that among all three learning algo-rithms, SVM achieves the best F-score for eachclass and also the best overall precision (76.6%).Compared with the baseline (59.0%) and humanagreement (87.7%), this level of performance isvery encouraging, especially as the learning isfrom such limited training data.Feature Evaluation.
The best performinglearning algorithm, SVM, was then used to ex-amine the utility of combinations of four differ-ent feature sets (i.e., event, local context, syntac-tic, and WordNet hypernym features).
The de-tailed comparison is shown in Table 7.We can see that most of the performancecomes from event word or phrase itself.
A sig-nificant improvement above that is due to theaddition of information about the subject andobject.
Local context does not help and in factmay hurt, and hypernym information also doesnot seem to help2.
It is of interest that the mostimportant information is that from the predicateand arguments describing the event, as our lin-guistic intuitions would lead us to expect.4.4 Test on WSJ DataSection 4.3 shows the experimental results withthe learned model trained and tested on the datawith the same genre, i.e., non-WSJ articles.In order to evaluate whether the learned modelcan perform well on data from different newsgenres, we tested it on the unseen WSJ data (156event instances).
The performance (including theprecision, recall, and F-score for each class) isshown in Table 8.
The precision (75.0%) is veryclose to the test performance on the non-WSJ2 In the ?Syn+Hyper?
cases, the learning algorithm with andwithout local context gives identical results, probably be-cause the other features dominate.398Event Only (n = 0) Event Only + Syntactic Event + Syn + Hyper ClassPrec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
FShort 0.742 0.465  0.571 0.758 0.587 0.662 0.707    0.606 0.653Long 0.748 0.908 0.821 0.792 0.893 0.839 0.793 0.857 0.823Overall Prec.
74.7% 78.2% 76.6%Local Context (n = 2) Context + Syntactic Context + Syn + HyperShort 0.672 0.568 0.615 0.710 0.600    0.650 0.707    0.606 0.653Long 0.774 0.842 0.806 0.791 0.860 0.824 0.793 0.857 0.823Overall Prec.
74.2% 76.6% 76.6%Table 7: Feature Evaluation with Different Feature Sets using SVM.Class Prec.
Rec.
FShort 0.692   0.610 0.649Long 0.779   0.835 0.806Overall Prec.
75.0%Table 8: Test Performance on WSJ data.P(A) P(E) Kappa0.151 0.762 0.7980.143 0.764Table 9: Inter-Annotator Agreement for MostLikely Temporal Unit.data, and indicates the significant generalizationcapacity of the learned model.5 Learning the Most Likely TemporalUnitThese encouraging results have prompted us totry to learn more fine-grained event duration in-formation, viz., the most likely temporal units ofevent durations (cf.
(Rieger 1974)?s ORDER-HOURS, ORDERDAYS).For each original event annotation, we can ob-tain the most likely (mean) duration by averagingits lower and upper bound durations, and assign-ing it to one of seven classes (i.e., second, min-ute, hour, day, week, month, and year) based onthe temporal unit of its most likely duration.However, human agreement on this more fine-grained task is low (44.4%).
Based on this obser-vation, instead of evaluating the exact agreementbetween annotators, an ?approximate agreement?is computed for the most likely temporal unit ofevents.
In ?approximate agreement?, temporalunits are considered to match if they are the sametemporal unit or an adjacent one.
For example,?second?
and ?minute?
match, but ?minute?
and?day?
do not.Some preliminary experiments have been con-ducted for learning this multi-classification task.The same data sets as in the binary classificationtask were used.
The only difference is that theclass for each instance is now labeled with oneAlgorithm PrecisionBaseline 51.5%C4.5 56.4%NB 65.8%SVM 67.9%Human Agreement 79.8%Table 10: Overall Test Precisions.of the seven temporal unit classes.The baseline for this multi-classification taskis always taking the temporal unit which with itstwo neighbors spans the greatest amount of data.Since the ?week?, ?month?, and ?year?
classestogether take up largest portion (51.5%) of thedata, the baseline is always taking the ?month?class, where both ?week?
and ?year?
are alsoconsidered a match.
Table 9 shows the inter-annotator agreement results for most likely tem-poral unit when using ?approximate agreement?.Human agreement (the upper bound) for thislearning task increases from 44.4% to 79.8%.10-fold cross validation was also used to trainthe learning models, which were then tested onthe unseen held-out test set.
The performance ofthe three algorithms is shown in Table 10.
Thebest performing learning algorithm is again SVMwith 67.9% test precision.
Compared with thebaseline (51.5%) and human agreement (79.8%),this again is a very promising result, especiallyfor a multi-classification task with such limitedtraining data.
It is reasonable to expect that whenmore annotated data becomes available, thelearning algorithm will achieve higher perform-ance when learning this and more fine-grainedevent duration information.Although the coarse-grained duration informa-tion may look too coarse to be useful, computershave no idea at all whether a meeting event takesseconds or centuries, so even coarse-grained es-timates would give it a useful rough sense of howlong each event may take.
More fine-grained du-ration information is definitely more desirablefor temporal reasoning tasks.
But coarse-grained399durations to a level of temporal units can alreadybe very useful.6 ConclusionIn the research described in this paper, we haveaddressed a problem -- extracting informationabout event durations encoded in event descrip-tions -- that has heretofore received very littleattention in the field.
It is information that canhave a substantial impact on applications wherethe temporal placement of events is important.Moreover, it is representative of a set of prob-lems ?
making use of the vague information intext ?
that has largely eluded empirical ap-proaches in the past.
In (Pan et al, 2006), weexplicate the linguistic categories of the phenom-ena that give rise to grossly discrepant judgmentsamong annotators, and give guidelines on resolv-ing these discrepancies.
In the present paper, wedescribe a method for measuring inter-annotatoragreement when the judgments are intervals on ascale; this should extend from time to other sca-lar judgments.
Inter-annotator agreement is toolow on fine-grained judgments.
However, for thecoarse-grained judgments of more than or lessthan a day, and of approximate agreement ontemporal unit, human agreement is acceptablyhigh.
For these cases, we have shown that ma-chine-learning techniques achieve impressiveresults.AcknowledgmentsThis work was supported by the Advanced Re-search and Development Activity (ARDA), nowthe Disruptive Technology Office (DTO), underDOD/DOI/ARDA Contract No.
NBCHC040027.The authors have profited from discussions withHoa Trang Dang, Donghui Feng, Kevin Knight,Daniel Marcu, James Pustejovsky, Deepak Ravi-chandran, and Nathan Sobo.ReferencesB.
Boguraev and R. K. Ando.
2005.
TimeML-Compliant Text Analysis for Temporal Reasoning.In Proceedings of International Joint Conferenceon Artificial Intelligence (IJCAI).E.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proceedings of the Third Conference onApplied Natural Language Processing.J.
Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Lin-gustics, 22(2):249?254.R.
O. Duda and P. E. Hart.
1973.
Pattern Classifica-tion and Scene Analysis.
Wiley, New York.E.
Filatova and E. Hovy.
2001.
Assigning Time-Stamps to Event-Clauses.
Proceedings of ACLWorkshop on Temporal and Spatial Reasoning.P.
Fortemps.
1997.
Jobshop Scheduling with Impre-cise Durations: A Fuzzy Approach.
IEEE Transac-tions on Fuzzy Systems Vol.
5 No.
4.D.
Gildea and D. Jurafsky.
2002.
Automatic Labelingof Semantic Roles.
Computational Linguistics,28(3):245-288.L.
Godo and L. Vila.
1995.
Possibilistic TemporalReasoning based on Fuzzy Temporal Constraints.In Proceedings of International Joint Conferenceon Artificial Intelligence (IJCAI).U.
Hermjakob and R. J. Mooney.
1997.
LearningParse and Translation Decisions from Exampleswith Rich Context.
In Proceedings of the 35th An-nual Meeting of the Association for ComputationalLinguistics (ACL).J.
Hitzeman, M. Moens, and C. Grover.
1995.
Algo-rithms for Analyzing the Temporal Structure ofDiscourse.
In Proceedings of EACL.
Dublin, Ire-land.J.
R. Hobbs and V. Kreinovich.
2001.
Optimal Choiceof Granularity in Commonsense Estimation: WhyHalf Orders of Magnitude, In Proceedings of Joint9th IFSA World Congress and 20th NAFIPS Inter-national Conference, Vacouver, British Columbia.K.
Krippendorf.
1980.
Content Analysis: An introduc-tion to its methodology.
Sage Publications.I.
Mani and G. Wilson.
2000.
Robust Temporal Proc-essing of News.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Lin-guistics (ACL).G.
A. Miller.
1990.
WordNet: an On-line Lexical Da-tabase.
International Journal of Lexicography 3(4).F.
Pan, R. Mulkar, and J. R. Hobbs.
2006.
An Anno-tated Corpus of Typical Durations of Events.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC),Genoa, Italy.J.
Pustejovsky, P. Hanks, R.
Saur?, A.
See, R. Gai-zauskas, A. Setzer, D. Radev, B. Sundheim, D.Day, L. Ferro and M. Lazo.
2003.
The timebankcorpus.
In Corpus Linguistics, Lancaster, U.K.J.
R. Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Francisco.C.
J. Rieger.
1974.
Conceptual memory: A theory andcomputer program for processing and meaningcontent of natural language utterances.
StanfordAIM-233.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer-Verlag, New York.I.
H. Witten and E. Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques, 2ndEdition, Morgan Kaufmann, San Francisco.400
