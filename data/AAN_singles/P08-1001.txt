Proceedings of ACL-08: HLT, pages 1?9,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMining Wiki Resources for Multilingual Named Entity RecognitionAlexander E. Richman Patrick SchoneDepartment of Defense Department of DefenseWashington, DC 20310 Fort George G. Meade, MD 20755arichman@psualum.com pjschon@tycho.ncsc.milAbstractIn this paper, we describe a system by whichthe multilingual characteristics of Wikipediacan be utilized to annotate a large corpus oftext with Named Entity Recognition (NER)tags requiring minimal human interventionand no linguistic expertise.
This process,though of value in languages for whichresources exist, is particularly useful for lesscommonly taught languages.
We show howthe Wikipedia format can be used to identifypossible named entities and discuss in detailthe process by which we use the Categorystructure inherent to Wikipedia to determinethe named entity type of a proposed entity.We further describe the methods by whichEnglish language data can be used tobootstrap the NER process in other languages.We demonstrate the system by using thegenerated corpus as training sets for a variantof BBN's Identifinder in French, Ukrainian,Spanish, Polish, Russian, and Portuguese,achieving overall F-scores as high as 84.7%on independent, human-annotated corpora,comparable to a system trained on up to40,000 words of human-annotated newswire.1 IntroductionNamed Entity Recognition (NER) has long been amajor task of natural language processing.
Most ofthe research in the field has been restricted to a fewlanguages and almost all methods require substan-tial linguistic expertise, whether creating a rule-based technique specific to a language or manuallyannotating a body of text to be used as a trainingset for a statistical engine or machine learning.In this paper, we focus on using the multilingualWikipedia (wikipedia.org) to automatically createan annotated corpus of text in any given language,with no linguistic expertise required on the part ofthe user at run-time (and only English knowledgerequired during development).
The expectation isthat for any language in which Wikipedia issufficiently well-developed, a usable set of trainingdata can be obtained with minimal humanintervention.
As Wikipedia is constantlyexpanding, it follows that the derived models arecontinually improved and that increasingly manylanguages can be usefully modeled by this method.In order to make sure that the process is aslanguage-independent as possible, we declined tomake use of any non-English linguistic resourcesoutside of the Wikimedia domain (specifically,Wikipedia and the English language Wiktionary(en.wiktionary.org)).
In particular, we did not useany semantic resources such as WordNet or part ofspeech taggers.
We used our automatically anno-tated corpus along with an internally modifiedvariant of BBN's IdentiFinder (Bikel et al, 1999),specifically modified to emphasize fast textprocessing,  called ?PhoenixIDF,?
to create severallanguage models that could be tested outside of theWikipedia framework.
We built on top of anexisting system, and left existing lists and tablesintact.
Depending on language, we evaluated ourderived models against human or machineannotated data sets to test the system.2 Wikipedia2.1  StructureWikipedia is a multilingual, collaborative encyclo-pedia on the Web which is freely available for re-search purposes.
As of October 2007, there wereover 2 million articles in English, with versionsavailable in 250 languages.
This includes 30 lan-guages with at least 50,000 articles and another 40with at least 10,000 articles.
Each language isavailable for download (download.wikimedia.org)in a text format suitable for inclusion in a database.For the remainder of this paper, we refer to thisformat.1Within Wikipedia, we take advantage of fivemajor features:?
Article links, links from one article to anotherof the same language;?
Category links, links from an article to special?Category?
pages;?
Interwiki links, links from an article to apresumably equivalent, article in anotherlanguage;?
Redirect pages, short pages which oftenprovide equivalent names for an entity; and?
Disambiguation pages, a page with littlecontent that links to multiple similarly namedarticles.The first three types are collectively referred to aswikilinks.A typical sentence in the database format lookslike the following:?Nescopeck Creek is a [[tributary]] of the [[NorthBranch Susquehanna River]] in [[Luzerne County,Pennsylvania|Luzerne County]].
?The double bracket is used to signify wikilinks.
Inthis snippet, there are three articles links to Englishlanguage Wikipedia pages, titled ?Tributary,?
?North Branch Susquehanna River,?
and ?LuzerneCounty, Pennsylvania.?
Notice that in the last link,the phrase preceding the vertical bar is the name ofthe article, while the following phrase is what isactually displayed to a visitor of the webpage.Near the end of the same article, we find thefollowing representations of Category links:[[Category:Luzerne County, Pennsylvania]],[[Category:Rivers of Pennsylvania]], {{Pennsyl-vania-geo-stub}}.
The first two are direct links toCategory pages.
The third is a link to a Template,which (among other things) links the article to?Category:Pennsylvania geography stubs?.
Wewill typically say that a given entity belongs tothose categories to which it is linked in these ways.The last major type of wikilink is the link be-tween different languages.
For example, in theTurkish language article ?Kanuni Sultan S?ley-man?
one finds a set of links including [[en:Sulei-man the Magnificent]] and [[ru:????????
I]].These represent links to the English languagearticle ?Suleiman the Magnificent?
and the Russianlanguage article ?????????
I.?
In almost allcases, the articles linked in this manner representarticles on the same subject.A redirect page is a short entry whose sole pur-pose is to direct a query to the proper page.
Thereare a few reasons that redirect pages exist, but theprimary purpose is exemplified by the fact that?USA?
is an entry which redirects the user to thepage entitled ?United States.?
That is, in the vastmajority of cases, redirect pages provide anothername for an entity.A disambiguation page is a special articlewhich contains little content but typically lists anumber of entries which might be what the userwas seeking.
For instance, the page ?Franklin?contains 70 links, including the singer ?ArethaFranklin,?
the town ?Franklin, Virginia,?
the?Franklin River?
in Tasmania, and the cartooncharacter ?Franklin (Peanuts).?
Most disambigua-tion pages are in Category:Disambiguation or oneof its subcategories.2.2 Related StudiesWikipedia has been the subject of a considerableamount of research in recent years includingGabrilovich and Markovitch (2007), Strube andPonzetto (2006), Milne et al (2006), Zesch et al(2007), and Weale (2007).
The most relevant toour work are Kazama and Torisawa (2007), Toraland Mu?oz (2006), and Cucerzan (2007).
Moredetails follow, but it is worth noting that all knownprior results are fundamentally monolingual, oftendeveloping algorithms that can be adapted to otherlanguages pending availability of the appropriatesemantic resource.
In this paper, we emphasize theuse of links between articles of different languages,specifically between English (the largest and bestlinked Wikipedia) and other languages.Toral and Mu?oz (2006) used Wikipedia to cre-ate lists of named entities.
They used the firstsentence of Wikipedia articles as likely definitionsof the article titles, and used them to attempt toclassify the titles as people, locations, organiza-tions, or none.
Unlike the method presented in thispaper, their algorithm relied on WordNet (or anequivalent resource in another language).
The au-thors noted that their results would need to pass amanual supervision step before being useful for theNER task, and thus did not evaluate their results inthe context of a full NER system.Similarly, Kazama and Torisawa (2007) usedWikipedia, particularly the first sentence of eacharticle, to create lists of entities.
Rather thanbuilding entity dictionaries associating words and2phrases to the classical NER tags (PERSON, LO-CATION, etc.)
they used a noun phrase followingforms of the verb ?to be?
to derive a label.
For ex-ample, they used the sentence ?Franz Fischler ... isan Austrian politician?
to associate the label ?poli-tician?
to the surface form ?Franz Fischler.?
Theyproceeded to show that the dictionaries generatedby their method are useful when integrated into anNER system.
We note that their technique reliesupon a part of speech tagger, and thus was not ap-propriate for inclusion as part of our non-Englishsystem.Cucerzan (2007), by contrast to the above,used Wikipedia primarily for Named Entity Dis-ambiguation, following the path of Bunescu andPa?ca (2006).
As in this paper, and unlike theabove mentioned works, Cucerzan made use of theexplicit Category information found within Wiki-pedia.
In particular, Category and related list-derived data were key pieces of information usedto differentiate between various meanings of anambiguous surface form.
Unlike in this paper,Cucerzan did not make use of the Category infor-mation to identify a given entity as a member ofany particular class.
We also note that the NERcomponent was not the focus of the research, andwas specific to the English language.3 Training Data Generation3.1   Initial Set-up and OverviewOur approach to multilingual NER is to pull backthe decision-making process to English wheneverpossible, so that we could apply some level of lin-guistic expertise.
In particular, by focusing ononly one language, we could take maximum ad-vantage of the Category structure, something verydifficult to do in the general multilingual case.For computational feasibility, we downloadedvarious language Wikipedias and the English lan-guage Wiktionary   in their text (.xml) format andstored each language as a table within a singleMySQL database.
We only stored the title, idnumber, and body (the portion between the<TEXT> and </TEXT> tags) of each article.We elected to use the ACE Named Entity typesPERSON, GPE (Geo-Political Entities), OR-GANIZATION, VEHICLE, WEAPON, LOCA-TION, FACILITY, DATE, TIME, MONEY, andPERCENT.
Of course, if some of these types werenot marked in an existing corpus or not needed fora given purpose, the system can easily be adapted.Our goal was to automatically annotate the textportion of a large number of non-English articleswith tags like <ENAMEX TYPE=?GPE?>PlaceName</ENAMEX> as used in MUC (MessageUnderstanding Conference).
In order to do so, oursystem first identifies words and phrases within thetext that might represent entities, primarily throughthe use of wikilinks.
The system then uses catego-ry links and/or interwiki links to associate thatphrase with an English language phrase or set ofCategories.
Finally, it determines the appropriatetype of the English language data and assumes thatthe original phrase is of the same type.In practice, the English language categorizationshould be treated as one-time work, since it isidentical regardless of the language model beingbuilt.
It is also the only stage of development atwhich we apply substantial linguistic knowledge,even of English.In the sections that follow, we begin by show-ing how the English language categorization isdone.
We go on to describe how individual non-English phrases are associated with English lan-guage information.
Next, we explain how possibleentities are initially selected.
Finally, we discusssome optional steps as well as how and why theycould be used.3.2   English Language CategorizationFor each article title of interest (specifically ex-cluding Template pages, Wikipedia admistrativepages, and articles whose title begins with ?Listof?
), we extracted the categories to which that en-try was assigned.
Certainly, some of these cate-gory assignments are much more useful than othersFor instance, we would expect that any entry in?Category:Living People?
or ?Category:BritishLawyers?
will refer to a person while any entry in?Category:Cities in Norway?
will refer to a GPE.On the other hand, some are entirely unhelpful,such as ?Category:1912 Establishments?
whichincludes articles on Fenway Park (a facility), theRepublic of China (a GPE), and the BetterBusiness Bureau (an organization).
Other catego-ries can reliably be used to determine that thearticle does not refer to a named entity, such as?Category:Endangered species.?
We manuallyderived a relatively small set of key phrases, themost important of which are shown in Table 1.3Table 1: Some Useful Key Category PhrasesPERSON ?People by?, ?People in?, ?People from?,?Living people?, ?births?,  ?deaths?,  ?byoccupation?, ?Surname?, ?Given names?,?Biography stub?, ?human names?ORG ?Companies?, ?Teams?, ?Organizations?,?Businesses?, ?Media by?, ?Politicalparties?, ?Clubs?, ?Advocacy groups?,?Unions?, ?Corporations?, ?Newspapers?,?Agencies?, ?Colleges?, ?Universities?
,?Legislatures?, ?Company stub?, ?Teamstub?, ?University stub?, ?Club stub?GPE ?Cities?, ?Countries?, ?Territories?,?Counties?, ?Villages?, ?Municipalities?,?States?
(not part of ?United States?
),?Republics?, ?Regions?, ?Settlements?DATE ?Days?, ?Months?, ?Years?, ?Centuries?NONE ?Lists?, ?List of?, ?Wars?, ?Incidents?For each article, we searched the categoryhierarchy until a threshold of reliability was passedor we had reached a preset limit on how far wewould search.For example, when the system tries to classify?Jacqueline Bhabha,?
it extracts the categories?British Lawyers,?
?Jewish American Writers,?and ?Indian Jews.?
Though easily identifiable to ahuman, none of these matched any of our keyphrases, so the system proceeded to extract thesecond order categories ?Lawyers by nationality,?
?British legal professionals,?
?American writers byethnicity,?
?Jewish writers,?
?Indian people byreligion,?
and ?Indian people by ethnic or nationalorigin?
among others.
?People by?
is on our keyphrase list, and the two occurrences passed ourthreshold, and she was then correctly identified.If an article is not classified by this method, wecheck whether it is a disambiguation page (whichoften are members solely of ?Category:Disam-biguation?).
If it is, the links within are checked tosee whether there is a dominant type.
For instance,the page ?Amanda Foreman?
is a disambiguationpage, with each link on the page leading to aneasily classifiable article.Finally, we use Wiktionary, an online colla-borative dictionary, to eliminate some commonnouns.
For example, ?Tributary?
is an entry inWikipedia which would be classified as a Locationif viewed solely by Category structure.
However,it is found as a common noun in Wiktionary, over-ruling the category based result.3.3 Multilingual CategorizationWhen attempting to categorize a non-English termthat has an entry in its language?s Wikipedia, weuse two techniques to make a decision based onEnglish language information.
First, wheneverpossible, we find the title of an associated Englishlanguage article by searching for a wikilinkbeginning with ?en:?.
If such a title is found, thenwe categorize the English article as shown inSection 3.2, and decide that the non-English title isof the same type as its English counterpart.
Wenote that links to/from English are the mostcommon interlingual wikilinks.Of course, not all articles worldwide have Eng-lish equivalents (or are linked to such even if theydo exist).
In this case, we attempt to make a deci-sion based on Category information, associatingthe categories with their English equivalents, whenpossible.
Fortunately, many of the most usefulcategories have equivalents in many languages.For example, the Breton town of Erquy has asubstantial article in the French language Wikipe-dia, but no article in English.
The system proceedsby determining that Erquy belongs to four Frenchlanguage categories:  ?Cat?gorie:Commune desC?tes-d'Armor,?
?Cat?gorie:Ville portuaire deFrance,?
?Cat?gorie:Port de plaisance,?
and?Cat?gorie:Station baln?aire.?
The system pro-ceeds to associate these, respectively, with ?Cate-gory:Communes of C?tes-d'Armor,?
UNKNOWN,?Category:Marinas,?
and ?Category:Seaside re-sorts?
by looking in the French language pages ofeach for wikilinks of the form [[en:...]].The first is a subcategory of ?Category:Cities,towns and villages in France?
and is thus easilyidentified by the system as a category consisting ofentities of type GPE.
The other two are ambiguouscategories (facility and organization elements inaddition to GPE).
Erquy is then determined to bea GPE by majority vote of useful categories.We note that the second French category actu-ally has a perfectly good English equivalent (Cate-gory:Port cities and towns in France), but no onehas linked them as of this writing.
We also notethat the ambiguous categories are much moreGPE-oriented in French.
The system still makesthe correct decision despite these factors.We do not go beyond the first level categoriesor do any disambiguation in the non-English case.Both are avenues for future improvement.43.4 The Full SystemTo generate a set of training data in a given lan-guage, we select a large number of articles from itsWikipedia (50,000 or more is recommended, whenpossible).
We prepare the text by removing exter-nal links, links to images, category and interlinguallinks, as well as some formatting.
The main pro-cessing of each article takes place in several stages,whose primary purposes are as follows:?
The first pass uses the explicit article linkswithin the text.?
We then search an associated English languagearticle, if available, for additional information.?
A second pass checks for multi-word phrasesthat exist as titles of Wikipedia articles.?
We look for certain types of person andorganization instances.?
We perform additional processing foralphabetic or space-separated languages,including a third pass looking for single wordWikipedia titles.?
We use regular expressions to locate additionalentities such as numeric dates.In the first pass, we attempt to replace all wiki-links with appropriate entity tags.
We assume atthis stage that any phrase identified as an entity atsome point in the article will be an entity of thesame type throughout the article, since it is com-mon for contributors to make the explicit link onlyon the first occasion that it occurs.
We also as-sume that a phrase in a bold font within the first100 characters is an equivalent form of the title ofthe article as in this start of the article on Erquy:?Erquy (Erge-ar-Mor en breton, Erqi en gallo)?.The parenthetical notation gives alternate names inthe Breton and Gallo languages.
(In Wiki databaseformat, bold font is indicated by three apostrophesin succession.
)If the article has an English equivalent, wesearch that article for wikilinked phrases as well,on the assumption that both articles will refer tomany of the same entities.
As the English lan-guage Wikipedia is the largest, it frequently con-tains explicit references to and articles onsecondary people and places mentioned, but notlinked, within a given non-English article.
Afterthis point, the text to be annotated contains noWikipedia specific information or formatting.In the second pass, we look for strings of 2 to 4words which were not wikilinked but which haveWikipedia entries of their own or are partialmatches to known people and organizations (i.e.
?Mary Washington?
in an article that contains?University of Mary Washington?).
We requirethat each such string contains something other thana lower case letter (when a language does not usecapitalization, nothing in that writing system isconsidered to be lower case for this purpose).When a word is in more than one such phrase, thelongest match is used.We then do some special case processing.When an organization is followed by something inparentheses such as <ENAMEX TYPE=?ORGAN-IZATION?>Maktab al-Khadam?t</ENAMEX>(MAK), we hypothesize that the text in theparentheses is an alternate name of the organiza-tion.
We also looked for unmarked strings of theform X.X.
followed by a capitalized word, whereX represents any capital letter, and marked eachoccurrence as a PERSON.For space-separated or alphabetic languages,we did some additional processing at this stage toattempt to identify more names of people.
Using alist of names derived from Wiktionary (Appen-dix:Names) and optionally a list derived fromWikipedia (see Section 3.5.1), we mark possibleparts of names.
When two or more are adjacent,we mark the sequence as a PERSON.
Also, we fillin partial lists of names by assuming single non-lower case words between marked names are actu-ally parts of names themselves.
That is, we wouldreplace <ENAMEX TYPE=?PERSON?>FredSmith</ENAMEX>, Somename <ENAMEXTYPE=?PERSON?>Jones </ENAMEX> with<ENAMEX TYPE=?PERSON?> Fred Smith</E-NAMEX>, <ENAMEX TYPE= ?PERSON?>Somename Jones</ENAMEX>.
At this point, weperformed a third pass through the article.
Wemarked all non-lower case single words which hadtheir own Wikipedia entry, were part of a knownperson's name, or were part of a knownorganization's name.Afterwards, we used a series of simple, lan-guage-neutral regular expressions to find addi-tional TIME, PERCENT, and DATE entities suchas ?05:30?
and ?12-07-05?.
We also executedcode that included quantities of money within aNUMEX tag, as in converting 500 <NUMEXTYPE=?MONEY?>USD</NUMEX> into <NU-MEX TYPE=?MONEY?>500 USD</NUMEX>.53.5 Optional Processing3.5.1 Recommended AdditionsAll of the above could be run with almost no un-derstanding of the language being modeled(knowing whether the language was space-sepa-rated and whether it was alphabetic or character-based were the only things used).
However, formost languages, we spent a small amount of time(less than one hour) browsing Wikipedia pages toimprove performance in some areas.We suggest compiling a small list of stopwords.
For our purposes, the determiners and themost common prepositions are sufficient, though alonger list could be used for the purpose of com-putational efficiency.We also recommend compiling a list of numberwords as well as compiling a list of currencies,since they are not capitalized in many languages,and may not be explicitly linked either.
Many lan-guages have a page on ISO 4217 which containsall of the currency information, but the formatvaries sufficiently from language to language tomake automatic extraction difficult.
Together,these allow phrases like this (taken from theFrench Wikipedia) to be correctly marked in itsentirety as an entity of type MONEY: ?25 millionsde dollars.
?If a language routinely uses honorifics such asMr.
and Mrs., that information can also be foundquickly.
Their use can lead to significant im-provements in PERSON recognition.During preprocessing, we typically collected alist of people names automatically, using the entityidentification methods appropriate to titles ofWikipedia articles.
We then used these namesalong with the Wiktionary derived list of namesduring the main processing.
This does introducesome noise as the person identification is not per-fect, but it ordinarily increases recall by more thanit reduces precision.3.5.2 Language Dependent AdditionsOur usual, language-neutral processing onlyconsiders wikilinks within a single article whendetermining the type of unlinked words andphrases.
For example, if an article included thesentence ?The [[Delaware River|Delaware]] formsthe boundary between [[Pennsylvania]] and [[NewJersey]]?, our system makes the assumption thatevery occurrence of the unlinked word ?Delaware?appearing in the same article is also referring to theriver and thus mark it as a LOCATION.For some languages, we preferred an alternateapproach, best illustrated by an example:  Theword ?Washington?
without context could refer to(among others) a person, a GPE, or an organiza-tion.
We could work through all of the explicitwikilinks in all articles (as a preprocessing step)whose surface form is Washington and count thenumber pointing to each.
We could then decidethat every time the word Washington appearswithout an explicit link, it should be marked as itsmost common type.
This is useful for the Slaviclanguages, where the nominative form is typicallyused as the title of Wikipedia articles, while othercases appear frequently (and are rarely wikilinked).At the same time, we can do a second type ofpreprocessing which allows more surface forms tobe categorized.
For instance, imagine that we werein a Wikipedia with no article or redirect associ-ated to ?District of Columbia?
but that someonehad made a wikilink of the form [[Washing-ton|District of Columbia]].
We would then makethe assumption that for all articles, District of Co-lumbia is of the same type as Washington.For less developed wikipedias, this can behelpful.
For languages that have reasonably welldeveloped Wikipedias and where entities rarely, ifever, change form for grammatical reasons (suchas French), this type of preprocessing is virtuallyirrelevant.
Worse, this processing is definitely notrecommended for languages that do not use capi-talization because it is not unheard of for people toinclude sections like: ?The [[Union Station|trainstation]] is located at ...?
which would cause thephrase ?train station?
to be marked as a FACILITYeach time it occurred.
Of course, even in lan-guages with capitalization, ?train station?
would bemarked incorrectly in the article in which theabove was located, but the mistake would be iso-lated, and should have minimal impact overall.4 Evaluation and ResultsAfter each data set was generated, we used the textas a training set for input to PhoenixIDF.
We hadthree human annotated test sets, Spanish, Frenchand Ukrainian, consisting of newswire.
Whenhuman annotated sets were not available, we heldout more than 100,000 words of text generated byour wiki-mining process to use as a test set.
For theabove languages, we included wiki test sets for6comparison purposes.
We will give our results asF-scores in the Overall, DATE, GPE,ORGANIZATION, and PERSON categories usingthe scoring metric in (Bikel et.
al, 1999).
Theother ACE categories are much less common, andcontribute little to the overall score.4.1 Spanish Language EvaluationThe Spanish Wikipedia is a substantial, well-de-veloped Wikipedia, consisting of more than290,000 articles as of October 2007.
We used twotest sets for comparison purposes.
The first con-sists of 25,000 words of human annotated news-wire derived from the ACE 2007 test set, manuallymodified to conform to our extended MUC-stylestandards.
The second consists of 335,000 wordsof data generated by the Wiki process held-outduring training.Table 2: Spanish ResultsF (prec.
/ recall) Newswire Wiki test setALL .827 (.851 / .805) .846 (.843 / .848)DATE .912 (.861 / .970) .925 (.918 / .932)GPE .877 (.914 / .843) .877 (.886 / .868)ORG .629 (.681 / .585) .701 (.703 / .698)PERSON .906 (.921 / .892) .821 (.810 / .833)There are a few particularly interesting resultsto note.
First, because of the optional processing,recall was boosted in the PERSON category at theexpense of precision.
The fact that this categoryscores higher against newswire than against thewiki data suggests that the not-uncommon, butisolated, occurrences of non-entities being markedas PERSONs in training have little effect on theoverall system.
Contrarily, we note that deletionsare the dominant source of error in the ORGANI-ZATION category, as seen by the lower recall.The better performance on the wiki set seems tosuggest that either Wikipedia is relatively poor inOrganizations or that PhoenixIDF underperformswhen identifying Organizations relative to othercategories or a combination.An important question remains: ?How do theseresults compare to other methodologies??
In par-ticular, while we can get these results for free, howmuch work would traditional methods require toachieve comparable results?To attempt to answer this question, we trainedPhoenixIDF on additional ACE 2007 Spanish lan-guage data converted to MUC-style tags, andscored its performance using the same set ofnewswire.
Evidently, comparable performance toour Wikipedia derived system requires between20,000 and 40,000 words of human-annotatednewswire.
It is worth noting that Wikipedia itselfis not newswire, so we do not have a perfect com-parison.Table 3: Traditional Training~ Words of Training  Overall F-score3500 .74610,000 .76020,000 .80740,000 .8474.2  French Language EvaluationThe French Wikipedia is one of the largestWikipedias, containing more than 570,000 articlesas of October 2007.
For this evaluation, we have25,000 words of human annotated newswire(Agence France Presse, 30 April and 1 May 1997)covering diverse topics.
We used 920,000 wordsof Wiki-derived data for the second test.Table 4: French ResultsF (prec.
/ recall) Newswire Wiki test setALL .847 (.877 / .819) .844 (.847 / .840)DATE .921 (.897 / .947) .910 (.888 / .934)GPE .907 (.933 / .882) .868 (.889 / .849)ORG .700 (.794 / .625) .718 (.747 / .691)PERSON .880 (.874 / .885) .823 (.818 / .827)The overall results seem comparable to the Span-ish, with the slightly better overall performancelikely correlated to the somewhat more developedWikipedia.
We did not have sufficient quantities ofannotated data to run a test of the traditional meth-ods, but Spanish and French are sufficiently similarlanguages that we expect this model is comparableto one created with about 40,000 words of human-annotated data.74.3 Ukrainian Language EvaluationThe Ukrainian Wikipedia is a medium-sizedWikipedia with 74,000 articles as of October 2007.Also, the typical article is shorter and less well-linked to other articles than in the French or Span-ish versions.
Moreover, entities tend to appear inmany surface forms depending on case, leading usto expect somewhat worse results.
In the Ukrain-ian case, the newswire consisted of approximately25,000 words from various online news sites cov-ering primarily political topics.
We also held outaround 395,000 words for testing.
We were alsoable to run a comparison test as in Spanish.Table 5: Ukrainian ResultsF (prec.
/ recall) Newswire Wiki test setALL .747 (.863 / .649) .807 (.809 / .806)DATE .780 (.759 / .803) .848 (.842 / .854)GPE .837 (.833 / .841) .887 (.901 / .874)ORG .585 (.800 / .462) .657 (.678 / .637)PERSON .764 (.899 / .664) .690 (.675 / .706)Table 6: Traditional Training~ Words of Training  Overall F-score5000 .66210,000 .69215,000 .74020,000 .761The Ukrainian newswire contained a much higherproportion of organizations than the French orSpanish versions, contributing to the overall lowerscore.
The Ukrainian language Wikipedia itselfcontains very few articles on organizations relativeto other types, so the distribution of entities of thetwo test sets are quite different.
We also see thatthe Wiki-derived model performs comparably to amodel trained on 15-20,000 words of human-annotated text.4.4 Other LanguagesFor Portuguese, Russian, and Polish, we did nothave human annotated corpora available for test-ing.
In each case, at least 100,000 words were heldout from training to be used as a test set.
It seemssafe to suppose that if suitable human-annotatedsets were available for testing, the PERSON scorewould likely be higher, and the ORGANIZATIONscore would likely be lower, while the DATE andGPE scores would probably be comparable.Table 7: Other Language ResultsF-score Polish Portuguese RussianALL .859 .804 .802DATE .891 .861 .822GPE .916 .826 .867ORG .785 .706 .712PERSON .836 .802 .7515 ConclusionsIn conclusion, we have demonstrated that Wikipe-dia can be used to create a Named Entity Recogni-tion system with performance comparable to onedeveloped from 15-40,000 words of human-anno-tated newswire, while not requiring any linguisticexpertise on the part of the user.
This level of per-formance, usable on its own for many purposes,can likely be obtained currently in 20-40 lan-guages, with the expectation that more languageswill become available, and that better models canbe developed, as Wikipedia grows.Moreover, it seems clear that a Wikipedia-de-rived system could be used as a supplement toother systems for many more languages.
In par-ticular, we have, for all practical purposes, embed-ded in our system an automatically generatedentity dictionary.In the future, we would like to find a way toautomatically generate the list of key words andphrases for useful English language categories.This could implement the work of Kazama andTorisawa, in particular.
We also believe perform-ance could be improved by using higher order non-English categories and better disambiguation.
Wecould also experiment with introducing automati-cally generated lists of entities into PhoenixIDFdirectly.
Lists of organizations might be parti-cularly useful, and ?List of?
pages are common inmany languages.8ReferencesBikel, D., R. Schwartz, and R. Weischedel.
1999.An algorithm that learns what's in a name.
Ma-chine  Learning, 211-31.Bunescu, R and M. Pa?ca.
2006.
Using Encyclope-dic knowledge for named entity disambigua-tion.
In  Proceedings of EACL, 9-16.Cucerzan, S. 2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In  Pro-ceedings of EMNLP/CoNLL, 708-16.Gabrilovitch, E. and S. Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceed-ings of IJCAI, 1606-11.Gabrilovitch, E. and S. Markovitch.
2006.
Over-coming the brittleness bottleneck usingWikipedia: enhancing text categorization withencyclopedic knowledge.
In Proceedings ofAAAI, 1301-06.Gabrilovitch, E. and S. Markovitch.
2005.
Featuregeneration for text categorization using worldknowledge.
In Proceedings of IJCAI, 1048-53.Kazama, J. and K. Torisawa.
2007.
ExploitingWikipedia as external knowledge for namedentity recognition.
In Proceedings ofEMNLP/CoNLL, 698-707.Milne, D., O. Medelyan and I. Witten.
2006.
Min-ing domain-specific thesauri from Wikipedia: acase study.
Web Intelligence 2006, 442-48Strube, M. and S. P. Ponzeto.
2006.
WikiRelate!Computing  semantic relatedness usingWikipedia.
In Proceedings of AAAI, 1419-24.Toral, A. and R.  Mu?oz.
2006.
A proposal toautomatically build and maintain gazetteers fornamed entity recognition by using Wikipedia.In Proceedings of  EACL, 56-61.Weale, T. 2006.
Using Wikipedia categories fordocument classification.
Ohio St. University,preprint.Zesch, T., I. Gurevych and M. M?hlh?user.
2007.Analyzing and accessing Wikipedia as a lexicalsemantic resource.
In Proceedings of GLDV,213-21.9
