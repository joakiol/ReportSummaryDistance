Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSimple Unsupervised Grammar Inductionfrom Raw Text with Cascaded Finite State ModelsElias Ponvert, Jason Baldridge and Katrin ErkDepartment of LinguisticsThe University of Texas at AustinAustin, TX 78712{ponvert,jbaldrid,katrin.erk}@mail.utexas.eduAbstractWe consider a new subproblem of unsuper-vised parsing from raw text, unsupervised par-tial parsing?the unsupervised version of textchunking.
We show that addressing this taskdirectly, using probabilistic finite-state meth-ods, produces better results than relying onthe local predictions of a current best unsu-pervised parser, Seginer?s (2007) CCL.
Thesefinite-state models are combined in a cascadeto produce more general (full-sentence) con-stituent structures; doing so outperforms CCLby a wide margin in unlabeled PARSEVALscores for English, German and Chinese.
Fi-nally, we address the use of phrasal punctua-tion as a heuristic indicator of phrasal bound-aries, both in our system and in CCL.1 IntroductionUnsupervised grammar induction has been an ac-tive area of research in computational linguistics forover twenty years (Lari and Young, 1990; Pereiraand Schabes, 1992; Charniak, 1993).
Recent work(Headden III et al, 2009; Cohen and Smith, 2009;Ha?nig, 2010; Spitkovsky et al, 2010) has largelybuilt on the dependency model with valence of Kleinand Manning (2004), and is characterized by its re-liance on gold-standard part-of-speech (POS) anno-tations: the models are trained on and evaluated us-ing sequences of POS tags rather than raw tokens.This is also true for models which are not successorsof Klein and Manning (Bod, 2006; Ha?nig, 2010).An exception which learns from raw text andmakes no use of POS tags is the common cover linksparser (CCL, Seginer 2007).
CCL established state-of-the-art results for unsupervised constituency pars-ing from raw text, and it is also incremental and ex-tremely fast for both learning and parsing.
Unfortu-nately, CCL is a non-probabilistic algorithm basedon a complex set of inter-relating heuristics and anon-standard (though interesting) representation ofconstituent trees.
This makes it hard to extend.Note that although Reichart and Rappoport (2010)improve on Seginer?s results, they do so by select-ing training sets to best match the particular testsentences?CCL itself is used without modification.Ponvert et al (2010) explore an alternative strat-egy of unsupervised partial parsing: directly pre-dicting low-level constituents based solely on wordco-occurrence frequencies.
Essentially, this meanssegmenting raw text into multiword constituents.
Inthat paper, we show?somewhat surprisingly?thatCCL?s performance is mostly dependent on its ef-fectiveness at identifying low-level constituents.
Infact, simply extracting non-hierarchical multiwordconstituents from CCL?s output and putting a right-branching structure over them actually works betterthan CCL?s own higher level predictions.
This resultsuggests that improvements to low-level constituentprediction will ultimately lead to further gains inoverall constituent parsing.Here, we present such an improvement by usingprobabilistic finite-state models for phrasal segmen-tation from raw text.
The task for these models ischunking, so we evaluate performance on identifica-tion of multiword chunks of all constituent types aswell as only noun phrases.
Our unsupervised chun-kers extend straightforwardly to a cascade that pre-dicts higher levels of constituent structure, similarto the supervised approach of Brants (1999).
Thisforms an overall unsupervised parsing system thatoutperforms CCL by a wide margin.1077Mrs.
Ward for one was relieved?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?1(a) Chunks: (Mrs. Ward), (for one), and (was relieved)AllcamefromCray Research?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(b) Only one chunk extracted: (Cray Research)Fig.
1: Examples of constituent chunks extracted fromsyntactic trees2 DataWe use the standard data sets for unsupervised con-stituency parsing research: for English, the WallStreet Journal subset of the Penn Treebank-3 (WSJ,Marcus et al 1999); for German, the Negra corpusv2 (Krenn et al, 1998); for Chinese, the Penn Chi-nese Treebank v5.0 (CTB, Palmer et al, 2006).
Welower-case text but otherwise do not alter the rawtext of the corpus.
Sentence segmentation and tok-enization from the treebank is used.
As in previouswork, punctuation is not used for evaluation.In much unsupervised parsing work the test sen-tences are included in the training material.
Like Co-hen and Smith, Headden III et al, Spitkovsky et al,we depart from this experimental setup and keep theevaluation sets blind to the models during training.For English (WSJ) we use sections 00-22 for train-ing, section 23 for test and we develop using section24; for German (Negra) we use the first 18602 sen-tences for training, the last 1000 sentences for de-velopment and the penultimate 1000 sentences fortesting; for Chinese (CTB) we adopt the data-splitof Duan et al (2007).3 Tasks and BenchmarkEvaluation.
By unsupervised partial parsing, orsimply unsupervised chunking, we mean the seg-mentation of raw text into (non-overlapping) multi-word constituents.
The models are intended to cap-ture local constituent structure ?
the lower branchesof a constituent tree.
For this reason we evaluateWSJChunks 203KNPs 172KChnk ?
NPs 161KNegraChunks 59KNPs 33KChnk ?
NPs 23KCTBChunks 92KNPs 56KChnk ?
NPs 43KTable 1: Constituent chunks and base NPs in the datasets.% constituents % wordsWSJChunks 32.9 57.7NPs 27.9 53.1NegraChunks 45.4 53.6NPs 25.5 42.4CTBChunks 32.5 55.4NPs 19.8 42.9Table 2: Percentage of gold standard constituents andwords under constituent chunks and base NPs.using what we call constituent chunks, the subsetof gold standard constituents which are i) branch-ing (multiword) but ii) non-hierarchical (do not con-tain subconstituents).
We also evaluate our modelsbased on their performance at identifying base nounphrases, NPs that do not contain nested NPs.Examples of constituent chunks extracted fromtreebank constituent trees are in Fig.
1.
In Englishnewspaper text, constituent chunks largely corre-spond with base NPs, but this is less the case withChinese and German.
Moreover, the relationship be-tween NPs and constituent chunks is not a subset re-lation: some base NPs do have internal constituentstructure.
The numbers of constituent chunks andNPs for the training datasets are in Table 1.
The per-centage of constituents in these datasets which fallunder these definitions, and the percentage of wordsunder these constituents, are in Table 2.For parsing, the standard unsupervised parsingmetric is unlabeled PARSEVAL.
It measures preci-sion and recall on constituents produced by a parseras compared to gold standard constituents.CCL benchmark.
We use Seginer?s CCL as abenchmark for several reasons.
First, there is afree/open-source implementation facilitating exper-1078imental replication and comparison.1 More im-portantly, until recently it was the only unsuper-vised raw text constituent parser to produce re-sults competitive with systems which use gold POStags (Klein and Manning, 2002; Klein and Man-ning, 2004; Bod, 2006) ?
and the recent improvedraw-text parsing results of Reichart and Rappoport(2010) make direct use of CCL without modifica-tion.
There are other raw-text parsing systems ofnote, EMILE (Adriaans et al, 2000), ABL (van Za-anen, 2000) and ADIOS (Solan et al, 2005); how-ever, there is little consistent treebank-based evalu-ation of these models.
One study by Cramer (2007)found that none of the three performs particularlywell under treebank evaluation.
Finally, CCL out-performs most published POS-based models whenthose models are trained on unsupervised wordclasses rather than gold POS tags.
The only excep-tion we are aware of is Ha?nig?s (2010) unsuParse+,which outperforms CCL on Negra, though this isshown only for sentences with ten or fewer words.Phrasal punctuation.
Though punctuation is usu-ally entirely ignored in unsupervised parsing re-search, Seginer (2007) departs from this in one keyaspect: the use of phrasal punctuation ?
punctuationsymbols that often mark phrasal boundaries within asentence.
These are used in two ways: i) they im-pose a hard constraint on constituent spans, in thatno constituent (other than sentence root) may extendover a punctuation symbol, and ii) they contribute tothe model, specifically in terms of the statistics ofwords seen adjacent to a phrasal boundary.
We fol-low this convention and use the following set:.
?
!
; , -- ?
?The last two are ideographic full-stop and comma.24 Unsupervised partial parsingWe learn partial parsers as constrained sequencemodels over tags encoding local constituent struc-ture (Ramshaw and Marcus, 1995).
A simple tagsetis unlabeled BIO, which is familiar from supervisedchunking and named-entity recognition: the tag B1http://www.seggu.net/ccl2This set is essentially that of Seginer (2007).
While it isclear from our analysis of CCL that it does make use of phrasalpunctuation in Chinese, we are not certain whether ideographiccomma is included.denotes the beginning of a chunk, I denotes mem-bership in a chunk andO denotes exclusion from anychunk.
In addition we use the tag STOP for sentenceboundaries and phrasal punctuation.HMMs and PRLGs.
The models we use for un-supervised partial parsing are hidden Markov mod-els, and a generalization we refer to as probabilis-tic right linear grammars (PRLGs).
An HMM mod-els a sequence of observed states (words) x ={x1, x2, .
.
.
, xN} and a corresponding set of hid-den states y = {y1, y2, .
.
.
, yN}.
HMMs may bethought of as a special case of probabilistic context-free grammars, where the non-terminal symbols arethe hidden state space, terminals are the observedstates and rules are of the form NONTERM ?TERM NONTERM (assuming y1 and yN are fixedand given).
So, the emission and transition emanat-ing from yn would be characterized as a PCFG ruleyn ?
xn yn+1.
HMMs factor rule probabilities intoemission and transition probabilities:P (yn ?
xn yn+1) = P (xn, yn+1|yn)?
P (xn|yn) P (yn+1|yn).However, without making this independence as-sumption, we can model right linear rules directly:P (xn, yn+1|yn) = P (xn|yn, yn+1) P (yn+1|yn).So, when we condition emission probabilities onboth the current state yn and the next state yn+1, wehave an exact model.
This direct modeling of theright linear grammar rule yn ?
xn yn+1 is whatwe call a probabilistic right-linear grammar.
To beclear, a PRLG is just an HMM without the indepen-dence of emissions and transitions.
See Smith andJohnson (2007) for a discussion, where they refer toPRLGs as Mealy HMMs.We use expectation maximization to estimatemodel parameters.
For the E step, the forward-backward algorithm (Rabiner, 1989) works identi-cally for the HMM and PRLG.
For the M step, weuse maximum likelihood estimation with additivesmoothing on the emissions probabilities.
So, forthe HMM and PRLG models respectively, for words1079STOP BO I1Fig.
2: Possible tag transitions as a state diagram.STOP B I OSTOP .33 .33 .33B 1I .25 .25 .25 .25O .33 .33 .33Fig.
3: Uniform initialization of transition probabilitiessubject to the constraints in Fig.
2: rows correspond toantecedent state, columns to following state.w and tags s, t:P?
(w|t) =C(t, w) + ?C(t) + ?VP?
(w|s, t) =C(t, w, s) + ?C(t, s) + ?Vwhere C are the soft counts of emissions C(t, w),rules C(t, w, s) = C(t ?
w s), tags C(t) and tran-sitions C(t, s) calculated during the E step; V is thenumber of terms w, and ?
is a smoothing parameter.We fix ?
= .1 for all experiments; more sophisti-cated smoothing could avoid dependence on ?.We do not smooth transition probabilities (soP?
(s|t) = C(t, s)/C(t)) for two reasons.
First, withfour tags, there is no data-sparsity concern with re-spect to transitions.
Second, the nature of the taskimposes certain constraints on transition probabili-ties: because we are only interested in multiwordchunks, we expressly do not want to generate a Bfollowing a B ?
in other words P (B|B) = 0.These constraints boil down to the observationthat the B and I states will only be seen in BII?
se-quences.
This may be expressed via the state transi-tion diagram in Fig.
2.
The constraints of also dic-tate the initial model input to the EM process.
Weuse uniform probability distributions subject to theconstraints of Fig.
2.
So, initial model transitionprobabilities are given in Fig.
3.
In EM, if a parame-ter is equal to zero, subsequent iterations of the EMprocess will not ?unset?
this parameter; thus, thisform of initialization is a simple way of encodingconstraints on model parameters.
We also experi-mented with random initial models (subject to theconstraints in Fig.
2).
Uniform initialization usu-ally works slightly better; also, uniform initializa-tion does not require multiple runs of each experi-ment, as random initialization does.Motivating the HMMand PRLG.
This approach?
encoding a chunking problem as a tagging prob-lem and learning to tag with HMMs ?
goes backto Ramshaw and Marcus (1995).
For unsupervisedlearning, the expectation is that the model will learnto generalize on phrasal boundaries.
That is, themodels will learn to associate terms like the and a,which often occur at the beginnings of sentences andrarely at the end, with the tag B, which cannot occurat the end of a sentence.
Likewise common nounslike company or asset, which frequently occur at theends of sentences, but rarely at the beginning, willcome to be associated with the I tag, which cannotoccur at the beginning.The basic motivation for the PRLG is the assump-tion that information is lost due to the independenceassumption characteristic of the HMM.
With so fewstates, it is feasible to experiment with the more fine-grained PRLG model.Evaluation.
Using the low-level predictions ofCCL as as benchmark, we evaluate the HMM andPRLG chunkers on the tasks of constituent chunkand base NP identification.
Models were initializeduniformly as illustrated in Fig.
3.
Sequence modelslearn via EM.
We report accuracy only after conver-gence, that is after the change in full dataset per-plexity (log inverse probability) is less than %.01between iterations.
Precision, recall and F-score arereported for full constituent identification ?
brack-ets which do not match the gold standard exactly arefalse positives.Model performance results on held-out testdatasets are reported in Table 3.
?CCL?
refers to thelowest-level constituents extracted from full CCLoutput, as a benchmark chunker.
The sequence mod-els outperform the CCL benchmark at both tasks andon all three datasets.
In most cases, the PRLG se-quence model performs better than the HMM; theexception is CTB, where the PRLG model is behindthe HMM in evaluation, as well as behind CCL.As the lowest-level constituents of CCL were notspecifically designed to describe chunks, we also1080English / WSJ German / Negra Chinese / CTBTask Model Prec Rec F Prec Rec F Prec Rec FChunkingCCL 57.5 53.5 55.4 28.4 29.6 29.0 23.5 23.9 23.7HMM 53.8 62.2 57.7 35.0 37.7 36.3 37.4 41.3 39.3PRLG 76.2 63.9 69.5 39.6 47.8 43.3 23.0 18.3 20.3NPCCL 46.2 51.1 48.5 15.6 29.2 20.3 10.4 17.3 13.0HMM 47.7 65.6 55.2 23.8 46.2 31.4 17.0 30.8 21.9PRLG 76.8 76.7 76.7 24.6 53.4 33.6 21.9 28.5 24.8Table 3: Unsupervised chunking results for local constituent structure identification and NP chunking on held-out testsets.
CCL refers to the lowest constituents extracted from CCL output.WSJ Negra CTBChunking 57.8 36.0 25.5NPs 57.8 38.8 23.2Table 4: Recall of CCL on the chunking tasks.checked the recall of all brackets generated by CCLagainst gold-standard constituent chunks.
The re-sults are given in Table 4.
Even compared to this,the sequence models?
recall is almost always higher.The sequence models, as well as the CCL bench-mark, show relatively low precision on the Negracorpus.
One possible reason for this lies in thedesign decision of Negra to use relatively flat treestructures.
As a result, many structures that inother treebanks would be prepositional phrases withembedded noun phrases ?
and thus non-local con-stituents ?
are flat prepositional phrases here.
Exam-ples include ?auf die Wiesbadener Staatsanwaelte?
(on Wiesbaden?s district attorneys) and ?in Han-novers Nachbarstadt?
(in Hannover?s neighbor city).In fact, in Negra, the sequence model chunkersoften find NPs embedded in PPs, which are not an-notated as such.
For instance, in the PP ?hinter denKulissen?
(behind the scenes), both the PRLG andHMM chunkers identify the internal NP, though thisis not identified in Negra and thus considered a falsepositive.
The fact that the HMM and PRLG havehigher recall on NP identification on Negra than pre-cision is further evidence towards this.Comparing the HMM and PRLG.
To outlinesome of the factors differentiating the HMM andPRLG, we focus on NP identification in WSJ.The PRLG has higher precision than the HMM,while the two models are closer in recall.
Com-paring the predictions directly, the two models of-POS Sequence # of errorsTO VB 673NNP NNP 450MD VB 407DT JJ 368DT NN 280Table 5: Top 5 POS sequences of the false positives pre-dicted by the HMM.ten have the same correct predictions and often missthe same gold standard constituents.
The improvedresults of the PRLG are based mostly on the feweroverall brackets predicted, and thus fewer false pos-itives: for WSJ the PRLG incorrectly predicts 2241NP constituents compared to 6949 for the HMM.Table 5 illustrates the top 5 POS sequences of thefalse positives predicted by the HMM.3 (Recall thatwe use gold standard POS only for post-experimentresults analysis?the model itself does not have ac-cess to them.)
By contrast, the sequence represent-ing the largest class of errors of the PRLG is DT NN,with 165 errors ?
this sequence represents the largestclass of predictions for both models.Two of the top classes of errors, MD VB andTO VB, represent verb phrase constituents, whichare often predicted by the HMM chunker, but notby the PRLG.
The class represented by NNP NNPcorresponds with the tendency of the HMM chun-ker to split long proper names: for example, it sys-tematically splits new york stock exchange into twochunks, (new york) (stock exchange), whereas thePRLG chunker predicts a single four-word chunk.The most interesting class is DT JJ, which rep-resents the difficulty the HMM chunker has at dis-3For the Penn Treebank tagset, see Marcus et al (1993).10811 Start with raw text:there is no asbestos in our products now2 Apply chunking model:there (is no asbestos) in (our products) now3 Create pseudowords:there is in our now4 Apply chunking model (and repeat 1?4 etc.
):(there is ) (in our ) now5 Unwind and create a tree:thereis no asbestosinour productsnow1Fig.
4: Cascaded chunking illustrated.
Pseudowords areindicated with boxes.tinguishing determiner-adjective from determiner-noun pairs.
The PRLG chunker systematically getsDT JJ NN trigrams as chunks.
The greater con-text provided by right branching rules allows themodel to explicitly estimate separate probabilitiesforP (I ?
recent I) versusP (I ?
recent O).
Thatis, recent within a chunk versus ending a chunk.
Bi-grams like the acquisition allow the model to learnrules P (B ?
the I) and P (I ?
acquisition O).So, the PRLG is better able to correctly pick out thetrigram chunk (the recent acquisition).5 Constituent parsing with a cascade ofchunkersWe use cascades of chunkers for full constituentparsing, building hierarchical constituents bottom-up.
After chunking is performed, all multiword con-stituents are collapsed and represented by a singlepseudoword.
We use an extremely simple, but effec-tive, way to create pseudoword for a chunk: pick theterm in the chunk with the highest corpus frequency,and mark it as a pseudoword.
The sentence is now astring of symbols (normal words and pseudowords),to which a subsequent unsupervised chunking modelis applied.
This process is illustrated in Fig.
4.Each chunker in the cascade chunks the raw text,then regenerates the dataset replacing chunks withpseudowords; this process is iterated until no newchunks are found.
The separate chunkers in the cas-Text : Mr. Vinken is chairman of Elsevier N.V.Level 1 :Mr. Vinkenis chairman ofElsevier N.V.1Level 2 :Mr. Vinken is chairmanofElsevier N.V.1Level 3 : Mr. Vinken is chairman ofElsevier N.V.1Fig.
5: PRLG cascaded chunker output.NPs PPsLev 1 Lev 2 Lev 1 Lev 2WSJHMM 66.5 68.1 20.6 70.2PRLG 77.5 78.3 9.1 77.6NegraHMM 54.7 62.3 24.8 48.1PRLG 61.6 65.2 40.3 44.0CTBHMM 33.3 35.4 34.6 38.4PRLG 30.9 33.6 31.6 47.1Table 7: NP and PP recall at cascade levels 1 and 2.
Thelevel 1 NP numbers differ from the NP chunking numbersfrom Table 3 since they include root-level constituentswhich are often NPs.cade are referred to as levels.
In our experiments thecascade process took a minimum of 5 levels, and amaximum of 7.
All chunkers in the cascade have thesame settings in terms of smoothing, the tagset andinitialization.Evaluation.
Table 6 gives the unlabeled PARSE-VAL scores for CCL and the two finite-state models.PRLG achieves the highest F-score for all datasets,and does so by a wide margin for German and Chi-nese.
CCL does achieve higher recall for English.While the first level of constituent analysis hashigh precision and recall on NPs, the second leveloften does well finding prepositional phrases (PPs),especially in WSJ; see Table 7.
This is illustratedin Fig.
5.
This example also illustrates a PP attach-ment error, which are a common problem for thesemodels.We also evaluate using short ?
10-word or less ?sentences.
That said, we maintain the training/testsplit from before.
Also, making use of the open1082Parsing English / WSJ German / Negra Chinese / CTBModel Prec Rec F Prec Rec F Prec Rec FCCL 53.6 50.0 51.7 33.4 32.6 33.0 37.0 21.6 27.3HMM 48.2 43.6 45.8 30.8 50.3 38.2 43.0 29.8 35.2PRLG 60.0 49.4 54.2 38.8 47.4 42.7 50.4 32.8 39.8Table 6: Unlabeled PARSEVAL scores for cascaded models.source implementation by F. Luque,4 we compareon WSJ and Negra to the constituent context model(CCM) of Klein and Manning (2002).
CCM learnsto predict a set of brackets over a string (in prac-tice, a string of POS tags) by jointly estimating con-stituent and distituent strings and contexts using aniterative EM-like procedure (though, as noted bySmith and Eisner (2004), CCM is deficient as a gen-erative model).
Note that this comparison is method-ologically problematic in two respects.
On the onehand, CCM is evaluated using gold standard POSsequences as input, so it receives a major source ofsupervision not available to the other models.
On theother hand, the other models use punctuation as anindicator of constituent boundaries, but all punctu-ation is dropped from the input to CCM.
Also, notethat CCM performs better when trained on short sen-tences, so here CCM is trained only on the 10-word-or-less subsets of the training datasets.5The results from the cascaded PRLG chunkerare near or better than the best performance byCCL or CCM in these experiments.
These and thefull-length parsing results suggest that the cascadedchunker strategy generalizes better to longer sen-tences than does CCL.
CCM does very poorly onlonger sentences, but does not have the benefit of us-ing punctuation, as do the raw text models; unfortu-nately, further exploration of this trade-off is beyondthe scope of this paper.
Finally, note that CCM hashigher recall, and lower precision, generally, thanthe raw text models.
This is due, in part, to the chartstructure used by CCM in the calculation of con-stituent and distituent probabilities: as in CKY pars-ing, the chart structure entails the trees predicted willbe binary-branching.
CCL and the cascaded modelscan predict higher-branching constituent structures,4http://www.cs.famaf.unc.edu.ar/?francolq/en/proyectos/dmvccm/5This setup is the same as Seginer?s (2007), except thetrain/test split.Prec Rec FWSJCCM 62.4 81.4 70.7CCL 71.2 73.1 72.1HMM 64.4 64.7 64.6PRLG 74.6 66.7 70.5NegraCCM 52.4 83.4 64.4CCL 52.9 54.0 53.0HMM 47.7 72.0 57.4PRLG 56.3 72.1 63.2CTBCCL 54.4 44.3 48.8HMM 55.8 53.1 54.4PRLG 62.7 56.9 59.6Table 8: Evaluation on 10-word-or-less sentences.
CCMscores are italicized as a reminder that CCM uses gold-standard POS sequences as input, so its results are notstrictly comparable to the others.so fewer constituents are predicted overall.6 Phrasal punctuation revisitedUp to this point, the proposed models for chunkingand parsing use phrasal punctuation as a phrasal sep-arator, like CCL.
We now consider how well thesemodels perform in absence of this constraint.Table 9a provides comparison of the sequencemodels?
performance on the constituent chunkingtask without using phrasal punctuation in trainingand evaluation.
The table shows absolute improve-ment (+) or decline (?)
in precision and recallwhen phrasal punctuation is removed from the data.The punctuation constraint seems to help the chun-kers some, but not very much; ignoring punctuationseems to improve chunker results for the HMM onChinese.
Overall, the effect of phrasal punctuationon the chunker models?
performance is not clear.The results for cascaded parsing differ stronglyfrom those for chunking, as Table 9b indicates.
Us-ing phrasal punctuation to constrain bracket predic-tion has a larger impact on cascaded parsing re-10830 20 40 6022.533.5EM IterationsLengtha) Average Predicted Constituent LengthActual average chunk length10 20 40 6020304050EM IterationsPrecisionW/ PunctuationNo Punctuationb) Chunking Precision10 20 40 6020304050EM IterationsPrecisionc) Precision at All Brackets1Fig.
6: Behavior of the PRLG model on CTB over the course of EM.WSJ Negra CTBPrec Rec Prec Rec Prec RecHMM ?5.8 ?9.8 ?0.1 ?0.4 +0.7 +4.9PRLG ?2.5 ?2.1 ?2.1 ?2.1 ?7.0 +1.2a) Constituent ChunkingWSJ Negra CTBPrec Rec Prec Rec Prec RecCCL ?14.1 ?13.5 ?10.7 ?4.6 ?11.6 ?6.0HMM ?7.8 ?8.6 ?2.8 +1.7 ?13.4 ?1.2PRLG ?10.1 ?7.2 ?4.0 ?4.5 ?22.0 ?11.8b) (Cascade) ParsingTable 9: Effects of dropping phrasal punctuation in un-supervised chunking and parsing evaluations relative toTables 3 and 6.sults almost across the board.
This is not surpris-ing: while performing unsupervised partial parsingfrom raw text, the sequence models learn two gen-eral patterns: i) they learn to chunk rare sequences,such as named entities, and ii) they learn to chunkhigh-frequency function words next to lower fre-quency content words, which often correlate withNPs headed by determiners, PPs headed by prepo-sitions and VPs headed by auxiliaries.
When thesepatterns are themselves replaced with pseudowords(see Fig.
4), the models have fewer natural cues toidentify constituents.
However, within the degreesof freedom allowed by punctuation constraints asdescribed, the chunking models continue to find rel-atively good constituents.While CCL makes use of phrasal punctuation inpreviously reported results, the open source imple-mentation allows it to be evaluated without this con-straint.
We did so, and report results in Table 9b.CCL is, in fact, very sensitive to phrasal punctu-ation.
Comparing CCL to the cascaded chunkerswhen none of them use punctuation constraints, thecascaded chunkers (both HMMs and PRLGs) out-perform CCL for each evaluation and dataset.For the CTB dataset, best chunking performanceand cascaded parsing performance flips from theHMM to the PRLG.
More to the point, the PRLGis actually with worst performing model at the con-stituent chunking task, but the best performing cas-cade parser; also, this model has the most seriousdegrade in performance when phrasal punctuation isdropped from input.
To investigate, we track theperformance of the chunkers on the developmentdataset over iterations of EM.
This is illustrated inFig.
6 with the PRLG model.
First of all, Fig.
6a re-veals the average length of the constituents predictedby the PRLG model increases over the course ofEM.
However, the average constituent chunk lengthis 2.22.
So, the PRLG chunker is predicting con-stituents that are longer than the ones targeted inthe constituent chunking task: regardless of whetherthey are legitimate constituents or not, often theywill likely be counted as false positives in this evalu-ation.
This is confirmed by observing the constituentchunking precision in Fig.
6b, which peaks whenthe average predicted constituent length is about thesame the actual average length of those in the eval-uation.
The question, then, is whether the longerchunks predicted correspond to actual constituentsor not.
Fig.
6c shows that the PRLG, when con-strained by phrasal punctuation, does continue toimprove its constituent prediction accuracy over thecourse of EM.
These correctly predicted constituentsare not counted as such in the constituent chunkingor base NP evaluations, but they factor directly into1084improved accuracy when this model is part of a cas-cade.7 Related workOur task is the unsupervised analogue of chunking(Abney, 1991), popularized by the 1999 and 2000Conference on Natural Language Learning sharedtasks (Tjong et al, 2000).
In fact, our models followRamshaw and Marcus (1995), treating structure pre-diction as sequence prediction using BIO tagging.In addition to Seginer?s CCL model, the unsu-pervised parsing model of Gao and Suzuki (2003)and Gao et al (2004) also operates on raw text.Like us, their model gives special treatment to lo-cal constituents, using a language model to char-acterize phrases which are linked via a dependencymodel.
Their output is not evaluated directly usingtreebanks, but rather applied to several informationretrieval problems.In the supervised realm, Hollingsheadet al (2005) compare context-free parsers withfinite-state partial parsing methods.
They find thatfull parsing maintains a number of benefits, in spiteof the greater training time required: they can trainon less data more effectively than chunkers, and aremore robust to shifts in textual domain.Brants (1999) reports a supervised cascadedchunking strategy for parsing which is strikinglysimilar to the methods proposed here.
In both,Markov models are used in a cascade to predict hi-erarchical constituent structure; and in both, the pa-rameters for the model at each level are estimatedindependently.
There are major differences, though:the models here are learned from raw text with-out tree annotations, using EM to train parameters;Brants?
cascaded Markov models use supervisedmaximum likelihood estimation.
Secondly, betweenthe separate levels of the cascade, we collapse con-stituents into symbols which are treated as tokensin subsequent chunking levels; the Markov modelsin the higher cascade levels in Brants?
work actu-ally emit constituent structure.
A related approachis that of Schuler et al (2010), who report a su-pervised hierarchical hidden Markov model whichuses a right-corner transform.
This allows the modelto predict more complicated trees with fewer levelsthan in Brants?
work or this paper.8 ConclusionIn this paper we have introduced a new subprob-lem of unsupervised parsing: unsupervised partialparsing, or unsupervised chunking.
We have pro-posed a model for unsupervised chunking from rawtext that is based on standard probabilistic finite-state methods.
This model produces better localconstituent predictions than the current best unsu-pervised parser, CCL, across datasets in English,German, and Chinese.
By extending these proba-bilistic finite-state methods in a cascade, we obtaina general unsupervised parsing model.
This modeloutperforms CCL in PARSEVAL evaluation on En-glish, German, and Chinese.Like CCL, our models operate from raw (albeitsegmented) text, and like it our models decode veryquickly; however, unlike CCL, our models are basedon standard and well-understood computational lin-guistics technologies (hidden Markov models andrelated formalisms), and may benefit from new re-search into these core technologies.
For instance,our models may be improved by the applicationof (unsupervised) discriminative learning techniqueswith features (Berg-Kirkpatrick et al, 2010); or byincorporating topic models and document informa-tion (Griffiths et al, 2005; Moon et al, 2010).UPPARSE, the software used for the experimentsin this paper, is available under an open-source li-cense to facilitate replication and extensions.6Acknowledgments.
This material is based uponwork supported in part by the U. S. Army ResearchLaboratory and the U. S. Army Research Office un-der grant number W911NF-10-1-0533.
Support forthe first author was also provided by Mike Hogg En-dowment Fellowship, the Office of Graduate Studiesat The University of Texas at Austin.This paper benefited from discussion in the Natu-ral Language Learning reading group at UT Austin,especially from Collin Bannard, David Beaver,Matthew Lease, Taesun Moon and Ray Mooney.
Wealso thank the three anonymous reviewers for in-sightful questions and helpful comments.6 http://elias.ponvert.net/upparse.1085ReferencesS.
Abney.
1991.
Parsing by chunks.
In R. Berwick,S.
Abney, and C. Tenny, editors, Principle-based Pars-ing.
Kluwer.P.
W. Adriaans, M. Trautwein, and M. Vervoort.
2000.Towards high speed grammar induction on large textcorpora.
In SOFSEM.T.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero, andD.
Klein.
2010.
Painless unsupervised learning withfeatures.
In HLT-NAACL.R.
Bod.
2006.
Unsupervised parsing with U-DOP.
InCoNLL.T.
Brants.
1999.
Cascaded markov models.
In EACL.E.
Charniak.
1993.
Statistical Language Learning.
MIT.S.
B. Cohen and N. A. Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in unsu-pervised grammar induction.
In HLT-NAACL.B.
Cramer.
2007.
Limitations of current grammar induc-tion algorithms.
In ACL-SRW.X.
Duan, J. Zhao, and B. Xu.
2007.
Probabilistic mod-els for action-based Chinese dependency parsing.
InECML/PKDD.J.
Gao and H. Suzuki.
2003.
Unsupervised learning ofdependency structure for language modeling.
In ACL.J.
Gao, J.Y.
Nie, G. Wu, and G. Cao.
2004.
Dependencelanguage model for information retrieval.
In SIGIR.T.
L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-baum.
2005.
Integrating topics and syntax.
In NIPS.C.
Ha?nig.
2010.
Improvements in unsupervised co-occurence based parsing.
In CoNLL.W.
P. Headden III, M. Johnson, and D. McClosky.2009.
Improving unsupervised dependency parsingwith richer contexts and smoothing.
In HLT-NAACL.K.
Hollingshead, S. Fisher, and B. Roark.
2005.
Com-paring and combining finite-state and context-freeparsers.
In HLT-EMNLP.D.
Klein and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In ACL.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In ACL.B.
Krenn, T. Brants, W. Skut, and Hans Uszkoreit.
1998.A linguistically interpreted corpus of German newspa-per text.
In Proceedings of the ESSLLI Workshop onRecent Advances in Corpus Annotation.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech & Language, 4:35 ?
56.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Treebank.
Compuational Linguistics, pages313?330.M.P.
Marcus, B. Santorini, M.A.
Marcinkiewicz, andA.
Taylor, 1999.
Treebank-3.
LDC.T.
Moon, J. Baldridge, and K. Erk.
2010.
CrouchingDirichlet, hidden Markov model: Unsupervised POStagging with context local tag generation.
In EMNLP.M.
Palmer, F. D. Chiou, N. Xue, and T. K. Lee, 2005.Chinese Treebank 5.0.
LDC.F.
Pereira and Y. Schabes.
1992.
Inside-outside reesti-mation from paritally bracketed corpora.
In ACL.E.
Ponvert, J. Baldridge, and K. Erk.
2010.
Simple unsu-pervised prediction of low-level constituents.
In ICSC.L.R.
Rabiner.
1989.
A tutorial on hidden Markov modelsand selected applications in speech recognition.
Pro-ceedings of the IEEE.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proc.
of ThirdWorkshop on Very Large Corpora.R.
Reichart and A. Rappoport.
2010.
Improved fullyunsupervised parsing with Zoomed Learning.
InEMNLP.W.
Schuler, S. AbdelRahman, T. Miller, and L. Schwartz.2010.
Broad-coverage parsing using human-likememory constraints.
Compuational Linguistics, 3(1).Y.
Seginer.
2007.
Fast unsupervised incremental parsing.In ACL.N.
A. Smith and J. Eisner.
2004.
Annealing techniquesfor unsupervised statistical language learning.
In ACL.N.
A. Smith and M. Johnson.
2007.
Weighted and prob-abilistic CFGs.
Computational Lingusitics.Z.
Solan, D. Horn, E. Ruppin, and S. Edelman.
2005.Unsupervised learning of natural languages.
PNAS,102.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010.From baby steps to leapfrog: How ?less is more?
inunsupervised dependency parsing.
In NAACL-HLT.E.
F. Tjong, K. Sang, and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 Shared Task: Chunking.
InCoNLL-LLL.M.
van Zaanen.
2000.
ABL: Alignment-based learning.In COLING.1086
