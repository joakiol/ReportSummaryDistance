Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121?124,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPReducing SMT Rule Table with Monolingual Key PhraseZhongjun He?
Yao Meng?
Yajuan Lj ?
Hao Yu?
Qun Liu??
Fujitsu R&D Center CO., LTD, Beijing, China{hezhongjun, mengyao, yu}@cn.fujitsu.com?
Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences, Beijing, China{lvyajuan, liuqun}@ict.ac.cnAbstractThis paper presents an effective approachto discard most entries of the rule table forstatistical machine translation.
The rule ta-ble is filtered by monolingual key phrases,which are extracted from source text us-ing a technique based on term extraction.Experiments show that 78% of the rule ta-ble is reduced without worsening trans-lation performance.
In most cases, ourapproach results in measurable improve-ments in BLEU score.1 IntroductionIn statistical machine translation (SMT) commu-nity, the state-of-the-art method is to use rules thatcontain hierarchical structures to model transla-tion, such as the hierarchical phrase-based model(Chiang, 2005).
Rules are more powerful thanconventional phrase pairs because they containstructural information for capturing long distancereorderings.
However, hierarchical translationsystems often suffer from a large rule table (thecollection of rules), which makes decoding slowand memory-consuming.In the training procedure of SMT systems, nu-merous rules are extracted from the bilingual cor-pus.
During decoding, however, many of them arerarely used.
One of the reasons is that these ruleshave low quality.
The rule quality are usually eval-uated by the conditional translation probabilities,which focus on the correspondence between thesource and target phrases, while ignore the qualityof phrases in a monolingual corpus.In this paper, we address the problem of reduc-ing the rule table with the information of mono-lingual corpus.
We use C-value, a measurementof automatic term recognition, to score sourcephrases.
A source phrase is regarded as a keyphrase if its score greater than a threshold.
Notethat a source phrase is either a flat phrase consistsof words, or a hierarchical phrase consists of bothwords and variables.
For rule table reduction, therule whose source-side is not key phrase is dis-carded.Our approach is different from the previous re-search.
Johnson et al (2007) reduced the phrasetable based on the significance testing of phrasepair co-occurrence in bilingual corpus.
The ba-sic difference is that they used statistical infor-mation of bilingual corpus while we use that ofmonolingual corpus.
Shen et al (2008) pro-posed a string-to-dependency model, which re-stricted the target-side of a rule by dependencystructures.
Their approach greatly reduced the ruletable, however, caused a slight decrease of trans-lation quality.
They obtained improvements byincorporating an additional dependency languagemodel.
Different from their research, we restrictrules on the source-side.
Furthermore, the systemcomplexity is not increased because no additionalmodel is introduced.The hierarchical phrase-based model (Chiang,2005) is used to build a translation system.
Exper-iments show that our approach discards 78% of therule table without worsening the translation qual-ity.2 Monolingual Phrase Scoring2.1 FrequencyThe basic metrics for phrase scoring is the fre-quency that a phrase appears in a monolingual cor-pus.
The more frequent a source phrase appears ina corpus, the greater possibility the rule that con-tains the source phrase may be used.However, one limitation of this metrics is that ifwe filter the rule table by the source phrase withlower frequency, most long phrase pairs will bediscarded.
Because the longer the phrase is, theless possibility it appears.
However, long phrases121are very helpful for reducing ambiguity since theycontains more information than short phrases.Another limitation is that the frequency metricsfocuses on a phrase appearing by itself while ig-nores it appears as a substring of longer phrases.It is therefore inadequate for hierarchical phrases.We use an example for illustration.
Consideringthe following three rules (the subscripts indicateword alignments):R1:?1??2o?34?5accept1President3Bush2?s4invitation5R2:?1?
?2X34?5accept1X3Bush2?s4invitation5R3:?1X23?4accept1X2?s3invitation4We use f1, f2and f3to represent their source-sides, respectively.
The hierarchical phrases f2and f3are sub-strings of f1.
However, R3is sug-gested to be more useful than R2.
The reason isthat f3may appears in various phrases, such as??
{I?, accept France ?s invitation?.While f2almost always appears in f1, indicatingthat the variable X may not be replaced with otherwords expect ?President?.
It indicates that R2isnot likely to be useful, although f2may appearsfrequently in a corpus.2.2 C-valueC-value, a measurement of automatic term recog-nition, is proposed by Frantzi and Ananiadou(1996) to extract nested collocations, collocationsthat substrings of other longer ones.We use C-value for two reasons: on one hand,it uses rich factors besides phrase frequency, e.g.the phrase length, the frequency that a sub-phraseappears in longer phrases.
Thus it is appropriatefor extracting hierarchical phrases.
On the otherhand, the computation of C-value is efficient.Analogous to (Frantzi and Ananiadou, 1996),we use 4 factors (L,F, S,N) to determine if aphrase p is a key phrase:1.
L(p), the length of p;2.
F (p), the frequency that p appears in a cor-pus;Algorithm 1 Key Phrase ExtractionInput: Monolingual TextOutput: Key Phrase Table KP1: Extract candidate phrases2: for all phrases p in length descending orderdo3: if N(p) = 0 then4: C-value = (L(p)?
1)?
F (p)5: else6: C-value = (L(p)?
1)?
(F (p)?
S(p)N(p))7: end if8: if C-value ?
?
then9: add p to KP10: end if11: for all sub-strings q of p do12: S(q) = S(q) + F (p)?
S(p)13: N(q) = N(q) + 114: end for15: end for3.
S(p), the frequency that p appears as a sub-string in other longer phrases;4.
N(p), the number of phrases that contain p asa substring.Given a monolingual corpus, key phrases can beextracted efficiently according to Algorithm 1.Firstly (line 1), all possible phrases are ex-tracted as candidates of key phrases.
This stepis analogous to the rule extraction as described in(Chiang, 2005).
The basic difference is that thereare no word alignment constraints for monolingualphrase extraction, which therefore results in a sub-stantial number of candidate phrases.
We use thefollowing restrictions to limit the phrase number:1.
The length of a candidate phrase is limited topl;2.
The length of the initial phrase used to createhierarchical phrases is limited to ipl;3.
The number of variables in hierarchicalphrases is limited to nv, and there should beat least 1 word between variables;4.
The frequency of a candidate phrase appearsin a corpus should be greater than freq.In our experiments, we set pl = 5, ipl = 10, nv =2, freq = 3.
Note that the first 3 settings are usedin (Chiang, 2005) for rule extraction.122Secondly (line 3 to 7), for each candidatephrase, C-value is computed according to thephrase appears by itself (line 4) or as a substringof other long phrases (line 6).
The C-value is indirect proportion to the phrase length (L) and oc-currences (F, S), while in inverse proportion to thenumber of phrases that contain the phrase as a sub-string (N ).
This overcomes the limitations of fre-quency measurement.
A phrase is regarded as akey phrase if its C-value is greater than a thresh-old ?.Finally (line 11 to 14), S(q) and N(q) are up-dated for each substring q.We use the example in Section 2.1 for illustra-tion.
The quadruple for f1is (5, 2, 0, 0), indicatingthat the phrase length is 5 and appears 2 times byitself in the corpus.
Therefore C-value(f1) = 8.The quadruple for f2is (4, 2, 2, 1), indicating thatthe phrase length is 4 and appears 2 times in thecorpus.
However, the occurrences are as a sub-string of the phrase f1.
Therefore, C-value(f2) =0.
While the quadruple for f3is (3, 11, 11, 9),which indicates that the phrase length is 3 and ap-pears 11 times as a substring in 9 phrases, thusC-value(f3) = 19.6.
Given the threshold ?
= 5,f1and f3are viewed as key phrases.
Thus R2willbe discarded because its source-side is not a keyphrase.3 ExperimentsOur experiments were carried out on two languagepairs:?
Chinese-English: For this task, the corporaare from the NIST evaluation.
The parallelcorpus 1 consists of 1M sentence pairs .
Wetrained two trigram language models: one onthe Xinhua portion of the Gigaword corpus,and the other on the target-side of the paral-lel corpus.
The test sets were NIST MT06GALE set (06G) and NIST set (06N) andNIST MT08 test set.?
German-English: For this task, the corporaare from the WMT 2 evaluation.
The paral-lel corpus contains 1.3M sentence pairs.
Thetarget-side was used to train a trigram lan-guage model.
The test sets were WMT06 andWMT07.1LDC2002E18 (4,000 sentences), LDC2002T01,LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T10,LDC2004T08 HK Hansards (500,000 sentences)2http://www.statmt.org/wmt07/shared-task.htmlFor both the tasks, the word alignment weretrained by GIZA++ in two translation directionsand refined by ?grow-diag-final?
method (Koehnet al, 2003).
The source-side of the parallel cor-pus is used to extract key phrases.3.1 ResultsWe reimplemented the state-of-the-art hierarchicalMT system, Hiero (Chiang, 2005), as the baselinesystem.
The results of the experiments are shownin Table 1 and Table 2.Table 1 shows the C-value threshold effecton the size of the rule table, as well as theBLEU scores.
Originally, 103M and 195M rulesare respectively extracted for Chinese-English andGerman-English.
For both the two tasks, about78% reduction of the rule table (for Chinese-English ?
= 200 and for German-English ?
=100) does not worsen translation performance.
Weachieved improvements in BLEU on most of thetest corpora, except a slight decrease (0.06 point)on WMT07.We also compared the effects of frequency andC-value metrics for the rule table reduction onChinese-English test sets.
The rule table is re-duced to the same size (22% of original table)using the two metrics, separately.
However, asshown in Table 2, the frequency method decreasesthe BLEU scores, while the C-value achieves im-provements.
It indicates that C-value is more ap-propriate than frequency to evaluate the impor-tance of phrases, because it considers more fac-tors.With the rule table filtered by key phrases onthe source side, the number of source phrases re-duces.
Therefore during decoding, a source sen-tence is suggested to be decomposed into a num-ber of ?key phrases?, which are more reliable thanthe discarded phrases.
Thus the translation qualitydoes not become worse.3.2 Adding C-value as a FeatureConventional phrase-based approaches performedphrase segmentation for a source sentence with auniform distribution.
However, they do not con-sider the weights of source phrases.
Although anystrings can be phrases, it is believed that somestrings are more likely phrases than others.
Weuse C-value to describe the weight of a phrase ina monolingual corpus and add it as a feature to thetranslation model:123C-value Chinese-English Germany-EnglishThreshold ?
Rule Table (%) 06G 06N 08 Rule Table (%) 06 070 100% 12.43 28.58 21.57 100% 27.30 27.955 61% 12.22 28.40 21.33 54% 27.39 28.0520 44% 12.24 28.29 21.21 37% 27.47 27.94100 28% 12.36 28.56 21.67 22% 27.54 27.89200 22% 12.66 28.69 22.12 17% 27.26 27.80300 20% 12.41 27.76 21.52 15% 27.41 27.69400 18% 11.88 26.98 20.70 13% 27.36 27.76500 16% 11.65 26.40 20.32 12% 27.25 27.76Table 1: C-value threshold effect on the rule table size and BLEU scores.System Rule Table (%) 06G 06N 08Baseline 100% 12.43 28.58 21.57Frequency 22% 12.24 27.77 21.20C-value 22% 12.66 28.69 22.12?+CV-Feature 22% 12.89?
29.22?+ 22.56?+Table 2: BLEU scores on the test sets of the Chinese-English task.
?
means significantly better thanbaseline at p < 0.01.
+ means significantly better than C-value at p < 0.05.h(FJ1) =K?k=1log(C-value(?fk)) (1)where, ?fkis the source-side of a rule.The results are shown in the row +CV-Featurein Table 2.
Measurable improvements are ob-tained on all test corpora of the Chinese-Englishtask by adding the C-value feature.
The improve-ments over the baseline are statistically significantat p < 0.01 by using the significant test methoddescribed in (Koehn, 2004).4 ConclusionIn this paper, we successfully discarded mostentries of the rule table with monolingual keyphrases.
Experiments show that about 78% of therule table is reduced and the translation qualitydoes not become worse.
We achieve measurableimprovements by incorporating C-value into thetranslation model.The use of key phrases is one of the simplestmethod for the rule table reduction.
In the future,we will use sophisticated metrics to score phrasesand reduce the rule table size with the informationof both the source and target sides.ReferencesDavid Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 263?270.Katerina T. Frantzi and Sophia Ananiadou.
1996.Extracting nested collocations.
In COLING1996,pages 41?46.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qual-ity by discarding most of the phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic,June.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof HLT-NAACL 2003, pages 127?133.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 388?395.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of ACL-08: HLT, pages 577?585,Columbus, Ohio, June.124
