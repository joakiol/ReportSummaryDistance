Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 20?29,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsContextual Bearing on Linguistic Variation in Social MediaStephan Gouws?, Donald Metzler, Congxing Cai and Eduard Hovy{gouws, metzler, ccai, hovy}@isi.eduUSC Information Sciences InstituteMarina del Rey, CA90292, USAAbstractMicrotexts, like SMS messages, Twitter posts,and Facebook status updates, are a popularmedium for real-time communication.
In thispaper, we investigate the writing conventionsthat different groups of users use to expressthemselves in microtexts.
Our empirical studyinvestigates properties of lexical transforma-tions as observed within Twitter microtexts.The study reveals that different populations ofusers exhibit different amounts of shortenedEnglish terms and different shortening styles.The results reveal valuable insights into howhuman language technologies can be effec-tively applied to microtexts.1 IntroductionMicrotexts, like SMS messages, Twitter posts, andFacebook status updates, are becoming a popularmedium for real-time communication in the moderndigital age.
The ubiquitous nature of mobile phones,tablets, and other Internet-enabled consumer devicesprovide users with the ability to express what ison their mind nearly anywhere and at just aboutany time.
Since such texts have the potential toprovide unique perspectives on human experiences,they have recently become the focus of many studieswithin the natural language processing and informa-tion retrieval research communities.The informal nature of microtexts allows usersto invent ad hoc writing conventions that suit their?This work was done while the first author was a visiting stu-dent at ISI from the MIH Media Lab at Stellenbosch University,South Africa.
Correspondence may alternatively be directed tostephan@ml.sun.ac.za.particular needs.
These needs strongly depend onvarious user contexts, such as their age, geographiclocation, how they want to be outwardly perceived,and so on.
Hence, social factors influence the waythat users express themselves in microtexts and otherforms of media.In addition to social influences, there are also us-ability and interface issues that may affect the way auser communicates using microtexts.
For example,the Twitter microblog service imposes an explicitmessage length limit of 140 characters.
Users ofsuch services also often send messages using mobiledevices.
There may be high input costs associatedwith using mobile phone keypads, thus directly im-pacting the nature of how users express themselves.In this paper, we look specifically at understand-ing the writing conventions that different groupsof users use to express themselves.
This is ac-complished by carrying out a novel empirical in-vestigation of the lexical transformation character-istics observed within Twitter microtexts.
Our em-pirical evaluation includes: (i) an analysis of howfrequently different user populations apply lexicaltransformations, and (ii) a study of the types oftransformations commonly employed by differentpopulations of users.
We investigate several ways ofdefining user populations (e.g., based on the Twitterclient, time zone, etc.).
Our results suggest that notall microtexts are created equal, and that certain pop-ulations of users are much more likely to use certaintypes of lexical transformations than others.This paper has two primary contributions.
First,we present a novel methodology for contextualizedanalysis of lexical transformations found within mi-20crotexts.
The methodology leverages recent ad-vances in automated techniques for cleaning noisytext.
This approach enables us to study the fre-quency and types of transformations that are com-mon within different user populations and user con-texts.
Second, we present results from an empiricalevaluation over microtexts collected from the Twit-ter microblog service.
Our empirical analysis re-veals that within Twitter microtexts, different userpopulations and user contexts give rise to differentforms of expression, by way of different styles oflexical transformations.The remainder of this paper is laid out as follows.Section 2 describes related work, while Section 3motivates our investigation.
Our multi-prongedmethodology for analyzing lexical transformationsis described in Section 4.
Section 5 describes ourexperimental results.
Finally, Section 6 concludesthe paper and describes possible directions for fu-ture work.2 Related WorkAlthough our work is primarily focused on analyz-ing the lexical variation in language found in on-line social media, our analysis methodology makesstrong use of techniques for normalizing ?noisy text?such as SMS-messages and Twitter messages intostandard English.Normalizing text can traditionally be approachedusing three well-known NLP metaphors, namelythat of spell-checking, machine translation (MT) andautomatic speech recognition (ASR) (Kobus et al,2008).In the spell-checking approach, corrections from?noisy?
words to ?clean?
words proceed on a word-by-word basis.
Choudhury (2007) implementsthe noisy channel model (Shannon and Weaver,1948) using a hidden Markov model to handle bothgraphemic and phonemic variations, and Cook andStevenson (2009) improve on this model by adapt-ing the channel noise according to several predefinedword formations such as stylistic variation, wordclipping, etc.
However, spelling correction is tra-ditionally conducted in media with relatively highpercentages of well-formed text where one can per-form word boundary detection and thus tokenizationto a high degree of accuracy.
The main drawback isthe strong confidence this approach places on wordboundaries (Beaufort et al, 2010), since detectingword boundaries in noisy text is not a trivial prob-lem.In the machine translation approach (Bangaloreet al, 2002; Aw et al, 2006), normalizing noisytext is considered as a translation task from a sourcelanguage (the noisy text) to a target language (thecleansed text).
Since noisy- and clean text typicallyvary wildly, it satisfies the notion of translating be-tween two languages.
However, since these trans-formations can be highly creative, they usually needa wide context (more than one word) to be resolvedadequately.
Kobus (2008) also points out that de-spite the fairly good results achieved with this sys-tem, such a purely phrase-based translation modelcannot adequately handle the wide level of lexicalcreativity found in these media.Finally, the ASR approach is based on the ob-servation that many noisy word forms in SMSesor other noisy text are based on phonetic plays ofthe clean word.
This approach starts by convert-ing the input message into a phone lattice, whichis converted to a word lattice using a phoneme-grapheme dictionary.
Finally the word lattice is de-coded by applying a language model to the word lat-tice and using a best-path algorithm to recover themost likely original word sequence.
This approachhas the advantage of being able to handle badly seg-mented word boundaries efficiently, however it pre-vents the next normalization steps from knowingwhat graphemes were in the initial sequence (Kobuset al, 2008).What fundamentally separates the noisy textcleansing task from the spell-checking problem isthat most often lexical ill-formedness in these me-dia is intentional.
Han (2011) proposes that thismight be in an attempt to save characters in length-constrained media (such as Twitter or SMS), forsocial identity (conversing in the dialect of a spe-cific group), or due to convention of the medium.Emotional context is typically expressed with re-peat characters such as ?I am sooooooo tired?
orexcessive punctuation.
At times, however, out-of-vocabulary tokens (spelling errors) might resultpurely as the result of cognitive oversight.Cook and Stevenson (2009) are one of the first toexplicitly analyze the types of transformations found21in short message domains.
They identify: 1) stylis-tic variation (better?betta), 2) subsequence abbre-viation (doing?dng), 3) clipping of the letter ?g?
(talking?talkin), 4) clipping of ?h?
(hello?ello),and 5) general syllable clipping (anyway?neway),to be the most frequent transformations.
Cook andStevenson then incorporate these transformationsinto their model.
The idea is that such an unsuper-vised approach based on the linguistic properties ofcreative word forms has the potential to be adaptedfor normalization in other similar genres without thecost of developing a large training corpus.
Most im-portantly, they find that many creative texting formsare the result of a small number of specific word for-mation processes.Han (2011) performs a simple analysis on the out-of-vocabulary words found in Twitter, and find thatthe majority of ill-formed words in Twitter can beattributed to instances where letters are missing orwhere there are extraneous letters, but the lexicalcorrespondence to the target word is trivially acces-sible.
They find that most ill-formed words are basedon morphophonemic variations.3 MotivationAll of the previous work described in Section 2 ei-theri) only focus on recovering the most likely ?stan-dard English?
form of a message, disregardingthe stylistic structure of the original noisy text,orii) considers the structure of the noisy text foundin a medium as a whole, only as a first step(the means) to identify common types of noisytransformations which can subsequently be ac-counted for (or ?corrected?)
to produce normal-ized messages (the desired end result).However, based on the fact that language is highlycontextual, we ask the question: What influencedoes the context in which a message is producedhave on the resulting observed surface structure andstyle of the message?In general, since some topics are for instancemore formal or informal than others, vocabulary andlinguistic style often changes based on the topic thatis being discussed.
Moreover, in social media onecan identify several other types of context.
Specif-ically in Twitter, one might consider a user?s geo-graphical location, the client from which a user isbroadcasting her message, how long she has beenusing the Twitter service, and so forth.The intuition is that the unconstrained nature ofthese media afford users the ability to invent writingconventions to suit their needs.
Since users?
needsdepend on their circumstances, and hence their con-text, we hypothesize that the observed writing sys-tems might be influenced by some elements of theircontext.
For instance, phonemic writing systemsmight be related to a user?s dialect which is re-lated to a user?s geographical location.
Furthermore,highly compressed writing conventions (throwingaway vowels, using prefixes of words, etc.)
mightresult from the relatively high input cost associ-ated with using unwieldy keypads on some mobileclients, etc.The present work is focused on looking at thesestylistic elements of messages found in social media,by analyzing the types of stylistic variation at thelexical level, across these contextual dimensions.4 MethodIn the following discussion we make a distinc-tion between within-tweet context and the generalmessage-context in which a message is created.Within-tweet context is the linguistic context (theother terms) that envelopes a term in a Twitter mes-sage.
The general context of a Twitter message is theobservable elements of the environment in which itwas conceived.
For the current study, we record1.
the user?s location, and2.
the client from which the message was sent,We follow a two-pronged analytic approach:Firstly, we conduct a na?
?ve, context-free analysis(at the linguistic level) of all words not commonlyfound in standard, everyday English.
This analy-sis purely looks at the terminology that are foundon Twitter, and does not attempt to normalize thesemessages in any way.
Therefore, different surfaceforms of the same word, such as ?today?, ?2day?,?2d4y?, are all considered distinct terms.
We thenanalyse the terminology over different contextual di-mensions such as client and location.22Secondly, we perform a more in-depth and con-textual analysis (at the word level) by first normaliz-ing the potentially noisy message to recover the mostlikely surface form of the message and recording thetypes of changes that were made, and then analyz-ing these types of changes across different generalcontextual dimensions (client and location).As noted in Section 2, text message normalizationis not a trivial process.
As shown by Han (2011),most transformations from in-vocabulary words toout-of-vocabulary words can be attributed to a singleletter that is changed, removed, or added.
Further-more, they note that most ill-formed words are re-lated to some morphophonemic variation.
We there-fore implemented a text cleanser based on the de-sign of Contractor (2010) using pre-processing tech-niques discussed in (Kaufmann and Kalita, 2010).It works as follows: For each input message, wereplace @-usernames with ?*USR*?
and urls with?*URL*?.
Hash tags can either be part of the sen-tence (?just got a #droid today?)
or be peripheral tothe sentence (?what a loooong day!
#wasted?).
Fol-lowing Kaufmann (2010) we remove hashtags at theend of messages when they are preceded by typicalend-of-sentence punctuation marks.
Hash tags in themiddle of messages are retained, and the hash signremoved.Next we tokenize this preprocessed message us-ing the NLTK tokenizer (Loper and Bird, 2002).
Asnoted earlier, standard NLP tools do not performwell on noisy text out-of-the-box.
Based on inspec-tion of incorrectly tokenized output, we therefore in-clude a post-tokenization phase where we split alltokens that include a punctuation symbol into the in-dividual one or two alphanumeric tokens (on eitherside of the punctuation symbol), and the punctuationsymbol1.
This heuristic catches most cases of run-onsentences.Given a set of input tokens, we process these oneby one, by comparing each token to the words inthe lexicon L and constructing a confusion networkCN.
Each in-vocabulary term, punctuation token orother valid-but-not-in-vocabulary term is added toCN with probability 1.0 as shown in Algorithm 1.1This is easily accomplished using a regular expressiongroup-substitution of the form (\w*)([P])(\w*)?
[\1,\2, \3], where \w represents the set of alphanumeric char-acters, and P is the set of all punctuation marks [.,;?".
.
.
]Character Transliteration candidates1 ?1?, ?l?, ?one?2 ?2?, ?to?, ?too?, ?two?3 ?3?, ?e?, ?three?4 ?4?, ?a?, ?for?, ?four?5 ?5?, ?s?, ?five?6 ?6?, ?b?, ?six?7 ?7?, ?t?, ?seven?8 ?8?, ?ate?, ?eight?9 ?9?, ?g?, ?nine?0 ?0?, ?o?, ?zero??@?
?
@?, ?at??&?
?&?, ?and?Table 1: Transliteration lookup table.valid tok(wi) checks for ?
*USR*?, ?
*URL*?, orany token longer than 1 character with no alphabet-ical characters.
This heuristic retains tokens such as?9-11?, ?12:44?, etc.At this stage, all out-of-vocabulary (OOV) termsrepresent the terms that we are uncertain about, andhence candidate terms for cleansing.
First, for eachOOV term, we enumerate each possibly ambiguouscharacter into all its possible interpretations with thetransliteration table shown in Table 1.
This expands,for example, ?t0day??
[?t0day?, ?today?
], and also?2day??
[?2day?, ?twoday?, ?today?
], etc.Each transliterated candidate word in each con-fusion set produced this way is then scored withthe original word and ranked using the heuristicfunction (sim()) described in (Contractor et al,2010)2.
We also evaluated a purely phonetic edit-distance similarity function, based on the DoubleMetaphone algorithm (Philips, 2000), but found thestring-similarity-based function to give more reli-able results.Each confusion set produced this way (see Al-gorithm 2) is joined to its previous set to form agrowing confusion lattice.
Finally this lattice is de-coded by converting it into the probabilistic finite-state grammar format, and by using the SRI-LMtoolkit?s (Stolcke, 2002) lattice-tool com-mand to find the best path through the lattice by2The longest common subsequence between the two words,normalized by the edit distances between their consonant skele-tons.23Transformation Type Rel %single char (?see??
?c?)
29.1%suffix (?why??
?y?)
18.8%drop vowels (?be??
?b?)
16.4%prefix (?tomorrow??
?tom?)
9.0%you to u (?you??
?u?)
8.3%drop last char (?running??
?runnin?)
7.0%repeat letter (?so??
?soooo?)
5.5%contraction (?you will??
?you?ll?)
5.0%th to d (?this??
?dis?)
1.0%Table 2: Most frequently observed types of transforma-tions with an example in parentheses.
Rel % shows therelative percentage of the top-10 transformations whichwere identified (excluding unidentified transformations)to belong to a specific class.making use of a language model to promote fluid-ity in the text, and trained as follows:We generated a corpus containing roughly 10Mtokens of clean English tweets.
We used a simpleheuristic for selecting clean tweets: For each tweetwe computed if #(OOV )#(IV )+1 < ?, where ?
= 0.5was found to give good results.
On this corpuswe trained a trigram language model, using Good-Turing smoothing.
Next, a subset of the LA Timescontaining 30M words was used to train a ?generalEnglish?
language model in the same way.
Thesetwo models were combined3 in the ratio 0.7 to 0.3.The result of the decoding process is the hypoth-esized clean tokens of the original sentence.
When-ever the cleanser makes a substitution, it is recordedfor further analysis.
Upon closer inspection, it wasfound that most transformation types can be recog-nized by using a fairly simple post-processing step.Table 2 lists the most frequent types of transforma-tions.
While these transformations do not have per-fect coverage, they account for over 90% of the (cor-rect) transformations produced by the cleanser.
Therules fail to cover relatively infrequent edge cases,such as ?l8r ?
later?, ?cuz ?
because?, ?dha ?the?, and ?yep?
yes?
4.3Using the -mix-lm and -lambda and -mix-lambda2options to the SRI-LM toolkit?s ngram module.4To our surprise these ?typical texting forms?
disappearedinto the long tail in our data set.Original CleansedSwet baby jeebus, some-one PLEASE WINE ME!sweet baby jesus , some-one please wine me !2 years with Katie today!
two years with katie to-day!k,hope nobody washurt.gud mornin jareokay , hope nobody washurt .
good morning jamieWhen u a bum but think uda best person on da courtyou doodooforthebooboowhen you a bum but thinkyou the best person on thecourt you dorothyNYC premiere 2morrow.
nice premiere tomorrow .Table 3: Examples of original and automatically cleansedversions of Twitter messages.Algorithm 1 Main cleanser algorithm pseudo code.The decode() command converts the confusionnetwork (CN) into PFSG format and decodes it us-ing the lattice-tool of the SRI-LM toolkit.Require: Lexicon L, Punctuation set Pfunction CLEANSE MAIN(Min)for wi ?Min doif wi ?
L ?
P or valid tok(wi) thenAdd (1.0, wi) to CNout .
Probability 1.0elseAdd conf set(wi) to CNoutend ifend forreturn decode(CNout)end functionTable 3 illustrates some example correctionsmade by the cleanser.
As the results show, thecleanser is able to correct many of the more com-mon types of transformations, but can fail when itencounters infrequent or out-of-vocabulary terms.5 EvaluationThis section describes our empirical evaluation andanalysis of how users in different contexts expressthemselves differently using microtexts.
We focusspecifically on the types of lexical transformationsthat are commonly applied globally, within popula-tions of users, and in a contextualized manner.24Algorithm 2 Algorithm pseudo code for generatingconfusion set CS.
L[wi] is the lexicon partitioningfunction for word wi.Require: Lexicon L, confusion set CS, implemented astop-K heap containing (si, wi), indexed on sifunction CONF SET(wi)W?
translits(wi)for wj ?W dofor wk ?
L[wj ] dosk ?
sim(wj , wk)if sk > min(CS) thenAdd (sk, wk) to CSend ifend forend forreturn CSend function5.1 Out-of-Vocabulary AnalysisWe begin by analyzing the types of terms that arecommon in microtexts but not typically used inproper, everyday English texts (such as newspapers).We refer to such terms as being out-of-vocabulary,since they are not part of the common written En-glish lexicon.
The goal of this analysis is to un-derstand how different contexts affect the numberof out-of-vocabulary terms found in microtexts.
Wehypothesize that certain contextual factors may in-fluence a user?s ability (or interest) to formulateclean microtexts that only contain common Englishterms.We ran our analysis over a collection of one mil-lion Twitter messages collected using the Twitterstreaming API during 2010.
Tweets gathered fromthe Twitter API are tagged with a language identifierthat indicates the language a user has chosen for hisor her account.
However, we found that many tweetspurported to be English were in fact not.
Hence,we ran all of the tweets gathered through a simpleEnglish language classifier that was trained using asmall set of manually labeled tweets, uses charactertrigrams and average word length as features, andachieves an accuracy of around 93%.
The every-day written English lexicon, which we treat as the?gold standard?
lexicon, was distilled from the samecollection of LA Times news articles described inSection 4.
This yielded a comprehensive lexicon ofapproximately half a million terms.Timezone % In-VocabularyAustralia 86%UK 85%US (Atlantic) 84%Hong Kong 83%US (Pacific) 81%Hawaii 81%Overall 81%Table 4: Percentage of in-vocabulary found in large En-glish lexicon for different geographic locations.For each tweet, the tokenized terms were lookedup in the LA Times lexicon to determine if theterm was out-of-vocabulary or not.
Not surprisingly,the most frequent out-of-vocabulary terms identi-fied are Twitter usernames, URLs, hasthags, and RT(the terminology for a re-broadcast, or re-tweeted,message).
These tokens alone account for approx-imately half of all out-of-vocabulary tokens.
Themost frequent out-of-vocabulary terms include ?lol?,?haha?, ?gonna?, ?lmao?, ?wanna?, ?omg?, ?gotta?.Numerous expletives also appear amongst the mostcommon out-of-vocabulary terms, since such termsnever appear in the LA Times.
Out of vocabularyterms make up 19% of all terms in our data set.In the remainder of this section, we examinethe out-of-vocabulary properties of different popu-lations of users based on their geographic locationand their client (e.g., Web-based or mobile phone-based).5.1.1 Geographic LocationsTo analyze the out-of-vocabulary properties ofusers in different geographic locations, we extractedthe time zone information from each Tweet in ourdata set.
Although Twitter allows users to specifytheir location, many users leave this field blank, useinformal terminology (?lower east side?
), or fabri-cate non-existent locations (e.g., ?wherever i wantto be?).
Therefore, we use the user?s time zone asa proxy for their actual location, in hopes that usershave less incentive to provide incorrect information.For the Twitter messages associated with a giventime zone, we computed the percentage of tokensfound within our LA Times-based lexicon.
The re-sults from this analysis are provided in Table 4.
It is25Client % In-VocabularyFacebook 88%Twitter for iPhone 84%Twitter for Blackberry 83%Web 82%UberTwitter 78%Snaptu 73%Overall 81%Table 5: Percentage of in-vocabulary found in large En-glish lexicon for different Twitter clients.important to note that these results were computedover hundreds of thousands of tokens, and hencethe variance of our estimates is very small.
Thismeans that the differences observed here are statis-tically meaningful, even though the absolute differ-ences tend to be somewhat small.These results indicate that microtexts composedby users in different geographic locations exhibitdifferent amounts of out-of-vocabulary terms.
Usersin Australia, the United Kingdom, Hong Kong, andthe East Coast of the United States (e.g., New YorkCity) include fewer out-of-vocabulary terms in theirTweets than average.
However, users from the WestCoast of the United States (e.g., Los Angeles, CA)and Hawaii are on-par with the overall average, butinclude 5% more out-of-vocabulary terms than theAustralian users.As expected, the locations with fewer-than-average in-vocabulary tokens are associated withnon-English speaking countries, despite the outputfrom the classifier.5.1.2 Twitter ClientsIn a similar experiment, we also investigated thefrequency of out-of-vocabulary terms conditionedon the Twitter client (or ?source?)
used to composethe message.
Example Twitter clients include theWeb-based client at www.twitter.com, officialTwitter clients for specific mobile platforms (e.g.,iPhone, Android, etc.
), and third-party clients.
Eachclient has its own characteristics, target user base,and features.In Table 5, we show the percentage of in-vocabulary terms for a sample of the most widelyused Twitter clients.
Unlike the geographic location-based analysis, which showed only minor differ-ences amongst the user populations, we see muchmore dramatic differences here.
Some clients, suchas Facebook, which provides a way of cross-postingstatus updates between the two services, has thelargest percentage of in-vocabulary terms of the ma-jor clients in our data.One interesting, but unexpected, finding is that themobile phone (i.e., iPhone and Blackberry) clientshave fewer out-of-vocabulary terms, on average,than the Web-based client.
This suggests that ei-ther the users of the clients are less likely to misspellwords or use slang terminology or that the clientsmay have better or more intuitive spell checking ca-pabilities.
A more thorough analysis is necessary tobetter understand the root cause of this phenomenon.At the other end of the spectrum are the UberTwit-ter and Snaptu clients, which exhibit a substantiallylarger number of out-of-vocabulary terms.
Theseclients are also typically used on mobile devices.
Aswith our previous analysis, it is difficult to pinpointthe exact cause of such behavior, but we hypothe-size that it is a function of user demographics anddifficulties associated with inputting text on mobiledevices.5.2 Contextual AnalysisIn this section, we test the hypothesis that differentuser populations make use of different types of lex-ical transformations.
To achieve this goal, we makeuse of our noisy text cleanser.
For each Twitter mes-sage run through the cleanser, we record the origi-nal and cleaned version of each term.
For all of theterms that the cleanser corrects, we automaticallyidentify which (if any) of the transformation ruleslisted in Table 2 explain the transformation betweenthe original and clean version of the term.
We usethis output to analyze the distribution of transforma-tions observed across different user populations.We begin by analyzing the types of transforma-tions observed across Twitter clients.
Figure 1 plotsthe (normalized) distribution of lexical transforma-tions observed for the Web, Twitter for Blackberry,Twitter for iPhone, and UberTwitter clients, groupedby the transformations.
We also group the trans-formations by the individual clients in Figure 2 formore direct comparison.The results show that Web users tend to use more26Figure 1: Proportion of transformations observed acrossTwitter clients, grouped by transformation type.contractions than Blackberry and UberTwitter users.We relate this result to the differences in typing ona virtual compared to a multi-touch keypad.
It wassurprising to see that iPhone users tended to use con-siderably more contractions than the other mobiledevice clients, which we relate to its word-predictionfunctionality.
Another interesting result is the factthat Web users often drop vowels to shorten termsmore than their mobile client counterparts.
Instead,mobile users often use suffix-style transformationsmore, which is often more aggressive than the drop-ping vowels transformation, and possibly a result ofthe pervasiveness of mobile phones: Large popu-lations of people?s first interaction with technologythese days are through a mobile phone, a devicewhere strict length limits are imposed on texting,and which hence enforce habits of aggressive lex-ical compression, which might transfer directly totheir use of PCs.
Finally, we observe that mobile de-vice users replace ?you?
with ?u?
substantially morethan users of the Web client.We also performed the same analysis across timezones/locations.
The results are presented in Fig-ure 3 by transformation-type, and again grouped bylocation for direct comparison in Figure 4.
We ob-serve, perhaps not surprisingly, that the East CoastUS, West Coast US, and Hawaii are the most similarwith respect to the types of transformations that theyFigure 2: Proportion of transformations observed acrossTwitter clients, grouped by client.commonly use.
However, the most interesting find-ing here is that British users tend to utilize a notice-ably different set of transformations than Americanusers in the Pacific time zones.
For example, Britishusers are much more likely to use contractions andsuffixes, but far less likely to drop the last letter ofa word, drop all of the vowels in a word, use prefix-style transformations, or to repeat a given letter mul-tiple times.
In a certain sense, this suggests thatBritish users tend to write more proper, less informalEnglish and make use of strikingly different stylesfor shortening words compared to American users.This might be related to the differences in dialectsbetween the two regions manifesting itself during aprocess of phonetic transliteration when composingthe messages: Inhabitants of the south-west regionsin the US are known for pronouncing for instancerunning as runnin?, which manifests as dropping thelast letter, and so forth.Therefore, when taken with our out-of-vocabularyanalysis, our experimental evaluation shows clearevidence that different populations of users expressthemselves differently online and use different typesof lexical transformations depending on their con-text.
It is our hope that the outcome of this studywill spark further investigation into these types ofissues and ultimately lead to effective contextually-aware natural language processing and informationretrieval approaches that can adapt to a wide rangeof user contexts.27Figure 3: Proportion of transformations observed acrossgeographic locations, grouped by transformation type.6 Conclusions and Future WorkThis paper investigated the writing conventions thatdifferent groups of users use to express themselvesin microtexts.
We analyzed characteristics of termsthat are commonly found in English Twitter mes-sages but are never seen within a large collectionof LA Times news articles.
The results showedthat a very small number of terms account for alarge proportion of the out-of-vocabulary terms.
Thesame analysis revealed that different populations ofusers exhibit different propensities to use out-of-vocabulary terms.
For example, it was found thatBritish users tend to use fewer out-of-vocabularyterms compared to users within the United States.We also carried out a contextualized analysis thatleveraged a state-of-the-art noisy text cleanser.
Byanalyzing the most common types of lexical trans-formations, it was observed that the types of trans-formations used varied across Twitter clients (e.g.,Web-based clients vs. mobile phone-based clients)and geographic location.
This evidence supportedour hypothesis that the measurable contextual indi-cators surrounding messages in social media play animportant role in determining how messages in thesemedia vary at the surface (lexical) level from whatmight be considered standard English.The outcome of our empirical evaluation andsubsequent analysis suggests that human languageFigure 4: Proportion of transformations observed acrossgeographic locations, grouped by location.technologies (especially natural language process-ing techniques that rely on well-formed inputs) arelikely to be highly susceptible to failure as the resultof lexical transformations across nearly all popula-tions and contexts.
However, certain simple rulescan be used to clean up a large number of out-of-vocabulary tokens.
Unfortunately, such rules wouldnot be able to properly correct the long tail ofthe out-of-vocabulary distribution.
In such cases,more sophisticated approaches, such as the noisytext cleanser used in this work, are necessary tocombat the noise.
Interestingly, most of the lexicaltransformations observed affect non-content words,which means that most information retrieval tech-niques will be unaffected by such transformations.As part of future work, we are generally interestedin developing population and/or context-aware lan-guage processing and understanding techniques ontop of microtexts.
We are also interested in ana-lyzing different user contexts, such as those basedon age and gender and to empirically quantify theeffect of noise on actual natural language process-ing and information retrieval tasks, such as part ofspeech tagging, parsing, summarization, etc.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir insightful comments.
Stephan Gouws wouldlike to thank MIH Holdings Ltd. for financial sup-port during the course of this work.28ReferencesA.T.
Aw, M. Zhang, J. Xiao, and J. Su.
2006.
A Phrase-based Statistical Model for SMS Text Normalization.In Proceedings of the COLING/ACL Main ConferencePoster Sessions, pages 33?40.
Association for Compu-tational Linguistics.S.
Bangalore, V. Murdock, and G. Riccardi.
2002.
Boot-strapping Bilingual Data Using Consensus Transla-tion for a Multilingual Instant Messaging System.
InProceedings of the 19th International Conference onComputational Linguistics-Volume 1, pages 1?7.
As-sociation for Computational Linguistics.R.
Beaufort, S. Roekhaut, L.A. Cougnon, and C. Fa-iron.
2010.
A Hybrid Rule/Model-based Finite-StateFramework for Normalizing SMS Messages.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 770?779.
Asso-ciation for Computational Linguistics.M.
Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,and A. Basu.
2007.
Investigation and Modeling of theStructure of Texting Language.
International Journalon Document Analysis and Recognition, 10(3):157?174.D.
Contractor, T.A.
Faruquie, and L.V.
Subramaniam.2010.
Unsupervised Cleansing of Noisy Text.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 189?196.Association for Computational Linguistics.P.
Cook and S. Stevenson.
2009.
An UnsupervisedModel for Text Message Normalization.
In Proceed-ings of the Workshop on Computational Approachesto Linguistic Creativity, pages 71?78.
Association forComputational Linguistics.Bo Han and Timothy Baldwin.
2011.
Lexical Normal-isation of Short Text Messages: Makn Sens a #twit-ter.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies.
Association for Compu-tational Linguistics.M.
Kaufmann and J. Kalita.
2010.
Syntactic Normaliza-tion of Twitter Messages.C.
Kobus, F. Yvon, and G. Damnati.
2008.
Normaliz-ing SMS: Are Two Metaphors Better Than One?
InProceedings of the 22nd International Conference onComputational Linguistics-Volume 1, pages 441?448.Association for Computational Linguistics.E.
Loper and S. Bird.
2002.
NLTK: The Natural Lan-guage Toolkit.
In Proceedings of the ACL-02 Work-shop on Effective tools and Methodologies for Teach-ing Natural Language Processing and ComputationalLinguistics-Volume 1, pages 63?70.
Association forComputational Linguistics.L.
Philips.
2000.
The Double Metaphone Search Algo-rithm.
CC Plus Plus Users Journal, 18(6):38?43.C.E.
Shannon and W. Weaver.
1948.
The Mathemati-cal Theory of Communication.
Bell System TechnicalJournal, 27:623?656.A.
Stolcke.
2002.
SRILM - An Extensible LanguageModeling Toolkit.
In Proceedings of the Interna-tional Conference on Spoken Language Processing,volume 2, pages 901?904.29
