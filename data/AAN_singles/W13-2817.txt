Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 117?122,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsUses of Monolingual In-Domain Corpora for Cross-DomainAdaptation with Hybrid MT ApproachesAn-Chang Hsieh, Hen-Hsen Huang and Hsin-Hsi ChenDepartment of Computer Science and Information EngineeringNational Taiwan UniversityNo.
1, Sec.
4, Roosevelt Road, Taipei, 10617 Taiwan{achsieh,hhhuang}@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.twAbstractResource limitation is challenging for cross-domain adaption.
This paper employs patternsidentified from a monolingual in-domain cor-pus and patterns learned from the post-editedtranslation results, and translation model aswell as language model learned from pseudobilingual corpora produced by a baseline MTsystem.
The adaptation from a governmentdocument domain to a medical recorddomain shows the rules mined from themonolingual in-domain corpus are useful,and the effect of using the selected pseudobilingual corpus is significant.1 IntroductionBilingual dictionary and corpus are importantresources for MT applications.
They are used forlexical choice and model construction.
However,not all resources are available in bilingual formsin each domain.
For example, medical recordsare in English only in some countries.
In such acase, only bilingual dictionary and monolingualcorpus is available.
Lack of bilingual corpusmakes domain adaptation more challenging.A number of adaptation approaches (Civeraand Juan, 2007; Foster and Kuhn 2007; Foster et al2010, Matsoukas et al 2009; Zhao et al 2004)have been proposed.
They address the reliabilityof a model in a new domain and count the do-main similarities between a model and the in-domain development data.
The domain relevancein different granularities including words,phrases, sentences, documents and corpora areconsidered.
Ueffing et al(2007) propose semi-supervised methods which use monolingual datain source language to improve translation per-formance.
Schwenk (2008) present lightly-supervised training to generate additional train-ing data from the translation results of monolin-gual data.
To deal with the resource-poor issue,Bertoldi and Federico (2009) generate a pseudobilingual corpus from the monolingual in-domaincorpus, and then train a translation model fromthe pseudo bilingual corpus.Besides counting similarities and generatingpseudo bilingual in-domain corpus, text simplifi-cation (Zhu et al 2010; Woodsend and Lapata,2011; Wubben et al 2012) is another direction.Simplifying a source language text makes thetranslation easier in a background MT system.Chen et al(2012a) propose a method to simplifya sentence before MT and to restore the transla-tion of the simplified part after MT.
They focuson the treatments of input text only, but do notconsider how to adapt the background MT to thespecific domain.
The translation performancedepends on the coverage of the simplificationrules and the quality of the background system.This paper adopts the simplification-translation-restoration methodology (Chen et al2012a), but emphasizes on how to update bilin-gual translation rules, translation model and lan-guage model, which are two kernels of rule-based and statistics-based MT systems, respec-tively.
This paper is organized as follows.
Sec-tion 2 specifies the proposed hybrid MT ap-proaches to resource-limited domains.
The char-acteristics of available resources including theirtypes, their linguality, their belonging domains,and their belonging languages are analyzed andtheir uses in translation rule mining and modelconstruction are presented.
Section 3 discusseshow to adapt an MT system from a governmentdocument domain to a medical record domain.The experimental setups reflect various settings.Section 4 concludes the remarks.117Figure 1: Hybrid MT Approaches2 Hybrid MT ApproachesFigure 1 sketches the overall picture of our pro-posed hybrid MT approaches.
A resource is rep-resented in terms of its linguality, domain, lan-guage, and type, where MO/BI denotes mono-lingual/bilingual, ID/OD denotes in-domain/out-domain, and SL/TL denotes source lan-guage/target language.
For example, an MO-ID-SL corpus and an MO-ID-TL corpus mean mon-olingual in-domain corpora in source and in tar-get languages, respectively.
Similarly, a BI-ODcorpus and a BI-ID dictionary denote a bilingualout-domain corpus, and a bilingual in-domaindictionary, respectively.Resources may be provided by some organi-zations such as LDC, or collected from hetero-geneous resources.
The MO-ID-SL/TL corpus,the BI-OD corpus, and the BI-ID dictionary be-long to this type.
Besides, some outputs generat-ed by the baseline MT systems are regarded asother kinds of resources for enhancing the pro-posed methods incrementally.
Initial translationresults, selected translation results, and post-edited translation results, which form pseudobilingual in-domain corpora, belong to this type.The following subsections first describe thebaseline systems with the original resources andthen specify the advanced systems with the gen-erated resources.2.1 A baseline translation systemIn an extreme case, only a bilingual out-domaincorpus, a monolingual in-domain corpus insource/target language, a bilingual in-domaindictionary and a monolingual in-domain thesau-rus in source language are available.
The bilin-gual out-domain corpus is used to train transla-tion and language models by Moses.
They forma background out-domain translation system.A pattern miner is used to capture the writtenstyles in the monolingual in-domain corpus insource language.
A monolingual in-domain the-saurus in source language is looked up to extractthe class (sense) information of words.
Mono-lingual patterns are mined by counting frequentword/class n-grams.
Then, the bilingual in-domain dictionary is introduced to formulatetranslation rules based on the mined monolin-gual patterns.
Here in-domain experts may beinvolved in reviewing the bilingual rules.
Thehuman cost will affect the number of translationrules formulated and thus its coverage.The baseline translation system is composedof four major steps shown as follows.
(1) and (2)are pre-processing steps before kernel MT, and(4) is a post-processing step after kernel MT.
(1) Identifying and translating in-domainsegments from an input sentence by usingtranslation rules.118(2) Simplifying the input sentence by replac-ing the in-domain segments as follows.
(a) If an in-domain segment is a term inthe bilingual in-domain dictionary,we find a related term (i.e., hypernymor synonym) in the in-domain thesau-rus which has relatively more occur-rences in the background SMT sys-tem to replace the term.
(b) If an in-domain segment is a nounphrase, we keep its head only, andfind a related term of the head as (a).
(c) If an in-domain segment is a verbphrase composed of a verb and anoun phrase, we keep the verb andsimplify the noun phrase as (b).
(d) If an in-domain segment is a verbphrase composed of a verb and aprepositional phrase, we keep theverb and remove the prepositionalphrase if it is optional.
If the preposi-tional phrase is mandatory, it is keptand simplified as (e).
(e) If an in-domain segment is a preposi-tional phrase, we keep the prepositionand simplify the noun phrase as (b).
(f) If an in-domain segment is a clause,we simplify its children recursively as(a)-(e).
(3) Translating the simplified source sentenceby using the out-domain background MTsystem.
(4) Restoring the results of the bilingual in-domain segments translated in (1) back tothe translation results generated in (3).The restoration is based on the internalalignment between the source and the tar-get sentences.2.2 Incremental learningThere are several alternatives to update the base-line translation system incrementally.
The firstconsideration is the in-domain translation rules.They are formed semi-automatically by domainexperts.
The cost of domain experts results thatonly small portion of n-gram patterns along withthe corresponding translation are generated.
Thepost-editing results suggests more translationrules and they are fed back to revise the baselinetranslation system.The second consideration is translation modeland language model in the Moses.
In an idealcase, the complete monolingual in-domain cor-pus in source language is translated by the base-line translation system, then the results are post-edited by domain experts, and finally the com-plete post-edited bilingual corpus is fed back torevise both translation model and languagemodel.
However, the post-editing cost by do-main experts is high.
Only some samples of theinitial translation are edited by domain experts.On the one hand, the sampled post-edited in-domain corpus in target language is used to re-vise the language model.
On the other hand, thein-domain bilingual translation result beforepost-editing is used to revise the translationmodel and the language model.
Size and transla-tion quality are two factors to be considered.
Wewill explore the effect of different size of imper-fect in-domain translation results on refining thebaseline MT system.
Moreover, a selectionstrategy, e.g., only those translation results com-pletely in target language are considered, is in-troduced to sample ?relatively more accurate?bilingual translation results.In the above incremental learning, translationrules, translation model and language model arerevised individually.
The third consideration isto merge some refinements together and exam-ine their effects on the translation performance.3 Cross-Domain AdaptationTo evaluate the feasibility of the proposed hy-brid MT approaches, we adapt an English-Chinese machine translation system from a gov-ernment document domain to a medical recorddomain.
The linguistic resources are describedfirst and then the experimental results.3.1 Resource descriptionHong Kong parallel text (LDC2004T08), whichcontains official records, law codes, and pressreleases of the Legislative Council, the Depart-ment of Justice, and the Information ServicesDepartment of the HKSAR, respectively, andUN Chinese-English Parallel Text collection(LDC2004E12) is used to train the translationmodel.
These two corpora contain total 6.8Msentences.
The Chinese counterpart of the aboveparallel corpus and the Central News Agencypart of the Tagged Chinese Gigaword(LDC2007T03) are used to train trigram lan-guage model.
These two corpora contain total18.8M sentences.
The trained models are used inStep (3) of the baseline translation system.Besides the out-domain corpora for the devel-opment of translation model and language model,we select 60,448 English medical records (1.8Msentences) from National Taiwan University119Hospital (NTUH) to learn the n-gram patterns.Metathesaurus of the Unified Medical LanguageSystem (UMLS) provides medical classes of in-domain words.
A bilingual medical domain dic-tionary composed of 71,687 pairs is collected.Total 7.2M word/class 2-grams~5-grams areidentified.
After parsing, there remain 57.2Klinguistic patterns.
A higher order pattern maybe composed of two lower order patterns.
Keep-ing the covering patterns and ruling out the cov-ered ones further reduce the size of the extractedpatterns.
The remaining 40.1K patterns aretranslated by dictionary look-up.
Because of thehigh cost of medical record domain experts (i.e.,physicians), only a small portion is verified.
Fi-nally, 981 translation rules are formulated.
Theyare used in Step (1) of the baseline MT system.The detail rule mining and human correctionprocess please refer to Chen et al(2012b).We further sample 2.1M and 1.1M sentencesfrom NTUH medical record datasets, translatethem by the baseline MT system, and get 2.1M-and 1.1M-pseudo bilingual in-domain corpora.We will experiment the effects of the corpus size.On the other hand, we apply the selection strate-gy to select 0.95M ?good?
translation from2.1M-pseudo bilingual in-domain corpus.
Fur-thermore, some other 1,004 sentences are post-edited by the domain experts.
They are used tolearn the advanced MT systems.To evaluate the baseline and the advancedMT systems, we sample 1,000 sentences differ-ent from the above corpora as the test data, andtranslate them manually as the ground truth.3.2 Results and discussionTable 1 lists the methods along with the re-sources they used.
B is the baseline MT system.Most patterns appearing in the 57.2K learned n-grams mentioned in Section 3.1 are not reviewedby physicians due to their cost.
Part of these un-reviewed patterns may occur in the post-editeddata.
They will be further introduced into M1.
Inthe experiments, patterns appearing at least twotimes in the post-edited result are integrated intoM1.
Total 422 new patterns are identified.Translation model and language model in M1 isthe same as those in baseline system.In M2-M6, the translation rules are the sameas those in baseline MT system, only translationmodel and/or language model are re-trained.
InTranslation Rules Translation Model Language Model Tuning DataB 981 bilingual translation rules 6.8M government domain bilingualsentences18.8M government/news domainChinese sentences1000 government domainbilingual sentencesM1 981 bilingual translation rules +422 mined  rules from post-editing6.8M government domain bilingualsentences18.8M government/news domainChinese sentences200 post-edited medicaldomain sentencesM2 981 bilingual translation rules 6.8M government domain bilingualsentences804 post-edited Chinese sentences 200 post-edited medicaldomain sentencesM3 981 bilingual translation rules 6.8M government domain bilingualsentences30,000 Chinese sentences selectedfrom medical literature200 post-edited medicaldomain sentencesM4 981 bilingual translation rules 1.1M pseudo medical domain bilingualsentences generated by M11.1M pseudo medical domain Chinesesentences generated by M1200 post-edited medicaldomain sentencesM5 981 bilingual translation rules 2.1M pseudo medical domain bilingualsentences generated by M12.1M pseudo medical domain Chinesesentences generated by M1200 post-edited medicaldomain sentencesM6 981 bilingual translation rules 0.95M selected pseudo medical do-main bilingual sentences generated byM10.95M selected pseudo medical do-main Chinese sentences generated byM1200 post-edited medicaldomain sentencesM12 981 bilingual translation rules +422 mined  rules from post-editing6.8M government domain bilingualsentences804 post-edited Chinese sentences 200 post-edited medicaldomain sentencesM13 981 bilingual translation rules +422 mined  rules from post-editing6.8M government domain bilingualsentences30,000 medical domain Chinese sen-tences200 post-edited medicaldomain sentencesM14 981 bilingual translation rules +422 mined  rules from post-editing1.1M pseudo medical domain bilingualsentences generated by M11.1M pseudo medical domain Chinesesentences generated by M1200 post-edited medicaldomain sentencesM15 981 bilingual translation rules +422 mined  rules from post-editing2.1M pseudo medical domain bilingualsentences generated by M12.1M pseudo medical domain Chinesesentences generated by M1200 post-edited medicaldomain sentencesM16 981 bilingual translation rules +422 mined  rules from post-editing0.95M selected pseudo medical do-main bilingual sentences generated byM10.95M selected pseudo medical do-main Chinese sentences generated byM1200 post-edited medicaldomain sentencesTable 1: Resources used in each hybrid MT methodMethod Bleu Method Bleu Method Bleu Method Bleu Method Bleu Method BleuB 28.04 M2 39.45 M3 32.03 M4 34.86 M5 35.09 M6 40.48M1 39.72 M12 39.72 M13 32.85 M14 35.11 M15 35.52 M16 40.71Table 2: BLEU of each hybrid MT method120M2, 804 post-edited sentences are used to train anew language model, without changing thetranslation model.
In M3, paper abstracts inmedical domain are used to derive a new lan-guage model.
M4, M5 and M6 are similar exceptthat different sizes of corpora are used.
M4 andM5 use 1.1M and 2.1M sentences, respectively,while M6 uses 0.95M sentences chosen by usingthe selection strategy.
M12-M16 are combina-tions of M1 and M2-M6, respectively.
Transla-tion rules, translation model and language modelare refined by using different resources.
Total200 of the 1,004 post-edited sentences are se-lected to tune the parameters of Moses in theadvanced methods.Table 2 shows the BLEU of various MTmethods.
The BLEU of the MT system withoutemploying simplification-translation-restorationmethodology (Chen et al 2012a) is 15.24.
Ap-parently, the method B, which employs themethodology, achieves the BLEU 28.04 and ismuch better than the original system.
All theenhanced systems are significantly better thanthe baseline system B by t-test (p<0.05).
Com-paring M1 and M12-M16 with the correspond-ing systems, we can find that introducing themined patterns has positive effects.
M1 is evenmuch better than B.
Although the number of thepost-edited sentences is small, M2 and M12show such a resource has the strongest effects.The results of M3 and M13 depict that 30,000sentences selected from medical literature arenot quite useful for medical record translation.Comparing M4 and M5, we can find largerpseudo corpus is useful.
M6 shows using theselected pseudo subset performs much better.Comparing the top 4 methods, the best method,M16, is significantly better than M12 and M1(p<0.05), but is not different from M6 signifi-cantly (p=0.1662).We further analyze the translation results ofthe best methods M6 and M16 from two per-spectives.
On the one hand, we show how themined rules improve the translation.
The follow-ing list some examples for reference.
The un-derlined parts are translated correctly by newmined patterns in M16.
(1) Example: Stenting was done from distalIVC through left common iliac vein to ex-ternal iliac vein.M6: ?????
?
?
??
??????
?
?????
?
????
?M16: ??
?????
?
??
????
??
?
?????
?
????
?
(2) Example: We shifted the antibiotic tocefazolin.M6: ??
?
???
????
?M16: ??
?
???
??
?
????
?
(3) Example: Enhancement of right side pleu-ral, and mild pericardial effusion was not-ed .M6: ??
??
?
??
??
?
?
???
??
?
????
?M16: ??
?
?
??
?
??
?????
???
?On the other hand, we touch on which factorsaffect the translation performance of M16.
Threefactors including word ordering errors, wordsense disambiguation errors and OOV (out-of-vocabulary) errors are addressed as follows.The erroneous parts are underlined.
(1) Ordering errorsExample: Antibiotics were discontinuedafter 8 days of treatment.M16: ???
??
?
8?
?
??
?Analysis: The correct translation result is?8 ?
?
??
?
???
???
?Thecurrent patterns are 2-5 grams, so that thelonger patterns cannot be captured.
(2) Word sense disambiguation errorsExample: After tracheostomy, he wastransferred to our ward for post operationcare.M16: ?????
?
?
?
?
???
??
??
?
??
??
??
?Analysis: The correct translation of ?postoperation care?
should be ?????
?.However, the 1,004 post-edited sentencesare still not large enough to cover the pos-sible patterns.
Incremental update will in-troduce more patterns and may decreasethe number of translation errors.
(3) OOV errorsExample: Transcatheter intravenous uro-kinase therapy was started on 1/11 for 24hours infusion.M16: transcatheter ??
???
?
1/11??
??
??
24 ??
??
?Analysis: The word ?transcatheter?
is anOOV.
Its translation should be???
".4 ConclusionThis paper considers different types of resourcesin cross-domain MT adaptation.
Several meth-ods are proposed to integrate the mined transla-121tion rules, translation model and language model.The adaptation experiments show that the rulesmined from the monolingual in-domain corpusare useful, and the effect of using the selectedpseudo bilingual corpus is significant.Several issues such as word ordering errors,word sense disambiguation errors, and OOVerrors still remain for further investigation in thefuture.AcknowledgmentsThis work was partially supported by NationalScience Council (Taiwan) and Excellent Re-search Projects of National Taiwan Universityunder contracts NSC101-2221-E-002-195-MY3and 102R890858.
We are very thankful to Na-tional Taiwan University Hospital for providingNTUH the medical record dataset.ReferencesN.
Bertoldi and M. Federico.
2009.
Domain adapta-tion for statistical machine translation with mono-lingual resources.
In Proceedings of the FourthWorkshop on Statistical Machine Translation,pages 182?189.H.B.
Chen, H.H.
Huang, H.H.
Chen and C.T.
Tan.2012a.
A simplification-translation-restorationframework for cross-domain SMT applications.
InProceedings of COLING 2012, pages 545?560.H.B.
Chen, H.H.
Huang, J. Tjiu, C.Ti.
Tan and H.H.Chen.
2012b.
A statistical medical summary trans-lation system.
In Proceedings of 2012 ACMSIGHIT International Health Informatics Sympo-sium, pages.
101-110.J.
Civera and A. Juan.
2007.
Domain adaptation instatistical machine translation with mixture model-ing.
In Proceedings of the Second Workshop onStatistical Machine Translation, pages 177?180.G.
Foster and R. Kuhn.
2007.
Mixture-model adapta-tion for SMT.
In Proceedings of the Second Work-shop on Statistical Machine Translation, pages128?135.G.
Foster, C. Goutte, and R. Kuhn.
2010.
Discrimina-tive instance weighting for domain adaptation instatistical machine translation.
In Proceedings ofEMNLP 2010, pages 451?459.S.
Matsoukas, A.I.
Rosti, and B. Zhang.
2009.
Dis-criminative corpus weight estimation for machinetranslation.
In Proceedings of EMNLP 2009, pages708?717.H.
Schwenk.
2008.
Investigations on large-scalelightly-supervised training.
In Proceedings ofIWSLT 2008, pages 182?189.N.
Ueffing, G. Haffari and A. Sarkar.
2007.
Trans-ductive learning for statistical machine translation.In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages25?32,K.
Woodsend and M. Lapata.
2011.
Learning to sim-plify sentences with quasi-synchronous grammarand integer programming.
In Proceedings ofEMNLP 2011, pages 409?420.S.
Wubben and  A. van den Bosch, and  E. Krahmer.2012.
Sentence simplification by monolingual ma-chine translation.
In Proceedings of ACL 2012,pages 1015?1024.B.
Zhao, M. Eck, M. and S. Vogel.
2004.
Languagemodel adaptation for statistical machine translationvia structured query models.
In Proceedings ofCOLING 2004, pages 411?417.Z.
Zhu, D. Bernhard, and I. Gurevych.
2010.
A mon-olingual tree-based translation model for sentencesimplification.
In Proceedings of COLING 2010,pages 1353?1361.122
