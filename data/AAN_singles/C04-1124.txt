Detecting Multiword Verbs in the EnglishSublanguage of MEDLINE AbstractsChun Xiao and Dietmar Ro?snerInstitut fu?r Wissens- und SprachverarbeitungOtto-von-Guericke Universita?t MagdeburgUniversita?tsplatz 2, Magdeburg, Germany 39106xiao|roesner@iws.cs.uni-magdeburg.deAbstractIn this paper, we investigate the multiwordverbs in the English sublanguage of MED-LINE abstracts.
Based on the integration ofthe domain-specific named entity knowledgeand syntactic as well as statistical information,this work mainly focuses on how to evaluate aproper multiword verb candidate.
Our resultspresent a sound balance between the low- andhigh-frequency multiword verb candidates inthe sublanguage corpus.
We get a F-measure of0.753, when tested on a manual sample subsetconsisting of multiword candidates with bothlow- and high-frequencies.1 IntroductionDuring the construction of an information extraction(IE) system in the biomedical domain, we foundthat not only the task of named entity recognition(NER), but also the appropriate handling of verbsin this domain plays an important role.
It is veryhelpful to determine the domain-specific verbs in aspecific domain when extracting useful information,because the domain-specific verbs construct seman-tic relations between named entities (NEs).
How-ever, three problems in the handling of verbs in aspecific domain are still open:The first problem is how to determine domain-specific verbs.
This problem has not receivedenough notice from most of the researchers yet.
Ac-tually, domain-specific verbs have been mentionedquite often in biomedical text processing (Thomaset al, 2000; Ono et al, 2001; Xiao and Ro?sner,2004b), but often referred to a set of manually orexperientially selected verbs.
Spasic?
et al (2003)briefly presented a method to find domain-specificverbs by filtering the verbs in a stoplist, at the sametime, using the co-occurrence of a verb and specificterms in the text.
In our experiment, the domain-specific verbs are determined through the compari-son between different corpora in different domains,or through genre analysis of the sublanguage domi-nated corpus.The second problem is how to determine multi-word verbs (MWVs).
Here we do not makedifferences between the more detailed classifi-cation of multiword verbs, especially the verb-particle constructions and verb-preposition con-structions (Baldwin and Villavicencio, 2002).
Asa subcategory of multiword expressions (Sag et al,2002), MWVs raise the complexity of our process-ing.
Because some MWVs share the same verbhead but lead to different semantic interpretations,like result in and result from, considering only verbheads in the processing is obviously not sufficient.A good IE system should deal with such MWVs au-tomatically and appropriately.The third problem is that there is a need to in-vestigate the inflectional and derivational forms ofthe verbs.
An IE system may have to deal with aset of patterns, in which the inflectional and deriva-tional forms of the verbs should be taken into ac-count.
For example, in biomedical texts, the verbinteract defines a binary relation between two sub-stances, whereas its nominalization morpheme in apattern such as the interaction of ... with ... alsoconstructs such a relation.
Note that such patternsoften have close relationship with its common verblemma, which is often a MWV.
For instance, theabove pattern can map to the MWV interact with.Table 1 shows the distribution of all inflectional andderivational forms of the verb inhibit in a corpus of800 MEDLINE abstracts1 extracted from the GE-NIA corpus.2 This verb is a very important domain-specific verb in the biomedical domain.
To deal withthose inflectional and derivational forms appropri-ately will improve the performance of the IE sys-tem.The following text focuses on the second problem1MEDLINE/PubMed is a collection of references and ab-stracts from 4600 biomedical journals from all over the world,available at http://www.ncbi.nlm.nih.gov/PubMed/.2The GENIA Corpus V3.0p consists of 2000 POS-taggedMEDLINE abstracts, V3.01 consists of the same 2000 abstractsannotated semantically with the GENIA ontology, available athttp://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.Form Times Typical patterninhibitor(s) 161 a/the ... inhibitor ...inhibition 167 ... inhibition of ...inhibitory 61 ... inhibitory effect/factor ...inhibiting 24 ... in inhibiting ...inhibited 119 ... inhibited ...be inhibited 73 ... be inhibited by ...inhibit 63 ... inhibit ...inhibits 57 ... inhibits ...Table 1: Typical patterns of syntactic structures con-taining the verb stem inhibit and their occurrencesin a test corpus with 800 MEDLINE abstracts.above.
Section 2 introduces a set of language pro-cessing tools used in the experiment.
Detailed de-scription of the approach for the extraction of properMWVs is presented in section 3.
The evaluation ofthe result and the aspects that have influence on theresult are discussed in section 4, as well as our fu-ture works.
Finally, in the appendix, we list a num-ber of MWVs that have been extracted by our ap-proach.2 Tokeniser, POS Tagger and ChunkerOur experiment in this paper is carried out mainlyon chunk sequences, therefore the following pro-cessing components are necessary:?
Tokeniser: Following the whitespace-delimited tokenisation discipline, the tokeniserdetermines the segmentation of the non-lexicalentries such as tokens with non-alphabet char-acters or abbreviations.
After tokenisation, thesentence boundaries are determined as well.?
POS tagger: The maximum entropy POS tag-ger developed by Ratnaparkhi (Ratnaparkhi,1996) and the rule-based POS tagger devel-oped by Brill (Brill, 1994) are trained with1200 abstracts extracted from the GENIA cor-pus, which achieve accuracies of 97.97% and98.06% respectively, when testing on the rest800 abstract of the GENIA corpus.
Since ourtest corpus is directly extracted from the POStagged GENIA Corpus V3.0p, we do not haveto apply the process of tokenisation and POStagging.?
Chunker: In this experiment, unlike the tra-ditional statistical method for collocation ex-traction, where sentences are treated as wordsequences (Manning and Schu?tze, 2002), ashallow chunking process is first carried out.Then, sentences in our test corpus are treatedas chunk sequences.Up to now, the chunker consists of two parts,both utilize WordNet 1.7.13 (Fellbaum, 1999)as the lexical resource for the lemmatization,i.e., as the verb and noun stemmer.?
Verb chunker, which extracts the small-est verb chunks (not including the MWVstructures) with the additional syntacticinformation such as number (3rd singu-lar present), voice (active/passive), andnegation.
Since most of the scientificabstracts are written in present or pasttense, the temporal information is not ex-tracted especially.
The verb chunker re-turns the common verb lemma of a verb,with the additional syntactic informationmentioned above.
For example, givenan input verb chunk has not been estab-lished, it returns [establish, singular, pas-sive, negation].?
Noun chunker, which determines thenoun chunk boundaries, negation, num-ber (singular/plural), as well as someinner dependencies of the noun chunkscontaining substructure(s).
For example,a noun chunk like [ [the retinoic acid-synthesizing enzyme] [aldehyde dehydro-genase 1] ] is actually an apposition struc-ture.4 In this experiment, the singularstem of a plural noun token is not returnedin order to avoid missing of necessary in-formation.
For example, although bothtake place and take places can map to thesame base structure take place, they mustbe treated separately.3 MWV Extraction3.1 Analysis of MWVs in the CorpusThe following experiment is carried out on a testcorpus consisting of 1800 abstracts from the GE-NIA Corpus V3.0p, with 14955 sentences and40.84K tokens (abstract titles are not included).In general, the methodologies for the extrac-tion of multiword expressions (MWEs, includingMWVs) can be classified into syntactic, statisticaland hybrid syntactic-statistical (Dias, 2003).
Purelysyntactic processing of MWEs requires specific lin-guistic knowledge across the different domains of alanguage, such as a semantic ontology (Piao et al,3http://www.cogsci.princeton.edu/?wn/index.shtml4presented in pairs of matching parentheses.2003).
Purely statistical processing overgeneratesthe MWE candidates (Gae?l Dias, 2002), and is notsensitive enough to the MWE candidates with lowfrequencies (Piao et al, 2003).
It is practical insome cases for a hybrid syntactic-statistical systemto pre-define a set of MWE pattern rules, then usestatistical techniques to filter proper candidates.
Butit lacks the flexibility to obtain a comprehensivecoverage of possible MWE candidates, especiallywhen a MWV is non-contiguous in our case.
Inaddition, it also suffers from the problem of over-generation, if the pre-defined syntactic pattern oc-curs rarely in the corpus.
Sag et al (2002) indicatedthat it is very necessary to find the balance betweenthe two methods in hybrid systems.
This point ofview is taken into account in our approach.3.2 Extraction of Contiguous MWVCandidatesA number of works about MWV extraction fromcorpora are based on the output of a POS tag-ger and a chunker (Baldwin and Villavicencio,2002; Bannard et al, 2003), or the output of aparser (McCarthy et al, 2003).
These works ex-tracted mainly the verb+particle structures.
Similarto those works, the MWV extraction in our exper-iment is also based on the chunking output.
But,since MWVs has various POS tag patterns, it is notpractical to assign each pattern an according syntac-tic rule.
Therefore a variation of finite state automa-ton is considered in our approach for the extractionof MWVs.
Let ?
denote this automaton.Definition ?
= {S, I,O, F,G, START},where:?
S is the set of automaton states,S = {nextOut, stop, nextIn, head, halt};?
I is the input set, namely the chunks in both POStag sequence and lexical sequence;?
O is the output set, namely the MWV candidates,O = {oi|oi is a successful MWV candidate.};?
F is the set of output controlling functions;?
G is the set of automaton state transition functions;?
START is the initial state of the automaton,START = head.Controlling functions in F define operations forthe output.
Controlling functions in G define statetransitions of the automaton with respect to the fea-tures from both POS tags and lexical entries of aninput chunk.
An example is given as following.?
Example sentence: The 3?NF-E2/AP1 motifis able to exert both positive and negative reg-ulatory effects on the zeta 2-globin promoteractivity in K562 cells.?
Chunk sequence of the example:Chunk Chunk tagThe 3?NF-E2/AP1 motif ENPbe EVPable ADJPto exert IVPboth positive and negative regulatory effects ENPSon INthe zeta 2-globin promoter activity ENPin INK562 cells ENPS.
SEPRwhere ADJP is an adjective phrase; ENP is asingular English noun phrase; ENPS is a plu-ral English noun phrase; EVP is a singular En-glish verb phrase; IN is a preposition; IVP isan infinitive verb phrase; and, SEPR is a sen-tence seperator.?
Extraction of Contiguous MWV Candi-dates: In the following table, the Input itemsare the combination of both lexical sequencesand the corresponding chunk tags, but onlychunk tags are presented in the table.
The out-put operation ?
means no operation.
For thisexample, it returns be able to as a MWV can-didate.Input State transition Output operation (oi)Initialization nextOut ?ENP nextOut ?EVP head oi = ?be?ADJP nextIn oi = ?be able?IVP stop oi = ?be able to?
(success)IVP head oi+1 = ?exert?ENPS stop oi+1 = ?exert?
(failure)IN nextOut ?ENP nextOut ?IN nextOut ?ENPS nextOut ?SEPR halt ?3.3 Extraction of Non-contiguous MWVCandidatesWhen a set of new controlling functions are given,the finite automaton mentioned above also extractsnon-contiguous MWV candidates.
We primarily fo-cuses on non-contiguous MWVs in form of verb +particle.
As the particles in verb + particle MWVsare often intransitive (Baldwin and Villavicencio,2002; McCarthy et al, 2003), which are differentfrom the transitive prepositions followed by a nounchunk, we use this feature and a nearest assumptionto extract non-contiguous MWV candidates.
In gen-eral, we assume that a non-contiguous MWV occursin a limited context window.5Because of the specific test corpus in our ex-periment, the non-contiguous MWV candidates ex-tracted in our experiment are a relative small sub-setof all the candidates.6 Most of them are not propercandidates.
We suppose that the genre of scientificabstracts is an important reason for that: there aremuch more specific nominal terms as well as spe-cific verbs (not MWVs) in scientific abstracts thanin everyday language.3.4 Solutions to Overgeneration of MWVCandidatesIt is not surprising that the finite automaton isalso sensitive to the low-frequent MWVs such as?take place?
(7 times in the test corpus) and ?shedlight on?
(4 times).
But several problems of over-generation7 are still found, which include:Case 1.
Example: [take place]1.1, [take placeat]1.2, [take place in]1.3.
In general, we assumethat the short structures are more reliable, espe-cially when the occurrences of the short structuresare much more frequent than the long structures.But in this example, all three phrases occur with thesame frequency in the test corpus, we still choosethe more reliable short structure, and add up all oc-currences of these structures.Case 2.
Example: [be able to]2.1, [be importantfor]2.2.
The structure [2.1] is a MWV, but the struc-ture [2.2], which has the same POS tag sequenceas [2.1], is actually not a MWV accepted by a lexi-con.
In this case, the verb head is usually one of themost frequent verbs such as be, take, go, and etc.
Ina previous experiment, we computed the logarithmlikelihood ratio of the two mutual hypotheses8 forthe contiguous MWV candidates extracted from thetest corpus, in order to find the reliability of suchcollocations.
But we got some unexpected results,like be important in be important for was a more re-liable structure than shed light in shed light on.
It in-dicates that this score is still not sensitive enough toextremely sparse samples.
In addition, this method5Similar assumption or experiment data can be also foundin (Baldwin and Villavicencio, 2002; Bannard et al, 2003;Dias, 2003).641 non-contiguous MWV candidates extracted in this ex-periment have the occurrences over ten, but there are 151 con-tiguous MWV candidates whose occurrences are over ten.7MWV candidates share the same lexical substring/string orPOS-tag subsequence/sequence.8Take the bigram (w1w2) model as an example, hypoth-esis H1: P (w2|w1) = p = P (w2|?w1), hypothesis H2:P (w2|w1) = p1 6= p2 = P (w2|?w1).
The likelihood ra-tio ?2log?
= ?2logL(H1)/L(H2) is more appropriate than?2 test, since some MWVs are quite sparse in our test cor-pus (Manning and Schu?tze, 2002).is not suitable for non-contiguous MWV candidates.In our experiment, we suppose that it is neitherthe verb head nor the preposition that determines thereliability of such MWV structures.
Therefore weonly focus on the distribution of the rest words (ableor important) in the corpus.
Such words, togetherwith the verb head in a MWV pattern like verb +particle, in the following parts of paper, are giventhe name MWV head.
For instance, we find that83% occurrences of able are in the MWV candidatestructure be able to, but only 8.4% occurrences ofimportant are in be important for.
Hence the struc-ture [2.1] is a much better candidate of MWV than[2.2].
By this means, the low-frequency candidateshed light on can also get a better rank than the rel-ative high-frequency candidate be important for.Case 3.
Example: [take place]3.1, [bind DNA]3.2.
[3.1] is a MWV, but [3.2], which also has thesame POS tag sequence as [3.1], is not a MWV.
Inour case, a set of domain-specific terms are avail-able from the NE-annotated GENIA corpus V3.01.Since we suppose that the MWVs contain only gen-eral words, the word like DNA in this corpus canbe found in the specific word list, then this structurecan be excluded from the list of MWV candidates.However, this method induces also problems.
Forexample, give rise to is a MWV, but rise is also inthe specific word list of this corpus.
In this case,the specific word list could be selected according tosome criteria (e.g., frequencies in the list of specificterms), so that a much more comprehensive list ofMWV candidates can be produced without losingthe generality.Case 4.
Example: [be able to]4.1, [beunaffected]4.2.
The POS tag pattern of [4.2] is asubstring of [4.1], i.e., EVP + ADJP, but [4.2] isobviously not a proper MWV candidate.
We as-sume that a proper MWV should have closed leftand right boundaries, which means, the left bound-ary of a MWV candidate should be a verb, andthe right boundary should be a preposition (includ-ing to) or a noun.
Therefore such patterns withopen right boundaries like [4.2] in this example aredeleted from the candidate list.Case 5.
Example: [be associated with]5.1 and[associate(d) with]5.2, [be used to]5.3 and [usedto]5.4.
The pair of [5.1] and [5.2] have no seman-tic differences between the past and present tense,as well as no semantic transition of the MWV itselfbetween the passive and active voice.
That means,they can all map to the MWV base form associatewith.
But there are semantic differences betweenthe pair of [5.3] and [5.4].
The past tense phraseused to is a fixed idiomatic verb phrase (e.g., Heused to smoke a pipe.
), like the present tense phraseaccording to, generally they do not occur in formsof other tense.
There is no semantic relationship be-tween [5.3] and [5.4] in some cases, although thebase forms of both structures are the same, i.e., useto.
In this experiment, we do not consider the latercase.
All MWV candidates have the mapping totheir base form, but the information about the pas-sive and active voice are reserved, so that some can-didates in passive forms (e.g., be inhibited by) canbe excluded.3.5 Evaluation of the Reliability of the MWVCandidatesAfter the above processing on the set of MWV can-didates extracted by the finite automaton, the fol-lowing task is to examine the reliability of the can-didates, especially for those candidates that sharethe same MWV head.
To solve this problem, sta-tistical measurement is necessary.
First, the fre-quencies of the MWV candidates in the test cor-pus are taken into account.
For instance, result inis the most frequent MWV candidate, which hasmore than 320 occurrences, it is obviously a properMWV candidate.
From Figure 1, we can find that alarge number of MWV candidates occur with rel-ative low frequencies ranged from about 1 to 10.In order to avoid accidental errors during the pro-cess (mainly the wrong assignment of POS tags),the MWV candidates with the lowest frequenciesfrom 1 to 4 are out of consideration.
Second, thedistribution of the MWV head in the MWV candi-dates is considered.
We assume that a verb head ofa certain MWV has the inertia (big probability) toconstruct other MWVs than to be isolated.
For in-stance, 89% of occurrences of the verb head resultare in result in, only 8.5% belongs to result from.Although result from is not a high frequent MWVcandidate, it is still a proper one.
Third, the con-tiguous and non-contiguous MWV candidates aretreated as the same structure, so that such structuresare not ignored by the statistical measurement.
Thatmeans, if a MWV candidate occurs in both con-tiguous and non-contiguous forms, then their occur-rences are added up.
According to our experimentresults, the occurrences of non-contiguous MWVcandidates are much less than the contiguous can-didates, which leads to a very small number of non-contiguous MWVs successfully extracted from ourtest corpus.To evaluate the reliability of a certain MWV can-didate c in the candidate set C, following definitionsare given.?
head(c), the MWV head of c, c ?
C;Figure 1: The distribution of frequencies (frequen-cies between 5 and 80).?
f(x), the frequency of x, x can be c, or head(c);?
F (c), the sum of occurrences of all candidates inC, which share the same MWV head with c;?
E(c), the evaluation score of c,E(c) = c1f(c) + c2f(c)f(head(c))+ c3F (c)f(head(c))(1)where c1, c2 and c3 (c1, c2, c3 ?
0) are coefficients;?
t, the threshold of score evaluation,t = a?
minE(c) + b (2)where c ?
C, a ?
1, b ?
0.If E(c) ?
t, c is a proper candidate.The flowchart of the process to filter properMWV candidates is shown in Figure 2.Contiguous MWVcandidatesNon-contiguous MWVcandidatesControls of OvergenerationDomain-specifictermsVerb chunkerfor commonverb lemmataExamine thecompatibilityof long andshortcandidatesExaminecandidates withpassive andactive  formsEvaluation of reliabilityIf: E(c) > tOutput: reliable MWV candidate cFiltercandidateswith openboundaries))(()())(()()()( 321 cheadfcFccheadfcfccfccE ?Figure 2: The process to find out proper MWVs.In order to obtain the satisfying values of the co-efficients and the threshold, a manual sample is cre-ated, so that the values of the coefficients can betuned.
It is not feasible that we give all extractedMWV candidates a human evaluation, therefore wec1 c2 c3 t Precision Recall F?=10.003 0.5 8 2.27 45.16% 100% 0.6220.003 0.5 10 2.81 45.65% 100% 0.6270.003 0.5 12 2.81 45.65% 100% 0.6270.003 1 10 2.88 45.65% 100% 0.6270.01 0.5 10 2.86 44.21% 100% 0.6130.1 0.5 10 3.49 41.58% 100% 0.587Table 2: Evaluation of the selection of proper MWVcandidates according to equation 1, when we set thethreshold t = 1?minE(c)+ 0, c ?
CP (baseline).chose the most frequent 33 candidates (f(c) ?
60),31 candidates with moderate frequencies (14 ?f(c) ?
19), and 95 candidates with low frequen-cies (6 ?
f(c) ?
7) as a manual sample set (CM ,|CM | = 159).
Those MWV candidates in the man-ual sample set are looked up in a dictionary,9 if thereis such a MWV entry in the dictionary, then we as-sign a proper flag to the candidate.From the manual test sample set CM , we anno-tate 42 items as proper MWVs (CP , |CP | = 42).In the experiment, we set the coefficient c1 to bethe reciprocal of the largest occurrence of the MWVcandidates (c1 = 1/maxf(c), c ?
C), and t is setto be the linear function of the smallest score ofthe MWV in CP by the reliability evaluation, e.g.,t = minE(c), c ?
CP .
We use the scores of recall(R), precision(P ), and F-measure (F?=1) to evalu-ate our result.
In the following equations, X denotesthe set of candidates in CP , whose scores of relia-bility evaluation are greater than t, i.e., X = {c|c?
CP and E(c) ?
t}; Y denotes the set of candi-dates in CM , whose scores of reliability evaluationare greater than t, i.e., Y = {c|c ?
CM and E(c) ?t}.R = |X|/|Cp|,P = |X|/|Y |,F?=1 = 2PR/(P + R).4 Result, Discussion, and Future WorksThe result in Table 2 indicates that it is neitherthe frequency of occurrences of a MWV candidate(c1 = 0.003), nor the proportion of a MWV candi-date to its head word (c2 = 0.5), especially the verbhead, but the inertia of a verb to construct MWVsthat determines a proper MWV candidate (c3 = 10).The result strongly supports this assumption.We also found that the initiation of the value of twas very important.
In Table 2, the minimum value9Since WordNet is lacking in MWV entries, we used theOxford advanced learner?s dictionary of current English (En-cyclopedic version, 1992), and the online English-German dic-tionary LEO additionally, available at http://dict.leo.org/.of E(c) in CP was set to be the baseline of all testdata.
But we found that if the value of t was prop-erly increased (according to equation 2), althoughthe precision was therefore reduced, the F-measurewas improved remarkably.
Figure 3 shows howthe changes of value t effect the result, given thesame values of the coefficients in equation 1, thatc1 = 0.003, c2 = 0.5, and c3 = 10.
We got a muchbetter F-measure when we set a = 2.3, b = 0.1 (orb = 0.2), so that F?=1 = 0.753, if compared to thedata in Table 2, where a = 1, b = 0, F?=1 = 0.627.The reason is that some MWV candidates in CP ,like use to and carry out, have the MWV heads thatseem not to follow our assumption.
Such verbs (use,carry, including be, and etc.)
are often the mostfrequent verbs both in specific and general Englishlanguage.
Thus the syntactic and semantic combi-nations of such verbs and other words are quite rich,which led to a relative low score of E(c) in our ex-periment.
Compared to recent other related works,we found that (Baldwin and Villavicencio, 2002)presented a F-measure of 0.896 by testing on WSJ.But they focused on the single prepositional particlesituation, whereas our approach has the special in-terest in multiple and non-preposition particle cases.Moreover, they used quite a lot of syntactic tech-niques for more precise extraction of verb-particleconstructions (not verb-preposition constructions),which is not the case in ours.Figure 3: Effect of different t values on P , R andF?=1, given c1 = 0.003, c2 = 0.5, and c3 = 10.In addition, several other aspects also have neg-ative effects on the result.
First, the sublanguageis anyway specific compared with the general lan-guage, therefore some MWV candidates were hardto give an evaluation.
For instance, transfectinto/with can be found in neither dictionaries weused in this experiment, then it is hard to give themhuman evaluation.
Second, the POS tag errors dur-ing the processing had also a negative effect.
E.g.,in MWV candidate be related to, related was POStagged as an adjective, which led to a reduction ofthe value of E(relate to), since the MWV head ofthis inflectional structure was set to be the adjectiverelated but not the root of the verb relate.
Third,the language resources used in our experiment pro-vided sometimes not the information we needed.For instance, WordNet was lacking in some specificlexical entries of verbs such as synergize, pretreat,and etc.
Hence the distribution of their inflectionaland derivational forms, such as synergizes and pre-treated, could not be analyzed correctly.Our following work is to combine this work withthe domain-specific single verbs determined in thecorpus (Xiao and Ro?sner, 2004a), in order to geta comprehensive understanding of domain-specificverbs.
And, it will also be investigated if moredomain specific resources (e.g., UMLS10 specialistlexicon, etc.
), as well as adaptation of general lan-guage resources (e.g., WordNet, etc.)
to this specificdomain can improve the evaluation in equation 1or not.
Another future work is to examine the dis-tribution of the inflectional and derivational formsof MWVs for both MWV candidate evaluation andother IE tasks.5 Appendix: Some extracted MWVcandidates, ordered by scores of E(c).Note: what in the parentheses before each candidate isthe occurrences.
If a MWV is annotated with a PAStag, it means that this MWV is often used in passiveform or as past participle phrase in this test corpus.The complete list is available at: http://wwwai.cs.uni-magdeburg.de/Members/xiao/mwvsAppendixTable.pdfNo.
MWV candidate No.
MWV candidate1 (8) be subject to 2 (5) subjectPAS to3 (7) give rise to 4 (7) take place5 (325) result in 6 (271) lead to7 (293) associatePAS with 8 (89) fail toReferencesTimothy Baldwin and Aline Villavicencio.
2002.Extracting the unextractable: A case study onverb-particles.
In Proceedings of CoNLL-2002,pages 98?104.
Taipei, Taiwan.Colin Bannard, Timothy Baldwin, and Alex Las-carides.
2003.
A statistical approach to the se-mantics of verb-particles.
In Proceedings of theACL 2003 Workshop on Multiword Expressions:Analysis, Acquisition and Treatment, pages 65?72.Eric Brill.
1994.
Some advances in rule-based partof speech tagging.
In Proceedings of the Twelfth10Unified Medical Language System (UMLS), seehttp://www.nlm.nih.gov/research/umls/.National Conference on Artificial Intelligence,pages vol(1):722?727.Gae?l Dias.
2003.
Multiword unit hybrid extrac-tion.
In Proceedings of the ACL 2003 Workshopon Multiword Expressions: Analysis, Acquisitionand Treatment, pages 41?48.Christiane Fellbaum, editor.
1999.
WordNet: AnElectronic Lexical Database.
The MIT Press.Cambridge.
Massachusetts.Christopher D. Manning and Hinrich Schu?tze.2002.
Foundations of Statistical Natural Lan-guage Processing.
MIT Press.Diana McCarthy, Bill Keller, and John Carroll.2003.
Detecting a continuum of compositionalityin phrasal verbs.
In Proceedings of the ACL 2003Workshop on Multiword Expressions: Analysis,Acquisition and Treatment, pages 73?80.T.
Ono, H. Hishigaki, A. Tanigami, and T. Tak-agi.
2001.
Automated extraction of informationon protein-protein interactions from the biologi-cal literature.
Bioinformatics, pages 17(2):155?161.Scott S. L. Piao, Paul Rayson, Dawn Archer, An-drew Wilson, and Tony McEnery.
2003.
Extract-ing multiword expressions with a semantic tag-ger.
In Proceedings of the ACL 2003 Workshopon Multiword Expressions: Analysis, Acquisitionand Treatment, pages 49?56.Adwait Ratnaparkhi.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of the Em-pirical Methods in Natural Language ProcessingConference, University of Pennsylvania.Ivan A.
Sag, T. Baldwin, F. Bond, A. Copestake,and D. Flickinger.
2002.
Multiword expressions:A pain in the neck for nlp.
In Proceedings ofthe Third International Conference on IntelligentText Processing and Computational Linguistics(CICLING 2002), Mexico city, pages 1?15.J.
Thomas, D. Milward, C. Ouzounis, S. Pulman,and M. Carroll.
2000.
Automatic extraction ofprotein interactions from scientific abstracts.
InThe Pacific Symposium on Biocomputing?2000,Hawaii, pages 541?551.Chun Xiao and Dietmar Ro?sner.
2004a.
Determin-ing domain-specific verb vocabulary through cor-pora comparison and genre analysis.
submitted.Chun Xiao and Dietmar Ro?sner.
2004b.
Find-ing high-frequent synonyms of a domain-specificverb in english sub-language of medline abstractsusing wordnet.
In The Second Global WordnetConference, Brno, Czech Republic, pages 242?247.
