Squibs and DiscussionsMeasuring Word Alignment Quality for StatisticalMachine TranslationAlexander Fraser?University of Southern CaliforniaDaniel Marcu?University of Southern CaliforniaAutomatic word alignment plays a critical role in statistical machine translation.
Unfortu-nately, the relationship between alignment quality and statistical machine translation perform-ance has not been well understood.
In the recent literature, the alignment task has frequentlybeen decoupled from the translation task and assumptions have been made about measuringalignment quality for machine translation which, it turns out, are not justified.
In particular,none of the tens of papers published over the last five years has shown that significant decreasesin alignment error rate (AER) result in significant increases in translation performance.
Thispaper explains this state of affairs and presents steps towards measuring alignment quality in away which is predictive of statistical machine translation performance.1.
IntroductionAutomatic word alignment (Brown et al 1993) is a vital component of all statisticalmachine translation (SMT) approaches.
There were a number of research papers pre-sented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and soforth, outlining techniques for attempting to increase word alignment quality.
Despitethis high level of interest, none of these techniques has been shown to result in a largegain in translation performance as measured by BLEU (Papineni et al 2001) or anyother metric.
We find this lack of correlation between previous word alignment qualitymetrics and BLEU counterintuitive, because we and other researchers have measuredthis correlation in the context of building SMT systems that have benefited from usingthe BLEU metric in improving performance in open evaluations such as the NISTevaluations.1We confirm experimentally that previous metrics do not predict BLEU well anddevelop a methodology for measuring alignment quality that is predictive of BLEU.
We?
USC/ISI - Natural Language Group, 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292-6601.E-mail: fraser@isi.edu, marcu@isi.edu.1 Because in our experiments we use BLEU to compare the performance of systems built using a commonframework where the only difference is the word alignment, we make no claims about the utility of BLEUfor measuring translation quality in absolute terms, nor its utility for comparing two completely differentMT systems.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 3also show that alignment error rate (AER) is not correctly derived from F-Measure andis therefore unlikely to be useful as a metric.2.
Experimental Methodology2.1 DataTo build an SMT system we require a bitext and a word alignment of that bitext, as wellas language models built from target language data.
In all of our experiments, we willhold the bitext and target language resources constant, and only vary how we constructthe word alignment.The gold standard word alignment sets we use have been manually annotatedusing links between words showing translational correspondence.
Links which mustbe present in a hypothesized alignment are called Sure links.
Some of the alignmentsets also have links which are not Sure links but are Possible links (Och and Ney 2003).Possible links which are not Sure2 may be present but need not be present.We evaluate the translation performance of SMT systems by translating a held-out translation test set and measuring the BLEU score of our hypothesized translationsagainst one or more reference translations.
We also have an additional held-out transla-tion set, the development set, which is employed by the MT system to train the weightsof its log-linear model to maximize BLEU (Och 2003).
We work with data sets for threedifferent language pairs, examining French to English, Arabic to English, and Romanianto English translation tasks.The training data for the French/English data set is taken from the LDC CanadianHansard data set, from which the word aligned data (presented in Och and Ney 2003)was also taken.
The English side of the bitext is 67.4 million words.
We used a separateCanadian Hansard data set (released by ISI) as the source of the translation test set anddevelopment set.
We evaluate two different tasks using this data, a medium task where1/8 of the data (8.4 million English words) is used as the fixed bitext, and a large taskwhere all of the data is used as the fixed bitext.
The 484 sentences in the gold standardword alignments have 4,376 Sure links and 19,222 Possible links.The Arabic/English training corpus is the data used for the NIST 2004 machinetranslation evaluation.3 The English side of the bitext is 99.3 million words.
The trans-lation development set is the ?NIST 2002 Dry Run,?
and the test set is the ?NIST 2003evaluation set.?
We have annotated gold standard alignments for 100 parallel sentencesusing Sure links, following the Blinker guidelines (Melamed 1998), which call for Surelinks only (there were 2,154 Sure links).
Here we also examine a medium task using 1/8of the data (12.4 million English words) and a large task using all of the data.The Romanian/English training data was used for the tasks on Romanian/Englishalignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea,and Pedersen 2005).
We carefully removed two sections of news bitext to use as thetranslation development and test sets.
The English side of the training corpus is 964,000words.
The alignment set is the first 148 annotated sentences used for the 2003 task(there were 3,181 Sure links).2 Sure links are by definition also Possible.3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm.294Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation2.2 Measuring Translation Performance Changes Caused By AlignmentIn phrased-based SMT (Koehn, Och, and Marcu 2003) the knowledge sources whichvary with the word alignment are the phrase translation lexicon (which maps sourcephrases to target phrases using counts from the word alignment) and some of the wordlevel translation parameters (sometimes called lexical smoothing).
However, manyknowledge sources do not vary with the final word alignment, such as rescoring withIBM Model 1, n-gram language models, and the length penalty.
In our experiments,we use a state-of-the-art phrase-based system, similar to Koehn, Och, and Marcu.The weights of the different knowledge sources in the log-linear model used by oursystem are trained using Maximum BLEU (Och 2003), which we run for 25 iterationsindividually for each system.
Two language models are used, one built using the targetlanguage training data and the other built using additional news data.2.3 Generating Alignments of Varying QualityWe have observed in the past that generative models used for statistical word alignmentcreate alignments of increasing quality as they are exposed to more data.
The intuitionbehind this is simple; as more co-occurrences of source and target words are observed,the word alignments are better.
If we wish to increase the quality of a word alignment,we allow the alignment process access to extra data, which is used only during thealignment process and then removed.
If we wish to decrease the quality of a wordalignment, we divide the bitext into pieces and align the pieces independently of oneanother, finally concatenating the results together.To generate word alignments we use GIZA++ (Och and Ney 2003), which im-plements both the IBM Models of Brown et al (1993) and the HMM model (Vogel,Ney, and Tillmann 1996).
We use Model 1, HMM, and Model 4, in that order.
Theoutput of these models is an alignment of the bitext which projects one language toanother.
GIZA++ is run end-to-end twice.
In one case we project the source languageto the target language, and in the other we project the target language to the sourcelanguage.
The output of GIZA++ is then post-processed using the three ?symmetriza-tion heuristics?
described in Och and Ney (2003).
We evaluate our approaches usingthese heuristics because we would like to account for alignments generated in differentfashions.
These three heuristics were used as the baselines in virtually all recent workon automatic word alignment, and most of the best SMT systems use these techniquesas well.When applying the union symmetrization heuristic, we take the transitive closureof the bipartite graph created, which results in fully connected components indicatingtranslational correspondence.4 Each of the presented alignments are equivalent from atranslational correspondence perspective and the first two will be mapped to the third4 We have no need to do this for the ?refined?
and ?intersection?
heuristics, because they only producealignments in which the components are fully connected.295Computational Linguistics Volume 33, Number 3in order to ensure consistency between the number of links an alignment has and thetranslational equivalences licensed by that alignment.A B CDEA B CDEA B CDE3.
Word Alignment Quality Metrics3.1 Alignment Error Rate is Not a Useful MeasureWe begin our study of metrics for word alignment quality by testing AER (Och and Ney2003).
AER requires a gold standard manually annotated set of Sure links and Possiblelinks (referred to as S and P).
Given a hypothesized alignment consisting of the link setA, three measures are defined:Precision(A, P) =|P ?
A||A| (1)Recall(A, S) =|S ?
A||S| (2)AER(A, P, S) = 1 ?
|P ?
A|+ |S ?
A||S|+ |A| (3)In our graphs, we will present 1 ?
AER so that we have an accuracy measure.We created alignments of varying quality for the medium French/English trainingset.
We broke the data into separate pieces corresponding to 1/16, 1/8, 1/4, and 1/2of the original data to generate degraded alignments, and we used 2, 4, and 8 timesthe original data to generate enhanced alignments.
For the ?fractional?
alignments wereport the average AER of the pieces.5The graph in Figure 1 shows the correlation of 1 ?
AER with BLEU.
High correlationwould look like a line from the bottom left corner to the top right corner.
As can be seenby looking at the graph, there is low correlation between 1 ?
AER and the BLEU score.
Aconcise mathematical description of correlation is the coefficient of determination (r2),5 For example, for 1/16, we perform 16 pairs of alignments, each of which includes the full gold standardtext, and another 16 pairs of alignments without the gold standard text.
We then apply thesymmetrization heuristics to these pairs.
We use the symmetrized alignments including the text from thegold standard set to measure AER and we concatenate those not including the gold standard text to buildSMT systems for measuring BLEU.296Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine TranslationFigure 1French 1 ?
AER versus BLEU, r2 = 0.16.which is the square of the Pearson product-moment correlation coefficient (r).
Here,r2 = 0.16, which is low.The correlation is low because of a significant shortcoming in the mathematicalformulation of AER, which to our knowledge has not been previously reported.
Ochand Ney (2003) state that AER is derived from F-Measure.
But AER does not share avery important property of F-Measure, which is that unbalanced precision and recallare penalized, where S ?
P (i.e., when we make the Sure versus Possible distinction).6We will show this using an example.We first define the measure ?F-Measure with Sure and Possible?
using Och andNey?s Precision and Recall formulae together with the standard F-Measure formula(van Rijsbergen 1979).
In the F-Measure formula (4) there is a parameter ?
which setsthe trade-off between Precision and Recall.
When an equal trade-off is desired, ?
is setto 0.5.F-Measure with Sure and Possible(A, P, S, ?)
= 1?Precision(A,P) +(1??
)Recall(A,S)(4)We compare two hypothesized alignments where |A|, the number of hypothesizedalignment links, is the same, for instance, |A| = 100.
Let |S| = 100.
In the first case, let|P ?
A| = 50 and |S ?
A| = 50.
Precision is 0.50 and Recall is 0.50.
In the second case, let|P ?
A| = 75 and |S ?
A| = 25.
Precision is 0.75 and Recall is 0.25.The basic property of F-Measure, if we set ?
equal to 0.5, is that unbalancedprecision and recall should be penalized.
The first hypothesized alignment has anF-Measure with Sure and Possible score of 0.50, whereas the second has a worse score,0.375.However, if we substitute the relevant values into the formula for AER (Equa-tion (3)), we see that 1 ?
AER for both of the hypothesized alignments is 0.5.
ThereforeAER does not share the property of F-Measure (with ?
= 0.5) that unbalanced precisionand recall are penalized.
Because of this, it is possible to maximize AER by favoringprecision over recall, which can be done by simply guessing very few alignment links.Unfortunately, when S ?
P, this leads to strong biases, which makes AER not useful asa metric.6 Note that if S = P then AER reduces to balanced F-Measure.297Computational Linguistics Volume 33, Number 3Figure 2French F-Measure with Sure and Possible ?
= 0.5 versus BLEU, r2 = 0.20.Goutte, Yamada, and Gaussier (2004) previously observed that AER could be un-fairly optimized by using a bias toward precision which was unlikely to improve theusefulness of the alignments.
Possible problems with AER were discussed at WPT 2003and WPT 2005.Examining the graph in Figure 2, we see that F-Measure with Sure and Possible hassome predictive power for the data points generated using a single heuristic, but theoverall correlation is still low, r2 = 0.20.
We need a measure that predicts BLEU withouthaving a dependency on the way the alignments are generated.3.2 Balanced F-Measure is Better, but Still InadequateWe wondered whether the low correlation was caused by the Sure and Possible dis-tinction.
We reannotated the first 110 sentences of the French test set using the Blinkerguidelines (there were 2,292 Sure links).
We define F-Measure without the Sure versusPossible distinction (i.e., all links are Sure) in Equation (5), and set ?
= 0.5.
This measurehas been extensively used with other word alignment test sets.
Figure 3 shows theresults.
Correlation is higher: r2 = 0.67.F-Measure(A, S, ?)
= 1?Precision(A,S) +(1??
)Recall(A,S)(5)3.3 Varying the Trade-Off Between Precision and Recall Works WellWe then hypothesized that the trade-off between precision and recall is important.
Thisis controlled in both formulae by the constant ?.
We search ?
= 0.1, 0.2, ..., 0.9.
The bestresults are: ?
= 0.1 for the original annotation annotated with Sure and Possible (seeFigure 4), and ?
= 0.4 for the first 110 sentences as annotated by us (see Figure 5).7 Therelevant r2 scores were 0.80 and 0.85, respectively.
With a good ?
setting, we are able7 We also checked the first 110 sentences using the original annotation to ensure that the differencesobserved were not an effect of restricting our annotation to these sentences.298Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine TranslationFigure 3French F-Measure ?
= 0.5 versus BLEU, r2 = 0.67.Figure 4French F-Measure with Sure and Possible ?
= 0.1 versus BLEU, r2 = 0.80.Figure 5French F-Measure ?
= 0.4 versus BLEU, r2 = 0.85.to predict the machine translation results reasonably well.
For the original annotation,recall is very highly weighted, whereas for our annotation, recall is still more importantthan precision.8 Our results also suggest that better correlation will be achieved whenusing Sure-only annotation than with Sure and Possible annotation.8 ?
less than 0.5 weights recall higher, whereas ?
greater than 0.5 weights precision higher; see theF-Measure formulae.299Computational Linguistics Volume 33, Number 3Figure 6Arabic F-Measure ?
= 0.1 versus BLEU, r2 = 0.93.Figure 7Large French F-Measure ?
= 0.4 (110 sentences) versus BLEU, r2 = 0.64.We then tried the medium Arabic training set.
Results are shown in Figure 6, thebest setting of ?
= 0.1, and r2 = 0.93.
F-Measure is effective in predicting machinetranslation performance for this set.We also tried the larger tasks, where we can only decrease alignment quality, aswe have no additional data.
For the large French/English corpus the best results arewith ?
= 0.2 for the original annotation of 484 sentences and ?
= 0.4 for the newFigure 8Large Arabic F-Measure ?
= 0.1 (100 sentences) versus BLEU, r2 = 0.90.300Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translationannotation of 110 sentences with only Sure links (see Figure 7).
Relevant r2 scores were0.62 and 0.64, respectively.
Disappointingly, our measures are not able to fully explainMT performance for the large French/English task.For the large Arabic/English corpus, the results were better: the best correlationwas at ?
= 0.1, for which r2 = 0.90 (see Figure 8).
We can predict MT performance forthis set.
It is worth noting that the Arabic/English translation task and data set hasbeen tested in conjunction with our translation system over a long period, but theFrench/English translation task and data has not.
As a result, there may be hiddenfactors that affect the performance of our MT system, which only appear in conjunctionwith the large French/English task.One well-studied task on a smaller data set is the Romanian/English shared wordalignment task from the Workshop on Parallel Text at ACL 2005 (Martin, Mihalcea, andPedersen 2005).
We only decreased alignment quality and used 5 data points for eachsymmetrization heuristic due to the small bitext.
The best setting of ?
was ?
= 0.2, forwhich r2 = 0.94, showing that F-Measure is again effective in predicting BLEU.4.
ConclusionWe have presented an empirical study of the use of simple evaluation metrics based ongold standard alignment of a small number of sentences to predict machine translationperformance.
Based on our experiments we can now draw the following conclusions:1.
When S ?
P, AER does not share the important property of F-Measurethat unequal precision and recall are penalized, making it easy to obtaingood AER scores by simply guessing fewer alignment links.
As a resultAER is a misleading metric that should no longer be used.2.
Good correlation was obtained for the medium French and Arabic datasets, the large Arabic data set, and the small Romanian data set.
We haveexplained most of the effect of alignment quality on these sets, and if weare given the F-Measure of a hypothesized word alignment for the bitext,we can make a reasonable prediction as to what the resulting BLEU scorewill be.3.
We have only partially explained the effect of alignment quality on BLEUfor the large French data set, and further investigation is warranted.4.
We recommend using the Blinker guidelines as a starting point for newalignment annotation efforts, and that Sure-only annotation be used.
If alarger gold standard is available and was already annotated using the Sureversus Possible distinction, this is likely to have only slightly worse results.Although we have addressed measuring alignment quality for phrasal SMT, similarwork is now required to see how to measure alignment quality for other settings ofmachine translation and for other tasks.
For an evaluation campaign the organizersshould pick a specific task, such as improving phrasal SMT, and calculate an appropriate?
to be used.
Individual researchers working on the same phrasal SMT tasks as thosereported here (or on very similar tasks) could use the values of ?
we calculated.Our work invalidates some of the conclusions of recent alignment work whichpresented only evaluations based on metrics like AER or balanced F-Measure, andexplains the lack of correlation in the few works which presented both such a metric301Computational Linguistics Volume 33, Number 3and final MT results.
A good example of the former are our own results (Fraser andMarcu 2005).
The work presented there had the highest balanced F-Measure scores forthe Romanian/English WPT05 shared task, but based on the findings here it is possiblethat a different algorithm tuned for the correct criterion would have had better MTperformance.
Other work includes many papers working on alignment models wherewords are allowed to participate in a maximum of one link.
These models generallyhave higher precision and lower recall than IBM Model 4 symmetrized using the ?Re-fined?
or ?Union?
heuristics.
Recall that in Section 3.1 we showed that AER is broken ina way that favors precision.
It is therefore likely that the results reported in these papersare affected by the AER bias and that the corresponding improvements in AER score donot correlate with increases in phrasal SMT performance.We suggest comparing alignment algorithms by measuring performance in anidentified final task such as machine translation.
F-Measure with an appropriate settingof ?
will be useful during the development process of new alignment models, or as amaximization criterion for discriminative training of alignment models (Cherry and Lin2003; Ayan, Dorr, and Monz 2005; Ittycheriah and Roukos 2005; Liu, Liu, and Lin 2005;Fraser and Marcu 2006; Lacoste-Julien et al 2006; Moore, Yih, and Bode 2006).AcknowledgmentsThis work was supported by DARPA-ITOgrant NN66001-00-1-9814, NSF grantIIS-0326276, and the DARPA GALE Program.We thank USC High PerformanceComputing & Communications.ReferencesAyan, Necip Fazil, Bonnie J. Dorr, andChristof Monz.
2005.
Neuralign:Combining word alignments using neuralnetworks.
In Proceedings of HLT-EMNLP,pages 65?72, Vancouver, Canada.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and R. L. Mercer.1993.
The mathematics of statisticalmachine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Cherry, Colin and Dekang Lin.
2003.
Aprobability model to improve wordalignment.
In Proceedings of ACL,pages 88?95, Sapporo, Japan.Fraser, Alexander and Daniel Marcu.
2005.Isi?s participation in the Romanian-Englishalignment task.
In Proceedings of the ACLWorkshop on Building and Using ParallelTexts, pages 91?94, Ann Arbor, MI.Fraser, Alexander and Daniel Marcu.
2006.Semi-supervised training for wordalignment.
In Proceedings of COLING-ACL,pages 769?776, Sydney, Australia.Goutte, Cyril, Kenji Yamada, and EricGaussier.
2004.
Aligning words usingmatrix factorisation.
In Proceedings of ACL,pages 502?509, Barcelona, Spain.Ittycheriah, Abraham and Salim Roukos.2005.
A maximum entropy word alignerfor Arabic-English machine translation.
InProceedings of HLT-EMNLP, pages 89?96,Vancouver, Canada.Koehn, Philipp, Franz J. Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of HLT-NAACL,pages 127?133, Edmonton, Canada.Lacoste-Julien, Simon, Dan Klein,Ben Taskar, and Michael Jordan.2006.
Word alignment via quadraticassignment.
In Proceedingsof HLT-NAACL, pages 112?119,New York, NY.Liu, Yang, Qun Liu, and Shouxun Lin.2005.
Log-linear models for wordalignment.
In Proceedings of ACL,pages 459?466, Ann Arbor, MI.Martin, Joel, Rada Mihalcea, andTed Pedersen.
2005.
Wordalignment for languages withscarce resources.
In Proceedings of theACL Workshop on Building andUsing Parallel Texts, pages 65?74,Ann Arbor, MI.Melamed, I. Dan.
1998.
Manual annotationof translational equivalence: TheBlinker project.
Technical Report 98?07,Institute for Research in CognitiveScience, University of Pennsylvania,Philadelphia, PA.Mihalcea, Rada and Ted Pederson.
2003.An evaluation exercise for wordalignment.
In Proceedings of theHLT-NAACL Workshop on Buildingand Using Parallel Texts, pages 1?10,Edmonton, Canada.302Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine TranslationMoore, Robert C., Wen-Tau Yih, and AndreasBode.
2006.
Improved discriminativebilingual word alignment.
In Proceedingsof COLING-ACL, pages 513?520,Sydney, Australia.Och, Franz J.
2003.
Minimum error ratetraining in statistical machine translation.In Proceedings of ACL, pages 160?167,Sapporo, Japan.Och, Franz J. and Hermann Ney.
2003.A systematic comparison of variousstatistical alignment models.Computational Linguistics, 29(1):19?51.Papineni, Kishore A., Salim Roukos,Todd Ward, and Wei-Jing Zhu.2001.
BLEU: A method forautomatic evaluation of machinetranslation.
Technical ReportRC22176 (W0109-022), IBM ResearchDivision, Thomas J. WatsonResearch Center, YorktownHeights, NY.van Rijsbergen, Keith.
1979.
InformationRetrieval.
Butterworth-Heinemann,Newton, MA.Vogel, Stephan, Hermann Ney, andChristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.In Proceedings of COLING, pages 836?841,Copenhagen, Denmark.303
