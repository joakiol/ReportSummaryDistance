FACTORIZAT ION OF LANGUAGE CONSTRAINTS IN SPEECH RECOGNIT IONRoberto Pieraccini and Chin-Hui LeeSpeech Research DepartmentAT&T Bell LaboratoriesMurray Hill, NJ 07974, USAABSTRACTIntegration of language constraints into alarge vocabulary speech recognition systemoften leads to prohibitive complexity.
Wepropose to factor the constraints into twocomponents.
The first is characterized by acovering grammar which is small and easilyintegrated into existing speech recognizers.
Therecognized string is then decoded by means of anefficient language post-processor in which thefull set of constraints is imposed to correctpossible errors introduced by the speechrecognizer.1.
IntroductionIn the past, speech recognition has mostlybeen applied to small domain tasks in whichlanguage constraints can be characterized byregular grammars.
All the knowledge sourcesrequired to perform speech recognition andunderstanding, including acoustic, phonetic,lexical, syntactic and semantic levels ofknowledge, are often encoded in an integratedmanner using a finite state network (FSN)representation.
Speech recognition is thenperformed by finding the most likely paththrough the FSN so that the acoustic distancebetween the input utterance and the recognizedstring decoded from the most likely path isminimized.
Such a procedure is also known asmaximum likelihood ecoding, and such systemsare referred to as integrated systems.
Integratedsystems can generally achieve high accuracymainly due to the fact that the decisions aredelayed until enough information, derived fromthe knowledge sources, is available to thedecoder.
For example, in an integrated systemthere is no explicit segmentation i to phoneticunits or words during the decoding process.
Allthe segmentation hypotheses consistent with theintroduced constraints are carried on until thefinal decision is made in order to maximize aglobal function.
An example of an integratedsystem was HARPY (Lowerre, 1980) whichintegrated multiple levels of knowledge into asingle FSN.
This produced relatively highperformance for the time, but at the cost ofmultiplying out constraints in a manner thatexpanded the grammar beyond reasonablebounds for even moderately complex domains,and may not scale up to more complex tasks.Other examples of integrated systems may befound in Baker (1975) and Levinson (1980).On the other hand modular systems clearlyseparate the knowledge sources.
Different fromintegrated systems, a modular system usuallymake an explicit use of the constraints at eachlevel of knowledge for making hard decisions.For instance, in modular systems there is anexplicit segmentation i to phones during anearly stage of the decoding, generally followedby lexical access, and by syntactic/semanticparsing.
While a modular system, like forinstance HWIM (Woods, 1976) or HEARSAY-II(Reddy, 1977) may be the only solution forextremely large tasks when the size of thevocabulary is on the order of 10,000 words ormore (Levinson, 1988), it generally achieveslower performance than an integrated system in arestricted omain task (Levinson, 1989).
Thedegradation i performance is mainly due to theway errors propagate through the system.
It iswidely agreed that it is dangerous to make a longseries of hard decisions.
The system cannotrecover from an error at any point along thechain.
One would want to avoid this chain-architecture and look for an architecture whichwould enable modules to compensate for eachother.
Integrated approaches have thiscompensation capability, but at the cost ofmultiplying the size of the grammar in such away that the computation becomes prohibitivefor the recognizer.
A solution to the problem isto factorize the constraints so that the size of the299grammar, used for maximum likelihooddecoding, is kept within reasonable boundswithout a loss in the performance.
In this paperwe propose an approach in which speechrecognition is still performed in an integratedfashion using a covering rammar with a smallerFSN representation.
The decoded string ofwords is used as input to a second module inwhich the complete set of task constraints isimposed to correct possible rrors introduced bythe speech recognition module.2.
Syntax Driven Continuous SpeechRecognitionThe general trend in large vocabularycontinuous speech recognition research is that ofbuilding integrated systems (Huang, 1990;Murveit, 1990; Paul, 1990; Austin, 1990) inwhich all the relevant knowledge sources,namely acoustic, phonetic, lexical, syntactic, andsemantic, are integrated into a uniquerepresentation.
The speech signal, for thepurpose of speech recognition, is represented bya sequence of acoustic patterns each consistingof a set of measurements taken on a smallportion of signal (generally on the order of 10reset).
The speech recognition process is carriedout by searching for the best path that interpretsthe sequence of acoustic patterns, within anetwork that represents, in its more detailedstructure, all the possible sequences of acousticconfigurations.
The network, generally called adecoding network, is built in a hierarehical way.In current speech recognition systems, thesyntactic structure of the sentence is representedgenerally by a regular grammar that is typicallyimplemented asa finite state network (syntacticFSN).
The ares of the syntactic FSN representvocabulary items, that are again represented byFSN's (lexical FSN), whose arcs are phoneticunits.
Finally every phonetic unit is againrepresented by an FSN (phonetic FSN).
Thenodes of the phonetic FSN, often referred to asacoustic states, incorporate particular acousticmodels developed within a statistical frameworkknown as hidden Markov model (HMM).
1 The1.
The reader is referred to Rabiner (1989) for a tutorialintroduction f HMM.model pertaining to an acoustic state allowscomputation of a likelihood score, whichrepresents he goodness of acoustic match for theobservation of a given acoustic patterns.
Thedecoding network is obtained by representing theoverall syntactic FSN in terms of acoustic states.Therefore the recognition problem can bestated as follows.
Given a sequence of acousticpatterns, corresponding to an uttered sentence,find the sequence of acoustic states in thedecoding network that gives the highestlikelihood score when aligned with the inputsequence of acoustic patterns.
This problem canbe solved efficiently and effectively using adynamic programming search procedure.
Theresulting optimal path through the network givesthe optimal sequence of acoustic states, whichrepresents a sequence of phonetic units, andeventually the recognized string of words.Details about the speech recognition system werefer to in the paper can be found in Lee(1990/1).
The complexity of such an algorithmconsists of two factors.
The first is thecomplexity arising from the computation of thelikelihood scores for all the possible pairs ofacoustic state and acoustic pattern.
Given anutterance of fixed length the complexity is linearwith the number of distinct acoustic states.
Sincea finite set of phonetic units is used to representall the words of a language, the number ofpossible different acoustic states is limited by thenumber of distinct phonetic units.
Therefore thecomplexity of the local likelihood computationfactor does not depend either on the size of thevocabulary or on the complexity of the language.The second factor is the combinatorics orbookkeeping that is necessary for carrying outthe dynamic programming optimization.Although the complexity of this factor stronglydepends on the implementation f the searchalgorithm, it is generally true that the number ofoperations grows linearly with the number ofarcs in the decoding network.
As the overallnumber of arcs in the decoding network is alinear function of the number of ares in thesyntactic network, the complexity of thebookkeeping factor grows linearly with thenumber of ares in the FSN representation f thegrammar.300The syntactic FSN that represents a certaintask language may be very large if both the sizeof the vocabulary and the munber of syntacticconstraints are large.
Performing speechrecognition with a very large syntactic FSNresults in serious computational nd memoryproblems.
For example, in the DARPA resourcemanagement task (RMT) (Price, 1988) thevocabulary consists of 991 words and there are990 different basic sentence structures ( entencegeneration templates, as explained later).
Theoriginal structure of the language (RMTgrammar), which is given as a non-deterministicfinite state semantic grammar (Hendrix, 1978),contains 100,851 rules, 61,928 states and247,269 arcs.
A two step automatic optimizationprocedure (Brown, 1990) was used to compile(and minimize) the nondeterministic FSN into adeterministic FSN, resulting in a machine with3,355 null arcs, 29,757 non-null arcs, and 5832states.
Even with compilation, the grammar isstill too large for the speech recognizer to handlevery easily.
It could take up to an hour of cputime for the recognizer to process a single 5second sentence, running on a 300 Mflop Alliantsupercomputer (more that 700 times slower thanreal time).
However, if we use a simplercovering grammar, then recognition time is nolonger prohibitive (about 20 times real time).Admittedly, performance does degradesomewhat, but it is still satisfactory (Lee,1990/2) (e.g.
a 5% word error rate).
A simplergrammar, however, represents a superset of thedomain language, and results in the recognitionof word sequences that are outside the definedlanguage.
An example of a covering grammarsfor the RMT task is the so called word-pair(WP) grammar where, for each vocabulary worda list is given of all the words that may followthat word in a sentence.
Another coveringgrammar is the so called null grammar (NG), inwhich a word can follow any other word.
Theaverage word branching factor is about 60 in theWP grammar.
The constraints imposed by theWP grammar may be easily imposed in thedecoding phase in a rather inexpensiveprocedural way, keeping the size of the FSNvery small (10 nodes and 1016 arcs in ourimplementation (Lee, 1990/1) and allowing therecognizer to operate in a reasonable time (anaverage of 1 minute of CPU time per sentence)(Pieraccini, 1990).
The sequence of wordsobtained with the speech recognition procedureusing the WP or NG grammar is then used asinput to a second stage that we call the semanticdecoder.3.
Semantic DecodingThe RMT grammar is represented, accordingto a context free formalism, by a set of 990sentence generation templates of the form:Sj = ~ ai2 .
.
.a~,  (1)where a generic ~ may be either a terminalsymbol, hence a word belonging to the 991 wordvocabulary and identified by its orthographictranscription, or a non-terminal symbol(represented by sharp parentheses in the rest ofthe paper).
Two examples of sentencegeneration templates and the correspondingproduction of non-terminal symbols are given inTable 1 in which the symbol e corresponds totheempty string.A characteristic of the the RMT grammar isthat there are no reeursive productions of thekind:(,4) = a l  a2 - ' .
(A) .
.
.
a/v (2)For the purpose of semantic decoding, eachsentence template may then be represented asaFSN where the arcs correspond either tovocabulary words or to categories of vocabularywords.
A category is assigned to a vocabularyword whenever that vocabulary word is a uniqueelement in the tight hand side of a production.The category is then identified with the symbolused to represent the non-terminal on the le f thand side of the production.
For instance,following the example of Table 1, the wordsSHIPS, FRIGATES, CRUISERS, CARRIERS,SUBMARINES, SUBS, and VESSELS belong tothe category <SH/PS>, while the word LISTbelongs to the category <LIST>.
A special word,the null word, is included in the vocabulary andit is represented bythe symbol e.Some of the non-terminal symbols in a givensentence generation template are essential for therepresentation f the meaning of the sentence,while others just represent equivalent syntacticvariations with the same meaning.
For instance,301GIVE A LIST OF <OPTALL> <OPTTHE> <SHIPS><LIST> <OPTTHE> <THREATS><OPTALL> AlJ.<OPTTHE> THE<SHIPS><LIST>SHIPSFRIGATESCRUISERSCARRIERSSUBMARINESSUBSVESSELSSHOW <OPTME>GIVE <OFrME>LISTGET <Oil\]dE>FIND <OPTME>GIVE ME A LIST OFGET <OPTME> A LIST OF<THREATS> AI .gRTSTHREATS<OPTME> MEETABLE 1.
Examples of sentence generation templates and semantic categoriesthe correct detection by the recognizer of thewords uttered in place of the non-terminals<SHIPS> and <THREATS>, in the formerexamples, is essential for the execution of thecorrect action, while an error introduced at thelevel of  the nonterminals <OPTALL>,<OP'ITHE> and <LIST> does not change themeaning of the sentence, provided that thesentence generation template associated to theuttered sentence has been correctly identified.Therefore there are non-terminals associatedwith essential information for the execution ofthe action expressed by the sentence that we callsemantic variables.
An analysis of the 990sentence generation templates allowed to definea set of 69 semantic variables.The function of the semantic decoder is thatof finding the sentence generation template thatmost likely produced the uttered sentence andgive the correct values to its semantic variables.The sequence of words given by the recognizer,that is the input of the semantic decoder, mayhave errors like word substitutions, insertions ordeletions.
Hence the semantic decoder should beprovided with an error correction mechanism.With this assumptions, the problem of semanticdecoding may be solved by introducing adistance criterion between a string of words anda sentence template that reflects the nature of thepossible word errors.
We defined the distancebetween a string of words and a sentencegeneration templates as the minimumLevenshtein 2 distance between the string ofwords and all the string of words that can begenerated by the sentence generation template.The Levenshtein distance can be easilycomputed using a dynamic programmingprocedure.
Once the best matching template hasbeen found, a traceback procedure is executed torecover the modified sequence of words.3.1 Semantic FilterAfter the alignment procedure describedabove, a semantic heck may be performed onthe words that correspond to the non-terminals2.
The Levenshtein distance (Levenshtein, 1966) betweentwo strings is defined as the minimum number ofediting operations (substitutions, deletions, andinsertions) for transforming one string into the other.302associated with semantic variables in theselected template.
If the results of the check ispositive, namely the words assigned to thesemantic variables belong to the possible valuesthat those variables may have, we assume thatthe sentence has been correctly decoded, and theprocess tops.
In the case of a negative responsewe can perform an additional acoustic orphonetic verification, using the availableconstraints, in order to find which production,among those related to the considered non-terminal, is the one that more likely produced theacoustic pattern.
There are different ways ofcarrying out the verification.
In the currentimplementation we performed a phoneticverification rather than an acoustic one.
Therecognized sentence (i.e.
the sequence of wordsproduced by the recognizer) is transcribed interms of phonetic units according to thepronunciation dictionary used in speechdecoding.
The template selected uring semanticdecoding is also transformed into an FSN interms of phonetic units.
The transformation isobtained by expanding all the non-terminals intothe corresponding vocabulary words and eachword in terms of phonetic units.
Finally amatching between the string of phonesdescribing the recognized sentence and thephone-transcribed sentence template isperformed to find the most probable sequence ofwords among those represented by the templateitself (phonetic verification).
Again, thematching is performed in order to minimize theLevenshtein distance.
An example of thisverification procedure is shown in Table 2.The first line in the example of Table 2shows the sentence that was actually uttered bythe speaker.
The second line shows therecognized sentence.
The recognizer deleted theword WERE, substituted the word THERE for theword THE and the word EIGHT for the wordDATE.
The semantic decoder found that, amongthe 990 sentence generation templates, the oneshown in the third line of Table 2 is the one thatminimizes the criterion discussed in the previoussection.
There are three semantic variables inthis template, namely <NUMBER>, <SHIPS> and<YEAR>.
The backtracking procedure associatedto them the words DATE, SUBMARINES, andEIGHTY TWO respectively.
The semantic heckgives a false response for the variable<NUMBER>.
In fact there are no productions ofthe kind <NUMBER> := DATE.
Hence therecognized string is translated into its phoneticrepresentation.
This representation is alignedwith the phonetic representation f the templateand gives the string shown in the last line of thetable as the best interpretation.3.2 Acoustic VerificationA more sophisticated system was alsoexperimented allowing for acoustic verificationafter semantic postprocessing.For some uttered sentences it may happen thatmore than one template shows the very sameminimum Levenshtein distance from therecognized sentence.
This is due to the simplemetric that is used in computing the distancebetween a recognized string and a sentencetemplate.
For example, if the uttered sentence is:WHEN WILL THE PERSONNEL CASUALTYREPORT FROM THE YORKTOWN BERESOLVEDuuered WERE THERE MORE THAN EIGHT SUBMARINES EMPLOYED IN EIGHTY TWOrecognized THE MORE THAN DATE SUBMARINES EMPLOYED END EIGHTY TWO.template !WERE THERE MORE THAN <NUMBER> <SHIPS> EMPLOYED IN <YEAR>semanticvariable value check<NUMBER> DATE FALSE<SHIPS> SUBMARINES TRUE<YEAR> EIGHTY TWO TRuEphonetic dh aet m ao r t ay I ae n d d ey t s ah b max r iy n z ix m p i oy d eh n d ey dx iytwehn iycorrected WERE THERE MORE THAN EIGHT SUBMARINES EMPLOYED IN EIGHTY TWOTABLE 2.
An example of semantic postprocessing303and the recognized sentence is:WILL THE PERSONNEL CASUALTY REPORTTHE YORKTOWN BE RESOLVEDthere are two sentence templates that show aminimum Levenshtein distance of 2 (i.e.
twowords are deleted in both cases) from therecognized sentence, namely:1) <WHEN+LL> <OPTTHE> <C-AREA><CASREP> FOR <OFITHE> <SHIPNAME> BERESOLVED2) <WHEN+LL> <OPTTHE> <C-AREA><CASREP> FROM <OPTTHE> <SHIPNAME> BERESOLVED.In this case both the templates are used as inputto the acoustic verification system.
The finalanswer is the one that gives the highest acousticscore.
For computing the acoustic score, theselected templates are represented as a FSN interms of the same word HMMs that were used inthe speech recognizer.
This FSN is used forconstraining the search space of a speechrecognizer that runs on the original acousticrepresentation of the uttered sentence.4.
Experimental ResultsThe semantic postproeessor was tested usingthe speech recognizer arranged in differentaccuracy conditions.
Results are summarized inFigures 1 and 2.
Different word accuracies weresimulated by using various phonetic unit modelsand the two covering grammars (i.e.
NG andWP).
The experiments were performed on a setof 300 test sentences known as the February 89test set (Pallett.
1989) The word accuracy,defined as1-  insertions deletions'e substitutions x l00  (3)number of words utteredwas computed using a standard program thatprovides an alignment of the recognizedsentence with a reference string of words.
Fig.
1shows the word accuracy after the semanticpostprocessing versus the original word accuracyof the recognizer using the word pair grammar.With the worst recognizer, that gives a wordaccuracy of 61.3%, the effect of the semanticpostprocessing is to increase the word accuracyto 70.4%.
The best recognizer gives a wordaccuracy of 94.9% and, after the postprocessing,the corrected strings show a word accuracy of97.7%, corresponding to a 55% reduction in theword error rate.
Fig.
2 reports the semanticaccuracy versus the original sentence accuracy ofthe various recognizers.
Sentence accuracy iscomputed as the percent of correct sentences,namely the percent of sentences for which therecognized sequence of words corresponds theuttered sequence.
Semantic accuracy is thepercent of sentences for which both the sentencegeneration template and the values of thesemantic variables are correctly decoded, afterthe semantic postprocessing.
With the bestrecognizer the sentence accuracy is 70.7% whilethe semantic accuracy is 94.7%.10090-80-70-O j ""JOO ?1~ S S0 SSSSAtSSS0 sSSSS~ S SsS50 sI I I I50 60 70 80 9O 100Original Word AccueraeyFigure 1.
Word accuracy after semantic postprocess-ing10080--60--40--20--?
I m?
i I?
S S~SSSSSSSSSSSSSJSSSSSSI I I I20 40 60 80 100Original Sentence AccuracyFigure 2.
Semantic accuracy after semantic postpro-cessingWhen using acoustic verification instead ofsimple phonetic verification, as described in304section 3.2, better word and sentence accuracycan be obtained with the same test data.
Using aNG covering grammar, the final word accuracyis 97.7% and the sentence accuracy is 91.0%(instead of 92.3% and 67.0%, obtained usingphonetic verification).
With a WP coveringgrammar the word accuracy is 98.6% and thesentence accuracy is 92% (instead of 97.7% and86.3% with phonetic verification).
The smalldifference in the accuracy between the NG andthe WP case shows the rebusmess introducedinto the system by the semantic postprocessing,especially when acoustic verification ispeformed.5.
SummaryFor most speech recognition andunderstanding tasks, the syntactic and semanticknowledge for the task is often represented in anintegrated manner with a finite state network.However for more ambitious tasks, the FSNrepresentation can become so large thatperforming speech recognition using such anFSN becomes computationally prohibitive.
Oneway to circumvent this difficulty is to factor thelanguage constraints such that speech decodingis accomplished using a covering rammar witha smaller FSN representation and languagedecoding is accomplished by imposing thecomplete set of task constraints in a post-processing mode using multiple word and stringhypotheses generated from the speech decoder asinput.
When testing on the DARPA resourcemanagement task using the word-pair grammar,we found (Lee, 1990/2) that most of the worderrors involve short function words (60% of theerrors, e.g.
a, the, in) and confusions amongmorphological variants of the same lexeme (20%of the errors, e.g.
six vs. sixth).
These errors arenot easily resolved on the acoustic level,however they can easily be corrected with asimple set of syntactic and semantic rulesoperating in a post-processing mode.The language constraint factoring schemehas been shown efficient and effective.
For theDARPA RMT, we found that the proposedsemantic post-processor improves both the wordaccuracy and the semantic accuracy significantly.However in the current implementation, oacoustic information is used in disambiguatingwords; only the pronunciations of words areused to verify the values of the semanticvariables in cases when there is semanticambiguity in finding the best matching string.The performance can further be improved if theacoustic matching information used in therecognition process is incorporated into thelanguage decoding process.6.
AcknowledgementsThe authors gratefully acknowledge thehelpful advice and consultation provided by K.-Y.
Su and K. Church.
The authors are alsothankful to J.L.
Gauvain for the implementationof the acoustic verification module.REFERENCESI.
S. Austin, C. Barry, Y.-L., Chow, A. Derr, O.Kimball, F. Kubala, J. Makhoul, P. Placeway,W.
Russell, R. Schwartz, G. Yu, "ImprovedHMM Models fort High Performance SpeechRecognition," Proc.
DARPA Speech andNatural Language Workshop, Somerset, PA,June 1990.2.
J. K. Baker, "The DRAGON System - AnOverview," IEEE Trans.
Acoust.
Speech, andSignal Process., vol.
ASSP-23, pp 24-29, Feb.1975.3.
M. K. Brown, J. G. Wilpon, "AutomaticGeneration of Lexical and GrammaticalConstraints for Speech Recognition," Proc.1990 IEEE Intl.
Conf.
on Acoustics, Speech,and Signal Processing, Albuquerque, NewMexico, pp.
733-736, April 1990.4.
G. Hendrix, E. Sacerdoti, D. Sagalowicz, J.Slocum, "Developing a Natural LanagugeInterface to Complex Data," ACMTranslations on Database Systems 3:2 pp.105-147, 1978.5.
X. Huang, F. Alleva, S. Hayamizu, H. W. Hon,M.
Y. Hwang, K. F. Lee, "Improved HiddenMarkov Modeling for Speaker-IndependentContinuous Speech Recognition," Proc.DARPA Speech and Natural LanguageWorkshop, Somerset, PA, June 1990.6.
C.-H. Lee, L. R. Rabiner, R. Pieraccini and J.G.
Wilpon, "Acoustic Modeling for LargeSpeech Recognition," Computer, Speech andLanguage, 4, pp.
127-165, 1990.3057.
C.-H. Lee, E. P. Giachin, L. R. Rabiner, R.Pieraccini and A. E. Rosenberg, "ImprovedAcoustic Modeling for Continuous SpeechRecognition," Prec.
DARPA Speech andNatural Language Workshop, Somerset, PA,June 1990.8.
V.I.
Levenshtein, "Binary Codes Capable ofCorrecting Deletions, Insertions, andReversals," Soy.
Phys.-Dokl., vol.
10, pp.707-710, 1966.9.
S. E. Leviuson, K. L. Shipley, "AConversational Mode Airline ReservationSystem Using Speech Input and Output," BSTJ59 pp.
119-137, 1980.10.
S.E.
Levinson, A. Ljolje, L. G. Miller, "LargeVocabulary Speech Recognition Using aHidden Markov Model for Acoustic/PhoneticClassification," Prec.
1988 IEEE Intl.
Conf.
onAcoustics, Speech, and Signal Processing, NewYork, NY, pp.
505-508, April 1988.11.
S.E.
Levinson, M. Y. Liberman, A. Ljolje, L.G.
Miller, "Speaker Independent PhoneticTranscription of Fluent Speech for LargeVocabulary Speech Recognition," Prec.
ofFebruary 1989 DARPA Speech and NaturalLanguage Workshop pp.
75-80, Philadelphia,PA, February 21-23, 1989.12.
B. T. Lowerre, D. R. Reddy, "'The HARPYSpeech Understanding System," Ch.
15 inTrends in Speech Recognition W. A. Lea, Ed.Prentice-Hall, pp.
340-360, 1980.13.
H. Murveit, M. Weintraub, M. Cohen,"Training Set Issues in SRI's DECIPHERSpeech Recognition System," Prec.
DARPASpeech and Natural Language Workshop,Somerset, PA, June 1990.14.
D. S. Pallett, "Speech Results on ResourceManagement Task," Prec.
of February 1989DARPA Speech and Natural LanguageWorkshop pp.
18-24, Philadelphia, PA,February 21-23, 1989.15.
R. Pieraccini, C.-H. Lee, E. Giachin, L. R.Rabiner, "Implementation Aspects of LargeVocabulary Recognition Based on Intrawordand Interword Phonetic Units," Prec.
ThirdJoint DARPA Speech and Natural LanguageWorkshop, Somerset, PA, June 1990.16.
D.B., Paul "The Lincoln Tied-Mixture HMMContinuous Speech Recognizer," Prec.DARPA Speech and Natural LanguageWorkshop, Somerset, PA, June 1990.17.
P.J.
Price, W. Fisher, J. Bemstein, D. Pallett,"The DARPA 1000-Word ResourceManagement Database for Continuous SpeechRecognition," Prec.
1988 IEEE Intl.
Conf.
onAcoustics, Speech, and Signal Processing, NewYork, NY, pp.
651-654, April 1988.18.
L.R.
Rabiner, "A Tutorial on Hidden MarkovModels, and Selected Applications in SpeechRecognition," Prec.
IEEE, Vol.
77, No.
2,pp.
257-286, Feb. 1989.19.
D. R. Reddy, et al, "Speech UnderstandingSystems: Final Report," Computer ScienceDepartment, Carnegie Mellon University,1977.20.
W. Woods, et al, "Speech UnderstandingSystems: Final Technical Progress Report,"Bolt Beranek and Newman, Inc. Report No.3438, Cambridge, MA., 1976.306
