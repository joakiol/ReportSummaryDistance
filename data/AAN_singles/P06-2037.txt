Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287?294,Sydney, July 2006. c?2006 Association for Computational LinguisticsLow-cost Enrichment of Spanish WordNet with Automatically TranslatedGlosses: Combining General and Specialized ModelsJesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research Center, LSI DepartmentUniversitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, E-08034, Barcelona{jgimenez,lluism}@lsi.upc.eduAbstractThis paper studies the enrichment of Span-ish WordNet with synset glosses automat-ically obtained from the English Word-Net glosses using a phrase-based Statisti-cal Machine Translation system.
We con-struct the English-Spanish translation sys-tem from a parallel corpus of proceed-ings of the European Parliament, and studyhow to adapt statistical models to the do-main of dictionary definitions.
We buildspecialized language and translation mod-els from a small set of parallel definitionsand experiment with robust manners tocombine them.
A statistically significantincrease in performance is obtained.
Thebest system is finally used to generate adefinition for all Spanish synsets, whichare currently ready for a manual revision.As a complementary issue, we analyze theimpact of the amount of in-domain dataneeded to improve a system trained en-tirely on out-of-domain data.1 IntroductionStatistical Machine Translation (SMT) is today avery promising approach.
It allows to build veryquickly and fully automatically Machine Trans-lation (MT) systems, exhibiting very competitiveresults, only from a parallel corpus aligning sen-tences from the two languages involved.In this work we approach the task of enrichingSpanish WordNet with automatically translatedglosses1.
The source glosses for these translationsare taken from the English WordNet (Fellbaum,1Glosses are short dictionary definitions that accompanyWordNet synsets.
See examples in Tables 5 and 6.1998), which is linked, at the synset level, to Span-ish WordNet.
This resource is available, amongother sources, through the Multilingual CentralRepository (MCR) developed by the MEANINGproject (Atserias et al, 2004).We start by empirically testing the performanceof a previously developed English?Spanish SMTsystem, built from the large Europarl corpus2(Koehn, 2003).
The first observation is that thissystem completely fails to translate the specificWordNet glosses, due to the large language varia-tions in both domains (vocabulary, style, grammar,etc.).
Actually, this is confirming one of the maincriticisms against SMT, which is its strong domaindependence.
Since parameters are estimated froma corpus in a concrete domain, the performanceof the system on a different domain is often muchworse.
This flaw of statistical and machine learn-ing approaches is well known and has been largelydescribed in the NLP literature, for a variety oftasks (e.g., parsing, word sense disambiguation,and semantic role labeling).Fortunately, we count on a small set of Spanishhand-developed glosses in MCR3.
Thus, we moveto a working scenario in which we introduce asmall corpus of aligned translations from the con-crete domain of WordNet glosses.
This in-domaincorpus could be itself used as a source for con-structing a specialized SMT system.
Again, ex-periments show that this small corpus alone doesnot suffice, since it does not allow to estimategood translation parameters.
However, it is wellsuited for combination with the Europarl corpus,to generate combined Language and Translation2The Europarl Corpus is available at: http://-people.csail.mit.edu/people/koehn/publications/europarl3About 10% of the 68,000 Spanish synsets contain a defi-nition, generated without considering its English counterpart.287Models.
A substantial increase in performance isachieved, according to several standard MT eval-uation metrics.
Although moderate, this boostin performance is statistically significant accord-ing to the bootstrap resampling test described byKoehn (2004b) and applied to the BLEU metric.The main reason behind this improvement isthat the large out-of-domain corpus contributesmainly with coverage and recall and the in-domaincorpus provides more precise translations.
Wepresent a qualitative error analysis to support theseclaims.
Finally, we also address the importantquestion of how much in-domain data is neededto be able to improve the baseline results.Apart from the experimental findings, our studyhas generated a very valuable resource.
Currently,we have the complete Spanish WordNet enrichedwith one gloss per synset, which, far from beingperfect, constitutes an axcellent starting point fora posterior manual revision.Finally, we note that the construction of aSMT system when few domain-specific data areavailable has been also investigated by other au-thors.
For instance, Vogel and Tribble (2002) stud-ied whether an SMT system for speech-to-speechtranslation built on top of a small parallel corpuscan be improved by adding knowledge sourceswhich are not domain specific.
In this work, welook at the same problem the other way around.We study how to adapt an out-of-domain SMTsystem using in-domain data.The rest of the paper is organized as follows.In Section 2 the fundamentals of SMT and thecomponents of our MT architecture are described.The experimental setting is described in Section 3.Evaluation is carried out in Section 4.
Finally, Sec-tion 5 contains error analysis and Section 6 con-cludes and outlines future work.2 BackgroundCurrent state-of-the-art SMT systems are based onideas borrowed from the Communication Theoryfield.
Brown et al (1988) suggested that MT canbe statistically approximated to the transmissionof information through a noisy channel.
Given asentence f = f1..fn (distorted signal), it is possi-ble to approximate the sentence e = e1..em (origi-nal signal) which produced f .
We need to estimateP (e|f), the probability that a translator producesf as a translation of e. By applying Bayes?
rule itis decomposed into: P (e|f) = P (f |e)?P (e)P (f) .To obtain the string e which maximizes thetranslation probability for f , a search in the prob-ability space must be performed.
Because the de-nominator is independent of e, we can ignore it forthe purpose of the search: e = argmaxeP (f |e) ?P (e).
This last equation devises three compo-nents in a SMT system.
First, a language modelthat estimates P (e).
Second, a translation modelrepresenting P (f |e).
Last, a decoder responsi-ble for performing the arg-max search.
Languagemodels are typically estimated from large mono-lingual corpora, translation models are built outfrom parallel corpora, and decoders usually per-form approximate search, e.g., by using dynamicprogramming and beam search.However, in word-based models the modelingof the context in which the words occur is veryweak.
This problem is significantly alleviated byphrase-based models (Och, 2002), which repre-sent nowadays the state-of-the-art in SMT.2.1 System ConstructionFortunately, there is a number of freely availabletools to build a phrase-based SMT system.
Weused only standard components and techniques forour basic system, which are all described below.The SRI Language Modeling Toolkit (SRILM)(Stolcke, 2002) supports creation and evaluationof a variety of language models.
We build trigramlanguage models applying linear interpolation andKneser-Ney discounting for smoothing.In order to build phrase-based translation mod-els, a phrase extraction must be performed ona word-aligned parallel corpus.
We used theGIZA++ SMT Toolkit4 (Och and Ney, 2003) togenerate word alignments We applied the phrase-extract algorithm, as described by Och (2002), onthe Viterbi alignments output by GIZA++.
Wework with the union of source-to-target and target-to-source alignments, with no heuristic refine-ment.
Phrases up to length five are considered.Also, phrase pairs appearing only once are dis-carded, and phrase pairs in which the source/targetphrase was more than three times longer than thetarget/source phrase are ignored.
Finally, phrasepairs are scored by relative frequency.
Note thatno smoothing is performed.Regarding the arg-max search, we used thePharaoh beam search decoder (Koehn, 2004a),which naturally fits with the previous tools.4http://www.fjoch.com/GIZA++.html2883 Data Sets and Evaluation MetricsAs a general source of English?Spanish paralleltext, we used a collection of 730,740 parallel sen-tences extracted from the Europarl corpus.
Thesecorrespond exactly to the training data from theShared Task 2: Exploiting Parallel Texts for Sta-tistical Machine Translation from the ACL-2005Workshop on Building and Using Parallel Texts:Data-Driven Machine Translation and Beyond5.To be used as specialized source, we extracted,from the MCR , the set of 6,519 English?Spanishparallel glosses corresponding to the already de-fined synsets in Spanish WordNet.
These defini-tions corresponded to 5,698 nouns, 87 verbs, and734 adjectives.
Examples and parenthesized textswere removed.
Parallel glosses were tokenizedand case lowered.
We discarded some of theseparallel glosses based on the difference in lengthbetween the source and the target.
The gloss av-erage length for the resulting 5,843 glosses was8.25 words for English and 8.13 for Spanish.
Fi-nally, gloss pairs were randomly split into training(4,843), development (500) and test (500) sets.Additionally, we counted on two large mono-lingual Spanish electronic dictionaries, consistingof 142,892 definitions (2,112,592 tokens) (?D1?)(Mart?
?, 1996) and 168,779 definitons (1,553,674tokens) (?D2?)
(Vox, 1990), respectively.Regarding evaluation, we used up to four dif-ferent metrics with the aim of showing whetherthe improvements attained are consistent or not.We have computed the BLEU score (accumu-lated up to 4-grams) (Papineni et al, 2001), theNIST score (accumulated up to 5-grams) (Dod-dington, 2002), the General Text Matching (GTM)F-measure (e = 1, 2) (Melamed et al, 2003),and the METEOR measure (Banerjee and Lavie,2005).
These metrics work at the lexical level byrewarding n-gram matches between the candidatetranslation and a set of human references.
Addi-tionally, METEOR considers stemming, and al-lows for WordNet synonymy lookup.The discussion of the significance of the resultswill be based on the BLEU score, for which wecomputed a bootstrap resampling test of signifi-cance (Koehn, 2004b).5http://www.statmt.org/wpt05/.4 Experimental Evaluation4.1 Baseline SystemsAs explained in the introduction we built two indi-vidual baseline systems.
The first baseline (?EU?
)system is entirely based on the training data fromthe Europarl corpus.
The second baseline system(?WNG?)
is entirely based on the training set fromof the in-domain corpus of parallel glosses.
In thesecond case phrase pairs occurring only once inthe training corpus are not discarded due to the ex-tremely small size of the corpus.Table 1 shows results of the two baseline sys-tems, both for the development and test sets.
Wecompare the performance of the ?EU?
baseline onthese data sets with respect to the (in-domain) Eu-roparl test set provided by the organizers of theACL-2005 MT workshop.
As expected, there isa very significant decrease in performance (e.g.,from 0.24 to 0.08 according to BLEU) when the?EU?
baseline system is applied to the new do-main.
Some of this decrement is also due to a cer-tain degree of free translation exhibited by the setof available ?quasi-parallel?
glosses.
We furtherdiscuss this issue in Section 5.The results obtained by ?WNG?
are also verylow, though slightly better than those of ?EU?.
Thisis a very interesting fact.
Although the amount ofdata utilized to construct the ?WNG?
baseline is150 times smaller than the amount utilized to con-struct the ?EU?
baseline, its performance is higherconsistently according to all metrics.
We interpretthis result as an indicator that models estimatedfrom in-domain data provide higher precision.We also compare the results to those of a com-mercial system such as the on-line version 5.0 ofSYSTRAN6, a general-purpose MT system basedon manually-defined lexical and syntactic trans-fer rules.
The performance of the baseline sys-tems is significantly worse than SYSTRAN?s onboth development and test sets.
This means thata rule-based system like SYSTRAN is more ro-bust than the SMT-based systems.
The differenceagainst the specialized ?WNG?
also suggests thatthe amount of data used to train the ?WNG?
base-line is clearly insufficient.4.2 Combining Sources: Language ModelsIn order to improve results, in first place we turnedour eyes to language modeling.
In addition to6http://www.systransoft.com/.289system BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEORdevelopmentEU-baseline 0.0737 2.8832 0.3131 0.2216 0.2881WNG-baseline 0.1149 3.3492 0.3604 0.2605 0.3288SYSTRAN 0.1625 3.9467 0.4257 0.2971 0.4394testEU-baseline 0.0790 2.8896 0.3131 0.2262 0.2920WNG-baseline 0.0951 3.1307 0.3471 0.2510 0.3219SYSTRAN 0.1463 3.7873 0.4085 0.2921 0.4295acl05-testEU-baseline 0.2381 6.5848 0.5699 0.2429 0.5153Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the ?EU?baseline system on the ACL-2005 SMT workshop test set extracted from the Europarl Corpus.
BLEU.n4 shows the accumulatedBLEU score for 4-grams.
NIST.n5 shows the accumulated NIST score for 5-grams.
GTM.e1 and GTM.e2 show the GTM F1-measure for different values of the e parameter (e = 1, e = 2, respectively).
METEOR reflects the METEOR score.the language model built from the Europarl cor-pus (?EU?)
and the specialized language modelbased on the small training set of parallel glosses(?WNG?
), two specialized language models, basedon the two large monolingual Spanish electronicdictionaries (?D1?
and ?D2?)
were used.
We triedseveral configurations.
In all cases, language mod-els are combined with equal probability.
See re-sults, for the development set, in Table 2.As expected, the closer the language model isto the target domain, the better results.
Observehow results using language models ?D1?
and ?D2?outperform results using ?EU?.
Note also that bestresults are in all cases consistently attained by us-ing the ?WNG?
language model.
This means thatlanguage models estimated from small sets of in-domain data are helpful.
A second conclusion isthat a significant gain is obtained by incrementallyadding (in-domain) specialized language modelsto the baselines, according to all metrics but BLEUfor which no combination seems to significantlyoutperform the ?WNG?
baseline alone.
Observethat best results are obtained, except in the caseof BLEU, by the system using ?EU?
as translationmodel and ?WNG?
as language model.
We inter-pret this result as an indicator that translation mod-els estimated from out-of-domain data are help-ful because they provide recall.
A third interest-ing point is that adding an out-of-domain languagemodel (?EU?)
does not seem to help, at least com-bined with equal probability than in-domain mod-els.
Same conclusions hold for the test set, too.4.3 Tuning the SystemAdjusting the Pharaoh parameters that controlthe importance of the different probabilities thatgovern the search may yield significant improve-ments.
In our case, it is specially important toproperly adjust the contribution of the languagemodels.
We adjusted parameters by means of asoftware based on the Downhill Simplex Methodin Multidimensions (William H. Press and Flan-nery, 2002).
The tuning was based on the improve-ment attained in BLEU score over the develop-ment set.
We tuned 6 parameters: 4 language mod-els (?lmEU , ?lmD1, ?lmD2, ?lmWNG), the transla-tion model (??
), and the word penalty (?w)7.Results improve substantially.
See Table 3.
Bestresults are still attained using the ?EU?
translationmodel.
Interestingly, as suggested by Table 2, theweight of language models is concentrated on the?WNG?
language model (?lmWNG = 0.95).4.4 Combining Sources: Translation ModelsIn this section we study the possibility of combin-ing out-of-domain and in-domain translation mod-els aiming at achieving a good balance betweenprecision and recall that yields better MT results.Two different strategies have been tried.
Ina first stragegy we simply concatenate the out-of-domain corpus (?EU?)
and the in-domain cor-pus (?WNG?).
Then, we construct the translatationmodel (?EUWNG?)
as detailed in Section 2.1.
Asecond manner to proceed is to linearly combinethe two different translation models into a singletranslation model (?EU+WNG?).
In this case, wecan assign different weights (?)
to the contributionof the different models to the search.
We can alsodetermine a certain threshold ?
which allows us7Final values when using the ?EU?
translation model are?lmEU = 0.22, ?lmD1 = 0, ?lmD2 = 0.01, ?lmWNG =0.95, ??
= 1, and ?w = ?2.97, while when using the?WNG?
translation model final values are ?lmEU = 0.17,?lmD1 = 0.07, ?lmD2 = 0.13, ?lmWNG = 1, ??
= 0.95,and ?w = ?2.64.290Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOREU EU 0.0737 2.8832 0.3131 0.2216 0.2881EU WNG 0.1062 3.4831 0.3714 0.2631 0.3377EU D1 0.0959 3.2570 0.3461 0.2503 0.3158EU D2 0.0896 3.2518 0.3497 0.2482 0.3163EU D1 + D2 0.0993 3.3773 0.3585 0.2579 0.3244EU EU + D1 + D2 0.0960 3.2851 0.3472 0.2499 0.3160EU D1 + D2 + WNG 0.1094 3.4954 0.3690 0.2662 0.3372EU EU + D1 + D2 + WNG 0.1080 3.4248 0.3638 0.2614 0.3321WNG EU 0.0743 2.8864 0.3128 0.2202 0.2689WNG WNG 0.1149 3.3492 0.3604 0.2605 0.3288WNG D1 0.0926 3.1544 0.3404 0.2418 0.3050WNG D2 0.0845 3.0295 0.3256 0.2326 0.2883WNG D1 + D2 0.0917 3.1185 0.3331 0.2394 0.2995WNG EU + D1 + D2 0.0856 3.0361 0.3221 0.2312 0.2847WNG D1 + D2 + WNG 0.0980 3.2238 0.3462 0.2479 0.3117WNG EU + D1 + D2 + WNG 0.0890 3.0974 0.3309 0.2373 0.2941Table 2: MT Results on development set, for several translation/language model configurations.
?EU?
and ?WNG?
refer tothe models estimated from the Europarl corpus and the training set of parallel WordNet glosses, respectively.
?D1?, and ?D2?denote the specialized language models estimated from the two dictionaries.Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEORdevelopmentEU EU + D1 + D2 + WNG 0.1272 3.6094 0.3856 0.2727 0.3695WNG EU + D1 + D2 + WNG 0.1269 3.3740 0.3688 0.2676 0.3452testEU EU + D1 + D2 + WNG 0.1133 3.4180 0.3720 0.2650 0.3644WNG EU + D1 + D2 + WNG 0.1015 3.1084 0.3525 0.2552 0.3343Table 3: MT Results on development and test sets after tuning for the ?EU + D1 + D2 + WNG?
language model configurationfor the two translation models, ?EU?
and ?WNG?.to discard phrase pairs under a certain probability.These weights and thresholds were adjusted8 asdetailed in Subsection 4.3.
Interestingly, at combi-nation time the importance of the ?WNG?
transla-tion model (?tmWNG = 0.9) is much higher thanthat of the ?EU?
translation model (?tmEU = 0.1).Table 4 shows results for the two strategies.As expected, the ?EU+WNG?
strategy consistentlyobtains the best results according to all metricsboth on the development and test sets, since itallows to better adjust the relative importance ofeach translation model.
However, both techniquesachieve a very competitive performance.
Resultsimprove, according to BLEU, from 0.13 to 0.16,and from 0.11 to 0.14, for the development andtest sets, respectively.We measured the statistical signficance ofthe overall improvement in BLEU.n4 attainedwith respect to the baseline results by ap-plying the bootstrap resampling technique de-scribed by Koehn (2004b).
The 95% confi-dence intervals extracted from the test set after8We used values ?tmEU = 0.1, ?tmWNG = 0.9,?tmEU = 0.1, and ?tmWNG = 0.0110,000 samples are the following: IEU?base =[0.0642, 0.0939], IWNG?base = [0.0788, 0.1112],IEU+WNG?best = [0.1221, 0.1572].
Since the in-tervals are not ovelapping, we can conclude thatthe performance of the best combined method isstatistically higher than the ones of the two base-line systems.4.5 How much in-domain data is needed?In principle, the more in-domain data we have thebetter, but these may be difficult or expensive tocollect.
Thus, a very interesting issue in the con-text of our work is how much in-domain data isneeded in order to improve results attained usingout-of-domain data alone.
To answer this questionwe focus on the ?EU+WNG?
strategy and analyzethe impact on performance (BLEU.n4) of special-ized models extracted from an incrementally big-ger number of example glosses.
The results arepresented in the plot of Figure 1.
We computethree variants separately, by considering the use ofthe in-domain data: only for the translation model(TM), only for the language model (LM), and si-multaneously in both models (TM+LM).
In order291Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEORdevelopmentEUWNG WNG 0.1288 3.7677 0.3949 0.2832 0.3711EUWNG EU + D1 + D2 + WNG 0.1182 3.6034 0.3835 0.2759 0.3552EUWNG EU + D1 + D2 + WNG (TUNED) 0.1554 3.8925 0.4081 0.2944 0.3998EU+WNG WNG 0.1384 3.9743 0.4096 0.2936 0.3804EU+WNG EU + D1 + D2 + WNG 0.1235 3.7652 0.3911 0.2801 0.3606EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1618 4.1415 0.4234 0.3029 0.4130testEUWNG WNG 0.1123 3.6777 0.3829 0.2771 0.3595EUWNG EU + D1 + D2 + WNG 0.1183 3.5819 0.3737 0.2772 0.3518EUWNG EU + D1 + D2 + WNG (TUNED) 0.1290 3.6478 0.3920 0.2810 0.3885EU+WNG WNG 0.1227 3.8970 0.3997 0.2872 0.3723EU+WNG EU + D1 + D2 + WNG 0.1199 3.7353 0.3846 0.2812 0.3583EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1400 3.8930 0.4084 0.2907 0.3963Table 4: MT Results on development and test sets for the two strategies for combining translations models.0.060.070.080.090.10.110.120.130.140  500  1000  1500  2000  2500  3000  3500  4000  4500BLEU.n4# glossesbaselineTM + LM impactTM impactLM impactFigure 1: Impact of the size of in-domain data onMT system performance for the test set.to avoid the possible effect of over-fitting we focuson the behavior on the test set.
Note that the opti-mization of parameters is performed at each pointin the x-axis using only the development set.A significant initial gain of around 0.3 BLEUpoints is observed when adding as few as 100glosses.
In all cases, it is not until around 1,000glosses are added that the ?EU+WNG?
system sta-bilizes.
After that, results continue improving asmore in-domain data are added.
We observe avery significant increase by just adding around3,000 glosses.
Another interesting observation isthe boosting effect of the combination of TM andLM specialized models.
While individual curvesfor TM and LM tend to be more stable with morethan 4,000 added examples, the TM+LM curvestill shows a steep increase in this last part.5 Error AnalysisWe inspected results at the sentence level based onthe GTM F-measure (e = 1) for the best config-uration of the ?EU+WNG?
system.
196 sentencesout from the 500 obtain an F-measure equal to orhigher than 0.5 on the development set (181 sen-tences in the case of test set), whereas only 54sentences obtain a score lower than 0.1.
Thesenumbers give a first idea of the relative useful-ness of our system.
Table 5 shows some trans-lation cases selected for discussion.
For instance,Case 1 is a clear example of unfair low score.
Theproblem is that source and reference are not par-allel but ?quasi-parallel?.
Both glosses define thesame concept but in a different way.
Thus, metricsbased on rewarding lexical similarities are not wellsuited for these cases.
Cases 2, 3, 4 are examplesof proper cooperation between ?EU?
and ?WNG?models.
?EU?
models provides recall, for instanceby suggesting translation candidates for ?bombs?or ?price below?.
?WNG?
models provide preci-sion, for instance by choosing the right translationfor ?an attack?
or ?the act of?.We also compared the ?EU+WNG?
system toSYSTRAN.
In the case of SYSTRAN 167 sen-tences obtain a score equal to or higher than 0.5whereas 79 sentences obtain a score lower than0.1.
These numbers are slightly under the per-formance of the ?EU+WNG?
system.
Table 6shows some translation cases selected for discus-sion.
Case 1 is again an example of both sys-tems obtaining very low scores because of ?quasi-parallelism?.
Cases 2 and 3 are examples of SYS-TRAN outperforming our system.
In case 2 SYS-TRAN exhibits higher precision in the translationof ?accompanying?
and ?illustration?, whereas incase 3 it shows higher recall by suggesting ap-propriate translation candidates for ?fibers?, ?silk-worm?, ?cocoon?, ?threads?, and ?knitting?.
Cases292FE FW FEW Source OutE OutW OutEW Reference0.0000 0.1333 0.1111 of the younger de acuerdo con de la younger de acuerdo con que tieneof two boys el ma?s joven de dos boys el ma?s joven de menos edadwith the same de dos boys tiene el mismo dos muchachosfamily name con la misma nombre familia tiene el mismofamilia fama nombre familia0.2857 0.2500 0.5000 an attack atacar por ataque ataque ataque conby dropping cayendo realizado por realizado por bombasbombs bombas dropping bombs cayendo bombas0.1250 0.7059 0.5882 the act of acto de la accio?n y efecto accio?n y efecto accio?n y efectoinforming by informacio?n de informing de informaba de informarverbal report por verbales por verbal por verbales con una expli-ponencia explicacio?n explicacio?n cacio?n verbal0.5000 0.0000 0.5000 a price below un precio por una price un precio por precio que esta?the standard debajo de la below nu?mbero debajo de la por debajo deprice norma precio esta?ndar price esta?ndar precio lo normalTable 5: MT output analysis of the ?EU?, ?WNG?
and ?EU+WNG?
systems.
FE , FW and FEW refer to the GTM (e = 1)F-measure attained by the ?EU?, ?WNG?
and ?EU+WNG?
systems, respectively.
?Source?, OutE , OutW and OutEW refer tothe input and the output of the systems.
?Reference?
corresponds to the expected output.4 and 5 are examples where our system outper-forms SYSTRAN.
In case 4, our system provideshigher recall by suggesting an adequate transla-tion for ?top of something?.
In case 5, our systemshows higher precision by selecting a better trans-lation for ?rate?.
However, we observed that SYS-TRAN tends in most cases to construct sentencesexhibiting a higher degree of grammaticality.6 ConclusionsIn this work, we have enriched every synset inSpanish WordNet with a preliminary gloss, whichcan be later updated in a lighter process of manualrevision.
Though imperfect, this material consti-tutes a very valuable resource.
For instance, Word-Net glosses have been used in the past to generatesense tagged corpora (Mihalcea and Moldovan,1999), or as external knowledge for Question An-swering systems (Hovy et al, 2001).We have also shown the importance of using asmall set of in-domain parallel sentences in or-der to adapt a phrase-based general SMT sys-tem to a new domain.
In particular, we haveworked on specialized language and translationmodels and on their combination with generalmodels in order to achieve a proper balance be-tween precision (specialized in-domain models)and recall (general out-of-domain models).
A sub-stantial increase is consistently obtained accordingto standard MT evaluation metrics, which has beenshown to be statistically significant in the caseof BLEU.
Broadly speaking, we have shown thataround 3,000 glosses (very short sentence frag-ments) suffice in this domain to obtain a signifi-cant improvement.
Besides, all the methods usedare language independent, assumed the availabil-ity of the required in-domain additional resources.In the future we plan to work on domain inde-pendent translation models built from WordNet it-self.
We may use the WordNet topology to pro-vide translation candidates weighted according tothe given domain.
Moreover, we are experiment-ing the applicability of current Word Sense Dis-ambiguation (WSD) technology to MT.
We couldfavor those translation candidates showing a closersemantic relation to the source.
We believe thatcoarse-grained is sufficient for the purpose of MT.AcknowledgementsThis research has been funded by the SpanishMinistry of Science and Technology (ALIADOTIC2002-04447-C02) and the Spanish Ministry ofEducation and Science (TRANGRAM, TIN2004-07925-C03-02).
Our research group, TALP Re-search Center, is recognized as a Quality ResearchGroup (2001 SGR 00254) by DURSI, the Re-search Department of the Catalan Government.Authors are grateful to Patrik Lambert for pro-viding us with the implementation of the SimplexMethod, and specially to German Rigau for moti-vating in its origin all this work.ReferencesJordi Atserias, Luis Villarejo, German Rigau, EnekoAgirre, John Carroll, Bernardo Magnini, and Piek293FEW FS Source OutEW OutS Reference0.0000 0.0000 a newspaper that perio?dico que un perio?dico publicacio?nis published se publica diario que se publica perio?dicaevery day cada d?
?a monotema?tica0.1818 0.8333 brief description breve descripcio?n breve descripcio?n pequen?a descripcio?naccompanying an adjuntas un aclaracio?n que acompan?a que acompan?aillustration una ilustracio?n una ilustracio?n0.1905 0.7333 fibers from silkworm fibers desde silkworm las fibras de los fibras de los capulloscocoons provide cocoons proporcionan capullos del gusano de gusano de sedathreads for knitting threads para knitting de seda proporcionan que proporcionanlos hilos de rosca hilos para tejerpara hacer punto1.0000 0.0000 the top of something parte superior de la tapa algo parte superior deuna cosa una cosa0.6667 0.3077 a rate at which un ritmo al que una tarifa en la ritmo al quesomething happens sucede algo cual algo sucede sucede una cosaTable 6: MT output analysis of the ?EU+WNG?
and SYSTRAN systems.
FEW and FS refer to the GTM (e = 1) F-measureattained by the ?EU+WNG?
and SYSTRAN systems, respectively.
?Source?, OutEW and OutS refer to the input and the outputof the systems.
?Reference?
corresponds to the expected output.Vossen.
2004.
The MEANING Multilingual Cen-tral Repository.
In Proceedings of 2nd GWC.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, Robert L.Mercer, , and Paul S. Roossin.
1988.
A statisticalapproach to language translation.
In Proceedings ofCOLING?88.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd In-ternation Conference on Human Language Technol-ogy, pages 138?145.C.
Fellbaum, editor.
1998.
WordNet.
An ElectronicLexical Database.
The MIT Press.Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin.2001.
The Use of External Knowledge of FactoidQA.
In Proceedings of TREC.Philipp Koehn.
2003.
Europarl: A Multilin-gual Corpus for Evaluation of Machine Transla-tion.
Technical report, http://people.csail.mit.edu/-people/koehn/publications/europarl/.Philipp Koehn.
2004a.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In Proceedings of AMTA?04.Philipp Koehn.
2004b.
Statistical Significance Testsfor Machine Translation Evaluation.
In Proceedingsof EMNLP?04.Mar?
?a Antonia Mart?
?, editor.
1996.
Gran dic-cionario de la Lengua Espan?ola.
Larousse Planeta,Barcelona.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Translation.In Proceedings of HLT/NAACL?03.Rada Mihalcea and Dan Moldovan.
1999.
An Au-tomatic Method for Generating Sense Tagged Cor-pora.
In Proceedings of AAAI.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2002.
Statistical Machine Transla-tion: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, RWTH Aachen.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation, IBM Research Re-port, RC22176.
Technical report, IBM T.J. WatsonResearch Center.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of IC-SLP?02.Stephan Vogel and Alicia Tribble.
2002.
Improv-ing Statistical Machine Translation for a Speech-to-Speech Translation Task.
In Proceedings of ICSLP-2002 Workshop on Speech-to-Speech Translation.Vox, editor.
1990.
Diccionario Actual de la LenguaEspan?ola.
Bibliograf, Barcelona.William T. Vetterling William H. Press, Saul A. Teukol-sky and Brian P. Flannery.
2002.
Numerical Recipesin C++: the Art of Scientific Computing.
CambridgeUniversity Press.294
