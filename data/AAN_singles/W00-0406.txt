Text Summarizer in Use: Lessons Learnedfrom Real World Deployment and EvaluationMary Ellen OkurowskiHarold WilsonJoaquin UrbinaDepartment ofDefense9800 Savage Rd.Fort Meade, MD.
20755Tony TaylorSRA Corp.4939 Elkridge LandingSuite #195Linthicum, MD.
21090Ruth Colvin ClarkClark Training & Consulting17801 CR 23Dolores, Colorado 81323Frank KrapchoKathpol Technologies Inc.6835 Deerpath Suite #102Elkridge, MD.
217051.0 IntroductionMuch of the historical and currentsummarization literature has been technology-centered with the questions posed andanswered having implications for technologydevelopment.
Though commercialsummarization products have appeared in themarket place and developers continue toexplore new summarization areas, few papershave been user-centered, examiningsummarization technology in-use.
In this~aaper, we show how applied work and theknowledge gleaned about technology in-use"can temper theoretical considerations and?
motivate as well as direct development likelyto result in higher eturn on investment.2.0 BackgroundThe importance of understanding thefunction a summary serves for users is widelyacknowledged, and seminal works definingsummary types by functions (Paice, 1990;Sparck-Jones, 1993) are frequently cited bydevelopers.
Task orientation defines extrinsictechnology assessments, and the researchliterature on how to assess performance formachine generated summaries in anexperimental task scenario has grown( Brandow et al, 1994; Morris et al, 1999;Jing et al, 1998; Merlino and Maybury,1999; Wasson, 1998; Tombros et al, 1998;Firmin and Chrzanowski, 1999; and Mani etal., 1999).
An increasing number of researchpapers on summarization systems now alsodescribe some type of extrinsic evaluativetask (e.g.
Salton et al, 1999; Strzalkowski etal., 1998).
A number of factors (i.e.characteristics of summaries, documents,users, and tasks) have surfaced which haveimplications for technology use.
More "research assessing technology (or any aspectof it) in-use on a user's own data even in adevelopment mode along the lines ofMcKeown et al (1998) is needed.
Whileexperimentation designs involving subjectsperforming short term controlled tasks mayyield results of statistical ?significance,generalizability to the user community islimited.In addition, the level of user supporttext.
summarization systems hould providealso continues to be speculative.
Moreinterest lies in new areas of inquiry likevisualization and browsing techniques (e.g.,Boguraev et al, 1998), multi-documentsummarization ( e.g., McKeown and Radev,1995), multi-media summarization (e.g.,Merlino and Maybury, 1999), summarization49of documents with graphics (e.g., Futrelle,1998) and multi-lingual summarization (e.g.,Cowie, 1998).
But systematic user studies oninterface support, applicability of proposedsummarization features, or on the real-worlduse of demonstration a d prototype systems oreven commercial systems have notmaterialized.3.0 OverviewThis paper presents a user study of asummarization system and provides insightson a number of technical issues relevant to thesummarization R&D community that arise in,the context of use, concerning technologyperformance and user support.
We describeinitial stages in the insertion of the SRAsummarizer in which (1) a large scale betatest was conducted, and (2) analysis of toolusage data, user surveys and observations, anduser requirements is leading to systemenhancements and more effectivesummarization technology insertion.
In ouruser study, we begin with a brief descriptionof the task and technology (3.1).
We thendescribe the beta test methodology (3.2) and:analysis of tool usage data (3.3).
We focus onwhat  we learned in our user-centered.
approach about how technology performancein a task and user support affect user?
acceptance (3.4) and what significanttechnology-related modifications resultedand what studies are in progress to  measuretool efficacy, summarization effectiveness,and the impact of training on tool use (3.5).Though work to enhance the textsummarization system is underway, we focusin this paper on user-centered issues.
Ourwork is predicated on the belief that there isno substitute for user generated ata to guidetool enhancement.3.1 Task and TechnologyThe task is indicative.
Our users relyon machine generated summaries (singledocument, either generic or query-based, withuser adjustment of compression rates) tojudge relevance of full documents to theirinformation eed.
As an information analyst,"our typical user routinely scans summaries tostay current with fields of interest andenhance domain knowledge.
This scanningtask is one of many jobs an analyst performsto support report writing for customers inother Government agencies.
Our goal is togenerate summaries that accelerateeliminating or selecting documents withoutmisleading or causing a user to access theoriginal text unnecessarily.The system in this user study is aversion of the SRA sentence xtraction systemdescribed in Aone et al (1997, 1998, 1999).Users retrieve documents from a database ofmultiple text collections of reports and press.Documents are generally written in ajournalistic style and average 2,000 charactersin length.
The number of documents in a batchmay vary from a few to hundreds.
Batches ofretrieved texts may be routinely routed to oursummary server or uploaded by the user.
Thesystem is web-.based and provides thecapability to tailor summary output bycreating multiple summary set-ups.
Useroptions include: number of sentencesviewed, summary type applied and sorting,other information viewed (e.g.
title, date),and high frequency document terms andnamed entities viewed.
Users can save, printor view full text originals with summariesappended.
Viewed originals highlightextracted sentences.All system use is voluntary.
Our usersare customers and, if dissatisfied, may elect toscan data without our technology.3.2 Beta Test MethodologyIn the fall of 1998, 90+ users wererecruited primarily through an IR system newsgroup and provided access to the SRA systemsummarizer to replace their full text reviewprocess of scanning concatenated files.Procedural (how-to) training was optional, but50IIIIIIapproximately 70 users opted to receive a one-on-one hands-on demonstration (about forty-five minutes in length) on texts that the newuser had retrieved.
The beta testing took placeover a six month period.
With no stipulationon the length of participation, many userssimply tried out the system a limited numberof times.
Initial feedback gave us a clearpicture of the likelihood of continued use.
Ourrelatively low retention rate highlighted thefact that the experimental conditions inprevious summary experiments may bemisleading and masked factors that do notsurface until users use a system in a dailywork in a real-world setting.3.3 Analysis of Tool Usage DataUsage data were collected for allsystem users and analyzed through web logs.
"These logs were a record of what users did ontheir actual work data.
For each user, our logsprovided a rich source of information: numberof summary batches, number of documents ineach, whether documents were viewed, andset up features--summary type, summary linesviewed, number of indicator (high frequencysignature terms) lines viewed, number ofentity (persons, places, organizations-) linesviewed, query terms).
Table 1 belowillustrates the type of representative datacollected, questions of interest, and findings.Table 1: Questions of Interest, Tool Usage Data, FindingsQuestions Data FindingWere documents number of sum- Users routinely accessed our system to readsummarized?
mary events machine generated summaries.Did users actually number of current Most users did not appear to fully exploit he flex-tailor the system?
set-ups ibility of the system.
The beta test population hada median of only two set-up types active.type of summary Did the users selectgeneric or query-based summaries?Is there a differenceamong summaryt3~pes for the num-ber of sentencesviewed?Do users choose touse indicators andentities when tailor-ing browsing capa-bility?number of sen-tences viewed bysummary types(generic, query-based, lead)indicator/entitypreferences fornon-default set-ups(on or off)Usage data indicated that about half the popula-tion selected generic and the other half query-based summaries.
(Note: The default set-up wasthe generic summarization.
)The hypothesis of equal median number of sen-tences available for viewing sentences was tested.The number of sentences viewed with genericsummary type (3) is significantly different fromeither query-based (5) or lead (6).Users tended to retain indicator and entity prefer-ences when tailoring capabilities.
(But users gen-erallymodified a default set-up in which bothpreferences have a line viewed.
)IIIIII51Table 1: Questions of Interest, Tool Usage Data, FindingsQuestions Data FindingtooliiDoes training makea difference on sys-tem use or user pro-file type?
Userswere categorized(advanced, interme-diate, novice) onthe basis of usagefeatures with Harti-gan's K-Meansclustering algo-rithm.training anduse dataA chi-squared test for independence betweentraining and use reflected a significant relation-ship (p value close to 0) i.e., training did impactthe user's decision to use the system.
However,training did not make a difference across the threeuser profile types.
A Fisher Exact test on a 3x2contingency table revealed that the relative num-bers of trained and untrained users at the threeuser profile types were the same (p-value=0.1916) i.e., training and type are independent.tIIIIIAs we began to analyze the data, werealized that we had only a record of use, butwere not sure of what motivated the usepatterns.
Therefore, the team supplementedtool usage data with an or/-line survey andone-on-one observations tohelp us understandand analyze the user behavior.
Theseadditional data points motivated much of ourwork described in 3.5.
Throughout the sixmonth cycle we also collected and categorizeduser requirements.3.4 Insights on Text Summarization?
3.4.1 Technology PerformanceInsight 1: For user acceptance, technology?
performance must go beyond a goodsuthmary.
It requires an understanding of theusers" work practices.We learned that many factors in thetask environment affect technologyperformance and user acceptance.Underpinning much work in summarization isthe view that summaries are time savers.
Maniet al (1999) report that summaries at a lowcompression rate reduced decision makingtime by 40% (categorization) and 50% (ad-hoc) with relevance asessments almost asaccurate as the full text.
Although evaluatorsacknowledge the role of data presentation (e.g., Firmin and Chrzanowski, 1999; Merlinoand Maybury, 1999), most studies usesummary system output as the metric forevaluation.
The question routinely posedseems to be "Do summaries ave the usertime without loss in accuracy?"
However, weconfirmed observations on the integration ofsummarization and retrieval technologies ofMcKeown et al (1998) and learned thatusers are not likely to consider usingsummaries as a time saver unless thesummaries are efficiently accessed.
For ourusers a tight coupling of retrieval andsummarization is pre-requisite.
Batchesautomatically routed to the summary serveravailable for user review were preferred overthose requiring the user to upload files forsummarization.
Users pointed out that theuploading took more time then they werewilling to spend.User needs and their work practicesoften constrain how technology is applied.For example, McKeown et al (1998) focusedon the needs of physicians who want toexamine only data for patients with similarcharacteristics to their own patients, andWasson (1998) focused on the needs of newsinformation customers who want to retrievedocuments likely to be on-topic.
We too62IIIIIIIlIIII!IIIIIIIIIIIIIIIIIdiscovered that the user needs affect theirinterest in summarization technology, butfrom a more general perspective.
TextREtrieval Conferences (e.g., Harman, 1996)have baselined system performance in termsof two types of tasks--routing or ad-h.oc.
Inour environment the ad-hoc users were lesslikely .to want a summary.
They simplywanted an answer to a question and did notwant to review summaries.
If too manydocuments were retrieved, they would simplycraft a more effective query.Measuring the efficiency gains with areal population was quite problematic fortechnology in-use.
We faced a number ofchallenges.
Note that in experimentalconditions, subjects perform, on full andreduced versions.
One challenge was tobaseline non-intrusively the current (non-summary) full text review process.
A secondwas to measure both accuracy and efficiencygains for users performing on the job.
Thesechallenges were further exacerbated by thefact that users in an indicative task primarilyuse a summary to eliminate most documents.They have developed effective skimming andscanning techniques and are already quiteefficient at this task..
In short, our experience showed thattechnologists deploying single documentsummarization capability are likely beconstrained by the following factors:?
?
the ease of technology use?
the type of user information eed?
how effective the user performs the taskwithout he technology.3.4.2 User SupportInsight 2: Users require more than just a goodsummary.
They require the right level oftechnology support,Although the bulk of the research workstill continues to focus on summarizationalgorithms, we now appreciate the importanceof user support o text summarization use.The SRA software was quite robust and fast.53The task of judging relevance with a summary(even a machine generated one) instead of thefull text version does not require a user toacquire a fundamentally different workpractice.
Yet our system was not apparentlysufficiently supporting tool navigation.
One ofthe reasons was that our on-line help was notdeveloped from a user perspective and wasrarely accessed.
Another was that browse andview features did not maximize performance.For example, the interface mployed a scrollbar for viewing summaries rather than moreeffective Next Or Previous buttons.
Usersfrequently asked the same questions, but wewere answering them individually.Terminology clear to the technologists wasnot understood by users.
We also noticed thatthough there were requirements forimprovement of summarization quality, manyrequirements were associated with these usersupport issues.One of the more unexpected findingswas the under-utilization f tailoring features.The system offered the user many ways totailor summaries to their individual needs, yetmost users simply relied on default set-ups.Observations revealed little understanding ofthe configurable features and how thesefeatures corresponded to user needs to saynothing of how the algorithm worked.
Someusers did not understand the differencebetween the two summary types or sortingeffects with query-based summary selection.Non-traditional summary types--indicatorsand named entities--did not appear to helprender a relevance judgment.
We came tounderstand that just because technologists seesthe value to these features does not mean thata user will or that the features, in fact, haveutility.3.5 Technology-related Modifications3.5.1 User-centered Changes to TechnologyWork PracticesOn technology performance, welearned that?
seamless integration with an IR systemwas preferred?
users with static queries were more likelycustomers for a summary service?
gains in efficiency are hard to measure fora task already efficiently performed in areal-world situations.In response, we have established a summaryservice in which retrieval results are directlyrouted to our summary server and await theuser.
We plan to integrate the summarizationtool into the IR system.
(Uploading batchesand then submission to the server is still anoption.)
We also abandoned the naive ideathat data overload equates to summarizationrequirements and realized that the technologydoes not apply to all users.
We have moreeffectively selected users by profilingcharacteristics of active ,users (e.g.
dailydocument viewing work practice, documentvolume, static query use, etc.)
and haveprioritized deployment o that populationwhich could most benefit from it.In order to demonstrate tool~summarization efficiency, we needed to:baseline full-text review.
We considered, but.
rejected a number of options--user self-reportand timing, observations, and even the?
creation of a viewing tool to monitor anddocument full text review.
Instead, weba, selined full text scanning throughinformation retrieval ogs for a subgroup ofusers by tracking per document viewing timefor a month period.
These users submit thesame queries daily and view their documentsthrough the IR system browser.
For theheaviest system users, 75% of the documentswere viewed in under 20 seconds perdocument, but note that users vary widelywith a tendency to spend a much longerbrowse time on a relatively small number ofdocuments.
We then identified a subgroup ofthese users and attempted to deploy thesummarizer to this baseline group to compare54scanning time required over a similar timeframe.
We are currently analyzing these data.System in a work environment isconsidered a good indicator of tool utility, butwe wanted some gauge of summary qualityand also anticipated user concerns about anemerging technology like automatic textsummarization.
We compromised andselected a method to measure theeffectiveness of our summaries that serves adual purpose--our users gain confidence in theutility of the summaries and we can collectand measure the effectiveness of the genericsummaries for some of our users on their data.We initially piloted and now haveincorporated a data collection procedure intoour software.
In our on-line training, weguide users to explore tool capabilitiesthrough a series of experiments or tasks.
In thefirst of these tasks, a user is asked to submit abatch for summarization, then for each of fiveto seven user-selected summaries to recordanswers to the question:"Is this document likely to be relevant ome?
"(based on the summary)~.yes  noThen, the  user was directed to open theoriginal documents for each of the summariesand record answers to the question:"Is the document relevant to me?
"(after eading the original text)yes noIn a prototype collection effort, weasked users to review the first ten documents,but in follow-on interviews the usersrecommended review of fewer documents.
Weunderstand the limits this places oninterpreting our data.
Also, the on-line trainingis optional so we are not able to collect thesedata for all our users uniformly.Most of the users tested exhibited bothhigh recall and precision, with six usersjudging relevance correctly for all documents(in Table 2 below).
The False Negative errorwas high for only one user, while the majorityof - the users exhibited no False NegativeIIIIIIIIIIIIIIIIIIITable 2: Relevance Classes by UserUser5 04 0I234517891011121314True False True FalsePositive Positive Negative Negative5 0 0 04715 04 004502000000 00 10 00 10 2~000001 0 2 20 1 6 0I 1 4errors, a worse error to commit han wastingtime viewing irrelevant data, False Positive.Across all the users, 79% of all relevantdocuments and 81% of the irrelevantdocuments were accurately categorized byexamination of the summary.3.5.2 User-centered Changes inSupportOn user support, we learned thatUserour system did not effectively support usertool navigation?
our users did not fully exploit systemtailorable featuresIn response, we addressed user support needsfrom three different angles, each of which wediscuss below: incorporation of ElectronicPerformance Support Systems, design andimplementation f procedural on-line trainingand guided discovery training, and useranalysis of summary quality.Electronic Performance SupportSystems (EPSS) is a widely acknowledgedstrategy for on the job performance support.Defined as "an optimized body of co-ordinated on-line methods and resources thatenable and maintain a person's or anorganization's performance," EPSSinterventions range from simple help systemsto intelligent wizard-types of support.
(Villachica and Stone, 1999; Gery 1991).
Weelected to incorporate EPSS rather thancl~issroom instruction.
Based on an analysis oftool usage data, user requirements, and userobservations, experts in interface design andtechnology performance support prototypedan EPSS enhanced interface.
Active systemusers reviewed these changes beforeimplementation.
The on-line perfomancesupport available at all times includes ystemfeature procedures, a term glossary, FAQ, anda new interface design.With incorporation of the EPSS, wealso addressed the under-utilization of theconfigurable features.
Although simpletechnologies with few options such astraditional telephones do not requireconceptual system understanding for effectiveuse, more complex systems with multipleoptions are often underutilized whensupported with procedural training alone.
Wedecided to incorporate both proceduraltraining in a "Getting Started" tutorial andconceptual training in "The Lab."
In "GettingStarted", users learn basic system actions (e.g.,creating set-ups, submitting batches forsummarization, viewing summaries).
"TheLabi', on the other hand, supports guideddiscovery training in which users explore thesystem through a series of experiments inwhich they use their own data against varioustool options and record their observations.Given our own experience with under-utilization and research reporting difficultieswith unguided exploratory learning (Hsu et55al., 1993; Tuovinen and Sweller, 1999), webuilt on the work of de Mul and VanOostendorf (1996) and Van Oostendorf and deMul (1999) and their finding that task-orientedexploratory support leads to more effectivelearning of computer systems.
We created aseries of experiments hat the user conducts todiscover how the summarization technologycan best meet their needs.
For example, usersare directed to change summary length and todetermine for themselves how the variation.affects their ability to judge relevance usingtheir data.In February, we conducted a study of.two groups, one with the EPSS and "GettingStarting" Tutorial and a second with the samelevel of support and additionally "The Lab".Earlier work by Kieras and Bovair (1984)compared straight procedural training withconceptual training and showed that theconceptually trained users made more efficientuse of system features.
The goal of our studywas to determine just what level of trainingsupport the summarization technologyrequires for effective use.
Through surveys,we planned to collect attitudes toward the tooland training and through web logs, tool usagedata and option trials.
We also planned toassess the users' understanding of the featuresand benefits of the tool.
We are currentlyanalyzing these data.In addition to the EPSS and the on-linetraining, we developed a method for takinginto account user assessment of our summaryquality in a systematic way.
User feedback onsummarization quality during the beta testwas far too general and uneven.
We recruitedtwo users to join our technology team andbecome informed rather than the typical naiveusers.
They designed an analysis tool throughwhich they database problematic machinegenerated summaries and assign them to error-type categories.
Though we expected users toaddress issues like summary coherence, theyhave identified categories like the following:?
sentence identification errors?
formatting errors?
sentence xtraction due to the "rare" wordphenomena?
sentence xtraction in "long" documents?
failure to identify abstracts when availableWe expect hat this approach can complementa technology-driven one by helping usprioritize changes we need based onmethodical data collection and analysis.4.0 SummaryOur experience with text summarizationtechnology in-use has been quite sobering.
Inthis paper, we have shown how beta testing anemerging technology has helped us tounderstand that for technology to enhance jobperformance many factors besides thealgorithm need to be addressed.5.0 ReferencesAone, C., Gorlinsky, J. and Okurowski, M.E.1997.
Trainable, scalable summarizationusing robust NLP.
In Intelligent ScalableText Summarization.
Madrid, Spain:Association of Computational Linguistics,pages 66-73.Aone, C., Gorlinsky, J. and Okurowski, M.E.1998.
Trainable scalable summarizationusing robust NLP and machine learning.
InColing-A CL 98.
Montreal, Quebec,Canada, pages 62-66.Aone, C., Gorlinsky, J., Larsen, B. andOkurowski, M.E.
1999.
A trainablesummarizer with knowledge acquiredfrom robust NLP techniques.
In Mani, I.and Maybury, M.
(eds.
), Advances inAutomatic Text Summarization.
pages 71-80, Cambridge, Massachusetts: MIT Press.Boguraev, B., Kennedy, C., Bellamey, R.,Brawer, S., Wong, Y.Y.
and Swartz, J.1998.
Dynamic presentation of documentcontent for rapid on-line skimming.Intelligent Text Summarization.
(Papers56from the 1998 AAAI Spring SymposiumTechnical Report SS-98-06), pages 109-118.Brandow, R., Mitze, K. and Rau, L. 1994.Automatic condensation of electronicpublications by sentence selection.Information Processing and Management,31(5):675-685.Cowie, J., Mahesh, K., Nirenburg, S. and.Zajac, R., 1998.
MINDS--Multi-lingualINteractive document summarization.Intelligent Text Summarization.
(Papers,from the 1998 AAAI Spring SymposiumTechnical Report SS-98-06), pages 122-123.de Mul, S. and van Oostendorp, H. 1996.Learning user interfaces by exploration.Acta Psychologica, 91:325-344:Firmin, T. and Chrzanowski, M. 1999.
Anevaluation of automatic textsummarization.
I  Mani, I. and Maybury,M.(eds.
), Advances in Automatic TextSummarization.
pages 325-336,Cambridge, Massachusetts: MIT Press.Futrelle, R. 1998.
Summarization ofdocuments that include graphics.Intelligent Text Summarization.
(Papers.from the 1998 AAAI Spring SymposiumTechnical Report SS-98-06), pages 92-101.Ger;y, G. 1991.
Electronic performancesupport systems: How and why to remakethe workplace through the stratgicapplication of technology.
Tolland, MA:Gery Performance Press.Harman, D.K.
1996.
The Fourth TextREtrieval Conference (TREC-4).
NationalInstitute of Standards and TechnologySpecial Publication, pages 500-236.Hsu, J-F., Chapelle, C. and Thompson, A.,1993.
Exploratory learning environments:What are they and do students explore?Journal Educational Computing Research,9(1): 1-15.Jing, H., McKeown, K., Barzilay, R. andElhadad, M. 1998.
Summarizationevaluation methods: Experiments andmethods.
Intelligent Text Summarization.
(Papers from the 1998 AAAI SpringSymposium Technical Report SS-98-06),pages 51-59.Kieras, D.E.
and Bovair, S. 1984.
The role ofa mental model in learning to operate adevice.
Cognitive Science, (8), 1-17.Mani, I., House, D., Klein, G., Hirschman, L.,Firmin, T. and Sundheim, B.
1999.
TheTIPSTER SUMMAC Text SummarizationEvaluation.
In Proceedings of EACL99Ninth Conference of the EuropeanChapter of the Association forComputational Linguistics.
pages 77-83.McKeown, K. and Radev, D. 1995.Generating summaries of multiple newsarticles.
In Proceedings of the 18th AnnualInternational SIGIR Conference onResearch and Development i  InformationRetrieval.
pages 74-78.McKeown, K. Jordan, D. aridHatzivassiloglou, V. 1998.
Generatingpatient specific summaries on onlineliterature.
Intelligent Text Summarization.
(Papers from the 1998' AAAI SpringSymposium Technical Report SS-98-06),pages 34-43.Merlino, A. and Maybury, M. 1999.
Anempirical study of the optimal presentationof multi-media summaries of broadcastnews.
In Mani, I. and Maybury, M.
(eds.
),Advances in Automatic Text57Summarization.
pages 391-401,Cambridge, Massachusetts: MIT Press.Morris, A., Kasper, G., and Adams, D. 1999.The effects and limitations of automatedtext condensing on reading comprehensionperformance.
In Mani, I. and Maybury,M.
(eds.
), Advances in Automatic TextSummarization.
pages 305-323,Cambridge, Massachusetts: MIT Press.Paice, C.D., 1990.
Constructing literatureabstracts by computer: Techniques andprospects.
Information Processing and~Management, 26(1): 171-186.Sparck-Jones, K. 1993.
What might be in asummary?
In Information Retrieval 93:Von der Modellierung zur Anwendung,pages 9-26.Salton, G., Singhal, A., Mitra, M. andBuckely, C. 1999.
Automatic textstructuring and summarization.
I  Mani, I.and Maybury, M.
(eds.
), Advances inAutomatic Text Summarization.
pages 342-: 355, Cambridge, Massachusetts: MITPress.Strzalkowski, T., Wang, J. and Wise, B.,1998.
A robust practical textsummarization.
Intelligent Text,Summarization.
(Papers from the 1998AAAI Spring Symposium TechnicalReport SS-98-06), pages 26-33.Tombros, A., and Sanderson, M. 1998.Advantages of query-based summaries ininformation retrieval.
In Proceedings ofthe 21st ACM S1GIR Conference(SIGIR98).
pages 2-10.Tuovinen, J. and Sweller, J.
1999.
Acomparison of cognitive load associatedwith discovery learning and worked58examples.
Journal ofPsychology, 9(2):334-341.EducationalVan Oostendorp, H. and de Mul, S. 1999.Learning by exploring: Thinking aloudwhile exploring an information system.Instruction Science, 27:269-284.Villachica, S.W.
and Stone, D. 1999.Performance support systems.
InStolovitch, H.D.
and Keeps, K.J., (eds.
),Handbook of Human PerformanceTechnology.
San Francisco: Jossey-Bass' Pfeiffer.Wasson, M. 1998.
Using leading text for newssummaries: Evaluation results andimplications for commercialsummarization.
In Coling-A CL 98.Montreal, Quebec, Canada.
pages 1364-1368.
