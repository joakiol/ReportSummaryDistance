Example-Based Metonymy Recognition for Proper NounsYves PeirsmanQuantitative Lexicology and Variational LinguisticsUniversity of Leuven, Belgiumyves.peirsman@arts.kuleuven.beAbstractMetonymy recognition is generally ap-proached with complex algorithms thatrely heavily on the manual annotation oftraining and test data.
This paper will re-lieve this complexity in two ways.
First,it will show that the results of the cur-rent learning algorithms can be replicatedby the ?lazy?
algorithm of Memory-BasedLearning.
This approach simply stores alltraining instances to its memory and clas-sifies a test instance by comparing it to alltraining examples.
Second, this paper willargue that the number of labelled trainingexamples that is currently used in the lit-erature can be reduced drastically.
Thisfinding can help relieve the knowledge ac-quisition bottleneck in metonymy recog-nition, and allow the algorithms to be ap-plied on a wider scale.1 IntroductionMetonymy is a figure of speech that uses ?one en-tity to refer to another that is related to it?
(Lakoffand Johnson, 1980, p.35).
In example (1), for in-stance, China and Taiwan stand for the govern-ments of the respective countries:(1) China has always threatened to use forceif Taiwan declared independence.
(BNC)Metonymy resolution is the task of automaticallyrecognizing these words and determining their ref-erent.
It is therefore generally split up into twophases: metonymy recognition and metonymy in-terpretation (Fass, 1997).The earliest approaches to metonymy recogni-tion identify a word as metonymical when it vio-lates selectional restrictions (Pustejovsky, 1995).Indeed, in example (1), China and Taiwan bothviolate the restriction that threaten and declarerequire an animate subject, and thus have to beinterpreted metonymically.
However, it is clearthat many metonymies escape this characteriza-tion.
Nixon in example (2) does not violate the se-lectional restrictions of the verb to bomb, and yet,it metonymically refers to the army under Nixon?scommand.
(2) Nixon bombed Hanoi.This example shows that metonymy recognitionshould not be based on rigid rules, but ratheron statistical information about the semantic andgrammatical context in which the target word oc-curs.This statistical dependency between the read-ing of a word and its grammatical and seman-tic context was investigated by Markert and Nis-sim (2002a) and Nissim and Markert (2003;2005).
The key to their approach was the in-sight that metonymy recognition is basically a sub-problem of Word Sense Disambiguation (WSD).Possibly metonymical words are polysemous, andthey generally belong to one of a number of pre-defined metonymical categories.
Hence, like WSD,metonymy recognition boils down to the auto-matic assignment of a sense label to a polysemousword.
This insight thus implied that all machinelearning approaches to WSD can also be applied tometonymy recognition.There are, however, two differences betweenmetonymy recognition and WSD.
First, theo-retically speaking, the set of possible readingsof a metonymical word is open-ended (Nunberg,1978).
In practice, however, metonymies tend tostick to a small number of patterns, and their la-bels can thus be defined a priori.
Second, classic71WSD algorithms take training instances of one par-ticular word as their input and then disambiguatetest instances of the same word.
By contrast, sinceall words of the same semantic class may undergothe same metonymical shifts, metonymy recogni-tion systems can be built for an entire semanticclass instead of one particular word (Markert andNissim, 2002a).To this goal, Markert and Nissim extractedfrom the BNC a corpus of possibly metonymicalwords from two categories: country names(Markert and Nissim, 2002b) and organizationnames (Nissim and Markert, 2005).
All thesewords were annotated with a semantic label?
either literal or the metonymical cate-gory they belonged to.
For the country names,Markert and Nissim distinguished betweenplace-for-people, place-for-eventand place-for-product.
For the organi-zation names, the most frequent metonymiesare organization-for-members andorganization-for-product.
In addition,Markert and Nissim used a label mixed forexamples that had two readings, and othermetfor examples that did not belong to any of thepre-defined metonymical patterns.For both categories, the results were promis-ing.
The best algorithms returned an accuracy of87% for the countries and of 76% for the orga-nizations.
Grammatical features, which gave thefunction of a possibly metonymical word and itshead, proved indispensable for the accurate recog-nition of metonymies, but led to extremely lowrecall values, due to data sparseness.
ThereforeNissim and Markert (2003) developed an algo-rithm that also relied on semantic information, andtested it on the mixed country data.
This algo-rithm used Dekang Lin?s (1998) thesaurus of se-mantically similar words in order to search thetraining data for instances whose head was sim-ilar, and not just identical, to the test instances.Nissim and Markert (2003) showed that a combi-nation of semantic and grammatical informationgave the most promising results (87%).However, Nissim and Markert?s (2003) ap-proach has two major disadvantages.
The first ofthese is its complexity: the best-performing al-gorithm requires smoothing, backing-off to gram-matical roles, iterative searches through clusters ofsemantically similar words, etc.
In section 2, I willtherefore investigate if a metonymy recognition al-gorithm needs to be that computationally demand-ing.
In particular, I will try and replicate Nissimand Markert?s results with the ?lazy?
algorithm ofMemory-Based Learning.The second disadvantage of Nissim and Mark-ert?s (2003) algorithms is their supervised nature.Because they rely so heavily on the manual an-notation of training and test data, an extension ofthe classifiers to more metonymical patterns is ex-tremely problematic.
Yet, such an extension is es-sential for many tasks throughout the field of Nat-ural Language Processing, particularly MachineTranslation.
This knowledge acquisition bottle-neck is a well-known problem in NLP, and manyapproaches have been developed to address it.
Oneof these is active learning, or sample selection, astrategy that makes it possible to selectively an-notate those examples that are most helpful to theclassifier.
It has previously been applied to NLPtasks such as parsing (Hwa, 2002; Osborne andBaldridge, 2004) and Word Sense Disambiguation(Fujii et al, 1998).
In section 3, I will introduceactive learning into the field of metonymy recog-nition.2 Example-based metonymy recognitionAs I have argued, Nissim and Markert?s (2003)approach to metonymy recognition is quite com-plex.
I therefore wanted to see if this complexitycan be dispensed with, and if it can be replacedwith the much more simple algorithm of Memory-Based Learning.
The advantages of Memory-Based Learning (MBL), which is implemented inthe TiMBL classifier (Daelemans et al, 2004)1, aretwofold.
First, it is based on a plausible psycho-logical hypothesis of human learning.
It holdsthat people interpret new examples of a phenom-enon by comparing them to ?stored representa-tions of earlier experiences?
(Daelemans et al,2004, p.19).
This contrasts to many other classi-fication algorithms, such as Naive Bayes, whosepsychological validity is an object of heavy de-bate.
Second, as a result of this learning hypothe-sis, an MBL classifier such as TiMBL eschews theformulation of complex rules or the computationof probabilities during its training phase.
Insteadit stores all training vectors to its memory, togetherwith their labels.
In the test phase, it computes thedistance between the test vector and all these train-1This software package is freely available and can bedownloaded from http://ilk.uvt.nl/software.html.72ing vectors, and simply returns the most frequentlabel of the most similar training examples.One of the most important challenges inMemory-Based Learning is adapting the algorithmto one?s data.
This includes finding a represen-tative seed set as well as determining the rightdistance measures.
For my purposes, however,TiMBL?s default settings proved more than satis-factory.
TiMBL implements the IB1 and IB2 algo-rithms that were presented in Aha et al (1991), butadds a broad choice of distance measures.
Its de-fault implementation of the IB1 algorithm, whichis called IB1-IG in full (Daelemans and Van denBosch, 1992), proved most successful in my ex-periments.
It computes the distance between twovectors X and Y by adding up the weighted dis-tances ?
between their corresponding feature val-ues xi and yi:?
(X,Y ) =n?i=1wi?
(xi, yi)(3)The most important element in this equation is theweight that is given to each feature.
In IB1-IG,features are weighted by their Gain Ratio (equa-tion 4), the division of the feature?s InformationGain by its split info.
Information Gain, the nu-merator in equation (4), ?measures how much in-formation it [feature i] contributes to our knowl-edge of the correct class label [...] by comput-ing the difference in uncertainty (i.e.
entropy) be-tween the situations without and with knowledgeof the value of that feature?
(Daelemans et al,2004, p.20).
In order not ?to overestimate the rel-evance of features with large numbers of values?
(Daelemans et al, 2004, p.21), this InformationGain is then divided by the split info, the entropyof the feature values (equation 5).
In the followingequations, C is the set of class labels, H(C) is theentropy of that set, and Vi is the set of values forfeature i.wi =H(C)?
?v?Vi P (v)?H(C|v)si(i)(4)si(i) = ?
?v?ViP (v)log2P (v)(5)The IB2 algorithm was developed alongside IB1in order to reduce storage requirements (Aha etal., 1991).
It iteratively saves only those instancesthat are misclassified by IB1.
This is because thesewill likely lie close to the decision boundary, andhence, be most informative to the classifier.
Myexperiments showed, however, that IB2?s best per-formance lay more than 2% below that of IB1.
Itwill therefore not be treated any further here.2.1 Experiments with grammaticalinformation onlyIn order to see if Memory-Based Learning is ableto replicate Nissim and Markert?s (2003; 2005) re-sults, I used their corpora for a number of experi-ments.
These corpora consist of one set with about1000 mixed country names, another with 1000 oc-currences of Hungary, and a final set with about1000 mixed organization names.2 Evaluation wasperformed with ten-fold cross-validation.The first round of experiments used only gram-matical information.
The experiments for the lo-cation data were similar to Nissim and Mark-ert?s (2003), and took the following features intoaccount:?
the grammatical function of the word (subj,obj, iobj, pp, gen, premod, passive subj,other);?
its head;?
the presence of a second head;?
the second head (if present).The experiments for the organization names usedthe same features as Nissim and Markert (2005):?
the grammatical function of the word;?
its head;?
its type of determiner (if present) (def, indef,bare, demonst, other);?
its grammatical number (sing, plural);?
its number of grammatical roles (if present).The number of words in the organization name,which Nissim and Markert used as a sixth and fi-nal feature, led to slightly worse results in my ex-periments and was therefore dropped.The results of these first experiments clearlybeat the baselines of 79.7% (countries) and 63.4%(organizations).
Moreover, despite its extremely2This data is publicly available and can be downloadedfrom http://homepages.inf.ed.ac.uk/mnissim/mascara.73Acc P R FTiMBL 86.6% 80.2% 49.5% 61.2%N&M 87.0% 81.4% 51.0% 62.7%Table 1: Results for the mixed country data.TiMBL: my TiMBL resultsN&M: Nissim and Markert?s (2003) resultssimple learning phase, TiMBL is able to replicatethe results from Nissim and Markert (2003; 2005).As table 1 shows, accuracy for the mixed coun-try data is almost identical to Nissim and Mark-ert?s figure, and precision, recall and F-score forthe metonymical class lie only slightly lower.3TiMBL?s results for the Hungary data were simi-lar, and equally comparable to Markert and Nis-sim?s (Katja Markert, personal communication).Note, moreover, that these results were reachedwith grammatical information only, whereas Nis-sim and Markert?s (2003) algorithm relied on se-mantics as well.Next, table 2 indicates that TiMBL?s accuracyfor the mixed organization data lies about 1.5% be-low Nissim and Markert?s (2005) figure.
This re-sult should be treated with caution, however.
First,Nissim and Markert?s available organization datahad not yet been annotated for grammatical fea-tures, and my annotation may slightly differ fromtheirs.
Second, Nissim and Markert used severalfeature vectors for instances with more than onegrammatical role and filtered all mixed instancesfrom the training set.
A test instance was treated asmixed only when its several feature vectors wereclassified differently.
My experiments, in contrast,were similar to those for the location data, in thateach instance corresponded to one vector.
Hence,the slightly lower performance of TiMBL is prob-ably due to differences between the two experi-ments.These first experiments thus demonstrate thatMemory-Based Learning can give state-of-the-artperformance in metonymy recognition.
In this re-spect, it is important to stress that the results forthe country data were reached without any se-mantic information, whereas Nissim and Mark-ert?s (2003) algorithm used Dekang Lin?s (1998)clusters of semantically similar words in orderto deal with data sparseness.
This fact, together3Precision, recall and F-score are given for the metonymi-cal class only, since this is the category that metonymy recog-nition is concerned with.Acc P R FTiMBL 74.63% 78.65% 55.53% 65.10%N&M 76.0% ?
?
?Table 2: Results for the mixed organization data.TiMBL: my TiMBL resultsN&M: Nissim and Markert?s (2005) resultswith the psychological plausibility and the simplelearning phase, adds to the attractivity of Memory-Based Learning.2.2 Experiments with semantic andgrammatical informationIt is still intuitively true, however, that the inter-pretation of a possibly metonymical word dependsmainly on the semantics of its head.
The ques-tion is if this information is still able to improvethe classifier?s performance.
I therefore performeda second round of experiments with the locationdata, in which I also made use of semantic infor-mation.
In this round, I extracted the hypernymsynsets of the head?s first sense from WordNet.WordNet?s hierarchy of synsets makes it possibleto quantify the semantic relatedness of two words:the more hypernyms two words share, the moreclosely related they are.
I therefore used the tenhighest hypernyms of the first head as features 5to 14.
For those heads with fewer than ten hyper-nyms, a copy of their lowest hypernym filled the?empty?
features.
As a result, TiMBL would firstlook for training instances with ten identical hy-pernyms, then with nine, etc.
It would thus com-pare the test example to the semantically most sim-ilar training examples.However, TiMBL did not perform better withthis semantic information.
Although F-scores forthe metonymical category went up slightly, thesystem?s accuracy hardly changed.
This result wasnot due to the automatic selection of the first (mostfrequent) WordNet sense.
By manually disam-biguating all the heads in the training and test setof the country data, I observed that this first sensewas indeed often incorrect, but that choosing thecorrect sense did not lead to a more robust system.Clearly, the classifier did not benefit from Word-Net information as Nissim and Markert?s (2003)did from Lin?s (1998) thesaurus.The learning curves for the country set alowus to compare the two types of feature vectors74Figure 1: Accuracy learning curves for the mixedcountry data with and without semantic informa-tion.in more detail.4 As figure 1 indicates, with re-spect to overall accuracy, semantic features havea negative influence: the learning curve with bothfeatures climbs much more slowly than that withonly grammatical features.
Hence, contrary to myexpectations, grammatical features seem to allowa better generalization from a limited number oftraining instances.
With respect to the F-score onthe metonymical category in figure 2, the differ-ences are much less outspoken.
Both features givesimilar learning curves, but semantic features leadto a higher final F-score.
In particular, the use ofsemantic features results in a lower precision fig-ure, but a higher recall score.
Semantic featuresthus cause the classifier to slightly overgeneralizefrom the metonymic training examples.There are two possible reasons for this inabil-ity of semantic information to improve the clas-sifier?s performance.
First, WordNet?s synsets donot always map well to one of our semantic la-bels: many are rather broad and allow for severalreadings of the target word, while others are toospecific to make generalization possible.
Second,there is the predominance of prepositional phrasesin our data.
With their closed set of heads, thenumber of examples that benefits from semanticinformation about its head is actually rather small.Nevertheless, my first round of experiments hasindicated that Memory-Based Learning is a sim-ple but robust approach to metonymy recogni-tion.
It is able to replace current approaches thatneed smoothing or iterative searches through a the-saurus, with a simple, distance-based algorithm.4These curves were obtained by averaging the results of10 experiments.
They show performance on a test set of 40%of the data, with the other 60% as training data.Figure 2: F-score learning curves for the mixedcountry data with and without semantic informa-tion.Moreover, in contrast to some other successfulclassifiers, it incorporates a plausible hypothesisof human learning.3 Distance-based sample selectionThe previous section has shown that a simple algo-rithm that compares test examples to stored train-ing instances is able to produce state-of-the-art re-sults in the field of metonymy recognition.
Thisleads to the question of how many examples weactually need to arrive at this performance.
Af-ter all, the supervised approach that we exploredrequires the careful manual annotation of a largenumber of training instances.
This knowledge ac-quisition bottleneck compromises the extrapola-tion of this approach to a large number of seman-tic classes and metonymical patterns.
This sectionwill therefore investigate if it is possible to auto-matically choose informative examples, so that an-notation effort can be reduced drastically.For this round of experiments, two smallchanges were made.
First, since we are focusingon metonymy recognition, I replaced all specificmetonymical labels with the label met, so thatonly three labels remain: lit, met and mixed.Second, whereas the results in the previous sectionwere obtained with ten-fold cross-validation, I ranthese experiments with a training and a test set.On each run, I used a random 60% of the data fortraining; 40% was set aside for testing.
All curvesgive the average of twenty test runs that use gram-matical information only.In general, sample selection proceeds on thebasis of the confidence that the classifier has inits classification.
Commonly used metrics are theprobability of the most likely label, or the entropy75Figure 3: Accuracy learning curves for the coun-try data with random and maximum-distance se-lection of training examples.over all possible labels.
The algorithm then picksthose instances with the lowest confidence, sincethese will contain valuable information about thetraining set (and hopefully also the test set) that isstill unknown to the system.One problem with Memory-Based Learning al-gorithms is that they do not directly output prob-abilities.
Since they are example-based, they canonly give the distances between the unlabelled in-stance and all labelled training instances.
Never-theless, these distances can be used as a measureof certainty, too: we can assume that the systemis most certain about the classification of test in-stances that lie very close to one or more of itstraining instances, and less certain about those thatare further away.
Therefore the selection functionthat minimizes the probability of the most likelylabel can intuitively be replaced by one that max-imizes the distance from the labelled training in-stances.However, figure 3 shows that for the mixedcountry instances, this function is not an option.Both learning curves give the results of an algo-rithm that starts with fifty random instances, andthen iteratively adds ten new training instances tothis initial seed set.
The algorithm behind the solidcurve chooses these instances randomly, whereasthe one behind the dotted line selects those thatare most distant from the labelled training exam-ples.
In the first half of the learning process, bothfunctions are equally successful; in the second thedistance-based function performs better, but onlyslightly so.There are two reasons for this bad initial per-formance of the active learning function.
First, itis not able to distinguish between informative andFigure 4: Accuracy learning curves for the coun-try data with random and maximum/minimum-distance selection of training examples.unusual training instances.
This is because a largedistance from the seed set simply means that theparticular instance?s feature values are relativelyunknown.
This does not necessarily imply thatthe instance is informative to the classifier, how-ever.
After all, it may be so unusual and so badlyrepresentative of the training (and test) set that thealgorithm had better exclude it ?
something thatis impossible on the basis of distances only.
Thisbias towards outliers is a well-known disadvantageof many simple active learning algorithms.
A sec-ond type of bias is due to the fact that the data hasbeen annotated with a few features only.
More par-ticularly, the present algorithm will keep addinginstances whose head is not yet represented in thetraining set.
This entails that it will put off addinginstances whose function is pp, simply becauseother functions (subj, gen, .
.
. )
have a widervariety in heads.
Again, the result is a labelled setthat is not very representative of the entire trainingset.There are, however, a few easy ways to increasethe number of prototypical examples in the train-ing set.
In a second run of experiments, I used anactive learning function that added not only thoseinstances that were most distant from the labelledtraining set, but also those that were closest to it.After a few test runs, I decided to add six distantand four close instances on each iteration.
Figure 4shows that such a function is indeed fairly success-ful.
Because it builds a labelled training set that ismore representative of the test set, this algorithmclearly reduces the number of annotated instancesthat is needed to reach a given performance.Despite its success, this function is obviouslynot yet a sophisticated way of selecting good train-76Figure 5: Accuracy learning curves for the organi-zation data with random and distance-based (AL)selection of training examples with a random seedset.ing examples.
The selection of the initial seed setin particular can be improved upon: ideally, thisseed set should take into account the overall dis-tribution of the training examples.
Currently, theseeds are chosen randomly.
This flaw in the al-gorithm becomes clear if it is applied to anotherdata set: figure 5 shows that it does not outper-form random selection on the organization data,for instance.As I suggested, the selection of prototypical orrepresentative instances as seeds can be used tomake the present algorithm more robust.
Again, itis possible to use distance measures to do this: be-fore the selection of seed instances, the algorithmcan calculate for each unlabelled instance its dis-tance from each of the other unlabelled instances.In this way, it can build a prototypical seed setby selecting those instances with the smallest dis-tance on average.
Figure 6 indicates that such analgorithm indeed outperforms random sample se-lection on the mixed organization data.
For thecalculation of the initial distances, each feature re-ceived the same weight.
The algorithm then se-lected 50 random samples from the ?most proto-typical?
half of the training set.5 The other settingswere the same as above.With the present small number of features, how-ever, such a prototypical seed set is not yet alaysas advantageous as it could be.
A few experimentsindicated that it did not lead to better performanceon the mixed country data, for instance.
However,as soon as a wider variety of features is taken intoaccount (as with the organization data), the advan-5Of course, the random algorithm in fi gure 6 still ran-domly selected its seeds from the entire training set.Figure 6: Accuracy learning curves for the organi-zation data with random and distance-based (AL)selection of training examples with a prototypicalseed set.tages of a prototypical seed set will definitely be-come more obvious.In conclusion, it has become clear that a carefulselection of training instances may considerablyreduce annotation effort in metonymy recognition.Functions that construct a prototypical seed setand then use MBL?s distance measures to select in-formative as well as typical samples are extremelypromising in this respect and can already consid-erably reduce annotation effort.
In order to reachan accuracy of 85% on the country data, for in-stance, the active learning algorithm above needs44% fewer training instances than its random com-petitor (on average).
On the organisation data, re-duction is typically around 30%.
These relativelysimple algorithms thus constitute a good basis forthe future development of robust active learningtechniques for metonymy recognition.
I believein particular that research in this field should gohand in hand with an investigation of new infor-mative features, since the present, limited featureset does not yet alow us to measure the classifier?sconfidence reliably.4 Conclusions and future workIn this paper I have explored an example-based ap-proach to metonymy recognition.
Memory-BasedLearning does away with the complexity of cur-rent supervised metonymy recognition algorithms.Even without semantic information, it is able togive state-of-the-art results similar to those in theliterature.
Moreover, not only is the complexity ofcurrent learning algorithms unnecessary; the num-ber of labelled training instances can be reduceddrastically, too.
I have argued that selective sam-77pling can help choose those instances that are mosthelpful to the classifier.
A few distance-based al-gorithms were able to drastically reduce the num-ber of training instances that is needed for a givenaccuracy, both for the country and the organizationnames.If current metonymy recognition algorithms areto be used in a system that can recognize all pos-sible metonymical patterns across a broad varietyof semantic classes, it is crucial that the requirednumber of labelled training examples be reduced.This paper has taken the first steps along this pathand has set out some interesting questions for fu-ture research.
This research should include theinvestigation of new features that can make clas-sifiers more robust and allow us to measure theirconfidence more reliably.
This confidence mea-surement can then also be used in semi-supervisedlearning algorithms, for instance, where the clas-sifier itself labels the majority of training exam-ples.
Only with techniques such as selective sam-pling and semi-supervised learning can the knowl-edge acquisition bottleneck in metonymy recogni-tion be addressed.AcknowledgementsI would like to thank Mirella Lapata, Dirk Geer-aerts and Dirk Speelman for their feedback on thisproject.
I am also very grateful to Katja Markertand Malvina Nissim for their helpful informationabout their research.ReferencesD.
W. Aha, D. Kibler, and M. K. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37?66.W.
Daelemans and A.
Van den Bosch.
1992.
Generali-sation performance of backpropagation learning on asyllabifi cation task.
In M. F. J. Drossaers and A. Ni-jholt, editors, Proceedings of TWLT3: Connection-ism and Natural Language Processing, pages 27?37,Enschede, The Netherlands.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2004.
TiMBL: TilburgMemory-Based Learner.
Technical report, Induction ofLinguistic Knowledge, Computational Linguistics,Tilburg University.D.
Fass.
1997.
Processing Metaphor and Metonymy.Stanford, CA: Ablex.A.
Fujii, K. Inui, T. Tokunaga, and H. Tanaka.1998.
Selective sampling for example-based wordsense disambiguation.
Computational Linguistics,24(4):573?597.R.
Hwa.
2002.
Sample selection for statistical parsing.Computational Linguistics, 30(3):253?276.G.
Lakoff and M. Johnson.
1980.
Metaphors We LiveBy.
London: The University of Chicago Press.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the International Con-ference on Machine Learning, Madison, USA.K.
Markert and M. Nissim.
2002a.
Metonymy res-olution as a classification task.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), Philadelphia,USA.K.
Markert and M. Nissim.
2002b.
Towards a cor-pus annotated for metonymies: the case of locationnames.
In Proceedings of the Third InternationalConference on Language Resources and Evaluation(LREC 2002), Las Palmas, Spain.M.
Nissim and K. Markert.
2003.
Syntactic featuresand word similarity for supervised metonymy res-olution.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics(ACL-03), Sapporo, Japan.M.
Nissim and K. Markert.
2005.
Learning to buy aRenault and talk to BMW: A supervised approachto conventional metonymy.
In H. Bunt, editor, Pro-ceedings of the 6th International Workshop on Com-putational Semantics, Tilburg, The Netherlands.G.
Nunberg.
1978.
The Pragmatics of Reference.Ph.D.
thesis, City University of New York.M.
Osborne and J. Baldridge.
2004.
Ensemble-basedactive learning for parse selection.
In Proceedingsof the Human Language Technology Conference ofthe North American Chapter of the Association forComputational Linguistics (HLT-NAACL).
Boston,USA.J.
Pustejovsky.
1995.
The Generative Lexicon.
Cam-bridge, MA: MIT Press.78
