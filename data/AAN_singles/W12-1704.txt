In: R. Levy & D. Reitter (Eds.
), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 41?50,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsFractal Unfolding: A Metamorphic Approach to Learning to ParseRecursive Structure?Whitney Taborwhitney.tabor@uconn.eduPyeong Whan Chopyeong.cho@uconn.eduEmily Szkudlarekemilyszkudlarek@gmail.comDepartment of Psychology and Cognitive Science ProgramUniversity of Connecticut406 Babbidge RoadStorrs, CT 06269-1020AbstractWe describe a computational framework forlanguage learning and parsing in which dy-namical systems navigate on fractal sets.
Weexplore the predictions of the framework inan artificial grammar task in which humansand recurrent neural networks are trained ona language with recursive structure.
The re-sults provide evidence for the claim of thedynamical systems models that grammaticalsystems continuously metamorphose duringlearning.
The present perspective permitsstructural comparison between the recursiverepresentations in symbolic and neural net-work models.1 IntroductionSome loci in the phrase structure systems of naturallanguages appear to employ center embedding re-cursion (Chomsky, 1957), or at least an approxima-tion of it (Christiansen and Chater, 1999).
For exam-ple, one can embed a clause within a clause in En-glish, using the object-extracted relative clause con-struction (e.g., the dog that the goat chased barked.
).But such recursion does not appear in every phraseand may not appear in every language (Everett,2005).
Therefore, the system that learns natural lan-guages must have a way of recognizing recursionwhen it occurs.
We are interested in the problem,?This material is based on work supported by the Na-tional Science Foundation Grant No.
1059662.
We thank themembers of SOLAB who helped run the experiment: OliviaHarold, Milod Kazerounian, Emily Pakstis, Bo Powers, KevinSemataska.How does a language learner, seeing only a finiteamount of data, decide on an unbounded recursiveinterpretation?Here, we use the term ?finite state?
to refer to asystem that can only be in a finite number of states.We use the term ?recursion?
to refer to situations inwhich multiple embeddings require the use of an un-bounded symbol memory to keep track of unfinisheddependencies.1 We focus here on the case of center-embedding recursion, which can be generated by acontext free grammar (one symbol on the left of eachrule, finitely many symbols on the right) or a push-down automaton (stack memory + finite state con-troller) but not by a finite state device (Hopcroft andUllman, 1979).One natural approach to the recursion recognitionproblem, recently explored by Perfors et al (2011),involves Bayesian grammar selection.
Perfors etal.
?s model considered a range of grammars, in-cluding both finite state and context free grammars.Their system, parameterized by data from English-speaking children in the Childes Database selecteda context free grammar.
Several features of this ap-proach are notable: (i) There is a rich set of struc-tural assumptions (the grammars in the pool of can-didates).
(ii) Because many plausible grammarsgenerate overlapping data sets, a complexity rankingis also assumed and the system operates under Oc-cam?s Razor: prefer simpler grammars.
(iii) Gram-mar selection and on-line parsing are treated as sep-1This is a narrow construal of the term ?recursion?.
Some-times the term is used for any situation in which a rule can beapplied arbitrarily many times in the generation of a single sen-tence, including finite-state cases.41arate problems in that the system is evaluated forcoverage of the observed sentences, but the partic-ular method of parsing plays no role in the selectionprocess.Here, we focus on a contrasting approach: recur-rent neural network models discover the structure ofgrammatical systems by sequentially processing thecorpus data, attempting to predict after each word,what word will come next (Elman, 1990; Elman,1991).
With respect to the properties mentionedabove, the neural network approach has some ad-vantages: (i) Formal analyses of some of the net-works and related systems (Moore, 1998; Siegel-mann, 1999; Tabor, 2009b) indicate that these mod-els make even richer structural assumptions than theBayesian approach: if the networks have infiniteprecision, then some of them recognize all stringlanguages, including non-computable ones.
For along while, theorists of cognition have adopted theview that positing a restrictive hypothesis space isdesirable?otherwise a theory of structure wouldseem to have little substance.
However, if one offersa hypothesis about the organization of the hypoth-esis space, and a principle that specifies the way alearning organism navigates in the space, then thetheory can still make strong, testable predictions.We suggest that assuming a very general functionclass is preferable to presupposing arbitrary gram-mar or class restrictions.
(ii) The recurrent networksdo not employ an independently defined complexitymetric.
Instead, the learning process successivelybreaks symmetries in the initially unbiased weightset, driven by asymmetries in the data.
The result isa bias toward simplicity.
We see this as an advantagein that the simplicity preference stems from the formof the architecture and learning mechanism.
(iii)Word-by-word parsing and grammar selection occuras part of a single process?the network updates itsweights every time it processes a word and this re-sults in the formation of a parsing system.
We seethis as an advantage in that the moment-to-momentinteraction of the system with data resembles the cir-cumstances of a learning child.On the other hand, there has long been a seri-ous difficulty with the network approach: the net-work dynamics and solutions have been very opaqueto analysis.
Although the systems sometimes learnwell and capture data effectively, they are not sci-entifically very revealing unless we can interpretthem.
The Bayesian grammar-selection approach ismuch stronger in this regard: the formal propertiesof the grammars employed are well understood andthe selection process is well-grounded in statisticaltheory?e.g., Griffiths et al (2010).Here, we take advantage of recent formal resultsindicating how recurrent neural networks can en-code abstract recursive structure (Moore, 1998; Pol-lack, 1987; Siegelmann, 1999; Tabor, 2000) An es-sential insight is that the network can use a spatialrecursive structure, a fractal, to encode the tempo-ral recursive structure of a symbol sequence.
Whenthe network is trained on short sentences exhibit-ing a few levels of embedding, it tends to general-ize to higher levels of embedding, suggesting thatit is not merely shaping itself to the training data,but discovers an abstract principle (Rodriguez et al,1999; Rodriguez, 2001; Tabor, 2003; Wiles and El-man, 1995).
During the course of learning, the frac-tal comes into being gradually in such a way thatlower-order finite-state approximations to the recur-sion develop before higher-order structure does?acomplexity cline phenomenon (Tabor, 2003).We examined human and neural network learningof a recursive language with an artificial grammarparadigm, the Box Prediction paradigm.
Whereasour previous investigations of this task (Cho etal., 2011) focused on counting recursion languages(only a single stack symbol is required to track therecursive dependencies), we provide evidence herefor mirror recursion learning by a few participants(multiple stack symbols required).
We show howthe theory of fractal grammars can be used to handwire a network that processes the recursive languageof our task.
We then provide evidence that a Sim-ple Recurrent Network (Elman, 1990; Elman, 1991),trained on the same task, also develops a fractal en-coding.
Moreover, the network shows evidence of aembodying a complexity-cline?similarly complexgrammars are adjacent in the parameter space.
Anindividual differences analysis indicates that a simi-lar pattern arises in the humans.
We conclude thatthe network encodings can be formally related tosymbolic recursive models, but are different in thatlearning occurs by continuous grammar metamor-phosis.422 The Box Prediction paradigmHuman participants sat in front of a computer screenon which five black outlines of boxes were displayed(Figure 1).
When the participant clicked on thescreen, one of the boxes changed color.
The taskwas to indicate, by clicking on it, which box wouldchange color next on each trial.
The sequence ofcolor changes corresponded to the structure of sen-tences generated by the center-embedding grammarin Table 1a.
The sentences can be divided into em-bedding level classes.
Level n sentences have (n-1) center-embedded clauses (Table 1b).
There werethree, distinct phases of the color-change sequence:during the first 60 trials, participants saw only Level1 sentences.
From trials 61 to 410, Level 2 sentenceswere introduced with increasing frequency.
We re-fer to these two phases of presentation together asthe ?Training Phase?.
Starting at Trial 411, Level3 sentences were included, along with more Level1 and Level 2 sentences.
We refer to the trialsfrom 411 to 553, the end of the experiment, as the?Test Phase?.
Other than by their structural differ-ences, these phases were not distinguished for theparticipants: the participants experienced them asone, long sequence of 553 trials.
We introducedthe deeper levels of embedding gradually becauseof evidence from the language acquisition litera-ture (Newport, 1990), from the connectionist liter-ature (Elman, 1993), and from the artificial gram-mar learning literature (Cho et al, 2011; Lai andPoletiek, 2011) that ?starting small?
facilitates learn-ing of complex syntactic structures.
Following stan-dard terminology, we call the trials in which boxes1 and 4 change colors ?push?
trials (because in anatural implementation of the grammar with a push-down automaton, the automaton pushes a symbolonto the stack at these trials).
We call the trials inwhich boxes 2, 3, and 5 change color ?pop?
trials.The push trials were fairly unpredictable: the choiceof whether to push 1 or 4 was approximately uni-formly distributed throughout the experiment, andthe choice about whether to embed was fairly ran-dom within the constraints of the ?starting small?scheme described above.
Because we did not wantparticipants to have to guess at these nondeterminis-tic events, we made the 1 and 4 boxes turn blue orgreen whenever they occurred and told the partici-Figure 1: Structure of the display for the Box PredictionTask.
The numerals were not present in the screen displayshown to the participants.pants that they did not need to predict blue or greenboxes.
On the other hand, we wanted them to predictthe pop trials whenever they occurred.
Therefore,we colored boxes 2, 3, and 5 a shade of red when-ever they occurred and told the participants that theyshould try to predict all boxes that turned a shade ofred.
When two of the same symbol occurred in a row(e.g., 1 1 2 2 5), we shifted the shade of the color ofthe repeated element so that participants would no-tice the change.
To reinforce this visual feedback,a beep sounded on any trial in which a participantfailed to predict a box that changed to a shade of red.Box 5 has a different structural status than the otherboxes: it marks the ends of sentences.
We includedbox 5, placing it in the center of the visual array, andmaking it smaller than the other boxes, to make thetask easier relative to a pilot version in which Box 5was absent.2.1 Simulation ExperimentWe employed Michal Cernansky?s implementa-tion of Elman (1990)?s Simple Recurrent Network(http://www2.fiit.stuba.sk/c?ernans/main/download.html).The network had five input units, five output unitsand ten hidden units.
Activations changed asspecified in (1) and weights changed according to(2).43a.
Root ?
S 5S ?
1 S 2S ?
1 2S ?
4 S 3S ?
4 3b.
Level 1 Level 2 Level 31 2 5 1 1 2 2 5 1 1 1 2 2 2 54 3 5 1 4 3 2 5 1 1 4 3 2 2 54 1 2 3 5 1 4 1 2 3 2 54 4 3 3 5 2 4 4 3 3 2 5. .
.Table 1: a. Grammar 1: a recursive grammar for gen-erating the color change sequence employed in the ex-periment.
?Root?
is the initial node of every sentencegeneration process.
Null stands for the empty string.
b.Examples of Level 1, 2, and 3 sentences generated byGrammar 1.~h(t) = f(Whh ?
~h(t?
1) + Whi ?
~s(t) +~bh)~o(t) = f(Woh ?
~h(t) +~bo)f(x) = 11+e?x(1)?wij ?
?
?E?wij(2)Here, ~s(t) is the vector of input unit activations attime step t, Whi are the weights from input to hid-den units, Whh are the recurrent hidden connec-tions, and Woh connect hidden to output.On each trial, the input to the network was an in-dexical bit vector corresponding to one of the fivesentence symbols.
The task of the network was topredict, on its output layer, what symbol would oc-cur next at each point.
The sequence of symbols wasmodeled on the sequence presented to the humanparticipants as follows: the human sequence wasdivided into 14 nearly equal-length segments, eachwith a whole number of sentences (the first 11 seg-ments corresponded to the Training Phase and thelast 3 to the Test Phase).
Each segment containedapproximately ten sentences.
For each segment, 400sentences were sampled randomly according to thedistribution of types found in the segment.
Thesegroups of 400 were concatenated end to end to formthe training sequence for the network (a total of22398 trials).The error gradient of equation (2) was ap-proximated using Backpropagation ThroughTime (Rumelhart et al, 1986) with eight time stepsunfolded.
To simulate the absence of negativefeedback on push trials in the human experiment,the network error signal on push trials was set tozero.
The constant of proportionality in equation 2(the ?learning rate?)
was set to 0.4.3 Fractal Encoding of Recursive Structurein Neural EnsemblesIn the past several decades, a number of re-searchers (Moore, 1998; Pollack, 1987; Siegel-mann, 1996; Siegelmann and Sontag, 1994; Ta-bor, 2000) have developed devices for symbol pro-cessing which compute on finite-dimensional com-plete metric spaces (distance is defined, no pointsare ?missing??
(Bryant, 1985)), like the neural net-works considered here.
A common strategy in allof these proposals is the use of spatially recursivesets?i.e., fractals?to encode the temporal recur-sive structure in symbol sequences.
For example,Tabor (2000) defines a Dynamical Automaton (orDA), M , as in (3).M = (H,F, P,?, IM, x0, FR) (3)Here, H is a complete metric space (Bryant, 1985;Barnsley, 1993).
F is a finite list of functions fi :H ?
H , P is a partition of the metric space, ?
isa finite symbol alphabet, IM is an Input Map?thatis, a function from symbols in ?
and compartmentsin P to functions in F .
The input to the machine isa finite string of symbols.
The machine starts at x0and invokes functions corresponding the symbols inthe input in the order in which they occur.
If, whenthe last symbol has been presented, the system is inthe region FR ?
H , then the DA accepts the string.Table 3 specifies DA 1, a dynamical automa-ton that recognizes (and generates) the language ofGrammar 1.
A good way of understanding the prin-ciple underlying this mechanism is to note that apushdown automaton (PDA) (Hopcroft and Ullman,1979) for processing this language must employ astack alphabet with one symbol for tracking?1?
andanother for tracking ?4?.
(See Table 3).
If DA 1 is tosuccessfully process the same language, it must dis-tinguish at least the states that PDA 1 distinguishes44(PDA 1 is minimal in this sense).
DA 1 does this byexecuting state transitions analogous to the push andpop operations of the PDA, arriving in its final re-gion when the PDA is in an accepting state.
Figure 3shows the correspondence between machine statesof PDA 1 and points in the metric space H that un-derlies DA 1?s language recognition capability.
Thisfigure makes it clear that DA 1 is structurally equiv-alent to PDA 1.The computing framework discussed here is verygeneral.
One can construct a fractal grammar thatgenerates any context free language (Tabor, 2000).In fact, similar mechanisms recognize and generatenot only all computable languages but all languagesof strings drawn from a finite alphabet (Moore,1998; Siegelmann, 1999; Siegelmann and Sontag,1994).
Wiles & Elman (1995) and Rodriguez (2001)showed that an SRN trained on a counting recur-sion language (anbn) uses a fractal principle to keeptrack of the embeddings and generalizes to deeperlevels of embedding than those found in its trainingset.
(Tabor et al, 2003) showed that a gradient de-scent mechanism operating in the parameter spaceof a fractal grammar model discovered close ap-proximations of several mirror recursion languages.These findings suggest that the fractal solutions arestable equilibria (?attractors?)
of recurrent networkgradient descent learning processes (Tabor, 2011).This observation argues against a widespread beliefabout neural networks that they are blank slate ar-chitectures, only performing ?associative process-ing?
without structural generalization (Fodor andPylyshyn, 1988).
It suggests a close relationship be-tween the classical theory of computation and neuralnetwork models even though the two frameworks arenot equivalent (Siegelmann, 1999; Tabor, 2009a) .The results of Tabor (2003) indicate that networklearning proceeds along a complexity cline: sen-tences with lower levels of embedding are correctlyprocessed before sentences with higher levels of em-bedding.
This indicates that there are proximityrelationships in the network parameter space: pa-rameterizations that parse successively deeper lev-els of embedding are adjacent to each other.
In thenext section, we investigate the outcome of the SRNlearning experiment with the Box Prediction train-ing data, first testing for evidence that the networkforms a fractal code, then testing for a proximity ef-M = (Q,?,?, ?, q0, Z0, F )Q = {q1, q2, q3}?
= {1, 2, 3, 4, 5}, ?
= {B,O, F}q0 = q3, Z0 = B, F = B?
(q3, 1, B) = (q1, OB), ?
(q3, 4, B) = (q1, FB)?
(q1, 1, O) = (q1, OO), ?
(q1, 4, O) = (q1, FO)?
(q1, 1, F ) = (q1, OF ), ?
(q1, 4, F ) = (q1, FF )?
(q1, 2, O) = (q2, ), ?
(q2, 2, O) = (q2, )?
(q1, 3, F ) = (q2, ), ?
(q2, 3, F ) = (q2, )?
(q2, 5, B) = (q3, B)Table 3: PDA 1.
A Pushdown Automaton for processingthe language of Grammar 1.
?O?
is pushed on ?1?.
?F?is pushed on ?4?.fect consistent with the complexity cline prediction.4 Results: Simple Recurrent Network BoxPredictionWe trained 71 networks, corresponding to the 71 hu-man participants on the sequence described above inSection 2.
The networks all used the same archi-tecture, but differed in the values of their randominitial weights and the precise ordering of the train-ing sentences (although all used the same progres-sive scheme described above).
To approximate theobserved variation in human performance, each net-work also had gaussian noise with constant varianceadded to the weights with each new word input.
Thevariance values were sampled from the uniform dis-tribution on [0,4].
This range was chosen to pro-duce a mean (57%) and standard deviation (20%)similar to that of the humans at the end of training(M = 51%, SD = 21%).Unlike some of the humans, none of the networksgeneralized immediately to Level 3 sentences on thefirst try.
Nevertheless, several of them learned toparse the Level 3 sentences with very few errors bythe end of the ?Test Phase?.
To determine accuracyof a deterministic transition, we normalized the net-work output vector by dividing all the outputs by thesum of the outputs.
If the highest normalized acti-vation was on the correct transition, we counted thetransition as accurate.
When tested on all eight typesof Level 3 sentences, the top 4 networks made 1, 3,3, and 3 errors among the 56 transitions in this sen-45Compartment Symbol Functionh1 > 0 & h2 > 0 1 ~h?
[12 00 12]~h+(10)h1 > 0 & h2 > 0 4 ~h?
[12 00 12]~h+(00)h1 > 1 2 ~h?
[?2 00 ?2]~h+(20)0 < h1 < 1 3 ~h?
[?2 00 ?2]~h+(00)h1 < ?1 2 ~h?
[2 00 2]~h+(20)?1 < h1 & h1 < 1 3 ~h?
[2 00 2]~h+(00)h1 = ?1 & h2 = ?1 5 ~h?
[?1 00 ?1]~h+(00)Table 2: Input Map for DA 1.
The automaton starts at the point, (1, 1).
It?s Final Region is also (1, 1).
?2 ?1.5 ?1 ?0.5 0 0.5 1 1.5 2?1?0.8?0.6?0.4?0.200.20.40.60.81BOBOOBOOOBOOBOBBFOOBFOBOFOBFOBFFOBFBOFBOOFBOFBFBFOFBFFBOFFBFFBFFFBq3q1q2Figure 2: Correspondence between states of DA 1 andPDA stack states.tence set.We hypothesized that the networks were approx-imating a fractal grammar with the same qualita-tive structure as that of DA 1, possibly in more thantwo dimensions.
We sought two kinds of evidence:?linear separability?
and ?branching structure?.
For?linear separability?, we asked if the SRN states cor-responding to a particular point in DA 1 (state ofPDA 1) were clustered so as to be linearly sepa-rable from SRN states corresponding to a differentpoint.
Two sets A and B of points in a vector spaceof dimension n are linearly separable if there is ann ?
1 dimensional hyperplane in the space with allthe points of A on one side of it and all the pointsof B on the other.
In fractal grammar parsing, pair-wise linear separability suffices to distinguish themachine states (Tabor, 2000).
Among the caseswhere more than one sample point corresponded tothe same PDA state, an average of 17.6/22 were pair-wise linearly separable from the other groups.
In sixof the networks, all the multi-element clusters werepairwise linearly separable.
This finding lends sup-port to the claim that the networks approximate frac-tal grammars.For ?branching structure?, we asked if the deploy-ment of these (largely) linearly separable clusterscorresponded to the branching structure of the frac-tal of DA 1.
In particular, for each cluster corre-sponding to a DA 1/PDA 1 state with more than onesymbol on the stack in PDA 1, we considered all theclusters with one-fewer symbols on the stack.
Weasked if the nearest cluster with one fewer symbolson the stack corresponded to the nearest one-fewerstack symbol point in DA 1.
In Level 1 to and 3 sen-tences, there are 20 such states to consider.
Acrossnetworks, the average rate of unexpected proxim-46ity relationships was 3.5/20 (SD = 1.7).
The bestnetworks we observed under this training method(noise reduced to 0) generated only 1 unexpectedproximity relationship.
These results also indicatea close correspondence between the organization ofthe network and fractal grammars.Up to this point, the evidence we have been pre-senting has supported a formal correspondence be-tween SRNs and fractal grammars.
In the finalpart of this section, we consider one prediction ofthe network approach that is not obviously pre-dicted by symbolic grammar mixture accounts likethe Bayesian model discussed in the introduction.Tabor (2003) shows how a fractal for process-ing another recursive language (similar to the lan-guage of Grammar 1) arises by gradual metamor-phosis of (Stage I) a single point into (Stage II) a lineof points, then into (Stage III) an infinite lattice, theninto (Stage IV) a fractal with overlapping branchesand finally into (Stage V) the fully-formed fractalthat very closely captures the recursive embeddingstructure.
During Stage IV, the system correctly pro-cesses shallow levels of embedding but fails to pro-cess deeper levels of embedding.
As the metamor-phosis progresses, this Fractal Learning Neural Net-work (FLNN) becomes able to process deeper anddeeper levels at an accelerating rate such that, afterfinite time, it reaches a point where it is effectivelyprocessing all levels, indicating a continuous com-plexity cline in parameter space.
An empirical im-plication is that a network that has mastered n lev-els of embedding, for n a natural number, will moreeasily (with less weight change) master n+1 levelsof embedding than one that has mastered fewer thann.To see if the SRN?s complexity cline predictionsare in line with those of the FLNNs, we correlatedthe network?s performance at the end of the TrainingPhase with its performance in the Test Phase.
Forthis purpose, we defined the training performance asthe mean prediction accuracy across all predictabletransitions of Level 1 and 2 sentences in the fourthquarter of the training phase.
The Test Phase per-formance was defined in two different ways.
It wasdefined as the mean accuracy across novel but pre-dictable transitions (a) in all Level 3 sentences inthe test phase or (b) only in the first instances offour different Level 3 sentences.
We used the sec-ond measure because the networks and humans con-tinue to learn in the Test Phase: correlation of train-ing performance with measure (a) might stem fromlearning facility alone; correlation with (b) indicatesgeneralization ability.
Both tests showed signifi-cant correlation (a: r(69) = 0.98, p < .0001; b:r(69) = 53, p < .0001).
These results are consis-tent with the claim that the SRN induces a complex-ity cline similar to that induced by the fractal learn-ing networks..To consider how well this prediction distin-guishes the fractal learning framework from otherapproaches to grammar learning, we now considerthe Bayesian grammar selection model of (Perfors etal., 2011).
We consider this case, which is naturallyrelated to our focus, as a first step toward developingconcrete approaches within the Bayesian frameworkthat could address the issues raised by the Box Pre-diction findings.Perfors et al?s model is also concerned with theinduction of recursive grammatical systems fromlanguage data.
They presented samples from theChildes Database (MacWhinney, 2000) to theirmodel over 6 stages, where each stage sampled thecorpus more thoroughly than the last.
This samplingmethod generally caused each stage to have heaviersampling of deeper recursive structures than the pre-vious stage because the deeper recursive structuresare less frequent in the master corpus.
The Bayesianmodel selects finite-state grammars during the ear-lier stages and then prefers recursive grammars dur-ing the later stages.
This shift occurs because, as thesampling goes deeper, the finite state systems needto employ many additional productions to handle theburgeoning variety of collocations, while the recur-sive grammars can handle them with few rules, sothe model?s anti-complexity bias causes it to preferthe recursive grammars (Perfors et al, p. 320).
Itseems likely that a version of their model, appliedto the training data in our experiment, would se-lect finite-state grammars during the Training Phaseand the switch to a recursive grammar in the TestPhase.
Perfors et al did not consider the questionof individual differences.
We can think of one waythat the basic correlational finding reported for theSRNs would obtain in the Bayesian system (finding(a) above): if the perception of the stimuli by somemodels was noisier than that of others, then one ex-47pects the general correlation between Training andTest performance to obtain: the noise interferes sim-ilarly with both phases so correlated accuracy is ob-served.
It is not as clear to us that the Bayesian sys-tem will predict finding (b), which shows that first-try performance on novel structures is better for peo-ple who show better Training performance.2 Theredoes not appear to be a proximity relationship be-tween grammars in Perfors et al?s model as there isin the network models.
Thus, if it predicts this effect,then it would have to do so for a different reason, apoint worthy of further research.5 Results: Human Box PredictionSeventy-one undergraduate students in the Univer-sity of Connecticut participated in the experimentfor course credit.
The range of human performancewas substantial.
The mean correct performance on37 predictable trials during the last 100 trials oftraining was 51% (SD = 21%).
Despite this overalllow rate of performance at test, there was a subsetof people who learned the training grammar well bythe end of training.Twelve of the 71 participants, scored over 80%correct on the pop trials within the last 100 train-ing trials.
80% is the level of correct performancethat a particular finite-state device we refer to as the?Simple Markov Model?
would yield during these100 trials.
The Simple Markov Model predicts 2 af-ter 1, 3 after 2, 4 after 3, and 1 after 4.
The two topscorers among these twelve generalized perfectly toeach first instance of the four Level 3 types in thetest phase.
If, contrary to our hypothesis, all 12 wereusing finite state mechanisms, and they guessed ran-domly on novel transitions, the chances of observing2 or more perfect scorers would be 0.9% (p = .009).We take this as evidence that the two strongest gen-eralizers developed a representation closely approx-imating a recursive system.Performance at the end of training correlated withaccuracy on 24 novel transitions in Level 3 sentencesat test (r(69) = 0.72, p < .0001).
This correspondsto test (a) of the SRN Results section above, suggest-ing some kind of grammar proximity model.
Re-garding (b), accuracy on the 8 novel transitions in2This is not single-trial learning.
It is immediate generaliza-tion to unseen cases.the 4 first instances of novel Level 3 sentences alsocorrelated with the performance at the end of train-ing, r(69) = 0.57, p < .0001.
These results lendsome empirical support to the complexity cline pre-dictions of the fractal model.6 General DiscussionWe studied the learning of recursion by trainingSimple Recurrent Networks (SRNs) and humansin an artificial grammar task.
We described met-ric space computing models that navigate on frac-tal sets and noted a complexity cline phenomenonin learning (learning of lower embeddings facilitatesthe learning of higher ones).
Previous work in thisarea has focused on counting recursion languages.Here, we explored learning of a mirror recursion lan-guage.
We showed that the SRN hidden unit repre-sentations had clustering and branching structure ap-proximating the predictions of the fractal grammarmodel.
They also showed evidence of the complex-ity cline.
The human learning results on the samelanguage provided evidence that at least a few peo-ple inferred a recursive principle for the mirror re-cursion language.
The complexity cline predictionwas also borne out by the human data: not only didperformance on lower levels of embedding correlatewith performance on higher levels of embedding,but it predicted generalization behavior, suggestingthat the representation continuously metamorphosesfrom a finite-state system into an infinite state sys-tem.We identified one closely related Bayesian gram-mar induction model (Perfors et al, 2011) whichseems well positioned to make similar, but probablynot the same, predictions about phenomenon of infi-nite state language learning.
We suggest that furtherexploration of the relationship between the Bayesianmodels and the recurrent neural network models willbe helpful.
A novel claim of the present work isthat they it is possible to compare recurrent neuralnetwork models and symbolic structure models onthe same terms.
We suggest that further examinationof this relationship may be helpful in addressing thechallenging problems of complex language learning.48ReferencesMichael Barnsley.
1993.
Fractals Everywhere, 2nd ed.Academic Press, Boston.Victor Bryant.
1985.
Metric Spaces.
Iteration and Appli-cation.
Cambridge University Press, Cambridge, UK.Pyeong Whan Cho, Emily Szkudlarek, Anuenue Kukona,and Whitney Tabor.
2011.
An artificial grammarinvestigation into the mental encoding of syntacticstructure.
In Laura Carlson, Christoph Hoelscher,and Thomas F. Shipley, editors, Proceedings of the33rd Annual Meeting of the Cognitive Science So-ciety (Cogsci2011), pages 1679?1684, Austin, TX.Cognitive Science Society.
Available online athttp://palm.mindmodeling.org/cogsci2011/.Noam Chomsky.
1957.
Syntactic Structures.
Moutonand Co., The Hague.Morten H. Christiansen and Nick Chater.
1999.
Towarda connectionist model of recursion in human linguisticperformance.
Cognitive Science, 23:157?205.Jeffrey L. Elman.
1990.
Finding structure in time.
Cog-nitive Science, 14:179?211.Jeffrey L. Elman.
1991.
Distributed representations,simple recurrent networks, and grammatical structure.Machine Learning, 7:195?225.Jeffrey L. Elman.
1993.
Learning and developmentin neural networks: the importance of starting small.Cognition, 48:71?99.Daniel L. Everett.
2005.
Cultural constraints on gram-mar and cognition in Piraha: another look at the designfeatures of human language.
Current Anthropology,46(4):621?646, August.J.
A. Fodor and Z. W. Pylyshyn.
1988.
Connectionistmand cognitive architecture: A critical analysis.
Cogni-tion, 28:3?71.Thomas L. Griffiths, Nick Chater, Charles Kemp, AmyPerfors, and Joshua B. Tenenbaum.
2010.
Proba-bilistic models of cognition: exploring representationsand inductive biases.
Trends in Cognitive Sciences,14(8):357?364.John E. Hopcroft and Jeffrey D. Ullman.
1979.
Intro-duction to Automata Theory, Languages, and Compu-tation.
Addison-Wesley, Menlo Park, California.Jun Lai and Fenna H. Poletiek.
2011.
The im-pact of adjacent-dependencies and staged-input onthe learnability of center-embedded hierarchical struc-tures.
Cognition, 118(2):265?273, February.B.
MacWhinney.
2000.
The CHILDES project: Tools foranalyzing talk.
Lawrence Erlbaum Associates, Mah-wah, NJ, third edition.Cris Moore.
1998.
Dynamical recognizers: Real-timelanguage recognition by analog computers.
Theoreti-cal Computer Science, 201:99?136.Elissa L. Newport.
1990.
Maturational constraints onlanguage learning.
Cognitive Science, 14(1):11?28,March.Amy Perfors, Joshua B. Tenenbaum, and Terry Regier.2011.
The learnability of abstract syntactic principles.Cognition, 118(3):306?338, December.Jordan Pollack.
1987.
On connectionist models of natu-ral language processing.
Unpublished doctoral disser-tation, University of Illinois.Paul Rodriguez, Janet Wiles, and Jeffrey Elman.
1999.A recurrent neural network that learns to count.
Con-nection Science, 11(1):5?40.Paul Rodriguez.
2001.
Simple recurrent networks learncontext-free and context-sensitive languages by count-ing.
Neural Computation, 13(9):2093?2118.David E. Rumelhart, Geoffrey E. Hinton, and R. J.Williams.
1986.
Learning internal representations byerror propagation.
In David E. Rumelhart, James L.McClelland, and the PDP Research Group, editors,Parallel Distributed Processing, v. 1, pages 318?362.MIT Press.H.
T. Siegelmann and E. D. Sontag.
1994.
Analog com-putation via neural networks.
Theoretical ComputerScience, 131:331?360.Hava Siegelmann.
1996.
The simple dynamics of su-per Turing theories.
Theoretical Computer Science,168:461?472.Hava T. Siegelmann.
1999.
Neural Networks and AnalogComputation: Beyond the Turing Limit.
Birkha?user,Boston.Whitney Tabor, Bruno Galantucci, and DanielRichardson.
2003.
Evidence for self-organizedsentence processing: Local coherence ef-fects.
Submitted manuscript, University ofConnecticut, Department of Psychology: Seehttp://www.sp.uconn.edu/ ps300vc/papers.html.Whitney Tabor.
2000.
Fractal encoding of context-freegrammars in connectionist networks.
Expert Systems:The International Journal of Knowledge Engineeringand Neural Networks, 17(1):41?56.Whitney Tabor.
2003.
Learning exponential state growthlanguages by hill climbing.
IEEE Transactions onNeural Networks, 14(2):444?446.Whitney Tabor.
2009a.
Affine dynamical automata.
Ms.,University of Connecticut Department of Psychology.Whitney Tabor.
2009b.
A dynamical systems per-spective on the relationship between symbolic andnon-symbolic computation.
Cognitive Neurodynam-ics, 3(4):415?427.Whitney Tabor.
2011.
Recursion and recursion-like structure in ensembles of neural elements.
InH.
Sayama, A. Minai, D. Braha, and Y. Bar-Yam, editors, Unifying Themes in Complex Sys-49tems.
Proceedings of the VIII International Confer-ence on Complex Systems, pages 1494?1508, Cam-bridge, MA.
New England Complex Systems Institute.http//necsi.edu/events/iccs2011/proceedings.html.Janet Wiles and Jeff Elman.
1995.
Landscapes in re-current networks.
In Johanna D. Moore and Jill FainLehman, editors, Proceedings of the 17th Annual Cog-nitive Science Conference.
Lawrence Erlbaum Asso-ciates.50
