Eric BrillMicrosoft ResearchOne Microsoft WayRedmond, Wa.
98052brill@microsoft.com'C1AbstractA wide range of natural anguage problemscan be viewed as disambiguating between asmall set of alternatives based upon thestring context surrounding the ambiguitysite.
In this paper we demonstrate thatclassification accuracy can be improved byinvoking a more descriptive feature set thanwhat is typically used.
We present atechnique that disambiguates by learningregular expressions describing the stnngcontexts in which the ambiguity sitesappear.IntroductionMany natural language tasks are essentially n-way classification problems, where classificationdecisions are made from a small set of choices,based upon the linguistic context in which theambiguity site occurs.
Examples of such tasksinclude: confusable word set disambiguation;word sense disambiguation; determining suchlexical features as pronoun case and determinernumber for machine translation; part of speechtagging; named entity labeling; spellingcorrection; and some formulations of skeletalparsing.
Very similar feature sets have beenused across machine learning algorithms andacross classification problems.
For example, inconfusable word set disambiguation, systemstypically use as features the occurrence of aparticular word within a window of +/- n wordsof the target, and collocations based on thewords and part of speech tags of up to twowords to the left and two words to the fight ofthe target.Below we present a machine learningalgorithm that learns from a much richer featureset than that typically used for classification innatural language.
Our algorithm learns rulesequences for n-way classification, where thecondition of a rule can be a restricted regularexpression on the string context in which theambiguity site appears.
We demonstrate thatusing this more powerful feature space leads toan improvement in disambiguation performanceon confusable words.1 Previous Work: Richer FeaturesMost previous work applying machine learningto linguistic disambiguafion has used as featuresvery local collocational information as well asthe presence of a word within a fixed window ofan ambiguity site.
Indeed, one of the greatinsights in both speech recognition and naturallanguage processing is the realization that fixedlocal cues provide a great deal of usefulinformation.While the n-gram reins supreme inlanguage modeling, there has been someinteresting work done building language modelsbased on linguistically richer features.
Bahl,Brown et al (1989) describe a language modelthat builds a decision tree that is allowed to askquestions about the history up to twenty wordsback.
Saul and Pereira (1997) describe alanguage model that can in essence skip overuninformative words in the history.
Della Pietraet al (1994) discuss an approach to languagemodeling based on link grammars, where themodel can look beyond the two previous wordsto condition on linguistically relevant words inthe history.
The language model described byChelba and Jelinek (1998) similarly conditionson linguistically relevant words by assigningpartial phrase structure to the history andpercolating headwords.Samuellson, Tapanainen et al (1996)describe a method for learning a particularPattern-Based Disambiguation for Natural Language Processinguseful type of pattern, which they call a barrier.Given two symbols X and Y, and a set ofsymbols S, they learn conditions of the form:take an action if there is an X preceded by a Y,with no intervening symbols from S. In theirpaper they demonstrate how such patterns can beuseful for part of speech tagging.
Even-Zoharand Roth (2000) show that by includinglinguistic features based on relations such assubject and object, they can better disambiguatebetween verb pairs.2 DefinitionsBelow we provide the standard definition forregular expressions, and then define a lessexpressive language formafism, which we willrefer to as reduced regular expressions.
Thelearning method we describe herein can learnrules conditioned on any reduced regularexpression.Regular Expression (RE): 1 Given a finitealphabet E ,  the set of regular expressions overthat alphabet is defined as (Hopcroft and Ullman1979):(1) Va~ E, a is a regular expression anddenotes the set {a}(2) if r and s are regular expressions denoting thelanguages R and S, respectively, then (r+s), (rs),and (r*) are regular expressions that denote thesets R ~_~ S, RS and R* respectively.Reduced Regular Expression (R.RE): Given afinite alphabet ~,  the set of reduced regularexpressions over that alphabet is defined as:(1) Va~ E:a is an RRE and denotes the set {a}a+ is an RRE and denotes the positiveclosure of the set {a}a* is an RRE and denotes the Kleeneclosure of the set {a}-a is an RRE and denotes the set ~ - a~a+ is an RRE and denotes the positiveclosure of the set Z - a~a* is an RRE and denotes the Kleeneclosure of the set E - a(2) .
is an RRE denoting the set E(3) .+ is an RRE denoting the positive closureof the set E(4) .
* is an RRE denoting the Kleene closure ofthe set(5) if r and s are RREs denoting the languages Rand S, respectively, then rs is an RRE denotingthe set RS.Some examples of strings that areregular expressions but not reduced regularexpressions include: (ab)*, a(b\]c)d, (a (bc)+)*Next, we need some definitions to allowus to make reference to particular positions in acollection of strings.Corpus: A corpus is an ordered set of strings.We will notate the jth String of a corpus C asC\[j\].
\]C\] is the number of strings in the corpus.IqJ\]l is the number of symbols in the jt~ string ofthe corpus.Corpus Position.'.
A corpus position for acorpus C is a tuple (j,k), meaning the k th symbolin the jtb string in the corpus, with therestrictions: 1_< j _<\[ C\ [and 0 _< k _<\[ CU\] \[.A Corpus Position Set is a set of corpuspositions.Next, we define an RRE-Tree, the datastructure we will use in learning RREs.RRE-Tree: An RRE-Tree over E is a tree(V,E), where V is a set of tuples <v,S>, v beinga unique vertex identifier and S being a CorpusPosition Set, and E is a set of labeled directededges <vi,vj,label>, where vi and vj are vertexidentifiers, label ~ LABEL_SET andLABEL SET = {dot, dot+, dot*} U{a,a+,a*,~a,~a+,~a* I 'Va~ E }.23 Rule Sequence LearningOur implemented learner is based upon thetransformation-based l arning paradigm (Bnll1995).
In this section we briefly reviewtransformation-based learning.I In all of our formulations, we ignore expressionsdenoting the empty set O and the set {e}.
2 We use "dot" for ".
"2(1) A start-state annotator, which assigns aninitial label to a string.
(2) A sequence of rules of the form: Change thelabel of a string from m to n if C(string), whereC is a predicate over strings and m,n ~ L.A string is labelled by first applying thestart-state annotator to it, and then applying eachrule, in order.To learn a transformation sequence, thesystem begins with a properly labelled trainingset.
It then removes the labels and applies thestart-state annotator to each string.
Then thelearner iteratively does the following:(1) Find the best rule to apply to the training set.
(2) Append that rule to the end of the learnedtransformation sequence.
(3) Apply that rule to the training set.4.1 RRE-Tree Construction: until the stopping criterion is met.4 Learning RRE RulesBelow we will demonstrate how to learntransformation sequences where the predicateC(stfing) is of the form "Does RRE R apply tothe stnng?"
We will show this for the binaryclassification case (where ILl = 2).In each learning iteration, we willconstruct an RRE-Tree in a particular way, findthe best node in that RRE-Tree, and then returnthe edge labels on the path from root to bestnode as the learned RRE.
The learner will learna sequence of rules of the form:Change the label of a string from li to lj if thestring matches reduced regular expression R.Before proceeding, we need to specifytwo things: the start-state annotator and thegoodness measure for determining what rule isbest.
The system will use a start-state annotatorthat initially labels all strings with the mostfrequent label in the training set, and thegoodness measure will simply be the number ofgood label changes minus the number of badlabel changes when a rule is applied to thetraining set.Take the following training set:String # String True Label Init.
Guess1 abc  0 12 abb  1 13 baa  1 1Since 1 is the most frequent label in thetraining set, the start-state annotator wouldinitially assign all three training set strings thelabel 1, meaning stnng 1 would be incorrectlylabelled and strings 2 and 3 would be correct.Now we want to learn a rule whose appficationwill best improve our labelling of the trainingset.We will first present an algorithm forconstructing an RRE-Tree for a training corpus,and then trace through the appficadon of thisalgorithm to our example training corpus above.To simplify the presentation, we will limitourselves to learning rules for a weaker languagetype, which we call Very Reduced RegularExpressions (VRREs).
The extension to RRElearning is straightforward.Very Reduced Regular Expression (VRRE):Given a finite alphabet E,  the set of veryreduced regular expressions over that alphabet isdefined as:(1) 'v'a~ E: a is a VRRE and denotes the set{a}(2) .
is a VRRE denoting the set g(3) .
* is a VRRE denoting the Kleene closure ofthe set E(4) if r and s are VRREs denoting the languagesR and S, respectively, then rs is a VRREdenoting the set RS.Say we have a training corpus C. Forevery string C\[j\]~ C, Tmth\[C\[j\]\] ~ {0,1 } is thetrue label of C\[j\] and Guess\[C\[j\]\] is the currentguess of the label of C\[j\].
The algorithm for oneiteration of rule learning follows.Main() {3In string classification, the goal is toassign the proper label to a string, from aprespecified set of labels L. A transformation-based system consists of:(1) Create root node with corpus position set S ={0,0) \ [ j  = 1 .. ICI).
Push this node ontoprocessing stack (STACK).
(2) While (STACK not empty) {STATE = pop(STACK);Push(dotexpand(STATE),STACK);Push(dotstarexpand (STATE), STACK);Va~ ZPush(atomexpand(a, STATE),STACK)}(3) Find best state S in the RRE-tree.
Let R bethe RRE obtained by following the edges fromthe root to S, outputting each edge label as theedge is traversed.
Return either the rule "0--~ 1 ifR" or "1--)0 if R" depending on which isappropriate for state S.}dotexpand(STATE) {create new state STATE"let P be the corpus position set of STATEP' = {0,k) I (j,k-1) E P and k-1 ~ ICorpusfj\]l}If (P' not empty) {Make P' the corpus position set ofSTATE'Add (STATE,STATE' ,DOT) to treeedgesreturn STATE'}Else return NULL}dotstarexpand(STATE) {create new state STATE'let P be the corpus position set of STATEP' = {(j,k) \[ (j,m) ~ P, m_< k, and k _<ICorpusU\]l}I f (P '?
P) {Make P' the corpus position set of STATE'Add (STATE,STATE',DOT*) to tree edgesreturn STATE'}Else return NULL}atomexpand(a, STATE) {create new state STATE'let P be the corpus position set of STATEP' = {(j,k) I ( j ,k-1) E P, k-1 ?
Icorpusfj\]l, andthe k-1 st symbol in Corpus\[j\] is a}If (P' not empty) {Make P' the corpus position set ofSTATEAdd (STATE,STATE',a) to tree edgesreturn STATE'}Else return NULL}Each state S in the RRE-tree represents he RREcorresponding to the edge labels on the pathfrom root to S. For a state S with corpusposition set P and corresponding RRE R, thegoodness of the rule: 0~1 if R, is computed aspGoodness_0_to_l (S) =(J,*~v Score_0to  1 ((j,k))whereScore_0_to_ l  ((j ,k)) =1 i f  k = Icfj\]l^ Guess \ [ j \ ]  = 0^ Truth\[ j \ ]  = 1-1 if  k = IC\[j\]l^ Guess \ [ j \ ]  = 0A Truth\[ j \ ]  = 00 o therwiseSimilarly, we can compute the score for therule : 1@0 if R. We then define Goodness(S) =max(Goodness0_to  l(S),Goodness_l_to_0(S))Returning to our example, for the 3strings in this training corpus, the root node ofthe RRE-Tree would have the corpus positionset: {(1,0),(2,0),(3,0)}.
The root nodecorresponds to the null RRE, and so the positionset consists of the beginning of each string in thetraining set.
In figure 1 (at the end of the paper)we show a partial RRE-Tree.
If we follow theedge labelled "dot" from the root node, we see itleads to a state with position set{(1,1),(2,1),(3,1)}, as a dot advances allpositions by one.3 This assumes an RRE must match the entire stringin order to accept it.4The square state in figure 1 representsthe RRE: "dot* c" and the triangular staterepresents "a dot c".
Both the square andtriangular states have a corpus position setconsisting of only one corpus position, namelythe end of stnng 1, and both would have agoodness core of 1 for the corresponding l&0rule.
If We prefer shorter ules, we will learn asour first rule in the rule list: l&0  if dot* c.After applying this rule to the training corpus, allstrings will be correctly labelled and trainingwill terminate.
If the stopping criterion were notmet, we would apply the learned rule to changethe values of our Guess array, then create a newRRE-tree, find the best state in that tree, and soon.It is easy to extend the above algorithmto learn RREs instead of VRREs.
Note, forinstance, that he corpus position set for a state Swith incoming edge labelled ~a can be found bytaking the position set for the sibling of S withincoming edge labelled dot and deleting thosecorpus positions that are found in the positionset for the sibling of S with incoming edgelabelled a...?5 OptimizationsThe algorithm above is exponential.
There aresome opfirnizations we can perform that make itfeasible to apply the learning algorithm.Optimization 1: Pruning states we knowcannot be on the path from root to the best state.Define GoodPotential 0 to I(S) as the numberof sentences s in the training corpus for whichGuess\[s\]=0, Truth\[s\]= 1 and3k : (s, k) ~ corpus_position_set(S).
We cansimilarly define GoodPotential 1 to0(S), andthen defineGoodPotential(S)=max(GoodPotential 0 to_l(S),GoodPotential 1 to O(S))As we construct the RRE-tree, we keeptrack of the largest Goodness(S) we haveencountered.
If that value is X, then for a stateS', if GoodPotential(S')_<X, it is impossible forany path through S' to reach a state with a bettergoodness score than the best found thus far.
Wecan check this condition when pushing statesonto the stack, and when popping off the stackto be processed, and if the pruning condition ismet, the state is discarded.Optimization 2: Merging states with identicalcorpus position sets.
If we are going to push astate onto the stack when a state already existswith an identical corpus position set, we do notneed to retain both states.
We may useheuristics to decide which of the states withidentical corpus position sets we should keep(such as choosing the one with the shortest pathto the root).6 ExperimentsTo test whether learning RREs can improvedisambiguafion accuracy, we explored the taskof confusion set disambiguation (Golding andRoth 1999).
We trained and applied twodifferent rule sequence l arners, one which usedthe standard feature set for this problem (e.g.
theidentical feature set to that used in (Golding andRoth 1999) and (Mangu and Brill 1997) anddescribed in the introduction, and one whichlearned RR.Es.
4 Because we wanted todeterinine what could be gained by using RREs,we ran an ablation study where we kepteverything else constant across the two runs, anddid not use performance enhancing techniquessuch as parameter tuning on held out data orclassifier combination.Both learners were given a window of+/- 5 words surrounding the ambiguity site.Context was not allowed to cross sentenceboundaries.
The training and test set werederived by finding all instances of theconfusable words in the Brown Corpus, usingthe Penn Treebank parts of speech andtokenization (Marcus, Santorini et al 1993), andthen dividing this set into 80% for training and20% for testing.For the RRE-based system, we mappedthe +/- 5 word window of context into a string asfollows (where wi is a word and ti is a part ofspeech tag):4 The set of RREs is a superset of what can belearned using the standard feature set.5Wi.
5 ti.
5 Wi-4 ti.
4 Wi.
3 ti.
3 Wi.
2 ti.2.
Wi.
I ti.
1 M IDDLEWi+l ti+l wi+2 ti+2 wi+3 ti+3 wi+4 ti+4 wi+5 ti+5where MIDDLE is the ambiguity site.Both for execution time and spaceconsiderations for the learner and for fear ofovertraining, we put a bound on the length of theRRE that could be learned, s We define anatomic RRE as any RRE derived without anyconcatenation perations.
Then the length of anRRE is defined as the number of atomic RREswhich that RRE is made up of.
The atom"MIDDLE" is not counted in length.Below we give two examples of rulesthat were learned for one confusion set:  6(1) past ~ passed if .
* ~DT MIDDLE DOT IN(2) past ~ passed if (~to)* NN MIDDLEThe first rule says to change thedisambiguation guess to << passed >> if the wordbefore is not a determiner and the word after is apreposition.
This matches contexts uch as : <<... they passed by ... >> while not matchingcontexts uch as : << ... made in the past by ... >>The  second rule captures contexts uch as : << ...the hike passed the campground ... >~ while notmatching contexts uch as : << ... want to take ahike past the campground... >>In Table 1, we show test set results fromrunning the rule sequence learner with both thestandard set of features and with RRE-basedfeatures.
7 The results are sorted by trainingcorpus size, with the raise/rise training corpusbeing the smallest and the then/than trainingcorpus being the largest.
Baseline accuracy is5 Note that this does not imply a bound on the lengthof a string to which an RRE can apply.6 DT= determiner, IN = preposition, biN = singularnoun.7 While these results look worse than those achievedby other systems, as reported in (Golding and Roth,1999), we used different data splits and tokenization.Our baseline accuracies are significantly lower thanthe baselines for their test sets.
If we account for thisby instead measuring percent error reductioncompared to baseline accuracy, then our averagereduction is better than that reported for the BaySpellsystem, but worse than that of WinSpell.
If we addvoting to our system (WinSpell employs voting), thenwe attain results on par with WinSpell.the accuracy attained on the test set by alwayspicking the word that appears more frequently inthe training set.Conf.
Pair Baseline StandardRaise/Rise 53.6 75.0Pnncipal/PfincipleAccept/ExceptAffect/Effect64.5 80.660.0 94.586.8 94.3Lead/Led 53.6 89.3Piece/Peace 51.1 83.0Weather/Whether 79.7 84.6Quiet/Quite 83.1 100County/Country 75.6 78.2Past/Passed 63.7 88.1Amount/Number 74.1 83.396.1 Begin/BeingAmong/BetweenThen/Than90.869.2 76.862.8 93.1RRE78.683.990.994.389.383.089.298.583.389.387.096.780.893.4Table 1 Test Set Results: Standard vs RRE-Based FeaturesIn Table 2 we see that the RRE-basedsystem outperforms the standard system on 9 ofthe confusion sets, the standard systemoutperforms the RRE-based system on 2 and thetwo systems attain identical results on 3.
We seethat the relative performance of the RRE-basedlearner is better overall on the larger trainingsets than on the smaller sets.
This is to beexpected, as more data is needed to supportlearning the more expressive RRE-based rules.RRE Standard IdenticMBe~er BetterAll 9 2 3Confusables1 3 7 SmallestSets7 LargestSets1 0Table 2 Performance Analysis Across DifferentSetsPooling all of the test sets into one bigset, the RRE-based system achieves an overallaccuracy of 89.9%, compared to 88.5% for thestandard learner.
Weighting each confusion pairequally, the RRE-based system achieves an6overall accuracy of 88.4%, compared to 86.9%for the standard learner.ConclusionsThe RRE-based rule sequence l arner presentedabove is able to learn rules using moreexpressive conditions than what is typically usedfor disambiguation tasks in natural languageprocessing.
These regular-expression basedconditions lead to higher accuracy than what isachieved when using the same learningparadigm with the traditionally used feature set.We hope that other learning algorithms canbenefit from the ideas presented here and thatthe idea of learning RREs can be generalized toallow other learners to incorporate morepowerful features as well.ReferencesBahl, L., P. Brown, et al (1989).
"'A Tree-BasedLanguage Model for Natural Language SpeechRecognition."
IEEE Transactions on Acoustics,Speech and Signal Processing 37: 1001-1008.Brill, E. (1995).
"Transformation-Based error-dnven learning and natural language processing:a case study in part of speech tagging.
"Computational Linguistics.Chelba, C. and F. Jelinek (1998).
ExploitingSyntactic Structure for Language Modeling.Proceedings of Coling/ACL, Montreal, Canada.Even-Zohar, Y. and D. Roth (2000).
AClassification Approach to Word Prediction.Proceedings ofNAACL, Seattle, Wa.Golding, A. and D. Roth (1999).
"A Winnow-Based Approach to Context-Sensitive SpellingCorrection."
Machine Learning.Hopcroft, J. and J. Ullman (1979).
Introductionto Automata Theory, Languages andComputation, Addison-Wesley.Mangu, L. and E. Brill (1997).
Automatic RuleAcquisition for Spelling Correction.
Proceedingsof the International Conference on MachineLearning, Nashville, Tn.Marcus, M., B. Santofini, et al (1993).
"Building a large annotated corpus of English:the Penn Treebank."
Computational Linguistics.Pietra, S. D., V. D. Pietra, et al (1994).Inference and Estimation of a Long-RangeTrigram Model.
Proceedings of the SecondInternational Colloquium on GrammaticalInference, Alicante, Spain.Samuellson, C., P. Tapanainen, et al (1996).Inducing Constraint Grammars.
GrammaticalInference: Learning Syntax from Sentences.
L.Miclet and C. D. 1.
Huguera, Springer.
1147.Saul, L. and F. Pereira (1997).
Aggregate andmixed-order Markov models for statisticallanguage processing.
Proceedings of the SecondConference on EMNLP.7(2,1)(2,0)(1,O),OA),(1,2),(1,3),(2,o),(2,1),(2,2),(2,3),(3,0),(3,1),(3,2),(3,3)C Da~ ~  ~ (2,2)b-@Figure I : A Partial RRE-Tree8
