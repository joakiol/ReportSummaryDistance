Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 39?48,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsToward Learning and Evaluation of Dialogue Policies with Text ExamplesDavid DeVault and Anton Leuski and Kenji SagaeInstitute for Creative TechnologiesUniversity of Southern CaliforniaPlaya Vista, CA 90094{devault,leuski,sagae}@ict.usc.eduAbstractWe present a dialogue collection and enrich-ment framework that is designed to explorethe learning and evaluation of dialogue poli-cies for simple conversational characters us-ing textual training data.
To facilitate learningand evaluation, our framework enriches a col-lection of role-play dialogues with additionaltraining data, including paraphrases of user ut-terances, and multiple independent judgmentsby external referees about the best policy re-sponse for the character at each point.
Asa case study, we use this framework to traina policy for a limited domain tactical ques-tioning character, reaching promising perfor-mance.
We also introduce an automatic policyevaluation metric that recognizes the validityof multiple conversational responses at eachpoint in a dialogue.
We use this metric to ex-plore the variability in human opinion aboutoptimal policy decisions, and to automaticallyevaluate several learned policies in our exam-ple domain.1 IntroductionThere is a large class of potential users of dialoguesystems technology who lack the background formany of the formal modeling tasks that typicallyare required in the construction of a dialogue sys-tem.
The problematic steps include annotating themeaning of user utterances in some semantic formal-ism, developing a formal representation of informa-tion state, writing detailed rules that govern dialoguemanagement, and annotating the meaning of systemutterances in support of language generation, amongother tasks.In this paper, we explore data collection and ma-chine learning techniques that enable the implemen-tation of domain-specific conversational dialoguepolicies through a relatively small data collection ef-fort, and without any formal modeling.
We presenta case study, which serves to illustrate some ofthe possibilities in our framework.
In contrast torecent work on data-driven dialogue policy learn-ing that learns dialogue behavior from existing datasources (Gandhe and Traum, 2007; Jafarpour et al,2009; Ritter et al, 2010), we address the task of au-thoring a dialogue policy from scratch with a spe-cific purpose, task and scenario in mind.
We exam-ine the data collection, learning and evaluation steps.The contributions of this work include a data col-lection and enrichment framework without formalmodeling, and the creation of dialogue policies fromthe collected data.
We also propose a framework forevaluating learned policies.
We show, for the sce-nario in our case study, that these techniques deliverpromising levels of performance, and point to possi-ble future developments in data-driven dialogue pol-icy creation and evaluation.2 Case studyFor our case study we selected an existing dialoguesystem scenario designed for Tactical Questioningtraining (Traum et al, 2008).
The character targetedin our study, Amani, is modeled closely after theAmani Tactical Questioning character described byGandhe et al (2009) and Artstein et al (2009).
Tac-tical Questioning dialogues are those in which smallunit military personnel, usually on patrol, hold con-versations with individuals to produce informationof military value.
A tactical questioning dialogue39system is a simulation training environment wherevirtual characters play the role of a person beingquestioned.
Tactical questioning characters are de-signed to be non-cooperative at times.
They mayanswer some of the interviewers questions in a coop-erative manner, but may refuse to answer other ques-tions, or intentionally provide incorrect answers.Therefore the interviewer is encouraged to conductthe interview in a manner that induces cooperationfrom the character: building rapport with the char-acter, addressing their concerns, making promisesand offers, as well as threatening or intimidating thecharacter; the purpose of the dialogue system is toallow trainees to practice these strategies in a realis-tic setting (Gandhe et al, 2009).This type of scenario is a good testbed for ourproposed learning and evaluation framework, sinceit involves both flexible conversational choices andwell-defined constraints regarding the disclosure ofspecific information.
In the Amani scenario, the userplays the role of a commander of a small militaryunit in Iraq whose unit had been attacked by sniperfire.
The user interviews a character named Amaniwho was a witness to the incident and is thought tohave some information about the identity of the at-tackers.
Amani is willing to tell the interviewer ev-erything she knows provided that the user promisesher safety, secrecy, and small monetary compensa-tion for the information (Artstein et al, 2009).An exhaustive formal definition of Amani?s idealdialogue policy might include a large number ofrules covering a wide range of user utterance types.The key constraints for the training simulation, how-ever, can be stated simply with a few rules governingthe release of five pieces of information that Amaniknows.
Amani will only reveal one of these pieces ofinformation if a precondition is met.
Table 1 showshow certain information relates to each of the pre-conditions in Amani?s dialogue policy.
Amani canonly reveal a fact from the first column if the userpromised her an item from the second column.
Forexample, Amani can only tell the user the shooter?sname if the user promised her safety.
If the userhas not promised safety, Amani will ask him forsafety.
If the user refuses to promise safety, Amaniwill either decline to answer the question or lie tothe interviewer.
Amani does keep track of the user?spromises and once she is promised safety, she wouldinformation preconditionabout shooter?s name safetyabout shooter?s description safetyabout shooter?s location secrecyabout the occupant of the shop secrecyabout shooter?s daily routine moneyTable 1: Amani?s dialogue policy.not ask for it again.While the key constraints for Amani?s policy, assummarized in Table 1, may be easily expressedin terms of rules involving dialogue-acts, the restof Amani?s behavior is more open-ended and un-derspecified.
Ideally, the system designers wouldlike for the character to obey conversational conven-tions (such as responding appropriately to greetings,thankings, etc.).
Her responses to other user utter-ances should match human intuition about what agood response would be, but specific responses arenot generally dictated by the goals for the trainingsimulation.
There is therefore room for some flex-ibility, and also for the character to reply that shedoes not understand.
Of course, her conversationalrepertoire is inevitably limited by the available au-thoring and development effort as well as languageprocessing challenges.3 Data collectionThe exponential number of possible utterances anddialogue paths in even a simple conversational dia-logue scenario such as the Amani scenario suggeststhat learning acceptable dialogue behavior from sur-face text examples without annotation or formalmodeling would require a seemingly insurmount-able quantity of dialogues to serve as training data.We address this problem in a data collection frame-work with four main characteristics: (1) we sidestepthe problem of learning natural language generationby using a fixed predefined set of utterances for theAmani character.
This so-called ?utterance selec-tion?
approach has been used in a number of dia-logue systems (Zukerman and Marom, 2006; Sell-berg and Jnsson, 2008; Kenny et al, 2007, for ex-ample) and often serves as a reasonable approxima-tion to generation (Gandhe and Traum, 2010); (2)we collect dialogues from human participants who40play the parts of Amani and the commander in astructured role play framework (Section 3.1); (3) weenrich the dialogues collected in the structured roleplay step with additional paraphrases for the utter-ances of the commander, in an attempt to deal withlarge variability of natural language input, even fora limited domain conversational dialogue scenario(Section 3.2); (4) we further augment the existingdialogue data by adding acceptable alternatives tothe dialogue acts of the Amani role through the useof external referees (Section 3.3).Our data collection procedure is designed to cap-ture the necessary information for learning dialoguepolicies and evaluating their quality by approxi-mating the exponentially large dialogue variabilitywhile keeping the data collection effort tractable.3.1 Structured role playTo examine the hypothesis that dialogue policiessuch as Amani?s can be learned from examples with-out explicit rules or any kind of formal modeling,we collected dialogue data through a constrainedform of role play, which we call structured role play,where the person playing the role of Amani is en-couraged, whenever possible, to only use utterancesfrom a fixed set.
Each utterance in the available setof Amani replies corresponds roughly to one of thedialogue acts (consisting of an illocutionary forceand some semantic content) described by Artstein etal.
(2009) for their version of the Amani character.The players in the roles of Amani and the com-mander take turns producing one utterance at a time,each in a separate terminal.
The commander player,who receives a natural language description of thescenario and the goal of the commander, enters utter-ances through a teletype (chat) interface.
The Amaniplayer, who receives a natural language descrip-tion of the scenario and of Amani?s dialogue policy,chooses an utterance from a list for each dialogueturn.
The Amani player is encouraged to use an ut-terance from this list whenever possible; however,for user utterances that the Amani player judges can-not possibly be handled by any existing response, anew response can be authored (as English text) andimmediately used in the role play.
Each player seesthe other?s utterance as text in their own terminal.This closely resembles a Wizard-of-Oz setup, withthey key difference being that both dialogue partic-ipants believe they are interacting with another per-son, which is in fact the case, and the idea of a wiz-ard controlling a system is not part of the exercise.However, because the Amani player is encouragedto limit Amani?s responses to a fixed utterance set,and the dialogue is constrained to a strict turn-takingsetup that interleaves utterances from each partici-pant, the situation also differs from conventional roleplay.We collected a total of 19 dialogues and 296 ut-terances for Amani, for an average of 15.6 Amaniutterances per dialogue.3.2 Paraphrase generationThe dialogues collected through structured role playare intended for serving as training data from whichAmani?s dialogue policy can be learned.
However,to cover the natural language variability with whichdialogue acts from the commander can be expressedwould require a much larger number of dialoguesthan it would be practical to collect, since a learnedsystem that deals only with the surface text in thedialogues would need to deal both with the dia-logue policy and natural language understanding forthe scenario.
Instead, we require only that the di-alogues collected cover the desired dialogue actsfor the player role in the scenario.
To address thelanguage understanding problem (indirectly), we at-tempt to cover the variability of expression of thesedialogue acts through the collection of paraphrasesfor the commander utterances in the set of dialogues.For each commander utterance in the 19 dialogueswe collected, we had annotators create a set of para-phrases.
In creating paraphrases, annotators wereasked to take not just the original utterance into ac-count, but also its context in the dialogue.
We didnot specify a fixed number of paraphrases per utter-ance, but instead asked for as many paraphrases asthe annotator could quickly think of.Figure 1 exemplifies the paraphrases created dur-ing this process, for a target user utterance of canyou tell me what you know of the incident?.
For thisutterance, a total of 6 paraphrases were generated.We used a total of 9 annotators, who created para-phrases for the 296 utterances in the 19 dialogues.Most annotators were responsible for no more thantwo dialogues, and took on average less than 30minutes per dialogue.
The average number of para-41Previous Dialogue HistoryLieutenant: hi amaniAmani: hello.Lieutenant: how are you doing?Amani: fine thank you.Lieutenant: thank you for meeting with meAmani: you?re welcome.Target User UtteranceLieutenant: can you tell me what you know of the incident?Paraphrases:please tell me what information you have about the incidentcould you please tell me what you saw?what can you tell me about the incident?can you tell me about the incident?please, tell me what you know about the incidenttell me what you saw, pleaseSystem ResponseAmani: i saw all the shooting from my window.
what do you want to know about it?External Referees:(3 referees) i saw all the shooting from my window.
what do you want to know about it?
(2 referees) i remember that the gun fire was coming from the window on the second floor of assad?sshop.
the shop is only one story but there are apartments on top of the shop.
(1 referee) what is it you want to know about the incident?Figure 1: An enriched dialogue turn from an Amani structured role play.phrases collected per user utterance was 5.5.Our 9 annotators had differing backgrounds, rang-ing from transcribers and summer interns to experi-enced NLP researchers.
It should be noted that allhad at least some experience working with naturallanguage processing technologies.
In future work,we would like to explore using less experienced an-notators for paraphrasing.3.3 External referee annotationAlthough the paraphrase generation step helps withcoverage of the language used by the commanderin our scenario, the combination of the original di-alogues collected through structured role play andthe paraphrases do not address one crucial issue inlearning of data-driven dialogue policies, and theirautomated evaluation: at each turn, a dialogue par-ticipant has multiple valid dialogue acts that can beperformed, not a single correct one.
In other words,given the same dialogue history up to a given point,multiple human dialogue participants following thesame underspecified policy may choose different di-alogue acts to continue the dialogue, and each ofthese different choices may be perfectly acceptableand coherent.
This is one of main challenges in cre-ation and evaluation of data-driven policies, sincethe exponentially many acceptable dialogue pathsare both difficult to model explicitly, and difficultto recognize automatically when performed duringtesting.
Of course, the degree to which this is a prac-tical problem in a specific dialogue scenario dependson several factors, including how underspecified thetargeted dialogue policy is.
In our case study, thepolicy has a high level of underspecification, sinceonly behaviors related to the information in Table 1are mentioned directly, and even those are only de-scribed in natural language, without formal rigor.The rest of the policy dictates only that human play-ers in the part of Amani act according to their com-monsense in playing the role of the Amani character.However, we limit the otherwise potentially infinitepossibilities for dialogue behavior by strongly en-couraging the Amani player to perform only one of aset of predefined utterances corresponding to certaindialogue acts in the scenario.
In our experiments, thenumber of utterances available for Amani was 96.We first investigate this issue by attempting tocharacterize the amount of human variation in thechoice of one of the 96 available dialogue acts atany given point in a dialogue.
To this end, we intro-duce the idea of the external referee, who essentiallyprovides a ?second opinion?
for dialogue acts per-formed by the original role player.
The external ref-eree annotation task works as follows: (1) Startingwith an existing dialogue containing n utterances42?u1, u2, ..., un?
for the participant whose utteranceswill be externally refereed (one of the dialoguescollected through structured role play, in our casestudy, where we externally referee the Amani utter-ances), produce n dialogue histories h1, h2, ..., hn,with each hi consisting of every utterance from eachdialogue participant from the beginning of the dia-logue down to, but not including, the ith utterance inthe dialogue.
(2) For each dialogue history hi, theexternal referee (who must not be the person whoplayed a part in the original dialogue) chooses anutterance u?i from the choices available for the sce-nario, without knowledge of the original utteranceui in the dialogue from which the history was pro-duced.Figure 1 provides an example of the choices madeby 6 external referees for a single target user ut-terance.
Given the previous dialogue history andthe target user utterance (can you tell me what youknow of the incident?
), each external referee inde-pendently chose a single best utterance for the char-acter to respond with.
In the example in the figure,it can be seen that 3 of the 6 external referees chosethe same response as the original Amani player, as-serting that Amani did indeed witness the incidentand asking what the commander would like to know.The other three chose alternative responses; twoof these selected a response asserting informationabout where the gun fire was coming from, whilea third referee chose a response simply asking whatthe commander would like to know.
It is importantto note that all three of these alternative responseswould be acceptable from a design and training per-spective.In this annotation task, the task is not to pro-vide alternative dialogues, but simply one charac-ter response to each individual utterance, assumingthe fixed history of the original dialogue.
In otherwords, the annotator has no control or impact overthe dialogue history at any point, and provides onlyadditional reference utterances for possible immedi-ate continuations for each dialogue history.
It is forthis reason we call the annotator an external referee.Annotations from multiple external referees forthe dialogues collected through structured role playdo not result in a representation of the lattice of themany possible dialogue paths in the scenario, butrather an approximation that represents the possibleoptions in the immediate future of a given dialoguehistory.
The main difference is that the available his-tories are limited to those in the original dialoguesfrom structured role play.
While this may be a lim-iting factor if one attempts to model dialogue be-havior based on entire dialogue histories, since theavailable histories represent only a very sparse sam-ple of the space of valid histories, it is possible thatgood approximate models can be achieved with fac-torization of dialogues by sequences of a fixed num-ber of consecutive turns, e.g.
a model that makes asecond-order Markov assumption, considering onlythe previous two turns in the dialogue as an approx-imation of the entire history (Gandhe and Traum,2007).
This is in a way the same approximation usedin n-gram language models, but at the level of gran-ularity of sentences, rather than words.We collected annotations from 6 different exter-nal referees, with each individual referee annotatingthe entire set of 19 dialogues, and taking on averageabout two hours to complete the annotation of theentire set.
All of our external referees were very fa-miliar with the design of the Amani character, andmost had natural language processing expertise.4 Evaluation of dialogue policies withmultiple external referees4.1 External referee agreementThe dialogues and external referee annotations col-lected using the procedure described in Section 3provide a way to characterize the targeted policywith respect to human variability in choosing utter-ances from a fixed set, since the annotations includethe choices made by multiple external referees.From the annotations of utterances chosen forAmani in our 19 dialogues, we see that human an-notators agree only 49.2% of the time when choos-ing an utterance in the external referee framework.That is, given the same dialogue history, we expectthat two human role players would agree on averageslightly less than 50% of the time on what the nextutterance should be1.Based on this level of pairwise agreement, onemight conclude that using these data for either policylearning or policy evaluation is a lost cause.
How-1This represents the averaged agreement over all pairs ofexternal referees.4301020301 2 3 4 5 6Utterancecount(%)Number of distinct utterance choicesFigure 2: Distribution in number of distinct choices byexternal refereesever, this result does not necessarily indicate that hu-man raters disagree on what the correct choice is; itis more likely to reflect that there are in fact mul-tiple ?correct?
(acceptable) choices, which we cancapture through multiple annotators.The annotations from multiple external refereesin our case study support this view: Figure 2 showsthe number of distinct utterance choices made byeach of the six external referees for each specific ut-terance in the 19 dialogues collected through struc-tured role play.
Each external referee chooses onlyone utterance (out of 96 options) per Amani turn inthe 19 dialogues.
Over the 296 Amani utterancesin the entire set of dialogues, all six referees agreedunanimously on their utterance choice only 23.3%of the time.
The most frequent case, totaling almost30% of all utterances, was that the set composed bythe single choice from each of the six wizards foran utterance had exactly two distinct elements.
Foronly 1.3% of the 296 utterances did that set containthe maximum number of distinct elements (six), in-dicating complete disagreement among the externalreferees.
We note that, in this case, very low agree-ment to complete disagreement reflects a situationin dialogue where it is likely that there are many di-alogue act choices considered acceptable by the col-lective body of external referees.
In our scenario,there were at most two choices from the six refereesfor more than 50% of the Amani turns, indicatingthat in the majority of the cases there is only a smallset of acceptable dialogue acts (from the 296 avail-able), while five or more options were chosen forless than 10% of all Amani turns.For a more direct characterization of dialogue sce-narios, and also for the purposes of evaluation, we4050607080901 2 3 4 5 6 7Weakagreement(%)Number of external refereesbcbcbcbcbcFigure 3: Weak agreement between external refereesnow define a metric that reflects overall agreementin a group of external referees.
Instead of compar-ing one choice from a single referee to another singlechoice, we instead check for membership of a singlechoice cij from a single referee Ri for utterance ujin the set of choices {ckj |k 6= i} from all of theother referees {Rk|k 6= i}.
In the positive case, wesay that Ri weakly agrees with the rest of the raters{Rk|k 6= i} on the annotation of utterance uj .
Wedefine the weak agreement agrn for a set of N ex-ternal referees over a set of m utterances to be rateat which each rater Ri weakly agrees with the n?
1raters {Rk|k 6= i}, for all integer values of i rangingfrom 1 to N , inclusive.
Intuitively, weak agreementreflects two important questions: (1) how often isthe choice of a referee supported by the choice ofat least one more referee?
and (2) given a set ofn ?
1 referees, how much new information (in theform of unseen choices) should I expect to see froma new nth referee?
Figure 3 addresses these ques-tions for the scenario in our case study by showingthe weak agreement figures obtained for sets of in-creasing numbers of external referees, from 2 to 6.Each point in the graph corresponds to the averageof the weak agreement values obtained for all possi-ble ways of holding out one external referee Ri, andcomputing the weak agreement between Ri and theother referees, assuming an overall pool containingthe given number of external referees.We note that with the dialogue act choices of asingle person, coverage of the possible acceptableoptions is quite poor, corresponding only to an aver-age of 50% of the choices made by another person.44The coverage increases rapidly as two more externalreferees are added, and more slowly, although stillsteadily from there.
The rightmost point in Figure 3indicates that with a set of five external referee weshould expect to cover almost 80% of the choices ofa sixth referee.4.2 Dialogue policy evaluation with multipleexternal refereesThe weak agreement metric defined in the previ-ous section can be used to measure the quality ofautomatically learned policies, and to provide in-sight into how a learned policy compares to human-level performance.
Because it recognizes the valid-ity of multiple responses, the weak agreement metriccan help distinguish true policy errors from policychoices that are consistent with the intuitions of atleast some human referees about what the charactershould say.In particular, given the choices made by five exter-nal referees for our 19 Amani dialogues, we can ex-pect their choices to cover about 80% of the choicesa sixth person would make for what Amani shouldsay at each turn in these dialogues.
(I.e., we knowthat the weak agreement among a group of six hu-man referees is about 80% for this Amani scenario.
)We proceed to rate the quality of an automaticpolicy by computing a one-vs-others version ofweak agreement?intuitively treating our policy asif it were such a ?sixth person?, and comparing itto the other five.
Instead of computing the averageweak agreement for referees randomly selected froman entire group, as in the previous section, to eval-uate a policy, we compute its weak agreement com-pared to the combined set of human external refer-ees, as follows.
For every system utterance uj in ourset of role play dialogues, a given automatic policyP is used to select a response c?j (corresponding toa dialogue act in the domain).
We then check formembership of c?j in the set that contains only andall dialogue act choices ckj for k ranging from 1 toN , inclusive, where N is the number of external ref-erees and ckj corresponds to the kth referee?s choicefor the jth utterance.
Another way to interpret thisevaluation metric is to consider it a form of accuracythat computes the number of correct choices madeby the policy divided by the total number of choicesmade by the policy, where a choice is considered?correct?
if it matches any of the external referees?choices for a specific utterance.
For this reason, werefer to this evaluation-focused one-vs-all version ofweak agreement as weak accuracy.Based on the definition above, an automatic pol-icy with quality indistinguishable from that of aperson choosing utterances for the Amani characterwould have a weak accuracy of about 80% or higherwhen measured using a set of five external referees.We see then that this metric is far from perfect, sinceit cannot rank two policies with weak accuracy lev-els of, say, 80% and 90%.
It is also possible for apolicy that results in dialogue behavior noticeablyinferior to that of a human referee to be rated atthe same weak accuracy value for a human referee(80%).
In practice, however, weak accuracy withfive or six external referees has far greater power fordiscriminating between policies of varying quality,and ranking them correctly, than a naive version ofaccuracy, which corresponds to weak accuracy us-ing a single referee.
Furthermore, the addition ofonly a few more external referees would very likelyincrease the efficacy of the weak agreement metric.Despite the shortcomings of weak accuracy as ametric for evaluation of quality of dialogue poli-cies, it opens up a wide range of opportunities fordevelopment of learned policies.
Without an auto-mated metric, development of such techniques canbe only vaguely incremental, relying on either costlyor, more likely, infrequent human evaluations withresults that are difficult to optimize toward with cur-rent machine learning techniques.
The use of im-perfect automated metrics in situations where idealmetrics are unavailable or are impractical to deployis fairly common in natural language processing.PARSEVAL (Abney et al, 1991), commonly usedfor parser evaluation, and BLEU (Papineni et al,2002), commonly used in machine translation, aretwo examples of well-known imperfect metrics thathave been the subject of much criticism, but that arewidely agreed to have been necessary for much ofthe progress enjoyed by their respective fields.
Un-like BLEU, however, which has been shown to cor-relate with certain types of human judgment on thequality of machine translation systems, our notionof weak accuracy has not yet been demonstrated tocorrelate with human judgments on the quality of di-alogue policies, and as such it is only hypothesized45to have this property.
We leave this important stepof validation as future work.5 Learning dialogue policies fromexamples without formal modelingEquipped with a dataset with 19 dialogues in theAmani scenario (including paraphrases for the un-constrained commander utterances, and external ref-eree annotations for the constrained Amani utter-ances), and an automatic evaluation framework fordistinguishing quality differences in learned poli-cies, we now describe our experiments on learningdialogue policies from data collected in structuredrole play sessions, and enriched with paraphrasesand external referee annotations.In each of our experiments we attempt to learna dialogue policy as a maximum entropy classi-fier (Berger et al, 1996) that chooses one utteranceout of the 96 possible utterances for Amani aftereach commander utterance, given features extractedfrom the dialogue history.
This policy could be in-tegrated in a dialogue system very easily, since itchooses system utterances directly given previoususer and system utterances.
We evaluate the dia-logue policies learned in each experiment through19-fold cross-validation of our set of 19 dialogues:in each fold, we hold out one dialogue (and all of itsrelated information, such as external referee anno-tations and user utterance paraphrases) and use theremaining 18 dialogues as training data.5.1 Learning from examplesUsing only the dialogues collected in structured roleplay sessions, and no additional information fromexternal referees or paraphrases, we train the maxi-mum entropy classifier to choose a system utterancesi based on features extracted from the two previoususer utterances ui and ui?1 and the previous systemutterance si?1.
The features extracted from these ut-terances are the words present in each user utterance,and the complete text of each system utterance.
Lowfrequency words occurring fewer than 5 times in thecorpus are excluded.The weak accuracy for this simple policy is 43%,a low value that indicates that for more than half itsturns the policy chooses an utterance that was notchosen by any of the referees, giving us a reasonablelevel of confidence that this policy is of poor quality.5.2 Enhanced training with external refereesThe next experiment expands the training set avail-able to the maximum entropy classifier by addingtraining instances based on the utterances chosen bythe external referees.
For each of the training in-stances (target utterance coupled with features fromui, si?1 and ui?1) we add six new training instances,each using the same features as the original train-ing instance, but replacing the target class with thechoice made by an external referee.
Note that thiscreates identical training instances for cases whenthe same utterance is chosen by multiple annotators,which has the effect of weighting training examples.With the additional information, weak accuracy forthis policy improves to 56%, which is a large gainthat still results in a mediocre dialogue policy.5.3 Expanding training examples withparaphrasesTo help determine how much of difficulty in ourpolicy learning task is due to the related problemof natural language understanding (NLU), and howmuch is due to modeling dialogue behavior regard-less of NLU, we performed manual annotation ofdialogue acts for the user utterances, and trained apolicy as in the previous section, but using manu-ally assigned dialogue acts instead of the words foruser utterances in the dialogue history.
With thisgold-standard NLU, weak accuracy improves from56% to 67%, approaching the level of human perfor-mance, and already at a level where two out of everythree choices made by the learned policy matchesthe choice of a human referee.To bridge the gap between learning purely fromsurface text (with no formal modeling) and learn-ing from manually assigned dialogue acts specifi-cally designed to capture important information inthe scenario, we turn to the paraphrases collectedfor user utterances in our 19 dialogues.
These para-phrases are used to create additional synthetic train-ing material for the classifier, as follows: for eachtraining instance produced from a chosen system ut-terance si and previous utterances ui, si?1 and ui?1(see previous section), we create additional traininginstances keeping the target system utterance si andprevious system utterance si?1 the same, but using46a paraphrase u?i in the place of ui, and a paraphraseu?i?1 in the place of ui?1.
Training instances areadded for all possible combinations of the availableparaphrases for ui and ui?1, providing some (arti-ficial) coverage for parts of the space of possibledialogue paths that would be otherwise completelyignored during training.Training the classifier with material from the ex-ternal referees (see previous section) and additionalsynthetic training examples from paraphrases as de-scribed above produces a dialogue policy with weakaccuracy of 66%, at the same level as the policylearned with manually assigned speech acts.
It isnoteworthy that this was achieved through a verysimple and intuitive paraphrase annotation task thatrequires no technical knowledge about dialogue sys-tems, dialogue acts or domain modeling.
As men-tioned in section 3.2, paraphrases for each of the 19dialogues were generated in less than 30 minutes onaverage.6 Conclusion and future workWe introduced a framework for collection and en-richment of scenario-specific dialogues based onlyon tasks that require no technical knowledge.
Datacollected in this framework support novel ap-proaches not just for learning dialogue policies,but perhaps more importantly for evaluating learnedpolicies, which allows us to examine different tech-niques using an objective automatic metric.Although research on both learning and evalu-ating dialogue policies is still in early stages, thiscase study and proof-of-concept experiments serveto illustrate the basic ideas of external referee andparaphrase annotation, and the use of multiple refer-ence dialogue act choices in evaluation of dialoguepolicies, in a way similar to how multiple referencetranslations are used in evaluation of machine trans-lation systems.
We do not consider this line of re-search a replacement for or an alternative to for-mal modeling of domains and dialogue behavior,but rather as an additional tool in the community?scollective arsenal.
There are many unexplored av-enues for including data-driven techniques withinrule-based frameworks and vice-versa.In future work we intend to further validate theideas presented in this paper by performing addi-tional collection of dialogues in the Amani domainto serve as a virgin test set, and applying thesetechniques to other dialogue domains and scenar-ios.
We also plan to refine the weak accuracy andweak agreement metrics to take into account thelevel of agreement within utterances to reflect thatsome parts of dialogues may be more open-endedthan others.
Finally, we will conduct human evalu-ations of different policies to begin validating weakaccuracy as an automatic metric for evaluation of di-alogue policies.AcknowledgmentsThe project or effort described here has been spon-sored by the U.S. Army Research, Development,and Engineering Command (RDECOM).
State-ments and opinions expressed do not necessarily re-flect the position or the policy of the United StatesGovernment, and no official endorsement should beinferred.
We would also like to thank Ron Artstein,Sudeep Gandhe, Fabrizio Morbini, Angela Nazar-ian, Susan Robinson, Michael Rushforth, and DavidTraum.ReferencesS.
Abney, S. Flickenger, C. Gdaniec, C. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, M. Marcus, S. Roukos, B. San-torini, and T. Strzalkowski.
1991.
Procedure for quan-titatively comparing the syntactic coverage of englishgrammars.
In E. Black, editor, Proceedings of theworkshop on Speech and Natural Language, HLT ?91,pages 306?311, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ron Artstein, Sudeep Gandhe, Michael Rushforth, andDavid R. Traum.
2009.
Viability of a simple dialogueact scheme for a tactical questioning dialogue system.In DiaHolmia 2009: Proceedings of the 13th Work-shop on the Semantics and Pragmatics of Dialogue,page 43?50, Stockholm, Sweden, June.Adam L. Berger, Stephen D. Della Pietra, and VincentJ.
D. Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
ComputationalLinguistics, 22(1):39?71.Sudeep Gandhe and David R. Traum.
2007.
Creatingspoken dialogue characters from corpora without an-notations.
In Proceedings of Interspeech-07, 08/2007.Sudeep Gandhe and David R. Traum.
2010.
I?ve said itbefore, and i?ll say it again: An empirical investigation47of the upper bound of the selection approach to dia-logue.
In 11th annual SIGdial Meeting on Discourseand Dialogue.Sudeep Gandhe, Nicolle Whitman, David R. Traum, andRon Artstein.
2009.
An integrated authoring tool fortactical questioning dialogue systems.
In 6th Work-shop on Knowledge and Reasoning in Practical Dia-logue Systems, Pasadena, California, July.Sina Jafarpour, Chris Burges, and Alan Ritter.
2009.
Fil-ter, rank, and transfer the knowledge: Learning to chat.In Proceedings of the NIPS Workshop on Advances inRanking.Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-ton Leuski, and Albert A. Rizzo.
2007.
Virtual pa-tients for clinical therapist skills training.
In Proceed-ings of the 7th international conference on IntelligentVirtual Agents, IVA ?07, pages 197?210, Berlin, Hei-delberg.
Springer-Verlag.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
In Pro-ceedings of HLT-NAACL.Linus Sellberg and Arne Jnsson.
2008.
Using ran-dom indexing to improve singular value decomposi-tion for latent semantic analysis.
In Proceedings of theSixth International Language Resources and Evalua-tion (LREC?08), Marrakech, Morocco, may.David R. Traum, Anton Leuski, Antonio Roque, SudeepGandhe, David DeVault, Jillian Gerten, Susan Robin-son, and Bilyana Martinovski.
2008.
Natural lan-guage dialogue architectures for tactical questioningcharacters.
In Army Science Conference, Florida,12/2008.Ingrid Zukerman and Yuval Marom.
2006.
A corpus-based approach to help-desk response generation.Computational Intelligence for Modelling, Controland Automation, International Conference on, 1:23.48
