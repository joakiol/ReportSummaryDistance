Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 298?301,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsEliciting interactional phenomena in human-human dialoguesJoakim GustafsonKTH Speech Music & Hearingjocke@speech.kth.seMiray MerkesKTH Speech Music & Hearingmiray@kth.seAbstractIn order to build a dialogue system that can inte-ract with humans in the same way as humans in-teract with each other, it is important to be ableto collect conversational data.
This paper intro-duces a dialogue recording method where aneavesdropping human operator sends instruc-tions to the participants in an ongoing human-human task-oriented dialogue.
The purpose ofthe instructions is to control the dialogue pro-gression or to elicit interactional phenomena.The recordings were used to build a Swedishsynthesis voice with disfluent diphones.1 BackgroundOur research group have a long-standing interest inhuman conversational behaviour and a special in-terest in its mimicry and evaluation in spoken dia-logue systems (Edlund et al, 2008).
In human-human conversations both parties continuously andsimultaneously contribute actively and interac-tively to the conversation.
Listeners actively con-tribute by providing feedback during the other?sspeech, and speakers continuously monitor the re-actions to their utterances (Clark, 1996).
If spokendialogue systems are to achieve the responsivenessand flexibility found in human-human interaction,it is essential that they process information incre-mentally and continuously rather than in turn sizedchunks (Dohsaka & Shimazu, 1997, Skantze &Schlangen, 2009).
These systems need to be able tostop speaking in different manners depending onwhether it has finished what it planned to say or ifit was interrupted mid-speech by the user.
In orderto be responsive, the system might also need tostart talking before it has decided exactly what tosay.
In this case it has to be able to generate inter-actional cues that restrain the user from start speak-ing while the system plans the last part.To date very few spoken dialogues systems cangenerate crucial and commonly used interactionalcues.
Adell et al (2007) have developed a set ofrules for synthesizing filled pauses and repetitionswith PSOLA.
Unit selection synthesizers are oftenused in dialogue systems, but a problem with theseis that even though most databases have been care-fully designed and read, they are not representativeof ?speech in use?
(Campbell & Mokhiari, 2003).There are examples of synthesizers that have beentrained on speech in use, like Sundaram & Naraya-nan (2003) that used a limited-domain dialoguecorpus of transcribed human utterances as input foroffline training of a machine learning system thatcould insert fillers and breathing at the appropriateplaces in new domain-related texts.
However, thesewere synthesized with a unit selection voice thathad been trained on lecture speech.When modelling talk-in-use it is important tostudy representative data.
The problem with study-ing real dialogues is that the interesting interac-tional phenomena often are sparsely occurring andvery context dependent.
When conducting researchon spontaneous speech you have the option to usecontrolled or uncontrolled conditions.
Anderson etal., 1991) recorded unscripted conversations in amap task exercise that had been carefully designedto elicit interactional phenomena.
When using con-trolled conditions in a study you risk to manipulatethe data, while in uncontrolled conditions there?s arisk that the conversation goes out of hand whichleads to a lot of unnecessary material (Bock, 1996).Bock suggests a set of eliciting methods to be usedwhen studying disfluent speech.
If the goal is tostudy speech errors and interruptions, a situationwith two competing humans is useful.
If the goal isto study hesitations and self-interruptions, distract-ing events can be used to disrupt the flow ofspeech.298Say nothing at pauses Talk slowlyThe Wizard?s GUISay nothing at pauses Talk slowlyThe Shopkeeper?s GUI      The Customer?s GUIFigure 1.
The GUIs used by the wizard and subjects.This paper presents a new method for elicitation ofinteractional phenomena, with the goal of reducingthe amount of necessary dialogue recordings.
Inthis method an eavesdropping human operatorsends instructions two subjects as they engage in atask-oriented dialogue.
The purpose of these in-structions is either to control the dialogue progres-sion or to elicit certain interactional phenomena.The recordings from two sessions were used tobuild a synthesis voice with disfluent diphones.
Ina small synthesis study on generation of disfluentconversational utterances this voice was comparedwith a commercial Swedish diphone voice basedon read speech.
The subjects rated the createdvoice as more natural than the commercial voice.2 MethodA dialogue collection environment has been devel-oped that allows a human operator (Wizard) to ea-vesdrop an ongoing computer-mediated human-human conversation.
It also allows the Wizard tosend instructions to the interlocutors during theirconversation, see Figure 1.
The purpose of the in-structions is to control the progression of the task-oriented dialogue and to elicit interactional pheno-mena, e.g.
interruptions and hesitations.
The Wizardhas access to graphical and textual instructions.Graphical instructions are pictures that are manipu-lated or text labels that are changed.
Textual instruc-tions are scrolled in from the right at the bottom ofthe screen.
They can be of three categories: Emo-tional instructions that tell the receiver to act emo-tional (e.g.
act grumpy); Task-related instructionsthat require the receiver to initiate a certain sub-tasks (e.g.
buy a red car); and Dialogue flow relatedinstructions that tell the receiver to change his wayof speaking, (e.g.
speak fast, do not pause).3 The pilot studyThe DEAL system is a speech-enabled computergame currently under development, that will beused for conversational training for second lan-guage learners of Swedish (Hjalmarsson, 2008).
Inthis system an embodied conversational character(ECA) acts as a shopkeeper in a flea trade-marketand the user is a customer.
The developed envi-ronment was adapted to the DEAL domain, and ina pilot study two human subjects were instructed toact as shopkeeper and customer.
They were givenwritten persona descriptions and were then placedin separate rooms.
They interacted via a three-partySkype call, which allowed the Wizard to eavesdroptheir conversation.
In order to get a situation thatwas similar to the DEAL system, the subjects sawan avatar with lip movements driven by, and insynchrony with, the other subjects?
speech.
In or-der to achieve this, the SynFace system was used,which introduced a 200 ms delay in each direction(Beskow et al, 2004).
Apart from the avatar theinterfaces also contained pictures of objects cur-rently for sale with accompanying prices, see Fig-ure 1.
At the bottom of the screen there was a blackarea where the subjects got the textual instructionsfrom the Wizard.The eavesdropping Wizard was placed in athird room, with an interface that allowed her tocontrol the current set of objects and prices on thesubjects?
screens.
The Wizard interface also con-tained an area for the textual instructions.
In orderto distort the dialogue flow some of the instruc-tions involved sending instructions to both subjectsat the same time.
A main idea is to instruct one ofthe interlocutors to display a verbal behavior thatwill elicit interactional phenomena in the other di-alogue partner's contributions.
Table 1 shows someexamples of the different types of textual instruc-tions to the subjects and their intended effect onthe shopkeeper party in an ongoing conversation.The Wizard interface also gave access to auto-mated instructions that follows a pre-scripted ma-nuscript in order to facilitate consistent instructionsacross different sessions.
This also made it possibleto transmit multiple successive instructions withhigh speed and a minimum risk of mistakes.299ShopkeeperreactionGraphical Emotional Task related Dialog flow relatedHesitation Show an ambiguouspicture (S)Be wining and talk abouthow unfair life is (S)Sell blue car (S)Buy red car (C)Talk slowly (S)Say nothing at pauses (C)Interruption Change picture in midspeech (S)Be a annoying customer (C) Tell your price (S)Tell your price (C)Speak without pauses (S)Try to speak all the time (C)Change ofsub-taskShow a picture (S) Discuss the advantages of acertain item (S)Sell the red car (S) Ask a lot of questions (C)Answer with questions (S)Table 1.
Examples of instruction types and their intended reaction in the shopkeeper?s subsequent turn(s).
The re-ceiver of the instruction is indicated by S (Shopkeeper) and C (Customer).4 The effect of the Wizard?s instructionTwo half-hour conversations were recorded wherethe same male subject (acting as shopkeeper) inte-racted with two different female subjects (acting ascustomers).
The audio recordings were synchro-nized with the instructions that had been submittedby the Wizard during the conversation.
The effectsof the instructions were analyzed by inspectingboth subjects?
turns following an instruction fromthe Wizard.
The analysis was focused on the dis-ruptive effect of the instructions, and it showedthat they often lead to turns that contained hesita-tions, interruptions and pauses.
The task-relatedinstructions lead to disfluent speech in half of thesucceeding turns, while the dialogue flow relatedinstructions, the emotional instructions and thegraphical instructions led to disfluent turns in twothirds of the cases.
The analysis of the instructions?effect on the disfluency rates revealed that the onesthat changed the task while the subjects talkedwere very efficient, e.g.
changing the price while itwas discussed.
The effect on the disfluency rateswas most substantial when contradictive instruc-tions were given to both subjects at the same time.In order to get a baseline of disfluency rates inhuman-human dialogues in the current domain, thedialogue data was compared with data recorded ina previous DEAL recording.
In this study 8 dialo-gues were recorded where two subjects role-playedas a shopkeeper and a customer, but without thecontrolling Wizard used in the present study(Hjalmarsson, 2008).
In these recordings approx-imately one third of the turns contained disfluentspeech.
This indicates that the disfluency ratesfound after the instructions in the current study area higher than in the previous DEAL recording.
Fi-nally we analyzed the effect of the instructions onthe dialogue progression.
The instructions werevery helpful in keeping the discussion going andthe task oriented instructions provided useful guid-ance to the subjects in their role-playing.5 A speech synthesis experimentIn a second experiment the goal was to evaluate twomethods for collecting conversational data for build-ing a corpus-based conversational speech synthesiz-er: collecting a controlled human-human role-playing dialogue or a recording a human that reads adialogue transcription with tags for interruptions andhesitations.
In this experiment the recordings of themale subject that acted as shopkeeper were used.
20of his utterances that contained hesitations, inter-ruptions and planned pauses were selected.
Newversions of these utterances were created, wherethe disruptions were removed.
In order to verifythat the disruptive sections could be synthesized innew places a set of test sentences were constructedthat included their immediate contexts.
Finally,new versions of the new test sentences werecreated, that had added tags for disruptions.
Alltypes of utterances were read by the original malespeaker.
Both the original dialogue recordings andthe read utterances were phonetically transcribedand aligned in order to build a small diphone voicewith the EXPROS tool (Gustafson & Edlund, 2008).This diphone voice contained fillers, truncated pho-nemes and audible breathing.All types of utterances were re-synthesized withthe newly created voice and with a Swedish com-mercial diphone voice that was trained on clearread speech.
While re-synthesizing the originalrecordings all prosodic features (pitch, durationand loudness) were kept.
The main difference be-tween the two voices was the voice quality: thecommercial voice is trained on clear read speech,while the new voice was created from the dialoguerecordings contains both reduced and truncateddiphones.Secondly, a number of utterances were synthe-sized, where disfluent sections were inserted intofluently read sentences.
For both voices the disflu-ent sections?
original pitch, duration and loudnesswere kept.
As in the previous case the main differ-300ence between the two cases is that the newlycreated also made use of its disfluent diphones.The disfluent sections were either taken from theoriginal dialogue recordings or from the set of readsentences with tags for disfluencies.6 Preliminary synthesis evaluation16 subjects participated in a listening test, wherethey were told to focus on the disrupted parts of theutterances.
They were instructed to indicate whenthey could detect the following disruptions: hesita-tion, pause, interruption and correction.
They werealso asked to assess on a six-graded likert scalehow natural these sounded and how easy it was todetect the disrupted parts.
Results show that dis-rupted utterances that were synthesized with thenew voice were rated as natural in two thirds of thecases, while the ones that were generated withcommercial synthesis voice, that lacked disfluentdiphones, was rated as natural in half of the cases.Kruskal-Wallis rank sums were performed, and theinterrupted utterances generated by new voice wassignificantly more natural than those generatedwith the commercial voice (p=0.001).
When com-paring how easy it was to detect the disrupted partsboth versions are comparable (90% of them wereeasy to detect, with no significant difference).In order to analyze the difference between realand pretended disruptions, the subjects were askedto compare re-synthesis of the of disrupted dialo-gue turns with corresponding read versions.
Theywere asked to judge which of the two they thoughtcontained a pretended disruption.
When comparingre-synthesis of complete utterances from either ofthese types they were able to detect the versionwith pretended disruptions in 60% of the cases.
Incases where the disfluent parts were moved to newfluently read sentences the users could not tellwhich version contained a pretended disruption.This is probably because they rated how the wholesentence sounded, rather than only the disruptedpart.
These differences were significant accordingto a chi-square test.
Finally, the subjects?
ability toidentify the different types of disfluencies whensynthesized by the two voices was compared.
Forboth voices, about 80% of the hesitations and inter-ruptions were correctly identified, while only 70%of the planned pauses were correctly identified.
Forboth voices about 85% of the missed pauses wereinstead identified as hesitations or interruptions.For the new voice most of them were identified ashesitations, while they were mostly misinterpretedas interruptions for the commercial voice.
Theshare of inserted interruptions is the only signifi-cant identification difference between the twovoices.
This is not surprising since they both usedthe pitch, power and durations from the originalhuman recordings, while only the new voice alsohad access to truncated diphones.This pilot study showed that the instructionsfrom the Wizards were useful both to control thedialogue flow and to elicit interactional phenome-na.
Finally, the male participant reported that itwas hard to pretend to be disfluent while readingdialogue transcripts where this was tagged.AcknowledgementsThis research is supported by MonAMI, an IntegratedProject under the European Commission (IP-035147).ReferencesAdell, J., Bonafonte, A., & Escudero, D. (2007).
Filled pausesin speech synthesis: towards conversational speech.
In Proc.of Conference on Text, Speech and Dialogue( LNAI 07)Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G.,Garrod, S., Isard, S., Kowtko, J., McAllister, J., Miller, J.,Sotillo, C., Thompson, H., & Weinert, R. (1991).
TheHCRC Map Task corpus.
Language and Speech, 34(4).Beskow, J., Karlsson, I., Kewley, J., & Salvi, G. (2004).SYNFACE - A talking head telephone for the hearing-impaired.
In Miesenberger, K., Klaus, J., Zagler, W., &Burger, D.
(Eds.
), Computers Helping People with SpecialNeeds.
Springer-Verlag.Bock, K. (1996).
Language production: Methods and metho-dologies.
In Psychonomic Bulletin and Review.Campbell, N., & Mokhiari, P. (2003).
Using a Non-Spontaneous Speech Synthesiser as a Driver for a Spontane-ous Speech Synthesiser.
In Proceedings of ISCA & IEEEWorkshop on Spontaneous Speech Processing and Recogni-tion.
Tokyo, Japan.Clark, H. H. (1996).
Using language.
Cambridge, UK: Cam-bridge University Press.Dohsaka, K., & Shimazu, A.
(1997).
System architecture forspoken utterance production in collaborative dialogue.
InWorking Notes of IJCAI 1997 Workshop on Collaboration,Cooperation and Conflict in Dialogue Systems.Edlund, J., Gustafson, J., Heldner, M., & Hjalmarsson, A.(2008).
Towards human-like spoken dialogue systems.Speech Communication, 50(8-9).Gustafson, J., & Edlund, J.
(2008).
expros: a toolkit for explo-ratory experimentation with prosody in customized diphonevoices.
In Proceedings of PIT 2008.Hjalmarsson, A.
(2008).
Speaking without knowing what tosay... or when to end.
In Proceedings of SIGDial 2008.Skantze, G., & Schlangen, D. (2009).
Incremental dialogueprocessing in a micro-domain.
In Proceedings of EACL-09.Sundaram, S., & Narayanan, S. (2003).
An empirical texttransformation method for spontaneous speech synthesizers.In Proceedings of Interspeech 2003, Switzerland.301
