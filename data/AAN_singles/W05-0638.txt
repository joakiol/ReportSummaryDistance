Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 233?236, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsExploiting Full Parsing Information to Label Semantic Roles Using anEnsemble of ME and SVM via Integer Linear ProgrammingTzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, Wen-Lian HsuInstitute of Information ScienceAcademia SinicaTaipei 115, Taiwan{thtsai, cwwu, sbb, hsu}@iis.sinica.edu.twAbstractIn this paper, we propose a method thatexploits full parsing information by repre-senting it as features of argument classifi-cation models and as constraints in integerlinear learning programs.
In addition, totake advantage of SVM-based and Maxi-mum Entropy-based argument classifica-tion models, we incorporate their scoringmatrices, and use the combined matrix inthe above-mentioned integer linear pro-grams.
The experimental results show thatfull parsing information not only in-creases the F-score of argument classifi-cation models by 0.7%, but alsoeffectively removes all labeling inconsis-tencies, which increases the F-score by0.64%.
The ensemble of SVM and MEalso boosts the F-score by 0.77%.
Oursystem achieves an F-score of 76.53% inthe development set and 76.38% in TestWSJ.1 IntroductionThe Semantic Role Labeling problem can be for-mulated as a sentence tagging problem.
A sentencecan be represented as a sequence of words, asphrases (chunks), or as a parsing tree.
The basicunits of a sentence are words, phrases, and con-stituents in these representations, respectively..Pradhan et al (2004) established that Constituent-by-Constituent (C-by-C) is better than Phrase-by-Phrase (P-by-P), which is better than Word-by-Word (W-by-W).
This is probably because theboundaries of the constituents coincide with thearguments; therefore, C-by-C has the highest ar-gument identification F-score among the three ap-proaches.In addition, a full parsing tree also providesricher syntactic information than a sequence ofchunks or words.
Pradhan et al (2004) comparedthe seven most common features as well as severalfeatures related to the target constituent?s parentand sibling constituents.
Their experimental resultsshow that using other constituents?
informationincreases the F-score by 6%.
Punyakanok et al(2004) represent full parsing information as con-straints in integer linear programs.
Their experi-mental results show that using such informationincreases the argument classification accuracy by1%.In this paper, we not only add more full parsingfeatures to argument classification models, but alsorepresent full parsing information as constraints ininteger linear programs (ILP) to resolve label in-consistencies.
We also build an ensemble of twoargument classification models: Maximum Entropyand SVM by combining their argument classifica-tion results and applying them to the above-mentioned ILPs.2 System ArchitectureOur SRL system is comprised of four stages: prun-ing, argument classification, classification modelincorporation, and integer linear programming.This section describes how we build these stages,including the features used in training the argu-ment classification models.2.1 Pruning233When the full parsing tree of a sentence is avail-able, only the constituents in the tree are consid-ered as argument candidates.
In CoNLL-2005, fullparsing trees are provided by two full parsers: theCollins parser (Collins, 1999)  and the Charniakparser (Charniak, 2000).
According to Punyakanoket al (2005), the boundary agreement of Charniakis higher than that of Collins; therefore, we choosethe Charniak parser?s results.
However, there aretwo million nodes on the full parsing trees in thetraining corpus, which makes the training time ofmachine learning algorithms extremely long.
Be-sides, noisy information from unrelated parts of asentence could also affect the training of machinelearning models.
Therefore, our system exploits theheuristic rules introduced by Xue and Palmer(2004) to filter out simple constituents that areunlikely to be arguments.
Applying pruning heuris-tics to the output of Charniak?s parser effectivelyeliminates 61% of the training data and 61.3% ofthe development data, while still achieves 93% and85.5% coverage of the correct arguments in thetraining and development sets, respectively.2.2 Argument ClassificationThis stage assigns the final labels to the candidatesderived in Section 2.1.
A multi-class classifier istrained to classify the types of the arguments sup-plied by the pruning stage.
In addition, to reducethe number of excess candidates mistakenly outputby the previous stage, these candidates can be la-beled as null (meaning ?not an argument?).
Thefeatures used in this stage are as follows.Basic Features?
Predicate ?
The predicate lemma.?
Path ?
The syntactic path through theparsing tree from the parse constituent be-ing classified to the predicate.?
Constituent Type?
Position ?
Whether the phrase is locatedbefore or after the predicate.?
Voice ?
passive: if the predicate has a POStag VBN, and its chunk is not a VP, or it ispreceded by a form of ?to be?
or ?to get?within its chunk; otherwise, it is active.?
Head Word ?
calculated using the headword table described by Collins (1999).?
Head POS ?
The POS of the Head Word.?
Sub-categorization ?
The phrase structurerule that expands the predicate?s parentnode in the parsing tree.?
First and Last Word/POS?
Named Entities ?
LOC, ORG, PER, andMISC.?
Level ?
The level in the parsing tree.Combination Features?
Predicate Distance Combination?
Predicate Phrase Type Combination?
Head Word and Predicate Combination?
Voice Position CombinationContext Features?
Context Word/POS ?
The two words pre-ceding and the two words following thetarget phrase, as well as their correspond-ing POSs.?
Context Chunk Type ?
The two chunkspreceding and the two chunks followingthe target phrase.Full Parsing FeaturesWe believe that information from related constitu-ents in the full parsing tree helps in labeling thetarget constituent.
Denote the target constituent byt.
The following features are the most commonbaseline features of t?s parent and sibling constitu-ents.
For example, Parent/ Left Sibling/ Right Sib-ling Path denotes t?s parents?, left sibling?s, andright sibling?s Path features.?
Parent / Left Sibling / Right SiblingPath?
Parent / Left Sibling / Right SiblingConstituent Type?
Parent / Left Sibling / Right Sibling Po-sition?
Parent / Left Sibling / Right SiblingHead Word?
Parent / Left Sibling / Right SiblingHead POS?
Head of PP parent ?
If the parent is a PP,then the head of this PP is also used as afeature.Argument Classification Models234We use all the features of the SVM-based and ME-based argument classification models.
All SVMclassifiers are realized using SVM-Light with apolynomial kernel of degree 2.
The ME-basedmodel is implemented based on Zhang?s MaxEnttoolkit1 and L-BFGS (Nocedal and Wright, 1999)method to perform parameter estimation.2.3 Classification Model IncorporationWe now explain how we incorporate the SVM-based and ME-based argument classification mod-els.
After argument classification, we acquire twoscoring matrices, PME and PSVM, respectively.
In-corporation of these two models is realized byweighted summation of PME and PSVM as follows:P?
= wMEPME + wSVMPSVMWe use P?
for the objective coefficients of theILP described in Section 2.4.2.4 Integer Linear Programming (ILP)To represent full parsing information as features,there are still several syntactic constraints on aparsing tree in the SRL problem.
For example, on apath of the parsing tree, there can be only one con-stituent annotated as a non-null argument.
How-ever, it is difficult to encode this constraint in theargument classification models.
Therefore, we ap-ply integer linear programming to resolve inconsis-tencies produced in the argument classificationstage.According to Punyakanok et al (2004), given aset of constituents, S, and a set of semantic rolelabels, A, the SRL problem can be formulated asan ILP as follows:Let zia be the indicator variable that representswhether or not an argument,  a, is assigned to anySi ?
S; and let pia = score(Si = a).
The scoring ma-trix P composed of all pia is calculated by the ar-gument classification models.
The goal of this ILPis to find a set of assignments for all zia that maxi-mizes the following function:???
?S AiS aiaia zp .Each Si?
S should have one of these argumenttypes, or no type (null).
Therefore, we have?
?=Aaiaz 1 .Next, we show how to transform the constraints in1 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlthe filter function into linear equalities or inequali-ties, and use them in this ILP.Constraint I: No overlapping or embeddingFor arguments Sj1 , .
.
.
, Sjk  on the same path in afull parsing tree, only one argument can be as-signed to an argument type.
Thus, at least k ?
1arguments will be null, which is represented by ?in the following linear equality:?=?
?kij ki11z ?
.Constraint II: No duplicate argument classesWithin the same sentence, A0-A5 cannot appearmore than once.
The inequality for A0 is therefore:?=?kiiz1A0 1.Constraint III: R-XXX argumentsThe linear inequalities that represent A0 and itsreference type R-A0 are:?=???
?kimi zzMm1A0RA0:},...,1{ .Constraint IV: C-XXX argumentsThe continued argument XXX has to occur beforeC-XXX.
The linear inequalities for A0 are:??=???
?11A0CA0:},...,2{mimj zzMm i .Constraint V: Illegal argumentsFor each verb, we look up its allowed roles.
Thisconstraint is represented by summing all the corre-sponding indicator variables to 0.3 Experiment Results3.1 Data and Evaluation MetricsThe data, which is part of the PropBank corpus,consists of sections from the Wall Street Journalpart of the Penn Treebank.
All experiments werecarried out using Section 2 to Section 21 for train-ing, Section 24 for development, and Section 23for testing.
Unlike CoNLL-2004, part of the Browncorpus is also included in the test set.3.2 ResultsTable 1 shows that our system makes little differ-ence to the development set and Test WSJ.
How-ever, due to the intrinsic difference between theWSJ and Brown corpora, our system performs bet-ter on Test WSJ than on Test Brown.235Precision Recall F?=1Development 81.13% 72.42% 76.53Test WSJ 82.77% 70.90% 76.38Test Brown 73.21% 59.49% 65.64Test WSJ+Brown 81.55% 69.37% 74.97Test WSJ Precision Recall F?=1Overall 82.77% 70.90% 76.38A0 88.25% 84.93% 86.56A1 82.21% 72.21% 76.89A2 74.68% 52.34% 61.55A3 78.30% 47.98% 59.50A4 84.29% 57.84% 68.60A5 100.00% 60.00% 75.00AM-ADV 64.19% 47.83% 54.81AM-CAU 70.00% 38.36% 49.56AM-DIR 38.20% 40.00% 39.08AM-DIS 83.33% 71.88% 77.18AM-EXT 86.67% 40.62% 55.32AM-LOC 63.71% 41.60% 50.33AM-MNR 63.36% 48.26% 54.79AM-MOD 98.00% 97.64% 97.82AM-NEG 99.53% 92.61% 95.95AM-PNC 44.44% 17.39% 25.00AM-PRD 50.00% 20.00% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 83.21% 61.09% 70.45R-A0 91.08% 86.61% 88.79R-A1 79.49% 79.49% 79.49R-A2 87.50% 43.75% 58.33R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 25.00% 40.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 92.31% 57.14% 70.59R-AM-MNR 25.00% 16.67% 20.00R-AM-TMP 72.73% 61.54% 66.67V 97.32% 97.32% 97.32Table 1.
Overall results (top) and detailed resultson the WSJ test (bottom).Precision Recall F?=1ME w/o parsing 77.28% 70.55% 73.76%ME 78.19% 71.08% 74.46%ME with ILP 79.57% 71.11% 75.10%SVM 79.88% 72.03% 75.76%Hybrid 81.13% 72.42% 76.53%Table 2.
Results of all configurations on the devel-opment set.From Table 2, we can see that the model withfull parsing features outperforms the model with-out the features in all three performance matrices.After applying ILP, the performance is improvedfurther.
We also observe that SVM slightly outper-forms ME.
However, the hybrid argument classifi-cation model achieves the best results in all threemetrics.4 ConclusionIn this paper, we add more full parsing features toargument classification models, and represent fullparsing information as constraints in ILPs to re-solve labeling inconsistencies.
We also integratetwo argument classification models, ME and SVM,by combining their argument classification resultsand applying them to the above-mentioned ILPs.The results show full parsing information increasesthe total F-score by 1.34%.
The ensemble of SVMand ME also boosts the F-score by 0.77%.
Finally,our system achieves an F-score of 76.53% in thedevelopment set and 76.38% in Test WSJ.AcknowledgementWe are indebted to Wen Shong Lin and Prof. FuChang for their invaluable advice in data pruning,which greatly speeds up the training of our ma-chine learning models.ReferencesX.
Carreras and L. M?rquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.In Proceedings of the CoNLL-2005.E.
Charniak.
2000.
A Maximum-Entropy-InspiredParser.
Proceedings of the NAACL-2000.M.
J. Collins.
1999.
Head-driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.J.
Nocedal and S. J. Wright.
1999.
Numerical Optimiza-tion, Springer.S.
Pradhan, K. Hacioglu, V. Kruglery, W. Ward,J.
H.Martin, and D. Jurafsky.
2004.
Support VectorLearning for Semantic Argument Classification.Journal of Machine Learning.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The Neces-sity of Syntactic Parsing for Semantic Role Labeling.In Proceedings of the 19th International Joint Con-ference on Artificial Intelligence (IJCAI-05).V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic Role Labeling via Integer Linear Pro-gramming Inference.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics(COLING-04).N.
Xue and M. Palmer.
2004.
Calibrating Features forSemantic Role Labeling.
In Proceedings of theEMNLP 2004.236
