Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 8?13,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsScalable Semi-Supervised Query Classification Using Matrix SketchingYoung-Bum Kim?Karl Stratos?Ruhi Sarikaya?
?Microsoft Corporation, Redmond, WA?Columbia University, New York, NY{ybkim, ruhi.sarikaya}@microsoft.comstratos@cs.columbia.eduAbstractThe enormous scale of unlabeled textavailable today necessitates scalableschemes for representation learning innatural language processing.
For instance,in this paper we are interested in classi-fying the intent of a user query.
Whileour labeled data is quite limited, we haveaccess to virtually an unlimited amountof unlabeled queries, which could beused to induce useful representations: forinstance by principal component analysis(PCA).
However, it is prohibitive to evenstore the data in memory due to its sheersize, let alne apply conventional batchalgorithms.
In this work, we apply therecently proposed matrix sketching algo-rithm to entirely obviate the problem withscalability (Liberty, 2013).
This algorithmapproximates the data within a specifiedmemory bound while preserving thecovariance structure necessary for PCA.Using matrix sketching, we significantlyimprove the user intent classificationaccuracy by leveraging large amounts ofunlabeled queries.1 IntroductionThe large amount of high quality unlabeled dataavailable today provides an opportunity to im-prove performance in tasks with limited supervi-sion through a semi-supervised framework: learnuseful representations from the unlabeled data anduse them to augment supervised models.
Un-fortunately, conventional exact methods are nolonger feasible on such data due to scalability is-sues.
Even algorithms that are considered rela-tively scalable (e.g., the Lanczos algorithm (Cul-lum and Willoughby, 2002) for computing eigen-value decomposition of large sparse matrices) fallapart in this scenario, since the data cannot bestored in the memory of a single machine.
Con-sequently, approximate methods are needed.In this paper, we are interested in improvingthe performance for sentence classification task byleveraging unlabeled data.
For this task, supervi-sion is precious but the amount of unlabeled sen-tences is essentially unlimited.
We aim to learnsentence representations from as many unlabeledqueries as possible via principal component anal-ysis (PCA): specifically, learn a projection matrixfor embedding a bag-of-words vector into a low-dimensional dense feature vector.
However, it isnot clear how we can compute an effective PCAwhen we are unable to even store the data in thememory.Recently, Liberty (2013) proposed a scheme,called matrix sketching, for approximating a ma-trix while preserving its covariance structure.
Thisalgorithm, given a memory budget, deterministi-cally processes a stream of data points while neverexceeding the memory bound.
It does so by occa-sionally computing singular value decomposition(SVD) on a small matrix.
Importantly, the algo-rithm has a theoretical guarantee on the accuracyof the approximated matrix in terms of its covari-ance structure, which is the key quantity in PCAcalculation.We propose to combine the matrix sketching al-gorithm with random hashing to completely re-move limitations on data sizes.
In experiments, wesignificantly improve the intent classification ac-curacy by learning sentence representations from8huge amounts of unlabeled sentences, outperform-ing a strong baseline based on word embeddingstrained on 840 billion tokens (Pennington et al,2014).2 Deterministic Matrix SketchingPCA is typically performed to reduce the dimen-sion of each data point.
Let X ?
Rn?dbe adata matrix whose n rows correspond to n datapoints in Rd.
For simplicity, assume that X is pre-processed to have zero column means.
The keyquantity in PCA is the empirical covariance ma-trix X>X ?
Rd?d(up to harmless scaling).
It iswell-known that the m length-normalized eigen-vectors u1.
.
.
um?
Rdof X>X correspondingto the largest eigenvalues are orthogonal directionsalong which the variance of the data is maximized.Then if ?
?
Rd?mbe a matrix whose i-th col-umn is ui, the PCA representation of X is given byX?.
PCA has been a workhorse in representationlearning, e.g., inducing features for face recogni-tion (Turk et al, 1991).Frequently, however, the number of samples nis simply too large to work with.
As n tends tobillions and trillions, storing the entire matrix Xin memory is practically impossible.
Processinglarge datasets often require larger memory thanthe capacity of a typical single enterprise server.Clusters may enable a aggregating many boxes ofmemory on different machines, to build distributedmemory systems achieving large memory capac-ity.
However, building and maintaining these in-dustry grade clusters is not trivial and thus not ac-cessible to everyone.
It is critical to have tech-niques that can process large data within a lim-ited memory budget available in most typical en-terprise servers.One solution is to approximate the matrix withsome Y ?
Rl?dwhere l  n. Many matrix ap-proximation techniques have been proposed, suchas random projection (Papadimitriou et al, 1998;Vempala, 2005), sampling (Drineas and Kannan,2003; Rudelson and Vershynin, 2007; Kim andSnyder, 2013; Kim et al, 2015b), and hashing(Weinberger et al, 2009).
Most of these tech-niques involve randomness, which can be undesir-able in certain situations (e.g., when experimentsneed to be exactly reproducible).
Moreover, manyare not designed directly for the objective that wecare about: namely, ensuring that the covariancematrices X>X and Y>Y remain ?similar?.Input: data stream x1.
.
.
xn?
Rd, sketch size l1.
Initialize zero-valued Y ?
0l?d.2.
For i = 1 .
.
.
n,(a) Insert xito the first zero-valued row of Y .
(b) If Y has no zero-valued row,i.
Compute SVD of Y = U?V>where ?
=diag(?1.
.
.
?l) with ?1?
?
?
?
?
?l.ii.
Compute a diagonal matrix ?
with at leastdl/2e zeros by setting?j,j=?max(?2j,j?
?2bl/2c, 0)iii.
Set Y = ?V>.Output:Y ?
Rl?ds.t.???
?X>X ?
Y>Y????2?
2 ||X||2F/lFigure 1: Matrix sketching algorithm by Liberty(2013).
In the output, X ?
Rn?ddenotes the datamatrix with rows x1.
.
.
xn.A recent result by Liberty (2013) gives a de-terministic matrix sketching algorithm that tightlypreserves the covariance structure needed forPCA.
Specifically, given a sketch size l, the algo-rithm computes Y ?
Rl?dsuch that?????
?X>X ?
Y>Y??????2?
2 ||X||2F/l (1)This result guarantees that the error decreasesin O(1/l); in contrast, other approximation tech-niques have a significantly worse convergencebound of O(1/?l).The algorithm is pleasantly simple and is givenin Figure 1 for completeness.
It processes one datapoint at a time to update the sketch Y in an on-line fashion.
Once the sketch is ?full?, its SVD iscomputed and the rows that fall below a thresholdgiven by the median singular value are eliminated.This operation ensures that every time SVD is per-formed at least a half of the rows are discarded.Consequently, we perform no more than O(2n/l)SVDs on a small matrix Y ?
Rl?d.
The analy-sis of the bound (1) is an extension of the ?mediantrick?
for count sketching and is also surprisinglyelementary; we refer to Liberty (2013) for details.3 Matrix Sketching for SentenceRepresentationsOur goal is to leverage enormous quantities of un-labeled sentences to augment supervised training9for intent classification.
We do so by learning aPCA projection matrix ?
from the unlabeled dataand applying it on both training and test sentences.The matrix sketching algorithm in Figure 1 en-ables us to compute ?
on arbitrarily large data.There are many design considerations for usingthe sketching algorithm for our task.3.1 Original sentence representationsWe use a bag-of-words vector to represent asentence.
Specifically, each sentence is a d-dimensional vector x ?
Rdwhere d is the sizeof the vocabulary and xiis the count of an n-grami in the sentence (we use up to n = 3 in exper-iments); we denote this representation by SENT.In experiments, we also use a modification of thisrepresentation, denoted by SENT+, in which weexplicitly define features over the first two wordsin a query and also use intent predictions made bya supervised model.3.2 Random hashingWhen we process an enormous corpus, it can becomputationally expensive just to obtain the vo-cabulary size d in the corpus.
We propose usingrandom hashing to avoid this problem.
Specif-ically, we pre-define the hash size H we want,and then on encountering any word w we mapw ?
{1 .
.
.
H} using a fixed hash function.
Thisallows us to compute a bag-of-words vector forany sentence without knowing the vocabulary size.See Weinberger et al (2009) for a justification ofthe hashing trick for kernel methods (applicable inour setting since PCA has a kernel (dual) interpre-tation).3.3 ParallelizationThe sketching algorithm works in a sequentialmanner, processing each sentence at a time.
Whileit leaves a small memory footprint, it can take pro-hibitively long time to process a large corpus.
Lib-erty (2013) shows it is trivial to parallelize the al-gorithm: one can compute several sketches in par-allel and then sketch the conjoined sketches.
Thetheory guarantees that such layered sketches doesnot degrade the bound (1).
We implement this par-allelization to obtain an order of magnitude speed-up.3.4 Final sentence representation:Once we learn a PCA projection matrix ?, we useit in both training and test times to obtain a densefeature vector of a bag-of-words sentence repre-sentation.
Specifically, if x is the original bag-of-words sentence vector, the new representation isgiven byxnew=x||x||?x?||x?||(2)where ?
is the vector concatenation operation.This representational scheme is shown to be effec-tive in previous work (e.g., see Stratos and Collins(2015)).3.5 ExperimentTo test our proposed method, we conduct in-tent classification experiments (Hakkani-T?ur et al,2013; Celikyilmaz et al, 2011; Ji et al, 2014;El-Kahky et al, 2014; Chen et al, 2016) acrossa suite of 22 domains shown in Table 1.
An in-tent is defined as the type of content the user isseeking.
This task is part of the spoken languageunderstanding problem (Li et al, 2009; Tur andDe Mori, 2011; Kim et al, 2015c; Mesnil et al,2015; Kim et al, 2015a; Xu and Sarikaya, 2014;Kim et al, 2015b; Kim et al, 2015d).The amount of training data we used rangesfrom 12k to 120k (in number of queries) acrossdifferent domains, the test data was from 2k to20k.
The number of intents ranges from 5 to 39per domains.
To learn a PCA projection matrixfrom the unlabeled data, we collected around 17billion unlabeled queries from search logs, whichgive the original data matrix whose columns arebag-of-n-grams vector (up to trigrams) and has di-mensions approximately 17 billions by 41 billions,more specifically,X ?
R17,032,086,719?40,986,835,008We use a much smaller sketching matrix Y ?R1,000,000?1,000,000to approximate X .
Note thatcolumn size is hashing size.
We parallelized thesketching computation over 1,000 machines; wewill call the number of machines parallelized over?batch?.
In all our experiments, we train a linearmulti-class SVM (Crammer and Singer, 2002).3.6 Results of Intent Classification TaskTable 1 shows the performance of intent classifica-tion across domains.
For the baseline, SVM with-out embedding (w/o Embed) achieved 91.99% ac-curacy, which is already very competitive.
How-ever, the models with word embedding trained on10w/o Embed 6B-50d 840B-300d SENT SENT+alarm 97.25 97.68 97.5 97.68 97.74apps 89.16 91.07 92.52 94.24 94.3calendar 91.34 92.43 92.32 92.53 92.43communication 99.1 99.13 99.08 99.08 99.12finance 90.44 90.84 90.72 90.76 90.82flights 94.19 92.99 93.99 94.59 94.59games 90.16 91.79 92.09 93.08 92.92hotel 93.23 94.21 93.97 94.7 94.78livemovie 90.88 92.64 92.8 93.28 93.37livetv 83.14 85.02 84.67 85.41 85.86movies 93.27 94.01 93.97 94.75 95.16music 87.87 90.37 90.9 91.75 91.33mystuff 94.2 94.4 94.51 94.51 94.95note 97.62 98.36 98.36 98.49 98.52ondevice 97.51 97.77 97.6 97.77 97.84places 97.29 97.68 97.68 98.01 97.75reminder 98.72 98.96 98.94 98.96 98.96sports 76.96 78.53 78.38 78.7 79.44timer 91.1 91.79 91.33 92.33 92.61travel 81.58 82.57 82.43 83.64 82.81tv 91.42 94.11 94.91 95.19 95.47weather 97.31 97.33 97.4 97.4 97.47Average 91.99 92.89 93.00 93.49 93.56Table 1: Performance comparison between different embeddings style.6 billion tokens (6B-50d) and 840 billion tokens(840B-300d) (Pennington et al, 2014) achieved92.89% and 93.00%, respectively.
50d and 300ddenote size of embedding dimension.
To use wordembeddings as a sentence representation, we sim-ply use averaged word vectors over a sentence,normalized and conjoined with the original rep-resentation as in (2).
Surprisingly, when we usesentence representation (SENT) induced from thesketching method with our data set, we can boostthe performance up to 93.49%, corresponding toa 18.78% decrease in error relative to a SVMwithout representation.
Also, we see that the ex-tended sentence representation (SENT+) can getadditional gains.As in Table 2 , we also measured performanceof our method (SENT+) as a function of the per-centage of unlabeled data we used from total un-labeled sentences.
The overall trend is clear: asthe number of sentences are added to the data forinducing sentence representation, the test perfor-mance improves because of both better coverageand better quality of embedding.
We believe thatif we consume more data, we can boost up the per-formance even more.3.7 Results of ParallelizationTable 3 shows the sketching results for vari-ous batch size.
To evaluate parallelization, wefirst randomly generate a matrix R1,000,000?100and it is sketched to R100?100.
And then wesketch run with different batch size.
The resultsshow that as the number of batch increases, wecan speed up dramatically, keeping residual value???
?X>X ?
Y>Y????2.
It indeed satisfies the boundvalue, ||X||2F/l, which was 100014503.16.4 ConclusionWe introduced how to use matrix sketching al-gorithm of (Liberty, 2013) for scalable semi-supervised sentence classification.
This algorithmapproximates the data within a specified mem-ory bound while preserving the covariance struc-ture necessary for PCA.
Using matrix sketching,we significantly improved the classification accu-racy by leveraging very large amounts of unla-beled sentences.110 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%apps 89.16 89.83 90.04 90.26 90.88 91.9 92.41 92.41 92.95 93.72 94.3music 87.87 89.12 89.61 90.4 90.83 91.26 91.31 91.33 91.38 91.33 91.33tv 91.42 92.28 92.83 93.61 93.96 94.67 94.91 95.12 95.34 95.44 95.47Table 2: Performance for selected domains as the number of unlabeled data increases.Batch Size???
?X>X ?
Y>Y???
?2time1 1019779.69 100.212 1019758.22 50.314 1019714.19 26.505 1019713.43 21.678 1019679.67 14.5310 1019692.67 12.1316 1019686.35 8.5320 1019709.03 7.3525 1019650.51 6.4040 1019703.24 4.9750 1019689.33 4.48Table 3: Results for corresponding batch size.Second column indicates the norm of gap betweenoriginal and sketching matrix.
Time represents therunning time for sketching methods, measured inseconds.ReferencesAsli Celikyilmaz, Dilek Hakkani-T?ur, and Gokhan T?ur.2011.
Leveraging web query logs to learn user intentvia bayesian discrete latent variable model.
ICML.Yun-Nung Chen, Dilek Hakkani-T?ur, and XiaodongHe.
2016.
Zero-shot learning of intent embeddingsfor expansion by convolutional deep structured se-mantic models.
In Proc.
of ICASSP.Koby Crammer and Yoram Singer.
2002.
On the learn-ability and design of output codes for multiclassproblems.
Machine Learning, 47(2-3):201?233.Jane K Cullum and Ralph A Willoughby.
2002.
Lanc-zos Algorithms for Large Symmetric EigenvalueComputations: Vol.
1: Theory, volume 41.
SIAM.Petros Drineas and Ravi Kannan.
2003.
Pass effi-cient algorithms for approximating large matrices.In SODA, volume 3, pages 223?232.Ali El-Kahky, Xiaohu Liu, Ruhi Sarikaya, Gokhan Tur,Dilek Hakkani-Tur, and Larry Heck.
2014.
Ex-tending domain coverage of language understand-ing systems via intent transfer between domainsusing knowledge graphs and search query clicklogs.
In Acoustics, Speech and Signal Processing(ICASSP), 2014 IEEE International Conference on,pages 4067?4071.
IEEE.Dilek Hakkani-T?ur, Asli Celikyilmaz, Larry P Heck,and G?okhan T?ur.
2013.
A weakly-supervised ap-proach for discovering new user intents from searchquery logs.
In INTERSPEECH, pages 3780?3784.Yangfeng Ji, Dilek Hakkani-Tur, Asli Celikyilmaz,Larry Heck, and Gokhan Tur.
2014.
A variationalbayesian model for user intent detection.
In Acous-tics, Speech and Signal Processing (ICASSP), 2014IEEE International Conference on, pages 4072?4076.
IEEE.Young-Bum Kim and Benjamin Snyder.
2013.
Opti-mal data set selection: An application to grapheme-to-phoneme conversion.
In HLT-NAACL, pages1196?1205.Young-Bum Kim, Minwoo Jeong, Karl Stratos, andRuhi Sarikaya.
2015a.
Weakly supervised slottagging with partially labeled sequences from websearch click logs.
In Proc.
of the Conference onthe North American Chapter of the Association forComputational Linguistics - Human Language Tech-nologies, pages 84?92.Young-Bum Kim, Karl Stratos, Xiaohu Liu, and RuhiSarikaya.
2015b.
Compact lexicon selection withspectral methods.
In Proc.
of Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies.Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.2015c.
Pre-training of hidden-unit crfs.
In Proc.of Annual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 192?198.Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, andMinwoo Jeong.
2015d.
New transfer learning tech-niques for disparate label sets.
In Proc.
of AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies.Xiao Li, Ye-Yi Wang, and Alex Acero.
2009.
Extract-ing structured information from user queries withsemi-supervised conditional random fields.
In Pro-ceedings of the 32nd international ACM SIGIR con-ference on Research and development in informationretrieval.Edo Liberty.
2013.
Simple and deterministic ma-trix sketching.
In Proceedings of the 19th ACMSIGKDD international conference on Knowledgediscovery and data mining, pages 581?588.
ACM.12Gr?egoire Mesnil, Yann Dauphin, Kaisheng Yao,Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-aodong He, Larry Heck, Gokhan Tur, Dong Yu,et al 2015.
Using recurrent neural networks forslot filling in spoken language understanding.
Au-dio, Speech, and Language Processing, IEEE/ACMTransactions on, 23(3):530?539.Christos H Papadimitriou, Hisao Tamaki, PrabhakarRaghavan, and Santosh Vempala.
1998.
La-tent semantic indexing: A probabilistic analy-sis.
In Proceedings of the seventeenth ACMSIGACT-SIGMOD-SIGART symposium on Princi-ples of database systems, pages 159?168.
ACM.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors forword representation.
Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), 12:1532?1543.Mark Rudelson and Roman Vershynin.
2007.
Sam-pling from large matrices: An approach through ge-ometric functional analysis.
Journal of the ACM(JACM), 54(4):21.Karl Stratos and Michael Collins.
2015.
Simple semi-supervised pos tagging.
In Proceedings of NAACL-HLT, pages 79?87.Gokhan Tur and Renato De Mori.
2011.
Spoken lan-guage understanding: Systems for extracting seman-tic information from speech.
John Wiley & Sons.Matthew Turk, Alex P Pentland, et al 1991.
Facerecognition using eigenfaces.
In Computer Vi-sion and Pattern Recognition, 1991.
ProceedingsCVPR?91., IEEE Computer Society Conference on,pages 586?591.
IEEE.Santosh S Vempala.
2005.
The random projectionmethod, volume 65.
American Mathematical Soc.Kilian Weinberger, Anirban Dasgupta, John Langford,Alex Smola, and Josh Attenberg.
2009.
Fea-ture hashing for large scale multitask learning.
InProceedings of the 26th Annual International Con-ference on Machine Learning, pages 1113?1120.ACM.Puyang Xu and Ruhi Sarikaya.
2014.
Contextual do-main classification in spoken language understand-ing systems using recurrent neural network.
InAcoustics, Speech and Signal Processing (ICASSP),2014 IEEE International Conference on, pages 136?140.
IEEE.13
