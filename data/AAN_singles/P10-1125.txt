Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230?1238,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsModeling Semantic Relevance for Question-Answer Pairsin Web Social CommunitiesBaoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, Lin SunSchool of Computer Science and TechnologyHarbin Institute of TechnologyHarbin, China{bxwang, wangxl, cjsun, liubq, lsun}@insun.hit.edu.cnAbstractQuantifying the semantic relevance be-tween questions and their candidate an-swers is essential to answer detection insocial media corpora.
In this paper, a deepbelief network is proposed to model thesemantic relevance for question-answerpairs.
Observing the textual similaritybetween the community-driven question-answering (cQA) dataset and the forumdataset, we present a novel learning strat-egy to promote the performance of ourmethod on the social community datasetswithout hand-annotating work.
The ex-perimental results show that our methodoutperforms the traditional approaches onboth the cQA and the forum corpora.1 IntroductionIn natural language processing (NLP) and infor-mation retrieval (IR) fields, question answering(QA) problem has attracted much attention overthe past few years.
Nevertheless, most of the QAresearches mainly focus on locating the exact an-swer to a given factoid question in the related doc-uments.
The most well known international evalu-ation on the factoid QA task is the Text REtrievalConference (TREC)1, and the annotated questionsand answers released by TREC have become im-portant resources for the researchers.
However,when facing a non-factoid question such as why,how, or what about, however, almost no automaticQA systems work very well.The user-generated question-answer pairs aredefinitely of great importance to solve the non-factoid questions.
Obviously, these natural QApairs are usually created during people?s com-munication via Internet social media, amongwhich we are interested in the community-driven1http://trec.nist.govquestion-answering (cQA) sites and online fo-rums.
The cQA sites (or systems) provide plat-forms where users can either ask questions or de-liver answers, and best answers are selected man-ually (e.g., Baidu Zhidao2 and Yahoo!
Answers3).Comparing with cQA sites, online forums havemore virtual society characteristics, where peoplehold discussions in certain domains, such as tech-niques, travel, sports, etc.
Online forums containa huge number of QA pairs, and much noise infor-mation is involved.To make use of the QA pairs in cQA sites andonline forums, one has to face the challengingproblem of distinguishing the questions and theiranswers from the noise.
According to our investi-gation, the data in the community based sites, es-pecially for the forums, have two obvious charac-teristics: (a) a post usually includes a very shortcontent, and when a person is initializing or re-plying a post, an informal tone tends to be used;(b) most of the posts are useless, which makesthe community become a noisy environment forquestion-answer detection.In this paper, a novel approach for modeling thesemantic relevance for QA pairs in the social me-dia sites is proposed.
We concentrate on the fol-lowing two problems:1.
How to model the semantic relationship be-tween two short texts using simple textual fea-tures?
As mentioned above, the user generatedquestions and their answers via social media arealways short texts.
The limitation of length leadsto the sparsity of the word features.
In addition,the word frequency is usually either 0 or 1, that is,the frequency offers little information except theoccurrence of a word.
Because of this situation,the traditional relevance computing methods basedon word co-occurrence, such as Cosine similarityand KL-divergence, are not effective for question-2http://zhidao.baidu.com3http://answers.yahoo.com1230answer semantic modeling.
Most researchers tryto introduce structural features or users?
behaviorto improve the models performance, by contrast,the effect of textual features is not obvious.2.
How to train a model so that it has good per-formance on both cQA and forum datasets?
Sofar, people have been doing QA researches on thecQA and the forum datasets separately (Ding etal., 2008; Surdeanu et al, 2008), and no one hasnoticed the relationship between the two kinds ofdata.
Since both the cQA systems and the onlineforums are open platforms for people to commu-nicate, the QA pairs in the cQA systems have sim-ilarity with those in the forums.
In this case, it ishighly valuable and desirable to propose a train-ing strategy to improve the model?s performanceon both of the two kinds of datasets.
In addition,it is possible to avoid the expensive and arduoushand-annotating work by introducing the method.To solve the first problem, we present a deepbelief network (DBN) to model the semantic rel-evance between questions and their answers.
Thenetwork establishes the semantic relationship forQA pairs by minimizing the answer-to-questionreconstructing error.
Using only word features,our model outperforms the traditional methods onquestion-answer relevance calculating.For the second problem, we make our modelto learn the semantic knowledge from the solvedquestion threads in the cQA system.
Instead ofmining the structure based features from cQApages and forum threads individually, we con-sider the textual similarity between the two kindsof data.
The semantic information learned fromcQA corpus is helpful to detect answers in forums,which makes our model show good performanceon social media corpora.
Thanks to the labels forthe best answers existing in the threads, no manualwork is needed in our strategy.The rest of this paper is organized as follows:Section 2 surveys the related work.
Section 3 in-troduces the deep belief network for answer de-tection.
In Section 4, the homogenous data basedlearning strategy is described.
Experimental resultis given in Section 5.
Finally, conclusions and fu-ture directions are drawn in Section 6.2 Related WorkThe value of the naturally generated question-answer pairs has not been recognized until recentyears.
Early studies mainly focus on extractingQA pairs from frequently asked questions (FAQ)pages (Jijkoun and de Rijke, 2005; Riezler et al,2007) or service call-center dialogues (Berger etal., 2000).Judging whether a candidate answer is seman-tically related to the question in the cQA pageautomatically is a challenging task.
A frame-work for predicting the quality of answers hasbeen presented in (Jeon et al, 2006).
Bernhardand Gurevych (2009) have developed a transla-tion based method to find answers.
Surdeanu etal.
(2008) propose an approach to rank the an-swers retrieved by Yahoo!
Answers.
Our work ispartly similar to Surdeanu et al (2008), for we alsoaim to rank the candidate answers reasonably, butour ranking algorithm needs only word informa-tion, instead of the combination of different kindsof features.Because people have considerable freedom topost on forums, there are a great number of irrel-evant posts for answering questions, which makesit more difficult to detect answers in the forums.In this field, exploratory studies have been done byFeng et al (2006) and Huang et al (2007), who ex-tract input-reply pairs for the discussion-bot.
Dinget al(2008) and Cong et al(2008) have also pre-sented outstanding research works on forum QAextraction.
Ding et al (2008) detect question con-texts and answers using the conditional randomfields, and a ranking algorithm based on the au-thority of forum users is proposed by Cong et al(2008).
Treating answer detection as a binary clas-sification problem is an intuitive idea, thus thereare some studies trying to solve it from this view(Hong and Davison, 2009; Wang et al, 2009).
Es-pecially Hong and Davison (2009) have achieveda rather high precision on the corpora with lessnoise, which also shows the importance of ?social?features.In order to select the answers for a given ques-tion, one has to face the problem of lexical gap.One of the problems with lexical gap embeddingis to find similar questions in QA achieves (Jeon etal., 2005).
Recently, the statistical machine trans-lation (SMT) strategy has become popular.
Lee etal.
(2008) use translate models to bridge the lexi-cal gap between queries and questions in QA col-lections.
The SMT based methods are effective onmodeling the semantic relationship between ques-tions and answers and expending users?
queries inanswer retrieval (Riezler et al, 2007; Berger et al,12312000; Bernhard and Gurevych, 2009).
In (Sur-deanu et al, 2008), the translation model is usedto provide features for answer ranking.The structural features (e.g., authorship, ac-knowledgement, post position, etc), also callednon-textual features, play an important role in an-swer extraction.
Such features are used in (Dinget al, 2008; Cong et al, 2008), and have signifi-cantly improved the performance.
The studies ofJeon et al (2006) and Hong et al (2009) show thatthe structural features have even more contributionthan the textual features.
In this case, the miningof textual features tends to be ignored.There are also some other research topics in thisfield.
Cong et al (2008) and Wang et al (2009)both propose the strategies to detect questions inthe social media corpus, which is proved to be anon-trivial task.
The deep research on questiondetection has been taken by Duan et al (2008).A graph based algorithm is presented to answeropinion questions (Li et al, 2009).
In email sum-marization field, the QA pairs are also extractedfrom email contents as the main elements of emailsummarization (Shrestha and McKeown, 2004).3 The Deep Belief Network for QA pairsDue to the feature sparsity and the low word fre-quency of the social media corpus, it is difficultto model the semantic relevance between ques-tions and answers using only co-occurrence fea-tures.
It is clear that the semantic link exists be-tween the question and its answers, even thoughthey have totally different lexical representations.Thus a specially designed model may learn se-mantic knowledge by reconstructing a great num-ber of questions using the information in the cor-responding answers.
In this section, we proposea deep belief network for modeling the seman-tic relationship between questions and their an-swers.
Our model is able to map the QA data intoa low-dimensional semantic-feature space, wherea question is close to its answers.3.1 The Restricted Boltzmann MachineAn ensemble of binary vectors can be modeled us-ing a two-layer network called a ?restricted Boltz-mann machine?
(RBM) (Hinton, 2002).
The di-mension reducing approach based on RBM ini-tially shows good performance on image process-ing (Hinton and Salakhutdinov, 2006).
Salakhut-dinov and Hinton (2009) propose a deep graphicalmodel composed of RBMs into the information re-trieval field, which shows that this model is able toobtain semantic information hidden in the word-count vectors.As shown in Figure 1, the RBM is a two-layernetwork.
The bottom layer represents a visiblevector v and the top layer represents a latent fea-ture h. The matrix W contains the symmetric in-teraction terms between the visible units and thehidden units.
Given an input vector v, the trainedFigure 1: Restricted Boltzmann machineRBM model provides a hidden feature h, whichcan be used to reconstruct v with a minimum er-ror.
The training algorithm for this paper will bedescribed in the next subsection.
The ability of theRBM suggests us to build a deep belief networkbased on RBM so that the semantic relevance be-tween questions and answers can be modeled.3.2 Pretraining a Deep Belief NetworkIn the social media corpora, the answers are al-ways descriptive, containing one or several sen-tences.
Noticing that an answer has strong seman-tic association with the question and involves moreinformation than the question, we propose to traina deep belief network by reconstructing the ques-tion using its answers.
The training object is tominimize the error of reconstruction, and after thepretraining process, a point that lies in a good re-gion of parameter space can be achieved.Firstly, the illustration of the DBN model isgiven in Figure 2.
This model is composed ofthree layers, and here each layer stands for theRBM or its variant.
The bottom layer is a variantform of RBM?s designed for the QA pairs.
Thislayer we design is a little different from the classi-cal RBM?s, so that the bottom layer can generatethe hidden features according to the visible answervector and reconstruct the question vector usingthe hidden features.
The pre-training procedure ofthis architecture is practically convergent.
In thebottom layer, the binary feature vectors based onthe statistics of the word occurrence in the answersare used to compute the ?hidden features?
in the1232Figure 2: The Deep Belief Network for QA Pairshidden units.
The model can reconstruct the ques-tions using the hidden features.
The processes canbe modeled as follows:p(h j = 1|a) = ?
(b j +?iwi jai) (1)p(qi = 1|h) = ?
(bi +?jwi jh j) (2)where ?
(x) = 1/(1 + e?x), a denotes the visiblefeature vector of the answer, qi is the ith elementof the question vector, and h stands for the hid-den feature vector for reconstructing the questions.wi j is a symmetric interaction term between wordi and hidden feature j, bi stands for the bias of themodel for word i, and b j denotes the bias of hiddenfeature j.Given the training set of answer vectors, the bot-tom layer generates the corresponding hidden fea-tures using Equation 1.
Equation 2 is used to re-construct the Bernoulli rates for each word in thequestion vectors after stochastically activating thehidden features.
Then Equation 1 is taken againto make the hidden features active.
We use 1-stepContrastive Divergence (Hinton, 2002) to updatethe parameters by performing gradient ascent:?wi j = (< qih j >qData ?
< qih j >qRecon) (3)where < qih j >qData denotes the expectation ofthe frequency with which the word i in a ques-tion and the feature j are on together when thehidden features are driven by the question data.< qih j >qRecon defines the corresponding expec-tation when the hidden features are driven by thereconstructed question data.
 is the learning rate.The classical RBM structure is taken to buildthe middle layer and the top layer of the network.The training method for the higher two layer issimilar to that of the bottom one, and we only haveto make each RBM to reconstruct the input datausing its hidden features.
The parameter updatesstill obeying the rule defined by gradient ascent,which is quite similar to Equation 3.
After train-ing one layer, the h vectors are then sent to thehigher-level layer as its ?training data?.3.3 Fine-tuning the WeightsNotice that a greedy strategy is taken to train eachlayer individually during the pre-training proce-dure, it is necessary to fine-tune the weights of theentire network for optimal reconstruction.
To fine-tune the weights, the network is unrolled, takingthe answers as the input data to generate the corre-sponding questions at the output units.
Using thecross-entropy error function, we can then tune thenetwork by performing backpropagation throughit.
The experiment results in section 5.2 will showfine-tuning makes the network performs better foranswer detection.3.4 Best answer detectionAfter pre-training and fine-tuning, a deep beliefnetwork for QA pairs is established.
To detect thebest answer to a given question, we just have tosend the vectors of the question and its candidateanswers into the input units of the network andperform a level-by-level calculation to obtain thecorresponding feature vectors.
Then we calculatethe distance between the mapped question vectorand each candidate answer vector.
We consider thecandidate answer with the smallest distance as thebest one.4 Learning with Homogenous DataIn this section, we propose our strategy to makeour DBN model to detect answers in both cQA andforum datasets, while the existing studies focus onone single dataset.4.1 Homogenous QA Corpora from DifferentSourcesOur motivation of finding the homogenousquestion-answer corpora from different kind of so-cial media is to guarantee the model?s performanceand avoid hand-annotating work.In this paper, we get the ?solved question?
pagesin the computer technology domain from BaiduZhidao as the cQA corpus, and the threads of1233Figure 3: Comparison of the post content lengths in the cQA and the forum datasetsComputerFansClub Forum4 as the online forumcorpus.
The domains of the corpora are the same.To further explain that the two corpora are ho-mogenous, we will give the detail comparison ontext style and word distribution.As shown in Figure 3, we have compared thepost content lengths of the cQA and the forumin our corpora.
For the comparison, 5,000 postsfrom the cQA corpus and 5,000 posts from the fo-rum corpus are randomly selected.
The left panelshows the statistical result on the Baidu Zhidaodata, and the right panel shows the one on the fo-rum data.
The number i on the horizontal axis de-notes the post contents whose lengths range from10(i?
1) + 1 to 10i bytes, and the vertical axis rep-resents the counts of the post contents.
From Fig-ure 3 we observe that the contents of most postsin both the cQA corpus and the forum corpus areshort, with the lengths not exceeding 400 bytes.The content length reflects the text style of theposts in cQA systems and online forums.
FromFigure 3 it can be also seen that the distributionsof the content lengths in the two figures are verysimilar.
It shows that the contents in the two cor-pora are both mainly short texts.Figure 4 shows the percentage of the concurrentwords in the top-ranked content words with highfrequency.
In detail, we firstly rank the words byfrequency in the two corpora.
The words are cho-sen based on a professional dictionary to guaranteethat they are meaningful in the computer knowl-edge field.
The number k on the horizontal axis inFigure 4 represents the top k content words in the4http://bbs.cfanclub.net/corpora, and the vertical axis stands for the per-centage of the words shared by the two corpora inthe top k words.Figure 4: Distribution of concurrent content wordsFigure 4 shows that a large number of meaning-ful words appear in both of the two corpora withhigh frequencies.
The percentage of the concur-rent words maintains above 64% in the top 1,400words.
It indicates that the word distributions ofthe two corpora are quite similar, although theycome from different social media sites.Because the cQA corpus and the forum corpusused in this study have homogenous characteris-tics for answer detecting task, a simple strategymay be used to avoid the hand-annotating work.Apparently, in every ?solved question?
page ofBaidu Zhidao, the best answer is selected by theuser who asks this question.
We can easily extractthe QA pairs from the cQA corpus as the training1234set.
Because the two corpora are similar, we canapply the deep belief network trained by the cQAcorpus to detect answers on both the cQA data andthe forum data.4.2 FeaturesThe task of detecting answers in social media cor-pora suffers from the problem of feature sparsityseriously.
High-dimensional feature vectors withonly several non-zero dimensions bring large timeconsumption to our model.
Thus it is necessary toreduce the dimension of the feature vectors.In this paper, we adopt two kinds of word fea-tures.
Firstly, we consider the 1,300 most fre-quent words in the training set as Salakhutdinovand Hinton (2009) did.
According to our statis-tics, the frequencies of the rest words are all lessthen 10, which are not statistically significant andmay introduce much noise.We take the occurrence of some function wordsas another kind of features.
The function wordsare quite meaningful for judging whether a shorttext is an answer or not, especially for the non-factoid questions.
For example, in the answers tothe causation questions, the words such as becauseand so are more likely to appear; and the wordssuch as firstly, then, and should may suggest theanswers to the manner questions.
We give an ex-ample for function word selection in Figure 5.Figure 5: An example for function word selectionFor this reason, we collect 200 most frequentfunction words in the answers of the training set.Then for every short text, either a question or ananswer, a 1,500-dimensional vector can be gener-ated.
Specifically, all the features we have adoptedare binary, for they only have to denote whetherthe corresponding word appears in the text or not.5 ExperimentsTo evaluate our question-answer semantic rele-vance computing method, we compare our ap-proach with the popular methods on the answerdetecting task.5.1 Experiment SetupArchitecture of the Network: To build the deepbelief network, we use a 1500-1500-1000-600 ar-chitecture, which means the three layers of the net-work have individually 1,500?1,500, 1,500?1,000and 1,000?600 units.
Using the network, a 1,500-dimensional binary vector is finally mapped to a600-dimensional real-value vector.During the pretraining stage, the bottom layeris greedily pretrained for 200 passes through theentire training set, and each of the rest two layers isgreedily pretrained for 50 passes.
For fine-tuningwe apply the method of conjugate gradients5, withthree line searches performed in each pass.
Thisalgorithm is performed for 50 passes to fine-tunethe network.Dataset: we have crawled 20,000 pages of?solved question?
from the computer and networkcategory of Baidu Zhidao as the cQA corpus.
Cor-respondingly we obtain 90,000 threads from Com-puterFansClub, which is an online forum on com-puter knowledge.
We take the forum threads asour forum corpus.From the cQA corpus, we extract 12,600 humangenerated QA pairs as the training set without anymanual work to label the best answers.
We get thecontents from another 2,000 cQA pages to forma testing set, each content of which includes onequestion and 4.5 candidate answers on average,with one best answer among them.
To get anothertesting dataset, we randomly select 2,000 threadsfrom the forum corpus.
For this training set, hu-man work are necessary to label the best answersin the posts of the threads.
There are 7 posts in-cluded in each thread on average, among whichone question and at least one answer exist.Baseline: To show the performance of ourmethod, three main popular relevance computingmethods for ranking candidate answers are con-sidered as our baselines.
We will briefly introducethem:Cosine Similarity.
Given a question q and itscandidate answer a, their cosine similarity can becomputed as follows:cos(q, a) =?nk=1 wqk ?
wak?
?nk=1 w2qk ??
?nk=1 w2ak(4)where wqk and wak stand for the weight of the kthword in the question and the answer respectively.5Code is available athttp://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/1235The weights can be get by computing the productof term frequency (tf ) and inverse document fre-quency (idf )HowNet based Similarity.
HowNet6 is an elec-tronic world knowledge system, which serves asa powerful tool for meaning computation in hu-man language technology.
Normally the similar-ity between two passages can be calculated bytwo steps: (1) matching the most semantic-similarwords in each passages greedily using the API?sprovided by HowNet; (2) computing the weightedaverage similarities of the word pairs.
This strat-egy is taken as a baseline method for computingthe relevance between questions and answers.KL-divergence Language Model.
Given a ques-tion q and its candidate answer a, we can con-struct unigram language model Mq and unigramlanguage model Ma.
Then we compute KL-divergence between Mq and Ma as below:KL(Ma||Mq) =?wp(w|Ma) log(p(w|Ma)/p(w|Mq))(5)5.2 Results and AnalysisWe evaluate the performance of our approach foranswer detection using two metrics: Precision@1(P@1) and Mean Reciprocal Rank (MRR).
Ap-plying the two metrics, we perform the baselinemethods and our DBN based methods on the twotesting set above.Table 1 lists the results achieved on the forumdata using the baseline methods and ours.
The ad-ditional ?Nearest Answer?
stands for the methodwithout any ranking strategies, which returns thenearest candidate answer from the question by po-sition.
To illustrate the effect of the fine-tuning forour model, we list the results of our method with-out fine-tuning and the results with fine-tuning.As shown in Table 1, our deep belief networkbased methods outperform the baseline methodsas expected.
The main reason for the improve-ments is that the DBN based approach is able tolearn semantic relationship between the words inQA pairs from the training set.
Although the train-ing set we offer to the network comes from a dif-ferent source (the cQA corpus), it still provideenough knowledge to the network to perform bet-ter than the baseline methods.
This phenomena in-dicates that the homogenous corpora for training is6Detail information can be found in:http://www.keenage.com/effective and meaningful.Method P@1 (%) MRR (%)Nearest Answer 21.25 38.72Cosine Similarity 23.15 43.50HowNet 22.55 41.63KL divergence 25.30 51.40DBN (without FT) 41.45 59.64DBN (with FT) 45.00 62.03Table 1: Results on Forum DatasetWe have also investigated the reasons for the un-satisfying performance of the baseline approaches.Basically, the low precision is ascribable to theforum corpus we have obtained.
As mentionedin Section 1, the contents of the forum posts areshort, which leads to the sparsity of the features.Besides, when users post messages in the onlineforums, they are accustomed to be casual and usesome synonymous words interchangeably in theposts, which is believed to be a significant situ-ation in Chinese forums especially.
Because thefeatures for QA pairs are quite sparse and the con-tent words in the questions are usually morpholog-ically different from the ones with the same mean-ing in the answers, the Cosine Similarity methodbecome less powerful.
For HowNet based ap-proaches, there are a large number of words notincluded by HowNet, thus it fails to compute thesimilarity between questions and answers.
KL-divergence suffers from the same problems withthe Cosine Similarity method.
Compared withthe Cosine Similarity method, this approach hasachieved the improvement of 9.3% in P@1, butit performs much better than the other baselinemethods in MRR.The baseline results indicate that the online fo-rum is a complex environment with large amountof noise for answer detection.
Traditional IRmethods using pure textual features can hardlyachieve good results.
The similar baseline resultsfor forum answer ranking are also achieved byHong and Davison (2009), which takes some non-textual features to improve the algorithm?s perfor-mance.
We also notice that, however, the baselinemethods have obtained better results on forum cor-pus (Cong et al, 2008).
One possible reason is thatthe baseline approaches are suitable for their data,since we observe that the ?nearest answer?
strat-egy has obtained a 73.5% precision in their work.Our model has achieved the precision of123645.00% in P@1 and 62.03% in MRR for answerdetecting on forum data after fine-tuning, whilesome related works have reported the results withthe precision over 90% (Cong et al, 2008; Hongand Davison, 2009).
There are mainly two rea-sons for this phenomena: Firstly, both of the pre-vious works have adopt non-textual features basedon the forum structure, such as authorship, po-sition and quotes, etc.
The non-textual (or so-cial based) features have played a significant rolein improving the algorithms?
performance.
Sec-ondly, the quality of corpora influences the resultsof the ranking strategies significantly, and eventhe same algorithm may perform differently whenthe dataset is changed (Hong and Davison, 2009).For the experiments of this paper, large amount ofnoise is involved in the forum corpus and we havedone nothing extra to filter it.Table 2 shows the experimental results on thecQA dataset.
In this experiment, each sample iscomposed of one question and its following sev-eral candidate answers.
We delete the ones withonly one answer to confirm there are at least twocandidate answers for each question.
The candi-date answers are rearranged by post time, so thatthe real answers do not always appear next to thequestions.
In this group of experiment, no hand-annotating work is needed because the real an-swers have been labeled by cQA users.Method P@1 (%) MRR (%)Nearest Answer 36.05 56.33Cosine Similarity 44.05 62.84HowNet 41.10 58.75KL divergence 43.75 63.10DBN (without FT) 56.20 70.56DBN (with FT) 58.15 72.74Table 2: Results on cQA DatasetFrom Table 2 we observe that all the approachesperform much better on this dataset.
We attributethe improvements to the high quality QA corpusBaidu Zhidao offers: the candidate answers tend tobe more formal than the ones in the forums, withless noise information included.
In addition, the?Nearest Answer?
strategy has reached 36.05% inP@1 on this dataset, which indicates quite a num-ber of askers receive the real answers at the firstanswer post.
This result has supported the idea ofintroducing position features.
What?s more, if thebest answer appear immediately, the asker tendsto lock down the question thread, which helps toreduce the noise information in the cQA corpus.Despite the baseline methods?
performanceshave been improved, our approaches still outper-form them, with a 32.0% improvement in P@1and a 15.3% improvement in MRR at least.
Onthe cQA dataset, our model shows better perfor-mance than the previous experiment, which is ex-pected because the training set and the testing setcome from the same corpus, and the DBN modelis more adaptive to the cQA data.We have observed that, from both of the twogroups of experiments, fine-tuning is effective forenhancing the performance of our model.
On theforum data, the results have been improved by8.6% in P@1 and 4.0% in MRR, and the improve-ments are 3.5% and 3.1% individually.6 ConclusionsIn this paper, we have proposed a deep belief net-work based approach to model the semantic rel-evance for the question answering pairs in socialcommunity corpora.The contributions of this paper can be summa-rized as follows: (1) The deep belief network wepresent shows good performance on modeling theQA pairs?
semantic relevance using only word fea-tures.
As a data driven approach, our model learnssemantic knowledge from large amount of QApairs to represent the semantic relevance betweenquestions and their answers.
(2) We have stud-ied the textual similarity between the cQA and theforum datasets for QA pair extraction, and intro-duce a novel learning strategy to make our methodshow good performance on both cQA and forumdatasets.
The experimental results show that ourmethod outperforms the traditional approaches onboth the cQA and the forum corpora.Our future work will be carried out along twodirections.
Firstly, we will further improve theperformance of our method by adopting the non-textual features.
Secondly, more research will betaken to put forward other architectures of the deepnetworks for QA detection.AcknowledgmentsThe authors are grateful to the anonymous re-viewers for their constructive comments.
Specialthanks to Deyuan Zhang, Bin Liu, Beidong Liuand Ke Sun for insightful suggestions.
This workis supported by NSFC (60973076).1237ReferencesAdam Berger, Rich Caruana, David Cohn, Dayne Fre-itag, and Vibhu Mittal.
2000.
Bridging the lexi-cal chasm: Statistical approaches to answer-finding.In In Proceedings of the 23rd annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 192?199.Delphine Bernhard and Iryna Gurevych.
2009.
Com-bining lexical semantic resources with question &answer archives for translation-based answer find-ing.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 728?736, Suntec,Singapore, August.
Association for ComputationalLinguistics.Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,and Yueheng Sun.
2008.
Finding question-answerpairs from online forums.
In SIGIR ?08: Proceed-ings of the 31st annual international ACM SIGIRconference on Research and development in infor-mation retrieval, pages 467?474, New York, NY,USA.
ACM.Shilin Ding, Gao Cong, Chin-Yew Lin, and XiaoyanZhu.
2008.
Using conditional random fields to ex-tract contexts and answers of questions from onlineforums.
In Proceedings of ACL-08: HLT, pages710?718, Columbus, Ohio, June.
Association forComputational Linguistics.Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and YongYu.
2008.
Searching questions by identifying ques-tion topic and question focus.
In Proceedings ofACL-08: HLT, pages 156?164, Columbus, Ohio,June.
Association for Computational Linguistics.Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.Hovy.
2006.
An intelligent discussion-bot for an-swering student queries in threaded discussions.
InCcile Paris and Candace L. Sidner, editors, IUI,pages 171?177.
ACM.G.
E. Hinton and R. R. Salakhutdinov.
2006.
Reduc-ing the dimensionality of data with neural networks.Science, 313(5786):504?507.Georey E. Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural Com-putation, 14.Liangjie Hong and Brian D. Davison.
2009.
Aclassification-based approach to question answeringin discussion boards.
In SIGIR ?09: Proceedingsof the 32nd international ACM SIGIR conference onResearch and development in information retrieval,pages 171?178, New York, NY, USA.
ACM.Jizhou Huang, Ming Zhou, and Dan Yang.
2007.
Ex-tracting chatbot knowledge from online discussionforums.
In IJCAI?07: Proceedings of the 20th in-ternational joint conference on Artifical intelligence,pages 423?428, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In CIKM ?05, pages 84?90, NewYork, NY, USA.
ACM.Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and SoyeonPark.
2006.
A framework to predict the quality ofanswers with non-textual features.
In SIGIR ?06,pages 228?235, New York, NY, USA.
ACM.Valentin Jijkoun and Maarten de Rijke.
2005.
Retriev-ing answers from frequently asked questions pageson the web.
In CIKM ?05, pages 76?83, New York,NY, USA.
ACM.Jung-Tae Lee, Sang-Bum Kim, Young-In Song, andHae-Chang Rim.
2008.
Bridging lexical gaps be-tween queries and questions on large online q&acollections with compact translation models.
InEMNLP ?08: Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 410?418, Morristown, NJ, USA.
Associationfor Computational Linguistics.Fangtao Li, Yang Tang, Minlie Huang, and XiaoyanZhu.
2009.
Answering opinion questions withrandom walks on graphs.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages737?745, Suntec, Singapore, August.
Associationfor Computational Linguistics.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical machine translation for query expansionin answer retrieval.
In Proceedings of the 45thAnnual Meeting of the Association of Computa-tional Linguistics, pages 464?471, Prague, CzechRepublic, June.
Association for ComputationalLinguistics.Ruslan Salakhutdinov and Geoffrey Hinton.
2009.Semantic hashing.
Int.
J. Approx.
Reasoning,50(7):969?978.Lokesh Shrestha and Kathleen McKeown.
2004.
De-tection of question-answer pairs in email conversa-tions.
In Proceedings of Coling 2004, pages 889?895, Geneva, Switzerland, Aug 23?Aug 27.
COL-ING.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to rank answers on largeonline QA collections.
In Proceedings of ACL-08:HLT, pages 719?727, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiao-long Wang, and Lin Sun.
2009.
Extracting chinesequestion-answer pairs from online forums.
In SMC2009: Proceedings of the IEEE International Con-ference on Systems, Man and Cybernetics, 2009.,pages 1159?1164.1238
