Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 973?981,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLearning to Predict Code-Switching PointsThamar Solorio and Yang LiuHuman Language Technology Research InstituteThe University of Texas at DallasRichardson, TX 75080, USAtsolorio,yangl@hlt.utdallas.eduAbstractPredicting possible code-switching points canhelp develop more accurate methods for au-tomatically processing mixed-language text,such as multilingual language models forspeech recognition systems and syntactic an-alyzers.
We present in this paper exploratoryresults on learning to predict potential code-switching points in Spanish-English.
Wetrained different learning algorithms using atranscription of code-switched discourse.
Toevaluate the performance of the classifiers, weused two different criteria: 1) measuring pre-cision, recall, and F-measure of the predic-tions against the reference in the transcrip-tion, and 2) rating the naturalness of artifi-cially generated code-switched sentences.
Av-erage scores for the code-switched sentencesgenerated by our machine learning approachwere close to the scores of those generated byhumans.1 IntroductionMultilingual speakers often switch back and forthbetween languages when speaking or writing,mostly in informal settings.
The mixing of lan-guages involves very elaborated patterns and formsand we usually use the term Code-Switching (CS)to encompass all of them (Lipski, 1978).
Before theInternet era, CS was mainly used in its spoken form.But with so many different informal interaction set-tings, such as chats, forums, blogs, and web siteslike Myspace and Facebook, CS is being used moreand more in written form.
For English and Spanish,CS has taken a step further.
It has become a hall-mark of the chicano culture as it is evident by thegrowing number of chicano writers publishing workin Spanish-English CS.We have not completely discovered the processof human language acquisition, especially dual lan-guage acquisition.
Findings in linguistics, soci-olinguistics, and psycholinguistics show that theproduction of code-switched discourse requires avery sophisticated knowledge of the languages be-ing mixed.
Some theories suggest bilingual speak-ers might have a third grammar for processing thistype of discourse.
The general agreement regardingCS is that switches do not take place at random andinstead it is possible to identify rules that bilingualspeakers adhere to.Understanding the CS process can lead to accu-rate methods for the automatic processing of bilin-gual discourse, and corpus-driven studies about CScan also inform linguistic theories.
In this paper wepresent exploratory work on learning to predict CSpoints using a machine learning approach.
Such anapproach can be used to reduce perplexity of lan-guage models for bilingual discourse.
We believethat CS behavior can be learned by a classifier andthe results presented in this paper support our belief.One of the difficult aspects of trying to predictCS points is how to evaluate the performance ofthe learner since switching is intrinsically motivatedand there are no forced switches (Sankoff, 1998b).Therefore, standard classification measures for thistask such as precision, recall, F-measure, or ac-curacy, are not the best approach for measuringthe effectiveness of a CS predictor.
To comple-973ment the evaluation of our approach, we designed atask involving human judgements on the naturalnessof automatically generated code-switched sentences.Both evaluations yielded encouraging results.The next section discusses theories explainingthe CS production process.
Then in Section 3 wepresent our framework for learning to predict CSpoints.
Section 4 discusses the empirical evaluationof the classifiers compared to the human reference.In Section 5 we present results of human evalua-tions on automatically generated code-switched sen-tences.
Section 6 describes previous work related tothe processing of code-switched text.
Finally, weconclude in Section 7 with a summary of our find-ings and directions for future work.2 Bilingual DiscourseThe combination of languages can be consideredto be a continuous spectrum where on each end ofthe spectrum we have one of the standard languagesand no blending.
As one moves closer to the mid-dle of the spectrum the amount and complexity ofthe blending pattern increases.
The blending patternmost widely known, and studied, is code-switching,which refers to the mixing of words from two lan-guages, but the words themselves do not suffer anysyntactic or phonological alterations.
The CS pointscan lie at sentence boundaries, but very often wewill also observe CS inside sentences.
According to(Sankoff, 1998b; Poplack, 1980; Lipski, 1978) whenCS is used inside a sentence, it can only happen atsyntactic boundaries shared by both languages, andthe resulting monolingual fragments will conform tothe grammar of the corresponding language.
In thisCS theory the relationship between both languagesis symmetric ?lexical items from one language canbe replaced by the corresponding items in the sec-ond language and vice versa.
Another prevalent lin-guistic theory argues the contrary: there is an asym-metric relation where the changes can occur only inone direction, which reflects the existence of a Ma-trix Language (ML), the dominant language, and anEmbedded Language (EL), or subordinate language(Joshi, 1982).
The Matrix Language Frame model,proposed and extended by Scotton-Myers, supportsthis asymmetric relation theory.
This formalism pre-scribes that content morphemes can come from theML or the EL, whereas late system morphemes,the elements that indicate grammatical relations, canonly be provided by the ML (Myers-Scotton, 1997).Until an empirical evaluation is carried out onlarge representative samples of discourse involvinga large number of different speakers, and differentlanguage-pairs, the production of CS discourse willnot be explained satisfactorily.
The goal of this workis to move closer to a better understanding of CS bylearning from corpora to predict possible CS points.3 Learning When To Code-Switch3.1 The English-Spanish Code-Switched DataSetWe recorded a conversation among three English-Spanish bilingual speakers that code-switch regu-larly when speaking to each other.
The conversa-tion lasts for about 40 minutes (?8k words, 922sentences).
It was manually transcribed and anno-tated with Part-of-Speech (POS) tags.
A total of239 switches were identified manually.
English isthe predominant language used, with a total of 576monolingual sentences.
We refer to this transcrip-tion as the Spanglish data set.
We are currently in theprocess of collecting new transcriptions of this con-versation in order to measure inter annotator agree-ment.3.2 ApproachMachine learning algorithms have proven to be sur-prisingly good at language processing tasks, in-cluding optical character recognition, text classifica-tion, named entity extraction, and many more.
Thepremise of our paper is that machine learning al-gorithms can also be successful at learning how tocode-switch as well as humans.
At the very leastwe want to provide encouraging evidence that thisis possible.
To the best of our knowledge, there isno previous work related to the problem of auto-matically predicting CS points.
Our machine learn-ing framework then is inspired by existing theoriesof CS and existing work on part-of-speech taggingcode-switched text (Solorio and Liu, 2008).In our approach, each word boundary is a poten-tial point for switching ?
an instance of the learningtask.
It should be noted that we can only rely on thehistory of words preceding potential CS points in or-974Feature id Description1 Word2 Language id3 Gold-standard POS tag4 BIO chunk5 English Tree Tagger POS6 English Tree Tagger prob7 English Tree Tagger lemma8 Spanish Tree Tagger POS9 Spanish Tree Tagger prob10 Spanish Tree Tagger lemmaTable 1: Features explored in learning to predict CSpoints.der to extract meaningful features.
Otherwise, if welook also into the future, we could just do languageidentification to extract the CS points.
However, ourgoal is to provide methods that can be used in realtime applications, where we do not have access toobservations beyond the point of interest.
Anotherrestriction we imposed on the method is related tothe size of the context used.
A sentence can be code-switched in different ways, with all different ver-sions adhering to the CS ?grammar?.
The numberof permissible CS sentences grows almost exponen-tially with the length of the sentence1.
By limitingthe length of the context to at most two words weare trying to avoid some sort of over fitting by hav-ing the model making assumptions over the interac-tion of the two languages that will be too weak, orspeaker-dependent.Previous studies have identified several socio-pragmatic functions of code-switching.
The mostcommon include direct quotation, emphasis, clari-fication, parenthetical comments, tags, and triggerswitches.
Other characteristics relevant to CS be-havior are the topic being discussed, the speakersinvolved, the setting where the conversation is tak-ing place, and the level of familiarity between thespeakers.
Having encoded information regarding theCS function and the aforementioned relevant factorsmight help in predicting upcoming CS points.
How-ever, annotating this information in the transcriptioncan be time consuming and very often this informa-1Almost exponentially because not all sentences will be con-sidered grammatical.tion is not readily available.
Therefore, at the ex-pense of making this task even more difficult, we de-cided against trying to include this type of informa-tion and include only lexical and syntactic features,to evaluate a practical and cost effective method forthis task.
Table 1 shows the list of features.
Allof these features are associated with word wn, theword immediately preceding boundary n. Feature 1is the word form2.
Feature 2 is language identifica-tion.
If the production of CS discourse adheres tothe matrix language frame model, then knowledgeof the language can potentially be a good sourceof information.
Feature 3 is the gold-standard POStag.
We also include as a feature the position ofthe word relative to the phrase constituent using aBeginning-Inside-Outside (BIO) scheme.
For in-stance, the word at the beginning of the verb phrasewill be labeled as B, the following words inside thisverb phrase will be tagged as I, and words that werenot identified as part of a phrase constituent werelabeled as O.
This chunking information was ex-tracted using the English and Spanish versions ofFreeLing3.
We did not measure accuracy on thechunking information.
Features 5 to 9 were gener-ated by tagging the Spanglish conversation using theSpanish and the English versions of the Tree Tagger(Schmid, 1994).
Attributes 5 to 7 are extracted fromthe English version, which include the POS tag, theconfidence, and the lemma for that word.
Similarly,features 8 to 10 were taken from the Spanish mono-lingual tree tagger.
Features from the monolingualtaggers will have some noisy labels when taggingfragments of the other language.
However, consider-ing that our feature set is small we want to explore ifadding these features, which include the lemmas andprobability estimates, can contribute to the learningtask.We also explored using a larger context.
In thiscase, we extract the same features shown in Table1 for the two words preceding the word boundary,resulting in 20 attributes representing each instance.Evaluation for this task is not straightforward.Within a sentence, there are several CS points thatwill result in a natural sounding code-switched sen-tence, but none of these CS points are mandatory.2Strictly speaking these should be called tokens, not wordssince punctuation marks are considered as well.3http://garraf.epsevg.upc.es/freeling/975CS has a lot to do with the speaker?s preferences,the topic being discussed, and the background of theparticipants involved.
Using the standard approachfor measuring performance of classifiers can be mis-leading, especially if the reference data set is smalland/or has only a small number of speakers.
It is un-realistic to just consider F-measure, or accuracy, astruthfully reflecting how well the learners generalizeto the task.
Therefore, we evaluated the classifier?sperformance using two different criteria, which arediscussed in the next sections.4 Evaluation 1: Using the Reference DataSetThis is the standard evaluation of machine learningclassifiers.
We randomly divided the data into sen-tences and grouped them into 10 subsets to performa cross-validation.
Tables 2 and 3 show results forNaive Bayes (NB) and Value Feature Interval (VFI)(Demiroz and Guvenir, 1997).
Using WEKA (Wit-ten and Frank, 1999), we experimented with differ-ent subsets of the attributes and two context win-dows: using only the preceding word and using theprevious two words.
The results presented here areoverall averages of 10-fold cross validation.
We alsoreport standard deviations.
It should be noted thatthe Spanglish data set is highly imbalanced, around96% of the instances belong to the negative class.Therefore, our comparisons are based on Precision,Recall, and F-measure, leaving accuracy aside, sincea weak classifier predicting that all instances belongto the negative class will reach an accuracy of 96%.The performance measures shown on Tables 2 and3 show that NB outperforms VFI in most of the con-figurations tested.
In particular, NB yields the bestresults when using a 1 word context with no lexicalforms nor lemmas as attributes (see Table 2 row 3).This is a fortunate finding ?for most practical prob-lems there will always be words in the test set thathave not been observed in the training set.
For oursmall Spanglish data set that will certainly be thecase.
In contrast, VFI achieves higher F-measureswhen using a context of two words and all the fea-tures are used.Analyzing the predictions of the learners we notedthat the NB classifier is heavily biased by the lan-guage attribute, close to 80% of the positive predic-tions made by NB are after seeing a word in Span-ish.
This preference seems to support the assump-tion of the asymmetry between the two languagesand the existence of an ML4.
This however is notthe case for VFI, only a little over 50% of the posi-tive predictions belong to this scenario.
Another in-teresting finding is the learner?s tendency to predicta code-switch after observing words like ?Yeah?,?anyway?, ?no?, and ?shower?.
The first two seemto fit the pattern of idiomatic expressions.
Accord-ing to Montes-Alcala?
this type of CS includes lin-guistic routines and fillers that are difficult to trans-late accurately (Montes-Alcala?, 2007), which mightbe the case of ?anyway?, and unconscious changes,which can explain the case of ?Yeah?.
The caseof ?shower?
and ?no?
are more difficult to explain,they might be overfitting patterns from the learners.We also found out that VFI learned to predict thata CS will take place right after seeing the sequenceof words le dije (I said).
This sequence of words isfrequently used when the speaker is about to quotehis/herself, and this quotation is one of the well-documented CS functions (Montes-Alcala?, 2007).A greedy search approach for attribute selectionusing WEKA showed that out of the 20 attributes(when using a two word context), the subset withthe highest predictive value included the languageidentification for word wn?1 and wn?2, the confi-dence threshold from the English tagger for wordwn?2, the lemma from the Spanish Tree tagger forwn?1, and the lexical form of the word wn?1.
Weexpected the chunk information to be useful and thisdoes not seem to be the case.
Another unexpectedoutcome is that higher F-measures are reached byadding features generated by the monolingual Treetaggers.
Even though these features are noisy, theystill carry useful information.We only show results from NB and VFI.
Initialexperiments with a subset of the data showed thatthese algorithms were the most promising for thistask.
They both yielded higher F-measures, evenwhen compared against Support Vector Machines(SVMs), C4.5, and neural networks.
On this ex-periment all the discriminative classifiers reacheda classification accuracy close to 96%, but an F-4We remind the reader that in this paper ML stands for Ma-trix Language.976Features UsedEnglish Spanish Naive BayesWord Lang POS BIO Tree tagger Tree taggerC Form id tag chunk POS Prob Lem POS Prob Lem P R F11 X X X 0.09(0.01) 0.01(0.00) 0.02(0.00)1 X X X X 0.23(0.01) 0.32(0.02) 0.27(0.02)1* X X X X X X X 0.19(0.00) 0.53(0.00) 0.28(0.00)1 X X X X X X X X X X 0.18(0.00) 0.59(0.00) 0.27(0.00)2 X X X 0.13(0.00) 0.35(0.00) 0.19(0.00)2 X X X X 0.16(0.00) 0.46(0.00) 0.23(0.00)2 X X X X X X X 0.14(0.00) 0.55(0.01) 0.23(0.00)2 X X X X X X X X X X 0.16(0.00) 0.59(0.01) 0.25(0.00)Table 2: Prediction results of CS points with NB using different features.
Column C indicates the size of the contextused, 1 indicates a 1 word context, and 2 indicates two words preceding the word boundary.
Columns P, R, andF1, show precision, recall, and F-measure, respectively.
Numbers in parenthesis show standard deviations.
The rowmarked with a ?*?
shows the configuration used for the generation of CS sentences presented in Section 5.measure on the positive class of around 0%.
NBand VFI estimate predictions for each class sepa-rately, which makes them robust to imbalanced datasets.
In addition, generative models are known tobe better for smaller data sets since they reach theirhigher asymptotic error much faster than discrimi-native models (Ng and Jordan, 2002).
This mightexplain why Naive Bayes outperformed strong clas-sifiers such as SVMs by a large margin.The overall prediction performance is not veryhigh.
However, we should remark that for this par-ticular task expecting a high F-measure is unrealis-tic.
Consider for example, a case where the learnerspredict a CS point where the speaker decided not toswitch, this does not imply that particular point isnot a good CS point.
And similarly, if the classifiermissed an existing CS point in the reference data setthe resulting sentence might still be grammatical andnatural sounding.
This motivated the use of an alter-native evaluation, which we discuss below.5 Evaluation 2: Using Human EvaluatorsThe goal of this evaluation is to explore how humansperceive our automatically generated CS sentences,and in particular, how do they compare to the orig-inal sentences and to the randomly generated ones.We selected 30 spontaneous and naturally occurringCS sentences from different sources.
Some of themwere selected from the Spanglish Times Magazine5,some others from blogs found in (Montes-Alcala?,2007).
Other sentences were taken from a paperdiscussing CS on e-mails (Montes-Alcala?, 2005).All of the sentences are true occurrences of writ-ten CS, from speakers different from the ones in theSpanglish data set.
The sentences were translatedto standard English and Spanish and were manuallyaligned.
We will use this parallel set of sentencesto predict CS points with our models.
Based on themodel predictions we will generate code-switchedsentences by combining monolingual fragments.It should be noted that the Spanglish data set isa transcription of spoken CS.
In contrast, this newevaluation set contains only written CS.
Recent stud-ies suggest written CS will adhere to the rules ofspoken CS (Montes-Alcala?, 2005), but there is stillsome controversy on this issue.
From our perspec-tive, both samples come from informal conversa-tional interactions.
It is expected that both will havesimilar patterns and therefore will provide a goodsource for our evaluation.5.1 Automatically Generated Code-SwitchingSentencesIn this subsection we describe how to generate code-switched sentences randomly and with the learnedmodels described in the previous sections.
For the5http://www.spanglishtimes.com/977Features UsedEnglish Spanish Voting Feature IntervalsWord Lang POS BIO Tree tagger Tree taggerC Form id tag chunk POS Prob Lem POS Prob Lem P R F11 X X X 0.12(0.00) 0.68(0.00) 0.21(0.00)1 X X X X 0.12(0.00) 0.65(0.01) 0.20(0.00)1* X X X X X X X 0.12(0.00) 0.72(0.01) 0.21(0.00)1 X X X X X X X X X X 0.13(0.00) 0.65(0.00) 0.22(0.00)2 X X X 0.13(0.00) 0.60(0.00) 0.21(0.00)2 X X X X 0.15(0.00) 0.52(0.01) 0.23(0.00)2 X X X X X X X 0.13(0.00) 0.68(0.00) 0.22(0.00)2 X X X X X X X X X X 0.15(0.00) 0.51(0.00) 0.24(0.00)Table 3: Prediction results of CS points with VFI using different features.
The notation on this table is the same as inTable 2classifier-based approach, we POS tagged each par-allel set of sentences, with the monolingual Englishand Spanish Tree Taggers, and we extracted thesame set of features described shown in Table 1.
Wedecided to train the models with a context size ofone word, even though both learners reached higherF-measures when using a two-word context.
Thisdecision was based on the observation that having atwo-word context will pose restrictions on possibleCS points, since we would not be able to switch un-less we have inserted into the sentence at least twotokens from the same language.We trained the NB and VFI models with the Span-glish data set (using features 2?6, 8, and 9, see Ta-ble 1) and generated CS predictions for each paral-lel file.
A code-switched sentence is generated byadding the first token of the sentence in language 1(L1), and continue adding more tokens from L1 untila CS point is found.
When a CS prediction is found,the following tokens are selected from the secondlanguage (L2), and we continue adding tokens fromL2 until the classifier has predicted a change.
Differ-ent versions of the sentences are generated by chang-ing the definition of L1 and L2.For the randomly generated CS sentences, switch-ing decisions are made randomly with a probabilityproportional to the positive predictions made by theclassifiers (in this case NB).
That is, for the Spanishsentences switch points are predicted randomly witha 30% chance of switching while for English switchpoints are predicted with a 10% chance.Generator Average ScoreHuman 3.64NB 3.33Random 2.68VFI 2.50Table 4: Average score of 18 judges over the set of 28code-switched sentences rated.In total we generated 180 CS sentences: 30 sen-tences per generator scheme (we have three genera-tors: NB, VFI, and random), and two versions fromeach generator corresponding to the two possibleconfigurations of L1-L2 (Spanish-English, English-Spanish).
We noticed that in some cases same sen-tences are generated by different methods and some-times there are no switches.
We narrowed down thesentences by randomly choosing the combination ofL1-L2 for each generator.
This reduced the num-ber of sentences from having 6 versions, to havingonly 3 versions of each sentence.
From the resulting30 sets, we removed 2 sets because one or more ofthe generator schemes produced a monolingual sen-tence.
Therefore, we used 28 sets for human evalua-tions.5.2 Human Evaluation ResultsWe had a total of 18 subjects participating in the ex-periment.
All of them identified themselves as be-ing able to read and write Spanish and English, andthe majority of them said to have used CS at least978some times.
We showed to the human subjects the28 sets of sentences.
This time we included the orig-inal version of the sentence.
Therefore, each judgewas given 4 versions of each of the 28 code-switchedsentences: the one generated from NB predictions,the one from VFI, the randomly generated, and theoriginal one.
Then we asked them to rate each sen-tence with a number from 1 to 5 indicating how nat-ural and human-like the sentence sounds.
A ratingof 5 means that they strongly agree, 4 means theyagree, 3 not sure, 2 disagree, 1 strongly disagree.The average results are presented in Table 4.
Thesentences generated by NB were scored consider-ably higher than those from VFI and random, andcloser to the human sentences.
According to thepaired t-test the difference between the NB score andthe random one is significant (p=0.01).
However theaverage score for VFI is lower than random.
Moreexperiments are needed to see if by choosing the set-ting where VFI had the highest F-measure wouldmake a difference in this respect.
Overall the sub-jects rated the human-generated CS sentences lowerthan what we were expecting, although it is clear thatthey consider these sentences more natural sound-ing than the rest.
This low rating might be related tothe attitude several evaluators expressed toward CS.In the evaluation form we asked the judges to ex-press their opinion on CS and several of them indi-cated feelings along the lines of ?we shouldn?t code-switch?.There are several ways in which two parallel sen-tences can be combined in CS, and possibly severalwill sound natural, but from our results, it is clearthat the NB algorithm was indeed able to generatea human-like CS behavior that was successfully dif-ferentiated from randomly-generated sentences.By looking at the set of automatically generatedcode-switched sentences, we realized that the ma-jority of the sentences are grammatical and naturalsounding.
We believe that for a large number of thesentences it would be hard for a human to distin-guish the sentences that were automatically gener-ated from the human-generated ones.
One of thegive away clues is when a multi-word expressionis CS, or a tag line.
Table 5 shows three examplesfrom the sentences evaluated.
In the table there isan example in sentence 1c where the noun phrase iscode-switched, the sentence is grammatical accord-ing to Spanish rules, but it sounds very odd to havethe noun carta followed by the adjective in English,?astrological?.
Other interesting features are presentin example 3 where for the same noun phrase ?pro-duce section?
we have both, the female marking de-terminer la and the masculine el.
The same thinghappens for the noun phrase ?check-out line?.
Wewould need to have a larger occurrence of these in-stances in our test set to determine if on average oneform is preferred over the other.In another experiment, we measured the predic-tion performance of NB and VFI on the 30 code-switched sentences used in this part of the evalua-tion.
The best results, an F-measure of 0.418, wereachieved by NB when a context of 1 word was used,and no words, nor lemmas were included as features.This is the same setting used for the generation pro-cess.
In contrast, VFI reached an F-measure of 0.351on this same setting.
30 sentences represent a verysmall dataset but the results are very promising sincethe speakers are different in the training and testingdataset.
Moreover, these results support the claimthat written and spoken CS obey similar rules.6 Related WorkThere is little prior work on computational linguis-tic approaches to code-switched discourse.
Mostof the previous work includes formalisms to pars-ing and generating mixed sentences, for example forMarathi and English (Joshi, 1982), or Hindi and En-glish (Goyal et al, 2003).
Sankoff proposed a pro-duction model of bilingual discourse that accountsfor the equivalence constraint and the unpredictabil-ity of code-switching (Sankoff, 1998a).
His real-time production model draws on the alternation offragments from two virtual monolingual sentences.But no statistical assessment has been conducted onreal corpora.Another related work deals with language iden-tification on English-Maltese code-switched SMSmessages (Rosner and Farrugia, 2007).
What the au-thors found to work best for language identificationin this noisy domain is a combination of a bigramHidden Markov Model, trained on language tran-sitions, and a trigram character Markov Model forhandling unknown words.9791a.
Naive Bayes:By unlocking the information in your astrological chart, puedo ver la respuesta!
Ask me!1b.
VFI:Puedo ver la answer by unlocking the information in your carta astrolo?gica!
Ask me !1c.
Random:By unlocking the information de tu carta astrological, I can see the answer!
Ask me !1d.
Human:By unlocking the information in your astrological chart, puedo ver the answer!
Pregu?ntame!1e.
English version:By unlocking the information in your astrological chart, I can see the answer!
Ask me!2a.
Naive Bayes:Pero siendo this a new year, es tiempo de empezar de nuevo que no?2b.
VFI:But this being a new year, es tiempo de empezar over isn?t it ?2c.
Random:But this being a new an?o, it?s tiempo to start over isn?t it?2d.
Human:Pero this being a new year, it?s a time to start over que no?2e.
English version:But this being a new year, it?s time to start over isn?t it?3a.
Naive Bayes:Juan confirmed me that it was very obvious, y no solamente en el produce section, en la check-out line as well.3b.
VFI:Me confirmo?
Juan que it was very obvious, y no solamente en el produce section, tambie?n en la check-out line.3c.
Random:Juan confirmed que fue very obvious, y not solamente en el a?rea de produce, in the check-out line as well.3d.
Human:Me confirmo?
Juan que fue muy obvio, y no solamente en la produce section, tambie?n en el check-out line.3e.
English version:Juan confirmed me that it was very obvious, and not only on the produce section, in the check-out line as well.Table 5: Examples of automatically generated CS sentences.7 ConclusionsWe presented preliminary results on learning to pre-dict CS points with machine learning.
One of thepossible applications of our method involves fine-tuning the weights in a multilingual language model,for instance, as part of a speech recognizer for Span-glish.
With this in mind, we restricted the possiblefeatures in the learning scenario allowing only lexi-cal and syntactic features that could be automaticallygenerated from the text.
Empirical evaluations ona Spanglish conversation showed that Naive Bayesand VFI can predict with acceptable F-measurespossible CS points, considering the difficulty of thetask.
Prediction of CS points can help improve mul-tilingual language models.Evaluation of our approach cannot be done basedonly on the gold-standard set since there is no sin-gle right answer in this task.
Therefore, we comple-mented the evaluation by involving judgements frombilingual speakers.
We generated CS sentences bytaking the predictions from the classifiers to mergeparallel sentences.
On average, the sentences gen-erated from the NB model were rated closer to theoriginal sentences, and a lot higher than the onesfrom a random generator.
Most of the sentencessounded human-like.
But because the process is au-tomatic we did find some awkward constructions,for example plural vs singular noun-verb agreement,or multi-word phrases that were code-switched inthe middle.
Perhaps a multi-word recognition fea-ture could improve results.One of the advantages of technological develop-ment and economic globalization is that more peo-ple from different regions of the world with differ-ent cultures, and therefore, different languages will980be in closer contact.
As a result, code-switching willbecome more popular.
It is important to start ad-dressing this type of bilingual communication froma computational linguistics point of view.
This workis one of the few attempts to fill the gap.Some directions for future work include: explor-ing the extent to which our results can be improvedby including a multi-word expression recognitionsystem.
We also want to investigate the integrationof our approach to multilingual language models andmove beyond CS to address other deeper linguisticphenomena.
Lastly, we would like to explore similarapproaches in other popular language combinations.AcknowledgementsThis research is supported by the National ScienceFoundation under grant 0812134.
We are grateful toRay Mooney, Melissa Sherman and the three anony-mous reviewers for insightful comments and sug-gestions.
Special thanks to the human judges thathelped with the sentence evaluations.ReferencesG.
Demiroz and H. A. Guvenir.
1997.
Classification byvoting feature intervals.
In European Conference onMachine Learning, ECML-97, pages 85?92.P.
Goyal, Manav R. Mital, A. Mukerjee, Achla M. Raina,D.
Sharma, P. Shukla, and K. Vikram.
2003.
Abilingual parser for Hindi, English and code-switchingstructures.
In Computational Linguistics for SouthAsian Languages ?Expanding Synergies with Europe,EACL-2003 Workshop, Budapest, Hungary.A.
Joshi.
1982.
Processing of sentences with intrasenten-tial code-switching.
In Ja?n Horecky?, editor, COLING-82, pages 145?150, Prague, July.J.
Lipski.
1978.
Code-switching and the problem ofbilingual competence.
In M. Paradis, editor, Aspectsof bilingualism, pages 250?264.
Hornbeam.C.
Montes-Alcala?.
2005.
Ma?ndame un e-mail: cam-bio de co?digos espan?ol-ingle?s online.
In Luis Ortizand Manel Lacorte, editors, In Contacto y contextoslingu??
?sticos: El espan?ol en los Estados Unidos y encontacto con otras lenguas.
Iberoamericana/Vervuert.C.
Montes-Alcala?.
2007.
Blogging in two languages:Code-switching in bilingual blogs.
In JonathanHolmquist, Augusto Lorenzino, and Lotfi Sayahi, edi-tors, In Selected Proc.
of the Third Workshop on Span-ish Sociolinguistics, pages 162?170, Somerville, MA.Cascadilla Proceedings Project.C.
Myers-Scotton.
1997.
Duelling Languages: Gram-matical Structure in Codeswitching.
Oxford Univer-sity Press, 2nd edition.A.
Ng and M. Jordan.
2002.
On discriminative vs. gen-erative classifiers: A comparison of logistic regressionand Naive Bayes.
In Advances in Neural InformationProcessing Systems (NIPS) 15.
MIT Press.S.
Poplack.
1980.
Sometimes I?ll start a sentence inSpanish y termino en espan?ol: toward a typology ofcode-switching.
Linguistics, 18(7/8):581?618.M.
Rosner and P. J. Farrugia.
2007.
A tagging algorithmfor mixed language identification in a noisy domain.In INTERSPEECH 2007, pages 190?193, Antwerp,Belguim, August.D.
Sankoff.
1998a.
A formal production-based expla-nation of the facts of code-switching.
Bilingualism,Language and Cognition, (1):39?50.D.
Sankoff.
1998b.
The production of code-mixed dis-course.
In 36th ACL, volume I, pages 8?21, Montreal,Quebec, Canada, August.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In International Conference onNew Methods in Language Processing, September.T.
Solorio and Y. Liu.
2008.
Part-of-speech taggingfor English-Spanish code-switched text.
In EMNLP-2008, Honolulu, Hawai, October.I.
H. Witten and E. Frank.
1999.
Data Mining, Practi-cal Machine Learning Tools and Techniques with JavaImplementations.
Morgan Kaufmann.981
