Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50?56,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsUsing Tweets to Help Sentence Compression for News HighlightsGenerationZhongyu Wei1, Yang Liu1, Chen Li1, Wei Gao21Computer Science Department, The University of Texas at DallasRichardson, Texas 75080, USA2Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar{zywei,yangl,chenli}@hlt.utdallas.edu1wgao@qf.org.qa2AbstractWe explore using relevant tweets of agiven news article to help sentence com-pression for generating compressive newshighlights.
We extend an unsuperviseddependency-tree based sentence compres-sion approach by incorporating tweet in-formation to weight the tree edge in termsof informativeness and syntactic impor-tance.
The experimental results on a pub-lic corpus that contains both news arti-cles and relevant tweets show that our pro-posed tweets guided sentence compres-sion method can improve the summariza-tion performance significantly comparedto the baseline generic sentence compres-sion method.1 Introduction?Story highlights?
of news articles are providedby only a few news websites such as CNN.com.The highlights typically consist of three or foursuccinct itemized sentences for readers to quicklycapture the gist of the document, and can dramat-ically reduce reader?s information load.
A high-light sentence is usually much shorter than its orig-inal corresponding news sentence; therefore ap-plying extractive summarization methods directlyto sentences in a news article is not enough to gen-erate high quality highlights.Sentence compression aims to retain the mostimportant information of an original sentence in ashorter form while being grammatical at the sametime.
Previous research has shown the effective-ness of sentence compression for automatic doc-ument summarization (Knight and Marcu, 2000;Lin, 2003; Galanis and Androutsopoulos, 2010;Chali and Hasan, 2012; Wang et al, 2013; Li etal., 2013; Qian and Liu, 2013; Li et al, 2014).
Thecompressed summaries can be generated througha pipeline approach that combines a generic sen-tence compression model with a summary sen-tence pre-selection or post-selection step.
Priorstudies have mostly used the generic sentencecompression approaches, however, a generic com-pression system may not be the best fit for thesummarization purpose because it does not takeinto account the summarization task in the com-pression module.
Li et al (2013) thus proposed asummary guided compression method to addressthis problem and showed the effectiveness of theirmethod.
But this approach relied heavily on thetraining data, thus has the limitation of domaingeneralization.Instead of using a manually generated corpus,we investigate using existing external sources toguide sentence compression for the purpose ofcompressive news highlights generation.
Nowa-days it becomes more and more common thatusers share interesting news content via Twitter to-gether with their comments.
The availability ofcross-media information provides new opportuni-ties for traditional tasks of Natural Language Pro-cessing (Zhao et al, 2011; Suba?si?c and Berendt,2011; Gao et al, 2012; Kothari et al, 2013;?Stajner et al, 2013).
In this paper, we propose touse relevant tweets of a news article to guide thesentence compression process in a pipeline frame-work for generating compressive news highlights.This is a pioneer study for using such parallel datato guide sentence compression for document sum-marization.Our work shares some similar ideas with (Weiand Gao, 2014; Wei and Gao, 2015).
They alsoattempted to use tweets to help news highlightsgeneration.
Wei and Gao (2014) derived externalfeatures based on the relevant tweet collection toassist the ranking of the original sentences for ex-tractive summarization in a fashion of supervisedmachine learning.
Wei and Gao (2015) proposed agraph-based approach to simultaneously rank the50original news sentences and relevant tweets in anunsupervised way.
Both of them focused on usingtweets to help sentence extraction while we lever-age tweet information to guide sentence compres-sion for compressive summary generation.We extend an unsupervised dependency-treebased sentence compression approach to incorpo-rate tweet information from the aspects of both in-formativeness and syntactic importance to weightthe tree edge.
We evaluate our method on a publiccorpus that contains both news articles and rele-vant tweets.
The result shows that generic com-pression hurts the performance of highlights gen-eration, while sentence compression guided byrelevant tweets of the news article can improve theperformance.2 FrameworkWe adopt a pipeline approach for compressivenews highlights generation.
The framework in-tegrates a sentence extraction component and apost-sentence compression component.
Each isdescribed below.2.1 Tweets Involved Sentence ExtractionWe use LexRank (Erkan and Radev, 2004) as thebaseline to select the salient sentences in a newsarticle.
This baseline is an unsupervised extractivesummarization approach and has been proved tobe effective for the summarization task.Besides LexRank, we also use HeterogeneousGraph Random Walk (HGRW) (Wei and Gao,2015) to incorporate relevant tweet informationto extract news sentences.
In this model, anundirected similarity graph is created, similar toLexRank.
However, the graph is heterogeneous,with two types of nodes for the news sentences andtweets respectively.Suppose we have a sentence set S and a tweetset T .
By considering the similarity between thesame type of nodes and cross types, the score of anews sentence s is computed as follows:p(s) =dN +M+ (1 ?
d)?
??m?Tsim(s,m)?v?Tsim(s, v)p(m)?
?+(1 ?
d)??
(1 ?
)?n?S\{s}sim(s, n)?v?S\{s}sim(s, v)p(n)??
(1)where N and M are the size of S and T , respec-tively, d is a damping factor, sim(x, y) is the simi-larity function, and the parameter  is used to con-trol the contribution of relevant tweets.
For a tweetnode t, its score can be computed similarly.
Bothd and sim(x, y) are computed following the setupof LexRank, where sim(x, y) is computed as co-sine similarity:sim(x, y) =?w?x,ytfw,xtfw,y(idfw)2??wi?x(tfwi,xidfwi)2??
?wi?y(tfwi,yidfwi)2(2)where tfw,xis the number of occurrences of wordw in instance x, idfwis the inverse document fre-quency of word w in the dataset.
In our task, eachsentence or tweet is treated as a document to com-pute the IDF value.Although both types of nodes can be ranked inthis framework, we only output the top news sen-tences as the highlights, and the input to the sub-sequent compression component.2.2 Dependency Tree Based SentenceCompressionWe use an unsupervised dependency tree basedcompression framework (Filippova and Strube,2008) as our baseline.
This method achieved ahigher F-score (Riezler et al, 2003) than other sys-tems on the Edinburgh corpus (Clarke and Lap-ata, 2006).
We will introduce the baseline in thispart and describe our extended model that lever-ages tweet information in the next subsection.The sentence compression task can be definedas follows: given a sentence s, consisting of wordsw1, w2, ..., wm, identify a subset of the words ofs, such that it is grammatical and preserves es-sential information of s. In the baseline frame-work, a dependency graph for an original sentenceis first generated and then the compression is doneby deleting edges of the dependency graph.
Thegoal is to find a subtree with the highest score:f(X) =?e?Exe?
winfo(e)?
wsyn(e) (3)where xeis a binary variable, indicating whethera directed dependency edge e is kept (xeis 1) orremoved (xeis 0), and E is the set of edges in thedependency graph.
The weighting of edge e con-siders both its syntactic importance (wsyn(e)) aswell as the informativeness (winfo(e)).
Supposeedge e is pointed from head h to node n with de-pendency label l, both weights can be computedfrom a background news corpus as:winfo(e) =Psummary(n)Particle(n)(4)51wsyn(e) = P (l|h) (5)where Psummary(n) and Particle(n) are the uni-gram probabilities of word n in the two languagemodels trained on human generated summariesand the original articles respectively.
P (l|h) isthe conditional probability of label l given headh.
Note that here we use the formula in (Filip-pova and Altun, 2013) for winfo(e), which wasshown to be more effective for sentence compres-sion than the original formula in (Filippova andStrube, 2008).The optimization problem can be solved underthe tree structure and length constraints by integerlinear programming1.
Given that L is the maxi-mum number of words permitted for the compres-sion, the length constraint is simply representedas:?e?Exe?
L (6)The surface realizatdion is standard: the wordsin the compression subtree are put in the same or-der they are found in the source sentence.
Dueto space limit, we refer readers to (Filippova andStrube, 2008) for a detailed description of thebaseline method.2.3 Leverage Tweets for Edge WeightingWe then extend the dependency-tree based com-pression framework by incorporating tweet infor-mation for dependency edge weighting.
We in-troduce two new factors, wTinfo(e) and wTsyn(e),for informativeness and syntactic importance re-spectively, computed from relevant tweets of thenews.
These are combined with the weights ob-tained from the background news corpus definedin Section 2.2, as shown below:winfo(e) = (1??)
?wNinfo(e)+?
?wTinfo(e) (7)wsyn(e) = (1?
?)
?
wNsyn(e) + ?
?
wTsyn(e) (8)where ?
and ?
are used to balance the contributionof the two sources, and wNinfo(e) and wNsyn(e) arebased on Equation 4 and 5.The new informative weight wTinfo(e) is calcu-lated as:wTinfo(e) =PrelevantT(n)PbackgroundT(n)(9)1In our implementation we use GNU Linear Pro-gramming Kit (GULP) (https://www.gnu.org/software/glpk/)PrelevantT(n) and PbackgroundT(n) are the uni-gram probabilities of word n in two language mod-els trained on the relevant tweet dataset and abackground tweet dataset respectively.The new syntactic importance score is:wTsyn(e) =NT (h, n)NT(10)NT (h, n) is the number of tweets where n andhead h appear together within a window frame ofK, and NT is the total number of tweets in therelevant tweet collection.
Since tweets are alwaysnoisy and informal, traditional parsers are not reli-able to extract dependency trees.
Therefore, weuse co-occurrence as pseudo syntactic informa-tion here.
Note wNinfo(e), wTinfo(e), wNsyn(e) andwTsyn(e) are normalized before combination.3 Experiment3.1 SetupWe evaluate our pipeline news highlights gen-eration framework on a public corpus based onCNN/USAToday news (Wei and Gao, 2014).
Thiscorpus was constructed via an event-oriented strat-egy following four steps: 1) 17 salient news eventstaking place in 2013 and 2014 were manuallyidentified.
2) For each event, relevant tweets wereretrieved via Topsy2search API using a set ofmanually generated core queries.
3) News arti-cles explicitly linked by URLs embedded in thetweets were collected.
4) News articles fromCNN/USAToday that have more than 100 explic-itly linked tweets were kept.
The resulting cor-pus contains 121 documents, 455 highlights and78,419 linking tweets.We used tweets explicitly linked to a news ar-ticle to help extract salience sentences in HGRWand to generate the language model for computingwTinfo(e).
The co-occurrence information com-puted from the set of explicitly linked tweets isvery sparse because the size of the tweet set issmall.
Therefore, we used all the tweets re-trieved for the event related to the target news arti-cle to compute the co-occurrence information forwTsyn(e).
Tweets retrieved for events were not pub-lished in (Wei and Gao, 2014).
We make it avail-able here3.
The statistics of the dataset can befound in Table.
1.2http://topsy.com3http://www.hlt.utdallas.edu/?zywei/data/CNNUSATodayEvent.zip52Event Doc # HLight #Linked RetrievedEvent Doc # HLight #Linked RetrievedTweet # Tweet # Tweet # Tweet #Aurora shooting 14 54 12,463 588,140 African runner murder 8 29 9,461 303,535Boston bombing 38 147 21,683 1,650,650 Syria chemical weapons use 1 4 331 11,850Connecticut shooting 13 47 3,021 213,864 US military in Syria 2 7 719 619,22Edward Snowden 5 17 1,955 379,349 DPRK Nuclear Test 2 8 3,329 103,964Egypt balloon crash 3 12 836 36,261 Asiana Airlines Flight 214 11 42 8,353 351,412Hurricane Sandy 4 15 607 189,082 Moore Tornado 5 19 1,259 1,154,656Russian meteor 3 11 6,841 239,281 Chinese Computer Attacks 2 8 507 28,988US Flu Season 7 23 6,304 1,042,169 Williams Olefins Explosion 1 4 268 14,196Super Bowl blackout 2 8 482 214,775 Total 121 455 78,419 6,890,987Table 1: Distribution of documents, highlights and tweets with respect to different eventsMethodROUGE-1 Compr.F(%) P(%) R(%) Rate(%)LexRank 26.1 19.9 39.1 100LexRank + SC 25.2 22.4 29.6 63.0LexRank + SC+wTinfo25.7 22.8 30.1 62.0LexRank + SC+wTsyn26.2 23.5 30.4 63.7LexRank + SC+both 27.5 25.0 31.4 61.5HGRW 28.1 22.6 39.5 100HGRW + SC 26.4 24.9 29.5 66.1HGRW + SC+wTinfo27.5 25.7 30.8 65.4HGRW + SC+wTsyn27.0 25.3 30.2 66.7HGRW + SC+both 28.4 26.9 31.2 64.8Table 2: Overall Performance.
Bold: the bestvalue in each group in terms of different metrics.Following (Wei and Gao, 2014), we output 4sentences for each news article as the highlightsand report the ROUGE-1 scores (Lin, 2004) usinghuman-generated highlights as the reference.The sentence compression rates are set to 0.8 forshort sentences containing fewer than 9 words, and0.5 for long sentences with more than 9 words, fol-lowing (Filippova and Strube, 2008).
We empiri-cally use 0.8 for ?, ?
and  such that tweets havemore impact for both sentence selection and com-pression.
We leveraged The New York Times An-notated Corpus (LDC Catalog No: LDC2008T19)as the background news corpus.
It has both theoriginal news articles and human generated sum-maries.
The Stanford Parser4is used to obtain de-pendency trees.
The background tweet corpus iscollected from Twitter public timeline via TwitterAPI, and contains more than 50 million tweets.3.2 ResultsTable 2 shows the overall performance5.
For sum-maries generated by both LexRank and HGRW,?+SC?
means generic sentence compression base-4http://nlp.stanford.edu/software/lex-parser.shtml5The performance of HGRW reported here is differentfrom (Wei and Gao, 2015) because the setup is different.
Weuse all the explicitly linked tweets in the ranking process herewithout considering redundancy while a redundancy filteringprocess was applied in (Wei and Gao, 2015) .line (Section.
2.2) is used, ?+wTinfo?
and ?+wTsyn?indicate tweets are used to help edge weightingfor sentence compression in terms of informative-ness and syntactic importance respectively, and?+both?
means both factors are used.
We haveseveral findings.?
The tweets involved sentence extraction modelHGRW can improve LexRank by 8.8% rela-tively in terms of ROUGE-1 F score, showingthe effectiveness of relevant tweets for sentenceselection.?
With generic sentence compression, theROUGE-1 F scores for both LexRank andHGRW drop, mainly because of a much lowerrecall score.
This indicates that generic sen-tence compression without certain guidanceremoves salient content of the original sentencethat may be important for summarization andthus hurts the performance.
This is consistentwith the finding of (Chali and Hasan, 2012).?
By adding either wTinfoor wTsyn, the perfor-mance of summarization increases, showingthat relevant tweets can be used to help thescores of both informativeness and syntactic im-portance.?
+SC+both improves the summarization perfor-mance significantly6compared to the corre-sponding compressive summarization baseline+SC, and outperforms the corresponding origi-nal baseline, LexRank and HGRW.?
The improvement obtained byLexRank+SC+both compared to LexRankis more promising than that obtained byHGRW+SC+both compared to HGRW.
Thismay be because HGRW has used tweet in-formation already, and leaves limited roomfor improvement for the sentence compres-sion model when using the same source ofinformation.6Significance throughout the paper is computed by twotailed t-test and reported when p < 0.05.53l l l l l l l l l l l0.0 0.2 0.4 0.6 0.8 1.00.250.260.270.280.290.30?ROUGE?1 Fscorel LexRankLexRank+SCLexRank+SC+bothHGRWHGRW+SCHGRW+SC+both(a) Impact of ?l l l l l l l l l l l0.0 0.2 0.4 0.6 0.8 1.00.250.260.270.280.290.30?ROUGE?1 Fscorel LexRankLexRank+SCLexRank+SC+bothHGRWHGRW+SCHGRW+SC+both(b) Impact of ?Figure 1: The influence of ?
and ?.
Solid lines are used for approaches based on LexRank; Dotted linesare used for HGRW based approaches.Method Example 1 Example 2LexRankBoston bombing suspect Tamerlan Tsarnaev, Three people were hospitalized in critical condition,killed in a shootout with police days after the according to information provided by hospitalsblast, has been buried at an undisclosed who reported receiving patients from the blast.location, police in Worcester, Mass., said.LexRank+SCsuspect Tamerlan Tsarnaev, killed in a Three people were hospitalized,shootout after the blast, has been buried at an according to information provided by hospitalslocation, police in Worcester Mass.
said.
who reported receiving from the blast.LexRank+SC+bothBoston bombing suspect Tamerlan Tsarnaev, Three people were hospitalized in critical condition,killed in a shootout after the blast, has been according to information provided by hospitals.buried at an location police said.Ground TruthBoston bombing suspect Tamerlan Tsarnaev Hospitals report three people in critical conditionhas been buried at an undisclosed locationTable 3: Example highlight sentences from different systems?
By incorporating tweet information for bothsentence selection and compression, the per-formance of HGRW+SC+both outperformsLexRank significantly.Table 3 shows some examples.
As we can seein Example 1, with the help of tweet informa-tion, our compression model keeps the valuablepart ?Boston bombing?
for summarization whilethe generic one abandons it.We also investigate the influence of ?
and ?.
Tostudy the impact of ?, we fix ?
to 0.8, and viceversa.
As shown in Figure 1, it is clear that larger?
or ?, i.e., giving higher weights to tweets relatedinformation, is generally helpful.4 Conclusion and Future WorkIn this paper, we showed that the relevant tweetcollection of a news article can guide the processof sentence compression to generate better storyhighlights.
We extended a dependency-tree basedsentence compression model to incorporate tweetinformation.
The experiment results on a publiccorpus that contains both news articles and rele-vant tweets showed the effectiveness of our ap-proach.
With the popularity of Twitter and increas-ing interaction between social media and newsmedia, such parallel data containing news and re-lated tweets is easily available, making our ap-proach feasible to be used in a real system.There are some interesting future directions.For example, we can explore more effective waysto incorporate tweets for sentence compression;we can study joint models to combine both sen-tence extraction and compression with the help ofrelevant tweets; it will also be interesting to use theparallel dataset of the news articles and the tweetsfor timeline generation for a specific event.AcknowledgmentsWe thank the anonymous reviewers for their de-tailed and insightful comments on earlier draftsof this paper.
The work is partially supportedby NSF award IIS-0845484 and DARPA ContractNo.
FA8750-13-2-0041.
Any opinions, findings,and conclusions or recommendations expressedare those of the authors and do not necessarily re-flect the views of the funding agencies.54ReferencesYLlias Chali and Sadid A Hasan.
2012.
On the effec-tiveness of using sentence compression models forquery-focused multi-document summarization.
InProceedings of the 25th International Conference onComputational Linguistics, pages 457?474.James Clarke and Mirella Lapata.
2006.
Modelsfor sentence compression: A comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for Computa-tional Linguistics, pages 377?384.
Association forComputational Linguistics.G?unes Erkan and Dragomir R Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, 22:457?479.Katja Filippova and Yasemin Altun.
2013.
Overcom-ing the lack of parallel data in sentence compression.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1481?1491.
Association for Computational Linguis-tics.Katja Filippova and Michael Strube.
2008.
Depen-dency tree based sentence compression.
In Proceed-ings of the Fifth International Natural LanguageGeneration Conference, pages 25?32.
Associationfor Computational Linguistics.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 885?893.
Association for Computa-tional Linguistics.Wei Gao, Peng Li, and Kareem Darwish.
2012.
Jointtopic modeling for event summarization across newsand social media streams.
In Proceedings of the 21stACM International Conference on Information andKnowledge Management, pages 1173?1182.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization-step one: Sentence compres-sion.
In Proceedings of The 7th National Confer-ence on Artificial Intelligence, pages 703?710.Alok Kothari, Walid Magdy, Ahmed Mourad Ka-reem Darwish, and Ahmed Taei.
2013.
Detectingcomments on news articles in microblogs.
In Pro-ceedings of The 7th International AAAI Conferenceon Weblogs and Social Media, pages 293?302.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013.Document summarization via guided sentence com-pression.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 490?500.
Association for ComputationalLinguistics.Chen Li, Yang Liu, Fei Liu, Lin Zhao, and FuliangWeng.
2014.
Improving multi-documents summa-rization by sentence compression based on expandedconstituent parse trees.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing, pages 691?701.
Association forComputational Linguistics.Chin-Yew Lin.
2003.
Improving summarization per-formance by sentence compression: a pilot study.
InProceedings of the sixth international workshop onInformation retrieval with Asian languages-Volume11, pages 1?8.
Association for Computational Lin-guistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.Xian Qian and Yang Liu.
2013.
Fast joint compres-sion and summarization via graph cuts.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 1492?1502.Association for Computational Linguistics.Stefan Riezler, Tracy H King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensa-tion using ambiguity packing and stochastic disam-biguation methods for lexical-functional grammar.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 118?125.
Association for Computational Lin-guistics.Tadej?Stajner, Bart Thomee, Ana-Maria Popescu,Marco Pennacchiotti, and Alejandro Jaimes.
2013.Automatic selection of social media responses tonews.
In Proceedings of the 19th ACM InternationalConference on Knowledge Discovery and Data Min-ing, pages 50?58.
ACM.Ilija Suba?si?c and Bettina Berendt.
2011.
Peddling orcreating?
investigating the role of twitter in newsreporting.
In Advances in Information Retrieval,pages 207?213.
Springer.Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-rian, and Claire Cardie.
2013.
A sentence com-pression based framework to query-focused multi-document summarization.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1384?1394.
Associationfor Computational Linguistics.Zhongyu Wei and Wei Gao.
2014.
Utilizing microblogfor automatic news highlights extraction.
In Pro-ceedings of the 25th International Conference onComputational Linguistics, pages 872?883.Zhongyu Wei and Wei Gao.
2015.
Gibberish, assis-tant, or master?
using tweets linking to news forextractive single-document summarization.
In Pro-ceedings of the 38th International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval.55Wayne Xin Zhao, Jing Jiang, Jianshu Weng, JingHe, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.2011.
Comparing twitter and traditional media us-ing topic models.
In Advances in Information Re-trieval, pages 338?349.
Springer.56
