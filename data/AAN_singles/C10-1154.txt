Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1371?1378,Beijing, August 2010Detecting Speech Repairs IncrementallyUsing a Noisy Channel ApproachSimon Zwarts, Mark Johnson and Robert DaleCentre for Language TechnologyMacquarie University{simon.zwarts|mark.johnson|robert.dale}@mq.edu.auAbstractUnrehearsed spoken language oftencontains disfluencies.
In order to cor-rectly interpret a spoken utterance,any such disfluencies must be identi-fied and removed or otherwise dealtwith.
Operating on transcripts ofspeech which contain disfluencies, ourparticular focus here is the identifica-tion and correction of speech repairsusing a noisy channel model.
Our aimis to develop a high-accuracy mecha-nism that can identify speech repairsin an incremental fashion, as the ut-terance is processed word-by-word.We also address the issue of the evalu-ation of such incremental systems.
Wepropose a novel approach to evalua-tion, which evaluates performance indetecting and correcting disfluenciesincrementally, rather than only assess-ing performance once the processing ofan utterance is complete.
This demon-strates some shortcomings in our ba-sic incremental model, and so we thendemonstrate a technique that improvesperformance on the detection of disflu-encies as they happen.1 IntroductionOne of the most obvious differences betweenwritten language and spoken language is thefact that the latter presents itself incremen-tally over some time period.
Most natural lan-guage processing applications operate on com-plete sentences; but for real time spontaneousspeech, there are potential benefits to incre-mentally processing the input so that a systemcan stay responsive and interact directly be-fore a speaker?s utterance is complete.
Workin psycholinguistics supports the view that thehuman parsing mechanism works incremen-tally, with partial semantic interpretations be-ing produced before the complete utterancehas been heard (Marslen-Wilson, 1973).
Ourinterest is in developing similarly incrementalprocessing techniques for natural language in-terpretation, so that, for example, a speechrecognizer might be able to interject duringa long utterance to object, cut the speakershort, or correct a mistaken assumption; sucha mechanism is even required for the appro-priate timing of backchannel signals.
Addi-tionally the incremental nature of the modelallows potential application of this model inspeech recognition models.Another feature of unrehearsed spoken lan-guage that has no obvious correlate in writtenlanguage is the presence of disfluencies.1 Dis-fluencies are of different types, ranging fromsimple filled pauses (such as um and uh) tomore complicated structures where the se-quence of words that make up the utterance is?repaired?
while it is being produced.
Whereassimpler disfluencies may be handled by sim-ply deleting them from the sequence of wordsunder consideration, the editing terms in aspeech repair are part of the utterance, andtherefore require more sophisticated process-ing.There are three innovations in the presentpaper.
First, we demonstrate that a noisychannel model of speech repairs can work ac-curately in an incremental fashion.
Second,we provide an approach to the evaluation of1Although some disfluencies can be consideredgrammatical errors, they are generally quite distinctin both cause and nature from the kinds of grammat-ical errors found in written text.1371such an incremental model.
Third, we tacklethe problem of the early detection of speechrepairs, and demonstrate a technique that de-creases the latency (as measured in tokens)involved in spotting that a disfluency has oc-curred.The rest of the paper is structured as fol-lows.
Section 2 provides some backgroundon speech repairs and existing approaches tohandling them, including Johnson and Char-niak?s (2004) model, which we use as a start-ing point for our incremental model.
Section3 describes our model in detail, focusing onthe noisy channel model and the incrementalcomponent of this model.
Section 4 introducessome considerations that arise in the develop-ment of techniques for the evaluation of in-cremental disfluency detection; we then pro-vide a quantitative assessment of our perfor-mance using these techniques.
Our evaluationreveals that our basic incremental model doesnot perform very well at detecting disfluenciesclose to where they happen, so in Section 5 wepresent a novel approach to optimise detectionof these disfluencies as early as possible.
Fi-nally Section 6 concludes and discusses futurework.2 Speech RepairsWe adopt the terminology and definitions in-troduced by Shriberg (1994) to discuss disflu-ency.
We are particularly interested in whatare called repairs.
These are the hardesttypes of disfluency to identify since they arenot marked by a characteristic vocabulary.Shriberg (1994) identifies and defines threedistinct parts of a repair, referred to as thereparandum, the interregnum and the re-pair.
Consider the following utterance:I want a flightreparandum?
??
?to Boston,uh, I mean?
??
?interregnumto Denver?
??
?repairon Friday (1)The reparandum to Boston is the part of theutterance that is being edited out; the inter-regnum uh is a filler, which may not always bepresent; and the repair to Denver replaces thereparandum.Given an utterance that contains such a re-pair, we want to be able to correctly detectthe start and end positions of each of thesethree components.
We can think of each wordin an utterance as belonging to one of fourcategories: fluent material, reparandum, in-terregnum, or repair.
We can then assess theaccuracy of techniques that attempt to detectdisfluencies by computing precision and recallvalues for the assignment of the correct cate-gories to each of the words in the utterance,as compared to the gold standard as indicatedby annotations in the corpus.An alternative means of evaluation wouldbe to simply generate a new signal with thereparandum and filler removed, and comparethis against a ?cleaned-up?
version of the ut-terance; however, Core and Schubert (1999)argue that, especially in the case of speechrepairs, it is important not to simply throwaway the disfluent elements of an utterance,since they can carry meaning that needs tobe recovered for proper interpretation of theutterance.
We are therefore interested in thefirst instance in a model of speech error detec-tion, rather than a model of correction.Johnson and Charniak (2004) describe sucha model, using a noisy-channel based approachto the detection of the start and end points ofreparanda, interregna and repairs.
Since weuse this model as our starting point, we pro-vide a more detailed explanation in Section 3.The idea of using a noisy channel modelto identify speech repairs has been exploredfor languages other than English.
Honal andSchultz (2003) use such a model, compar-ing speech disfluency detection in spontaneousspoken Mandarin against that in English.
Theapproach performs well in Mandarin, althoughbetter still in English.Both the models just described operate ontranscripts of completed utterances.
Ideally,however, when we deal with speech we wouldlike to process the input word by word as it isreceived.
Being able to do this would enabletighter integration in both speech recognition1372and interpretation, which might in turn im-prove overall accuracy.The requirement for incrementality is recog-nised by Schuler et al (2010), who employan incremental Hierarchical Hidden MarkovModel (HHMM) to detect speech disfluen-cies.
The HHMM is trained on manually an-notated parse trees which are transformed bya right corner transformation; the HHMM isthen used in an incremental fashion on un-seen data, growing the parse structure eachtime a new token comes in.
Special subtreesin this parse can carry a marker indicatingthat the span of the subtree consists of tokenscorresponding to a speech disfluency.
Schuleret al?s approach thus provides scope for de-tecting disfluencies in an incremental fashion.However, their reported accuracy scores arenot as good as those of Johnson and Char-niak (2004): they report an F-score of 0.690for their HHMM+RCT model, as comparedto 0.797 for Johnson and Charniak?s parsermodel.Our aim in this paper, then, is to investigatewhether it is possible to adapt Johnson andCharniak?s model to process utterances incre-mentally, without any loss of accuracy.
Todefine the incremental component more pre-cisely, we investigate the possibility of mark-ing the disfluencies as soon as possible duringthe processing of the input.
Given two modelsthat provide comparable accuracy measuredon utterance completion, we would prefer amodel which detects disfluencies earlier.3 The ModelIn this section, we describe Johnson and Char-niak?s (2004) noisy channel model, and showhow this model can be made incremental.As a data set to work with, we use theSwitchboard part of the Penn Treebank 3 cor-pus.
The Switchboard corpus is a corpus ofspontaneous conversations between two par-ties.
In Penn Treebank 3, the disfluencies aremanually annotated.
Following Johnson andCharniak (2004), we use all of sections 2 and3 for training; we use conversations 4[5-9]* fora held-out training set; and conversations 40*,41[0-4]* and 415[0-3]* as the held-out test set.3.1 The Noisy Channel ModelTo find the repair disfluencies a noisy channelmodel is used.
For an observed utterance withdisfluencies, y, we wish to find the most likelysource utterance, x?, where:x?
= argmaxx p(x | y) (2)= argmaxx p(y | x) p(x)Here we have a channel model p(y|x) whichgenerates an utterance y given a source x anda language model p(x).
We assume that xis a substring of y, i.e., the source utterancecan be obtained by marking words in y as adisfluency and effectively removing them fromthis utterance.Johnson and Charniak (2004) experimentwith variations on the language model; theyreport results for a bigram model, a trigrammodel, and a language model using the Char-niak Parser (Charniak, 2001).
Their parsermodel outperforms the bigram model by 5%.The channel model is based on the intuitionthat a reparandum and a repair are generallyvery alike: a repair is often almost a copy ofthe reparandum.
In the training data, over60% of the words in a reparandum are lexicallyidentical to the words in the repair.
Exam-ple 1 provides an example of this: half of therepair is lexically identical to the reparandum.The channel model therefore gives the high-est probability when the reparandum and re-pair are lexically equivalent.
When the poten-tial reparandum and potential repair are notidentical, the channel model performs dele-tion, insertion or substitution.
The proba-bilities for these operations are defined on alexical level and are derived from the trainingdata.
This channel model is formalised us-ing a Synchronous Tree Adjoining Grammar(STAG) (Shieber and Schabes, 1990), whichmatches words from the reparandum to therepair.
The weights for these STAG rules arelearnt from the training text, where reparandaand repairs are aligned to each other using aminimum edit-distance string aligner.1373For a given utterance, every possible ut-terance position might be the start of areparandum, and every given utterance po-sition thereafter might be the start of a re-pair (to limit complexity, a maximum distancebetween these two points is imposed).
Ev-ery disfluency in turn can have an arbitrarylength (again up to some maximum to limitcomplexity).
After every possible disfluencyother new reparanda and repairs might occur;the model does not attempt to generate cross-ing or nested disfluencies, although they dovery occasionally occur in practice.
To findthe optimal selection for reparanda and re-pairs, all possibilities are calculated and theone with the highest probability is selected.A chart is filled with all the possible startand end positions of reparanda, interregnaand repairs; each entry consists of a tuple?rmbegin, irbegin, rrbegin, rrend?, where rm is thereparandum, ir is the interregnum and rr isthe repair.
A Viterbi algorithm is used to findthe optimal path through the utterance, rank-ing each chart entry using the language modeland channel model.
The language model, abigram model, can be easily calculated giventhe start and end positions of all disfluencycomponents.
The channel model is slightlymore complicated because an optimal align-ment between reparandum and repair needsto be calculated.
This is done by extendingeach partial analysis by adding a word to thereparandum, the repair or both.
The start po-sition and end position of the reparandum andrepair are given for this particular entry.
Thetask of the channel model is to calculate thehighest probable alignment between reparan-dum and repair.
This is done by initialisingwith an empty reparandum and repair, and?growing?
the analysis one word at a time.
Us-ing a similar approach to that used in calculat-ing the edit-distance between reparandum andrepair, the reparandum and repair can both beextended with one of four operations: deletion(only the reparandum grows), insertion (onlythe repair grows), substitution (both grow),or copy (both grow).
When the reparandumand the repair have their length correspond-ing to the current entry in the chart, the chan-nel probability can be calculated.
Since thereare multiple alignment possibilities, we use dy-namic programming to select the most proba-ble solutions.
The probabilities for insertion,deletion or substitution are estimated fromthe training corpus.
We use a beam-searchstrategy to find the final optimum when com-bining the channel model and the languagemodel.3.2 IncrementalityTaking Johnson and Charniak?s model as astarting point, we would like to develop an in-cremental version of that algorithm.
We sim-ulate incrementality by maintaining for eachutterance to be processed an end-of-prefixboundary; tokens after this boundary arenot available for the model to use.
At eachstep in our incremental model, we advance thisboundary by one token (the increment), un-til finally the entire utterance is available.
Wemake use of the notion of a prefix, which isa substring of the utterance consisting of alltokens up to this boundary marker.Just as in the non-incremental model, wekeep track of all the possible reparanda and re-pairs in a chart.
Every time the end-of-prefixboundary advances, we update the chart: weadd all possible disfluencies which have theend position of the repair located one tokenbefore the end-of-prefix boundary, and we addall possible start points for the reparandum,interregna and repair, and end points for thereparandum and interregna, given the order-ing constraints of these components.In our basic incremental model, we leave theremainder of the algorithm untouched.
Whenthe end-of-prefix boundary reaches the end ofthe utterance, and thus the entire utteranceis available, this model results in an iden-tical analysis to that provided by the non-incremental model, since the chart containsidentical entries, although calculated in a dif-ferent order.
Intuitively, this model shouldperform well when the current prefix is veryclose to being a complete utterance; and itshould perform less well when a potential dis-1374fluency is still under construction, since thesesituations are not typically found in the train-ing data.
We will return to this point furtherbelow.We do not change the training phase of themodel and we assume that the optimal valuesfound for the non-incremental model are alsooptimal for the incremental model, since mostweights which need to be learned are based onlexical values.
Other weights are bigram basedvalues, and values dealing with unknown to-kens (i.e., tokens which occur in the test data,but not in the training data); it is not unrea-sonable to assume these weights are identicalor very similar in both the incremental andthe non-incremental model.4 Evaluation Models and TheirApplicationAs well as evaluating the accuracy of the anal-ysis returned at the end of the utterance, itseems reasonable to also evaluate how quicklyand accurately an incremental algorithm de-tects disfluencies on a word-by-word basis asthe utterance is processed.
In this section, weprovide the methodological background to ourapproach, and in Section 5.2 we discuss theperformance of our model when evaluated inthis way.Incremental systems are often judged solelyon the basis of their output when the utter-ance being processed is completed.
Althoughthis does give an insight into how well a systemperforms overall, it does not indicate how wellthe incremental aspects of the mechanism per-form.
In this section we present an approachto the evaluation of a model of speech repairdetection which measures the performance ofthe incremental component.One might calculate the accuracy over allprefixes using a simple word accuracy score.However, because each prefix is a superstringof each previous prefix, such a calculationwould not be fair: tokens that appear in earlyin the utterance will be counted more oftenthan tokens that appear later in the utterance.In theory, the analysis of the early tokens canchange at each prefix, so arguably it wouldmake sense to reevaluate the complete analy-sis so far at every step.
In practice, however,these changes do not happen, and so this mea-surement would not reflect the performance ofthe system correctly.Our approach is to define a measure of re-sponsiveness: that is, how soon is a dis-fluency detected?
We propose to measureresponsiveness in two ways.
The time-to-detection score indicates how many tokensfollowing a disfluency are read before the givendisfluency is marked as one; the delayed ac-curacy score looks n tokens back from theboundary of the available utterance and, whenthere is a gold standard disfluency-marked to-ken at that distance, counts how often thesetokens are marked correctly.We measure the time-to-detection score bytwo numbers, corresponding to the number oftokens from the start of the reparandum andthe number of tokens from the start of the re-pair.
We do this because disfluencies can be ofdifferent lengths.
We assume it is unlikely thata disfluency will be found before the reparan-dum is completed, since the reparandum it-self is often fluent.
We measure the time-to-detection by the first time a given disfluencyappears as one.Since the model is a statistical model, itis possible that the most probable analysismarks a given word at position j as a disflu-ency, while in the next prefix the word in thesame position is now no longer marked as be-ing disfluent.
A prefix later this word mightbe marked as disfluent again.
This presentsus with a problem.
How do we measure whenthis word was correctly identified as disfluent:the first time it was marked as such or the sec-ond time?
Because of the possibility of suchoscillations, we take the first marking of thedisfluency as the measure point.
Disfluencieswhich are never correctly detected are not partof the time-to-detection score.Since the evaluation starts with disfluenciesfound by the model, this measurement hasprecision-like properties only.
Consequently,there are easy ways to inflate the score arti-ficially at the cost of recall.
We address this1375by also calculating the delayed accuracy.
Thisis calculated at each prefix by looking back ntokens from the prefix boundary, where n = 0for the prefix boundary.
For each n we cal-culate the accuracy score at that point overall prefixes.
Each token is only assessed oncegiven a set value of n, so we do not sufferfrom early prefixes being assessed more often.However, larger values of n do not take all to-kens into account, since the last y tokens ofan utterance will not play a part in the ac-curacy when y < n. Since we evaluate givena gold standard disfluency, this measurementhas recall-like properties.Together with the final accuracy score overthe entire utterance, the time-to-detectionand delayed accuracy scores provide differentinsights and together give a good measure-ment of the responsiveness and performanceof the model.Our incremental model has the same fi-nal accuracy as the original non-incrementalmodel; this corresponds to an F-score (har-monic mean) of 0.778 on a word basis.We found the average time to detection,measured in tokens for this model to be 8.3measured from the start of reparandum and5.1 from the start of repair.
There are situ-ations where disfluencies can be detected be-fore the end of the repair; by counting fromthe start rather than the end of the disfluencycomponents, we provide a way of scoring insuch cases.
To provide a better insight intowhat is happening, we also report the averagedistance since the start of the reparandum.We find that the time to detect is larger thanthe average repair length; this implies that,under this particular model, most disfluenciesare only detected after the repair is finished.In fact the difference is greater than 1, whichmeans that in most cases it takes one more to-ken after the repair before the model identifiesthe disfluency.Table 1 shows the delayed accuracy.
We cansee that the score first rises quickly after whichthe increases become much smaller.
As men-tioned above, a given disfluency detection intheory might oscillate.
In practice, however,oscillating disfluencies are very rare, possiblybecause a bigram model operates on a very lo-cal level.
Given that oscillation is rare, a quickstabilisation of the score indicates that, whenwe correctly detect a disfluency, this happensrather quickly after the disfluency has com-pleted, since the accuracy for the large n iscalculated over the same tokens as the accu-racy for the smaller n (although not in thesame prefix).5 Disfluencies around PrefixBoundaries5.1 Early detection algorithmOur model uses a language model and a chan-nel model to locate disfluencies.
It calculatesa language model probability for the utterancewith the disfluency taken out, and it calculatesthe probability of the disfluency itself with theSTAG channel model.Consider the following example utterancefragment where a repair disfluency occurs:.
.
.
wireparandum?
??
?rni+1 rni+2repair?
??
?rri+3 rri+4 wi+5 .
.
.
(3)Here, the subscripts indicate token position insequence; w is a token outside the disfluency;and rn is a reparandum being repaired bythe repair rr.
The language model estimatesthe continuation of the utterance without thedisfluency.
The model considers whether theutterance continuation after the disfluency isprobable given the language model; the rel-evant bigram here is p(rri+3|wi), continuingwith p(rri+4|rri+3).
However, under the in-cremental model, it is possible the utterancehas only been read as far as token i + 3, inwhich case the probability p(wi+4|wi+3) is un-defined.We would like to address the issue of look-ing beyond a disfluency under construction.We assume the issue of not being able to lookfor an utterance continuation after the repaircomponent of the disfluency can be found backin the incremental model scores.
A disfluencyis usually only detected after the disfluency iscompletely uttered, and always requires one1376n tokens back 1 2 3 4 5 6accuracy 0.500 0.558 0.631 0.665 0.701 0.714Table 1: delayed accuracy, n tokens back from the end of prefixesn tokens back 1 2 3 4 5 6accuracy 0.578 0.633 0.697 0.725 0.758 0.770Table 2: delayed accuracy under the updated modelmore token in the basic model.
In the giveninstance this means it is unlikely that we willdetect the disfluency before i + 5.In order to make our model more respon-sive, we propose a change which makes itpossible for the model to calculate channelprobabilities and language model probabili-ties before the repair is completed.
Assum-ing we have not yet reached the end of utter-ance, we would like to estimate the continua-tion of the utterance with the relevant bigramp(rri+4|rri+3).
Since rri+4 is not yet avail-able we cannot calculate this probability.
Thecorrect thing to do is to sum over all possiblecontinuations, including the end of utterancetoken (for the complete utterance, as opposedto the current prefix).
This results in the fol-lowing bigram estimation:?t?vocabularyp(t|wi) (4)This estimation is not one we need to derivefrom our data set, since p is a true probability.In this case, the sum over all possible continu-ations (this might include an end of utterancemarker, in which case the utterance is alreadycomplete) equals 1.
We therefore modify thealgorithm so that it takes this into account.This solves the problem of the language modelassessing the utterance with the disfluency cutout, when nothing from the utterance contin-uation after a disfluency is available.The other issue which needs to be addressedis the alignment of the reparandum with therepair when the repair is not yet fully avail-able.
Currently the model is encouraged toalign the individual tokens of the reparandumwith those of the repair.
The algorithm haslower estimations when the reparandum can-not be fully aligned with the repair becausethe reparandum and repair differ considerablyin length.We note that most disfluencies are veryshort: reparanda and repairs are often onlyone or two tokens each in length, and the inter-regnum is often empty.
To remove the penaltyfor an incomplete repair, we allow the repair togrow one token beyond the prefix boundary;given the relative shortness of the disfluencies,this seems reasonable.
Since this token is notavailable, we cannot calculate the lexical sub-stitution value.
Instead we define a new opera-tion in the channel model: in addition to dele-tion, insertion, copy, and substitution, we addan additional substitution operation, the in-cremental completion substitution.
Thisoperation does not compete with the copy op-eration or the normal substitution operation,since it is only defined when the last token ofthe repair falls at the prefix boundary.5.2 Results for the Early detectionalgorithmThe results of these changes are reflectedin new time-to-detection and delayed accu-racy scores.
Again we calculated the time-to-detection, and found this to be 7.5 fromthe start of reparandum and 4.6 from thestart of repair.
Table 2 shows the results un-der the new early completion model using thedelayed accuracy method.
We see that theupdated model has lower time-to-detectionscores (close to a full token earlier); for de-layed accuracy, we note that the scores sta-bilise in a similar fashion, but the scores forthe updated model rise slightly more quickly.13776 Conclusions and Future WorkWe have demonstrated an incremental modelfor finding speech disfluencies in spoken lan-guage transcripts.
When we consider com-plete utterances, the incremental model pro-vides identical results to those of a non-incremental model that delivers state-of-the-art accuracy in speech repair detection.
Wehave investigated a number of measures whichallow us to evaluate the model on an incremen-tal level.
Most disfluencies are identified veryquickly, typically one or two tokens after thedisfluency has been completed.
We addressedthe problems of the model around the end ofprefix boundaries.
These are repairs which areeither still in the process of being uttered orhave just been completed.
We have addressedthis issue by making some changes to how themodel deals with prefix boundaries, and wehave shown that this improves the responsive-ness of the model.The work reported in this paper uses a n-gram model as a language model and a STAGbased model for the repair.
We would liketo replace the n-gram language model with abetter language model.
Previous work (John-son and Charniak, 2004) has shown that dis-fluency detection can be improved by replac-ing the n-gram language model with a statis-tical parser.
Besides a reported 5% accuracyimprovement, this also provides a structuralanalysis, something which an n-gram modeldoes not.
We would like to investigate a sim-ilar extension in our incremental approach,which will require the integration of an in-cremental statistical parser with our noisychannel model.
While transcripts of spokentexts come with manually annotated sentenceboundaries, real time spoken language doesnot.
The language model in particular takesthese sentence boundaries into account.
Wetherefore propose to investigate the proper-ties of this model when sentence boundariesare removed.AcknowledgementsThis work was supported by the AustralianResearch Council as part of the ThinkingHead Project, ARC/NHMRC Special Re-search Initiative Grant # TS0669874.
Wethank the anonymous reviewers for their help-ful comments.ReferencesCharniak, Eugene.
2001.
Immediate-head pars-ing for language models.
In Proceedings of the39th Annual Meeting on Association for Com-putational Linguistics, pages 124?131.Core, Mark and Lenhart Schubert.
1999.
A modelof speech repairs and other disruptions.
InAAAI Fall Symposium on Psychological Mod-els of Communication in Collaborative Systems,pages 48?53.Honal, Matthias and Tanja Schultz.
2003.
Correc-tion of Disfluencies in Spontaneous Speech us-ing a Noisy-Channel Approach.
In Proceedingsof the 8th Eurospeech Conference.Johnson, Mark and Eugene Charniak.
2004.
Atag-based noisy channel model of speech repairs.In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics,pages 33?39.Marslen-Wilson, W. 1973.
Linguistic structureand speech shadowing at very short latencies.Nature, 244:522?533.Schuler, William, Samir AbdelRahman, TimMiller, and Lane Schwartz.
2010.
Broad-Coverage Parsing using Human-Like Mem-ory Constraints.
Computational Linguistics,36(1):1?30.Shieber, Stuart M. and Yves Schabes.
1990.
Syn-chronous tree-adjoining grammars.
In Proceed-ings of the 13th International Conference onComputational Linguistics, pages 253?258.Shriberg, Elizabeth.
1994.
Preliminaries to aTheory of Speech Disuencies.
Ph.D. thesis, Uni-versity of California, Berkeley.1378
