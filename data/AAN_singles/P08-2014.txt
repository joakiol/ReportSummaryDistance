Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 53?56,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsActive Sample Selection for Named Entity TransliterationDan Goldwasser Dan RothDepartment of Computer ScienceUniversity of IllinoisUrbana, IL 61801{goldwas1,danr}@uiuc.eduAbstractThis paper introduces a new method foridentifying named-entity (NE) transliterationswithin bilingual corpora.
Current state-of-the-art approaches usually require annotated dataand relevant linguistic knowledge which maynot be available for all languages.
We showhow to effectively train an accurate transliter-ation classifier using very little data, obtainedautomatically.
To perform this task, we intro-duce a new active sampling paradigm for guid-ing and adapting the sample selection process.We also investigate how to improve the clas-sifier by identifying repeated patterns in thetraining data.
We evaluated our approach us-ing English, Russian and Hebrew corpora.1 IntroductionThis paper presents a new approach for constructinga discriminative transliteration model.Our approach is fully automated and requires littleknowledge of the source and target languages.Named entity (NE) transliteration is the process oftranscribing a NE from a source language to a targetlanguage based on phonetic similarity between theentities.
Figure 1 provides examples of NE translit-erations in English Russian and Hebrew.Identifying transliteration pairs is an importantcomponent in many linguistic applications such asmachine translation and information retrieval, whichrequire identifying out-of-vocabulary words.In our settings, we have access to source languageNE and the ability to label the data upon request.We introduce a new active sampling paradigm thatFigure 1: NE in English, Russian and Hebrew.aims to guide the learner toward informative sam-ples, allowing learning from a small number of rep-resentative examples.
After the data is obtained it isanalyzed to identify repeating patterns which can beused to focus the training process of the model.Previous works usually take a generative approach,(Knight and Graehl, 1997).
Other approaches ex-ploit similarities in aligned bilingual corpora; for ex-ample, (Tao et al, 2006) combine two unsupervisedmethods.
(Klementiev and Roth, 2006) bootstrapwith a classifier used interchangeably with an un-supervised temporal alignment method.
Althoughthese approaches alleviate the problem of obtain-ing annotated data, other resources are still required,such as a large aligned bilingual corpus.The idea of selectively sampling training sampleshas been wildly discussed in machine learning the-ory (Seung et al, 1992) and has been applied suc-cessfully to several NLP applications (McCallumand Nigam, 1998).
Unlike other approaches,our ap-proach is based on minimizing the distance betweenthe feature distribution of a comprehensive referenceset and the sampled set.2 Training a Transliteration ModelOur framework works in several stages, as summa-rized in Algorithm 1.
First, a training set consisting53of NE transliteration pairs (ws, wt) is automaticallygenerated using an active sample selection scheme.The sample selection process is guided by the Suf-ficient Spanning Features criterion (SSF) introducedin section 2.2, to identify informative samples in thesource language.An oracle capable of pairing a NEin the source language with its counterpart in the tar-get language is then used.
Negative training samplesare generated by reshuffling the terms in these pairs.Once the training data has been collected, the datais analyzed to identify repeating patterns in the datawhich are used to focus the training process by as-signing weights to features corresponding to the ob-served patterns.
Finally, a linear model is trained us-ing a variation of the averaged perceptron (Freundand Schapire, 1998) algorithm.
The remainder ofthis section provides details about these stages; thebasic formulation of the transliteration model andthe feature extraction scheme is described in section2.1, in section 2.2 the selective sampling process isdescribed and finally section 2.3 explains how learn-ing is focused by using feature weights.Input: Bilingual, comparable corpus (S , T ), set ofnamed entities NES from S, ReferenceCorpus RS , Transliteration Oracle O,Training Corpora D=DS ,DTOutput: Transliteration model MGuiding the Sampling Process1repeat2select a set C ?
NES randomly3ws = argminw?Cdistance(R,DS ?
{ws})4D = D ?
{Ws, O(Ws)}5until distance(R,DS ?
{Ws}) ?
distance(R,DS) ;6Determining Features Activation Strength7Define W:f ?
< s.t.
foreach feature f ={fs, ft}8W (f) = ](fs,ft)](fs) ?
](fs,ft)](ft)9Use D to train M;10Algorithm 1: Constructing a transliterationmodel.2.1 Transliteration ModelOur transliteration model takes a discriminative ap-proach; the classifier is presented with a word pair(ws, wt) , where ws is a named entity and it isasked to determine whether wt is a transliterationFigure 2: Features extraction processof the NE in the target language.
We use a linearclassifier trained with a regularized perceptron up-date rule (Grove and Roth, 2001) as implementedin SNoW, (Roth, 1998).
The classifier?s confi-dence score is used for ranking of positively taggedtransliteration candidates.
Our initial feature extrac-tion scheme follows the one presented in (Klemen-tiev and Roth, 2006), in which the feature space con-sists of n-gram pairs from the two languages.
Givena sample, each word is decomposed into a set of sub-strings of up to a given length (including the emptystring).
Features are generated by pairing substringsfrom the two sets whose relative positions in theoriginal words differ by one or less places; first eachword is decomposed into a set of substrings thensubstrings from the two sets are coupled to completethe pair representation.
Figure 2 depicts this process.2.2 Guiding the Sampling Process with SSFThe initial step in our framework is to generate atraining set of transliteration pairs; this is done bypairing highly informative source language candi-date NEs with target language counterparts.
We de-veloped a criterion for adding new samples, Suffi-ciently Spanning Features (SSF), which quantifiesthe sampled set ability to span the feature space.This is done by evaluating the L-1 distance be-tween the frequency distributions of source languageword fragments in the current sampled set and ina comprehensive set of source language NEs, serv-ing as reference.
We argue that since the featuresused for learning are n-gram features, once thesetwo distributions are close enough, our examplesspace provides a good and concise characterizationof all named entities we will ever need to con-sider.
A special care should be given to choos-ing an appropriate reference; as a general guide-line the reference set should be representative ofthe testing data.
We collected a set R, consisting54of 50,000 NE by crawling through Wikipedia?s arti-cles and using an English NER system available at- http://L2R.cs.uiuc.edu/ cogcomp.
The frequencydistribution was generated over all character levelbi-grams appearing in the text, as bi-grams best cor-relate with the way features are extracted.
Given areference text R, the n-grams distribution of R can bedefined as follows -DR(ngi) = ]ngi?j ]ngj,where ngis an n-gram in R. Given a sample set S, we measurethe L1 distance between the distributions:distance (R,S) =?ng?R | DR(ng)?DS(ng) | Sam-ples decreasing the distance between the distribu-tions were added to the training data.
Given a setC of candidates for annotation, a sample ws ?
Cwas added to the training set, if -ws = argminw?Cdistance(R,DS ?
{ws}).A sample set is said to have SSF, if the distance re-mains constant as more samples are added.2.2.1 Transliteration Oracle ImplementationThe transliteration oracle is essentially a mappingbetween the named entities, i.e.
given an NE in thesource language it provides the matching NE in thetarget language.
An automatic oracle was imple-mented by crawling through Wikipedia topic aligneddocument pairs.
Given a pair of topic aligned doc-uments in the two languages, the topic can be iden-tified either by identifying the top ranking terms orby simply identifying the title of the documents.
Bychoosing documents in Wikipedia?s biography cate-gory we ensured that the topic of the documents isperson NE.2.3 Training the transliteration modelThe feature extraction scheme we use generates fea-tures by coupling substrings from the two terms.Ideally, given a positive sample, it is desirable thatpaired substrings would encode phonetically simi-lar or a distinctive context in which the two scriptscorrelate.
Given enough positive samples, such fea-tures will appear with distinctive frequency.
Tak-ing this idea further, these features were recognizedby measuring the co-occurrence frequency of sub-strings of up to two characters in both languages.Each feature f=(fs, ft) composed of two substringstaken from English and Hebrew words was associ-ated with weight.
W (f) = ](fs,ft)](fs) ?
](fs,ft)](ft) whereData Set Method Rus Heb1 SSF 0.68 NA1 KR?06 0.63 NA2 SSF 0.71 0.52Table 1: Results summary.
The numbers are the pro-portion of NE recognized in the target language.
Lines 1and 2 compare the results of SSF directed approach withthe baseline system on the first dataset.
Line 3 summa-rizes the results on the second dataset.
](fs, ft) is the number of occurrences of that featurein the positive sample set, and ](fL) is the number ofoccurrences of an individual substring, in any of thefeatures extracted from positive samples in the train-ing set.
The result of this process is a weight table,in which, as we empirically tested, the highest rank-ing weights were assigned to features that preservethe phonetic correlation between the two languages.To improve the classifier?s learning rate, the learn-ing process is focused around these features.
Givena sample, the learner is presented with a real-valuedfeature vector instead of a binary vector, in whicheach value indicates both that the feature is activeand its activation strength - i.e.
the weight assignedto it.3 EvaluationWe evaluated our approach in two settings; first, wecompared our system to a baseline system describedin (Klementiev and Roth, 2006).
Given a bilingualcorpus with the English NE annotated, the systemhad to discover the NE in target language text.
Weused the English-Russian news corpus used in thebaseline system.
NEs were grouped into equiva-lence classes, each containing different variations ofthe same NE.
We randomly sampled 500 documentsfrom the corpus.
Transliteration pairs were mappedinto 97 equivalence classes, identified by an expert.In a second experiment, different learning parame-ters such as selective sampling efficiency and featureweights were checked.
300 English-Russian andEnglish-Hebrew NE pairs were used; negative sam-ples were generated by coupling every English NEwith all other target language NEs.
Table 1 presentsthe key results of these experiments and comparedwith the baseline system.55Extraction Number Recall Recallmethod of Top one Top twosamplesDirected 200 0.68 0.74Random 200 0.57 0.65Random 400 0.63 0.71Table 2: Comparison of correctly identified English-Russian transliteration pairs in news corpus.
The modeltrained using selective sampling outperforms modelstrained using random sampling, even when trained withtwice the data.
The top one and top two resultscolumns describe the proportion of correctly identifiedpairs ranked in the first and top two places, respectively.3.1 Using SSF directed samplingTable 2 describes the effect of directed samplingin the English-Russian news corpora NE discoverytask.
Results show that models trained using selec-tive sampling can outperform models trained withmore than twice the amount of data.3.2 Training using feature weightsTable 3 describes the effect training the model withweights.The training set consisted of 150 samplesextracted using SSF directed sampling.
Three varia-tions were tested - training without feature weights,using the feature weights as the initial networkweights without training and training with weights.The results clearly show that using weights for train-ing improve the classifier?s performance for bothRussian and Hebrew.
It can also be observed thatin many cases the correct pair was ranked in any ofthe top five places.4 Conclusions and future workIn this paper we presented a new approach for con-structing a transliteration model automatically andefficiently by selectively extracting transliterationsamples covering relevant parts of the feature spaceand focusing the learning process on these features.We show that our approach can outperform sys-tems requiring supervision, manual intervention anda considerable amount of data.
We propose a newmeasure for selective sample selection which can beused independently.
We currently investigate apply-ing it in other domains with potentially larger featureLearning Russian HebrewTrain- Feature Top Top Top Toping weights one five one five+ + 0.71 0.89 0.52 0.88- + 0.63 0.82 0.33 0.59+ - 0.64 0.79 0.37 0.68Table 3: The proportion of correctly identified transliter-ation pairs with/out using weights and training.
The topone and top five results columns describe the proportionof correctly identified pairs ranked in the first place andin any of the top five places, respectively.
The resultsdemonstrate that using feature weights improves perfor-mance for both target languages.space than used in this work.
Another aspect inves-tigated is using our selective sampling for adaptingthe learning process for data originating from dif-ferent sources; using the a reference set representa-tive of the testing data, training samples, originatingfrom a different source , can be biased towards thetesting data.5 AcknowledgmentsPartly supported by NSF grant ITR IIS-0428472 andDARPA funding under the Bootstrap Learning Pro-gram.ReferencesY.
Freund and R. E. Schapire.
1998.
Large margin clas-sification using the perceptron algorithm.
In COLT.A.
Grove and D. Roth.
2001.
Linear concepts and hiddenvariables.
ML, 42.A.
Klementiev and D. Roth.
2006.
Weakly supervisednamed entity transliteration and discovery from multi-lingual comparable corpora.
In ACL.K.
Knight and J. Graehl.
1997.
Machine transliteration.In EACL.D.
K. McCallum and K. Nigam.
1998.
Employing EMin pool-based active learning for text classification.
InICML.D.
Roth.
1998.
Learning to resolve natural language am-biguities: A unified approach.
In AAAI.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In COLT.T.
Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai.
2006.Unsupervised named entity transliteration using tem-poral and phonetic correlation.
In EMNLP.56
