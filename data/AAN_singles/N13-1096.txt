Proceedings of NAACL-HLT 2013, pages 783?788,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsMeasuring the Structural Importance through Rhetorical Structure IndexNarine Kokhlikyan?, Alex Waibel?
?, Yuqi Zhang?, Joy Ying Zhang?
?Karlsruhe Institute of TechnologyAdenauerring 276131 Karlsruhe, Germany?
Carnegie Mellon UniversityNASA Research Park, Bldg.
23Moffett Field, CA 94035narine.kokhlikyan@student.kit.edu, waibel@cs.cmu.edu,yuqi.zhang@kit.edu, joy.zhang@sv.cmu.eduAbstractIn this paper, we propose a novel RhetoricalStructure Index (RSI) to measure the struc-tural importance of a word or a phrase.
Un-like TF-IDF and other content-driven mea-surements, RSI identifies words or phrasesthat are structural cues in an unstructured doc-ument.
We show structurally motivated fea-tures with high RSI values are more usefulthan content-driven features for applicationssuch as segmenting unstructured lecture tran-scripts into meaningful segments.
Experi-ments show that using RSI significantly im-proves the segmentation accuracy comparedto TF-IDF, a traditional content-based featureweighting scheme.1 IntroductionOnline learning, a new trend in distance learning,provides numerous lectures to students all over theworld.
More than 19,000 colleges offer thousandsof free online lectures1.
Starting from video record-ings of lectures which sometimes also come with thepresentation material, a set of processes can be ap-plied to extract information from the unstructureddata to assist students in browsing, searching andunderstanding the content of the lecture.
These pro-cesses include automatic speech recognition (ASR)which converts the audio to text, lecture segmen-tation which inserts paragraph boundaries and addssection titles to the lecture transcriptions, automaticsummarization that generates a short summary from1http://www.thebestcolleges.org/free-online-classes-and-course-lectures/the full lecture, and lecture translation that translatesthe lecture from the original language to the nativelanguage of the student.The transcription of a lecture generated by theASR system is a sequence of words which doesnot contain any structural information such as para-graph, section boundaries and section titles.
Zhanget al(2007; 2008; 2010) used acoustic and lin-guistic features for rhetorical structure detection andsummarization.
They showed that linguistic featuressuch as TF-IDF are the most influential in segmenta-tion and summarization and that knowing the struc-ture of a lecture can significantly improve the perfor-mance of lecture summarization.
Our experimentswith a real-time lecture translation system also showthat displaying the rolling translation results of a livelecture with proper paragraphing and inserted sec-tion titles makes it easier for students to grasp thekey points during a lecture.In this paper, we apply existing algorithms,namely the Hidden Markov Model (HMM) (Galesand Young, 2007) to unstructured lecture transcrip-tion to infer the underlying structure for better lec-ture segmentation and summarization.
HMM hasbeen successfully applied in early works (van Mul-bregt et al 1998; Sherman and Liu, 2008) for textsegmentation, event tracking and boundary detec-tion.
The focus of this work is to identify cuewords and phrases that are good indicators of lec-ture structure.
Intuitively, words and phrases suchas ?last week we talked about?, ?this is an out-line of my talk?, ?now I am going to talk about?,?in conclusion?, and ?any questions?
should beimportant features to recognize lecture structure.783These words/phrases, however, may not be so im-portant content-wise.
Thus, content-driven met-rics such as the TF-IDF score usually do not as-sign higher weights to these structurally impor-tant words/phrases.
We propose a novel metriccalled Rhetorical Structural Index (RSI) to weighwords/phrases based on their structural importance.2 Rhetorical Structural IndexRSI incorporates both frequency of occurrences and,more importantly, the position distribution of occur-rences of a word/phrase.
The intuition is that if aterm is a structural marker, it usually occurs at a cer-tain position in a lecture.
Because the term is mainlyabout the structure rather than the content of a lec-ture, it can appear with high frequency in lecturesthat are of different topics.
For example, ?today we?occurs at the beginning of a lecture and ?thank you?usually appears towards the end (Figure 1) no mat-ter whether the lecture is about history or computerscience.We define the RSI of a word w as:RSI(w) =1?Var(Lw) + (1?
?
)idf(w,D)(1)where Lw is the random variable of ?normalized po-sitions?
of a word w in a lecture.
For each occur-rence of w in a particular lecture d, we divide itsposition by the length of the lecture |d| to estimateits ?normalized position?.
Lw takes a value between[0, 1].
A value close to 0 indicates this word occursat the beginning and close to 1 means w is close tothe end of the lecture.
Var(Lw) is the variance of thenormalized position of a word w. A small Var(Lw)indicates that w always occurs at certain positionsof a typical lecture (e.g., ?bye?)
while a large valuemeans w can occur at any position (e.g., functionwords ?of?
and ?the?
).The second part of RSI is the inverse documentfrequency (idf), or effectively the document fre-quency since RSI is proportional to the 1/idf term.Lectures, such as different research talks, can varyin content but usually have a very similar structureand share some common structural cues.
A goodstructural cue word should be common to many lec-tures.
idf has been widely used in information re-trieval research to assign higher weights to wordsthat occur in just a few documents as compared toTable 1: Examples of n-grams with high RSI valueswhich are likely to be structural cues.n-gram Var(Lw) idf RSInow 0.0004 0.60 1.04here 0.0004 0.62 1.03class 0.0001 2.12 0.90week 0.0001 2.23 0.89goodbye 0.0001 3.62 0.80thank you 0.0003 1.53 0.95talk about 0.0003 1.90 0.92dealing with 0.0002 2.00 0.91today we 0.0003 2.51 0.87see how 0.0009 2.69 0.85ladies and gentlemen 0.0008 1.35 0.96last time we 0.0004 2.22 0.89here we have 0.0005 2.35 0.88next time we 0.0002 2.51 0.86common words that occur in all documents.
Definethe idf of a word w given a collection of lectures Das:idf(w,D) = log|D||{d ?
D|w ?
d}|(2)|D| is the number of all lectures in the collection and|{d ?
D|w ?
d}| is the number of lectures where wappears.
A low idf (w,D) value indicates that wordw occurs in many documents and thus is more likelyto be a common structural cue.Combining the variance of normalized positionand idf by scaling factor ?, we define RSI as in equa-tion 1.
We found 0.9 as an optimal value of ?
accord-ing our experiments over all data sets.
A word wwith high RSI value is more likely to be structurallyimportant.
Similarly, we can calculate the RSI val-ues for phrases (n-grams) such as ?I would like totalk about?, ?I will switch gear to?
and ?thank youfor your attention?.Table 1 shows examples of n-grams and the cal-culated variance, idf-scores and RSI values from acollection of lectures.3 Incorporating RSI in LectureSegmentationSeveral algorithms have been developed for text seg-mentation including the Naive Bayes classifier forkeyword extraction (Balagopalan et al 2012), the784Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams ?today we?
and ?thank you?.
?today we?
appears more frequently at the beginning of a lecture, whereas ?thank you?
more in the concluding partof a lecture.
The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word ata position.Hidden Markov Model (Gales and Young, 2007),the Maximum entropy Markov model (McCallum etal., 2000), the Conditional Random Field (Laffertyet al 2001) and the Latent Content Analysis (Ponteand Croft, 1997).
In this paper, we evaluate the ef-fectiveness of the proposed RSI feature on lecturesegmentation using an HMM.We represent each segment in a lecture as a statein the Markov model and use the EM algorithmto learn HMM parameters from unlabeled lecturedata.
We use a fully connected HMM with fivestates.
Typical state labels for lecture are: ?Introduc-tion?, ?Background?, ?Main Topic?, ?Questions?and ?Conclusion?
as shown in Figure 2.
HMM statesemit word tokens.
Instead of considering the fullvocabulary as the possible emission alphabet, whichusually leads to model over-fitting, we only considerterms with high RSI values and high TF-IDF* scoresfor comparison.
For a word w, define its TF-IDF*score as:TF-IDF*(w) = maxdTF-IDF(w, d), (3)which is the highest TF-IDF score of a word in anydocument in the collection.
Our experiments try toanswer the question that ?if HMM is meant to cap-ture the underlying structure of lectures no matterwhich topic the lecture is about, what kind of fea-tures should be emitted from each state to reflectsuch structural patterns among lectures?
?The learned HMM model is then applied to un-seen lecture data to label each sentence to be ?In-troduction?, ?Background?, ?Main Topic?, ?Ques-tions?
or ?Conclusion?
and, based on the label, wesegment the lecture to different sections for evalu-ation.
Segment boundaries are defined in the posi-tions where sentence labels change.3.0.1 Bootstrap HMM from K-MeansClustering SegmentationInitial HMM parameters are bootstrapped usingresults from K-means clustering where we cluster asequence of sentences to form a ?segment?.
K cor-responds to the number of desired segments of a lec-ture.
Similarities are computed based on the contentsimilarity (using n-gram matches) and the relative785sentence position defined as:Sim(Si, Cj) = ?M(Si, Cj) + (1?
?
)P (Si, Cj),(4)where Si is the i-th sentence, Cj is the centroid ofthe j-th cluster.
M(Si, Cj) is the content similaritybetween sentence Si and centroid Cj and P(Si, Cj)is the position similarity (distance).
?
is a scalingfactor (set to optimal value 0.2 based on all data setsin our experiments).Content similarity is based on the number of com-mon words between two sentences, or between asentence and the centroid vector of a cluster.
Denotethe binary word frequency vector (bag of words) insentence Si as ~Si and similarly ~Cj for cluster cen-troid Cj , define:M(Si, Cj) =~Si?
~Cj?
~Si ??
~Cj ?.
(5)P(Si, Cj) measures the position similarity of twosentences.
Position similarity is based on the rela-tive position distance between the sentence and thecluster:DefineP (Si, Cj) =L|Pos(Si)?
Pos(Cj)|+ , (6)where Si is the i-th sentence, Cj is the j-th cluster.Pos(Si) is the position of sentence Si.
Pos(Cj) isthe average sentence position of all members belongto cluster Cj and L is the total number of sentencesin a lecture.
 is a small constant to avoid divisionby zeros.4 Experiments and EvaluationWe evaluated segmentation on three different datasets: college lectures recorded by Karlsruhe In-stitute of Technology (KIT), Microsoft research(MSR) lectures2 and scientific papers3.
Both col-lege and Microsoft research lectures are manuallytranscribed.
The reason why we do not include ex-periments on ASR output is that current ASR qualityof lecture data is still quite poor.
Word-Error-Rates(WER) of ASR output range from 24.37 to 30.80 forKIT lectures.
Roughly speaking, every one out of 3or 4 words is mis-recognized.2http://research.microsoft.com/apps/catalog/3http://aclweb.org/anthology-new/For evaluation, human annotators annotated a fewlectures to create test/reference sets.
The test datafrom KIT is annotated by one human annotator andMSR lectures are annotated by four annotators.
Thesegmentation gold standard is created based on theagreed annotations.
Since the number of annotatedlectures is small and human annotation is subjective,we also used ACL papers as an additional data set.ACL papers are in a way ?lectures in written form?and have titles for section and subsections which canbe used to identify the segments and annotate thedata set automatically.
The statistics of each data setare listed in Table 2.Table 2: Statistics of three data sets used in the exper-iments: our own lecture data (KIT), Microsoft researchtalks (MSR) and conference proceedings from ACL an-thology archive.
We removed equations, short titles suchas ?Abstract?
and ?Conclusion?, when extracting textfrom PDF files from the ACL anthology, which resultsin a relatively small number of words per paper.
Wordsare simply tokenized without case normalization or stem-ming, which results in relatively large vocabulary sizes.Properties KIT MSR ACLNum.
74 1,182 3,583Avg.
Num.
of Sent.
484 655 212Avg.
Num.
of Words 10,078 10,225 3,896Avg.
Duration (Min.)
43.57 39.15 -Vocabulary Size 1.3K 22K 24KFirst, we calculate the RSI and TF-IDF* scoresfor each word in the dataset and choose the top Nwords as the HMM emission vocabulary.
To avoidover-fitting, we choose N that is much smaller thanthe full vocabulary size of the data set.
In our ex-periments, we set N=300 for KIT, N=5000 for MSRand N=5400 for ACL.
The top 5 words with thehighest TF-IDF* scores from the MSR data set are:?RFID?, ?Cherokee?, ?tree-to-string?, ?GPU?, and?data-triggered?, whereas the top 5 words selectedby RSI are ?today?, ?work?, ?question?, ?now?, and?thank?, which are more structurally informative.To estimate the accuracy of the segmentationmodule, we used Recall, Precision, F-Measure andPk (Beeferman et al 1999) as evaluation metric.We used an error window of length 6 to calculatePrecision, Recall and F-Measure and a sliding win-dow with a length equal to half of the average seg-786Introduction Main Topic Conclusion Background QuestionsIntroductionword tokensBackgroundword tokensContentword tokensQuestionsword tokensConclusionword tokensFigure 2: Fully connected 5-state HMM representing Introduction, Background, Main Topic, Questions, Conclusionin a typical lecture.ment length to estimate the Pk score.
With errorwindow we mean that hypothesis boundaries do nothave to be exactly the same as the reference segmentboundaries.
Hypothesis boundaries are acceptableif they are close enough to reference boundaries inthat window.
The Pk score indicates the probabilityof segmentation inconsistency.
Therefore, the lowerthe Pk score the better the segmentation is.Table 3: Segmentation results measured by Pk (thesmaller the better), Precision, Recall and F-Measurescores (the higher the better) for three data sets compar-ing HMM using TF-IDF*-filtered word tokens as emis-sion and RSI-filtered words as emissions.Evaluation Score KIT MSR ACLPkHMM + TF-IDF* 0.06 0.06 0.05HMM + RSI 0.01 0.02 0.01PrecisionHMM + TF-IDF* 32.01 30.47 32.85HMM + RSI 41.10 41.01 42.70RecallHMM + TF-IDF* 39.32 36.09 38.08HMM + RSI 47.38 46.39 48.95F-MeasureHMM + TF-IDF* 35.29 33.04 35.27HMM + RSI 44.01 43.53 45.61The evaluation results on all data sets listed inTable 3 show that according F-Measure and Pkscores, considering words with high RSI values asHMM emission significantly improve over the base-line method of choosing word tokens with high TF-IDF* scores.5 ConclusionsIn this work we propose the Rhetorical Structure In-dex (RSI), a method to identify structurally impor-tant terms in lectures.
Experiments show that termswith high RSI values are better candidates than thosewith high TF-IDF values when used by an HMM-based segmenter as state emissions.
In other words,terms with high RSI values are more likely to bestructural cues in lectures independent of the lecturetopic.
In the future we will run experiments on ASRoutput and incorporate other prosodic features suchas pitch, intensity, duration into the RSI to improvethis metric for structural analysis of lectures and ap-ply the RSI to other structure discovery applicationssuch as dialogue segmentation.AcknowledgmentsThe authors gratefully acknowledge the support byan interACT student exchange scholarship.
The re-search leading to these results has received fundingfrom the European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreement no287658.We would like to thank Jan Niehues and TeresaHerrmann for their suggestions and help.787ReferencesA.
Balagopalan, L.L.
Balasubramanian, V. Balasubrama-nian, N. Chandrasekharan, and A. Damodar.
2012.Automatic keyphrase extraction and segmentation ofvideo lectures.
In Technology Enhanced Education(ICTEE), 2012 IEEE International Conference on,pages 1?10.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Mach.Learn., 34(1-3):177?210, February.Mark J. F. Gales and Steve J.
Young.
2007.
The ap-plication of hidden markov models in speech recog-nition.
Foundations and Trends in Signal Processing,1(3):195?304.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.Andrew McCallum, Dayne Freitag, and Fernando C. N.Pereira.
2000.
Maximum entropy markov models forinformation extraction and segmentation.
In Proceed-ings of the Seventeenth International Conference onMachine Learning, ICML ?00, pages 591?598, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Jay M. Ponte and W. Bruce Croft.
1997.
Text segmenta-tion by topic.
In ECDL, pages 113?125.Melissa Sherman and Yang Liu.
2008.
Using hid-den markov models for topic segmentation of meetingtranscripts.
In SLT, pages 185?188.Paul van Mulbregt, Ira Carp, Lawrence Gillick, SteveLowe, and Jon Yamron.
1998.
Text segmentation andtopic tracking on broadcast news via a hidden markovmodel approach.
In ICSLP.Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.2007.
Improving lecture speech summarization usingrhetorical information.
In ASRU, pages 195?200.Justin Jian Zhang, Shilei Huang, and Pascale Fung.
2008.Rshmm++ for extractive lecture speech summariza-tion.
In SLT, pages 161?164.Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.2010.
Extractive speech summarization using shallowrhetorical structure modeling.
IEEE Transactions onAudio, Speech & Language Processing, 18(6):1147?1157.788
