IActive and Passive Gestures - Problems with the Resolut ion ofDeict ic and Elliptic Expressions in a Mult imodal  SystemMichael StreitSiemens AG c/o German Research Center for Artificial IntelligenceStuhlsatzenhausweg 366123 Saarbrfickenstreit@dfki, uni-sb, demAbst rac tThis paper deals with aspects of the resolu-tion of deictic and elliptic expressions thatare related to gestures.
It discusses differ-ent approaches todistinguish between deic-tic pointing and manipulative gestures.
Wecompare two strategies of combining nat-ural multimodal communication with di-rect manipulation.
The first approach usesclick free mouse gestures for deictic point-ing, while manipulative gestures are per-formed by using mouse button events as isusual in graphic interfaces.
The second ap-proach uses a touchscreen asgestural inputdevice.1 In t roduct ionThis paper deals with aspects of the resolution ofdeictic and elliptic expressions that are related togestures.
It discusses two approaches to distinguishbetween deictic pointing and manipulative gestures.The resolution methods for both approaches havebeen implemented in a project for the developmentof a multimodal interface for route-planning, traf-fic information and driver guidance MOFA (Ger-man acronym for "Multimodale Fahrerinformation"-"Multimodal Driver Information").
1 We provedthe reusability of the methods and the architectureof MOFA in a completely different domain by imple-menting a prototype for multimodal calendar man-agement (the system TALKY).The input modalities supported by our system arespoken and written natural language, deictic point-ing gestures and the interaction methods knownfrom direct manipulation systems.
As input devicesfor gestures we use either a mouse or touch screen1For a description of MOFA see \[Streit 96\].technology.
The latter allows the user to performdeictical pointing in an (almost) natural way.Our approach to multimodal systems aims at asmooth integration of a conversational communica-tion style with the features of direct manipulation.By conversational style of multimodal communica-tion we understand the use of natural speech sup-ported by deictical gestures, as in the combinationof the utterance "an dem Tag gesch~iftlich in Kalser-slautern", ("at this day in Kalserslautern for busi-ness") with a pointing act to a graphical presenta-tion of the day under consideration.
In this examplethe gesture acts only passively, supplying the spo-ken utterance with additional information.
In con-trast gestures used in direct manipulation are active,they are expected to trigger some action by the sys-tem, e.g.
clicking at an icon which represents "1stof May" opens a representation f this day.
Verbalutterances that accompany or follow such gesturesmay be related to this gesture:?
If in this context he user utters "um 12 Uhrin Raum 17" ("at 12 o'clock in room 17") it ismost likely that he specifies an appointment athe 1st of May.
In this case the elliptic verbalutterance can be resolved by the manipulativegesture.?
But if the user utters "at 2nd of May meetingwith Mr. X" he performs an unrelated tasks.2 Overview of the SystemArchitecture of MOFAIn our architecture the MOFA interface is organizedinto four main components hat are realized as in-dependent processes that communicate via sockets:?
The Graphical Interface (GI) for gestural inputand graphical output,?
the Speech Recognizer (SR),Active and Passive Gestures 45?
the multimodal interpretation, dialogue andpresentation planning process (MIDP)?
and the speech synthesis component (SYN).The back end application (AP) may be integratedin the MIDP or realized as separate process (in caseof MOFA, the AP is the route planner).
This or-ganization allows for parallel input and output andalso an overlapp of user input, interpretation andpresentation.The GI sends both gestural events like pointingto an object or selecting a menu item, and writteninput to the MIDP.
It also realizes graphical presen-tations, which are planned by the MIDP.
The speechrecognizer is activated by the user via a speech-button which can be operated from the GI.
We dothis to avoid problems with recognition results thatare not intended by the user (see \[Gauvain 96\]).Both the results of the speech recognition and theinput processed by the GI are augmented with timeinformation.The MIDP consists of the following components:1.
The input controller checks if modalities areused in parallel or subsequently and determinestemporal relations between events.2.
The interpretation module is responsible for ro-bust parsing and plan recognition.
Parsing andplan recognition work in close interaction andproduce a task oriented representation.
The in-terpretation depends on the dialogue state asspecified by the clarification or the continua-tion module.
The interpretation module alsohandles gestures that the input controler recog-nizes as monomodal input.3.
Deictic and elliptic expressions are resolved inthe resolution module.4.
A plan library and a simple domain model sup-port plan recognition and resolution.5.
The clarification module checks in a case basedmanner the partly instantiated task hypotheses.It first tries silently to repair certain inconsis-tencies and then starts a clarification dialogueor calls the back end application.
The clarifica-tion module has an interface to the user modelwhich proposes fillers for parameters which arenot specified by the user.6.
The continuation module decides if the systemkeeps the initiative or if it simply waits forfurther user input.
The modules (5) and (6)also control speech recognition.
Depending onthe dialog state they select he appropriate lan-guage model.7.
The presentation module plans the presentationof the result of an application call.
It controlsthe output of SYN and GI.8.
The user model follows a memory based learn-ing approach.
The user model is an optionalmodule, since it can be left out from the systemwithout any changes to modules.1(7) Presentation I=%@ (2) InterpretationP,a?
Libra li(5)Clarification .~_~~_6)C~o n fin ~ ~ O d D  ?
r~i n ~xxx~ (3) ResolutionUserm?dell \[ NNNN~@Figure 1: Architecture3 Two implemented Applications:Sample Interactions with MOFAand TALKYIn MOFA, the interaction style is mainly conversa-tional, menus are only used for commands on themeta level (e.g starting a trip simualtion along aplanned route, or introducing some external events46 M. Streitinto the trip simulation (e.g.
invoking a traffic jam).TALKY is much more a mixture between manipu-lative interaction and multimodal conversation.
Ifthe system is running on a SUN Ultra Sparc, speechrecognition is real time, if the user speaks slowly.In the following we give three examples which areprocessed by our systems MOFA and TALKY.
Inthese examples, natural language is spoken, unlessotherwise stated.
System reactions are given in spo-ken and written form.
In our sample dialogs ,/a,indicates a pointing gesture, '/~(Y)' means that thegesture is unambiguously pointing to some object,where Y refers to an object on the screen.Figure 2: MOFAExample 1: Route Planning?
USER: "Von hier" ("From here") /~(A)?
MOFA: "Sie wollen nach A. Bitte zeigen Sie mirIhr Ziel."
("You want to start in A.
Please showme your destination.")?
MOFA: highlights A.?
USER: "Karte nach Giesing" ("move map togiesing")?
MOFA: moves map.?
USER: " Zeige mir diesen Platz" ("show me thisp lace") /~ (X)?
MOFA: shows a picture?
USER: okay?
MOFA: removes the picture?
USER: "Dorthin" ("to this location") ,/a.
.
"Uber diese Kreuzungen" ("via these cross-ings")/~(B),  2~(C)?
MOFA: "Wollen Sie nach Steinhausen oderzum Prinzregentenplatz oder zum Boehmer-waldplatz ?"
(Is your destination Steinhausenor Prinzregentenplatz or Boehmerwaldplatz ?")?
USER: "Boehmerwaldplatz" (Boehmerwald-platz is a tube-station)?
MOFA: "Von A fiber B nach C habe ich diegelbe Route gefunden.
Von B aus ffihre ich Siezu dem Park-und-Ride Platz U. Ffir diese Routegibt es einen Detallplan.
Sie haben dort diefolgenden U-Bahnverbindungen ... (etc) " ("Ifound the yellow route from A via B to C. FromB I lead you to the park-and-ride parking U.That route is shown on a more detailed map.There, you have the following tube connection... (etc)") ( this information is only given astext)Example 2: Route Planning?
USER: "Ab h ier /~ (A) zum Kiesselbachplatz"("from here to Kiesselbachplatz")?
MOFA: ...presents route ...?
USER: "dorthin" ("to this location") ,2 ~ (B)?
MOFA: "Wollen Sie eine neueRoute definieren?"
("Do you want to define anew route?")?
(MOFA does not know, if the user wants to per-form a modification task referring to the routepreviously defined, or if he wants to define anew route)?
USER: "Nein" (" No")?
MOFA: ... presents route from A via Kiessel-bachplatz to B ...Example 3: Appointment Scheduling?
USER: 7 1st of May button"?
USER: "Morgen von viertel vor vier bis sechsmit Maier" (Tomorrow at a quarter to four untilsix with Maier")?
TALKY: presents the appointment in a struc-tured graphic dialog box with an okay button(cf Figure 3).
The box contains default infor-mation, that is proposed by the user model.Active and Passive Gestures ~ 7Figure 3: TALKYUSER: "Im Konferenzraum, bis sieben Uhr"("in the conference room, until seven o'clock")TALKY: Adds the information that the Meet-ing Room is Konferenzraum and sets a new endtime (cf.
Figure 4).
The continuation modulepropses as follw up task the information of theparticipants of the meeting.
To avoid a clarifi-cation dialog, the system assumes, as long theuser has not confirmed a proposal, he will stillfurther modify the appointment.USER: /~ (okay button)TALKY: removes Dialog boxUSER: "von zwei bis drei" (" from two to three")TALKY: presents a new appointment presenta-tion boxFigure 4: TALKY4 Problems with the integration ofdirect manipulation and naturalmult imoda l  dialogIn direct manipulation gestures lead to an imme-diate reaction.
The referent of a gesture is alwaysunambiguous: Either there is a single object selectedor the gesture is not successful at all.
In this pro-cess of selection only the gesture and the objectsare involved.
In natural communication deictic ges-tures may be vague or ambiguous.
They have tobe interpreted by considering context and the nat-ural language utterances that occur together withthe gesture.
The possibility of modifying a pointinggesture by spoken information is not a weakness buta valuable feature multimodal communication, thatmakes it easier to refer to structured objects or toclosely assembled tiny objects.For multimodal systems we are faced with theproblem that speech recognition and natural lan-guage analysis takes some time, which is somewhatcontrary to the immediate reaction expected fromdirect manipulative gestures.
The problem cannotbe completely solved by making analysis faster be-cause the user may want to perform some manipula-tion and see the result while he is speaking a longerutterance.If we could distinguish the manipulative or deicticnature of the gesture by analysing it's form and theobject referred to we could avoid waiting for linguis-tic analysis.
In the following we will discuss someapproaches for a solution of this problem.5 Act ive  and  Pass ive  gesturesWithout the support of other modalities, an activegesture determines an ction of the system (in caseof a graphical user interface) or it causes the dia-log partner to perform an action (in case of naturalcommunication).
A passive or "merely referential"pointing gesture serves only as a reference to objectsand does not call for any reaction, aside from rec-ognizing the act of reference.
The gesture may beambiguous or vague.
Passive gestures are not alwayscommunicative in nature (e.g someone may point ata map to support his own perception without anyintention to communicate an act of reference).If a gesture is active depends on the form of thegesture (e.g.
moving the mouse to some object is apassive form, pressing a mouse button at an objectis an active form), but also on the object, which be-ing referred to with the gesture.
E.G.
a mouse clickperformed on a menu item will start an action, whileclicking on a picture may be without any result.
Wewill now give a short definition of passive and ac-48 M. Streittive gesture forms and of passive and active objects.Then we analyse possible combinations.Passive gesture forms are not used to trigger ac-tions, they may be used non-communicatively.
Ac-tive forms are always intended communication, theymay be used to trigger an action.
Passive objectsserve only as potential referents of referential acts,while active objects react if they are activated byan active gesture form without the support by othermodalities.
There may be mixed objects as well,that behave actively using certain active gestureforms and passively with others.
With passive ges-ture forms every object behaves passive by defini-tion.There are six possible cases for a combination ofobjects with gesture forms:1.
Passive gesture forms performed on passive ob-ject2.
Passive gesture forms performed on mixed ob-ject3.
Passive gesture forms performed on active ob-jects4.
Active gesture forms performed on passive ob-ject5.
Active gesture forms performed on mixed object6.
Active gesture forms performed on active ob-jectsWe consider cases (1) to (4) as passive gestures,while (6) is considered an active one.
If (5) is ac-tive or passive depends on the concrete gesture form.Passive gestures are candidates for conversationalyused deictic gestures, while manipulative gesturesmust be active.
In the following, we will discusstwo approaches to distinguish between deictic andmanipulative uses of gestures.5.1 Distinction between Deietic andManipulative Gestures by Active andPassive Gesture FormsTo allow for a coexistence of natural communica-tion style and direct manipulative interaction in onemultimodal interface we dedicate?
(1),(2) and (3) to natural communication?
and (4),(5) and (6) to graphical interaction ((4)could be seen as a communication failure or asan attempt o perform natural communicationby manipulative gestures).This results in a clear cut between the communi-cation styles: GUIs take passive gesture forms asnon-communicative.
They may give feedback andhighlight the object the user is pointing to, but wemust not count this as an action, because it doesnot change the state of the dialog.
This means thatthe gestures dedicated to natural multimodal inter-action can operate on every object of the graphicallyrepresented universe of discourse, without unwantedmanipulative ffects.
Another advantage of this ap-proach is, that the user can keep with the graphicinteraction style, he is familiar with from graphi-cal interfaces.
There is no conflict with the selec-tion process (i.e.
the direct manipulative principleof reference resolution).
We can introduce an addi-tional process, which combines ambiguous or vagueinformation from deictic pointing with natural lan-guage information.
Furthermore, passive pointinggestures with the mouse are much more convenientthan active ones if they are performed in parallelwith speech.
We noticed that the coordination ofmouse clicks with deictic expressions requires highconcentration on the side of the user and frequentlyleads to errors.
This is quite different with touch-screens.
We followed the approach, presented in thissection, in an earlier version of MOFA (cf.
section 6Experience with Act ive  and  Passive Gesture Forms- MOFA with the Mouse as Input Device).
We ob-served two problems with that approach.?
It may be difficult to decide between commu-nicative and non-communicative uses of passivegesture forms.?
If we use other input devices than the mouse,the distinction between passive and active ges-ture forms may be not available , or only beachievable by an artificial introduction of newgestures.5.2 Distinction between Deictic andManipulative Gestures by differentActive Gesture FormsIf we make all graphically represented objects mixedor passive we arrive again at a clear cut betweenstyles, by distinguishing between certain types of ac-tive gesture forms.
The advantage of this approachis, that there is no problem with non-communicativeuses of gestures.
But with this approach we have tochange the usual meaning of gestures and the usualbehaviour of objects, that are not mixed (e.g.
menuitems or buttons are active objects).
Gestures willalso tend to become more complicated (in some caseswe need double clicks instead of simple clicks to ac-tivate an action).Active and Passive Gestures 49If we do not change the normal behaviour of ob-jects, we stay with objects for which we cannot de-cide if a gesture is meant deicticly or manipulatively.In particular, this means that graphical selectionmay prevent pointing gestures from being modifiedby speech.There is another small problem with active gestureforms.
The user may expect that using them willcause some action even with passive objects, if thecontext or the nature of the objects suggests how tointerpret such gestures.We will elaborate on these problems in sections8.1 Deictic Expressions and Pointing Gestures and8.3 Deictic Pointing Gestures as Autonumuos Com-municative Acts.6 Exper ience  w i th  Act ive  andPass ive  Gesture  Forms - MOFAwi th  the  Mouse  as  Input  Dev iceThis version is implemented with mouse-based ges-tural communication.
We used active gesture formsto achieve manipulative ffects and passive ones fordeictic pointing.
Because the mouse has to moveacross the screen to point to a certain referent, it isvery likely that objects are touched without inten-tion.
This is especially important for route descrip-tions, where several pointing acts occur during onespoken command.
Different filters are used to takeout unintended referents.?
First, the search for referents is restricted to atime frame which is a bit longer than the inter-val within the user is speaking.?
Next, type restrictions are applied to the pos-sible referents.
Type restrictions are inferredfrom deictic expressions e.g.
"diese Strasse"" this street","diese U-Bahnstation" - "thistube station", but also from inherent restric-tions concerning the recognized task(s).?
Finally, we exploit the fact that deictic expres-sions and deictic gestures are strictly synchro-nized in natural speech.
The problem withthis approach is that speech recognizers usu-ally do not deliver time information on the wordlevel.
Therefore time stamps on the word levelare interpolations.
There is only a rudimen-tary analysis of the track and the temporalcourse of the mouse movement.
Such an analy-sis would certainly improve referent resolution,though we noticed that pointing was sometimesnot marked and on the other hand, users madepauses during mouse movement without point-ing intentionally.In this MOFA version we can only use linguisticinformation to identify a task.
The identificationof referents by gesture analysis alone is to uncer-tain to use the type of the referents for task recog-nition.
This is different in the recent touchscreenbased MOFA version, which we will describe in thefollowing.7 MOFA and TALKY - the  vers ionfo r  touchscreen  inputThe recent versions of MOFA and TALKY are im-plemented with touchscreen technology as input de-vice.
The version also works with a mouse as inputdevice, but in this case the user must perform mouseclicks for deictic pointing.With touchscreens, every pointing to the screen isnormally mapped to mouse button events.
There areno passive gestures at all.
The distinction of deicticgestures must rely on active gesture forms.
Further-more, the problem of vague or ambiguous pointingbecomes more prominent: In the usual implementa-tion of touchscreens, pointing with the finger will bemapped to some exact coordination, but these co-ordinates are only vaguely related to the point theuser wanted to refer to.
A big advantage of touch-screen technology is that there is no problem withunintended pointing.
Although there is additionalvagueness in pointing, we can use type informationthat we get from referents much easier than withpassive mouse gestures.
Also, active pointing atthe touchscreen is completely natural in combina-tion with speech.8 Reference  Phenomena in  MOFAand TALKYThe system handles temporal and spatial deixis, andalso certain phenomena t the borderline of deicticand anaphoric reference.
It is able to treat ellipticexpressions, in which gestures upply missing argu-ments (deictical ellipsis).
The system resolves ellip-tic expressions and anaphora that occur in in modi-fication and follow up tasks.
Also dialog ellipsis andelliptic utterances that introduce new tasks are han-dled.
Many expressions including temporal deixisare not resolved by deictical gestures, but by com-putation of values, depending on the speaking time(e.g.
heute (today)).
Because of the graphic repre-sentation of time objects especially in the calendarapplication, there are also deictic gestures referringto temporal entities.
Deictic expressions occur asdemonstrative NPs (e.g.
diese Kreuzung (this cross-ing)) definite NPs (die Route (the route)) and alsoas adverbs (dorthin (there), dann (then), heute (to-50 M. Streitday)).
The NPs and some of the adverbs are alsoused anaphoricly.?
In MOFA the universe of discourse is the map.The objects on the map are not of the activetype.
The interaction with the map is not ma-nipulative, but conversational.
There are alsosome menu items, to which the user will hardlyrefer deicticly.?
In TALKY there are many active objects.
Theuser may navigate in the calendar by speech orby manipulating graphical interaction elements.In contrast to MOFA there are manipulative ob-jects, that are likely to be referred to also bydeictic gestures.8.1 Deictic Expressions and Point ingGesturesReference resolution for gestures as usual in graph-ical user interfaces works in an unambiguously way.To handle vague pointing we introduce transparentfields that constitute neighbourhoods for clusters ofobjects.
The selection of these fields or of one ob-ject on such a fields makes the neighbouring objectssalient as referents.
(cf.
section 3 Two implementedApplications: Sample Interactions with MOFA andTALKY, example 1 Route Planning).
Now type in-formation is used as a filter.
In example 1 every ob-ject that is a possible starting point for a car routeand also every tube-stations i  of appropriate type.If there remains more than one referent, or there isno referent left, a clarification dialog is invoked, thedialog must not be performed by natural languageonly, zooming on the cluster is also a possible reac-tion.?
(1) " zu dieser U-Bahnstation ("to that tube-stat ion") /~In (1) referent resolution applies the type restrictiontube-node before any clarification dialog is invoked.?
(2) "dorthin" ("to there") ,2(U1) "vondort" (from there") ~2~(U2)In (2) the system first recognizes an abstract askroute planning.
If the two referents of the gesturesare tube stations the system does that know by anunambiguous gesture or after a clarification dialog.The system will use these type constraints to recog-nize the concrete task "find a tube connection".8.2 Elliptic Expression and Point ingGestures?
(3) mit der U-Bahn (by tube) /~ ,~(3) is an example for an elliptic expression relatedto deictic gestures.
The gestures upply additionalarguments to the task "plan a tube connection".
Inaccount of the order of the arguments, MOFA willguess the first referent is the start, the second isthe destination of the route.
The task is identifiedby analyzing the elliptic expression.
The Task nowimposes type restriction on the arguments.8.3 Deictic Point ing Gestures asAutonumuos  Communicat ive  ActsWe recall the fact, that we use active gesture formsas deictic (i.e.
passive) gestures in the touchscreenversion of MOFA.
As mentioned in section 5.2 thismay give the user the idea to use them activelyto communicate without speech.
It is very naturalto order a ticket just by e.g.
saying "SaarbrfickenMfinchen" with the first town as starting point andthe second town as destination.
Similar one candescribe a route on a map by pointing first to thestart and then to the destination.
In contrast, if theuser is pointing only once, it is most likely, that hemeans the destination of a route.
Such pointing actscommunicate he same informations as speaking thenames of the locations.
This way to communicate isa sort of natural communication, that does not fit todirect manipulation.
The interpretation of the firstpointing depends on the fact if there is a second one,which is not the way as direct manipulation works.We handle these natural gestural communication bythe following steps.?
The input control checks if the speech channelis active.?
If speech is not active it waits a short time untiltimeout.?
If there is a second gesture before timeout itwaits again a short time.?
Otherwise the interpretation module is calledfor monomodal gesture interpretation.?
The interpretation module proceeds this inputlike a sequence of names, perhaps after a clari-fication dialog to resolve ambiguous reference.8.4 Are Temporal  Relat ions necessary forthe Resolut ion of  Deictic Expressions\[Huls 1996\] proposes instead of analysing temporalrelation to solve the problem of parallel gestures byincrementally parsing and resolving deictic expres-sions.
We think that approach will not work withspoken input and in certain cases will also not workfor text input.Active and Passive Gestures 511.
With spoken input the deictic expressions con-tained in the utterance can be analysed in generalonly after all deictic gestures are performed, becausespeech recognizers do not provide incremental input.Therefore the temporal order has to be accountedfor.2.
A deictic gesture may be refer to an object,that is not of appropriate type by an error of theuser.
From the temporal synchronization f deicticexpressions and deictic gestures, we infer, that thisgesture was intend to communicate he referent ofthe deictic expression.
But if we apply anaphora res-olution methods, it is very likely that we exclude bytype considerations the referent of the gesture fromthe set of possible referents of the deictic phrase.Perhaps we may instead find some other referent,from which we could now by temporal consideration,that it is not a possible referent.3.
If deictic gestures are used as in section 8.3Deictic Pointing Gestures as Autonumuos Commu-nicative Acts, it is obvious that we need the temporalorder of the gestural events for interpretation.
Thisargument applies also to elliptic utterances as (4).?
(4) "mit der U-Bahn" ("by tube") ~, /~In (4) we must now the order of the gestures to dis-tribute the referent in the right order to the argu-ments the route planning task.9 Open Quest ionAs mentioned in section 8 Reference Phenomena inMOFA and TALKY some active objects in TALKYcould appropriately be used for deictic reference.
Ifthe user opens a day-sheet in the calendar by manip-ulation and defines an appointment, without spec-ifying a day, by speech, TALKY will schedule theappointment to the day, opened by the user.
Theeffect is the same as with an deictical reference toa passive representation f the day under considera-tion.?
(5) "Urlaub von dann/~ bis dann.
/~" ("Holi-days from then to then")In (5) it is doubtful, if the user really wants to openthe two day-sheets he is referring to.
Is there a solu-tion for this problem, when you have a touchscreenas input device?
Does the problem mean, that wemust use or invent other gestural input technics?ReferencesC.
Huls, E. Bos, W. Claassen, "Automatic Refer-ent Resolution of Deictic and Anaphoric Expres-sions", Computational Linguistics, 1996.J.L.
Gauvain, J.J. Gangolf, L. Lamel, "SpeechRecognition for an Information Kiosk", Proc.
IC-SLP 96, Philadelphia, 1996.M.
Streit, A. Krueger, "Eine agentenorien-tierte Architektur fuer multimediale Benutzer-schnittstellen", Online 96 - Congressband VI,Hamburg, 1996.
