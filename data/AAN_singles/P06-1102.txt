Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809?816,Sydney, July 2006. c?2006 Association for Computational LinguisticsNames and Similarities on the Web: Fact Extraction in the Fast LaneMarius Pas?caGoogle Inc.Mountain View, CA 94043mars@google.comDekang LinGoogle Inc.Mountain View, CA 94043lindek@google.comJeffrey Bigham?University of WashingtonSeattle, WA 98195jbigham@cs.washington.eduAndrei Lifchits?University of British ColumbiaVancouver, BC V6T 1Z4alifchit@cs.ubc.caAlpa Jain?Columbia UniversityNew York, NY 10027alpa@cs.columbia.eduAbstractIn a new approach to large-scale extrac-tion of facts from unstructured text, dis-tributional similarities become an integralpart of both the iterative acquisition ofhigh-coverage contextual extraction pat-terns, and the validation and ranking ofcandidate facts.
The evaluation mea-sures the quality and coverage of factsextracted from one hundred million Webdocuments, starting from ten seed factsand using no additional knowledge, lexi-cons or complex tools.1 Introduction1.1 BackgroundThe potential impact of structured fact reposito-ries containing billions of relations among namedentities on Web search is enormous.
They en-able the pursuit of new search paradigms, the pro-cessing of database-like queries, and alternativemethods of presenting search results.
The prepa-ration of exhaustive lists of hand-written extrac-tion rules is impractical given the need for domain-independent extraction of many types of facts fromunstructured text.
In contrast, the idea of boot-strapping for relation and information extractionwas first proposed in (Riloff and Jones, 1999), andsuccessfully applied to the construction of seman-tic lexicons (Thelen and Riloff, 2002), named en-tity recognition (Collins and Singer, 1999), extrac-tion of binary relations (Agichtein and Gravano,2000), and acquisition of structured data for taskssuch as Question Answering (Lita and Carbonell,2004; Fleischman et al, 2003).
In the context offact extraction, the resulting iterative acquisition?Work done during internships at Google Inc.framework starts from a small set of seed facts,finds contextual patterns that extract the seed factsfrom the underlying text collection, identifies alarger set of candidate facts that are extracted bythe patterns, and adds the best candidate facts tothe previous seed set.1.2 ContributionsFigure 1 describes an architecture geared towardslarge-scale fact extraction.
The architecture is sim-ilar to other instances of bootstrapping for infor-mation extraction.
The main processing stages arethe acquisition of contextual extraction patternsgiven the seed facts, acquisition of candidate factsgiven the extraction patterns, scoring and rankingof the patterns, and scoring and ranking of the can-didate facts, a subset of which is added to the seedset of the next round.Within the existing iterative acquisition frame-work, our first contribution is a method for au-tomatically generating generalized contextual ex-traction patterns, based on dynamically-computedclasses of similar words.
Traditionally, the ac-quisition of contextual extraction patterns requireshundreds or thousands of consecutive iterationsover the entire text collection (Lita and Carbonell,2004), often using relatively expensive or restric-tive tools such as shallow syntactic parsers (Riloffand Jones, 1999; Thelen and Riloff, 2002) ornamed entity recognizers (Agichtein and Gravano,2000).
Comparatively, generalized extraction pat-terns achieve exponentially higher coverage inearly iterations.
The extraction of large sets of can-didate facts opens the possibility of fast-growth it-erative extraction, as opposed to the de-facto strat-egy of conservatively growing the seed set by asfew as five items (Thelen and Riloff, 2002) aftereach iteration.809Acquisition of contextual extraction patternsDistributional similaritiesText collectionCandidate factsAcquisition of candidate factsOccurrences of extraction patternsValidation of candidate factsScored extraction patternsScored candidate factsScoring and rankingValidated candidate factsSeed factsOccurrences of seed facts Extraction patternsValidated extraction patternsValidation of patternsGeneralized extraction patternsFigure 1: Large-scale fact extraction architectureThe second contribution of the paper is amethod for domain-independent validation andranking of candidate facts, based on a similar-ity measure of each candidate fact relative to theset of seed facts.
Whereas previous studies as-sume clean text collections such as news cor-pora (Thelen and Riloff, 2002; Agichtein and Gra-vano, 2000; Hasegawa et al, 2004), the valida-tion is essential for low-quality sets of candidatefacts collected from noisy Web documents.
With-out it, the addition of spurious candidate facts tothe seed set would result in a quick divergence ofthe iterative acquisition towards irrelevant infor-mation (Agichtein and Gravano, 2000).
Further-more, the finer-grained ranking induced by simi-larities is necessary in fast-growth iterative acqui-sition, whereas previously proposed ranking crite-ria (Thelen and Riloff, 2002; Lita and Carbonell,2004) are implicitly designed for slow growth ofthe seed set.2 Similarities for Pattern Acquisition2.1 Generalization via Word SimilaritiesThe extraction patterns are acquired by matchingthe pairs of phrases from the seed set into docu-ment sentences.
The patterns consist of contigu-ous sequences of sentence terms, but otherwisediffer from the types of patterns proposed in earlierwork in two respects.
First, the terms of a patternare either regular words or, for higher generality,any word from a class of similar words.
Second,the amount of textual context encoded in a pat-tern is limited to the sequence of terms between(i.e., infix) the pair of phrases from a seed fact thatcould be matched in a document sentence, thus ex-cluding any context to the left (i.e., prefix) and tothe right (i.e., postfix) of the seed.The pattern shown at the top of Figure 2, which(Irving Berlin, 1888)NNP       NNP       CDInfixAurelio de la Vega was born November 28 , 1925 , in Havana , Cuba .FW       FW FW  NNP VBD  VBN      NNP           CD  ,    CD    ,  IN    NNP      ,   NNP    .foundnot foundInfixnot foundPrefix PostfixInfixMatching on sentencesSeed fact Infix?only patternThe poet was born Jan. 13 , several years after the revolution .not foundBritish ?
native Glenn Cornick of Jethro Tull was born April 23 , 1947 .NNP     :      JJ         NNP        NNP       IN    NNP     NNP  VBD  VBN   NNP   CD  ,   CD     .InfixfoundfoundChester Burton Atkins was born June 20 , 1924 , on a farm near Luttrell .NNP          NNP       NNP     VBD  VBN  NNP  CD  ,   CD     ,  IN DT  NN     IN       NNP       .InfixInfixfoundThe youngest child of three siblings , Mariah Carey was born March 27 ,1970 in Huntington , Long Island in New York .DT       JJS            NN     IN   CD        NNS       ,    NNP        NNP    VBD  VBN    NNP     CD  ,CD    IN       NNP             ,    JJ         NN      IN  NNP    NNP   .foundfoundfound(S1)(S2)(S3)(S4)(S5)(Jethro Tull, 1947)  (Mariah Carey, 1970)  (Chester Burton Atkins, 1924)Candidate factsDT    NN   VBD  VBN  NNP CD ,       JJ           NNS     IN     DT        NN           .N/A          CL1 born CL2 00 ,              N/AFigure 2: Extraction via infix-only patternscontains the sequence [CL1 born CL2 00 .
], illus-trates the use of classes of distributionally similarwords within extraction patterns.
The first wordclass in the sequence, CL1, consists of words suchas {was, is, could}, whereas the second class in-cludes {February, April, June, Aug., November}and other similar words.
The classes of words arecomputed on the fly over all sequences of termsin the extracted patterns, on top of a large set ofpairwise similarities among words (Lin, 1998) ex-tracted in advance from around 50 million newsarticles indexed by the Google search engine overthree years.
All digits in both patterns and sen-tences are replaced with a common marker, such810that any two numerical values with the same num-ber of digits will overlap during matching.Many methods have been proposed to computedistributional similarity between words, e.g., (Hin-dle, 1990), (Pereira et al, 1993), (Grefenstette,1994) and (Lin, 1998).
Almost all of the methodsrepresent a word by a feature vector, where eachfeature corresponds to a type of context in whichthe word appeared.
They differ in how the featurevectors are constructed and how the similarity be-tween two feature vectors is computed.In our approach, we define the features of aword w to be the set of words that occurred withina small window of w in a large corpus.
The contextwindow of an instance of w consists of the clos-est non-stopword on each side of w and the stop-words in between.
The value of a feature w?
is de-fined as the pointwise mutual information betweenw?
and w: PMI(w?, w) = ?
log( P (w,w?
)P (w)P (w?)).
Thesimilarity between two different words w1 and w2,S(w1, w2), is then computed as the cosine of theangle between their feature vectors.While the previous approaches to distributionalsimilarity have only applied to words, we appliedthe same technique to proper names as well aswords.
The following are some example similarwords and phrases with their similarities, as ob-tained from the Google News corpus:?
Carey: Higgins 0.39, Lambert 0.39, Payne0.38, Kelley 0.38, Hayes 0.38, Goodwin 0.38,Griffin 0.38, Cummings 0.38, Hansen 0.38,Williamson 0.38, Peters 0.38, Walsh 0.38, Burke0.38, Boyd 0.38, Andrews 0.38, Cunningham0.38, Freeman 0.37, Stephens 0.37, Flynn 0.37,Ellis 0.37, Bowers 0.37, Bennett 0.37, Matthews0.37, Johnston 0.37, Richards 0.37, Hoffman0.37, Schultz 0.37, Steele 0.37, Dunn 0.37, Rowe0.37, Swanson 0.37, Hawkins 0.37, Wheeler 0.37,Porter 0.37, Watkins 0.37, Meyer 0.37 [..];?
Mariah Carey: Shania Twain 0.38, ChristinaAguilera 0.35, Sheryl Crow 0.35, Britney Spears0.33, Celine Dion 0.33, Whitney Houston 0.32,Justin Timberlake 0.32, Beyonce Knowles 0.32,Bruce Springsteen 0.30, Faith Hill 0.30, LeAnnRimes 0.30, Missy Elliott 0.30, Aretha Franklin0.29, Jennifer Lopez 0.29, Gloria Estefan 0.29,Elton John 0.29, Norah Jones 0.29, MissyElliot 0.29, Alicia Keys 0.29, Avril Lavigne0.29, Kid Rock 0.28, Janet Jackson 0.28, KylieMinogue 0.28, Beyonce 0.27, Enrique Iglesias0.27, Michelle Branch 0.27 [..];?
Jethro Tull: Motley Crue 0.28, Black Crowes0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sab-bath 0.26, Doobie Brothers 0.26, Judas Priest 0.26,Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24,Black Flag 0.24, Godsmack 0.24, Grateful Dead0.24, Grand Funk Railroad 0.24, Smashing Pump-kins 0.24, Led Zeppelin 0.24, Aerosmith 0.24,Limp Bizkit 0.24, Counting Crows 0.24, EchoAnd The Bunnymen 0.24, Cold Chisel 0.24, ThinLizzy 0.24 [..].To our knowledge, the only previous study thatembeds similarities into the acquisition of extrac-tion patterns is (Stevenson and Greenwood, 2005).The authors present a method for computing pair-wise similarity scores among large sets of poten-tial syntactic (subject-verb-object) patterns, to de-tect centroids of mutually similar patterns.
By as-suming the syntactic parsing of the underlying textcollection to generate the potential patterns in thefirst place, the method is impractical on Web-scalecollections.
Two patterns, e.g.
chairman-resignand CEO-quit, are similar to each other if theircomponents are present in an external hand-builtontology (i.e., WordNet), and the similarity amongthe components is high over the ontology.
Sincegeneral-purpose ontologies, and WordNet in par-ticular, contain many classes (e.g., chairman andCEO) but very few instances such as Osasuna,Crewe etc., the patterns containing an instancerather than a class will not be found to be simi-lar to one another.
In comparison, the classes andinstances are equally useful in our method for gen-eralizing patterns for fact extraction.
We mergebasic patterns into generalized patterns, regardlessof whether the similar words belong, as classes orinstances, in any external ontology.2.2 Generalization via Infix-Only PatternsBy giving up the contextual constraints imposedby the prefix and postfix, infix-only patterns rep-resent the most aggressive type of extraction pat-terns that still use contiguous sequences of terms.In the absence of the prefix and postfix, the outerboundaries of the fact are computed separately forthe beginning of the first (left) and end of the sec-ond (right) phrases of the candidate fact.
For gen-erality, the computation relies only on the part-of-speech tags of the current seed set.
Startingforward from the right extremity of the infix, wecollect a growing sequence of terms whose part-of-speech tags are [P1+ P2+ .. Pn+], where the811notation Pi+ represents one or more consecutiveoccurrences of the part-of-speech tag Pi.
The se-quence [P1 P2 .. Pn] must be exactly the sequenceof part of speech tags from the right side of one ofthe seed facts.
The point where the sequence can-not be grown anymore defines the boundary of thefact.
A similar procedure is applied backwards,starting from the left extremity of the infix.
Aninfix-only pattern produces a candidate fact froma sentence only if an acceptable sequence is foundto the left and also to the right of the infix.Figure 2 illustrates the process on the infix-only pattern mentioned earlier, and one seed fact.The part-of-speech tags for the seed fact are [NNPNNP] and [CD] for the left and right sides respec-tively.
The infix occurs in all sentences.
How-ever, the matching of the part-of-speech tags of thesentence sequences to the left and right of the in-fix, against the part-of-speech tags of the seed fact,only succeeds for the last three sentences.
It failsfor the first sentence S1 to the left of the infix, be-cause [.. NNP] (for Vega) does not match [NNPNNP].
It also fails for the second sentence S2 toboth the left and the right side of the infix, since [..NN] (for poet) does not match [NNP NNP], and[JJ ..] (for several) does not match [CD].3 Similarities for Validation and Ranking3.1 Revisiting Standard Ranking CriteriaBecause some of the acquired extraction patternsare too generic or wrong, all approaches to iter-ative acquisition place a strong emphasis on thechoice of criteria for ranking.
Previous literaturequasi-unanimously assesses the quality of eachcandidate fact based on the number and qual-ity of the patterns that extract the candidate fact(more is better); and the number of seed facts ex-tracted by the same patterns (again, more is bet-ter) (Agichtein and Gravano, 2000; Thelen andRiloff, 2002; Lita and Carbonell, 2004).
However,our experiments using many variations of previ-ously proposed scoring functions suggest that theyhave limited applicability in large-scale fact ex-traction, for two main reasons.
The first is thatit is impractical to perform hundreds of acquisi-tion iterations on terabytes of text.
Instead, oneneeds to grow the seed set aggressively in eachiteration.
Previous scoring functions were im-plicitly designed for cautious acquisition strate-gies (Collins and Singer, 1999), which expand theseed set very slowly across consecutive iterations.In that case, it makes sense to single out a smallnumber of best candidates, among the other avail-able candidates.
Comparatively, when 10,000 can-didate facts or more need to be added to a seed setof 10 seeds as early as after the first iteration, itis difficult to distinguish the quality of extractionpatterns based, for instance, only on the percent-age of the seed set that they extract.
The secondreason is the noisy nature of the Web.
A substan-tial number of factors can and will concur towardsthe worst-case extraction scenarios on the Web.Patterns of apparently high quality turn out to pro-duce a large quantity of erroneous ?facts?
such as(A-League, 1997), but also the more interesting(Jethro Tull, 1947) as shown earlier in Figure 2, or(Web Site David, 1960) or (New York, 1831).
Asfor extraction patterns of average or lower quality,they will naturally lead to even more spurious ex-tractions.3.2 Ranking of Extraction PatternsThe intuition behind our criteria for ranking gen-eralized pattern is that patterns of higher preci-sion tend to contain words that are indicative ofthe relation being mined.
Thus, a pattern is morelikely to produce good candidate facts if its in-fix contains the words language or spoken if ex-tracting Language-SpokenIn-Country facts, or theword capital if extracting City-CapitalOf-Countryrelations.
In each acquisition iteration, the scor-ing of patterns is a two-pass procedure.
The firstpass computes the normalized frequencies of allwords excluding stopwords, over the entire set ofextraction patterns.
The computation applies sep-arately to the prefix, infix and postfix of the pat-terns.
In the second pass, the score of an extractionpattern is determined by the words with the high-est frequency score in its prefix, infix and postfix,as computed in the first pass and adjusted for therelative distance to the start and end of the infix.3.3 Ranking of Candidate FactsFigure 3 introduces a new scheme for assessing thequality of the candidate facts, based on the compu-tation of similarity scores for each candidate rela-tive to the set of seed facts.
A candidate fact, e.g.,(Richard Steele, 1672), is similar to the seed set ifboth its phrases, i.e., Richard Steele and 1672, aresimilar to the corresponding phrases (John Lennonor Stephen Foster in the case of Richard Steele)from the seed facts.
For a phrase of a candidatefact to be assigned a non-default (non-minimum)812...LennonLambertMcFaddenBatesonMcNamaraCostelloCroninWooleyBaker...FosterHansenHawkinsFisherHollowaySteeleSweeneyChrisJohnJamesAndrewMikeMattBrianChristopher...John Lennon         1940Seed factsStephen Foster      1826Brian McFadden           1980(4)(3)Robert S. McNamara    1916(6)(5)Barbara Steele               1937(7) (2)Stan Hansen                  1949(9)(8)Similar wordsSimilar wordsfor: JohnSimilar wordsfor: Stephenfor: LennonSimilar wordsfor: Foster...StephenRobertMichaelPeterWilliamStanRichard(1)Barbara(3)(5)(7) (2)(8)(9)(4)(6)(2)(1)Candidate factsJethro Tull                     1947Richard Steele               1672Figure 3: The role of similarities in estimating thequality of candidate factssimilarity score, the words at its extremities mustbe similar to one or more words situated at thesame positions in the seed facts.
This is the casefor the first five candidate facts in Figure 3.
For ex-ample, the first word Richard from one of the can-didate facts is similar to the first word John fromone of the seed facts.
Concurrently, the last wordSteele from the same phrase is similar to Fosterfrom another seed fact.
Therefore Robert Fosteris similar to the seed facts.
The score of a phrasecontaining N words is:{C1 +?Ni=1 log(1 + Simi) , if Sim1,N > 0C2 , otherwise.where Simi is the similarity of the componentword at position i in the phrase, and C1 and C2are scaling constants such that C2C1.
Thus,the similarity score of a candidate fact aggregatesindividual word-to-word similarity scores, for theleft side and then for the right side of a candidatefact.
In turn, the similarity score of a componentword Simi is higher if: a) the computed word-to-word similarity scores are higher relative to wordsat the same position i in the seeds; and b) the com-ponent word is similar to words from more thanone seed fact.The similarity scores are one of a linear com-bination of features that induce a ranking over thecandidate facts.
Three other domain-independentfeatures contribute to the final ranking: a) a phrasecompleteness score computed statistically over theentire set of candidate facts, which demotes candi-date facts if any of their two sides is likely to beincomplete (e.g., Mary Lou vs. Mary Lou Retton,or John F. vs. John F. Kennedy); b) the averagePageRank value over all documents from whichthe candidate fact is extracted; and c) the pattern-based scores of the candidate fact.
The latter fea-ture converts the scores of the patterns extractingthe candidate fact into a score for the candidatefact.
For this purpose, it considers a fixed-lengthwindow of words around each match of a candi-date fact in some sentence from the text collection.This is equivalent to analyzing all sentence con-texts from which a candidate fact can be extracted.For each window, the word with the highest fre-quency score, as computed in the first pass of theprocedure for scoring the patterns, determines thescore of the candidate fact in that context.
Theoverall pattern-based score of a candidate fact isthe sum of the scores over all its contexts of occur-rence, normalized by the frequency of occurrenceof the candidate over all sentences.Besides inducing a ranking over the candidatefacts, the similarity scores also serve as a valida-tion filter over the candidate facts.
Indeed, anycandidates that are not similar to the seed set canbe filtered out.
For instance, the elimination of(Jethro Tull, 1947) is a side effect of verifying thatTull is not similar to any of the last-position wordsfrom phrases in the seed set.4 Evaluation4.1 DataThe source text collection consists of three chunksW1, W2, W3 of approximately 100 million doc-uments each.
The documents are part of a largersnapshot of the Web taken in 2003 by the Googlesearch engine.
All documents are in English.The textual portion of the documents is cleanedof Html, tokenized, split into sentences and part-of-speech tagged using the TnT tagger (Brants,2000).The evaluation involves facts of type Person-BornIn-Year.
The reasons behind the choice ofthis particular type are threefold.
First, manyPerson-BornIn-Year facts are probably availableon the Web (as opposed to, e.g., City-CapitalOf-Country facts), to allow for a good stress testfor large-scale extraction.
Second, either side ofthe facts (Person and Year) may be involved inmany other types of facts, such that the extrac-tion would easily divergence unless it performscorrectly.
Third, the phrases from one side (Per-son) have an utility in their own right, for lexicon813Table 1: Set of seed Person-BornIn-Year factsName Year Name YearPaul McCartney 1942 John Lennon 1940Vincenzo Bellini 1801 Stephen Foster 1826Hoagy Carmichael 1899 Irving Berlin 1888Johann Sebastian Bach 1685 Bela Bartok 1881Ludwig van Beethoven 1770 Bob Dylan 1941construction or detection of person names.The Person-BornIn-Year type is specifiedthrough an initial set of 10 seed facts shown in Ta-ble 1.
Similarly to source documents, the facts arealso part-of-speech tagged.4.2 System SettingsIn each iteration, the case-insensitive matching ofthe current set of seed facts onto the sentences pro-duces basic patterns.
The patterns are convertedinto generalized patterns.
The length of the infixmay vary between 1 and 6 words.
Potential pat-terns are discarded if the infix contains only stop-words.When a pattern is retained, it is used as aninfix-only pattern, and allowed to generate at most600,000 candidate facts.
At the end of an itera-tion, approximately one third of the validated can-didate facts are added to the current seed set.
Con-sequently, the acquisition expands the initial seedset of 10 facts to 100,000 facts (after iteration 1)and then to one million facts (after iteration 2) us-ing chunk W1.4.3 PrecisionA separate baseline run extracts candidate factsfrom the text collection following the traditionaliterative acquisition approach.
Pattern general-ization is disabled, and the ranking of patternsand facts follows strictly the criteria and scoringfunctions from (Thelen and Riloff, 2002), whichare also used in slightly different form in (Litaand Carbonell, 2004) and (Agichtein and Gravano,2000).
The theoretical option of running thou-sands of iterations over the text collection is notviable, since it would imply a non-justifiable ex-pense of our computational resources.
As a morerealistic compromise over overly-cautious acqui-sition, the baseline run retains as many of the topcandidate facts as the size of the current seed,whereas (Thelen and Riloff, 2002) only add thetop five candidate facts to the seed set after each it-eration.
The evaluation considers all 80, a sampleof the 320, and another sample of the 10,240 factsretained after iterations 3, 5 and 10 respectively.The correctness assessment of each fact consistsin manually finding some Web page that containsclear evidence that the fact is correct.
If no suchpage exists, the fact is marked as incorrect.
Thecorresponding precision values after the three iter-ations are 91.2%, 83.8% and 72.9%.For the purpose of evaluating the precision ofour system, we select a sample of facts fromthe entire list of one million facts extracted fromchunk W1, ranked in decreasing order of theircomputed scores.
The sample is generated auto-matically from the top of the list to the bottom, byretaining a fact and skipping the following consec-utive N facts, where N is incremented at each step.The resulting list, which preserves the relative or-der of the facts, contains 1414 facts.
The 115 factsfor which a Web search engine does not return anydocuments, when the name (as a phrase) and theyear are submitted together in a conjunctive query,are discarded from the sample of 1414 facts.
Inthose cases, the facts were acquired from the 2003snapshot of the Web, but queries are submitted toa search engine with access to current Web doc-uments, hence the difference when some of the2003 documents are no longer available or index-able.Based on the sample set, the average preci-sion of the list of one million facts extracted fromchunk W1 is 98.5% over the top 1/100 of the list,93.1% over the top half of the list, and 88.3% overthe entire list of one million facts.
Table 2 showsexamples of erroneous facts extracted from chunkW1.
Causes of errors include incorrect approxima-tions of the name boundaries (e.g., Alma in AlmaTheresa Rausch is incorrectly tagged as an adjec-tive), and selection of the wrong year as birth year(e.g., for Henry Lumbar).In the case of famous people, the extracted factstend to capture the correct birth year for severalvariations of the names, as shown in Table 3.
Con-versely, it is not necessary that a fact occur withhigh frequency in order for it to be extracted,which is an advantage over previous approachesthat rely strongly on redundancy (cf.
(Cafarella etal., 2005)).
Table 4 illustrates a few of the cor-rectly extracted facts that occur rarely on the Web.4.4 RecallIn contrast to the assessment of precision, recallcan be evaluated automatically, based on external814Table 2: Incorrect facts extracted from the WebSpurious Fact Context in Source Sentence(Theresa Rausch, Alma Theresa Rausch was born1912) on 9 March 1912(Henry Lumbar, Henry Lumbar was born 18611937) and died 1937(Concepcion Paxety, Maria de la Concepcion Paxety1817) b.
08 Dec. 1817 St. Aug., FL.
(Mae Yaeger, Ella May/Mae Yaeger was born1872) 20 May 1872 in Mt.
(Charles Whatley, Long, Charles Whatley b.
161821) FEB 1821 d. 29 AUG(HOLT George W. HOLT (new line) George W. HoltHolt, 1845) was born in Alabama in 1845(David Morrish David Morrish (new line)Canadian, 1953) Canadian, b.
1953(Mary Ann, 1838) had a daughter, Mary Ann, whowas born in Tennessee in 1838(Mrs. Blackmore, Mrs. Blackmore was born April1918) 28, 1918, in LabaddieyTable 3: Birth years extracted for bothpseudonyms and corresponding real namesPseudonym Real Name YearGloria Estefan Gloria Fajardo 1957Nicolas Cage Nicolas Kim Coppola 1964Ozzy Osbourne John Osbourne 1948Ringo Starr Richard Starkey 1940Tina Turner Anna Bullock 1939Tom Cruise Thomas Cruise Mapother IV 1962Woody Allen Allen Stewart Konigsberg 1935lists of birth dates of various people.
We start bycollecting two gold standard sets of facts.
The firstset is a random set of 609 actors and their birthyears from a Web compilation (GoldA).
The sec-ond set is derived from the set of questions usedin the Question Answering track (Voorhees andTice, 2000) of the Text REtrieval Conference from1999 through 2002.
Each question asking for thebirth date of a person (e.g., ?What year was RobertFrost born??)
results in a pair containing the per-son?s name and the birth year specified in the an-swer keys.
Thus, the second gold standard setcontains 17 pairs of people and their birth years(GoldT ).
Table 5 shows examples of facts in eachof the gold standard sets.Table 6 shows two types of recall scores com-puted against the gold standard sets.
The recallscores over ?Gold take into consideration only theset of person names from the gold standard withsome extracted year(s).
More precisely, given thatsome years were extracted for a person name, itverifies whether they include the year specified inthe gold standard for that person name.
Compar-atively, the recall score denoted AllGold is com-Table 4: Extracted facts that occur infrequentlyFact Source Domain(Irvine J Forcier, 1912) geocities.com(Marie Louise Azelie Chabert, 1861) vienici.com(Jacob Shalles, 1750) selfhost.com(Robert Chester Claggett, 1898) rootsweb.com(Charoltte Mollett, 1843) rootsweb.com(Nora Elizabeth Curran, 1979) jimtravis.comTable 5: Composition of gold standard setsGold Set Composition and Examples of FactsGoldA Actors (Web compilation) Nr.
facts: 609(Andie MacDowell, 1958), (Doris Day,1924), (Diahann Carroll, 1935)GoldT People (TREC QA track) Nr.
facts: 17(Davy Crockett, 1786), (Julius Caesar,100 B.C.
), (King Louis XIV, 1638)puted over the entire set of names from the goldstandard.For the GoldA set, the size of the ?Gold set ofperson names changes little when the facts are ex-tracted from chunk W1 vs. W2 vs. W3.
The re-call scores over ?Gold exhibit little variation fromone Web chunk to another, whereas the AllGoldscore is slightly higher on the W3 chunk, prob-ably due to a higher number of documents thatare relevant to the extraction task.
When the factsare extracted from a combination of two or threeof the available Web chunks, the recall scorescomputed over AllGold are significantly higher asthe size of the ?Gold set increases.
In compar-ison, the recall scores over the growing ?Goldset increases slightly with larger evaluation sets.The highest value of the recall score for GoldAis 89.9% over the ?Gold set, and 70.7% overAllGold.
The smaller size of the second gold stan-dard set, GoldT , explains the higher variation ofthe values shown in the lower portion of Table 6.4.5 Comparison to Previous ResultsAnother recent approach specifically addresses theproblem of extracting facts from a similarly-sizedcollection of Web documents.
In (Cafarella et al,2005), manually-prepared extraction rules are ap-plied to a collection of 60 million Web documentsto extract entities of types Company and Country,as well as facts of type Person-CeoOf-Companyand City-CapitalOf-Country.
Based on manualevaluation of precision and recall, a total of 23,128company names are extracted at precision of 80%;the number decreases to 1,116 at precision of 90%.In addition, 2,402 Person-CeoOf-Company facts815Table 6: Automatic evaluation of recall, over twogold standard sets GoldA (609 person names) andGoldT (17 person names)Gold Set Input Data Recall (%)(Web Chunk) ?Gold AllGoldGoldA W1 86.4 49.4W2 85.0 50.5W3 86.3 54.1W1+W2 88.5 64.5W1+W2+W3 89.9 70.7GoldT W1 81.8 52.9W2 90.0 52.9W3 100.0 64.7W1+W2 81.8 52.9W1+W2+W3 91.6 64.7are extracted at precision 80%.
The recall value is80% at precision 90%.
Recall is evaluated againstthe set of company names extracted by the system,rather than an external gold standard with pairs ofa CEO and a company name.
As such, the result-ing metric for evaluating recall used in (Cafarellaet al, 2005) is somewhat similar to, though morerelaxed than, the recall score over the ?Gold setintroduced in the previous section.5 ConclusionThe combination of generalized extraction pat-terns and similarity-driven ranking criteria resultsin a fast-growth iterative approach for large-scalefact extraction.
From 10 Person-BornIn-Year factsand no additional knowledge, a set of one millionfacts of the same type is extracted from a collec-tion of 100 million Web documents of arbitraryquality, with a precision around 90%.
This cor-responds to a growth ratio of 100,000:1 betweenthe size of the extracted set of facts and the sizeof the initial set of seed facts.
To our knowledge,the growth ratio and the number of extracted factsare several orders of magnitude higher than in anyof the previous studies on fact extraction based oneither hand-written extraction rules (Cafarella etal., 2005), or bootstrapping for relation and infor-mation extraction (Agichtein and Gravano, 2000;Lita and Carbonell, 2004).
The next research stepsconverge towards the automatic construction of asearchable repository containing billions of factsregarding people.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Extractingrelations from large plaintext collections.
In Proceedingsof the 5th ACM International Conference on Digital Li-braries (DL-00), pages 85?94, San Antonio, Texas.T.
Brants.
2000.
TnT - a statistical part of speech tagger.In Proceedings of the 6th Conference on Applied NaturalLanguage Processing (ANLP-00), pages 224?231, Seattle,Washington.M.
Cafarella, D. Downey, S. Soderland, and O. Etzioni.2005.
KnowItNow: Fast, scalable information extrac-tion from the web.
In Proceedings of the Human Lan-guage Technology Conference (HLT-EMNLP-05), pages563?570, Vancouver, Canada.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed entity classification.
In Proceedings of the 1999Conference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora (EMNLP/VLC-99),pages 189?196, College Park, Maryland.M.
Fleischman, E. Hovy, and A. Echihabi.
2003.
Offlinestrategies for online question answering: Answering ques-tions before they are asked.
In Proceedings of the 41stAnnual Meeting of the Association for Computational Lin-guistics (ACL-03), pages 1?7, Sapporo, Japan.G.
Grefenstette.
1994.
Explorations in Automatic ThesaurusDiscovery.
Kluwer Academic Publishers, Boston, Mas-sachusetts.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Discover-ing relations among named entities from large corpora.
InProceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL-04), pages 415?422, Barcelona, Spain.D.
Hindle.
1990.
Noun classification from predicate-argument structures.
In Proceedings of the 28th AnnualMeeting of the Association for Computational Linguistics(ACL-90), pages 268?275, Pittsburgh, Pennsylvania.D.
Lin.
1998.
Automatic retrieval and clustering of similarwords.
In Proceedings of the 17th International Confer-ence on Computational Linguistics and the 36th AnnualMeeting of the Association for Computational Linguistics(COLING-ACL-98), pages 768?774, Montreal, Quebec.L.
Lita and J. Carbonell.
2004.
Instance-based ques-tion answering: A data driven approach.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-04), pages 396?403,Barcelona, Spain.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distributional clus-tering of english words.
In Proceedings of the 31st AnnualMeeting of the Association for Computational Linguistics(ACL-93), pages 183?190, Columbus, Ohio.E.
Riloff and R. Jones.
1999.
Learning dictionaries for in-formation extraction by multi-level bootstrapping.
In Pro-ceedings of the 16th National Conference on Artificial In-telligence (AAAI-99), pages 474?479, Orlando, Florida.M.
Stevenson and M. Greenwood.
2005.
A semantic ap-proach to IE pattern induction.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL-05), pages 379?386, Ann Arbor, Michigan.M.
Thelen and E. Riloff.
2002.
A bootstrapping method forlearning semantic lexicons using extraction pattern con-texts.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP-02),pages 214?221, Philadelphia, Pennsylvania.E.M.
Voorhees and D.M.
Tice.
2000.
Building a question-answering test collection.
In Proceedings of the 23rdInternational Conference on Research and Developmentin Information Retrieval (SIGIR-00), pages 200?207,Athens, Greece.816
