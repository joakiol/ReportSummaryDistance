Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1166?1169,Prague, June 2007. c?2007 Association for Computational LinguisticsStructural Correspondence Learning for Dependency ParsingNobuyuki ShimizuInformation Technology CenterUniversity of TokyoTokyo, Japanshimizu@r.dl.itc.u-tokyo.ac.jpHiroshi NakagawaInformation Technology CenterUniversity of TokyoTokyo, Japannakagawa@dl.itc.u-tokyo.ac.jpAbstractFollowing (Blitzer et al, 2006), we presentan application of structural correspondencelearning to non-projective dependency pars-ing (McDonald et al, 2005).
To induce thecorrespondences among dependency edgesfrom different domains, we looked at ev-ery two tokens in a sentence and examinedwhether or not there is a preposition, a de-terminer or a helping verb between them.Three binary linear classifiers were trainedto predict the existence of a preposition,etc, on unlabeled data and we used singu-lar value decomposition to induce new fea-tures.
During the training, the parser wastrained with these additional features in ad-dition to these described in (McDonald etal., 2005).
We discriminatively trained ourparser in an on-line fashion using a vari-ant of the voted perceptron (Collins, 2002;Collins and Roark, 2004; Crammer andSinger, 2003).1 IntroductionWe have recently seen growing popularity of depen-dency parsing.
It is no longer rare to see dependencyrelations used as features, in tasks such as machinetranslation (Ding and Palmer, 2005) and relation ex-traction (Bunescu and Mooney, 2005).
However,there is one factor that prevents the use of depen-dency parsing: sparseness of annotated corpora out-side Wall Street Journal.
In many situations we needto parse sentences from a target domain with no la-beled data, which is a different distribution from asource domain where plentiful labeled training datais available.In this paper, we investigate the effectiveness ofstructural correspondence learning (SCL) (Blitzeret al, 2006) in the domain adaptation task given bythe CoNLL 2007.
They hypothesize that a modeltrained in the source domain using this common fea-ture representation will generalize better to the tar-get domain, and focus on using unlabeled data fromboth the source and target domains to learn a com-mon feature representation that is meaningful acrossboth domains.The paper is structured as follows: in section2, we review the decoding and learning aspects of(McDonald et al, 2005), in section 3, structural cor-respondence learning applied to dependency pars-ing, and in section 4, we describe the experimentsand the features needed for the CoNLL 2006 sharedtask.2 Non-Projective Dependency Parsing2.1 Dependency StructureLet us define x to be a generic sequence of input to-kens together with their POS tags and other morpho-logical features, and y to be a generic dependencystructure, that is, a set of edges for x.A labeled edge is a tuple ?DEPREL, i ?
j?
wherei is the start point of the edge, j is the end point, andDEPREL is the label of the edge.
The token at i isthe head of the token at j.Table 1 shows our formulation of a structured pre-diction problem.
Given x, the input tokens and theirfeatures (column 2 and 3, Table 1), the task is to pre-1166Index Token POS Labeled Edge1 John NN ?SUBJ, 2 ?
1?2 saw VBD ?PRED, 0 ?
2?3 a DT ?DET, 4 ?
3?4 dog NN ?OBJ, 2 ?
4?5 yesterday RB ?ADJU, 2 ?
5?6 which WDT ?MODWH, 7 ?
6?7 was VBD ?MODPRED, 4 ?
7?8 a DT ?DET, 10 ?
8?9 Yorkshire NN ?MODN, 10 ?
9?10 Terrier NN ?OBJ, 7 ?
10?11 .
.
?., 10 ?
11?Table 1: Example Edgesdict y, the set of labeled edges (column 4, Table 1).In this paper we use the common method of fac-toring the score of the dependency structure as thesum of the scores of all the labeled edges.
A de-pendency structure is characterized by its labelededges, and for each labeled edge, we have featuresand corresponding weights.
The score of a depen-dency structure is the sum of these weights.For example, let us say we would like to find thescore of the labeled edge ?OBJ, 2 ?
4?.
This is theedge going to the 4th token ?dog?
in Table 1.
Thefeatures for this edge could be:?
There is an edge starting at saw, with the POS tag VBD,and the distance between the head and the child is 2.
(head = wordj , headPOS = posj , dist(i, j) = |i?
j| )?
There is an edge ending at dog, with the POS tag NN,and the distance between the head and the child is 2.
(child = wordi, childPOS = posi, dist(i, j) = |i?
j| )In the upcoming section, we explain a decodingalgorithm for the dependency structures, and laterwe give a method for learning the weight vector usedin the decoding.2.2 Maximum Spanning Tree AlgorithmAs in (McDonald et al, 2005), we use Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Ed-monds, 1967) for decoding.
CLE finds the Maxi-mum Spanning Tree in a directed graph.
The follow-ing is a summary given in (McDonald et al, 2005).Informally, the algorithm has each vertex in thegraph greedily select the incoming edge with high-est weight.Note that the edge is coming from the parent tothe child.
That is, given a child node wordj , we arefinding the parent, or the head wordi such that theedge (i, j) has the highest weight among all i, i 6= j.If a tree results, then this must be the maximumspanning tree.
If not, there must be a cycle.
Theprocedure identifies a cycle and contracts it into asingle vertex and recalculates edge weights goinginto and out of the cycle.
It can be shown that amaximum spanning tree on the contracted graph isequivalent to a maximum spanning tree in the orig-inal graph (Leonidas, 2003).
Hence the algorithmcan recursively call itself on the new graph.2.3 Online LearningAgain following (McDonald et al, 2005), we haveused the single best MIRA (Crammer and Singer,2003), which is a ?margin aware?
variant of percep-tron (Collins, 2002; Collins and Roark, 2004) forstructured prediction.
In short, the update is exe-cuted when the decoder fails to predict the correctparse, and we compare the correct parse yt and theincorrect parse y?
suggested by the decoding algo-rithm.
The weights of the features in y?
will be low-ered, and the weights of the features in yt will beincreased accordingly.3 Domain AdaptationFollowing (Blitzer et al, 2006), we present an appli-cation of structural correspondence learning (SCL)to non-projective dependency parsing (McDonaldet al, 2005).
SCL is a method for adapting a clas-sifier learned in a source domain to a target domain.We assume that both domains have unlabeled data,but only the source domain has labeled training data.SCL works as follows: 1.
Define a set of pivotfeatures on the unlabeled data from both domains.
2.Use these pivot features to learn a mapping from theoriginal feature spaces of both domains to a shared,low-dimensional real-valued feature space.
A highinner product in this new space indicates a high de-gree of correspondence.
3.
Use both the transformedand original features from the source domain.
4.Again using both the transformed and original fea-tures, test the samples from the target domain.
If welearned a good mapping, then the effectiveness ofthe classifier in the source domain should transfer tothe target domain.To induce the correspondences among depen-dency edges in the source domain and the targetdomain, we looked at every two tokens in a sen-tence and examined whether or not there is a prepo-sition, a determiner or a helping verb between them.Although no edge is present in unlabeled data, the1167presence of a preposition indicates that this edge be-tween the tokens, if existed, will not be a noun mod-ifier (in English corpus, this label is NMOD).
Thus,this induced feature should correlate with the labelof an edge candidate.
We postulate that the label ofan edge candidate, if known, may allow the super-vised learner to choose the correct edge among theedge candidates in the target domain.In the first step, we chose the presence of a prepo-sition, a determiner or a helping verb between tokensas pivot features.
Then three binary linear classifierswere trained to predict the existence of a preposi-tion (prep), determiner (det) and helping verb (hv)on unlabeled data and obtained a weight vector foreach classifier.classifierprep(e) = sign(wprep?
(e))classifierdet(e) = sign(wdet?
(e))classifierhv(e) = sign(whv?
(e))The input to the above classifiers is an edge e in-stead of a whole sentence x. ?
is a mapping froman edge to a feature vector.
Since POS tags werenot available in unlabeled data, for pivot predictors,we took the subset of the features given by an edge.The features for pivot predictors are listed in Table 2.The reminder of the features are the same as onesused in (McDonald et al, 2005).Using each weight vector as a column, we createda weight matrix.
W = [wprep|wdet|whv ].
And run asingular value decomposition to induce a lower di-mensional feature space.
W = U?V .
We then tookthe transpose of the resulting unitary matrix, U?which maps the original data to the space spannedby the principal components, and applied it to thefeature vector of every potential edge.
The origi-nal feature vector is(fsubsetfreminder).
We argument thefeature vector with the additional feature induced byU?.
The augmented feature vectors( fsubsetfreminderU?fsubset)were used throughout the training and testing of thedependency parser.4 ExperimentsOur experiments were conducted on CoNLL-2007shared task domain adaptation track (Nivre et al,2007) using treebanks (Marcus et al, 1993; Johans-son and Nugues, 2007; Kulick et al, 2004).Given an edge ?DEPREL, i, j?head?1 = wordi?1head = wordihead+1 = wordi+1child?1 = wordj?1child = wordjchild+1 = wordj+1Table 2: Binary Features for Pivot Predictors4.1 Dependency RelationThe CLE algorithm works on a directed graph withunlabeled edges.
Since the CoNLL shared taskrequires the labeling of edges, as a preprocessingstage, we created a directed complete graph.
Thenwe labeled each edge with the highest scoring de-pendency relation.
This complete graph was givento the CLE algorithm and the edge labels were neveraltered in the course of finding the maximum span-ning tree.4.2 FeaturesThe features we used for pivot predictors to classifyeach edge ?DEPREL, i, j?
are shown in Table 2.
Theindex i is the position of the parent and j is that ofthe child.wordj = the word token at the position j.posj = the coarse part-of-speech at j.No other features were used beyond the combina-tions of the word token in Table 2.The hardware used was an Intel CPU at 3.0 Ghzwith 32 GB of memory, and the software was writ-ten in C++.
While more iterations should help, dueto the time constraints, we were unable to completemore training.
The parser required a few days totrain.5 ResultsUnfortunately, we have discovered a bug in ourcodes after submitting our results for the blind tests,and the reported results in (Nivre et al, 2007) werenot representative of our approach.
The current re-sults (closed class) are shown in Table 3.For the explanations of Labeled AttachmentScore, Unlabeled Attachment Score and Label Ac-curacy, the readers are suggested to refer to theshared task introductory paper (Nivre et al, 2007).WSJ represents the application of the parser withoutSCL to the source domain test set, and WSJ-SCLthe parser with SCL to the same test set.
Similarily1168Domain LAS UAS Label AccuracyWSJ 83.01% ?
83.43% 86.43% ?
86.81% 88.77% ?
88.99%WSJ-SCL 83.43% ?
83.59% 86.87% ?
86.93% 88.75% ?
89.01%Chem 74.75% ?
75.18% 80.74% ?
81.24% 82.34% ?
82.70%Chem-SCL 75.04% ?
74.91% 81.02% ?
80.82% 82.18% ?
82.18%Table 3: Labeled Attachment Score, Unlabeled Attachment Score and Label AccuracyChem and Chem-SCL represents the application ofthe parser without SCL and with SCL to the sourcedomain test set respectively.
We did batch learn-ing by running the online algorithm 4 times.
Anarrow ?
indicates how the results after 2nd itera-tion changed at the end of 4th iteration.
Contraryto our expectations, we seem to see SCL overfittingto the source domain WSJ in this experiment.
Dueto the lack of POS tags in unlabeled data, our fea-ture set for pivot predictors uses tokens extensivelyunlike that for the dependency parser.
Since tokensare not as abstract as POS tags, we suspect inducedfeatures may have caused overfitting.6 ConclusionWe presented an application of structural correspon-dence learning to non-projective dependency pars-ing.
Effectiveness of SCL for domain adaptation ismixed in this experiment perhaps due to the mis-match between feature sets.
Future work includesuse of more sophisticated features such as POS andother morphological features, possibly a joint do-main adaptation of POS tagging and dependencyparsing for unlabeled data as well as re-examinationof pivot features.ReferencesJ.
Blitzer, R. McDonald, and F. Pereira.
2006.
Domainadaptation with structural correspondence learning.
InProc.
of Empirical Methods in Natural Language Pro-cessing (EMNLP).R.
Bunescu and R. Mooney.
2005.
A shortest path de-pendency kernel for relation extraction.
In Proc.
ofthe Joint Conf.
on Human Language Technology andEmpirical Methods in Natural Language Processing(HLT/EMNLP).Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
In Science Sinica, page14:13961400.M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proc.
of the 42rd AnnualMeeting of the ACL.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proc.
of Empirical Methodsin Natural Language Processing (EMNLP).K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
In JMLR.Y.
Ding and M. Palmer.
2005.
Machine translation usingprobabilistic synchronous dependency insertion gram-mars.
In Proc.
of the 43rd Annual Meeting of the ACL.J.
Edmonds.
1967.
Optimum branchings.
In Journal ofResearch of the National Bureau of Standards, page71B:233240.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human Language Technol-ogy Conference and the Annual Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT/NAACL).G.
Leonidas.
2003.
Arborescence optimization problemssolvable by edmonds algorithm.
In Theoretical Com-puter Science, page 301:427 437.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
of the Joint Conf.
on Hu-man Language Technology and Empirical Methods inNatural Language Processing (HLT/EMNLP).J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
of theJoint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).1169
