Exploiting Social Q&A Collection in Answering Complex QuestionsYouzheng Wu Hisashi KawaiSpoken Language Communication Group, MASTAR ProjectNational Institute of Information and Communications Technology2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, Japan{youzheng.wu, hisashi.kawai}@nict.go.jpAbstractThis paper investigates techniques to au-tomatically construct training data fromsocial Q&A collections such as Yahoo!Answer to support a machine learning-based complex QA system1.
We extractcue expressions for each type of questionfrom collected training data and buildquestion-type-specific classifiers to im-prove complex QA system.
Experimentson 10 types of complex Chinese ques-tions verify that it is effective to mineknowledge from social Q&A collectionsfor answering complex questions, for in-stance, the F3 improvement of our sys-tem over the baseline and translation-based model reaches 7.9% and 5.1%, re-spectively.1 IntroductionResearch on the topic of QA systems has mainlyconcentrated on answering factoid, definitional,reason and opinion questions.
Among the ap-proaches proposed to answer these questions,machine learning techniques have been foundmore effective in constructing QA componentsfrom scratch.
Yet these supervised techniques re-quire a certain scale of (question, answer), shortfor Q&A, pairs as training data.
For example,(Echihabi et al, 2003) and (Sasaki, 2005) con-structed 90,000 English Q&A pairs and 2,000Japanese Q&A pairs, respectively for their fac-toid QA systems.
(Cui et al, 2004) constructed1Complex questions cannot be answered by simply ex-tracting named entities.
In this paper complex questions donot include definitional questions.76 term-definition pairs for their definitional QAsystems.
(Stoyanov et al, 2005) required aknown subjective vocabulary for their opinionQA.
(Higashinaka and Isozaki, 2008) used 4,849positive and 521,177 negative examples in theirreason QA system.
Among complex QA sys-tems, many other types of questions have notbeen well studied, apart from reason and defi-nitional questions.
Appendix A lists 10 types ofcomplex Chinese questions and their exampleswe discussed in this paper.According to the related studies on QA, su-pervised machine-learning technique may be ef-fective for answering these questions.
To em-ploy the supervised approach, we need to re-construct training Q&A pairs for each type ofquestion, though this is an extremely expensiveand labor-intensive task.
To deal with the ac-quisition problem of training Q&A pairs, we in-vestigate techniques to automatically constructtraining data by utilizing social Q&A collectionscrawled from the Web, which contains millionsof user-generated Q&A pairs.
Many studies(Surdeanu et al, 2008) (Duan et al, 2008) havebeen done on retrieving similar Q&A pairs fromsocial QA websites as answers to test questions.Our study, however, regards social Q&A web-sites as a knowledge repository and aims to mineknowledge from them for synthesizing answersto questions from multiple documents.
There isvery little literature on this aspect.
Our work canbe seen as a kind of query-based summarization(Dang, 2006) (Harabagiu et al, 2006) (Erkanand Radev, 2004), and can also be employed toanswer questions that have not been answered insocial Q&A websites.This paper mainly focuses on the following threesteps: (1) automatically constructing question -type-specific training Q&A pairs from the so-cial Q&A collection; (2) extracting cue expres-sions for each type of question from the col-lected training data, and (3) building question-type-specific classifiers to filer out noise sen-tences before using a state-of-the-art IR formulato select answers.We evaluate our system on 10 types of Chi-nese questions by using the Pourpre evalua-tion tool (Lin and Demner-Fushman, 2006).The experimental results show the effectivenessof our system, for instance, the F3/NR im-provement of our system over the baseline andtranslation-based model reaches 7.9%/11.1%,and 5.1%/5.6%, respectively.2 Social Q&A CollectionRecently launched social QA websites such asYahoo!
Answer2 and Baidu Zhidao3 providean interactive platform for users to post ques-tions and answers.
After questions are answeredby users, the best answer can be chosen by theasker or nominated by the community.
The num-ber of Q&A pairs on such sites has risen dra-matically.
These pairs could collectively form asource of training data that is required in super-vised machine-learning-based QA systems.In this paper we aim to explore such user-generated Q&A collections to automatically col-lect Q&A training data.
However, social col-lections have two salient characteristics: tex-tual mismatch between questions and answers(i.e., question words are not necessarily usedin answers); and user-generated spam or flip-pant answers, which are unfavorable factors inour study.
Thus, we only crawl questions andtheir best answers to form Q&A pairs, whereinthe best answers are longer than the empiri-cal threshold.
Finally, 60.0 million Q&A pairswere crawled from Chinese social QA websites.These pairs will be used as the source of trainingdata required in our study.2http://answers.yahoo.com/3http://zhidao.baidu.com/3 Our Complex QA SystemThe typical complex QA system architecture isa cascade of three modules.
The Question Ana-lyzer analyzes test questions and identifies an-swer types of questions.
The Document Re-triever & Answer Candidate Extractor retrievesdocuments related to questions from the givencollection (Xinhua and Lianhe Zaobao newspa-pers from 1998-2001 were used in this study) forconsideration, and segments the documents intosentences as answer candidates.
The Answer Ex-traction module applies state-of-the-art IR for-mulas (e.g., KL-divergence language model) todirectly estimate similarities between sentences(1,024 sentences were used in our case) andquestions, and selects the most similar sentencesas the final answers.
Given three answer candi-dates, s1 = ?Solutions to global warming rangefrom changing a light bulb to engineering giantreflectors in space ...?, s2 = ?Global warmingwill bring bigger storms and hurricanes that willhold more water ...?, and s3 = ?nuclear poweris the relatively low emission of carbon diox-ide (CO2), one of the major causes of globalwarming,?
to the question of ?What are the haz-ards of global warming?
?, however, it is hard forthis architecture to select the correct answer, s2,because the three candidates contain the samequestion words ?global warming?.According to our observation, answers to atype of question usually contain some type-of-question dependent cue expressions (?willbring?
in this case).
This paper argues thatthe above QA system can be improved by us-ing such question-type-specific cue expressions.For each test question, we perform the follow-ing three steps.
(1) Collecting question-type-specific Q&A pairs from the social Q&A collec-tion which question types are same as the testquestion to form positive training data.
Sim-ilarly, negative Q&A pairs are also collectedwhich question types are different from thetest question.
(2) Extracting and weightingquestion-type-specific cue expressions from thecollected Q&A pairs.
(3) Building a question-type-specific classifier by employing the cue ex-pressions and the collected Q&A pairs, which re-moves noise sentences from answer candidatesbefore using the Answer Extraction module.3.1 Collecting Q&A PairsWe first introduce the notion of the answer typeinformer of the question as follows.
In a ques-tion, a short subsequence of tokens (typically 1-3 words) that are adequate for question classi-fication is considered an answer-type informer,e.g., ?hazard?
in the question of ?What are thehazards of global warming??
This paper makesthe following assumption: type of complex ques-tion is determined by its answer type informer.For example, the question of ?What are the haz-ards of global warming??
belongs to hazard-typequestion, because its answer type informer is?hazard?.
Therefore, the task of recognizingquestion-types is shifted to identifying answertype informer of question.In this paper, we regard answer-type informerrecognition as a sequence tagging problem andadopt conditional random fields (CRFs) becausemany work has shown that CRFs have a con-sistent advantage in sequence tagging.
Wemanually label 3,262 questions with answer-type informers to train a CRF, which classi-fies each question word into a set of tags O ={IB , II , IO}: IB for a word that begins an in-former, II for a word that occurs in the mid-dle of an informer, and IO for a word thatis outside of an informer.
In the followingfeature templates used in the CRF model, wnand tn, refer to word and PoS, respectively;n refers to the relative position from the cur-rent word n=0.
The feature templates in-clude the following four types: unigrams ofwn and tn, where n=?2,?1, 0, 1, 2; bigramsof wnwn+1 and tntn+1, where n=?1, 0; tri-grams of wnwn+1wn+2 and tntn+1tn+2, wheren=?2,?1, 0; and bigrams of OnOn+1, wheren=?1, 0.The trained CRF model is then employed torecognize answer-type informers from questionsof social Q&A pairs.
Finally, we recognized 103answer-type informers in which frequencies arelarger than 10,000.
Moreover, the numbers ofanswer type informers for which frequencies arelarger than 100, 1,000, and 5,000 are 2,714, 807,and 194, respectively.Based on answer-type informers of questionsrecognized, we can collect training data for eachtype of question as follows: (1) Q&A pairs aregrouped together in cases in which the answer-type informers X of their questions are the same,and (2) Q&A pairs clustered by informers Xare regarded as the positive training data ofX-type questions.
For instance, 10,362 Q&Apairs grouped via informer X (=?hazard?)
areregarded as positive training data of answeringhazard-type questions.
Table 1 lists some ques-tions, which, together with their best answers,are employed as the training data of the corre-sponding type of questions.
For each type ofquestion, we also randomly select some Q&Apairs that do not contain informers in questionsas negative training data.
Preprocessing of thetraining data, including word segmentation, PoStagging, and named entity (NE) tagging (Wu etal., 2005), is conducted.
We also replace eachNE with its tag type.Qtype Questions of Q&A pairsHazard-typeWhat are the hazards of the tro-jan.psw.misc.kah virus?What are the hazards of RMB appreciationon China?s economy?Hazards of smokeWhat are the hazards of contact lenses?What are the hazards of waste accumula-tion?Casualty-typeWhat were the casualties on either side fromthe U.S.-Iraq war?What were the casualties of the Sino-FrenchWar?What were the casualties of the Sichuanearthquake in 2008?What were the casualties of highway acci-dents over the years?What were the casualties of the Ryukyu Is-lands tsunami?Reason-typeWhat are the main reasons of China?s watershortage?What are the reasons of asthma?What are the reasons of blurred photos?What are the reasons of air pollution?The reasons for the soaring prices!Table 1: Questions (translated from Chinese) ofsocial Q&A pairs (words in bold denote answer-type informers of questions).
These questionsand their best answers are regarded as positivetraining data for hazard-type question.3.2 Cue ExpressionsWe extract lexical and PoS-based n-grams as cueexpressions from the collected training data.
Toreduce the dimensionality of the cue expressionspace, we first select the top 3,000 lexical un-igrams using the formula: scorew = tfw ?log(idfw), where tf(w) denotes the frequency ofword w, and idf(w) represents the inverted doc-ument frequency of w that indicates its globalimportance.
Table 2 shows some of the learnedunigrams.
The top 300 unigrams are then used asseeds to learn lexical bigrams and trigrams iter-atively.
Only lexical bigrams and trigrams thatcontain seed unigrams with frequencies largerthan the thresholds are retained as lexical fea-tures.
Moreover, we extract PoS-based unigramsand bigrams as cue expressions.Further, we assign each extracted feature si aweight calculated using the equation weightsi =csi1 /(csi1 + csi2 ), where, csi1 and csi2 denote its fre-quencies in positive and negative training Q&Apairs, respectively.Qtype Top UnigramsHazard-type ?3/hazard s?/lead to ?/causeZ?/give rise to ?
/bring about k//influence?3/damageCasualty-type ?
}/casualty ?
}/death I?/hurt/missing ?/wrecked j}/diein battle?
?/woundedTable 2: Top unigrams learned from hazard-typeand casualty-type Q&A pairs3.3 ClassifiersAs mentioned above, we use the extracted cueexpressions and the collected Q&A pairs to buildquestion-type-specific classifiers, which is usedto remove noise sentences from answer candi-dates.
For classifiers, we employ multivariateclassification SVMs (Thorsten Joachims, 2005)that can directly optimize a large class of perfor-mance measures like F1-Score, prec@k (preci-sion of a classifier that predicts exactly k = 100examples to be positive) and error-rate (percent-age of errors in predictions).
Instead of learn-ing a univariate rule that predicts the label of asingle example in conventional SVMs (Vapnik,1998), multivariate SVMs formulate the learn-ing problem as a multivariate prediction of allexamples in the data set.
Considering hypothe-ses h that map a tuple x of n feature vectorsx = (x1, ...,xn) to a tuple y of n labels y =(y1, ..., yn), multivariate SVMs learn a classifierhw(x) = argmaxy?
?Y {wT?
(x, y?)}
(1)by solving the following optimization problem.minw,?
?012?w?2 +C?
(2)s.t.
: ?y?
?
Y \y : wT [?
(x, y) ?
?
(x, y?)]?
?
(y?, y) ?
?
(3)where, w is a parameter vector, ?
is a functionthat returns a feature vector describing the matchbetween (x1, ...,xn) and (y?1, ..., y?n), ?
denotestypes of multivariate loss functions, and ?
is aslack variable.4 ExperimentsThe NTCIR 2008 test data set (Mitamura et al,2008) contains 30 complex questions4 we dis-cussed here.
However, a small number of testquestions are included for some question types,e.g.
; it contains only 1 hazard-type, 1 scale-type,and 3 significance-type questions.
To form amore complete test set, we create another 65 testquestions5 .
Therefore, the test data used in thispaper includes 95 complex questions.For each test question we also provide a listof weighted nuggets, which are used as the goldstandard answers for evaluation.
The evaluationis conducted by employing Pourpre v1.0c (Linand Demner-Fushman, 2006), which uses thestandard scoring methodology for TREC otherquestions (Voorhees, 2003), i.e., answer nuggetrecall NR, nugget precision NP , and a combi-nation score F3 of NR and NP .
For better un-derstanding, we evaluate the systems when out-putting the top N sentences as answers.4Because definitional, biography, and relationship ques-tions in the NTCIR 2008 test set are not discussed here.5The approach of creating test data is same as that in theNTCIR 2008.F3 (%) NR (%) NP (%)N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 N = 1 N = 5 N = 10Baseline 9.82 18.18 21.95 9.44 19.85 27.64 34.35 25.32 18.96TransM 9.76 20.47 24.76 9.44 19.85 33.10 31.96 21.73 13.57Ourslin 10.92 22.61 25.74 10.49 25.95 34.70 34.98 23.40 15.11Ourserrorrate 12.37 23.10 27.74 12.05 26.98 37.03 33.22 26.48 18.67Ourspre@k 8.96 22.85 29.85 8.72 25.67 38.78 26.28 28.82 20.45Table 3: Overall performance for the test data4.1 Overall ResultsTable 3 summarizes the evaluation results forseveral N values.
The baseline refers to the con-ventional method introduced in Section 3, whichdoes not employ question-type-specific classi-fiers before the Answer Extraction.
The baselinecan be expressed by the formula:sim(q, s) = ?Vq ?
Vs??Vq?
?
?Vs?
(4)where, Vq and Vs are the vectors of the ques-tion and candidate answer.
The TransM de-notes a translation model for QA (Xue, et al,2008) (Bernhard et al, 2009), which uses Q&Apairs as the parallel corpus, with questions to the?source?
language and answers corresponding tothe ?target?
language.
This model can be ex-pressed by:P (q|S) =?w?q((1 ?
?
)Pmx(w|S) + ?Pml(w|C))Pmx(w|S) = (1 ?
?)Pml(w|S)+?
?t?SP (w|t)Pml(t|S)(5)where, q is the question, S the sentence, P (w|t)the probability of translating a sentence term t tothe question term w, which is obtained by usingthe GIZA++ toolkit (Och and Ney, 2003).
Weuse six million Q&A pairs to train IBM model 1for obtaining word-to-word probability P (w|t).Ourserrorrate and Ourspre@k denote our modelsthat are based on classifiers optimizing perfor-mance measure error-rate and prec@k, respec-tively.
Ourslin, a linear interpolation model, thatcombines scores of classifiers and the baseline,which is similar to (Mori et al, 2008) and can beexpressed by the equation:sim(q, s)?
= sim(q, s) + ?
?
?
(s) (6)where, ?
(s) is the score calculated by classi-fiers (Thorsten Joachims, 2005) and ?
denotesthe weight of the score.This experiment shows that: (1) Question-type-specific classifiers can greatly outperformthe baseline; for example, the F3 improvementsof Ourserrorrate and Ourspre@k over the base-line in terms of N=10 are 5.8% and 7.9%,respectively.
(2) Ourserrorrate is better thanOurspre@k when N < 10.
The average num-bers of sentences retained in Ourserrorrate andOurspre@k are 130, and 217, respectively.
Thatmeans the precision of the classifier optimiz-ing errorrate is superior to the classifier optimiz-ing prec@k, while the recall is relatively infe-rior.
(3) Ourslin is worse than Ourserrorrate andOurspre@k, which indicates that using question-type-specific classifiers by classification is betterthan using it by interpolation like (Mori et al,2008).
(4) Our models also outperform TransM,e.g.
; the F3 improvement is 5.1% when N isset to 10.
TransM exploits the social Q&A col-lection without consideration of question types,while our models select and exploit the socialQ&A pairs of the same question types.
Thereby,this experiment also indicates that it is better toexploit social Q&A pairs by type of question.The performance ranking of these models whenN=10 is: Oursprec@k > Ourserrorrate > Ourslin> TransM > Baseline.4.2 Impact of FeaturesIn order to evaluate the contributions of indi-vidual features to our models, this experimentis conducted by gradually adding them.
Table4 summarizes the performance of Ourprec@k ondifferent set of features, L and P represent lex-ical and PoS-based features, respectively.
Thistable demonstrates that all the lexical and PoSfeatures can positively impact Ourprec@k, espe-cially, the contribution of the PoS-based featuresis largest.Features F3 NR NPLunigram 23.44 31.23 17.32+Lbigram +Ltrigram 25.34 33.15 18.87+Punigram 28.24 36.27 20.18+Pbigram 29.85 38.78 20.45Table 4: Impact of features on Ourprec@k.4.3 ImprovementAs discussed in Section 2, the writing style ofsocial Q&A collections slightly differs from thatof our complex QA system, which is an unfavor-able circumstance in utilizing social Q&A col-lections.
For better understanding we randomlyselect 100 Q&A training pairs of each type ofquestion acquired in Section 3, and manuallyclassify each Q&A pair into NON-NOISE andNOISE6 categories.
Figure 1 reports the percent-age of NON-NOISE.
This figure indicates that71% of the training pairs of the scale-type ques-tions are noises, which may lead to a small im-provement.0.870.790.860.5 0.510.790.54 0.580.850.2900.10.20.30.40.50.60.70.80.91Figure 1: Percentage of NON-NOISE pairs bytype of questions.To further improve the performance, we em-6NOISE means that the Q&A pair is not useful in ourstudy.ploy k-fold cross validation to remove noisesfrom the collected training data in Section 3.1.Specifically, the collected training data are firstdivided into k (= 5) sets.
Secondly, k-1 sets areused to train classifiers that are applied to clas-sify the Q&A pairs in the remaining set.
Finally,part of the Q&A pairs classified as negative pairsare removed7.
According to Figure 1, we re-move 20% of the training data from the nega-tive pairs for the hazard-type, impact-type, andfunction-type questions, and 40% of the train-ing data for significance-type, event-type, andreason-type questions.
Because the sizes of thetraining pairs of the other four types of ques-tions are small, we do not use this approach onthem.
Table 5 shows the results of Ourspre@k onthe above six types of questions.
The numbersin brackets indicate absolute improvements overthe system based on the data without removingnoises.
N is the number of answer sentences to aquestion.
The experiment shows that the perfor-mance is generally improved by removing noisein the training Q&A pairs using k-fold cross-validation.F3 (%) NR (%) NP (%)N = 1 9.6+2.1 9.3+2.0 30.8+7.4N = 5 21.6+0.7 24.9+1.2 26.0?1.3N = 10 28.6+0.9 37.9+1.7 19.2?0.2Table 5: Performance of Ourspre@k after remov-ing noises in the training Q&A pairs.4.4 Subjective evaluationPourpre v1.0c evaluation is based on n-gramoverlap between the automatically produced an-swers and the human generated reference an-swers.
Thus, it is not able to measure concep-tual equivalent.
In subjective evaluation, the an-swer sentences returned by systems are labeledby a native Chinese assessor.
Figure 2 shows thedistribution of the ranks of the first correct an-swers for all questions.
This figure demonstratesthat the Ourspre@k answers 57 questions which7We do not remove all negative Q&A pairs to ensurethe coverage of training data because the classifiers haverelatively lower recall, as mentioned in Section 3.3.first answers are ranked in top 3, which is largerthan that of the baseline, i.e., 49.
Moreover,the Ourspre@k contains only 11.5% of questionswhich answers are ranked after top 10, while thisnumber of the baseline is 20.7%.26161560323 2 4309 1016821 2 0051015202530351 2 3 4 5 6 7 8 9 10Ourprec@kBaselineFigure 2: Distribution of the ranks of first an-swers.5 Related WorkRecently, some pioneering studies on the socialQ&A collection have been conducted.
Amongthem, much of the research aims to retrieve an-swers to queried questions from the social Q&Acollection.
For example, (Surdeanu et al, 2008)proposed an answer ranking engine for non-factoid questions by incorporating textual fea-tures into a machine learning approach.
(Duanet al, 2008) proposed searching questions se-mantically equivalent or close to the queriedquestion for a question recommendation sys-tem.
(Agichtein et al, 2008) investigated tech-niques of finding high-quality content in the so-cial Q&A collection, and indicated that 94% ofanswers to questions with high quality have highquality.
(Xue, et al, 2008) proposed a retrievalmodel that combines a translation-based lan-guage model for the question part with a querylikelihood approach for the answer part.Another category of study regards the socialQ&A collection as a kind of knowledge reposi-tory and aims to mine knowledge from it for gen-erating answers to questions.
To the best of ourknowledge, there is very limited work reportedon this aspect.
This paper is similar to (Mori etal., 2008), but different from it as follows.
(1)(Mori et al, 2008) collects training data for eachtest question using 7-grams for which centers areinterrogatives, while this paper collects trainingdata for each type of question using answer typeinformers.
(2) About the knowledge learned,we extract lexical/class-based, PoS-based uni-grams, bigrams, and trigrams.
(Mori et al, 2008)only extracts lexical bigrams.
(3) They incor-porated knowledge learned by interpolating withthe baseline.
However, we utilize the learnedknowledge to train a binary classifier, which canremove noise sentences before answer selection.6 ConclusionThis paper investigated a technique for miningknowledge from social Q&A websites for im-proving a sentence-based complex QA system.More specifically, it explored a social Q&A col-lection to automatically construct training data,and created question-type-specific classifier foreach type of question to filter out noise sentencesbefore answer selection.The experiments on 10 types of complex Chi-nese questions show that the proposed approachis effective; e.g., the improvement in F3 reaches7.9%.
In the future, we will endeavor to reduceNOISE pairs in the training data, and to extracttype-of-question dependent features.
Future re-search tasks also include adapting the QA systemto a topic-based summarization system, which,for example, summarizes accidents according to?casualty?, ?reason?, and summarizes events ac-cording to ?reason?, ?measure,?
?impact?, etc.Appendix A.
Examples of 10 Types of Ques-tions.ReferencesAbdessamad Echihabi and Daniel Marcu.
2003.
ANoisy-Channel Approach to Question Answering.In Proc.
of ACL 2003, Japan.Delphine Bernhard and Iryna Gurevych.
2009.
Com-bining Lexical Semantic Resources with Question& Answer Archives for Translation-based AnswerFinding.
In Proc.
of ACL-IJCNLP 2009, Singa-pore, pp728-736.Ellen M. Voorhees.
2003 Overview of the TREC2003 Question Answering Track.
In Proc.
ofTREC 2003, pp54-68, USA.Qtype Examples?
3/Hazard-type\E?#F{?34?
?Whatare the hazards of global warming?
*~/Function-type?\){*~4?
?What are thefunctions of the United Nations?k //Impact-type??911/G??
){k/Listthe impact of the 911 attacks on theUnited States.
?B/ ???)
?WTO{?BSignificance-typeList the significance of China?s acces-sion to the WTO.?
?/Attitude-type???)??1?B{?
?Listthe attitudes of other countries towardthe Israeli-Palestinian conflict.D/Measure-type????>\0?fR?
?JD?What measures havebeen taken for energy-saving andemissions-reduction in Japan??
O/Reason-type\E?#F{?O4?
?Whatare the reasons for global warming??}/Casualty-type??b.8{?
}List thecasualties of the Lockerbie Air Disas-ter./ G/Event-type????}Z?Z?
?g/GList the events in the NorthernIreland peace process. ?/Scale-type??f?-??2F??
{?Give information about the scaleof the Kunming World HorticultureExposition.Eugene Agichtein, Carlos Castillo, Debora Donato.2008 Finding High-Quality Content in Social Me-dia.
In Proc.
of WSDM 2008, California, USA.Franz J. Och and Hermann Ney.
2003.
A system-atic Comparison of Various Statistical AlignmentModels.
In Computational Linguistics, 29(1):19-51.Gunes Erkan and Dragomir Radev.
2004.
LexRank:Graph-based Lexical Centrality as Salience inText.
In Journal of Artificial IntelligenceResearch,22:457-479.Hang Cui, Min Yen Kan, and Tat Seng Chua.
2004.Unsupervised Learning of Soft Patterns for Defini-tion Question Answering.
In Proc.
of WWW 2004.Hoa Trang Dang.
2006.
Overview of DUC 2006.
InProc.
of TREC 2006.Huizhong Duan, Yunbo Cao, Chin Yew Lin, andYong Yu.
2008.
Searching Questions by Identify-ing Question Topic and Question Focus.
In Proc.of ACL 2008, Canada, pp 156-164.Jimmy Lin and Dina Demner-Fushman.
2006.
WillPyramids Built of Nuggets Topple Over.
In Proc.of HLT/NAACL2006, pp 383-390.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to Rank Answers onLarge Online QA Collections.
In Proc.
of ACL2008, Ohio, USA, pp 719-727.Ryuichiro Higashinaka and Hideki Isozaki.
2008.Corpus-based Question Answering for why-Questions.
In Proc.
of IJCNLP 2008, pp 418-425.Tatsunori Mori, Takuya Okubo, and Madoka Ish-ioroshi.
2008.
A QA system that can answer anyclass of Japanese non-factoid questions and its ap-plication to CCLQA EN-JA task.
In Proc.
of NT-CIR2008, Tokyo, pp 41-48.Sanda Harabagiu, Finley Lacatusu, Andrew Hickl.2006.
Answering Complex Questions with Ran-dom Walk Models.
In Proc.
of the 29th SIGIR, pp220-227, ACM.Ves Stoyanov, Claire Cardie, and Janyce Wiebe.2005.
Multi-Perspective Question Answering Us-ing the OpQA Corpus.
In Proc.
of HLT/EMNLP2005, Canada, pp 923-930.Teruko Mitamura, Eric Nyberg, Hideki Shima,Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin,Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai,Donghong Ji and Noriko Kando.
2008.
Overviewof the NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual Information Access.
In Proc.
of NTCIR2008.Thorsten Joachims.
2005.
A Support Vector Methodfor Multivariate Performance Measures.
In Proc.of ICML2005, pp 383-390.Vladimir Vapnik 1998.
Statistical learning theory.John Wiley.Xiaobing Xue, Jiwoon Jeon, W.Bruce Croft.
2008.Retrieval Models for Question and AnswerArchives.
In Proc.
of SIGIR 2008, pp 475-482.Yutaka Sasaki.
2005.
Question Answering asQuestion-biased Term Extraction: A New Ap-proach toward Multilingual QA.
In Proc.
of ACL2005, pp 215-222.Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu.
2005.Chinese Named Entity Recognition Model basedon Multiple Features.
In Proc.
of HLT/EMNLP2005, Canada, pp 427-434.Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,Dingyi Han, Yong Yu.
2008.
Understandingand Summarizing Answers in Community-BasedQuestion Answering Services.
In Proc.
of COL-ING 2008, Manchester, pp 497-504.
