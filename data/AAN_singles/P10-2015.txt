Proceedings of the ACL 2010 Conference Short Papers, pages 80?85,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEvaluating Machine Translations using mNCDMarcus Dobrinkat and Tero Tapiovaara and Jaakko Va?yrynenAdaptive Informatics Research CentreAalto University School of Science and TechnologyP.O.
Box 15400, FI-00076 Aalto, Finland{marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fiKimmo KettunenKymenlaakso University of Applied SciencesP.O.
Box 9, FI-48401 Kotka, Finlandkimmo.kettunen@kyamk.fiAbstractThis paper introduces mNCD, a methodfor automatic evaluation of machine trans-lations.
The measure is based on nor-malized compression distance (NCD), ageneral information theoretic measure ofstring similarity, and flexible word match-ing provided by stemming and synonyms.The mNCD measure outperforms NCD insystem-level correlation to human judg-ments in English.1 IntroductionAutomatic evaluation of machine translation (MT)systems requires automated procedures to en-sure consistency and efficient handling of largeamounts of data.
In statistical MT systems, au-tomatic evaluation of translations is essential forparameter optimization and system development.Human evaluation is too labor intensive, time con-suming and expensive for daily evaluations.
How-ever, manual evaluation is important in the com-parison of different MT systems and for the valida-tion and development of automatic MT evaluationmeasures, which try to model human assessmentsof translations as closely as possible.
Furthermore,the ideal evaluation method would be language in-dependent, fast to compute and simple.Recently, normalized compression distance(NCD) has been applied to the evaluation ofmachine translations.
NCD is a general in-formation theoretic measure of string similar-ity, whereas most MT evaluation measures, e.g.,BLEU and METEOR, are specifically constructedfor the task.
Parker (2008) introduced BAD-GER, an MT evaluation measure that uses NCDand a language independent word normalizationmethod.
BADGER scores were directly comparedagainst the scores of METEOR and word errorrate (WER).
The correlation between BADGERand METEOR were low and correlations betweenBADGER and WER high.
Kettunen (2009) usesthe NCD directly as an MT evaluation measure.He showed with a small corpus of three languagepairs that NCD and METEOR 0.6 correlated fortranslations of 10?12 MT systems.
NCD was notcompared to human assessments of translations,but correlations of NCD and METEOR scoreswere very high for all the three language pairs.Va?yrynen et al (2010) have extended the workby including NCD in the ACL WMT08 evaluationframework and showing that NCD is correlatedto human judgments.
The NCD measure did notmatch the performance of the state-of-the-art MTevaluation measures in English, but it presented aviable alternative to de facto standard BLEU (Pa-pineni et al, 2001), which is simple and effectivebut has been shown to have a number of drawbacks(Callison-Burch et al, 2006).Some recent advances in automatic MT evalu-ation have included non-binary matching betweencompared items (Banerjee and Lavie, 2005; Agar-wal and Lavie, 2008; Chan and Ng, 2009), whichis implicitly present in the string-based NCD mea-sure.
Our motivation is to investigate whether in-cluding additional language dependent resourceswould improve the NCD measure.
We experimentwith relaxed word matching using stemming anda lexical database to allow lexical changes.
Theseadditional modules attempt to make the referencesentences more similar to the evaluated transla-tions on the string level.
We report an experimentshowing that document-level NCD and aggregatedNCD scores for individual sentences produce verysimilar correlations to human judgments.80Figure 1: An example showing the compressedsizes of two strings separately and concatenated.2 Normalized Compression DistanceNormalized compression distance (NCD) is a sim-ilarity measure based on the idea that a string x issimilar to another string y when both share sub-strings.
The description of y can reference sharedsubstrings in the known x without repetition, in-dicating shared information.
Figure 1 shows anexample in which the compression of the concate-nation of x and y results in a shorter output thanindividual compressions of x and y.The normalized compression distance, as de-fined by Cilibrasi and Vitanyi (2005), is given inEquation 1, with C(x) as length of the compres-sion of x and C(x, y) as the length of the com-pression of the concatenation of x and y.NCD(x, y) =C(x, y)?min {C(x), C(y)}max {C(x), C(y)}(1)NCD computes the distance as a score closer toone for very different strings and closer to zero formore similar strings.NCD is an approximation of the uncomputablenormalized information distance (NID), a generalmeasure for the similarity of two objects.
NIDis based on the notion of Kolmogorov complex-ity K(x), a theoretical measure for the informa-tion content of a string x, defined as the shortestuniversal Turing machine that prints x and stops(Solomonoff, 1964).
NCD approximates NID bythe use of a compressor C(x) that is an upperbound of the Kolmogorov complexity K(x).3 mNCDNormalized compression distance was not con-ceived with MT evaluation in mind, but rather itis a general measure of string similarity.
Implicitnon-binary matching with NCD is indicated bypreliminary experiments which show that NCD isless sensitive to random changes on the characterlevel than, for instance, BLEU, which only countsthe exact matches between word n-grams.
Thuscomparison of sentences at the character levelcould account better for morphological changes.Variation in language leads to several accept-able translations for each source sentence, whichis why multiple reference translations are pre-ferred in evaluation.
Unfortunately, it is typicalto have only one reference translation.
Paraphras-ing techniques can produce additional translationvariants (Russo-Lassner et al, 2005; Kauchak andBarzilay, 2006).
These can be seen as new refer-ence translations, similar to pseudo references (Maet al, 2007).The proposed method, mNCD, works analo-gously to M-BLEU and M-TER, which use theflexible word matching modules from METEORto find relaxed word-to-word alignments (Agar-wal and Lavie, 2008).
The modules are able toalign words even if they do not share the samesurface form, but instead have a common stem orare synonyms of each other.
A similarized transla-tion reference is generated by replacing words inthe reference with their aligned counterparts fromthe translation hypothesis.
The NCD score is com-puted between the translations and the similarizedreferences to get the mNCD score.Table 1 shows some hand-picked German?English candidate translations along with a) thereference translations including the 1-NCD scoreto easily compare with METEOR and b) the simi-larized references including the mNCD score.
Forcomparison, the corresponding METEOR scoreswithout implicit relaxed matching are shown.4 ExperimentsThe proposed mNCD and the basic NCD measurewere evaluated by computing correlation to hu-man judgments of translations.
A high correlationvalue between an MT evaluation measure and hu-man judgments indicates that the measure is ableto evaluate translations in a more similar way tohumans.Relaxed alignments with the METEOR mod-ules exact, stem and synonym were createdfor English for the computation of the mNCDscore.
The synonym module was not availablewith other target languages.4.1 Evaluation DataThe 2008 ACL Workshop on Statistical MachineTranslation (Callison-Burch et al, 2008) sharedtask data includes translations from a total of 30MT systems between English and five Europeanlanguages, as well as automatic and human trans-81Candidate C/ Reference R/ Similarized Reference S 1-NCD METEORC There is no effective means to stop a Tratsch, which was already included in the world.R There is no good way to halt gossip that has already begun to spread.
.41 .31S There is no effective means to stop gossip that has already begun to spread.
.56 .55C Crisis, not only in AmericaR A Crisis Not Only in the U.S. .51 .44S A Crisis not only in the America .72 .56C Influence on the whole economy should not have this crisis.R Nevertheless, the crisis should not have influenced the entire economy.
.60 .37S Nevertheless, the crisis should not have Influence the entire economy.
.62 .44C Or the lost tight meeting will be discovered at the hands of a gentlemen?R Perhaps you see the pen you thought you lost lying on your colleague?s desk.
.42 .09S Perhaps you meeting the pen you thought you lost lying on your colleague?s desk.
.40 .13Table 1: Example German?English translations showing the effect of relaxed matching in the 1-mNCDscore (for rows S) compared with METEOR using the exact module only, since the modules stemand synonym are already used in the similarized reference.
Replaced words are emphasized.lation evaluations for the translations.
There areseveral tasks, defined by the language pair and thedomain of translated text.The human judgments include three differentcategories.
The RANK category has human qualityrankings of five translations for one sentence fromdifferent MT systems.
The CONST category con-tains rankings for short phrases (constituents), andthe YES/NO category contains binary answers if ashort phrase is an acceptable translation or not.For the translation tasks into English, the re-laxed alignment using a stem module and thesynonym module affected 7.5% of all words,whereas only 5.1% of the words were changed inthe tasks from English into the other languages.The data was preprocessed in two differentways.
For NCD we kept the data as is, which wecalled real casing (rc).
Since the used METEORalign module lowercases all text, we restored thecase information in mNCD by copying the correctcase from the reference translation to the similar-ized reference, based on METEOR?s alignment.The other way was to lowercase all data (lc).4.2 System-level correlationWe follow the same evaluation methodology as inCallison-Burch et al (2008), which allows us tomeasure how well MT evaluation measures corre-late with human judgments on the system level.Spearman?s rank correlation coefficient ?
wascalculated between each MT evaluation measureand human judgment category using the simplifiedequation?
= 1?6?i din(n2 ?
1)(2)where for each system i, di is the difference be-tween the rank derived from annotators?
input andthe rank obtained from the measure.
From the an-notators?
input, the n systems were ranked basedon the number of times each system?s output wasselected as the best translation divided by the num-ber of times each system was part of a judgment.We computed system-level correlations fortasks with English, French, Spanish and Germanas the target language1.5 ResultsWe compare mNCD against NCD and relate theirperformance to other MT evaluation measures.5.1 Block size effect on NCD scoresVa?yrynen et al (2010) computed NCD between aset of candidate translations and references at thesame time regardless of the sentence alignments,analogously to document comparison.
We experi-mented with segmentation of the candidate trans-lations into smaller blocks, which were individ-ually evaluated with NCD and aggregated into asingle value with arithmetic mean.
The resultingsystem-level correlations between NCD and hu-man judgments are shown in Figure 2 as a functionof the block size.
The correlations are very simi-lar with all block sizes, except for Spanish, wheresmaller block size produces higher correlation.
Anexperiment with geometric mean produced similarresults.
The reported results with mNCD use max-imum block size, similar to Va?yrynen et al (2010).1The English-Spanish news task was left out as most mea-sures had negative correlation with human judgments.822 5 10 20 50 100 500 2000 50000.00.20.40.60.81.0block size in linessystemlevel correlation withhumanjudgementsinto eninto deinto frinto esFigure 2: The block size has very little effect onthe correlation between NCD and human judg-ments.
The right side corresponds to documentcomparison and the left side to aggregated NCDscores for sentences.5.2 mNCD against NCDTable 2 shows the average system level correlationof different NCD and mNCD variants for trans-lations into English.
The two compressors thatworked best in our experiments were PPMZ andbz2.
PPMZ is slower to compute but performsslightly better compared to bz2, except for theMethod Parameters RANKCONSTYES/NOMeanmNCD PPMZ rc .69 .74 .80 .74NCD PPMZ rc .60 .66 .71 .66mNCD bz2 rc .64 .73 .73 .70NCD bz2 rc .57 .64 .69 .64mNCD PPMZ lc .66 .80 .79 .75NCD PPMZ lc .56 .79 .75 .70mNCD bz2 lc .59 .85 .74 .73NCD bz2 lc .54 .82 .71 .69Table 2: Mean system level correlations overall translation tasks into English for variants ofmNCD and NCD.
Higher values are emphasized.Parameters are the compressor PPMZ or bz2 andthe preprocessing choice lowercasing (lc) or realcasing (rc).Target Lang CorrMethod Parameters EN DE FR ESmNCD PPMZ rc .69 .37 .82 .38NCD PPMZ rc .60 .37 .84 .39mNCD bz2 rc .64 .32 .75 .25NCD bz2 rc .57 .34 .85 .42mNCD PPMZ lc .66 .33 .79 .23NCD PPMZ lc .56 .37 .77 .21mNCD bz2 lc .59 .25 .78 .16NCD bz2 lc .54 .26 .77 .15Table 3: mNCD versus NCD system correlationRANK results with different parameters (the sameas in Table 2) for each target language.
Highervalues are emphasized.
Target languages DE, FRand ES use only the stem module.lowercased CONST category.Table 2 shows that real casing improves RANKcorrelation slightly throughout NCD and mNCDvariants, whereas it reduces correlation in the cat-egories CONST, YES/NO as well as the mean.The best mNCD (PPMZ rc) improves the bestNCD (PPMZ rc) method by 15% in the RANKcategory.
In the CONST category the best mNCD(bz2 lc) improves the best NCD (bz2 lc) by 3.7%.For the total average, the best mNCD (PPMZ rc)improves the the best NCD (bz2 lc) by 7.2%.Table 3 shows the correlation results for theRANK category by target language.
As shown al-ready in Table 2, mNCD clearly outperforms NCDfor English.
Correlations for other languages showmixed results and on average, mNCD gives lowercorrelations than NCD.5.3 mNCD versus other methodsTable 4 presents the results for the selected mNCD(PPMZ rc) and NCD (bz2 rc) variants along withthe correlations for other MT evaluation methodsfrom the WMT?08 data, based on the results inCallison-Burch et al (2008).
The results are av-erages over language pairs into English, sortedby RANK, which we consider the most signifi-cant category.
Although mNCD correlation withhuman evaluations improved over NCD, the rank-ing among other measures was not affected.
Lan-guage and task specific results not shown here, re-veal very low mNCD and NCD correlations in theSpanish-English news task, which significantly83Method RANKCONSTYES/NOMeanDP .81 .66 .74 .73ULCh .80 .68 .78 .75DR .79 .53 .65 .66meteor-ranking .78 .55 .63 .65ULC .77 .72 .81 .76posbleu .75 .69 .78 .74SR .75 .66 .76 .72posF4gram-gm .74 .60 .71 .68meteor-baseline .74 .60 .63 .66posF4gram-am .74 .58 .69 .67mNCD (PPMZ rc) .69 .74 .80 .74NCD (PPMZ rc) .60 .66 .71 .66mbleu .50 .76 .70 .65bleu .50 .72 .74 .65mter .38 .74 .68 .60svm-rank .37 .10 .23 .23Mean .67 .62 .69 .66Table 4: Average system-level correlations overtranslation tasks into English for NCD, mNCDand other MT evaluations measuresdegrades the averages.
Considering the mean ofthe categories instead, mNCD?s correlation of .74is third best together with ?posbleu?.Table 5 shows the results from English.
The ta-ble is shorter since many of the better MT mea-sures use language specific linguistic resourcesthat are not easily available for languages otherthan English.
mNCD performs competitively onlyfor French, otherwise it falls behind NCD andother methods as already shown earlier.6 DiscussionWe have introduced a new MT evaluation mea-sure, mNCD, which is based on normalized com-pression distance and METEOR?s relaxed align-ment modules.
The mNCD measure outperformsNCD in English with all tested parameter com-binations, whereas results with other target lan-guages are unclear.
The improved correlationswith mNCD did not change the position in theRANK category of the MT evaluation measures inthe 2008 ACL WMT shared task.The improvement in English was expected onthe grounds of the synonym module, and indicatedalso by the larger number of affected words in theMethodTarget Lang CorrDE FR ES Meanposbleu .75 .80 .75 .75posF4gram-am .74 .82 .79 .74posF4gram-gm .74 .82 .79 .74bleu .47 .83 .80 .68NCD (bz2 rc) .34 .85 .42 .66svm-rank .44 .80 .80 .66mbleu .39 .77 .83 .63mNCD (PPMZ rc) .37 .82 .38 .63meteor-baseline .43 .61 .84 .58meteor-ranking .26 .70 .83 .55mter .26 .69 .73 .52Mean .47 .77 .72 .65Table 5: Average system-level correlations for theRANK category from English for NCD, mNCDand other MT evaluation measures.similarized references.
We believe there is poten-tial for improvement in other languages as well ifsynonym lexicons are available.We have also extended the basic NCD measureto scale between a document comparison mea-sure and aggregated sentence-level measure.
Therather surprising result is that NCD produces quitesimilar scores with all block sizes.
The differentresult with Spanish may be caused by differencesin the data or problems in the calculations.After using the same evaluation methodology asin Callison-Burch et al (2008), we have doubtswhether it presents the most effective method ex-ploiting all the given human evaluations in the bestway.
The system-level correlation measure onlyawards the winner of the ranking of five differ-ent systems.
If a system always scored second,it would never be awarded and therefore be overlypenalized.
In addition, the human knowledge thatgave the lower rankings is not exploited.In future work with mNCD as an MT evalu-ation measure, we are planning to evaluate syn-onym dictionaries for other languages than En-glish.
The synonym module for English doesnot distinguish between different senses of words.Therefore, synonym lexicons found with statis-tical methods might provide a viable alternativefor manually constructed lexicons (Kauchak andBarzilay, 2006).84ReferencesAbhaya Agarwal and Alon Lavie.
2008.
METEOR,M-BLEU and M-TER: evaluation metrics for high-correlation with human rankings of machine trans-lation output.
In StatMT ?08: Proceedings of theThird Workshop on Statistical Machine Translation,pages 115?118, Morristown, NJ, USA.
Associationfor Computational Linguistics.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEUin machine translation research.
In Proceedings ofEACL-2006, pages 249?256.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christoph Monz, and Josh Schroeder.
2008.Further meta-evalutation of machine translation.ACL Workshop on Statistical Machine Translation.Yee Seng Chan and Hwee Tou Ng.
2009.
MaxSim:performance and effects of translation fluency.
Ma-chine Translation, 23(2-3):157?168.Rudi Cilibrasi and Paul Vitanyi.
2005.
Clusteringby compression.
IEEE Transactions on InformationTheory, 51:1523?1545.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedingsof the main conference on Human Language Tech-nology Conference of the North American Chap-ter of the Association of Computational Linguistics,pages 455?462, Morristown, NJ, USA.
Associationfor Computational Linguistics.Kimmo Kettunen.
2009.
Packing it all up in search fora language independent MT quality measure tool.
InIn Proceedings of LTC-09, 4th Language and Tech-nology Conference, pages 280?284, Poznan.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 304?311, Prague, Czech Republic, June.
Association forComputational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.2001.
BLEU: a method for automatic evaluationof machine translation.
Technical Report RC22176(W0109-022), IBM Research Division, Thomas J.Watson Research Center.Steven Parker.
2008.
BADGER: A new machine trans-lation metric.
In Metrics for Machine TranslationChallenge 2008, Waikiki, Hawai?i, October.
AMTA.Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2005.
A paraphrase-based approach to machinetranslation evaluation.
Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-versity of Maryland, College Park.Ray Solomonoff.
1964.
Formal theory of inductiveinference.
Part I.
Information and Control,, 7(1):1?22.Jaakko J. Va?yrynen, Tero Tapiovaara, Kimmo Ket-tunen, and Marcus Dobrinkat.
2010.
Normalizedcompression distance as an automatic MT evalua-tion metric.
In Proceedings of MT 25 years on.
Toappear.85
