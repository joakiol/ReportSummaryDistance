Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 60?68,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsContextually?Mediated Semantic Similarity Graphs forTopic SegmentationGeetu AmbwaniStreamSage/ComcastWashington, DC, USAambwani@streamsage.comAnthony R. DavisStreamSage/ComcastWashington, DC, USAdavis@streamsage.comAbstractWe present a representation of documentsas directed, weighted graphs, modeling therange of influence of terms within thedocument as well as contextually deter-mined semantic relatedness among terms.We then show the usefulness of this kindof representation in topic segmentation.Our boundary detection algorithm usesthis graph to determine topical coherenceand potential topic shifts, and does not re-quire labeled data or training of parame-ters.
We show that this method yields im-proved results on both concatenated pseu-do-documents and on closed-captions fortelevision programs.1 IntroductionWe present in this paper a graph-based represen-tation of documents that models both the long-range scope  "influence" of terms and the seman-tic relatedness of terms in a local context.
Inthese graphs, each term is represented by a seriesof nodes.
Each node in the series corresponds toa sentence within the span of that term?s influ-ence, and the weights of the edges are propor-tional to the semantic relatedness among terms inthe context.
Semantic relatedness between termsis reinforced by the presence of nearby, closelyrelated terms, reflected in increased connectionstrength between their nodes.We demonstrate the usefulness of our repre-sentation by applying it to partitioning of docu-ments into topically coherent segments.
Oursegmentation method finds locations in the graphof a document where one group of strongly con-nected nodes ends and another begins, signalinga shift in topicality.
We test this method both onconcatenated news articles, and on a more realis-tic segmentation task, closed-captions fromcommercial television programs, in which topictransitions are more subjective and less distinct.Our methods are unsupervised and require notraining; thus they do not require any labeled in-stances of segment boundaries.
Our method at-tains results significantly superior to that of Choi(2000), and approaches human performance onsegmentation of television closed-captions,where inter-annotator disagreement is high.2 Graphs of lexical influence2.1 Summary of the approachSuccessful topic segmentation requires some re-presentation of semantic and discourse cohesion,and the ability to detect where such cohesion isweakest.
The underlying assumption of segmen-tation algorithms based on lexical chains or otherterm similarity measures between portions of adocument is that continuity in vocabulary reflectstopic continuity.
Two short examples illustratingtopic shifts in television news programs, withaccompanying shift in vocabulary, appear inFigure 1.We model this continuity by first modelingwhat the extent of a term's influence is.
This dif-fers from a lexical chain approach in that we donot model text cohesion through recurrence ofterms.
Rather, we determine, for each occurrenceof a term in the document (excluding terms gen-erally treated as stopwords), what interval of sen-tences surrounding that occurrence is the bestestimate of the extent of its relevance.
This ideastems from work in Davis, et al (2004), whodescribe the use of relevance intervals in multi-media information retrieval.
We summarize theirprocedure for constructing relevance intervals in60section 2.2.
Next, we calculate the relatedness ofthese terms to one another.
We use pointwisemutual information (PMI) as a similarity meas-ure between terms, but other measures, such asWordNet-based similarity or Wikipedia Minersimilarity (Milne and Witten, 2009), could aug-ment or replace it.S_44 Gatorade has discontinued a drink with hisimage but that was planned before the company hassaid and they have issued a statement in support oftiger woods.S_45 And at t says that while it supports tigerwoods personally, it is evaluating their ongoing busi-ness relationship.S_46 I'm sure, alex, in the near future we're goingto see more of this as companies weigh the short termdifficulties of staying with tiger woods versus thelong term gains of supporting him fully.S_47 Okay.S_48 Mark potter, miami.S_49 Thanks for the wrapup of that.S_50 We'll go now to deep freeze that's blanketingthe great lakes all the way to right here on the eastcoast.S_190 We've got to get this addressed and holddown health care costs.S_191 Senator ron wyden the optimist from oregon,we appreciate your time tonight.S_192 Thank you.S_193 Coming up, the final day of free health clinicin kansas city, missouri.The next step is to construct a graphical repre-sentation of the influence of terms throughout adocument.
When constructing topically coherentsegments, we wish to assess coherence from onesentence to the next.
We model similarity be-tween successive sentences as a graph, in whicheach node represents both a term and a sentencethat lies within its influence (that is, a sentencebelonging to a relevance interval for that term).For example, if the term ?drink?
occurs in sen-tence 44, and its relevance interval extends tosentence 47, four nodes will be created for?drink?, each corresponding to one sentence inthat interval.
The edges in the graph connectnodes in successive sentences.
The weight of anedge between two terms t and t' consists not onlyof their relatedness, but is reinforced by the pres-ence of other nodes in each sentence associatedwith terms related to t and t'.The resulting graph thus consists of cohorts ofnodes, one cohort associated with each sentence,and edges connecting nodes in one cohort tothose in the next.
Edges with a low weight arepruned from the graph.
The algorithm for deter-mining topic segment boundaries then seeks lo-cations in which a relatively large number of re-levance intervals for terms with relatively highrelatedness end or begin.In sum, we introduce two innovations here incomputing topical coherence.
One is that we usethe extent of each term's relevance intervals tomodel the influence of that term, which thus ex-tends beyond the sentences it occurs in.
Second,we amplify the semantic relatedness of a term tto a term t' when there are other nearby termsrelated to t and t'.
Related terms thereby rein-force one another in establishing coherence fromone sentence to the next.2.2 Constructing relevance intervalsAs noted, the scope of a term's influence is cap-tured through relevance intervals (RIs).
We de-scribe here how RIs are created.
A corpus?inthis case, seven years of New York Times texttotaling approximately 325 million words?isrun through a part-of-speech tagger.
The point-wise mutual information between each pair ofterms is computed using a 50-word window.1PMI values provide a mechanism to measurerelatedness between a term and terms occurringin nearby sentences of a document.
Whenprocessing a document for segmentation, we firstcalculate RIs for all the terms in that document.An RI for a term t is built sentence-by-sentence,beginning with a sentence where t occurs.
A sen-tence immediately succeeding or preceding thesentences already in the RI is added to that RI ifit contains terms with sufficiently high PMI val-ues with t. An adjacent sentence is also added toan RI if there is a pronominal believed to refer tot; the algorithm for determining pronominal ref-erence is closely based on Kennedy and Bogu-raev (1996).
Expansion of an RI is terminated ifthere are no motivations for expanding it further.Additional termination conditions can be in-cluded as well.
For example, if large local voca-1 PMI values are constructed for all words other than thosein a list of stopwords.
They are also constructed for a li-mited set of about 100,000 frequent multi-word expressions.In our segmentation system, we use only the RIs for nounsand for multiword expressions.Figure 1.
Two short closed-caption excerpts fromtelevision news programs, each containing a top-ic shift61bulary shifts or discourse cues signaling the startof end of a section are detected, RIs can beforced to end at those points.
In one version ofour system, we set these ?hard?
boundaries usingan algorithm based on Choi (2000).
In this paperwe report segmentation results with and withoutthis limited use of Choi?s algorithm.
Lastly, iftwo RIs for t are sufficiently close (i.e., the endof one lies within 150 words of the start ofanother), then the two RIs are merged.The aim of constructing RIs is to determinewhich portions of a document are relevant to aparticular term.
While this is related to the goalof finding topically coherent segments, it is ofcourse distinct, as a topic typically is determinedby the influence of multiple terms.
However, RIsdo provide a rough indication of how far a term'sinfluence extends or, put another way, of "smear-ing out" the occurrence of a term over an ex-tended region.2.3 From relevance intervals to graphsConsider a sentence Si, and its immediate succes-sor Si+1.
Each of these sentences is contained invarious relevance intervals; let Wi denote the setof terms with RIs containing Si, and Wi+1 denotethe set containing Si+1.For each pair of terms a in Wi and b in Wi+1,we compute a connection strength c(a,b), a non-negative real number that reflects how the twoterms are related in the context of Si and Si+1.
Toinclude the context, we take into account thatsome terms in Si may be closely related, andshould support one another in their connectionsto terms in Si+1, and vice versa, as suggestedabove.
Here, we use PMI values between termsas the basis for connection strength, normalizedto a similarity score that ranges between 0 and 1,as follows:The similarity between two terms is set to 0 ifthis quantity is negative.
Also, we assign themaximum value of 1 for self-similarity.
We thendefine connection strength in the following way:That is, the similarity of another term in Wi orWi+1 to b or a respectively, will add to the con-nection strength between a and b, weighted bythe similarity of that term to a or b respectively.Note that this formula also includes in the sum-mation the similarity s(a,b) between a and bthemselves, when either x or y is set to either a orb.2 Figure 2 illustrates this procedure.
We nor-malize the connection strength by the total num-ber of pairs in equation (2).We note in passing that many possible modifi-cations of this formula are easily imagined.
Oneobvious alternative to using the product of twosimilarity scores is to use the minimum of thetwo scores.
This gives more weight to pair valuesthat are both moderately high, with respect topairs where one is high and the other low.
Apartfrom this, we could incorporate terms from RIsin sentences beyond these two adjoining sen-tences, we could weight individual terms in Wi orWi+1 according to some independent measure oftopical salience, and so on.Figure 2.
Calculation of connection strength be-tween two nodesWhat emerges from this procedure is aweighted graph of connections across slices of adocument (sentences, in our experiments).
Eachnode in the graph is labeled with a term and asentence number, and represents a relevance in-terval for that term that includes the indicatedsentence.
The edges of the graph connect nodesassociated with adjacent sentences, and areweighted by the connection strength.
Becausemany weak connections are present in this graph,we remove edges that are unlikely to contributeto establishing topical coherence.
There are vari-ous options for pruning: removing edges withconnection strengths below a threshold, retainingonly the top n edges, cutting the graph betweentwo sentences where the total connectionstrength of edges connecting the sentences issmall, and using an edge betweenness algorithm(e.g., Girvan and Newman, 2002) to removeedges that have high betweenness (and hence areindicative of a "thin" connection).2 In fact, the similarity s(ai,bj) will be counted twice, oncein each summation in the formula above; we retain thisadditional weighting of s(ai,bj).62Figure 3.
A portion of the graph generated from the first excerpt in Figure 1.
Each node is labeledS_i__term_pos, where i indicates the sentence indexWe have primarily investigated the first me-thod, removing edges with a connection strengthless than 0.5.
Two samples of the graphs we pro-duce, corresponding to the excerpts in figure 1,appear in figures 3 and 4.2.4 Finding segment boundaries in graphsSegment boundaries in these graphs are hypothe-sized where there are relatively few, relativelyweak connections from a cohort of nodes asso-ciated with one sentence to the cohort of nodesassociated with the following sentence.
If a termhas a node in one cohort and in the succeedingcohort (that is, its RI continues across the twocorresponding sentences) it counts against asegment boundary at that location, whereas termswith nodes on only one side of the boundarycount in favor of a segment.
For example, in fig-ure 3, a new set of RIs start in sentence 48,where we see nodes for ?Buffalo?, ?Michigan?,?Worth?, Marquette?, and ?Miami?, and RIs inpreceding sentences for ?Tiger Woods?, ?Gato-rade?, etc.
end.
Note that the corresponding ex-cerpt in figure 1 shows a clear topic shift be-tween a story on Tiger Woods ending at sentence46, and a story about Great Lakes weather be-ginning at sentence 48.Similarly, in figure 4, RIs for ?Missouri?,?city?
and ?health clinic?
include sentences 190.191, and 192; thus these are evidence against asegment boundary at this location.
On the otherhand, several other terms, such as ?Oregon?,?Ron?, ?Senator?, and ?bill?, have RIs that endat sentence 191, which argues in favor of aboundary there.
We present further details of ourboundary heuristics in section 4.1.3 Related WorkThe literature on topic segmentation has mostlyfocused on detecting a set of segments, typicallynon-hierarchical and non-overlapping, exhaus-tively composing a document.
Evaluation is thenrelatively simple, employing pseudo-documentsconstructed by concatenating a set of documents.This is a suitable technique for detecting coarse-grained topic shifts.
As Ferret (2007) points out,approaches to the problem vary both in the kindsof knowledge they depend on, and on the kindsof features they employ.Research on topic segmentation has exploitedinformation internal to the corpus of documentsto be segmented and information derived fromexternal resources.
If a corpus of documents per-tinent to a domain is available, statistical topicmodels such as those developed by Beeferman etal.
(1999) or Blei and Moreno (2001) can be tai-lored to documents of that type.
Lexical cohesiontechniques include similarity measures betweenadjacent blocks of text, as in TextTiling (Hearst,1994, 1997) and lexical chains based on recur-rences of a term or related terms, as in Morrisand Hirst (1991), Kozima (1993), and Galley, etal.
(2003).
In Kan, et al (1998) recurrences ofthe same term within a certain number of sen-tences are used for chains (the number varieswith the type of term), and chains are based onentity reference as well as lexical identity.
Ourmethod is related to lexical chain techniques, inthat the graphs we construct contain chains ofnodes that extend the influence of a term beyondthe site where it occurs.
But we differ in that wedo not require a term (or a semantically relatedterm) to recur, in order to build such chains.63Figure 4.
A portion of the graph generated from the second excerpt in Figure 1.
Each node is labeledS_i__term_pos, where i indicates the sentence indexIn this respect, our approach also resemblesthat of Matveeva and Levow (2007), who buildsemantic similarity among terms into their lexi-cal cohesion model through latent semantic anal-ysis.
Our techniques differ in that we incorporatesemantic relatedness between terms directly intoa graph, rather than computing similarities be-tween blocks of text.In our experiments, we compare our method toC99 (Choi, 2000), an algorithm widely treated asa baseline.
Choi?s algorithm is based on a meas-ure of local coherence; vocabulary similarity be-tween each pair of sentences in a document iscomputed and the similarity scores of nearbysentences are ranked, with boundaries hypothe-sized where similarity across sentences is low.4 Experiments, results, and evaluation4.1 Systems comparedAs noted above, we tested our system against theC99 segmentation algorithm (Choi, 2000).
Theimplementation of C99 we use comes from theMorphAdorner website (Burns, 2006).
We alsocompared our system to two simpler baselinesystems without RIs.
One uses graphs that do notrepresent a term?s zone of influence, but containjust a single node for each occurrence of a term.The second represents a term?s zone of influencein an extremely simple fashion, as a fixed num-ber of sentences starting from each occurrence ofthat term.
We tried several values ranging from5 to 20 sentences for this extension.
In addition,we varied two parameters to find the best-performing combination of settings: the thre-shold for pruning low-weight edges, and thethreshold for positing a segment boundary.
Inboth the single-node and fixed-extension sys-tems, the connection strength between nodes iscalculated in the same way as for our full system.These comparisons aim to demonstrate twothings.
First, segmentation is greatly improvedwhen we extend the influence of terms beyondthe sentences they occur in.
Second, the RIsprove more effective than fixed-length exten-sions in modeling that influence accurately.Lastly, to establish how much we can gainfrom using Choi?s algorithm to determine termi-nation points for RIs, we also compared two ver-sions of our system: one in which RIs are calcu-lated without information from Choi?s algorithmand a second with these boundaries included.Table 1 lists the systems we compare in theexperiments described below.C99 Implementation of Choi (2000)SS+COur full Segmentation System, incor-porating ?hard?
boundaries determinedby modified Choi algorithmSSOur system, using RIs without ?hard?boundaries determined by modifiedChoi algorithmFEOur system, using fixed extension of aterm from its occurrenceSNOur system, using a single node foreach term occurrence (no extension)Table 1.
Systems compared in our experiments4.2 Data and parameter settingsWe tested our method on two sets of data.
Oneset consists of concatenated news stories, follow-ing the approach of Choi (2000) and others since;the other consists of closed captions for twelveU.S.
commercial television programs.
Becausethe notion of a topic is inherently subjective, wefollow many researchers who have reported re-sults on "pseudo-documents"?documents formedby concatenating several randomly selected doc-uments?so that the boundaries of segments areknown, sharp, and not dependent on annotatorvariability (Choi, 2000).
However, we also are64interested in our system?s performance on morerealistic segmentation tasks, as noted in the in-troduction.In testing our algorithm, we first generatedgraphs from the documents in each dataset, asdescribed in section 2.
We pruned edges in thegraphs with connection strength of less than 0.5.To find segment boundaries, we seek locationswhere the number of common terms associatedwith successive sentences is at a minimum.
Thisquantity needs to be normalized by some meas-ure of how many nodes are present on either sideof a potential boundary.
We tested three normali-zation factors: the total number of nodes on bothsides of the potential segment boundary, themaximum of the numbers of nodes on each sideof the boundary, and the minimum of the num-bers of nodes on each side of the boundary.
Theresults for all three of these were very similar, sowe report only those for the maximum.
Thismeasure provides a ranking of all possible boun-daries in a document (that is, between each pairof consecutive sentences), with a value of 0 be-ing most indicative of a boundary.
After experi-menting with a few threshold values, we selecteda threshold of 0.6, and posit a boundary at eachpoint where the measure falls below this thre-shold.4.3 Evaluation metricsWe compute precision, recall, and F-measurebased on exact boundary matches between thesystem and the reference segmentation.
As nu-merous researchers have pointed out, this aloneis not a perspicacious way to evaluate a segmen-tation algorithm, as a system that misses a gold-standard boundary by one sentence would betreated just like one that misses it by ten.
Wetherefore computed two additional, widely usedmeasures, Pk (Beeferman, et al, 1997) and Win-dowDiff (Pevzner and Hearst, 2002).
Pk assessesa penalty against a system for each position of asliding window across a document in which thesystem and the gold standard differ on the pres-ence or absence of (at least one) segment boun-dary.
WindowDiff is similar, but where the sys-tem differs from the gold standard, the penalty isequal to the difference in the number of bounda-ries between the two.
This penalizes missedboundaries and ?near-misses?
less than Pk (butsee Lamprier, et al, (2007) for further analysisand some criticism of WindowDiff).
For both Pkand WindowDiff, we used a window size of halfthe average reference segment length, as sug-gested by Beeferman, et al (1997).
Pk and Win-dowDiff values range between 0 and 1, withlower values indicating better performance indetecting segment boundaries.
Note that both Pkand WindowDiff are asymmetrical measures;different values will result if the system?s and thegold-standard?s boundaries are switched.4.4 Concatenated New York Times articlesThe concatenated pseudo-documents consist ofNew York Times articles selected at random fromthe New York Times Annotated Corpus.3  Eachpseudo-document contains twenty articles, withan average of 623.6 sentences.
Our test set con-sists of 185 of these pseudo-documents.4N = 185Prec.
Rec.
F Pk WDC99?
0.404 0.569 0.467 0.338 0.360s.d 0.106 0.121 0.105 0.109 0.135SS?
0.566 0.383 0.448 0.292 0.317s.d.
0.176 0.135 0.140 0.070 0.084SS+C?
0.578 0.535 0.537 0.262 0.283s.d.
0.148 0.197 0.150 0.081 0.098FE?
0.265 0.140 0.176 0.478 0.536s.d.
0.123 0.042 0.055 0.055 0.076SN?
0.096 0.112 0.099 0.570 0.702s.d.
0.040 0.024 0.027 0.072 0.164Table 2.
Performance of C99 and SS on segmen-tation of concatenated New York Times articles,without specifying a number of boundariesTables 2 and 3 provide summary results on theconcatenated news articles.
We ran the five sys-tems listed in table 1 on the full dataset withoutany additional restrictions on the number of ar-ticle boundaries to be detected.
Means and stan-dard deviations for each method on the five me-trics are displayed in table 2.
C99 typically findsmany more boundaries than the 20 that arepresent (30.65 on average).
Our SS system findsfewer than the true number of boundaries (14.52on average), while the combined system SS+Cfinds almost precisely the correct number (19.98on average).
We used one-tailed paired t-tests ofequal means to determine statistical significanceat the 0.01 level.
Although only SS+C?s perfor-mance is significantly better in terms of F-3www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T194 Only article text is used, though occasionally some ob-vious heading material, such as book title, author and pub-lisher at the beginning of a book review, is present also.65measure, both versions of our system outperformC99 according to Pk and WindowDiff.Using the baseline single node system (SN)yields very poor performance.
These results (ta-ble 2, row SN) are obtained with the edge-pruning threshold set to a connection strength of0.9, and the boundary threshold set to 0.2, atwhich the average number of boundaries found is26.86.
Modeling the influence of terms beyondthe sentences they occur in is obviously valuable.The baseline fixed-length extensions system(FE) does better than SN but significantly worsethan RIs.
We found that, among the parametersettings yielding between 10 and 30 boundariesper document on average, the best results occurwith the extension set to 6 sentences, the edge-pruning threshold set to a connection strength of0.5, and the boundary threshold set to 0.7.
Theresults for this setting are reported in table 2, rowFE (the average number of segments per docu-ment is 12.5).
Varying these parameters has onlyminor effects on performance, although thenumber of boundaries found can of course betuned.
RIs clearly provide a benefit over this typeof system, by modeling a term?s influence dy-namically rather than as a fixed interval.From here on, we report results only for thetwo systems: C99 and our best-performing sys-tem, SS+C.For 86 of the documents, in which both C99and SS+C found more than 20 boundaries, wealso calculate the performance on the best-scoring 20 boundaries found by each system.These results are displayed in table 3.
Note thatwhen the number of boundaries to be found byeach system is fixed at the actual number ofboundaries, the values of precision and recall arenecessarily identical.
Here too our system out-performs C99, and the differences are statistical-ly significant, according to a one-tailed paired t-test of equal means at the 0.01 level.N = 86Prec.=Rec.=F Pk WDC99?
0.530 0.222 0.231s.d 0.105 0.070 0.074SS + C?
0.643 0.192 0.201s.d.
0.130 0.076 0.085Table 3.
Performance of C99 and SS on segmen-tation of concatenated New York Times articles,selecting the 20 best-scoring boundaries4.5 Human-annotated television programclosed-captionsWe selected twelve television programs forwhich we have closed-captions; they are a mix ofheadline news (3 shows), news commentary (4shows), documentary/lifestyle (3 shows), onecomedy/drama episode, and one talk show.
Onlythe closed captions are used, no speaker intona-tion, video analysis, or metadata is employed.The closed captions are of variable quality, withnumerous spelling errors.Five annotators were instructed to indicatetopic boundaries in the closed-caption text files.Their instructions were open-ended in the sensethat they were not given any definition of what atopic or a topic shift should be, beyond two shortexamples, were not told to find a specific numberof boundaries, but were allowed to indicate howimportant a topic was on a five-point scale, en-couraging them to indicate minor segments orsubtopics within major topics if they chose to doso.
For some television programs, particularlythe news shows, major boundaries between sto-ries on disparate topics are likely be broadlyagreed on, whereas in much of the remainingmaterial the shifts may be more fine-grained andjudgments varied.
In addition, the scripted natureof television speech results in many carefullystaged transitions and teasers for upcoming ma-terial, making boundaries more diffuse or con-founded than in some other genres.We combined the five annotators?
segmenta-tions, to produce a single set of boundaries as areference.
We used a three-sentence sliding win-dow, and if three or more of the five annotatorsplace a boundary in that window, we assign aboundary where the majority of them place it (incase of a tie, we choose one location arbitrarily).Although the annotators are rather inconsistent intheir use of this rating system, a given annotatortends to be consistent in the granularity of seg-mentation employed across all documents.
Thisobservation is consistent with the remarks of Ma-lioutov and Barzilay (2006) regarding varyingtopic granularity across human annotators onspoken material.
We thus computed two versionsof the combined boundaries, one in which allboundaries are used, and another in which weignore minor boundaries?those the annotatorassigned a score of 1 or 2.
We ran our experi-ments with both versions of the combined boun-daries as the reference segmentation.We use Pk to assess inter-annotator agreementamong our five annotators.
Table 4 presents two66Pk values for each pair of annotators; one set ofvalues is for all boundaries, while the other is for?major?
boundaries, assigned an importance of 3or greater on the five-point scale.
The Pk valuefor each annotator with respect to the two refer-ence segmentations is also provided.A B C D E RefA0.360.480.300.450.270.440.420.670.200.38B0.290.400.290.320.270.330.330.550.200.25C0.570.480.600.440.410.200.670.610.400.18D0.360.460.410.460.270.200.530.630.220.26E0.330.350.310.340.330.300.320.310.250.27Ref0.250.390.320.350.240.170.210.220.420.58Table 4.
Pk values for the segmentations pro-duced by each pair of annotators (A-E) and forthe combined annotation described in section4.5; upper values are for all boundaries and low-er values are for boundaries of segments scored 3or higherThese numbers are rather high, but compara-ble to those obtained by Malioutov and Barzilay(2006) in a somewhat similar task of segmentingvideo recordings of physics lectures.
The Pk val-ues are lower for the reference boundary set,which we therefore feel some confidence in us-ing as a reference segmentation.Prec.
Rec.
F Pk WDAll topic boundariesC99?
0.197 0.186 0.184 0.476 0.507s.d 0.070 0.072 0.059 0.078 0.102SS+C?
0.315 0.208 0.240 0.421 0.462s.d.
0.089 0.073 0.064 0.072 0.084Major topic boundaries onlyC99?
0.170 0.296 0.201 0.637 0.812s.d.
0.063 0.134 0.060 0.180 0.405SS+C?
0.271 0.316 0.271 0.463 0.621s.d.
0.102 0.138 0.077 0.162 0.445Table 5.
Performance of C99 and SS+C on seg-mentation of closed-captions for twelve televi-sion programs, with the two reference segmenta-tions using ?all topic boundaries?
and ?majortopic boundaries only?As the television closed-captions are noisywith respect to data quality and inter-annotatordisagreement, the performance of both systems isworse than on the concatenated news articles, asexpected.
We present the summary performanceof C99 and SS+C in table 5, again using two ver-sions of the reference.
Because of the small testset size, we cannot claim statistical significancefor any of these results, but we note that on aver-age SS+C outperforms C99 on all measures.5 Conclusions and future workWe have presented an approach to text segmen-tation that relies on a novel graph based repre-sentation of document structure and semantics.
Itsuccessfully models topical coherence usinglong-range influence of terms and a contextuallydetermined measure of semantic relatedness.
Re-levance intervals, calculated using PMI and othercriteria, furnish an effective model of a term?sextent of influence for this purpose.
Our measureof semantic relatedness reinforces global co-occurrence statistics with local contextual infor-mation, leading to an improved representation oftopical coherence.
We have demonstrated signif-icantly improved segmentation resulting fromthis combination, not only on artificially con-structed pseudo-documents, but also on noisydata with more diffuse boundaries, where inter-annotator agreement is fairly low.Although the system we have described here isnot trained in any way, it provides an extensiveset of parameters that could be tuned to improveits performance.
These include various tech-niques for calculating the similarity betweenterms and combining those similarities in con-nection strengths, heuristics for scoring potentialboundaries, and thresholds for selecting thoseboundaries.
Moreover, the graph representationlends itself to techniques for finding communitystructure and centrality, which may also proveuseful in modeling topics and topic shifts.We have also begun to explore segment labe-ling, identifying the most ?central?
terms in agraph according to their connection strengths.Those terms whose nodes are strongly connectedto others within a segment appear to be goodcandidates for segment labels.Finally, although we have so far applied thismethod only to linear segmentation, we plan toexplore its application to hierarchical or overlap-ping topical structures.
We surmise that stronglyconnected subgraphs may correspond to thesemore fine-grained aspects of discourse structure.67AcknowledgementsWe thank our colleagues David Houghton,Olivier Jojic, and Robert Rubinoff, as well as theanonymous referees, for their comments andsuggestions.ReferencesDoug Beeferman, Adam Berger, and John Laf-ferty.
1997.
Text Segmentation Using Expo-nential Models.
Proceedings of the SecondConference on Empirical Methods in NaturalLanguage Processing, 35-46.Doug Beeferman, Adam Berger, and John Laf-ferty.
1999.
Statistical models for text segmen-tation.
Machine Learning, 34(1):177?210.David M. Blei and Pedro J. Moreno.
2001.
Topicsegmentation with an aspect hidden Markovmodel.
Proceedings of the 24th Annual Meet-ing of ACM SIGIR, 343?348.Burns, Philip R. 2006.
MorphAdorner: Morpho-logical Adorner for English Text.http://morphadorner.northwestern.edu/morphadorner/textsegmenter/.Freddy Y.Y.
Choi.
2000.
Advances in domainindependent linear text segmentation.
Pro-ceedings of NAACL 2000, 109-117.Anthony Davis, Phil Rennert, Robert Rubinoff,Tim Sibley, and Evelyne Tzoukermann.
2004.Retrieving what's relevant in audio and video:statistics and linguistics in combination.
Pro-ceedings of RIAO 2004, 860-873.Olivier Ferret.
2007.
Finding document topics forimproving topic segmentation.
Proceedings ofthe 45th Annual Meeting of the ACL, 480?487.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
DiscourseSegmentation of Multi-Party Conversation.Proceedings of the 41st Annual Meeting of theACL, 562-569.Michelle Girvan and M.E.J.
Newman.
2002.Community structure in social and biologicalnetworks.
Proceedings of the National Acad-emy of Sciences, 99:12, 7821-7826.Marti A. Hearst.
1994.
Multi-paragraph segmen-tation of expository text.
Proceedings of the32nd Annual Meeting of the ACL, 9-16.Marti A. Hearst.
1997.
TextTiling: SegmentingText into Multi-Paragraph Subtopic Passages.Computational Linguistics, 23:1, 33-64.Min-Yen Kan, Judith L. Klavans, and KathleenR.
McKeown.
1998.
Linear Segmentation andSegment Significance.
Proceedings of the 6thInternational Workshop on Very Large Cor-pora, 197-205.Christopher Kennedy and Branimir Boguraev.1996.
Anaphora for Everyone: PronominalAnaphora Resolution without a Parser.
Pro-ceedings of the 16th International Conferenceon Computational Linguistics, 113-118.Hideki Kozima.
1993.
Text segmentation basedon similarity between words.
Proceedings ofthe 31st Annual Meeting of the ACL (StudentSession), 286-288.Sylvain Lamprier, Tassadit Amghar, BernardLevrat and Frederic Saubion.
2007.
On Evalu-ation Methodologies for Text SegmentationAlgorithms.
Proceedings of the 19th IEEE In-ternational Conference on Tools with Artifi-cial Intelligence, 19-26.Igor Malioutov and Regina Barzilay.
2006.
Min-imum Cut Model for Spoken Lecture Segmen-tation.
Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the ACL, 25?32.Irina Matveeva and Gina-Anne Levow.
2007.Topic Segmentation with Hybrid DocumentIndexing.
Proceedings of the 2007 Joint Con-ference on Empirical Methods in NaturalLanguage Processing and ComputationalNatural Language Learning, 351?359,David Milne and Ian H. Witten.
2009.
An Open-Source Toolkit for Mining Wikipedia.http://www.cs.waikato.ac.nz/~dnk2/publications/AnOpenSourceToolkitForMiningWikipedia.pdf.Jane Morris and Graeme Hirst.
1991.
Lexicalcohesion computed by thesaural relations.
asan indicator of the structure of text.
Computa-tional Linguistics, 17:1, 21-48.Lev Pevzner and Marti A. Hearst.
2002.
A criti-que and improvement of an evaluation metricfor text segmentation.
Computational Linguis-tics, 28:1, 19?36.68
