Handling Pragmatic InformationWith A Reversible ArchitectureMasato IshizakiNTT Communications and Information ProcessingLaboratones1-2356, Take, Yokosuka, Kanagawa, 238-03 JapanE-mail: ishizaki%nttnly.ntt.jp@relay.cs.netAbstractThis paper propo~s areversible architectureto handle not orily syntactic and semanticinformation but also pragmatic nformation.?
I Existing architectures cannot represent prag-matic informatioil explicitly, and lack rea-soning capability;given insufficient informa-tion.
I argue that he techniques ofplan rep-resentation and approximate r asoning are,in the enhanced argumentation system pro-posed here, effective for solving these prob-lems.1.
IntroductionReversibility orbi-directionality of grammarsseems to play a quite important role in naturallanguage procesSing.
It reduces the cost ofconstructing a grammar; we need to use onlyone grammar instead of two for parsing andgeneration.
Cost ~here includes not only themaking of grammar rules but also verifyingthe rules and the algorithms for parsing andgeneration.
Reversibility differs from bi-directionality: the former equires the samemechanism and 'grammar for parsing andgeneration; the latter equires just the samegrammar as shown Figure l(Noord, 1990).Pragmatic in:formation is not rigidlydefined; rather it is thought of as informationother than syntaqtic and semantic informa-tion.
It is indispetlsable for explaining manylinguistic pheno/nena from cleft sentencesto discourse structures.
As pragmatics annotrestrain language in a strict manner like syn-tax, it must be processed differently; that is,a distinction must be made between con-straints that need to be fully satisfied andthose that do not.
'Plan representation seemsto be appropriate for collecting differentlevel information.
A plan consists of preconditions, constraints, plan expansion (usuallytermed the body), and effects.
The relation-ship between preconditions and constraintsparallels that between pragmatic and syntacticinformation.
Thus, the difference betweenpreconditions and constraints can be easilymodeled.Handling pragmatic information clearlydepends on assumption ofbelief: Generatingreferring expressions requires inferencingthe hearer's belief (Appelt, 1985); Producingtext requires the usage of a one-sided mutualbeliefl(Moore t a1.,1989); the listener'sinference about he speaker's belief greatlyhelps to resolve anaphora or to analyze thespeaker's intention.
In any case, belief be-comes a condition for further inference; how-ever, it is difficult if not impossible to confirmthe assumed belief.
Thus, a new mechanismbased on a new architecture is needed.
Ap-proximate reasoning (Elkan,1990) is stat-able for this purpose.
Processing can con-"tinue, even if some preconditions are notfully satisfied; they are held as assump-tions 2.
This approach seems to be verynatural.
For example, in conversations, thespeaker should conceptualize the listener's1One-sided mutual belief is one half ofmutual knowledge, soto speak, namely the set ofthose pieces of mutual knowledge that constitutethe knowledge ofone speaker (Bunt, 1989: 60).2Since approximate r asoning can fail,assumptions must be held explicitly for further in-ference.
Plan representation is adequate for thatreason.119Strings Stri0gsAnalyzer FormsFigure 1.
Reversible And Bi-directional Architectures.understanding.
In most conversations, how-ever, the speaker does not keep confirmingthe other's belief because this would disruptthe conversation flow.This paper describes areversible archi-tecture to handle not only syntactic and se-mantic information, but also pragmatic in-formation.
Existing architectures cannot rep-resent pragmatic nformation explicitly, andlack reasoning capability given insufficientinformation.
I argue that the techniques ofplan representation a d approximate r ason-ing in the argumentation system introducedhere are effective for solving these problems.First, the difficulties of existing architec-tures have in handling pragmatic nformationare described.
Next, plan representation flinguistic information and approximate r a-soning are mentioned in the context of theargumentation system.
Third, parsing andgeneration examples are shown.
Finally, theproblem of proposed ata structure, the de-composition of semantic representation, therole of syntactic information and the differ-ence between active and passive vocabularyare discussed.2.
Existing Architectures For Re-versible And Bi-directional Gram-marShieber proposed auniform architecture forsentence parsing and generation based onthe Early type deduction mechanism(Shieber,1988).
He parametrized the archi-tecture with the initial condition, a priorityfunction on lemmas, and a predicate x-pressing the concept of successful proof.Shieber emedied the inefficiency of the generation algorithm in his uniform architectureto introduce the concept of semantic head(Shieber et a1.,1989).
Although DefiniteClause Grammar (DCG) is reversible, itssynthesis mode is inefficient.
Dymetmanand Strzalkowski approached the problemby compiling DCG into efficient analysisand synthesis programs (Dymetman eta1.,1988) (Strzalkowski,1990).
The compi-lation is realized by changing oal orderingstatically.
Since Shieber's, Dymetman's andStrzalkowski's architectures are based onsyntax deduction, they have difficulties inhandling pragmatic nformation.Dependency propagation was suggestedfor parsing and generation in (Hasida eta1.,1987).
His idea was developed usinghorn clauses imilar to PROLOG.
The worddependency indicates the states where vari-ables are shared by constraints 3.
Problemsolving or parsing and generation can bemodeled by resolving the dependencies.
De-pendency resolution is executed by fold/un-fold transformations.
Dependency propaga-tion is a very elegant mechanism for problemsolving, but it seems to be difficult to repre-sent syntactic, semantic and pragmatic nfor-mation with indiscrete constraints.
In addi-tion, to that, since dependency propagationis a kind of co-routine process, programsare very hard to debug and so constraintsare tough to stipulate.Ait-Kaci's typed unification was appliedto a reversible architecture (Emele and Zajac,1990).
All features in sign are sorted andplaced in hierarchical structures.
Parsing andgeneration can be executed by rewriting the3Constraints are represented bythe usualPROLOG predicates.120Ifeatures into their most specific forms.
Theirmechanism greatly depends on the hierarchyof information, but with information otherthan syntactic, especially pragmatic, it is hardto construct the hierarchy.3.
Introduction To the New Re-versible Architecture3.1.
The Linguistic ObjectsWe introduce the!linguistic object sign whichincorporates syntactic, semantic and prag-matic information.
Sign is represented byfeature structures and consists of featuresphn, syn, sem and prag.
Phn representssurface string information for words, phras-es and sentences.
Syn stands for syntacticinformation like the part of speech and sub-categorization i formation using HPSG.HPSG inherits the fundamental propertiesof Generalized: Phrase Structure Gram-mar(GPSG).
That is, HPSG makes use ofa set of feature+value pairs, feature con-straints and unification to stipulate grammarinstead of rewriting rules for terminal andnonterminal symbols.
The major differencebetween HPSGi and GPSG is that subcat-egorizafion i formation is stored in lexicalentries, instead of being stored in grammarrules (Pollard et a1.,1987,1990).
Sere de-notes emantic nformation or logical forms.Logical forms are expressed by the semanticrepresentation l~guage proposed by Gazdar(Gazdar et a1,1989).
Since the language is afeature representation of Woods' representa-tion (Woods,1978), it has the advantagesthat it can represent quantifier scope ambigu-ities.
It consistsi of the features qnt, var,rest, and body : qnt is for quantifier expres-sions; var is for variables bound by theqnt; rest is for restrictions of the var ; whilebody represents flae predication ofthe logicalform.
Prag deliiaeates pragmatic informa-tion.
Pragmatic conditions are not necessarilytrue but are held a s assumptions.
Uniquenessand novelty conditions in cleft sentences areinstances of the conditions.mphn: "It was the girl that a boy loved"syn: pos: verbsubcat:subc(\[\])sem: qnt: indefinitevar: Xrest: argO: Xpred: BOYbody: qnt: definitevar: Yrest: argO: Ypred: GIRLbody: argO: Xargl: Ypred: LOVEDprag: \[novel(Y), unique(Y)\]- -  SignFigure 2.
Linguistic Object Example.Figure 2 shows an example of thelinguistic object sign.
The feature phn indi-cates that surface string is "It was a girlthat the boy loved".
Syn represents hat: 1)the part of speech is verb; 2) subcategoriza-tion information is satisfied.
Sem showsthat: 1) the quantifier at the top level is in-definite; 2) the property of the variable X isa boy; 3) the property of the variable Ybounded by the quantifier definite is a girl;4) the boy loved the girl.
Prag mentionsthat the variable Y is constrained withuniquness and novelty conditions.3.2.
The Plan Representation ofLinguistic ObjectsTo handle syntactic, semantic, and pragmaticinformation, our generator represents hemas plans.
Plans are composed of precondi-tions, constraints, plan expansion, and ef-fects.
Preconditions include pragmatic nfor-mation which are the criteria needed to selecta plan.
Constraints include syntactic ondi-tions such as the head feature principle andconditions on surface strings.
Plan expan-sion contains ub-semantic expressions foreffects, which are complete semantic ex-pressions.
Constraints and preconditions aresimilar, but differ in that the former must besatisfied, but the latter are retained as as-121sumpfions if not satisfied.Figure 3 describes aplan relating to thesemantic nformation LOVED.
No precondi-tions exists because the expression "loved"has no pragmatic information.
Constraintsindicate that: 1) the part of speech equalsverb; 2) the subcategofization nformationis subc(\[Sbj,Obj\]); 3) The sem features ofprecond: \[\]const: (Sign:syn:pos = verb),(Sign:syn:subcat = subc(\[Sbj,Obj\])),(ArgO = Sbj:sem), (Argl = Obj:sem),(Sign:phn = "loved")eplan: \[\]effect: argO: ArgOargl: Arglpred: LOVEDFigure 3.
Plan Example.Sbj and Obj are semantic arguments ofpred-icate LOVED; 4) the surface string is "loved".There is no plan expansion because lexicalinformation does not need to be expanded.Effects mention semantic expression ofLOVED.3.3.
An Argumentation SystemFor PlanningA plan recognition scheme, named the argu-mentation system, was proposed by Kono-lige and Pollack (Konolige and Pollack,1989).
It can defeasibly reason about beliefand intention ascription 4, and can processpreferences over candidate ascriptions.
Theframework is so general and powerful thatit can perform other processes other thanbelief and intention ascription.
For example,Shimazu has shown that it can model parsingmechanismThe argumentation system consists ofarguments.
An argument is a relation between4 Defeasible r asoning and approximatereasoning are very similar, but differ in that: theformer addresses the result after ule application; thelatter considers just rule application.a set of propositions (the premises of theargument), and another set of propositions(the conclusion of the argument) (Konoligeand Pollack,1989: 926).
The system has alanguage containing the following operators:t(P) which indicates the truth of the proposi-tion P; beI(A,PF) which mentions that agentA believes plan fragment PF; int(A,PF)which shows that agent A intends plan frag-ment PF; exp(A,P) which means that agentA expects proposition P to be true; andby(Actexpl,Actexp2,Pexp) which signifiesthe complex plan fragment which consistsof the action expression Actexp2, by doingaction expression Actexpl while proposi-tional expression Pexp is true 5.
The argu-ments to the operators, action expressionsinform(S,H,Sem) and utter(S,H,Str), are in-troduced to mimic informing and utteringactivities: the former is designated such thatspeaker S informs hearer H about semanticcontent Sem; the latter indicates peaker Sutters tring Str to hearer H.Plan expansion, effects and constraintsmentioned in subsection 3.2 correspond toActexpl, Actexp2 and Pexp, respectively.To represent the difference between precon-ditions and constraints, the operator by isrevised to include preconditions a the fourthargument.
Thus, the new operatorby(Actexp 1,Actexp2,Pexp 1,Pexp2) is de-fined as the complex plan fragment, consist-ing of doing Actexp2 (effect(s)), by doingActexpl (plan expansion) while Pexpl (con-straints) is true, and Pexp2 (precondition(s))is true or held as assumptions.
The plan infigure 2 was redefined by using axiom (1) 6.Axiom (2) shows another example cor-5Action expressions are formed from anaction ame and parameters; Propositional expres-sions are formed from a property name and parameters.~ecause of space limitations, abbrevia-tions are used as necessary.
For example, Pos istaken to mean the value of the features of partof speech of syntactic information f sign.122I ?responding to the context free grammar fora cleft sentence.Axiom (1):t(by(utter(S,H,"loved"),inform(S,H,LOVED),((Pos=verb),(Subcat=subc(\[Sbj,Obj\])),(Sbj:sem=Arg0),(Obj:semaArgl)),0)Axiom (2):rt(by((inform(S,H,LF17),inform(S,H,LF2S)),inform(S,H,LF9),((Pos=Pos2),(Sign 1 =Sbj),(Subcat=subc(\[\])),(Slash=sl(\[\])),(Slash2=s\[(\[Obj\])),(Phn="It was"+Phn 1 +"that" +Phn2)),(Prag))).7LF1 is designated as:Signl: Sem: qnt: Q2var: Yrest: argO: Ypred: YRest.ZSLF2 is designated as:Sign2: sere: qnt: Q1var: Xrest: argO: Xpred: XRestbody: argO: ArgOargl: Arglpred: Pred.9LF designated as:Sign: s m: qnt: Q1var: Xrest: argO: Xpred: XRestbody: qnt: Q2var: Yrest: argO: Ypred: YRestbody: argO: ArgOargl: Arglpred: Pred.Plan expansion and effects indicate that ifspeaker S wants to inform hearer H aboutLF, the speaker should inform the hearerabout LF1 and LF2, while observing con-straints 1) -4).
Constraints state that: 1) thepart of speech of Sign equals one of Sign2;2) Subcategorized and slash information ofSign is nil; 3) Subcategorized informationof Sign2 equals nil; 4) Slash information ofSign2 is equivalent toObj; 4) a surface stringconsists of the string "It was", the stringrelating to Signl, the string "that" and thestring relating to Sign2.
Other axioms whichare necessary for explaining parsing and gen-eration examples are listed in the appendix.4.
Reversibility In Proposed Ar-chitecture4.1.
Sentence ParsingParsing techniques were simulated using anargumentation system in (Shimazu,1990).Since he faithfully tried to model existingtechniques, many parsing oriented termssuch as complete and addition were intro-duced.
This seems to be the cause of thedifficulty he experienced in integrating pars-ing with other processes.Argument (a):belasc ~?t(P) ..... > bel(S,P).Argument (b):beI(S,by(PE,E,C,PR)),int(S,PE),exp(S,C),exp 1 (S,PR)by2'..... > int(S,by(PE,E,C,PR)),int(S,E).1?The expression above the arrowindicates the class of an argument.123Since syntactic, semantic and pragmatic n-formation can be represented with the newby relation, arguments (a) and (b) enableus to simulate parsing: (a) says that truepropositions can be ascribed to a speaker'sbelief; (b) states that, if a speaker is assumedto believe that E is an effect of performingplan expansion PE, while constraint C istrue and precondition PR is assumed to betrue H, then it is plausible that his reasonfor doing PE is his intention to do E.Parsing is executed as follows: first,axioms whose constraints match an inputword are collected; second, the axiom whichsatisfies the constraint is selected (precondi-tions are asserted); third, an effect, or se-mantic information is derived using an in-stance of argument (b); fourth, another in-stance of the argument isapplied to the effectand the effect which was already derived toobtain a new effect.
If the application cannotproceed further, a new word is analyzed;Lastly, ff all words in a sentence are analyzedsuccessfully, the execution is complete.Parsing is exactly the same as planrecognition in the sense of (Konolige andPollack, 1989:925):Plan recognition is essentially a "bottom-up"recognition process, with global coherenceused mostly as an evaluative measure, toeliminate ambiguous plan fragments thatemerge.from local cues.Maximizing head elements can realizeright association and minimal attachment,but handling semantic ambiguities, that is toclarify global coherence, is a further issue.4.2.
Sentence GenerationGeneration can be simulated using arguments(a) and (c).
(c) says that, if a speaker believesthat E is an effect of performing plan expan-sion PE, while constraint C is true and pre-condition PR is assumed to be true, and heintends to do E, then it is plausible that hisintention PE is to achieve E.HAction expression expl(A,P)means that agent A expects proposition Pto be assumed to be true if not fully satisfied.Argument (c):bel(S,by(PE,E,C,PR)),int(S,E),exp(S ,C),exp 1 (S,PR)by3'..... > int(by(PE,E,C,PR)),int(S,PE)Generation is executed in a similar wayto parsing except hat axioms are collectedusing semantic information and the result isa string 12.
Figure 4 describes the generationprocess.
The input linguistic object is equiv-alent o the object in Figure 2 whose surfacestring infoiTnation is parametefized.
The gen-eration result is the input object with theinstafiated surface string, that is Figure 1.In Figure 4, axiom (2) creates ubgoalsrelated to the variable Y and others (cor-responding to (2) and (3)) because the se-mantic and pragmatic nformation ofthe inputequals the effect and preconditions of theaxiom.
As the head features propagate tothe linguistic object (2), execution addressingobject (2) is preferred.
The axiom (6) con-stnacts ubgoals by referring to the objectswhose semantic information is related to thefeatures qnt, vat, rest and the logical formconcerned with the bound variable X. Thehead feature preference makes the generatorexecute axioms about object (4).
This resultsin the axiom (1) of lexical information.
Sim-ilar to the above process, the remaining sub-goals (5) and (2) are executed.
Finally, thesurface string "It was a girl that the boyloved" is obtained.5.
DiscussionsAs mentioned in (Emele and Zajac,1990),the proposed approach inevitably leads tothe consequence that the data structure be-comes slightly complicated.
However, due12Because of space limitation, actionexpressions inform and utter are omittedin the figure.124(2)02) I(14)(1)1 Sign:phn:"It syn:pos:verb sla h:sl(\[\]) ubcat:subc(\[D w "+Exp 1 + " t h a t " + E x p 2 , ~sem:qnt:indefiniteSign:phn:Explsyn:Sy/flsem:qnhdefinitev~:Yrest:arg0:Y: pred:GIRLvar:Xrest:argO:Xpred:BOYbody:qnt:definitevar:Yrest:argO:Ypred:GIRLbody:argO:Xargl:Ypred:LOVEDprag: \[\] prag: \[novel(Y),unique(Y)\]Sign:phn:Expl 1syn:Synl isem:qnt:definitevar:Yping:I\]ISign:phn:"~e"syn:l~s:deisem:qnt:definitevar:Yprag:\[\](11).l(13) ISign:phn:Expl2syn:Synlsem:rest:arg0:Yprag:\[\]Sign:phn:"girl"syn:pos:nounsem:rest:arg0:Yprag:\[\](5)Sign:phn:Exp211syn:Syn211sem:qnt:indefiniteprag:\[\](3) Sign:phn:Exp2syn:pos:verbsubcat:subc(\])slash:sl(\[Obj\])sem:qnt:indefinitevar:Xrest:arg0:Xpred:BOYbody:arg0:Xargl:Ypred:LOVEDi(10~ Sign:phn:"a"il syn:pos:det| sem:qnt:indefinite:1 var:Xii prag:\[\]prag:\[\]pred:GIRLpred:GIRLSign:phn:Exp21 (4)syn:Syn21sem:qnt:indefinitevar:Xrest:arg0:Xpred:BOYprag:\[\](6),...-.-------""-N(7) I Sign:phn:Exp212\[ syn:Syn21I sem:rest:arg0:Xvar:X I pred:BOY\[ prag:\[\]I I(9) \[ Sign:phn:"boy"\[ syn:pos:noun\[ sem:rest:arg0~XI prag:\[\] pred.BOYSign:phn:Exp22syn:pos:verbsubcat:subc(\[Sbj,Obj\]sem:argO:Xargl:Ypred:LOVEDprag:\[\]ISign:phn:"loved"syn:pos:verbsubcat:subc(\[S bj,Obj\]sem:arg0:Xarg 1 :Ypred:LOVEDprag:\[\]Figure 4.
Generation Example.125to the segregation of the structure such asthe distinction between preconditions andconstraints, the task of developing rules canbe done independently.
Thus, if you want,you can concentrate ondeveloping rammarrules irrespective of the pragmatic informa-tion.
If desired, however, pragmatics canbe used to precisely stipulate some linguisticphenomena (Delin, 1990).The semantic representation utilized heredepends on the strong assumption that itcan be systematically decomposed.
It is ad-vantageous that the assumption supportssymmetry as discussed in (Noord,1990)and naturally realizes semantic indexingwhich leads to efficient processing(Calder, 1989).
However, it limits the repre-sentation capability for semantic processing(Shieber et a1.,1989).
The problems of se-mantic representation are still difficult, sotheir study is an ongoing task.Syntactic information, or grammarrules in the paper is neutral in the sensethat only one kind of rules, or axioms aresufficient for both parsing and generation;but their difference lies in their usage ofinformation.
In the case of parsing, syntacticinformation is used as a local cue to derivethe semantic and pragmatic nformation.
Inthe case of generation, it is used to preventthe production of ungrammatical strings.This difference appears to mirror asymmetrybetween writing and reading (or speakingand hearing).
The reading process lets un-known words be hypothesized by referringto neighboring words that are understood.Writing, on the other hand, is a process inwhich unknown words cannot be developedby examining adjacent words.
Hypothesiz-ing is used both for parsing and generation,but its role in these processes i different.
Itis used to derive a coherent interpretationfrom all words in the parsing, while it isused to smooth the conversational (or text)flow in generation.
The difference of thehypothesis use seems to be one of the factorsin explaining this asymmetry.
The proposedarchitecture is certain to provide a basis toexamine this claim in the sense that it inte-grates linguistic processes with a reasoningmechanism.6.
ConclusionThis paper has proposed a reversible archi-tecture to handle not only syntactic and se-mantic information but also pragmatic nfor-mation.
Existing architectures cannot repre-sent pragmatic information explicitly, andlack reasoning capability given insufficientinformation.
I argue that the techniques ofplan representation a d approximate r ason-ing in the enhanced argumentation systemare effective for solving these problems.AcknowledgmentsThe author wishes to extend his sincere grat-itude to Tsuneaki Kato and Yoshihiko Ha-yashi for discussing this architecture.
Healso thank Masanobu Higashida nd YoichiSakai for encouraging him to study this sub-ject.AppendixAxiom (3):t(by(utter(S,H,"the"),inform(S,H,definite),((Pos=det),(Adjacent=noun)),0)).Axiom (4):t(by(utter(S,H,"boy"),inform(S,H,BOY),(Pos=noun),0)).Axiom (5):t(by(utter(S,H,"a"),in form (S,H,indefinite),((Pos=det),(Adjacent=noun)),0)).126Axiom (6):t(by(utter(S,H,"girl"),inform(S,H,GIRL),(Pos=noun),0)).Axiom (7):t(by((inform(S,H,LF 11),inform(S,H,LF12)),inform(S,H,LF1),((Posl=Pos\]2),(Phn I =Phn 1 l+Phn 12)),0)).Axiom (8):t(by((inform(S,H,LF21),inform(S,H,LF22)),inform(S,H,LF2),((Pos2=Pos21),(Subcat2=~subc(\[\])),(Slash2=s!
(\[Obj\])),(Subcat22~subc(\[Sbj,Obj\])),(Phn2=Phh21+Phn22)),0)).Bibliography(Appelt,1985i Douglas E. Appelt.
1985.
"Planning English Sentences".
CambridgeUniversity Press.
(Calder et a!
:.,1989) Jonathan Calder,Mike Reape, and Henk Zeevat.
1989.
"AnAlgorithm for Generation i  Unification Cat-egorial Gramme".
Proceedings of the 4thConference of the European Chapter of theAssociation for Computational Linguistics.Manchester, U.K.. 10-12, April.
233-240.
(Delin,1990) J.L.Delin.
1990.
"A Multi-Level Account 0f Cleft Constructions inDis-course".
Proceedings of the 13th Interna-tional Conference on Computational Linguis-tics.
Aug. 20-25.
Helsinki, Finland.
83-88.
(Dymetman et a1.,1989) Marc Dyrnet-man and Pierre Isabelle.
1989.
"ReversibleLogic Grammars For Machine Translation".Proceedings ofthe 2nd International Confer-ence on Theoretical nd Methodological Is-sues In Machine Translation of Natural Lan-guages.
Jun.
12-14.
Pittsburgh, Pennsylva-nia, U.S.A..(Elkan,1990) Charles Elkan.
1990.
"In-cremental, Approximate Planning".
Pro-ceedings of the 8th National Conference onArtificial Intelligence.
Jul.29-Aug.3.
Bos-ton, MA, U.S.A..
145-150.
(Emele and Zajac,1990) Martin Emeleand Remi Zajac.
1990.
"Typed UnificationGrammars".
Proceedings of the 13th Inter-national Conference on Computational Lin-guistics.
Aug. 20-25.
Helsinki, Finland.293-298.
(Gazdar et al, 1989) Gerald Gazdar andChris Mellish.
1989.
"Natural LanguageProcessing in PROLOG".
Addison-WelsleyPublishing Company.
(Has ida ,1987)  Hasida Koiti.
1987.
"De-pendency Propagation: A Unified Theoryof Sentence Comprehension a d Genera-tion".
Proceedings of the 10th InternationalJoint Conference on Artificial Intelligence.Aug.
23-28.
Milan, Italy.
664-670.
(Konol ige and Po l lack ,1989)Kur tKonolige and Martha E. Pollack.
1989.
"As-cribing Plans To Agents - Preliminary Report- .
Proceedings of the 11th InternationalJoint Conference on Artificial Intelligence.Aug.
20-25.
Detroit, Michigan, U.S.A..924-930.
(Moore et a1,1989) JohannaD.
Mooreand Cecile Paris.
1989.
"Planning Text ForAdvisory Dialogue".
Proceedings of the 27thAnnual Meeting of the Association for Com-putational Linguistics.
Jun.
26-29.
Van cou-ver, British Columbia, Canada.
203-211.
(Pollard et a1.,1987) Carl Pollard andIvan A.
Sag.
1987.
"An Information-BasedSyntax and Semantics (Volume 1)".
CSLI127Lecture Notes.
Number 13.
(Pollard et a!.,1990) Carl Pollard andIvan A.
Sag.
1990.
"An Information-BasedSyntax and Semantics (Volume 2)".
ms.(Shieber,1988) Stuart M. Shieber.
1988.
"A Uniform Architecture For Parsing andGeneration".
Proceedings of the 12th Inter-national Conference on Computational Lin-guistics.
Aug. 22-27.
Bonn, Germany.
614-619.
(Shieber et a1.,1989) Stuart M. Shieber,Gertjan van Noord, Robert C. Moore, andFemando C.N.
Perreira.
1989.
"A Semantic-Head-Driven Generation Algorithms forUnification-Based Formalisms".
Proceed-ings of the 27th Annual Meeting of the As-sociation for Computational Linguistics.Jun.
26-29.
Van couver, British Columbia,Canada.
7-17.
(Shimazu,1990) Akira Shimazu.
1990.
"Japanese Sentence Analysis as Argumenta-tion".
Proceedings of the 13th InternationalConference on Computational Linguistics.Aug.
20-25.
Helsinki, Finland.
259-264.
(Strzalkowski ,1990) Tomek Strza-lkowski.
1990.
"How To Invert A NaturalLanguage Parser Into An Efficient Generator.An Algorithm For Logic Grammars".
Pro-ceedings of the 13th Intemational Conferenceon Computational Linguistics.
Aug. 20-25.Helsinki, Finland.
347-352.
(Woods,1978) W.A.Woods.
1978.
"Se-mantics and Quantification i Natural Lan-guage Question Answering".
Advances inComputers.
Vol.
17.
M.Yovits (ed.).
Aca-demic Press.128
