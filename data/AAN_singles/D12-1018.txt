Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 194?204, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLearning Verb Inference Rules from Linguistically-Motivated EvidenceHila Weisman?, Jonathan Berant?, Idan Szpektor?, Ido Dagan??
Computer Science Department, Bar-Ilan University?
The Blavatnik School of Computer Science, Tel Aviv University?
Yahoo!
Research Israel{weismah1,dagan}@cs.biu.ac.il{jonatha6}@post.tau.ac.il{idan}@yahoo-inc.comAbstractLearning inference relations between verbs isat the heart of many semantic applications.However, most prior work on learning suchrules focused on a rather narrow set of in-formation sources: mainly distributional sim-ilarity, and to a lesser extent manually con-structed verb co-occurrence patterns.
In thispaper, we claim that it is imperative to uti-lize information from various textual scopes:verb co-occurrence within a sentence, verb co-occurrence within a document, as well as over-all corpus statistics.
To this end, we proposea much richer novel set of linguistically mo-tivated cues for detecting entailment betweenverbs and combine them as features in a su-pervised classification framework.
We empir-ically demonstrate that our model significantlyoutperforms previous methods and that infor-mation from each textual scope contributes tothe verb entailment learning task.1 IntroductionInference rules are an important building block ofmany semantic applications, such as Question An-swering (Ravichandran and Hovy, 2002) and In-formation Extraction (Shinyama and Sekine, 2006).For example, given the sentence ?Churros arecoated with sugar?, one can use the rule ?coat ?cover?
to answer the question ?What are Churroscovered with??.
Inference rules specify a directionalinference relation between two text fragments, andwe follow the Textual Entailment modeling of infer-ence (Dagan et al2006), which refers to such rulesas entailment rules.
In this work we focus on oneof the most important rule types, namely, lexical en-tailment rules between verbs (verb entailment), e.g.,?whisper ?
talk?, ?win ?
play?
and ?buy ?
own?.The significance of such rules has led to active re-search in automatic learning of entailment rules be-tween verbs or verb-like structures (Zanzotto et al2006; Abe et al2008; Schoenmackers et al2010).Most prior efforts to learn verb entailment rulesfrom large corpora employed distributional similar-ity methods, assuming that verbs are semanticallysimilar if they occur in similar contexts (Lin, 1998;Berant et al2012).
This led to the automatic ac-quisition of large scale knowledge bases, but withlimited precision.
Fewer works, such as VerbOcean(Chklovski and Pantel, 2004), focused on identi-fying verb entailment through verb instantiation ofmanually constructed patterns.
For example, thesentence ?he scared and even startled me?
impliesthat ?startle?
scare?.
This led to more precise ruleextraction, but with poor coverage since contraryto nouns, in which patterns are common (Hearst,1992), verbs do not co-occur often within rigid pat-terns.
However, verbs do tend to co-occur in thesame document, and also in different clauses of thesame sentence.In this paper, we claim that on top of standardpattern-based and distributional similarity methods,corpus-based learning of verb entailment can greatlybenefit from exploiting additional linguistically-motivated cues that are specific to verbs.
For in-stance, when verbs co-occur in different clauses ofthe same sentence, the syntactic relation between theclauses can be viewed as a proxy for the semantic re-lation between the verbs.
Moreover, we claim that to194improve performance it is crucial to combine infor-mation sources from different textual scopes: verbco-occurrence within a sentence and within a docu-ment, distributional similarity over the entire corpus,etc.Our contribution in this paper is two-fold.
First,we suggest a novel set of entailment indicators thathelp to detect the likelihood of verb entailment.Our novel indicators are specific to verbs and arelinguistically-motivated.
Second, we encode ournovel indicators as features within a supervised clas-sification framework and integrate them with otherstandard features adapted from prior work.
This re-sults in a supervised corpus-based learning methodthat combines verb entailment information at thesentence, document and corpus levels.We test our model on a manually labeled dataset, and show that it outperforms the best perform-ing previous work by 24%.
In addition, we ex-amine the effectiveness of indicators that operate atthe sentence-level, document-level and corpus-level.This analysis reveals that using a rich and diverseset of indicators that capture sentence-level interac-tions between verbs substantially improves verb en-tailment detection.2 BackgroundThe main approach for learning entailment rules be-tween verbs and verb-like structures has employedthe distributional hypothesis, which assumes thatwords with similar meanings appear in similar con-texts.
For example, we expect the words ?buy?
and?purchase?
to occur with similar subjects and objectsin a large corpus.
This observation has led to amplework on developing both symmetric and directionalsimilarity measures that attempt to capture semanticrelations between lexical items by comparing theirneighborhood context (Lin, 1998; Weeds and Weir,2003; Geffet and Dagan, 2005; Szpektor and Dagan,2008; Kotlerman et al2010).A far less explored direction for learning verb en-tailment involves exploiting verb co-occurrence ina sentence or a document.
One prominent workis Chklovsky and Pantel?s VerbOcean (2004).
InVerbOcean, the authors manually constructed 33patterns and divided them into five pattern groups,where each group signals one of the following fivesemantic relations: similarity, strength, antonymy,enablement and happens-before.
For example, thepattern ?Xed and later Yed?
signals the happens-before relation between the verbs ?X?
and ?Y?.
Start-ing with candidate verb pairs based on a distribu-tional similarity measure, the patterns are used tochoose a semantic relation per verb pair based onthe different patterns this pair instantiates.
Thismethod is more precise than distributional similarityapproaches, but it is highly susceptible to sparsenessissues, since verbs do not typically co-occur withinrigid patterns.
Utilizing verb co-occurrence at thedocument level, Chambers and Jurafsky (2008) es-timate whether a pair of verbs is narratively relatedby counting the number of times the verbs share anargument in the same document.
In a similar man-ner, Pekar (2008) detects entailment rules betweentemplates from shared arguments within discourse-related clauses in the same document.Recently, supervised classification has becomestandard in performing various semantic tasks.Mirkin et al2006) introduced a system for learn-ing entailment rules between nouns (e.g., ?novel?book?)
that combines distributional similarity andHearst patterns as features in a supervised clas-sifier.
Pennacchiotti and Pantel (2009) augmentMirkin et alfeatures with web-based features forthe task of entity extraction.
Hagiwara et al2009)perform synonym identification based on both dis-tributional and contextual features.
Tremper (2010)extract ?loose?
sentence-level features in order toidentify the presupposition relation (e.g., , the verb?win?
presupposes the verb ?play?).
Last, Be-rant et al2012) utilized various distributionalsimilarity features to identify entailment betweenlexical-syntactic predicates.In this paper, we follow the supervised approachfor semantic relation detection in order to identifyverb entailment.
While we utilize and adapt usefulfeatures from prior work, we introduce a diverse setof novel features for the task, effectively combiningverb co-occurrence information at the sentence, doc-ument, and corpus levels.3 Linguistically-Motivated IndicatorsAs mentioned in Section 1, verbs behave quite dif-ferently from nouns in corpora.
In this section, we195introduce linguistically motivated indicators that arespecific to verbs and may signal the semantic re-lation between verb pairs.
Then, in Section 4 wedescribe how these indicators are exactly encodedas features within a supervised classification frame-work.Verb co-occurrence When (non-auxiliary) verbsco-occur in a sentence, they are often the main verbsof different clauses.
We thus aim to use informationabout the relation between clauses to learn aboutthe relation between the clauses?
main verbs.
Dis-course markers (Hobbs, 1979; Schiffrin, 1988) arelexical terms such as ?because?
and ?however?
thatindicate a semantic relation between discourse frag-ments (i.e., propositions or speech acts).
We suggestthat these markers can indicate semantic relationsbetween the main verbs of the connected clauses.For example, in the sentence ?He always snoreswhile he sleeps?, the marker ?while?
indicates a tem-poral relation between the clauses, indicating that?snoring?
occurs while ?sleeping?
(and so ?snore?sleep?
).Often the relation between clauses is not ex-pressed explicitly with an overt discourse marker,but is still implied by the syntactic structure ofthe sentence.
For example, in dependency parsingthe relation can be captured by labeled dependencyedges expressing that one clause is an adverbial ad-junct of the other, or that two clauses are coordi-nated.
This can indicate the existence (or lack) ofentailment between verbs.
For instance, in the sen-tence ?When I walked into the room, he was workingout?, the verb ?walk?
is an adverbial adjunct of theverb ?work out?.
Such co-occurrence structure doesnot indicate a deep semantic relation, such as entail-ment, between the two verbs.Verb classes Verb classes are sets of semantically-related verbs sharing some linguistic properties(Levin, 1993).
One of the most general verb classesare stative vs. event verbs (Jackendoff, 1983).
Sta-tive verb, such as ?love?
and ?think?, usually describea state that lasts some time.
On the other hand, eventverbs, such as ?run?
and ?kiss?, describe an action.We hypothesize that verb classes are relevant for de-termining entailment, for example, that stative verbsare not likely to entail event verbs.Verb generality Verb-particle constructions aremulti-word expressions consisting of a head verband a particle, e.g., switch off (Baldwin and Villav-icencio, 2002).
We conjecture that the more gen-eral a verb is, the more likely it is to appear withmany different particles.
Detecting verb generalitycan help us tackle an infamous property of distribu-tional similarity methods, namely, the difficulty indetecting the direction of entailment (Berant et al2012).
For example, the verb ?cover?
appears withmany different particles such as ?up?
and ?for?, whilethe verb ?coat?
does not.
Thus, assuming we haveevidence for an entailment relation between the twoverbs, this indicator can help us discern the directionof entailment and determine that ?coat?
cover?.Typed Distributional Similarity As discussed insection 2, distributional similarity is the most com-mon source of information for learning semantic re-lations between verbs.
Yet, we suggest that on topof standard distributional similarity measures, whichtake several verbal arguments into account (such assubject, object, etc.)
simultaneously, we should alsofocus on each type of argument independently.
Inparticular, we apply this approach to compute simi-larity between verbs based on the set of adverbs thatmodify them.
Our hypothesis is that adverbs maycontain relevant information for capturing the direc-tion of entailment.
If a verb appears with a small setof adverbs, it is more likely to be a specific verb thatalready conveys a specific action or state, making anadditional adverb redundant.
For example, the verb?whisper?
conveys a specific manner of talking andwill probably not appear with the adverb ?loudly?,while the verb ?talk?
is more likely to appear withsuch an adverb.
Thus, measuring similarity basedsolely on adverb modifiers could reveal this phe-nomenon.4 Supervised Entailment DetectionIn the previous section, we discussed linguistic ob-servations regarding novel indicators that may helpin detecting entailment relations between verbs.
Wenext describe how to incorporate these indicators asfeatures within a supervised framework for learninglexical entailment rules between verbs.
We followprior work on supervised lexical semantics (Mirkinet al2006; Hagiwara et al2009; Tremper, 2010)196and address the rule learning task as a classificationtask.
Specifically, given an ordered verb pair (v1, v2)as input, we learn a classifier that detects whether theentailment relation ?v1?
v2?
holds for this pair.We next detail how our novel indicators, as wellas other diverse sources of information found usefulin prior work, are encoded as features.
Then, wedescribe the learning model and our feature analysisprocedure.4.1 Entailment featuresMost of our features are based on information ex-tracted from the target verb pair co-occurring withinvarying textual scopes (sentence, document, cor-pus).
Hence, we group the features according totheir related scope.
Naturally, when the scope issmall, i.e., at a sentence level, the semantic rela-tion between the verbs is easier to discern but theinformation may be sparse.
Conversely, when co-occurrence is loose the relation is harder to discernbut coverage is increased.4.1.1 Sentence-level co-occurrenceWe next detail features that address co-occurrenceof the target verb pair within a sentence.
These in-clude our novel linguistically-motivated indicators,as well as features that were adapted from priorwork.Discourse markers As discussed in Section 3,discourse markers may signal relations between themain verbs of adjacent clauses.
The literature isabundant with taxonomies that classify markers tovarious discourse relations (Mann and Thompson,1988; Hovy and Maier, 1993; Knott and Sanders,1998).
Inspired by Marcu and Echihabi (2002), weemploy markers that are mapped to four discourserelations ?Contrast?, ?Cause?, ?Condition?
and ?Tem-poral?, as specified in Table 1.
This definitioncan be viewed as a relaxed version of VerbOcean?s(Chklovski and Pantel, 2004) patterns, although theunderlying intuition is different (see Section 3).For a target verb pair (v1, v2) and each discourserelation r, we count the number of times that v1 isthe main verb in the main clause, v2 is the main verbin the subordinate clause, and the clauses are con-nected via a marker mapped to r. For example, giventhe sentence ?You must enroll in the competition be-fore you can participate in it?, the verb pair (?en-roll?,?participate?)
appears in the ?Temporal?
rela-tion, indicated by the marker ?before?, where ?enroll?is in the main clause.
Each count is then normalizedby the total number of times (v1, v2) appear with anymarker.
The same procedure is done when v1 is inthe subordinate clause and v2 in the main clause.
Weterm the features by the relevant discourse relation,e.g., ?v1-contrast-v2?
refers to v1 being in the mainclause and connected to the subordinate clause via acontrast marker.Dependency relations between clauses As notedin Section 3, the syntactic structure of verb co-occurrence can indicate the existence or lack of en-tailment.
In dependency parsing this may be ex-pressed via the label of the dependency relation con-necting the main and subordinate clauses.
In our ex-periments we used the ukWaC corpus1 (Baroni et al2009) which was parsed by the MALT parser (Nivreet al2006).
Hence, we identified three MALT de-pendency relations that connect a main clause withits subordinate clause.
The first relation is the objectcomplement relation ?obj?.
In this case the subor-dinate clause is an object complement of the mainclause.
For example, in ?it surprised me that thelizard could talk?
the verb pair (?surprise?,?talk?)
isconnected by the ?obj?
relation.
The second rela-tion is the adverbial adjunct relation ?adv?, in whichthe subordinate clause is adverbial and describes thetime, place, manner, etc.
of the main clause, e.g., ?hegave his consent without thinking about the reper-cussions?.
The last relation is the coordination rela-tion ?coord?, e.g., ?every night my dog Lucky sleepson the bed and my cat Flippers naps in the bathtub?.Similar to discourse markers, we compute foreach verb pair (v1,v2) and each dependency label dthe proportion of times that v1 is the main verb of themain clause, v2 is the main verb of the subordinateclause, and the clauses are connected by dependencyrelation d, out of all the times they are connected byany dependency relation.
We term the features bythe dependency label, e.g., ?v1-adv-v2?
refers to v1being in the main clause and connected to the subor-dinate clause via an adverbial adjunct.1http://wacky.sslmit.unibo.it/doku.php?id=corpora197Discourse Rel.
Discourse MarkersContrast although , despite , but , whereas , notwithstanding , thoughCause because , therefore , thusCondition if , unlessTemporal whenever , after , before , until , when , finally , during , afterwards , meanwhileTable 1: Discourse relations and their mapped markers.Pattern-based We follow Chklovski and Pan-tel (2004) and extract occurrences of VerbOcean pat-terns that are instantiated by the target verb pair.
Asmentioned in Section 2, VerbOcean patterns wereoriginally grouped into five semantic classes.
Basedon a preliminary study we conducted, we decidedto utilize only four strength-class patterns as posi-tive indicators for entailment, e.g., ?he scared andeven startled me?, and three antonym-class patternsas negative indicators for entailment, e.g., ?you caneither open or close the door?.
We note that thesepatterns are also commonly used by RTE systems2.Since the corpus pattern counts were very sparse,we defined for a target verb pair (v1, v2) two bi-nary features: the first denotes whether the verbpair instantiates at least one positive pattern, andthe second denotes whether the verb pair instanti-ates at least one negative pattern.
For example, giventhe aforementioned sentences, the value of the pos-itive feature for the verb pair (?startle?,?scare?)
is?1?.
Patterns are directional, and so the value of(?scare?,?startle?)
is ?0?.Polarity We compute the proportion of times thatthe two verbs appear in different polarity.
For exam-ple, in ?he didn?t say why he left?, the verb ?say?
ap-pears in negative polarity and the verb ?leave?
in pos-itive polarity.
Such change in polarity is usually anindicator of non-entailment between the two verbs.Tense ordering The temporal relation betweenverbs may provide information about their seman-tic relation.
For each verb pair co-occurrence, weextract the verbs?
tenses and order them as follows:past < present < future.
We then add the fea-tures ?tense-v1<tense-v2?, ?tense-v1=tense-v2?, and?tense-v1>tense-v2?, corresponding to the propor-2http://aclweb.org/aclwiki/index.php?title=RTE_Knowledge_Resources#Ablation_Teststion of times the tense of v1 is smaller, equal to,or bigger than the tense of v2.
This indicates theprevalent temporal relation between the verbs in thecorpus and may assist in detecting the direction ofentailment.
e.g., if tense-v1>tense-v2, the verb pairis less likely to entail.Co-reference Following Tremper (2010), in everyco-occurrence of (v1,v2) we extract for each verbthe set of arguments at either the subject or objectpositions, denoted A1 and A2 (for v1 and v2, re-spectively).
We then compute the proportion of co-occurrences in which v1 and v2 share an argument,i.e., A1 ?
A2 6= ?, out of all the co-occurrences inwhich bothA1 andA2 are non-empty.
The intuition,which is similar to distributional similarity, is thatsemantically related verbs tend to share arguments.Syntactic and lexical distance Following Trem-per (2010) again, we compute the average distanced in dependency edges between the co-occurringverbs.
We compute three features corresponding tothree bins indicating if d < 3, 3 ?
d ?
7, ord > 7.
Similar features are computed for the dis-tance in words (bins are 0 < d < 5, 5 ?
d ?
10,d > 10).
This feature provides insight into the syn-tactic relatedness of the verbs.Sentence-level pmi Pointwise mutual information(pmi) between v1 and v2 is computed, where the co-occurrence scope is a sentence.
Higher pmi shouldhint at semantically related verbs.4.1.2 Document-level co-occurrenceThis group of features addresses co-occurrence ofa target verb pair within the same document.
Thesefeatures are less sparse, but tend to capture coarsersemantic relations between the target verbs.Narrative score Chambers and Jurafsky (2008)suggested a method for learning sequences of ac-tions or events (expressed by verbs) in which a sin-198gle entity is involved.
They proposed a pmi-like nar-rative score (see Eq.
(1) in their paper) that esti-mates whether a pair consisting of a verb and oneof its dependency relations (v1, r1) is narratively-related to another such pair (v2, r2).
Their estima-tion is based on quantifying the likelihood that twoverbs will share an argument that instantiates boththe dependency position (v1, r1) and (v2, r2) withindocuments in which the two verbs co-occur.
For ex-ample, given the document ?Lindsay was prosecutedfor DUI.
Lindsay was convicted of DUI.?
the pairs(?prosecute?,?subj?)
and (?convict?,?subj?)
share theargument ?Lindsay?
and are part of a narrative chain.Such narrative relations may provide cues to the se-mantic relatedness of the verb pair.We compute for every target verb pair nine fea-tures using their narrative score.
In four features,r1 = r2 and the common dependency is either a sub-ject, an object, a preposition complement (e.g., ?wemeet at the station.
), or an adverb (termed chamb-subj, chamb-obj, and so on).
In the next three fea-tures, r1 6= r2 and r1, r2 denote either a subject,object, or preposition complement3 (termed chamb-subj-obj and so on).
Last, we add as features theaverage of the four features where r1 = r2 (termedchamb-same), and the average of the three featureswhere r1 6= r2 (termed chamb-diff ).Document-level pmi Similar to sentence-levelpmi, we compute the pmi between v1 and v2, butthis time the co-occurrence scope is a document.4.1.3 Corpus-level statisticsThe final group of features ignores sentence ordocument boundaries and is based on overall corpusstatistics.Distributional similarity Following our hypoth-esis regarding typed distributional similarity (Sec-tion 3), we first compute for each verb and eachargument (subject, object, preposition complementand adverb) a separate vector that counts the num-ber of times each word in the corpus instantiatesthe argument of that verb.
In addition, we alsocompute a vector that is the concatenation of theprevious separate vectors, which captures the stan-dard distributional similarity statistics.
We then3adverbs never instantiate the subject, object or prepositioncomplement positions.apply three state-of-the-art distributional similaritymeasures, Lin (Lin, 1998), Weeds precision (Weedsand Weir, 2003) and BInc (Szpektor and Dagan,2008), to compute for every verb pair a similarityscore between each of the five count vectors4.
Weterm each feature by the method and argument, e.g.,weeds-prep and lin-all represent the Weeds measureover prepositional complements and the Lin mea-sure over all arguments.Verb classes Following our discussion in Sec-tion 3, we first measure for each target verb v a ?sta-tive?
feature f by computing the proportion of timesit appears in progressive tense, since stative verbsusually do not appear in the progressive tense (e.g.,?knowing?).
Then, given a verb pair (v1,v2) and theircorresponding stative features f1 and f2, we add twofeatures f1 ?
f2 andf1f2, which capture the interactionbetween the verb classes of the two verbs.Verb generality For each verb, we add as a featurethe number of different particles it appears with inthe corpus, following the hypothesis that this is acue to its generality.
Then, given a verb pair (v1,v2)and their corresponding features f1 and f2, we addthe feature f1f2 .
We expect that whenf1f2is high, v1 ismore general than v2, which is a negative entailmentindicator.4.2 Learning model and feature analysisThe total number of features in our model as de-scribed above is 63.
We combine the features ina supervised classification framework with a linearSVM.
Since our model contains many novel fea-tures, it is important to investigate their utility fordetecting verb entailment.
To that end, we employfeature ranking methods as suggested by Guyon etal.
(2003).
In feature ranking methods, features areranked by some score computed for each feature in-dependently.
In this paper we use Pearson correla-tion between the feature values and the correspond-ing labels as the ranking criterion.4We employ the common practice of using the pmi betweena verb and an argument rather than the argument count as theargument?s weight.1995 Evaluation and Analysis5.1 Experimental SettingTo evaluate our proposed supervised model, we con-structed a dataset containing labeled verb pairs.
Westarted by randomly sampling 50 verbs out of thecommon verbs in the RCV1 corpus5, which we de-note here as seed verbs.
Next, we extracted the 20most similar verbs to each seed verb according tothe Lin similarity measure (Lin, 1998), which wascomputed on the RCV1 corpus.
Then, for each seedverb vs and one of its extracted similar verbs vis wegenerated the two directed pairs (vs, vis) and (vis, vs),which represent the candidate rules ?vs ?
vis?
and?vis ?
vs?
respectively.
To reduce noise, we filteredout verb pairs where one of the verbs is an auxiliaryor a light verb such as ?do?, ?get?
and ?have?.
Thisstep resulted in 812 verb pairs as our dataset6, whichwere manually annotated by the authors as repre-senting a valid entailment rule or not.
To annotatethese pairs, we generally followed the rule-based ap-proach for entailment rule annotation, where a rule?v1 ?
v2?
is considered as correct if the annotatorcould think of reasonable contexts under which therule holds (Dekang and Pantel, 2001; Szpektor etal., 2004).
In total 225 verb pairs were labeled asentailing (the rule ?v1 ?
v2?
was judged as correct)and 587 verb pairs were labeled as non-entailing (therule ?v1 ?
v2?
was judged as incorrect).
The Inter-Annotator Agreement (IAA) for a random sample of100 pairs was moderate (0.47), as expected from therule-based approach (Szpektor et al2007).For each verb pair, all 63 features within ourmodel (Section 4) were computed using the ukWaCcorpus (Baroni et al2009), which contains 2 billionwords.
For classification, we utilized SVM-perf?s(Joachims, 2005) linear SVM implementation withdefault parameters, and evaluated our model by per-forming 10-fold cross validation (CV) over the la-beled dataset.5http://trec.nist.gov/data/reuters/reuters.html6The data set is available at http://www.cs.biu.ac.il/?nlp/downloads/verb-pair-annotation.html5.2 Feature selection and analysisAs discussed in Section 4.2, we followed the featureranking method proposed by Guyon et al2003) toinvestigate the utility of our proposed features.
Ta-ble 2 depicts the 10 most positively and negativelycorrelated features with entailment according to thePearson correlation measureFrom Table 2, it is clear that distributional simi-larity features are amongst the most positively cor-related with entailment, which is in line with priorwork (Geffet and Dagan, 2005; Kotlerman et al2010).
Looking more closely, our suggestion fortyped distributional similarity proved to be useful,and indeed most of the highly correlated distribu-tional similarity features are typed measures.
Stand-ing out are the adverb-typed measures, with two fea-tures in the top 10, including the highest, ?Weeds-adverb?, and ?BInc-adverb?.
We also note that thehighly correlated distributional similarity measuresare directional, Weeds and BInc.The table also indicates that document-level co-occurrence contributes positively to entailment de-tection.
This includes both the Chambers narrativemeasure, with the typed feature Chambers-obj, anddocument-level PMI, which captures a more looseco-occurrence relationship between verbs.
Again,we point at the significant correlation of our noveltyped measures with verb entailment, in this case thetyped narrative measure.Last, our feature analysis shows that many of ournovel co-occurrence features at the sentence levelcontribute useful negative information.
For exam-ple, verbs connected via an adverbial adjunct (?v2-adverb-v1?)
or an object complement (?v1-obj-v2?
)are negatively correlated with entailment.
In addi-tion, the novel ?verb generality?
feature as well asthe tense difference feature (?tense-v1 > tense-v2?
)are also strong negative indicators.
On the otherhand, ?v2-coord-v1?
is positively correlated with en-tailment.
This shows that encoding various aspectsof verb co-occurrence at the sentence level can leadto better prediction of verb entailment.
Finally, wenote that PMI at the sentence level is highly corre-lated with entailment even more than at the docu-ment level, since the local textual scope is more in-dicative, though sparser.To conclude, our feature analysis shows that fea-200Rank Top Positive Top Negative1 Weeds-adverb tense-v1 > tense-v22 Sentence-level PMI v2-adverb-v1 co-occurrence3 Weeds-subj v2-obj-v1 co-occurrence4 Weeds-prep v1-obj-v2 co-occurrence5 Weeds-all v1-adverb-v2 co-occurrence6 Chambers-obj verb generality f1f27 v2-coord-v1 co-occurrence v1-contrast-v28 BInc-adverb tense-v1 < tense-v29 Document-level PMI lexical-distance 0-510 Chambers-same Lin-subjTable 2: Top 10 positive and negative features according to the Pearson correlation score.tures at all levels: sentence, document and corpus,contain useful information for entailment detection,both positive and negative, and should be combinedtogether.
Moreover, many of our novel features areamong the highly correlated features, showing thatdevising a rich set of verb-specific and linguistically-motivated features provides better discriminative ev-idence for entailment detection.5.3 Results and AnalysisWe compared our method to the following baselineswhich were mostly taken from or inspired by priorwork:Random: A simple decision rule: for anypair (v1, v2), randomly classify as ?yes?
with aprobability equal to the number of entailing verbpairs out of all verb pairs in the labeled dataset (i.e.,225812 = 0.277).VO-KB: A simple unsupervised rule: for anypair (v1, v2), classify as ?yes?
if the pair appears inthe strength relation (corresponding to entailment)in the VerbOcean knowledge-base, which was com-puted over Web counts.VO-ukWaC: A simple unsupervised rule: for anypair (v1, v2), classify as ?yes?
if the value of thepositive VerbOcean feature is ?1?
(Section 4.1, com-puted over ukWaC).TDS: Include only the 15 distributional similarityfeatures in our supervised model.
This baseline ex-tends Berant et al2012), who trained an entailmentMethod P% R% AUC F1All 40.2 71.0 0.65 0.51TDS+VO 36.8 53.2 0.58 0.41TDS 34.6 44.8 0.56 0.37Random 27.9 28.8 0.51 0.28VO-KB 33.1 14.8 0.53 0.2VO-ukWaC 23.3 4.7 0.29 0.08Table 3: Average precision, recall, AUC and F1 for ourmethod and the baselines.classifier over several distributional similarity fea-tures, and provides an evaluation of the discrimina-tive power of distributional similarity alone, withoutco-occurrence features.TDS+VO: Include only the 15 typed distribu-tional similarity features and the two VerbOceanfeatures in our supervised model.
This baselineis inspired by Mirkin et al2006), who combineddistributional similarity features and Hearst pat-terns (Hearst, 1992) for learning entailment betweennouns.All: Our full-blown model, including all featuresdescribed in Section 4.1.For all tested methods, we performed 10-foldcross validation and averaged Precision, Recall,Area under the ROC curve (AUC) and F1 over the 10folds.
Table 3 presents the results of our full-blownmodel as well as the baselines.First, we note that, as expected, the VerbOceanbaselines VO-KB and VO-ukWaC provide low recall,201Method P% R% AUC F1All 40.2 71.0 0.65 0.51Sent+Corpus-level 39.7 70.4 0.64 0.50Sent+Doc-level 39.0 70.0 0.63 0.50Doc+Corpus-level 37.7 64.0 0.62 0.47Sent-level 35.8 63.8 0.59 0.46Doc-level 30.0 45.4 0.52 0.35Corpus-level 35.4 58.1 0.58 0.44Table 4: Average precision, recall, AUC and F1 for eachsubset of the feature groups.due to the sparseness of rigid pattern instantiationfor verbs both in the ukWaC corpus and on the web.Yet, VerbOcean positive and negative patterns doadd some discriminative power over only distribu-tional similarity measures, as seen by the improve-ment of TDS+VO over TDS in all criteria.
But, it isthe combination of all types of information sourcesthat yields the best performance.
Our completemodel, employing the full set of features, outper-forms all other models in terms of both precision andrecall.
Its improvement in terms of F1 over the sec-ond best model (TDS+VO), which includes all distri-butional similarity features as well as pattern-basedfeatures, is by 24%.
This result shows the benefitsof integrating linguistically motivated co-occurrencefeatures with traditional pattern-based and distribu-tional similarity information.To further investigate the contribution of fea-tures at various co-occurrence levels, we trainedand tested our model with all possible combina-tions of feature groups corresponding to a certainco-occurrence scope (sentence, document and cor-pus).
Table 4 presents the results of these tests.The most notable result of this analysis is thatsentence-level features play an important role withinour model.
Indeed, removing either the document-level features (Sent+Corpus-level) or the corpus-level features (Sent+Doc-level) results in only aslight decline in performance.
Yet, removing thesentence-level features (Doc+Corpus-level), ends ina more substantial decline of 8.5% in F1.
In addi-tion, sentence-level features alone (Sent-level) pro-vide the best discriminative power for verb entail-ment, compared to document and corpus levels,which include distributional similarity features.
Yet,we note that sentence-level features alone do notcapture all the information within our model, andthey should be combined with one of the other fea-ture groups to reach performance close to the com-plete model.
This shows again the importance ofcombining co-occurrence indicators at different lev-els.As an additional insight from Table 4, we pointout that document-level features are not good en-tailment indicators by themselves (Doc-level in Ta-ble 4), and they perform worse than the distribu-tional similarity baseline (TDS at Table 3).
Still, theydo complement each of the other feature groups.
Inparticular, since the Sent+Doc-level model performsalmost as good as the full model, this subset maybe a good substitute to the full model, since its fea-tures are easier to extract from large corpora, as theymay be extracted in an on-line fashion, processingone document at a time (contrary to corpus-level fea-tures).As a final analysis, we randomly sampled cor-rect entailment rules learned by our model butmissed by the typed distributional similarity classi-fier (TDS).
Our overall impression is that employ-ing co-occurrence information helps to better cap-ture entailment relations other than synonymy andtroponymy.
For example, our model learns that ac-quire?
own, corresponding to the cause-effect en-tailment relation, and that patent ?
invent, corre-sponding to the presupposition entailment relation.6 Conclusions and Future WorkWe presented a supervised classification model fordetecting lexical entailment between verbs.
At theheart of our model stand novel linguistically moti-vated indicators that capture positive and negativeentailment information.
These indicators encom-pass co-occurrence relationships between verbs atthe sentence, document and corpus level, as wellas more fine-grained typed distributional similaritymeasures.
Our model incorporates these novel indi-cators together with useful features from prior work,combining co-occurrence and distributional similar-ity information about verb pairs.Our experiment over a manually labeled datasetshowed that our model significantly outperformsseveral state-of-the-art models both in terms of Pre-202cision and Recall.
Further feature analysis indicatedthat our novel indicators contribute greatly to theperformance of the model, and that co-occurrenceat multiple levels, combined with distributional sim-ilarity features, is necessary to achieve the model?sbest performance.In future work we?d like to investigate which in-dicators may contribute to learning different fine-grained types of entailment, such as presuppositionand cause-effect, and attempt to perform a morefine-grained classification to subtypes of entailment.AcknowledgmentsThis work was partially supported by the IsraelScience Foundation grant 1112/08, the PASCAL-2 Network of Excellence of the European Com-munity FP7-ICT-2007-1-216886, and the Euro-pean Community?s Seventh Framework Programme(FP7/2007-2013) under grant agreement no.
287923(EXCITEMENT).ReferencesShuya Abe, Kentaro Inui, and Yuji Matsumoto.
2008.Acquiring event relation knowledge by learning cooc-currence patterns and fertilizing cooccurrence sampleswith verbal nouns.
In Proceedings of IJCNLP.Timothy Baldwin and Aline Villavicencio.
2002.
Ex-tracting the unextractable: a case study on verb-particles.
In proceedings of COLING.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2012.
Learning entailment relations by global graphstructure optimization.
Computational Linguistics,38(1):73?111.Nathanael Chambers and Dan Jurafsky.
2008.
Unsuper-vised learning of narrative event chains.
In Proceed-ings of ACL.Timothy Chklovski and Patrick Pantel.
2004.
Verbocean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of EMNLP.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailment chal-lenge.
In Machine Learning Challenges, volume 3944of Lecture Notes in Computer Science, pages 177?190.Springer.Lin Dekang and Patrick Pantel.
2001.
Dirt - discoveryof inference rules from text.
In In Proceedings of theACM SIGKDD Conference on Knowledge Discoveryand Data Mining.Maayan Geffet and Ido Dagan.
2005.
The distributionalinclusion hypotheses and lexical entailment.
In Pro-ceedings of ACL.Isabelle Guyon and Andre Elisseeff.
2003.
An intro-duction to variable and feature selection.
Journal ofMachine Learning Research, 3:1157?1182.Masato Hagiwara, Yasuhiro Ogawa, and KatsuhikoToyama.
2009.
Supervised synonym acquisition us-ing distributional features and syntactic patterns.
InJournal of Natural Language Processing.Marti Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of COLING.Jerry Hobbs.
1979.
Coherence and coreference.
Cogni-tive Science, 3:67?90.Eduard Hovy and Elisabeth Maier.
1993.
OrganizingDiscourse Structure Relations using Metafunctions.Pinter Publishing.Ray Jackendoff.
1983.
Semantics and Cognition.
TheMIT Press.T.
Joachims.
2005.
A support vector method for mul-tivariate performance measures.
In Proceedings ofICML.Alistair Knott and Ted Sanders.
1998.
The classificationof coherence relations and their linguistic markers: Anexploration of two languages.
In Journal of Pragmat-ics.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distributionalsimilarity for lexical inference.
Natural Language En-gineering, 16(4):359?389.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University OfChicago Press.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML.William Mann and Sandra Thompson.
1988.
Rhetoricalstructure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of ACL.Shachar Mirkin, Ido Dagan, and Maayan Geffet.
2006.Integrating pattern-based and distributional similaritymethods for lexical entailment acquisition.
In Pro-ceedings of the COLING/ACL.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of LREC.203Viktor Pekar.
2008.
Discovery of event entailmentknowledge from text corpora.
Comput.
Speech Lang.,22(1):1?16.Marco Pennacchiotti and Patrick Pantel.
2009.
Entityextraction via ensemble semantics.
In Proceedings ofEMNLP.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of ACL.Deborah Schiffrin.
1988.
Discourse Markers.
Cam-bridge University Press.Stefan Schoenmackers, Jesse Davis, Oren Etzioni, andDaniel S. Weld.
2010.
Learning first-order hornclauses from web text.
In Proceedings of EMNLP.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of NAACL-HLT.Idan Szpektor and Ido Dagan.
2008.
Learning entailmentrules for unary templates.
In Proceedings of COLING.Idan Szpektor, Hristo Tanev, and Ido Dagan.
2004.
Scal-ing web-based acquisition of entailment relations.
InIn Proceedings of EMNLP.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.
In-stance based evaluation of entailment rule acquisition.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics.Galina Tremper.
2010.
Weakly supervised learning ofpresupposition relations between verbs.
In Proceed-ings of ACL student workshop.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofEMNLP.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using se-lectional preferences.
In Proceedings of the COL-ING/ACL.204
