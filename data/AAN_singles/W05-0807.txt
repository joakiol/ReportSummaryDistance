Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 49?56,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Induction of Fine-grained Part-of-speech Taggers viaClassifier Combination and Crosslingual ProjectionElliott Franco Dra?bekDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218edrabek@cs.jhu.eduDavid YarowskyDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218yarowsky@cs.jhu.eduAbstractThis paper presents an original approachto part-of-speech tagging of fine-grainedfeatures (such as case, aspect, and adjec-tive person/number) in languages such asEnglish where these properties are gener-ally not morphologically marked.The goals of such rich lexical taggingin English are to provide additional fea-tures for word alignment models in bilin-gual corpora (for statistical machine trans-lation), and to provide an informationsource for part-of-speech tagger inductionin new languages via tag projection acrossbilingual corpora.First, we present a classifier-combinationapproach to tagging English bitext withvery fine-grained part-of-speech tags nec-essary for annotating morphologicallyricher languages such as Czech andFrench, combining the extracted fea-tures of three major English parsers,and achieve fine-grained-tag-level syntac-tic analysis accuracy higher than any indi-vidual parser.Second, we present experimental resultsfor the cross-language projection of part-of-speech taggers in Czech and French viaword-aligned bitext, achieving success-ful fine-grained part-of-speech tagging ofthese languages without any Czech orFrench training data of any kind.1 IntroductionMost prior research in part-of-speech (POS) tag-ging has focused on supervised learning over atagset such as the Penn Treebank tagset for En-glish, which is restricted to features that are mor-phologically distinguished in the focus language.Thus the only verb person/number distinction madein the Brown Corpus/Penn Treebank tagset is VBZ(3rd-person-singular-present), with no correspond-ing person/number distinction in other tenses.
Sim-ilarly, adjectives in English POS tagsets typicallyhave no distinctions for person, number or case be-cause such properties have no morphological surfacedistinction, although they do for many other lan-guages.This essential limitation of the Brown/Penn POSsubtag inventory to morphologically realized dis-tinctions in English dramatically simplifies the prob-lem by reducing the tag entropy per surface form(the adjective tall has only one POS tag (JJ) ratherthan numerous singular, plural, nominative, ac-cusative, etc.
variants), increasing both the stand-alone effectiveness of lexical prior models and word-suffix models for part-of-speech tagging.However, for many multilingual applications, in-cluding feature-based word alignment in bilingualcorpora and machine translation into morphologi-cally richer languages, it is helpful to extract finer-grained lexical analyses on the English side thatmore closely parallel the morphologically realizedtagset of the second (source or target) language.In particular, prior work on translingual part-of-speech tagger projection via parallel bilingual cor-pora (e.g.
Yarowsky et al, 2001) has been limitedto inducing part-of-speech taggers in second lan-guages (such as French or Czech) that only assigntags at the granularity of their source language (i.e.49the Penn Treebank-granularity distinctions from En-glish).
The much richer English tagsets achievedhere can allow these tagger projection techniques totransfer richer tag distinctions (such as case and verbperson/number) that are important to the full analy-sis of these languages, using only bilingual corporawith the morphologically impoverished English.For quickly retargetable machine translation, theprimary focus of effort is overcoming the extremescarcity of resources for the low density source lan-guage.
Sparsity of conditioning events for a transla-tion model can be greatly reduced by the availabil-ity of automatic source-language analysis.
In thisresearch we attempt to induce models for the au-tomatic analysis of morphological features such ascase, tense, number, and polarity in both the sourceand target languages with this end in mind.2 Prior Work2.1 Fine-grained part-of-speech taggingMost prior work in fine-grained part-of-speech tag-ging has been limited to languages such as Czech(e.g.
Hajic?
and Hladka?, 1998) or French (e.g.
Fos-ter etc.)
where finer-grained tagset distinctions aremorphologically marked and hence natural for thelanguage.
In support of supervised tagger learn-ing of these languages, fine-trained tagset inven-tories have been developed by the teams aboveat Charles University (Czech) and Universite?
deMontre?al (French).
The tagset developed by Hajic?forms the basis of the distinctions used in this paper.The other major approach to fine-grained tagginginvolves using tree-based tags that capture grammat-ical structure.
Bangalore and Joshi (1999) have uti-lized ?supertags?
based on tree-structures of variouscomplexity in the tree-adjoining grammar model.Using such tags, Brants (2000) has achieved the au-tomated tagging of a syntactic-structure-based set ofgrammatical function tags including phrase-chunkand syntactic-role modifiers trained in supervisedmode from a treebank of German.2.2 Classifier combination for part-of-speechtaggingThere has been broad work in classifier combinationat the tag-level for supervised POS tagging mod-els.
For example, Ma`rquez and Rodr?
?guez (1998)have performed voting over an ensemble of decisiontree and HMM-based taggers for supervised En-glish tagging.
Murata et al (2001) have combinedneural networks, support vector machines, decisionlists and transformation-based-learning approachesfor Thai part-of-speech tagging.
In each of thesecases, annotated corpora containing the full tagsetgranularity are required for supervision.Henderson and Brill (1999) have approachedparsing through classifier combination, using bag-ging and boosting for the performance-weightedvoting over the parse-trees from three anonymousstatistical phrase-structure-based parsers.
However,as their switching and voting models assumed equiv-alent phrase-structure conventions for merger com-patibility, it is not clear how a dependency parsingmodel or other divergent syntactic models could beintegrated into this framework.
In contrast, the ap-proach presented below can readily combine syntac-tic analyses from highly diverse parse structure mod-els by first projecting out all syntactic analyses ontoa common fine-grained lexical tag inventory.2.3 Projection-based BootstrappingYarowsky et al (2001) performed early work in thecross-lingual projection of part-of-speech tag anno-tations from English to French and Czech, by way ofword-aligned parallel bilingual corpora.
They alsoused noise-robust supervised training techniques totrain stand-alone French and Czech POS taggersbased on these projected tags.
Their projectedtagsets, however, were limited to those distinctionscaptured in the English Penn treebank inventory,and hence failed to make many of the finer graineddistinctions traditionally assumed for French andCzech POS tagging, such as verb person, number,and polarity and noun/adjective case.Probst (2003) pursued a similar methodology forthe purposes of tag projection, using a somewhatexpanded tagset inventory (e.g.
including adjec-tive number but not case), and focusing on target-language monolingual modeling using morphemeanalysis.
Cucerzan and Yarowsky (2003) addressedthe problem of grammatical gender projection viathe use of small seed sets based on natural gender.Another distinct body of work addresses the prob-lem of parser bootstrapping based on syntactic de-pendency projection (e.g.
Hwa et al 2002), oftenusing approaches based in synchronous parsing (e.g.Smith and Smith, 2004).50Word Core Prsn Num.
Case Tns/ Pol.
Voi.POS Asp.The DT 3 PL.
NOM.books NN 3 PL.
NOM.were VB 3 PL.
PAST + ACT.provoking VB 3 PL.
PAST- + ACT.PROG.laughter NN 3 S. ACC.with INtheir DT 3 PL.
?WITH?curious JJ 3 PL.
?WITH?titles NN 3 PL.
?WITH?Figure 1: Example of fine-grained English POS tagsWord Core Prsn Num.
Case Tns/ Pol.
VoicePOS Asp.Les DT 3 PL.
NOM.livres NN 3 PL.
NOM.provoquaient VB 3 PL.
PAST- + ACT.PROGR.des DT 3 PL.
ACC.rires NN 3 PL.
ACC.avec INses DT 3 PL.
?WITH?titres NN 3 PL.
?WITH?curieux JJ 3 PL.
?WITH?Figure 2: Example of fined-grained POS tags pro-jected onto a French translation3 TagsetsWe use Penn treebank-style part-of-speech tags asa substrate for further enrichment (for all of the ex-periments described here, text was first tagged us-ing the fnTBL part-of-speech tagger (Ngai and Flo-rian, 2001)).
Each Penn tag is mapped to a corepart-of-speech tag, which determines the set of fine-grained tags further applicable to each word.
Thefine-grained tags applicable to nouns, verbs, and ad-jective are shown in Table 1.
This paper concentrateson these most important core parts-of-speech.The example English sentence in Figure 1 illus-trates several key points about our tagset.
Some ofthe information we are interested in is already ex-pressed by the Penn-style tags ?
the NN titles is plu-ral; the VBD were is in the past tense.
For these, ourgoal is simply to make these facts explicit.On the other hand, curious could also be meaning-fully said to be semantically plural, and most impor-tantly for us, the corresponding word in a translationof this sentence into many other languages wouldbe morphologically plural.
Similarly, the head verbprovoking is also semantically in the past tense, andis likely to be translated to a past-tense form in manylanguages, even though in this example the actualtense marking is on were.
We expect the ?past-ness?
of the action to be much more stable cross-linguistically, than the particular division of laborbetween the head word and the auxiliary.
By prop-VB JJ NN RangePerson       1 / 2 / 3Number       SINGULARPLURALCase     NOMINATIVEACCUSATIVEGENITIVEPREPOSITION-?IN?PREPOSITION-?OF?.
.
.Degree   POSITIVECOMPARATIVESUPERLATIVETense   PASTPRESENTFUTUREPerfectivity   + / ?Progressivity   + / ?Polarity   + / ?Voice   ACTIVE / PASSIVETable 1: The fine-grained POS inventory used forEnglishagating these features from where they are explicitto where they are not, we hope to make informationmore directly available for projection.
Another im-portant class of information we would like to makeavailable concerns syntactic relations, which manylanguages mark with morphological case.
This is anissue that involves deep, complex, and ambiguousmappings, which we are not yet prepared to treat intheir fullness.
For now, we observe that curious andtitles are both dominated by with.Because of intent to mark whatever informationis recoverable, some of our tags require some in-terpretation.
For example, English has little or nomorphological realization of syntactic case, but theessential information of case, relationship of a nounwith its governor, is recoverable from contextualinformation, so we defined it in these terms.
Toavoid loss of information, we chose to remain ag-nostic about deeper analyses, such as the identifi-cation of theta roles or predicate-argument relation-ships, and restricted ourselves to a direct represen-tation of surface relationships.
We identified sub-jects, direct and indirect objects, non-heads of nouncompounds, possessives, and temporal adjuncts, andcreated a distinct tag for the objects of each distinctpreposition.Our ideal would be to have as expansive and de-tailed a tagset as possible, a ?quasi-universal?
tagsetwhich could cover whatever set of distinctions mightbe relevant for any language onto which we might51Feature Antecedent   CONSEQUENTNoun Number NN   SINGULARNNS   PLURALVerb Tense VBD   PAST(willshall) RB* VB   FUTUREFigure 3: Examples of locally recoverable featuresproject our analysis.
A completely universal tagsetwould require that the morphological distinctionsmade by the world?s languages come from a limitedpool of possibilities, based on non-arbitrary seman-tic distinctions, and further would require that therelevant semantic information be recoverable fromEnglish text.
The tagset we are using now is shapedin part by exceptions to these conditions.
For ex-ample, we have put off implementing tagging ofgender given the notoriously arbitrary and inconsis-tent assignment of grammatical gender across lan-guages (although Cucerzan and Yarowsky (2003)were able to show success on projection-based anal-ysis of grammatical gender as well).In the end, we have settled on a set of distinc-tions very similar to those realized by the morpho-logically richer of the European languages, with thenoticeable absence of gender.
Table 1 describes thefeatures we chose on this basis (definiteness andmood features were developed for English but notprojected to French or Czech, and are not treated inthis paper).4 Methods ?
English TaggingThe features we tagged vary widely in their degreeof morphological versus syntactic marking, and thedifficulty of their monolingual English detection.For some, tagging is simply a matter of explicitlyseparating information contained in the Penn part-of-speech tags, while others can be tagged to a highdegree of accuracy with simple heuristics based onlocal word and part-of-speech tag patterns.
Theseinclude number for nouns and adjectives, person(trivially) for nouns, degree for adjectives, polarity,voice, and aspect (perfectivity and progressivity) forverbs, as well as tense for some verbs.
Figure 3shows example rules for some of these easier cases.The more difficult features are those whose de-tection requires some degree of syntactic analysis.These include case, which summarizes the relationof each noun with its governor, and the agreement-based features: we define person, number, and casefor attributive adjectives by agreement with theirhead nouns, number and person for verbs and predi-cate adjectives by agreement with their subjects, andtense for some verbs by agreement with their in-flected auxiliaries.We investigated four individual approaches forthe syntax-features ?
a regular-expression-basedquasi-parser, a system based on Dekang Lin?s Mini-Par (Lin, 1993), a system based on the Collins parser(Collins, 1999), and one based on the CMU LinkGrammar Parser (Sleator and Temperley, 1993),as well as a family of voting-based combinationschemes.4.1 Regular-expression Quasi-parserThe regular-expression ?quasi-parser?
takes a directapproach, using several dozen heuristics based onregular-expression-like patterns over words, Pennpart-of-speech tags, and the output of the fnTBLnoun chunker.
Use of the noun chunker fa-cilitates identification of noun/dependent relation-ships within chunks, and extends the range of pat-terns identifying noun/governor relationships acrosschunks.The output of the quasi-parser consists of twoparts: a case tag for each noun in a sentence, anda set of agreement links across which other featuresare then spread.
We call this a direct approach be-cause the links are defined operationally, directly in-dicating the spreading action, rather than represent-ing any deeper syntactic analysis.In the diagram of the example sentence below, anarrow from one word to another indicates that theformer takes features from the latter.
The examplealso shows the context patterns by which the nounsin the sentence receive case.+<<<<<+ +>>>>>>>>>>>>>+| | | |+>>>>+ +<<<<<<<+ | +>>>>>>+| | | | | | |<The books> were provoking laughter with <their curious titles>Word Context Pattern   CASE TAGlaughter VB (genitive-NP)*     ACCUSATIVEtitles with (genitive-NP)*     PREP-WITHbooks default   NOMINATIVE4.2 MiniPar and the CMU Link GrammarParserFor MiniPar, the Collins parser, and the CMUlink grammar parser, we developed for each a setof minimal-complexity heuristics to transform theparser output into the specific conceptions of depen-dency and case we had developed for the first pass.52MiniPar produces a labeled dependency graph,which yields a straightforward extraction of the in-formation needed for this task.
Case tagging is asimple matter of mapping the set of dependency la-bels to our case inventory.
Our agreement linksare almost a subset of MiniPar?s dependencies (withsome special treatment of subject/auxiliary/main-verb triads, as shown in the example sentence).The figure below presents MiniPar?s raw outputfor the example sentence, along with some exam-ple dependency-label/case-tag rules.
The agreementlinks extracted from the dependency graph are iden-tical (in this case) to those produced by the regular-expression quasi-parser.mod pcomp-n+<<<<<<+<<<<<<<<<<<<<<<<<<<+| | |s | | gen |+>>>>>>>>>>>>>+ | | +>>>>>>>>>>>>>+| | | | | |det| be | obj | | | mod |+>>>+ +>>>>>>>+<<<<<<<<+ | | +>>>>>>+| | | | | | | | || | | | | | | | |The books were provoking laughter with their curious titlesWord Dependency Label   CASE TAGbooks s   NOMINATIVElaughter obj   ACCUSATIVEtitles pcomp-n:with   PREP-WITHThe output of the CMU link grammar parser hasproperties similar to MiniPar, and thus tag extractionwas handled in a similar fashion.4.3 Collins ParserThe Collins Parser produces a Penn-Treebank-styleconstituency tree, with head labels.
Although wecould have used the head-labels to operate on thedependency graph as with MiniPar, we chose to con-centrate on addressing the weakest point of our pre-vious systems, the identification of case.
Our algo-rithm traces the path from each noun to the root ofthe tree, stopping at the first node which we judgedto reliably indicate case.We did not directly extract any further informa-tion from the Collins parser output.
Instead, theremainder of the system is identical to the regular-expression quasi-parser.
However, because the sys-tem uses nominative case to identify verb sub-jects, we did expect to see some improvements inagreement-based features as well.SNPBThe booksVPwere VPprovoking NPBlaughterPPwith NPBtheir curious titlesWord Path to Root   CASE TAGbooks NPB:S   NOMINATIVElaughter NPB:VP:VP:VP:S   ACCUSATIVEtitles NPB:PP(with):VP:VP:S   PREP-WITH4.4 Parser CombinationThe fine-grained taggers based on the four partic-ipating parsers exhibited significant differences intheir strengths and weaknesses, suggesting poten-tial benefit from combining them.
Lacking tag-levelnumerical scores and development data for weight-training, we restricted ourselves to simple votingmechanisms.
We chose to do all of the combinationsat the end of the process, voting separately on tagsfor specific features of specific words.
Without tag-level probabilities from the one-best parser outputs,we were still able to use the combination protocolsto achieve a coarse-grained confidence measure.We compared a series of seven combination pro-tocols of increasing leniency to investigate preci-sion/recall tradeoffs.
The strictest, ?4:0?, producesan output only when there are four votes for the fa-vored tag, and no votes for any other.
Analogously,protocols ?3:0?, ?2:0?
and ?1:0?
also allow no dissent,but allow progressively more abstentions.
Continu-ing the sequence, protocol ?2:1?
proposes a tag aslong as there is a clear majority, ?2:2?
as long as sup-porters are not outnumbered by dissenters, and ?1:3?whenever possible.
To break ties in the latter twoprotocols, we favored first the CMU Link Parser,then Collins, then MiniPar, then Regexp.
(Lackingsufficient labeled data for fine-tuning, we orderedthem arbitrarily.
)5 Evaluation of English POS TaggingBefore we began the development of our taggers, wecreated standard tagging guidelines, and hand anno-tated a 3013-word segment of the English side of theCanadian Hansards, to be used for evaluation.53Core Feature MiniPar Regexp Collins CMU Link 1:3POSnum 86.8 87.7 87.7 87.9 88.4case 65.1 74.5 76.4 79.2 80.6JJ deg 100 100 100 100 100?French?
86.8 87.7 87.7 87.9 88.4?Czech?
57.9 64.3 67.1 68.1 70.5num 99.7 99.7 99.7 99.7 99.7NN case 65.9 74.8 77.8 77.3 80.0?French?
99.7 99.7 99.7 99.7 99.7?Czech?
65.0 74.8 77.8 77.2 79.9num 77.2 64.8 65.5 66.8 78.1tns 77.2 66.8 67.1 67.1 76.3prsn 88.0 75.0 74.3 73.4 86.5VB pol 96.3 96.6 96.6 96.6 96.6voice 88.0 88.0 88.0 88.0 88.0?French?
61.8 61.3 61.0 61.3 67.5?Czech?
61.3 61.1 60.8 61.1 67.1overall ?French?
82.6 82.5 82.4 83.2 85.2?Czech?
62.5 67.8 69.4 70.5 73.3Table 2: English tagging forced-choice accuracyCore Feature Mini Regexp Collins CMU 2:0 1:0 1:2POS Par Linknum 79.1 81.3 81.3 82.2 81.2 83.8 83.9JJ case 72.1 79.2 83.0 78.9 78.1 79.1 84.2deg 100 100 100 100 100 100 100?Czech?
67.6 72.2 76.0 74.3 70.4 73.4 77.9num 99.7 99.7 99.7 99.7 99.7 99.7 99.7NN case 68.5 75.5 78.6 77.9 72.6 72.5 78.1?Czech?
68.1 75.2 78.3 77.7 72.2 72.1 77.8tns 78.0 68.5 68.7 68.0 68.7 78.3 78.3num 72.7 61.3 61.2 61.3 61.1 76.1 77.1prsn 77.2 66.5 65.4 63.9 64.0 78.3 79.0VB pol 96.3 96.6 96.6 96.5 96.5 96.5 96.6voice 88.0 88.0 88.0 88.0 88.0 88.0 88.0?French?
61.7 50.7 50.2 50.1 50.6 64.8 65.6?Czech?
61.1 50.5 49.9 49.8 50.4 64.5 65.2all ?French?
81.9 78.7 78.5 78.5 83.6 78.9 83.9?Czech?
65.4 66.0 67.8 69.3 68.9 63.5 72.9Table 3: English tagging F-measure3:055606570758085909570  75  80  85  90  95  100RecallConsensusMiniPar2:0PrecisionNoun CaseCMULinkCollinsRegExp2:21:32:11:04:050Figure 4: Precision versus Recall ?
Noun caseVerb number40455055606570758086  88  90  92  94RecallPrecisionConsensus2:21:3 2:1 1:0MiniParCollinsCMULink RegExp3:02:04:035Figure 5: Precision versus Recall ?
Verb numberTable 2 shows system accuracy on test data ina forced-choice evaluation, where abstentions werereplaced by the most common tag for the each situa-tion (the combination system is that one biased mostheavily towards recall.
)In addition to the individual features, we also list?pseudo-French?
and ?pseudo-Czech?.
These rep-resent exact-match accuracies for composite fea-tures comprising those features typically realized inFrench or Czech POS taggers.
For example, pseudo-Czech verb accuracy of 67.1% indicates that for67.1% of verb instances, the Czech-realized featuresof number, tense, perfectivity, progressivity, polar-ity, and voice were all correct.
These give an indica-tion of the quality of the starting point for crosslin-gual bootstrapping to the respective languages.Besides the forced-choice scenario, we were alsointerested in the effect of allowing abstentions forlow-confidence cases.
Table 3 shows the F-measureof precision and recall for the individual systems, aswell as a range of combination systems.
Figures 4and 5 show (for two example features) the clear pre-cision/recall tradeoff.
Performance of the consensussystems is higher than the individual parser-basedtaggers at all levels of tag precision or recall.Unfortunately, because MiniPar does its own inte-grated tokenization and part-of-speech tagging, wefound that a significant portion of the errors seemedto stem from discrepancies where MiniPar disagreedon the segmentation or the core part-of-speech of thewords in question.6 Cross-lingual POS Tag Projection andBootstrappingOur cross-lingual POS tag projection process is sim-ilar to Yarowsky et al (2001).
It begins by perform-ing a statistical sentence and word alignment of thebilingual corpora (described below), and then trans-fers both the coarse- and fine-grained tags achievedfrom classifier combination on the English side viathe higher confidence word alignments (based on theintersection of the 1-best word alignments inducedfrom French to English and English to French.
Theprojected tags then serve as noisy monolingual train-ing data in the source language.There are several notable differences and exten-sions: The first major difference is that the projectedfine-grained tag set is much more detailed, includingsuch additional properties as noun case, adjective54case and number, and verb person, number, voice,and polarity.
Because these span the subtag featuresnormally assumed for Czech and French part-of-speech taggers, the projection work presented herefor the first time shows the translingual projectionand induction of full-granularity Czech and Frenchtaggers, rather than the much less complete andcoarser-grained prior projection work.The other major differences are in the methodof target-language monolingual tagger generaliza-tion from the projected tags.
We pursue a combi-nation of trie-based lexical prior models and local-agreement-based context models.
The lexical priortrie model, as illustrated in Figure 6 for noun num-ber, shows how the hierarchically-smoothed lexicalprior conditioned on variable length suffixes can as-sign noun number probabilities to both previouslyseen words (with full-word-length suffixes stored)and to new words in test data, based on backoff topartially matching suffixes.The context models are based on exploiting agree-ment phenomena of the fine-grained tag features inlocal context.
  	ff	flfi for each wordtoken is a distance-weighted linear interpolation ofthe posterior tag distributions assigned to its neigh-bors by the trie-based lexical-prior model.
Finally  subtag  word fi is an equally-weighted linear inter-polation of the   subtag  affix fi trie model probabil-ity and   subtag  context fi context-agreement prob-ability.
Table 4 contrasts the performance of thesetwo models in isolation and combination.All of these models condition their probabilitiesfirst on the core part-of-speech of a word.
We usedthe methods of Yarowsky et al (2001) to developa core part-of-speech tagger for French, based onlyon the projected core tags, and used this as a basisfor fine-grained tags.
We also ran experiments iso-lating the question of fine-grained tagging, assumingas input externally supplied core tags from the gold-standard data.
Table 4 shows results under both ofthese assumptions.For French, the training data was 15 millionwords from the Canadian Hansards.
Word align-ments were produced using GIZA++ (Och and Ney,2000) set to produce a maximum of one Englishword link for each French word (i.e., a French-to-English model).
The test data was 111,000 words oftext from the Laboratoire de Recherche Applique?een Linguistique Informatique at the Universite?
deMontre?al, annotated with person, number, and tense.Suffix Pr(PLURAL  suffix) Pr(SINGULAR  suffix)none 32.5 67.5-s 66.5 33.5-is 35.3 64.7-ais 16.2 83.8Figure 6: Example smoothed suffix trie probabilitiesfor French noun numberSeveral factors contributed to a fairly successfulset of results.
The quality of the alignments is sub-jectively very good; the morphological system ofFrench is relatively simple, and is a good match forour suffix tries; Perhaps most importantly, the map-pings between the English and the French tagsetswere for the most part simple and consistent.
Themost prominent exception is verb tense.For Czech, the training and testing data were fromthe Reader?s Digest corpus.
We used the first 63,000words for testing, and the remaining 551,000 fortraining, ignoring the translations of the test data andthe gold-standard tags on the training data.It should be noted that the baseline (most likelytag) performance is actually a supervised model us-ing the target language monolingual goldstandarddata frequencies.
The other results based on translin-gual projection have no knowledge of the true mostlikely tag, and hence occasionally underperform thissupervised ?baseline?.
Finally, one of the major rea-sons for lower Czech performance is the currentlyvery poor quality of the bilingual word alignments.However, using these diverse POS subtags as fea-tures offers the potential for substantially improvedword alignment for morphologically rich languages,one of the central downstream benefits of this re-search.7 ConclusionWe have demonstrated the feasibility of automati-cally annotating English text with morphosyntacticinformation at a much finer POS tag granularity thanin the standard Brown/Penn tagset, but at a POS de-tail appropriate for tagging morphologically richerlanguage such as Czech or French.
This is accom-plished by using a classifier combination strategy tointegrate the analyses of four independent parsers,achieving a consensus tagging with higher accuracythan the best component parser.Furthermore, we have demonstrated that the re-sulting fine-grained POS tags can be successfully55Feature Engl.
Baseline Trie Vic.
Comb.Comb.French (using correct core POS)JJ-num 1:0 67.0 97.6 98.0 98.22:0 67.0 97.6 98.0 98.2NN-num 1:0 71.2 94.3 94.7 94.62:0 71.2 94.3 94.7 94.6VB-num 1:0 53.4 91.9 73.2 90.22:0 53.4 73.1 72.7 73.2VB-prsn 1:0 88.0 76.9 78.7 77.72:0 88.0 92.9 93.0 93.4VB-tns 1:0 47.6 86.2 71.7 73.92:0 47.6 54.7 51.9 53.8VB- 1:0 26.8 48.1 43.4 47.1exact 2:0 26.8 50.0 46.9 49.2overall- 1:0 56.2 79.7 78.5 79.6exact 2:0 56.2 80.3 79.6 80.3French (induced core POS)JJ-num 1:0 65.1 87.1 89.0 88.32:0 65.1 87.1 89.1 88.5NN-num 1:0 66.6 87.5 87.8 87.92:0 66.6 87.5 87.8 87.9VB-num 1:0 53.0 86.4 79.5 84.92:0 53.0 71.2 70.6 71.4VB-prsn 1:0 75.1 67.4 69.7 68.42:0 75.1 80.4 80.8 81.1VB-tns 1:0 43.3 65.1 62.0 64.22:0 43.3 49.0 46.3 48.2VB-exact 1:0 24.1 43.9 40.2 43.02:0 24.1 45.3 42.2 44.6overall- 1:0 52.6 73.3 72.5 73.4exact 2:0 52.6 73.7 73.1 73.9Czech (using correct core POS)JJ-num 1:0 28.0 46.4 44.5 45.12:0 28.0 47.0 44.6 46.0JJ-case 1:0 7.1 40.2 42.0 40.92:0 7.1 37.9 41.4 40.2JJ-deg 1:0 89.2 85.6 86.8 86.62:0 89.2 85.6 86.8 86.6JJ-exact 1:0 6.9 20.6 19.1 19.42:0 6.9 20.9 20.0 20.5NN-num 1:0 52.2 71.1 69.6 70.72:0 52.2 71.1 69.4 70.8NN-case 1:0 53.5 39.5 39.2 39.62:0 53.5 39.2 38.6 39.1NN-exact 1:0 23.7 29.5 28.7 29.42:0 23.7 29.7 28.6 29.4VB-num 1:0 57.0 71.6 69.1 70.72:0 57.0 71.2 69.7 71.4VB-prsn 1:0 55.1 65.9 64.9 65.42:0 55.1 65.3 64.3 64.9VB-voice 1:0 97.3 93.2 93.9 93.42:0 97.3 93.2 93.9 93.4VB-pol 1:0 91.1 93.8 89.9 92.12:0 91.1 93.8 89.9 92.1VB-exact 1:0 9.9 15.2 14.6 14.82:0 9.9 14.5 14.3 14.7overall- 1:0 15.7 22.6 21.8 22.2exact 2:0 15.7 22.5 21.7 22.3Table 4: Accuracy of induced fine-grained tag-gers, by core part-of-speech, feature, underlying en-glish tagger combination (eng-comb.
), and frenchtagging method (most likely tag ?
baseline, suf-fix trie (prefix trie for Czech verb polarity) ?
trie,vicinity voting ?
vic., or trie/vicinity combination ?comb.
)projected to additional languages such as French andCzech, generating stand-alone taggers capturing thesalient fine-grained POS subtag distinctions appro-priate for these languages, including features suchas adjective number and case that are not morpho-logically marked in the original English.ReferencesS.
Bangalore and A. K. Joshi.
1999.
Supertagging: AnApproach to Almost Parsing.
Computational Linguistics,25(2): 237?265.T.
Brants.
2000.
TnT ?
a Statistical Part-of-Speech Tagger.
InProceedings of ANLP-2000, pp.
224?231.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. Dissertation, University of Penn-sylvania, 1999.S.
Cucerzan and D. Yarowsky.
2003.
Minimally Super-vised Induction of Grammatical Gender.
In Proceedings ofHLT/NAACL-2003, pp.
40?47.J.
Hajic?
and B. Hladk a?.
1998.
Tagging inflective languages:prediction of morphological categories for a rich, structuredtagset.
In Proceedings of COLING-ACL Conference, pp.483?490.R.
Hwa, P. Resnik, and A. Weinberg.
2002.
Breaking the Re-source Bottleneck for Multilingual Parsing.
In Proceedingsof LREC-2002.D.
Lin.
1993.
Principle-based parsing without overgeneration.In Proceedings of ACL-93, pp.
112?120.L.
Ma`rquez and H. Rodr ??guez.
1998.
Part-of-speech taggingusing decision trees.
In Proceedings of the European Con-ference on Machine Learning.M.
Murata, Q. Ma, and H. Isahara.
2001.
Part of Speech Tag-ging in Thai Language Using Support Vector Machine.
InProceedings of NLPNN-2001, pp.
24?30.G.
Ngai and R. Florian.
2001.
Transformation-based Learningin the Fast Lane.
In Proceedings of NAACL-2001, pp.
40?47.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proceedings of ACL-2000, pp.
440?447.K.
Probst.
2003.
Using ?smart?
bilingual projection to feature-tag a monolingual dictionary.
In Proceedings of CoNLL-2003, pp.
103?110.D.
Sleator and D. Temperley.
1993.
Parsing English with aLink Grammar.
In Proceedings, Third International Work-shop on Parsing Technologies, pp.
277?292.D.
Smith and N. Smith 2004.
Bilingual Parsing with FactoredEstimation: Using English to Parse Korean.
In Proceedingsof EMNLP-2004, pp.
49?56.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.
Inducingmultilingual text analysis tools via robust projection acrossaligned corpora.
In Proceedings of HLT-2001, First Inter-national Conference on Human Language Technology Re-search, pp.
161?168.56
