Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 29?38,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsACL 2013 MultiLing Pilot OverviewJeff KubinaU.S.
Department of Defense9800 Savage Rd., Ft. Meade, MD 20755jmkubin@tycho.ncsc.milJohn M. Conroy, Judith D. SchlesingerIDA/Center for Computing Sciences17100 Science Dr., Bowie, MDconroy@super.org, drj1945@gmail.comAbstractThe 2013 Association for ComputationalLinguistics MultiLing Pilot posed a taskto measure the performance of multi-lingual, single-document, summarizationsystems using a dataset derived from manyWikipedias.
The objective of the pilotwas to assess automatic summarization ofmultilingual text documents outside thenews domain and the potential of usingWikipedia articles for such research.
Thisreport describes the pilot task, the dataset,the methods used to evaluate the submittedsummaries, and the overall performance ofeach participant?s system.1 IntroductionDocument summarization is an active subject ofresearch and development.
The ACM Digital Li-brary has about 806 reports on the subject pub-lished since 1993, with over half of them appear-ing in the last five years.
While the impetus formuch of this research is the annual Text Anal-ysis Conference (TAC) workshop on documentsummarization, there is a growing demand in theconsumer market for news summarization appli-cations being met by tablet and smart-phone ap-plications such as Clipped1, Summoner2, TLDR3,and Yahoo News.
Yahoo and Google even ac-quired two companies developing such applica-tions, Summly (Stelter, 2013) and Wavii (Tsotsis,2013) respectively, earlier this year.
While sum-marization technology for news sources is com-ing to fruition, the performance of such technol-ogy on non-English documents outside the newsdomain has not been throughly assessed and mayneed further research.
Since the datasets used by1http://goo.gl/dFKD92http://goo.gl/0QFaZ3http://goo.gl/qEgCsthe TAC summarization workshops have predom-inately been English news articles, with some ex-ceptions (Giannakopoulos et al 2011), the objec-tive of the 2013 ACL MultiLing Pilot was to assessthe performance of automatic multilingual single-document summarization systems on non-Englishtext outside the news domain and to determine thepotential of using Wikipedia articles for such re-search.This report starts with a description of the taskand dataset, the methods used to evaluate the sub-mitted summaries, the performance of each partic-ipating system, and concludes with an assessmentof the pilot and potential future work.2 Task and Dataset DescriptionThe objective of each participant system of the pi-lot was simple: compute a summary for each doc-ument in at least two of the datasets languages.No restrictions were placed on the languages thatcould be chosen nor was any target summary sizespecified.The dataset was derived from a corpus cre-ated in 2010 to measure the performance of theCLASSY (Conroy et al 2009) summarization al-gorithm on non-English documents outside thenews domain.
At the time such a corpus did notexist so one was created from the Wikipedias.To date there are Wikipedias in 285 languagescomprising over 75 million pages.
Some of theWikipedias maintain a list of Feature Articles,which are articles reviewed and voted upon by ed-itors as the best that fulfill Wikipedia?s require-ments in accuracy, neutrality, completeness, andstyle.
One such requirement is that the article havea lead section that should.
.
.
be able to stand alone as a conciseoverview.
It should .
.
.
summarize themost important points .
.
.
[and] materialin the lead should roughly reflect its im-29portance to the topic .
.
.
4So the lead section of a featured article isan excellent summary of it, hence, the fea-tured articles were used to create the corpus.In 2010 there were 41 Wikipedias with morethan nine featured articles.
The Perl moduleText::Corpus::Summaries::Wikipedia5 was de-veloped to automatically create the corpus fromthe featured articles of those Wikipedias.
The cor-pus is publicly available (Kubina, 2010) and thePerl module can be used to create an updated cor-pus.The dataset for the pilot was created from asubset of the 2010 corpus.
This was done to en-sure that each language had 30 articles and thatthe size of each article?s body text was sufficientlylarge.
First, for each article the summary and bodywere compressed to approximate their informa-tion content size.
For example, given a Chineseand English article with the same character lengththe Chinese article will usually contain more in-formation than an English article and their com-pressed sizes will approximation their true infor-mation content.
Next, if the compressed bodysize of an article was less than five times its com-pressed summary size, then the article was dis-carded.
The factor of five was simply chosen toensure the body of each article was sufficientlylarge relative to the summary size.
For each lan-guage the median of the ratio of compressed bodysize to compressed summary size was computedand only the 30 articles closest to the median wereincluded in the dataset.
This filtering reduced thecorpus from 12, 819 articles in 41 languages to thedataset containing 1, 200 articles in 40 languages.For each language in the dataset Table 1 containsthe mean size of the articles, their bodies, and theirsummaries, in characters.3 Evaluation Methods and ResultsFour teams submitted the results of six summa-rization systems.
The teams are denoted by AIS,LAN, MD, and MUS; the MD team submittedthree systems.
Throughout this report the systemsare denoted by AIS, LAN, MD1, MD2, MD3, andMUS.
Table 2 contains the list of languages sub-mitted for each system and the mean size, in char-acters, of the summaries submitted.4http://en.wikipedia.org/wiki/Wikipedia:LEAD5http://goo.gl/ySgOSFor the evaluation a baseline summary was ex-tracted from the each article in the dataset that isthe prefix substring of the article?s body text withthe same length as the text in the lead section ofthe article.
For the remainder of this report thelead section of an article is called the human sum-mary.
An oracle summary was also computed foreach article by heuristically extracting sentencesfrom its body text to maximize its ROUGE-2 scoreagainst the human summary until its size exceededthe human summary, upon which it was truncated.Submitted summaries were automatically eval-uated against the human summary of each arti-cle using ROUGE-1, ROUGE-2 (Lin, 2004) andMeMoG (Giannakopoulos et al 2008).
ForROUGE, the languages Chinese, Japanese, Ko-rean, and Thai were tokenized into individualcharacters.
For MeMoG the character n-gram sizeused for each language is listed in Table 3, whichis the n-gram size that maximized the standard de-viation divided by the mean of the n-gram fre-quency distribution of the language in the dataset.So the selected n-gram size maximizes the vari-ability of the distribution values relative to theirmean.
A shorter n-gram size would inflate theMeMoG scores because of their inherent frequentco-occurrence and conversely a longer size wouldpenalize MeMoG scores due to their infrequentco-occurrence.Each scoring method was performed twice, firstby truncating, if necessary, each system summaryto the size of the human summary, which is calledHSS-scoring.
The second set of scores were com-puted by truncating all the summaries of an ar-ticle, including the human summary, to the sizeof the shortest summary amongst the system andhuman summaries for the article, which is calledSSS-scoring.
For HSS-scoring the system sum-maries shorter that the human summary are penal-ized since ROUGE is recall oriented.
Alternately,SSS-scoring gives preference to shorter systemsummaries that have their best content (extractedsentences) first.The performance for HSS-scoring of the sys-tems on the seven languages that at least two teamssubmitted summaries for are given in Figures 1, 2,and 3.
Table 4 gives an overview of how often sig-nificant differences in each of the three automaticmetrics was observed.
In particular, the last rowgives the fraction of times that an non-parametricanalysis of variance (ANOVA) indicated that the30Table 1: Dataset Languages and SizesISO LANGUAGE ARTICLE BODY SUMMARYaf Afrikaans 24752 (10214) 23448 (10230) 1303 (196)ar Arabic 27845 (9490) 26354 (9530) 1491 (220)bg Bulgarian 23965 (9248) 22981 (9250) 984 (134)ca Catalan 30611 (15248) 29322 (15274) 1289 (140)cs Czech 26300 (10453) 24777 (10414) 1522 (190)de German 32023 (12522) 31160 (12530) 862 (53)el Greek 26072 (11113) 24937 (11096) 1134 (224)en English 26572 (9010) 24860 (9013) 1712 (114)eo Esperanto 22295 (10031) 21304 (10022) 990 (106)es Spanish 40467 (19563) 38726 (19533) 1740 (113)eu Basque 17886 (9845) 17231 (9821) 655 (91)fa Persian 15132 (7630) 14099 (7217) 1032 (517)fi Finnish 27379 (11783) 26353 (11805) 1025 (105)fr French 41578 (21952) 40186 (21959) 1392 (73)he Hebrew 18492 (8283) 17697 (8283) 794 (82)hr Croatian 21132 (11094) 20276 (11113) 855 (96)hu Hungarian 26256 (12161) 25175 (12139) 1081 (90)id Indonesian 18550 (9131) 17649 (9124) 901 (148)it Italian 39189 (19235) 38042 (19220) 1146 (80)ja Japanese 14352 (11890) 14131 (11895) 221 (38)ka Georgian 15282 (9570) 14558 (9551) 723 (124)ko Korean 17140 (7899) 16416 (7889) 724 (175)ml Malayalam 27329 (10645) 26158 (10639) 1170 (331)ms Malay 19346 (16577) 18436 (16348) 909 (411)nl Dutch 29575 (16346) 28580 (16363) 994 (89)nn Norwegian-Nynorsk 16107 (8056) 15384 (7917) 722 (297)no Norwegian-Bokmal 30225 (17652) 29218 (17594) 1006 (125)pl Polish 23028 (12853) 22067 (12861) 960 (66)pt Portuguese 30967 (17998) 29310 (18004) 1657 (110)ro Romanian 21921 (12812) 20782 (12773) 1139 (108)ru Russian 34069 (13792) 33134 (13771) 934 (70)sh Serbo-Croatian 21776 (21469) 21060 (21341) 716 (308)sk Slovak 21694 (10067) 20983 (10071) 711 (169)sl Slovenian 17900 (7222) 17077 (7194) 823 (135)sr Serbian 30239 (9812) 28927 (9764) 1312 (176)sv Swedish 23476 (10169) 22314 (10156) 1162 (99)th Thai 27041 (8312) 25425 (8291) 1616 (226)tr Turkish 32956 (16423) 31346 (16338) 1610 (257)vi Vietnamese 35376 (16099) 33857 (16050) 1518 (161)zh Chinese 10110 (4341) 9608 (4357) 501 (42)Table 1: The table lists the languages in the dataset with the first column containing the ISO code foreach the language, the second column the name of the language, and the remaining columns containingthe mean size, in characters, and standard deviation, in parentheses, of the entire article, their bodies, andtheir summaries.
For example, for English the mean size of the human summaries is 1,712 characters.31Table 2: Mean Summary Size For Submitted Languages of SystemsISO LANGUAGE AIS LAN MD1 MD2 MD3 MUS SUMaf Afrikaans 966 953 967 1303ar Arabic 1461 876 858 874 2232 1491bg Bulgarian 1302 969 946 967 984ca Catalan 911 921 925 1289cs Czech 1061 1020 1062 1522de German 1492 1072 1037 1087 862el Greek 1367 989 979 991 1134en English 1262 1551 944 957 958 1197 1712eo Esperanto 947 933 956 990es Spanish 922 916 927 1740eu Basque 1154 1151 1167 655fa Persian 793 792 800 1032fi Finnish 1328 1284 1323 1025fr French 936 930 952 1392he Hebrew 871 867 876 1098 794hr Croatian 979 954 976 855hu Hungarian 1092 1064 1089 1081id Indonesian 1091 1085 1091 901it Italian 981 952 975 1146ja Japanese 546 564 563 221ka Georgian 1180 1195 1218 723ko Korean 663 638 656 724ml Malayalam 670 648 676 1170ms Malay 1089 1089 1098 909nl Dutch 994 974 1000 994nn Norwegian-Nynorsk 928 908 929 722no Norwegian-Bokmal 967 937 977 1006pl Polish 1086 1056 1083 960pt Portuguese 942 936 939 1657ro Romanian 1311 938 940 948 1139ru Russian 1095 1046 1078 934sh Serbo-Croatian 969 955 983 716sk Slovak 1026 997 1031 711sl Slovenian 967 949 981 823sr Serbian 990 954 979 1312sv Swedish 997 990 1006 1162th Thai 553 566 563 1616tr Turkish 1166 1132 1152 1610vi Vietnamese 696 684 691 1518zh Chinese 523 559 552 501Table 2: The mean summary size, in characters, for each language submitted by each system includingthe mean of the human summaries in the last column named SUM.32Table 3: N-gram Size Per Language for MeMoGISO LANGUAGE SIZE ISO LANGUAGE SIZEaf Afrikaans 5 ka Georgian 3ar Arabic 3 ko Korean 1bg Bulgarian 4 ml Malayalam 3ca Catalan 4 ms Malay 4cs Czech 4 nl Dutch 4de German 4 nn Norwegian-Nynorsk 4el Greek 4 no Norwegian-Bokmal 4en English 5 pl Polish 4eo Esperanto 4 pt Portuguese 4es Spanish 4 ro Romanian 4eu Basque 4 ru Russian 4fa Persian 4 sh Serbo-Croatian 3fi Finnish 4 sk Slovak 4fr French 4 sl Slovenian 4he Hebrew 3 sr Serbian 4hr Croatian 4 sv Swedish 5hu Hungarian 4 th Thai 3id Indonesian 5 tr Turkish 5it Italian 5 vi Vietnamese 5ja Japanese 1 zh Chinese 1Table 3: The table lists the n-gram size used for each language when evaluating summaries usingMeMoG, which is the n-gram size that maximized the standard deviation divided by the mean of then-gram frequency distribution of the language in the dataset.33Figure 1: ROUGE-1 scores for HSS.Figure 2: ROUGE-2 scores for HSS.34Figure 3: MeMoG scores for HSS.Table 4: Fraction of time a system beat the base-line for HSS.System ROUGE-1 ROUGE-2 MeMoGAIC 2/5 0/5 0/5LAN 0/2 0/2 0/2MD1 15/40 4/40 2/39MD2 16/40 4/40 0/39MD3 15/40 4/40 0/39MUS 2/3 1/3 0/3ANOVA 28/40 13/40 5/39Table 4: The table gives the fraction of languageseach system significantly outperform the base-line.
The last line gives the number of times anANOVA rejected the null hypothesis, indicatingsignificance.medians of the system scores were not the same,using a rejection threshold of 0.05.
Also, the frac-tion of time that each system significantly outper-formed the lead baseline is also recorded.
A pairedWilcoxon test was invoked whenever the ANOVAindicated a significant difference was present, witha threshold of 0.05.Lastly, each systems performance for SSS-scoring is provided in Figures 4, 5, and 6.
Sur-prisingly, the results change little.
Lastly Table 5contains the number of times that each system beatthe baseline summary with a 95% confidence mea-sured as a result of the non-parametric ANOVAand the Wilcoxon paired sign rank test.
The resultsshow that the number of significant differences godown for ROUGE scores and up for MeMoG.4 SummaryOverall, the authors believe the pilot was success-ful in that it exposed researchers to the poten-tial for using Wikipedia articles for summarizationresearch and demonstrated that generating sum-maries for the genre of Wikipedia articles is a morechallenging task than newswire documents.
No-tably, no system outperformed the baseline for En-glish!
In hindsight this is not too surprising sincenews articles have a prose style6 significantly dif-ferent from Wikipedia articles7.
Wikipedia arti-cles are written as expositions having a topicalflow that can vary significantly between sectionsbut news articles are written in a style8 that ad-dresses the most important information first?the6http://en.wikipedia.org/wiki/News_style7http://en.wikipedia.org/wiki/MOS:8http://en.wikipedia.org/wiki/Inverted_pyramid35Figure 4: ROUGE-1 scores for SSS.Figure 5: ROUGE-2 scores for SSS.36Figure 6: MeMoG scores for SSS.Table 5: Fraction of the time a system beat the leadbaseline for SSS.System ROUGE-1 ROUGE-2 MeMoGAIC 0/5 0/5 0/5LAN 0/2 0/2 0/2MD1 8/40 2/40 6/39MD2 10/40 2/40 2/39MD3 7/40 2/40 4/39MUS 0/3 0/3 0/3ANOVA 11/40 5/40 7/39Table 5: The table gives the fraction of lan-guages that each system significantly outperformthe baseline on.
The last line contains the numberof times an ANOVA rejected the null hypothesis,indicating significance.who, what, when, where and why?with the sub-sequent text providing more details.
Hence newsarticles have a more even topical flow.
The authorshope these results stimulate research and devel-opment of summarization algorithms outside thenews domain.As for the metrics, ROUGE-1 observed themost significant differences among the systemsand MeMoG observed the least as measured bya non-parametric ANOVA.
However, a humanevaluation of the summaries generated would beneeded to detemine which of the automatic metricsis best at predicting significant differences amongsystems for such data.ReferencesJohn M. Conroy, Judith D. Schlesinger, and Dianne P.O?leary.
2009.
Classy 2009: summarization andmetrics.
In Proceedings of the text analysis confer-ence (TAC).George Giannakopoulos, Vangelis Karkaletsis, GeorgeVouros, and Panagiotis Stamatopoulos.
2008.Summarization system evaluation revisited: N-gram graphs.
ACM Trans.
Speech Lang.
Process.,5(3):5:1?5:39, October.George Giannakopoulos, Mahmoud El-Haj, Beno?tFavre, Marina Litvak, Josef Steinberger, and Va-37sudeva Varma.
2011.
Tac 2011 multiling pilotoverview.Jeff Kubina.
2010.
Wikipedia featured article cor-pus.
http://goo.gl/AmMGN.
[Online; accessed30-May-2013].Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.Brian Stelter.
2013.
He has millions and a new job atyahoo.
soon, he?ll be 18.
New York Times.Alexia Tsotsis.
2013.
Google buys wavii for north of$30 million.
TechCrunch.38
