Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 752?759,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsComputationally Efficient M-Estimation of Log-Linear Structure Models?Noah A. Smith and Douglas L. Vail and John D. LaffertySchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213 USA{nasmith,dvail2,lafferty}@cs.cmu.eduAbstractWe describe a new loss function, due to Jeonand Lin (2006), for estimating structuredlog-linear models on arbitrary features.
Theloss function can be seen as a (generative) al-ternative to maximum likelihood estimationwith an interesting information-theoretic in-terpretation, and it is statistically consis-tent.
It is substantially faster than maximum(conditional) likelihood estimation of condi-tional random fields (Lafferty et al, 2001;an order of magnitude or more).
We com-pare its performance and training time to anHMM, a CRF, an MEMM, and pseudolike-lihood on a shallow parsing task.
These ex-periments help tease apart the contributionsof rich features and discriminative training,which are shown to be more than additive.1 IntroductionLog-linear models are a very popular tool in naturallanguage processing, and are often lauded for per-mitting the use of ?arbitrary?
and ?correlated?
fea-tures of the data by a model.
Users of log-linearmodels know, however, that this claim requires somequalification: any feature is permitted in principle,but training log-linear models (and decoding underthem) is tractable only when the model?s indepen-dence assumptions permit efficient inference proce-dures.
For example, in the original conditional ran-dom fields (Lafferty et al, 2001), features were con-?This work was supported by NSF grant IIS-0427206 andthe DARPA CALO project.
The authors are grateful for feed-back from David Smith and from three anonymous ACL re-viewers, and helpful discussions with Charles Sutton.fined to locally-factored indicators on label bigramsand label unigrams (with any of the observation).Even in cases where inference in log-linear mod-els is tractable, it requires the computation of a parti-tion function.
More formally, a log-linear model forrandom variables X and Y over X,Y defines:pw(x, y) =ew>f(x,y)?x?,y?
?X?Y ew>f(x?,y?
)=ew>f(x,y)Z(w)(1)where f : X?Y?
Rm is the feature vector-functionand w ?
Rm is a weight vector that parameterizesthe model.
In NLP, we rarely train this model bymaximizing likelihood, because the partition func-tion Z(w) is expensive to compute exactly.
Z(w)can be approximated (e.g., using Gibbs sampling;Rosenfeld, 1997).In this paper, we propose the use of a new lossfunction that is computationally efficient and statis-tically consistent (?2).
Notably, repeated inferenceis not required during estimation.
This loss func-tion can be seen as a case of M-estimation1 thatwas originally developed by Jeon and Lin (2006) fornonparametric density estimation.
This paper givesan information-theoretic motivation that helps eluci-date the objective function (?3), shows how to ap-ply the new estimator to structured models used inNLP (?4), and compares it to a state-of-the-art nounphrase chunker (?5).
We discuss implications andfuture directions in ?6.2 Loss FunctionAs before, let X be a random variable over a high-dimensional space X, and similarly Y over Y. X1?M-estimation?
is a generalization of MLE (van der Vaart,1998); space does not permit a full discussion.752might be the set of all sentences in a language, andY the set of all POS tag sequences or the set of allparse trees.
Let q0 be a ?base?
distribution that isour first approximation to the true distribution overX ?
Y. HMMs and PCFGs, while less accurate aspredictors than the rich-featured log-linear modelswe desire, might be used to define q0.The model we estimate will have the formpw(x, y) ?
q0(x, y)ew>f(x,y) (2)Notice that pw(x, y) = 0 whenever q0(x, y) = 0.It is therefore important for q0 to be smooth, sincethe support of pw is a subset of the support of q0.Notice that we have not written the partition functionexplicitly in Eq.
2; it will never need to be computedduring estimation or inference.
The unnormalizeddistribution will suffice for all computation.Suppose we have observations ?x1, x2, ..., xn?with annotations ?y1, ..., yn?.
The (unregularized)loss function, due to Jeon and Lin (2006), is2`(w) =1nn?i=1e?w>f(xi,yi)+?x,yq0(x, y)(w>f(x, y))(3)=1nn?i=1e?w>f(xi,yi) + w>?x,yq0(x, y)f(x, y)=1nn?i=1e?w>f(xi,yi) + w> Eq0(X,Y )[f(X,Y )]?
??
?constant(w)Before explaining this objective, we point outsome attractive computational properties.
Noticethat f(xi, yi) (for all i) and the expectations of thefeature vectors under q0 are constant with respectto w. Computing the function in Eq.
3, then, re-quires no inference and no dynamic programming,only O(nm) floating-point operations.3 An InterpretationHere we give an account of the loss function as away of ?cleaning up?
a mediocre model (q0).
We2We give only the discrete version here, because it is mostrelevant for an ACL audience.
Also, our linear functionw>f(xi, yi) is a simple case; another kernel (for example)could be used.show that this estimate aims to model a presumedperturbation that created q0, by minimizing the KLdivergence between q0 and a perturbed version of thesample distribution p?.Consider Eq.
2.
Given a training dataset, maxi-mizing likelihood under this model means assumingthat there is some w?
for which the true distribu-tion p?
(x, y) = pw?
(x, y).
Carrying out MLE, how-ever, would require computing the partition function?x?,y?
q0(x?, y?)ew>f(x?,y?
), which is in general in-tractable.
Rearranging Eq.
2 slightly, we haveq0(x, y) ?
p?
(x, y)e?w>f(x,y) (4)If q0 is close to the true model, e?w>f(x,y) shouldbe close to 1 and w close to zero.
In the sequencemodel setting, for example, if q0 is an HMM that ex-plains the data well, then the additional features arenot necessary (equivalently, their weights should be0).
If q0 is imperfect, we might wish to make it morepowerful by adding features (e.g., f ), but q0 nonethe-less provides a reasonable ?starting point?
for defin-ing our model.So instead of maximizing likelihood, we will min-imize the KL divergence between the two sides ofEq.
4.3DKL(q0(x, y)?p?
(x, y)e?w>f(x,y)) (5)=?x,yq0(x, y) logq0(x, y)p?
(x, y)e?w>f(x,y)(6)+?x,yp?
(x, y)e?w>f(x,y) ?
?x,yq0(x, y)= ?H(q0) +?x,yp?
(x, y)e?w>f(x,y) ?
1?
?x,yq0(x, y) log(p?
(x, y)e?w>f(x,y))= constant(w) +?x,yp?
(x, y)e?w>f(x,y)+?x,yq0(x, y)(w>f(x, y))3The KL divergence here is generalized for unnormalizeddistributions, following O?Sullivan (1998):DKL(u?v) =Pj?uj logujvj?
uj + vj?where u and v are nonnegative vectors defining unnormal-ized distributions over the same event space.
Note that whenPj uj =Pj vj = 1, this formula takes on the more familiarform, as ?Pj uj andPj vj cancel.753If we replace p?
with the empirical (sampled) dis-tribution p?, minimizing the above KL divergence isequivalent to minimizing `(w) (Eq.
3).
It may behelpful to think of?w as the parameters of a processthat ?damage?
the true model p?, producing q0, andthe estimation ofw as learning to undo that damage.In the remainder of the paper, we use the generalterm ?M-estimation?
to refer to the minimization of`(w) as a way of training a log-linear model.4 Algorithms for Models of Sequences andTreesWe discuss here some implementation aspects of theapplication of M-estimation to NLP models.4.1 Expectations under q0The base distribution q0 enters into implementationin two places: Eq0(X,Y )[f(X,Y )] must be computedfor training, and q0(x, y) is a factor in the modelused in decoding.If q0 is a familiar stochastic grammar, such as anHMM or a PCFG, or any generative model fromwhich sampling is straightforward, it is possible toestimate the feature expectations by sampling fromthe model directly; for sample ?
(x?i, y?i)?si=1 let:Eq0(X,Y )[fj(X,Y )]?1ss?i=1fj(x?i, y?i) (7)If the feature space is sparse under q0 (likely in mostsettings), then smoothing may be required.If q0 is an HMM or a PCFG, the expectation vec-tor can be computed exactly by solving a system ofequations.
We will see that for the common caseswhere features are local substructures, inference isstraightforward.
We briefly describe how this can bedone for a bigram HMM and a PCFG.4.1.1 Expectations under an HMMLet S be the state space of a first-order HMM.If s = ?s1, ..., sk?
is a state sequence and x =?x1, ..., xk?
is an observed sequence of emissions,then:q0(s,x) =(k?i=1tsi?1(si)esi(xi))tsk(stop) (8)(Assume s0 = start is the single, silent, initial state,and stop is the only stop state, also silent.
We as-sume no other states are silent.
)The first step is to compute path-sums into and outof each state, under the HMM q0.
To do this, defineis as the total weight of state-prefixes (beginning instart) ending in s and os as the total weight of state-suffixes beginning in s (and ending in stop):4istart = ostop = 1 (9)?s ?
S \ {start, stop} :is =??n=1??s1,...,sn??Sn(n?i=1tsi?1(si))tsn(s)=?s??Sis?ts?
(s) (10)os =??n=1??s1,...,sn??Snts(s1)(n?i=2tsi?1(si))=?s??Sts(s?)os?
(11)This amounts to two linear systems given the tran-sition probabilities t, where the variables are i?
ando?, respectively.
In each system there are |S| vari-ables and |S| equations.
Once solved, expectedcounts of transition and emission features under q0are straightforward:Eq0 [stransit?
s?]
= ists(s?
)os?Eq0 [semit?
x] = ises(x)osGiven i and o, Eq0 can be computed for other fea-tures in the model in a similar way, provided theycorrespond to contiguous substructures.
For exam-ple, a feature f627 that counts occurrences of ?Si =s and Xi+3 = x?
has expected value Eq0 [f627] =?s?,s??,s????Sists(s?)ts?(s??)ts??(s???)es???(x)os???
(12)Non-contiguous substructure features with ?gaps?require summing over paths between any pair ofstates.
This is straightforward (we omit it for space),but of course using such features (while interesting)would complicate inference in decoding.4It may be helpful to think of i as forward probabilities, butfor the observation set Y?
rather than a particular observationy.
o are like backward probabilities.
Note that, because somecounted prefixes are prefixes of others, i can be > 1; similarlyfor o.7544.1.2 Expectations under a PCFGIn general, the expectations for a PCFG requiresolving a quadratic system of equations.
The anal-ogy this time is to inside and outside probabilities.Let the PCFG have nonterminal set N, start symbolS ?
N, terminal alphabet ?, and rules of the formA ?
B C and A ?
x.
(We assume Chomsky nor-mal form for clarity; the generalization is straight-forward.)
Let rA(B C) and rA(x) denote the proba-bilities of nonterminal A rewriting to child sequenceB C or x, respectively.
Then ?A ?
N:oA =?B?N?C?NoBiC [rB(A C) + rB(C A)]+{1 if A = S0 otherwiseiA =?B?N?C?NrA(B C)iBiC +?xrA(x)ixox =?A?NoArA(x),?x ?
?ix = 1,?x ?
?In most practical applications, the PCFG will be?tight?
(Booth and Thompson, 1973; Chi and Ge-man, 1998).
Informally, this means that the proba-bility of a derivation rooted in S failing to terminateis zero.
If that is the case, then iA = 1 for allA ?
N,and the system becomes linear (see also Corazzaand Satta, 2006).5 If tightness is not guaranteed,iterative propagation of weights, following Stolcke(1995), works well in our experience for solving thequadratic system, and converges quickly.As in the HMM case, expected counts of arbitrarycontiguous tree substructures can be computed asproducts of probabilities of rules appearing withinthe structure, factoring in the o value of the struc-ture?s root and the i values of the structure?s leaves.4.2 OptimizationTo carry out M-estimation, we minimize the func-tion `(w) in Eq.
3.
To apply gradient de-scent or a quasi-Newton numerical optimizationmethod,6 it suffices to specify the fixed quantities5The same is true for HMMs: if the probability of non-termination is zero, then for all s ?
S, os = 1.6We use L-BFGS (Liu and Nocedal, 1989) as implementedin the R language?s optim function.f(xi, yi) (for all i ?
{1, 2, ..., n}) and the vectorEq0(X,Y )[f(X,Y )].
The gradient is:7?`?wj= ?n?i=1e?w>f(xi,yi)fj(xi, yi) + Eq0 [fj ](13)The Hessian (matrix of second derivatives) can alsobe computed with relative ease, though the space re-quirement could become prohibitive.
For problemswherem is relatively small, this would allow the useof second-order optimization methods that are likelyto converge in fewer iterations.It is easy to see that Eq.
3 is convex in w. There-fore, convergence to a global optimum is guaranteedand does not depend on the initializing value of w.4.3 RegularizationRegularization is a technique from pattern recogni-tion that aims to keep parameters (likew) from over-fitting the training data.
It is crucial to the perfor-mance of most statistical learning algorithms, andour experiments show it has a major effect on thesuccess of the M-estimator.
Here we use a quadraticregularizer, minimizing `(w) + (w>w)/2c.
Notethat this is also convex and differentiable if c > 0.The value of c can be chosen using a tuning dataset.This regularizer aims to keep each coordinate of wclose to zero.In the M-estimator, regularization is particularlyimportant when the expectation of some feature fj ,Eq0(X,Y )[fj(X,Y )] is equal to zero.
This can hap-pen either due to sampling error (fj simply failedto appear with a positive value in the finite sample)or because q0 assigns zero probability mass to anyx ?
X, y ?
Y where fj(x, y) 6= 0.
Without regular-ization, the weight wj will tend toward ?
?, but thequadratic penalty term will prevent that undesirabletendency.
Just as the addition of a quadratic regular-izer to likelihood can be interpreted as a zero-meanGaussian prior on w (Chen and Rosenfeld, 2000), itcan be so-interpreted here.
The regularized objectiveis analogous to maximum a posteriori estimation.5 Shallow ParsingWe compared M-estimation to a hidden Markovmodel and other training methods on English noun7Taking the limit as n ?
?
and setting equal to zero, wehave the basis for a proof that `(w) is statistically consistent.755HMM CRF MEMM PL M-est.2 sec.
64:18 3:40 9:35 1:04Figure 1: Wall time (hours:minutes) of training theHMM and 100 L-BFGS iterations for each of theextended-feature models on a 2.2 GHz Sun Opteronwith 8GB RAM.
See discussion in text for details.phrase (NP) chunking.
The dataset comes fromthe Conference on Natural Language Learning(CoNLL) 2000 shallow parsing shared task (TjongKim Sang and Buchholz, 2000); we apply the modelto NP chunking only.
About 900 sentences were re-served for tuning regularization parameters.Baseline/q0 In this experiment, the simple base-line is a second-order HMM.
The states correspondto {B, I,O} labels, denoting the beginning, inside,and outside of noun phrases.
Each state emits atag and a word (independent of each other given thestate).
We replaced the first occurrence of every tagand of every word in the training data with an OOVsymbol, giving a fixed tag vocabulary of 46 and afixed word vocabulary of 9,014.
Transition distribu-tions were estimated using MLE, and tag- and word-emission distributions were estimated using add-1smoothing.
The HMM had 27,213 parameters.
ThisHMM achieves 86.3% F1-measure on the develop-ment dataset (slightly better than the lowest-scoringof the CoNLL-2000 systems).
Heavier or weakersmoothing (an order of magnitude difference in add-?)
of the emission distributions had very little effect.Note that HMM training time is negligible (roughly2 seconds); it requires counting events, smoothingthe counts, and normalizing.Extended Feature Set Sha and Pereira (2003) ap-plied a conditional random field to the NP chunk-ing task, achieving excellent results.
To improve theperformance of the HMM and test different estima-tion methods, we use Sha and Pereira?s feature tem-plates, which include subsequences of labels, tags,and words of different lengths and offsets.
Here,we use only features observed to occur at least oncein the training data, accounting (in addition to ourOOV treatment) for the slight drop in performanceprec.
recall F1HMM features:HMM 85.60 88.68 87.11CRF 90.40 89.56 89.98PL 80.31 81.37 80.84MEMM 86.03 88.62 87.31M-est.
85.57 88.65 87.08extended features:CRF 94.04 93.68 93.86PL 91.88 91.79 91.83MEMM 90.89 92.15 91.51M-est.
88.88 90.42 89.64Table 1: NP chunking accuracy on test data us-ing different training methods.
The effects of dis-criminative training (CRF) and extended feature sets(lower section) are more than additive.compared to what Sha and Pereira report.
There are630,862 such features.Using the original HMM feature set and the ex-tended feature set, we trained four models that canuse arbitrary features: conditional random fields(a near-replication of Sha and Pereira, 2003), maxi-mum entropy Markov models (MEMMs; McCal-lum et al, 2000), pseudolikelihood (Besag, 1975;see Toutanova et al, 2003, for a tagging applica-tion), and our M-estimator with the HMM as q0.CRFs and MEMMs are discriminatively-trained tomaximize conditional likelihood (the former is pa-rameterized using a sequence-normalized log-linearmodel, the latter using a locally-normalized log-linear model).
Pseudolikelihood is a consistent esti-mator for the joint likelihood, like our M-estimator;its objective function is a sum of log probabilities.In each case, we trained seven models foreach feature set with quadratic regularizers c ?
[10?1, 10], spaced at equal intervals in the log-scale,plus an unregularized model (c =?).
As discussedin ?4.2, we trained using L-BFGS; training contin-ued until relative improvement fell within machineprecision or 100 iterations, whichever came first.After training, the value of c is chosen that maxi-mizes F1 accuracy on the tuning set.Runtime Fig.
1 compares the wall time ofcarefully-timed training runs on a dedicated server.Note that Dyna, a high-level programming language,was used for dynamic programming (in the CRF)756and summations (MEMM and pseudolikelihood).The runtime overhead incurred by using Dyna is es-timated as a slow-down factor of 3?5 against a hand-tuned implementation (Eisner et al, 2005), thoughthe slow-down factor is almost certainly less for theMEMM and pseudolikelihood.
All training (exceptthe HMM, of course) was done using the R languageimplementation of L-BFGS.
In our implementation,the M-estimator trained substantially faster than theother methods.
Of the 64 minutes required to trainthe M-estimator, 6 minutes were spent precomput-ing Eq0(X,Y )[f(X,Y )] (this need not be repeated ifthe regularization settings are altered).Accuracy Tab.
1 shows how NP chunking accu-racy compares among the models.
With HMMfeatures, the M-estimator is about the same as theHMM and MEMM (better than PL and worse thanthe CRF).
With extended features, the M-estimatorlags behind the slower methods, but performs aboutthe same as the HMM-featured CRF (2.5?3 pointsover the HMM).
The full-featured CRF improvesperformance by another 4 points.
Performance asa function of training set size is plotted in Fig.
2;the different methods behave relatively similarly asthe training data are reduced.
Fig.
3 plots accuracy(on tuning data) against training time, for a vari-ety of training dataset sizes and regularizaton set-tings, under different training methods.
This illus-trates the training-time/accuracy tradeoff: the M-estimator, when well-regularized, is considerablyfaster than the other methods, at the expense of ac-curacy.
This experiment gives some insight into therelative importance of extended features versus es-timation methods.
The M-estimated model is, likethe maximum likelihood-estimated HMM, a gener-ative model.
Unlike the HMM, it uses a much largerset of features?the same features that the discrimina-tive models use.
Our result supports the claim thatgood features are necessary for state-of-the-art per-formance, but so is good training.5.1 Effect of the Base DistributionWe now turn to the question of the base distributionq0: how accurate does it need to be?
Given that theM-estimator is consistent, it should be clear that, inthe limit and assuming that our model family p iscorrect, q0 should not matter (except in its support).q0 selection prec.
recall F1HMM F1, prec.
88.88 90.42 89.64l.u.
F1 72.91 57.56 64.33prec.
84.40 37.68 52.10emp.
F1 84.38 89.43 86.83Table 2: NP chunking accuracy on test data usingdifferent base models for the M-estimator.
The ?se-lection?
column shows which accuracy measure wasoptimized when selecting the hyperparameter c.In NLP, we deal with finite datasets and imperfectmodels, so q0 may have practical importance.We next consider an alternative q0 that is far lesspowerful; in fact, it is uninformative about the vari-able to be predicted.
Let x be a sequence of words,t be a sequence of part-of-speech tags, and y be asequence of {B, I,O}-labels.
The model is:ql.u.0 (x, t,y)def=??|x|?i=1puni(xi)puni(ti)1Nyi?1?
?1Ny|x|(14)where Ny is the number of labels (including stop)that can follow y (3 for O and y0 = start, 4 forB and I).
puni are the tag and word unigram distri-butions, estimated using MLE with add-1 smooth-ing.
This model ignores temporal effects.
On itsown, this model achieves 0% precision and recall,because it labels every word O (the most likely labelsequence is O|x|).
We call this model l.u.
(?locallyuniform?).Tab.
2 shows that, while an M-estimate that usesql.u.0 is not nearly as accurate as the one based onan HMM, the M-estimator did manage to improveconsiderably over ql.u.0 .
So the M-estimator is farbetter than nothing, and in this case, tuning c tomaximize precision (rather than F1) led to an M-estimated model with precision competitive with theHMM.
We point this out because, in applications in-volving very large corpora, a model with good preci-sion may be useful even if its coverage is mediocre.Another question about q0 is whether it shouldtake into account all possible values of the inputvariables (here, x and t), or only those seen in train-ing.
Consider the following model:qemp0 (x, t,y)def= q0(y | x, t)p?
(x, t) (15)Here we use the empirical distribution over tag/word7577075808590951000 2000 4000 6000 8000 10000training set sizeF 1 CRFPLMEMMM-est.HMMFigure 2: Learning curves for different estimators;all of these estimators except the HMM use the ex-tended feature set.657075808590951000 1 10 100 1000 10000 100000 1000000training time (seconds)F 1M-est.
CRFHMM PLMEMMFigure 3: Accuracy (tuning data) vs. training time.The M-estimator trains notably faster.
The pointsin a given curve correspond to different regulariza-tion strengths (c); M-estimation is more damaged byweak than strong regularization.sequences, and the HMM to define the distri-bution over label sequences.
The expectationsEqemp0 (X)[f(X)] can be computed using dynamicprogramming over the training data (recall that thisonly needs to be done once, cf.
the CRF).
Strictlyspeaking, qemp0 assigns probability zero to any se-quence not seen in training, but we can ignore thep?
marginal at decoding time.
As shown in Tab.
2,this model slightly improves recall over the HMM,but damages precision; the gains of M-estimationseen with the HMM as q0, are not reproduced.
Fromthese experiments, we conclude that theM-estimatormight perform considerably better, given a better q0.5.2 Input-Only FeaturesWe present briefly one negative result.
Noting thatthe M-estimator is a modeling technique that esti-mates a distribution over both input and output vari-ables (i.e., a generative model), we wanted a wayto make the objective more discriminative while stillmaintaining the computational property that infer-ence (of any kind) not be required during the innerloop of iterative training.The idea is to reduce the predictive burden onthe feature weights for f .
When designing a CRF,features that do not depend on the output variable(here, y) are unnecessary.
They cannot distinguishbetween competing labelings for an input, and sotheir weights will be set to zero during conditionalestimation.
The feature vector function in Sha andPereira?s chunking model does not include suchfeatures.
In M-estimation, however, adding such?input-only?
features might permit better modelingof the data and, more importantly, use the origi-nal features primarily for the discriminative task ofmodeling y given the input.Adding unigram, bigram, and trigram featuresto f for M-estimation resulted in a very small de-crease in performance: selecting for F1, this modelachieves 89.33 F1 on test data.6 DiscussionM-estimation fills a gap in the plethora of train-ing techniques that are available for NLP mod-els today: it permits arbitrary features (like so-called conditional ?maximum entropy?
models suchas CRFs) but estimates a generative model (permit-ting, among other things, classification on input vari-ables and meaningful combination with other mod-els).
It is similar in spirit to pseudolikelihood (Be-sag, 1975), to which it compares favorably on train-ing runtime and unfavorably on accuracy.Further, since no inference is required duringtraining, any features really are permitted, so longas their expected values can be estimated under thebase model q0.
Indeed, M-estimation is consider-ably easier to implement than conditional estima-tion.
Both require feature counts from the train-ing data; M-estimation replaces repeated calculationand differentiation of normalizing constants with in-ference or sampling (once) under a base model.
So758the M-estimator is much faster to train.Generative and discriminative models have beencompared and discussed a great deal (Ng and Jordan,2002), including for NLP models (Johnson, 2001;Klein and Manning, 2002).
Sutton and McCallum(2005) present approximate methods that keep a dis-criminative objective while avoiding full inference.We see M-estimation as a particularly promisingmethod in settings where performance depends onhigh-dimensional, highly-correlated feature spaces,where the desired features ?large,?
making discrimi-native training too time-consuming?a compellingexample is machine translation.
Further, in somesettings a locally-normalized conditional log-linearmodel (like an MEMM) may be difficult to design;our estimator avoids normalization altogether.8 TheM-estimator may also be useful as a tool in design-ing and selecting feature combinations, since moretrials can be run in less time.
After selecting a fea-ture set under M-estimation, discriminative trainingcan be applied on that set.
The M-estimator mightalso serve as an initializer to discriminative mod-els, perhaps reducing the number of times inferencemust be performed?this could be particularly use-ful in very-large data scenarios.
In future work wehope to explore the use of the M-estimator withinhidden variable learning, such as the Expectation-Maximization algorithm (Dempster et al, 1977).7 ConclusionsWe have presented a new loss function for genera-tively estimating the parameters of log-linear mod-els.
The M-estimator is fast to train, requiringno repeated, expensive calculation of normalizationterms.
It was shown to improve performance ona shallow parsing task over a baseline (generative)HMM, but it is not competitive with the state-of-the-art.
Our sequence modeling experiments supportthe widely accepted claim that discriminative, rich-feature modeling works as well as it does not justbecause of rich features in the model, but also be-cause of discriminative training.
Our technique fillsan important gap in the spectrum of learning meth-ods for NLP models and shows promise for applica-tion when discriminative methods are too expensive.8Note that MEMMs also require local partition functions?which may be expensive?to be computed at decoding time.ReferencesJ.
E. Besag.
1975.
Statistical analysis of non-lattice data.
TheStatistician, 24:179?195.T.
L. Booth and R. A. Thompson.
1973.
Applying probabil-ity measures to abstract languages.
IEEE Transactions onComputers, 22(5):442?450.S.
Chen and R. Rosenfeld.
2000.
A survey of smoothing tech-niques for ME models.
IEEE Transactions on Speech andAudio Processing, 8(1):37?50.Z.
Chi and S. Geman.
1998.
Estimation of probabilis-tic context-free grammars.
Computational Linguistics,24(2):299?305.A.
Corazza and G. Satta.
2006.
Cross-entropy and estimationof probabilistic context-free grammars.
In Proc.
of HLT-NAACL.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximum likeli-hood estimation from incomplete data via the EM algorithm.Journal of the Royal Statistical Society B, 39:1?38.J.
Eisner, E. Goldlust, and N. A. Smith.
2005.
CompilingComp Ling: Practical weighted dynamic programming andthe Dyna language.
In Proc.
of HLT-EMNLP.Y.
Jeon and Y. Lin.
2006.
An effective method for high-dimensional log-density ANOVA estimation, with applica-tion to nonparametric graphical model building.
StatisticalSinica, 16:353?374.M.
Johnson.
2001.
Joint and conditional estimation of taggingand parsing models.
In Proc.
of ACL.D.
Klein and C. D. Manning.
2002.
Conditional structure vs.conditional estimation in NLP models.
In Proc.
of EMNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proc.
of ICML.D.
C. Liu and J. Nocedal.
1989.
On the limited memory BFGSmethod for large scale optimization.
Math.
Programming,45:503?528.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maximumentropy Markov models for information extraction and seg-mentation.
In Proc.
of ICML.A.
Ng and M. Jordan.
2002.
On discriminative vs. generativeclassifiers: A comparison of logistic regression and na??veBayes.
In NIPS 14.J.
A. O?Sullivan.
1998.
Alternating minimization algo-rithms: from Blahut-Armijo to Expectation-Maximization.In A. Vardy, editor, Codes, Curves, and Signals: CommonThreads in Communications, pages 173?192.
Kluwer.R.
Rosenfeld.
1997.
A whole sentence maximum entropy lan-guage model.
In Proc.
of ASRU.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In Proc.
of HLT-NAACL.A.
Stolcke.
1995.
An efficient probabilistic context-free pars-ing algorithm that computes prefix probabilities.
Computa-tional Linguistics, 21(2):165?201.C.
Sutton and A. McCallum.
2005.
Piecewise training of undi-rected models.
In Proc.
of UAI.E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Introductionto the CoNLL-2000 shared task: Chunking.
In Proc.
ofCoNLL.K.
Toutanova, D. Klein, C. D. Manning, and Y.
Singer.
2003.Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In Proc.
of HLT-NAACL.A.
W. van der Vaart.
1998.
Asymptotic Statistics.
CambridgeUniversity Press.759
