Identification of Non-Linguistic Speech FeaturesJean-Luc Gauvain and Lori F. LamelLIMSI-CNRS, BP 13391403 Orsay cedex, FRANCE{lamel,gauvain}@limsi.frABSTRACTOver the last decade technological dvances have been made which en-able us to envision real-world applications of speech technologies.
It ispossible to foresee applications where the spoken query is to be recognizedwithout even prior knowledge of the language being spoken, for exana-pie, information centers in public places such as train stations and airports.Other applications may require accurate identification of the speaker for se-curity reasons, including control of access to confidential information or fortelephone-based transactions.
Ideally, the speaker's identity can be verifiedcontinually during the transaction, in a manner completely transparent tothe user.
With these views in mind, this paper presents a unified approachto identifying non-linguistic speech features from the recorded signal usingphone-based acoustic likelihoods.This technique is shown to be effective for text-independent language,sex, and speaker identification and can enable better and more friendlyhuman-machine teraction.
With 2s of speech, the language can be identi-fied with better than 99% accuracy.
Error in sexddentification is about 1%on a per-sentence basis, and speaker identification accuracies of 98.5% on'lIMIT (168 speakers) and 99.2% on BREF (65 speakers), were obtainedwith one utterance per speaker, and 100% with 2 utterances for both corpora.An experiment using unsupervised a aptation for speaker identification onthe 168 TIMIT speakers had the same identification accuracies obtainedwith supervised adaptation.INTRODUCTIONAs speech recognition technology advances, so do theaims of system designers, and the prospects of potential ap-plications.
One of the main efforts underway in the com-munity is the development of speaker-independent, task-independent large vocabulary speech recognizers that caneasily be adapted to new tasks.
It is becoming apparentthat many of the portability issues may depend more on thespecification of the task, and the ergonomy, than on the per-formance of the speech recognition component itself.
Theacceptance of speech technology in the world at large willdepend on how well the technology can be integrated in sys-tems which simplify the life of the users.
This in turns meansthat the service provided by such a system must be easy touse, and as fast as other providers of the service (i.e., such asusing a human operator).While the focus has been on improving the performanceof the speech recognizers, it is also of interest o be ableto identify what we refer to as some of the "non-linguistic"speech features present in the acoustic signal.
For example,it is possible to envision applications where the spoken queryis to be recognized without prior knowledge of the languagebeing spoken.
This is the case for information centers inpublic places, such as train stations and airports, where thelanguage may change from one user to the next.
The abilityto automatically identify the language being spoken, and torespond appropriately, is possible.Other applications, uch as for financial or banking trans-actions, or access to confidential information, such as fi-nancial, medical or insurance records, etc., require accurateidentification or verification of the user.
Typically securityis provided by the human who "recognizes" the voice ofthe client he is used to dealing with (and often will also beconfirmed by a fax), or for automated systems by the useof cards and/or codes, which must be provided in order toaccess the data.
With the widespread use of telephones,and the new payment and information retrieval services of-fered by telephone, it is a logical extension to explore theuse of speech for user identification.
An advantage is thatif text-independent speaker verification techniques are used,the speaker's identity can be continually verified during thetransaction, in a manner completely transparent to the user.This can avoid the problems encountered by theft or dupli-cation of cards, and pre-recording of the user's voice duringan earlier transaction.With these future views in mind, this paper presents a uni-fied approach for identifying non-linguistic speech features,such as the language being spoken, and the identity or sexof the speaker, using phone-based acoustic likelihoods.
Thebasic idea is similar to that of using sex-dependent models forrecognition, but instead of the output being the recognizedstring, the output is the characteristic associated with themodel set having the highest likelihood.
This approach asbeen evaluated for French/English language identification,and speaker and sex identification i both languages.PHONE-BASED ACOUSTIC  L IKEL IHOODSThe basic idea is to train a set of large phone-based rgodichidden Markov models (HMMs) for each non-linguistic fea-ture to be identified (language, gender, speaker, ...).
Featureidentification on the incoming signal x is then performed bycomputing the acoustic likelihoods f(xlAi) for all the mod-els Ai of a given set.
The feature value corresponding to themodel with the highest likelihood is then hypothesized.
This96decoding procedure can efficiently be implemented by pro-cessing all the models in parallel using a time-synchronousbeam search strategy.This approach as the following advantages:?
It can perform text-independent feature recognition.
(Text-dependent feature recognition can also be performed.)?
It is more precise than methods based on long-term statis-tics such as long term spectra, VQ codebooks, or proba-bilistic acoustic maps\[26, 28\].?
It can easily take advantage of phonotactic constraints.
(Theseare shown to be useful for language identification.)?
It can easily be integrated in recognizers which are basedon phone models as all the components already exist.A disadvantage of the approach is that, at least in the cur-rent formulation, phonetic labels are required for training themodels.
However, there is in theory no absolute need forphonetic labeling of the speech training data to estimate theHMM parameters.
Labeling of a small portion of the train-ing data can be enough to bootstrap the training procedureand insure the phone-based nature of the resulting models.
(In this case, phonotactic constraints must be obtained onlyfrom speech corpora.)
We have sucessfully experimentedwith this approach for speaker identification.In our implementation, each large ergodic HMM is builtfrom small left-to-right phonetic HMMs.
The Viterbi algo-rithm is used to compute the joint likelihood f (x,  s lAi) of theincoming signal and the most likely state sequence instead off(xlAi).
This implementation is therefore nothing more thana slightly modified phone recognizer with language-, sex-, orspeaker- dependent model sets used in parallel, and wherethe output phone string is ignored 1 and only the acousticlikelihood for each model is taken into account.The phone recognizer can use either context-dependent orcontext-independent phone models, where ach phone modelis a 3-state left-to-fight continuous density hidden Markovmodel (CDHMM) with Gaussian mixture observation den-sities.
The covariance matrices of all Gaussian componentsare diagonal.
Duration is modeled with a gamma distribu-tion per phone model.
As proposed by Rabiner et a1.\[23\], theHMM and duration parameters are estimated separately andcombined in the recognition process for the Viterbi search.Maximum likelihood estimators are used to derive lan-guage specific models whereas maximum a posteriori (MAP)estimators are used to generate sex- and speaker- specificmodels as has already been proposed in \[11\].
The MAPestimates are obtained with the segmental MAP algorithm\[16, 9, 10\] using speaker-independent seed models.
Theseseed models are used to estimate the parameters of the priordensities and to serve as an initial estimate for the segmentalMAP algorithm.
This approach provides a way to incorpo-rate prior information i to the model training process and is~The likelihood computation can in fact be simplified since there isno need to maintain the backtracking infomlation ecessary to know therecognized phone sequence.particularly useful to build the speaker specific models whenusing only a small amount of speaker specific data.In our earlier reported results using this approach forlanguage- and speaker-identification\[13, 14, 7\], the acous-tic likelihoods were computed sequentially for each of themodels.
As mentioned earlier, the Viterbi decoder is nowimplemented asa one-pass beam search procedure appliedon all the models in parallel, resulting in an efficient decodingprocedure which saves a lot of computation.EXPERIMENTAL CONDIT IONSFour corpora have been used to carry out the experimentsreported in this paper: BDSONS\[2\] and BREF\[15, 8\] forFrench; and TIMIT\[4\] and WSJ0122\] for English.
From theBDSONS corpus only the phonetically equilibrated sentencesub-corpus (CDROM 6) has been used for testing, whereasdepending on experiment, the 3 other corpora have been usedfor training and testing.The BDSONS Corpus: BDSONS, Base de Donn6es desSons du Fran~ais\[2\], was designed to provide a large cor-pus of French speech data for the study of the sounds inthe French language and to aid speech research.
The cor-pus contains an "evaluation" subcorpus consisting primarilyof isolated and connected letters, digits and words from 32speakers (16m/16f), and an "acoustic" subcorpus which in-cludes phonetically balanced words and sentences from 12speakers (6m/6f).The BREF Corpus: BREF is a large read-speech cor-pus, containing over 100 hours of speech material, from 120speakers (55m/65f)\[15\].
The text materials were selectedverbatim from the French newspaper Le Monde, so as toprovide a large vocabulary (over 20,000 words) and a widerange of phonetic environments\[8\].
Containing 1115 distinctdiphones and over 17,500 triphones, BREF can be used totrain vocabulary-independenet phonetic models.
The textmaterial was read without verbalized punctuation.The DARPA WSJ0 Corpus: The DARPA Wall StreetJournal-based Continuous-Speech Corpus (WSJ)\[22\] hasbeen designed to provide general-purpose speech data (pri-marily, read speech data) with large vocabularies.
Textmaterials were selected to provide training and test datafor 5K and 20K word, closed and open vocabularies, andwith both verbalized and non-verbalized punctuation.
Therecorded speech material supports both speaker-dependentand speaker-independent training and evaluation.The DARPA TIMIT Corpus: The DARPA TIMITAcoustic-Phonetic Continuous Speech Corpus\[4\] is a cor-pus of read speech designed to provide speech data for theacquisition of acoustic-phonetic knowledge and for the de-velopment and evaluation of automatic speech recognitionsystems.
TIMIT contains a total of 6300 sentences, 10 sen-tences poken by each of 630 speakers from 8 major dialectregions of the U.S.
The TIMIT CDROM\[4\] contains atrain-ing/test subdivision of the data that ensures that there is no97overlap in the text materials.
All of the utterances in TIMIThave associated time-aligned phonetic transcriptions.Since the identification of non-linguistic speech featuresis based ,on phone recognition, some phone recognition re-sults for the above corpora are given here.
The speaker-independent (SI) phone recognizers use sets of context-dependent (CD) models which were automatically selectedbased on their frequencies in the training data.
There are428 sex-dependent CD models for BREF, 1619 for WSJ and459 for TIMIT.
Phone errors rates are given in Table 1.
ForBREF and WSJ phone errors are reported after removingsilences, whereas for TIMIT silences are included as tran-scribed.
Scoring without the sentence initial/final silenceincreases the phone error by about 1.5%.
The phone er-ror for BREF is 21.3%, WSJ (Feb-92 5knvp) is 25.7% andTIMIT (complete testset) is 27.6% scored using the 39 phoneset proposed by\[18\].
These results are provided to calibratethe recognizers used in the experiments in this paper, andobserve differences in the corpora.
It appears that the BREFdata is easiest to recognize at the phone level, and that TIMITis more difficult than WSJ.i Condition Correct Subs.BREF 81.7 13.7WSJ nvp 79.3 16.2TIMIT 77.3 17.3Del.
Ins.
Errors4.6 3.0 21.34.5 5.0 25.75.4 4.9 27.6Table 1: Phone error (%) with CD models and phonebigram.SEX IDENTIF ICAT IONIt is well known that he use of sex-dependent models givesimproved performance over one set of speaker-independentmodels.
However, this approach can be costly in terms ofcomputation for medium-to-large-size tasks, since recogni-tion of the unknown sentence is typically carried out twice,once for each sex.
A logical alternative is to first determinethe speaker's sex, and then to perform word recognition us-ing the models of selected sex.
This is the approach used inour Nov-92 WSJ system\[6\].
In these experiments he stan-dard SI-84 training material, containing 7240 sentences from84 speakers (42m/42f) is used to build speaker-independentphone models.
Sex-dependent models are then obtained us-ing MAP estimation\[11\] with the SI seed models.
The phonelikelihoods using context-dependent male and female mod-els were computed, and the sex of the speaker was selectedas the sex associated with the models that gave the highestlikelihood.
Since these CD male and female models are thesame as are used for word recognition, there is no need for ad-ditional training material or effort.
No errors were observedin sex identification for WSJ on the Feb92 or Nov92 5k testdata containing 851 sentences, from 18 speakers (10m/8f).For BREF, sex-dependent models were also obtained fromSI seeds by MAP estimation.
The training data consisted of2770 sentences from 57 speakers (28m/29f).
No errors insex-identification were observed on 109 test sentences from21 test speakers (10m/1 If).To further investigate sex identification based on acous-tic likelihoods on a larger set of speakers, the approach wasevaluted on the 168 speakers of the TIMIT test corpus.
TheSI seed models were trained using all the available trainingdata, i.e., 4620 sentences from 462 speakers, and adaptedusing data from the 326 males speakers and 136 females toform gender-specific models.
The test data consist of 1344sentences, comprised of 8 sentences from each of the 168test speakers (112m/560.
Results are shown in the first rowof Table 2 where the error rate is given as a function of thespeech duration.
Each speech segment used for the test ispart of a single sentence, and always starts at the beginning ofthe sentence, preceeded by about lOOms of silence 2.
Theseresults on this more significant test show that sex identifica-tion error ate using phone-based acoustic likelihoods is 2.8 %with 400ms of speech and is under 1% with 2s of speech.The 400ms of speech signal (which includes about lOOms ofsilence) represents about 4 phones, about the number foundin a typical word (avg.
3.9 phones/word) in TIMIT.
This im-plies that before the speaker has finished enunciating the firstword, one is fairly certain of the speaker's ex.
Sentencesmisclassified with regards to the speaker's ex had betterphone recognition accuracies with the cross-sex models.Using exactly the same test data and the same phone mod-els, an experiment of text-dependent sex identification wascarried out in order to assess if by adding linguistic informa-tion the speaker's gender can be more easily identified.
To dothis a long left-to-right HMM is built for each sex by concate-nating the sex-dependent CD phone models correspondingto the TIMIT transcriptions.
The basic idea is to measure thelower bound on the error ate that would be obtained if higherorder knowledge such as lexical information were provided.The acoustic likelihoods are then computed for the two mod-els.
These likelihood values are lower than are obtained fortext-independent identification.
The results are given in thesecond row of Table 2 where it can be seen that the errorrate is not any better than the error rate obtained with thetext-independent method.
This shows that acoustic-phoneticknowledge is sufficient o accomplish this task.Duration 0.4s 0.8s 1.2s 1.6s 2.0s EOSText indep.
2.8 1.9 1.5 1.2 0.9 ~ 1.2Text dep.
3.4 2.2 1.0 1.0 1.2 i 1.3JTable 2: Error rate in sex identification as a function of duration.
(EOS isEnd Of Sentence identification error rate.
)While in our previous work\[6\], sex-identification was usedprimarily as a means to reduce the computation, sex identifi-cation can permit he synthesis module of a system to respondappropriately tothe unknown speaker.
In French, where the2The initial and final silences of each test sentence have been automati-cally reduced to lOOms.98formalities are used perhaps more than in English, the systemacceptance may be easier if the familiar "Bonjour Madame"or "Je vous en prie Monsieur" is foreseen.Since sex-identification is not perfect, some fall-backmechanism ust be integrated to avoid including the signs ofpoliteness if the system is unsure of the sex.
This can be ac-complished by comparing the likelihoods of the model sets,or by being wary of speakers for whom the better likelihoodjumps back and forth between models.LANGUAGE IDENTIF ICAT IONLanguage identification is another feature that can beidentified using the same approach.
In this case language-dependent models are used instead of sex-dependent ones.The basic idea is to process in parallel the unknown incom-ing speech by different sets of phone models (each set is alarge ergodic HMM) for each of the languages under con-sideration, and to choose the language associated with themodel set providing the highest normalized likelihood.
3 Inthis way, it is no longer necessary to ask the speaker to selectthe language, before using the system.
If the language can beaccurately identified, it simplifies using speech recognitionfor a variety of applications, from selecting an appropriateoperator, or aiding with emergency assistance.
Languageidentification can also be done using word recognition, but itis much more efficient o use phone recognition, which hasthe added advantage of being task independent.Experimental results for language identification for En-glish/French were given in \[13, 14\], where models trained onTIMIT \[4\] and BREF \[15\], were tested on different sentencestaken from the same corpus.
While these results gave highidentification accuracies (100% if an entire sentence is used,and greater than 97% with 4ooms, and error free with 1.6s ofspeech signal), it is difficult to discern that the language andnot the corpus are being identified.
Identification of inde-pendent data taken from the WSJ0 corpus was less accurate:85% with 400ms, and 4% error with 1.6s of speech signal.In these experiments we attempted to avoid the bias due tocorpus, by testing on data from the same corpora from whichthe models are built, and on independent test data from dif-ferent corpora.
The language-dependent models are trainedfrom similar-style corpora, BREF for French and WSJ0 forEnglish, both containing read newspaper texts and similarsize vocabularies\[8, 15, 22\].
For each language a set ofcontext-independent phone models were built, 35 for Frenchand 46 for English.
4 Each phone model has 32 gaussians per3 In fact, this is not a new idea: House and Neuberg (1977)\[ 12\] proposed asimilar approach for language identification using models of broad phoneticclasses, where we use phone models.
Their experimental results, however,were synthetic, based on phonetic transcriptions derived from texts.4The 35 phones used to represent French include 14 vowels (including3 nasal vowels), 20 consonants (6 plosives, 6 fricatives, 3nasals, and 5semivowels), and silence.
The phone table can be found in \[5\].
For English,the set of 46 phones include 21 vowels (including 3 diphthongs and 3schwas), 24 consonants (6 plosives, 8fricatives, 2 affricates, 3 nasals, 5mixture, and no duration model is used.
In order to mini-mize influences due to the use of different microphones andrecording conditions a 4 kHz bandwidth is used.
The train-ing data were the same as for sex-identification on BREF(2770 sentences from 57 speakers) and WSJ (standard SI-84training: 7240 sentences from 84 speakers).Language identification accuracies are given in Tables 3and 4 without and with phonotactic constraints provided bya phone bigram.
Results are given for 4 test corpora, WSJand TIMIT for English, and BREF and BDSONS for French,as a function of the duration of the speech signal whichincludes approximately OOms of silence.
As for speaker-identification, the initial and final silences were automaticallyremoved based on HMM segmentation, so as to be able tocompare language identification as a function of durationwithout biases due to long initial silences.
The test datafor WSJ are the first 10 sentences for each of the 10 speak-ers (5m/5f) in the Feb92-si5knvp (speaker-independent, 5k,non-verbalized punctuation) test data.
For TIMIT, the 192sentences in the "coretest" set containing 8 sentences fromeach of 24 speakers (16m/80 was used.
The BREF test dataconsists of 130 sentences from 20 speakers (10m/100 andfor BDSONS the data is comprised of 121 sentences from 11speakers (5m/60.Duration 0.4s 0.8s 1.2s 1.6s 2.0s 2.4sEng.
WSJ 7.0 3.0 2.0 2.0 1.0 1.0Eng.
TIMIT 10.9 6.3 3.1 2.1 0 0Fr.
BREF 10.8 2.3 2.3 0.8 0.8 0.8Fr.
BDSONS 7.5 4.1 1.7 1.7 0.8 0Overall 9.4 4.2 2.4 1.7 0.5 0.4Table 3: Language identification error ates as a function of duration andlanguage (without phonotactic constraints).Duration 0.4s 0.8s 1.2s 1.6s 2.0s 2.4sEng.
WSJ 5.0 3.0 1.0 2.0 1.0 1.0Eng.
TIMIT 9.4 5.7 2.6 2.1 0.5 0Fr.
BREF 8.5 1.5 0.8 0 0.8 0.8Fr.
BDSONS 7.4 2.5 2.5 1.7 0.8 0Overall 7.9 3.5 1.8 1.5 0.7 0.4Table 4: Language identification error ates as a function of duration andlanguage (with phonotactic constraints).While WSJ sentences are more easily identified as Englishfor short durations, errors persist longer than for TIMIT.
Incontrast for French with 4ooms of signal, BDSONS data isbetter identified than BREF, perhaps because the sentencesare phonetically balanced.
For longer durations, BREF isslightly better identified than BDSONS.
The performanceindicates that language identification is task independent.Using phonotactic constraints i seen to improve languageidentification, particularly for short signals.
The smallestimprovement is seen for TIMIT, probably due to the naturesemivowels), and silence.99of the selected sentences which emphasized rare phone se-quences.
The error rate with 2s of speech is less than 1% andwith Is of speech (not shown in the tables) is about 2%.
With3s of speech, language identification is almost error free.Due to the source of the BREF and WSJ data, languageidentification is complicated by the inclusion of foreignwords.
One of the errors on BREF involved such a sentence.The sentence was identified as French at the beginning andthen all of a sudden switched to English.
The sentence was"Durant mon adolescence, je d6vorais les r6cits westerns deZane Grey, Luke Short, et Max Brand...", where the italicizedwords were pronounced in correct English.We are in the process of obtaining corpora for other lan-guages to extend our language identification work.
However,there are variety of applications where a bilingual system,justFrench/English would be of use, including air traffic control(where both French and English are permitted languages forflights within France), telecommunications applications, andmany automated information centers, ticket distributors, andtellers, where already you can select between English andFrench with the keyboard or touch screen.SPEAKER IDENTIF ICAT IONSpeaker identification has been the topic of active re-search for many years (see, for example, \[3, 21, 26\]), andhas many potential applications where propriety of informa-tion is a concern.
In our experiments with speaker iden-tification, a set of CI phone models were built for eachspeaker, by supervised adaptation of SI models\[l 1\], andthe unknown speech was recognized by all of the speakersmodels in parallel) Speaker-identification experiments wereperformed using BREF for French and TIMIT for English.TIMIT has recently been used in a few studies on speakeridentification\[l, 20, 27, 14\] with high speaker identificationrates reported using subsets of 100 to all 462 speakers.For the experiments with TIMIT, a speaker-independentset of 40 CI models were built using data from all of the 462training speakers with 8kHz Mel frequency-based cepstralcoefficients and their first order differences.
31-phone modelsets were then adapted to each of the 168 test speakers using8 sentences (2 SA, 3 SX, and 3 SI) for adaptation.
We chosethis set for identification test so as to evaluate the performancefor speakers not in the original SI training material, whichgreatly simplifies the enrollment procedure for new speakers.A reduced number of phones was used so as to minimizesubtle distinctions, and to reduce the number of models tobe adapted.
The remaining 2 SX sentences for each speakerwere reserved for the identification test.
While the original CImodels had a maximum of 32 Gaussians, the adapted modelswere limited to 4 mixture components, ince the amount ofadaptation data was relatively limited.5Using HMM for speaker recognition has been previously proposed, see\[26\] for a review, and also \[24, 25\].The unknown speech was recognized by all of the speakersmodels in parallel by building one large HMM.
Error ratesare shown as a function of the speech signal duration in Ta-ble 5, for text-independent speaker identification.
As for sexand language identification, the initial and final silences wereadjusted to have a maximum duration of lOOms accordingto the provided time-aligned transcriptions.
Using the en-tire utterance the identification accuracy is 98.5%.
With2.5s of speech the speaker identification accuracy is 98.3%.For the small number of sentences longer than 3s, speakeridentification was correct, suggesting that with longer sen-tences performance will improve.
This is also supported bythe result hat speaker-identification using both sentences foridentification was 100%.Duration 0.5s 1.0s 1.5s 2.0s 2.5s EOSTIMIT 36.9 19.6 7.8 3.9 1.7 1.5BREF 33.8 13.1 7.8 3.3 2.6 0.8Table 5: Text-independent speaker identification error rate as a function ofduration for 168 test speakers of TIMIT, and 65 speakers from BREE (EOSis End Of Sentence identification error ate.
)For French, the acoustic seed models were 35 SI CI mod-els, built using data from 57 BREF training speakers, exclud-ing 10 sentences to be used for adaptation and test.
In orderto have a similar situation to English, these models wereadapted to each of 65 speakers (including 8 new speakersnot used in training) using only 8 sentences for adaptation,and reserving 2 sentences for identification test.
Using onlyone sentence per speaker for identification, there is one er-ror, giving an identification accuracy of 99.2%, and when2 sentences are used all speakers are correctly identified (asobserved for TIMIT).
Speaker-identification results are givenin Table 5 for 65 speakers (27m/38f) as a function of signalduration.
It can be noted that the identification accuraciesas a function of time are similar for both corpora.
However,since BREF sentences are somewhat longer than TIMIT sen-tences, the overall identification error rate per sentence islower for BREF (EOS), even though the error for BREF at2.5s is greater.
For both TIMIT and BREF, when there wasa confusion, the speaker was always identified by anotherspeaker of the same sex.Experiments for text-dependent speaker identification us-ing exactly the same models and test sentences were per-formed.
For both TIMIT and BREF a performance degrada-tion was observed (on the order of 4% using the accuracy atthe end of the sentence.)
These results were contrary to ourexpectations, in that typically text-dependent speaker verifi-cation is considered to outperform text-independent\[3, 19\].An experiment was also performed in which speaker-adapted models were built for each of the 168 test speakersfrom TIMIT without knowledge of the phonetic transcrip-tion, using the same 8 sentences for adaptation.
Performingtext-independent speaker identification as before on the re-maining 2 sentences give the results hown in Table 6.
As be-100fore if both sentences are used for identification, the speakeridentification accuracy is 100%.
This experimental resultindicates that the time consuming step of providing phonetictranscriptions is not needed for accuracte text-independentspeaker identification.DurationlO5s 1.Os 16.~s\[2.0s\]2.5s EOS TIMIT 37.5 21.2 .
4.0 2.1 1.5Table 6: Text-independent speaker identification error rate as a functionof duration for 168 test speakers of TIMIT with unsupervised adaptation.
(EOS is End Of Sentence identification error rate.
)SUMMARYIn this paper we have reported on recent work on the iden-tification of non-linguistic speech features from recorded sig-nals using phone-based acoustic likelihoods.
The inclusionof this technique in speech-based systems, can broaden thescope of applications of speech technologies, and lead tomore user-friendly systems.The approach is based on training a set of large phone-based ergodic HMMs for each non-linguistic feature to beidentified (language, gender, speaker, ...), and identifying thefeature as that associated with the model having the high-est acoustic likelihood of the set.
The decoding procedureis efficiently implemented by processing all the models inparallel using a time-synchronous beam search strategy.This has been shown to be a powerful technique for sex-,language-, and speaker-identification, and has other possibleapplications uch as for dialect identification (including for-eign accents), or identification of speech disfluencies.
Sex-identification for BREF and WSJ was error-free, and 99%accurate for TIMIT with 2s of speech.
With 2s of speechthe language is correctly identified as English or French withover 99% accuracy.
Speaker identification accuracies of98.5% on TIMIT (168 speakers) and 99.1% on BREF (65speakers) were obtained with one utterance per speaker, and100% if 2 utterances were used for identification.
The sameidentification accuracy was obtained on the 168 speakers ofTIMIT using unsupervised adaptation, verifying that it isnot necessary to provide phonetic transcription for accuratespeaker identification.
Being independent of the spoken text,and requiring only a small amount of speech (on the orderof 2.5s), this technique is promising for a variety of appli-cations, particularly those for which continual verification ispreferable.In conclusion, we propose a unified approach to identify-ing non-linguistic speech features from the recorded signalusing phone-based acoustic likelihoods.
This technique hasbeen shown to be effective for language, sex, and speakeridentification and can enable better and more friendly humanmachine interaction.REFERENCES\[1\] Y. Bennani, "Speaker Identification through a Modular ConnectionistArchitecture: Evaluation on the TIMIT Database," ICSLP-92.\[2\] R. Carr6, R. Descout, M. Esk6nazi, J. Mariani, M. Rossi, "TheFrench language database: defining, planning, and recording a largedatabase," ICASSP-84.\[3\] G.R.
Doddington,"SpeakerRecognitinn- Identifying People by theirVoices," Proc.
IEEE, 73,(1 I), Nov. 1985.\[4\] J.S.
Garofolo, L.E Lamel, W.M.
Fisher, J.G.
Fiscus, D.S.
Pallett,N.L.
Dahlgren, "The DARPA TIMIT Acoustic-Phonetic ContinuousSpeech Corpus CDROM" NTIS order number PB91-100354.\[5\] J.L.
Gauvain, L.E Lamel, "Speaker-Independent Phone RecognitionUsing BREE" DARPA Speech & Nat.
Lang.
Workshop, Feb. 1992.\[6\] J.L.
Gauvain, L.E Lamel, G. Adda, "LIMSI Nov92 WSJ Evaluation,"presented at the DARPA Spoken Language Systems Technology Work-shop, MIT, Cambridge, MA, Jan., 1993.\[7\] J.L.
Ganvain, L.E Lamel, G. Adda, J. Mariani, "Speech-to-Text Con-version in French," to appear in Int.
J. Pat.
Rec.
& AJ., 1993.\[8\] J.L.
Ganvain, L.E Lamel, M. Esk6nazi, "Design considerations & textselection for BREF, a large French read-speech corpus," ICSLP-90.\[9\] J.L.
Gauvain, C.H.
Lee, "Bayesian Leaming of Gaussian MixtureDensities for Hidden Markov Models," DARPA Speech &Nat.
Lang.Workshop, Feb. 1991.\[10\] J.L.
Gauvain, C.H.
Lee, "MAP Estimation of Continuous DensityHMM: Theory and Applications,"DARPA Speech & Nat.
Lang.
Work-shop, Feb. 1992.\[11\] J.L.
Gauvain, C.H.
Lee, "Bayesian Leaming for Hidden MarkovModel with Ganssian Mixture State Observation Densities," SpeechCommunication, 11(2-3), 1992.\[12\] A.S. House, E.E Neuburg,"Toward automatic identification ofthe lan-guage of an utterance.
I.
Preliminary methodological considerations,"JASA, 62(3).\[13\] L.E Lamel, J.L.
Gauvain, "Continuous Speech Recognition at LIMSI,"DARPA Speech & Nat.
Lang.
Workshop, Sep. 1992.\[14\] L.E Lamel, J.L Ganvain, "Cross-Lingual Experiments with PhoneRecognition," ICASSP-93.\[15\] L.E Lamel, J.L.
Gauvain, M. Esk6nazi, "BREE a Large VocabularySpoken Corpus for French," EUROSPEECH-91.\[16\] C.H.
Lee, C.H.
Lin, B.H.
Juang, "A Study on Speaker Adaptation ofthe Parameters of Continuous Density Hidden Markov Models,"IEEETrans.
on ASSP, April 1991.\[17\] C.H.
Lee, L.R.
Rabiner, R. Pieraccini, J.G.
Wilpon, "Acoustic mod-eling for large vocabulary speech recognition," Computer Speech &Language, 4, 1990.\[18\] K.E Lee, H.W.
Hon, "Speaker-lndependentPhone Recognition UsingHidden Markov MOdels,"IEEE Trans.
ASSP, 37(11), 1989.\[19\] T. Matsui, S. Fumi, "Speaker Recognition using ConcatenatedPhoneme Models," ICSLP-92.\[20\] C. Montaci6, J.L Le Floch, "AR-Vector Models for Free-Text SpeakerRecognition," ICSLP- 92.\[21\] J.M.
Naik, "SpeakerVerification: A Tntorial,'IEEE CommunicationsMagazine, 28(1 ), 1990.\[22\] D. Paul, J. Baker, "The Design for the Wall Street Joumal-based CSRCorpus" DARPA Speech & Nat.
Lang.
Workshop, Feb. 1992.\[23\] L.R.
Rabiner, B.H.
Juang, S.E.
Levinson, M.M.
Sondhi, "Recognitionof Isolated Digits Using Hidden Markov Models with ContinuousMixture Densities," AT&T Technical Journal, 64(6), 1985.\[24\] R.C.
Rose and D.A.
Reynolds, "Text Independent Speaker Identifica-tion using Automatic Acoustic Segmentation," ICASSP.90.\[25\] A.E.
Rosenberg, C.H.
Lee, EK.
Soong, "Sub-Word Unit Talker Veri-fication Using Hidden Markov Models," ICASSP-90.\[26\] A.E.
Rosenberg, EK.
Soong, "Recent Research in Automatic SpeakerRecognition," inAdvances in Speech Signal Processing, (Eds.
Fumi,Sondhi), Marcel Dekker, NY, 1992.\[27\] M. Savic, J. Sorenson, "Phoneme Based Speaker Verification,"ICASSP-92.\[28\] B.L.
Tseng, EK.
Soong, A.E.
Rosenberg, "Continuous ProbabilisticAcoustic MAP for Speaker Recognition," ICASSP-92.101
