Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 68?75,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsMulti-slot semantics for natural-language call routing systemsJohan Boye and Mats Wir?nTeliaSonera R&DVitsandsgatan 9SE-123 86 Farsta, Swedenjohan.boye@teliasonera.com, mats.wiren@teliasonera.comAbstractStatistical classification techniques fornatural-language call routing systemshave matured to the point where it is pos-sible to distinguish between several hun-dreds of semantic categories with anaccuracy that is sufficient for commercialdeployments.
For category sets of thissize, the problem of maintaining consis-tency among manually tagged utterancesbecomes limiting, as lack of consistencyin the training data will degrade perform-ance of the classifier.
It is thus essentialthat the set of categories be structured in away that alleviates this problem, and en-ables consistency to be preserved as thedomain keeps changing.
In this paper, wedescribe our experiences of using a two-level multi-slot semantics as a way ofmeeting this problem.
Furthermore, weexplore the ramifications of the approachwith respect to classification, evaluationand dialogue design for call routing sys-tems.1 IntroductionCall routing is the task of directing callers to a ser-vice agent or a self-service that can provide therequired assistance.
To this end, touch-tone menusare used in many call centers, but such menus arenotoriously difficult to navigate if the number ofdestinations is large, resulting in many misdirectedcalls and frustrated customers.
Natural-languagecall routing provides an approach to come to termswith these problems.
The caller gets the opportu-nity to express her reasons for calling using herown words, whereupon the caller?s utterance isautomatically categorized and routed.This paper focuses on experiences obtainedfrom the deployment of a call-routing applicationdeveloped for the TeliaSonera residential customercare.1 The application was launched in 2006, re-placing a previous system based on touch-tonemenus.
The customer care annually handles some14 million requests and questions concerning awide range of products in fixed telephony, mobiletelephony, modem-connected Internet, broadband,IP telephony and digital TV.The crucial step in any call routing application isclassification, that is, the mapping of natural-language utterances to categories that correspondto routing destinations.
Early systems used quitesmall numbers of categories.
For example, theoriginal ?How May I Help You?
system had 15categories (Gorin et al 1997), the system of Chu-Carroll and Carpenter (1999) had 23 categories,and Cox and Shahshahani (2001) had 32.
Nowa-days, it is possible to distinguish between severalhundreds of categories with high accuracy (see, forexample, Speech Technology Magazine 2004).The TeliaSonera system currently distinguishesbetween 123 categories with an accuracy of 85%(using a speech recognizer and classifier developedby Nuance2).
Moreover, according to our experi-ments the same classification technology can be1TeliaSonera (www.teliasonera.com) is the largest telecom operator in theNordic?Baltic region in Europe.2www.nuance.com.68used to distinguish between 1,500 categories with80% accuracy.3For large category sets like these, the problem ofmaintaining consistency among manually taggedutterances becomes limiting, as lack of consistencyin the training data will degrade performance of theclassifier.
The problem is exacerbated by the factthat call-routing domains are always in a state offlux: Self-services are being added, removed,modified, split and merged.
Organizationalchanges and product development regularly call forredefinitions of human expertise areas.
All of thesechanges must be accommodated in the categoryset.
Hence, it must be possible to update this setefficiently and at short intervals.To meet this problem, it is crucial that the set ofcategories be structured in a way that facilitates thetask of manual tagging and enables consistency tobe preserved.
However, in spite of the fact that thesize of category sets for call routing have increaseddramatically since the original ?How May I HelpYou?
system, we are not aware of any papers thatsystematically discuss how such large sets shouldbe structured in order to be efficiently maintain-able.
Rather, many papers in the call-routing litera-ture consider the call routing problem as anabstract classification task with atomic categoriesat a single level of abstraction.
Such atomic cate-gories are typically taken to correspond to depart-ments and self-services of the organization towhich the call center belongs.
In a real-life imple-mentation, the situation is often more complicated.At TeliaSonera, we have adopted a two-levelmulti-slot semantics as a way of maintainingmodularity and consistency of a large set of cate-gories over time.The aim of this paper is to share our experiencesof this by providing a detailed description of theapproach and its implications for classification,dialogue design and evaluation.
The rest of the pa-per is organized as follows: Section 2 describes themulti-slot category system.
Sections 3?5 outlineconsequences of the multi-slot semantics for dis-ambiguation, classification and evaluation, respec-tively.
Section 6 concludes.3In both cases, the classifier was trained on 60,000 utterances.2 What?s in a category?2.1 MotivationAs pointed out above, call-routing domains arealways to some extent moving targets because ofconstant changes with respect to products and or-ganization.
It would be cumbersome to manuallyre-tag old data each time the category set is up-dated.
Retagging the training data for the statisticalclassifier might introduce inconsistencies into thetraining set and degrade classifier performance.Thus, it is a good idea to define two sets of catego-ries at different levels; one set of semantic catego-ries reflecting the contents of the utterance, andone set of application categories reflecting how thecall should be handled.
These two sets of catego-ries are related by means of a many-to-one map-ping from the semantic domain to the applicationdomain.
Figure 1 gives the general picture.Figure 1: Mapping between semantic categories andapplication categories.The utterances in the training set for the auto-matic classifier are manually categorized usingsemantic categories.
The automatic classifier canbe trained to work either in the semantic domain orin the application domain (see further Section 4).Semantic categories Application categories692.2 Semantic categoriesIn the TeliaSonera system, semantic categories aretriples of the form( family, intention, object )where family is the general product family whichthe call concerns (e.g.
fixed telephony, mobile te-lephony, broadband, etc.
), intention represents thenature of the request (e.g.
order, want-info,change-info, activate, want-support, report-error,etc.
), and object represents more specifically whatthe call is about (e.g.
particular names of products,or concepts like ?telephone number?, ?SIM card?,or ?password?).
Currently there are 10 families,about 30 intentions, and about 170 objects thatspan the semantic domain.Some (in fact, the majority) of the possible tri-ples are disallowed because they are nonsensical.For instance, it is not meaningful to combine?fixed telephony?
in the family slot with ?SIMcard?
in the object slot.
To cater for this, we havedefined a set of combination rules weeding out theillegal combinations of values.
These rules disal-low about 80% of the possible combinations, leav-ing about 10,000 permissible semantic triples.
Ofthese 10,000 triples, about 1,500 have actuallyturned up in real data.The three-slot structure of categories is very use-ful when performing manual tagging of the train-ing material for the statistical classifier.
Althoughthere are 10,000 categories, the person performingthe tagging needs only to keep track of about 210concepts (10 families + 30 intentions + 170 ob-jects).
In contrast, it is safe to say that an unstruc-tured category system containing 10,000 atomiccategories would be quite impractical to use.In addition, the combination rules can further al-leviate the manual tagging task.
It is straightfor-ward to implement a tagging tool that allows thehuman tagger to select a value for one semanticslot, and then restrict the selection for the otherslots only to include the possible values.
For ex-ample, if ?fixed telephony?
is chosen for the familyslot, ?SIM card?
would not appear among the pos-sible values for the object slot.
This approach hasbeen successfully adopted in the project.2.3 Application categoriesThere is one application category for each type ofaction from the system.
Actions come in two fla-vors; either the call is routed (in the cases wherethe caller has given sufficient information), or thesystem asks a counter-question in order to extractmore information from the caller.
That is, applica-tion categories can be labeled either as routingcategories or disambiguation categories.
For con-venience, names of application categories are alsotriples, chosen among the set of semantic triplesthat map to that application category.2.4 Information orderingEach slot in a semantic triple can take the valueunknown, representing the absence of information.For instance, the most accurate semantic categoryfor the caller utterance ?Broadband?4 is (broad-band, unknown, unknown), since nothing is knownabout the intention of the caller or the specifictopic of the request.
Thus, in the information order-ing, ?unknown?
is situated below all other values.There are also some intermediate values in theinformation ordering.
The value telephony repre-sents ?either fixed telephony or mobile telephony?,and has been incorporated in the category set sincemany callers tend not be explicit about this point.In the same vein, internet represents ?either broad-band or modem-connected internet?, and billingrepresents the disjunction of a whole range of bill-ing objects, some of which can be handled by aself-service and some can not.Figure 2: Parts of the semantic information ordering.The information ordering extends naturally totriples.
In particular, the triple (unknown, unknown,4Many callers express themselves in this telegraphic fashion.unknowntelephony internetmodemConnected broadband fixed mobile70unknown) represents complete absence of informa-tion.3 DisambiguationThe caller?s request might be ambiguous in onesense or another, in which case the system willneed to perform disambiguation by asking a fol-low-up question.
This might either be a generalquestion encouraging the user to describe his re-quest in greater detail, or a directed question of thetype ?Would that be fixed telephony or mobile te-lephony?
?Ambiguous utterances might be represented inat least two fundamentally different ways.
In vec-tor-based approaches, routing destinations and in-put utterances alike are represented by vectors in amulti-dimensional space.
An input utterance isrouted to a specific destination if the vector repre-sentation of the utterance is close to that of the des-tination.
An ambiguous utterance is characterizedby the fact that the Euclidean distances from theutterance vector to the n closest routing destinationvectors are roughly the same.Chu-Carroll and Carpenter (1999) describe amethod of disambiguation, where disambiguationquestions are dynamically constructed on the basisof an analysis of the differences among the closestrouting destination vectors.
However, it is not clearthat the disambiguation questions produced bytheir proposed method would make sense in allpossible situations.
Furthermore, their method doesnot take into account the fact that some ambiguitiestend to be more important and arise more oftenthan others.
We think it is worthwhile to concen-trate on these important cases (in terms of promptdesign, speech recognition grammar construction,etc.
), rather than trying to solve every conceivableambiguity, most of which would never appear inreal life.As previously mentioned, in the TeliaSonerasystem we have chosen another way of treatingambiguities, namely that certain application cate-gories are disambiguation categories; they repre-sent foreseen, frequently occurring, ambiguousinput utterances.
The three-slot structure of catego-ries provides a handy way of identifying ambigu-ous cases; they are represented by triples whereone or more slots are unknown, or where some slothas an intermediate value, like telephony or inter-net.
Examples of such ambiguous utterances are?broadband?
(broadband-unknown-unknown) and?I want to have a telephone subscription?
(teleph-ony-order-subscription).
All categories that repre-sent ambiguities have pre-prepared disambiguationquestions, speech recognition grammars, and dia-logue logic to handle the replies from the callers.Of course, there are still problematic caseswhere an utterance can not be assigned any uniquecategory with any tolerable level of confidence,neither a routing category nor a disambiguationcategory.
In those cases, the system simply re-phrases the question: ?Sorry, I didn?t quite under-stand that.
Could you please rephrase?
?4 Classification4.1 Atomic vs. multi-slot classificationFor the purpose of automatic classification of ut-terances, there are at least two different views onemay adopt.
In one view, the ?atomic?
view, thethree-slot structure of category names is consideredas merely a linguistic convention, convenient onlywhen manually tagging utterances (as discussed inSection 2.1).
When adopting this view, we stillregard the categories to be distinct atomic entitiesas concerns automatic classification.
For instance,to the human eye it is obvious that two categorieslike (internet, order, subscription) and (broadband,order, subscription) are related, but the automaticclassifier just considers them to be any two catego-ries, each with its separate set of training examples.An alternative view, the ?multi-slot view?, is tosee the category as actually consisting of threeslots, each of which should be assigned a valueindependently.
This means that a separate classifieris needed for each of the three slots.It is not clear which view is preferable.
An ar-gument in favor of the multi-slot view is the fol-lowing: If some categories have the same value inone slot, then these categories are semanticallyrelated in some way.
Most likely this semantic re-lation is reflected by the use of common words andphrases; for instance, expressions like ?order?
and?get a new?
presumably are indicative for all cate-gories having the value order in the intention slot.Therefore, classifying each slot separately wouldbe a way to take a priori semantic knowledge intoaccount.To this, proponents of the atomic view may re-spond that such similarities between categories71would emerge anyway when using a single classi-fier that decides the entire semantic triple in one go(provided that enough training data is available).
Inaddition, if each slot is categorized separately, it isnot certain that the resulting three values wouldconstitute a permissible semantic triple (as men-tioned in Section 2.1, about 80% of the possiblecombinations are illegal).
In contrast, if a singleclassifier is used, the result will always be a legaltriple, since only legal triples appear in the trainingmaterial.The statistical classifier actually used in the livecall routing system treats categories as atomic enti-ties and, as mentioned in the introduction, it workswell.
The encouraging numbers bear out that the?atomic?
view is viable when lots of data is athand.
On the other hand, if training data is sparse,one might consider using a hand-written, rule-based classifier, and in these cases the multi-slotview seems more natural.4.2 Rule-based multi-slot classificationTo obtain a baseline for the performance of thestatistical classifier used in the live system, we im-plemented an alternative classifier that solves theclassification task using hand-written rules.
Thus,the purpose of this was to investigate the perform-ance of a na?ve classification method, and use thatfor comparison with other methods.
In addition,the rule-based classifier provides an example ofhow the multi-slot approach can support the inclu-sion of human a priori domain knowledge into theclassification process.The rule-based classifier has three kinds ofrules: Firstly, phrase-spotting rules associate aword or a phrase with a value for a semantic slot(i.e.
a family, an intention, or an object).
Rules ofthe second kind are domain axioms that encodeinvariant relationships, such as the fact that ob-ject=SIMcard implies family=mobileTelephony.Finally, rules of the third kind specify how seman-tic values can be combined into a legal semantictriple (these rules are also used for manual tagging,as mentioned in Section 2.1).
Each semantic valueis also (manually) given a score that reflects itsinformation content; a higher score means that thevalue contains more information.
For instance, thevalue subscription has a lower information scorethan have the names of specific subscription typesthat TeliaSonera offers its customers.The classifier works in three phases, which wewill demonstrate on a running example.
In the firstphase, it applies the phrase-spotting rules to theinput sentence, returning a list of slot-value pairs.For instance, the input sentence ?I want to order anew SIM card?
would yield the list [ inten-tion=order, object=SIMcard ], using rules trigger-ing on the phrases ?order?
and ?SIM card?
in theinput sentence.Secondly, the classifier adds semantic compo-nents as a result of applying the domain axioms tomembers of the list.
Using the domain axiom men-tioned above, the semantic component fam-ily=mobileTelephony would be added to the list,due to the presence of object=SIMcard.
Thus, afterthe two first phases, the intermediate result in thisexample is [intention=order, object=SIMcard,family=mobileTelephony].In the final phase, semantic components are se-lected from the list to form a semantic triple.
In theexample, this step is straightforward since the listcontains exactly one value for each component,and these values are combinable according to thecombination rules.
The final result is:( mobileTelephony, order, SIMcard )In cases where the semantic values in the list arenot combinable (a situation often originating froma speech recognition error), one or several valueshave got to be relaxed to unknown.
According toour experiments, the best heuristic is to first relaxthe object component and then the intention com-ponent.
For example, in the list [family = fixed-Telephony, intention=order, object=SIMcard], thefirst and third elements are not combinable; thusthis list yields the triple:( fixedTelephony, order, unknown )In the case where some slots are not filled inwith a value, the values of those slots are set tounknown.
Thus, the list [ family=fixedTelephony,intention=order ] would also yield the semantictriple above.Finally, consider the case where the input listcontains more than one value for one or severalslots.
In this case, the algorithm picks the valuewith the highest information content score.
Forinstance, consider the utterance ?I want to have abroadband subscription, this eh ADSL I?ve read72about?.
After the first two phases, the algorithmhas found family=broadband, intention=order,and two possible values for the object slot, namelyobject=subscription and object=ADSL.
Since thelatter has higher information score, the final resultis:( broadband, order, ADSL )The rule-based classifier was developed in aboutfive man-weeks, and contains some 3,000 hand-written rules.
When evaluated on a set of 2,300utterances, it classified 67% of the utterances cor-rectly.
Thus, not surprisingly, its performance issignificantly below the statistical classifier used inthe deployed system.
Still, the rule-based approachmight be a viable alternative in less complex do-mains.
It might also be usable for data collectionpurposes in early prototypes of natural-languagecall routing systems.5 Evaluation of call-routing dialogues5.1 MotivationAn important issue in the development of any dia-logue system is the selection of an evaluation met-ric to quantify performance improvements.
In thecall-routing area, there have been many technicalpapers specifically comparing the performance ofclassifiers, using standard metrics such as accuracyof the semantic categories obtained over a test cor-pus (see e.g.
Kuo and Lee, 2000, and Sarikaya etal., 2005).
Accuracy is then stated as a percentagefigure showing the degree of the categories thathave been completely correctly classified, giventhat categories are atomic.
There have also beensome design-oriented papers that try to assess theeffects of different prompt styles by looking at theproportion of routable versus unroutable callsgiven callers?
first utterances.
Thus, both of thesestrands of work base their evaluations on binarydivisions between correct/incorrect and rout-able/unroutable, respectively.
Furthermore, theyboth constitute utterance-based metrics in the sensethat they focus on the outcome of a single system?caller turn.An excellent example of a design-oriented call-routing paper is Williams and Witt (2004), whichamong other things compares open and directedprompt styles in the initial turn of the dialogue.Williams and Witt divide callers?
responses intoRoutable (if the utterance contained sufficient in-formation for the call to be routed) or Failure (ifthe utterance did not contain sufficient informationfor routing).
Depending on why a call is not rout-able, Williams and Witt further subdivide instancesof Failure into three cases: Confusion (utterancessuch as ?Hello??
and ?Is this a real person??
),Agent (the caller requests to speak to a humanagent), and Unroutable (which corresponds to ut-terances that need disambiguation).
Thus, Williamsand Witt?s performance metric uses altogether fourlabels.
(In addition, they have three labels relatedto non-speech events: silence, DTMF and hang-up.Since such events are not handled by the classifier,they fall outside of the scope of this paper.
)Although all of Williams?
and Witt?s measuresare needed in evaluating call-routing dialogue, thefield clearly needs more in-depth evaluation.
Inparticular, we need more fine-grained metrics inorder to probe more exactly to what extent Failureactually means that the dialogue is off track.
Fur-thermore, given that call-routing dialogues typi-cally consist of between one and (say) five turns,we need not just utterance-based metrics, but alsodialogue-based metrics ?
in other words, beingable to evaluate the efficiency of an overall dia-logue.5.2 Utterance-based metricsWhen assessing the performance of classificationmethods, it is perfectly reasonable to use the binarydistinction correct/incorrect if only few categoriesare used.
In such a context it can be assumed thatdifferent categories correspond to different de-partments of the organization, and that a misclassi-fication would lead the call being routed the wrongway.
However, with a richer category system, it isimportant to realize that the classifier can be par-tially correct.
For instance, if the caller expressesthat he wants technical support for his broadbandconnection, then the information that the purposeof the call has something to do with broadband issurely better than no information at all.
If the sys-tem obtains this information, it could ask a directedfollow-up question: OK broadband.
Please tell meif your call concerns an order, billing, deliveries,support, error report, or something else, or some-thing to that effect.
Otherwise, the system can onlyrestate the original question.73In the field of task-oriented dialogue, severalevaluation metrics have been put forward that gobeyond a simple division into correct/incorrect.
Inparticular, concept accuracy (Boros et al 1996) isan attempt to find a semantic analogue of wordaccuracy as used in speech recognition.
Basically,the idea is to compute the degree of correctness ofa semantic analysis based on a division of the rep-resentation into subunits, and by taking into ac-count insertions, deletions and replacements ofthese subunits.Making use of our multi-slot semantics, we cantake subunits to correspond to semantic slot values.An insertion has occurred if the classifier spuri-ously has added information to some slot value(e.g.
if the classifier outputs the value broadbandfor the family slot, when the correct value is inter-net or unknown).
Conversely, a deletion has oc-curred when semantic triple output from theclassifier contains a slot value which is situatedlower than the correct value in the information or-dering (a part of which is depicted in Figure 2).Finally, a replacement has occurred when the com-puted slot value and the correct slot value are unre-lated in the information ordering.By using concept accuracy as an evaluation met-ric for classifiers rather than the binary distinctioncorrect/incorrect, we can arrive at more informa-tive assessments.
This possibility is brought aboutby the multi-slot structure of categories.5.3 Dialogue-based metricsIn the literature, there have also been proposals fordialogue-based metrics.
In particular, Glass et al(2000) put forward two such metrics, query density(QD) and concept efficiency (CE).
Query density isthe mean number of new ?concepts?
introducedper user query, assuming that each concept corre-sponds to a slot?filler pair in the representation ofthe query.
For example, a request such as ?I?d likea flight from Stockholm to Madrid on Sunday af-ternoon?
would introduce three new concepts, cor-responding to departure, destination and time.Query density thus measures the rate at which theuser communicates content.
In contrast, conceptefficiency measures the average number of turns ittakes for a concept to be successfully understoodby the system.
Concept efficiency thus measuresthe rate at which the system understands content.Using the multi-slot semantics, we can adapt thenotions of query density and concept efficiency inorder to arrive at a more fine-grained performancemetric for call routing.
The basic idea is to regardevery element in the semantic triple as one ?con-cept?.
We can then obtain a measure of how in-formation increases in the dialogue by computingthe difference between triples in each user utter-ance, where ?difference?
means that the values oftwo corresponding elements are not equal.An example of computing query density is givenbelow.
We assume that the value of the semantictriple is initially (unknown, unknown, unknown).System: Welcome to TeliaSonera.
How may I helpyou?Caller: Fixed telephony.
(fixedTelephony, unknown, unknown)1 new conceptSystem: Could you tell me some more about whatyou want to do?Caller: I can?t use my broadband while I?m speak-ing on the phone.
(broadband, reportProb-lem, lineOrPhone)3 new conceptsNote that query density and concept efficiencyare both applicable on a per-utterance basis as wellas on the whole dialogue (or indeed arbitrarystretches of the dialogue).
To compute these meas-ures for the whole dialogue, we simply computethe mean number of new concepts introduced peruser utterance and the average number of turns ittakes for a concept to be successfully understood,respectively.The principal application of this methodology isto measure the effectiveness of system utterances.When using a fine-grained system of categories, itis important that callers express themselves at asuitable level of detail.
Too verbose user utterancesare usually difficult to analyse, but too telegraphicuser utterances are not good either, as they mostoften do not contain enough information to routethe call directly.
Therefore it is very important todesign system utterances so as to make users givesuitably expressive descriptions of their reasons forcalling.By using the query density metric it is possibleto asses the effectiveness (in the above sense) ofdifferent alternative system utterances at variouspoints in the dialogue, most notably the first sys-74tem utterance.
Again, this possibility is broughtabout by the multi-slot structure of categories.
It isalso possible to evaluate more general dialoguestrategies over longer stretches of dialogue (e.g.the use of general follow-up questions like ?Couldyou please tell me some more about what you wantto do?
as opposed to more directed questions like?Please tell me if your call concerns an order, bill-ing, deliveries, support, error report, or somethingelse?).
By calculating the average query densityover a number of consecutive utterances, it is pos-sible to compare the relative merits of differentsuch dialogue strategies.We have not yet adopted this metric for evalua-tion of dialogues from the live system.
However,elsewhere we have applied it to dialogues from theinitial Wizard-of-Oz data collection for the Telia-Sonera call routing system (Wir?n et al 2007).Here, we used it to compare two styles of disam-biguation prompts, one completely open and onemore directed.6 Concluding remarksIn the literature, the natural-language call routingproblem is often presented as the problem of clas-sifying spoken utterances according to a set ofatomic categories.
The hypothesis underlying thispaper is that this view is inadequate, and that thereis a need for a more structured semantics.
We baseour claims on experiences gathered from the de-velopment and deployment of the TeliaSonera callcenter, for which we developed a multi-slot systemof categories.A multi-slot semantics offers several advan-tages.
First of all, it makes the set of categoriesmanageable for human taggers, and provides ameans to break down the tagging task into sub-tasks.
Furthermore, we have shown how multi-slotsemantics for call-routing systems allows straight-forward division of categories into routing catego-ries and disambiguation categories, the possibilityof multi-slot categorization, and the use of morefine-grained evaluation metrics like concept accu-racy and query density.AcknowledgementsThis work has benefited greatly from discussionson category systems and classification with MarcoPetroni, Linda Brostr?m, Per-Olof G?llstedt, AlfBergstrand and Erik Demmelmaier, and we thankthem all.
We would also like to thank RobertSandberg and Erik N?slund for their support of thiswork.ReferencesBoros, M., Eckert, W., Gallwitz, F., G?rz, G., Han-rieder, G. and Niemann, H. (1996).
Towards under-standing spontaneous speech: Word accuracy vs.concept accuracy.
Proc.
Fourth International Con-ference on Spoken Language Processing (ICSLP),pp.
1009?1012.Chu-Carroll, J. and Carpenter, B.
(1999) Vector-basednatural language call routing.
Computational linguis-tics, 25(3), pp.
361-388.Cox, S. and Shahshahani, B.
(2001).
A comparison ofsome different techniques for vector based call-routing.
Proc.
Eurospeech, Aalborg, Denmark.Glass, J., Polifroni, J., Seneff, S. and Zue, V. Data col-lection and performance evaluation of spoken dia-logue systems: The MIT experience.
In Proc.
SixthInternational Conference on Spoken Language Proc-essing (ICSLP), Beijing, China.Gorin, A., Riccardi, G., and Wright, J.
(1997) How mayI help you?.
Journal of Speech Communication, 23,pp.
113-127.Kuo, H-K J. and Lee, C-H. (2000)  Discriminative train-ing in natural language call routing.
Proc.
Sixth In-ternational Conference on Spoken LanguageProcessing (ICSLP), Beijing, China.Sarikaya, R, Kuo, H-K J., Goel, V. and Gao, Y.
(2005)Exploiting unlabeled data using multiple classifiersfor improved natural language call-routing.
Proc.
In-terspeech, Lisbon, Portugal.Speech Technology Magazine (2004) Q&A with BellCanada?s Belinda Banks, senior associate director,customer care.
Speech Technology Magazine, vol 9,no 3.Williams, Jason D. and Witt, Silke M. (2004).
A com-parison of dialog strategies for call routing.
Interna-tional Journal of Speech Technology 7(1), pp.
9?24.Wir?n, M., Eklund, R., Engberg, F. and Westermark, J.(2007).
Experiences of an in-service Wizard-of-Ozdata collection for the deployment of a call-routingapplication.
Proc.
Bridging the gap: Academic andindustrial research in dialog technology.
NAACLworkshop, Rochester, New York, USA.75
