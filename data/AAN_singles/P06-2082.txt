Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 635?642,Sydney, July 2006. c?2006 Association for Computational LinguisticsAnalysis of Selective Strategies to Build a Dependency-Analyzed CorpusKiyonori OhtakeNational Institute of Information and Communications Technology (NICT),ATR Spoken Language Communication Research Labs.2-2-2 Hikaridai ?Keihanna Science City?
Kyoto 619-0288 Japankiyonori.ohtake [at] nict.go.jpAbstractThis paper discusses sampling strategiesfor building a dependency-analyzed cor-pus and analyzes them with different kindsof corpora.
We used the Kyoto TextCorpus, a dependency-analyzed corpus ofnewspaper articles, and prepared the IPALcorpus, a dependency-analyzed corpus ofexample sentences in dictionaries, as anew and different kind of corpus.
The ex-perimental results revealed that the lengthof the test set controlled the accuracy andthat the longest-first strategy was goodfor an expanding corpus, but this was notthe case when constructing a corpus fromscratch.1 IntroductionDependency-structure analysis plays a very impor-tant role in natural language processing (NLP).Thus, so far, much research has been done onthis subject, with many analyzers being developedsuch as rule-based analyzers and corpus-basedanalyzers that use machine-learning techniques.However, the maximum accuracy achieved bystate-of-the art analyzers is almost 90% for news-paper articles; it seems very difficult to exceed thisfigure of 90%.
To improve our analyzers, we haveto write more rules for rule-based analyzers or pre-pare more corpora for corpus-based analyzers.If we take a machine-learning approach, itis important to consider what features are used.However, there are several machine-learning tech-niques, such as support vector machines (SVMs)with a kernel function, that have strong general-ization ability and are very robust for choosing theright features.
If we use such machine-learningtechniques, we will be free from choosing a fea-ture set because it will be possible to use all pos-sible features with little or no decline in perfor-mance.
Actually, Sasano tried to expand the fea-ture set for a Japanese dependency analyzer usingSVMs in (Sasano, 2004), with a small improve-ment in accuracy.To write rules for a rule-based analyzer, and toproduce an analyzer using machine-learning tech-niques, it is crucial to construct a dependency-analyzed corpus.
Such a corpus is very useful notonly for constructing a dependency analyzer butalso for other natural language processing appli-cations.
However, building this kind of resourceis very expensive and labor-intensive because it isdifficult to annotate a large amount of dependency-analyzed corpus in short time.At present, one promising approach to mitigat-ing the annotation bottleneck problem is to useselective sampling, a variant of active learning(Cohn et al, 1994; Fujii et al, 1998; Hwa, 2004).In general, selective sampling is an interactivelearning method in which the machine takes theinitiative in selecting unlabeled data for the humanto annotate.
Under this framework, the system hasaccess to a large pool of unlabeled data, and it hasto predict how much it can learn from each candi-date in the pool if that candidate is labeled.Most of the experiments that had been carriedout in the previous works for selective samplingused an annotated corpus in a limited domain.
Themost typical corpus is WSJ of Penn Treebank.
Thereason why the domain was so limited is very sim-ple; corpus annotation is very expensive.
How-ever, we want to know the effects of selective sam-pling for corpora in various domains because a de-pendency analyzer constructed from a corpus doesnot always analyze a text in limited domain.635On the other hand, there is no clear guide-line nor development strategy for constructing adependency-analyzed corpus to produce a highlyaccurate dependency analyzer.
Thus in this paper,we discuss fundamental sampling strategies fora dependency-analyzed corpus for corpus-baseddependency analyzers with several types of cor-pora.
This paper unveils the essential characteris-tics of basic sampling strategies for a dependency-analyzed corpus.2 Dependency-Analyzed CorporaWe use two dependency-analyzed corpora.
One isthe Kyoto Text Corpus, which consists of news-paper articles, and the other one is the IPAL cor-pus, which contains sentences extracted from the?example of use?
section of the enties in severaldictionaries for computers.
The IPAL corpus wasrecently annotated for this study as a different kindof corpus.2.1 Kyoto Text CorpusIn this study we use Kyoto Text Corpus version3.0.
The corpus consists of newspaper articlesfrom Mainichi Newspapers from January 1st toJanuary 17th, 1995 (almost 20,000 sentences) andall editorials of the year 1995 (almost 20,000 sen-tences).
All of the articles were analyzed by mor-phological analyzer JUMAN and dependency an-alyzer KNP1.
After that, the analyzed results weremanually corrected.
Kyoto Text Corpus version4.0 is now available, holding on additional 5,000annotated sentences in the corpus to version 3.0for case relations, anaphoric relations, omissioninformation and co-reference information2 .The original POS system used in the KyotoText Corpus is JUMAN?s POS system.
We con-verted the POS system used in the Kyoto Text Cor-pus into ChaSen?s POS system because we usedChaSen, a Japanese morphological analyzer, andCaboCha3 (Kudo and Matsumoto, 2002), a depen-dency analyzer incorporating SVMs, as a state-of-the art corpus-based Japanese dependency struc-ture analyzer that prefers ChaSen?s POS system tothat of JUMAN.
In addition, we modified some1http://www.kc.t.u-tokyo.ac.jp/nl-resource2http://www.kc.t.u-tokyo.ac.jp/nl-resource/corpus.html3http://chasen.org/?taku/software/cabocha/bunsetu segmentations because there were severalinconsistencies in bunsetu segmentation.Table 1 shows the details of the Kyoto Text Cor-pus.Kyoto Text Corpus(General) (Editorial)# of sentences 19,669 18,714# of bunsetu 192,154 171,461# of morphemes 542,334 480,005vocabulary size 29,542 17,730bunsetu / sentence 9.769 9.162Table 1: Kyoto Text Corpus2.2 IPAL corpusIPAL (IPA, Information-technology PromotionAgency, Lexicon of the Japanese language forcomputers) dictionaries consist of three dictionar-ies, the IPAL noun dictionary, the IPAL verb dic-tionary and the IPAL adjective dictionary.
Each ofthe dictionaries includes example sentences.
Weextracted 7,720 sentences from IPAL Noun, 5,244sentences from IPAL Verb, and 2,366 sentencesfrom IPAL Adjective.
We analyzed them usingCaboCha and manually corrected the errors.
Wenamed this dependency-analyzed corpus the IPALcorpus.
Table 2 presents the details of the IPALcorpus.
One characteristic of the IPAL corpus isthat the average sentence length is very short; inother words, the sentences in the IPAL corpus arevery simple.# of sentences 15,330# of bunsetu 67,170# of morphemes 156,131vocabulary size 11,895bunsetu / sentence 4.382Table 2: IPAL corpus3 ExperimentsWe carried out several experiments to determinethe basic characteristics of several selective strate-gies for a Japanese dependency-analyzed corpus.First, we briefly introduce Japanese dependencystructure.
Second, we carry out basic experimentswith our dependency-analyzed corpora and ana-lyze the errors.
Finally, we conduct simulations to636ascertain the fundamental characteristics of thesestrategies.3.1 Japanese dependency structureThe Japanese dependency structure is usually de-fined in terms of the relationship between phrasalunits called bunsetu segments.
Conventionalmethods of dependency analysis have assumed thefollowing three syntactic constraints (Kurohashiand Nagao, 1994a):1.
All dependencies are directed from left toright.2.
Dependencies do not cross each other.3.
Each bunsetu segment, except the last one,depends on only one bunsetu segment.Figure 1 shows examples of Japanese dependencystructure.Jack-wa Kim-ni hon-o okuttaJack to Kim a book presented(Jack presented a thick book to Kim.
)atsuithickKim-wa Jack-ga kureta hon-o nakushitaKim losta bookJack(Kim lost the book Jack gave her.
)gaveFigure 1: Examples of Japanese dependency struc-tureIn this paper, we refer to the beginning of a de-pendency direction as a ?modifier?
and the end ofthat as a ?head.
?3.2 Analyzing errorsWe performed a cross-validation test with ourdependency-analyzed corpora by using the SVM-based dependency analyzer CaboCha.
The featureset used for SVM in CaboCha followed the defaultsettings of CaboCha.First, we arbitrarily divided each corpus intotwo parts.
General articles of the Kyoto Text Cor-pus were arbitrarily divided into KG0 and KG1,while editorials were also divided into ED0 andED1.
The IPAL corpus was arbitrarily divided intoIPAL0 and IPAL1.
Second, we carried out cross-validation tests on these divided corpora.Table 3 shows the results of the cross-validationtests.
We employed a polynomial kernel for theSVM of CaboCha, and tested with second- andthird-degree polynomial kernels.
The input datafor each test were correct for morphological anal-ysis and bunsetu segmentation, though in practicalsituations we have to expect some morphologicalanalysis errors and bunsetu mis-segmentations.In Table 3 ?Learning?
indicates the learning cor-pus, ?Test?
represents the test corpus, and ?De-gree?
denotes the degree of the polynomial func-tion.
In addition, ?Acc.?
indicates the accuracyof dependency-analyzed results and ?S-acc.?
in-dicates the sentence accuracy that is the ratio ofsentences that were analyzed without errors.Learning Test Degree Acc.
(%) S-acc.
(%)KG0 KG0 2 94.06 65.51KG0 KG0 3 99.96 99.71KG0 KG1 2 89.50 50.35KG0 KG1 3 89.23 49.33KG1 KG0 2 89.60 49.89KG1 KG0 3 89.21 49.05ED0 ED1 2 90.77 55.58ED1 ED0 2 90.52 54.62IPAL0 IPAL1 2 97.43 92.25IPAL1 IPAL0 2 97.69 93.06KG0 IPAL0 2 97.76 93.15ED0 IPAL0 2 97.56 92.81Table 3: Results of cross-validation testsTable 3 also shows the biased evaluation (closedtest; the test was the training set itself) results.
Inthe cross-validation results of KG0 and KG1, theaverage accuracy of the second-degree kernel was89.55 (154,455 / 172,485)% and the average sen-tence accuracy was 50.12 (9,858 / 19,669)%.
Inother words, there were 18,030 dependency errorsin the cross validation test.
We analyzed these er-rors.Against the average length (9.769) of the cor-pus shown in Table 1, the average length of thesentences with errors in the cross-validation test is12.53 (bunsetu / sentence).
These results confirmthat longer sentences tend to be analyzed incor-rectly.Next we analyzed modifier bunsetu that weremis-analyzed.
Table 4 shows the top ten POS se-quences that consisted of modifier mis-analyzedbunsetu.We also analyzed the distance between modi-fier bunsetu and head bunsetu of the mis-analyzeddependencies.
Table 5 shows top ten cases ofthe distance.
In Table 5 ?Err.?
indicates the dis-tance between a modifier and a head bunsetu ofmis-analyzed dependencies, ?Correct?
indicates637POS sequence Frequencynoun, case marker 835verb, comma 576noun, topic marker 444adverbial noun, comma 370verb 336number, numeral classifier, comma 318noun, adnominal particle 304adverb 304verb, verbal auxiliary 281verb, conjunctive particle, comma 265Table 4: Modifier POS sequences of mis-analyzeddependencies and their frequencies in the cross-validation test (top 10)the distance between a modifier and a correct(should modify) head bunsetu in each case of mis-analyzed dependencies, and ?Freq.?
denotes theirfrequency.Err.
Correct Freq.
Err.
Correct Freq.1 2 3,117 2 4 4782 1 1,362 3 2 4363 1 919 4 1 4341 3 863 4 2 3792 3 482 1 4 329Table 5: Frequencies of dependency distances aterror and correct cases in the cross-validation test(top 10)3.3 Selective sampling simulationIn this section, we discuss selective strategiesthrough two simulations.
One is expanding adependency-analyzed corpus to construct a moreaccurate dependency analyzer, and the other is aninitial situation just beginning to build a corpus.3.3.1 Expanding situationThe situation is as follows.
First, the corpus,Kyoto Text Corpus KG1, is given.
Second, we ex-pand the corpus using the editorials component ofthe Kyoto Text Corpus.
Then we consider the fol-lowing six strategies: (1) Longest first, (2) Max-imizing vocabulary size first, (3) Maximizing un-seen dependencies first, (4) Maximizing averagedistance of dependencies first, (5) Chronologicalorder, and (6) Random.We briefly introduce these six strategies as fol-lows:1.
Longest first (Long)Since longer sentences tend to have com-plex structures and be analyzed incorrectly,we prepare the corpus in descending order oflength.
The length is measured by the num-ber of bunsetu in a sentence.2.
Maximizing vocabulary size first (VSort)Unknown words cause unknown dependen-cies, thus we sort the corpus to maximize itsvocabulary size.3.
Maximizing unseen dependencies first(UDep)This is similar to (2).
However, we cannotknow the true dependencies.
The analyzedresults by the dependency analyzer basedon the current corpus are used to estimatethe unseen dependencies.
The accuracy ofthe estimated results was 90.25% and thesentence accuracy was 54.03%.4.
Maximizing average distance of dependen-cies first (ADist)It is difficult to analyze long-distance depen-dencies correctly.
Thus, the average distanceof dependencies is an approximation for thedifficulty of analysis.5.
Chronological order (Chrono)Since there is a chronological order in news-paper articles, this strategy should feel quitenatural.6.
Random (ED0)Chronological order seems natural, but news-paper articles also have cohesion.
Thus, thevocabulary might be unbalanced when weconsider the chronological order.
We also tryrandomized order; actually, we used the cor-pus ED0 as the randomized corpus.We sorted the editorial component of the KyotoText Corpus by each strategy mentioned above.After sorting, corpora were constructed by takingthe top N sentences of each corpus sorted by eachstrategy.
The size of each corpus was balancedwith the number dependencies.We constructed dependency analyzers based oneach corpus, KG1 plus each prepared corpus, thentested them by using the following corpora: (a) K-mag, (b) IPAL0, and (c) KG0.638Corpus # of sent.
# of bunsetu vocabulary size # of dependencies # of bunsetu / sent.Long 5,490 81,759 13,266 76,269 14.89VSort 8,762 85,031 16,428 76,269 9.705UDep 5,524 81,793 13,371 76,269 14.81ADist 6,950 83,223 13,074 76,273 11.97Chrono 9,342 85,609 13,278 76,267 9.164ED0 9,357 85,628 13,561 76,271 9.151K-mag 489 4,851 2,501 4,362 9.920IPAL0 7,665 33,484 8,617 25,819 4.368KG0 9,835 96,283 21,616 86,448 9.790I-Long 5,523 91,972 20,068 86,449 16.65I-VSort 8,437 94,881 28,867 86,444 11.25Table 6: Detailed information of corporaK-mag consists of articles from the KoizumiCabinet?s E-Mail Magazine.
This magazine wasfirst published on May 29th 1999 and is still re-leased weekly.
K-mag consists of articles of themagazine published from May 29th 1999 to July19th 1999.
In addition, since March 25th 2004 anEnglish version of this E-Mail Magazine has beenavailable.
Thus, currently this E-mail Magazine isbilingual.
The articles of this magazine were an-alyzed by the dependency analyzer CaboCha, andwe manually corrected the errors.K-mag includes a wide variety articles, and theaverage sentence length is longer than in newspa-pers.
Basic information on K-mag is also providedin Table 6.Learning corpus Acc.
(%) S-acc.
(%)KG1 87.25 49.69KG1+LONG 87.67 51.53KG1+Vsort 87.25 50.10KG1+UDep 87.57 51.12KG1+ADist 87.67 50.72KG1+Chrono 87.57 50.31KG1+Rand 87.60 49.69Table 7: Analyzed results of K-mag (which isdifferent domain and has long average sentencelength) with these learning corpora3.3.2 Simulation for initial situationThe results revealed that the longest-first strat-egy seems the best way.
Here, however, a questionarises: ?Does the longest-first strategy always pro-vide good predictions??
We carried out an exper-iment to answer the question.
The experimentalLearning corpus Acc.
(%) S-acc.
(%)KG1 97.68 93.02KG1+LONG 97.75 93.22KG1+Vsort 97.70 93.06KG1+UDep 97.75 93.18KG1+ADist 97.70 93.10KG1+Chrono 97.71 93.06KG1+Rand 97.69 93.06Table 8: Analyzed results of IPAL0 (which isdifferent domain and has short average sentencelength) with these learning corporaresults we presented above were simulations of anexpanding corpus.
On the other hand, it is alsopossible to consider an initial situation for build-ing a dependency-analyzed corpus.
In such a situ-ation, which would be the best strategy to take?We carried out a simulation experiment inwhich there was no annotated corpus; instead webegan to construct a new one.
We used generalarticles from the Kyoto Text Corpus and tried thefollowing three strategies: (a) Random (actually,KG0 was used), (b) Longest first (I-Long), and (c)maximizing vocabulary size first (I-VSort).
Threecorpora were prepared by these strategies.
Table6 also shows the corpora information.
In this ex-periment, the corpora were balanced with respectto the number of dependencies.
We used CaboChawith these corpora and tested them with K-mag,ED0, and IPAL0.
Table 10 shows the results ofthe experiment.639K-mag ED0 IPAL0Corpus Acc.
(%) S-acc.
(%) Acc.
(%) S-acc.
(%) Acc.
(%) s-acc(%)Random (KG0) 87.87 49.69 90.17 53.64 97.76 93.15I-Long 87.41 49.28 90.11 52.96 97.66 92.94I-VSort 87.92 50.31 90.14 53.86 97.72 93.06Table 10: Results of initial situation experimentLearning corpus Acc.
(%) S-acc.
(%)KG1 89.60 49.89KG1+LONG 89.99 51.25KG1+Vsort 89.97 51.31KG1+UDep 89.98 51.39KG1+ADist 89.98 51.01KG1+Chrono 89.86 51.09KG1+Rand 89.95 51.20Table 9: Analyzed results of KG0 (which is thesame domain and has almost the same averagesentence length) with these learning corpora4 Discussion4.1 Error analysisTo analyze corpora, we employed the dependencyanalyzer CaboCha, an SVM-based system.
In gen-eral, when one attempts to solve a classificationproblem with kernel functions, it is difficult toknow the kernel function that best fits the prob-lem.
To date, second- and third-degree polynomialkernels have been empirically used in Japanese de-pendency analysis with SVMs.In the biased evaluation (the test corpus was thelearning corpus), the third-degree polynomial ker-nel produced very accurate results, almost 100%.On the other hand, in the open test, however, thethird-degree polynomial kernel did not produce re-sults as good as the second-degree one.
We con-clude from these results that the third-degree poly-nomial kernel suffered the over-fitting problem.The second-degree polynomial kernel producedon accuracy of almost 94% in the biased evalua-tion, and this can be considered as the upper boundfor the second degree polynomial kernel to ana-lyze Japanese dependency structure.
The accuracywas stable when we adjusted the soft-margin pa-rameter of the SVM.
However, there were severalannotation errors in the corpus.
Thus, if we cor-rect such annotation errors, the accuracy wouldimprove.Table 4 indicates that case elements consistingof nouns and case markers were frequently mis-analyzed.
From a grammatical point of view, acase element should depend on a verb.
However,the number of relations between verbs and case el-ements is combinatorial explosion.
Thus, we canconclude that the learning data were not sufficientfor relations between verbs and case elements toanalyze unseen relations.On the other hand, in Table 4, verbs take manyplaces in comparison to their distribution in thetest set corpus.
These verbs tend to form conjunc-tive structures and it is known that analyzing con-junctive structure is difficult (Kurohashi and Na-gao, 1994b).
Particularly when a verb is a head ofan adverbial clause, it seems very difficult to de-tect a head bunsetu, which is modified by the verb.From Table 5, we can conclude that the ana-lyzed errors centered on short-distance relations;the analyzer especially tends to mis-analyze thecorrect distance of two as one.
Typical casesof such mis-analysis are ?N1-no N2-no N3?
and?
[adnominal clause] N1-no N2.?
In some cases, itis also difficult for humans to analyze these pat-terns correctly.4.2 Selective sampling simulationThe results revealed very small differences be-tween strategies possibly due to insufficient cor-pus size.
However, there was an overall tendencythat the accuracy depended heavily whether howmany long sentences with very long dependencieswere included in the test set.
Table 3 shows a sim-ple example of this.
In the cross-validation teststhe accuracy of the general articles, the averagelength of which was 9.769 bunsetu / sentence, wasalmost 1% lower than that of the editorial articles,whose average length was 9.162 bunsetu / sen-tence.
The reason why sentence length controlledthe accuracy was that an error in the long-distancedependency may have caused other errors in orderto satisfy the condition that dependencies do notcross each other in Japanese dependencies.
Thus,640many errors occurred in longer sentences.
To im-prove the accuracy, it is vital to analyze very long-distance dependencies correctly.From Tables 7, 8 and 9, the strategy of longestfirst appears good for the expanding situation evenif the average length of the test set is very short likein IPAL0.
However, in the initial situation, sincethere is no labeled data, the longest-first strategyis not a good method.
Table 10 shows that therandom strategy (KG0) and the strategy of max-imizing vocabulary size first (I-VSort) were bet-ter than the longest-first strategy (I-Long).
Thisis because the test sets comprised short sentencesand we can imagine that there were dependen-cies included only in such short sentences.
Inother words, the longest-first strategy was heav-ily biased toward long sentences and the strategycould not cover the dependencies that were onlyincluded in short sentences.On the other hand, the number of such depen-dencies that were only included in short sentenceswas quite small, and this number would soon besaturated when we built a dependency analyzedcorpus.
Thus, in the initial situation, the randomstrategy was better, whereas after we prepared acorpus to some extent, the longest-first strategywould be better because analyzing long sentencesis difficult.In the case of expansion, the longest-first strat-egy was good, though we have to consider the ac-tual time required to annotate such long sentencesbecause in general longer sentences tend to havemore complex structures and introduce more op-portunities for ambiguous parses.
This means itis difficult for humans to annotate such long sen-tences.5 Related worksTo date, many works on selective sampling wereconducted in the field related to natural languageprocessing (Fujii et al, 1998; Hwa, 2004; Kammand Meyer, 2002; Riccardi and Hakkani-Tu?r,2005; Ngai and Yarowsky, 2000; Banko and Brill,2001; Engelson and Dagan, 1996).
The basic con-cepts are the same and it is important to predict thetraining utility value of each candidate with highaccuracy.
The work most closely related to thispaper is Hwa?s (Hwa, 2004), which proposed a so-phisticated method for selective sampling for sta-tistical parsing.
However, the experiments carriedout in that paper were done with just one corpus,WSJ Treebank.
The study by Baldridge and Os-borne (Baldridge and Osborne, 2004) is also veryclose to this paper.
They used the Redwoods tree-bank environment (Oepen et al, 2002) and dis-cussed the reduction in annotation cost by an ac-tive learning approach.In this paper, we focused on the analysis of sev-eral fundamental sampling strategies for buildinga Japanese dependency-analyzed corpus.
A com-plete estimating function of training utility valuewas not shown in this paper.
However, we testedseveral strategies with different types of corpora,and these results can be used to design such a func-tion for selective sampling.6 ConclusionThis paper discussed several sampling strategiesfor Japanese dependency-analyzed corpora, test-ing them with the Kyoto Text Corpus and theIPAL corpus.
The IPAL corpus was constructedespecially for this study.
In addition, although itwas quite small, we prepared the K-mag corpus totest the strategies.
The experimental results usingthese corpora revealed that the average length of atest set controlled the accuracy in case of expan-sion; thus the longest-first strategy outperformedother strategies.
On the other hand, in the initialsituation, the longest-first strategy was not suitablefor any test set.The current work points us in several futuredirections.
First, we shall continue to builddependency-analyzed corpora.
While newspaperarticles may be sufficient for our purpose, otherresources seem still inadequate.
Second, whilein this work we focused on analysis using severalfundamental selective strategies for a dependency-analyzed corpus, it is necessary to provide a func-tion to build a selective sampling framework toconstruct a dependency-analyzed corpus.ReferencesJason Baldridge and Miles Osborne.
2004.
Activelearning and the total cost of annotation.
In Pro-ceedings of EMNLP.Michele Banko and Eric Brill.
2001.
Scaling tovery very large corpora for natural language disam-biguation.
In Proceedings of the 39th Annual Meet-ing of the Association for Computational Linguistics(ACL-2001), pages 26?33.David A. Cohn, Les Atlas, and Richard E. Ladner.6411994.
Improving generalization with active learn-ing.
Machine Learning, 15(2):201?221.Sean P. Engelson and Ido Dagan.
1996.
Minimizingmanual annotation cost in supervised training fromcorpora.
In Proceedings of the 34th Annual meetingof Association for Computational Linguistics, pages319?326.Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, andHozumi Tanaka.
1998.
Selective sampling forexample-based word sense disambiguation.
Com-putational Linguistics, 24(4):573?598.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Teresa M. Kamm and Gerard G. L. Meyer.
2002.
Se-lective sampling of training data for speech recogni-tion.
In Proceedings of Human Language Technol-ogy.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InCoNLL 2002: Proceedings of the 6th Conference onNatural Language Learning 2002 (COLING 2002Post-Conference Workshops), pages 63?69.Sadao Kurohashi and Makoto Nagao.
1994a.
KNParser: Japanese dependency/case structure ana-lyzer.
In Proceedings of Workshop on Sharable Nat-ural Language Resources, pages 48?55.Sadao Kurohashi and Makoto Nagao.
1994b.
A syn-tactic analysis method of long Japanese sentencesbased on the detection of conjunctive structures.Computational Linguistics, 20(4):507?534.Grace Ngai and David Yarowsky.
2000.
Rule writ-ing or annotation: Cost-efficient resource usage forbase noun phrase chunking.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics, pages 117?125.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christopher Manning, Dan Flickinger, and ThorstenBrants.
2002.
The LinGO Redwoods treebank: Mo-tivation and preliminary applicatoins.
In Proceed-ings of COLING 2002, pages 1?5.Giuseppe Riccardi and Dilek Hakkani-Tu?r.
2005.
Ac-tive learning: Theory and applications to automaticspeech recognition.
IEEE Transactions on Speechand Audio Processing, 13(4):504?511.Manabu Sasano.
2004.
Linear-time dependency anal-ysis for Japanese.
In Proceedings of Coling 2004,pages 8?14.642
