First Joint Conference on Lexical and Computational Semantics (*SEM), pages 543?546,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsLIMSI: Learning Semantic Similarity by Selecting Random Word SubsetsArtem SokolovLIMSI-CNRSB.P.
133, 91403 Orsay, Franceartem.sokolov@limsi.frAbstractWe propose a semantic similarity learningmethod based on Random Indexing (RI) andranking with boosting.
Unlike classical RI, weuse only those context vector features that areinformative for the semantics modeled.
De-spite ignoring text preprocessing and dispens-ing with semantic resources, the approach wasranked as high as 22nd among 89 participantsin the SemEval-2012 Task6: Semantic TextualSimilarity.1 IntroductionOne of the popular and flexible tools of semanticsmodeling are vector distributional representations oftexts (also known as vector space models, seman-tic word spaces or distributed representations).
Theprinciple idea behind vector space models is to useword usage statistics in different contexts to gen-erate a high-dimensional vector representations foreach word.
Words are represented by context vec-tors whose closeness in the vector space is postu-lated to reflect semantic similarity (Sahlgren, 2005).The approach rests upon the distributional hypothe-sis: words with similar meanings or functions tendto appear in similar contexts.
The prominent ex-amples of vector space models are Latent Seman-tic Analysis (or Indexing) (Landauer and Dutnais,1997) and Random Indexing (Kanerva et al, 2000).Because of the heuristic nature of distributionalmethods, they are often designed with a specificsemantic relation in mind (synonymy, paraphrases,contradiction, etc.).
This complicates their adaptionto other application domains and tasks, requiringmanual trial-and-error feature redesigns and tailoredpreprocessing steps to remove morphology/syntaxvariations that are not supposed to contribute to thesemantics facet in question (e.g., stemming, stop-words).
Further, assessing closeness of semanticvectors is usually based on a fixed simple similarityfunction between distributed representations (often,the cosine function).
The cosine function implicitlyassigns equal weights to each component of the se-mantic vectors regardless of its importance for theparticular semantic relation and task.
Finally, dur-ing production of training and evaluation sets, thecontinuum of possible grades of semantic similar-ity is usually substituted with several integer values,although often only the relative grade order mattersand not their absolute values.
Trying to reproducethe same values or the same gaps between gradeswhen designing a semantic representation schememay introduce an unnecessary bias.In this paper we address all of the above draw-backs and present a semantic similarity learningmethod based on Random Indexing.
It does not re-quire manual feature design, and is automaticallyadapted to the specific semantic relations by select-ing needed important features and/or learning neces-sary feature transformations before calculating sim-ilarity.
In the proof-of-concept experiments on theSemEval-2012 data we deliberately ignored all rou-tine preprocessing steps, that are often consideredobligatory in semantic text processing, we did notuse any of the semantic resources (like WordNet)nor trained different models for different data do-mains/types.
Despite such over-constrained setting,the method showed very positive performance and543was ranked as high as 22nd among 89 participants.2 Random IndexingRandom Indexing (RI) is an alternative to LSA-like models with large co-occurrence matrices andseparate matrix decomposition phase to reduce di-mension.
RI constructs context vectors on-the-flybased on the occurrence of words in contexts.
First,each word is assigned a unique and randomly gener-ated high-dimensional sparse ternary vector.
Vec-tors contain a small number (between 0.1-1%) ofrandomly distributed +1s and -1s, with the rest ofthe elements set to 0.
Next, the final context vectorsfor words are produced by scanning through the textwith a sliding window of fixed size, and each timethe word occurs in the text, the generated vectors ofall its neighbors in the sliding context window areadded to the context vector of this word1.
Finally,the obtained context vectors are normalized by theoccurrence count of the word.RI is a practical variant of the well-knowndimension reduction technique of the Johnson-Lindenstrauss (JL) lemma (Dasgupta and Gupta,2003).
An Euclidean space can be projected with arandom Gaussian matrix R onto smaller dimensionEuclidean space, such that with high probability thedistance between any pair of points in the new spaceis within a distortion factor of 1 ?
?
of their origi-nal distance.
Same or similar guarantees also holdfor a uniform {?1,+1}-valued or ternary (from acertain distribution) randomR (Achlioptas, 2003) orfor even sparser matrices (Dasgupta et al, 2010)Restating the JL-lemma in the RI-terminology,one can think of the initial space of characteristicvectors of word sets of all contexts (each compo-nent counts corresponding words seen in the contextwindow over the corpus) embedded into a smallerdimension space, and approximately preserving dis-tances between characteristic vectors.
Becauseof the ternary generation scheme, each resultingfeature-vector dimension either rewards, penalizesor ?switches off?
certain words for which the cor-responding row of R contained, resp., +1, ?1 or 0.So far, RI has been a na?
?ve approach to feature1Although decreasing discounts dampening contribution offar-located context words may by beneficial, we do not use itputting our method in more difficult conditions.learning ?
although it produces low-dimensionalfeature representations, it is unconscious of thelearning task behind.
There is no guarantee that theEuclidean distance (or cosine similarity) will cor-rectly reflect the necessary semantic relation: for apair of vectors, not all word subsets are characteris-tic of a particular semantic relation or specific to it,as presence or absence of certain words may play norole in assessing given similarity type.
Implicationsof RI in the context of learning textual similarityare coming from the feature selection (equivalently,word subset selection) method, based on boosting,that selects only those features that are informativefor the semantic relation being learned (Section 4).Thus, the supervision information on sentence simi-larity guides the choose of word subsets (among allrandomly generated by the projection matrix) thathappen to be relevant to the semantic annotations.3 Semantic Textual Similarity TaskLet {(si1, si2)} be the training set of N pairs of sen-tences, provided along with similarity labels yi.
Thehigher the value of yi the more semantically similaris the pair (si1, si2).
Usually absolute values of yi arechosen arbitrary; only their relative order matters.We would learn semantic similarity between(si1, si2) as a function H(x?i), where x?i is a sin-gle vector combining sentence context vectors v(si1)and v(si2).
Context representation v(s) for a sen-tence s is defined as an average of the word contextvectors v(w) contained in it, found using a large textcorpus with the RI approach, described in the pre-vious section: v(s) =?w?s v(w)/ |s|.
Possibletransformations into x?i include a concatenation ofv(si1) and v(si2), concatenation of the sum and dif-ference vectors or a vector composed of component-wise symmetric functions (e.g., a product of cor-responding components).
In order to learn a sym-metric H , one can either use each pair twice duringtraining, or symmetrize the construction of x?.4 Feature Selection with BoostingWe propose to exploit natural ordering of (si1, si2)according to yi to learn a parameterized similarityfunction H(x?i).
In this way we do not try learn-ing the absolute values of similarity provided in thetraining.
Also, by using boosting approach we allow544for gradual inclusion of features into similarity func-tion H , implementing in this way feature selection.For a given number of training steps T , a boost-ing ranking algorithm learns a scoring function H ,which is a linear combination of T simple, non-linear functions ht called weak learners: H(x?)
=?Tt=1 ?tht(x?
),where each ?t is the weight assignedto ht at step t of the learning process.Usually the weak learner is defined on only fewcomponents of x?.
Having build H at step t, the nextin turn (t + 1)?s leaner is selected, optimized andweighted with the corresponding coefficient ?t+1.In this way the learning process selects only thosefeatures in x?
(or, if viewed from the RI perspective,random word subsets) that contribute most to learn-ing the desired type input similarity.As the first ranking method we applied the pair-wise ranking algorithm RankBoost (Freund et al,2003), that learns H by minimizing a convex ap-proximation to a weighted pair-wise loss:?
(si1,si2),(sj1,sj2):yi<yjP (i, j)[[H(x?i) ?
H(x?j)]].Operator [[A]] = 1 if the A = true and 0 other-wise.
Positive values of P weight pairs of x?i and x?j?
the higher is P (i, j), the more important it is topreserve the relative ordering of x?i and x?j .
We usedthe simplest decision stumps that depend on one fea-ture as weak learners: h(x; ?, k) = [[xk > ?
]], wherek is a feature index and ?
is a learned threshold.The second ranking method we used was a point-wise ranking algorithm, based on gradient boostingregression for ranking (Zheng et al, 2007), calledRtRank and implemented by Mohan et al (2011)2.The loss optimized by RtRank is slightly different:?
(si1,si2),(sj1,sj2):yi<yj(max{0, H(x?i)?H(x?j)})2.Another difference is in the method for selectingweak learner at each boosting step, that relies on re-gression loss and not scalar product as RankBoost.Weak learners for RtRank were regression trees offixed depth (4 in our experiments).5 ExperimentsWe learned context vectors on the GigaWord En-glish corpus.
The only preprocessing of the cor-2http://sites.google.com/site/rtrankinglearner transform correl.
?baseline pure RI, cos - 0.264 0.005logistic reg.
- 0.508 0.041logistic reg.
concat 0.537 0.052boosting RankBoostsumdiff 0.685 0.027product 0.663 0.018crossprod 0.648 0.028crossdiff 0.643 0.023concat 0.625 0.025absdiff 0.602 0.021RtRanksumdiff 0.730 0.020product 0.721 0.023Table 1: Mean performance of the transformation andboosting methods for N = 100 on train data.pus was stripping all tag data, removing punctuationand lowercasing.
Stop-words were not removed.Context vectors were built with the JavaSDM pack-age (Hassel, 2004)3 of dimensionality N = 100 andN = 105, resp., for preliminary and final experi-ments, with random degree 10 (five +1s and -1s ineach initial vector), right and left context windowsize of 4 words4 and constant weighting scheme.Training and test data provided in the SemEval-2012 Task 6 contained 5 training and 5 testing textsets each of different domains or types of sentences(short video descriptions, pairs of outputs of a ma-chine translation system, etc.).
Although the 5 setshad very different characteristics, we concatenatedall training files and trained a single model.
Theprincipal evaluation metrics was Pearson correlationcoefficient, that we report here.
Two related othermeasures were also used (Agirre et al, 2012).Obtained sentence vectors v(s) for were trans-formed into vectors x?
with several methods:?
?sumdiff?
: x?
= (v?
(s1) + v?
(s2), sgn(v1(s1) ?v1(s2))(v(s1)?
v(s2)))?
?concat?
: x?
= (v(s1), v(s2)), and x??
=(v(s2), v(s1))?
?product?
: xi = vi(s1) ?
vi(s2)?
?crossprod?
: xij = vi(s1) ?
vj(s2)?
?crossdiff?
: xij = vi(s1)?
vj(s2)?
?absdiff?
: xi = |vi(s1)?
vi(s2)|.Methods ?concat?
and ?sumdiff?
were proposedby Hertz et al (2004) for distance learning for clus-3http://www.csc.kth.se/?xmartin/java4Little sensitivity was found to the window sizes from 3 to 6.545learner transform train??
test rank MSRpar MSRvid SMTeur OnWN SMTnewsRankBoostproduct 0.748?0.017 0.6392 32 0.3948 0.6597 0.0143 0.4157 0.2889sumdiff 0.735?0.016 0.6196 45 0.4295 0.5724 0.2842 0.3989 0.2575RtRankproduct 0.784?0.017 0.6789 22 0.4848 0.6636 0.0934 0.3706 0.2455sumdiff 0.763?0.014Table 2: Mean performance of the best-performing two transformation and two boosting methods for N = 105.tering.
Comparison of mean performance of differ-ent transformation and learning methods on the 5-fold splitting of the training set is given in Table 1for short context vectors (N = 100).
The correlationis given for the optimal algorithms?
parameters (Tfor RankBoost and, additionally, tree depth and ran-dom ratio for RtRank), found with cross-validationon 5 folds.
With these results for smallN , two trans-formation methods were preselected (?sumdiff?
and?product?)
for testing and submission with N = 105(Table 2), as increasing N usually increased perfor-mance.
Yet, only about 103 features were actuallyselected by RankBoost, meaning that a relativelyfew random word subsets were informative for ap-proximating semantic textual similarity.In result, RtRank showed better performance,most likely because of more powerful learners, thatdepend on several features (word subsets) simulta-neously.
Performance on machine translation testsets was the lowest that can be explained by verypoor quality of the training data5: models for thesesubsets should have been trained separately.6 ConclusionWe presented a semantic similarity learning ap-proach that learns a similarity function specific tothe semantic relation modeled and that selects onlythose word subsets in RI, presence of which in thecompared sentences is indicative of their similarity,by using only relative order of the labels and nottheir absolute values.
In spite of paying no atten-tion to preprocessing, nor using semantic corpora,and with no domain adaptation the method showedpromising results.AcknowledgmentsThis work has been funded by OSEO under the Quaeroprogram.5A reviewer suggested another reason: more varied or evenincorrect lexical choice that is sometimes found in MT output.ReferencesDimitris Achlioptas.
2003.
Database-friendly randomprojections: Johnson-Lindenstrauss with binary coins.Comput.
Syst.
Sci., 66:671?687.Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-lez.
2012.
Semeval-2012 task 6: A pilot on semantictextual similarity.
In Proc.
of the Int.
Workshop on Se-mantic Evaluation (SemEval 2012) // Joint Conf.
onLexical & Computational Semantics (*SEM 2012).Sanjoy Dasgupta and Anupam Gupta.
2003.
An elemen-tary proof of a theorem of Johnson and Lindenstrauss.Random Struct.
Algorithms, 22(1):60?65.Anirban Dasgupta, Ravi Kumar, and Tama?s Sarlos.
2010.A sparse Johnson-Lindenstrauss transform.
In Proc.
ofthe ACM Symp.
on Theory of Comput., pages 341?350.Yoav Freund, Raj Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
Mach.Learn.Res., 4:933?969.Martin Hassel.
2004.
JavaSDM - a Java package forworking with Random Indexing and Granska.Tomer Hertz, Aharon Bar-hillel, and Daphna Weinshall.2004.
Boosting margin based distance functions forclustering.
In ICML, pages 393?400.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for latent se-mantic analysis.
In Proc.
of the Conf.
of the Cogn.Science Society.Thomas K. Landauer and Susan T. Dutnais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychol.
Rev., pages 211?240.Ananth Mohan, Zheng Chen, and Kilian Q. Weinberger.2011.
Web-search ranking with initialized gradientboosted regression trees.
Mach.Learn.Res., 14:77?89.Magnus Sahlgren.
2005.
An introduction to random in-dexing.
In Workshop on Methods & Applic.
of Sem.Indexing // Int.
Conf.
on Terminol.
& Knowl.
Eng.Zhaohui Zheng, Hongyuan Zha, Tong Zhang, OlivierChapelle, Keke Chen, and Gordon Sun.
2007.
Ageneral boosting method and its application to learn-ing ranking functions for web search.
In NIPS.546
