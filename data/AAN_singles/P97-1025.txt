Planning Reference Choices for Argumentative TextsXiaorong  Huang*Techne Knowledge Systems439 Univers i ty  AvenueToronto,  Ontar io  M5S 3G4CanadaxhCFormalSyst  ems.
caAbst rac tThis paper deals with the reference choices in-volved in the generation of argumentative text.Since a natual segmentation of discourse intoattentional spaces is needed to carry out thistask, this paper first proposes an architecturefor natural anguage generation that combineshierarchical planning and focus-guided naviga-tion, a work in its own right.
While hierarchi-cal planning spans out an attentional hierarchyof the discourse produced, local navigation fillsdetails into the primitive discourse spaces.
Theusefulness of this architecture actually goes be-yond the particular domain of application forwhich it is developed.A piece of argumentative t xt such as the proofof a mathematical theorem conveys a sequenceof derivations.
For each step of derivation, thepremises derived in the previous context andthe inference method (such as the applicationof a particular theorem or definition) must bemade clear.
Although not restricted to nominalphrases, our reference decisions are similar tothose concerning nominal subsequent referringexpressions.
Based on the work of Reichmann,this paper presents a discourse theory that han-dles reference choices by taking into accountboth textual distance as well as the attentionalhierarchy.1 In t roduct ionThis paper describes how reference decisions aremade in PROVERB, a system that verbalizesmachine-found natural deduction (ND) proofs.
Apiece of argumentative t xt such as the proof of amathematical theorem can be viewed as a sequence*Much of this research was carried out while the au-thor was at Dept.
of CS, Univ.
of the Saarland, sup-ported by DFG (German Research Council).
This paperwas written while the author was a visitor at Dept.
ofCS, Univ.
of Toronto, using facilities supported by agrant from the Natural Sciences and Engineering Re-search Council of Canada.of derivations.
Each such derivation is realized inPROVERB by a proof communicative act (PEA),following the viewpoint hat language utterances areactions.
PeAs involve referring phrases that shouldhelp a reader to unambiguously identify an object ofa certain type from a pool of candidates.
Concretely,such references must be made for previously derivedconclusions used as premises and for the inferencemethod used in the current step.As an example, let us look at the PeA with thename Derive below:(Derive Derived-Formula: u * Iv = uReasons : (unit(1u, U, *), u 6U)Method : Def-Semigroup*unit)Here, the slot Derived-Formula is filled by a newconclusion which this PeA aims to convey.
It can beinferred by applying the filler of Method to the fillerof Reasons as prernises.
There are alternative waysof referring to both the Reasons and the Method.Depending on the discourse history, the followingare two of the possible verbalizations:1.
(inference method omitted):"Since 1~ is the unit element of U, and u isan element of U, u * lu -- u."2.
(reasons omitted):"According to the definition of unit element,u * 1U - -  U.
"An explicit reference to a premise or an inferencemethod is not restricted to a nominal phrase, asopposed to many of the treatments of subsequentreferences found in the literature.
Despite this dif-ference, the choices to be made here have much incommon with the choices of subsequent referencesdiscussed in more general frameworks (Reichman,1985; Grosz and Sidner, 1986; Dale, 1992): theydepend on the availability of the object to be re-ferred to in the context and are sensitive to the seg-mentation of a context into an attentional hierarchy.Therefore, we have first to devise an architecture fornatural language generation that facilitates a nat-ural and effective segmentation of discourse.
The190basic idea is to distinguish between language pro-duction activities that effect the global shift of at-tention, and language production activities that in-volve only local attentional movement.
Concretely,PROVERB uses an architecture that models textgeneration as a combination of hierarchical planningand focus-guided navigation.
Following (Grosz andSidner, 1986) we further assume that every postingof a new task by the hierarchical planning mecha-nism creates new attentional spaces.
Based on thissegmentation, PROVERB makes reference choicesaccording to a discourse theory adapted from Reich-man (Reichman, 1985; Huang, 1990).2 The  System PROVERBPROVERB is a text planner that verbalizes naturaldeduction (ND) style proofs (Gentzen, 1935).
Sev-eral similar attempts can be found in previous work.The system EXPOUND (Chester, 1976) is an exam-ple of direct translation: Although a sophisticatedlinearization is applied on the input ND proofs, thesteps are then translated locally in a template-drivenway.
ND proofs were tested as inputs to an earlyversion of MUMBLE (McDonald, 1983); the mainaim, however, was to show the feasibility of the ar-chitecture.
A more recent attempt can be found inTHINKER (Edgar and Pelletier, 1993), which imple-ments everal interesting but isolated proof presenta-tion strategies.
PROVERB however can be seen asthe first serious attempt for a comprehensive systemthat produces adequate argumentative t xts fromND style proofs.
Figure 1 shows the architectureof PROVERB(Huang, 1994a; HuangFiedler, 1997):the macroplanner p oduces a sequence of PCAs, theDRCC (Derive Reference Choices Component) mod-ule of the microplanner enriches the PCAs with ref-erence choices.
The TSG (Text Structure Genera-tor) module subsequently produces the text struc-tures as the output of the microplanner.
Finally,text structures are realized by TAG-GEN (Kilgerand Finkler, 1995), our realization component.
Inthis paper, we concentrate only on the macroplan-ner and the DRCC component.2.1 Arch i tec ture  of  the  Macrop lannerMost current ext planners adopt a hierarchical plan-ning approach (How, 1988; Moore and Paris, 1989;Dale, 1992; Reithinger, 1991).
Nevertheless thereis psychological evidence that language has an un-planned, spontaneous aspect as well (Ochs, 1979).Based on this observation, Sibun (Sibun, 1990) im-plemented a system for generating descriptions ofobjects with a strong domain structure, such ashouses, chips, and families.
Her system producestext using a technique she called local organization.While a hierarchical planner ecursively breaks gen-eration tasks into subtasks, local organization avi-gates the domain-object following the local focus ofNatural Deduction Proofi -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
:~lacroplanner,,: i&p\]An-e; .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.VPMs7.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
L LT_e_x_t_S_t__m_c_t_u_r_ e .
.
.
.
.
.
.
.
.
.
.
.
.
.
.Transformer )(  e ,izo  )Figure 1: Architecture of PROVERBattention.PROVERB combines both of these approachesin a uniform planning framework (Huang, 1994a).The hierarchical planning splits the task of present-ing a particular proof into subtasks of presentingsubproofs.
While the overall planning mechanismfollows the RST-based planning approach (How,1988; Moore and Paris, 1989; Reithinger, 1991),the planning operators more resemble the schematain schema-based planning (McKeown, 1985; Paris,1988) since presentation patterns associated withspecific proof patterns normally contain multipleRST-relations.
PROVERB's hierarchical planningis driven by proof patterns that entail or suggest es-tablished ways of presentation.
For trivial proofsthat demonstrate no characteristic patterns, how-ever, this technology will fail.
PRO VERB navigatessuch relatively small parts of a proof and chooses thenext conclusion to be presented under the guidanceof a local focus mechanism.While most existing systems follow one of the twoapproaches exclusively, PROVERB uses them ascomplementary techniques in an integrated frame-work.
In this way, our architecture provides a clearway of factoring out domain-dependent presenta-tion knowledge from more general NLG techniques.While PROVERB's hierarchical planning operatorsencodes accepted format for mathematical text, itslocal navigation embodies more generic principles of191language production.The two kinds of planning operators are treated ac-cordingly.
Since hierarchical planning operators em-body explicit communicative norms, they are givena higher priority.
Only when none of them is appli-cable, will a local navigation operator be chosen.2.2 P roo f  Communicative ActsPCAs are the primitive actions planned by themacroplanner of PROVERB?
Like speech acts, theycan be defined in terms of the communicative goalsthey fulfill as well as their possible verbalizations.The simplest one conveying the derivation of a newintermediate conclusion is illustrated in the intro-duction.
There are also PCAs that convey a partialplan for further presentation and thereby update thereader's global attentional structure.
For instance,the PCA(Begin-Cases Goal : FormulaAssumptions: (A B))creates two attentional spaces with A and B as theassumptions, and Formula as the goal by producingthe verbalization:"To prove Formula, let us consider the two casesby assuming A and B.
"2.3 Hierarchical PlanningHierarchical planning operators represent commu-nicative norms concerning how a proof is to be pre-sented can be split into subproofs, how the subproofscan be mapped onto some linear order, and howprimitive subproofs hould be conveyed by PCAs.Let us look at one such operator, which handlesproof by case analysis.
The corresponding schemaof such a proof tree I is shown in Figure 2, whereF G: : :?L4 : F V G ~ ?L3 :~ CASE?L1 :A~-QFigure 2: Proof Schema Casethe subproof rooted by ?L4 leads to F V G, whilesubproofs rooted by ?L2 and ?L3 are the two casesproving Q by assuming F or G, respectively?
Theapplicability encodes the two scenarios of case anM-ysis, where we do not go into details.
In both circum-stances this operator first presents the part leadingto F V G, and then proceeds with the two cases.
Italso inserts certain PCAs to mediate between parts*We adopt for proof tree the notation of Gentzen.Each bar represents a step of derivation, where the for-mula beneath the bar is derived from the premises abovethe bar.
For the convenience of discussion, some formu-lae are given an identifying label, such as ?L1.of proofs.
This procedure is captured by the plan-ning operator below.Case-Implicit?
Applicability Condition: ((task ?L1) V(local-focus ?L4)) A (not-conveyed (?L2 ?L3))?
Acts:1. if ?L4 has not been conveyed, then present ?L4(subgoal 1)2. a PCA with the verbalization: "First, let usconsider the first case by assuming F."3. present ?L2 (subgoal 2)4. a PCA with the verbalization: "Next, we con-sider the second case by assuming G."5. present ?L3 (subgoal 3)6. mark ?L1 as conveyed.features: (hierarchical-planning compulsory im-plicit)2.4 Planning as NavigationThe local navigation operators simulate the un-planned part of proof presentation.
Instead of split-ting presentation goals into subgoals, they follow thelocal derivation relation to find a proof step to bepresented next.2.4.1 The  Local  FocusThe node to be presented next is suggested by themechanism of local focus.
In PROVERB, our localfocus is the last derived step, while focal centers aresemantic objects mentioned in the local focus.
Al-though logically any proof node that uses the localfocus as a premise could be chosen for the next step,usually the one with the greatest semantic overlapwith the focal centers is preferred?
In other words, ifone has proved a property about some semantic ob-jects, one will tend to continue to talk about theseparticular objects, before turning to new objects.Let us examine the situation when the proof belowis awaiting presentation.\[1\]: P(a,b) \[1\]: P(a,b), \[3\]: S(c)\[ 2\] Q(a;b)' \[4\]: R(b,c)\[5\]: Q(a, b) A R(b, c)Assume that node \[1\] is the local focus, {a, b} is theset of focal centers, \[3\] is a previously presented nodeand node \[5\] is the root of the proof to be presented?\[2\] is chosen as the next node to be presented, sinceit does not introduce any new semantic object andits overlap with the focal centers ({a,b}) is largerthan the overlap of \[4\] with the focal centers ({b}).For local focus mechanisms used in another do-main of application, readers are referred to (McKe-own, 1985).3 The  At tent iona l  H ie rarchyThe distinction between hierarchical planning andlocal navigation leads to a very natural segmentation192NNo S;D Formula7.
7; ~- group(F, *) A subgroup(U, F, *) A unit(F, 1, *) Aunit(U, lt\], *)8.
7; ~- U C F9.
7; I- lrr EU10.
7; I- 3zx E U11.
;11 I- u E U12.
7;11 b u* lt\] = u13.
7;11 b u E F14.
7;11 I- It\] E F15.
7;11 I- semigroup(F, *)16.
7;11 b solution(u, u, lu, F, *)17.
7;11 b u* 1 = u18.
7;11 I- 1 E F19.
7;11 I- solution(u, u, 1, F, *)20.
7;11 b- 1 = lrr21.
7; t- 1 = 1u22.
; I- group(F, *) A subgroup(U, F, *) A unit(F, 1, *) Aunit(U, lt\], *) :=~ 1 = It\]Reason(Hyp)(Def-subgroup 7)(Def-unit 7)(::1 9)(Hyp)(Def-unit 7 11)(Def-subset 8 11)(Def-subset 8 9)(Def-group 7)(Def-sohition 12 13 14 15)(Def-unit 7 13)(Def-unit 7)(Def-soluti0n 13 17 18 15)(Th-solution 17 16 19)(Choice 10 20)(Ded 7:21)Figure 3: Abstracted Proof about Unit Element of Subgroupsof a discourse into an attentional hierarchy, since fol-lowing the theory of Grosz and Sidner (Grosz andSidner, 1986), there is a one-to-one correspondencebetween the intentional hierarchy and the atten-tional hierarchy.
In this section, we illustrate theattentional hierarchy with the help of an example,which will be used to discuss reference choices later.The input proof in Figure 3 is an ND style prooffor the following theorem2:Theorem:Let F be a group and U a subgroup of F. If i andlv  are unit elements of F and U respectively, then1=1u.The definitions of semigroup, group, and unit areobvious, solution(a, b, c, F, ,) stands for "c is a so-lution of the equation a ,  z = b in F." Each line inthe proof is of the form:Label A F- Conclusion (Justification reasons)where Justification is either an ND inference rule, adefinition or theorem, which justifies the derivationof the Conclusion using as premises the formulas inthe lines given as reasons.
A can be ignored for ourpurpose.We assume a reader will build up a (partial) prooftree as his model of the ongoing discourse.
Thecorresponding discourse model after the completionof the presentation of the proof in Figure 3 is aproof tree shown in Figure 4.
Note that the barsin Gentzen's notion (Figure 2) are replaced by linksfor clarity.
The numbers associated with nodes arethe corresponding line numbers in Figure 4.
Chil-dren of nodes are given in the order they have beenpresented.
The circles denote nodes which are first2The first 6 lines are definitions and theorems used inthis proof, which are omitted.derived at this place, and nodes in the form of smallboxes are copies of some previously derived nodes(circled nodes), which are used as premises again.For nodes in a box, a referring expression must havebeen generated in the text.
The big boxes representattentional spaces (previously called proof units bythe author), created during the presentation process.The naturalness of this segmentation is largely dueto the naturalness of the hierarchical planning oper-ators.
For example, attentional space U2 has twosubordinate spaces U3 and U4.
This reflects a natu-ral shift of attention between a subproof that de-rives a formula of the pattern 3 ,P (z )  (node 10,3,x E U), and the subproof that proceeds afterassuming a new constant u satisfying P (node 11,u E U).
When PROVERB opens a new attentionalspace, the reader will be given information to post anopen goal and the corresponding premises.
Elemen-tary attentional spaces are often composed of multi-ple PCAs produced by consecutive navigation steps,such as U5 and U6.
It is interesting to note thatelementary attentional space cannot contain PCAsthat are produced by consecutive planning operatorsin a pure hierarchical planning framework.Adapting the theory of Reichman for our purpose(Reichman, 1985), we assume that each attentionalspace may have one of the following status:?
an attentional space is said to be open if its rootis still an open goal.-The  active attentional space is the innermostattentional space that contains the local focus.-The  controlling attentional space is the inner-most proof unit that contains the active atten-tional space.-precontrol attentional spaces are attentionalspaces that contain the controlling attentionalspace.193U4U5 ~ U6U1Figure 4: Proof Tree as Discourse Model?
Closed spaces are attentional spaces without opengoals.4 A C lass i f i ca t ion  o f  ReferenceFormsA referring expression should help a reader to iden-tify an object from a pool of candidates, This sec-tion presents a classification of the possible formswith which mathematicians refer to conclusions pre-viously proved (called reasons) or to methods of in-ference available in a domain.4.1 Reference Forms for ReasonsThree reference forms have been identified by theauthor for reasons in naturally occurring proofs(Huang, 1990):1.
The omit form: where a reason is not mentionedat all.2.
The explicit form: where a reason is literally re-peated.3.
The implicit form: By an implicit form we meanthat although a reason is not verbalized irectly,a hint is given in the verbalization of either theinference method, or of the conclusion.
For in-stance, in the verbalization below"Since u is an element in U, u ?
1u = u bythe definition of unit.
"the first reason of the PCA in Section 1, "since1v is the unit element of U" is hinted at by theinference method which reads "by the definitionof unit".Although omit and implicit forms lead to the samesurface structure, the existence of an implicit hint inthe other part of the verbalization affects a reader'sunderstanding.4.2 Reference Forms for MethodsPROVERB must select referring expressions formethods of inference in PCAs as well.
Below arethe three reference forms identified by the author,which are analogous to the corresponding cases forreasons:1. the explicit form: this is the case where a writermay decide to indicate explicitly which inferencerule he is using.
For instance, explicit translationsof a definition may have the pattern: "by the def-inition of unit element", or "by the uniqueness ofsolution."
ND rules have usually standard verbal-izations.2.
the omit form: in this case a word such as "thus"or "therefore" will be used.3.
The implicit form: Similar to the implicit formfor the expression of reasons, an implicit hint toa domain-specific inference method can be giveneither in the verbalization of the reasons, or inthat of the conclusion.5 Reference  Cho ices  in  PROVERB5.1 Referr ing to ReasonsBecause reasons are intermediate conclusions provedpreviously in context, their reference choices havemuch in common with the problem of choosinganaphoric referring expressions in general.
To ac-count for this phenomenon , concepts like activat-194edness, foregroundness and consciousness have beenintroduced.
More recently, the shift of focus hasbeen further investigated in the light of a structuredflow of discourse (Reichman, 1985; Grosz and Sid-net, 1986; Dale, 1992).
The issue of salience is alsostudied in a broader framework in (Pattabhiramanand Cercone, 1993).
Apart from salience, it is alsoshown that referring expressions are strongly influ-enced by other aspects of human preference.
For ex-ample, easily perceivable attributes and basic-levelattributes values are preferred (Dale and Haddock,1991; Dale, 1992; Reiter and Dale, 1992).In all discourse-based theories, the update of thefocus status is tightly coupled to the factoring ofthe flux of text into segments.
With the segmenta-tion problem settled in section 3, the DRCC modulemakes reference choices following a discourse theoryadapted from Reichman (Reichman, 1985).
Basedon empirical data, Reichman argues that the choiceof referring expressions is constrained both by thestatus of the discourse space and by the object'slevel of focus within this space.
In her theory, thereare seven status assignments a discourse space mayhave.
Within a discourse space, four levels of focuscan be assigned to individual objects: high, medium,low, or zero, since there are four major ways of re-ferring to an object using English, namely, by usinga pronoun, by name, by a description, or implicitly.Our theory uses the notions of structural closenessand textual closeness, and takes both of these factorsinto account for argumentative discourse.5.1.1 S t ruc tura l  C losenessThe structural closeness of a reason reflects theforeground and background character of the inner-most attentional space containing it.
Reasons thatmay still remain in the focus of attention at the cur-rent point from the structural perspective are con-sidered as structurally close.
Otherwise they areconsidered as structurally distant.
If a reason, forinstance, is last mentioned or proved in the activeattentional space (the subproof which a reader issupposed to concentrate on), it is likely that thisreason still remains in his focus of attention.
In con-trast, if a reason is in a closed subproof, but is notits conclusion, it is likely that the reason has alreadybeen moved out of the reader's focus of attention.Although finer differentiation may be needed, ourtheory only distinguishes between reasons residingin attentional spaces that are structurally close orstructurally distant.
DRCC assigns the structuralstatus by applying the following rules.1.
Reasons in the active attentional space are struc-turally close.2.
Reasons in the controlling attentional space arestructurally close.3.
Reasons in closed attentional spaces:(a) reasons that are the root of a closed attentionalspace immediate subordinate to the active at-tentional space are structurally close.
(b) Other reasons in a closed attentional spac e arestructurally distant.4.
Reasons in precontrol attentional spaces are struc-turally distant.Note that the rules are specified with respect tothe innermost proof unit containing a proof node.Rule 3 means that only the conclusions of closedsubordinated subproofs till remain in the reader'sfocus of attention.5.1.2 Textua l  C losenessThe textual closeness is used as a measure of thelevel of focus of an individual reason.
In general,the level of focus of an object is established whenit is activated, and decreases with the flow of dis-course.
In Reichman's theory, although four levelsof focus can be established upon activation, only oneis used in the formulation of the four reference rules.In other words, it suffices to track the status highalone.
Therefore, we use only two values to denotethe level of focus of individual intermediate conclu-sions, which is calculated from textual distance be-tween the last mentioning of a reason and the currentsentence where the reason is referred to.5.1.3 Reference  Ru lesWe assume that each intermediate conclusion isput into high focus when it is presented as a newlyderived conclusion or cited as a reason supportingthe derivation of another intermediate result.
Thislevel of focus decreases, either when a attentionalspace is moved out of the foreground of discussion,or with the increase of textual distance.
The DRCCcomponent of PRO VERB models this behavior withthe following four reference rules.Refer r ing  Express ions  for  Reasons1.
If a reason is both structurally and textually close,it will be omitted.2.
If a reason is structurally close but textually dis-tant, first try to find an implicit form; if impossi-ble, use an explicit form.3.
If a reason is structurally distant but textuallyclose, first try to find an implicit form; if impossi-ble, omit it.4.
An explicit form will be used for reasons that areboth structurally and textually far.Note that the result of applying rule 2 and rule3 depends on the availability of an implicit form,which often interacts with the verbalization of therest of a PCA, in particular with that of the inferencemethod.
Since the reference choice for methods ishandled independent of the discourse segmentation(Huang, 1996), however, it is not discussed in thispaper.Fourteen PCAs are generated by the macroplannerof PROVERB for our example in Figure 3.
The195microplanner and the realizer of PROVERB finallyproduces:Proof:Let F be a group, U be a subgroup of F, 1and 1u be unit elements of F and U, respec-tively.
According to the definition of unit ele-ment, 1v E U.
Therefore there is an X, X E U.Now suppose that u is such an X. Accordingto the definition of unit element, u ?
ltr = u.Since U is a subgroup of F, U C F. Thereforelv E F. Similarly u E F, since u E U.
Since Fis a group, F is a semigroup.
Because u*lv -= u,1v is a solution of the equation u * X --= u.Since 1 is a unit element of F, u* 1 = u.
Since 1is a unit element of F, 1 E F. Because u E F, 1is a solution of the equation u* X = u.
Since Fis a group, 1v = 1 by the uniqueness of solution.Some explanations are in order.
PROVERB'smicroplanner cuts the entire text into three para-graphs, basically mirroring the larger attentionalspaces U3, U5 and U6 in Figure 4.
Since nodes 22and 21 are omitted in this verbalization, node 20(the last sentence) is merged into the paragraph forU6.Let's examine the reference choices in the secondlast sentence:Because u E F, 1 is a solution of the equationwhich is actually line 19 in Figure 3 and node 19in Figure 4.
Among the four reason nodes 13, 17,18, 15, only node 13 is explicitly mentioned, sinceit is in a closed attentional space (U5) and is men-tioned five sentences ago.
Node 17 and 18 are in thecurrent space (U6) and was activated only one ortwo sentence ago, they are therefore omitted.
Node15 is also omitted although also in the same closedspace U5, but it was mentioned one sentence afternode 13 and is considered as near concerning textualdistance.6 Conc lus ionThis paper describes the way in which PROVERBrefers to previouslyderived results while verbalizingmachine-found proofs.
By distinguishing betweenhierarchical planning and focus-guided navigation,PROVERB achieves a natural segmentation f con-text into an attentional hierarchy.
Based on thissegmentation, PRO VERB makes reference decisionsaccording to a discourse theory adapted from Reich-man for this special application.PROVERB works in a fully automatic way.
Theoutput texts are close to detailed proofs in text-books and are basically accepted by the communityof automated reasoning.
With the increasing size ofproofs which PROVERB is getting as input, inves-tigation is needed both for longer proofs as well asfor more concise styles.Although developed for a specific application, webelieve the main rationales behind of our system ar-chitecture are useful for natural anguage generationin general.
Concerning segmentation f discourse, anatural segmentation can be easily achieved if wecould distinguish between language generation ac-tivities affecting global structure of attention andthose only moving the local focus.
We believe aglobal attentional hierarchy plays a crucial role inchoosing reference xpressions beyond this particu-lar domain of application.
Furthermore, it turnedout to be also important for other generation deci-sions, such as paragraph scoping and layout.
Finally,the combination of hierarchical planning with localnavigation eeds more research as a topic in its ownright.
For many applications, these two techniquesare a complementary pair.AcknowledgmentSincere thanks are due to all three anonymous re-viewers of ACL/EACL'97, who provided valuablecomments and constructive suggestions.
I would liketo thank Graeme Hirst as well, who carefully readthe final version of this paper.ReferencesChester, Daniel.
1976.
The translation of formalproofs into English.
Artificial Intelligence, 7:178-216.Dale, Robert.
1992.
Generating Referring Expres-sions.
ACL-MIT PressSeries in Natural LanguageProcessing.
MIT Press.Dale, Robert and Nicholas Haddock.
1991.
Con-tent determination i the generation of referringexpressions.
Computational Intelligence, 7(4).Edgar, Andrew and Francis Jeffry Pelletier.
1993.Natural language xplanation of natural deduc-tion proofs.
In Proc.
of the first Conference of thePacific Association for Computational Linguistics,Vancouver, Canada.
Centre for Systems Science,Simon Fraser University.Gentzen, Gerhard.
1935.
Untersuchungen fiber daslogische SchlieBen I.
Math.
Zeitschrift, 39:176-210.Grosz, Barbara J. and Candace L. Sidner.
1986.
At-tention, intentions, and the structure of discourse.Computational Linguistics, 12(3):175-204.Hovy, Eduard H. 1988.
Generating Natural Lan-guage under Pragmatic Constraints.
LawrenceErlbaum Associates, Hillsdale, New Jersey.Huang, Xiaorong.
1990.
Reference choices in math-ematical proofs.
In L. C. Aiello, editor, Proc.
of1969th European Conference on Artificial Intelligence,pages 720-725.
Pitman Publishing.Huang, Xiaorong.
1994.
Planning argumentativetexts.
In Proc.
of COLING-94, pages 329-333,Kyoto, Japan.Huang, Xiaorong.
1996.
Human Oriented ProofPresentation: A Reconstructive Approach.
Infix,Sankt Augustin.Huang, Xiaorong and Armin Fiedler 1997.
ProofVerbalization as an Application of NLG.
In Proc.of IJCA1-97, Nagoya, Japan, forthcoming.Kilger, Anne and Wolfgang Finkler.
1995.
Incre-mental generation for real-time applications.
Re-search Report RR-95-11, DFKI, Saarbriicken, Ger-many.McDonald, David D. 1983.
Natural anguage gen-eration as a computational problem.
In Bradyand Berwick: Computational Models of Discourse.MIT Press.McKeown, Kathleen.
1985.
Text Generation.
Cam-bridge University Press, Cambridge, UK.Moore, Johanna and C6cile Paris.
1989.
Plan-ning text for advisory dialogues.
In Proc.
27thAnnual Meeting of the Association for Compu-tational Linguistics, pages 203-211, Vancouver,British Columbia.Ochs, Elinor.
1979.
Planned and unplanned is-course.
Syntax and Semantics, 12:51-80.Paris, C~cile.
1988.
Tailoring object descriptions toa user's level of expertise.
Computational Linguis-tics, 14:64-78.Pattabhiraman, T. and Nick Cercone.
1993.Decision-theoreticsalience int ractions in lan-guage generation.
In Ruzena Bajcsy, editor,Proc.
of IJCAI-93, volume 2, pages 1246-1252,Chamb~ry, France.
Morgan Kaufmann.Reichman, Rachel.
1985.
Getting Computers to TalkLike You and Me.
Discourse Context, Focus, andSemantics.
MIT Press.Reiter, Ehud and Robert Dale.
1992.
A fast algo-rithm for the generation of referring expressions.In Proc.
of COLING-92, volume 1, pages 232-238.Reithinger, Norbert.
1991.
Eine parallele Architek-tur zur inkrementellen Generierung multimodalerDialogbeitriige.
Ph.D. thesis, Universit~t des Saar-landes.
Also available as book, Infix, Sankt Au-gustin, 1991.Sibun, Penelope.
1990.
The local organization oftext.
In K. McKeown, J. Moore, and S. Niren-burg, editors, Proc.
of the fifth international nat-ural language generation workshop, pages 120-127,Dawson, Pennsylvania.197
