Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1500?1511,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsStructured Sparsity in Structured PredictionAndre?
F. T.
Martins??
Noah A. Smith?
Pedro M. Q. Aguiar?
Ma?rio A. T.
Figueiredo?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, Lisboa, Portugal?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.ptAbstractLinear models have enjoyed great success instructured prediction in NLP.
While a lot ofprogress has been made on efficient train-ing with several loss functions, the problemof endowing learners with a mechanism forfeature selection is still unsolved.
Commonapproaches employ ad hoc filtering or L1-regularization; both ignore the structure of thefeature space, preventing practicioners fromencoding structural prior knowledge.
We fillthis gap by adopting regularizers that promotestructured sparsity, along with efficient algo-rithms to handle them.
Experiments on threetasks (chunking, entity recognition, and de-pendency parsing) show gains in performance,compactness, and model interpretability.1 IntroductionModels for structured outputs are in demand acrossnatural language processing, with applications in in-formation extraction, parsing, and machine transla-tion.
State-of-the-art models usually involve linearcombinations of features and are trained discrim-inatively; examples are conditional random fields(Lafferty et al, 2001), structured support vectormachines (Altun et al, 2003; Taskar et al, 2003;Tsochantaridis et al, 2004), and the structured per-ceptron (Collins, 2002a).
In all these cases, the un-derlying optimization problems differ only in thechoice of loss function; choosing among them hasusually a small impact on predictive performance.In this paper, we are concerned with model se-lection: which features should be used to define theprediction score?
The fact that models with fewfeatures (?sparse?
models) are desirable for severalreasons (compactness, interpretability, good gener-alization) has stimulated much research work whichhas produced a wide variety of methods (Della Pietraet al, 1997; Guyon and Elisseeff, 2003; McCallum,2003).
Our focus is on methods which embed thisselection into the learning problem via the regular-ization term.
We depart from previous approachesin that we seek to make decisions jointly about allcandidate features, and we want to promote sparsitypatterns that go beyond the mere cardinality of theset of features.
For example, we want to be able toselect entire feature templates (rather than featuresindividually), or to make the inclusion of some fea-tures depend on the inclusion of other features.We achieve the goal stated above by employ-ing regularizers which promote structured sparsity.Such regularizers are able to encode prior knowl-edge and guide the selection of features by model-ing the structure of the feature space.
Lately, thistype of regularizers has received a lot of attentionin computer vision, signal processing, and compu-tational biology (Zhao et al, 2009; Kim and Xing,2010; Jenatton et al, 2009; Obozinski et al, 2010;Jenatton et al, 2010; Bach et al, 2011).
Eisensteinet al (2011) employed structured sparsity in com-putational sociolinguistics.
However, none of theseworks have addressed structured prediction.
Here,we combine these two levels of structure: struc-ture in the output space, and structure in the featurespace.
The result is a framework that allows build-ing structured predictors with high predictive power,while reducing manual feature engineering.
We ob-tain models that are interpretable, accurate, and of-ten much more compact than L2-regularized ones.Compared with L1-regularized models, ours are of-ten more accurate and yield faster runtime.15002 Structured PredictionWe address structured prediction problems, whichinvolve an input set X (e.g., sentences) and an out-put set Y, assumed large and structured (e.g., tags orparse trees).
We assume that each x ?
X has a setof candidate outputs Y(x) ?
Y.
We consider linearmodels, in which predictions are made according toy?
= arg maxy?Y(x) ?
?
?
(x, y), (1)where ?
(x, y) ?
RD is a vector of features, and ?
?RD is the vector of corresponding weights.
Let D ={?xi, yi?
}Ni=1 be a training sample.
We assume a costfunction is defined such that c(y?, y) is the cost ofpredicting y?
when the true output is y; our goal is tolearn ?
with small expected cost on unseen data.
Toachieve this goal, linear models are usually trainedby solving a problem of the form??
= arg min?
?(?)
+ 1N?Ni=1 L(?, xi, yi), (2)where ?
is a regularizer and L is a loss function.Examples of losses are: the negative conditional log-likelihood used in CRFs (Lafferty et al, 2001),LCRF(?, x, y) = ?
logP?
(y|x), (3)where P?
(y|x) ?
exp(?
?
?
(x, y)) is a log-linearmodel; the margin rescaled loss of structured SVMs(Taskar et al, 2003; Tsochantaridis et al, 2004),LSVM(?, x, y) = maxy??Y(x)?
?
??(y?)
+ c(y?, y), (4)where ??(y?)
= ?
(x, y?)??
(x, y); and the loss un-derlying the structured perceptron (Collins, 2002a),LSP(?, x, y) = maxy?
?Y(x) ?
?
??(y?).
(5)Empirical comparison among these loss functionscan be found in the literature (see, e.g., Martins et al,2010, who also consider interpolations of the lossesabove).
In practice, it has been observed that thechoice of loss has far less impact than the model de-sign and choice of features.
Hence, in this paper,we focus our attention on the regularization term inEq.
2.
We specifically address ways in which thisterm can be used to help design the model by pro-moting structured sparsity.
While this has been atopic of intense research in signal processing andcomputational biology (Jenatton et al, 2009; Liuand Ye, 2010; Bach et al, 2011), it has not yet re-ceived much attention in the NLP community, wherethe choice of regularization for supervised learninghas essentially been limited to the following:?
L2-regularization (Chen and Rosenfeld, 2000):?L2?
(?)
, ?2??
?22 = ?2?Dd=1 ?2d; (6)?
L1-regularization (Kazama and Tsujii, 2003;Goodman, 2004):?L1?
(?)
, ???
?1 = ?
?Dd=1 |?d|.
(7)The latter is known as ?Lasso,?
as popularized byTibshirani (1996) in the context of sparse regres-sion.
In the two cases above, ?
and ?
are nonneg-ative coefficients controlling the intensity of the reg-ularization.
?L2?
usually leads to easier optimizationand robust performance; ?L1?
encourages sparsermodels, where only a few features receive nonzeroweights; see Gao et al (2007) for an empirical com-parison.
More recently, Petrov and Klein (2008b)applied L1 regularization for structure learning inphrase-based parsing; a comparison with L2 appearsin Petrov and Klein (2008a).
Elastic nets interpolatebetween L1 and L2, having been proposed by Zouand Hastie (2005) and used by Lavergne et al (2010)to regularize CRFs.Neither of the regularizers just described ?looks?at the structure of the feature space, since they alltreat each dimension independently?we call themunstructured regularizers, as opposed to the struc-tured ones that we next describe.3 Structured SparsityWe are interested in regularizers that share with ?L1?the ability to promote sparsity, so that they can beused for selecting features.
In addition, we want toendow the feature space RD with additional struc-ture, so that features are not penalized individually(as in the L1-case) but collectively, encouraging en-tire groups of features to be discarded.
The choice ofgroups will allow encoding prior knowledge regard-ing the kind of sparsity patterns that are intended inthe model.
This can be achieved with group-Lassoregularization, which we next describe.15013.1 The Group LassoTo capture the structure of the feature space, wegroup our D features into M groups G1, .
.
.
, GM ,where each Gm ?
{1, .
.
.
, D}.
Ahead, we dis-cuss meaningful ways of choosing group decompo-sitions; for now, let us assume a sensible choice isobvious to the model designer.
Denote by ?m =?
?d?d?Gm the subvector of those weights that cor-respond to the features in the m-th group, and letd1, .
.
.
, dM be nonnegative scalars (one per group).We consider the following group-Lasso regularizers:?GLd =?Mm=1 dm??m?2.
(8)These regularizers were first proposed by Bakin(1999) and Yuan and Lin (2006) in the context of re-gression.
If d1 = .
.
.
= dM , ?GLd becomes the ?L1norm of the L2 norms.?
Interestingly, this is alsoa norm, called the mixed L2,1-norm.1 These regu-larizers subsume the L1 and L2 cases, which corre-spond to trivial choices of groups:?
If each group is a singleton, i.e., M = D andGd = {?d}, and d1 = .
.
.
= dM = ?
, we recoverL1-regularization (cf.
Eqs.
7?8).?
If there is a single group spanning all the features,i.e., M = 1 and G1 = {1, .
.
.
, D}, then the righthand side of Eq.
8 becomes d1???2.
This is equiv-alent to L2 regularization.2We next present some non-trivial examples con-cerning different topologies of G = {G1, .
.
.
, GM}.Non-overlapping groups.
Let us first considerthe case where G is a partition of the featurespace: the groups cover all the features (?mGm ={1, .
.
.
, D}), and they do not overlap (Ga?Gb = ?,?a 6= b).
Then, ?GLd is termed a non-overlappinggroup-Lasso regularizer.
It encourages sparsity pat-terns in which entire groups are discarded.
A ju-dicious choice of groups can lead to very compact1In the statistics literature, such mixed-norm regularizers,which group features and then apply a separate norm for eachgroup, are called composite absolute penalties (Zhao et al,2009); other norms besides L2,1 can be used, such as L?,1(Quattoni et al, 2009; Wright et al, 2009; Eisenstein et al,2011).2Note that Eqs.
8 and 6 do not become exactly the same: inEq.
6, the L2 norm is squared.
However it can be shown thatboth regularizers lead to identical learning problems (Eq.
2) upto a transformation of the regularization constant.models and pinpoint relevant groups of features.The following examples lie in this category:?
The two cases above (L1 and L2 regularization).?
Label-based groups.
In multi-label classification,where Y = {1, .
.
.
, L}, features are typically de-signed as conjunctions of input features with la-bel indicators, i.e., they take the form ?
(x, y) =?(x)?
ey, where ?
(x) ?
RDX , ey ?
RL has allentries zero except the y-th entry, which is 1, and?
denotes the Kronecker product.
Hence ?
(x, y)can be reshaped as aDX -by-Lmatrix, and we canlet each group correspond to a row.
In this case,all groups have the same size and we typically setd1 = .
.
.
= dM .
A similar design can be madefor sequence labeling problems, by considering asimilar grouping for the unigram features.3?
Template-based groups.
In NLP, features are com-monly designed via templates.
For example, atemplate such as w0 ?
p0 ?
p?1 denotes the wordin the current position (w0) conjoined with itspart-of-speech (p0) and that of the previous word(p?1).
This template encloses many features cor-responding to different instantiantions of w0, p0,and p?1.
In ?5, we learn feature templates fromthe data, by associating each group to a featuretemplate, and letting that group contain all fea-tures that are instantiations of this template.
Sincegroups have different sizes, it is a good idea tolet dm increase with the group size, so that largergroups pay a larger penalty for being included.Tree-structured groups.
More generally, we maylet the groups in G overlap but be nested, i.e., we maywant them to form a hierarchy (two distinct groupseither have empty intersection or one is contained inthe other).
This induces a partial order on G (the setinclusion relation ?
), endowing it with the structureof a partially ordered set (poset).A convenient graphical representation of the poset?G,??
is its Hasse diagram.
Each group is a nodein the diagram, and an arc is drawn from group Gato group Gb if Gb ?
Ga and there is no b?
s.t.Gb ?
Gb?
?
Ga.
When the groups are nested, thisdiagram is a forest (a union of directed trees).
Thecorresponding regularizer enforces sparsity patterns3The same idea is also used in multitask learning, wherelabels correspond to tasks (Caruana, 1997).1502where a group of features is only selected if all itsancestors are also selected.4 Hence, entire subtreesin the diagram can be pruned away.
Examples are:?
The elastic net.
The diagram of G has a root nodefor G1 = {1, .
.
.
, D} and D leaf nodes, one pereach singleton group (see Fig.
1).?
The sparse group-Lasso.
This regularizer wasproposed by Friedman et al (2010):?SGLd,?
(?)
=?M ?m=1 (dm?
?m?2 + ?m?
?m?1) ,(9)where the total number of groups is M = M ?
+D, and the components ?1, .
.
.
,?M ?
are non-overlapping.
This regularizer promotes sparsityat both group and feature levels (i.e., it eliminatesentire groups and sparsifies within each group).Graph-structured groups.
In general, the groupsin G may overlap without being nested.
In this case,the Hasse diagram of G is a directed acyclic graph(DAG).
As in the tree-structured case, a group offeatures is only selected if all its ancestors are alsoselected.
Based on this property, Jenatton et al(2009) suggested a way of reverse engineering thegroups from the desired sparsity pattern.
We nextdescribe a strategy for coarse-to-fine feature tem-plate selection that directly builds on that idea.Suppose that we are given M feature templatesT = {T1, .
.
.
, TM} which are partially ordered ac-cording to some criterion, such that if Ta  Tb wewould like to include Tb in our model only if Tais also included.
This criterion could be a measureof coarseness: we may want to let coarser part-of-speech features precede finer lexical features, e.g.,p0 ?
p1  w0 ?
w1, or conjoined features come af-ter their elementary parts, e.g., p0  p0 ?
p1.
Theorder does not need to be total, so some templatesmay not be comparable (e.g., we may want p0 ?
p?1and p0 ?
p1 not to be comparable).
To achievethe sparsity pattern encoded in ?T,?, we chooseG = ?G1, .
.
.
, GM ?
as follows: let I(Ta) be theset of features that are instantiations of template Ta;then define Ga = ?b:ab I(Tb), for a = 1, .
.
.
,M .It is easy to see that ?G,??
and ?T,?
are isomorphposets (their Hasse diagrams have the same shape;4We say that a group of features Gm is selected if some fea-ture in Gm (but not necessarily all) has a nonzero weight.see Fig.
1).
The result is a ?coarse-to-fine?
regular-izer, which prefers to select feature templates thatare coarser before zooming into finer features.3.2 Bayesian InterpretationThe prior knowledge encoded in the group-Lassoregularizer (Eq.
8) comes with a Bayesian inter-pretation, as we next describe.
In a probabilisticmodel (e.g.
in the CRF case, where L = LCRF),the optimization problem in Eq.
2 can be seen asmaximum a posteriori estimation of ?, where theregularization term ?(?)
corresponds to the neg-ative log of a prior distribution (call it p(?)).
Itis well-known that L2-regularization corresponds tochoosing independent zero-mean Gaussian priors,?d ?
N(0, ?
?1), and that L1-regularization resultsfrom adopting zero-mean Laplacian priors, p(?d) ?exp(?
|?d|).Figueiredo (2002) provided an alternative inter-pretation of L1-regularization in terms of a two-level hierarchical Bayes model, which happens togeneralize to the non-overlapping group-Lasso case,where ?
= ?GLd .
As in the L2-case, we also assumethat each parameter receives a zero-mean Gaussianprior, but now with a group-specific variance ?m,i.e., ?m ?
N(0, ?mI) for m = 1, .
.
.
,M .
Thisreflects the fact that some groups should have theirfeature weights shrunk more towards zero than oth-ers.
The variances ?m ?
0 are not pre-specified butrather generated by a one-sided exponential hyper-prior p(?m|dm) ?
exp(?d2m?m/2).
It can be shownthat after marginalizing out ?m, we obtainp(?m|dm) =?
?0p(?m|?m)p(?m|dm)d?m?
exp (?dm??m?)
.
(10)Hence, the non-overlapping group-Lasso corre-sponds to the following two-level hierachical Bayesmodel: independently for each m = 1, .
.
.
,M ,?m ?
Exp(d2m/2), ?m ?
N(0, ?mI).
(11)3.3 Prox-operatorsBefore introducing our learning algorithm for han-dling group-Lasso regularization, we need to definethe concept of a ?-proximity operator.
This is thefunction prox?
: RD ?
RD defined as follows:prox?(?)
= arg min??
12???
?
?
?2 + ?(??).
(12)1503Figure 1: Hasse diagrams of several group-based regularizers.
For all tree-structuredcases, we use the same plate notation thatis traditionally used in probabilistic graphicalmodels.
The rightmost diagram represents acoarse-to-fine regularizer: each node is a tem-plate involving contiguous sequences of words(w) and POS tags (p); the symbol order ?
p  w induces a template order (Ta  Tbiff at each position i [Ta]i  [Tb]i).
Digitsbelow each node are the group indices whereeach template belongs.Proximity operators generalize Euclidean projec-tions and have many interesting properties; see Bachet al (2011) for an overview.
By requiring zero to bea subgradient of the objective function in Eq.
12, weobtain the following closed expression (called soft-thresholding) for the ?L1?
-proximity operator:[prox?L1?
(?
)]d =???
?d ?
?
if ?d > ?0 if |?d| ?
?
?d + ?
if ?d < ??
.
(13)For the non-overlapping group Lasso case, the prox-imity operator is given by[prox?GLd (?
)]m ={0 if ?
?m?2 ?
dm??m?2?dm?
?m?2 ?m otherwise.
(14)which can be seen as a generalization of Eq.
13: ifthe L2-norm of the m-th group is less than dm, theentire group is discarded; otherwise it is scaled sothat its L2-norm decreases by an amount of dm.When groups overlap, the proximity operatorlacks a closed form.
When G is tree-structured, itcan still be efficiently computed by a recursive pro-cedure (Jenatton et al, 2010).
When G is not tree-structured, no specialized procedure is known, and aconvex optimizer is necessary to solve Eq.
12.4 Online Prox-Grad AlgorithmWe now turn our attention to efficient ways of han-dling group-Lasso regularizers.
Several fast andscalable algorithms having been proposed for train-ing L1-regularized CRFs, based on quasi-Newtonoptimization (Andrew and Gao, 2007), coordinatedescent (Sokolovska et al, 2010; Lavergne et al,2010), and stochastic gradients (Carpenter, 2008;Langford et al, 2009; Tsuruoka et al, 2009).
Thealgorithm that we use in this paper (Alg.
1) extendsthe stochastic gradient methods for group-Lasso reg-ularization; a similar algorithm was used by Martinset al (2011) for multiple kernel learning.Alg.
1 addresses the learning problem in Eq.
2 byalternating between online (sub-)gradient steps withrespect to the loss term, and proximal steps withrespect to the regularizer.
Proximal-gradient meth-ods are very popular in sparse modeling, both inbatch (Liu and Ye, 2010; Bach et al, 2011) and on-line (Duchi and Singer, 2009; Xiao, 2009) settings.The reason we have chosen the algorithm of Martinset al (2011) is that it effectively handles overlap-ping groups, without the need of evaluating prox?
(which, as seen in ?3.3, can be costly if G is not tree-structured).
To do so, it decomposes ?
as?(?)
= ?Jj=1 ?j?j(?)
(15)for some J ?
1, and nonnegative ?1, .
.
.
, ?J ; each?j-proximal operator is assumed easy to compute.Such a decomposition always exists: if G does nothave overlapping groups, take J = 1.
Otherwise,find J ?
M disjoint sets G1, .
.
.
,GJ such that?Jj=1 Gj = G and the groups on each Gj are non-overlapping.
The proximal steps are then appliedsequentially, one per each ?j .
Overall, Alg.
1 satis-fies the following important requirements:?
Computational efficiency.
Each gradient step atround t is linear in the number of features thatfire for that instance and independent of the totalnumber of features D. Each proximal step is lin-ear in the number of groupsM , and does not needbe to performed every round (as we will see later).1504Algorithm 1 Online Sparse Prox-Grad Algorithm1: input: D, ?
?j?Jj=1, T , gravity sequence??
?jt?Jj=1?Tt=1, stepsize sequence ?
?t?Tt=12: initialize ?
= 03: for t = 1 to T do4: take training pair ?xt, yt?
?
D5: ?
?
?
?
?t?L(?
;xt, yt) (gradient step)6: for j = 1 to J do7: ?
= prox?t?jt?j (?)
(proximal step)8: end for9: end for10: output: ??
Memory efficiency.
Only a small active set of fea-tures (those that have nonzero weights) need tobe maintained.
Entire groups of features can bedeleted after each proximal step.
Furthermore,only the features which correspond to nonzero en-tries in the gradient vector need to be inserted inthe active set; for some losses (LSVM and LSP)many irrelevant features are never instantianted.?
Convergence.
With high probability, Alg.
1 pro-duces an -accurate solution after T ?
O(1/2)rounds, for a suitable choice of stepsizes and hold-ing ?jt constant, ?jt = ?j (Martins et al, 2011).This result can be generalized to any sequence?
?jt?Tt=1 such that ?j = 1T?Tt=1 ?jt.We next describe several algorithmic ingredientsthat make Alg.
1 effective in sparse modeling.Budget-Driven Shrinkage.
Alg.
1 requires thechoice of a ?gravity sequence.?
We follow Lang-ford et al (2009) and set ?
?jt?Jj=1 to zero for all twhich is not a multiple of some prespecified integerK; this way, proximal steps need only be performedeachK rounds, yielding a significant speed-up whenthe number of groups M is large.
A direct adop-tion of the method of Langford et al (2009) wouldset ?jt = K?j for those rounds; however, we haveobserved that such a strategy makes the number ofgroups vary substantially in early epochs.
We use adifferent strategy: for each Gj , we specify a budgetof Bj ?
0 groups (this may take into considerationpractical limitations, such as the available memory).If t is a multiple of K, we set ?jt as follows:1.
If Gj does not have more than Bj nonzerogroups, set ?jt = 0 and do nothing.2.
Otherwise, sort the groups in Gj by decreasingorder of their L2-norms.
Check the L2-normsof the Bj-th and Bj+1-th entries in the list andset ?jt as the mean of these two divided by ?t.3.
Apply a ?t?jt?j-proximal step using Eq.
14.At the end of this step, no more than Bj groupswill remain nonzero.5If the average of the gravity steps converge,limT??
1T?Tt=1 ?jt ?
?j , then the limit points?j implicitly define the regularizer, via ?
=?Jj=1 ?j?j .6 Hence, we have shifted the control ofthe amount of regularization to the budget constantsBj , which unlike the ?j have a clear meaning andcan be chosen under practical considerations.Space and Time Efficiency.
The proximal stepsin Alg.
1 have a scaling effect on each group, whichaffects all features belonging to that group (seeEq.
14).
We want to avoid explicitly updating eachfeature in the active set, which could be time con-suming.
We mention two strategies that can be usedfor the non-overlapping group Lasso case.?
The first strategy is suitable when M is large andonly a few groups ( M ) have features that firein each round; this is the case, e.g., of label-basedgroups (see ?3.1).
It consists of making lazy up-dates (Carpenter, 2008), i.e., to delay the updateof all features in a group until at least one ofthem fires; then apply a cumulative penalty.
Theamount of the penalty can be computed if one as-signs a timestamp to each group.?
The second strategy is suitable when M is smalland some groups are very populated; this is thetypical case of template-based groups (?3.1).
Twooperations need to be performed: updating eachfeature weight (in the gradient steps), and scalingentire groups (in the proximal steps).
We adapta trick due to Shalev-Shwartz et al (2007): repre-sent the weight vector of them-th group, ?m, by a5When overlaps exist (e.g.
the coarse-to-fine case), we spec-ify a total pseudo-budget B ignoring the overlaps, which in-duces budgets B1, .
.
.
, BJ which sum to B.
The number ofactually selected groups may be less than B, however, since inthis case some groups can be shrunk more than once.
Otherheuristics are possible.6The convergence assumption can be sidestepped by freez-ing the ?j after a fixed number of iterations.1505triple ?
?m, cm, ?m?
?
R|Gm|?R+?R+, such that?m = cm?m and ?
?m?2 = ?m.
This representa-tion allows performing the two operations abovein constant time, and it keeps track of the groupL2-norms, necessary in the proximal updates.For sufficient amounts of regularization, our al-gorithm has a low memory footprint.
Only featuresthat, at some point, intervene in the gradient com-puted in line 5 need to be instantiated; and all fea-tures that receive zero weights after some proximalstep can be deleted from the model (cf.
Fig.
2).Sparseptron and Debiasing.
Although Alg.
1 al-lows to simultaneously select features and learn themodel parameters, it has been observed in the sparsemodeling literature that Lasso-like regularizers usu-ally have a strong bias which may harm predictiveperformance.
A post-processing stage is usuallytaken (called debiasing), in which the model is re-fitted without any regularization and using only theselected features (Wright et al, 2009).
If a final de-biasing stage is to be performed, Alg.
1 only needsto worry about feature selection, hence it is appeal-ing to choose a loss function that makes this pro-cedure as simple as possible.
Examining the inputof Alg.
1, we see that both a gravity and a stepsizesequence need to be specified.
The former can betaken care of by using budget-driven shrinkage, asdescribed above.
The stepsize sequence can be setas ?t = ?0/?dt/Ne, which ensures convergence,however ?0 requires tuning.
Fortunately, for thestructured perceptron loss LSP (Eq.
5), Alg.
1 is in-dependent of ?0, up to a scaling of ?, which does notaffect predictions (see Eq.
1).7 We call the instanti-ation of Alg.
1 with a group-Lasso regularizer andthe loss LSP the sparseptron.
Overall, we proposethe following two-stage approach:1.
Run the sparsepton for a few epochs and dis-card the features with zero weights.2.
Refit the model without any regularization andusing the loss L which one wants to optimize.7To see why this is the case, note that both gradient andproximal updates come scaled by ?0; and that the gradient ofthe loss is?LSP(?, xt, yt) = ?
(xt, y?t)?
?
(xt, yt), where y?tis the prediction under the current model, which is insensitive tothe scaling of ?.
This independence on ?0 does not hold whenthe loss is LSVM or LCRF.5 ExperimentsWe present experiments in three structured predic-tion tasks for several group choices.Text Chunking.
We use the English dataset pro-vided in the CoNLL 2000 shared task (Sang andBuchholz, 2000), which consists of 8,936 trainingand 2,012 testing sentences (sections 15?18 and 20of the WSJ.)
The input observations are the tokenwords and their POS tags; we want to predict thesequences of IOB tags representing phrase chunks.We built 96 contextual feature templates as follows:?
Up to 5-grams of POS tags, in windows of 5 to-kens on the left and 5 tokens on the right;?
Up to 3-grams of words, in windows of 3 tokenson the left and 3 tokens on the right;?
Up to 2-grams of word shapes, in windows of2 tokens on the left and 2 tokens on the right.Each shape replaces characters by their types(case sensitive letters, digits, and punctuation),and deletes repeated types?e.g., Confidenceand 2,664,098 are respectively mapped to Aaand 0,0+,0+ (Collins, 2002b).We defined unigram features by conjoining thesetemplates with each of the 22 output labels.
An ad-ditional template was defined to account for labelbigrams?features in this template do not look at theinput string, but only at consecutive pairs of labels.8We evaluate the ability of group-Lasso regular-ization to perform feature template selection.
Todo that, we ran 5 epochs of the sparseptron algo-rithm with template-based groups and budget-drivenshrinkage (budgets of 10, 20, 30, 40, and 50 tem-plates were tried).
For each group Gm, we set dm =log2 |Gm|, which is the average number of bits nec-essary to encode a feature in that group, if all fea-tures were equiprobable.
We set K = 1000 (thenumber of instances between consecutive proximalsteps).
Then, we refit the model with 10 iterationsof the max-loss 1-best MIRA algorithm (Crammeret al, 2006).9 Table 1 compares the F1 scores and8State-of-the-art models use larger output contexts, such aslabel trigrams and 4-grams.
We resort to bigram labels as weare mostly interested in identifying relevant unigram templates.9This variant optimizes theLSVM loss (Martins et al, 2010).For the refitting, we used unregularized MIRA.
For the baseline1506Table 1: Results fortext chunking.MIRA Group Lasso B = 10 B = 20 B = 30 B = 40 B = 50F1 (%) 93.10 92.99 93.28 93.59 93.42 93.40model size (# features) 5,300,396 71,075 158,844 389,065 662,018 891,378MIRA Lasso C = 0.1 C = 0.5 C = 1 Group-Lasso B = 100 B = 200 B = 300Spa.
dev/test 70.38/74.09 69.19/71.9 70.75/72.38 71.7/74.03 71.79/73.62 72.08/75.05 71.48/73.38,598,246 68,565 1,017,769 1,555,683 83,036 354,872 600,646Dut.
dev/test 69.15/71.54 64.07/66.35 66.82/69.42 70.43/71.89 69.48/72.83 71.03/73.33 71.2/72.595,727,004 164,960 565,704 953,668 128,320 447,193 889,660Eng.
dev/test 83.95/79.81 80.92/76.95 82.58/78.84 83.38/79.35 85.62/80.26 85.86/81.47 85.03/80.918,376,901 232,865 870,587 1,114,016 255,165 953,178 1,719,229Table 2: Results for named entity recognition.
Each cell shows F1 (%) and the number of features.0 5 10 150246 x 106# Epochs# FeaturesMIRASparceptron + MIRA (B=30)Figure 2: Memory footprints of the MIRA and sparsep-tron algorithms in text chunking.
The oscillation in thefirst 5 epochs (bottom line) comes from the proximalsteps each K = 1000 rounds.
The features are thenfrozen and 10 epochs of unregularized MIRA follow.Overall, the sparseptron requires < 7.5% of the memoryas the MIRA baseline.the model sizes obtained with the several budgetsagainst those obtained by running 15 iterations ofMIRA with the original set of features.
Note thatthe total number of iterations is the same; yet, thegroup-Lasso approach has a much smaller memoryfootprint (see Fig.
2) and yields much more com-pact models.
The small memory footprint comesfrom the fact that Alg.
1 may entertain a large num-ber of features without ever instantiating all of them.The predictive power is comparable (although somechoices of budget yield slightly better scores for thegroup-Lasso approach).10Named Entity Recognition.
We experiment withthe Spanish, Dutch, and English datasets pro-vided in the CoNLL 2002/2003 shared tasks (Sang,2002; Sang and De Meulder, 2003).
For Span-ish, we use the POS tags provided by Car-(described next), we used L2-regularized MIRA and tuned theregularization constant with cross-validation.10We also tried label-based group-Lasso and sparse group-Lasso (?3.1), with less impressive results (omitted for space).reras (http://www.lsi.upc.es/?nlp/tools/nerc/nerc.html); for English, we ignore the syn-tactic chunk tags provided with the dataset.
Hence,all datasets have the same sort of input observations(words and POS) and all have 9 output labels.
Weuse the feature templates described above plus someadditional ones (yielding a total of 452 templates):?
Up to 3-grams of shapes, in windows of size 3;?
For prefix/suffix sizes of 1, 2, 3, up to 3-grams ofword prefixes/suffixes, in windows of size 3;?
Up to 5-grams of case, punctuation, and digit in-dicators, in windows of size 5.As before, an additional feature template was de-fined to account for label bigrams.
We do featuretemplate selection (same setting as before) for bud-get sizes of 100, 200, and 300.
We compare withboth MIRA (using all the features) and the sparsep-tron with a standard Lasso regularizer ?L1?
, for sev-eral values of C = 1/(?N).
Table 2 shows the re-sults.
We observe that template-based group-Lassowins both in terms of accuracy and compactness.Note also that the ability to discard feature tem-plates (rather than individual features) yields fastertest runtime than models regularized with the stan-dard Lasso: fewer templates will need to be instan-tiated, with a speed-up in score computation.Multilingual Dependency Parsing.
We trainednon-projective dependency parsers for 6 languagesusing the CoNLL-X shared task datasets (Buchholzand Marsi, 2006): Arabic, Danish, Dutch, Japanese,Slovene, and Spanish.
We chose the languages withthe smallest datasets, because regularization is moreimportant when data is scarce.
The output to be pre-dicted from each input sentence is the set of depen-dency links, which jointly define a spanning tree.15072 4 6 8 10 12x 10676.57777.57878.5Number of FeaturesUAS (%)Arabic0 5 10 15x 1068989.289.489.689.890 Danish0 2 4 6 8x 1069292.59393.5 Japanese0 2 4 6 8 10x 10681828384 Slovene0 0.5 1 1.5 2x 1078282.58383.584 Spanish0 5 10 15x 1067474.57575.576 TurkishGroup?LassoGroup?Lasso (C2F)LassoFilter?based (IG)Figure 3: Comparison between non-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-basedmethod based on information gain for selecting feature templates in multilingual dependency parsing.
The x-axis isthe total number of features at different regularization levels, and the y-axis is the unlabeled attachment score.
Theplots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method.
The standardLasso (which does not select templates, but individual features) is also shown for comparison.We use arc-factored models, for which exact infer-ence is tractable (McDonald et al, 2005).
We de-fined M = 684 feature templates for each candi-date arc by conjoining the words, shapes, lemmas,and POS of the head and the modifier, as well asthe contextual POS, and the distance and directionof attachment.
We followed the same two-stageapproach as before, and compared with a baselinewhich selects feature templates by ranking them ac-cording to the information gain criterion.
This base-line assigns a score to each template Tm which re-flects an empirical estimate of the mutual informa-tion between Tm and the binary variable A that indi-cates the presence/absence of a dependency link:IGm ,?f?Tm?a?
{0,1}P (f, a) log2P (f, a)P (f)P (a) , (16)where P (f, a) is the joint probability of feature ffiring and an arc being active (a = 1) or innactive(a = 0), and P (f) and P (a) are the correspondingmarginals.
All probabilities are estimated from theempirical counts of events observed in the data.The results are plotted in Fig.
3, for budget sizesof 200, 300, and 400.
We observe that for allbut one language (Spanish is the exception), non-overlapping group-Lasso regularization is more ef-fective at selecting feature templates than the in-formation gain criterion, and slightly better thancoarse-to-fine group-Lasso.
For completeness, wealso display the results obtained with a standardLasso regularizer.
Table 3 shows what kind offeature templates were most selected for each lan-guage.
Some interesting patterns can be observed:morphologically-rich languages with small datasets(such as Turkish and Slovene) seem to avoid lexi-cal features, arguably due to potential for overfitting;in Japanese, contextual POS appear to be speciallyrelevant.
It should be noted, however, that someof these patterns may be properties of the datasetsrather than of the languages themselves.6 Related WorkA variant of the online proximal gradient algorithmused in this paper was proposed by Martins et al1508Ara.
Dan.
Jap.
Slo.
Spa.
Tur.Bilexical ++ + +Lex.?
POS + +POS?
Lex.
++ + + + +POS?
POS ++ +Middle POS ++ ++ ++ ++ ++ ++Shape ++ ++ ++ ++Direction + + + + +Distance ++ + + + + +Table 3: Variation of feature templates that were selectedaccross languages.
Each line groups together similar tem-plates, involving lexical, contextual POS, word shape in-formation, as well as attachment direction and length.Empty cells denote that very few or none of the templatesin that category was selected; + denotes that some wereselected; ++ denotes that most or all were selected.
(2011), along with a theoretical analysis.
The fo-cus there, however, was multiple kernel learning,hence overlapping groups were not considered intheir experiments.
Budget-driven shrinkage and thesparseptron are novel techniques, at the best of ourknowledge.
Apart from Martins et al (2011), theonly work we are aware of which combines struc-tured sparsity with structured prediction is Schmidtand Murphy (2010); however, their goal is to pre-dict the structure of graphical models, while weare mostly interested in the structure of the featurespace.
Schmidt and Murphy (2010) used to gener-ative models, while our approach emphasizes dis-criminative learning.Mixed norm regularization has been used for awhile in statistics as a means to promote structuredsparsity.
Group Lasso is due to Bakin (1999) andYuan and Lin (2006), after which a string of variantsand algorithms appeared (Bach, 2008; Zhao et al,2009; Jenatton et al, 2009; Friedman et al, 2010;Obozinski et al, 2010).
The flat (non-overlapping)case has tight links with learning formalisms suchas multiple kernel learning (Lanckriet et al, 2004)and multi-task learning (Caruana, 1997).
The tree-structured case has been addressed by Kim and Xing(2010), Liu and Ye (2010) and Mairal et al (2010),along with L?,1 and L2,1 regularization.
Graph-structured groups are discussed in Jenatton et al(2010), along with a DAG representation.
In NLP,mixed norms have been used recently by Grac?a et al(2009) in posterior regularization, and by Eisensteinet al (2011) in a multi-task regression problem.7 ConclusionsIn this paper, we have explored two levels of struc-ture in NLP problems: structure on the outputs, andstructure on the feature space.
We have shown howthe latter can be useful in model design, through theuse of regularizers which promote structured spar-sity.
We propose an online algorithm with mini-mal memory requirements for exploring large fea-ture spaces.
Our algorithm, which specializes intothe sparseptron, yields a mechanism for selectingentire groups of features.
We apply sparseptronfor selecting feature templates in three structuredprediction tasks, with advantages over filter-basedmethods, L1, and L2 regularization in terms of per-formance, compactness, and model interpretability.AcknowledgmentsWe would like to thank all reviewers for their comments,Eric Xing for helpful discussions, and Slav Petrov for hiscomments on a draft version of this paper.
A. M. was sup-ported by a FCT/ICTI grant through the CMU-PortugalProgram, and also by Priberam.
This work was partiallysupported by the FET programme (EU FP7), under theSIMBAD project (contract 213250).
N. S. was supportedby NSF CAREER IIS-1054319.ReferencesY.
Altun, I. Tsochantaridis, and T. Hofmann.
2003.
Hid-den Markov support vector machines.
In Proc.
ofICML.G.
Andrew and J. Gao.
2007.
Scalable training ofL1-regularized log-linear models.
In Proc.
of ICML.ACM.F.
Bach, R. Jenatton, J. Mairal, and G. Obozinski.
2011.Convex optimization with sparsity-inducing norms.
InOptimization for Machine Learning.
MIT Press.F.
Bach.
2008.
Exploring large feature spaces with hier-archical multiple kernel learning.
NIPS, 21.S.
Bakin.
1999.
Adaptive regression and model selec-tion in data mining problems.
Ph.D. thesis, AustralianNational University.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL.B.
Carpenter.
2008.
Lazy sparse stochastic gradient de-scent for regularized multinomial logistic regression.Technical report, Technical report, Alias-i.R.
Caruana.
1997.
Multitask learning.
Machine Learn-ing, 28(1):41?75.1509S.
F. Chen and R. Rosenfeld.
2000.
A survey ofsmoothing techniques for maximum entropy models.IEEE Transactions on Speech and Audio Processing,8(1):37?50.M.
Collins.
2002a.
Discriminative training methods forhidden Markov models: theory and experiments withperceptron algorithms.
In Proc.
of EMNLP.M.
Collins.
2002b.
Ranking algorithms for named-entityextraction: Boosting and the voted perceptron.
InProc.
of ACL.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, andY.
Singer.
2006.
Online Passive-Aggressive Algo-rithms.
JMLR, 7:551?585.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
1997.Inducing features of random fields.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,19:380?393.J.
Duchi and Y.
Singer.
2009.
Efficient online and batchlearning using forward backward splitting.
JMLR,10:2873?2908.J.
Eisenstein, N. A. Smith, and E. P. Xing.
2011.
Discov-ering sociolinguistic associations with structured spar-sity.
In Proc.
of ACL.M.A.T.
Figueiredo.
2002.
Adaptive sparseness using Jef-freys?
prior.
Advances in Neural Information Process-ing Systems.J.
Friedman, T. Hastie, and R. Tibshirani.
2010.
A noteon the group lasso and a sparse group lasso.
Unpub-lished manuscript.J.
Gao, G. Andrew, M. Johnson, and K. Toutanova.
2007.A comparative study of parameter estimation methodsfor statistical natural language processing.
In Proc.
ofACL.J.
Goodman.
2004.
Exponential priors for maximum en-tropy models.
In Proc.
of NAACL.J.
Grac?a, K. Ganchev, B. Taskar, and F. Pereira.
2009.Posterior vs. parameter sparsity in latent variable mod-els.
Advances in Neural Information Processing Sys-tems.I.
Guyon and A. Elisseeff.
2003.
An introduction to vari-able and feature selection.
Journal of Machine Learn-ing Research, 3:1157?1182.R.
Jenatton, J.-Y.
Audibert, and F. Bach.
2009.
Struc-tured variable selection with sparsity-inducing norms.Technical report, arXiv:0904.3523.R.
Jenatton, J. Mairal, G. Obozinski, and F. Bach.
2010.Proximal methods for sparse hierarchical dictionarylearning.
In Proc.
of ICML.J.
Kazama and J. Tsujii.
2003.
Evaluation and exten-sion of maximum entropy models with inequality con-straints.
In Proc.
of EMNLP.S.
Kim and E.P.
Xing.
2010.
Tree-guided group lasso formulti-task regression with structured sparsity.
In Proc.of ICML.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.G.
R. G. Lanckriet, N. Cristianini, P. Bartlett, L. ElGhaoui, and M. I. Jordan.
2004.
Learning the kernelmatrix with semidefinite programming.
JMLR, 5:27?72.J.
Langford, L. Li, and T. Zhang.
2009.
Sparse onlinelearning via truncated gradient.
JMLR, 10:777?801.T.
Lavergne, O.
Cappe?, and F. Yvon.
2010.
Practicalvery large scale CRFs.
In Proc.
of ACL.J.
Liu and J. Ye.
2010.
Moreau-Yosida regularization forgrouped tree structure learning.
In Advances in NeuralInformation Processing Systems.J.
Mairal, R. Jenatton, G. Obozinski, and F. Bach.
2010.Network flow algorithms for structured sparsity.
InAdvances in Neural Information Processing Systems.A.
F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,and M. A. T. Figueiredo.
2010.
Turbo parsers: Depen-dency parsing by approximate variational inference.In Proc.
of EMNLP.A.
F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,N.
A. Smith, and E. P. Xing.
2011.
Online learning ofstructured predictors with multiple kernels.
In Proc.
ofAISTATS.A.
McCallum.
2003.
Efficiently inducing features ofconditional random fields.
In Proc.
of UAI.R.
T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing using span-ning tree algorithms.
In Proc.
of HLT-EMNLP.G.
Obozinski, B. Taskar, and M.I.
Jordan.
2010.
Joint co-variate selection and joint subspace selection for multi-ple classification problems.
Statistics and Computing,20(2):231?252.S.
Petrov and D. Klein.
2008a.
Discriminative log-lineargrammars with latent variables.
Advances in NeuralInformation Processing Systems, 20:1153?1160.S.
Petrov and D. Klein.
2008b.
Sparse multi-scale gram-mars for discriminative latent variable parsing.
InProc.
of EMNLP.A.
Quattoni, X. Carreras, M. Collins, and T. Darrell.2009.
An efficient projection for l1,?
regularization.In Proc.
of ICML.E.F.T.K.
Sang and S. Buchholz.
2000.
Introduction to theCoNLL-2000 shared task: Chunking.
In Proceedingsof CoNLL-2000 and LLL-2000.E.F.T.K.
Sang and F. De Meulder.
2003.
Introduction tothe CoNLL-2003 shared task: Language-independentnamed entity recognition.
In Proc.
of CoNLL.E.F.T.K.
Sang.
2002.
Introduction to the CoNLL-2002 shared task: Language-independent named entityrecognition.
In Proc.
of CoNLL.1510M.
Schmidt and K. Murphy.
2010.
Convex structurelearning in log-linear models: Beyond pairwise poten-tials.
In Proc.
of AISTATS.S.
Shalev-Shwartz, Y.
Singer, and N. Srebro.
2007.
Pe-gasos: Primal estimated sub-gradient solver for SVM.In ICML.N.
Sokolovska, T. Lavergne, O.
Cappe?, and F. Yvon.2010.
Efficient learning of sparse conditional randomfields for supervised sequence labelling.
IEEE Journalof Selected Topics in Signal Processing, 4(6):953?964.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Advances in Neural InformationProcessing Systems.R.
Tibshirani.
1996.
Regression shrinkage and selectionvia the lasso.
Journal of the Royal Statistical SocietyB., pages 267?288.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdepen-dent and structured output spaces.
In ICML.Y.
Tsuruoka, J. Tsujii, and S. Ananiadou.
2009.
Stochas-tic gradient descent training for l1-regularized log-linear models with cumulative penalty.
In Proc.
ofACL.S.J.
Wright, R. Nowak, and M.A.T.
Figueiredo.
2009.Sparse reconstruction by separable approximation.IEEE Transactions on Signal Processing, 57(7):2479?2493.L.
Xiao.
2009.
Dual averaging methods for regular-ized stochastic learning and online optimization.
InAdvances in Neural Information Processing Systems.M.
Yuan and Y. Lin.
2006.
Model selection and estima-tion in regression with grouped variables.
Journal ofthe Royal Statistical Society (B), 68(1):49.P.
Zhao, G. Rocha, and B. Yu.
2009.
Grouped and hi-erarchical model selection through composite absolutepenalties.
Annals of Statistics, 37(6A):3468?3497.H.
Zou and T. Hastie.
2005.
Regularization and vari-able selection via the elastic net.
Journal of the RoyalStatistical Society Series B (Statistical Methodology),67(2):301?320.1511
