Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 66?73, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsNTNU-CORE: Combining strong features for semantic similarityErwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bjo?rn Gamba?ck, Andre?
LynumNorwegian University of Science and TechnologyDepartment of Computer and Information and ScienceSem S?lands vei 7-9NO-7491 Trondheim, Norway{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.noAbstractThe paper outlines the work carried out atNTNU as part of the *SEM?13 shared taskon Semantic Textual Similarity, using an ap-proach which combines shallow textual, dis-tributional and knowledge-based features bya support vector regression model.
Featuresets include (1) aggregated similarity basedon named entity recognition with WordNetand Levenshtein distance through the calcula-tion of maximum weighted bipartite graphs;(2) higher order word co-occurrence simi-larity using a novel method called ?Multi-sense Random Indexing?
; (3) deeper seman-tic relations based on the RelEx semanticdependency relationship extraction system;(4) graph edit-distance on dependency trees;(5) reused features of the TakeLab and DKProsystems from the STS?12 shared task.
TheNTNU systems obtained 9th place overall (5thbest team) and 1st place on the SMT data set.1 IntroductionIntuitively, two texts are semantically similar if theyroughly mean the same thing.
The task of formallyestablishing semantic textual similarity clearly ismore complex.
For a start, it implies that we havea way to formally represent the intended meaning ofall texts in all possible contexts, and furthermore away to measure the degree of equivalence betweentwo such representations.
This goes far beyond thestate-of-the-art for arbitrary sentence pairs, and sev-eral restrictions must be imposed.
The SemanticTextual Similarity (STS) task (Agirre et al 2012,2013) limits the comparison to isolated sentencesonly (rather than complete texts), and defines sim-ilarity of a pair of sentences as the one assigned byhuman judges on a 0?5 scale (with 0 implying norelation and 5 complete semantic equivalence).
It isunclear, however, to what extent two judges wouldagree on the level of similarity between sentences;Agirre et al(2012) report figures on the agreementbetween the authors themselves of about 87?89%.As in most language processing tasks, there aretwo overall ways to measure sentence similarity, ei-ther by data-driven (distributional) methods or byknowledge-driven methods; in the STS?12 task thetwo approaches were used nearly equally much.Distributional models normally measure similarityin terms of word or word co-occurrence statistics, orthrough concept relations extracted from a corpus.The basic strategy taken by NTNU in the STS?13task was to use something of a ?feature carpet bomb-ing approach?
in the way of first automatically ex-tracting as many potentially useful features as possi-ble, using both knowledge and data-driven methods,and then evaluating feature combinations on the datasets provided by the organisers of the shared task.To this end, four different types of features wereextracted.
The first (Section 2) aggregates similar-ity based on named entity recognition with WordNetand Levenshtein distance by calculating maximumweighted bipartite graphs.
The second set of features(Section 3) models higher order co-occurrence sim-ilarity relations using Random Indexing (Kanervaet al 2000), both in the form of a (standard) slidingwindow approach and through a novel method called?Multi-sense Random Indexing?
which aims to sep-arate the representation of different senses of a term66from each other.
The third feature set (Section 4)aims to capture deeper semantic relations using ei-ther the output of the RelEx semantic dependencyrelationship extraction system (Fundel et al 2007)or an in-house graph edit-distance matching system.The final set (Section 5) is a straight-forward gath-ering of features from the systems that fared best inSTS?12: TakeLab from University of Zagreb (S?aric?et al 2012) and DKPro from Darmstadt?s Ubiqui-tous Knowledge Processing Lab (Ba?r et al 2012).As described in Section 6, Support Vector Regres-sion (Vapnik et al 1997) was used for solving themulti-dimensional regression problem of combiningall the extracted feature values.
Three different sys-tems were created based on feature performance onthe supplied development data.
Section 7 discussesscores on the STS?12 and STS?13 test data.2 Compositional Word MatchingCompositional word matching similarity is basedon a one-to-one alignment of words from the twosentences.
The alignment is obtained by maximalweighted bipartite matching using several word sim-ilarity measures.
In addition, we utilise named entityrecognition and matching tools.
In general, the ap-proach is similar to the one described by Karnicket al(2012), with a different set of tools used.
Ourimplementation relies on the ANNIE components inGATE (Cunningham et al 2002) and will thus bereferred to as GateWordMatch.The processing pipeline for GateWordMatchis: (1) tokenization by ANNIE English Tokeniser,(2) part-of-speech tagging by ANNIE POS Tagger,(3) lemmatization by GATE Morphological Anal-yser, (4) stopword removal, (5) named entity recog-nition based on lists by ANNIE Gazetteer, (6) namedentity recognition based on the JAPE grammar bythe ANNIE NE Transducer, (7) matching of namedentities by ANNIE Ortho Matcher, (8) computingWordNet and Levenstein similarity between words,(9) calculation of a maximum weighted bipartitegraph matching based on similarities from 7 and 8.Steps 1?4 are standard preprocessing routines.In step 5, named entities are recognised based onlists that contain locations, organisations, compa-nies, newspapers, and person names, as well as date,time and currency units.
In step 6, JAPE grammarrules are applied to recognise entities such as ad-dresses, emails, dates, job titles, and person namesbased on basic syntactic and morphological features.Matching of named entities in step 7 is based onmatching rules that check the type of named entity,and lists with aliases to identify entities as ?US?,?United State?, and ?USA?
as the same entity.In step 8, similarity is computed for each pairof words from the two sentences.
Words that arematched as entities in step 7 get a similarity valueof 1.0.
For the rest of the entities and non-entitywords we use LCH (Leacock and Chodorow, 1998)similarity, which is based on a shortest path betweenthe corresponding senses in WordNet.
Since wordsense disambiguation is not used, we take the simi-larity between the nearest senses of two words.
Forcases when the WordNet-based similarity cannot beobtained, a similarity based on the Levenshtein dis-tance (Levenshtein, 1966) is used instead.
It is nor-malised by the length of the longest word in the pair.For the STS?13 test data set, named entity matchingcontributed to 4% of all matched word pairs; LCHsimilarity to 61%, and Levenshtein distance to 35%.In step 9, maximum weighted bipartite matchingis computed using the Hungarian Algorithm (Kuhn,1955).
Nodes in the bipartite graph represent wordsfrom the sentences, and edges have weights that cor-respond to similarities between tokens obtained instep 8.
Weighted bipartite matching finds the one-to-one alignment that maximizes the sum of similaritiesbetween aligned tokens.
Total similarity normalisedby the number of words in both sentences is used asthe final sentence similarity measure.3 Distributional SimilarityOur distributional similarity features use RandomIndexing (RI; Kanerva et al 2000; Sahlgren, 2005),also employed in STS?12 by Tovar et al(2012);Sokolov (2012); Semeraro et al(2012).
It is anefficient method for modelling higher order co-occurrence similarities among terms, comparable toLatent Semantic Analysis (LSA; Deerwester et al1990).
It incrementally builds a term co-occurrencematrix of reduced dimensionality through the use ofa sliding window and fixed size index vectors usedfor training context vectors, one per unique term.A novel variant, which we have called ?Multi-67sense Random Indexing?
(MSRI), inspired byReisinger and Mooney (2010), attempts to captureone or more ?senses?
per unique term in an unsu-pervised manner, each sense represented as an indi-vidual vector in the model.
The method is similar toclassical sliding window RI, but each term can havemultiple context vectors (referred to as ?sense vec-tors?
here) which are updated individually.
Whenupdating a term vector, instead of directly adding theindex vectors of the neighbouring terms in the win-dow to its context vector, the system first computes aseparate window vector consisting of the sum of theindex vectors.
Then cosine similarity is calculatedbetween the window vector and each of the term?ssense vectors.
Each similarity score is in turn com-pared to a set similarity threshold: if no score ex-ceeds the threshold, the sentence vector is added asa new separate sense vector for the term; if exactlyone score is above the threshold, the window vectoris added to that sense vector; and if multiple scoresare above the threshold, all the involved senses aremerged into one sense vector, together with the win-dow vector.
This accomplishes an incremental clus-tering of senses in an unsupervised manner while re-taining the efficiency of classical RI.As data for training the models we used theCLEF 2004?2008 English corpus (approx.
130Mwords).
Our implementation of RI and MSRI isbased on JavaSDM (Hassel, 2004).
For classicalRI, we used stopword removal (using a customisedversions of the English stoplist from the Luceneproject), window size of 4+4, dimensionality set to1800, 4 non-zeros, and unweighted index vector inthe sliding window.
For MSRI, we used a simi-larity threshold of 0.2, a vector dimensionality of800, a non-zero count of 4, and window size of5+5.
The index vectors in the sliding window wereshifted to create direction vectors (Sahlgren et al2008), and weighted by distance to the target term.Rare senses with a frequency below 10 were ex-cluded.
Other sliding-window schemes, includingunweighted non-shifted vectors and Random Permu-tation (Sahlgren et al 2008), were tested, but noneoutperformed the sliding-window schemes used.Similarity between sentence pairs was calcu-lated as the normalised maximal bipartite similar-ity between term pairs in each sentence, resultingin the following features: (1) MSRI-Centroid:each term is represented as the sum of its sensevectors; (2) MSRI-MaxSense: for each termpair, the sense-pair with max similarity is used;(3) MSRI-Context: for each term, its neigh-bouring terms within a window of 2+2 is used ascontext for picking a single, max similar, sensefrom the target term to be used as its represen-tation; (4) MSRI-HASenses: similarity betweentwo terms is computed by applying the HungarianAlgorithm to all their possible sense pair mappings;(5) RI-Avg: classical RI, each term is representedas a single context vector; (6) RI-Hungarian:similarity between two sentences is calculated us-ing the Hungarian Algorithm.
Alternatively, sen-tence level similarity was computed as the cosinesimilarity between sentence vectors composed oftheir terms?
vectors.
The corresponding featuresare (1) RI-SentVectors-Norm: sentence vec-tors are created by summing their constituent terms(i.e., context vectors), which have first been normal-ized; (2) RI-SentVectors-TFIDF: same as be-fore, but TF*IDF weights are added.4 Deeper Semantic RelationsTwo deep strategies were employed to accompanythe shallow-processed feature sets.
Two existingsystems were used to provide the basis for these fea-tures, namely the RelEx system (Fundel et al 2007)from the OpenCog initiative (Hart and Goertzel,2008), and an in-house graph-edit distance systemdeveloped for plagiarism detection (R?kenes, 2013).RelEx outputs syntactic trees, dependency graphs,and semantic frames as this one for the sentence?Indian air force to buy 126 Rafale fighter jets?
:Commerce buy:Goods(buy,jet)Entity:Entity(jet,jet)Entity:Name(jet,Rafale)Entity:Name(jet,fighter)Possibilities:Event(hyp,buy)Request:Addressee(air,you)Request:Message(air,air)Transitive action:Beneficiary(buy,jet)Three features were extracted from this: first, ifthere was an exact match of the frame found in s1with s2; second, if there was a partial match until thefirst argument (Commerce buy:Goods(buy);and third if there was a match of the frame category68(Commerce buy:Goods).In STS?12, Singh et al(2012) matched UniversalNetworking Language (UNL) graphs against eachother by counting matches of relations and univer-sal words, while Bhagwani et al(2012) calculatedWordNet-based word-level similarities and createda weighted bipartite graph (see Section 2).
Themethod employed here instead looked at the graphedit distance between dependency graphs obtainedwith the Maltparser dependency parser (Nivre et al2006).
Edit distance is the defined as the minimumof the sum of the costs of the edit operations (in-sertion, deletion and substitution of nodes) requiredto transform one graph into the other.
It is approx-imated with a fast but suboptimal algorithm basedon bipartite graph matching through the Hungarianalgorithm (Riesen and Bunke, 2009).5 Reused FeaturesThe TakeLab ?simple?
system (S?aric?
et al 2012) ob-tained 3rd place in overall Pearson correlation and1st for normalized Pearson in STS?12.
The sourcecode1 was used to generate all its features, that is,n-gram overlap, WordNet-augmented word overlap,vector space sentence similarity, normalized differ-ence, shallow NE similarity, numbers overlap, andstock index features.2 This required the full LSAvector space models, which were kindly providedby the TakeLab team.
The word counts required forcomputing Information Content were obtained fromGoogle Books Ngrams.3The DKPro system (Ba?r et al 2012) obtained firstplace in STS?12 with the second run.
We used thesource code4 to generate features for the STS?12and STS?13 data.
Of the string-similarity features,we reused the Longest Common Substring, LongestCommon Subsequence (with and without normaliza-tion), and Greedy String Tiling measures.
From thecharacter/word n-grams features, we used Charac-ter n-grams (n = 2, 3, 4), Word n-grams by Con-tainment w/o Stopwords (n = 1, 2), Word n-grams1http://takelab.fer.hr/sts/2We did not use content n-gram overlap or skip n-grams.3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html, version 20120701,with 468,491,999,592 words4http://code.google.com/p/dkpro-similarity-asl/by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-card w/o Stopwords (n = 2, 4).
Semantic similaritymeasures include WordNet Similarity based on theResnik measure (two variants) and Explicit Seman-tic Similarity based on WordNet, Wikipedia or Wik-tionary.
This means that we reused all features fromDKPro run 1 except for Distributional Thesaurus.6 SystemsOur systems follow previous submissions to the STStask (e.g., S?aric?
et al 2012; Banea et al 2012) inthat feature values are extracted for each sentencepair and combined with a gold standard score in or-der to train a Support Vector Regressor on the result-ing regression task.
A postprocessing step guaran-tees that all scores are in the [0, 5] range and equal 5if the two sentences are identical.
SVR has beenshown to be a powerful technique for predictive dataanalysis when the primary goal is to approximate afunction, since the learning algorithm is applicableto continuous classes.
Hence support vector regres-sion differs from support vector machine classifica-tion where the goal rather is to take a binary deci-sion.
The key idea in SVR is to use a cost functionfor building the model which tries to ignore noise intraining data (i.e., data which is too close to the pre-diction), so that the produced model in essence onlydepends on a more robust subset of the extracted fea-tures.Three systems were created using the suppliedannotated data based on Microsoft Research Para-phrase and Video description corpora (MSRpar andMSvid), statistical machine translation system out-put (SMTeuroparl and SMTnews), and sense map-pings between OntoNotes and WordNet (OnWN).The first system (NTNU1) includes all TakeLab andDKPro features plus the GateWordMatch featurewith the SVR in its default setting.5 The trainingmaterial consisted of all annotated data available,except for the SMT test set, where it was limited toSMTeuroparl and SMTnews.
The NTNU2 system issimilar to NTNU1, except that the training materialfor OnWN and FNWN excluded MSRvid and thatthe SVR parameter C was set to 200.
NTNU3 issimilar to NTNU1 except that all features availableare included.5RBF kernel,  = 0.1, C = #samples, ?
= 1#features69Data NTNU1 NTNU2 NTNU3MSRpar 0.7262 0.7507 0.7221MSRvid 0.8660 0.8882 0.8662SMTeuroparl 0.5843 0.3386 0.5503SMTnews 0.5840 0.5592 0.5306OnWN 0.7503 0.6365 0.7200mean 0.7022 0.6346 0.6779Table 1: Correlation score on 2012 test data7 ResultsSystem performance is evaluated using the Pearsonproduct-moment correlation coefficient (r) betweenthe system scores and the human scores.
Results onthe 2012 test data (i.e., 2013 development data) arelisted in Table 1.
This basically shows that exceptfor the GateWordMatch, adding our other fea-tures tends to give slightly lower scores (cf.
NTNU1vs NTNU3).
In addition, the table illustrates that op-timizing the SVR according to cross-validated gridsearch on 2012 training data (here C = 200), rarelypays off when testing on unseen data (cf.
NTNU1vs NTNU2).Table 2 shows the official results on the test data.These are generally in agreement with the scores onthe development data, although substantially lower.Our systems did particularly well on SMT, holdingfirst and second position, reasonably good on head-lines, but not so well on the ontology alignment data,resulting in overall 9th (NTNU1) and 12th (NTNU3)system positions (5th best team).
Table 3 lists thecorrelation score and rank of the ten best individualfeatures per STS?13 test data set, and those amongthe top-20 overall, resulting from linear regressionon a single feature.
Features in boldface are gen-uinely new (i.e., described in Sections 2?4).Overall the character n-gram features are the mostinformative, particularly for HeadLine and SMT.The reason may be that these not only capture wordoverlap (Ahn, 2011), but also inflectional forms andspelling variants.The (weighted) distributional similarity featuresbased on NYT are important for HeadLine and SMT,which obviously contain sentence pairs from thenews genre, whereas the Wikipedia based feature ismore important for OnWN and FNWN.
WordNet-based measures are highly relevant too, with variantsNTNU1 NTNU2 NTNU3Data r n r n r nHead 0.7279 11 0.5909 59 0.7274 12OnWN 0.5952 31 0.1634 86 0.5882 32FNWN 0.3215 45 0.3650 27 0.3115 49SMT 0.4015 2 0.3786 9 0.4035 1mean 0.5519 9 0.3946 68 0.5498 12Table 2: Correlation score and rank on 2013 test datarelying on path length outperforming those based onResnik similarity, except for SMT.As is to be expected, basic word and lemma uni-gram overlap prove to be informative, with overallunweighted variants resulting in higher correlation.Somewhat surprisingly, higher order n-gram over-laps (n > 1) seem to be less relevant.
Longest com-mon subsequence and substring appear to work par-ticularly well for OnWN and FNWN, respectively.GateWordMatch is highly relevant too, inagreement with earlier results on the developmentdata.
Although treated as a single feature, it is ac-tually a combination of similarity features where anappropriate feature is selected for each word pair.This ?vertical?
way of combining features can po-tentially provide a more fine-grained feature selec-tion, resulting in less noise.
Indeed, if two words arematching as named entities or as close synonyms,less precise types of features such as character-basedand data-driven similarity should not dominate theoverall similarity score.It is interesting to find that MSRI outper-forms both classical RI and ESA (Gabrilovich andMarkovitch, 2007) on this task.
Still, the more ad-vanced features, such as MSRI-Context, gave in-ferior results compared to MSRI-Centroid.
Thissuggests that more research on MSRI is neededto understand how both training and retrieval canbe optimised.
Also, LSA-based features (seetl.weight-dist-sim-wiki) achieve betterresults than both MSRI, RI and ESA.
Then again,larger corpora were used for training the LSA mod-els.
RI has been shown to be comparable to LSA(Karlgren and Sahlgren, 2001), and since a relativelysmall corpus was used for training the RI/MSRImodels, there are reasons to believe that betterscores can be achieved by both RI- and MSRI-basedfeatures by using more training data.70HeadLine OnWN FNWN SMT MeanFeatures r n r n r n r n r nCharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42Table 3: Correlation score and rank of the best features8 Conclusion and Future WorkThe NTNU system can be regarded as continuationof the most successful systems from the STS?12shared task, combining shallow textual, distribu-tional and knowledge-based features into a supportvector regression model.
It reuses features from theTakeLab and DKPro systems, resulting in a verystrong baseline.Adding new features to further improveperformance turned out to be hard: onlyGateWordMatch yielded improved perfor-mance.
Similarity features based on both classicaland innovative variants of Random Indexing wereshown to correlate with semantic textual similarity,but did not complement the existing distributionalfeatures.
Likewise, features designed to revealdeeper syntactic (graph edit distance) and semanticrelations (RelEx) did not add to the score.As future work, we would aim to explore avertical feature composition approach similar toGateWordMatch and contrast it with the ?flat?composition currently used in our systems.AcknowledgementsThanks to TakeLab for source code of their ?simple?system and the full-scale LSA models.
Thanks to theteam from Ubiquitous Knowledge Processing Labfor source code of their DKPro Similarity system.71ReferencesAgirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,A.
(2012).
SemEval-2012 Task 6: A pilot on se-mantic textual similarity.
In *SEM (2012), pages385?393.Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,and Guo, W. (2013).
*SEM 2013 Shared Task:Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The SecondJoint Conference on Lexical and ComputationalSemantics.
Association for Computational Lin-guistics.Ahn, C. S. (2011).
Automatically detecting authors?native language.
PhD thesis, Monterey, Califor-nia.
Naval Postgraduate School.Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.(2012).
UNT: a supervised synergistic approachto semantic text similarity.
In *SEM (2012),pages 635?642.Ba?r, D., Biemann, C., Gurevych, I., and Zesch, T.(2012).
UKP: Computing semantic textual sim-ilarity by combining multiple content similaritymeasures.
In *SEM (2012), pages 435?440.Bhagwani, S., Satapathy, S., and Karnick, H. (2012).sranjans : Semantic textual similarity using maxi-mal weighted bipartite graph matching.
In *SEM2012: The First Joint Conference on Lexical andComputational Semantics ?
Volume 1: Proceed-ings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation (Sem-Eval 2012), pages 579?585, Montre?al, Canada.Association for Computational Linguistics.Cunningham, H., Maynard, D., Bontcheva, K., andTablan, V. (2002).
GATE: A framework andgraphical development environment for robustNLP tools and applications.
In Proceedings of the40th Annual Meeting of the Association for Com-putational Linguistics, pages 168?175, Philadel-phia, Pennsylvania.
ACL.Deerwester, S., Dumais, S., Furnas, G., Landauer,T., and Harshman, R. (1990).
Indexing by latentsemantic analysis.
Journal of the American Soci-ety for Information Science, 41(6):391?407.Fundel, K., Ku?ffner, R., and Zimmer, R. (2007).RelEx - Relation extraction using dependencyparse trees.
Bioinformatics, 23(3):365?371.Gabrilovich, E. and Markovitch, S. (2007).
Comput-ing semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of TheTwentieth International Joint Conference for Ar-tificial Intelligence., pages 1606?1611.Hart, D. and Goertzel, B.
(2008).
Opencog: A soft-ware framework for integrative artificial generalintelligence.
In Proceedings of the 2008 confer-ence on Artificial General Intelligence 2008: Pro-ceedings of the First AGI Conference, pages 468?472, Amsterdam, The Netherlands, The Nether-lands.
IOS Press.Hassel, M. (2004).
JavaSDM package.Kanerva, P., Kristoferson, J., and Holst, A.
(2000).Random indexing of text samples for latent se-mantic analysis.
In Gleitman, L. and Josh, A.,editors, Proceedings of the 22nd Annual Confer-ence of the Cognitive Science Society, page 1036.Erlbaum.Karlgren, J. and Sahlgren, M. (2001).
From Wordsto Understanding.
In Uesaka, Y., Kanerva, P., andAsoh, H., editors, Foundations of real-world in-telligence, chapter 26, pages 294?311.
Stanford:CSLI Publications.Karnick, H., Satapathy, S., and Bhagwani, S. (2012).sranjans: Semantic textual similarity using max-imal bipartite graph matching.
In *SEM (2012),pages 579?585.Kuhn, H. (1955).
The Hungarian method for the as-signment problem.
Naval research logistics quar-terly, 2:83?97.Leacock, C. and Chodorow, M. (1998).
Combin-ing local context and WordNet similarity for wordsense identification.
WordNet: An electronic lexi-cal .
.
.
.Levenshtein, V. I.
(1966).
Binary codes capable ofcorrecting deletions, insertions and reversals.
So-viet Physics Doklady, 10(8):707?710.Nivre, J., Hall, J., and Nilsson, J.
(2006).
Malt-parser: A data-driven parser-generator for depen-dency parsing.
In In Proc.
of LREC-2006, pages2216?2219.72Reisinger, J. and Mooney, R. (2010).
Multi-prototype vector-space models of word meaning.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,number June, pages 109?117.Riesen, K. and Bunke, H. (2009).
Approximategraph edit distance computation by means of bi-partite graph matching.
Image and Vision Com-puting, 27(7):950?959.R?kenes, H. (2013).
Graph-Edit Distance Applied tothe Task of Detecting Plagiarism.
Master?s the-sis, Norwegian University of Science and Tech-nology.Sahlgren, M. (2005).
An introduction to random in-dexing.
In Methods and Applications of Seman-tic Indexing Workshop at the 7th InternationalConference on Terminology and Knowledge En-gineering, TKE, volume 5.Sahlgren, M., Holst, A., and Kanerva, P. (2008).
Per-mutations as a Means to Encode Order in WordSpace.
Proceedings of the 30th Conference of theCognitive Science Society.S?aric?, F., Glavas?, G., Karan, M., S?najder, J., andBas?ic?, B. D. (2012).
TakeLab: systems for mea-suring semantic text similarity.
In *SEM (2012),pages 441?448.
*SEM (2012).
Proceedings of the First Joint Con-ference on Lexical and Computational Seman-tics (*SEM), volume 2: Proceedings of the SixthInternational Workshop on Semantic Evaluation,Montreal, Canada.
Association for ComputationalLinguistics.Semeraro, G., Aldo, B., and Orabona, V. E. (2012).UNIBA: Distributional semantics for textual sim-ilarity.
In *SEM (2012), pages 591?596.Singh, J., Bhattacharya, A., and Bhattacharyya, P.(2012).
janardhan: Semantic textual similarity us-ing universal networking language graph match-ing.
In *SEM 2012: The First Joint Conferenceon Lexical and Computational Semantics ?
Vol-ume 1: Proceedings of the main conference andthe shared task, and Volume 2: Proceedings of theSixth International Workshop on Semantic Evalu-ation (SemEval 2012), pages 662?666, Montre?al,Canada.
Association for Computational Linguis-tics.Sokolov, A.
(2012).
LIMSI: learning semantic simi-larity by selecting random word subsets.
In *SEM(2012), pages 543?546.Tovar, M., Reyes, J., and Montes, A.
(2012).
BUAP:a first approximation to relational similarity mea-suring.
In *SEM (2012), pages 502?505.Vapnik, V., Golowich, S. E., and Smola, A.
(1997).Support vector method for function approxima-tion, regression estimation, and signal process-ing.
In Mozer, M. C., Jordan, M. I., and Petsche,T., editors, Advances in Neural Information Pro-cessing Systems, volume 9, pages 281?287.
MITPress, Cambridge, Massachusetts.73
