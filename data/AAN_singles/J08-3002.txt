A Twin-Candidate Model for Learning-BasedAnaphora ResolutionXiaofeng Yang?Institute for Infocomm ResearchJian Su?
?Institute for Infocomm ResearchChew Lim Tan?School of Computing,National University of SingaporeThe traditional single-candidate learning model for anaphora resolution considers the antecedentcandidates of an anaphor in isolation, and thus cannot effectively capture the preference relation-ships between competing candidates for its learning and resolution.
To deal with this problem,we propose a twin-candidate model for anaphora resolution.
The main idea behind the modelis to recast anaphora resolution as a preference classification problem.
Specifically, the modellearns a classifier that determines the preference between competing candidates, and, duringresolution, chooses the antecedent of a given anaphor based on the ranking of the candidates.
Wepresent in detail the framework of the twin-candidate model for anaphora resolution.
Further,we explore how to deploy the model in the more complicated coreference resolution task.
Weevaluate the twin-candidate model in different domains using the Automatic Content Extractiondata sets.
The experimental results indicate that our twin-candidate model is superior to thesingle-candidate model for the task of pronominal anaphora resolution.
For the task of coreferenceresolution, it also performs equally well, or better.1.
IntroductionAnaphora is reference to an entity that has been previously introduced into the dis-course (Jurafsky and Martin 2000).
The referring expression used is called the anaphorand the expression being referred to is its antecedent.
The anaphor is usually usedto refer to the same entity as the antecedent; hence, they are coreferential with eachother.
The process of determining the antecedent of an anaphor is called anaphoraresolution.
As a key problem in discourse and language understanding, anaphoraresolution is crucial inmany natural language applications, such asmachine translation,text summarization, question answering, information extraction, and so on.
In recent?
21 Heng Mui Keng Terrace, Singapore, 119613.
E-mail: xiaofengy@i2r.a-star.edu.sg.??
21 Heng Mui Keng Terrace, Singapore, 119613.
E-mail: sujian@i2r.a-star.edu.sg.?
3 Science Drive 2, Singapore 117543.
E-mail: tancl@comp.nus.edu.sg.Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication:5 May 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 3years, supervised learning approaches have been widely applied to anaphora resolu-tion, and they have achieved considerable success (Aone and Bennett 1995; McCarthyand Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube andMueller 2003; Luo et al2004; Ng et al 2005).The strength of learning-based anaphora resolution is that resolution regularitiescan be automatically learned from annotated data.
Traditionally, learning-based ap-proaches to anaphora resolution adopt the single-candidate model, in which the po-tential antecedents (i.e., antecedent candidates) are considered in isolation for bothlearning and resolution.
In such a model, the purpose of classification is to determine ifa candidate is the antecedent of a given anaphor.
A training or testing instance is formedby an anaphor and each of its candidates, with features describing the properties of theanaphor and the individual candidate.
During resolution, the antecedent of an anaphoris selected based on the classification results for each candidate.One assumption behind the single-candidate model is that whether a candidateis the antecedent of an anaphor is completely independent of the other competingcandidates.
However, anaphora resolution can be more accurately represented as aranking problem in which candidates are ordered based on their preference and the bestone is the antecedent of the anaphor (Jurafsky and Martin 2000).
The single-candidatemodel, which only considers the candidates of an anaphor in isolation, is incapableof effectively capturing the preference relationship between candidates for its training.Consequently, the learned classifier cannot produce reliable results for preference deter-mination during resolution.To deal with this problem, we propose a twin-candidate learning model foranaphora resolution.
Themain idea behind themodel is to recast anaphora resolution asa preference classification problem.
The purpose of the classification is to determine thepreference between two competing candidates for the antecedent of a given anaphor.
Inthe model, an instance is formed by an anaphor and two of its antecedent candidates,with features used to describe their properties and relationships.
The antecedent isselected based on the judged preference among the candidates.In the article we focus on two issues about the twin-candidate model.
In the firstpart, we will introduce the framework of the twin-candidate model for anaphora reso-lution, including detailed training procedures and resolution schemes.
In the secondpart, we will further explore how to deploy the twin-candidate model in the morecomplicated task of coreference resolution.
We will present an empirical evaluation ofthe twin-candidate model in different domains, using the Automatic Content Extraction(ACE) data sets.
The experimental results indicate that the twin-candidate model issuperior to the single-candidate model for the task of pronominal anaphora resolution.For the coreference resolution task, it also performs equally well, or better.2.
Related WorkTo our knowledge, the first work on the twin-candidate model for anaphora resolutionwas proposed by Connolly, Burger, and Day (1997).
Their work relied on a set of featuresthat included lexical type, grammatical role, recency, and number/gender/semanticagreement, and employed a simple linear search scheme to choose the most preferredcandidate.
Their system produced a relatively low accuracy rate for pronoun reso-lution (55.3%) and definite NP resolution (37.4%) on a set of selected news articles.Iida et al (2003) used the twin-candidate model (called the tournament model in theirwork) to perform Japanese zero-anaphora resolution.
They utilized the same linear328Yang, Su, and Tan A Twin-Candidate Model for ARscheme to search for antecedents.
Compared with Connolly, Burger, and Day (1997),they adopted richer features in which centering information was incorporated to cap-ture contextual knowledge.
Their system achieved an accuracy of around 70% on adata set drawn from a corpus of newspaper articles.
Both of these studies were carriedout on uncommon data sets, which makes it difficult to compare their results withother baseline systems.
In contrast to the previous work, we will explore the twin-candidate model comprehensively by describing the model in more detail, trying moreeffective resolution schemes, deploying the model in the more complicated coreferenceresolution task, performing more extensive experiments, and evaluating the model inmore depth.Denis and Baldridge (2007) proposed a pronoun resolution system that directlyused a ranking learning algorithm (based on Maximal Entropy) to train a preferenceclassifier for antecedent selection.
They reported an accuracy of around 72?76% forthe different domains in the ACE data set.
In our study, we will also investigate thesolution of using a general ranking learner (e.g., Ranking-SVM).
By comparison, thetwin-candidate model is applicable to any discriminative learning algorithm, no matterwhether it is capable of ranking learning or not.
Moreover, as the model is trained andtested on pairwise candidates, it can effectively capture various relationships betweencandidates for better preference learning and determination.Ng (2005) presented a ranking model for coreference resolution.
The model focusedon the preference between the potential partitions of NPs, instead of the potentialantecedents of an NP as in our work.
Given an input document, the model first em-ployed n pre-selected coreference resolution systems to generate n candidate partitionsof NPs.
The model learned a preference classifier (trained using Ranking-SVM) thatcould distinguish good and bad partitions during testing.
The best rank partition wouldbe selected as the resolution output of the current text.
The author evaluated the modelon the ACE data set and reported an F-measure of 55?69% for the different domains.Although ranking-based, Ng?s model is quite different from ours as it operates at thecluster-level whereas ours operates at the mention-level.
In fact, the result of our twin-candidate system can be used as an input to his model.3.
The Twin-Candidate Model for Anaphora Resolution3.1 The Single-Candidate ModelLearning-based anaphora resolution uses a machine learning method to obtain p(ante(Ck)|ana,C1,C2, .
.
.
,Cn), the probability that a candidate Ck is the antecedent of theanaphor ana in the context of its antecedent candidates, C1,C2, .
.
.
,Cn.
The single-candidate model assumes that the probability that Ck is the antecedent is only de-pendent on the anaphor ana and Ck, and independent of all the other candidates.That is:p (ante(Ck) | ana,C1,C2, .
.
.
,Cn) = p (ante(Ck) | ana,Ck) (1)Thus, the probability of a candidate Ck being the antecedent can be approximated usingthe classification result on the instance describing the anaphor and Ck alone.The single-candidate model is widely used in most anaphora resolution sys-tems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube andMueller 2003; Kehler et al 2004; Ng et al 2005).
In our study, we also build as the329Computational Linguistics Volume 34, Number 3Table 1A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]proposed to [3 legislators] in [4 September].
If [5 the government] canstick with [6 them], [7 it] will be able to halve this year?s 120 billionruble (US $193 billion) deficit.Table 2Training instances generated under the single-candidate model for anaphora resolution.Anaphor Training Instance Labeli{[6 them] , [1 Those figures]} 1[6 them] i{[6 them] , [2 the government]} 0i{[6 them] , [3 legislators]} 0i{[6 them] , [4 September]} 0i{[6 them] , [5 the government]} 0i{[7 it] , [1 Those figures]} 0i{[7 it] , [3 legislators]} 0[7 it] i{[7 it] , [4 September]} 0i{[7 it] , [5 the government]} 1i{[7 it] , [6 them]} 0baseline a system for pronominal anaphora resolution based on the single-candidatemodel.In the single-candidate model, an instance has the form of i{ana, candi}, where anais an anaphor and candi is an antecedent candidate.1 For training, instances are createdfor each anaphor occurring in an annotated text.
Specifically, given an anaphor ana andits antecedent candidates, a set of negative instances (labeled ?0?)
is formed by pairingana and each of the candidates that is not coreferential with ana.
In addition, a singlepositive instance (labeled ?1?)
is formed by pairing ana and the closest antecedent, thatis, the closest candidate that is coreferential with ana.2 Note that it is possible that ananaphor has two or more antecedents, but we only create one positive instance for theclosest antecedent as its reference relationship with the anaphor is usually the mostdirect and thus the most confident.As an example, consider the text in Table 1.Here, [6 them] and [7 it] are two anaphors.
[1 Those figures] and [5 the government] aretheir closest antecedents, respectively.
Supposing that the antecedent candidates of thetwo anaphors are just all their preceding NPs in the current text, the training instancesto be created for the text segment are listed in Table 2.1 In our study, we only consider anaphors whose antecedents are noun phrases.
Typically, all the NPspreceding an anaphor can be taken as the initial antecedent candidates.
For better learning andresolution, however, candidates can be filtered so that only those ?confident?
NPs, which occur in thespecified search scope and meet constraints such as number/gender agreement, are considered.
Thedetails of candidate selection in our system will be discussed later in the section on experiments.2 We assume that at least one antecedent exists in the candidate set of an anaphor.
However, for realresolution, if none of the antecedents of an anaphor occur in the candidate set, we simply discard theanaphor and do not create any training instance for it.330Yang, Su, and Tan A Twin-Candidate Model for ARTable 3Feature set for pronominal anaphora resolution.ana Reflexive whether the anaphor is a reflexive pronounana PronType type of the anaphor if it is a pronoun (he, she, it or they?
)candi Def whether the candidate is a definite descriptioncandi Indef whether the candidate is an indefinite NPcandi Name whether the candidate is a named entitycandi Pron whether the candidate is a pronouncandi FirstNP whether the candidate is the first mentioned NP in the sentencecandi Subject whether the candidate is the subject of a sentence, the subject of a clause, or not.candi Oject whether the candidate is the object of a verb, the object of a preposition,or notcandi ParallelStruct whether the candidate has an identical collocation pattern with the anaphorcandi SentDist the sentence distance between the candidate and the anaphorcandi NearestNP whether the candidate is the candidate closest to the anaphor in positionNote that for [7 it], we do not use [2 the government] to create a positive traininginstance as it is not the closest candidate that is coreferential with the anaphor.A vector of features is specified for each training instance.
The featuresmay describethe characteristics of the anaphor and the candidate, as well as their relationships fromlexical, syntactic, semantic, and positional aspects.
Table 3 lists the features used in ourstudy.
All these features can be computed with high reliability, and have been proveneffective for pronoun resolution in previous work.Based on the generated feature vectors, a classifier is trained using a certain learningalgorithm.
During resolution, given a newly encountered anaphor, a test instance isformed for each of the antecedent candidates.
The instance is passed to the classifier,which then returns a confidence value indicating the likelihood that the candidate is theantecedent of the anaphor.
The candidate with the highest confidence is selected as theantecedent.
For example, suppose [7 it] is an anaphor to be resolved.
Six test instanceswill be created for its six antecedent candidates, as listed in Table 4.
The learned classifieris supposed to give the highest confidence to i{[7 it] , [5 the government]}, indicating thecandidate [5 the government] is the antecedent of [7 it].3.2 A Problem with the Single-Candidate ModelAs described, the assumption behind the single-candidate model is that the probabilityof a candidate being the antecedent of a given anaphor is completely independent ofTable 4Test instances generated under the single-candidate model for anaphora resolution.Anaphor Test Instancei{[7 it] , [1 Those figures]}i{[7 it] , [2 the government]}i{[7 it] , [3 legislators]}[7 it] i{[7 it] , [4 September]}i{[7 it] , [5 the government]}i{[7 it] , [6 them]}331Computational Linguistics Volume 34, Number 3the other competing candidates.
However, for an anaphor, the determination of theantecedent is often subject to preference among the candidates (Jurafsky and Martin2000).
Whether a candidate is the antecedent depends on whether it is the ?best?
amongthe candidate set, that is, whether there exists no other candidate that is preferred overit.
Hence, simply considering one candidate individually is an indirect and unreliableway to select the correct antecedent.The idea of preference is common in linguistic theories on anaphora.
Garnham(2001) summarizes different factors that influence the interpretation of anaphoricexpressions.
Some factors such as morphology (gender, number, animacy, and case)or syntax (e.g., the role of binding and commanding relations [Chomsky 1981]) are?eliminating,?
forbidding certain NPs from being antecedents.
However, many othersare ?preferential,?
giving more preference to certain candidates over others; examplesinclude: Sentence-based factors: Pronouns in one clause prefer to refer to theNP that is the subject of the previous clause (Crawley, Stevenson, andKleinman 1990).
Also, the NP that is the first-mentioned expression ispreferred regardless of the syntactic and semantic role played by thereferring expression (Gernsbacher and Hargreaves 1988). Stylistic factors: Pronouns preferentially take parallel antecedents that playthe same role as the anaphor in their respective clauses (Grober, Beardsley,and Caramazza 1978; Stevenson, Nelson, and Stenning 1995). Discourse-based factors: Items currently in focus are the prime candidatesfor providing antecedents for anaphoric expressions.
According tocentering theory (Grosz, Joshi, and Weinstein 1995), each utterance has aset of forward-looking centers that have higher preference to be referred toin later utterances.
The forward-looking centers can be ranked basedon grammatical roles or other factors. Distance-based factors: Pronouns prefer candidates in the previoussentence compared with those two or more sentences back (Clarkand Sengul 1979).As a matter of fact, ?eliminating?
factors could also be considered ?preferential?
ifwe think of the act of eliminating candidates as giving them low preference.Preference-based strategies are also widely seen in earlier manual approaches topronominal anaphora resolution.
For example, the SHRDLU system byWinograd (1972)prefers antecedent candidates in the subject position over those in the object position.The system by Wilks (1973) prefers candidates that satisfy selectional restrictions withthe anaphor.
Hobbs?s algorithm (Hobbs 1978) prefers candidates that are closer to theanaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) preferscandidates that have a high salience value computed by aggregating the weights ofdifferent factors.During resolution, the single-candidate model does select an antecedent based onpreference by using classification confidence for candidates; that is, the higher con-fidence value the classifier returns, the more likely the candidate is preferred as theantecedent.
Nevertheless, as the model considers only one candidate at a time duringtraining, it cannot effectively capture the preference between candidates for classifierlearning.
For example, consider an anaphor and a candidate Ci.
If there are no ?better?332Yang, Su, and Tan A Twin-Candidate Model for ARcandidates in the candidate set, Ci is the antecedent and forms a positive instance.Otherwise, Ci is not selected as the antecedent and thus forms a negative instance.Simply looking at a candidate alone cannot explain this, and may possibly result ininconsistent training instances (i.e., the same feature vector but different class labels).Consequently, the confidence values returned by the learned classifier cannot reliablyreflect the preference relationship between candidates.3.3 The Twin-Candidate ModelTo address the problem with the single-candidate model, we propose a twin-candidatemodel to handle anaphora resolution.
As opposed to the single-candidate model, themodel explicitly learns a preference classifier to determine the preference relationshipbetween candidates.
Formally, the model considers the probability that a candidateis the antecedent as the probability that the candidate is preferred over all the othercompeting candidates.
That is:p (ante(Ck) | ana,C1,C2, .
.
.
,Cn)= p (Ck  {C1, .
.
.
,Ck?1,Ck+1, .
.
.Cn} | ana,C1,C2, .
.
.
,Cn) (2)= p(Ck  C1, .
.
.
,Ck  Ck?1,Ck  Ck+1, .
.
.
,Ck  Cn | ana,C1,C2, .
.
.
,Cn)Assuming that the preference between Ck and Ci is independent of the preferencebetween Ck and the candidates other than Ci, we have:p(Ck  C1, .
.
.
,Ck  Ck?1,Ck  Ck+1, .
.
.
,Ck  Cn | ana,C1,C2, .
.
.
,Cn)=?1<i<n,i =kp(Ck  Ci | ana,Ck,Ci) (3)Thus:ln p (ante(Ck) | ana,C1,C2, .
.
.
,Cn)=?1<i<n,i =kln p(Ck  Ci | ana,Ck,Ci) (4)This suggests that the probability that a candidate Ck is the antecedent can be esti-mated using the classification results on the set of instances describing Ck and each ofthe other competing candidates.
To do this, we learn a classifier that, given any two can-didates of a given anaphor, can determine which one is preferred to be the antecedentof the anaphor.
The final antecedent is identified based on the classified preferencerelationships among the candidates.
This is the main idea of the twin-candidate model.In such a model, each instance consists of three elements: i{ana, Ci, Cj}, where anais an anaphor, and Ci and Cj are two of its antecedent candidates.
The class label ofan instance represents the preference between the two candidates for the antecedent,for example, ?01?
indicating Cj is preferred over Ci and ?10?
indicating Ci is preferred.Being trained with instances built based on this principle, the classifier is capable of de-termining the preference between any two candidates of a given anaphor by returning333Computational Linguistics Volume 34, Number 3Table 5A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]proposed to [3 legislators] in [4 September].
If [5 the government]can stick with [6 them], [7 it] will be able to halve this year?s120 billion ruble (US $193 billion) deficit.a class label, either ?01?
or ?10?, accordingly.
In the next section, we will introduce indetail a system based on the twin-candidate model for anaphora resolution.3.4 Framework of the Twin-Candidate Model3.4.1 Instance Representation.
In the twin-candidate model, an instance takes the formi{ana,Ci,Cj}, where ana is an anaphor andCi andCj are two of its antecedent candidates.We stipulate that Cj should be closer to ana than Ci in position (i.e., i < j).
An instance islabeled ?10?
if Ci is preferred over Cj as the antecedent, or ?01?
if otherwise.A feature vector is associated with an instance, and it describes different propertiesand relationships between ana and each of the candidates, Ci or Cj.
In our study, thesystem with the twin-candidate model adopts the same feature set as the baselinesystem with the single-candidate model (shown in Table 3).
The difference is that afeature for the single candidate, candi X, has to be replaced by a pair of features forthe twin candidates, candi1 X and candi2 X.
For example, feature candi Pron, whichdescribes whether a candidate is a pronoun, will be replaced by two features candi1 Pronand candi2 Pron, which describe whether Ci and Cj are pronouns, respectively.3.4.2 Training Instances Creation.
To learn a preference classifier, a training instance for ananaphor should be composed of two candidates with an explicit preference relationship,for example, one being an antecedent and the other being a non-antecedent.
A pairof candidates that are both antecedents or both non-antecedents are not suitable forinstance creation because their preference cannot be explicitly represented for training,although it does exist.Based on this idea, during training, for an encountered anaphor ana, we take theclosest antecedent, Cante, as the anchor candidate.3 Cante is paired with each of thecandidates Cnc that is not coreferential with ana.
If Cante is closer to ana than Cnc, aninstance i{ana, Cnc, Cante} is created and labeled ?01?.
Otherwise, if Cnc is closer, aninstance i{ana, Cante, Cnc} is created and labeled ?10?
instead.Consider again the sample text given in Table 1, which is repeated in Table 5.
For theanaphor [7 it], the closest antecedent, [5 the government] (denoted as NP5), is chosen asthe anchor candidate.
It is paired with the four non-coreferential candidates (i.e., NP1,NP3,NP4, andNP6) to create four training instances.
Among them, the instances formedwithNP1,NP3 orNP4 are labeled ?01?
and the one withNP6 is labeled ?10?.
Table 6 listsall the training instances to be generated for the text.3.4.3 Classifier Generation.
Based on the feature vectors for the generated training in-stances, a classifier can be trained using a discriminative learning algorithm.
Given atest instance i{ana, Ci, Cj} (i < j), the classifier is supposed to return a class label of ?10?,3 If no antecedent is found in the candidate set, we do not generate any training instance for the anaphor.334Yang, Su, and Tan A Twin-Candidate Model for ARTable 6Training instances generated under the twin-candidate model for anaphora resolution.Anaphor Training Instance Labeli{[6 them], [1 Those figures], [2 the government]} 10[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10i{[6 them], [1 Those figures], [4 September]} 10i{[6 them], [1 Those figures], [5 the government]} 10i{[7 it], [1 Those figures], [5 the government]} 01i{[7 it], [3 legislators], [5 the government]} 01[7 it] i{[7 it], [4 September], [5 the government]} 01i{[7 it], [5 the government], [6 them]} 10indicating that Ci is preferred over Cj for the antecedent of ana, or ?01?, indicating thatCj is preferred.3.4.4 Antecedent Identification.
After training, the preference classifier can be used toresolve anaphors.
The process of determining the antecedent of a given anaphor, calledantecedent identification, could be thought of as a tournament, a competition in whichmany participants play against each other in individual matches.
The candidates are likeplayers in a tournament.
A series of matches between candidates is held to determinethe champion of the tournament, that is, the final antecedent of the anaphor under con-sideration.
Here, the preference classifier is like the referee who judges which candidatewins or loses in a match.If an anaphor has only one antecedent candidate, it is resolved to the candidatedirectly.
For anaphors that have more than one candidate, two possible schemes can beemployed to find the antecedent.Tournament Elimination Tournament Elimination is a type of tournament wherethe loser in a match is immediately eliminated.
Such a scheme is also applicable toantecedent identification.
In the scheme, candidates are compared linearly from thebeginning to the end.
Specifically, the first candidate is compared with the second one,forming a test instance, which is then passed to the classifier to determine the prefer-ence.
The ?losing?
candidate that is judged less preferred by the classifier is eliminatedand never considered.
The winner, that is, the preferred candidate, is compared withthe third candidate.
The process continues until all the candidates are compared, andthe candidate that wins in the last comparison is selected as the antecedent.For demonstration, we use the text in Table 5 as a test example.
Suppose we have a?perfect?
classifier that can correctly determine the preference between candidates.
Thatis, the candidates that are coreferential with the anaphor will be classified as preferredover those that are not.
(If the two candidates are both coreferential or both non-coreferential with the anaphor, the one closer to the anaphor in position is preferred.
)To resolve the anaphor [7 it], the candidate NP1 is first compared with NP2.
The formedinstance is classified as ?01?, indicating NP2 is preferred.
Thus, NP1 is eliminated andNP2 continues to compete with NP3 and NP4 until it fails in the comparison with NP5.Finally, NP5 beats NP6 in the last match and is selected as the antecedent.
All the testinstances to be generated in sequence for the resolution of [6 them] and [7 it] are listed inTable 7.The Tournament Elimination scheme has a computational complexity of O(N),where N is the number of the candidates.
Thus, it enables a relatively large number335Computational Linguistics Volume 34, Number 3Table 7Test instances generated under the twin-candidate model with the Tournament Eliminationscheme.Anaphor Test Instance Resulti{[6 them], [1 Those figures], [2 the government]} 10[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10i{[6 them], [1 Those figures], [4 September]} 10i{[6 them], [1 Those figures], [5 the government]} 10i{[7 it ], [1 Those figures], [2 the government]} 01i{[7 it ], [2 the government], [3 legislators]} 10[7 it ] i{[7 it ], [2 the government], [4 September]} 10i{[7 it ], [2 the government], [5 the government]} 01i{[7 it ], [5 the government], [6 them]} 10of candidates to be processed.
However, as our twin-candidate model imposes noconstraints that enforce transitivity of the preference relation, the preference classifierwould likely output C1  C2, C2  C3, and C3  C1.
Hence, it is unreliable to eliminatea candidate once it happens to lose in one comparison, without considering all of itswinning/losing results against the other candidates.Round Robin In Section 3.3, we have shown that the probability that a candidate isthe antecedent can be calculated using the preference classification results between thecandidate and its opponents.
The candidate with the highest preference is selected asthe antecedent, that is:Antecedent(ana) = argimax p (ante(Ci) | ana,C1,C2, .
.
.
,Cn)?
argimax?j =iCF(i{ana,Ci,Cj},Ci) (5)where CF(i{ana, Ci, Cj}, Ci) is the confidence with which the classifier determines Ci tobe preferred over Cj as the antecedent of ana.
If we define the score of Ci as:Score(Ci) =?j =iCF(i{ana,Ci,Cj},Ci) (6)Then, the most preferred candidate is the candidate that has the maximum score.
If wesimply use 1 to denote the result that Ci is classified as preferred over Cj, and ?1 if Cj ispreferred otherwise, then:Score(Ci) = |{Cj|Ci  Cj}| ?
|{Cj|Cj  Ci}| (7)That is, the score of a candidate is the number of the opponents to which it is preferred,less the number of the opponents to which it is less preferred.
To obtain the scores, theantecedent candidates are comparedwith each other.
For each candidate, its comparison336Yang, Su, and Tan A Twin-Candidate Model for ARTable 8Test instances generated under the twin-candidate model with the Round Robin scheme.Anaphor Test Instance Resulti{[7 it], [1 Those figures], [2 the government]} 01i{[7 it], [1 Those figures], [3 legislators]} 01i{[7 it], [1 Those figures], [4 September]} 01i{[7 it], [1 Those figures], [5 the government]} 01i{[7 it], [1 Those figures], [6 them]} 01i{[7 it], [2 the government], [3 legislators]} 10i{[7 it], [2 the government], [4 September]} 10[7 it] i{[7 it], [2 the government], [5 the government]} 01i{[7 it], [2 the government], [6 them]} 10i{[7 it], [3 legislators], [4 September]} 01i{[7 it], [3 legislators], [5 the government]} 01i{[7 it], [3 legislators], [6 them]} 01i{[7 it], [4 September], [5 the government]} 01i{[7 it], [4 September], [6 them]} 01i{[7 it], [5 the government], [6 them]} 10result against every other candidate is recorded.
Its score increases by one if it wins amatch, or decreases by one if it loses.
The candidate with the highest score is selected asthe antecedent.Antecedent identification carried out in such a way corresponds to a type of tourna-ment called Round Robin in which each participant plays every other participant once,and the final champion is selected based on the winning?losing records of the players.In contrast to the Elimination scheme, the Round Robin scheme is more reliable inthat the preference of a candidate is determined by overall comparisons with the othercompeting candidates.
The computational complexity of the scheme is O(N2), where Nis the number of the candidates.To illustrate this, consider the example in Table 5 again.
The test instances to begenerated for resolving the anaphor [7 it] are listed in Table 8.
As shown, each ofthe candidates is compared with every other competing candidate.
The scores of thecandidates are summarized in Table 9.
Here, the candidate NP5 beats all the opponentsin the comparisons and obtains the maximum score of five.
Thus it will be selected asthe antecedent.An extension of the above Round Robin scheme is called the Weighted RoundRobin scheme.
In the weighted version, the confidence values returned by the classifier,Table 9Scores for the candidates under the Round Robin scheme.NP1 NP2 NP3 NP4 NP5 NP6 ScoreNP1 ?1 ?1 ?1 ?1 ?1 ?5NP2 +1 +1 +1 ?1 +1 +3NP3 +1 ?1 ?1 ?1 ?1 ?3NP4 +1 ?1 +1 ?1 ?1 ?1NP5 +1 +1 +1 +1 +1 +5NP6 +1 ?1 +1 +1 ?1 +1337Computational Linguistics Volume 34, Number 3Table 10Statistics for the training and testing data sets.NWire NPaper BNews# Tokens 85k 72k 67kTrain# Files 130 76 216# Tokens 20k 18k 18kTest# Files 29 17 51instead of the simple 0 and 1, are employed to calculate the score of a candidate basedon the formulaScore(Ci) =?CiCjCF(Ci  Cj)?
?CkCiCF(Ck  Ci) (8)Here, CF is the confidence value that the classifier returns for the correspondinginstance.3.5 Evaluation3.5.1 Experimental Setup.We used the ACE (Automatic Content Extraction)4 coreferencedata set for evaluation.
All the experiments were done on the ACE-2 V1.0 corpus.
Itcontains two data sets, training and devtest, which were used for training and testing,respectively.
Each of these sets is further divided into three domains: newswire (NWire),newspaper (NPaper), and broadcast news (BNews).
Statistics for the data sets are sum-marized in Table 10.For both training and resolution, a raw input document was processed by apipeline of NLP modules including a Tokenizer, Part-of-Speech tagger, NP chunker,Named-Entity (NE) Recognizer, and so on.
These preprocessing modules were meant todetermine the boundary of each NP in a text, and to provide the necessary informationabout an NP for subsequent processing.
Trained and tested on the UPENWSJ TreeBank,the POS tagger (Zhou and Su 2000) could obtain an accuracy of 97% and the NPchunker (Zhou and Su 2000) could produce an F-measure above 94%.
Evaluated forthe MUC-6 and MUC-7 Named-Entity task, the NER module (Zhou and Su 2002) couldprovide an F-measure of 96.6% (MUC-6) and 94.1% (MUC-7).In our experiments, we focused on the resolution of the third-personal pronominalanaphors, including she, he, it, they as well as their morphologic variants (such as her, his,him, its, itself, them, etc.).
For both training and testing, we considered all the pronounsthat had at least one preceding NP in their respective annotated coreferential chains.
Weused the accuracy rate as the evaluation metric, and defined it as follows:Accuracy =number of anaphors being correctly resolvedtotal number of anaphors to be resolved(9)Here, an anaphor is deemed ?correctly resolved?
if the found antecedent is in the co-referential chain of the anaphor.4 See http://www.itl.nist.gov/iad/894.01/tests/ace for a detailed description of the ACE program.338Yang, Su, and Tan A Twin-Candidate Model for ARTable 11Statistics of the training instances generated for the pronominal anaphora resolution task.NWire NPaper BNews0 instances 8,200 11,648 6,037Single-Candidate 1 instances 1,241 1,466 1,29101 instances 6,899 9,861 5,004Twin-Candidate 10 instances 1,301 1,787 1,033For pronoun resolution, the distance between the closest antecedent and theanaphor is usually short, predominantly (98% for the current data set) limited to onlyone or two sentences (McEnery, Tanaka, and Botley 1997).
For this reason, given ananaphor, we only took the NPs occurring within the current and previous two sentencesas initial antecedent candidates.
The candidates with mismatched number and genderagreement were filtered automatically from the candidate set.
Also, pronouns or NEsthat disagreed in person with the anaphor were removed in advance.
For training, therewere 1,241 (NWire), 1,466 (NPaper), and 1,291 (BNews) anaphors found with at leastone antecedent in the candidate set.
For testing, the numbers were 313 (NWire), 399(NPaper), and 271 (BNews).
On average, an anaphor had nine antecedent candidates.Table 11 summarizes the statistics of the training instances as well as the classdistribution.
Note that for the single-candidate model, the number of ?1?
instanceswas identical to the number of anaphors in the training data, because we only usedthe closest antecedents of anaphors to create the positive instances.
The number of ?0?instances was equal to the total number of ?01?
and ?10?
training instances for the twin-candidate model.We examined three different learning algorithms: C5 (Quinlan 1993), MaximumEntropy (Berger, Della Pietra, and Della Pietra 1996), and SVM (linear kernel) (Vapnik1995),5 using the software See5,6 OpenNlp.MaxEnt,7 and SVM-light,8 respectively.
Allthe classifiers were learned with the default learning parameters set in the respectivelearning software.3.5.2 Results and Discussions.
Table 12 lists the performance of the different anaphoraresolution systems with the single-candidate (SC) and the twin-candidate (TC) models.For the TC model, two antecedent identification schemes, Tournament Elimination andRound Robin, were compared.From the table, we can see that our baseline systemwith the single-candidate modelcan obtain accuracy of up to 72.9% (NWire), 77.1% (NPaper), and 74.9% (BNews).5 As MaxEnt learns a probability model, we used the returned probability as the confidence of a candidatebeing the antecedent.
For C5, the confidence value of a candidate was estimated based on the followingsmoothed ratio:CF =p+ 1t+ 2where cwas the number of positive instances and twas the total number of instances stored in thecorresponding leaf node.
For SVM, the returned value was used as the confidence value: the lower(maybe negative) the less confident.6 http://www.rulequest.com/see5-info.html7 http://MaxEnt.sourceforge.net/8 http://svmlight.joachims.org/339Computational Linguistics Volume 34, Number 3Table 12Accuracy in percent for the pronominal anaphora resolution.NWire NPaper BNews AverageC5 SC 71.6 75.6 69.5 72.7TC- Elimination 71.6 81.3 74.5 76.4- Round Robin 72.9 81.3 74.9 76.9- Weighted Round Robin 72.9 80.5 75.6 76.7MaxEnt SC 72.9 77.1 74.9 75.2TC- Elimination 75.1 79.1 77.5 77.4- Round Robin 75.1 79.1 77.5 77.4- Weighted Round Robin 75.7 78.6 77.1 77.3SVM SC 72.9 77.3 74.2 75.1TC- Elimination 73.5 82.0 78.9 78.5- Round Robin 74.4 82.0 78.9 78.7- Weighted Round Robin 74.6 79.3 78.2 77.5Rank SVM 73.5 79.3 76.4 76.7The average accuracy is comparable to that reported by Kehler et al (2004) (around75%), who also used the single-candidate model to do pronoun resolution with similarfeatures (using MaxEnt) on the ACE data sets.
By contrast, the systems with the twin-candidate model are able to achieve accuracy of up to 75.7% (NWire), 82.0% (NPaper),and 78.9% (BNews).
The average accuracy is 76.9% for C5, 77.4% for MaxEnt, and 78.7%for SVM,which is statistically significantly9 better than the results of the baselines (4.2%,2.2%, and 3.6% in accuracy).
These results confirm our claim that the twin-candidatemodel is more effective than the single-candidate model for the task of pronominalanaphora resolution.We see no significant difference between the accuracy rates (less than 1.0% accuracy)produced by the two antecedent identification schemes, Tournament Elimination andRound Robin.
This is in contrast to our belief that the Round Robin scheme, which ismore reliable than the Tournament Elimination, should lead to much better results.
Onepossible reason could be that the classifier in our systems can make a correct preferencejudgement (with accuracy above 92% as in our test) in the cases where one candidate isthe antecedent and the other is not.
As a consequence, the simple linear search can findthe final antecedent as well as the Round Robin method.
These results suggest that wecan use the Elimination scheme in a practical system to make antecedent identificationmore efficient.
(Recall that the Elimination scheme requires complexity ofO(N), insteadof O(N2) as in Round Robin.
)Ranking-SVM In our experiments, we were particularly interested in comparingthe results using the twin-candidate model and those directly using a preference learn-ing algorithm.
For this purpose, we built a system based on Ranking-SVM (Joachims2002), an extension of SVM capable of preference learning.9 Throughout our experiments, the significance was examined by using the paired t-test, with p < 0.05.340Yang, Su, and Tan A Twin-Candidate Model for ARThe system uses a similar framework to the single-candidate-based system.
Fortraining, given an anaphor, a set of instances is created for each of the antecedent candi-dates.
To learn the preference between competing candidates, a ?query-ID?
is specifiedfor each training instance in such a way that the instances formed by the candidates ofthe same anaphor bear the same query-ID.
The label of an instance represents the rank ofthe candidate in the candidate set; here, ?1?
for the instances formed by the candidatesthat are the antecedents, and ?0?
for the instances formed by the others.
The traininginstances are associated with features as defined in Table 3, to which the Ranking-SVM algorithm is then applied to generate a preference classifier.
During resolution, foreach candidate of a given anaphor, a test instance is formed and passed to the learnedclassifier, which in turn returns a value to represent the rank of the candidate among allthe candidates.
The anaphor is resolved to the one with the highest value.In fact, if we look into the learning mechanism of Ranking-SVM, we can findthat the algorithm will, in the background, pair any two instances that have the samequery-ID but different rank labels.
This is quite similar to the twin-candidate model,which creates an instance by putting together two candidates with different preferences.However, one advantage of the twin-candidate model is that it can explicitly recordvarious relationships between two competing candidates, for example, ?which oneof the two candidates is closer to the anaphor in position/syntax/semantics?
?10 Suchinter-candidate information can make the preference between candidates clearer, andthus facilitate both preference learning and determination.
In contrast, Ranking-SVM,which constructs instances in the single-candidate form, cannot effectively capture thiskind of information.The last line of Table 12 shows the results from such a system based on Ranking-SVM.
We can see that the system achieves an average accuracy of 76.7%, statisticallysignificantly better than the baseline system with the single-candidate model by 1.6%(0.4% for NWire, 2.0% for NPaper, and 2.2% for BNews).
The results lend support to ourclaim that the preference relationships between candidates, if taken into considerationfor classifier training, can lead to better resolution performance.
Still, we observe thatour twin-candidate model beats Ranking-SVM in average accuracy by 1.8% (Elimina-tion scheme) and 2.0% (Round Robin).Decision Tree One advantage of the C5 learning algorithm is that the generatedclassifier can be easily interpreted by humans, and the importance of the featurescan be visually illustrated.
In Figures 1 and 2, we show the decision trees (top fourlevels) output by C5 for the NWire domain, based on the single-candidate and thetwin-candidate models, respectively.
As the twin-candidate model uses a larger poolof features, the tree for the twin-candidate model is more complicated (180 nodes) thanthe one for the single-candidate model (36 nodes).From the two trees, we can see that bothmodels rely on similar features such as lexi-cal, positional, and grammatical properties for pronoun resolution.
However, we can seethat the preferential factors (e.g., subject preference, parallelism preference, and distancepreference as discussed in Section 3.2) are more clearly presented in the twin-candidate-based tree.
For example, if two candidates are both pronouns, the twin-candidate-basedtree will suggest that the one closer to the anaphor has a higher preference to be theantecedent.
By contrast, such a preference relationship has to be implicitly represented10 In the current work, we only consider the positional relationship between candidates by stipulatingthat i < j for an instance i{ana, Ci, Cj}.
In our future work, we will explore more inter-candidaterelationships that are helpful for preference determination.341Computational Linguistics Volume 34, Number 3Figure 1Decision tree generated for pronoun resolution under the single-candidate model.
For featureana Type, the values PRON SHE,PRON SHE,PRON SHE, and PRON THEY represent whetherthe anaphor is a pronoun such as she, he, it, and they, respectively.
For candi Subject, the valuesSUBJ MAIN, SUBJ CLAUSE and NO represent whether the candidate is the subject of a mainsentence, or the subject of a clause, or not.
For candi Object, the values OBJ VERB, OBJ PREP, andNO represent whether the candidate is the object of a verb, a preposition, or not, respectively.For other features, 0 and 1 represent yes/no.in the single-candidate-based tree, with different confidence values being assigned tothe candidates in different sentences.Learning Curve In our experiments, we were also concerned about how trainingdata size might influence anaphora resolution performance.
For this purpose, we di-vided the anaphors in the training documents into 10 batches, and then performedresolution using the classifiers trained with 1, 2, .
.
.
, 10 batches of anaphors.
Figure 3plots the learning curves of the systems with the single-candidate model and the twin-candidate model (Round Robin scheme) for the NPaper domain.
Each accuracy rateshown in the figure is the average of the results from three trials trained on differentanaphors.From the figure we can see that both the single-candidate model and the twin-candidate model reach their peak performance with around six batches (around880 anaphors).
As shown, the twin-candidate model is not apparently superior tothe single-candidate model when the size of the training data is small (below twobatches, 290 anaphors).
This is due to the fact that the number of features in the twin-candidate model is nearly double that in the single-candidate model.
As a result, thetwin-candidate model requires more training data than the single-candidate model toavoid the data sparseness problem.
Nevertheless, it does not need too much trainingdata to beat the latter; it can produce the accuracy rates consistently higher than the342Yang, Su, and Tan A Twin-Candidate Model for ARFigure 2Decision tree generated for pronoun resolution under the twin-candidate model.Figure 3Learning curves of different models for pronominal anaphora resolution in the NPaper Domain(120 anaphors per batch).343Computational Linguistics Volume 34, Number 3Table 13A sample text for coreference resolution.
[1 Globalstar] still needs to raise [2 $600 million], and[3 Schwartz] said [4 that company] would try to raise [5 themoney] in [6 the debt market].single-candidate model when trained with more than two batches of anaphors.
Thisfigure further demonstrates that the twin-candidate model is reliable and effective forthe pronominal anaphora resolution task.4.
Deploying the Twin-Candidate Model to Coreference ResolutionOne task that is closely related to anaphora resolution is coreference resolution, theprocess of identifying all the coreferential expressions in texts.11 Coreference resolutionis different from anaphor resolution.
The latter focuses on how an anaphor can be suc-cessfully resolved, and the resolution is done on given anaphors.
The former, in contrast,focuses on how the NPs that are coreferential with each other can be found correctlyand completely, and the resolution is done on all possible NPs.
In a text, many NPs,especially the non-pronouns, are non-anaphors that have no antecedent to be foundin the previous text.
Hence, the task of coreference resolution is a more complicatedchallenge than anaphora resolution, as a solution should not only be able to resolvean anaphor to the correct antecedent, but should also refrain from resolving a non-anaphor.
In this section, we will explore how to deploy the learning models for anaphorresolution in the coreference resolution task.
As pronouns are usually anaphors, we willfocus mainly on the resolution of non-pronouns.4.1 Coreference Resolution Based on the Single-Candidate ModelIn practice, the single-candidate model can be applied to coreference resolution directly,using the similar training and testing procedures to those used in anaphora resolution(described in Section 2).For training, we create ?0?
and ?1?
training instances for each encountered anaphor,that is, the NP that is coreferential with at least one preceding NP.
Specifically, given ananaphor and its antecedent candidates, a positive instance is generated for the closestantecedent and a set of negative instances is generated for each of the candidates that isnot coreferential with the anaphor.12Consider the text in Table 13 as an example.
In the text, [4 that company] and [5 themoney] are two anaphors, with [1 Globalstar] and [2 $600 million] being their antecedents,respectively.
Table 14 lists the training instances to be created for this text.11 In our study, we only consider within-document noun phrase coreference resolution.12 In some coreference resolution systems (Soon, Ng, and Lim 2001; Ng and Cardie 2002b), only thenon-coreferential candidates occurring between the closest antecedent and the anaphor are used to createnegative instances.
In the experiments, we found that these sampling strategies for negative instancesled to a trade-off between recall and precision, but no significant difference in the overall F-measure.344Yang, Su, and Tan A Twin-Candidate Model for ARTable 14Training instances generated under the single-candidate model for coreference resolution.Anaphor Training Instance Labeli{[4 that company] , [1 Globalstar]} 1[4 that company] i{[4 that company] , [2 $600 million]} 0i{[4 that company] , [3 Schwartz]} 0i{[5 the money] , [1 Globalstar]} 0[5 the money] i{[5 the money] , [2 $600 million]} 1i{[5 the money] , [3 Schwartz]} 0i{[5 the money] , [4 that company]} 0Table 15Feature set for coreference resolution.ana Def whether the possible anaphor is a definite descriptionana Indef whether the possible anaphor is an indefinite NPana Name whether the possible anaphor is a named entitycandi Def whether the candidate is a definite descriptioncandi Indef whether the candidate is an indefinite descriptioncandi Name whether the candidate is a named-entitycandi SentDist the sentence distance between the possible anaphor and the candidatecandi NameAlias whether the candidate and the candidate are aliases for each othercandi Appositive whether the possible anaphor and the candidate are in an appositivestructurecandi NumberAgree whether the possible anaphor and the candidate agree in numbercandi GenderAgree whether the possible anaphor and the candidate agree in gendercandi HeadStrMatch whether the possible anaphor and the candidate have the same headstringcandi FullStrMatch whether the possible anaphor and the candidate contain the samestrings (excluding the determiners)candi SemAgree whether the possible anaphor and the candidate belong to the samesemantic category in WordNetIn Table 15, we list the features used in our study for coreference resolution, whichare similar to those proposed in Soon, Ng, and Lim?s (2001) system.13 All these featuresare domain independent and the values can be computed with low cost but highreliability.After training, the learned classifier can be directly used for coreference resolution.Given an NP to be resolved, a test instance is generated for each of its antecedentcandidates.
The classifier, being given the instance, will determine the likelihood thatthe candidate is the antecedent of the possible anaphor.
If the confidence is belowa pre-specified threshold, the candidate is discarded.
In the case where none of thecandidates have a confidence higher than the threshold, the current NP is deemed a13 As we focus on coreference resolution for non-pronouns, we do not use the feature that describeswhether or not the NP to be resolved is a pronoun.
Also, we do not use the feature that describeswhether or not a candidate is a pronoun, because, as will be discussed together with the experiments,a pronoun is not taken as an antecedent candidate for a non-pronoun to be resolved.345Computational Linguistics Volume 34, Number 3non-anaphor and left unresolved.
Otherwise, it is resolved to the candidate with thehighest confidence.144.2 Coreference Resolution Based on the Twin-Candidate ModelThe twin-candidate model presented in the previous section focuses on the preferencebetween candidates.
The model will always select a ?best?
candidate as the antecedent,even if the current NP is a non-anaphor.
To deal with this problem, we will teach thepreference classifier how to identify non-anaphors, by incorporating non-anaphors tocreate a special class of training instances.
For resolution, if the newly learned classifierreturns the special class label, wewill know that the current NP is a non-anaphor, and nopreference relationship holds between the two candidates under consideration.
In thisway, the twin-candidate model is capable of carrying out both antecedent identificationand anaphoricity determination by itself, and thus can be deployed for coreferenceresolution directly.
In this section, we will describe the modified training and resolutionprocedures of the twin-candidate model.4.2.1 Training.
As with anaphora resolution, an instance of the twin-candidate modelfor coreference resolution takes the form i{ana, Ci, Cj}, where ana is a possible anaphor,and Cj and Cj are two of its antecedent candidates (i < j).
The feature set is similar tothat for the single-candidate model as defined in Table 15, except that a candi X featureis replaced by a pair of features, cand1 x and candi2 x, for the two competing candi-dates, respectively.During training, if an encountered NP is an anaphor, we create ?01?
or ?10?
traininginstances in the same way as in the original learning framework.
If the NP is a non-anaphor, we do the following: From the antecedent candidates,15 randomly select one as the anchorcandidate. Create a set of instances by pairing the anchor candidate and each of theother non-coreferential candidates.The instances formed by the non-anaphors are labeled ?00.
?Consider the sample text in Table 13.
For the two anaphors [4 that company] and[5 the money], we create the ?01?
and ?10?
instances as usual.
For the non-anaphors[3 Schwartz] and [6 the debt market], we generate two sets of ?00?
instances.
Table 16 listsall the training instances for the text (supposing [1 Globalstar] and [2 $600 million] are theanchor candidates for [3 Schwartz] and [6 the debt market], respectively).The ?00?
training instances are used together with the ?01?
and ?10?
ones to traina classifier.
Given a test instance i{ana, Ci, Cj} (i < j), the newly learned classifier issupposed to return ?01?
(or ?10?
), indicating ana is an anaphor and Ci (or Cj) is preferredas its antecedent, or return ?00?, indicating ana is a non-anaphor and no preferenceexists between Ci and Cj.14 Other clustering strategies are also available, for example, ?closest-first?
where a possible anaphor isresolved to the closest candidate with the confidence above the specified threshold, if any (Soon, Ng,and Lim 2001).15 For a non-anaphor, we also take the preceding NPs as its antecedent candidates.
We will discuss thisissue later together with the experimental setup.346Yang, Su, and Tan A Twin-Candidate Model for ARTable 16Training instances generated under the twin-candidate model for coreference resolution.Possible Anaphor Training Instance Labeli{[4 that company], [1 Globalstar], [2 $600 million]} 10[4 that company] i{[4 that company], [1 Globalstar], [3 Schwartz]} 10i{[5 the money], [1 Globalstar], [2 $600 million]} 01[5 the money] i{[5 the money], [2 $600 million], [3 Schwartz]} 10i{[5 the money], [2 $600 million], [4 that company]} 10[3 Schwartz] i{[3 Schwartz], [1 Globalstar], [2 $600 million]} 00i{[6 the debt market], [1 Globalstar], [2 $600 million]} 00i{[6 the debt market], [2 $600 million], [3 Schwartz]} 00[6 the debt market] i{[6 the debt market], [2 $600 million], [4 that company]} 00i{[6 the debt market], [2 $600 million], [5 the money]} 004.2.2 Antecedent Identification.
Accordingly, we make a modification to the original Tour-nament Elimination and the Round Robin schemes:Tournament Elimination Scheme As with anaphora resolution, given an NP to beresolved, candidates are compared linearly from the beginning to the end.
If an instancefor two competing candidates is classified as ?01?
or ?10?, the preferred candidate willbe compared with subsequent competitors while the loser is eliminated immediately.If the instance is classified as ?00?, both the two candidates are discarded and thecomparison restarts with the next two candidates.16 The process continues until all thecandidates have been compared.
If both of the candidates in the last match are judged tobe ?00?, the current NP is left unresolved.
Otherwise, the NPwill be resolved to the finalwinner, on the condition that the highest confidence that the winner has ever obtainedis above a pre-specified threshold.Round Robin Scheme In the Round Robin scheme, each candidate is comparedwith every other candidate.
If two candidates are labeled ?00?
in a match, both candi-dates receive a penalty of ?1 in their respective scores.
If no candidate has a positivefinal score, then the NP is considered non-anaphoric and left unresolved.
Otherwise,it is resolved to the candidate with the highest score as usual.
Here, we can also use athreshold.
That is, we will update the scores of the two candidates in a match if andonly if the preference confidence returned by the classifier is higher than a pre-specifiedthreshold.In rare cases where an NP to be resolved has only one antecedent candidate, apseudo-instance is created by pairing the candidate with itself.
The NP will be resolvedto the candidate unless the instance is labeled ?00?.4.3 Evaluation4.3.1 Experimental Setup.
We used the same ACE data sets for coreference resolutionevaluation, as described in the previous section for anaphora resolution.
A raw inputdocument was processed in advance by the same pipeline of NLP modules including16 If only one candidate remains, it will be compared with the candidate eliminated last.347Computational Linguistics Volume 34, Number 3Table 17Statistics of the training instances generated for coreference resolution (non-pronoun).NWire NPaper BNews0 instances 78,191 105,152 33,748Single-Candidate 1 instances 3,197 3,792 2,09400 instances 296,000 331,957 159,752Twin-Candidate 01 instances 50,499 70,433 21,17010 instances 27,692 34,719 12,578POS-tagger, NP chunker, NE recognizer, and so on, to obtain all possible NPs andrelated information (see Section 3.5.1).For evaluation, we adopted Vilain et al?s (1995) scoring algorithm in which recalland precision17 were computed by comparing the key chains (i.e., the annotated ?stan-dard?
coreferential chains) and the response chains (i.e., the chains generated by thecoreference resolution system).As already mentioned, the twin-candidate model described in this section is mainlymeant for non-pronouns that are often not anaphoric.
To better examine the utilityof the model in our experiments, we first focused on coreference resolution for non-pronominal NPs.
The recall and precision to be reported were computed based on theresponse chains and the key chains from which all the pronouns are removed.
We willlater show the results of overall coreference resolution for whole NPs by combining theresolution of pronouns and non-pronouns.In non-pronoun resolution, an anaphor and its antecedent do not often occur a shortdistance apart as they do in pronoun resolution.
For this reason, during training, wetook as antecedent candidates all the preceding non-pronominal NPs18 in the currentand previous four sentences; while during testing, we used all the preceding non-pronouns, regardless of distance, as candidates.19 The statistics of the training instancesfor each data set are summarized in Table 17.Again, we examined the three learning algorithms: C5, MaxEnt, and SVM.20 Asboth the single-candidate and the twin-candidate models used a threshold to block low-confidence coreferential pairs, we performed three-fold cross-evaluation on the trainingdata to determine the thresholds for the coreference resolution systems.4.3.2 Results and Discussions.
Table 18 lists the results for the different systems on thenon-pronominal NP coreference resolution.
We used as the baseline the system with thesingle-candidate model described in Section 4.1.
As mentioned, the system was trained17 The overall F-measure was defined as2 ?
Recall ?
PrecisionRecall+ Precision18 As suggested in Ng and Cardie (2002b), we did not include pronouns in the candidate set of anon-pronoun, because a pronoun is usually anaphoric and cannot give much information aboutthe entity to which it refers.19 Unlike in the case of pronoun resolution, we did not filter candidates that had mismatchednumber/gender agreement as these constraints are not reliable for non-pronoun resolution (e.g.,in our data set, around 15% of coreferential pairs do not agree in number).
Instead, we took thesefactors as features (see Table 15) and let the learning algorithm make the preference decision.20 For SVM, we employed the one-against-all aggregation method for the 3-class learning and testing.348Yang, Su, and Tan A Twin-Candidate Model for ARTable 18Recall (R), Precision (P), and F-measure (F) in percent for coreference resolution (non-pronoun).NWire NPaper BNewsR P F R P F R P FC5 SC- baseline 63.3 48.1 54.7 63.8 42.2 50.8 63.5 53.7 58.2- with non-anaphors 40.9 81.5 54.4 39.8 81.4 53.4 35.1 76.8 48.2TC- Elimination 50.8 63.0 56.2 56.6 60.1 58.3 44.6 71.2 54.9- Round Robin 58.7 57.9 58.3 56.5 60.5 58.4 49.0 70.1 57.7MaxEnt SC- baseline 62.1 52.3 56.8 56.4 58.8 57.6 61.8 54.1 57.7- with non-anaphors 59.6 54.0 56.7 54.2 62.6 58.1 53.8 58.4 56.0TC- Elimination 59.1 55.4 57.2 52.2 69.0 59.5 53.5 61.9 57.4- Round Robin 58.7 55.9 57.2 53.4 65.9 59.0 54.3 62.8 58.3SVM SC- baseline 64.1 49.0 55.5 65.5 42.1 51.3 63.5 53.7 58.2- with non-anaphors 42.3 70.0 52.7 40.0 76.6 52.5 35.7 77.0 48.8TC- Elimination 57.8 53.2 55.4 51.7 56.5 54.0 63.3 53.8 58.2- Round Robin 54.3 56.9 55.6 56.1 58.1 57.1 63.7 53.8 58.3on the instances formed by anaphors.
For better comparison with the twin-candidatemodel, we built another single-candidate-based system inwhich the non-anaphors werealso incorporated for training.
Specifically, for each encountered non-anaphor duringtraining, we created a set of ?0?
instances by pairing the non-anaphor with each of thecandidates.
These instances were added to the original instances formed by anaphorsto learn a classifier,21 which was then applied for the resolution as usual.The results for the two single-candidate based systems are listed in Table 18.
Whentrained with the instances formed only by anaphors, the system could achieve recallabove 60% and precision of around 50% for the three domains.
When trained with theinstances formed by both anaphors and non-anaphors, the system yielded a significantimprovement in precision.
In the case of using C5 and SVM, the system is capable ofproducing precision rates of up to 80%.
The increase in precision is reasonable since theclassifier tends to be stricter in blocking non-anaphors.
Unfortunately, however, at thesame time recall drops significantly, and no apparent improvement can be observed inthe resulting overall F-measure.When trained with non-anaphors incorporated, the systems with the twin-candidate model, described in Section 4.2, are capable of yielding higher precisionagainst the baseline.
Although recall also drops at the same time, the increase inprecision can compensate it well: We observe that in most cases, the system with thetwin-candidate model can achieve a better F-measure than the baseline system withthe single-candidate model.
Also, the improvement is statistically significant (t-test,p < 0.05) in the NWire domain when C5 is used (3.6%), and in the NPaper domain21 The statistics of the ?0?
instances shown in Table 17 become 392,646, 455,167, and 207,667 for NWire,NPaper, and BNews, respectively.349Computational Linguistics Volume 34, Number 3when any of the three learning algorithms, C5 (5.0%), MaxEnt (1.4%), and SVM (4.6%),is used.
These results suggest that our twin-candidate model can effectively identifynon-anaphors and block their invalid resolution, without affecting the accuracy ofdetermining antecedents for anaphors.Compared with the pronoun resolution described in the previous section, herewe find that for non-pronoun resolution the superiority of the twin-candidate modelagainst the single-candidate model is not apparent.
In some domains such as BNews,the difference between the two models is not statistically significant.
One possibleexplanation is that for non-pronoun resolution, the features that really matter are quitelimited, that is, NameAlias, String-Matching, and Appositive (we will later show thisin the decision trees).
A candidate that has any one of these features is most likely theantecedent, regardless of the other competing candidates.
In this situation, the single-candidate model, which considers candidates in isolation, does as well as the twin-candidate model.
Still, the results suggest that the twin-candidate model is suitable forboth resolution tasks, no matter whether the features involved are strongly indicative(as with non-pronoun resolution) or not (as with pronoun resolution).As with anaphora resolution, we do not observe any apparent performance differ-ence between the two twin-candidate identification schemes, Tournament Eliminationand Round Robin.
The Round Robin scheme performs better than Elimination whentrained using C5 and SVM, by up to 2.8% and 2.9% in F-measure, respectively.
However,the Elimination scheme, when trained using MaxEnt, is capable of performing equallywell or slightly better (0.5% F-measure) than the Round Robin scheme.Recall vs.
Precision As discussed, the results in Table 18 show different recall andprecision patterns for different systems.
The baseline system with the single-candidatemodel tends to yield higher recall while the system with the twin-candidate modeltends to produce higher precision.
Thus, a fairer comparison of the two systems is toexamine the precision rates that these systems achieve under the same recall rates.
Forthis purpose, in Figure 4, we plot the variant recall and precision rates that the twosystems are capable of obtaining (tested using MaxEnt, Round Robin scheme, for theNPaper domain), focusing on precision rates above 50% and recall rates above 40%.From the figure, we find that the systemwith the twin-candidate model achieves higherprecision for recall rates ranging from 40% and 55%, and performs equally well for recallrates above 55%, which further proves the reliability of our twin-candidate model forcoreference resolution.Decision Trees In Figures 5 and 6, we show the two decision trees (NWire domain)generated by the systems with the single-candidate model and the twin-candidatemodel, respectively.
The tree from the single-candidate model contains only 13 nodes,considerably smaller than that from the twin-candidate model, which contains around1.2k nodes.
From the figure, we can see that both models heavily rely on string-matching, name-alias, and appositive features to perform non-pronoun resolution, incontrast to pronoun resolution where lexical and positional features seem more impor-tant (as shown in Figures 1 and 2).Learning Curves In our experiments, we were also interested in evaluating theresolution performance of the two learning models on different quantities of trainingdata.
Figure 7 plots the learning curves for the systems using the single-candidate modeland the system using the twin-candidate model (NPaper domain).
The F-measure isaveraged over three random trials trained on 5, 10, 15, .
.
.
documents.
Consistent withthe curves for the anaphora resolution task as depicted in Figure 3, the system withthe twin-candidate model outperforms the one with the single-candidate model on asmall amount of training data (less than five documents).
When more data is available,350Yang, Su, and Tan A Twin-Candidate Model for ARFigure 4Various recall (%) and precision (%) of different models for non-pronoun resolution.Figure 5Decision tree generated for non-pronoun resolution under the single-candidate model.the twin-candidate model also yields a consistently better F-measure than the single-candidate model.Overall Coreference Resolution Having demonstrated the performance of thetwin-candidate model on coreference resolution for non-pronouns, we now furtherexamine overall coreference resolution for whole NPs, combining both pronounresolution and non-pronoun resolution.
Specifically, given an input test document,we check each encountered NP from beginning to end.
If it is a pronoun,22 we use22 We identify the pleonastic use of it in advance (79.2% accuracy) using a set of predefined pattern rulesbased on regular expressions.
The first-person and second-person pronouns are heuristically resolvedto the closest pronoun of the same type or a speaker nearby, if any, with an average 61.8% recall and79.5% precision.351Computational Linguistics Volume 34, Number 3Figure 6Decision tree generated for non-pronoun resolution under the twin-candidate model.Figure 7Learning curves of different models for non-pronoun resolution.the pronominal anaphora resolution systems, as described in the previous section, toresolve it to an antecedent.
Otherwise, we use the non-pronoun coreference resolutionsystems described in this section to resolve the NP to an antecedent, if any is found.
Allthe coreferential pairs are put together in a coreferential chain.
The recall and precisionrates are computed by comparing the standard key chains and generated responsechains using Vilain et al?s (1995) algorithm.352Yang, Su, and Tan A Twin-Candidate Model for ARTable 19Recall (R), Precision (P), and F-Measure (F) in percent for coreference resolution.NWire NPaper BNewsR P F R P F R P FC5 SC 62.2 52.6 57.0 64.9 50.6 56.9 62.9 58.5 60.6TC- Elimination 53.8 65.9 59.2 61.2 64.4 62.8 53.1 70.9 60.7- Round Robin 59.0 61.2 60.1 62.0 64.3 63.1 56.0 69.9 62.2MaxEnt SC 60.7 56.0 58.3 60.8 62.2 61.5 63.8 60.6 62.2TC- Elimination 59.5 59.2 59.3 58.6 67.8 62.9 59.3 66.4 62.7- Round Robin 60.6 57.9 59.2 59.4 69.2 63.3 61.7 64.5 63.0SVM SC 62.3 53.3 57.5 66.2 50.5 57.3 64.7 60.1 62.3TC- Elimination 57.6 57.0 57.3 58.5 62.6 60.5 65.0 60.6 62.7- Round Robin 56.0 60.6 58.2 60.4 63.6 62.0 65.4 60.7 63.0Table 19 lists the coreference resolution results of the systemswith different learningmodels.
We observe that the results for overall coreference resolution are better thanthose of non-pronoun coreference resolution as shown in Table 18, which is due to thecomparatively high accuracy of the resolution of pronouns.In line with the previous results for pronoun resolution and non-pronoun resolu-tion, the twin-candidate model outperforms the single-candidate model in coreferenceresolution for whole NPs.
Consider the system trained with MaxEnt as an example.The single-candidate-based system obtains F-measures of 58.3%, 61.5%, and 62.2% forthe NWire, NPaper, and BNews domains.23 By comparison, the twin-candidate-basedsystem (Round Robin scheme) can achieve F-measures of 59.2%, 63.3%, and 63.0% forthe three domains.
The improvement over the single-candidate model in F-measure(0.9%, 1.8%, and 0.8%) is larger than that for non-pronoun resolution (0.4%, 1.4%,and 0.6% as shown in Table 18), owing to the higher gains obtained from pronounresolution.
For the systems trained using C5 and SVM, similar patterns of performanceimprovement may be observed.5.
ConclusionIn this article, we have presented a twin-candidate model for learning-based anaphoraresolution.
The traditional single-candidate model considers candidates in isolation,and thus cannot accurately capture the preference relationships between competingcandidates to provide reliable resolution.
To deal with this problem, our proposed twin-candidate model recasts anaphora resolution as a preference classification problem.It learns a classifier that can explicitly determine the preference between competingcandidates, and then during resolution, choose the antecedent of an anaphor based onthe ranking of the candidates.23 The results are comparable to the baseline system by Ng (2005), which also uses the single-candidatemodel and is capable of F-measures of 50.1%, 62.1%, and 57.5% for the three domains, respectively.353Computational Linguistics Volume 34, Number 3We have introduced in detail the framework of the twin-candidate model foranaphora resolution, including instance representation, training procedure, and the an-tecedent identification scheme.
The efficacy of the twin-candidate model for pronominalanaphora resolution has been evaluated in different domains, using ACE data sets.
Theexperimental results show that the model yields statistically significantly higher accu-racy rates than the traditional single-candidate model (up to 4.2% in average accuracyrate), suggesting that the twin-candidate model is superior to the latter for pronominalanaphora resolution.We have further investigated the deployment of the twin-candidate model in themore complicated coreference resolution task, where not all the encountered NPs areanaphoric.
We have modified the model to make it directly applicable for coreferenceresolution.
The experimental results for non-pronoun resolution indicate that the twin-candidate-based system performs equally well, and, in some domains, statisticallysignificantly better than the single-candidate based systems.
When combined withthe results for pronoun resolution, the twin-candidate based system achieves furtherimprovement against the single-candidate-based systems in all the domains.A number of further contributions can be made by extending this work in newdirections.
Currently, we only adopt simple domain-independent features for learning.Our recent work (Yang, Su, and Tan 2005) suggests that more complicated features, suchas statistics-based semantic compatibility, can be effectively incorporated in the twin-candidate model for pronoun resolution.
In future work, we intend to provide a morein-depth investigation into the various kinds of knowledge that are suitable for the twin-candidate model.
Furthermore, in our current work for coreference resolution, all theNPs preceding an anaphor are used as antecedent candidates, and all encountered non-anaphors in texts are incorporated without filtering into training instance creation.
Formore balanced training data and better classifier learning, we intend to explore someinstance-sampling techniques, such as those proposed by Ng and Cardie (2002a), toremove in advance low-confidence candidates and the less informative non-anaphors.We hope that these efforts can further improve the performance of the twin-candidatemodel in both anaphora resolution and coreference resolution.AcknowledgmentsWe would like to thank Guodong Zhou,Alexia Leong, Stanley Wai Keong Yong,and three anonymous reviewers for theirhelpful comments and suggestions.ReferencesAone, Ghinatsu and Scott W. Bennett.1995.
Evaluating automated andmanual acquisition of anaphoraresolution strategies.
In Proceedings ofthe 33rd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 122?129, Cambridge, Massachusetts.Berger, Adam L., Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to naturallanguage processing.
ComputationalLinguistics, 22(1):39?71.Chomsky, Noam.
1981.
Lectures onGovernment and Binding.
Foris,Dordrecht, The Netherlands.Clark, Herber H. and C. J. Sengul.
1979.In search of referents for noun phrasesand pronouns.Memory and Cognition,7:35?41.Connolly, Dennis, John D. Burger, andDavid S. Day, 1997.
A machine learningapproach to anaphoric reference.
InNew Methods in Language Processing,pages 133?144, Taylor and Francis,Bristol, Pennsylvania.Crawley, Rosalind A., Rosemary J.Stevenson, and David Kleinman.
1990.The use of heuristic strategies in theinterpretation of pronouns.
Journal ofPsycholinguistic Research, 19:245?264.Denis, Pascal and Jason Baldridge.
2007.
Aranking approach to pronoun resolution.In Proceedings of the 20th International Joint354Yang, Su, and Tan A Twin-Candidate Model for ARConference on Artificial Intelligence (IJCAI),pages 1588?1593, Hyderabad, India.Garnham, Alan, 2001.Mental Models and theInterpretation of Anaphora.
PsychologyPress Ltd., Hove, East Sussex, UK.Ge, Niyu, John Hale, and Eugene Charniak.1998.
A statistical approach to anaphoraresolution.
In Proceedings of the 6thWorkshop on Very Large Corpora,pages 161?171, Montreal, Quebec, Canada.Gernsbacher, Morton A. and DavidHargreaves.
1988.
Accessing sentenceparticipants: The advantage of firstmention.
Journal of Memory and Language,27:699?717.Grober, Ellen H., William Beardsley, andAlfonso Caramazza.
1978.
Parallelfunction in pronoun assignment.Cognition, 6:117?133.Grosz, Barbara J., Aravind K. Joshi, and ScottWeinstein.
1995.
Centering: A frameworkfor modeling the local coherence ofdiscourse.
Computational Linguistics,21(2):203?225.Hobbs, Jerry.
1978.
Resolving pronounreferences.
Lingua, 44:339?352.Iida, Ryu, Kentaro Inui, Hiroya Takamura,and Yuji Matsumoto.
2003.
Incorporatingcontextual cues in trainable models forcoreference resolution.
In Proceedings of the10th Conference of EACL, Workshop ?TheComputational Treatment of Anaphora,?pages 23?30, Budapest, Hungary.Joachims, Thorsten.
2002.
Optimizing searchengines using clickthrough data.
InProceedings of the ACM Conference onKnowledge Discovery and Data Mining(KDD), pages 133?142, Edmonton,Alberta, Canada.Jurafsky, Daniel and James H. Martin.2000.
Speech and Language Processing:An Introduction to Natural LanguageProcessing, Computational Linguistics,and Speech Recognition.
Prentice Hall,Upper Saddle River, New Jersey.Kehler, Andrew.
1997.
Probabilisticcoreference in information extraction.In Proceedings of the 2nd Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 163?173,Providence, Rhode Island.Kehler, Andrew, Douglas Appelt,Lara Taylor, and Aleksandr Simma.
2004.The (non)utility of predicate-argumentfrequencies for pronoun interpretation.In Proceedings of the North AmericanChapter of the Association for ComputationalLinguistics annual meeting (NAACL),pages 289?296, Boston, MA.Lappin, Shalom and Herbert J. Leass.
1994.An algorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):525?561.Luo, Xiaoqiang, Abe Ittycheriah, HongyanJing, Nanda Kambhatla, and SalimRoukos.
2004.
A mention-synchronouscoreference resolution algorithmbased on the bell tree.
In Proceedingsof the 42nd Annual Meeting of theAssociation for ComputationalLinguistics (ACL), pages 135?142,Barcelona, Spain.McCarthy, Joseph F. and Wendy G. Lehnert.1995.
Using decision trees for coreferenceresolution.
In Proceedings of the 14thInternational Conference on ArtificialIntelligences (IJCAI), pages 1050?1055,Montreal, Quebec, Canada.McEnery, A., I. Tanaka, and S. Botley.1997.
Corpus annotation and referenceresolution.
In Proceedings of the ACLWorkshop on Operational Factors inPractical Robust Anaphora Resolutionfor Unrestricted Texts, pages 67?74,Madrid, Spain.Ng, Hwee Tou, Yu Zhou, Robert Dale,and Mary Gardiner.
2005.
Machinelearning approach to identificationand resolution of one-anaphora.
InProceedings of the Nineteenth InternationalJoint Conference on Artificial Intelligence(IJCAI), pages 1105?1110, Edinburgh,Scotland.Ng, Vincent.
2005.
Machine learning forcoreference resolution: From localclassification to global ranking.
InProceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 157?164,Ann Arbor, Michigan.Ng, Vincent and Claire Cardie.
2002a.Combining sample selection anderror-driven pruning for machinelearning of coreference rules.
InProceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 55?62,Philadelphia, PA.Ng, Vincent and Claire Cardie.
2002b.Improving machine learning approachesto coreference resolution.
In Proceedings ofthe 40th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 104?111, Philadelphia, PA.Preiss, Judita.
2001.
Machine learning foranaphora resolution.
Technical ReportCS-01-10, University of Sheffield,Sheffield, England.355Computational Linguistics Volume 34, Number 3Quinlan, J. Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan KaufmannPublishers, San Francisco, CA.Soon, Wee Meng, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machinelearning approach to coreferenceresolution of noun phrases.
ComputationalLinguistics, 27(4):521?544.Stevenson, Rosemary J., Alexander W. R.Nelson, and Keith Stenning.
1995.
The roleof parallelism in strategies of pronouncomprehension.
Language and Speech,29:393?418.Strube, Michael and Christoph Mueller.2003.
A machine learning approach topronoun resolution in spoken dialogue.In Proceedings of the 41st Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 168?175,Sapporo, Japan.Vapnik, Vladimir N. 1995.
The Nature ofStatistical Learning Theory.
Springer-Verlag,New York, NY.Vilain, Marc, John Burger, John Aberdeen,Dennis Connolly, and Lynette Hirschman.1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the SixthMessage Understanding Conference (MUC-6),pages 45?52, San Francisco, CA.Wilks, Yorick.
1973.
Preference Semantics.Stanford AI Laboratory Memo AIM-206.Stanford University.Winograd, Terry.
1972.
Understanding NaturalLanguage.
Academic Press, New York.Yang, Xiaofeng, Jian Su, and Chew Lim Tan.2005.
Improving pronoun resolution usingstatistics-based semantic compatibilityinformation.
In Proceedings of the 43rdAnnual Meeting of the Association forComputational Linguistics (ACL),pages 165?172, Ann Arbor, MI.Zhou, Guodong and Jian Su.
2000.Error-driven HMM-based chunk taggerwith context-dependent lexicon.
InProceedings of the Joint Conference onEmpirical Methods in Natural LanguageProcessing and Very Large Corpora,pages 71?79, Hong Kong.Zhou, Guodong and Jian Su.
2002.
NamedEntity recognition using a HMM-basedchunk tagger.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 473?480, Philadelphia, PA.356
