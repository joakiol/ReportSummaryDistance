Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130?135,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsThe Karlsruhe Institute of Technology Translation Systemsfor the WMT 2014Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex WaibelInstitute for Anthropomatics and RoboticsKIT - Karlsruhe Institute of Technologyfirstname.lastname@kit.eduAbstractIn this paper, we present the KITsystems participating in the SharedTranslation Task translating betweenEnglish?German and English?French.All translations are generated usingphrase-based translation systems, usingdifferent kinds of word-based, part-of-speech-based and cluster-based languagemodels trained on the provided data.Additional models include bilingual lan-guage models, reordering models basedon part-of-speech tags and syntactic parsetrees, as well as a lexicalized reorderingmodel.
In order to make use of noisyweb-crawled data, we apply filteringand data selection methods for languagemodeling.
A discriminative word lexiconusing source context information provedbeneficial for all translation directions.1 IntroductionWe describe the KIT systems for the Shared Trans-lation Task of the ACL 2014 Ninth Workshop onStatistical Machine Translation.
We participatedin the English?German and English?Frenchtranslation directions, using a phrase-based de-coder with lattice input.The paper is organized as follows: the next sec-tion describes the data used for each translationdirection.
Section 3 gives a detailed description ofour systems including all the models.
The trans-lation results for all directions are presented after-wards and we close with a conclusion.2 DataWe utilize the provided EPPS, NC and CommonCrawl parallel corpora for English?German andGerman?English, plus Giga for English?Frenchand French?English.
The monolingual partof those parallel corpora, the News Shufflecorpus for all four directions and additionallythe Gigaword corpus for English?French andGerman?English are used as monolingual train-ing data for the different language models.
Foroptimizing the system parameters, newstest2012and newstest2013 are used as development andtest data respectively.3 System DescriptionBefore training we perform a common preprocess-ing of the raw data, which includes removing longsentences and sentences with a length mismatchexceeding a certain threshold.
Afterwards, we nor-malize special symbols, dates, and numbers.
Thenwe perform smart-casing of the first letter of everysentence.
Compound splitting (Koehn and Knight,2003) is performed on the source side of the cor-pus for German?English translation.
In order toimprove the quality of the web-crawled CommonCrawl corpus, we filter out noisy sentence pairs us-ing an SVM classifier for all four translation tasksas described in Mediani et al.
(2011).Unless stated otherwise, we use 4-gram lan-guage models (LM) with modified Kneser-Neysmoothing, trained with the SRILM toolkit (Stol-cke, 2002).
All translations are generated byan in-house phrase-based translation system (Vo-gel, 2003), and we use Minimum Error RateTraining (MERT) as described in Venugopal etal.
(2005) for optimization.
The word align-ment of the parallel corpora is generated usingthe GIZA++ Toolkit (Och and Ney, 2003) forboth directions.
Afterwards, the alignments arecombined using the grow-diag-final-and heuris-tic.
For English?German, we use discrimi-native word alignment trained on hand-aligneddata as described in Niehues and Vogel (2008).The phrase table (PT) is built using the Mosestoolkit (Koehn et al., 2007).
The phrase scoringfor the small data sets (German?English) is also130done by the Moses toolkit, whereas the bigger sets(French?English) are scored by our in-house par-allel phrase scorer (Mediani et al., 2012a).
Thephrase pair probabilities are computed using mod-ified Kneser-Ney smoothing as described in Fosteret al.
(2006).Since German is a highly inflected language,we try to alleviate the out-of-vocabulary prob-lem through quasi-morphological operations thatchange the lexical entry of a known word form toan unknown word form as described in Niehuesand Waibel (2011).3.1 Word Reordering ModelsWe apply automatically learned reordering rulesbased on part-of-speech (POS) sequences and syn-tactic parse tree constituents to perform sourcesentence reordering according to the target lan-guage word order.
The rules are learnedfrom a parallel corpus with POS tags (Schmid,1994) for the source side and a word align-ment to learn reordering rules that cover shortrange (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009).In addition, we apply a tree-based reorderingmodel (Herrmann et al., 2013) to better addressthe differences in word order between German andEnglish.
Here, a word alignment and syntacticparse trees (Rafferty and Manning, 2008; Kleinand Manning, 2003) for the source side of thetraining corpus are required to learn rules on howto reorder the constituents in the source sentence.The POS-based and tree-based reordering rulesare applied to each input sentence before transla-tion.
The resulting reordered sentence variants aswell as the original sentence are encoded in a re-ordering lattice.
The lattice, which also includesthe original position of each word, is used as inputto the decoder.In order to acquire phrase pairs matching thereordered sentence variants, we perform latticephrase extraction (LPE) on the training corpuswhere phrase are extracted from the reorderedword lattices instead of the original sentences.In addition, we use a lexicalized reorderingmodel (Koehn et al., 2005) which stores reorder-ing probabilities for each phrase pair.
Duringdecoding the lexicalized reordering model deter-mines the reordering orientation of each phrasepair at the phrase boundaries.
The probability forthe respective orientation with respect to the orig-inal position of the words is included as an addi-tional score in the log-linear model of the transla-tion system.3.2 AdaptationIn the French?English and English?French sys-tems, we perform adaptation for translation mod-els as well as for language models.
The EPPS andNC corpora are used as in-domain data for the di-rection English?French, while NC corpus is thein-domain data for French?English.Two phrase tables are built: one is the out-of-domain phrase table, which is trained on allcorpora; the other is the in-domain phrase table,which is trained on in-domain data.
We adapt thetranslation model by using the scores from the twophrase tables with the backoff approach describedin Niehues and Waibel (2012).
This results in aphrase table with six scores, the four scores fromthe general phrase table as well as the two condi-tional probabilities from the in-domain phrase ta-ble.
In addition, we take the union of the candidatephrase pairs collected from both phrase tables Adetailed description of the union method can befound in Mediani et al.
(2012b).The language model is adapted by log-linearlycombining the general language model and an in-domain language model.
We train a separate lan-guage model using only the in-domain data.
Thenit is used as an additional language model duringdecoding.
Optimal weights are set during tuningby MERT.3.3 Special Language ModelsIn addition to word-based language models, weuse different types of non-word language modelsfor each of the systems.
With the help of a bilin-gual language model (Niehues et al., 2011) weare able to increase the bilingual context betweensource and target words beyond phrase bound-aries.
This language model is trained on bilin-gual tokens created from a target word and all itsaligned source words.
The tokens are ordered ac-cording to the target language word order.Furthermore, we use language models basedon fine-grained part-of-speech tags (Schmid andLaws, 2008) as well as word classes to allevi-ate the sparsity problem for surface words.
Theword classes are automatically learned by clus-tering the words of the corpus using the MKCLSalgorithm (Och, 1999).
These n-gram languagemodels are trained on the target language corpus,131where the words have been replaced either by theircorresponding POS tag or cluster ID.
During de-coding, these language models are used as addi-tional models in the log-linear combination.The data selection language model is trainedon data automatically selected using cross-entropydifferences between development sets from pre-vious WMT workshops and the noisy crawleddata (Moore and Lewis, 2010).
We selected thetop 10M sentences to train this language model.3.4 Discriminative Word LexiconA discriminative word lexicon (DWL) models theprobability of a target word appearing in the trans-lation given the words of the source sentence.DWLs were first introduced by Mauser et al.(2009).
For every target word, they train a maxi-mum entropy model to determine whether this tar-get word should be in the translated sentence ornot using one feature per source word.We use two simplifications of this model thathave shown beneficial to translation quality andtraining time in the past (Mediani et al., 2011).Firstly, we calculate the score for every phrase pairbefore translating.
Secondly, we restrict the nega-tive training examples to words that occur withinmatching phrase pairs.In this evaluation, we extended the DWLwith n-gram source context features proposedby Niehues and Waibel (2013).
Instead of rep-resenting the source sentence as a bag-of-words,we model it as a bag-of-n-grams.
This allows usto include information about source word order inthe model.
We used one feature per n-gram up tothe order of three and applied count filtering forbigrams and trigrams.4 ResultsThis section presents the participating systemsused for the submissions in the four translationdirections of the evaluation.
We describe the in-dividual components that form part of each ofthe systems and report the translation qualitiesachieved during system development.
The scoresare reported in case-sensitive BLEU (Papineni etal., 2002).4.1 English-FrenchThe development of our English?French systemis shown in Table 1.It is noteworthy that, for this direction, we choseto tune on a subset of 1,000 pairs from news-test2012, due to the long time the whole set takesto be decoded.
In a preliminary set of experiments(not reported here), we found no significant differ-ences between tuning on the small or the big devel-opment sets.
The translation model of the baselinesystem is trained on the whole parallel data afterfiltering (EPPS, NC, Common Crawl, Giga).
Thesame data was also used for language modeling.We also use POS-based reordering.The biggest improvement was due to using twoadditional language models.
One consists of a log-linear interpolation of individual language modelstrained on the target side of the parallel data, theNews shuffle, Gigaword and NC corpora.
In ad-dition, an in-domain language model trained onlyon NC data is used.
This improves the score bymore than 1.4 points.
Adaptation of the translationmodel towards a smaller model trained on EPPSand NC brings an additional 0.3 points.Another 0.3 BLEU points could be gained byusing other special language models: a bilinguallanguage model together with a 4-gram clusterlanguage model (trained on all monolingual datausing the MKCLS tool and 500 clusters).
Incor-porating a lexicalized reordering model into thesystem had a very noticeable effect on test namelymore than half a BLEU point.Finally, using a discriminative word lexiconwith source context has a very small positive ef-fect on the test score, however more than 0.3 ondev.
This final configuration was the basis of oursubmitted official translation.System Dev TestBaseline 15.63 27.61+ Big LMs 16.56 29.02+ PT Adaptation 16.77 29.32+ Bilingual + Cluster LM 16.87 29.64+ Lexicalized Reordering 16.92 30.17+ Source DWL 17.28 30.19Table 1: Experiments for English?French4.2 French-EnglishSeveral experiments were conducted for theFrench?English translation system.
They aresummarized in Table 2.The baseline system is essentially a phrase-based translation system with some preprocess-132ing steps on the source side and utilizing theshort-range POS-based reordering on all paralleldata and fine-grained monolingual corpora such asEPPS and NC.Adapting the translation model using a small in-domain phrase table trained on NC data only helpsus gain more than 0.4 BLEU points.Using non-word language models including abilingual language model and a 4-gram 50-clusterlanguage model trained on the whole parallel dataattains 0.24 BLEU points on the test set.Lexicalized reordering improves our system onthe development set by 0.3 BLEU points but hasless effect on the test set with a minor improve-ment of around 0.1 BLEU points.We achieve our best system, which is used forthe evaluation, by adding a DWL with source con-text yielding 31.54 BLEU points on the test set.System Dev TestBaseline 30.16 30.70+ LM Adaptation 30.58 30.94+ PT Adaptation 30.69 31.14+ Bilingual + Cluster LM 30.85 31.38+ Lexicalized Reordering 31.14 31.46+ Source DWL 31.19 31.54Table 2: Experiments for French?English4.3 English-GermanTable 3 presents how the English-German transla-tion system is improved step by step.In the baseline system, we used parallel datawhich consists of the EPPS and NC corpora.
Thephrase table is built using discriminative wordalignment.
For word reordering, we use word lat-tices with long range reordering rules.
Five lan-guage models are used in the baseline system; twoword-based language models, a bilingual languagemodel, and two 9-gram POS-based language mod-els.
The two word-based language models use 4-gram context and are trained on the parallel dataand the filtered Common Crawl data separately,while the bilingual language model is built onlyon the Common Crawl corpus.
The two POS-based language models are also based on the paral-lel data and the filtered crawled data, respectively.When using a 9-gram cluster language model,we get a slight improvement.
The cluster is trainedwith 1,000 classes using EPPS, NC, and CommonCrawl data.We use the filtered crawled data in addition tothe parallel data in order to build the phrase table;this gave us 1 BLEU point of improvement.The system is improved by 0.1 BLEU pointswhen we use lattice phrase extraction along withlexicalized reordering rules.Tree-based reordering rules improved the sys-tem performance further by another 0.1 BLEUpoints.By reducing the context of the two POS-basedlanguage models from 9-grams to 5-grams andshortening the context of the language modeltrained on word classes to 4-grams, the score onthe development set hardly changes but we can seea slightly improvement for the test case.Finally, we use the DWL with source contextand build a big bilingual language model usingboth the crawled and parallel data.
By doing so,we improved the translation performance by an-other 0.3 BLEU points.
This system was used forthe translation of the official test set.System Dev TestBaseline 16.64 18.60+ Cluster LM 16.76 18.66+ Common Crawl Data 17.27 19.66+ LPE + Lexicalized Reordering 17.45 19.75+ Tree Rules 17.53 19.85+ Shorter n-grams 17.55 19.92+ Source DWL + Big BiLM 17.82 20.21Table 3: Experiments for English?German4.4 German-EnglishTable 4 shows the development steps of theGerman-English translation system.For the baseline system, the training data of thetranslation model consists of EPPS, NC and thefiltered parallel crawled data.
The phrase tableis built using GIZA++ word alignment and latticephrase extraction.
All language models are trainedwith SRILM and scored in the decoding processwith KenLM (Heafield, 2011).
We use word lat-tices generated by short and long range reorderingrules as input to the decoder.
In addition, a bilin-gual language model and a target language modeltrained on word clusters with 1,000 classes are in-cluded in the system.Enhancing the word reordering with tree-basedreordering rules and a lexicalized reordering133model improved the system performance by 0.6BLEU points.Adding a language model trained on selecteddata from the monolingual corpora gave anothersmall improvement.The DWL with source context increased thescore on the test set by another 0.5 BLEU pointsand applying morphological operations to un-known words reduced the out-of-vocabulary rate,even though no improvement in BLEU can be ob-served.
This system was used to generate thetranslation submitted to the evaluation.System Dev TestBaseline 24.40 26.34+ Tree Rules 24.71 26.86+ Lexicalized Reordering 24.89 26.93+ LM Data Selection 24.96 27.03+ Source DWL 25.32 27.53+ Morphological Operations - 27.53Table 4: Experiments for German?English5 ConclusionIn this paper, we have described the systemsdeveloped for our participation in the SharedTranslation Task of the WMT 2014 evaluationfor English?German and English?French.
Alltranslations were generated using a phrase-basedtranslation system which was extended by addi-tional models such as bilingual and fine-grainedpart-of-speech language models.
Discriminativeword lexica with source context proved beneficialin all four language directions.For English-French translation using a smallerdevelopment set performed reasonably well andreduced development time.
The most noticeablegain comes from log-linear interpolation of multi-ple language models.Due to the large amounts and diversity ofthe data available for French-English, adapta-tion methods and non-word language models con-tribute the major improvements to the system.For English-German translation, the crawleddata and a DWL using source context to guideword choice brought most of the improvements.Enhanced word reordering models, namelytree-based reordering rules and a lexicalized re-ordering model as well as the source-side fea-tures for the discriminative word lexicon helpedimprove the system performance for German-English translation.In average we achieved an improvement of over1.5 BLEU over the respective baselines for all oursystems.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement n?287658.ReferencesGeorge F. Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical Ma-chine Translation.
In Proceedings of the 2006 Con-ference on Empirical Methods on Natural LanguageProcessing (EMNLP), Sydney, Australia.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,Edinburgh, Scotland, United Kingdom.Teresa Herrmann, Jan Niehues, and Alex Waibel.2013.
Combining Word Reordering Methods ondifferent Linguistic Abstraction Levels for Statisti-cal Machine Translation.
In Proceedings of the Sev-enth Workshop on Syntax, Semantics and Structurein Statistical Translation, Altanta, Georgia, USA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate Unlexicalized Parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics (ACL 2003), Sapporo, Japan.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In Proceedingsof the Eleventh Conference of the European Chap-ter of the Association for Computational Linguistics(EACL 2003), Budapest, Hungary.Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
InProceedings of the Second International Workshopon Spoken Language Translation (IWSLT 2005),Pittsburgh, PA, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL2007), Demonstration Session, Prague, Czech Re-public.134Arne Mauser, Sa?sa Hasan, and Hermann Ney.
2009.Extending Statistical Machine Translation with Dis-criminative and Trigger-based Lexicon Models.In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Suntec, Singapore.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The KITEnglish-French Translation systems for IWSLT2011.
In Proceedings of the Eight InternationalWorkshop on Spoken Language Translation (IWSLT2011), San Francisco, CA, USA.Mohammed Mediani, Jan Niehues, and Alex Waibel.2012a.
Parallel Phrase Scoring for Extra-large Cor-pora.
In The Prague Bulletin of Mathematical Lin-guistics, number 98.Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, JanNiehues, Eunach Cho, Teresa Herrmann, RainerK?argel, and Alexander Waibel.
2012b.
The KITTranslation Systems for IWSLT 2012.
In Proceed-ings of the Ninth International Workshop on SpokenLanguage Translation (IWSLT 2012), Hong Kong,HK.R.C.
Moore and W. Lewis.
2010.
Intelligent Selectionof Language Model Training Data.
In Proceedingsof the ACL 2010 Conference Short Papers, Uppsala,Sweden.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InProceedings of the Fourth Workshop on StatisticalMachine Translation (WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proceedings of the Third Workshop on StatisticalMachine Translation (WMT 2008), Columbus, OH,USA.Jan Niehues and Alex Waibel.
2011.
Using Wikipediato Translate Domain-specific Terms in SMT.
InProceedings of the Eight International Workshop onSpoken Language Translation (IWSLT 2008), SanFrancisco, CA, USA.J.
Niehues and A. Waibel.
2012.
Detailed Analysis ofDifferent Strategies for Phrase Table Adaptation inSMT.
In Proceedings of the Tenth Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA 2012), San Diego, CA, USA.J.
Niehues and A. Waibel.
2013.
An MT Error-DrivenDiscriminative Word Lexicon using Sentence Struc-ture Features.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, Sofia, Bul-garia.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InSixth Workshop on Statistical Machine Translation(WMT 2011), Edinburgh, Scotland, United King-dom.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
1999.
An Efficient Method for Deter-mining Bilingual Word Classes.
In Proceedings ofthe Ninth Conference of the European Chapter of theAssociation for Computational Linguistics (EACL1999), Bergen, Norway.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Technical Re-port RC22176 (W0109-022), IBM Research Divi-sion, T. J. Watson Research Center.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing Three German Treebanks: Lexicalized andUnlexicalized Baselines.
In Proceedings of theWorkshop on Parsing German, Columbus, OH,USA.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In Proceedings of the11th International Conference on Theoretical andMethodological Issues in Machine Translation (TMI2007), Sk?ovde, Sweden.Helmut Schmid and Florian Laws.
2008.
Estimationof Conditional Probabilities with Decision Trees andan Application to Fine-Grained POS Tagging.
In In-ternational Conference on Computational Linguis-tics (COLING 2008), Manchester, Great Britain.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, United Kingdom.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In International Confer-ence on Spoken Language Processing, Denver, Col-orado, USA.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Pro-ceedings of the ACL Workshop on Building and Us-ing Parallel Texts, Ann Arbor, Michigan, USA.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In International Conference on NaturalLanguage Processing and Knowledge Engineering,Beijing, China.135
