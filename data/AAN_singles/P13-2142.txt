Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 816?821,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExtra-Linguistic Constraints on Stance Recognition in Ideological DebatesKazi Saidul Hasan and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{saidul,vince}@hlt.utdallas.eduAbstractDetermining the stance expressed by anauthor from a post written for a two-sided debate in an online debate forumis a relatively new problem.
We seek toimprove Anand et als (2011) approachto debate stance classification by model-ing two types of soft extra-linguistic con-straints on the stance labels of debateposts, user-interaction constraints and ide-ology constraints.
Experimental results onfour datasets demonstrate the effectivenessof these inter-post constraints in improv-ing debate stance classification.1 IntroductionWhile a lot of work on document-level opinionmining has involved determining the polarity ex-pressed in a customer review (e.g., whether a re-view is ?thumbs up?
or ?thumbs down?)
(see Pangand Lee (2008) and Liu (2012) for an overviewof the field), researchers have begun exploringnew opinion mining tasks in recent years.
Onesuch task is debate stance classification: givena post written for a two-sided topic discussed inan online debate forum (e.g., ?Should abortion bebanned??
), determine which of the two sides (i.e.,for and against) its author is taking.Debate stance classification is potentially moreinteresting and challenging than polarity classifi-cation for at least two reasons.
First, while in po-larity classification sentiment-bearing words andphrases have proven to be useful (e.g., ?excellent?correlates strongly with the positive polarity), indebate stance classification it is not uncommon tofind debate posts where stances are not expressedin terms of sentiment words, as exemplified in Fig-ure 1, where the author is for abortion.Second, while customer reviews are typicallywritten independently of other reviews in an on-line forum, the same is not true for debate posts.
InThe fetus is simply a part of the mother?s body and shecan have an abortion because it is her human rights.
AlsoI take this view because every woman can face with sit-uation when two lives are at stake and the moral obli-gation is to save the one closest at hand ?
namely, thatof the mother, whose life is always more immediate thanthat of the unborn child within her body.
Permission foran abortion could then be based on psychiatric consider-ations such as prepartum depression, especially if thereis responsible psychiatric opinion that a continued preg-nancy raises the strong probability of suicide in a clini-cally depressed patient.Figure 1: A sample post on abortion.a debate forum, debate posts form threads, wherelater posts often support or oppose the viewpointsraised in earlier posts in the same thread.Previous approaches to debate stance classifica-tion have focused on three debate settings, namelycongressional floor debates (Thomas et al 2006;Bansal et al 2008; Balahur et al 2009; Yesse-nalina et al 2010; Burfoot et al 2011), company-internal discussions (Murakami and Raymond,2010), and online social, political, and ideologi-cal debates in public forums (Agrawal et al 2003;Somasundaran and Wiebe, 2010; Wang and Rose?,2010; Biran and Rambow, 2011; Hasan and Ng,2012).
As Walker et al(2012) point out, debatesin public forums differ from congressional debatesand company-internal discussions in terms of lan-guage use.
Specifically, online debaters use color-ful and emotional language to express their points,which may involve sarcasm, insults, and question-ing another debater?s assumptions and evidence.These properties can potentially make stance clas-sification of online debates more challenging thanthat of the other two types of debates.Our goal in this paper is to improve the state-of-the-art supervised learning approach to debatestance classification of online debates proposed byAnand et al(2011), focusing in particular on ideo-logical debates.
Specifically, we hypothesize thatthere are two types of soft extra-linguistic con-straints on the stance labels of debate posts that,816Number ?for?
% of posts Average threadDomain of posts posts (%) in a thread lengthABO 1741 54.9 75.1 4.1GAY 1376 63.4 74.5 4.0OBA 985 53.9 57.1 2.6MAR 626 69.5 58.0 2.5Table 1: Statistics of the four datasets.if explicitly modeled, could improve a learning-based stance classification system.
We refer tothese two types of inter-post constraints as user-interaction constraints and ideology constraints.We show how they can be learned from stance-annotated debate posts in Sections 4.1 and 4.2, re-spectively.2 DatasetsFor our experiments, we collect debate postsfrom four popular domains, Abortion (ABO),Gay Rights (GAY), Obama (OBA), and Marijuana(MAR), from an online debate forum1.
All de-bates are two-sided, so each post receives one oftwo domain labels, for or against, depending onwhether the author of the post supports or opposesabortion, gay rights, Obama, or the legalization ofmarijuana.We construct one dataset for each domain (seeTable 1 for statistics).
The fourth column of thetable shows the percentage of posts in each domainthat appear in a thread.
More precisely, a threadis a tree with one or more nodes such that (1) eachnode corresponds to a debate post, and (2) a post yiis the parent of another post yj if yj is a reply to yi.Given a thread, we can generate post sequences,each of which is a path from the root of the threadto one of its leaves.3 Baseline SystemsWe employ as baselines two stance classificationsystems, Anand et als (2011) approach and an en-hanced version of it, as described below.Our first baseline, Anand et als approach is asupervised method that trains a stance classifierfor determining whether the stance expressed ina debate post is for or against the topic.
Hence,we create one training instance from each post inthe training set, using the stance it expresses asits class label.
Following Anand et al we repre-sent a training instance using three types of lexico-syntactic features, which are briefly summarizedin Table 2.
In our implementation, we train the1http://www.createdebate.com/Feature type FeaturesBasic Unigrams, bigrams, syntactic and POS-generalized dependenciesSentiment LIWC counts, opinion dependenciesArgument Cue words, repeated punctuation, contextTable 2: Anand et als features.stance classifier using SVMlight (Joachims, 1999).After training, we can apply the classifier to clas-sify the test instances, which are generated in thesame way as the training instances.Related work on stance classification of con-gressional debates has found that enforcing au-thor constraints (ACs) can improve classificationperformance (e.g., Thomas et al(2006), Bansal etal.
(2008), Burfoot et al(2011), Lu et al(2012),Walker et al(2012)).
ACs are a type of inter-post constraints that specify that two posts writtenby the same author for the same debate domainshould have the same stance.
We hypothesize thatACs could similarly be used to improve stanceclassification of ideological debates, and thereforepropose a second baseline where we enhance thefirst baseline with ACs.
Enforcing ACs is simple.We first use the learned stance classifier to classifythe test posts as in the first baseline, and then post-process the labels of the test posts.
Specifically,we sum up the confidence values2 assigned to theset of test posts written by the same author for thesame debate domain.
If the sum is positive, thenwe label all the posts in this set as for; otherwisewe label them as against.4 Extra-Linguistic ConstraintsIn this section, we introduce two types of inter-post constraints on debate stance classification.4.1 User-Interaction ConstraintsWe call the first type of constraints user-interaction constraints (UCs).
UCs are motivatedby the observation that the stance labels of theposts in a post sequence are not independent ofeach other.
Consider the post sequence in Fig-ure 2, where each post is a response to the preced-ing post.
It shows an opening anti-abortion post(P1), followed by a pro-abortion comment (P2),which is in turn followed by another anti-abortionview (P3).
While this sequence contains alternat-ing posts from opposing stances, in general thereis no hard constraint on the stance of a post given2We use as the confidence value the signed distance of theassociated test point from the SVM hyperplane.817[P1: Anti-abortion] There are thousands of people whowant to take these children because they cannot have theirown.
If you do not want a child, have it and put it up foradoption.
At least you will be preserving a human life ratherthan killing one.
[P2: Pro-abortion] I agree that if people don?t wanttheir babies, they should have the choice of putting itup for adoption.
But it should not be made compulsory,which is essentially what happens if you ban abortion.
[P3: Anti-abortion] Why should it not be madecompulsory?
Those children have as much right tolive as you and I.
Besides, no one loses with adop-tion, so why wouldn?t you utilize it?Figure 2: A sample post sequence.
P2 and P3 arereplies to P1 and P2, respectively.the preceding sequence of posts.
Nevertheless, wefound that in our training data, a for (against) postis followed by a against (for) post 80% of the time.UCs aim to model the regularities in how usersinteract with each other in a post sequence as softconstraints.
These kinds of soft constraints can benaturally encoded as factors over adjacent posts ina post sequence (see Kschischang et al(2001)),which can in turn be learned by recasting stanceclassification as a sequence labeling task.
In ourexperiments, we seek to derive the best sequenceof stance labels for each post sequence of length ?1 using a Conditional Random Field (CRF) (Laf-ferty et al 2001).We train the CRF model using the CRF im-plementation in Mallet (McCallum, 2002).
Eachtraining sequence corresponds to a post sequence.Each post in a sequence is represented using thesame set of features as in the baselines.After training, the resulting CRF model can beused to assign a stance sequence to each test postsequence.
There is a caveat, however.
Since agiven test post may appear in more than one se-quence, different occurrences of it may be as-signed different stance labels by the CRF.
To deter-mine the final stance label for the post, we averagethe probabilities assigned to the for stance over allits occurrences; if the average is ?
0.5, then itsfinal label is for; otherwise, its label is against.4.2 Ideology ConstraintsNext, we introduce our second type of inter-postconstraints, ideology constraints (ICs).
ICs arecross-domain, author-based constraints: they areonly applicable to debate posts written by the sameauthor in different domains.
ICs model the factthat for some authors, their stances on various is-sues are determined in part by their ideologicalvalues, and in particular, their stances on differentissues may be correlated.
For example, someonewho opposes abortion is likely to be a conserva-tive and has a good chance of opposing gay rights.ICs aim to capture this kind of inter-domain corre-lation of stances.
Below we describe how we im-plement ICs and show how they can be integratedwith ACs.4.2.1 Implementing Ideology ConstraintsWe first compute a set of conditional probabil-ities, P (stance(dq )=sd|stance(dp)=sc), where (1)dp, dq ?
Domains (i.e., the set of four domains),(2) sc, sd ?
{for, against}, and (3) dp 6= dq .To compute P (stance(dq )=sd|stance(dp)=sc), we(1) determine for each author a in the train-ing set and each domain dp the stance of ain dp (denoted by author-stance(dp ,a)), whereauthor-stance(dp ,a) is computed as the majoritystance labels associated with the debate postsin the training set that a wrote for dp; and(2) compute P (stance(dq )=sd|stance(dp)=sc) asthe ratio of?a?A Count(author-stance(dp ,a)=sc,author-stance(dq ,a)=sd) to?a?A Count(author-stance(dp,a)=sc), where A is the set of authors inthe training set who posted in both dp and dq.
Itshould be fairly easy to see that these conditionalprobabilities measure the degree of correlation be-tween the stances in different domains.4.2.2 Inference Using ILPRecall that in our second baseline, we employACs to postprocess the output of the stance clas-sifier simply by summing up the confidence val-ues assigned to the posts written by the same au-thor for the same debate domain.
However, sincewe now want to enforce two types of inter-postconstraints (namely, ACs and ICs), we will haveto employ a more sophisticated inference mecha-nism.
Previous work has focused on employinggraph minimum cut (MinCut) as the inference al-gorithm.
However, since MinCut suffers from theweakness of not being able to enforce negativeconstraints (i.e., two posts cannot receive the samelabel) (Bansal et al 2008), we propose to use in-teger linear programming (ILP) as the underlyinginference mechanism.
Below we show how to im-plement ACs and ICs within the ILP framework.Owing to space limitations, we refer the readerto Roth and Yih (2004) for details of the ILPframework.
Briefly, ILP seeks to optimize anobjective function subject to a set of linear con-818straints.
Below we focus on describing the ILPprogram and how the ACs and ICs can be encoded.Let Y = y1, .
.
.
, yn be the set of debate posts.For each yi, we create one (binary-valued) indi-cator variable xi, which will be used in the ILPprogram.
Let pi = P (for|yi) be the ?benefit?
ofsetting xi to 1, where P (for|yi) is provided by theCRF.
Consequently, after optimization, yi?s stanceis for if its xi is set to 1.
We optimize the followingobjective function:max?ipixi + (1?
pi)(1?
xi)subject to a set of linear constraints, which encodethe ACs and the ICs, as described below.Implementing author constraints.
If yi and yjare composed by the same author, we ensure thatxi and xj will be assigned the same value by em-ploying the linear constraint |xi ?
xj| = 0.Implementing ideology constraints.
For con-venience, below we use the notation introduced inSection 4.2.1, and assume that yi and yj are twoarbitrary posts written by the same author in do-mains dp and dq, respectively.Case 1: If P (stance(dq )=for|stance(dp)=for) ?
t,we want to ensure that xi=1 =?
xj=1.3 This canbe achieved using the constraint (1?xj) ?
(1?xi).Case 2: If P (stance(dq )=against|stance(dp )=against)?
t, we want to ensure that xi=0 =?
xj=0.
Thiscan be achieved using the constraint xj ?
xi.Case 3: If P (stance(dq )=against|stance(dp )=for)?
t, we want to ensure that xi=1 =?
xj=0.
Thiscan be achieved using the constraint xj ?
(1?xi).Case 4: If P (stance(dq )=for|stance(dp)=against)?
t, we want to ensure that xi=0 =?
xj=1.
Thiscan be achieved using the constraint (1?xj) ?
xi.Two points deserve mention.
First, cases 3 and4 correspond to negative constraints, and unlike inMinCut, they can be implemented easily in ILP.Second, if ICs are used, one ILP program will becreated to perform inference over the debate postsin all four domains.5 Evaluation5.1 Experimental SetupResults are expressed in terms of accuracy ob-tained via 5-fold cross validation, where accuracy3Intuitively, if this condition is satisfied, it means thatthere is sufficient evidence that the two nodes from differ-ent domains should have the same stance, and so we convertthe soft ICs into (hard) linear constraints in ILP.
Note that t isa threshold to be tuned using development data.System ABO GAY OBA MARAnand 61.4 62.6 58.1 66.9Anand+AC 72.0 64.9 62.7 67.8Anand+AC+UC 73.7 69.9 64.1 75.4Anand+AC+UC+IC 74.9 70.9 72.7 75.4Table 3: 5-fold cross-validation accuracies.is the percentage of test instances correctly classi-fied.
Since all experiments require the use of de-velopment data for parameter tuning, we use threefolds for model training, one fold for development,and one fold for testing in each fold experiment.5.2 ResultsResults are shown in Table 3.
Row 1 shows theresults of the Anand et al(2011) baseline (seeSection 3) on the four datasets, obtained by train-ing a SVM stance classifier using the SVMlightsoftware.4 Row 2 shows the results of the sec-ond baseline, Anand et als system enhanced withACs.
As we can see, incorporating ACs intoAnand et als system improves its performancesignificantly on all datasets and yields a systemthat achieves an average improvement of 4.6 ac-curacy points.5Next, we incorporate our first type of con-straints, UCs, into the better of the two baselines(i.e., the second baseline).
Results of applying theCRF for modeling UCs to the test posts and post-processing them using the ACs are shown in row 3of Table 3.
As we can see, incorporating UCs intothe second baseline significantly improves its per-formance and yields a system that achieves an av-erage improvement of 3.93 accuracy points.Finally, we incorporate our second type of con-straints, ICs, effectively performing inference overthe CRF output using ILP with ACs and ICs as theinter-post constraints.
Results of this experimentare shown in row 4 of Table 3.
As we can see, in-corporating the ICs significantly improves the per-formance of the system on all but MAR and yieldsa system that achieves an average improvement of2.7 accuracy points.Overall, our inter-post constraints yield a stanceclassification system that significantly outper-forms the better baseline on all four datasets, withan average improvement of 6.63 accuracy points.4For all SVM experiments, the regularization parameter Cis tuned using development data, but the remaining learningparameters are set to their default values.5All significance tests are paired t-tests, with p < 0.05.8195.3 DiscussionNext, we make some observations on the results ofapplying ICs to our datasets.First, ICs do not improve the MAR dataset.
Anexamination of the domains reveals the reason.
Wefind three pairs of ICs involving the other three do-mains ?
ABO, GAY, and OBA ?
in our trainingdata.
More specifically, the stances of the postswritten by an author for these three domains areall positively co-related.
In other words, if an au-thor supports abortion, it is likely that she supportsboth gay rights and Obama as well.
On the otherhand, we find no co-relation between MAR andthe remaining domains.
This means that no ICscan be established between the posts in MAR andthose in the remaining domains.Second, the improvement resulting from the ap-plication of ICs is much larger on the OBA datasetthan on ABO and GAY.
The reason can be at-tributed to the fact that ICs exist more frequentlybetween OBA and ABO and between OBA andGAY than between ABO and GAY.
Specifically,ICs are seen in all five folds of the data in thefirst two pairs of domains, whereas they are seenin only two folds in the last pair of domains.6 Related WorkPrevious work has investigated the use of extra-linguistic constraints to improve stance classifica-tion.
Introduced by Thomas et al(2006), ACs arearguably the most commonly used extra-linguisticconstraints.
Since then, they have been employedand extended in different ways (see, for example,Bansal et al(2008), Burfoot et al(2011), Lu et al(2012), and Walker et al(2012)).ICs are different from ACs in at least two re-spects.
First, ICs are softer than ACs, so accu-rate modeling of ICs has to be based on stance-annotated data.
Although we employ ICs as hardconstraints (owing in part to our use of the ILPframework), they can be used directly as soft con-straints in other frameworks, such as MinCut.
Sec-ond, ICs are inter-domain constraints, whereasACs are intra-domain constraints.
To our knowl-edge, this is the first time inter-domain constraintsare employed for stance classification.There has been work related to the modeling ofuser interaction in a post sequence.
Recall that be-tween two adjacent posts in a post sequence thathave opposing stances, there exists a rebuttal link.Walker et al(2012) employ manually identifiedrebuttal links as hard inter-post constraints dur-ing inference.
However, since automatic discov-ery of rebuttal links is a non-trivial problem, em-ploying gold rebuttal links substantially simplifiesthe stance classification task.
Lu et al(2012), onthe other hand, predict whether a link is of typeagreement or disagreement using a bootstrappedclassifier.
Anand et al(2011) do not predict links.Instead, hypothesizing that the content of the pre-ceding post in a post sequence would be usefulfor predicting the stance of the current post, theyemploy features computed based on the precedingpost when training a stance classifier.
Hence, un-like us, they classify each post independently ofthe others, whereas we classify the posts in a se-quence in dependent relation to each other.The ILP framework has been applied to performjoint inference for a variety of stance predictiontasks.
Lu et al(2012) address the task of discov-ering opposing opinion networks, where the goalis to partition the authors in a debate (e.g., gayrights) based on whether they support or opposethe given issue.
To this end, they employ ILPto coordinate different sources of information.
Inour previous work on debate stance classification(Hasan and Ng, 2012), we employ ILP to coor-dinate the output of two classifiers: a post-stanceclassifier, which determines the stance of a debatepost written for a domain (e.g., gay rights); anda topic-stance classifier, which determines the au-thor?s stance on each topic mentioned in her post(e.g., gay marriage, gay adoption).
In this work,on the other hand, we train only one classifier,but use ILP to coordinate two types of constraints,ACs and ICs.7 ConclusionsWe examined the under-studied task of stanceclassification of ideological debates.
Employingour two types of extra-linguistic constraints yieldsa system that outperforms an improved version ofAnand et als approach by 2.9?10 accuracy points.While the effectiveness of ideology constraints de-pends to some extent on the ?relatedness?
of theunderlying ideological domains, we believe thatthe gains they offer will increase with the num-ber of authors posting in different domains and thenumber of related domains.66Only a small fraction of the authors posted in multipledomains in our datasets: 12% and 5% of them posted in twoand three domains, respectively.820ReferencesRakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social behavior.
In Pro-ceedings of the 12th International Conference onWorld Wide Web, WWW ?03, pages 529?535.Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.Fox Tree, Robeson Bowmani, and Michael Minor.2011.
Cats rule and dogs drool!
: Classifying stancein online debate.
In Proceedings of the 2nd Work-shop on Computational Approaches to Subjectivityand Sentiment Analysis (WASSA 2011), pages 1?9.Alexandra Balahur, Zornitsa Kozareva, and Andre?sMontoyo.
2009.
Determining the polarity andsource of opinions expressed in political debates.
InProceedings of the 10th International Conference onComputational Linguistics and Intelligent Text Pro-cessing, CICLing ?09, pages 468?480.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.The power of negative thinking: Exploiting labeldisagreement in the min-cut classification frame-work.
In Proceedings of the 22nd InternationalConference on Computational Linguistics: Com-panion volume: Posters, pages 15?18.Or Biran and Owen Rambow.
2011.
Identifying justi-fications in written dialogs.
In Proceedings of the2011 IEEE Fifth International Conference on Se-mantic Computing, ICSC ?11, pages 162?168.Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressionalfloor-debate transcripts.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1506?1515.Kazi Saidul Hasan and Vincent Ng.
2012.
Predict-ing stance in ideological debate with rich linguisticknowledge.
In Proceedings of the 24th InternationalConference on Computational Linguistics: Posters,pages 451?460.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Methods -Support Vector Learning, pages 44?56.
MIT Press.Frank Kschischang, Brendan J. Frey, and Hans-AndreaLoeliger.
2001.
Factor graphs and the sum-productalgorithm.
IEEE Transactions on Information The-ory, 47:498?519.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning, pages 282?289.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool Publishers.Yue Lu, Hongning Wang, ChengXiang Zhai, and DanRoth.
2012.
Unsupervised discovery of opposingopinion networks from forum discussions.
In Pro-ceedings of the 21st ACM International Conferenceon Information and Knowledge Management, CIKM?12, pages 1642?1646.Andrew Kachites McCallum.
2002.
Mallet: A ma-chine learning for language toolkit.
http://mallet.cs.umass.edu.Akiko Murakami and Rudy Raymond.
2010.
Supportor oppose?
Classifying positions in online debatesfrom reply activities and opinion expressions.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 869?875.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proceedings of the Eighth Confer-ence on Computational Natural Language Learning,pages 1?8.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 327?335.Marilyn Walker, Pranav Anand, Rob Abbott, and RickyGrant.
2012.
Stance classification using dialogicproperties of persuasion.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 592?596.Yi-Chia Wang and Carolyn P. Rose?.
2010.
Makingconversational structure explicit: Identification ofinitiation-response pairs within online discussions.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 673?676.Ainur Yessenalina, Yisong Yue, and Claire Cardie.2010.
Multi-level structured models for document-level sentiment classification.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1046?1056.821
