Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 43?48,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTranscRater:a Tool for Automatic Speech Recognition Quality EstimationShahab Jalalvand(1,2), Matteo Negri(1), Marco Turchi(1),Jos?e G. C. de Souza(1,2), Daniele Falavigna(1), Mohammed R. H. Qwaider(1)(1)Fondazione Bruno Kessler, Trento, Italy(2)University of Trento, Italy{jalalvand,negri,turchi,desouza,falavi,qwaider}@fbk.euAbstractWe present TranscRater, an open-sourcetool for automatic speech recognition(ASR) quality estimation (QE).
The toolallows users to perform ASR evaluationbypassing the need of reference tran-scripts and confidence information, whichis common to current assessment proto-cols.
TranscRater includes: i) methodsto extract a variety of quality indicatorsfrom (signal, transcription) pairs and ii)machine learning algorithms which makepossible to build ASR QE models exploit-ing the extracted features.
Confirming thepositive results of previous evaluations,new experiments with TranscRater indi-cate its effectiveness both in WER predic-tion and transcription ranking tasks.1 IntroductionHow to determine the quality of an automatic tran-scription without reference transcripts and with-out confidence information?
This is the key prob-lem addressed by research on ASR quality estima-tion (Negri et al, 2014; C. de Souza et al, 2015;Jalalvand et al, 2015b), and the task for whichTranscRater, the tool described in this paper, hasbeen designed.The work on ASR quality estimation (ASR QE)has several motivations.
First, the steady increaseof applications involving automatic speech recog-nition (e.g.
video/TV programs subtitling, voicesearch engines, voice question answering, spokendialog systems, meeting and broadcast news tran-scriptions) calls for an accurate method to esti-mate ASR output quality at run-time.
Often, in-deed, the nature of such applications (consider forinstance spoken dialog systems) requires quick re-sponse capabilities that are incompatible with tra-ditional reference-based protocols.Second, even when real-time processing is nota priority, standard evaluation based on computingword-error rate (WER) against gold references isnot always a viable solution.
In many situations(as in the case of languages for which even theASR training data is scarce), the bottleneck repre-sented by the limited availability of reference tran-scripts and the costs of their manual creation callsfor a method to predict ASR output quality thatis reference-independent.Third, even when designed to bypass the needof references, current quality prediction methodsheavily depend on confidence information aboutthe inner workings of the ASR system that pro-duced the transcriptions (Evermann and Wood-land, 2000; Wessel et al, 2001).
Such informa-tion, describing how the system is certain aboutthe quality of its own hypotheses, often reflectsa biased perspective influenced by individual de-coder features.
More importantly, it is not alwaysaccessible and, in this frequent case, the sole ele-ments available for quality prediction are the sig-nal and its transcription (consider, for instance, theincreasing amount of captioned Youtube videosgenerated by a ?black-box?
ASR system1).
Theseissues call for a method to predict ASR outputquality that is also confidence-independent.TranscRater (Transcription Rater) provides aunified ASR QE framework designed to meet thethree aforementioned requirements.
Its develop-ment was inspired by software previously releasedfor the machine translation (MT) (Specia et al,2013; Shah et al, 2014; Servan et al, 2015) equiv-alent of ASR QE, in which MT quality has to beestimated at run-time and without reference trans-1More than 157 millions in 10 languages, as announcedby Google already in 2012 (source: http://goo.gl/5Wlkjl).43lations (Mehdad et al, 2012; Camargo de Souzaet al, 2013; C. de Souza et al, 2014).
Indeed, thetwo tasks deal with similar issues.
In both cases,we have an input ?source?
(a written sentence anda recorded signal) and an output text (a translationand a transcription) that has to be assessed with-out any pre-created term of comparison.
They canalso be approached with similar supervised clas-sification (C. de Souza et al, 2015) or regressionstrategies (Negri et al, 2014; C. de Souza et al,2015).
Finally, they have similar applications like:?
Deciding if an input source has been correctlyprocessed;?
Ranking the output of multiple independentsystems (Jalalvand et al, 2015b);?
Estimating the human effort required to man-ually revise an output segment;?
Performing data selection for system im-provement based on active learning.To support these applications, TranscRater pro-vides an extensible ASR QE framework consist-ing of a variety of feature extractors and ma-chine learning algorithms.
The implemented fea-ture extraction methods allow capturing predictivequality indicators both from the input signal andfrom the output transcription.
This basic set of?black box?
indicators has been successfully eval-uated in a number of experiments, both on regres-sion and on classification tasks, showing that ASRQE predictions can closely approximate the qual-ity scores obtained with standard reference-basedmethods.
The existing feature extractors can beeasily extended to integrate new features, eithercapturing additional system-independent aspects,or relying on confidence information about theASR system that produced the transcriptions, ifavailable.
Experimental results demonstrate that,also in the ?glass-box?
scenario in which the ASRsystem is known, the available features are ableto improve the performance obtained with confi-dence information.The integration of different machine learningalgorithms makes TranscRater a powerful frame-work to quickly set up an ASR QE model givensome training data, tune it by choosing among thepossible feature configurations and process new,unseen test data to predict their quality.
As a stand-alone environment, with few documented externaldependencies, TranscRater provides the first off-the-shelf solution to approach ASR QE and extendits application to new scenarios.2 ASR QEThe basic ASR QE task consists in training amodel from (signal, transcription, label) triplets,and using it to return quality predictions for a testset of unseen (signal, transcription) instances.
Inthis supervised learning setting, the training la-bels can be either numeric scores (Negri et al,2014) or class identifiers (binary or multi-class)(C. de Souza et al, 2015).
Class assignmentscan be manually done according to some criteria,or inferred by thresholding numeric scores.
Nu-meric quality indicators can be easily obtained bymeasuring the similarity (or the distance) betweenthe transcription and its manually-created refer-ence.
For instance, the models described in pre-vious works on ASR QE learn from training datalabelled with real values obtained by computingthe transcription word error rate (WER2).According to the type of training labels, theproblem can be approached either as a regressionor as a classification task.
As a consequence,also the evaluation metrics will change.
Preci-sion/recall/F1 (or other metrics, such as balancedaccuracy, in case of very unbalanced distributions)will be used for classification while, similar toMT QE, the mean absolute error (MAE) or sim-ilar metrics will be used for regression.A variant of the basic ASR QE task is to con-sider it as a QE-based ranking problem (Jalalvandet al, 2015b), in which each utterance is capturedby multiple microphones or transcribed by multi-ple ASR systems.
In this case, the capability torank transcriptions from the best to the worst canbe evaluated in terms of normalized discounted cu-mulative gain (NDCG) or similar metrics.3 The TranscRater toolTranscRater combines in a single open-sourceframework: i) a set of features capturing differ-ent aspects of transcription quality and ii) differentlearning algorithms suitable to address the chal-lenges posed by different application scenarios.TranscRater internally consists of two main2WER is the minimum edit distance between the tran-scription and the reference.
Edit distance is calculated as thenumber of edits (word insertions, deletions, substitutions) di-vided by the number of words in the reference.44modules: feature extraction and machine learn-ing.
At training stage, the tool receives as in-put a set of signal recordings, their transcriptionsand the corresponding reference transcripts.
Thespeech signals are provided as separate files in theRIFF Microsoft PCM format with 16K samplingrate.
Their transcriptions and the correspondingreferences are provided in single separate text files(one transcription per row).
References are usedto compute the WER label of each training in-stance, thus connecting the problem to the taskformulation provided in ?2.
The features extractedfrom each training instance are passed to the learn-ing module, together with the corresponding label.The label is a WER score which, depending on thetype of problem addressed, can be used either todirectly train a regressor or to infer a ranking formultiple hypotheses.
In either case, the learningmodule will train the corresponding model withthe proper learning algorithm, and tune it using k-fold cross-validation.At test stage, the model is used to predict thelabel of new, unseen (signal, transcription) in-stances.
For each test point, the output is either aWER prediction or a rank, whose reliability can berespectively evaluated in terms of MAE or NDCG(as discussed in ?2).
Output predictions are pro-vided in a single file (one WER prediction per rowfor regression and one rank prediction per row forranking).
MAE or NDCG scores are provided asthe standard output of the test functions.Internally, TranscRater stores the extracted fea-tures in the SVM-light3format.
This makes pos-sible to use the tool as a feature extractor and toembed it in applications different from the onesdescribed in this paper.
The features to be used,the type of learning algorithm, the input files andthe links to resources and libraries can be easilyset through a configuration file.3.1 Feature setsThe feature extraction module of TranscRater al-lows the user to extract 72 features that can be cat-egorized in the following four groups:?
Signal (SIG) features, designed to capturethe difficulty of transcribing the input sig-nal given the general recording conditions inwhich it was acquired;?
Lexical (LEX) features, designed to capture3http://svmlight.joachims.org/the difficulty to transcribe the input signalgiven the pronunciation difficulty and the am-biguity of the terms it contains;?
Language model (LM) features, designed tocapture the plausibility of the transcriptionfrom the fluency point of view;?
Part-of-speech (POS) features, designed tocapture the plausibility of the transcriptionfrom the syntax point of view.SIG (44).
Signal features are extracted usingthe OpenSmile4toolkit (Eyben et al, 2013).
Eachspeech signal is broken down into 25ms lengthframes with 10ms overlap.
For each frame, wecompute 13 Mel Frequency Cepstral Coefficients(MFCC), their delta, acceleration and log-energyas well as the prosody features like fundamentalfrequency (F0), voicing probability, loudness con-tours and pitch.
The final SIG feature vector forthe entire input signal is obtained by averaging thevalues of each feature computed on all the frames.LEX (7).
Lexicon-based features are extractedusing a lexical feature dictionary (optionally pro-vided by the user).
In this dictionary each indi-vidual word is assigned to a feature vector con-taining the frequency of fricatives, liquids, nasals,stops and vowels in its pronunciation.
Other ele-ments of the vector are the number of homophones(words with the same pronunciation) and quasi-homophones (words with similar pronunciation).LM (12).
Language model features includethe mean of word probabilities, the sum of thelog probabilities and the perplexity score for eachtranscription.
In previous experiments (Jalalvandet al, 2015b; Jalalvand and Falavigna, 2015) weshowed that, instead of only one LM, using acombination of neural network and n-gram LMstrained on task-specific and generic data can sig-nificantly improve the accuracy of quality predic-tion.
For this reason, TranscRater allows using upto four different language models: two RNNLM(Mikolov et al, 2010) trained on generic and spe-cific data and two n-gramLM trained on genericand specific data.
To work with neural networkLMs, the tool makes use of RNNLM,5while forn-gram LMs it uses SRILM6(Stolcke et al, 2000).4http://www.audeering.com/research/opensmile\#download5http://www.fit.vutbr.cz/?imikolov/rnnlm/rnnlm-0.3e.tgz6http://www.speech.sri.com/projects/srilm/download.html45POS (9).
Part-of-speech features are extractedusing the TreeTagger.7For each word in the tran-scription, they consider the score assigned to thepredicted POS of the word itself, the previous andthe following one.
This sliding window is usedto compute the average value for the entire tran-scription and obtain the sentence-level POS fea-ture vector.
The intuition is that a low confidenceof the POS tagger in labeling a sentence is an indi-cator of possible syntax issues and, in turn, of poortranscription quality.
POS features also includethe number and the percentage of content words(numbers, nouns, verbs, adjectives, adverbs).These feature groups were successfully tested invarious conditions including clean/noisy data, sin-gle/multiple microphones and ASR systems (Jalal-vand et al, 2015b; Jalalvand et al, 2015a).
In suchconditions, they proved to be a reliable predictorwhen confidence information about the ASR sys-tem inner workings is not accessible.3.2 Learning algorithmsFor regression-based tasks (WER prediction),TranscRater includes an interface to the Scikit-learn package (Pedregosa et al, 2011), a Pythonmachine learning library that contains a large setof classification and regression algorithms.
Basedon the empirical results reported in (Negri et al,2014; C. de Souza et al, 2015; Jalalvand et al,2015b), which indicate that Extremely Random-ized Trees (XRT (Geurts et al, 2006)) is a verycompetitive algorithm in several WER predictiontasks, the current version of the tool exploits XRT.However, adapting the interface to apply other al-gorithms is an easy task and one of the futureextension directions.
The main hyper-parametersof the model, such as the number of tree bags,the number of trees per bag, the number of fea-tures per tree and the number of instances in theleaves, are tuned using grid search with k-foldcross-validation on the training set to minimizethe mean absolute error (MAE) between the trueWERs and the predicted ones.As mentioned before, TranscRater provides thepossibility to evaluate multiple transcriptions (e.g.obtained from different microphones or ASR sys-tems) and rank them based on their quality.
Thiscan be done either indirectly, by exploiting the pre-dicted WER labels in a ?ranking by regression?7http://www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.tar.gzapproach (RR) or directly, by exploiting machine-learned ranking methods (MLR).
To train and testMLR models, TranscRater exploits RankLib8, a li-brary of learning-to-rank algorithms.
The currentversion of the tool includes an interface to the Ran-dom Forest algorithm (RF (Breiman, 2001)), thesame used in (Jalalvand et al, 2015b).MLR predicts ranks through pairwise compari-son between the transcriptions.
The main param-eters such as the number of bags, the number oftrees per bag and the number of leaves per tree aretuned on training set using k-fold cross-validationto maximize the NDCG measure.3.3 ImplementationTranscRater is written in Python and is made ofseveral parts linked together using bash scripts.In order to run the toolkit on Linux, the follow-ing libraries are required: i) Java 8 (JDK-1.8); ii)Python 2.7 (or above) and iii) Scikit-learn (ver-sion 0.15.2).
Moreover, the user has to downloadand compile the following libraries: OpenSmile,RNNLM, SRILM and TreeTagger for the featureextraction module as well as RankLib for usingmachine-learned ranking option.4 BenchmarkingThe features and algorithms contained in Tran-scRater have been successfully used in previousworks (Negri et al, 2014; C. de Souza et al, 2015;Jalalvand et al, 2015b; Jalalvand et al, 2015a).To further investigate their effectiveness, in thissection we provide new results, both in WER pre-diction (MAE) and transcription ranking (NDCG),together with some efficiency analysis (Time inseconds9).
To this aim, we use data from the3rdCHiME challenge,10which were collected formultiple distant microphone speech recognition innoisy environments (Barker et al, 2015).
CHiME-3 data consists of sentences of the Wall StreetJournal corpus, uttered by four speakers in fournoisy environments, and recorded by five micro-phones placed on the frame of a tablet PC (a sixthone, placed on the back, mainly records back-ground noise).
Training and test respectively con-tain 1,640 and 1,320 sentences.
Transcriptions are8https://people.cs.umass.edu/?vdang/data/RankLib-v2.1.tar.gz9Experiments were run with a PC with 8 Intel Xeon pro-cessors 3.4 GHz and 8 GB RAM.10http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/data.html46produced by a baseline ASR system, provided bythe task organizers, which uses the deep neuralnetwork recipe of Kaldi (Povey et al, 2011).In WER prediction, different models built withTranscRater are compared with a baseline com-monly used for regression tasks, which labels allthe test instances with the average WER valuecomputed on the training set.
In ranking mode,baseline results are computed by averaging theNDCG scores obtained in one hundred iterationsin which test instances are randomly ranked.FeaturesTrain&TestTimeTotalTimeMAE?Baseline ?
?
28.7SIG 00m18s 09m32s 27.3LEX+LM+POS 00m19s 01m19s 22.2SIG+LEX+LM+POS 00m26s 10m22s 23.5Table 1: Time and MAE results in regression mode.Table 1 shows the results of models trained withdifferent feature groups for WER prediction witha single microphone.
In terms of time, in this asin the following experiments, the total time (fea-ture extraction + training + test) is mostly de-termined by feature extraction and the bottleneckis clearly represented by the extraction of signal(SIG) features.
In terms of MAE, SIG features arealso those achieving the worst result.
Althoughthey significantly improve over the baseline, theyare outperformed by LEX+LM+POS and, even incombination with them, they do not help.
How-ever, as suggested by previous works like (Ne-gri et al, 2014) in which some of the SIG fea-tures are among the most predictive ones, the use-fulness of signal features highly depends on dataand, in specific conditions, they definitely improveresults.
Their ineffectiveness in the experimentsof this paper likely depends on the lack of word-level time boundaries, which prevented us to com-pute more discriminative features like word log-energies, noise log-energies and signal-to-noiseratio (the best indicator of the acoustic quality ofan input utterance).FeaturesTrain&TestTimeTotalTimeNDCG?Baseline ?
?
73.6SIG 02m03s 15m11s 73.5LEX+LM+POS 01m10s 03m13s 80.4SIG+LEX+LM+POS 05m53s 19m23s 79.4Table 2: Time and NDCG results in ranking by regression.Table 2 shows the results achieved by the samefeature groups when ranking by regression (RR)the transcriptions from five microphones.
In termsof computation time, the higher costs of SIG fea-tures are still evident (the significant increase forall groups is due to the higher number of audiofiles to be processed).
Also in this case, SIGfeatures do not help, neither alone nor in combi-nation with the other groups.
Indeed, the high-est results are achieved by the combination ofLEX+LM+POS.
Their large NDCG improvementover the baseline (+6.8), combined with the sig-nificantly lower computation time, seems to makethis combination particularly suitable for the rank-ing by regression strategy.FeaturesTrain&TestTimeTotalTimeNDCG?Baseline ?
?
73.6SIG 01m14s 13m00s 78.1LEX+LM+POS 00m59s 03m05s 81.3SIG+LEX+LM+POS 01m41s 15m10s 83.1Table 3: Time and NDCG with machine-learned ranking.Table 3 shows the results achieved, in thesame multi-microphone scenario, by the machine-learned ranking approach (MLR).
In terms oftime, MLR is slightly more efficient than RR, atleast on this dataset.
Though surprising (MLRperforms lots of pairwise comparisons, which arein principle more demanding), such difference isnot very informative as it might depend on hyper-parameter settings (e.g.
the number of iterationsfor XRT, manually set to 20), whose optimizationwas out of the scope of our analysis.
In terms ofNDCG, the results are higher compared to RR butthe differences between feature groups are con-firmed.
Interestingly, with MLR even the SIG fea-tures in isolation significantly improve over thebaseline (+4.5 points).
The NDCG improvementwith the combined feature groups is up to 9.5points, confirming the effectiveness of the com-bined features shown in previous works.5 ConclusionWe presented TranscRater, an open-source toolfor ASR quality estimation.
TranscRater pro-vides an extensible framework including featureextractors, machine learning algorithms (for WERprediction and transcription ranking), optimiza-tion and evaluation functions.
Its source codecan be downloaded from https://github.com/hlt-mt/TranscRater.
Its license isFreeBSD, a lax permissive non-copyleft license,compatible with the GNU GPL and with any use,including commercial.47ReferencesJ.
Barker, R. Marxer, E. Vincent, and S. Watanabe.2015.
The third ?CHiME?
Speech Separation andRecognition Challenge: Dataset, Task and Base-lines.
In Proc.
of the 15th IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU),pages 1?9, Scottsdale, Arizona, USA.L.
Breiman.
2001.
Random Forests.
Machine Learn-ing, 45(1):5?32.J.
G. C. de Souza, J. Gonz?alez-Rubio, C. Buck,M.
Turchi, and M. Negri.
2014.
FBK-UPV-UEdin participation in the WMT14 Quality Estima-tion shared-task.
In Proc.
of the Ninth Workshopon Statistical Machine Translation, pages 322?328,Baltimore, Maryland, USA.J.
G. C. de Souza, H. Zamani, M. Negri, M. Turchi,and D. Falavigna.
2015.
Multitask Learning forAdaptive Quality Estimation of Automatically Tran-scribed Utterances.
In Proc.
of the 2015 Conferenceof the North American Chapter of the Associationfor Computational Linguistics - Human LanguageTechnologies (NAACL-HLT), pages 714?724, Den-ver, Colorado, USA.J.
G. Camargo de Souza, C. Buck, M. Turchi, andM.
Negri.
2013.
FBK-UEdin Participation tothe WMT13 Quality Estimation Shared Task.
InProc.
of the Eighth Workshop on Statistical MachineTranslation, pages 352?358, Sofia, Bulgaria.G.
Evermann and P. Woodland.
2000.
Large Vo-cabulary Decoding and Confidence Estimation usingWord Posterior Probabilities.
In Proc.
of the IEEEInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP), pages 1655?1658, Is-tanbul, Turkey.F.
Eyben, F. Weninger, and B. Schuller.
2013.
Recentdevelopments in opensmile, the munich open-sourcemultimedia feature extractor.
In Proc.
of ACM Mul-timedia, pages 835?838, Barcelona, Spain.
ACM.P.
Geurts, D. Ernst, and L. Wehenkel.
2006.
ExtremelyRandomized Trees.
Machine Learning, 63(1):3?42.S.
Jalalvand and D. Falavigna.
2015.
Stacked Auto-Encoder for ASR Error Detection and Word Er-ror Rate Prediction.
In Proc.
of the 16th AnnualConference of the International Speech Communi-cation Association (INTERPSEECH), pages 2142?2146, Dresden, Germany.S.
Jalalvand, D. Falavigna, M. Matassoni, P. Svaizer,and M. Omologo.
2015a.
Boosted Acoustic ModelLearning and Hypotheses Rescoring on the CHiME-3 Task.
In Proc.
of the IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU),pages 409?415, Scottsdale, Arizona, USA.S.
Jalalvand, M. Negri, D. Falavigna, and M. Turchi.2015b.
Driving ROVER With Segment-based ASRQuality Estimation.
In Proc.
of the 53rd AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 1095?1105, Beijing, China.Y.
Mehdad, M. Negri, and M. Federico.
2012.
Matchwithout a Referee: Evaluating MT Adequacy with-out Reference Translations.
In Proc.
of the MachineTranslation Workshop (WMT2012), pages 171?180,Montr?eal, Canada.T.
Mikolov, M. Karafi?at, L. Burget, J. Cernock?y,and S. Khudanpur.
2010.
Recurrent Neural Net-work Based Language Model.
In Proc.
of IN-TERSPEECH, pages 1045?1048, Makuhari, Chiba,Japan.M.
Negri, M. Turchi, J. G. C. de Souza, and F. Daniele.2014.
Quality Estimation for Automatic SpeechRecognition.
In Proc.
of the 25th InternationalConference on Computational Linguistics: Techni-cal Papers (COLING), pages 1813?1823, Dublin,Ireland.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, et al 2011.
Scikit-learn: Ma-chine Learning in Python.
Machine Learning Re-search, 12:2825?2830.D.
Povey, A. Ghoshal, G. Boulianne, L. Burget,O.
Glembek, N. Goel, M. Hannemann, P. Motlicek,Y.
Qian, P. Schwarz, J. Silovsky, G. Stemmer, andK.
Vesely.
2011.
The Kaldi Speech Recogni-tion Toolkit.
In Proc.
of the IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU), Hawaii, USA.C.
Servan, N.-T.
Le, N. Q. Luong, B. Lecouteux, andL.
Besacier.
2015.
An open source toolkit for word-level confidence estimation in machine translation.In Proc.
of the 12th International Workshop on Spo-ken Language Translation (IWSLT), Vietnam.K.
Shah, M. Turchi, and L. Specia.
2014.
An Effi-cient and User-friendly Tool for Machine Transla-tion Quality Estimation.
In Proc.
of the Ninth In-ternational Conference on Language Resources andEvaluation (LREC?14), Reykjavik, Iceland.L.
Specia, K. Shah, J. G. de Souza, and T. Cohn.
2013.Quest - a translation quality estimation framework.In Proc.
of the 51st Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations, pages 79?84, Sofia, Bulgaria.
Associationfor Computational Linguistics.A.
Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R.Gadde, M. Plauche, C. Richey, E. Shriberg, K. Son-mez, F. Weng, and J. Zheng.
2000.
The SRI March2000 HUBS Conversational Speech TranscriptionSystem.
In Proc.
of the NIST Speech TranscriptionWorkshop.F.
Wessel, R. Schluter, K. Macherey, and H. Ney.
2001.Confidence Measures for Large Vocabulary Contin-uous Speech Recognition.
IEEE Transactions onAudio, Speech and Language Processing, 9(3):288?298.48
