Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 238?242,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsUsing Search-Logs to Improve Query TaggingKuzman Ganchev Keith Hall Ryan McDonald Slav PetrovGoogle, Inc.{kuzman|kbhall|ryanmcd|slav}@google.comAbstractSyntactic analysis of search queries is im-portant for a variety of information-retrievaltasks; however, the lack of annotated datamakes training query analysis models diffi-cult.
We propose a simple, efficient proce-dure in which part-of-speech tags are trans-ferred from retrieval-result snippets to queriesat training time.
Unlike previous work, ourfinal model does not require any additional re-sources at run-time.
Compared to a state-of-the-art approach, we achieve more than 20%relative error reduction.
Additionally, we an-notate a corpus of search queries with part-of-speech tags, providing a resource for futurework on syntactic query analysis.1 IntroductionSyntactic analysis of search queries is important fora variety of tasks including better query refinement,improved matching and better ad targeting (Barret al, 2008).
However, search queries differ sub-stantially from traditional forms of written language(e.g., no capitalization, few function words, fairlyfree word order, etc.
), and are therefore difficultto process with natural language processing toolstrained on standard corpora (Barr et al, 2008).
Inthis paper we focus on part-of-speech (POS) taggingqueries entered into commercial search engines andcompare different strategies for learning from searchlogs.
The search logs consist of user queries andrelevant search results retrieved by a search engine.We use a supervised POS tagger to label the resultsnippets and then transfer the tags to the queries,producing a set of noisy labeled queries.
These la-beled queries are then added to the training data andthe tagger is retrained.
We evaluate different strate-gies for selecting which annotation to transfer andfind that using the result that was clicked by the usergives comparable performance to using just the topresult or to aggregating over the top-k results.The most closely related previous work is that ofBendersky et al (2010, 2011).
In their work, un-igram POS tag priors generated from a large cor-pus are blended with information from the top-50results from a search engine at prediction time.
Suchan approach has the disadvantage that it necessitatesaccess to a search engine at run-time and is com-putationally very expensive.
We re-implement theirmethod and show that our direct transfer approach ismore effective, while being simpler to instrument:since we use information from the search engineonly during training, we can train a stand-alone POStagger that can be run without access to additionalresources.
We also perform an error analysis andfind that most of the remaining errors are due to er-rors in POS tagging of the snippets.2 Direct TransferThe main intuition behind our work, Bendersky etal.
(2010) and Ru?d et al (2011), is that standard NLPannotation tools work better on snippets returned bya search engine than on user supplied queries.
Thisis because snippets are typically well-formed En-glish sentences, while queries are not.
Our goal is toleverage this observation and use a supervised POStagger trained on regular English sentences to gen-erate annotations for a large set of queries that canbe used for training a query-specific model.
Perhapsthe simplest approach ?
but also a surprisingly pow-erful one ?
is to POS tag some relevant snippets for238a given query, and then to transfer the tags from thesnippet tokens to matching query tokens.
This ?di-rect?
transfer idea is at the core of all our experi-ments.
In this work, we provide a comparison oftechniques for selecting snippets associated with thequery, as well as an evaluation of methods for align-ing the matching words in the query to those in theselected snippets.Specifically, for each query1 with a correspondingset of ?relevant snippets,?
we first apply the baselinetagger to the query and all the snippets.
We matchany query terms in these snippets, and copy over thePOS tag to the matching query term.
Note that thiscan produce multiple labelings as the relevant snip-pet set can be very diverse and varies even for thesame query.
We choose the most frequent taggingas the canonical one and add it to our training set.We then train a query tagger on all our training data:the original human annotated English sentences andalso the automatically generated query training set.The simplest way to match query tokens to snip-pet tokens is to allow a query token to match anysnippet token.
This can be problematic when wehave queries that have a token repeated with differ-ent parts-of-speech such as in ?tie a tie.?
To make amore precise matching we try a sequence of match-ing rules: First, exact match of the query n-gram.Then matching the terms in order, so the query ?tieaa tieb?
matched to the snippet ?to tie1 a neck tie2?would match tiea:tie1 and tieb:tie2.
Finally, wematch as many query terms as possible.
An earlyobservation showed that when a query term occursin the result URL, e.g., searching for ?irs mileagerate?
results in the page irs.gov, the query termmatching the URL domain name is usually a propernoun.
Consequently we add this rule.In the context of search logs, a relevant snippetset can refer to the top k snippets (including the casewhere k = 1) or the snippet(s) associated with re-sults clicked by users that issued the query.
In ourexperiments we found that different strategies for se-lecting relevant snippets, such as selecting the snip-pets of the clicked results, using the top-10 resultsor using only the top result, perform similarly (seeTable 1).1We skip navigational queries, e.g, amazon or amazon.com,since syntactic analysis of such queries is not useful.Query budget/NN rent/VB a/DET car/NN ClicksSnip 1 .
.
.
Budget/NNP Rent/NNP 2A/NNP Car/NNP .
.
.Snip 2 .
.
.
Go/VB to/TO Budget/NNP 1to/TO rent/VB a/DET car/NN .
.
.Snip 3 .
.
.
Rent/VB a/DET car/NN 1from/IN Budget/NNP .
.
.Figure 1: Example query and snippets as tagged by abaseline tagger as well as associated clicks.By contrast Bendersky et al (2010) use a lin-ear interpolation between a prior probability and thesnippet tagging.
They define pi(t|w) as the relativefrequency of tag t given by the baseline tagger toword w in some corpus and ?
(t|w, s) as the indica-tor function for word w in the context of snippet shas tag t. They define the tagging of a word asargmaxt0.2pi(t|w) + 0.8means:w?s?
(t|w, s) (1)We illustrate the difference between the two ap-proaches in Figure 1.
The numbered rows of thetable correspond to three snippets (with non-queryterms elided).
The strategy that uses the clicks to se-lect the tagging would count two examples of ?Bud-get/NNP Rent/NNP A/NNP Car/NNP?
and one foreach of two other taggings.
Note that snippet 1and the query get different taggings primarily dueto orthographic variations.
It would then add ?bud-get/NNP rent/NNP a/NNP car/NNP?
to its trainingset.
The interpolation approach of Bendersky et al(2010) would tag the query as ?budget/NNP rent/VBa/DET car/NN?.
To see why this is the case, considerthe probability for rent/VB vs rent/NNP.
For rent/VBwe have 0.2 + 0.8?
23 , while for rent/NNP we have0 + 0.8?
13 assuming that pi(VB|rent) = 1.3 Experimental SetupWe assume that we have access to labeled Englishsentences from the PennTreebank (Marcus et al,1993) and the QuestionBank (Judge et al, 2006), aswell as large amounts of unlabeled search queries.Each query is paired with a set of relevant resultsrepresented by snippets (sentence fragments con-taining the search terms), as well as informationabout the order in which the results were shown tothe user and possibly the result the user clicked on.Note that different sets of results are possible for the239same query, because of personalization and rankingchanges over time.3.1 Evaluation DataWe use two data sets for evaluation.
The first is theset of 251 queries from Microsoft search logs (MS-251) used in Bendersky et al (2010, 2011).
Thequeries are annotated with three POS tags represent-ing nouns, verbs and ?other?
tags (MS-251 NVX).We additionally refine the annotation to cover 14POS tags comprising the 12 universal tags of Petrovet al (2012), as well as proper nouns and a specialtag for search operator symbols such as ?-?
(forexcluding the subsequent word).
We refer to thisevaluation set as MS-251 in our experiments.
Wehad two annotators annotate the whole of the MS-251 data set.
Before arbitration, the inter-annotatoragreement was 90.2%.
As a reference, Barr et al(2008) report 79.3% when annotating queries with19 POS tags.
We then examined all the instanceswhere the annotators disagreed, and correctedthe discrepancy.
Our annotations are available athttp://code.google.com/p/query-syntax/.The second evaluation set consists of 500 socalled ?long-tail?
queries.
These are queries that oc-curred rarely in the search logs, and are typicallydifficult to tag because they are searching for less-frequent information.
They do not contain naviga-tional queries.3.2 Baseline ModelWe use a linear chain tagger trained with the aver-aged perceptron (Collins, 2002).
We use the follow-ing features for our tagger: current word, suffixesand prefixes of length 1 to 3; additionally we useword cluster features (Uszkoreit and Brants, 2008)for the current word, and transition features of thecluster of the current and previous word.
Whentraining on Sections 1-18 of the Penn Treebankand testing on sections 22-24, our tagger achieves97.22% accuracy with the Penn Treebank tag set,which is state-of-the-art for this data set.
When weevaluate only on the 14 tags used in our experiments,the accuracy increases to 97.88%.We experimented with 4 baseline taggers (see Ta-ble 2).
WSJ corresponds to training on only thestandard training sections of Wall Street Journal por-tion of the Penn Treebank.
WSJ+QTB adds theMethodMS-251NVXMS-251 long-tailDIRECT-CLICK 93.43 84.11 78.15DIRECT-ALL 93.93 84.39 77.73DIRECT-TOP-1 93.93 84.60 77.60Table 1: Evaluation of snippet selection strategies.QuestionBank as training data.
WSJ NOCASE andWSJ+QTB NOCASE use case-insensitive version ofthe tagger (conceptually lowercasing the text beforetraining and before applying the tagger).
As we willsee, all our baseline models are better than the base-line reported in Bendersky et al (2010); our lower-cased baseline model significantly outperforms eventheir best model.4 ExperimentsFirst, we compared different strategies for selectingrelevant snippets from which to transfer the tags.These systems are: DIRECT-CLICK, which usessnippets clicked on by users; DIRECT-ALL, whichuses all the returned snippets seen by the user;2and DIRECT-TOP-1, which uses just the snippet inthe top result.
Table 1 compares these systems onour three evaluation sets.
While DIRECT-ALL andDIRECT-TOP-1 perform best on the MS-251 datasets, DIRECT-CLICK has an advantage on the longtail queries.
However, these differences are small(<0.6%) suggesting that any strategy for selectingrelevant snippet sets will return comparable resultswhen aggregated over large amounts of data.We then compared our method to the baselinemodels and a re-implementation of Bendersky et al(2010), which we denote BSC.
We use the samematching scheme for both BSC and our system, in-cluding the URL matching described in Section 2.The URL matching improves performance by 0.4-3.0% across all models and evaluation settings.Table 2 summarizes our final results.
For com-parison, Bendersky et al (2010) report 91.6% fortheir final system, which is comparable to our im-plementation of their system when the baseline tag-ger is trained on just the WSJ corpus.
Our best sys-tem achieves a 21.2% relative reduction in error ontheir annotations.
Some other trends become appar-2Usually 10 results, but more if the user viewed the secondpage of results.240MethodMS-251NVXMS-251 long-tailWSJ 90.54 75.07 53.06BSC 91.74 77.82 57.65DIRECT-CLICK 93.36 85.81 76.13WSJ + QTB 90.18 74.86 53.48BSC 91.74 77.54 57.65DIRECT-CLICK 93.01 85.03 76.97WSJ NOCASE 92.87 81.92 74.31BSC 93.71 84.32 76.63DIRECT-CLICK 93.50 84.46 77.48WSJ + QTB NOCASE 93.08 82.70 74.65BSC 93.57 83.90 77.27DIRECT-CLICK 93.43 84.11 78.15Table 2: Tagging accuracies for different baseline settingsand two transfer methods.DIRECT-CLICK is the approachwe propose (see text).
Column MS-251 NVX evaluateswith tags from Bendersky et al (2010).
Their baselineis 89.3% and they report 91.6% for their method.
MS-251 and Long-tail use tags from Section 3.1.
We observesnippets for 2/500 long-tail queries and 31/251 MS-251queries.ent in Table 2.
Firstly, a large part of the benefit oftransfer has to do with case information that is avail-able in the snippets but is missing in the query.
Theuncased tagger is insensitive to this mismatch andachieves significantly better results than the casedtaggers.
However, transferring information from thesnippets provides additional benefits, significantlyimproving even the uncased baseline taggers.
Thisis consistent with the analysis in Barr et al (2008).Finally, we see that the direct transfer method fromSection 2 significantly outperforms the method de-scribed in Bendersky et al (2010).
Table 3 confirmsthis trend when focusing on proper nouns, which areparticularly difficult to identify in queries.We also manually examined a set of 40 querieswith their associated snippets, for which our bestDIRECT-CLICK system made mistakes.
In 32 cases,the errors in the query tagging could be traced backto errors in the snippet tagging.
A better snippettagger could alleviate that problem.
In the remain-ing 8 cases there were problems with the matching?
either the mis-tagged word was not found at all,or it was matched incorrectly.
For example one ofthe results for the query ?bell helmet?
had a snippetcontaining ?Bell cycling helmets?
and we failed tomatch helmet to helmets.Method P R FWSJ + QTB NOCASE 72.12 79.80 75.77BSC 82.87 69.05 75.33BSC + URL 83.01 70.80 76.42DIRECT-CLICK 79.57 76.51 78.01DIRECT-ALL 75.88 78.38 77.11DIRECT-TOP-1 78.38 76.40 77.38Table 3: Precision and recall of the NNP tag on the long-tail data for the best baseline method and the three trans-fer methods using that baseline.5 Related WorkBarr et al (2008) manually annotate a corpus of2722 queries with 19 POS tags and use it to trainand evaluate POS taggers, and also describe the lin-guistic structures they find.
Unfortunately their datais not available so we cannot use it to compare totheir results.
Ru?d et al (2011) create features basedon search engine results, that they use in an NERsystem applied to queries.
They report report sig-nificant improvements when incorporating featuresfrom the snippets.
In particular, they exploit capital-ization and query terms matching URL components;both of which we have used in this work.
Li et al(2009) use clicks in a product data base to train a tag-ger for product queries, but they do not use snippetsand do not annotate syntax.
Li (2010) and Manshadiand Li (2009) also work on adding tags to queries,but do not use snippets or search logs as a source ofinformation.6 ConclusionsWe described a simple method for training a search-query POS tagger from search-logs by transfer-ring context from relevant snippet sets to queryterms.
We compared our approach to previous work,achieving an error reduction of 20%.
In contrast tothe approach proposed by Bendersky et al (2010),our approach does not require access to the searchengine or index when tagging a new query.
By ex-plicitly re-training our final model, it has the abilityto pool knowledge from several related queries andincorporate the information into the model param-eters.
An area for future work is to transfer othersyntactic information, such as parse structures or su-pertags using a similar transfer approach.241ReferencesCory Barr, Rosie Jones, and Moira Regelson.
2008.The linguistic structure of English web-search queries.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages1021?1030, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.M.
Bendersky, W.B.
Croft, and D.A.
Smith.
2010.Structural annotation of search queries using pseudo-relevance feedback.
In Proceedings of the 19th ACMinternational conference on Information and knowl-edge management, pages 1537?1540.
ACM.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proc.
of EMNLP.John Judge, Aoife Cahill, and Josef van Genabith.
2006.Questionbank: Creating a corpus of parse-annotatedquestions.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for ComputationalLinguistics, pages 497?504, Sydney, Australia, July.Association for Computational Linguistics.X.
Li, Y.Y.
Wang, and A. Acero.
2009.
Extractingstructured information from user queries with semi-supervised conditional random fields.
In Proceedingsof the 32nd international ACM SIGIR conference onResearch and development in information retrieval,pages 572?579.
ACM.X.
Li.
2010.
Understanding the semantic structure ofnoun phrase queries.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 1337?1345.
Association for Com-putational Linguistics.M.
Manshadi and X. Li.
2009.
Semantic tagging of websearch queries.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2-Volume 2, pages861?869.
Association for Computational Linguistics.M.
P. Marcus, Mary Ann Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpus ofEnglish: the Penn treebank.
Computational Linguis-tics, 19.S.
Petrov, D. Das, and R. McDonald.
2012.
A universalpart-of-speech tagset.
In Proc.
of LREC.Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, andHinrich Schu?tze.
2011.
Piggyback: Using search en-gines for robust cross-domain named entity recogni-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 965?975, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.J.
Uszkoreit and T. Brants.
2008.
Distributed word clus-tering for large scale class-based language modeling inmachine translation.
In Proc.
of ACL.242
