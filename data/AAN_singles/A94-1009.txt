Does  Baum-Welch  Re-est imat ion :Help Taggers?David ElworthySharp Laboratories of Europe Ltd.Edmund Halley RoadOxford Science ParkOxford OX4 4GAUnited Kingdomdahe@sharp ,  co .
ukAbstractIn part of speech tagging by HiddenMarkov Model, a statistical model is usedto assign grammatical categories to wordsin a text.
Early work in the field re-lied on a corpus which had been taggedby a human annotator to train the model.More recently, Cutting et al (1992) sug-gest that training can be achieved witha minimal lexicon and a limited amountof a priori information about probabilities,by using an Baum-Welch re-estimation toautomatically refine the model.
In thispaper, I report two experiments designedto determine how much manual traininginformation is needed.
The first exper-iment suggests that initial biasing of ei-ther lexical or transition probabilities i es-sential to achieve a good accuracy.
Thesecond experiment reveals that there arethree distinct patterns of Baum-Welch re-estimation.
In two of the patterns, there-estimation ultimately reduces the accu-racy of the tagging rather than improvingit.
The pattern which is applicable canbe predicted from the quality of the ini-tial model and the similarity between thetagged training corpus (if any) and the cor-pus to be tagged.
Heuristics for decid-ing how to use re-estimation i  an effec-tive manner are given.
The conclusions arebroadly in agreement with those of Meri-aldo (1994), but give greater detail aboutthe contributions of different parts of themodel.1 BackgroundPart-of-speech tagging is the process of assigninggrammatical categories to individual words in a cor-pus.
One widely used approach makes use of astatistical technique called a Hidden Markov Model(HMM).
The model is defined by two collections ofparameters: the transition probabilities, which ex-press the probability that a tag follows the precedingone (or two for a second order model); and the lexicalprobabilities, giving the probability that a word has agiven tag without regard to words on either side of it.To tag a text, the tags with non-zero probability arehypothesised for each word, and the most probablesequence of tags given tbe sequence of words is de-termined from the probabilities.
Two algorithms arecommonly used, known as the Forward-Backward(FB) and Viterbi algorithms.
FB assigns a probabil-ity to every tag on every word.
while Viterbi prunestags which cannot be chosen because their proba-bility is lower than the ones of competing hypothe-ses, with a corresponding gain in computational ef-ficiency.
For an introduction to the algorithms, seeCutting et al (1992), or the lucid description bySharman (1990).There are two principal sources for the param-eters of the model.
If a tagged corpus preparedby a human annotator is available, the transitionand lexical probabilities can be estimated from thefrequencies of pairs of tags and of tags associatedwith words.
Alternatively~ a procedure called Baum-Welch (BW) re-estimation may be used, in which anuntagged corpus is passed through the FB algorithmwith some initial ruodel, and the resulting probabili-ties used to determine new values for the lexical andtransition probabilities.
By iterating the algorithmwith the same corpus, the parameters of the modelcan be made to converge on values which are lo-cally optimal for the given text.
The degree of con-vergence can be measured using a perplexity mea-sure, the sum of plog2p for hypothesis probabilitiesp, which gives an estimate of the degree of disorderin the model.
The algorithm is again described byCutting et al and by Sharman, and a mathemati-cal justification for it can be tbund in Huang et al(1990).The first major use of HMMs for part of speechtagging was in CLAWS (Garside et al, 1987) in the1970s.
With the availability of large corpora andfast computers, there has been a recent resurgenceof interest, and a number of variations on and alter-53natives to the FB, Viterbi and BW algorithms havebeen tried; see the work of, for example, Church(Church, 1988), Brill (Brill and Marcus, 1992; Brill,1992), DeRose (DeRose, 1988) and gupiec (Kupiec,1992).
One of the most effective taggers based on apure HMM is that developed at Xerox (Cutting etal., 1992).
An important aspect of this tagger is thatit will give good accuracy with a minimal amount ofmanually tagged training data.
96% accuracy cor-rect assignment of tags to word token, comparedwith a human annotator, is quoted, over a 500000word corpus.The Xerox tagger attempts to avoid the need fora hand-tagged training corpus as far as possible.
In-stead, an approximate model is constructed by hand,which is then improved by BW re-estimation on anuntagged training corpus.
In the above example,8 iterations were sufficient.
The initial model setup so that some transitions and some tags in thelexicon are favoured, and hence having a higher ini-tial probability.
Convergence of the model is im-proved by keeping the number of parameters in themodel down.
To assist in this, low frequency itemsin the lexicon are grouped together into equivalenceclasses, such that all words in a given equivalenceclass have the same tags and lexical probabilities,and whenever one of the words is looked up, then thedata common to all of them is used.
Re-estimationon any of the words in a class therefore counts to-wards re-estimation for all of them 1.The results of the Xerox experiment appear veryencouraging.
Preparing tagged corpora either byhand is labour-intensive and potentially error-prone,and although a semi-automatic approach can beused (Marcus et al, 1993), it is a good thing to re-duce the human involvement as much as possible.However, some careful examination of the experi-ment is needed.
In the first place, Cutting et al donot compare the success rate in their work with thatachieved from a hand-tagged training text with nore-estimation.
Secondly, it is unclear how much theinitial biasing contributes the success rate.
If signif-icant human intervention is needed to provide thebiasing, then the advantages of automatic trainingbecome rather weaker, especially if such interven-tion is needed on each new text domain.
The kindof biasing Cutting et al describe reflects linguisticinsights combined with an understanding of the pre-dictions a tagger could reasonably be expected tomake and the ones it could not.The aim of this paper is to examine the role thattraining plays in the tagging process, by an experi-mental evaluation of how the accuracy of the taggervaries with the initial conditions.
The results sug-gest that a completely unconstrained initial modeldoes not produce good quality results, and that one1The technique was originally developed by Kupiec(Kupiec, 1989).54accurately trained from a hand-tagged corpus willgenerally do better than using an approach based onre-estimation, even when the training comes from adifferent source.
A second experiment shows thatthere are different patterns of re-estimation, andthat these patterns vary more or less regularly with abroad characterisation f the initial conditions.
Theoutcome of the two experiments together points toheuristics for making effective use of training and re-estimation, together with some directions for furtherresearch.Work similar to that described here has been car-ried out by Merialdo (1994), with broadly similarconclusions.
We will discuss this work below.
Theprincipal contribution of this work is to separate theeffect of the lexical and transition parameters of themodel, and to show how the results vary with dif-ferent degree of similarity between the training andtest data.2 The  tagger  and  corporaThe experiments were conducted using two taggers,one written in C at Cambridge University ComputerLaboratory, and the other in C-t-+ at Sharp Labora-tories.
Both taggers implement the FB, Viterbi andBW algorithms.
For training from a hand-taggedcorpus, the model is estimated by counting the num-ber of transitions from each tag i to each tag j,  thetotal occurrence of each tag i, and the total occur-rence of word w with tag i.
Writing these as f ( i , j ) ,f(i) and f ( i ,w) respectively, the transition proba-bility from tag i to tag j is estimated as f ( i , j ) / f ( i )and the lexical probability as f( i ,  w)/f( i ) .
Other es-timation formulae have been used in the past.
Forexample, CLAWS (Garside ct al., 1987) normalisesthe lexical probabilities by the total frequency of theword rather than of the tag.
Consulting the Baum-Welch re-estimation formulae suggests that the ap-proach described is more appropriate, and this isconfirmed by slightly greater tagging accuracy.
Anytransitions not seen in the training corpus are givena small, non-zero probabilityThe lexicon lists, for each word, all of tags seenin the training corpus with their probabilities.
Forwords not found in the lexicon, all open-class tagsare hypothesised, with equal probabilities.
Thesewords are added to the lexicon at the end of firstiteration when re-estimation is being used, so thatthe probabilities of their hypotheses subsequently di-verge from being uniform.To measure the accuracy of the tagger, we com-pare the chosen tag with one provided by a humanannotator.
Various methods of quoting accuracyhave been used in the literature, the most commonbeing the proport ion of words (tokens) receiving thecorrect tag.
A better measure is the proportion ofambiguous words which are given the correct tag,where by ambiguous we mean that more than onetag was hypothesised.
The former figure looks moreimpressive, but the latter gives a better measure ofhow well the tagger is doing, since it factors out thetrivial assignment of tags to non-ambiguous words.For a corpus in which a fraction a of the wordsare ambiguous, and p is the accuracy on ambiguouswords, the overall accuracy can be recovered from1 - a + pa. All of the accuracy figures quoted beloware for ambiguous words only.The training and test corpora were drawn fromthe LOB corpus and the Penn treebank.
The handtagging of these corpora is quite different.
For exam-ple, the LOB tagset used 134 tags, while the Penntreebank tagset has 48.
The general pattern of theresults presented oes not vary greatly with the cor-pus and tagset used.3 The effect of the initial conditionsThe first experiment concerned the effect of the ini-tial conditions on the accuracy using Baum-Welchre-estimation.
A model was trained from a hand-tagged corpus in the manner described above, andthen degraded in various ways to simulate the effectof poorer training, as follows:LexiconDO Un-degraded lexical probabilities, calcu-lated from f ( i ,  w) / f ( i ) .D1 Lexical probabilities are correctly ordered,so that the most frequent ag has the high-est lexical probability and so on, but theabsolute values are otherwise unreliable.D2 Lexical probabilities are proportional tothe overall tag frequencies, and are henceindependent of the actual occurrence of theword in the training corpus.D3 All lexical probabilities have the samevalue, so that the lexicon contains no in-formation other than the possible tags foreach word.Transit ionsTO Un-degraded transition probabilities, cal-culated from f ( i ,  j ) / f ( i ) .T1 All transition probabilities have the samevalue.We could expect to achieve D1 from, say, a printeddictionary listing parts of speech in order of fre-quency.
Perfect training is represented by caseD0+T0.
The Xerox experiments (Cutting et al,1992) correspond to something between D1 and D2,and between TO and T1, in that there is some initialbiasing of the probabilities.For the test, four corpora were constructed fromthe LOB corpus: LOB-B from part B, LOB-L frompart L, LOB-B-G from parts B to G inclusive andLOB-B-J from parts B to J inclusive.
Corpus LOB-B-J was used to train the model, and LOB-B.
LOB-55L and LOB-B-G were passed through thirty itera-tions of the BW algorithm as untagged data.
Ineach case, the best accuracy (on ambiguous words,as usual) from the FB algorithm was noted.
As anadditional test, we tried assigning the most probabletag from the DO lexicon, completely ignoring tag-tagtransitions.
The results are summarised in table 1,for various corpora, where F denotes the "most fre-quent tag" test.
As an example of how these figuresrelate to overall accuracies, LOB-B contains 32.35%ambiguous tokens with respect o the lexicon fromLOB-B-J, and the overall accuracy in the D0+T0case is hence 98.69%.
The general pattern of theresults is similar across the three test corpora, withthe only difference of interest being that case D3+T0does better for LOB-L than tbr the other two cases,and in particular does better than cases D0+T1 andDI+T1.
A possible explanation is that in this casethe test data does not overlap with the training data,and hence the good quality lexicons (DO and D1)have less of an influence.
It is also interesting thatD3+T1 does better than D2+T1.
The reasons forthis are unclear, and the results are not always thesame with other corpora, which suggests that theyare not statistically significant.Several follow-up experiments were used to con-firm the results: using corpora from the Penn tree-bank, using equivalence classes to ensure that alllexical entries have a total relative frequency of atleast 0.01, and using larger corpora.
The specific ac-curacies were different in the various tests, but theoverall patterns remained much the same, suggest-ing that they are not an artifact of the tagset or ofdetails of the text.The observations we can make about these resultsare as follows.
Firstly, two of the tests, D2+T1 andD3+T1, give very poor performance.
Their accuracyis not even as good as that achieved by picking themost frequent ag (although this of course implies alexicon of DO or D1 quality).
It follows that i fBaum-Welch re-estimation is to be an effective technique,the initial data must have either biasing in the tran-sitions (the TO cases) or in the lexical probabilities(cases D0+T1 and DI+T1),  but it is not necessaryto have both (D2/D3+T0 and D0/DI+T1) .Secondly, training from a hand-tagged corpus(case D0+T0) always does best, even when the testdata is from a different source to the training data,as it is for LOB-L.
So perhaps it is worth invest-ing effort in hand-tagging training corpora after all,rather than just building a lexicon and letting re-estimation sort out the probabilities.
But how canwe ensure that re-estimation will produce a goodquality model?
We look further at this issue in thenext section.Table 1: Accuracy using Baum-Welch re-estimation with various initial conditionsDict Trans LOB-B (%)DO TO 95.96D1 TO 95.40D2 TO 90.52D3 TO 92.96DO T1 94.06D1 T1 94.06D2 T1 66.51D3 T1 75.49F - 89.22LOB-L (%)94.7794.4491.8292.8092.2792.2772.4880.8785.32LOB-B-G (%)96.1795.4092.3693.4894.5194.5155.8879.1288.714 Pat terns  of  re -es t imat ionDuring the first experiment, it became apparent thatBaum-Welch re-estimation sometimes decreases theaccuracy as the iteration progresses.
A second ex-periment was conducted to decide when it is ap-propriate to use Baum-Welch re-estimation at all.There seem to be three patterns of behaviour:Classical  A general trend of rising accuracy on eachiteration, with any falls in accuracy being lo-cal.
It indicates that the model is convergingtowards an optimum which is better than itsstarting point.In i t ia l  max imum Highest accuracy on the first it-eration, and falling thereafter.
In this case theinitial model is of better quality than BW canachieve.
That is, while BW will converge on anoptimum, the notion of optimality is with re-spect to the HMM rather than to the linguisticjudgements about correct tagging.Ear ly  max imum Rising accuracy for a small num-ber of iterations (2-4), and then falling as ininitial maximum.An example of each of the three behaviours i shownin figure 1.
The values of the accuracies and the testconditions are unimportant here; all we want to showis the general patterns.
The second experiment hadthe aim of trying to discover which pattern appliesunder which circumstances, in order to help decidehow to train the model.
Clearly, if the expectedpattern is initial maximum, we should not use BWat all, if early maximum, we should halt the processafter a few iterations, and if classical, we should haltthe process in a "standard" way, such as comparingthe perplexity of successive models.The tests were conducted in a similar manner tothose of the first experiment, by building a lexiconand transitions from a hand tagged training corpus,and then applying them to a test corpus with vary-ing degrees of degradation.
Firstly, four differentdegrees of degradation were used: no degradationat all, D2 degradation of the lexicon, T1 degrada-tion of the transitions, and the two together.
Sec-ondly, we selected test corpora with varying degreesof similarity to the training corpus: the same text,text from a similar domain, and text which is signifi-cantly different.
Two tests were conducted with eachcombination of the degradation and similarity, usingdifferent corpora (from the Penn treebank) rangingin size from approximately 50000 words to 500000words.
The re-estimation wa.s allowed to run for teniterations.The results appear ill table 2, showing thebest accuracy achieved (on ambiguous words).the iteration at which it occurred, and thepattern of re-estimation (I = initial maximum,E = early maximum, C = classical).
The patternsare summarised in table 3, each entry in the ta-ble showing the patterns for the two tests under thegiven conditions.
Although there is some variationsin the readings, for example ill the "similar/D0+T0"case, we can draw some general conclusions aboutthe patterns obtained from different sorts of data.When the lexicon is degraded (D2), the pattern isalways classical.
With a good lexicon but either de-graded transitions or a test corpus differing from thetraining corpus, the pattern tends to be early max-imum.
When the test corpus is very similar to themodel, then the pattern is initial maximum.
Fur-thermore, examining the accuracies in table 2, inthe cases of initial maximum and early maximum,the accuracy tends to be significantly higher thanwith classical behaviour.
It seems likely that whatis going on is that the model is converging to to-wards something of similar "quality" in each case,but when the pattern is classical, the convergencestarts from a lower quality model and improves, andin the other cases, it starts from a higher qualityone and deteriorates.
In the case of early maximum,the few iterations where the accuracy is improvingcorrespond to the creation of entries for unknownwords and th~ , fine tuning of ones for known ones,and these changes outweigh those produced by there-estimation.5 D iscuss ionFrom the obserw~tions in the previous section, wepropose the following guidelines for how to train a5694929088Accuracy 86 (%)848280780I I I\_ ~ .
.
- -I I5 10?
.
.
.
.
.
.
.
.
.
.
.
)InitialEarly .
.
.
.Classical.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.I I I15 20 25IterationFigure 1: Example Baum-Welch behaviour30HMM for use in tagging:?
I f  a hand-tagged training corpus is available, useit .
I f  the test and training corpora are near-identical, do not use BW re-estimation; other-wise use for a small number of iterations.?
If no such training corpus is available, but a lexi-con with at least relative frequency data is avail-able, use BW re-estimation for a small numberof iterations.?
If neither training corpus nor lexicon are avail-able, use BW re-estimation with standard con-vergence tests such as perplexity.
Without alexicon, some initial biasing of the transitions isneeded if good results are to be obtained.Similar results are presented by Merialdo (1994),who describes experiments to compare the effectof training from a hand-tagged corpora and us-ing the Baum-Welch algorithm with various initialconditions.
As in the experiments above, BW re-estimation gave a decrease in accuracy when thestarting point was derived from a significant amountof hand-tagged text.
In addition, although Meri-aldo does not highlight the point, BW re-estimationstarting from less than 5000 words of hand-taggedtext shows early maximum behaviour.
Merialdo'sconclusion is that taggers should be trained usingas much hand-tagged text as possible to begin with,and only then applying BW re-estimation with un-tagged text.
The step forward taken in the workhere is to show that there are three patterns of re-estimation behaviour, with differing guidelines forhow to use BW effectively, and that to obtain agood starting point when a hand-tagged corpus is57not available or is too small, either the lexicon orthe transitions must be biased.While these may be useful heuristics from a prac-tical point of view, the next step forward is to lookfor an automatic way of predicting the accuracy ofthe tagging process given a corpus and a model.Some preliminary experiments with using measuressuch as perplexity and the average probability ofhypotheses how that, while they do give an indi-cation of convergence during re-estimation, neithershows a strong correlation with the accuracy.
Per-haps what is needed is a "similarity measure" be-tween two models M and M ~, such that if a cor-pus were tagged with model M, M ~ is the modelobtained by training from the output corpus fromthe tagger as if it were a hand-tagged corpus.
How-ever, preliminary experiments using such measuresas the Kullback-Liebler distance between the initialand new models have again showed that it does notgive good predictions of accuracy.
In the end it mayturn out there is simply no way of making the pre-diction without a source of intbrmation extrinsic toboth model and corpus.AcknowledgementsThe work described here was carried out at the Cam-bridge University Computer Laboratory as part ofEsprit BR Project 7315 "The Acquisition of Lexi-cal Knowledge" (Acquilex-II).
The results were con-firmed and extended at Sharp Laboratories of Eu-rope.
I thank Ted Briscoe for his guidance and ad-vice, and the ANLP referees for their comments.CorpusrelationSameSimilarDifferentSameSimilarDifferentSameSimilarDifferentSameSimilarDifferent* These tests gave anTable 2: Baum-Welch patterns (data)DegradationD0+T0D0+T0D0+T0D0+T1D0+T1D0+T1D2+T0D2+T0D2+T0D2+T1D2+T1D2+T1Test 1Best (%) at93.11 189.95 184.59 291.71 287.93 28O.87 384.87 1081.07 978.54 572.58 968.35 1065.64 10patternIIEEEECCC*CCCTest 2Best (%) at92.83 175.03 286.00 290.52 270.63 382.68 387.31 871.40 480.81 980.53 1062.76 1068.95 10patternIEEEEECC*CCCCearly peak, but the graphs of accuracy against number of iterations how the patternto be classical rather than early maximum.Table 3: Baum-Welch patterns (summary)Degradation D0+T0 D0+T1 D2+T0 D2+T1Corpus relationSame I, I E, ESimilar I, E E, EDifferent E, E E, EC, CC, CC, CC, CC, CC, CReferencesEric Brill and Mitch Marcus (1992).
Tagging anUnfamiliar Text With Minimal Human Supervi-sion.
In AAAI Fall Symposium on ProbabilisticApproaches to Natural Language, pages 10-16.Eric Brill (1992).
A Simple Rule-Based Part ofSpeech Tagger.
In Third Conference on AppliedNatural Language Processing.
Proceedings of theConference.
Trento, Italy, pages 152-155, Associ-ation for Computational Linguistics.Kenneth Ward Church (1988).
A Stochastic PartsProgram and Noun Phrase Parser for UnrestrictedText.
In Second Conference on Applied NaturalLanguage Processing.
Proceedings of the Confer-ence, pages 136-143, Association for Computa-tional Linguistics.Doug Cutting, Julian Kupiec, Jan Pedersen, andPenelope Sibun (1992).
A Practical Part-of-Speech Tagger.
In Third Conference on AppliedNatural Language Processing.
Proceedings of theConference.
Trento, Italy, pages 133-140, Associ-ation for Computational Linguistics.Steven J. DeRose (1988).
Grammatical Cate-gory Disambiguation by Statistical Optimization.Computational Linguistics, 14(1) :31-39.Roger Garside, Geoffrey Leech, and Geoffrey Samp-son (1987).
The Computational Analysis of En-glish: A Corpus-based Approach.
Longman, Lon-don.X.
D. Huang, Y. Ariki, and M. A. Jack (1990).
Hid-den Markov Models for Speech Recognition.
Edin-burgh University Press.J.
M. Kupiec {1989).
Probabilistic Models ofShort and Long Distance Word Dependenciesin Running Text.
In P1vceedings of the 1989DARPA Speech and Natural Language Workshop,pages 290-295.Julian Kupiec (1992).
Robust Part-of-speech Tag-ging Using a Hidden Markov Model.
ComputerSpeech and Language, 6.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz (1993).
Building aLarge Annotated Corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313-330.Bernard Merialdo (1994).
Tagging English Textwith a Probabilistic Model.
Computational Lin-guistics, 20(2): t55-171.R.
A. Sharman (1990).
Hidden Markov Model Meth-ods for Word Tagging.
Technical Report UKSC214, IBM UK Scientific Centre.58
