Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 329?332,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsInformation Content Measures of Semantic SimilarityPerform Better Without Sense-Tagged TextTed PedersenDepartment of Computer ScienceUniversity of Minnesota, DuluthDuluth, MN 55812tpederse@d.umn.eduhttp://wn-similarity.sourceforge.netAbstractThis paper presents an empirical comparisonof similarity measures for pairs of conceptsbased on Information Content.
It shows thatusing modest amounts of untagged text to de-rive Information Content results in higher cor-relation with human similarity judgments thanusing the largest available corpus of manuallyannotated sense?tagged text.1 IntroductionMeasures of semantic similarity based on WordNethave been widely used in Natural Language Pro-cessing.
These measures rely on the structure ofWordNet to produce a numeric score that quantifiesthe degree to which two concepts (represented bya sense or synset) are similar (or not).
In their sim-plest form these measures use path length to identifyconcepts that are physically close to each other andtherefore considered to be more similar than con-cepts that are further apart.While this is a reasonable first approximation tosemantic similarity, there are some well known limi-tations.
Most significant is that path lengths betweenvery specific concepts imply much smaller distinc-tions in semantic similarity than do comparable pathlengths between very general concepts.
One pro-posed improvement is to augment concepts in Word-Net with Information Content values derived fromsense?tagged corpora or from raw unannotated cor-pora (Resnik, 1995).This paper shows that Information Content mea-sures based on modest amounts of unannotated cor-pora have greater correlation with human similarityjudgements than do those based on the largest corpusof sense-tagged text currently available.1 The keyto this success is not in the specific type of corporaused, but rather in increasing the number of con-cepts in WordNet that have counts associated withthem.
These results show that Information Contentmeasures of semantic similarity can be significantlyimproved without requiring the creation of sense?tagged corpora (which is very expensive).1.1 Information ContentInformation Content (IC) is a measure of specificityfor a concept.
Higher values are associated withmore specific concepts (e.g., pitch fork), while thosewith lower values are more general (e.g., idea).
In-formation Content is computed based on frequencycounts of concepts as found in a corpus of text.
Thefrequency associated with a concept is incrementedin WordNet each time that concept is observed, asare the counts of the ancestor concepts in the Word-Net hierarchy (for nouns and verbs).
This is neces-sary because each occurrence of a more specific con-cept also implies the occurrence of the more generalancestor concepts.When a corpus is sense?tagged, mapping occur-rences of a word to a concept is straightforward(since each sense of a word corresponds with a con-cept or synset in WordNet).
However, if the text hasnot been sense?tagged then all of the possible sensesof a given word are incremented (as are their ances-tors).
For example, if tree (as a plant) occurs in asense?tagged text, then only the concept associated1These experiments were done with version 2.05 of Word-Net::Similarity (Pedersen et al, 2004).329with tree as a kind of plant would be incremented.
Ifthe text is untagged, then all of the possible sensesof tree would be incremented (such as the mathe-matical sense of tree, a shoe tree, a plant, etc.)
Inthis case the frequency of all the occurrences of aword are divided equally among the different pos-sible senses.
Thus, if a word occurs 42 times in acorpus and there are six possible senses (concepts),each sense and all of their ancestors would have theirfrequency incremented by seven.2For each concept (synset) c in WordNet, Informa-tion Content is defined as the negative log of theprobability of that concept (based on the observedfrequency counts):IC(c) = ?logP (c)Information Content can only be computed fornouns and verbs in WordNet, since these are the onlyparts of speech where concepts are organized in hi-erarchies.
Since these hierarchies are separate, In-formation Content measures of similarity can onlybe applied to pairs of nouns or pairs of verbs.2 Semantic Similarity MeasuresThere are three Information Content measures im-plemented in WordNet::Similarity: (res) (Resnik,1995), (jcn) (Jiang and Conrath, 1997), and (lin)(Lin, 1998).These measures take as input two concepts c1 andc2 (i.e., senses or synsets in WordNet) and output anumeric measure of similarity.
These measures allrely to varying degrees on the idea of a least com-mon subsumer (LCS); this is the most specific con-cept that is a shared ancestor of the two concepts.For example, the LCS of automobile and scooter isvehicle.The Resnik (res) measure simply uses the Infor-mation Content of the LCS as the similarity value:res(c1, c2) = IC(LCS(c1, c2))The Resnik measure is considered somewhatcoarse, since many different pairs of concepts mayshare the same LCS.
However, it is less likely tosuffer from zero counts (and resulting undefined val-ues) since in general the LCS of two concepts willnot be a very specific concept (i.e., a leaf node in2This is the ?resnik counting option in WordNet::Similarity.WordNet), but will instead be a somewhat more gen-eral concept that is more likely to have observedcounts associated with it.Both the Lin and Jiang & Conrath measures at-tempt to refine the Resnik measure by augmenting itwith the Information Content of the individual con-cepts being measured in two different ways:lin(c1, c2) =2?res(c1,c2)IC(c1)+IC(c2)jcn(c1, c2) = 1IC(c1)+IC(c2)?2?res(c1,c2)All three of these measures have been widelyused in the NLP literature, and have tended to per-form well in a wide range of applications such asword sense disambiguation, paraphrase detection,and Question Answering (c.f., (Resnik, 1999)).3 Experimental DataInformation Content in WordNet::Similarity is (bydefault) derived from SemCor (Miller et al, 1993), amanually sense?tagged subset of the Brown Corpus.It is made up of approximately 676,000 words, ofwhich 226,000 are sense?tagged.
SemCor was orig-inally created using sense?tags from version 1.6 ofWordNet, and has been mapped to subsequent ver-sions to stay current.3 This paper uses version 3.0 ofWordNet and SemCor.WordNet::Similarity also includes a utility (raw-textFreq.pl) that allows a user to derive InformationContent values from any corpus of plain text.
Thisutility is used with the untagged version of SemCorand with various portions of the English GigaWordcorpus (1st edition) to derive alternative InformationContent values.English GigaWord contains more than 1.7 billionwords of newspaper text from the 1990?s and early21st century, divided among four different sources:Agence France Press English Service (afe), Associ-ated Press Worldstream English Service (apw), TheNew York Times Newswire Service (nyt), and TheXinhua News Agency English Service (xie).This paper compares the ranking of pairs of con-cepts according to Information Content measures inWordNet::Similarity with a number of manually cre-ated gold standards.
These include the (RG) (Ruben-stein and Goodenough, 1965) collection of 65 noun3http://www.cse.unt.edu/?rada/downloads.html330Table 1: Rank Correlation of Existing Measuresmeasure WS MC RGvector .46 .89 .73lesk .42 .83 .68wup .34 .74 .69lch .28 .71 .70path .26 .68 .69random -.20 -.16 .15pairs, the (MC) (Miller and Charles, 1991) collec-tion of 30 noun pairs (a subset of RG), and the (WS)WordSimilarity-353 collection of 353 pairs (Finkel-stein et al, 2002).
RG and MC have been scored forsimilarity, while WS is scored for relatedness, whichis a more general and less well?defined notion thansimilarity.
For example aspirin and headache areclearly related, but they aren?t really similar.4 Experimental ResultsTable 1 shows the Spearman?s rank correlation ofseveral other measures of similarity and relatednessin WordNet::Similarity with the gold standards dis-cussed above.
The WordNet::Similarity vector relat-edness measure achieves the highest correlation, fol-lowed closely by the adapted lesk measure.
Theseresults are consistent with previous findings (Pat-wardhan and Pedersen, 2006).
This table also showsresults for several path?based measures.4Table 2 shows the correlation of jcn, res, and linwhen Information Content is derived from 1) thesense-tagged version of SemCor (semcor), 2) Sem-Cor without sense tags (semcor-raw), and 3) steadilyincreasing subsets of the 133 million word xie por-tion of the English GigaWord corpus.
These sub-sets start with the entire first month of xie (199501,from January 1995) and then two months (199501-02), three months (199501-03), up through all of1995 (199501-12).
Thereafter the increments are an-nual, with two years of data (1995-1996), then three(1995-1997), and so on until the entire xie corpus isused (1995-2001).
The afe, apw, and nyt portions ofGigaWord are also used individually and then com-bined all together along with xie (all).4wup is the Wu & Palmer measure, lch is the Leacock &Chodorow measure, path relies on edge counting, and randomprovides a simple sanity check.The size (in tokens) of each corpus is shown in thesecond column of Table 2 (size), which is expressedin thousands (k), millions (m), and billions (b).The third column (cover) shows what percentageof the 96,000 noun and verb synsets in WordNet re-ceive a non-zero frequency count when InformationContent is derived from the specified corpus.
Thesevalues show that the 226,000 sense?tagged instancesin SemCor cover about 24%, and the untagged ver-sion of SemCor covers 37%.
As it happens the cor-relation results for semcor-raw are somewhat betterthan semcor, suggesting that coverage is at least asimportant (if not more so) to the performance of In-formation Content measures than accurate mappingof words to concepts.A similar pattern can be seen with the xie resultsin Table 2.
This again shows that an increase inWordNet coverage is associated with increased per-formance of the Information Content measures.
Ascoverage increases the correlation improves, and infact the results are better than the path?based mea-sures and approach those of lesk and vector (see Ta-ble 1).
The one exception is with respect to the WSgold standard, where vector and lesk perform muchbetter than the Information Content measures.
How-ever, this seems reasonable since they are related-ness measures, and the WS corpus is annotated forrelatedness rather than similarity.As a final test of the hypothesis that coveragematters as much or more than accurate mapping ofwords to concepts, a simple baseline method wascreated that assigns each synset a count of 1, andthen propagates that count up to the ancestor con-cepts.
This is equivalent to doing add-1 smoothingwithout any text (add1only).
This results in corre-lation nearly as high as the best results with xie andsemcor-raw, and is significantly better than semcor.5 ConclusionsThis paper shows that semantic similarity mea-sures based on Information Content can be signif-icantly improved by increasing the coverage of thefrequency counts used to derive Information Con-tent.
Increased coverage can come from unannotatedtext or simply assigning counts to every concept inWordNet and does not require sense?tagged text.331Table 2: Rank Correlation of Information Content Measures From Different Corporajcn lin rescorpus size cover WS MC RG WS MC RG WS MC RGsemcor 226 k .24 .21 .72 .51 .30 .73 .58 .38 .74 .69semcor-raw 670 k .37 .26 .82 .58 .32 .79 .65 .38 .76 .70xie:199501 1.2 m .35 .35 .78 .57 .37 .75 .63 .37 .73 .68199501-02 2.3 m .39 .31 .79 .65 .32 .75 .67 .36 .73 .68199501-03 3.8 m .42 .34 .88 .69 .34 .81 .70 .37 .75 .69199501-06 7.9 m .46 .36 .88 .69 .36 .81 .70 .37 .75 .69199501-09 12 m .49 .36 .88 .69 .36 .81 .70 .37 .75 .69199501-12 16 m .51 .37 .87 .73 .36 .81 .71 .37 .75 .691995-1996 34 m .56 .37 .88 .73 .36 .81 .72 .37 .75 .691995-1997 53 m .58 .37 .88 .73 .36 .81 .71 .37 .75 .691995-1998 73 m .60 .37 .89 .73 .36 .81 .72 .37 .75 .691995-1999 94 m .62 .36 .88 .73 .36 .81 .72 .37 .76 .691995-2000 115 m .63 .36 .89 .73 .36 .81 .71 .37 .76 .701995-2001 133 m .64 .36 .88 .73 .36 .81 .71 .37 .76 .70afe 174 m .66 .36 .88 .81 .36 .80 .78 .37 .77 .79apw 560 m .75 .36 .84 .78 .36 .79 .78 .37 .76 .79nyt 963 m .83 .36 .84 .78 .36 .79 .77 .37 .77 .80all 1.8 b .85 .34 .85 .79 .35 .80 .78 .37 .77 .79add1only 96 k 1.00 .36 .85 .73 .37 .77 .73 .39 .76 .70AcknowledgementsMany thanks to Siddharth Patwardhan and JasonMichelizzi for their exceptional work on Word-Net::Similarity over the years, which has made thisand a great deal of other research possible.ReferencesL.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
ACMTransactions on Information Systems, 20(1):116?131.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceed-ings on International Conference on Research in Com-putational Linguistics, pages 19?33, Taiwan.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the International Con-ference on Machine Learning, Madison, August.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.G.A.
Miller, C. Leacock, R. Tengi, and R. Bunker.
1993.A semantic concordance.
In Proceedings of the Work-shop on Human Language Technology, pages 303?308.S.
Patwardhan and T. Pedersen.
2006.
Using WordNet-based Context Vectors to Estimate the Semantic Relat-edness of Concepts.
In Proceedings of the EACL 2006Workshop on Making Sense of Sense: Bringing Com-putational Linguistics and Psycholinguistics Together,pages 1?8, Trento, Italy, April.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::Similarity - Measuring the relatedness ofconcepts.
In Proceedings of Fifth Annual Meetingof the North American Chapter of the Association forComputational Linguistics, pages 38?41, Boston, MA.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In Proceedings ofthe 14th International Joint Conference on ArtificialIntelligence, pages 448?453, Montreal, August.P.
Resnik.
1999.
Semantic similarity in a taxonomy: Aninformation-based measure and its application to prob-lems of ambiguity in natural language.
Journal of Ar-tificial Intelligence Research, 11:95?130.H.
Rubenstein and J.B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Computational Linguistics,8:627?633.332
