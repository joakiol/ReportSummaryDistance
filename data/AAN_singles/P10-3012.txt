Proceedings of the ACL 2010 Student Research Workshop, pages 67?72,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsA Framework for Figurative Language Detection Based on SenseDifferentiationDaria BogdanovaUniversity of Saint PetersburgSaint Petersburgdasha.bogdanova@gmail.comAbstractVarious text mining algorithms require theprocess of feature selection.
High-level se-mantically rich features, such as figurativelanguage uses, speech errors etc., are verypromising for such problems as e.g.
writ-ing style detection, but automatic extrac-tion of such features is a big challenge.In this paper, we propose a framework forfigurative language use detection.
Thisframework is based on the idea of sensedifferentiation.
We describe two algo-rithms illustrating the mentioned idea.
Weshow then how these algorithms work byapplying them to Russian language data.1 IntroductionVarious text mining algorithms require the pro-cess of feature selection.
For example, author-ship attribution algorithms need to determine fea-tures to quantify the writing style.
Previous workon authorship attribution among computer scien-tists is mostly based on low-level features such asword frequencies, sentence length counts, n-gramsetc.
A significant advantage of such features isthat they can be easily extracted from any corpus.But the study by Batov and Sorokin (1975) showsthat such features do not always provide accuratemeasures for authorship attribution.
The linguisticapproach to the problem involves such high-levelcharacteristics as the use of figurative language,irony, sound devices and so on.
Such character-istics are very promising for the mentioned abovetasks, but the extraction of these features is ex-tremely hard to automate.
As a result, very fewattempts have been made to exploit high-level fea-tures for stylometric purposes (Stamatatos, 2009).Therefore, our long-term objective is the extrac-tion of high-level semantically rich features.Since the mentioned topic is very broad, we fo-cus our attention only on some particular prob-lems and approaches.
In this paper, we examineone of such problems, the problem of automaticfigurative language use detection.
We propose aframework for figurative language detection basedon the idea of sense differentiation.
Then, we de-scribe two algorithms illustrating the mentionedidea.
One of them is intended to decide whethera usage is literal by comparing the texts related tothe target expression and the set of texts relatedto the context itself.
The other is aimed at group-ing instances into literal and non-literal uses and isbased on DBSCAN clustering (Ester et al 1996).We illustrate then how these algorithms work byapplying them to Russian language data.
Finally,we propose some ideas on modifications whichcan significantly improve the accuracy of the al-gorithms.2 Related WorkSporleder and Li (April 2009) proposed an unsu-pervised method for recognition of literal and non-literal use of idiomatic expressions.
Given an id-iom the method detects the presence or absence ofcohesive links between the words the idiom con-sists of and the surrounding text.
When such linksexist, the occurence is considered as a literal us-age and as a non-literal when there are no suchlinks.
For most idioms the experiments showed anaccuracy above 50% (it varies between 11% and98% for different idioms).
The authors then pro-posed an improvement of the algorithm (Li andSporleder, August 2009) by adding the SupportVector Machine classifier as a second stage.
Theyuse the mentioned above unsupervised algorithmto label the training data for the supervised classi-fier.
The average accuracy of the improved algo-rithm is about 90%.
Our approach is also basedon the idea of the relatedness between the expres-sion and the surrounding context.
Unlike the men-tioned study, we do not focus our attention onlyon idioms.
So far we have mostly dealt with ex-67pressions, which are not necessarily idiomatic bythemselves, but become metaphors in a particularcontext (e.g.
?she is the sunshine?, ?life is a jour-ney?)
and expressions that are invented by an au-thor (e.g.
?my heart?s in the Highlands?).
More-over, the improved algorithm (Li and Sporleder,August 2009) is supervised, and our approach isunsupervised.The study by Katz and Giesbrecht (2006) is alsosupervised, unlike ours.
It also considers multi-word expressions that have idiomatic meanings.They propose an algorithm, which computes thevectors for literal and non-literal usages and thenuse the nearest neighbor classifier to label an un-seen occurence of the given idiom.The approach proposed by Birke andSarkar (2006) is nearly unsupervised.
Theyconstructed two seed sets: one consists of literalusages of different expressions and the otherconsists of non-literal usages.
They calculatethe distance between an occurence in questionand these two sets and assign to the occurencethe label of the closest set.
This work, as wellas ours, refers to the ideas from Word SenseDisambiguation area.
Unlike our approach, theauthors focus their attention only on the detectionof figuratevely used verbs and, whereas we onlyrefer to the concepts and ideas of WSD, they adapta particular existing one-word disambiguationmethod.As we have already said, we deal with dif-ferent types of figurative language (metaphors,metonymies etc.).
However, there are some worksaimed at extracting particular types of figura-tive language.
For example, Nissim and Mark-ert (2003) proposed a machine learning algorithmfor metonymy resolution.
They state the problemof metonymy resolution as a classification task be-tween literal use of a word and a number of prede-fined metonymy types.3 Sense DifferentiationWe could treat a figurative meaning of a word asan additional, not common meaning of this word.Actually, some metaphors are quite common (e.g.eye of a needle, leg of a table, etc.)
and are calledcatachretic metaphors.
They appear in a languageto remedy the gap in vocabulary (Black, 1954).These metaphors do not indicate an author?s writ-ing style: an author uses such metaphor for an ob-ject because the language has no other name forthat object.
Therefore the algorithms we are de-veloping do not work with this type of metaphors.Our approach to figurative language detectionis based on the following idea: the fact that thesense of a word significantly differs from the senseof the surrounding text usually indicates that theword is used figuratively.
Two questions arise im-mediately:1.
How do we represent the sense of both theword and the surrounding context?2.
How do we find out that these senses differsignificantly?To answer the first question, we refer to theideas popular in the Word Sense Disambiguationcommunity: sense is a group of contextually simi-lar occurrences of a word (Sch?utze, 1996).
Hence,we represent the senses of both a word and its con-text as sets of documents related to the word andthe context respectively.
These sets can be ob-tained e.g.
by searching Wikipedia, Google or an-other web search engine.
For a word the query canbe the word itself.
As for a text, this query can beformulated as the whole text or as a set of somewords contained in this text.
It seems to us thatquerying the lexical chains (Halliday and Hasan,1976) extracted from the text should provide bet-ter results than querying the whole text.As soon as we have a sense representation forsuch objects as a word and a text, we should finda way to measure the difference between thesesense representations and find out what differenceis strong enough for the considered occurence tobe classified as a non-literal usage.
One way todo this is representing sets of documents as setsof vectors and measuring the distance between thecenters of the obtained vector sets.
Another wayis to apply clustering techniques to the sets and tomeasure the accuracy of the produced clustering.The higher the accuracy is, the more different thesets are.Besides, this can be done by calculating text-to-text semantic similarity using for example themeasure proposed by Mihalcea et al(2006).
Thisis rather difficult in case of the Russian languagebecause at the moment there is no WordNet-liketaxonomies for Russian.In the next section, we propose two algorithmsbased on the mentioned above idea.
We state thealgorithms generally and try to find out experi-68mentally what combination of the described tech-niques provides the best results.4 Finding the Distance to the TypicalContext SetThe algorithm is intended to determine whether aword (or an expression) in a given context is usedliteraly or not.As it was mentioned above, we decided to rep-resent senses of both an expression and a contextas sets of documents.
Our hypothesis is that thesedocument sets differ significantly if and only ifan expression is used figuratevely.
Thus, the al-gorithm decides whether the occurence is literalby comparing two sets of documents: the typicalcontext set, which represents a sense of the expres-sion, and the related context set, which representsa sense of the context.
A naive way to constructthe typical context set is searching some searchingengine (e.g.
Google) for the expression.
Given acontext with a target expression, the related con-text set can be constructed as follows:1.
Remove the target expression from the con-text;2.
Extract the longest lexical chains from the re-sulting context;3.
For every chain put to the set the first N arti-cles retrieved by searching a searching enginefor the chain;After constructing the sets the algorithm shouldestimate the similarity between these two sets.This, for example, can be done by applying anyclustering algorithm to the data and measuring theaccuracy.
Evidently, the higher the accuracy of theobtained clustering is, the more separated the setsare.
It means that, when the usage is literal, theaccuracy should be lower because we try to maketwo clusters out of data that should appear as theonly cluster.We hypothesize that in case of non-literal us-ages these two sets should be significantly sepa-rated.Our experiments include two stages.
During thefirst one we test our idea and estimate the param-eters of the algorithms.
During the second stagewe test the more precise algorithm obtained dur-ing the first stage.For the first stage, we found literal and non-literal occurences of the following Russian wordsand expressions:?????
(snowstorm), ???????
(breath),??????????
(dagger), ???????
(dance),???????
??????
(flexible (flower) stalk),???????
(be drowned), ???????????
(crystal),???????????
???????
(bagpipes), ???
(honey), ?????????
(medicine).For every expression, the typical context set con-sists of the first 10 articles retrieved by searchingGoogle for the expression.
In order to constructthe second set we removed the target expressionfrom the context and manually extracted lexicalchains from the texts, although, the process of lex-ical chains extraction can be done automatically.However the algorithms on lexical chains extrac-tion usually use WordNet to calculate the related-ness, but as it was already mentioned WordNetfor the Russian language does not exist yet.
An-other way to calculate semantic relatedness is us-ing Wikipedia (Mihalcea, 2007; Turdakov and Ve-likhov, 2008), but it takes much effort.
The sec-ond set for each occurence consists of the first 10articles retrieved by searching Google for the ex-tracted chains.
Then we applied k-means cluster-ing algorithm (k = 2) to these sets.
To evaluate theclustering we used measures from the clusteringliterature.
We denote our sets by G = g1, g2andthe clusters obtained by k-means as C = c1, c2.We define a mapping f from the elements of G tothe elements of C, such that each set giis mappedto a cluster cj= f(gi) that has the highest per-centage of common elements with gi.
Precisionand recall for a cluster gi, i = 1, 2 are defined asfollows:Pri=| f(gi) ?
gi|| f(gi) |and Rei=| f(gi) ?
gi|| gi|Precision, Pr, and recall, Re, of the clusteringare defined as the weighted averages of the preci-sion and recall values over the sets:Pr =12(Pr1+ Pr2) and Re =12(Re1+ Re2)F1-measure is defined as the harmonic mean ofprecision and recall, i.e.,F1=2?
Pr ?RePr + Re.Table 1 shows the results of the clustering.
For9 expressions out of 10, the clustering accuracyis higher in case of a metaphorical usage than incase of a literal one.
Moreover, for 9 out of 1069Figurative usage Literal usagePr Re F Pr Re F????
?0,85 0,85 0,85 0,50 0,50 0,50??????
?0,83 0,75 0,79 0,65 0,60 0,63?????????
?0,85 0,85 0,85 0,70 0,65 0,67??????
?0,95 0,95 0,95 0,66 0,65 0,66????????????
?0,85 0,85 0,85 0,88 0,85 0,86?????
?0,85 0,85 0,85 0,81 0,70 0,75??????????
?0,95 0,95 0,95 0,83 0,75 0,78?????????????????
?0,88 0,85 0,86 0,70 0,70 0,70??
?0,90 0,90 0,90 0,88 0,85 0,87????????
?0,90 0,90 0,90 0,81 0,70 0,75Table 1: Results provided by k-means clusteringmetaphorical usages, F-measure is 0,85 or higher.And for 7 out of 10 literal usages, F-measure is0,75 or less.The first stage of the experiments illustrates theidea of sense differentiation.
Based on the ob-tained results, we have concluded, that F-measurevalue equal to 0,85 or higher indicates a figurativeusage, and the value equal to 0,75 or less indicatesa literal usage.At the second stage, we applied the algorithmto several Russian language expressions used lit-erally or figuratively.
The accuracy of the k-meansclustering is shown in Table 2.Figurative usages?????
??????
??
?????
?
????
0,76 0,55 0,64???
1,00 1,00 1,00????
0,90 0,90 0,90????
0,95 0,93 0,94?????
?????
0,88 0,85 0,87???????
0,92 0,90 0,91???????
?
??????
0,88 0,85 0,86?????
????????
????
0,95 0,95 0,95????????
????
0,76 0,55 0,64??????
?????
0,95 0,95 0,95??????
0,80 0,80 0,80?????????
0,91 0,90 0,90Literal usages?????
????
0,71 0,70 0,70?????
?
??????
0,74 0,70 0,71???????
0,66 0,65 0,66?????
0,88 0,85 0,87????
0,95 0,95 0,95?????
0,50 0,50 0,50????
????
0,85 0,85 0,85?????
0,90 0,90 0,90??????
0,65 0,65 0,65???????
0,81 0,70 0,75?????
0,85 0,85 0,85?????
0,91 0,90 0,90Table 2: Testing the algorithm.
Accuracy of thek-means clusteringFor 75% of metaphorical usages F-measure is0,85 or more as was expected and for 50% of lit-eral usages F-measure is 0,75 or less.5 Figurative Language Uses as OutliersThe described above approach is to decidewhether a word in a context is used literally or not.Unlike the first one, the second approach we pro-pose, deals with a set of occurences of a word as tolabel every occurence as ?literal?
or ?non-literal?.We formulate this task as a clustering problem andapply DBSCAN (Ester et al 1996) clustering al-gorithm to the data.
Miller and Charles (1991) hy-pothesized that words with similar meanings areoften used in similar contexts.
As it was men-tioned, we can treat a meaning of a metaphoricusage of an expression as an additional, not com-mon for the expression.
That?s why we expectmetaphorical usages to be ouliers, while clusteringtogether with common (i.e.
literal) usages.
Theo-retically, the algorithm should also distinguish be-tween all literal senses so that the contexts of thesame meaning appear in the same cluster and thecontexts of different meanings - in different clus-ters.
Therefore, ideally, the algorithm should solveword sense discrimination and non-literal usagesdetection tasks simultaneously.For each Russian word shown in Table 3,we extracted from the Russian National Cor-pora (http://ruscorpora.ru/) several lit-eral and non-literal occurences.
Some of thesewords have more than one meaning in Russian,e.g.
????
can be translated as a key or waterspring and the word ????
as a plait, scythe or spit.word literal non-literal???????
(butterfly, bow-tie) 12 2????
(frost) 14 2????
(key, spring(water)) 14 2????
(plait, scythe, spit) 21 2???
(lion, Bulgarian lev) 17 5???
(onion, bow) 17 1????
(flour, pain) 21 2????
(dust) 14 4Table 3: Data used in the first experimentAll the documents are stemmed and all stop-words are removed with the SnowBall Stem-mer (http://snowball.tartarus.org/)for the Russian language.As it was mentioned above, this algorithm isaimed at providing word sense discrimination andnon-literal usages detection simultaneously.
Sofar we have paid attention only to the non-literalusages detection aspects.
DBSCAN algorithm isa density-based clustering algorithm designed to70discover clusters of arbitrary shape.
This algo-rithm requires two parameters: ?
(eps) and theminimum number of points in a cluster (minPts).We set minPts to 3 and run the algorithm fordifferent eps between 1.45 and 1.55.As was mentioned, so far we have consideredonly figurative language detection issues: The al-gorithm marks an instance as a figurative usage iffthe instance is labeled as an outlier.
Thus, we mea-sure the accuracy of the algorithm as follows:precision =| figurative uses |?| outliers || outliers |,recall =| figurative uses |?| outliers || figurative uses |.Figures 1 and 2 show the dependency betweenthe eps parameter and the algorithm?s accuracy fordifferent words.Figure 1: Dependency between eps and F-measureFigure 2: Dependency between eps and F-measureTable 4 shows ?the best?
eps for each word andthe corresponding accuracies of metaphor detec-tionword eps precision recall???????
1.520 0.66 1.00????
1.520 0.50 0.50????
1.500 0.33 1.00????
1.510 0.40 1.00???
1.490 1.00 0.83???
1.505 0.17 1.00????
1.525 0.67 0.50????
1.505 0.50 0.60Table 4: The best eps parameters and correspond-ing accuracies of the algorithm6 Future WorkSo far we have worked only with tf-idf and wordfrequency model for both algorithms.
The nextstep in our study is utilizing different text repre-sentation models, e.g.
second order context vec-tors.
We are also going to develop an efficientparameter estimation procedure for the algorithmbased on DBSCAN clustering.As for the other algorithm, we are going to dis-tinguish between different figurative language ex-pressions:?
one word expressions?
monosemous word?
polysemous word?
multiword expressionsWe expect the basic algorithm to provide dif-ferent accuracy in case of different types of ex-pressions.
Dealing with multiword expressionsand monosemous words should be easier than withpolysemous words: i.e., for monosemous wordwe expect the second set to appear as one cluster,whereas this set for a polysemous word is expectedto have the number of clusters equal to the numberof senses it has.Another direction of the future work is develop-ing an algorithm for figurative language uses ex-traction.
The algorithm has to find figurativelyused expressions in a text.7 ConclusionIn this paper, we have proposed a framework forfigurative language detection based on the idea ofsense differentiation.
We have illustrated how this71idea works by presenting two clustering-based al-gorithms.
The first algorithm deals with only onecontext.
It is based on comparing two context sets:one is related to the expression and the other is se-mantically related to the given context.
The sec-ond algorithm groups the given contexts in literaland non-literal usages.
This algorithm should alsodistinguish between different senses of a word, butwe have not yet paid enough attention to this as-pect.
By applying these algorithms to small datasets we have illustrated how the idea of sense dif-ferentiation works.
These algorithms show quitegood results and are worth further work.AcknowledgmentsThis work was partially supported by RussianFoundation for Basic Research RFBR, grant 10-07-00156.ReferencesVitaly I. Batov and Yury A. Sorokin.
1975.
Text at-tribution based on objective characteristics.
Seriyayazyka i literatury, 34, 1.Julia Birke and Anoop Sarkar.
2006.
A Clustering Ap-proach for the Nearly Unsupervised Recognition ofNonliteral Language.
Proceedings of EACL-06Max Black.
1954.
Metaphor.
Proceedings of the Aris-totelian Society, 55, pp.
273-294.Martin Ester, Hans-Peter Kriegel, J?org Sander andXiaowei Xu.
1996.
A density-based algorithm fordiscovering clusters in large spatial databases withnoise.
Proceedings of the Second International Con-ference on Knowledge Discovery and Data Mining(KDD-96), AAAI Press, pp.
226231Michael Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman, LondonGraham Katz and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.Proceedings of the ACL/COLING-06 Workshop onMultiword Expressions: Identifying and ExploitingUnderlying PropertiesLinlin Li and Caroline Sporleder.
August 2009.
Clas-sifier combination for contextual idiom detectionwithout labeled data.
Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pp.
315-323.Rada Mihalcea, Courtney Corley and Carlo Strappa-rava.
2006.
Corpus-based and Knowledge-basedMeasures of Text Semantic Similarity.
Proceedingsof AAAI-06Rada Mihalcea.
2007.
Using Wikipedia for Auto-matic Word Sense Disambiguation.
Proceedings ofthe North American Chapter of the Association forComputational Linguistics (NAACL 2007)George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):128.Malvina Nissim and Katja Markert.
2003.
Syn-tactic features and word similarity for supervisedmetonymy resolution.
Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics (ACL-03) (Sapporo, Japan, 2003).
56-63.Hinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1), pp.97-123Caroline Sporleder and Linlin Li.
April 2009.
Unsu-pervised recognition of literal and non-literal use ofidiomatic expressions.
Proceedings of EACL-09Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for information Science and Technology,60(3): 538-556.Denis Turdakov and Pavel Velikhov.
2008.
SemanticRelatedness Metric for Wikipedia Concepts Basedon Link Analysis and its Application to Word SenseDisambiguation SYRCoDIS 200872
