Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601?610,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsComputational Approaches to Sentence CompletionGeoffrey Zweig, John C. PlattChristopher MeekChristopher J.C. BurgesMicrosoft ResearchRedmond, WA 98052Ainur YessenalinaCornell UniversityComputer Science Dept.Ithaca, NY 14853Qiang LiuUniv.
of California, IrvineInfo.
& Comp.
Sci.Irvine, California 92697AbstractThis paper studies the problem of sentence-level semantic coherence by answering SAT-style sentence completion questions.
Thesequestions test the ability of algorithms to dis-tinguish sense from nonsense based on a vari-ety of sentence-level phenomena.
We tacklethe problem with two approaches: methodsthat use local lexical information, such as then-grams of a classical language model; andmethods that evaluate global coherence, suchas latent semantic analysis.
We evaluate thesemethods on a suite of practice SAT questions,and on a recently released sentence comple-tion task based on data taken from five ConanDoyle novels.
We find that by fusing localand global information, we can exceed 50%on this task (chance baseline is 20%), and wesuggest some avenues for further research.1 IntroductionIn recent years, standardized examinations haveproved a fertile source of evaluation data for lan-guage processing tasks.
They are valuable for manyreasons: they represent facets of language under-standing recognized as important by educational ex-perts; they are organized in various formats designedto evaluate specific capabilities; they are yardsticksby which society measures educational progress;and they affect a large number of people.Previous researchers have taken advantage of thismaterial to test both narrow and general languageprocessing capabilities.
Among the narrower tasks,the identification of synonyms and antonyms hasbeen studied by (Landauer and Dumais, 1997; Mo-hammed et al, 2008; Mohammed et al, 2011; Tur-ney et al, 2003; Turney, 2008), who used ques-tions from the Test of English as a Foreign Lan-guage (TOEFL), Graduate Record Exams (GRE)and English as a Second Language (ESL) exams.Tasks requiring broader competencies include logicpuzzles and reading comprehension.
Logic puzzlesdrawn from the Law School Administration Test(LSAT) and the GRE were studied in (Lev et al,2004), which combined an extensive array of tech-niques to solve the problems.
The DeepRead sys-tem (Hirschman et al, 1999) initiated a long line ofresearch into reading comprehension based on testprep material (Charniak et al, 2000; Riloff and The-len, 2000; Wang et al, 2000; Ng et al, 2000).In this paper, we study a new class of problemsintermediate in difficulty between the extremes ofsynonym detection and general question answer-ing - the sentence completion questions found onthe Scholastic Aptitude Test (SAT).
These questionspresent a sentence with one or two blanks that needto be filled in.
Five possible words (or short phrases)are given as options for each blank.
All possible an-swers except one result in a nonsense sentence.
Twoexamples are shown in Figure 1.The questions are highly constrained in the sensethat all the information necessary is present in thesentence itself without any other context.
Neverthe-less, they vary widely in difficulty.
The first of theseexamples is relatively simple: the second half of thesentence is a clear description of the type of behaviorcharacterized by the desired adjective.
The secondexample is more sophisticated; one must infer from6011.
One of the characters in Milton Murayama?snovel is considered because he deliber-ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)industrious (E) tyrannical2.
Whether substances are medicines or poisonsoften depends on dosage, for substances that arein small doses can be in large.
(A) useless .. effective(B) mild .. benign(C) curative .. toxic(D) harmful .. fatal(E) beneficial .. miraculousFigure 1: Sample sentence completion questions(Educational-Testing-Service, 2011).the contrast between medicine and poison that thecorrect answer involves a contrast, either useless vs.effective or curative vs. toxic.
Moreover, the first, in-correct, possibility is perfectly acceptable in the con-text of the second clause alone; only irrelevance tothe contrast between medicine and poison eliminatesit.
In general, the questions require a combination ofsemantic and world knowledge as well as occasionallogical reasoning.
We study the sentence comple-tion task because we believe it is complex enough topose a significant challenge, yet structured enoughthat progress may be possible.As a first step, we have approached the prob-lem from two points-of-view: first by exploiting lo-cal sentence structure, and secondly by measuringa novel form of global sentence coherence basedon latent semantic analysis.
To investigate the use-fulness of local information, we evaluated n-gramlanguage model scores, from both a conventionalmodel with Good-Turing smoothing, and with a re-cently proposed maximum-entropy class-based n-gram model (Chen, 2009a; Chen, 2009b).
Alsoin the language modeling vein, but with potentiallyglobal context, we evaluate the use of a recurrentneural network language model.
In all the languagemodeling approaches, a model is used to compute asentence probability with each of the potential com-pletions.
To measure global coherence, we proposea novel method based on latent semantic analysis(LSA).
We find that the LSA based method performsbest, and that both local and global information canbe combined to exceed 50% accuracy.
We report re-sults on a set of questions taken from a collectionof SAT practice exams (Princeton-Review, 2010),and further validate the methods with the recentlyproposed MSR Sentence Completion Challenge set(Zweig and Burges, 2011).Our paper thus makes the following contributions:First, we present the first published results on theSAT sentence completion task.
Secondly, we eval-uate the effectiveness of both local n-gram informa-tion, and global coherence in the form of a novelLSA-based metric.
Finally, we illustrate that the lo-cal and global information can be effectively fused.The remainder of this paper is organized as fol-lows.
In Section 2 we discuss related work.
Section3 describes the language modeling methods we haveevaluated.
Section 4 outlines the LSA-based meth-ods.
Section 5 presents our experimental results.
Weconclude with a discussion in Section 6.2 Related WorkThe past work which is most similar to ours is de-rived from the lexical substitution track of SemEval-2007 (McCarthy and Navigli, 2007).
In this task,the challenge is to find a replacement for a word orphrase removed from a sentence.
In contrast to ourSAT-inspired task, the original answer is indicated.For example, one might be asked to find alternatesfor match in ?After the match, replace any remain-ing fluid deficit to prevent problems of chronic de-hydration throughout the tournament.?
Two consis-tently high-performing systems for this task are theKU (Yuret, 2007) and UNT (Hassan et al, 2007)systems.
These operate in two phases: first they finda set of potential replacement words, and then theyrank them.
The KU system uses just an N-gram lan-guage model to do this ranking.
The UNT systemuses a large variety of information sources, and alanguage model score receives the highest weight.N-gram statistics were also very effective in (Giu-liano et al, 2007).
That paper also explores the useof Latent Semantic Analysis to measure the degreeof similarity between a potential replacement and itscontext, but the results are poorer than others.
Sincethe original word provides a strong hint as to the pos-602sible meanings of the replacements, we hypothesizethat N-gram statistics are largely able to resolve theremaining ambiguities.
The SAT sentence comple-tion sentences do not have this property and thus aremore challenging.Related to, but predating the Semeval lexical sub-stitution task are the ESL synonym questions pro-posed by Turney (2001), and subsequently consid-ered by numerous research groups including Terraand Clarke (2003) and Pado and Lapata (2007).These questions are similar to the SemEval task, butin addition to the original word and the sentencecontext, the list of options is provided.
Jarmasz andSzpakowicz (2003) used a sophisticated thesaurus-based method and achieved state-of-the art perfor-mance, which is 82%.Other work on standardized tests includes the syn-onym and antonym tasks mentioned in Section 1,and more recent work on a SAT analogy task in-troduced by (Turney et al, 2003) and extensivelyused by other researchers (Veale, 2004; Turney andLittman, 2005; D. et al, 2009).3 Sentence Completion via LanguageModelingPerhaps the most straightforward approach to solv-ing the sentence completion task is to form the com-plete sentence with each option in turn, and to eval-uate its likelihood under a language model.
Asdiscussed in Section 2, this was found be be veryeffective in the ranking phase of several SemEvalsystems.
In this section, we describe the suite ofstate-of-the-art language modeling techniques forwhich we will present results.
We begin with n-gram models; first a classical n-gram backoff model(Chen and Goodman, 1999), and then a recently pro-posed class-based maximum-entropy n-gram model(Chen, 2009a; Chen, 2009b).
N-gram models havethe obvious disadvantage of using a very limitedcontext in predicting word probabilities.
There-fore we evaluate the recurrent neural net model of(Mikolov et al, 2010; Mikolov et al, 2011b).
Thismodel has produced record-breaking perplexity re-sults in several tasks (Mikolov et al, 2011a), and hasthe potential to encode sentence-span information inthe network hidden-layer activations.
We have alsoevaluated the use of parse scores, using an off-the-shelf stochastic context free grammar parser.
How-ever, the grammatical structure of the alternatives isoften identical.
With scores differing only in the fi-nal non-terminal/terminal rewrites, this did little bet-ter than chance.
The use of other syntactically de-rived features, for example based on a dependencyparse, are likely to be more effective, but we leavethis for future work.3.1 Backoff N-gram Language ModelOur baseline model is a Good-Turing smoothedmodel trained with the CMU language modelingtoolkit (Clarkson and Rosenfeld, 1997).
For the SATtask, we used a trigram language model trained on1.1B words of newspaper data, described in Section5.1.
All bigrams occurring at least twice were re-tained in the model, along with all trigrams occur-ring at least three times.
The vocabulary consistedof all words occurring at least 100 times in the data,along with every word in the development or testsets.
This resulted in a 124k word vocabulary and59M n-grams.
For the Conan Doyle data, which wehenceforth refer to as the Holmes data (see Section5.1), the smaller amount of training data allowed usto use 4-grams and a vocabulary cutoff of 3.
This re-sulted in 26M n-grams and a 126k word vocabulary.3.2 Maximum Entropy Class-Based N-gramLanguage ModelWord-class information provides a level of abstrac-tion which is not available in a word-level lan-guage model; therefore we evaluated a state-of-the-art class based language model.
Model M (Chen,2009a; Chen, 2009b) is a recently proposed classbased exponential n-gram language model whichhas shown improvements across a variety of tasks(Chen, 2009b; Chen et al, 2009; Emami et al,2010).
The key ideas are the modeling of word n-gram probabilities with a maximum entropy model,and the use of word-class information in the defini-tion of the features.
In particular, each word w isassigned deterministically to a class c, allowing then-gram probabilities to be estimated as the productof class and word partsP (wi|wi?n+1 .
.
.
wi?2wi?1) =P (ci|ci?n+1 .
.
.
ci?2ci?1, wi?n+1 .
.
.
wi?2wi?1)P (wi|wi?n+1 .
.
.
wi?2wi?1, ci).603Both components are themselves maximum entropyn-gram models in which the probability of a wordor class label l given history h is determined by1Z exp(?k fk(h, l)).
The features fk(h, l) used arethe presence of various patterns in the concatena-tion of hl, for example whether a particular suffixis present in hl.3.3 Recurrent Neural Net Language ModelMany of the questions involve long-range depen-dencies between words.
While n-gram models haveno ability to explicitly maintain long-span context,the recently proposed recurrent neural-net model of(Mikolov et al, 2010) does.
Related approacheshave been proposed by (Sutskever et al, 2011;Socher et al, 2011).
In this model, a set of neu-ral net activations s(t) is maintained and updated ateach sentence position t. These activations encapsu-late the sentence history up to the tth word in a real-valued vector which typically has several hundreddimensions.
The word at position t is represented asa binary vector w(t) whose length is the vocabularysize, and with a ?1?
in a position uniquely associatedwith the word, and ?0?
elsewhere.
w(t) and s(t) areconcatenated to predict an output distribution overwords, y(t).
Updating is done with two weight ma-trices u and v and nonlinear functions f() and g()(Mikolov et al, 2011b):x(t) = [w(t)T s(t ?
1)T ]Tsj(t) = f(?ixi(t)uji)yk(t) = g(?jsj(t)vkj)with f() being a sigmoid and g() a softmax:f(x) =11 + exp(?z), g(zm) =exp(zm)?k exp(zk)The output y(t) is a probability distribution overwords, and the parameters u and v are trained withback-propagation to minimize the Kullback-Leibler(KL) divergence between the predicted and observeddistributions.
Because of the recurrent connections,this model is similar to a nonlinear infinite impulseresponse (IIR) filter, and has the potential to modellong span dependencies.
Theoretical considerations(Bengio et al, 1994) indicate that for many prob-lems, this may not be possible, but in practice it isan empirical question.4 Sentence Completion via LatentSemantic AnalysisLatent Semantic Analysis (LSA) (Deerwester et al,1990) is a widely used method for representingwords and documents in a low dimensional vectorspace.
The method is based on applying singularvalue decomposition (SVD) to a matrix W repre-senting the occurrence of words in documents.
SVDresults in an approximation of W by the productof three matrices, one in which each word is rep-resented as a low-dimensional vector, one in whicheach document is represented as a low dimensionalvector, and a diagonal scaling matrix.
The simi-larity between two words can then be quantified asthe cosine-similarity between their respective scaledvectors, and document similarity can be measuredlikewise.
It has been used in numerous tasks, rang-ing from information retrieval (Deerwester et al,1990) to speech recognition (Bellegarda, 2000; Coc-caro and Jurafsky, 1998).To perform LSA, one proceeds as follows.
Theinput is a collection of n documents which are ex-pressed in terms of words from a vocabulary of sizem.
These documents may be actual documents suchas newspaper articles, or simply as in our case no-tional documents such as sentences.
Next, a m x nmatrix W is formed.
At its simplest, the ijth entrycontains the number of times word i has occurred indocument j - its term frequency or TF value.
Moreconventionally, the entry is weighted by some no-tion of the importance of word i, for example thenegative logarithm of the fraction of documents thatcontain it, resulting in a TF-IDF weighting (Saltonet al, 1975).
Finally, to obtain a subspace represen-tation of dimension d, W is decomposed asW ?
USV Twhere U is m x d, V T is d x n, and S is a d x d diag-onal matrix.
In applications, d << n and d << m;for example one might have a 50, 000 word vocab-ulary and 1, 000, 000 documents and use a 300 di-mensional subspace representation.An important property of SVD is that the rowsof US - which represents the words - behave sim-ilarly to the original rows of W , in the sense thatthe cosine similarity between two rows in US ap-proximates the cosine similarity between the corre-604sponding rows in W .
Cosine similarity is defined assim(x,y) = x?y?x??y?
.4.1 Total Word SimilarityPerhaps the simplest way of doing sentence comple-tion with LSA is to compute the total similarity of apotential answer a with the rest of the words in thesentence S, and to choose the most related option.We define the total similarity as:totsim(a,S) =?w?Ssim(a,w)When the completion requires two words, total sim-ilarity is the sum of the contributions for both words.This is our baseline method for using LSA, and oneof the best methods we have found.4.2 Sentence ReconstructionRecall that LSA approximates a weighted word-document matrix W as the product of low rankmatrices U and V along with a scaling matrix S:W ?
USV T .
Using singular value decomposition,this is done so as to minimize the mean square re-construction error?ij Q2ij whereQ = W?USVT .From the basic definition of LSA, each column ofW(representing a document) is represented asWj = USV Tj , (1)that is, as a linear combination of the set of basisfunctions formed by the columns of US, with thecombination weights specified in V Tj .
When a newdocument is presented, it is also possible to repre-sent it in terms of the same basis vectors.
Moreover,we may take the reconstruction error induced by thisrepresentation to be a measure of how consistent thenew document is with the original set of documentsused to determine U S and V (Bellegarda, 2000).It remains to represent a new document in termsof the LSA bases.
This is done as follows (Deer-wester et al, 1990; Bellegarda, 2000), again withthe objective of minimizing the reconstruction error.First, note that since U is column-orthonormal, (1)implies thatVj = W Tj US?1 (2)Thus, if we notionally index a new document by p,we proceed by forming a new column (document)vector Wp using the standard term-weighting, andthen find its LSA-space representation Vp using (2).We can evaluate the reconstruction quality by insert-ing the result in (1).
The reconstruction error is then||(UUT ?
I)Wp||2Note that if all the dimensions are retained, the re-construction error is zero; in the case that only thehighest singular vectors are used, however, it is not.Due to the fact that the sentences vary in length wechoose the number of retained singular vectors as afraction f of the sentence length.
If the answer hasn words we use the top nf components.
In practice,a f of 1.2 was selected on the basis of developmentset results.4.3 A LSA N-gram Language ModelIn the context of speech recognition, LSA has beencombined with classical n-gram language modelsin (Coccaro and Jurafsky, 1998; Bellegarda, 2000).The crux of this idea is to interpolate an n-gram lan-guage model probability with one based on LSA,with the intuition that the standard n-gram modelwill do a good job predicting function words, andthe LSA model will do a good job on words pre-dicted by their long-span context.
This logic makessense for the sentence completion task as well, mo-tivating us to evaluate it.To do this, we adopt the procedure of (Coccaroand Jurafsky, 1998), using linear interpolation be-tween the n-gram and LSA probabilities:p(w|history) =?png(w|history) + (1 ?
?
)plsa(w|history)The probability of a word given its history is com-puted by the LSA model in the following way.
Let hbe the sum of all the LSA word vectors in the his-tory.
Let m be the smallest cosine similarity be-tween h and any word in the vocabulary V : m =minw?V sim(h,w).
The probability of a word w inthe context of history h is given byPlsa(w|h) =sim(h,w) ?
m?q?V (sim(h, q) ?
m)Since similarity can be negative, subtracting theminimum (m) ensures that all the estimated prob-abilities are between 0 and 1.6054.4 Improving Efficiency and ExpressivenessGiven the basic framework described above, a num-ber of enhancements are possible.
In terms of ef-ficiency, recall that it is necessary to perform SVDon a term-document matrix.
The data we used wasgrouped into paragraph ?documents,?
of which therewere over 27 million, with 2.6 million unique words.While the resulting matrix is highly sparse, it is nev-ertheless impractical to perform SVD.
We overcomethis difficulty in two ways.
First, we restrict the setof documents used to those which are ?relevant?
toa given test set.
This is done by requiring that a doc-ument contain at least one of the potential answer-words.
Secondly, we restrict the vocabulary to theset of words present in the test set.
For the sentence-reconstruction method of Section 4.2, we have foundit convenient to do data selection per-sentence.To enhance the expressive power of LSA, the termvocabulary can be expanded from unigrams to bi-grams or trigrams of words, thus adding informationabout word ordering.
This was also used in the re-construction technique.5 Experimental Results5.1 Data ResourcesWe present results with two datasets.
The first istaken from 11 Practice Tests for the SAT & PSAT2011 Edition (Princeton-Review, 2010).
This bookcontains eleven practice tests, and we used all thesentence completion questions in the first five testsas a development set, and all the questions in the lastsix tests as the test set.
This resulted in sets with 95and 108 questions respectively.
Additionally, we re-port results on the recently released MSR SentenceCompletion Challenge (Zweig and Burges, 2011).This consists of a set of 1, 040 sentence completionquestions based on sentences occurring in five Co-nan Doyle Sherlock Holmes novels, and is identicalin format to the SAT questions.
Due to the source ofthis data, we refer to it as the Holmes data.To train models, we have experimented with avariety of data sources.
Since there is no publi-cally available collection of SAT questions suitableto training, our methods have all relied on unsu-pervised data.
Early on, we ran a set of experi-ments to determine the relevance of different typesof data.
Thinking that data from an encyclopediaData Dev % Correct Test % CorrectEncarta 26 33Wikipedia 32 31LA Times 39 42Table 1: Effectiveness of different types of training data.might be useful, we evaluated an electronic versionof the 2003 Encarta encyclopedia, which has ap-proximately 29M words.
Along similar lines, weused a collection of Wikipedia articles consisting of709M words.
This data is the entire Wikipedia as ofJanuary 2011, broken down into sentences, with fil-tering to remove sentences consisting of URLs andWiki author comments.
Finally, we used a com-mercial newspaper dataset consisting of all the LosAngeles Times data from 1985 to 2002, containingabout 1.1B words.
These data sources were evalu-ated using the baseline n-gram LM approach of Sec-tion 3.1.
Initial experiments indicated that that theLos Angeles Times data is best suited to this task(see Table 1), and our SAT experiments use thissource.
For the MSR Sentence Completion data,we obtained the training data specified in (Zweigand Burges, 2011), consisting of approximately 50019th-century novels available from Project Guten-berg, and comprising 48M words.5.2 Human PerformanceTo provide human benchmark performance, weasked six native speaking high school students andfive graduate students to answer the questions on thedevelopment set.
The high-schoolers attained 87%accuracy and the graduate students 95%.
Zweig andBurges (2011) cite a human performance of 91%on the Holmes data.
Statistics from a large cross-section of the population are not available.
As a fur-ther point of comparison, we note that chance per-formance is 20%.5.3 Language Modeling ResultsTable 2 summarizes our language modeling resultson the SAT data.
With the exception of the base-line backoff n-gram model, these techniques weretoo computationally expensive to utilize the full LosAngeles Times corpus.
Instead, as with LSA, a ?rel-evant?
corpus was selected of the sentences whichcontain at least one answer option from either the606Method Data (Dev / Test) Dev Test3-gram GT 1.1B / 1.1B 39% 42%Model M 193M / 236M 35 41RNN 36M / 44M 37 42LSA-LM 293M / 358 M 48 44Table 2: Performance of language modeling methods onSAT questions.Method Dev ppl Dev Test ppl Test3-gram GT 195 36% 190 44%Model M 178 36 175 42RNN 147 37 144 42Table 3: Performance of language modeling methods us-ing identical training data and vocabularies.development or test set.
Separate subsets were madefor development and test data.
This data was furthersub-sampled to obtain the training set sizes indicatedin the second column.
For the LSA-LM, an interpo-lation weight of 0.1 was used for the LSA score, de-termined through optimization on the developmentset.
We see from this table that the language modelsperform similarly and achieve just above 40% on thetest set.To make a more controlled comparison that nor-malizes for the amount of training data, we havetrained Model M, and the Good-Turing model onthe same data subset as the RNN, and with the samevocabulary.
In Table 3, we present perplexity re-sults on a held-out set of dev/test-relevant Los Ange-les Times data, and performance on the actual SATquestions.
Two things are notable.
First, the re-current neural net has dramatically lower perplexitythan the other methods.
This is consistent with re-sults in (Mikolov et al, 2011a).
Secondly, despitethe differences in perplexity, the methods show littledifference on SAT performance.
Because Model Mwas not better, only uses n-gram context, and wasused in the construction of the Holmes data (Zweigand Burges, 2011), we do not consider it further.5.4 LSA ResultsTable 4 presents results for the methods of Sections4.1 and 4.2.
Of all the methods in isolation, the sim-ple approach of Section 4.1 - to use the total cosinesimilarity between a potential answer and the otherwords in the sentence - has performed best.
The ap-Method Dev TestTotal Word Similarity 46% 46%Reconstruction Error 53 41Table 4: SAT performance of LSA based methods.Method Test3-input LSA 46%LSA + Good-Turing LM 53LSA + Good-Turing LM + RNN 52Table 5: SAT test set accuracy with combined methods.proach of using reconstruction error performed verywell on the development set, but unremarkably onthe test set.5.5 Combination ResultsA well-known trick for obtaining best results froma machine learning system is to combine a set ofdiverse methods into a single ensemble (Dietterich,2000).
We use ensembles to get the highest accuracyon both of our data sets.We use a simple linear combination of the out-puts of the other models discussed in this paper.
Forthe LSA model, the linear combination has three in-puts: the total word similarity, the cosine similaritybetween the sum of the answer word vectors and thesum of the rest of sentence?s word vectors, and thenumber of out-of-vocabulary terms in the answer.Each additional language model beyond LSA con-tributes an additional input: the probability of thesentence under that language model.We train the parameters of the linear combinationon the SAT development set.
The training minimizesa loss function of pairs of answers: one correct andone incorrect fill-in from the same question.
We usethe RankNet loss function (Burges et al, 2005):min~wf(~w ?
(~x ?
~y)) + ?||~w||2where ~x are the input features for the incorrect an-swer, ~y are the features for the correct answer, ~ware the weights for the combination, and f(z) =log(1 + exp(z)).
We tune the regularizer via 5-fold cross validation, and minimize the loss usingL-BFGS (Nocedal and Wright, 2006).
The resultson the SAT test set for combining various modelsare shown in Table 5.6075.6 Holmes Data ResultsTo measure the robustness of our approaches, wehave applied them to the MSR Sentence Completionset (Zweig and Burges, 2011), termed the Holmesdata.
In Table 6, we present the results on this set,along with the comparable SAT results.
Note thatthe latter are derived from models trained with theLos Angeles Times data, while the Holmes resultsare derived from models trained with 19th-centurynovels.
We see from this table that the results aresimilar across the two tasks.
The best performingsingle model is LSA total word similarity.For the Holmes data, combining the models out-performs any single model.
We train the linear com-bination function via 5-fold cross-validation: themodel is trained five times, each time on 3/5 of thedata, the regularization tuned on 1/5 of the data, andtested on 1/5.
The test results are pooled across all5 folds and are shown in Table 6.
In this case, thebest combination is to blend LSA, the Good-Turinglanguage model, and the recurrent neural network.6 DiscussionTo verify that the differences in accuracy betweenthe different algorithms are not statistical flukes, weperform a statistical significance test on the out-puts of each algorithm.
We use McNemar?s test,which is a matched test between two classifiers (Di-etterich, 1998).
We use the False Discovery Ratemethod (Benjamini and Hochberg, 1995) to controlthe false positive rate caused by multiple tests.
Ifwe allow 2% of our tests to yield incorrectly falseresults, then for the SAT data, the combination ofthe Good-Turing smoothed language model with anLSA-based global similarity model (52% accuracy)is better that the baseline alone (42% accuracy).Secondly, for the Holmes data, we can state thatLSA total similarity beats the recurrent neural net-work, which in turn is better than the baseline n-gram model.
The combination of all three is sig-nificantly better than any of the individual models.To better understand the system performance andgain insight into ways of improving it, we have ex-amined the system?s errors.
Encouragingly, one-third of the errors involve single-word questionswhich test the dictionary definition of a word.
Thisis done either by stating the definition, or provid-Method SAT HolmesChance 20% 20%GT N-gram LM 42 39RNN 42 45LSA Total Similarity 46 49Reconstruction Error 41 41LSA-LM 44 42Combination 53 52Human 87 to 95 91Table 6: Performance of methods on the MSR SentenceCompletion Challenge, contrasted with SAT test set.ing a stereotypical use of the word.
An example ofthe first case is: ?Great artists are often prophetic(visual): they perceive what we cannot and antici-pate the future long before we do.?
(The system?sincorrect answer is in parentheses.)
An exampleof the second is: ?One cannot help but be movedby Theresa?s heartrending (therapeutic) struggle toovercome a devastating and debilitating accident.
?At the other end of the difficulty spectrum arequestions involving world knowledge and/or logicalimplications.
An example requiring both is, ?Manyfear that the ratification (withdrawal) of more le-nient tobacco advertising could be detrimental topublic health.?
About 40% of the errors require thissort of general knowledge to resolve.
Based on ouranalysis, we believe that future research could prof-itably exploit the structured information present ina dictionary.
However, the ability to identify andmanipulate logical relationships and embed worldknowledge in a manner amenable to logical manip-ulation may be necessary for a full solution.
It isan interesting research question if this could be doneimplicitly with a machine learning technique, for ex-ample recurrent or recursive neural networks.7 ConclusionIn this paper we have investigated methods foranswering sentence-completion questions.
Thesequestions are intriguing because they probe the abil-ity to distinguish semantically coherent sentencesfrom incoherent ones, and yet involve no more con-text than the single sentence.
We find that both localn-gram information and an LSA-based global coher-ence model do significantly better than chance, andthat they can be effectively combined.608ReferencesJ.
Bellegarda.
2000.
Exploiting latent semantic informa-tion in statistical language modeling.
Proceedings ofthe IEEE, 88(8).Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gradi-ent descent is difficult.
IEEE Transactions on NeuralNetworks, 5(2):157 ?166.Y.
Benjamini and Y. Hochberg.
1995.
Controlling thefase discovery rate: a practical and powerful approachto multiple testing.
J. Royal Statistical Society B,53(1):289?300.C.
Burges, T.
Shaked., E. Renshaw, A. Lazier, M. Deeds,N.
Hamilton, and G. Hullender.
2005.
Learning torank using gradient descent.
In Proc.
ICML, pages 89?96.Eugene Charniak, Yasemin Altun, Rodrigo de SalvoBraz, Benjamin Garrett, Margaret Kosmala, TomerMoscovich, Lixin Pang, Changhee Pyo, Ye Sun,Wei Wy, Zhongfa Yang, Shawn Zeller, and LisaZorn.
2000.
Reading comprehension programs ina statistical-language-processing class.
In Proceed-ings of the 2000 ANLP/NAACL Workshop on Read-ing comprehension tests as evaluation for computer-based language understanding sytems - Volume 6,ANLP/NAACL-ReadingComp ?00, pages 1?5.
Asso-ciation for Computational Linguistics.Stanley Chen and Joshua Goodman.
1999.
An empiricalstudy of smoothing techniques for language modeling.Computer Speech and Language, 13(4):359?393.S.
Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, andA.
Sethy.
2009.
Scaling shrinkage-based languagemodels.
In ASRU.S.
Chen.
2009a.
Performance prediction for exponentiallanguage models.
In NAACL-HLT.S.
Chen.
2009b.
Shrinking exponential language models.In NAACL-HLT.P.R.
Clarkson and R. Rosenfeld.
1997.
Statisticallanguage modeling using the CMU-CambridgeToolkit.
In Proceedings ESCA Eurospeech,http://www.speech.cs.cmu.edu/SLM/toolkit.html.N.
Coccaro and D. Jurafsky.
1998.
Towards better in-tegration of semantic predictors in statistical languagemodeling.
In Proceedings, ICSLP.Bollegala D., Matsuo Y., and Ishizuka M. 2009.
Measur-ing the similarity between implicit semantic relationsfrom the web.
InWorldWideWeb Conference (WWW).S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41(96).T.G.
Dietterich.
1998.
Approximate statistical testsfor comparing supervised classification learning algo-rithms.
Neural Computation, 10:1895?1923.T.G.
Dietterich.
2000.
Ensemble methods in machinelearning.
In International Workshop on Multiple Clas-sifier Systems, pages 1?15.
Springer-Verlag.Educational-Testing-Service.
2011.https://satonlinecourse.collegeboard.com/sr/digital assets/assessment/pdf/0833a611-0a43-10c2-0148-cc8c0087fb06-f.pdf.A.
Emami, S. Chen, A. Ittycheriah, H. Soltau, andB.
Zhao.
2010.
Decoding with shrinkage-based lan-guage models.
In Interspeech.Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.2007.
Fbk-irst: Lexical substitution task exploitingdomain and syntagmatic coherence.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations, SemEval ?07, pages 145?148, Stroudsburg, PA,USA.
Association for Computational Linguistics.Samer Hassan, Andras Csomai, Carmen Banea, RaviSinha, and Rada Mihalcea.
2007.
Unt: Subfinder:Combining knowledge sources for automatic lexicalsubstitution.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, SemEval ?07,pages 410?413, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Lynette Hirschman, Mark Light, Eric Breck, and John D.Burger.
1999.
Deep read: A reading comprehensionsystem.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics.Thomas Landauer and Susan Dumais.
1997.
A solutionto Plato?s problem: The latent semantic analysis the-ory of the acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2), pages 211?240.Iddo Lev, Bill MacCartney, Christopher D. Manning, andRoger Levy.
2004.
Solving logic puzzles: from ro-bust processing to precise semantics.
In Proceedingsof the 2nd Workshop on Text Meaning and Interpreta-tion, pages 9?16.
Association for Computational Lin-guistics.Jarmasz M. and Szpakowicz S. 2003.
Roget?s thesaurusand semantic similarity.
In Recent Advances in Natu-ral Language Processing (RANLP).Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations (SemEval-2007), pages 48?53.Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-jeev Khudanpur.
2010.
Recurrent neural networkbased language model.
In Proceedings of Interspeech2010.609Tomas Mikolov, Anoop Deoras, Stefan Kombrink, LukasBurget, and Jan Cernocky.
2011a.
Empirical evalua-tion and combination of advanced language modelingtechniques.
In Proceedings of Interspeech 2011.Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky, and Sanjeev Khudanpur.
2011b.
Ex-tensions of recurrent neural network based languagemodel.
In Proceedings of ICASSP 2011.Saif Mohammed, Bonnie Dorr, and Graeme Hirst.
2008.Computing word pair antonymy.
In Empirical Meth-ods in Natural Language Processing (EMNLP).Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, andPeter D. Turney.
2011.
Measuring degrees of seman-tic opposition.
Technical report, National ResearchCouncil Canada.Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai PhengKwan.
2000.
A machine learning approach to answer-ing questions for reading comprehension tests.
In Pro-ceedings of the 2000 Joint SIGDAT conference on Em-pirical methods in natural language processing andvery large corpora: held in conjunction with the 38thAnnual Meeting of the Association for ComputationalLinguistics - Volume 13, EMNLP ?00, pages 124?132.J.
Nocedal and S. Wright.
2006.
Numerical Optimiza-tion.
Springer-Verlag.Sebastian Pado and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33 (2), pages 161?199.Princeton-Review.
2010.
11 Practice Tests for the SAT& PSAT, 2011 Edition.
The Princeton Review.Ellen Riloff and Michael Thelen.
2000.
A rule-basedquestion answering system for reading comprehensiontests.
In Proceedings of the 2000 ANLP/NAACL Work-shop on Reading comprehension tests as evaluation forcomputer-based language understanding sytems - Vol-ume 6, ANLP/NAACL-ReadingComp ?00, pages 13?19.G.
Salton, A. Wong, and C. S. Yang.
1975.
A VectorSpace Model for Automatic Indexing.
Communica-tions of the ACM, 18(11).Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,and Christopher D. Manning.
2011.
Parsing naturalscenes and natural language with recursive neural net-works.
In Proceedings of the 2011 International Con-ference on Machine Learning (ICML-2011).Ilya Sutskever, James Martens, and Geoffrey Hinton.2011.
Generating text with recurrent neural networks.In Proceedings of the 2011 International Conferenceon Machine Learning (ICML-2011).E.
Terra and C. Clarke.
2003.
Frequency estimates forstatistical word similarity measures.
In Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL).Peter Turney and Michael Littman.
2005.
Corpus-basedlearning of analogies and semantic relations.
MachineLearning, 60 (1-3), pages 251?278.Peter D. Turney, Michael L. Littman, Jeffrey Bigham,and Victor Shnayder.
2003.
Combining independentmodules to solve multiple-choice synonym and anal-ogy problems.
In Recent Advances in Natural Lan-guage Processing (RANLP).Peter D. Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In European Confer-ence on Machine Learning (ECML).Peter Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
In In-ternational Conference on Computational Linguistics(COLING).T.
Veale.
2004.
Wordnet sits the sat: A knowledge-basedapproach to lexical analogy.
In European Conferenceon Artificial Intelligence (ECAI).W.
Wang, J. Auer, R. Parasuraman, I. Zubarev,D.
Brandyberry, and M. P. Harper.
2000.
A ques-tion answering system developed as a project in anatural language processing course.
In Proceed-ings of the 2000 ANLP/NAACL Workshop on Read-ing comprehension tests as evaluation for computer-based language understanding sytems - Volume 6,ANLP/NAACL-ReadingComp ?00, pages 28?35.Deniz Yuret.
2007.
Ku: word sense disambiguationby substitution.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, SemEval?07, pages 207?213, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Geoffrey Zweig and Christopher J.C. Burges.
2011.
TheMicrosoft Research sentence completion challenge.Technical Report MSR-TR-2011-129, Microsoft.610
