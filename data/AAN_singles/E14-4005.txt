Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 22?27,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMeasuring the Similarity between Automatically Generated TopicsNikolaos Aletras and Mark StevensonDepartment of Computer Science,University of Sheffield,Regent Court, 211 Portobello,Sheffield,United Kingdom S1 4DP{n.aletras, m.stevenson}@dcs.shef.ac.ukAbstractPrevious approaches to the problem ofmeasuring similarity between automati-cally generated topics have been based oncomparison of the topics?
word probabilitydistributions.
This paper presents alterna-tive approaches, including ones based ondistributional semantics and knowledge-based measures, evaluated by compari-son with human judgements.
The bestperforming methods provide reliable esti-mates of topic similarity comparable withhuman performance and should be used inpreference to the word probability distri-bution measures used previously.1 IntroductionTopic models (Blei et al., 2010) have proved to beuseful for interpreting and organising the contentsof large document collections.
It seems intuitivelyplausible that some automatically generated topicswill be similar while others are dis-similar.
For ex-ample, a topic about basketball (team game jamesseason player nba play knicks coach league) ismore similar to a topic about football (world cupteam soccer africa player south game match goal)than one about the global finance (fed financialbanks federal reserve bank bernanke rule crisiscredit).
Methods for automatically determiningthe similarity between topics have several poten-tial applications, such as analysis of corpora to de-termine topics being discussed (Hall et al., 2008)or within topic browsers to decide which topicsshould be shown together (Chaney and Blei, 2012;Gretarsson et al., 2012; Hinneburg et al., 2012).Latent Dirichlet Allocation (LDA) (Blei et al.,2003) is a popular type of topic model but can-not capture such correlations unless the seman-tic similarity between topics is measured.
Othertopic models, such as the Correlated Topic Model(CTM) (Blei and Lafferty, 2006), overcome thislimitation and identify correlations between top-ics.Approaches to identifying similar topics for arange of tasks have been described in the litera-ture but they have been restricted to using informa-tion from the word probability distribution to com-pare topics and have not been directly evaluated.Word distributions have been compared using avariety of measures such as KL-divergence (Li andMcCallum, 2006; Wang et al., 2009; Newman etal., 2009), cosine measure (He et al., 2009; Ram-age et al., 2009) and the average Log Odds Ratio(Chaney and Blei, 2012).
Kim and Oh (2011) alsoapplied the cosine measure and KL-Divergencewhich were compared with four other measures:Jaccard?s Coefficient, Kendall?s ?
coefficient, Dis-count Cumulative Gain and Jensen Shannon Di-vergence (JSD).This paper compares a wider range of ap-proaches to measuring topic similarity than pre-vious work.
In addition these measures are eval-uated directly by comparing them against humanjudgements.2 Measuring Topic SimilarityWe compare measures based on word probabilitydistributions (Section 2.1), distributional semanticmethods (Sections 2.2-2.4), knowledge-based ap-proaches (Section 2.5) and their combination (Sec-tion 2.6).2.1 Topic Word Probability DistributionWe first experimented with measures based oncomparison of the topics?
word distributions (seeSection 1), by applying the JSD, KL-divergenceand Cosine approaches and the Log Odds Ratio(Chaney and Blei, 2012).222.2 Topic Model Semantic SpaceThe semantic space generated by the topic modelcan be used to represent the topics and the topicwords.
By definition each topic is a probabilitydistribution over the words in the training corpus.For a corpus with D documents and V words, atopic model learns a relation between words andtopics, T , as a T ?V matrix,W, that indicates theprobability of each word in each topic.
W is thetopic model semantic space and each topic wordcan be represented as a vector, Vi, with topics asfeatures weighted by the probability of the wordin each topic.
The similarity between two topicsis computed as the average pairwise cosine sim-ilarity between their top-10 most probable words(TS-Cos).2.3 Reference Corpus Semantic SpaceTopic words can also be represented as vectorsin a semantic space constructed from an externalsource.
We adapt the method proposed by Aletrasand Stevenson (2013) for measuring topic coher-ence using distributional semantics1.Top-N Features A semantic space is con-structed considering only the top n most frequentwords in Wikipedia (excluding stop words) as con-text features.
Each topic word is represented as avector of n features weighted by computing thePointwise Mutual Information (PMI) (Church andHanks, 1989) between the topic word and eachcontext feature, PMI(wi, wj)?.
?
is a variable forassigning more importance to higher PMI values.In our experiments, we set ?
= 3 and found thatthe best performance is obtained for n = 5000.Similarity between two topics is defined as the av-erage cosine similarity of the topic word vectors(RCS-Cos-N).Topic Word Space Alternatively, we consideronly the top-10 topic words from the two topicsas context features to generate topic word vectors.Then, topic similarity is computed as the pairwisecosine similarity of the topic word vectors (RCS-Cos-TWS).Word Association Topic similarity can also becomputed by applying word association measuresdirectly.
Newman et al.
(2010) measure topiccoherence as the average PMI between the topicwords.
This approach can be adapted to measure1Wikipedia is used as a reference corpus to count wordco-occurrences and frequencies using a context window of?10 words centred on a topic word.topic similarity by computing the average pairwisePMI between the topic words in two topics (PMI).2.4 Training Corpus Semantic SpaceTerm-Document Space A matrixX can be cre-ated using the training corpus.
Each term (row)represents a topic word vector.
Element xijin Xis the tf.idf of the term i in document j. Topicsimilarity is computed as the pairwise cosine sim-ilarity of the topic word vectors (TCS-Cos-TD).Word Co-occurrence in Training DocumentsAlternatively, we generate a matrix Z of co-document frequencies.
The matrix Z consists ofV rows and columns representing the V vocab-ulary words.
Element zijis the log of the num-ber of documents that contains the words i andj normalised by the document frequency, DF, ofthe word j. Mimno et al.
(2011) introduced thatmetric to measure topic coherence.
We adaptedit to estimate topic similarity by aggregating theco-document frequency of the words between twotopics (Doc-Co-occ).2.5 Knowledge-based MethodsUKB (Agirre et al., 2009) is used to generate aprobability distribution over WordNet synsets foreach word in the vocabulary V of the topic modelusing the Personalized PageRank algorithm.
Thesimilarity between two topic words is calculatedby transforming these distributions into vectorsand computing the cosine metric.
The similar-ity between two topics is computed by measur-ing pairwise similarity between their top-10 topicwords and selecting the highest score.Explicit Semantic Analysis (ESA) proposed byGabrilovich and Markovitch (2007) transformsthe topic keywords into vectors that consist ofWikipedia article titles weighted by their relevanceto the keyword.
For each topic, the centroid iscomputed from the keyword vectors.
Similaritybetween topics is computed as the cosine similar-ity of the ESA centroid vectors.2.6 Feature Combination Using SVRWe also evaluate the performance of a supportvector regression system (SVR) (Vapnik, 1998)with a linear kernel using a combination of ap-proaches described above as features2.
The systemis trained and tested using 10-fold cross validation.2With the exception of JSD, features based on the topics?word probability distributions were not used by SVR since itwas found that including them reduced performance.233 EvaluationData We created a data set consisting of pairs oftopics generated by two topic models (LDA andCTM) over two document collections using differ-ent numbers of topics.
The first consists of 47,229news articles from New York Times (NYT) in theGigaWord corpus and the second contains 50,000articles from ukWAC (Baroni et al., 2009).
Eacharticle is tokenised then stop words and words ap-pearing fewer than five times in the corpora re-moved.
This results in a total of 57,651 unique to-kens for the NYT corpus and 72,672 for ukWAC.LDA Topics are learned by training LDA mod-els over the two corpora using gensim3.
The num-ber of topics is set to T = 50, 100, 200 and hy-perparameters, ?
and ?, are set to1T.
Randomlyselecting pairs of topics will result to a data setin which the majority of pairs would not be simi-lar.
We overcome that problem by assuming thatthe JSD between likely relevant pairs will be lowwhile it will be higher for less relevant pairs oftopics.
We selected 800 pairs of topics.
600 pairsrepresent topics with similar word distributions (inthe top 6 most relevant topics ranked by JSD).
Theremaining 200 pairs were selected randomly.CTM is trained using the EM algorithm4.
Thenumber of topics to learn is set to T =50, 100, 200 and the rest of the settings are set totheir default values.
The topic graph generated byCTM was used to create all the possible pairs be-tween topics that are connected.
This results in atotal of 70, 468 and 695 pairs in NYT, and a totalof 80, 246 and 258 pairs in ukWAC for the 50, 100and 200 topics respectively.Incoherent topics are removed using an ap-proach based on distributional semantics (Aletrasand Stevenson, 2013).
Each topic is representedusing the top 10 words with the highest marginalprobability.Human Judgements of Topic Similarity wereobtained using an online crowdsourcing platform,Crowdflower.
Annotators were provided withpairs of topics and were asked to judge how simi-lar the topics are by providing a rating on a scale of0 (completely unrelated) to 5 (identical).
The av-erage response for each pair was calculated in or-der to create the final similarity judgement for useas a gold-standard.
The average Inter-Annotator3http://radimrehurek.com/gensim4http://www.cs.princeton.edu/?blei/ctm-c/index.htmlagreement (IAA) across all pairs for all of the col-lections is in the range of 0.53-0.68.
The data settogether with gold-standard annotations is freelyavailable5.4 ResultsTable 1 shows the correlation (Spearman) betweenthe topic similarity metrics described in Section 2and average human judgements for the LDA andCTM topic pairs.
It also shows the performanceof a Word Overlap baseline which measures thenumber of terms that two topics have in commonnormalised by the total number of topic terms.The correlations obtained using the topics?word probability distributions (Section 2.1), i.e.JSD, KL-divergence and Cos, are comparable withthe baseline for all of the topic collections andtopic models.
The metric proposed by Chaneyand Blei (2012) also compares probability distri-butions and fails to perform well on either dataset.
These results suggest that these metrics maybe sensitive to the high dimensionality of the vo-cabulary.
They also assign high similarity to top-ics that contain ambiguous words, resulting in lowcorrelations with human judgements.Performance of the cosine of the word vec-tor (TS-Cos) in the Topic Model Semantic Space(Section 2.2) varies implying that the quality of thelatent space generated by LDA and CTM is sensi-tive to the number of topics.The similarity metrics that use the referencecorpus (Section 2.3) consistently produce goodcorrelations for topic pairs generated using bothLDA and CTM.
The best overall correlation for asingle feature in most cases is obtained using av-erage PMI (in a range of 0.43-0.74).
The perfor-mance of the distributional semantic metric usingthe Topic Word Space (RCS-Cos-TWS) is com-parable and slightly lower for the top-N features(RCS-Cos-N).
This indicates that the referencecorpus covers a broader range of semantic subjectsthan the latent space produced by the topic model.When the term-document matrix from the train-ing corpus is used as a vector space (Section 2.4)performance is worse than when the referencecorpus is used.
In addition, using co-documentfrequency derived from the training corpus doesnot correlate particularly well with human judge-ments.
These methods are sensitive to the sizeof the corpus, which may be too small to gener-5http://staffwww.dcs.shef.ac.uk/people/N.Aletras/resources/topicSim.tar.gz24Spearman?s rLDA CTMNYT ukWAC NYT ukWACMethod 50 100 200 50 100 200 50 100 200 50 100 200BaselineWord Overlap 0.32 0.40 0.51 0.22 0.32 0.41 0.56 0.45 0.49 0.35 0.33 0.53Topic Word Probability DistributionJSD 0.37 0.44 0.53 0.29 0.30 0.34 0.59 0.43 0.49 0.38 0.34 0.60KL-Divergence 0.29 0.29 0.41 0.20 0.24 0.33 0.54 0.39 0.56 0.31 0.29 0.47Cos 0.31 0.37 0.59 0.30 0.30 0.36 0.58 0.45 0.52 0.50 0.40 0.58Chaney and Blei (2012) 0.16 0.26 0.18 0.29 0.21 0.25 0.29 0.40 0.31 -0.23 0.12 0.61Topic Model Semantic SpaceTS-Cos 0.35 0.41 0.67 0.29 0.35 0.42 0.67 0.51 0.49 0.51 0.42 0.42Reference Corpus Semantic SpaceRCS-Cos-N 0.37 0.46 0.61 0.35 0.32 0.39 0.60 0.47 0.61 0.57 0.42 0.41RCS-Cos-TWS 0.40 0.54 0.70 0.38 0.43 0.51 0.63 0.59 0.62 0.60 0.55 0.54PMI 0.43 0.63 0.74 0.43 0.53 0.64 0.68 0.70 0.64 0.58 0.62 0.64Training Corpus Semantic SpaceTCS-Cos-TD 0.36 0.42 0.67 0.29 0.31 0.40 0.64 0.54 0.58 0.49 0.43 0.43Doc-Co-occ 0.28 0.29 0.45 0.28 0.22 0.30 0.65 0.36 0.57 0.31 0.26 0.34Knowledge-basedUKB 0.25 0.38 0.56 0.22 0.35 0.41 0.52 0.41 0.40 0.41 0.43 0.42ESA 0.43 0.58 0.71 0.46 0.55 0.61 0.69 0.67 0.64 0.70 0.62 0.61Feature CombinationSVR 0.46 0.64 0.75 0.46 0.58 0.66 0.72 0.71 0.62 0.60 0.65 0.66IAA 0.54 0.58 0.61 0.53 0.56 0.60 0.68 0.68 0.64 0.67 0.63 0.64Table 1: Results for various approaches to topic similarity.
All correlations are significant p < 0.001.Underlined scores denote best performance of a single feature.
Bold denotes best overall performance.ate reliable estimates of tf.idf or co-document fre-quency.ESA, one of the knowledge-based methods(Section 2.5), performs well and is comparable to(or in some cases better than) PMI.
UKB doesnot perform particularly well because the topicsoften contain named entities that do not exist inWordNet.
ESA is based on Wikipedia and doesnot suffer from this problem.
Overall, metrics forcomputing topic similarity based on rich semanticresources (e.g.
Wikipedia) are more appropriatethan metrics based on the topic model itself be-cause of the limited size of the training corpus.Combining the features using SVR gives thebest overall result for LDA (in the range 0.46-0.75) and CTM (0.60-0.72).
However, the fea-ture combination performs slightly lower than thebest single feature in two cases when CTM isused (T=200, NYT and T=50, ukWAC).
Analy-sis of the coefficients produced by the SVR ineach fold demonstrated that including JSD andthe Word Overlap reduce SVR performance.
Werepeated the experiments by removing these fea-tures6which resulted in higher correlations (0.64and 0.65 respectively).Another interesting observation is that usingLDA the correlations of the various similarity met-6These features are useful for the other experiments sinceperformance drops when they are removed.rics with human judgements increase with thenumber of topics for both corpora.
This resultis consistent with the findings of Stevens et al.
(2012) that topic model coherence increases withthe number of topics.
Fewer topics makes the taskof identifying similar topics more difficult becauseit is likely that they will contain some terms that donot relate to the topic?s main subject.
Correlationsin CTM are more stable for different number oftopics because of the nature of the model, the pairshave been generated using the topic graph whichby definition contains correlated topics.5 ConclusionsWe explored the task of determining the similar-ity between pairs of automatically generated top-ics and described a range of approaches to theproblem.
We constructed a data set of pairs oftopics generated by two topic models, LDA andCTM, together with human judgements of simi-larity.
The data set was used to evaluate a widerange of approaches.
The most interesting findingis the poor performance of the metrics based onword probability distributions previously used forthis task.
Our results demonstrate that word asso-ciation measures, such as PMI, and state-of-the-arttextual similarity metrics, such as ESA, are moreappropriate.25ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics (NAACL-HLT ?09), pages 19?27, Boulder, Col-orado.Nikolaos Aletras and Mark Stevenson.
2013.
Evaluat-ing topic coherence using distributional semantics.In Proceedings of the 10th International Conferenceon Computational Semantics (IWCS 2013) ?
LongPapers, pages 13?22, Potsdam, Germany.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wideweb: a collection of very large linguistically pro-cessed web-crawled corpora.
Language resourcesand evaluation, 43(3):209?226.David Blei and John Lafferty.
2006.
Correlated topicmodels.
In Y. Weiss, B. Sch?olkopf, and J. Platt,editors, Advances in Neural Information ProcessingSystems 18, pages 147?154.
MIT Press, Cambridge,MA.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.David Blei, Lawrence Carin, and David Dunson.
2010.Probabilistic topic models.
Signal Processing Mag-azine, IEEE, 27(6):55?65.Allison June-Barlow Chaney and David M. Blei.
2012.Visualizing topic models.
In Proceedings of theSixth International AAAI Conference on Weblogsand Social Media, Dublin, Ireland.Kenneth Ward Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information, and lexicog-raphy.
In Proceedings of the 27th Annual Meetingof the Association for Computational Linguistics,pages 76?83, Vancouver, British Columbia, Canada.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofthe International Joint Conference on Artificial In-telligence (IJCAI ?07), pages 1606?1611.Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-jiev, Tobias H?ollerer, Arthur Asuncion, David New-man, and Padhraic Smyth.
2012.
TopicNets: Visualanalysis of large text corpora with topic modeling.ACM Trans.
Intell.
Syst.
Technol., 3(2):23:1?23:26.David Hall, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Studying the history of ideas usingtopic models.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 363?371, Honolulu, Hawaii.Qi He, Bi Chen, Jian Pei, Baojun Qiu, Prasenjit Mi-tra, and Lee Giles.
2009.
Detecting topic evolutionin scientific literature: how can citations help?
InProceedings of the 18th ACM Conference on Infor-mation and Knowledge Management (CIKM ?09),pages 957?966, Hong Kong, China.Alexander Hinneburg, Rico Preiss, and Ren?e Schr?oder.2012.
TopicExplorer: Exploring document collec-tions with topic models.
In Peter A. Flach, TijlBie, and Nello Cristianini, editors, Machine Learn-ing and Knowledge Discovery in Databases, volume7524 of Lecture Notes in Computer Science, pages838?841.
Springer Berlin Heidelberg.Dongwoo Kim and Alice Oh.
2011.
Topic chainsfor understanding a news corpus.
In ComputationalLinguistics and Intelligent Text Processing, pages163?176.
Springer.Wei Li and Andrew McCallum.
2006.
Pachinko allo-cation: Dag-structured mixture models of topic cor-relations.
In Proceedings of the 23rd InternationalConference on Machine Learning (ICML ?06), pages577?584.David Mimno, Hanna Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages262?272, Edinburgh, Scotland, UK.David Newman, Arthur Asuncion, Padhraic Smyth,and Max Welling.
2009.
Distributed algorithms fortopic models.
J. Mach.
Learn.
Res., 10:1801?1828.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation oftopic coherence.
In Human Language Technologies:The 2010 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (NAACL-HLT ?10), pages 100?108, LosAngeles, California.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA:A supervised topic model for credit attribution inmulti-labeled corpora.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP ?09), pages 248?256,Singapore.Keith Stevens, Philip Kegelmeyer, David Andrzejew-ski, and David Buttler.
2012.
Exploring topic co-herence over many models and many topics.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP?12), pages 952?961, Jeju Island, Korea.Vladimir N Vapnik.
1998.
Statistical learning theory.Wiley, New York.26Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.2009.
Mining common topics from multiple asyn-chronous text streams.
In Proceedings of the Sec-ond ACM International Conference on Web Searchand Data Mining (WSDM ?09), pages 192?201,Barcelona, Spain.27
