Automatic Text Categorization by Unsupervised LearningYoungjoong KoDepartment of Colnputer Science,Sogang University1 Sinsu-dong, Mapo-guSeoul, 121-742, Koreakyj @nlpzodiac.sogang.ac.kr,Jungyun SeoDepmlment of Computer Science,Sogang University1 Sinsu-dong, Mapo-guSeoul, 121-742, Koreaseoiy @ccs.sogang.ac.krAbstractThe goal of text categorization is to classifydocuments into a certain number of pre-defined categories.
The previous works inthis area have used a large number oflabeled training doculnents for supervisedlearning.
One problem is that it is difficult tocreate the labeled training documents.
Whileit is easy to collect the unlabeled ocuments,it is not so easy to manually categorize themfor creating traiuing documents.
In thispaper, we propose an unsupervised learningmethod to overcome these difficulties.
Theproposed lnethod divides the documents intosentences, and categorizes each sentenceusing keyword lists of each category andsentence simihuity measure.
And then, ituses the categorized sentences for refining.The proposed method shows a similardegree of performance, compared with thetraditional supervised learning inethods.Therefore, this method can be used in areaswhere low-cost ext categorization is needed.It also can be used for creating trainingdocuments.IntroductionWith the rapid growth of the internet, theavailability of on-line text information has beenconsiderably increased.
As a result, textcategorization has become one of the keytechniques fox handling and organizing text data.Automatic text categorization in the previousworks is a supervised learning task, defined asassigning category labels (pro-defined) to textdocuments based on the likelihood suggested bya training set of labeled doculnents.
However,the previous learning algorithms have someproblems.
One of them is that they require alarge, often prohibitive, number of labeledtraining documents for the accurate learning.Since the application area of automatic textcategorization has diversified froln newswirearticles and web pages to electronic mails andnewsgroup postings, it is a difficult task tocreate training data for each application area(Nigam K. et al, 1998).In this paper, we propose a new automatic textcategorization lnethod based on unsupervisedlearning.
Without creating training documentsby hand, it automatically creates trainingsentence sets using keyword lists of eachcategory.
And then, it uses them for training andclassifies text documents.
The proposed methodcan provide basic data fox" creating trainingdoculnents from collected documents, and canbe used in an application area to classify textdocuments in low cost.
We use the 2 / statistic(Yang Y. et al, 1998) as a feature selectionmethod and the naive Bayes classifier(McCailum A. et al, 1998) as a statistical textclassifier.
The naive Bayes classifier is one ofthe statistical text classifiers that use wordfrequencies as features.
Other examples includek-nearest-neighbor (Yang Y. et al, 1994),TFIDF/Roccio (Lewis D.D.
et al, 1996),support vector machines (Joachilns T. et al,1998) and decision tree (Lewis D.D.
et al,1994).1 Proposal: A text categorization schemeThe proposed system consists of three modulesas shown in Figure 1; a module to preprocesscollected ocuments, a module to create trainingsentence sets, and a module to extract featuresand to classify text doculnents.453............. 1\]iJText (~// ",L',.~g,,r}i, I ).1.l&ll ego 13',Ax~J~llJll~ ', /,I( ",'11 ello I'1\]Figurel : Architecture for the proposed system1.1 PreprocessingFirst, the html tags and special characters in thecollected ocuments are removed.
And then, thecontents of the documents are segmented intosentences.
We extract content words for eachsentence using only nouns.
In Korean, there areactive-predicative common nouns which becomeverbs when they am combined with verb-derivational suffixes (e.g., ha-ta 'do', toy-la'become', etc.).
There are also stative-predicative common nouns which becomeadjectives when they are combined withadjective-derivational suffixes such as ha.
Thesederived verbs and adjectives are productive inKorean, and they are classified as nounsaccording to the Korean POS tagger.
Otherverbs and adjectives are not informative in manycases.1.2 Creating training sentence setsBecause the proposed system does not havetraining documents, training sentence sets foreach category corresponding to the trainingdocuments have to be created.
We definekeywords for each category by hand, whichcontain special features of each categorysufficiently.
To choose these keywords, we firstregard category names and their synonyms askeywords.
And we include several words thathave a definite meaning of each category.
Theaverage number of keywords for each categoryis 3.
(Total 141 keywords for 47 categories)Table 1 lists the examples of keywords foreach category.Table 1: Examples of keywords for each categoryCategory Keywordsye-hayng (trip),kwan-kwang(sightseeing)Um-ak(music)Cong-kyo(religion)Pang-song(broadcasting)ye-hayng (trip),kwan-kwang (sightseeing)Um-ak (music)Cong-kyo (religion),chen-cwu-kyo(Catholicism)ki-tok-kyo(Christianity),pwul-kyo(Buddhism)Pang-song (broadcasting), TV thal-ley-pi-cyen(television), la-ti-o(radio)Next, the sentences which contain pre-definedkeywords of each category in their contentwords are chosen as the initial representativesentences.
The remaining sentences am calledunclassified sentences.
We scale up therepresentative sentence sets by assigning theunclassified sentences to their related category.This assignment has been done throughmeasuring similarities of the unclassifiedsentences to the representative sentences.
Wewill elaborate this process in the next twosubsections.1.2.1 Extracting and verifying representativesentencesWe define the representative s ntence as whatcontains pre-defined keywords of the category inits content words.
But there exist error sentencesin the representative sentences.
They do nothave special features of a category even thoughthey contain the keywords of the category.
Torelnove such error sentences, we can rank therepresentative sentences by computing theweight of each sentence as follows:1) Word weights are computed using TermFrequency (TF) and Inverse Category Frequency(ICF) (Cho K. et al, 1997).
@ The within-category word frequency(TF~j),TFij = the number of times words ti occursin the j th category (1)?
In Information Retrival, Inverse DocumentFrequency (IDF) are used generally.
But asentence is a processing unit in theproposed method.
Therefore, the documentfrequency cannot be counted.
Also, sinceICF was defined by Cho K. et al (1997)454and its efficiency was verified, we use it intile proposed method.
ICF is computed asfollows:ICF i -- Iog(M ) - I og(CF  i ) (2)?where CF is tile number of categories thatcontain t;, and M is tile total number ofcategories.Tile Colnbination (TFICF) of the above (9and ?, i.e., weight w~ i of word t; in ./titcategory is computed as follows:wij --- TFii x ,cl~,.
: Tl ' i j  X ( log(M)  - l og(CF  i ) ) (3)2) Using word weights (%) computed in 1), asentence weight (We) in jth category arecomputed as follows:W Ij q- W2j +...-F WNjW!/ = (4)Nwhere N is the total number of words in asentence.3) The representative s ntences of each categoryare sorted in the decreasing order of weight,which was computed in 2).
And then, the lop70% of tile representative s ntences are selectedand used in our experiment.
It is decidedempirically.1.2.2 Extending representative s ntence setsTo extend lhe representative s ntence sets, theunclassified sentences are classified into theirrelated category through measuring similaritiesof the unclassified sentences to therepresentative s ntences.
(l) Measurement of word and sentencesimilaritiesAs similar words tend to appear in similarcontexts, we compute the similarity by usingcontextual information (Kim H. et al, 1999;Karov Y. et al, 1999).
In this paper, words andsentences play COlnplementary roles.
That is, asentence is represented by the set of words itcontains, and a word by the set of sentences inwhich it appears.
Sentences are simihu" to theextent that they contain similar words, andwords are similar to the extent that they appearin similar sentences.
This definition is circular.Titus, it is applied iteratively using two matricesas shown in Figure 2. in this paper, we set thenumber of iterations as 3, as is recommended byKarov Y. et al (1999).S i rn i la r~~ Wod " Sente,,C; :i A i dFigure 2: llerative computation of word andsentence similaritiesIn Figure 2, each category has a wordsilnilarity matrix WSM,, and a sentence similaritymatrix SSM,,.
In each iteration n, we updateWSM,, whose rows and columns are labeled byall content words encountered in thercpresentatwe sentences of each category andinput unclassified sentences.
In that lnatrix, thecell (i j) hokls a value between 0 and l,indicating the extent to which the ith word iscontextually similar to the jth word.
Also, wekeep and update a SSM,,, which holds similaritiesamong sentences.
The rows of SSM,, correspondto the unclassified sentences and the cohmms tothe representative s ntences.
In this paper, thenumber of input sentences of row and column inSSM is limited to 200, considering executiontime and memory allocation.To compute tile similarities, we initializeWSM, to the identity matrix.
That is, each wordis fully similar (1) to itself and completelydissimilar (0) to other words.
The followingsteps are iterated until the changes in thesimilarity values are small enough.1.
Update the sentence similarity lnatrix SSM,,,using the word similarity matrix WSM,.2.
Update the word similarity matrix WSM,,,using the sentence similarity matrix SSM,.
(2) Affinity formulaeqb simplify tile symmetric iterative treatment ofsimilarity between words and sentences, wedel'ine an auxiliary relation between words andsentences as affinity.
A woM W is assumed tohave a certain affinity to every sentence, which455is a real number between 0 and 1.
It reflects thecontextual relationships between W and thewords of the sentence.
If W belongs to asentence S, its affinity to S is 1.
If W is totallyunrelated to S, the affinity is close to 0.
If W iscontextually similar to the words of S, its affinityto S is between 0 and 1.
In a similar manner, asentence S has some affinity to every word,reflecting the similarity of S to the sentencesinvolving that word.Affinity formulae are defined as follows(Karov Y. et al, 1999).
In these formulae, W ~ Smeans that a word belongs to a sentence:aft,, (W, S) = max w, es sire,, (W , W i )aff,, (S, W) = max w~s; sire,, (S, S~ )(5)(6)In the above formulae, n denotes the iterationnumber, and the similarity values are defined byWSM,, and SSM,,.
Every word has some affinityto the sentence, and the sentence can berepresented by a vector indicating the affinity ofeach word to it.
(3) Similarity formulaeThe similarity of Wj to W2 is the average affinityof the sentences that include W~ to 1+'2, and thesimilarity of a sentence S~ to $2 is a weightedaverage of the affinity of the words in S~ to Se.Similarity formulae are defined as follows(Karov Y. et al, 1999):sim,,+l (Sj, S 2 ) = Z weight(W, S1 ).
qlJ',, (W, S 2 ) (7)WE ,~'~if W I =W 2sim,,+l (W l , W 2 ) = 1C/,?esim"+l (Wl' W2 ) = Z weight(S, W l ).
aft,, (S, W 2 ) (8)W~eSThe weights in Formula 7 are computedfollowing the methodology in the next section.The sum of weights in Formula 8, which is areciprocal number of sentences that contain W,is !.
These values are used to update thecorresponding entries of WSM and SSM,,.
(4) Word weightsIn Formula 7, the weight of a word is a productof three factors.
It excludes the words that areexpected to be given unreliable similarity values.The weights are not changed in their process ofiterations.l.
Global frequency: Frequent words in totalsentences are less informative of sense and ofsentence similarity.
For example, a word like'phil-yo(necessity)' frequently appears in anysentence.
The formula is as follows (Karov Y.et al, 1999):max{0,1 freq(W) "1max 5, freq(x) J(9)In (9), max52\[req(x) is the sum of the fivehighest frequencies in total sentences.2.Log-likelihood faclor: In general, the wordsthat are indicative of the sense appear inrepresentative s ntences more frequently thanin total sentences.
The log-likelihood factorcaptures this tendency.
It is computed asfollows (Karov Y. et al, 1999):log pr(w; l w) (lO)Pr(Wi )In (10), Pr(Wi) is estimated from thefrequency of Wi in the total sentences, andPr(WilW) fi'om the frequency of Wi inrepresentative sentences.
To avoid poorestimation for words with a low count inrepresentative s ntences, we nmltiply the log-likelihood by (11) where count(Wi) is thenumber of occurrences of Wi in representativesentences.
For the words which do not appearin representative s ntences, we assign weight(1.0) to them.
And the other words areassigned weight that adds 1.0 to computedvalue:c?unt(Wi) t (11) min.
1, 33.Part of ,q~eech: Each part of speech isassigned a weight.
We assign weight (1.0) toproper noun, non-predicative common noun,and foreign word, and assign weight (0.6) toactive-predicative common noun and stative-predicative common noun.456The total weight of a word is the product of theabove t'actors, each norlnalized by the sum offactors of the words in a sentence as follows(Karov Y. et al, 1999):,&ctor(Wi, S)weight?
J 'actor(Wi, S)IVieS(12)In (12), factor(W, S) is the weight beforenormalization.
(5) Assigning unclassified sentences to acategoryWe first computed similarities of theunclassified sentences to the representativesentences.
And then, we decided a Silnilarityvalue o1' each unclassified sentence for eachcategory using two alternate ways.1 tl sint(X,ci)=-- ?
Sil!l (X,Sj) (13)tiE(' I1 .
( ,'~'jcR,.,j=  Jsinl(X,ci)=nlnxl.siul(X Si)} (14)(:'~(" I SjcRc,In (13) and (14), i) X is au unclassified sentence,ii) C = {c l,c2 ..... c,,,} is a category set, and iii)R,,,={&,Sa ...... S',,} is a representative sentenceset of category c..Each unclassified sentence is assigned to acategory which has a maxinmln similarity wflue.But there exist unclassified sentences which donot belong to any category.
To remove theseunclassified sentences, we set up a thresholdvalue using normal distribution of similarityvalues as follows:max{sim(X,c i )  } >_ tt + 017 (15)ciECIn (15), i) X is an unclassified sentence, ii) It isan average of similarity wflues, iii) o is astandard eviation of similarity wdues, and iv) 0is a numerical wdue corresponding tothreshold(%) in normal distribution table.1.3 Feature selection and text classifier1.3.1 Feature SelectionThe size of the vocabulary used in ourexperiment is selected by ranking wordsaccording to their Z 2 statislic with respect o thecategory.
Using the two-way contingency tableof a word t and a category c - i) A is the numberof times t and c co-occur, ii) B is the number oftimes t occurs without c, iii) C is the number oftimes c occurs without t, iv) D is the number ot'times ueither c nor t occurs, and vi) N is the totalnumber of sentences - the word-goodnessmeasure is defined as follows (Yang Y. et al,1997):Z2(t,c) = N?
(AD-CB)2 (16)(A + C)(B + D)(A + B)(C + D)To measure the goodness of a word in aglobal feature selection, we combine thecategory-specific s ores of a word as follows:I I I  9 2 ZF,,.,~, (t) = n~a,x{ Z .= (t, (:~)} (17)1.3.2 Text classifierThe method that we use for classifyingdocuments is uaivc Bayes, with minormodifications based on Kullback-LeiblerDivergence (Craven M. et al, 1999).
The basicidea in naive Bayes approaches i to use the jointprobabilities of words and categories to estimatethe probabilities of categories given a document.Given a document d for chtssit'ication, wecalculate the probabilities of each category c asfollows:Pr(cld) Pr(c) Pr(d lc) 7' _ _ P r (c )H Pr(t i Ic.)
N(rM~Pr(d) i<T !c)),, ;=, Id) )In tile above l'ormula, i) 11, is the number ofwords in d, ii) N(t~ld) is the frequency of woM t iin clocument d, iii) 7" is the size of tilevocabulary, and iv) t~ is tile ith word in thevocabulary.
Pr(tAc ) thus represents theprobability that a randomly drawn woM from arandolnly drawn docmnent in category c will bethe word 6 Pr(tild) represents the proportion ofwoMs in docmnent d that are word t c Eachprobability is estimated by formulae (19) and(20), which are called the expected likelihood457estimator (Li H. et al, 1997).
The categorypredicted by the method for a given document issimply the category with the greatest score.
Thismethod performs exactly the sameclassifications as naive Bayes does, but producesclassification scores that are less extreme.N(ti, c) + 0.5Pr(ti \[ c) = T( (l 9)Z N(t i' c) + 0.5 x T cj=lPr(t i id) = N( t .
i ,d )+O.5xT ,  z (20)0 if N(t i ,d) :  02 Evaluation of experiment2.1 Performance measuresIn this paper, a document is assigned to only onecategory.
We use the standard definition ofrecall, precision, and F, measure as performancemeasures.
For evaluating performance averageacross categories, we use the micro-averagingmethod.
F~ measure is defined by the followingformula (Yang Y. et al, 1997):2 q)F 1 ( r ,  p )  - (21 )r+ pwhere r represents recall and p precision.
Itbalances recall and precision in a way that givesthem equal weight.2.2 Experiment settingsWe used total 47 categories in our experiment.They consist of 2,286 documents to be collectedin web.
We did not use tag information of webdocuments.
And a so-called bag of words orunigraln representation was used.
Table 2 showsthe settings of experiment data in detail.Table 2: Setting experiment data........................... i i avg# avg # of I #of '  #of  I .... of doc, sen. .
.
.
.
.
d0c  sen: ...... ~ .......... ~ inacat, inadoc.Training 1 ,383 67,506 29.4 48.8Set (60%)903 Test Set 56,446 19.2 62.5 (40%)2.3 Prinmry results2.3.1 Results of the different combinations ofsimilarity value decisions and thresholdsWe evaluated our method according to thedifferent combinations of similarity valuedecisions and thresholds in section 1.2.2.
Weused thresholds of top 5%, top 10%, top 15%,top20% in formula (15), and tested the twooptions, average and maximum in formulae (13)and (14).
We limited our vocabulary to 2,000words in this experiment.+Close  Test(max) -~N-~Close Test(avg)+Open Test(max) ---')(~Open Test(avo)0.73 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0.72UL 0.710.70.69E 0.680.670.665% 10% 15% 20%Threshold(%)Figure 3: Results of the different combinations ofsimilarity wdue decisions and thresholdsFigure 3 shows results according to the twooptions in each threshold.
Here, the result usingmaxinmm was better than that using averagewith regrad to all thresholds.
The results of top10% and top 15% were best.
Therefore, we usedthe maximum in the decision of similarity valueand top 15% in threshold in our experiments.2.3.2 The proposed system vs. the system bysupervised learningFor the fair evaluation, we embodied atraditional system by supervised learning usingthe same feature selection method (2/ statistic)and classifier (naive Bayes Classifier), as used inthe proposed system.
And we tested thesesystems and compared their performance:458+method by supervised learning ' -~t r -p roposed  method0.8 .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0.7700.750.725E.
o7>~ 0.6750.05E 0.006750.!i50,5256.5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Vooabulary Sizelqgure 4: Comparison of the proposed system and thesyslem by supervised learningFigure 4 displays the performance urves for theproposed system and the system by supervisedlearning.
The best F~ score of the proposedsystem is 71.8% and that of the system bysupervised learning is 75.6%.
Therefore, thedifference between them is only 3.8%.ConclusionThis paper has described a new automatic textcategorization method.
This methodautomatically created training sets usingkeyword lists of each category and used themfor training.
And then, it classified textdocuments.
This could be a significant methodin text learning because of the high cost of hand-labeling training docmnents and the awfilabilityof huge volumes of unlabeled ocuments.
Theexperiment results showed that with respect operformance, the difference between theproposed method and the method by supervisedlearning is insignificant.
Therefore, this methodcan be used in areas where low-cost textcategorization is required, and can be used forcreating training data.This study awaits further research.
First, amore scientific approach for defining keywordlists should be investigated.
Next, if we use aword sense disambiguation systeln in theextraction step of representative s ntences, wewould be able to achieve a better performance.AcknowledgmentsThis work was supported by KOSEF underGrant No.
97-0102-03-01-3.
We wish to thankJeoung-seok Kiln for his valuable COlnments tothe earlier version of this paper.Refel-elicesCho K. and Kim J.
(1997) Automatic TextCategorization on Hierarchical Category Structureby using ICF(Inverted Category Frequency)Weighting.
In Proceedings of KISS coqference,pp.507-510.Craven M., DiPasquo D., Freitag I)., McCallum A.,Mitchell T., Nigam K. and Slauery S. (1999)l,earning to Conslruct Knowledge Bases from lheWorld Wide Web.
to appear in Artificialhttelligence.Joachims T. (1998)Text Categorization with SupporlVector Machines: Learning with Many RelevantFeatures.
In European Conference on MachineLearning(ECML).Karov Y. and 17,dehnan S. (1998) Similarity-basedWord Sense l)isambiguation.
ComputationalLinguistics, Vol 24, No I, pp.
41-60.Kim H., KEY., Park S. and See J.
(1999) hfformalRequirements Analysis St,1)porling System forHtlulall Engineer.
111 Proceedings of Conference m~IEEE- SMC99.
Vol 3, pp.
1013-1018.Lewis D.D.
and Ringuette M. (t994) A comparisonof '\['wo 1,earning Algorithms for Text categorizalion.In Proceeding of the 3 ''~ Ammal &,ml~osium o~Document Attalysis and h{/brmation Retrieval.Lewis I).D., Schapire P,.E., Calhm J.P. and PapkaP,.
(1996) Training Algorilhms for IJnear TextClassifiers.
In Proceedings of the 19" htter~tatiomtlCoqference on Research and Deveh)l)ment inh!/btwtation Retrieval (SIGIR'96), pp.
289-297.Li H. and Yamanishi K. (1997) DocumentClassification Using a Finite Mixture Model.
TheAssociation for Co,qmtatiomtl Littguistics,ACE '97.McCallum A. and Nigram K. (1998) A comparisonof Event Models for Naive Bayes TextClassification.
AAAI '98 workshop on Leanting for7kvt Categorization.Nigam K., McCallum A., Thrun S. and Mitchell T.(1998) Learning to Classify Text from Labeled andUnlabeled l)oeuments.
In Proceedings of 15"National Conference on Artificial httelligence(AAAI-98).Yang Y.
(1999) An ewduation of statisticalapproaches to text categol"ization, ht.formationRetrieval Journal, May.Yaug Y.
(1994) Expert netword: Effective andefficient learning fi'om human decisions in textcatego,izatin and retriewfl.
In 17" Ammallnternatiomd A CM SIG1R Conference on Researchand Development in hi formation Retrieval(SIGIR'94), pp.
13-22.Yang Y. and Pederson J.O.
(1997) A comparativestudy on feature selection in text categorization, htProceedings of the 14" International Conference onMachine Learning.459
