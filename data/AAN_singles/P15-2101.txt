Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 609?615,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsReducing infrequent-token perplexity via variational corporaYusheng Xie1,#Pranjal Daga11Northwestern UniversityEvanston, IL USAYu Cheng2#yxi389@eecs.northwestern.eduKunpeng Zhang32IBM ResearchYorktown Heights, NY USAAnkit Agrawal1Alok Choudhary13University of MarylandCollege Park, MD USAAbstractRecurrent neural network (RNN) is recog-nized as a powerful language model (LM).We investigate deeper into its performanceportfolio, which performs well on frequentgrammatical patterns but much less so onless frequent terms.
Such portfolio is ex-pected and desirable in applications likeautocomplete, but is less useful in socialcontent analysis where many creative, un-expected usages occur (e.g., URL inser-tion).
We adapt a generic RNN model andshow that, with variational training cor-pora and epoch unfolding, the model im-proves its performance for the task of URLinsertion suggestions.1 IntroductionJust 135 most frequent words account for 50% textof the entire Brown corpus (Francis and Kucera,1979).
But over 44% (22,010 out of 49,815) ofBrown?s vocabulary are hapax legomena1.
The in-tricate relationship between vocabulary words andtheir utterance frequency results in some impor-tant advancements in natural language process-ing (NLP).
For example, tf-idf results from rulesapplied to word frequencies in global and localcontext (Manning and Sch?utze, 1999).
A com-mon preprocessing step for tf-idf is filtering rarewords, which is usually justified for two reasons.First, low frequency cutoff promises computa-tional speedup due to Zipf?s law (1935).
Second,many believe that most NLP and machine learningalgorithms demand repetitive patterns and reoc-currences, which are by definition missing in lowfrequency words.1.1 Should infrequent words be filtered?Infrequent words have high probability of becom-ing frequent as we consider them in a larger con-1Words appear only once in corpus.text (e.g., Ishmael, the protagonist name in Moby-Dick, appears merely once in the novel?s dialoguesbut is a highly referenced word in the discus-sions/critiques around the novel).
In many modernNLP applications, context grows constantly: freshnews articles come out on CNN and New YorkTimes everyday; conversations on Twitter are up-dated in real time.
In processing online social me-dia text, it would seem premature to filter wordssimply due to infrequency, the kind of infrequencythat can be eliminated by taking a larger corpusavailable from the same source.To further undermine the conventional justifica-tion, computational speedup is attenuated in RNN-based LMs (compared to n-gram LMs), thanks tomodern GPU architecture.
We train a large RNN-LSTM (long short-term memory unit) (Hochreiterand Schmidhuber, 1997) model as our LM on twoversions of Jane Austen?s complete works.
Deal-ing with 33% less vocabulary in the filtered ver-sion, the model only gains marginally on runningtime or memory usage.
In Table 1.1, ?Filtered cor-pus?
filters out all the hapax legomena in ?Full cor-pus?.Full corpus Filtered corpuscorpus length 756,273 751,325vocab.
size 15,125 10,177running time 1,446 sec 1,224 secGPU memory 959 MB 804 MBTable 1: Filtered corpus gains little in running timeor memory usage when using a RNN LM.Since RNN LMs suffer only small penalty inkeeping the full corpus, can we take advantage ofthis situation to improve the LM?1.2 Improving performance portfolio of LMOne improvement is LM?s performance portfo-lio.
A LM?s performance is usually quantified as609perplexity, which is exponentialized negative log-likelihood in predictions.For our notation, let VXdenote the vocabu-lary of words that appear in a text corpus X ={x1, x2, .
.
.}.
Given a sequence x1, x2, .
.
.
, xm?1,where each x ?
VX, the LM predicts the nextin sequence, xm?
VX, as a probability distribu-tion over the entire vocabulary V (its predictiondenoted as p).
If vm?
VXis the true token atposition m, the model?s perplexity at index m isquantified as exp(?
ln(p[vm])).
The training goalis to minimize average perplexity across X .However, a deeper look into perplexity beyondcorpus-wide average reveals interesting findings.Using the same model setting as for Table 1.1,Figure 1 illustrates the relationship between word-level perplexity and its frequency in corpus.
Ingeneral, the less frequent a word appears, themore unpredictable it becomes.
In Table 1.2, thetrained model achieves an average perplexity of78 on filtered corpus.
But also shown in Table1.2, many common words register with perplexityover 1,000, which means they are practically un-predictable.
More details are summarized in Table1.2.
The LM achieves exceptionally low perplex-ity on words such as <apostr.>s (?s, the posses-sive case), <comma> (, the comma).
And thesetokens?
high frequencies in corpus have promisedthe model?s average performance.
Meanwhile, theLM has bafflingly high perplexity on common-place words such as read and considering.64?128?256?512?1024?2048?4096?8192?16384?32768?65536?0?1000?2000?3000?4000?5000?6000?7000?8000?9000?10000?Word?frequency?in?corpus?Word?level?average?perplexity?word?perplexity?word?frequency?
(log?scale)?Figure 1: (best viewed in color) We look at wordlevel perplexity with respect to the word frequencyin corpus.
The less frequent a word appears, themore unpredictable it becomes.2 MethodologyWe describe a novel approach of constructing andutilizing pre-training corpus that eventually reduceLMs?s high perplexity on rare tokens.
The stan-dard way to utilize a pre-training corpus W is toToken Freq.
Perplexity 1 Perplexity 2corpus avg.
N/A 78 82<apostr.>s 4,443 1.1 1.1of 23,046 4.9 5.0<comma> 57,552 5.2 5.1been 3,452 5.4 5.7read 224 3,658 3,999quiet 108 6,807 6,090returning 89 7,764 6,268considering 80 9,573 8,451Table 2: A close look at RNN-LSTM?s perplexityat word level.
?Perplexity 1?
is model perplexitybased on filtered corpus (c.f., Table 1.1) and ?Per-plexity 2?
is based on full corpus.first train the model on W then fine-tune it on tar-get corpus X .
Thanks to availability of text, Wcan be orders of magnitude larger than X , whichmakes pre-training on W challenging.A more efficient way to utilize W is to constructvariational corpora based on X and W .
In the fol-lowing subsections, we first describe how replace-ment tokens are selected from a probability massfunction (pmf), which is built from W ; then ex-plain how the variational corpora variates with re-placement tokens through epochs.2.1 Learn from pre-training corpusOne way to alleviate the impact from infrequentvocabulary is to expose the model to a largerand overarching pre-training corpus (Erhan et al.,2010), if available.
Let W be a larger corpusthan X and assume that VX?
VW.
For exam-ple, if X is Herman Melville?s Moby-Dick, Wcan be Melville?s complete works.
Further, weuse VX,1to denote the subset of VXthat are ha-pax legonema in corpus X; similarly, VX,n(forn = 2, 3, .
.
.)
denotes the subset of VXthat occurn times in X .
Many hapax legomena in VX,1arelikely to become more frequent tokens in VW.Suppose that x ?
VX,1.
Denoted byReplacePMF(W,VW, x) in Algorithm 1, we rep-resent x as a probability mass function (pmf) over{x?1, x?2, .
.
.
}, where each x?iis selected from VW?VX,nfor n > 1 using one of the two methods be-low.
For illustration purpose, suppose the hapaxlegomenon, x, in question is matrimonial:1) e.g., matrimony.
Words that have very highliteral similarity with x.
We measure literal sim-ilarity using Jaro-Winkler measure, which is anempirical, weighted measure based on string edit610distance.
We set the measure threshold very high(> 0.93), which minimizes false positives as wellas captures many hapax legonema due to adv./adj.,pl./singular (e.g, -y/-ily and -y/-ies).2) e.g., marital Words that are direct syno/hypo-nyms to x in the WordNet (Miller, 1995).getContextAround(x?)
function in Algorithm 1simply extracts symmetric context words fromboth left and right sides of x?.
Although the in-vestigated LM only uses left context in predictingword x?, context right of x?is still useful informa-tion in general.
Given a context word c right of x?,the LM can learn x?
?s predictability over c, whichis beneficial to the corpus-wide perplexity reduc-tion.In practice, we select no more than 5 substitu-tion words from each method above.
The prob-ability mass on each x?iis proportional to its fre-quency in W and then normalized by softmax:pmf(x?i) = freq(x?i)/?5k=1freq(x?k).
This sub-stitution can help LMs learn better because we re-place the un-trainable VX,1tokens with tokens thatcan be trained from the larger corpus W .
In con-cept, it is like explaining a new word to school kidsby defining it using vocabulary words in their ex-isting knowledge.2.2 Unfold training epochsEpoch in machine learning terminology usuallymeans a complete pass of the training dataset.many iterative algorithms take dozens of epochson the same training data as they update themodel?s weights with smaller and smaller adjust-ments through the epochs.We refer to the the training process proposedin Figure 2 (b) as ?variational corpora?.
Com-pared to the traditional structure in Figure 2 (a),the main advantage of using variational corpora isthe ability to freely adjust the corpus at each ver-sion.
Effectively, we unfold the training into sep-arate epochs.
This allows us to gradually incorpo-rate the replacement tokens without severely dis-torting the target corpus X , which is the learninggoal.
In addition, variational corpora can furtherregularize the training of LM in batch mode (Sri-vastava et al., 2014).Algorithm 1 constructs variational corporaX(s) at epoch s. Assuming X(s+1) being avail-able, Algorithm 1 appends snippets, which aresampled from W , into X(s) for the sth epoch.
Forthe last epoch s = S, X(S) = X .
As the epochmodel&same&text&&corpus&Load&in&batch&update&parameters&epoch&1&model&same&text&&corpus&update&parameters&epoch&2&&??.&Load&in&batch&epoch&?&same&text&&corpus&Load&in&batch&(a)model&text&&corpus&S1&randomized&batch&update&parameters&epoch&1&&?
?.&epoch&?&model&text&&corpus&S2&randomized&batch&update&parameters&epoch&2&text&&corpus&S3&randomized&batch&overwrite&VX,n&weights& overwrite&VX,n&weights&(b)Figure 2: Unfold the training process in units ofepochs.
(a) Typical flow where model parses thesame corpus at each epoch.
(b) The proposedtraining architecture with variational corpora to in-corporate the substitution algorithm.Algorithm 1: Randomly constructs varia-tional corpus at epoch s.Input: W,X, VW, VX, VX,n, n, as defined inSection 1.2&2.1,s, S, current and max epoch number.Output: X(s), variational corpus at epoch s1 X(s)?
X(s+ 1)2 for each x ?
VX,ndo3 p?
ReplacePMF(W,VW, x)4 i?
Dirichlet(p).generate()5 while i?
X .getNextIdxOf(x) do6 x??
i.draw()7 c?W .getContextAround(x?
)8 c.substr([0, uniformRnd(0,S?sS|c|)])9 X(s).append(c)10 return X(s)number increases, fewer and shorter snippets areappended, which alleviates training stress.
By fix-ing an n value, the algorithm applies to all wordsin VX,n.In addition, as a regularization trick (Mikolovet al., 2013; Pascanu et al., 2013) , we use a uni-form random context window (line 8) when inject-ing snippets from W into X(s).611Freq.
nofilter 3filter ptw vc10 28,542 (668.1) 23,649 (641.2) 27,986 (1,067.2) 20,994 (950.9)100 1,180.3 (21.7) 1,158.2 (19.2) 735.8 (29.8) 755.8 (31.5)1K 163.2 (12.9) 163.9 (12.2) 138.5 (14.1) 137.7 (15.7)5K 47.5 (3.3) 47.2 (3.1) 40.2 (3.2) 40.2 (3.3)10K 16.4 (0.31) 16.7 (0.29) 14.4 (0.42) 14.1 (0.41)40K 7.6 (0.09) 7.6 (0.09) 7.0 (0.09) 7.0 (0.10)all tokens 82.1 (2.0) 77.9 (1.9) 68.6 (2.1) 68.9 (2.1)GPU memory 959MB 783MB 1.8GB 971MBrunning time 1,446 sec 1,181 sec 9,061 sec 6,960 secTable 3: Experiments compare average perplexity produced by the proposed variational corpora approachand other methods on a same test corpus.
Bold fonts indicate best.
?Freq.?
indicates the average corpus-frequency (e.g., Freq.=1K means that words in this group, on average, appear 1,000 times in corpus).Perplexity numbers are averaged over 5 runs with standard deviation reported in parentheses.
GPUmemory usage and running time are also reported for each method.Err.
type Context before True token LM predictionFalse neg.
<unk>, via, <unk>, banana, muffin, chocolate, URL to a cooking blog recipeFalse neg.
sewing, ideas, <unk>, inspiring, picture, on, URL to favim.com estyFalse neg.
nike, sports, fashion, <unk>, women, <unk>, URL to nelly.com macyFalse pos.
new, york, yankees, endless, summer, tee, <unk>, shop <url>False pos.
take, a, rest, from, your, #harrodssale, shopping <url>Table 4: False positives and false negatives predicted by the model in the Pinterest application.
Thecontext words preceding to token in questions are provided for easier analysis3.3 Experiments3.1 Perplexity reductionWe validate our method in Table 3 by showing per-plexity reduction on infrequent words.
We splitJane Austen?s novels (0.7 million words) as tar-get corpus X and test corpus, and her contem-poraries?
novels4as pre-training corpus W (2.7million words).
In Table 3, nofilter is the unfil-tered corpus; 3filter replaces all tokens in VX,3by <unk>; ptw performs naive pre-training on Wthen on X; vc performs training with the proposedvariational corpora.
Our LM implements the RNNtraining as described in (Zaremba et al., 2014).
Ta-ble 3 also illustrates the GPU memory usage andrunning time of the compared methods and showsthat vc is more efficient than simply ptw.vc has the best performance on low-frequencywords by some margin.
ptw is the best on frequentwords because of its access to a large pre-training3Favim.com is a website for sharing crafts, creativityideas.
Esty.com is a e-commerce website for trading hand-made crafts.
Nelly.com is Scandinavia?s largest online fash-ion store.
Macy?s a US-based department store.
Harrod?s is aluxury department store in London.4Dickens and the Bronte sisterscorpus.
But somewhat to our surprise, ptw per-forms badly on low-frequency words, which wereckon is due to the rare words introduced in W :while pre-training on W helps reduce perplexityof words in VX,1but also introduces additional ha-pax legomena in VW,1\ VX,1.0?0.1?0.2?0.3?0.4?0.5?tech?
animals?
travel?
food?
home?
fashion?Accuracy?nofilter?
3filter?ptw?
vc?Figure 3: Accuracy of suggested URL positionsacross different categories of Pinterest captions.3.2 Locating URLs in Pinterest captionsBeyond evaluations in Table 3.
We apply ourmethod to locate URLs in over 400,000 Pinterestcaptions.
Unlike Facebook, Twitter, Pinterest isnot a ?social hub?
but rather an interest-discovery612site (Linder et al., 2014; Zhong et al., 2014).
Tomaximally preserve user experience, postings onPinterest embed URLs in a natural, nonintrusivemanner and a very small portion of the posts con-tain URLs.In Figure 3, we ask the LM to suggest a po-sition for the URL in the context and verify thesuggest with test data in each category.
For ex-ample, the model is presented with a sequenceof tokens: find, more, top, dresses, at, afford-able, prices, <punctuation>, visit, and is askedto predict if the next token is an URL link.
Inthe given example, plausible tokens after visit canbe either <http://macys.com> or nearest, Macy,<apostr.>s, store.
The proposed vc mechanismoutperforms others in 5 of the 6 categories.
InFigure 3, accuracy is measured as the percentageof correctly suggested positions.
Any predictionnext to or close to the correct position is countedas incorrect.In Table 4, we list some of the false nega-tive and false positive errors made by the LM.Many URLs on Pinterest are e-commerce URLsand the vendors often also have physical stores.
Soin predicting such e-commerce URLs, some mis-takes are ?excusable?
because the LM is confusedwhether the upcoming token should be an URL(web store) or the brand name (physical store)(e.g, http://macys.com vs. Macy?s).4 Related workRecurrent neural network (RNN) is a type of neu-ral sequence model that have high capacity acrossvarious sequence tasks such as language model-ing (Bengio et al., 2000), machine translation (Liuet al., 2014), speech recognition (Graves et al.,2013).
Like other neural network models (e.g.,feed-forward), RNNs can be trained using back-propogation algorithm (Sutskever et al., 2011).Recently, the authors in (Zaremba et al., 2014)successfully apply dropout, an effective regular-ization method for feed-forward neural networks,to RNNs and achieve strong empirical improve-ments.Reducing perplexity on text corpus is proba-bly the most demonstrated benchmark for mod-ern language models (n-gram based and neuralmodels alike) (Chelba et al., 2013; Church et al.,2007; Goodman and Gao, 2000; Gao and Zhang,2002).
Based on Zipf?s law (Zipf, 1935), a fil-tered corpus greatly reduces the vocabulary sizeand computation complexity.
Recently, a rigor-ous study (Kobayashi, 2014) looks at how per-plexity can be manipulated by simply supplyingthe model with the same corpus reduced to vary-ing degrees.
Kobayashi (2014) describes his studyfrom a macro point of view (i.e., the overall corpuslevel perplexity).
In this work, we present, at wordlevel, the correlation between perplexity and wordfrequency.Token rarity is a long-standing issue with n-gram language models (Manning and Sch?utze,1999).
Katz smoothing (Katz, 1987) and Kneser-Ney based smoothing methods (Teh, 2006) arewell known techniques for addressing sparsity inn-gram models.
However, they are not directlyused to resolve unigram sparsity.Using word morphology information is anotherway of dealing with rare tokens (Botha and Blun-som, 2014).
By decomposing words into mor-phemes, the authors in (Botha and Blunsom, 2014)are able to learn representations on the morphemelevel and therefore scale the language modeling tounseen words as long as they are made of previ-ously seen morphemes.
Shown in their work, thistechnique works with character-based language inaddition to English.5 AcknowledgementsThis work is supported in part by the followinggrants: NSF awards CCF-1029166, IIS-1343639,and CCF-1409601; DOE award DESC0007456;AFOSR award FA9550-12-1-0458; NIST award70NANB14H012.6 Conclusions & future workThis paper investigates the performance portfolioof popular neural language models.
We proposea variational training scheme that has the advan-tage of a large pre-training corpus but without us-ing as much computing resources.
On low fre-quency words, our proposed scheme also outper-forms naive pre-training.In the future, we want to incorporate WordNetknowledge to further reduce perplexity on infre-quent words.ReferencesYoshua Bengio, Rjean Ducharme, Pascal Vincent,Departement D?informatique Et Recherche Opera-tionnelle, and Centre De Recherche.
2000.
A neural613probabilistic language model.
Journal of MachineLearning Research, 3:1137?1155.Jan A. Botha and Phil Blunsom.
2014.
Compositionalmorphology for word representations and languagemodelling.
In Proceedings of the 31th InternationalConference on Machine Learning, ICML 2014, Bei-jing, China, 21-26 June 2014, pages 1899?1907.Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One billion word benchmark for measur-ing progress in statistical language modeling.
Tech-nical report, Google.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with golombcoding.
In EMNLP-CoNLL 2007, Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Compu-tational Natural Language Learning, June 28-30,2007, Prague, Czech Republic, pages 199?207.Dumitru Erhan, Yoshua Bengio, Aaron Courville,Pierre-Antoine Manzagol, Pascal Vincent, and SamyBengio.
2010.
Why does unsupervised pre-traininghelp deep learning?
J. Mach.
Learn.
Res., 11:625?660, March.Nelson Francis and Henry Kucera.
1979.
Brown cor-pus manual.
Technical report, Department of Lin-guistics, Brown University, Providence, Rhode Is-land, US.Jianfeng Gao and Min Zhang.
2002.
Improving lan-guage model size reduction using better pruning cri-teria.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 176?182, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Joshua Goodman and Jianfeng Gao.
2000.
Languagemodel size reduction by pruning and clustering.
InSixth International Conference on Spoken LanguageProcessing, ICSLP 2000 / INTERSPEECH 2000,Beijing, China, October 16-20, 2000, pages 110?113.Alex Graves, Abdel-rahman Mohamed, and Geof-frey E. Hinton.
2013.
Speech recognition withdeep recurrent neural networks.
In IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing, ICASSP 2013, Vancouver, BC, Canada,May 26-31, 2013, pages 6645?6649.Sepp Hochreiter and J?urgen Schmidhuber.
1997.
Longshort-term memory.
Neural Comput., 9(8):1735?1780, November.S.
Katz.
1987.
Estimation of probabilities from sparsedata for the language model component of a speechrecognizer.
Acoustics, Speech and Signal Process-ing, IEEE Transactions on, 35(3):400?401, Mar.Hayato Kobayashi.
2014.
Perplexity on reduced cor-pora.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 797?806.
Associationfor Computational Linguistics.Rhema Linder, Clair Snodgrass, and Andruid Kerne.2014.
Everyday ideation: all of my ideas are onpinterest.
In CHI Conference on Human Factors inComputing Systems, CHI?14, Toronto, ON, Canada- April 26 - May 01, 2014, pages 2411?2420.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.A recursive recurrent neural network for statisticalmachine translation.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1491?1500, Baltimore, Maryland, June.
Association forComputational Linguistics.Christopher D. Manning and Hinrich Sch?utze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In NIPS, pages 3111?3119.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Commun.
ACM, 38(11):39?41, Novem-ber.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neuralnetworks.
In Proceedings of the 30th InternationalConference on Machine Learning, ICML 2013, At-lanta, GA, USA, 16-21 June 2013, pages 1310?1318.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search, 15:1929?1958.Ilya Sutskever, James Martens, and Geoffrey Hinton.2011.
Generating text with recurrent neural net-works.
In Lise Getoor and Tobias Scheffer, editors,Proceedings of the 28th International Conferenceon Machine Learning (ICML-11), ICML ?11, pages1017?1024, New York, NY, USA, June.
ACM.Yee Whye Teh.
2006.
A bayesian interpretation ofinterpolated kneserney.
Technical report.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.arXiv preprint arXiv:1409.2329.Changtao Zhong, Mostafa Salehi, Sunil Shah, Mar-ius Cobzarenco, Nishanth Sastry, and MeeyoungCha.
2014.
Social bootstrapping: how pinterestand last.fm social communities benefit by borrowinglinks from facebook.
In 23rd International WorldWide Web Conference, WWW ?14, Seoul, Republicof Korea, April 7-11, 2014, pages 305?314.614G.K.
Zipf.
1935.
The Psycho-biology of Language:An Introduction to Dynamic Philology.
The MITpaperback series.
Houghton Mifflin.615
