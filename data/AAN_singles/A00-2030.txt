A Novel Use of Statistical Parsing to Extract Information fromTextScott Miller, Heidi Fox, Lance Ramshaw, and Ralph WeischedelBBN Technologies70 Fawcett Street, Cambridge, MA 02138szmiller@bbn.comAbstractSince 1995, a few statistical parsingalgorithms have demonstrated abreakthrough in parsing accuracy, asmeasured against the UPenn TREEBANKas a gold standard.
In this paper we reportadapting a lexicalized, probabilisticcontext-free parser to informationextraction and evaluate this new techniqueon MUC-7 template elements and templaterelations.1 IntroductionSince 1995, a few statistical parsingalgorithms (Magerman, 1995; Collins, 1996and 1997; Charniak, 1997; Rathnaparki, 1997)demonstrated a breakthrough in parsingaccuracy, as measured against he Universityof Pennsylvania TREEBANK as a goldstandard.
Yet, relatively few have embeddedone of these algorithms in a task.
Chiba,(1999) was able to use such a parsingalgorithm to reduce perplexity with the longterm goal of improved speech recognition.In this paper, we report adapting a lexicalized,probabilistic context-free parser with headrules (LPCFG-HR) to information extraction.The technique was benchmarked in theSeventh Message Understanding Conference(MUC-7) in 1998.Several technical challenges confronted us andwere solved:?
How could the limited semanticinterpretation required in informationextraction be integrated into the statisticallearning algorithm?
We were able to integrateboth syntactic and semantic information intothe parsing process, thus avoiding potentialerrors of syntax first followed by semantics.?
Would TREEBANKing of the variety ofnews sources in MUC-7 be required?
Orcould the University of Pennsylvania'sTREEBANK on Wall Street Journaladequately train the algorithm for New YorkTimes newswire, which includes dozens ofnewspapers?
Manually creating source-specific training data for syntax was notrequired.
Instead, our parsing algorithm,trained on the UPenn TREEBANK, was runon the New York Times source to createunsupervised syntactic training which wasconstrained to be consistent with semanticannotation.
* Would semantic annotation requirecomputational linguists?
We were able tospecify relatively simple guidelines thatstudents with no training in computationallinguistics could annotate.2 Information Extraction TasksWe evaluated the new approach to informationextraction on two of the tasks of the SeventhMessage Understanding Conference (MUC-7)and reported in (Marsh, 1998).
The TemplateElement (TE) task identifies organizations,persons, locations, and some artifacts (rocketand airplane-related artifacts).
For eachorganization i an article, one must identify allof its names as used in the article, its type(corporation, government, or other), and anysignificant description of it.
For each person,one must find all of the person's names withinthe document, his/her type (civilian ormilitary), and any significant descriptions(e.g., titles).
For each location, one must alsogive its type (city, province, county, body ofwater, etc.).
For the following example, the226template element in  Figure I was to begenerated: "...according to the report byEdwin Dorn, under secretary of defense forpersonnel and readiness .
.
.
.
Dorn's conclusionthat Washington..."<ENTITY-9601020516-13> :=ENT_NAME: "Edwin Dorn""Dorn"ENT_TYPE: PERSONENT_DESCRIPTOR: "under secretary ofdefense for personnel and readiness"ENT_CATEGORY: PER_CIVFigure 1: An example of the information to beextracted for TE.The Template Relations (TR) task involvesidentifying instances of three relations in thetext:?
the products made by each company?
the employees ofeach organization,?
the (headquarters) location of eachorganization.TR builds on TE in that TR reports binaryrelations between elements of TE.
For thefollowing example, the template relation inFigure 2 was to be generated: "Donald M.Goldstein, a historian at the University ofPittsburgh who helped write..."<EMPLOYEE_OF-9601020516-5> :=PERSON: <ENTITY-9601020516-18>ORGANIZATION: <ENTITY-9601020516-9><ENTITY-9601020516-9> :=ENT_NAME: "University of Pittsburgh"ENT_TYPE: ORGANIZATIONENT_CATEGORY: ORG_CO<ENTITY-9601020516-18> :=ENT_NAME: "Donald M. Goldstein"ENT_TYPE: PERSONENT_DESCRIPTOR: "a historian at theUniversity of Pittsburgh"Figure 2: An example of information to beextracted for TR3 Integrated Sentential ProcessingAlmost all approaches to informationextraction - even at the sentence level - arebased on the divide-and-conquer st ategy ofreducing acomplex problem to a set of simplerones.
Currently, the prevailing architecture fordividing sentential processing is a four-stagepipeline consisting of:1. part-of-speech tagging2.
name finding3.
syntactic analysis, often limited to nounand verb group chunking4.
semantic interpretation, usually based onpattern matchingSince we were interested in exploiting recentadvances in parsing, replacing the syntacticanalysis tage of the standard pipeline with amodem statistical parser was an obviouspossibility.
However, pipelined architecturessuffer from a serious disadvantage: rrorsaccumulate as they propagate through thepipeline.
For example, an error made duringpart-of-speech-tagging may cause a futureerror in syntactic analysis, which may in turncause a semantic interpretation failure.
Thereis no opportunity for a later stage, such asparsing, to influence or correct an earlier stagesuch as part-of-speech tagging.An integrated model can limit the propagationof errors by making all decisions jointly.
Forthis reason, we focused on designing anintegrated model in which tagging, name-finding, parsing, and semantic interpretationdecisions all have the opportunity to mutuallyinfluence ach other.A second consideration influenced ourdecision toward an integrated model.
We werealready using a generative statistical model forpart-of-speech tagging (Weischedel et al1993), and more recently, had begun using agenerative statistical model for name finding(Bikel et al 1997).
Finally, our newlyconstructed parser, like that of (Collins 1997),was based on a generative statistical model.Thus, each component of what would be thefirst three stages of our pipeline was based on227the same general class of statistical model.Although each model differed in its detailedprobability structure, we believed that theessential elements of all three models could begeneralized in a single probability model.If the single generalized model could then beextended to semantic anal);sis, all necessarysentence level processing would be containedin that model.
Because generative statisticalmodels had already proven successful for eachof the first three stages, we were optimisticthat some of their properties - especially theirability to learn from large amounts of data, andtheir robustness when presented withunexpected inputs - would also benefitsemantic analysis.4 Representing Syntax and SemanticsJointlyOur integrated model represents yntax andsemantics jointly using augmented parse trees.In these trees, the standard TREEBANKstructures are augmented to convey semanticinformation, that is, entities and relations.
Anexample of an augmented parse tree is shownin Figure 3.
The five key facts in this exampleare:?
"Nance" is the name of a person.?
"A paid consultant to ABC News"describes a person.t "ABC News" is the name of anorganization.?
The person described as "a paid consultantto ABC News" is employed by ABC News.?
The person named "Nance" and the persondescribed as "a paid consultant to ABC News"are the same person.Here, each "reportable" name or description isidentified by a "-r" suffix attached to itssemantic label.
For example, "per-r" identifies"Nance" as a named person, and "per-desc-r"identifies "a paid consultant to ABC News" asa person description.
Other labels indicaterelations among entities.
For example, the co-reference relation between "Nance" and "apaid consultant to ABC News" is indicated by"per-desc-of."
In this case, because theargument does not connect directly to therelation, the intervening nodes are labeled withsemantics "-ptr" to indicate the connection.Further details are discussed in the sectionTree Augmentation.5 Creating the Training DataTo train our integrated model, we required alarge corpus of augmented parse trees.
Since itwas known that the MUC-7 evaluation datawould be drawn from a variety of newswiresources, and that the articles would focus onrocket launches, it was important that ourtraining corpus be drawn from similar sourcesand that it cover similar events.
Thus, we didnot consider simply adding semantic labels tothe existing Penn TREEBANK, which isdrawn from a single source - the Wall StreetJournal - and is impoverished in articles aboutrocket launches.Instead, we applied an information retrievalsystem to select a large number of articlesfrom the desired sources, yielding a corpusrich in the desired types of events.
Theretrieved articles would then be annotated withaugmented tree structures to serve as a trainingcorpus.Initially, we tried to annotate the trainingcorpus by hand marking, for each sentence, theentire augmented tree.
It soon becamepainfully obvious that this task could not beperformed in the available time.
Ourannotation staff found syntactic analysisparticularly complex and slow going.
Bynecessity, we adopted the strategy of handmarking only the semantics.Figure 4 shows an example of the semanticannotation, which was the only type of manualannotation we performed.To produce a corpus of augmented parse trees,we used the following multi-step trainingprocedure which exploited the PennTREEBANK228Sper/np vpper-r/npIper/nnpINance , who is also a paid consultant to/ ~~-~1~p \ /// \/ / I / o rg~ \, wp vbz rb det vbn per-desc/nn to org'/nnporg/nnp , vbdI I I I I I I I I I I IABe News , said ...Figure 3: An example of an augmented parse tree.1.
The model (see Section 7) was first trainedon purely syntactic parse trees from theTREEBANK, producing a model capableof broad-coverage syntactic parsing.parses that were consistent with thesemantic annotation.
A parse wasconsidered consistent if no syntacticconstituents crossed an annotated entity ordescription boundary.2.
Next, for each sentence in the semanticallyannotated corpus:a.
The model was applied to parse thesentence, constrained to produce onlyb.
The resulting parse tree was thenaugmented to reflect semantic structure inaddition to syntactic structure./F.
?rso?lNancecoreference ~ employee  .ationperson-descriptor -.Iorganization 1, who is also a paid consultant to ABC News said ...Figure 4: An example of semantic annotation.229Applying this procedure yielded a new versionof the semantically annotated corpus, nowannotated with complete augmented trees likethat in Figure 3.6 Tree AugmentationIn this section, we describe the algorithm thatwas used to automatically produce augmentedtrees, starting with a) human-generatedsemantic annotations and b) machine-generated syntactic parse trees.
For eachsentence, combining these two sourcesinvolved five steps.
These steps are givenbelow:Tree Augmentation Algorithm.
Nodes are inserted into the parse tree todistinguish names and descriptors that arenot bracketed in the parse.
For example,the parser produces a single noun phrasewith no internal structure for "Lt. Cmdr.David Edwin Lewis".
Additional nodesmust be inserted to distinguish thedescription, "Lt.
Cmdr.," and the name,"David Edwin Lewis.".
Semantic labels are attached to all nodesthat correspond to names or descriptors.These labels reflect he entity type, such asperson, organization, or location, as wellas whether the node is a proper name or adescriptor.. For relations between entities, where oneentity is not a syntactic modifier of theother, the lowermost parse node that spansboth entities is identified.
A semantic tagis then added to that node denoting therelationship.
For example, in the sentence"Mary Fackler Schiavo is the inspectorgeneral of the U.S. Department ofTransportation," a co-reference semanticlabel is added to the S node spanning thename, "Mary Fackler Schiavo," and thedescriptor, "the inspector general of theU.S.
Department of Transportation.".
Nodes are inserted into the parse tree todistinguish the arguments to each relation.In cases where there is a relation betweentwo entities, and one of the entities is asyntactic modifier of the other, the insertednode serves to indicate the relation as wellas the argument.
For example, in thephrase "Lt. Cmdr.
David Edwin Lewis," anode is inserted to indicate that "Lt.Cmdr."
is a descriptor for "David EdwinLewis.".
Whenever a relation involves an entity thatis not a direct descendant of that relationin the parse tree, semantic pointer labelsare attached to all of the intermediatenodes.
These labels serve to form acontinuous chain between the relation andits argument.7 Model StructureIn our statistical model, trees are generatedaccording to a process imilar to that describedin (Collins 1996, 1997).
The detailedprobability structure differs, however, in that itwas designed to jointly perform part-of-speechtagging, name finding, syntactic parsing, andrelation finding in a single process.For each constituent, the head is generatedfirst, followed by the modifiers, which aregenerated from the head outward.
Headwords, along with their part-of-speech tags andfeatures, are generated for each modifier assoon as the modifier is created.
Word featuresare introduced primarily to help with unknownwords, as in (Weischedel et al 1993).We illustrate the generation process bywalking through a few of the steps of the parseshown in Figure 3.
At each step in theprocess, a choice is made from a statisticaldistribution, with the probability of eachpossible selection dependent on particularfeatures of previously generated elements.
Wepick up the derivation just after the topmost Sand its head word, said, have been produced.The next steps are to generate in order:1.
A head constituent for the S, in this case aVP.2.
Pre-modifier constituents for the S. In thiscase, there is only one: a PER/NP.3.
A head part-of-speech tag for the PER/NP,in this case PER/NNP.2304.
A head word for the PER/NP, in this casenance.5.
Word features for the head word of thePER/NP, in this case capitalized.6.
A head constituent for the PER/NP, in thiscase a PER-R/NP.7.
Pre-modifier constituents for the PER/NP.In this case, there are none.. Post-modifier constituents for thePER/NP.
First a comma, then an SBARstructure, and then a second comma areeach generated in turn.This generation process is continued until theentire tree has been produced.We now briefly summarize the probabilitystructure of the model.
The categories forhead constituents, ch, are predicted basedsolely on the category of the parent node, cp:e(c h Icp), e.g.
P(vpls )Modifier constituent categories, Cm, arepredicted based on their parent node, cp, thehead constituent of their parent node, Chp, thepreviously generated modifier, Cm-1, and thehead word of their parent, wp.
Separateprobabilities are maintained for left (pre) andright (post) modifiers:PL (Cm I Cp,Chp,Cm_l,Wp), e.g.PL ( per I np I s, vp, null, said)PR(c~ I Ce,Ch~,Cm-l, Wp), e.g.PR(null \[ s, vp, null, said)Part-of-speech tags, tin, for modifiers arepredicted based on the modifier, Cm, the part-of-speech tag of the head word, th, and thehead word itself, wh:P(t m ICm,th,wh), e.g.P(per / nnp \[ per /np, vbd, said)Head words, win, for modifiers are predictedbased on the modifier, cm, the part-of-speechtag of the modifier word , t,,, the part-of-speech tag of the head word, th, and the headword itself, Wh:P(W m ICm,tmth,Wh),  e.g.P(nance I per / np, per / nnp, vbd, said)Finally, word features, fro, for modifiers arepredicted based on the modifier, cm, the part-of-speech tag of the modifier word , tin, thepart-of-speech tag of the head word , th, thehead word itself, Wh, and whether or not themodifier head word, w,,, is known or unknown.P(fm \[Cm,tm,th,Wh,known(Wm)), e.g.P( cap I per I np, per / nnp, vbd, said, true)The probability of a complete tree is theproduct of the probabilities of generating eachelement in the tree.
If we generalize the treecomponents (constituent labels, words, tags,etc.)
and treat them all as simply elements, e,and treat all the conditioning factors as thehistory, h, we can write:P(tree) = H e(e I h)e~tree8 Training the ModelMaximum likelihood estimates for the modelprobabilities can be obtained by observingfrequencies in the training corpus.
However,because these estimates are too sparse to berelied upon, we use interpolated estimatesconsisting of mixtures of successively lower-order estimates (as in Placeway et al 1993).For modifier constituents,components are:P'(cm I cp, chp, Cm_ l , w p) =21 P(c,, ICp,Chp,C,,_I,W,)+22 P(cm I%,chp,Cm_,)the mixtureFor part-of-speech tags,components are:P'(t m ICm,th,Wh)=21 P(t m Icm,wh)+'~2 e(tm I cm, th)+~3 P(t,, I C~,)the mixtureFor head words, the mixture components are:P'(w m I Cm,tm,th, wh) = JL 1 P(w m I Cm,tm, Wh)+22 P(wm Icm,tm,th)+23 P(w m I Cm,t,,)+~4 P(w, It,,)Finally, for word features, the mixturecomponents are:231P'(f,, \[c,,,t~,t h, w h, known(w,,)) =21 P(f,, )c,,,t,,,wh,known(w,,))+)\[2 e(f., \[c~,t,,,th,kn?wn(w,,))+A3 e(L, \[c,,,t ,,known(w,,))+As P(fm \[t,,,known(w,,))9 Searching the ModelGiven a sentence to be analyzed, the searchprogram must find the most likely semanticand syntactic interpretation.
More precisely, itmust find the most likely augmented parsetree.
Although mathematically the modelpredicts tree elements in a top-down fashion,we search the space bottom-up using a chart-based search.
The search is kept tractablethrough a combination of CKY-style dynamicprogramming and pruning of low probabilityelements.9.1 Dynamic ProgrammingWhenever two or more constituents areequivalent relative to all possible later parsingdecisions, we apply dynamic programming,keeping only the most likely constituent in thechart.
Two constituents are consideredequivalent if:1.
They have identical category labels.2.
Their head constituents have identicallabels.3.
They have the same head word.4.
Their leftmost modifiers have identicallabels.. Their rightmost modifiers have identicallabels.9.2 PruningGiven multiple constituents that coveridentical spans in the chart, only thoseconstituents with probabilities within athreshold of the highest scoring constituent aremaintained; all others are pruned.
Forpurposes of pruning, and only for purposes ofpruning, the prior probability of eachconstituent category is multiplied by thegenerative probability of that constituent(Goodman, 1997).
We can think of this priorprobability as an estimate of the probability ofgenerating a subtree with the constituentcategory, starting at the topmost node.
Thus,the scores used in pruning can be consideredas the product of:.
The probability of generating a constituentof the specified category, starting at thetopmost node..
The probability of generating the structurebeneath that constituent, having alreadygenerated a constituent ofthat category.Given a new sentence, the outcome of thissearch process is a tree structure that encodesboth the syntactic and semantic structure of thesentence.
The semantics - that is, the entitiesand relations - can then be directly extractedfrom these sentential trees.10 Experimental ResultsOur system for MUC-7 consisted of thesentential model described in this paper,coupled with a simple probability model forcross-sentence merging.
The evaluationresults are summarized in Table 1.In both Template Entity (TE) and TemplateRelation (TR), our system finished in secondplace among all entrants.
Nearly all of thework was done by the sentential model;disabling the cross-sentence model entirelyreduced our overall F-Score by only 2 points.Task Recall PrecisionEntities (TE) 83% 84%Relations (TR) 64% 81%Table 1:MUC-7 scores.F-Score83.49%71.23%232Task ScorePart-of-Speech Tagging 95.99 (% correct)Parsing (sentences <40 words) 85.06 (F-Score)Name Finding 92.28 (F-Score)Table 2: Component task performance.While our focus throughout the project was onTE and TR, we became curious about howwell the model did at part-of-speech tagging,syntactic parsing, and at name finding.
Weevaluated part-of-speech tagging and parsingaccuracy on the Wall Street Journal using anow standard procedure (see Collins 97), andevaluated name finding accuracy on the MUC-7 named entity test.
The results aresummarized in Table 2.While performance did not quite match thebest previously reported results for any ofthese three tasks, we were pleased to observethat the scores were at or near state-of-the-artlevels for all cases.11 ConclusionsWe have demonstrated, at least for oneproblem, that a lexicalized, probabilisticcontext-free parser with head rules (LPCFG-HR) can be used effectively for informationextraction.
A single model proved capable ofperforming all necessary sentential processing,both syntactic and semantic.
We were able touse the Penn TREEBANK to estimate thesyntactic parameters; no additional syntactictraining was required.
The semantic trainingcorpus was produced by students according toa simple set of guidelines.
This simplesemantic annotation was the only source oftask knowledge used to configure the model.AcknowledgementsThe work reported here was supported in partby the Defense Advanced Research ProjectsAgency.
Technical agents for part of this workwere Fort Huachucha and AFRL undercontract numbers DABT63-94-C-0062,F30602-97-C-0096, and 4132-BBN-001.
Theviews and conclusions contained in thisdocument are those of the authors and shouldnot be interpreted as necessarily representingthe official policies, either expressed orimplied, of the Defense Advanced ResearchProjects Agency or the United StatesGovernment.We thank Michael Collins of the University ofPennsylvania for his valuable suggestions.ReferencesBikel, Dan; S. Miller; R. Schwartz; and R.Weischedel.
(1997) "NYMBLE: A High-Performance Learning Name-finder."
InProceedings of the Fifth Conference on AppliedNatural Language Processing, Association forComputational Linguistics, pp.
194-201.Collins, Michael.
(1996) "A New Statistical ParserBased on Bigram Lexical Dependencies."
InProceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics, pp.184-191.Collins, Michael.
(1997) "Three Generative,Lexicalised Models for Statistical Parsing."
InProceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics, pp.16-23.Marcus, M.; B. Santorini; and M.
Marcinkiewicz.
(1993) "Building a Large Annotated Corpus ofEnglish: the Penn Treebank."
ComputationalLinguistics, 19(2):313-330.Goodman, Joshua.
(1997) "Global Thresholdingand Multiple-Pass Parsing."
In Proceedings ofthe Second Conference on Empirical Methods inNatural Language Processing, Association forComputational Linguistics, pp.
11-25.Placeway, P., R. Schwartz, et al (1993).
"TheEstimation of Powerful Language Models fromSmall and Large Corpora."
IEEE ICASSPWeischedel, Ralph; Marie Meteer; RichardSchwartz; Lance Ramshaw; and Jeff Palmucci.
(1993) "Coping with Ambiguity and UnknownWords through Probabilistic Models.
"Computational Linguistics, 19(2):359-382.233
