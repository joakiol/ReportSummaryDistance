Two Statistical Parsing Models Applied to the Chinese TreebankDaniel M. Bikel David ChiangDepartment of Computer & Information ScienceUniversity of Pennsylvania200 South 33rd StreetPhiladelphia, PA 19104-6389(dbikel, dchiang)?cis, upenn, eduAbst ractThis paper presents the first-everresults of applying statistical pars-ing models to the newly-availableChinese Treebank.
We have em-ployed two models, one extractedand adapted from BBN's SIFT Sys-tem (Miller et al, 1998) and a TAG-based parsing model, adapted from(Chiang, 2000).
On sentences with<40 words, the former model per-forms at 69% precision, 75% recall,and the latter at 77% precision and78% recall.1 In t roduct ionEver since the success of HMMs' applica-tion to part-of-speech tagging in (Church,1988), machine learning approaches to nat-ural language processing have steadily be-come more widespread.
This increase has ofcourse been due to their proven efficacy inmany tasks, but also to their engineering effi-Cacy.
Many machine learning approaches letthe data speak for itself (data ipsa loquun-tur), as it were, allowing the modeler to focuson what features of the data are important,rather than on the complicated interactionof such features, as had often been the casewith hand-crafted NLP systems.
The successof statistical methods in particular has beenquite evident in the area of syntactic pars-ing, most recently with the outstanding re-sults of (Charniak, 2000) and (Colhns, 2000)on the now-standard English test set of thePenn Treebank (Marcus et al, 1993).
A sig-nificant trend in parsing models has been theincorporation of linguistically-motivated f a-tures; however, it is important o note that"linguistically-motivated" does not necessarilymean "language-dependent"---often, it meansjust the opposite.
For example, almost all sta-tistical parsers make use of lexicalized non-terminals in some way, which allows lexicalitems' indiosyncratic parsing preferences tobe modeled, but the paring between headwords and their parent nonterminals i  deter-mined almost entirely by the training data,thereby making this feature--which modelspreferences of particular words of a par-ticular language---almost entirely language-independent.
In this paper, we will explore theuse of two parsing models, which were origi-nally designed for English parsing, on parsingChinese, using the newly-available ChineseTreebank.
We will show that the language-dependent components of these parsers arequite compact, and that with little effort theycan be adapted to produce promising resultsfor Chinese parsing.
We also discuss directionsfor future work.2 Mode ls  and  Mod i f i ca t ionsWe will briefly describe the two parsing mod-els employed (for a full description of the BBNmodel, see (Miller et al, 1998) and also (Bikel,2000); for a full description Of the TAG model,see (Chiang, 2000)).2.1 Model  2 of (Collins, 1997)Both parsing models discussed in this paperinherit a great deal from this model, so webriefly describe its "progenitive" features here,describing only how each of the two models ofthis paper differ in the subsequent two sec-tions.The lexicalized PCFG that sits behindModel 2 of (Collins, 1997) has rules of theformP ~ LnLn-I"'" L IHR I " "  .Rn-IRn (1)S(will-MD)NP(AppI,~NNP) VP(wilI-MD)NNPI Apple MD VP (buy-VB)VB PRT(out-RP) NP(Microsoft--NNP)I \[ Ibuy RP NNPI Iout MicrosoftFigure 1: A sample sentence with parse tree.where P, Li, R/ and H are all lexicalizednonterminals, and P inherits its lexical headfrom its distinguished head child, H. In thisgenerative model, first P is generated, thenits head-child H, then each of the left- andright-modifying nonterminals are generatedfrom the head outward.
The modifying non-terminals Li and R/are  generated condition-ing on P and H, as well as a distance met-ric (based on what material intervenes be-tween the currently-generated modifying non-terminal and H) and an incremental subcatframe feature (a multiset containing the com-plements of H that have yet to be gener-ated on the side of H in which the currently-generated nonterminal falls).
Note that if themodifying nonterminals were generated com-pletely independently, the model would bevery impoverished, but in actuality, by includ-ing the distance and subcat frame features,the model captures a crucial bit of linguis-tic reality, viz., that words often have well-defined sets of complements and adjuncts, dis-persed with some well-defined istribution inthe right hand sides of a (context-free) rewrit-ing system.2.2 BBN Mode l2.2.1 Overv iewThe BBN model is also of the lexicalizedPCFG variety.
In the BB.N model, as withModel 2 of (Collins, 1997), modifying non-terminals are generated conditioning both onthe parent P and its head child H. UnlikeModel 2 of (Collins, 1997), they are also gen-erated conditioning on the previously gener-ated modifying nonterminal, L/-1 or Pq-1,and there is no subcat frame or distance fea-ture.
While the BBN model does not per-form at the level of Model 2 of (Collins, 1997)on Wall Street Journal text, it is also lesslanguage-dependent, eschewing the distancemetric (which relied on specific features of theEnglish Treebank) in favor of the "bigrams onnonterminals" model.2.2.2 Mode l  ParametersThis section briefly describes the top-levelparameters used in the BBN parsing model.We use p to denote the unlexicalized nonter-minal corresponding to P in (1), and simi-larly for li, ri and h. We now present he top-level generation probabilities, along with ex-amples from Figure 1.
For brevity, we omit thesmoothing details of BBN's model (see (Milleret al, 1998) for a complete description); wenote that all smoothing weights are computedvia the technique described in (Bikel et al,1997).The probability of generating p as theroot label is predicted conditioning on only+TOP+,  which is the hidden root of all parsetrees:P (Pl + TOP+) ,  e.g., P(S I + TOP+).
(2)The probability of generating a head node hwith a parent p isP(h I P), e.g., P(VP \] S).
(3)The probability of generating a left-modifierli isPL(li I Z -i,p, h, wh),, e.g., (4)PL(NP \] + BEGIN+, S, VP, will)2when generating the NP for NP(Apple-NNP),and the probability of generating a right mod-ifier ri isPR(ri i ri-i,p, h, Wh), e.g., (5)Pn(NP I PRT, VP, VB, buy)when generating the NP for NP(Microsoft-NNP).
1The probabilities for generating lexical ele-ments (part-of-speech tags and words) are asfollows.
The part of speech tag of the head ofthe entire sentence, th, is computed condition-ing only on the top-most symbol p:2P(th I P)- (6)Part of speech tags of modifier constituents,tli and tri, are predicted conditioning on themodifier constituent li or r/, the tag of thehead constituent, h, and the word of the headconstituent, WhP(tl, \[li, th, Wh) and P(tr~ \[ ri, th, Wh).
(7)The head word of the entire sentence, Wh, ispredicted conditioning only on the top-mostsymbol p and th:P(whlth,p).
(8)Head words of modifier constituents, w h andWry, are predicted conditioning on all the con-text used for predicting parts of speech in (7),as well as the parts of speech themslevesP(wt, \[ tl,, li, th, Wh)and P(wri \[ try, ri, th, Wh).
(9)The original English model also included aword feature to heIp reduce part-of-speechambiguity for unknown words, but this com-ponent of the model was removed for Chinese,as it was language-dependent.The probability of an entire parse tree isthe product of the probabilities of generat-ing all of the elements of that parse tree,1The hidden nonterminal +BEGIN+ is used toprovide a convenient mechanism for determining theinitial probability of the underlying Markov processgenerating the modifying nonterminals; the hiddennonterminal +END+ is used to provide consistency tothe underlying Markov process, i.e., so that he proba-bilities of all possible nonterminal sequences sum to 1.2This is the one place where we altered the originalmodel, as the lexical components of the head of theentire sentence were all being estimated incorrectly,causing an inconsistency in the model.
We correctedthe estimation of th and Wh in our implementation.where an element is either a constituent la-bel, a part of speech tag or a word.
We obtainmaximum-likelihood estimates of the param-eters of this model using frequencies gatheredfrom the training data.2.3 TAG Mode lThe model of (Chiang, 2000) is basedon stochastic TAG (Resnik, 1992; Schabes,1992).
In this model a parse tree is built up notout of lexicalized phrase-structure rules but bytree fragments (called elementary trees) whichare texicalized in the sense that each fragmentcontains exactly one lexical item (its anchor).In the variant of TAG we use, there arethree kinds of elementary tree: initial, (pred-icative) auxiliary, and modifier, and threecomposition operations: substitution, adjunc-tion, and sister-adjunction.
Figure 2 illus-trates all three of these operations, c~i is aninitial tree which substitutes at the leftmostnode labeled NP$;/~ is an auxiliary tree whichadjoins at the node labeled VP.
See (Joshi andSchabes, 1997) for a more detailed explana-tion.Sister-adjunction is not a standard TAG op-eration, but borrowed from D-Tree Grammar(Rainbow et al, 1995).
In Figure 2 the modi-fier tree V is sister adjoined between the nodeslabeled VB and NP$.
Multiple modifier treescan adjoin at the same place, in the spirit of(Schabes and Shieber, 1994).In stochastic TAG, the probability of gen-erating an elementary tree depends on the el-ementary tree itself and the elementary treeit attaches to.
The parameters are as follows:= iI + P (NONE I = i#where c~ ranges over initial trees,/~ over aux-iliary trees, 3' over modifier trees, and T/overnodes.
Pi(c~) is the probability of beginninga derivation with c~; Ps(o~ I 77) is the prob-ability of substituting o~ at 7; Pa(/~ I r/) isthe probability of adjoining ~ at 7/; finally,Pa(NONE I 7) is the probability of nothingadjoining at ~/.Our variant adds another set of parameters:3~2(O~2) S l ~.
2NPJ~ ..... VP S, . "
gap...... !
~ 4....
?.......
!
i ~.. .
............ NP VP .,._.re d{ \[ buy \ ": INP  VP  PRT  NP ~ NNPI ~ .
I I I MD VPNNP MD VP* RP NNP Apple II I I I willApple will out Microsoft VB PitT NPI I I(oq) (fl) (7) (a3) buy RP NNPI Iout Microsoftde.vat .n  treetreeFigure 2: Grammar and derivation for "Apple will buy out Microsoft.
"~ Psa(T I ~7, i , f )  + Psa(STOP I ~l,i,f) = 1This is the probability of sister-adjoining 7between the ith and i + lth children of ~ (al-lowing for two imaginary children beyond theleftmost and rightmost children).
Since multi-ple modifier trees can adjoin at the same lo-cation, Psa(7) is also conditioned on a flag fwhich indicates whether '7 is the first modi-fier tree (i.e., the one closest o the head) toadjoin at that location.For our model we break down these prob-abilities further: first the elementary tree isgenerated without its anchor, and then its an-chor is generated.
See (Chiang, 2000) for moredetails.During training each example is brokeninto elementary trees using head rules andargument/adjunct rules similar to those of(Collins, 1997).
The rules are interpreted asfollows: a head is kept in the same elemen-tary tree in its parent, an argument is brokenoff into a separate initial tree, leaving a sub-stitution node, and an adjunct is broken offinto a separate modifier tree.
A different ruleis used for extracting auxiliary trees; see (Chi-ang, 2000) for details.
Xia (1999) describes asimilar process, and in fact our rules for theXinhua corpus are based on hers.2.4 Modif icat ionsThe primary language-dependent componentthat had to be changed in both models wasthe head table, used to determine heads whentraining.
We modified the head rules describedin (Xia, 1999) for the Xinhua corpus and sub-stituted these new rules into both models.The (Chiang, 2000) model had the followingadditional modifications.?
The new corpus had to be prepared foruse with the trainer and parser.
Asidefrom technicalities, this involved retrain-ing the part-of-speech tagger described in(Ratnaparkhi, 1997), which was used fortagging unknown words.
We also loweredthe unknown word threshold from 4 to 2because the Xinhua corpus was smallerthan the WSJ corpus.?
In addition to the change to the head-finding rules, we also changed the rulesfor classifying modifiers as arguments oradjuncts.
In both cases the new rules wereadapted from (Xia, 1999).?
For the tests done in this paper, a beamwidth of 10 -4 was used.The BBN model had the following additionalmodifications:?
As with the (Chiang, 2000) model, wesimilarly lowered the unknown wordthreshold of the BBN model from its de-fault 5 to 2.?
The language-dependent word-featurewas eliminated, causing parts of speechfor unknown words to be predicted solelyon the head relations in the model.?
The default beam size in the probabilis-tic CKY parsing algorithm was widened.The default beam pruned away chart en-tries whose scores were not within a fac-tor of e -5 of the top-ranked subtree; this4Model, test setBBN-allt, WSJ-allBBN-small-h WSJ-small*BBN,  Xinhua:~Chiang-all, WSJ-allChiang-small, WSJ-smallCh iang,  X inhuaBBN-allt, WSJ-allBBN-smallI, WSJ-small*BBN,  Xinhua:~Chiang-all, WSJ-allChiang-small, WSJ-smallCh iang,  X inhua<40 wordsLR LP CB 0CB <2CB84.7 86.5 1.12 60.6 83.279.0 80.7 1.66 47.0 74.669.0 74.8 2.05 45.0 68.586.9 86.6 1.09 63.2 84.378.9 79.6 1.75 44.8 72.476.8 77.8 1.99 50.8 74.1-< 100 wordsLR LP CB 0CB _<2CB83.9 85.7 1.31 57.8 80.878.4 80.0 1.92 44.3 71.367.5 73.5 2.87 39.9 61.886.2 85.8 1.29 60.4 81.877.1 78.8 2.00 43.25 70.573.3 74.6 3.03 44.8 66.8Table 1: Results for both parsing models on all test sets.
Key: LR = labeled recall, LP = labeledprecision, CB = avg.
crossing brackets, 0CB = zero crossing brackets, <2CB = <2 crossingbrackets.
All results are percentages, except for those in the CB column, tUsed larger beamsettings and lower unknown word threshold than the defaults.
*3 of the 400 sentences were notparsed due to timeouts and/or pruning problems.
:~3 of the 348 sentences did not get parsed dueto pruning problems, and 2 other sentences had length mismatches ( coring program errors).tight limit was changed to e -9.
Also, thedefault decoder pruned away all but thetop 25-ranked chart entries in each cell;this limit was expanded to 50.3 Exper iments  and  Resu l tsThe Chinese Treebank consists of 4185 sen-tences of Xinhua newswire text.
We blindlyseparated this into training, devtest and testsets, with a roughly 80/10/10 split, puttingfiles 001-270 (3484 sentences, 84,873 words)into the training set, 301-325 (353 sentences,6776 words) into the development test set andreserving 271-300 (348 sentences, 7980 words)for testing.
See Table 1 for results.In order to put the new Chinese Treebankresults into context with the unmodified (En-glish) parsing models, we present results ontwo test sets from the Wall Street Journal:WSJ-all, which is the complete Section 23 (thede facto standard test set for English pars-ing), and WSJ-small, which is the first 400sentences of Section 23 and which is roughlycomparable in size to the Chinese test set.Furthermore, when testing on WSJ-small, wetrained on a subset of our English trainingdata roughly equivalent in size to our Chinesetraining set (Sections 02 and 03 of the PennTreebank); we have indicated models trainedon all English training with "-all", and mod-els trained with the reduced English train-ing set with "-small".
Therefore, by compar-ing the WSJ-small results with the Chineseresults, one can reasonably gauge the perfor-mance gap between English parsing on thePenn Treebank and Chinese parsing on theChinese Treebank.The reader will note that the modified BBNmodel does significantly poorer than (Chiang,2000) on Chinese.
While more investigation isrequired, we suspect part of the difference maybe due to the fact that currently, the BBNmodel uses language-specific rules to guesspart of speech tags for unknown words.4 Conc lus ions  and  Future  WorkThere is no question that a great deal of careand expertise went into creating the ChineseTreebank, and that it is a source of importantgrammatical information that is ufiique to theChinese language.
However, there are definitesimilarities between the grammars of Englishand Chinese, especially when viewed throughthe lens of the statistical models we employedhere.
In both languages, the nouns, adjec-tives, adverbs, and verbs have preferences forcertain arguments and adjuncts, and thesepreferences--in spite of the potentially vastly-different configurations of these items--are f-fectively modeled.
As discussed in the intro-duction, lexica!
items' idiosyncratic parsingpreferences are modeled by lexicalizing thegrammar formalism, using a lexicalized PCFGin one case and a lexicalized stochastic TAGin the other.
Linguistically-reasonable inde-pendence assumptions are made, such as theindependence of grammar productions in thecase of the PCFG model, or the independenceof the composition operations in the case ofthe LTAG model, and we would argue thatthese assumptions are no less reasonable forthe Chinese grammar than they are for thatof English.
While results for the two languagesare far from equal, we believe that further tun-ing of the head rules, and analysis of develop-ment test set errors will yield significant per-formance gains on Chinese to close the gap.Finally, we fully expect hat absolute perfor-mance will increase greatly as additional high-quality Chinese parse data becomes available.5 AcknowledgementsThis research Was funded in part by NSFgrant SBR-89-20230-15.
We would greatlylike to acknowledge the researchers at BBNwho allowed us to use their model: RalphWeischedel, Scott Miller, Lance Rarnshaw,Heidi Fox and Sean Boisen.
We would alsolike to thank Mike Collins and our advisorsAravind Joshi and Mitch Marcus.Re ferencesDaniel M. Bikel, Richard Schwartz, RalphWeischedel, and Scott Miller.
1997.
Nymble:A high-performance learning name-finder.
InFifth Conference on Applied Natural LanguageProcessing, pages 194-201,, Washington, D.C.Daniel M. Bikel.
2000.
A statistical model forparsing and word-sense disarnbiguation.
InJoint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and VeryLarge Corpora, Hong Kong, October.Eugene Charniak.
2000.
A maximum entropy-inspired parser.
In Proceedings of the 1st Meet-ing of the North American Chapter of the As-sociation for Computational Linguistics, pages132-139, Seattle, Washington, April 29 to May4.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tr eadjoining gram-mar.
In Proceedings of the 38th Annual Meetingof the Association for Computational Linguis-tics.Kenneth Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
In Second Conference on Applied NaturalLanguage Processing, pages 136-143, Austin,Texas.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of ACL-EACL '97, pages 16-23.Michael Collins.
2000.
Discriminative r rankingfor natural language parsing.
In InternationalConference on Machine Learning.
(to appear).Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In A. Salomma andG.
Rosenberg, editors, Handbook of FormalLanguages and Automata, volume 3, pages 69-124.
Springer-Verlag, Heidelberg.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: The PennTreebank.
Computational Linguistics, 19:313-330.Scott Miller, Heidi Fox, Lance Ramshaw, andRalph Weischedel.
1998.
SIFT - Statistically-derived Information From Text.
In SeventhMessage Understanding Conference (MUC-7),Washington, D.C.Owen Rainbow, K. Vijay-Shanker, and DavidWeir.
1995.
D-tree grammars.
In Proceedingsof the 33rd Annual Meeting of the Assocationfor Computational Linguistics, pages 151-158.Adwait Ratnaparkhi.
1997.
A simple introduc-tion to maximum entropy models for naturallanguage processing.
Technical Report 1RCSReport 97-08, Institute for Research in Cogni-tive Science, May.Philip Resnik.
1992.
Probabilistic tree-adjoininggrammar as a framework for statistical nat-ural language processing.
In Proceedings ofCOLING-92, pages 418-424.Yves Schabes and Stuart M. Shieber.
1994.
Analternative conception of tree-adjoining deriva-tion.
Computational Linguistics, 20(1):91-124.Yves Schabes.
1992.
Stochastic lexicalizedtree-adjoining grammars.
In Proceedings ofCOLING-92, pages 426-432.Fei Xia.
1999.
Extracting tree adjoining ram-mars from bracketed corpora.
In Proceedingsof the 5th Natural Language Processing PacificRim Symposium (NLPRS-99).
