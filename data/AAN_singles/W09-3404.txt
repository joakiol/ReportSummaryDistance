Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 24?31,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAnalysis and Development of Urdu POS Tagged CorpusAhmed MuazCenter for Research in UrduLanguage ProcessingNUCES, Pakistanahmed.muaz@nu.edu.pkAasim AliCenter for Research in UrduLanguage Processing,NUCES, Pakistanaasim.ali@nu.edu.pkSarmad HussainCenter for Research in UrduLanguage ProcessingNUCES, Pakistansarmad.hussain@nu.edu.pkAbstractIn this paper, two corpora of Urdu (with 110Kand 120K words) tagged with different POStagsets are used to train TnT and Tree taggers.Error analysis of both taggers is done to identi-fy frequent confusions in tagging.
Based onthe analysis of tagging, and syntactic structureof Urdu, a more refined tagset is derived.
Theexisting tagged corpora are tagged with thenew tagset to develop a single corpus of 230Kwords and the TnT tagger is retrained.
The re-sults show improvement in tagging accuracyfor individual corpora to 94.2% and also forthe merged corpus to 91%.
Implications ofthese results are discussed.1 IntroductionThere is increasing amount of work on computa-tional modeling of Urdu language.
As variousgroups work on the language, diversity in analy-sis is also developed.
In this context, there hasbeen some work on Urdu part of speech (POS)tagging, which has caused multiple tagsets toappear.
Thus, there is also need to convergethese efforts.Current work compares the existing tag-sets of Urdu being used for tagging corpora in anattempt to look at the differences, and understandthe reasons for the variation.
The work then un-dertakes experiments to develop a common tag-set, which is syntactically and computationallycoherent.
The aim is to make a robust tagset andthen to port the differently tagged Urdu corporaonto the same tagset.
As Urdu already has veryfew annotated corpora, this will help consolidat-ing them for better modeling.The next sections present the existing tag-sets and accuracies of the POS taggers reportedusing them.
Sections 4 and 5 present baselineexperiment and the methodology used for theanalysis for updating the tagset.
Section 6 de-scribes the proposed tagset.
Section 7 reportsexperiments comparing the new tagset with ex-isting ones.
Section 8 discusses the resultsachieved and future directions.2 Relevant  Resources of Urdu2.1 Urdu CorporaSeveral annotated corpora have been built duringlast few years to facilitate computationalprocessing for Urdu language.
The initial workwas undertaken through EMILLE project tobuild multi-lingual corpora for South Asian lan-guages (McEnery et al, 2000).
They released200,000 words parallel corpus of English, Urdu,Bengali, Gujarati, Hindi and Punjabi.
In addition,there are 1,640,000 words of Urdu text in thiscorpus.
These text collections are also annotatedwith part of speech tags (Hardie 2003).Center for Research in Urdu LanguageProcessing (CRULP1) gathered 18 million wordscorpus in order to build a lexicon.
It has cleanedtext from news websites from multiple domains(Ijaz et.al.
2007).
Following this work, a syntac-tic tagset was developed based on work by exist-ing grammarians and a corpus of 110,000 wordswas manually tagged.
This annotated corpus isavailable through the center (Sajjad 2007, Hus-sain 2008).Recently an English-Urdu parallel cor-pus has also been developed by CRULP, bytranslating the first 140,000 words of PENNTreebank corpus.
In addition, a tagset has alsobeen designed following the PENN Treebankguidelines.
These words have been tagged ma-nually with this new tagset.
This collection isalso available from CRULP, and the tagset is stillunpublished.2.2 Urdu Part of Speech tagsetsHardie (2003) developed the first POS tagset forUrdu using EAGLES guidelines for computa-tional processing.
The tagset contains 282 mor-pho-syntactic tags, differentiating on the basis ofnumber, gender and other morphological details1 www.crulp.org24in addition to the syntactic categories.
Punctua-tion marks are tagged as they are, and not in-cluded in 282 tags.
The tags include the genderand number agreement, in addition to syntacticinformation.The complications of Urdu tagset designare also discussed.
One of these complexities isword segmentation issue of the language.
Suffix-es in Urdu are written with an orthographicspace.
Words are separated on the basis of spaceand so suffixes are treated same as lexical words.Hence it is hard to assign accurate tag for an au-tomatic tagger.
Although the tagset is designedconsidering details, but due to larger number oftags it is hard to get a high accuracy with a smallsized corpus.
Due to its morphological depen-dence and its large size, this tagset is not consi-dered in our analysis.Two much smaller tagsets are consideredfor this work.
They are compared in detail inSection 6.
The first tagset, containing 42 tags, isdesigned by Sajjad (2007), based on the work ofUrdu grammarians (e.g.
Schmidt 1999, Haq1987, Javed 1981, Platts 1909) and computation-al work by Hardie (2003).
The main features ofthe tagset include multiple pronouns (PP, KP,AKP, AP, RP, REP, G, and GR) and demonstra-tives (PD, KD, AD, and RD).
It has only one tagfor all forms of verbs (VB), except for auxiliariesto show aspect (AA) and tense (TA) informationabout the verb.
All noun types are assigned sin-gle tag (NN) except for Proper Nouns (PN).
Italso has a special tag NEG to mark any occur-rence negation words (?Yl?
?not?
and ??
?no?
or?neither?)
regardless of context.
It also has a tagSE to mark every occurrence of ?5 (?from?
)without considering the context.
Another exam-ple of such a context-free lexical tag is WALA tomark every occurrence (including all the inflec-tions) of the word ???.
This tagset is referred to asT1 subsequently in this paper.Recently Sajjad and Schmid (2009) usedthe tagged data of 107,514 words and carried outan experiment for tagger comparison.
A total of100,000 words are used as training set and rest astest data.
Four taggers (TnT, Tree, RF and SVM)are trained using training corpus and then testedaccordingly.
Reported results of this work showthat SVM tagger is the most accurate, showing94.15% correct prediction of tags.
Remainingthree taggers have accuracies of 93.02% (Treetagger), 93.28% (RF tagger) and 93.40% (TnTtagger).Another tagset has recently been devel-oped as a part of a project to develop English-Urdu parallel corpus at CRULP, following thePenn Treebank guidelines (Santorini 1990).
Itcontains 46 tags, with fewer grades of pronouns(PR, PRP$, PRRF, PRRFP$, and PRRL) anddemonstratives (DM and DMRL), as comparedto T1.
It has several tags for verbs on the basis oftheir forms and semantics (VB, VBI, VBL,VBLI, and VBT) in addition to the tags for aux-iliaries showing aspect (AUXA) and tense(AUXT).
The NN tag is assigned for both singu-lar and plural nouns and includes adverbial kafpronoun, kaf pronoun, and adverbial pronouncategories of T1.
Yet, it has several other gradesof common nouns (NNC, NNCR, NNCM).
Italso has two shades of Proper Nouns (NNP,NNPC), which are helpful in identifying phraseboundary of compound proper nouns.
It also hasa tag WALA that is assigned to every occurrence(and inflection) of word ???
)wala( .
However,marking of token ?5 (?from?)
is context depen-dent: either it is CM when marking case or it isRBRP when occurring as an adverbial particle.This tagset is referred to as T2 subsequently inthis paper.3 Tools and Resource SelectionThe decision of selecting the tagger, the tagset,and the data is the starting point for the task ofPOS tagging.
This section gives details of thetaggers chosen and the corpora used for the expe-riments conducted.3.1 Selection of taggersThere are a number of existing taggers availablefor tagging.
Two POS taggers are used in theinitial step of this work to compare the initialtagging accuracies.One of the selected taggers is Trigram-and-Tag (TnT).
It is a trigram based HMM tag-ger in which two preceding tags are used to findthe transition probability of a tag.
Brants (2000)tested PENN Treebank (English) and NEGRA(German) corpora and reported 96-97% accuracyof the tagger.Schmid (1994) proposed probabilisticPOS tagger that uses decision trees to store thetransition probabilities.
The trained decision treeis used for identification of highest probable tags.Schmid reported an accuracy of 95-96% onPENN Treebank for this tagger.25Both taggers give good accuracy for Ur-du tagging, as reported by Sajjad and Schmid(2009).3.2 Data Used for ExperimentationCorpora annotated with the different tagsets areacquired from CRULP.
The corpus originallytagged with T1 tagset is referred to as C1 (newsfrom non-business domain) and the corpus in-itially annotated with T2 tagset is referred to asC2 (news from business domain), subsequentlyin the current work.
Both C1 and C2 are takenand cleaned.
The data is re-counted and approx-imately 100,000 words are separated for trainingand rest are kept for testing.
The details of dataare given in Tables 1 and 2 below.Table 1.
Number of tokens in Urdu corporaTokens C1 C2Training 101,428 102,454Testing 8,670 21,181Total 110,098 123,635Table 2.
Number of sentences in Urdu corporaSentences C1 C2Training 4,584 3,509Testing 404 755Total 4,988 4,2644 Baseline EstimationThe comparison is initiated with training of ex-isting tagsets on their respective annotated data(T1 on C1 and T2 on C2).
Both corpora aretested on TnT and Tree Tagger to obtain the con-fusion matrices for errors.
These confusion ma-trices are used to analyze misclassification oftags.
TnT tagger shows that overall accuracy ofusing T1 with C1 is 93.01% and is significantlybetter than using T2 with C2, which gives88.13% accuracy.
Tree tagger is also trained onthe corpora.
The overall accuracy of T1 on C1(93.37%) is better than that of T2 on C2(90.49%).
The results are shown in Table 3.Table 3.
Results of both tagsets on their respec-tive corpora with TnT and Tree taggersT1 on C1 T2 on C2TnT Tagger 93.01% 88.13%Tree Tagger 93.37% 90.49%The accuracies reported (for T1 on C1) bySajjad and Schmid (2009) are comparable tothese accuracies.
They have reported 93.40% forTnT Tagger and 93.02% for Tree Tagger.Further experimentation is performed onlyusing TnT tagger.5 MethodologyThe current work aims to build a larger corpus ofaround 230,000 manually tagged words for Urduby combining C1 and C2.
These collections areinitially annotated with two different tagsets (T1and T2 respectively, and as described above).For this unification, it was necessary to indentifythe differences in the tagsets on which these cor-pora are annotated, analyzed the differences andthen port them to unified tagset.The work starts with the baseline estimation(described in Section 4 above).
The results ofbaseline estimation are used to derive a new tag-set (detailed in Section 6 below), referred to asT3 in this paper.
Then a series of experiments areexecuted to compare the performance of threetagsets (T1, T2, and T3) on data from two differ-ent domains (C1 and C2), as reported in Section7 below and summarized in Table 4.Table 4.
Summary of experiments conductedExperiment Tagset Corpus0Baseline Estimation:Original tagsets withrespective corporaT1 C1T2 C21 Experiment1: Forcomparison of resultsof T1 and T3 on C1T3 C12Experiment2: Forcomparison of T1, T2and T3 on C2T3 C2T1 C23Experiment3: Compar-ison of T1 and T3 withno unknownsT3 C2T1 C24Experiment4: Compar-ison of T1 and T3 overcomplete corpusT3 C1+C2T1 C1+C2The performance of T1 on C1 is alreadybetter than T2 on C2, so the first comparison forthe merged tagset T3 is with T1 on C1, which isthe basis of the first experiment.
Then the per-formance of better performing tagsets (T1 andT3) are compared on the corpus C2 in the second26experiment to compare them with T2.
One possi-ble reason of relatively better performance couldbe the difference in application of open classesfor unknown words in the test data.
Therefore,the third experiment is performed using the samedata as in second experiment (i.e.
corpus C2)with combined lexicon of training and test data(i.e.
no unknown words).
Finally, an experimentis conducted with the merged corpus.
Followingtable summarizes these experiments.6 Tagset designAfter establishing the baseline, the existing tag-sets are reviewed with the following guidelines:?
Focus on the syntactic variation (instead ofmorphological or semantic motivation) to ei-ther collapse existing tags or introduce newones?
Focus on word level tagging and not try to ac-commodate phrase level tagging (e.g.
to sup-port chunking, compounding or other similartasks)?
Tag according to the syntactic role instead ofhaving a fixed tag for a string, where possible?
Use PENN Treebank nomenclature to keep thetagset easy to follow and shareComparison of T1 and T2 showed that thereare 33 tags in both tagsets which represent samesyntactic categories, as shown in Appendix A.The tag I (Intensifier) in T2 labels the wordswhich are marked as ADV in T1.
The words an-notated as NNC, NNCR and NNCM (under T2)are all labeled as NN under T1.
The wordstagged as VBL, VBLI, VBI, and VBLI (underT2) are all labeled as VB under T1.
Range ofdistinct tags for demonstratives of T1 are allmapped to DM in T2 except RD (of T1) whichmaps to DMRL (of T2).In order to identify the issues in tagging, adetailed error analysis of existing tagsets is per-formed.
Following tables represent the major tagconfusions for tagging C2 with T2 using Treeand TnT taggers.Table 5.
Major misclassifications in C2 with T2tagset using Tree taggerTagTotaltokens ErrorsMaximummisclassificationVB 888 214 183 VBLVBL 328 168 151 VBVBI 202 47 38 VBLIVBLI 173 52 46 VBIAUXT 806 145 121 VBTTable 6.
Major misclassifications in C2 with T2tagset using TnT-taggerTagTotaltokens ErrorMaximummisclassificationVB 888 240 181 VBLVBL 328 154 135 VBVBI 202 46 34 VBLIVBLI 173 61 55 VBIAUXT 806 136 111 VBTThe proposed tagset for Urdu part-of-speech tagging contains 32 tags.
The construc-tion of new tagset (T3) is initiated by adoptingT2 as the baseline, because T2 uses the taggingconventions of PENN Treebank.
There are 17tags in T3 that are same as in T1 and T2.
Thesetags (CC, CD, DM, DMRL, JJ, NN, OD, PM,PRP, PRP$, PRRF, PRRF$, PRRL, Q, RB, SM,SYM) are not discussed in detail.
The completetagset alng with short description and examplesof each tag is given in Appendix B.RBRP (Adverbial Particle) and CM(Case Marker) are merged to make up a new tagPP (Postposition), so every postposition particlecomes under this new tag ignoring semantic con-text.
I (Intensifier) is used to mark the intensifi-cation of an adjective, which is a semantic grada-tion, and syntactically merged with Q (Quantifi-er).
NNCM (Noun after Case Marker), NNC(Noun Continuation), NNCR (Continuing NounTermination) are merged into NN (Noun) be-cause syntactically they always behave similarlyand the difference is motivated by phrase levelmarking.
U (Unit) is also merged with NN be-cause the difference is semantically motivated.DATE is not syntactic, and may be eithertreated as NN (Noun) or CD (Cardinal), depend-ing upon the context.
Similarly, R (Reduplica-tion), MOPE (Meaningless Pre-word), and MO-PO (Meaningless Post-word) always occur inpair with NN, JJ, or another tag.
Thus they arephrasal level tags, and can be replaced by rele-vant word level tag in context.
NNPC (ProperNoun Continuation) tag identifies compoundingbut syntactically behaves as NNP (Proper Noun),and is not used.VBL (Light Verb) is used in complex predi-cates (Butt 1995), but its syntactic similarity withVB (Verb) is a major source of confusion in au-tomatic tagging.
It is collapsed with VB (Verb).Similarly, VBLI (Light Verb Infinitive) ismerged with VBI (Verb Infinitive).
AUXT(Tense Auxiliary) is highly misclassified as VBT(To be Verb) because both occur as last token ina clause or sentence, and both include tense in-27formation.
The word is labeled as VBT onlywhen there is no other verb in the sentence orclause, otherwise these words are tagged asAUXT.
The syntactic similarity of both tags isalso evident from statistically misclassifyingAUXT as VBT.
Therefore both are collapsedinto single tag VBT (Tense Verb).In T1, NEG (Negation) is used to mark allthe negation words without context, but theymostly occur as adverbs.
Therefore, NEG tag isremoved.
Similarly, SE (Postposition ?5 ,?from?)
is not separated from postpositions andmarked accordingly.
PRT (Pre-Title) and POT(Post-Title) always occur before or after ProperNoun, respectively.
Therefore, they behave asProper Nouns, hence proposed to be labeled asNNP (Proper Noun).7 ExperimentsAfter designing a new tagset, a series of experi-ments are conducted to investigate the proposedchanges.
The rationale of the sequence of expe-riments has been discussed in Section 5 above,however the reasoning for each experiment isalso given below.
As T2 tags have much moresemantic and phrasal information, and C2 taggedwith T2 shows lower accuracy than T1 on C1,therefore further experiments are conducted tocompare the performance of T1 and T3 only.Comparisons on C2 with T3 may also be drawn.7.1 Experiment 1As baseline estimation shows that T1 on C1 out-performs T2 on C2, the first experiment is tocompare the performance of T3 on C1.
In thisexperiment C1 is semi-automatically tagged withT3.
TnT tagger is then trained and tested.
T3gives 93.44% accuracy, which is slightly betterthan the results already obtained for T1(93.01%).
The results are summarized in Table 7.Table 7.
Accuracies of T3 and T1 on C1Corpus Tagset AccuracyC1 T3 93.44%C1 T1 93.01%7.2 Experiment 2Now to test the effect of change in domain of thecorpus, the performance T1 and T3 on C2 iscompared in this experiment.
C2 is manuallytagged with T3, then trained and tested usingTnT tagger.
The results obtained with T3 are91.98%, which are significantly better than theresults already obtained for T2 on C2 (88.13%).C2 is also semi-automatically re-taggedwith T1.
T1 shows better performance (91.31%)than T2 (88.13%).
However, the accuracy of us-ing T3 (on C2) is still slightly higher.
The re-sults are summarized in Table 8.Table 8.
Accuracies of T3 on C1, and accura-cies of T3 and T1 on C2Corpus Tagset AccuracyC2 T3 91.98%C2 T1 91.31%7.3 Experiment 3Due to the change in open class set there may bea difference of performance on unknown words,therefore in this experiment, all the unknownwords of test set are also included in the vocabu-lary.
This experiment again involves T3 and T1with C2.
Combined lexica are built using testingand training parts of the corpus, to eliminate thefactor of unknown words.
This experiment alsoshows that T3 performs better than T1, as shownin Table 9.Table 9.
Accuracies of T3 and T1 with ALLknown words in test dataCorpus Tagset AccuracyC2 T3 94.21%C2 T1 93.47%7.4 Experiment 4Finally both corpora (C1 and C2) were com-bined, forming a training set of 203,882 wordsand a test set of 29,851 words.
The lexica aregenerated only from the training set.
Then TnTtagger is trained separately for both T1 and T3tagsets and the accuracies are compared.
Theresults show that T3 gives better tagging accura-cy, as shown in Table 10.Table 10.
Accuracies of T3 and T1 usingcombined C1 and C2 corporaCorpus Tagset AccuracyC1+C2 T3 90.99%C1+C2 T1 90.00%Partial confusion matrices for both the tag-sets are given in Tables 11 and 12.28The error analysis shows that the accuracydrops for both tagsets when trained on multi-domain corpus, which is expected.
The highesterror count is for the confusion between nounand adjective.
There is also confusion betweenproper and common nouns.
T3 also gives signif-icant confusion between personal pronouns anddemonstratives, as they represent the same lexi-cal entries.Table 11.
Major misclassifications in mergedcorpus with T1 using TnT taggerTagTotaltokens ErrorMaximummisclassificationA 18 5 3 ADJAD 18 7 4 ADJADJ 2510 551 371 NNADV 431 165 59 ADJINT 8 6 6 ADVKD 16 9 6 QKER 77 28 19 PNN 7642 548 218 PNOR 75 24 9 QPD 205 55 12 PPPN 2246 385 264 NNPP 239 51 11 PDQ 324 119 53 ADJQW 24 12 11 VBRD 71 62 61 RPRP 11 5 2 NNU 24 8 8 NNTable 12.
Major misclassifications in mergedcorpus with T3 using TnT taggerTagTotaltokens ErrorMaximummisclassificationCVRP 77 24 15 PPDM 242 77 58 PRPDMRL 71 64 63 PRRLINJ 8 6 6 RBJJ 2510 547 376 NNJJRP 18 4 4 JJNN 7830 589 234 NNPNNP 2339 390 267 NNOD 75 23 8 JJPRP 642 119 33 DM8 Discussion and ConclusionThe current work looks at the existing tagsets ofUrdu being used for tagging corpora and analyz-es them from two perspectives.
First, the tagsetsare analyzed to see their linguistic level differ-ences.
Second, they are compared based on theirinter-tag confusion after training with two differ-ent POS taggers.
These analyses are used to de-rive a more robust tagset.The results show that collapsing categorieswhich are not syntactically motivated improvesthe tagging accuracy in general.
Specifically,light and regular verbs are merged, because theymay come in similar syntactic frames.
Redupli-cated categories are given the same category tag(instead of a special repetition tag).
Units anddates are also not considered separately as thedifferences have been semantically motivatedand they can be categorized with existing tags atsyntactic level.Though, the measuring unit is currentlytreated as a noun, it could be collapsed as an ad-jective as well.
The difference is sometimes lex-ical, where kilogram is more adjectival, vs.minute is more nominal in nature in Urdu,though both are units.NNP (Proper Noun) tag could also havebeen collapsed with NN (Common Noun), asUrdu does not make clear between them at syn-tactic level.
However, these two tags are keptseparate due to their cross-linguistic importance.One may expect that extending the genre ordomain of corpus reduces accuracy of taggingbecause of increase in the variety in the syntacticpatterns and diverse use of lexical items.
Onemay also expect more accuracy with increase insize.
The current results show that effect on ad-ditional domain (when C1 and C2 are mixed) ismore pronounced than the increase in size (fromapproximately 100k to 200k), reducing accuracyfrom 94.21% (T3 with C2) to 90.99% (T3 withC1 + C2).
The increase in accuracy for T3 vs.T1 may be caused by reduced size of T3.
How-ever, the proposed reduction does not compro-mise the syntactic word level information, as thecollapsed categories are where they were eithersemantically motivated or motivated due tophrasal level tags.The work has been motivated to consolidatethe existing Urdu corpora annotated with differ-ent tagsets.
This consolidation will help buildmore robust computational models for Urdu.ReferencesBrants, T. 2000.
TnT ?
A statistical part-of-speechtagger.
Proceedings of the Sixth Applied NaturalLanguage Processing Conference ANLP-2000Seattle, WA, USA.29Butt, M.  1995.
The structure of complex predicatesin Urdu.
CSLI, USA.
ISBN: 1881526585.Haq, A,.
1987.
?\?a ?a?
?Da ????
Amjuman-e-TaraqqiUrdu.Hardie, A.
2003.
Developing a tag-set for automatedpart-of-speech tagging in Urdu.
Archer, D, Rayson,P, Wilson, A, and McEnery, T (eds.)
Proceedingsof the Corpus Linguistics 2003 conference.
UCRELTechnical Papers Volume 16.
Department of Lin-guistics, Lancaster University, UK.Hussain, S. 2008.
Resources for Urdu LanguageProcessing.
The Proceedings of the 6th Workshopon Asian Language Resources, IJCNLP?08, IIITHyderabad, India.Ijaz, M. and Hussain, S. 2007.
Corpus Based UrduLexicon Development.
The Proceedings of Confe-rence on Language Technology (CLT07), Universi-ty of Peshawar, Pakistan.Javed, I.
1981.
?<?
?Ka ???
?a t5?aTaraqqi Urdu Bureau,New Delhi, India.Platts, J.
1909.
A grammar of the Hindustani or Urdulanguage.
Reprinted by Sang-e-Meel Publishers,Lahore, Pakistan.Sajjad, H.  2007.
Statistical Part of Speech Tagger forUrdu.
Unpublished MS Thesis, National Universi-ty of Computer and Emerging Sciences, Lahore,Pakistan.Sajjad, H. and Schmid, H. 2009.
Tagging Urdu Textwith Parts Of Speech: A Tagger Comparison.12thconference of the European chapter of the associa-tion for computational LinguisticsSantorini, B.
1990.
Part_of_Speech Tagging Guide-lines for the Penn Treebank Project (3rd printing,2nd revision).
Accessed fromftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gzon 3rd May, 2009.Schmid, H. 1994.
Probabilistic part-of-speech tag-ging using decision trees.
Institut f?r MaschinelleSprachverarbeitung, Universit?t Stuttgart, Germa-ny.Schmidt, R. 1999.
Urdu: an essential grammar.
Rout-ledge, London, UK.McEnery, A., Baker, J. Gaizauskas, R. and Cunning-ham, H. 2000.
EMILLE: towards a corpus of SouthAsian languages, British Computing Society Ma-chine Translation Specialist Group, London, UK.Appendix A: Mappings of tags between Tag-sets T1 and T2.Tagset T1 Tagset T21.
A JJRP2.
AA AUXA3.
ADJ JJ4.
ADV RB5.
CA CD6.
CC CC7.
DATE DATE8.
EXP SYM9.
FR FR10.
G PRP$11.
GR PRRFP$12.
I ITRP13.
INT INJ14.
KER KER15.
MUL MUL16.
NN NN17.
OR OD18.
P CM19.
PD DM20.
PM PM21.
PN NNP22.
PP PR23.
Q Q24.
QW QW25.
RD DMRL26.
REP PRRL27.
RP PRRF28.
SC SC29.
SE RBRP30.
SM SM31.
TA AUXT32.
U U33.
WALA WALA30Appendix B: New Tagset T3.Tag Meaning Example1.
AUX Auxiliary a?La?gTWO?5hBa?P a May2.
CC Coordinate Conjunction asYO??O??a?????a?5a?????a?
?lHat5O?h@ a Or3.
CD Cardinal ???a?La??????a??
?>?O a One4.
CVRP Conjunctive Verb Par-ticleavY?a??jKat?Y^???J?La??at5??a?7a?5?Q?6a?
?WJat?6aa After5.
DM Demonstrative a???7????aa??a??dK?
?a?58a?6?6?a?5a?5?P a Like this6.
DMRL Demonstrative Relativea?5a?????at5H?C?a???>aa23??aa?5?a??a?8a??Ba?Y?aThat7.
FR Fraction ?5?
?a?YOa?5WmM a Half8.
INJ Interjection ???a!
?5a?6?a?YL a Hurrah9.
ITRP Intensive Particle a??a?m8a??lMa??t5a???a?5?
?at5?6 a Too10.
JJ Adjective ?8a?Wj6a?8?Ba?5a??TM?
a Taller11.
JJRP Adjective Particle apl6at5a?5??a???6t5a?Y?a?5hBa??Ba?La????>?
a As12.
MRP Multiplicative Particle at5M??K?
a Double13.
NN Noun ??Baa?5??I?aa?YO??P?
?J?a?7 a Year14.
NNP Proper Noun a??
?6?a?lLa?5 a Robert15.
OD Ordinal ?l7a?6?`WOaqWO?<?U??
a First16.
PM Phrase Marker ?a ,17.
PP Postposition ?5?Pa?5?Q?6a?8a??apYWL?a?6?
?a To18.
PRP Pronoun Personal??aa???La?W^7a???Pa???a?5a??kdTB?a?La???a???F?5aThey19.
PRP$ Pronoun Personal  Possessive??YOa?5at???a?WYMa?Y?
a My20.
PRRF Pronoun Reflexive a?5??a?5at5SkL??at5?
]6a?L a Oneself21.
PRRF$ Pronoun Reflexive  Possessive?5??a?8?J?at5?kT>?
a Own22.
PRRL Pronoun Relativea?5a?????at5H?C?a???>aa23??aa?5?a??a?8a??Ba?Y?aThat23.
Q Quantitative ?W?a?
?N a Some24.
QW Question Word a?W`Oa?????YLa??a??LasYg?
a Why25.
RB Adverb ?_YkPat5Mat5Y?
a Always26.
SC Subordinate Conjunction at5a?5L?a?WTL?h??YLa????L??a?
[L a Because27.
SM Sentence Marker ?a ?28.
SYM any Symbol $a $29.
VB Verb a??SLa?5WlO?5??
?a?58 a Wanted30.
VBI Verb Infinitive form a?5a?5?5?
?>a?5Na?5 a To go31.
VBT Verb Tense `8a?kHa?6?Ka??5?
a Is32.
WALA Association Marking Morphemea?5mL???5?
aa?5?La???<???
aAssociatedBearing31
