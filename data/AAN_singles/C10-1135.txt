Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200?1208,Beijing, August 2010Joint Tokenization and TranslationXinyan Xiao ?
Yang Liu ?
Young-Sook Hwang ?
Qun Liu ?
Shouxun Lin ?
?Key Lab.
of Intelligent Info.
Processing ?HILab Convergence Technology CenterInstitute of Computing Technology C&I BusinessChinese Academy of Sciences SKTelecom{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.comAbstractAs tokenization is usually ambiguous formany natural languages such as Chineseand Korean, tokenization errors might po-tentially introduce translation mistakes fortranslation systems that rely on 1-best to-kenizations.
While using lattices to of-fer more alternatives to translation sys-tems have elegantly alleviated this prob-lem, we take a further step to tokenizeand translate jointly.
Taking a sequenceof atomic units that can be combined toform words in different ways as input, ourjoint decoder produces a tokenization onthe source side and a translation on thetarget side simultaneously.
By integrat-ing tokenization and translation featuresin a discriminative framework, our jointdecoder outperforms the baseline trans-lation systems using 1-best tokenizationsand lattices significantly on both Chinese-English and Korean-Chinese tasks.
In-terestingly, as a tokenizer, our joint de-coder achieves significant improvementsover monolingual Chinese tokenizers.1 IntroductionTokenization plays an important role in statisticalmachine translation (SMT) because tokenizing asource-language sentence is always the first stepin SMT systems.
Based on the type of input, Miand Huang (2008) distinguish between two cat-egories of SMT systems : string-based systems(Koehn et al, 2003; Chiang, 2007; Galley et al,sourcetargettokenize+translatestring tokenizationtranslationsourcetargetstringtokenizetokenizationtranslatetranslation(a)(b)Figure 1: (a) Separate tokenization and translation and (b)joint tokenization and translation.2006; Shen et al, 2008) that take a string as inputand tree-based systems (Liu et al, 2006; Mi et al,2008) that take a tree as input.
Note that a tree-based system still needs to first tokenize the inputsentence and then obtain a parse tree or forest ofthe sentence.
As shown in Figure 1(a), we refer tothis pipeline as separate tokenization and transla-tion because they are divided into single steps.As tokenization for many languages is usuallyambiguous, SMT systems that separate tokeniza-tion and translation suffer from a major drawback:tokenization errors potentially introduce transla-tion mistakes.
As some languages such as Chi-nese have no spaces in their writing systems, howto segment sentences into appropriate words hasa direct impact on translation performance (Xu etal., 2005; Chang et al, 2008; Zhang et al, 2008).In addition, although agglutinative languages suchas Korean incorporate spaces between ?words?,which consist of multiple morphemes, the gran-ularity is too coarse and makes the training data1200considerably sparse.
Studies reveal that seg-menting ?words?
into morphemes effectively im-proves translating morphologically rich languages(Oflazer, 2008).
More importantly, a tokenizationclose to a gold standard does not necessarily leadsto better translation quality (Chang et al, 2008;Zhang et al, 2008).
Therefore, it is necessaryto offer more tokenizations to SMT systems toalleviate the tokenization error propagation prob-lem.
Recently, many researchers have shown thatreplacing 1-best tokenizations with lattices im-proves translation performance significantly (Xuet al, 2005; Dyer et al, 2008; Dyer, 2009).We take a next step towards the direction ofoffering more tokenizations to SMT systems byproposing joint tokenization and translation.
Asshown in Figure 1(b), our approach tokenizesand translates jointly to find a tokenization anda translation for a source-language string simul-taneously.
We integrate translation and tokeniza-tion models into a discriminative framework (Ochand Ney, 2002), within which tokenization andtranslation models interact with each other.
Ex-periments show that joint tokenization and trans-lation outperforms its separate counterparts (1-best tokenizations and lattices) significantly onthe NIST 2004 and 2005 Chinese-English testsets.
Our joint decoder also reports positive resultson Korean-Chinese translation.
As a tokenizer,our joint decoder achieves significantly better to-kenization accuracy than three monolingual Chi-nese tokenizers.2 Separate Tokenization and TranslationTokenization is to split a string of characters intomeaningful elements, which are often referred toas words.
Typically, machine translation sepa-rates tokenization from decoding as a preprocess-ing step.
An input string is first preprocessed by atokenizer, and then is translated based on the tok-enized result.
Take the SCFG-based model (Chi-ang, 2007) as an example.
Given the charactersequence of Figure 2(a), a tokenizer first splits itinto the word sequence as shown in Figure 2(b),then the decoder translates the word sequence us-ing the rules in Table 1.This approach makes the translation processsimple and efficient.
However, it may not be?
?
?
?
?
?
0?
1 2 3 4 5 6 7Figure 2: Chinese tokenization: (a) character sequence; (b)and (c) tokenization instances; (d) lattice created from (b)and (c).
We insert ?-?
between characters in a word just forclarity.r1 tao-fei-ke ?Taufikr2 duo fen ?
gain a pointr3 x1 you-wang x2 ?
x1 will have the chance to x2Table 1: An SCFG derivation given the tokenization of Fig-ure 2(b).optimal for machine translation.
Firstly, optimalgranularity is unclear for machine translation.
Wemight face severe data sparseness problem by us-ing large granularity, while losing much useful in-formation with small one.
Consider the examplein Figure 2.
It is reasonable to split duo fen intotwo words as duo and fen, since they have one-to-one alignments to the target side.
Nevertheless,while you and wang also have one-to-one align-ments, it is risky to segment them into two words.Because the decoder is prone to translate wang asa verb look without the context you.
Secondly,there may be tokenization errors.
In Figure2(c),tao fei ke is recognized as a Chinese person namewith the second name tao and the first name fei-ke,but the whole string tao fei ke should be a name ofthe Indonesian badminton player.Therefore, it is necessary to offer more tok-enizations to SMT systems to alleviate the tok-enization error propagation problem.
Recently,many researchers have shown that replacing 1-best tokenizations with lattices improves transla-tion performance significantly.
In this approach, alattice compactly encodes many tokenizations andis fixed before decoding.12010 1 2 3 4 5 6 71 23Figure 3: A derivation of the joint model for the tokenizationin Figure 2(b) and the translation in Figure 2 by using therules in Table 1.
N means tokenization while  representstranslation.3 Joint Tokenization and Translation3.1 ModelWe take a next step towards the direction of of-fering more tokenizations to SMT systems byproposing joint tokenization and translation.
Asshown in Figure 1(b), the decoder takes an un-tokenized string as input, and then tokenizes thesource side string while building the correspond-ing translation of the target side.
Since the tradi-tional rules like those in Table 1 natively includetokenization information, we can directly applythem for simultaneous construction of tokeniza-tion and translation by the source side and targetside of rules respectively.
In Figure 3, our jointmodel takes the character sequence in Figure 2(a)as input, and synchronously conducts both trans-lation and tokenization using the rules in Table 1.As our model conducts tokenization during de-coding, we can integrate tokenization models asfeatures together with translation features underthe discriminative framework.
We expect tok-enization and translation could collaborate witheach other.
Tokenization offers translation withgood tokenized results, while translation helps to-kenization to eliminate ambiguity.
Formally, theprobability of a derivation D is represented asP (D) ?
?i?i(D)?i (1)where ?i are features defined on derivations in-cluding translation and tokenization, and ?i arefeature weights.
We totally use 16 features:?
8 traditional translation features (Chiang,2007): 4 rule scores (direct and reverse trans-lation scores; direct and reverse lexical trans-lation scores); language model of the targetside; 3 penalties for word count, extractedrule and glue rule.?
8 tokenization features: maximum entropymodel, language model and word count ofthe source side (Section 3.2).
To handlethe Out Of Vocabulary (OOV) problem (Sec-tion 3.3), we also introduce 5 OOV features:OOV character count and 4 OOV discountfeatures.Since our model is still a string-based model, theCKY algorithm and cube pruning are still applica-ble for our model to find the derivation with maxscore.3.2 Adding Tokenization FeaturesMaximum Entropy model (ME).
We first intro-duce ME model feature for tokenization by cast-ing it as a labeling problem (Xue and Shen, 2003;Ng and Low, 2004).
We label a character with thefollowing 4 types:?
b: the begin of a word?
m: the middle of a word?
e: the end of a word?
s: a single-character wordTaking the tokenization you-wang of the stringyou wang for example, we first create a label se-quence b e for the tokenization you-wang and thencalculate the probability of tokenization byP (you-wang | you wang)= P (b e | you wang)= P (b | you, you wang)?
P (e | wang, you wang)Given a tokenization wL1 with L words for acharacter sequence cn1 , we firstly create labels ln1for every characters and then calculate the proba-bility byP (wL1 |cn1 ) = P (ln1 |cn1 ) =n?i=1P (li|ci, cn1 ) (2)1202Under the ME framework, the probability of as-signing the character c with the label l is repre-sented as:P (l|c, cn1 ) =exp[?i ?ihi(l, c, cn1 )]?l?
exp[?i ?ihi(l?, c, cn1 )](3)where hi is feature function, ?i is the featureweight of hi.
We use the feature templates thesame as Jiang et al, (2008) to extract features forME model.
Since we directly construct tokeniza-tion when decoding, it is straight to calculate theME model score of a tokenization according toformula (2) and (3).Language Model (LM).
We also use the n-gram language model to calculate the probabilityof a tokenization wL1 :P (wL1 ) =L?i=1P (wi|wi?1i?n+1) (4)For instance, we compute the probability of thetokenization shown in Figure 2(b) under a 3-grammodel byP (tao-fei-ke)?P (you-wang | tao-fei-ke)?P (duo | tao-fei-ke, you-wang)?P (fen | you-wang, duo)Word Count (WC).
This feature counts thenumber of words in a tokenization.
Languagemodel is prone to assign higher probabilities toshort sentences in a biased way.
This feature cancompensate this bias by encouraging long sen-tences.
Furthermore, using this feature, we canoptimize the granularity of tokenization for trans-lation.
If larger granularity is preferable for trans-lation, then we can use this feature to punish thetokenization containing more words.3.3 Considering All TokenizationsObviously, we can construct the potential tok-enizations and translations by only using the ex-tracted rules, in line with traditional translationdecoding.
However, it may limits the potential to-kenization space.
Consider a string you wang.
Ifyou-wang is not reachable by the extracted rules,the tokenization you-wang will never be consid-ered under this way.
However, the decoder maystill create a derivation by splitting the string assmall as possible with tokenization you wang andtranslating you with a and wang with look, whichmay hurt the translation performance.
This casehappens frequently for named entity especially.Overall, it is necessary to assure that the de-coder can derive all potential tokenizations (Sec-tion 4.1.3).To assure that, when a span is not tokenized intoa single word by the extracted rules, we will addan operation, which is considering the entire spanas an OOV.
That is, we tokenize the entire spaninto a single word with a translation that is thecopy of source side.
We can define the set of allpotential tokenizations ?
(cn1 ) for the character se-quence cn1 in a recursive way by?
(cn1 ) =n?1?i{?(ci1)?
{w(cni+1)}} (5)here w(cni+1) means a word contains characterscni+1 and?means the times of two sets.
Ac-cording to this recursive definition, it is easy toprove that all tokenizations is reachable by usingthe glue rule (S ?
SX,SX) and the added op-eration.
Here, glue rule is used to concatenate thetranslation and tokenization of the two variables Sand X, which acts the role of the operator ?
inequation (5).Consequently, this introduces a large numberof OOVs.
In order to control the generation ofOOVs, we introduce the following OOV features:OOV Character Count (OCC).
This featurecounts the number of characters covered by OOV.We can control the number of OOV characters bythis feature.
It counts 3 when tao-fei-ke is an OOV,since tao-fei-ke has 3 characters.OOV Discount (OD).
The chances to be OOVsvary for words with different counts of characters.We can directly attack this problem by addingfeatures ODi that reward or punish OOV wordswhich contains with i characters, or ODi,j forOOVs contains with i to j characters.
4 OD fea-tures are used in this paper: 1, 2, 3 and 4+.
Forexample, OD3 counts 1 when the word tao-fei-keis an OOV.1203Method Train #Rule Test TFs MT04 MT05 SpeedSeparateICT 151M ICT ?
34.82 33.06 2.48SF 148M SF ?
35.29 33.22 2.55ME 141M ME ?
33.71 30.91 2.34All 219M Lattice ?
35.79 33.95 3.83?
35.85 33.76 6.79JointICT 151MCharacter?36.92 34.69 17.66SF 148M 37.02 34.56 17.37ME 141M 36.78 34.17 17.23All 219M 37.25** 34.88** 17.52Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence).
Columns Trainand Test represents the tokenization methods for training and testing respectively.
Column TFs stands for whether the 8tokenization features is used (?)
or not (?).
ICT, SF and ME are segmenter names for preprocessing.
All means combinedcorpus processed by the three segmenters.
Lattice represent the system implemented as Dyer et al, (2008).
** meanssignificantly (Koehn, 2004) better than Lattice (p < 0.01).4 ExperimentsIn this section, we try to answer the followingquestions:1.
Does the joint method outperform conven-tional methods that separate tokenizationfrom decoding.
(Section 4.1)2.
How about the tokenization performance ofthe joint decoder?
(Section 4.2)4.1 Translation EvaluationWe use the SCFG model (Chiang, 2007) for ourexperiments.
We firstly work on the Chinese-English translation task.
The bilingual trainingdata contains 1.5M sentence pairs coming fromLDC data.1 The monolingual data for trainingEnglish language model includes Xinhua portionof the GIGAWORD corpus, which contains 238MEnglish words.
We use the NIST evaluation setsof 2002 (MT02) as our development data set, andsets of 2004(MT04) and 2005(MT05) as test sets.We use the corpus derived from the People?s Daily(Renmin Ribao) in Feb. to Jun.
1998 containing6M words for training LM and ME tokenizationmodels.Translation Part.
We used GIZA++ (Och andNey, 2003) to perform word alignment in both di-rections, and grow-diag-final-and (Koehn et al,2003) to generate symmetric word alignment.
Weextracted the SCFG rules as describing in Chiang(2007).
The language model were trained by the1including LDC2002E18, LDC2003E07, LDC2003E14,Hansards portion of LDC2004T07, LDC2004T08 andLDC2005T06SRILM toolkit (Stolcke, 2002).2 Case insensitiveNIST BLEU (Papineni et al, 2002) was used tomeasure translation performance.Tokenization Part.
We used the toolkit imple-mented by Zhang (2004) to train the ME model.Three Chinese word segmenters were used forcomparing: ICTCLAS (ICT) developed by insti-tute of Computing Technology Chinese Academyof Sciences (Zhang et al, 2003); SF developed atStanford University (Huihsin et al, 2005) and MEwhich exploits the ME model described in section(3.2).4.1.1 Joint Vs. SeparateWe compared our joint tokenization and trans-lation with the conventional separate methods.The input of separate tokenization and translationcan either be a single segmentation or a lattice.The lattice combines the 1-best segmentations ofsegmenters.
Same as Dyer et al, (2008), we alsoextracted rules from a combined bilingual corpuswhich contains three copies from different seg-menters.
We refer to this version of rules as All.Table 2 shows the result.3 Using all rule ta-ble, our joint method significantly outperforms thebest single system SF by +1.96 and +1.66 pointson MT04 and MT05 respectively, and also out-performs the lattice-based system by +1.46 and+0.93 points.
However, the 8 tokenization fea-tures have small impact on the lattice system,probably because the tokenization space limited2The calculation of LM probabilities for OOVs is doneby the SRILM without special treatment by ourself.3The weights are retrained for different test conditions, sodo the experiments in other sections.1204ME LM WC OCC OD MT05?
?
?
?
?
24.97?
?
?
?
?
25.30?
?
?
?
?
24.70?
?
?
?
?
24.84?
?
?
?
?
25.51?
?
?
?
?
25.34?
?
?
?
?
25.74?
?
?
?
?26.37Table 3: Effect of tokenization features on Chinese-Englishtranslation task.
???
denotes using a tokenization featurewhile ???
denotes that it is inactive.by lattice has been created from good tokeniza-tion.
Not surprisingly, our decoding method isabout 2.6 times slower than lattice method withtokenization features, since the joint decoder takescharacter sequences as input, which is about 1.7times longer than the corresponding word se-quences tokenized by segmenters.
(Section 4.1.4).The number of extracted rules with differentsegment methods are quite close, while the Allversion contains about 45% more rules than thesingle systems.
With the same rule table, our jointmethod improves the performance over separatemethod up to +3.03 and +3.26 points (ME).
In-terestingly, comparing with the separate method,the tokenization of training data has smaller effecton joint method.
The BLEU scores of MT04 andMT05 fluctuate about 0.5 and 0.7 points when ap-plying the joint method, while the difference ofseparate method is up to 2 and 3 points respec-tively.
It shows that the joint method is more ro-bust to segmentation performance.4.1.2 Effect of Tokenization ModelWe also investigated the effect of tokenizationfeatures on translation.
In order to reduce the timefor tuning weights and decoding, we extractedrules from the FBIS part of the bilingual corpus,and trained a 4-gram English language model onthe English side of FBIS.Table 3 shows the result.
Only using the 8 trans-lation features, our system achieves a BLEU scoreof 24.97.
By activating all tokenization features,the joint decoder obtains an absolute improve-ment by 1.4 BLEU points.
When only addingone single tokenization feature, the LM and WCfail to show improvement, which may result fromtheir bias to short or long tokenizations.
How-Method BLEU #Word Grau #OOVICT 33.06 30,602 1.65 644SF 33.22 30,119 1.68 882ME 30.91 29,717 1.70 1,614Lattice 33.95 30,315 1.66 494JointICT 34.69 29,723 1.70 996JointSF 34.56 29,839 1.69 972JointME 34.17 29,771 1.70 1,062JointAll 34.88 29,644 1.70 883Table 4: Granularity (Grau, counts of character per word)and counts of OOV words of different methods on MT05.The subscript of joint means the type of rule table.ever, these two features have complementary ad-vantages and collaborate well when using them to-gether (line 8).
The OCC and OD features alsocontribute improvements which reflects the factthat handling the generation of OOV is importantfor the joint model.4.1.3 Considering All Tokenizations?In order to explain the necessary of consideringall potential tokenizations, we compare the perfor-mances of whether to tokenize a span as a singleword or not as illustrated in section 3.3.
Whenonly tokenizing by the extracted rules, we obtain34.37 BLEU on MT05, which is about 0.5 pointslower than considering all tokenizations shown inTable 2.
This indicates that spuriously limitationof the tokenization space may degenerate transla-tion performance.4.1.4 Results AnalysisTo better understand why the joint method canimprove the translation quality, this section showssome details of the results on the MT05 data set.Table 4 shows the granularity and OOV wordcounts of different configurations.
The latticemethod reduces the OOV words quite a lot whichis 23% and 70% comparing with ICT and ME.
Incontrast, the joint method gain an absolute im-provement even thought the OOV count do notdecrease.
It seems the lattice method prefers totranslate more characters (since smaller granular-ity and less OOVs), while our method is inclinedto maintain integrity of words (since larger granu-larity and more OOVs).
This also explains the dif-ficulty of deciding optimal tokenization for trans-lation before decoding.There are some named entities or idioms that1205Method Type F1 TimeMonolingualICT 97.47 0.010SF 97.48 0.007ME 95.53 0.008JointICT 97.68 9.382SF 97.68 10.454ME 97.60 10.451All 97.70 9.248Table 5: Comparison of segmentation performance in termsof F1 score and speed (second per sentence).
Type columnmeans the segmenter for monolingual method, while repre-sents the rule tables used by joint method.are split into smaller granularity by the seg-menters.
For example:????
which is an Englishname ?Stone?
or ??-g-u?
which means?teenage?.
Although the separate method is possi-ble to translate them using smaller granularity, thetranslation results are in fact wrong.
In contrast,the joint method tokenizes them as entire OOVwords, however, it may result a better translationfor the whole sentence.We also count the overlap of the segmentsused by the JointAll system towards the singlesegmentation systems.
The tokenization resultof JointAll contains 29, 644 words, and shares28, 159 , 27, 772 and 27, 407 words with ICT ,SF and ME respectively.
And 46 unique wordsappear only in the joint method, where most ofthem are named entity.4.2 Chinese Word Segmentation EvaluationWe also test the tokenization performance of ourmodel on Chinese word segmentation task.
Werandomly selected 3k sentences from the corpusof People?s Daily in Jan. 1998.
1k sentenceswere used for tuning weights, while the other 2ksentences were for testing.
We use MERT (Och,2003) to tune the weights by minimizing the errormeasured by F1 score.As shown in Table 5, with all features activated,our joint decoder achieves an F1 score of 97.70which reduces the tokenization error comparingwith the best single segmenter ICT by 8.7%.
Sim-ilar to the translation performance evaluation, ourjoint decoder outperforms the best segmenter withany version of rule tables.Feature F1TFs 97.37TFs + RS 97.65TFs + LM 97.67TFs + RS + LM 97.62All 97.70Table 6: Effect of the target side information on Chineseword segmentation.
TFs stands for the 8 tokenization fea-tures.
All represents all the 16 features.4.2.1 Effect of Target Side InformationWe compared the effect of the 4 Rule Scores(RS), target side Language Model (LM) on tok-enization.
Table 6 shows the effect on Chineseword segmentation.
When only use tokenizationfeatures, our joint decoder achieves an F1 scoreof 97.37.
Only integrating language model or rulescores, the joint decoder achieves an absolute im-provement of 0.3 point in F1 score, which reducesthe error rate by 11.4%.
However, when combin-ing them together, the F1 score deduces slightly,which may result from the weight tuning.
Us-ing all feature, the performance comes to 97.70.Overall, our experiment shows that the target sideinformation can improve the source side tokeniza-tion under a supervised way, and outperform state-of-the-art systems.4.2.2 Best Tokenization = Best Translation?Previous works (Zhang et al, 2008; Chang etal., 2008) have shown that preprocessing the in-put string for decoder by better segmenters donot always improve the translation quality, we re-verify this by testing whether the joint decoderproduces good tokenization and good translationat the same time.
To answer the question, weused the feature weights optimized by maximiz-ing BLEU for tokenization and used the weightsoptimized by maximizing F1 for translation.
Wetest BLEU on MT05 and F1 score on the test dataused in segmentation evaluation experiments.
Bytuning weights regarding to BLEU (the configura-tion for JointAll in table 2), our decoder achievesa BLEU score of 34.88 and an F1 score of 92.49.Similarly, maximizing F1 (the configuration forthe last line in table 6) leads to a much lowerBLEU of 27.43, although the F1 is up to 97.70.This suggests that better tokenization may not al-ways lead to better translations and vice versa1206Rule #Rule Method Test TimeMorph 46M Separate 21.61 4.12Refined 55M 21.21 4.63All 74M Joint 21.93* 5.10Table 7: Comparison of Separate and Joint method in termsof BLEU score and decoding speed (second per sentence) onKorean-Chinese translation task.even by the joint decoding.
This also indicates thehard of artificially defining the best tokenizationfor translation.4.3 Korean-Chinese TranslationWe also test our model on a quite different task:Korean-Chinese.
Korean is an agglutinative lan-guage, which comes from different language fam-ily comparing with Chinese.We used a newswire corpus containing 256ksentence pairs as training data.
The developmentand test data set contain 1K sentence each withone single reference.
We used the target side oftraining set for language model training.
The Ko-rean part of these data were tokenized into mor-pheme sequence as atomic unit for our experi-ments.We compared three methods.
First is directlyuse morpheme sequence (Morph).
The secondone is refined data (Refined), where we use selec-tive morphological segmentation (Oflazer, 2008)for combining morpheme together on the trainingdata.
Since the selective method needs alignmentinformation which is unavailable in the decod-ing, the test data is still of morpheme sequence.These two methods still used traditional decodingmethod.
The third one extracting rules from com-bined (All) data of methods 1 and 2, and usingjoint decoder to exploit the different granularityof rules.Table 7 shows the result.
Since there is no goldstandard data for tokenization, we do not use MEand LM tokenization features here.
However, ourjoint method can still significantly (p < 0.05) im-prove the performance by about +0.3 points.
Thisalso reflects the importance of optimizing granu-larity for morphological complex languages.5 Related WorkMethods have been proposed to optimize tok-enization for word alignment.
For example, wordalignment can be simplified by packing (Ma et al,2007) several consecutive words together.
Wordalignment and tokenization can also be optimizedby maximizing the likelihood of bilingual corpus(Chung and Gildea, 2009; Xu et al, 2008).
In fact,these work are orthogonal to our joint method,since they focus on training step while we are con-cerned of decoding.
We believe we can furtherthe performance by combining these two kinds ofwork.Our work also has connections to multilingualtokenization (Snyder and Barzilay, 2008).
Whilethey have verified that tokenization can be im-proved by multilingual learning, our work showsthat we can also improve tokenization by collabo-rating with translation task in a supervised way.More recently, Liu and Liu (2010) also showsthe effect of joint method.
They integrate parsingand translation into a single step and improve theperformance of translation significantly.6 ConclusionWe have presented a novel method for joint tok-enization and translation which directly combinesthe tokenization model into the decoding phase.Allowing tokenization and translation to collab-orate with each other, tokenization can be opti-mized for translation, while translation also makescontribution to tokenization performance under asupervised way.
We believe that our approach canbe applied to other string-based model such asphrase-based model (Koehn et al, 2003), string-to-tree model (Galley et al, 2006) and string-to-dependency model (Shen et al, 2008).AcknowledgementThe authors were supported by SK Telecom C&IBusiness, and National Natural Science Founda-tion of China, Contracts 60736014 and 60903138.We thank the anonymous reviewers for their in-sightful comments.
We are also grateful to Wen-bin Jiang, Zhiyang Wang and Zongcheng Ji fortheir helpful feedback.1207ReferencesChang, Pi-Chuan, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In theThird Workshop on SMT.Chiang, David.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):201?228.Chung, Tagyoung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Proc.EMNLP 2009.Dyer, Christopher, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proc.
ACL 2008.Dyer, Chris.
2009.
Using a maximum entropy modelto build segmentation lattices for mt.
In Proc.NAACL 2009.Galley, Michel, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.ACL 2006.Huihsin, Tseng, Pichuan Chang, Galen Andrew,Daniel Jurafsky, and Christopher Manning.
2005.A conditional random field word segmenter.
InFourth SIGHAN Workshop.Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.2008.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProc.
ACL 2008.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL 2003.Koehn, Philipp.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP2004.Liu, Yang and Qun Liu.
2010.
Joint parsing and trans-lation.
In Proc.
Coling 2010.Liu, Yang, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
ACL 2006.Ma, Yanjun, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.
InProc.
ACL 2007.Mi, Haitao, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL 2008.Ng, Hwee Tou and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proc.
EMNLP2004.Och, Franz J. and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proc.
ACL 2002.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Och, Franz Josef.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proc.
ACL2003.Oflazer, Kemal.
2008.
Statistical machine translationinto a morphologically complex language.
In Proc.CICL 2008.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
In Proc.ACL 2002.Shen, Libin, Xu Jinxi, and Weischedel Ralph.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProc.
ACL 2008.Snyder, Benjamin and Regina Barzilay.
2008.
Un-supervised multilingual learning for morphologicalsegmentation.
In Proc.
ACL 2008.Stolcke, Andreas.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.Xu, Jia, Evgeny Matusov, Richard Zens, and Her-mann Ney.
2005.
Integrated chinese word segmen-tation in statistical machine translation.
In Proc.IWSLT2005.Xu, Jia, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervisedchinese word segmentation for statistical machinetranslation.
In Proc.
Coling 2008.Xue, Nianwen and Libin Shen.
2003.
Chinese wordsegmentation as LMR tagging.
In SIGHAN Work-shop.Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, andQun Liu.
2003.
Hhmm-based chinese lexical an-alyzer ictclas.
In the Second SIGHAN Workshop.Zhang, Ruiqiang, Keiji Yasuda, and Eiichiro Sumita.2008.
Improved statistical machine translation bymultiple Chinese word segmentation.
In the ThirdWorkshop on SMT.Zhang, Le.
2004.
Maximum entropy modeling toolkitfor python and c++.1208
