Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1542?1551,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCombining Orthogonal Monolingual and Multilingual Sources ofEvidence for All Words WSDWeiwei GuoComputer Science DepartmentColumbia UniversityNew York, NY, 10115weiwei@cs.columbia.eduMona DiabCenter for Computational Learning SystemsColumbia UniversityNew York, NY, 10115mdiab@ccls.columbia.eduAbstractWord Sense Disambiguation remains oneof the most complex problems facing com-putational linguists to date.
In this pa-per we present a system that combinesevidence from a monolingual WSD sys-tem together with that from a multilingualWSD system to yield state of the art per-formance on standard All-Words data sets.The monolingual system is based on amodification of the graph based state of theart algorithm In-Degree.
The multilingualsystem is an improvement over an All-Words unsupervised approach, SALAAM.SALAAM exploits multilingual evidenceas a means of disambiguation.
In thispaper, we present modifications to bothof the original approaches and then theircombination.
We finally report the highestresults obtained to date on the SENSEVAL2 standard data set using an unsupervisedmethod, we achieve an overall F measureof 64.58 using a voting scheme.1 IntroductionDespite advances in natural language processing(NLP), Word Sense Disambiguation (WSD) is stillconsidered one of the most challenging problemsin the field.
Ever since the field?s inception, WSDhas been perceived as one of the central problemsin NLP.
WSD is viewed as an enabling technologythat could potentially have far reaching impact onNLP applications in general.
We are starting to seethe beginnings of a positive effect of WSD in NLPapplications such as Machine Translation (Carpuatand Wu, 2007; Chan et al, 2007).Advances in WSD research in the current mil-lennium can be attributed to several key factors:the availability of large scale computational lexi-cal resources such as WordNets (Fellbaum, 1998;Miller, 1990), the availability of large scale cor-pora, the existence and dissemination of standard-ized data sets over the past 10 years through differ-ent testbeds such as SENSEVAL and SEMEVALcompetitions,1 devising more robust computingalgorithms to handle large scale data sets, and sim-ply advancement in hardware machinery.In this paper, we address the problem of WSDof all content words in a sentence, All-Words data.In this framework, the task is to associate all to-kens with their contextually relevant meaning defi-nitions from some computational lexical resource.Our work hinges upon combining two high qual-ity WSD systems that rely on essentially differ-ent sources of evidence.
The two WSD systemsare a monolingual system RelCont and a multi-lingual system TransCont.
RelCont is an en-hancement on an existing graph based algorithm,In-Degree, first described in (Navigli and Lapata,2007).
TransCont is an enhancement over anexisting approach that leverages multilingual evi-dence through projection, SALAAM, described indetail in (Diab and Resnik, 2002).
Similar to theleveraged systems, the current combined approachis unsupervised, namely it does not rely on trainingdata from the onset.
We show that by combiningboth sources of evidence, our approach yields thehighest performance for an unsupervised systemto date on standard All-Words data sets.This paper is organized as follows: Section 2delves into the problem of WSD in more detail;Section 3 explores some of the relevant relatedwork; in Section 4, we describe the two WSDsystems in some detail emphasizing the improve-ments to the basic systems in addition to a de-scription of our combination approach; we presentour experimental set up and results in Section 5;we discuss the results and our overall observationswith error analysis in Section 6; Finally, we con-1http://www.semeval.org1542clude in Section 7.2 Word Sense DisambiguationThe definition of WSD has taken on several differ-ent practical meanings in recent years.
In the latestSEMEVAL 2010 workshop, there are 18 tasks de-fined, several of which are on different languages,however we recognize the widening of the defi-nition of the task of WSD.
In addition to the tra-ditional All-Words and Lexical Sample tasks, wenote new tasks on word sense discrimination (nosense inventory needed, the different senses aremerely distinguished), lexical substitution usingsynonyms of words as substitutes both monolin-gually and multilingually, as well as meaning def-initions obtained from different languages namelyusing words in translation.Our paper is about the classical All-Words(AW) task of WSD.
In this task, all content bear-ing words in running text are disambiguated froma static lexical resource.
For example a sen-tence such as ?I walked by the bank and sawmany beautiful plants there.?
will have the verbs?walked, saw?, the nouns ?bank, plants?, the ad-jectives ?many, beautiful?, and the adverb ?there?,be disambiguated from a standard lexical resource.Hence, using WordNet,2 ?walked?
will be assignedthe corresponding meaning definitions of: to useone?s feet to advance; to advance by steps, ?saw?will be assigned the meaning definition of: to per-ceive by sight or have the power to perceive bysight, the noun ?bank?
will be assigned the mean-ing definition of: sloping land especially the slopebeside a body of water, and so on.3 Related WorksMany systems over the years have been proposedfor the task.
A thorough review of the state ofthe art through the late 1990s (Ide and Veronis,1998) and more recently in (Navigli, 2009).
Sev-eral techniques have been used to tackle the prob-lem ranging from rule based/knowledge basedapproaches to unsupervised and supervised ma-chine learning techniques.
To date, the best ap-proaches that solve the AW WSD task are super-vised as illustrated in the different SenseEval andSEMEVAL AW task (Palmer et al, 2001; Snyderand Palmer, 2004; Pradhan et al, 2007).In this paper, we present an unsupervised com-bination approach to the AW WSD problem that2http://wordnet.princeton.edurelies on WN similarity measures in conjunctionwith evidence obtained through exploiting multi-lingual evidence.
We will review the closely rele-vant related work on which this current investiga-tion is based.34 Our ApproachOur current investigation exploits two basic unsu-pervised approaches that perform at state-of-the-art for the AW WSD task in an unsupervised set-ting.
Crucially the two systems rely on differ-ent sources of evidence allowing them to comple-ment each other to a large extent leading to betterperformance than for each system independently.Given a target content word and co-occurring con-textual clues, the monolingual system RelContattempts to assign the approporiate meaning def-inition to the target word.
Such words by defini-tion are semantically related words.
TransCont,on the other hand, is the multilingual system.TransCont defines the notion of context in thetranslational space using a foreign word as a fil-ter for defining the contextual content words fora given target word.
In this multilingual setting,all the words that are mapped to (aligned with)the same orthographic form in a foreign languageconstitute the context.
In the next subsectionswe describe the two approaches RelCont andTransCont in some detail, then we proceed todescribe two combination methods for the two ap-proaches: MERGE and VOTE.4.1 Monolingual System RelContRelCont is based on an extension of a state-of-the-art WSD approach by (Sinha and Mihal-cea, 2007), henceforth (SM07).
In the basicSM07 work, the authors combine different seman-tic similarity measures with different graph basedalgorithms as an extension to work in (Mihal-cea, 2005).
Given a sequence of words W ={w1, w2...wn}, each word wi with several senses{si1, si2...sim}.
A graph G = (V,E) is defined suchthat there exists a vertex v for each sense.
Twosenses of two different words may be connected byan edge e, depending on their distance.
That twosenses are connected suggests they should haveinfluence on each other, accordingly a maximum3We acknowledge the existence of many research papersthat tackled the AW WSD problem using unsupervised ap-proaches, yet for lack of space we will not be able to reviewmost of them.1543allowable distance is set.
They explore 4 differ-ent graph based algorithms.
The highest yield-ing algorithm in their work is the In-Degree al-gorithm combining different WN similarity mea-sures depending on POS.
They used the Jiangand Conrath (JCN) (Jiang and Conrath., 1997)similarity measure within nouns, the Leacock &Chodorow (LCH) (Leacock and Chodorow, 1998)similarity measure within verbs, and the Lesk(Lesk, 1986) similarity measure within adjectives,within adverbs, and among different POS tag pair-ings.
They evaluate their work against the SEN-SEVAL 2 AW test data (SV2AW).
They tune theparameters of their algorithm ?
namely, the nor-malization ratio for some of these measures ?
onthe SENSEVAL 3 data set.
They report a state-of-the-art unsupervised system that yields an overallperformance across all AW POS sets of 57.2%.In our current work, we extend the SM07 workin some interesting ways.
A detailed narrativeof our approach is described in (Guo and Diab,2009).
Briefly, we focus on the In-Degreegraph based algorithm since it is the best per-former in the SM07 work.
The In-Degree al-gorithm presents the problem as a weighted graphwith senses as nodes and the similarity betweensenses as weights on edges.
The In-Degreeof a vertex refers to the number of edges inci-dent on that vertex.
In the weighted graph, theIn-Degree for each vertex is calculated by sum-ming the weights on the edges that are incident onit.
After all the In-Degree values for each senseare computed, the sense with maximum value ischosen as the final sense for that word.In this paper, we use the In-Degree algo-rithm while applying some modifications to thebasic similarity measures exploited and the WNlexical resource tapped into.
Similar to the orig-inal In-Degree algorithm, we produce a prob-abilistic ranked list of senses.
Our modificationsare described as follows:JCN for Verb-Verb Similarity In our imple-mentation of the In-Degree algorithm, we usethe JCN similarity measure for both Noun-Nounsimilarity calculation similar to SM07.
However,different from SM07, instead of using LCH forVerb-Verb similarity, we use the JCN metric as ityields better performance in our experimentations.Expand Lesk Following the intuition in (Ped-ersen et al, 2005), henceforth (PEA05), we ex-pand the basic Lesk similarity measure to take intoaccount the glosses for all the relations for thesynsets on the contextual words and compare themwith the glosses of the target word senses, there-fore going beyond the is-a relation.
We exploit theobservation that WN senses are too fine-grained,accordingly the neighbors would be slightly variedwhile sharing significant semantic meaning con-tent.
To find similar senses, we use the relations:hypernym, hyponym, similar attributes, similarverb group, pertinym, holonym, and meronyms.4The algorithm assumes that the words in the inputare POS tagged.
In PEA05, the authors retrieve allthe relevant neighbors to form a bag of words forboth the target sense and the surrounding senses ofthe context words, they specifically focus on theLesk similarity measure.
In our current work, weemploy the neighbors in a disambiguation strategyusing different similarity measures one pair at atime.
Our algorithm takes as input a target senseand a sense pertaining to a word in the surroundingcontext, and returns a sense similarity score.
Wedo not apply the WN relations expansion to thetarget sense.
It is only applied to the contextualword.5For the monolingual system, we employ thesame normalization values used in SM07 for thedifferent similarity measures.
Namely for the Leskand Expand-Lesk, we use the same cut-off value of240, accordingly, if the Lesk or Expand-Lesk sim-ilarity value returns 0 <= 240 it is converted toa real number in the interval [0,1], any similarityover 240 is by default mapped to 1.
We will referto the Expand-Lesk with this threshold as Lesk2.We also experimented with different thresholds forthe Lesk and Expand-Lesk similarity measure us-ing the SENSEVAL 3 data as a tuning set.
Wefound that a cut-off threshold of 40 was also use-ful.
We will refer to this variant of Expand-Leskwith a cut off threshold of 40 as Lesk3.
For JCN,similar to SM07, the values are from 0.04 to 0.2,we mapped them to the interval [0,1].
We did notrun any calibration studies beyond the what wasreported in SM07.4In our experiments, we varied the number of relations toemploy and they all yielded relatively similar results.
Hencein this paper, we report results using all the relations listedabove.5We experimented with expanding both the contextualsense and the target sense and we found that the unreliabil-ity of some of the relations is detrimental to the algorithm?sperformance.
Hence we decided empirically to expand onlythe contextual word.1544SemCor Expansion of WN A part of theRelCont approach relies on using the Lesk al-gorithm.
Accordingly, the availability of glossesassociated with the WN entries is extremely bene-ficial.
Therefore, we expand the number of glossesavailable in WN by using the SemCor data set,thereby adding more examples to compare.
TheSemCor corpus is a corpus that is manually sensetagged (Miller, 1990).6 In this expansion, depend-ing on the version of WN, we use the sense-indexfile in the WN Database to convert the SemCordata to the appropriate version sense annotations.We augment the sense entries for the different POSWN databases with example usages from SemCor.The augmentation is done as a look up table exter-nal to WN proper since we did not want to dabblewith the WN offsets.
We set a cap of 30 additionalexamples per synset.
We used the first 30 exam-ples with no filtering criteria.
Many of the synsetshad no additional examples.
WN1.7.1 comprises atotal of 26875 synsets, of which 25940 synsets areaugmented with SemCor examples.74.2 Multilingual System TransContTransCont is based on the WSD systemSALAAM (Diab and Resnik, 2002), henceforth(DR02).
The SALAAM system leverages wordalignments from parallel corpora to perform WSD.The SALAAM algorithm exploits the word corre-spondence cross linguistically to tag word senseson words in running text.
It relies on several un-derlying assumptions.
The first assumption is thatsenses of polysemous words in one language couldbe lexicalized differently in other languages.
Forexample, ?bank?
in English would be translated asbanque or rive de fleuve in French, depending oncontext.
The other assumption is that if Language1 (L1) words are translated to the same ortho-graphic form in Language 2 (L2), then they sharethe some element of meaning, they are semanti-cally similar.8The SALAAM algorithm can be described asfollows.
Given a parallel corpus of L1-L2 that6Using SemCor in this setting to augment WN does hintof using supervised data in the WSD process, however, sinceour approach does not rely on training data and SemCor is notused in our algorithm directly to tag data, but to augment arich knowledge resource, we contend that this does not affectour system?s designation as an unsupervised system.7Some example sentences are repeated across differentsynsets and POS since the SemCor data is annotated as anAll-Words tagged data set.8We implicitly make the underlying simplifying assump-tion that the L2 words are less ambiguous than the L1 words.is sentence and word aligned, group all the wordtypes in L1 that map to same word in L2 creat-ing clusters referred to as typesets.
Then performdisambiguation on the typeset clusters using WN.Once senses are identified for each word in thecluster, the senses are propagated back to the origi-nal word instances in the corpus.
In the SALAAMalgorithm, the disambiguation step is carried outas follows: within each of these target sets con-sider all possible sense tags for each word andchoose sense tags informed by semantic similaritywith all the other words in the whole group.
Thealgorithm is a greedy algorithm that aims at maxi-mizing the similarity of the chosen sense across allthe words in the set.
The SALAAM disambigua-tion algorithm used the noun groupings (Noun-Groupings) algorithm described in DR02.
The al-gorithm applies disambiguation within POS tag.The authors report only results on the nouns onlysince NounGroupings heavily exploits the hierar-chy structure of the WN noun taxonomy, whichdoes not exist for adjectives and adverbs, and isvery shallow for verbs.Essentially SALAAM relies on variability intranslation as it is important to have multiplewords in a typeset to allow for disambiguation.In the original SALAAM system, the authors au-tomatically translated several balanced corpora inorder to render more variable data for the approachto show it?s impact.
The corpora that were trans-lated are: the WSJ, the Brown corpus and all theSENSEVAL data.
The data were translated to dif-ferent languages (Arabic, French and Spanish) us-ing state of art MT systems.
They employed theautomatic alignment system GIZA++ (Och andNey, 2003) to obtain word alignments in a singledirection from L1 to L2.For TransCont we use the basic SALAAMapproach with some crucial modifications thatlead to better performance.
We still rely on par-allel corpora, we extract typesets based on the in-tersection of word alignments in both alignmentdirections using more advanced GIZA++ machin-ery.
In contrast to DR02, we experiment withall four POS: Verbs (V), Nouns (N), Adjectives(A) and Adverbs (R).
Moreover, we modified theunderlying disambiguation method on the type-sets.
We still employ WN similarity, however, wedo not use the NounGroupings algorithm.
Ourdisambiguation method relies on calculating thesense pair similarity exhaustively across all the1545word types in a typeset and choosing the combi-nation that yields the highest similarity.
We exper-imented with all the WN similarity measures inthe WN similarity package.9 We also experimentwith Lesk2 and Lesk3 as well as other measures,however we do not use SemCor examples withTransCont.
We found that the best results areyielded using the Lesk2/Lesk3 similarity measurefor N, A and R POS tagsets, while the Lin and JCNmeasures yield the best performance for the verbs.In contrast to the DR02 approach, we modify theinternal WSD process to use the In-Degree al-gorithm on the typeset, so each sense obtains aconfidence, and the sense(s) with the highest con-fidences are returned.4.3 Combining RelCont and TransContOur objective is to combine the different sourcesof evidence for the purposes of producing an effec-tive overall global WSD system that is able to dis-ambiguate all content words in running text.
Wecombine the two systems in two different ways.4.3.1 MERGEIn this combination scheme, the words in the type-set that result from the TransCont approach areadded to the context of the target word in theRelCont approach.
However the typeset wordsare not treated the same as the words that comefrom the surrounding context in the In-Degreealgorithm as we recognize that words that areyielded in the typesets are semantically similar interms of content rather than being co-occurringwords as is the case for contextual words in Rel-Cont.
Heeding this difference, we proceed tocalculate similarity for words in the typesets us-ing different similarity measures.
In the case ofnoun-noun similarity, in the original RelContexperiments we use JCN, however with the wordspresent in the TransCont typesets we use oneof the Lesk variants, Lesk2 or Lesk3.
Our obser-vation is that the JCN measure is relatively coarsergrained, compared to Lesk measures, therefore itis sufficient in case of lexical relatedness thereforeworks well in case of the context words.
Yet forthe words yielded in the TransCont typesets amethod that exploits the underlying rich relationsin the noun hierarchy captures the semantic sim-ilarity more aptly.
In the case of verbs we stillmaintain the JCN similarity as it most effective9http://wn-similarity.sourceforge.net/given the shallowness of the verb hierarchy andthe inherent nature of the verbal synsets which aredifferentiated along syntactic rather than semanticdimensions.
We employ the Lesk algorithm stillwith A-A and R-R similarity and when comparingacross different POS tag pairings.4.3.2 VOTEIn this combination scheme, the output of theglobal disambiguation system is simply an inter-section of the two outputs from the two underly-ing systems RelCont and TransCont.
Specif-ically, we sum up the confidence ranging from0 to 1 of the two system In-Degree algo-rithm outputs to obtain a final confidence for eachsense, choosing the sense(s) that yields the high-est confidences.
The fact that TransCont usesIn-Degree internally allows for a seamless in-tegration.5 Experiments and Results5.1 DataThe parallel data we experiment with are thesame standard data sets as in (Diab and Resnik,2002), namely, Senseval 2 English AW data sets(SV2AW) (Palmer et al, 2001), and Seneval 3 En-glish AW (SV3AW) data set.
We use the true POStag sets in the test data as rendered in the PennTree Bank.10 We present our results on WordNet1.7.1 for ease of comparison with previous results.5.2 Evaluation MetricsWe use the scorer2 software to report fine-grained (P)recision and (R)ecall and (F)-measure.5.3 BaselinesWe consider here several baselines.
1.
A randombaseline (RAND) is the most appropriate base-line for an unsupervised approach.2.
We includethe most frequent sense baseline (MFBL), thoughwe note that we consider the most frequent senseor first sense baseline to be a supervised baselinesince it depends crucially on SemCor in rankingthe senses within WN.11 3.
The SM07 results as a10We exclude the data points that have a tag of ?U?
in thegold standard for both baselines and our system.11From an application standpoint, we do not find the firstsense baseline to be of interest since it introduces a stronglevel of uniformity ?
removing semantic variability ?
whichis not desirable.
Even if the first sense achieves higher resultsin data sets, it is an artifact of the size of the data and the verylimited number of documents under investigation.1546monolingual baseline.
4.
The DR02 results as themultilingual baseline.5.4 Experimental Results5.4.1 RelContWe present the results for 4 different experi-mental conditions for RelCont: JCN-V whichuses JCN instead of LCH for verb-verb similar-ity comparison, we consider this our base con-dition; +ExpandL is adding the Lesk Expansionto the base condition, namely Lesk2;12 +SemCoradds the SemCor expansion to the base condi-tion; and finally +ExpandL SemCor, adds the lat-ter both conditions simultaneously.
Table 1 illus-trates the obtained results for the SV2AW usingWordNet 1.7.1 since it is the most studied data setand for ease of comparison with previous studies.We break the results down by POS tag (N)oun,(V)erb, (A)djective, and Adve(R)b.
The coveragefor SV2AW is 98.17% losing some of the verb andadverb target words.Our overall results on all the data sets clearlyoutperform the baseline as well as state-of-the-art performance using an unsupervised system(SM07) in overall f-measure across all the datasets.
We are unable to beat the most frequentbaseline (MFBL) which is obtained using the firstsense.
However MFBL is a supervised baselineand our approach is unsupervised.
Our implemen-tation of SM07 is slightly higher than those re-ported in (Sinha and Mihalcea, 2007) (57.12% )is probably due to the fact that we do not considerthe items tagged as ?U?
and also we resolve someof the POS tag mismatches between the gold setand the test data.
We note that for the SV2AW dataset our coverage is not 100% due to some POS tagmismatches that could not have been resolved au-tomatically.
These POS tag problems have to domainly with multiword expressions.
In observingthe performance of the overall RelCont, we notethat using JCN for verbs clearly outperforms us-ing the LCH similarity measure.
Using SemCor toaugment WN examples seems to have the biggestimpact.
Combining SemCor with ExpandL yieldsthe best results.Observing the results yielded per POS in Ta-ble 1, ExpandL seems to have the biggest impacton the Nouns only.
This is understandable sincethe noun hierarchy has the most dense relationsand the most consistent ones.
SemCor augmen-12Using Lesk3 yields almost the same resultstation of WN seemed to benefit all POS signifi-cantly except for nouns.
In fact the performanceon the nouns deteriorated from the base conditionJCN-V from 68.7 to 68.3%.
This maybe due to in-consistencies in the annotations of nouns in Sem-Cor or the very fine granularity of the nouns inWN.
We know that 72% of the nouns, 74% ofthe verbs, 68.9% of the adjectives, and 81.9% ofthe adverbs directly exploited the use of SemCoraugmented examples.
Combining SemCor andExpandL seems to have a positive impact on theverbs and adverbs, but not on the nouns and adjec-tives.
These trends are not held consistently acrossdata sets.
For example, we see that SemCor aug-mentation helps all POS tag sets over using Ex-pandL alone or even when combined with Sem-Cor.
We note the similar trends in performance forthe SV3AW data.Compared to state of the art systems, RelContwith an overall F-measure performance of 62.13%outperforms the best unsupervised system of57.5% UNED-AW-U2 for SV2 (Navigli, 2009).
Itis worth noting that it is higher than several of thesupervised systems.
Moreover, RelCont yieldsbetter overall results on SV3 at 59.87 compared tothe best unsupervised system IRST-DDD-U whichyielded an F-measure of 58.3% (Navigli, 2009).5.4.2 TransContFor the TransCont results we illustrate the orig-inal SALAAM results as our baseline.
Simi-lar to the DR02 work, we actually use the sameSALAAM parallel corpora comprising more than5.5M English tokens translated using a single ma-chine translation system GlobalLink.
Thereforeour parallel corpus is the French English transla-tion condition mentioned in DR02 work as FrGl.We have 4 experimental conditions: FRGL usingLesk2 for all POS tags in the typeset disambigua-tion (Lesk2); FRGL using Lesk3 for all POS tags(Lesk3); using Lesk3 for N, A and R but LIN simi-larity measure for verbs (Lesk3 Lin); using Lesk3for N, A and R but JCN for verbs (Lesk3 JCN).In Table 3 we note the the Lesk3 JCN followedimmediately by Lesk3 Lin yield the best perfor-mance.
The trend holds for both SV2AW andSV3AW.
Essentially our new implementation ofthe multilingual system significantly outperformsthe original DR02 implementation for all experi-mental conditions.1547Condition N V A R Global F MeasureRAND 43.7 21 41.2 57.4 39.9MFBL 71.8 41.45 67.7 81.8 65.35SM07 68.7 33.01 65.2 63.1 59.2JCN-V 68.7 35.46 65.2 63.1 59.72+ExpandL 70.2 35.86 65.4 62.45 60.48+SemCor 68.5 38.66 69.2 67.75 61.79+ExpandL SemCor 69.0 38.66 68.8 69.45 62.13Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.Condition N V A R Global F MeasureRAND 39.67 19.34 41.85 92.31 32.97MFBL 70.4 54.15 66.7 92.88 63.96SM07 60.9 43.4 57 92.88 53.98JCN-V 60.9 48.5 57 92.88 55.87+ExpandL 59.9 48.55 57.95 92.88 55.62+SemCor 66 48.95 65.55 92.88 59.87+ExpandL SemCor 65 49.2 65.55 92.88 59.52Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.5.4.3 Global Combined WSDIn this section we present the results of the globalcombined WSD system.
All the combined ex-perimental conditions have the same percentagecoverage.13 We present the results combining us-ing MERGE and using VOTE.
We have chosen4 baseline systems: (1) SM07; (2) the our base-line monolingual system using JCN for verb-verbcomparisons (RelCont-BL), so as to distinguishthe level of improvement that could be attributedto the multilingual system in the combination re-sults; as well as (3) and (4) our best individual sys-tem results from RelCont (ExpandL SemCor)referred to in the tables below as (RelCont-Final)and TransCont using the best experimental con-dition (Lesk3 JCN).
Table 5 and 6 illustrates theoverall performance of our combined approach.In Table 5 we note that the combined conditionsoutperform the two base systems independently,using TransCont is always helpful for any of the3 monolingual systems, no matter we use VOTE orMERGE.
In general the trend is that VOTE outper-forms MERGE, however they exhibit different be-haviors with respect to what works for each POS.In Table 6 the combined result is not alwaysbetter than the corresponding monolingual sys-tem.
When applying to our baseline monolin-13We do not back off in any of our systems to a defaultsense, hence the coverage is not at a 100%.gual system, the combined result is still bet-ter.
However, we observed worse results for Ex-pandL Semcor, RelCont-Final.
There may be 2main reasons for the loss: (1) SV3 is the tuningset in SM07, and we inherit the thresholds forsimilarity metrics from that study.
Accordingly,an overfitting of the thresholds is probably hap-pening in this case; (2) TransCont results arenot good enough on the SV3AW data.
Compar-ing the RelCont and TransCont system re-sults, we find a drop in f-measure of ?1.37%in SV2AW, in contrast to a much larger drop inperformance for the SV3AW data set where thedrop in performance is ?6.38% when comparingRelCont-BL to TransCont and nearly ?10%comparing against RelCont-Final.6 DiscussionWe looked closely at the data in the combined con-ditions attempting to get a feel for the data andunderstand what was captured and what was not.Some of the good examples that are captured in thecombined system that are not tagged in RelContis the case of ringer in Like most of the other 6,000churches in Britain with sets of bells , St. Michaelonce had its own ?
band ?
of ringers , who wouldherald every Sunday morning and evening service..
The RelCont answer is ringer sense number 4:(horseshoes) the successful throw of a horseshoe1548Condition N V A R Global F MeasureRAND 43.7 21 41.2 57.4 39.9DR02-FRGL 54.5SALAAM 65.48 31.77 56.87 67.4 57.23Lesk2 67.05 30 59.69 68.01 57.27Lesk3 67.15 30 60.2 68.01 57.41Lesk3 Lin 67.15 29.27 60.2 68.01 57.61Lesk3 JCN 67.15 33.88 60.2 68.01 58.35Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.Condition N V A R Global F MeasureRAND 39.67 19.34 41.85 92.31 32.93SALAAM 52.42 29.27 54.14 88.89 45.63Lesk2 53.57 33.58 53.63 88.89 47Lesk3 53.77 33.30 56.48 88.89 47.5Lesk3 Lin 53.77 29.24 56.48 88.89 46.37Lesk3 JCN 53.77 38.43 56.48 88.89 49.29Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.or quoit so as to encircle a stake or peg.
Whenthe merged system is employed we see the cor-rect sense being chosen as sense number 1 in theMERGE condition: defined in WN as a personwho rings church bells (as for summoning the con-gregation) resulting from a corresponding transla-tion into French as sonneur.We did some basic data analysis on the itemswe are incapable of capturing.
Several of themare cases of metonymy in examples such as ?theEnglish are known...?, the sense of English hereis clearly in reference to the people of England,however, our WSD system preferred the languagesense of the word.
These cases are not gotten byany of our systems.
If it had access to syntac-tic/semantic roles we assume it could capture thatthis sense of the word entails volition for example.Other types of errors resulted from the lack of away to explicitly identify multiwords.Looking at the performance of TransCont wenote that much of the loss is a result of the lack ofvariability in the translations which is a key factorin the performance of the algorithm.
For examplefor the 157 adjective target test words in SV2AW,there was a single word alignment for 51 of thecases, losing any tagging for these words.7 Conclusions and Future DirectionsIn this paper we present a framework that com-bines orthogonal sources of evidence to create astate-of-the-art system for the task of WSD disam-biguation for AW.
Our approach yields an over-all global F measure of 64.58 for the standardSV2AW data set combining monolingual and mul-tilingual evidence.
The approach can be fur-ther refined by adding other types of orthogo-nal features such as syntactic features and seman-tic role label features.
Adding SemCor exam-ples to TransCont should have a positive im-pact on performance.
Also adding more languagesas illustrated by the DR02 work should also yieldmuch better performance.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves statisti-cal machine translation.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 33?40, Prague, Czech Republic,June.
Association for Computational Linguistics.Mona Diab and Philip Resnik.
2002.
An unsuper-vised method for word sense tagging using parallelcorpora.
In Proceedings of 40th Annual Meetingof the Association for Computational Linguistics,1549Condition N V A R Global F MeasureSM07 68.7 33.01 65.2 63.1 59.2RelCont-BL 68.7 35.46 65.2 63.1 59.72RelCont-Final 69.0 38.66 68.8 69.45 62.13TransCont 67.15 33.88 60.2 68.01 58.35MERGE: RelCont-BL+TransCont 69.3 36.91 66.7 64.45 60.82VOTE: RelCont-BL+TransCont 71 37.71 66.5 66.1 61.92MERGE: RelCont-Final+TransCont 70.7 38.66 69.5 70.45 63.14VOTE: RelCont-Final+TransCont 74.2 38.26 68.6 71.45 64.58Table 5: F-measure % for all Combined experimental conditions on SV2AWCondition N V A R Global F MeasureSM07 60.9 43.4 57 92.88 53.98RelCont-BL 60.9 48.5 57 92.88 55.87RelCont-Final 65 49.2 65.55 92.88 59.52TransCont 53.77 38.43 56.48 88.89 49.29MERGE: RelCont-BL+TransCont 60.6 49.5 58.85 92.88 56.47VOTE: RelCont-BL+TransCont 59.3 49.5 59.1 92.88 55.92MERGE: RelCont-Final+TransCont 63.2 50.3 65.25 92.88 59.07VOTE: RelCont-Final+TransCont 62.4 49.65 65.25 92.88 58.47Table 6: F-measure % for all Combined experimental conditions on SV3AWpages 255?262, Philadelphia, Pennsylvania, USA,July.
Association for Computational Linguistics.Christiane Fellbaum.
1998.
?wordnet: An electroniclexical database?.
MIT Press.Weiwei Guo and Mona Diab.
2009.
Improvements tomonolingual english word sense disambiguation.
InProceedings of the Workshop on Semantic Evalua-tions: Recent Achievements and Future Directions(SEW-2009), pages 64?69, Boulder, Colorado, June.Association for Computational Linguistics.N.
Ide and J. Veronis.
1998.
Word sense disambigua-tion: The state of the art.
In Computational Linguis-tics, pages 1?40, 24:1.J.
Jiang and D. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the International Conference on Re-search in Computational Linguistics, Taiwan.C.
Leacock and M. Chodorow.
1998.
Combining lo-cal context and wordnet sense similarity for wordsense identification.
In WordNet, An Electronic Lex-ical Database.
The MIT Press.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: How to tell a pinecone from an ice cream cone.
In In Proceedings ofthe SIGDOC Conference, Toronto, June.Rada Mihalcea.
2005.
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 411?418, Vancouver,British Columbia, Canada, October.
Association forComputational Linguistics.George A. Miller.
1990.
Wordnet: a lexical databasefor english.
In Communications of the ACM, pages39?41.Roberto Navigli and Mirella Lapata.
2007.
Graphconnectivity measures for unsupervised word sensedisambiguation.
In Proceedings of the 20th Inter-national Joint Conference on Artificial Intelligence(IJCAI), pages 1683?1688, Hyderabad, India.Roberto Navigli.
2009.
Word sense disambiguation:a survey.
In ACM Computing Surveys, pages 1?69.ACM Press.Franz Joseph Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.M.
Palmer, C. Fellbaum, S. Cotton, L. Delfs, , andH.
Dang.
2001.
English tasks: all-words and verblexical sample.
In In Proceedings of ACL/SIGLEXSenseval-2, Toulouse, France, June.Ted Pedersen, Satanjeev Banerjee, and Siddharth Pat-wardhan.
2005.
Maximizing semantic relatednessto perform word sense disambiguation.
In Univer-sity of Minnesota Supercomputing Institute ResearchReport UMSI 2005/25, Minnesotta, March.1550Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 task-17: En-glish lexical sample, srl and all words.
In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), pages 87?92,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-based word sense disambiguation using mea-sures of word semantic similarity.
In Proceedingsof the IEEE International Conference on SemanticComputing (ICSC 2007), Irvine, CA.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Rada Mihalcea and PhilEdmonds, editors, Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, pages 41?43, Barcelona,Spain, July.
Association for Computational Linguis-tics.1551
