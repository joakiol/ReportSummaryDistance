Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1183?1191,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsAccurate Evaluation of Segment-level Machine Translation MetricsYvette Graham?
?Nitika Mathur?Timothy Baldwin?
?Department of Computing and Information Systems, The University of Melbourne?ADAPT Research Centre, Trinity College Dublinygraham@scss.tcd.ie, nmathur@student.unimelb.edu.au, tb@ldwin.netAbstractEvaluation of segment-level machine transla-tion metrics is currently hampered by: (1) lowinter-annotator agreement levels in human as-sessments; (2) lack of an effective mechanismfor evaluation of translations of equal quality;and (3) lack of methods of significance testingimprovements over a baseline.
In this paper,we provide solutions to each of these chal-lenges and outline a new human evaluationmethodology aimed specifically at assessmentof segment-level metrics.
We replicate the hu-man evaluation component of WMT-13 andreveal that the current state-of-the-art perfor-mance of segment-level metrics is better thanpreviously believed.
Three segment-level met-rics ?
METEOR, NLEPOR and SENTBLEU-MOSES ?
are found to correlate with humanassessment at a level not significantly outper-formed by any other metric in both the individ-ual language pair assessment for Spanish-to-English and the aggregated set of 9 languagepairs.1 IntroductionAutomatic segment-level machine translation (MT)metrics have the potential to greatly advance MT byproviding more fine-grained error analysis, increas-ing efficiency of system tuning methods and leverag-ing techniques for system hybridization.
However, amajor obstacle currently hindering the developmentof segment-level metrics is their evaluation.
Humanassessment is the gold standard against which met-rics must be evaluated, but when it comes to the taskof evaluating translation quality, human annotatorsare notoriously inconsistent.
For example, the mainvenue for evaluation of metrics, the annual Work-shop on Statistical Machine Translation (WMT),reports disturbingly low inter-annotator agreementlevels and highlights the need for better humanassessment of MT.
WMT-13, for example, reportKappa coefficients ranging from 0.075 to 0.324 forassessors from crowd-sourcing services, only in-creasing to between 0.315 and 0.457 for MT re-searchers (Bojar et al, 2013a).
For evaluation ofmetrics that operate at the system or document-levelsuch as BLEU, inconsistency in individual humanjudgments can, to some degree, be overcome by ag-gregation of individual human assessments over thesegments within a document.
However, for evalua-tion of segment-level metrics, there is no escapingthe need to boost the consistency of human annota-tion of individual segments.This motivates our analysis of current methods ofhuman evaluation of segment-level metrics, and pro-posal of an alternative annotation mechanism.
Weexamine the accuracy of segment scores collectedwith our proposed method by replicating compo-nents of the WMT-13 human evaluation (Bojar etal., 2013b), with the sole aim of optimizing agree-ment in segment scores to provide an effective goldstandard for evaluating segment-level metrics.
Ourmethod also supports the use of significance test-ing of segment-level metrics, and tests applied tothe WMT-13 metrics over nine language pairs re-veal for the first time which segment-level metricsoutperform others.
We have made available code foracquiring accurate segment-level MT human eval-uations from the crowd, in addition to significance1183testing competing segment-level metrics, at:https://github.com/ygraham/segment-mteval2 WMT-style Evaluation of Segment-levelMT MetricsSince 2008, the WMT workshop series has includeda shared task for automatic metrics, and as with thetranslation shared task, human evaluation remainsthe official gold standard for evaluation.
In orderto minimize the amount of annotation work and en-force consistency between the primary shared tasksin WMT, the same evaluations are used to evalu-ate MT systems in the shared translation task, aswell as MT evaluation metrics in the document-levelmetrics and segment-level metrics tasks.
AlthoughWMT have trialled several methods of human eval-uation over the years, the prevailing method takesthe form of ranking a set of five competing trans-lations for a single source language (SL) input seg-ment from best to worst.
A total of ten pairwise hu-man relative preference judgments can be extractedfrom each set of five translations.
Performance ofa segment-level metric is assessed by the degree towhich it corresponds with human judgment, mea-sured by the number of metric scores for pairs oftranslations that are either concordant (Con) or dis-cordant (Dis) with those of a human assessor, whichthe organizers describe as ?Kendall?s ??:?
=|Con| ?
|Dis||Con|+ |Dis|Pairs of translations deemed equally good by ahuman assessor are omitted from evaluation ofsegment-level metrics (Bojar et al, 2014).There is a mismatch between the human judg-ments data used to evaluate segment-level metricsand the standard conditions under which Kendall?s?
is applied, however: Kendall?s ?
is used to mea-sure the association between a set of observations ofa single pair of joint random variables, X (e.g.
thehuman rank of a translation) and Y (e.g.
the met-ric score for the same translation).
A conventionalapplication of Kendall?s ?
would be comparison ofall pairs of values within X with each correspond-ing pair within Y .
Since the human assessment datais, however, a large number of separately ranked setsof five competing translations and not a single rank-ing of all translations, it is not possible to compute asingle Kendall?s ?
correlation.1The formula usedto assess the performance of a metric in the task,therefore, is not what is ordinarily understood to bea Kendall?s ?
coefficient, but, in fact, equivalent to aweighted average of all Kendall?s ?
for each human-ranked set of five translations.A more significant problem, however, lies in theinconsistency of human relative preference judg-ments within data sets.
Since overall scores for met-rics are described as correlations, possible valuesachievable by any metric could be expected to liein the range [?1, 1] (or ??1?).
This is not the case,and achievements of metrics are obscured by con-tradictory human judgments.
Before any metric hasprovided scores for segments, for example, the max-imum and minimum correlation achievable by a par-ticipating metric can be computed as, in the case ofWMT-13:?
Russian-to-English: ?0.92?
Spanish-to-English: ?0.90?
French-to-English: ?0.90?
German-to-English: ?0.92?
Czech-to-English: ?0.89?
English-to-Russian: ?0.90?
English-to-Spanish: ?0.90?
English-to-French: ?0.91?
English-to-German: ?0.90?
English-to-Czech: ?0.87If we are interested in the relative performance ofmetrics and take a closer look at the formula usedto contribute a score to metrics, we can effectivelyignore the denominator (|Con|+ |Dis|), as it is con-stant for all metrics.
The numerator (|Con| ?
|Dis|)is what determines our evaluation of the relative per-formance of metrics, and although the formula ap-pears to be a straightforward subtraction of countsof concordant and discordant pairs, due to the largenumbers of contradictory human relative preferencejudgments in data sets, what this number actuallyrepresents is not immediately obvious.
If, for exam-ple, translations A and B were scored by a metricsuch that metric score(A) > metric score(B), one1This would in fact require all (|MT systems| ?
|distinctsegments|) translations included in the evaluation to be placedin a single rank order.1184might expect an addition or subtraction of 1 depend-ing on whether or not the metric?s scores agreed withthose of a human.
Instead, however, the following isadded:(max(|A > B|, |A < B|)?min(|A > B|, |A < B|))?
dwhere:|A > B| = # human judgments where A waspreferred over B|A < B| = # human judgments where B waspreferred over Ad ={1 if |A < B| > |A > B|?1 if |A < B| < |A > B|For example, translations of segment 971 for Czech-to-English systems uedin-heafield and uedin-wmt13were compared by human assessors a total of 12times: the first system was judged to be best 4 times,the second system was judged to be best 2 times, andthe two systems were judged to be equal 6 times.This results in a score of 4?2 for a system-level met-ric that scores the uedin-heafield translation higherthan uedin-wmt13 (tied judgments are omitted), orscore of 2?
4 in the converse case.Another challenge is how to deal with relativepreference judgments where two translations aredeemed equal quality (as opposed to strictly better orworse).
In the current setup, tied translation pairs areexcluded from the data, meaning that the ability forevaluation metrics to evaluate similar translations isnot directly evaluated, and a metric that manages toscore two equal quality translations closer, does notreceive credit.
A segment-level metric that can ac-curately predict not just disparities between transla-tions but also similarities is likely to have high util-ity for MT system optimization, and is possibly thestrongest motivation for developing segment-levelmetrics in the first place.
In WMT-13, however, 24%of all relative preference judgments were omitted onthe basis of ties, broken down as follows:?
Spanish-to-English: 28%?
French-to-English: 26%?
German-to-English: 27%?
Czech-to-English: 25%?
Russian-to-English: 24%?
English-to-Spanish: 23%?
English-to-French: 23%?
English-to-German: 20%?
English-to-Czech: 16%?
English-to-Russian: 27%Although significance tests for evaluation of MTsystems and document-level metrics have been iden-tified (Koehn, 2004; Graham and Baldwin, 2014;Graham et al, 2014b), no such test has been pro-posed for segment-level metrics, and it is unfortu-nately common to conclude success without takinginto account the fact that an increase in correlationcan occur simply by chance.
In the rare cases wheresignificance tests have been applied, tests or confi-dence intervals for individual correlations form thebasis for drawing conclusions (Aziz et al, 2012;Machacek and Bojar, 2014).
However, such tests donot provide insight into whether or not a metric out-performs another, as all that?s required for rejectionof the null hypothesis with such a test is a likelihoodthat an individual metric?s correlation with humanjudgment is not equal to zero.
In addition, data setsfor evaluation in both document and segment-levelmetrics are not independent and the correlation thatexists between pairs of metrics should also be takeninto account by significance tests.3 Segment-Level Human EvaluationMany human evaluation methodologies attempt toelicit precisely the same quality judgment for indi-vidual translations from all assessors, and inevitablyproduce large numbers of conflicting assessments inthe process, including from the same individual hu-man judge (Callison-Burch et al, 2007; Callison-Burch et al, 2008; Callison-Burch et al, 2009).
Analternative approach is to take into account the factthat different judges may genuinely disagree, andallow assessments provided by individuals to eachcontribute to an overall estimate of the quality of agiven translation.In an ideal world in which we had access to as-sessments provided by the entire population of qual-ified human assessors, for example, the mean ofthose assessments would provide a statistic that,in theory at least, would provide a meaningfulsegment-level human score for translations.
If itwere possible to collect assessments from the entire1185population we could directly compute the true meanscore for a translation segment.
This is of course notpossible, but thanks to the law of large numbers wecan make the following assumption:Given a sufficiently large assessment sam-ple for a given translation, the mean of as-sessments will provide a very good esti-mate of the true mean score of the transla-tion sourced from the entire assessor pop-ulation.What the law of large numbers does not tell us,however, is, for our particular case of translationquality assessment, precisely how large the sampleof assessments needs to be, so that the mean ofscores provides a close enough estimate to the truemean score for any translation.
For a sample meanfor which the variance is known, the required sam-ple size can be computed for a specified standard er-ror.
However, due to the large number of distincttranslations we deal with, the variance in samplescore distributions may change considerably fromone translation to the next.
In addition, the choiceas to what exactly is an acceptable standard error insample means would be somewhat arbitrary.
On theone hand, if we specify a standard error that?s lowerthan is required, and subsequently collect more re-peat assessments than is needed, we would be wast-ing resources that could, for example, be targeted atthe annotation of additional translation segments.Our solution is to empirically investigate the im-pact on sample size of repeat assessments on themean score for a given segment, and base our de-termination of sample size on the findings.
Sincewe later motivate the use of Pearson?s correlation tomeasure the linear association between human andmetric scores (see Section 4), we base our investiga-tion on Pearson?s correlation.We collect multiple assessments per segment tocreate score distributions for segments for a fixed setper language pair.
This is repeated twice over thesame set of segments to generate two distinct setsof annotations: one set is used to estimate the truemean score, and the second set is randomly down-sampled to simulate a set of assessments of fixedsample size.
We measure the Pearson correlation be-tween the true mean score and different numbers ofLanguage# translations# assessmentspair per translationes-en 280 40en-es 140 19en-ru 140 15en-de 140 14Table 1: Datasets used to assess translation assessmentsample size0 10 20 30 400.20.40.60.81.0Nz?scorees?enen?esen?ruen?derawes?enen?esen?ruen?deFigure 1: Correlation (r) of translation quality estimatesbetween the initial and repeat experiment runs for each ofthe four language pairs from WMT-13, for sample sizeNand based on raw and standardized (z) scores.assessments for a given assessment, to ask the ques-tion: how many assessments must be collected fora given segment to obtain mean segment scores thattruly reflects translation quality?
Scores are sampledaccording to annotation time to simulate a realisticsetting.3.1 Translation Assessment Sample SizeMTurk was used to collect large numbers of transla-tion assessments, in sets of 100 translations per as-sessment task (or ?HIT?
in MTurk parlance).
TheHITS were structured to include degraded transla-tions and repeat translations, and rated on a contin-uous Likert scale with a single translation assess-ment displayed to the assessor at one time (Grahamet al, 2014a; Graham et al, 2013).
This supportsaccurate quality-control as well as normalisation oftranslation scores for each assessor.
The assessment1186?3 ?2 ?1 0 1 2 3?3?2?10123N = 1r = 0.41?3 ?2 ?1 0 1 2 3?3?2?10123N = 2r = 0.6?3 ?2 ?1 0 1 2 3?3?2?10123N = 5r = 0.76?3 ?2 ?1 0 1 2 3?3?2?10123N = 10r = 0.86?3 ?2 ?1 0 1 2 3?3?2?10123N = 15r = 0.91?3 ?2 ?1 0 1 2 3?3?2?10123N = 40r = 0.97Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (y-axis) for Spanish-to-English over WMT-13, where each point represents a standardized segment-level score computedas the mean of the N individual assessments for that plot.task was posed as a monolingual task, where asses-sors were asked to rate the degree to which the MTsystem output adequately expressed the meaning ofthe corresponding reference translation.
Transla-tions were sampled at random from the WMT-13data sets for the four language pairs, as detailed inTable 1.
Due to low-quality assessors on MTurkand the need for assessments solely for quality as-surance purposes, the exercise required a substantialnumber of individual assessments.
For Spanish-to-English, for example, a total of (280 translations +120 translations for quality-control purposes) ?
40assessments per translation ?
2 separate data col-lections ?
?2 to allow for filtering of low-qualityassessors = ?64k assessments were collected; afterquality control filtering and removing the quality-control translations, around 22k assessments wereused for the actual experiment.Figure 1 shows the Pearson correlation betweenmean segment-level scores calculated for varyingnumbers of assessments (N ), and the full set of as-sessments for the second set of assessments.
Foreach language pair, we calculate the correlation firstover the raw segment scores and second over stan-dardized scores, based on the method of Graham etal.
(2014a).2For all language pairs, although thecorrelation is relatively low for single assessments,as the sample size increases, it increases, and byapproximately N = 15 assessments, for all fourlanguage pairs, the correlation reaches r = 0.9.For Spanish-to-English, for which most assessmentswere collected, when we increase the number of as-sessments to N = 40 per translation, the correla-tion reaches r = 0.97.
Figure 2 is a set of scatterplots for mean segment-level scores for Spanish-to-English rising, for varying sample sizes N .As expected, the larger the sample size of assess-ments, the greater the agreement with the true meanscore, but what is more surprising is that with as fewas 15 assessments, the scores collected in the twoseparate experiments correlate extremely well, andprovide what we believe to be a sufficient stabilityto evaluate segment-level metrics.2Standardized segment scores are computed by standardiz-ing individual raw scores according to the mean and standarddeviation of individual assessors, and then combined into meansegment scores.11874 Segment-level Metric EvaluationSince the scores generated by our method are contin-uous and segment-level metrics are also required tooutput continuous-valued scores, we can now com-pare the scores directly using Pearson?s correlation.Pearson?s correlation has three main advantages forthis purpose.
Firstly, the measure is unit-free, sometrics do not have to produce scores on the samescale as the human assessments.
Secondly, scoresare absolute as opposed to relative and thereforemore intuitive and ultimately more powerful; for ex-ample, we are able to evaluate metrics over the 20%of translations of highest or lowest quality in the testset.
Finally, the use of Pearson?s correlation facil-itates the measurement of statistical significance incorrelation differences.It is important to point out, however, that mov-ing from Kendall?s ?
over relative preference judg-ments to Pearson?s r over absolute scores does, infact, change the task required of metrics in one re-spect: previously, there was no direct evaluation ofthe scores generated by a metric, nor indeed did theevaluation ever directly compare translations for dif-ferent source language inputs (as relative preferencejudgments were always relative to other translationsfor the same input).
Pearson?s correlation, on theother hand, compares scores across the entire testset.4.1 Significance Testing of Segment-levelMetricsWith the move to Pearson?s correlation, we canalso test statistical significance in differences be-tween metrics, based on the Williams test (Williams,1959),3which evaluates significance in a differencein dependent correlations (Steiger, 1980).
As sug-gested by Graham and Baldwin (2014), the test isappropriate for evaluation of document-level MTmetrics since the data is not independent, and forsimilar reasons, the test can also be used for evalua-tion of segment-level metrics.4.2 Spanish-to-English Segment-level MetricsWe first carry out tests for Spanish-to-Englishsegment-level metrics from WMT-13.
In our exper-iments in Section 3.1, we used only a sub-sample3Also sometimes referred to as the Hotelling?Williams test.Metric r ?METEOR 0.484 0.324NLEPOR 0.483 0.281SENTBLEU-MOSES 0.465 0.266DEP-REF-EX 0.453 0.307DEP-REF-A 0.453 0.312SIMPBLEUP 0.450 0.287SIMPBLEUR 0.444 0.388LEPOR 0.408 0.236UMEANT 0.353 0.202MEANT 0.342 0.202TERRORCAT 0.313 0.313Table 2: Pearson?s correlation and Kendall?s ?
betweenWMT-13 segment-level metrics and human assessmentfor Spanish-to-English (ES-EN).
Note that Kendall?s ?is based on the WMT-13 formulation, and the preferencejudgments from WMT-13.of segments, so the first thing is to collect assess-ments for the remaining Spanish-to-English transla-tion segments using MTurk, based on a sample ofat least 15 assessments.
A total of 24 HITs of 100translations each were posted on MTurk; after re-moval of low quality workers (?50%) and qualitycontrol items (a further 30%), this resulted in 840translation segments with 15 or more assessmentseach.
The scores were standardized and combinedinto mean segment scores.Table 2 shows the Pearson?s correlation for eachmetric that participated in the WMT-13 Spanish-to-English evaluation task, along with the Kendall?s?
based on the original WMT-13 methodology andrelative preference assessments.
Overall, whenwe compare correlations using the new evaluationmethodology to those from the original evaluation,even though we have raised the bar by assessingthe raw numeric outputs rather than translating theminto preference judgments relative to other trans-lations for the same SL input, all metrics achievehigher correlation with human judgment than re-ported in the original evaluation.
This indicates thatthe new evaluation setup is by no means unreal-istically difficult, and that even though it was notrequired of the metrics in the original task setup,the metrics are doing a relatively good job of ab-solute scoring of translation adequacy.
In addition,11880 0.2 0.4 0.6 0.8 1rFigure 3: Pearson?s correlation between every pair ofsegment-level metric competing in the WMT-13 Spanish-to-English task.the new assessment reflects how well metrics scoretranslations of very close or equal quality, and, asdescribed in Section 2, ameliorates the issue of lowinter-annotator agreement as well as resolving theoriginal mismatch between discrete human relativepreference judgments and continuous metric scores.Figure 3 is a heat map of the Pearson?s cor-relation between each pair of segment-level met-rics for Spanish-to-English from WMT-13, and Fig-ure 4 shows correspondence between scores of threesegment-level metrics with our human evaluationdata.
Figure 5 displays the outcome of the Williamssignificance test as applied to each pairing of com-peting metrics.
Since the power of Williams test in-creases with the strength of correlation between apair of metrics, it is important not to conclude thebest system by the number of other metrics it outper-forms.
Instead, the best choice of metric for that lan-guage pair is any metric that is not signicifantly out-performed by any other metric.
Three metrics provenot to be significantly outperformed by any othermetric for Spanish-to-English, and tie for best per-formance: METEOR (Denkowski and Lavie, 2011),NLEPOR (Han et al, 2013) and SENTBLEU-MOSES(sBLEU-moses).p-value0 0.05 0.1Figure 5: Evaluation of significance of increase incorrelation with human judgment between every pairof segment-level metrics competing in the Spanish-to-English WMT-13 metrics task.
A colored cell (i,j) in-dicates that system named in row i significantly outper-forms system named in column j at p < 0.1, and greencells at p < 0.05.Metric rMETEOR 0.441NLEPOR 0.416SENTBLEU-MOSES 0.422SIMPBLEUP 0.418SIMPBLEUR 0.404LEPOR 0.326Table 3: Pearson?s correlation between each WMT-13segment-level metric and human assessment for the com-bined set of nine language pairs.4.3 9 Language PairsSince human assessments are now absolute, scoreseffectively have the same meaning across languagepairs, facilitating the combination of data acrossmultiple language pairs.
Since many approachesto MT are language-pair independent, the ability toknow what segment-level metric works best acrossall language pairs is useful for choosing an appro-priate default metric or simply avoiding having toswap and change metrics across different language1189?3 ?2 ?1 0 1 2?4?2024HumanMeteorr= 0.484?3 ?2 ?1 0 1 2?4?2024HumanLEPOR_v3.1r= 0.408?3 ?2 ?1 0 1 2?4?2024HumanTerrorCatr= 0.313Figure 4: Standardized segment-level scores for human vs. metric over the WMT-13 Spanish-to-English segment-levelmetric task, for a metric achieving highest, mid-range and lowest Pearson?s correlation with human judgment.pairs.Assessments of translations were crowd-sourcedfor nine language pairs used in the WMT-13 sharedmetrics task: Russian-to-English, Spanish-to-English, French-to-English, German-to-English,Czech-to-English, English-to-Russian, English-to-Spanish, English-to-French and English-to-German.4Again, we obtain a minimum of 15assessments per translation, and collect scores for100 translations per language pair.
After removalof quality control items, this leaves 70 distincttranslations per language pair, combined into across-lingual test set of 630 distinct translationsspanning nine language pairs.Table 3 shows Pearson?s correlation with humanassessment for the six segment-level metrics thatcompeted across all language pairs in WMT-13, andFigure 6 shows the outcomes of Williams test forstatistical significance between different pairings ofmetrics.
Results reveal that the same three metricsas before (METEOR, SENTBLEU-MOSES and NLE-POR), in addition to SIMPBLEUP and SIMPBLEURare not significantly outperformed by any other met-ric at p<0.05.
However, since the latter two wereshown to be outperformed for Spanish-to-English,all else being equal, METEOR, SENTBLEU-MOSESand NLEPOR are still a superior choice of defaultmetric.4We were regrettably unable to include English-to-Czech,due to a lack of Czech-speaking MTurk workers.p-value0 0.05 0.1Figure 6: Evaluation of significance of increase in cor-relation with human judgment between every pair ofsegment-level metrics competing in all nine in WMT-13metrics task.
A colored cell (i,j) indicates that systemnamed in row i significantly outperforms system namedin column j at p < 0.1 and green cells specificallyp < 0.05.5 ConclusionWe presented a new evaluation methodology forsegment-level metrics that overcomes the issue oflow inter-annotator agreement levels in human as-sessments, includes evaluation of very close andequal quality translations, and provides a signif-icance test that supports system comparison withconfidence.
Our large-scale human evaluation re-veals three metrics to not be significantly outper-formed by any other metric in both Spanish-to-1190English and a combined evaluation across ninelanguage pairs, namely: METEOR, NLEPOR andSENTBLEU-MOSES.AcknowledgementsWe wish to thank the anonymous reviewers for their valu-able comments.
This research was supported by fundingfrom the Australian Research Council and Science Foun-dation Ireland (Grant 12/CE/12267).ReferencesW.
Aziz, S. Castilho, and L. Specia.
2012.
PET: a toolfor post-editing and assessing machine translation.
InProc.
of the 8th International Conference on LanguageResources and Evaluation (LREC 2012), pages 3982?3987, Istanbul, Turkey.O.
Bojar, C. Buck, C. Callison-Burch, C. Federmann,B.
Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,and L. Specia.
2013a.
Findings of the 2013 Workshopon Statistical Machine Translation.
In Proceedings ofthe Eighth Workshop on Statistical Machine Transla-tion, pages 1?44, Sofia, Bulgaria.O.
Bojar, C. Buck, C. Callison-Burch, C. Federmann,B.
Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,and L. Specia.
2013b.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Proc.
8thWkshp.
Statistical Machine Translation, pages 1?44,Sofia, Bulgaria.O.
Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn,J.
Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, and A. Tamchyna.2014.
Findings of the 2014 Workshop on StatisticalMachine Translation.
In Proc.
9th Wkshp.
StatisticalMachine Translation, pages 12?58, Baltimore, USA.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machinetranslation.
In Proc.
2nd Wkshp.
Statistical MachineTranslation, pages 136?158, Prague, Czech Republic.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2008.
Further meta-evaluation of ma-chine translation.
In Proc.
3rd Wkshp.
Statistical Ma-chine Translation, pages 70?106, Columbus, USA.C.
Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.2009.
Findings of the 2009 Workshop on StatisticalMachine Translation.
In Proc.
4th Wkshp.
StatisticalMachine Translation, pages 1?28, Athens, Greece.M.
Denkowski and A. Lavie.
2011.
Meteor 1.3: Auto-matic metric for reliable optimization and evaluationof machine translation systems.
In Proc.
6th Wkshp.Statistical Machine Translation, pages 85?91, Edin-burgh, Scotland.Y.
Graham and T. Baldwin.
2014.
Testing for signifi-cance of increased correlation with human judgment.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 172?176,Doha, Qatar.Y.
Graham, T. Baldwin, A. Moffat, and J. Zobel.
2013.Continuous measurement scales in human evaluationof machine translation.
In Proc.
7th Linguistic An-notation Wkshp.
& Interoperability with Discourse,pages 33?41, Sofia, Bulgaria.Y.
Graham, T. Baldwin, A. Moffat, and J. Zobel.
2014a.Is machine translation getting better over time?
InProceedings of the European Chapter of the Associ-ation of Computational Linguistics, pages 443?451,Gothenburg, Sweden.Y.
Graham, N. Mathur, and T. Baldwin.
2014b.
Ran-domized significance tests in machine translation.
InProc.
Ninth ACL Wkshp.
Statistical Machine Transla-tion, pages 266?74, Baltimore, MD.A.L.
Han, D.F.
Wong, L.S.
Chao, Y. Lu, L. He, Y. Wang,and J. Zhou.
2013.
A description of tunable ma-chine translation evaluation systems in WMT13 met-rics task.
In Proceedings of the Eight Workshop onStatistical Machine Translation, pages 414?421, Sofia,Bulgaria.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proc.
of Empirical Meth-ods in Natural Language Processing, pages 388?395,Barcelona, Spain.M.
Machacek and O. Bojar.
2014.
Results of the wmt14metrics shared task.
In Proceedings of the Ninth Work-shop on Statistical Machine Translation, pages 293?301, Baltimore, USA.J.H.
Steiger.
1980.
Tests for comparing elements of acorrelation matrix.
Psychological Bulletin, 87(2):245.E.J.
Williams.
1959.
Regression analysis, volume 14.Wiley, New York, USA.1191
