Towards a Workbench for Acquisit ion ofDomain Knowledge from Natural  LanguageAndre i  M ikheev  and  Steven  F inchHCRC Language Technology GroupUniversity of Ed inburgh2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UKE-maih Andrei .MikheevQed.ac.ukAbstractIn this paper we describe an architectureand functionality ofmain components ofa workbench for an acquisition of do-main knowledge from large text corpora.The workbench supports an incrementalprocess of corpus analysis tarting froma rough automatic extraction and or-ganization of lexico-semantic regularitiesand ending with a computer supportedanalysis of extracted ata and a semi-automatic refinement of obtained hypo-theses.
For doing this the workbench em-ploys methods from computational lin-guistics, information retrieval and kno-wledge engineering.
Although the work-bench is currently under implementationsome of its components are already im-plemented and their performance is il-lustrated with samples from engineeringfor a medical domain.1 In t roduct ionOne of the standard methods for the extractionof domain knowledge (or domain schema in ano-ther terminology) from texts is known as Distribu-tional Analysis (Hirshman 1986).
It is based onthe identification of the sublanguage specific co-occurrence properties of words in the syntactic re-lations in which they occur in the texts.
These co-occurrence properties indicate important seman-tic characteristics of the domain: classes of ob-jects and their hierarchical inclusion, propertiesof these classes, relations among them, lexico-semantic patterns for referring to certain concep-tual propositions, etc.
This knowledge about do-main in the form it is extracted is not quite sui-table to be included into the knowledge base andrequire a post-processing of the linguistically trai-ned knowledge ngineer.
This is known as a con-ceptual analysis of the acquired lingistic data.
Ingeneral all this is a time consuming process andoften requires the help of a domain expert.
Ho-wever, it seems to be possible to automate sometasks and facilitate human intervention i manyparts using a combination of NLP and statisticaltechniques for data extraction, type oriented pat-terns for conceptual characterization f this dataand an intuitive user interface.All these resources are to be put together intoa Knowledge Acquisition Workbench (KAWB)which is under development a LTG of the Uni-versity of Edinburgh.
The workbench supportsan incremental process of corpus analysis star-ting from a rough automatic extraction and orga-nization of lexico-semantic regularities and endingwith a computer supported analysis of extracteddata and a refinement of obtained hypotheses.2 KAW Arch i tec tureThe workbench we are aiming at integrates com-putational tools and a user interface to supportphases of data extraction, data analysis and hypo-theses refinement.
The target domain descriptionconsists of words grouped into domain-specific se-mantic categories which can be further refinedinto a conceptual type lattice (CTL) and lexico-semantic patterns further efined into conceptualstructures as shown elsewhere in the paper.
KAWarchitecture is displayed in figure 1.A data extract ion module provides the kno-wledge engineer with manageable units of lexi-cal data (words, phrases etc.)
grouped togetheraccording to certain semantically important pro-perties.
The data extraction phase can be subdivi-ded into a stage of semantic category identificationand a stage of lexico-semantic pattern extraction.Both of these stages complement each other: thediscovery of semantic categories allows the systemto look for patterns and discovered patterns erveas diagnostic units for further extraction of thesecategories.
Thus both these activities can be ap-plied iteratively until a certain level of precisionand coverage is achieved.194Data Extraction ModuleTermclusteringtool---ICorpusIcase attachementrobust parsertaggerLinguistic analysis toolsWord Class Identifierf/ClustersrefinementtoolExternalsourcesaccessFuzzy IfLexical Pattern FinderCollocationidentificationtoolCluster RefinementtoolGeneralisationtoolfmatcherAnalysissupportIIIIIIIIIIAnalysis/Refinement ModuleTarget data structures;emantic Lexico-:ategodes SemanticpatternsConcept ConceptType StructLatticeThese data structures are initallyproduced by the proceduresdescribed above, and refined bythe analysis/refinement tooltobecome aconceptual type latticeand a set of frames.t~Figure 1: This figure shows main KAWB components and modules and SGML marked ata flow betweenthem.195The word class identification component en-compasses tools for the linguistic annotation oftexts, word clustering tools and tools for accessto external inguistic and semantic sources likethesauri, machine-readable dictionaries and lexi-cal data bases.
Statistical clustering can be au-tomatically checked and subcategorized with thehelp of external inguistic and semantic sources.The pat tern  finder component makes use ofphrasal annotations of texts produced by a ge-neral robust partial parser.
First, the corpus ischecked for stable phrasal collocations for singlewords and entire semantic lusters by a specialtool - a collocator.
After collocations are collec-ted another tool - a generalizer t ies automaticallydeduce regularities and contract multiple patternsinto their general representations.
Such patternsare then presented for a conceptual characteriza-tion to the knowledge ngineer and some prede-fined generic onceptual structures are suggestedfor specialization.The main aim of the analysis and ref inementmodule is to uncover and refine structural gene-ralities found in the previous phases.
It matchesin the text patterns which represent hypothesesof the knowledge ngineer, groups together andgeneralizes the found cases and presents them tothe knowledge ngineer for a final decision.
Thematcher evaluates how good a given piece of textmatches the pattern and returns matches at va-rious levels of exactness.If modules are to communicate flexibly then aninter-module information representation formatneeds to be specified.
Standard Generalized Mar-kup Language (Goldfab 1990) is an internationalstandard for marking up text.
We use SGML asa way of exchanging information between modulesin a knowledge acquisition system, and of storingthat information in persistent store when it hasbeen processed.In the rest of the paper we will embark on a moredetailed characterization f tools themselves.
We,however, will not present any technical details andsuggestions on an actual implementation becausethe workbench should be able to incorporate dif-ferent implementations .
Some of the tools axealready implemented while others still need im-plementation r reimplementation n terms of theopen architecture of the workbench.
For an illu-stration we have used samples from engineeringfor the cardiac-failure domain using OHSUMED(Hersh 1994) corpus and a corpus of patient di-scharge summaries ( PDS ) described in Mikheev1994.3 Linguistic AnnotationThe simplest form of linguistic description of thecontent of a machine-readable document is in theform of a sequence (or a set) of words.
More so-phisticated linguistic information comes in severalforms, all of which may need to be represented ifperformance in an automatic acquisition of lexicalregularities i to be improved.
The NLP moduleof the KAWB consists of a word tagger (e.g.
Ku-piec 1993), a specialized partial robust parser anda case attachment module.The tagger assigns categorial features to words.This is not a straightforward process due to thegeneral exical ambiguity of any natural angu-age but state-of-the-art taggers do this quite well(more than 97% correctness) using different stra-tegies usually based on an application of HiddenMarkov Models (HMMs).It is well-known that a general text parsing is veryfragile and ambiguous by its nature.
Syntacticambiguity can lead to hundreds of parses even forfairly simple sentences.
This is clearly inappro-priate.
However, general and full scale parsingis not required for many tasks of knowledge ac-quisition but rather a robust identification of cer-tain text segments i needed.
Among these seg-ments are compound noun phrases, verb phrasesetc.
To increase a precision of knowledge xtrac-tion in some cases it is quite important to resolvereferences of pronominal anaphora.
At the mo-ment in parsing sentences we are using a temporalexpressions recognizer, a noun-phrase r cognizer,a simple verb-phrase r cognizer and a simple ana-phoric binder.
This can be further extended totreat other phenomena of natural anguage, pro-viding that new components are robust and fast.The parser supplies information to a case atta-chement module.
This module using semanticallydriven role filler expectations for verbs provides amore precise attachment ofnoun phrases to verbs.To do this we are using ESK - an event and stateknowledge base (Whittemore 94) which for morethat 700 verbs contains information on thematicroles, semantic types for arguments, expected ad-juncts, syntactic information, propositional types,WordNet concept types and sense indices.4 Automatic PrecategorizationSemantic lustering of words from an underlyingcorpora llows the knowledge ngineer to find outmain semantic ategories or types which exist inthe domain in question and sort out the lexiconin accordance with these types.
It is importantboth that information about typology the know-ledge engineer adds to the system is accurate, andthat enough information is added.
In this regard,196the Zipf-Mandelbrot law, which states that thefrequency of the nth most frequent word in a na-tural language is (roughly) inversely proportionalto n. Thus the majority of word tokens appear ina small fraction of the possible word types.Finch & Chater (1991) show how it is possible toinfer a syntactic and semantic lassification of aset of words by analyzing how they are used ina very large corpus.
This is useful because verylarge corpora frequently exist for many domains.For example, in the medical domain, the freelyavailable OHSUMED corpus (Hersh 1994) containssome 40 million words of medical texts.
We nowdescribe this method for inferring a syntactic andsemantic lassification of words from scratch.Firstly, we measure the contexts in which wordsw E W occur, and define a statistically motiva-ted similarity measure between contexts of occur-rence of words to infer a similarity between words,d(wl, w2), wl, w2 E W. In our case the context isdefined to be a vector of word bigram statisticsacross the corpus for one and two words to theleft and right, thus representing each word to beclassified by a vector of bigram statistics.
Thenwe apply a classification procedure to produce ahierarchical single link clustering (or dendrogram)(Sokal &Sneath,  1963) of words which we use asa basis for further classification.
If this technique(as more fully described in Finch 1993) is appliedto the OHSUMED corpus, some of the structurewhich is uncovered is displayed in figure 2.
Thisfigure displays part of a 3,000 word dendrogramwhich can then be "cut" at an appropriate l vel toform a set of disjoint classes which can then be or-dered according to their frequency of occurrence.This gives the knowledge engineer a means toquickly and relatively accurately classify the mostfrequent vocabulary used in a particular domain.5 Lexico-Semantic PattrenAcquisit ionLexico-semantic patterns are structures where lin-guistic entries, semantic types and entire lexico-semantic patterns can be used in combinationsto denote certain conceptual propositions of theunderlying domain and cover certain sequences ofwords in the text.
Linguistic entries can be words,phrases and linguistic types, for example: "of" -word, <NP head = "infarction"> - a noun phrasewith the head-word "infarction", <SYNT type =N> - a noun etc.
Patterns themselves are the ba-sis for induction of conceptual structures.An example of a correspondence of many phra-ses to one lexico-semantic pattern is shown in fi-gure 3.
This pattern covers all strings which havea reference to a person followed by one of the li-sted verbs in any form followed by a compoundnoun with the head "infarction" and followed bya date expression.
In this pattern $PERSON and$DATE are patterns themselves and all other con-stituents are linguistic entries.
If instead of "inf-arction" we use a type \[DISEASE\] we can achieveeven broader coverage.
Also note that the $DATEconstituent is optional which is expressed by "?
".A conceptual structure which corresponds to thepattern adds more implicit information to the pat-tern.
For instance, it states explicitly that a bodycomponent which is a location of a disease belongsto the person who is an experiencer ofthat disease:\[@infarction:V\]--~ (is-a)--~\[@disease\]-+(expr)-~\[@person:y\]-+ (loc)--+\[@body-COMP\]+--(has) +-\[@person:*y\]--+ (cul)-+ \[@time-point\]From the NL Processing point of view lexico-semantic patterns provide a way for going aboutwithout the definition of a general semantics forevery word in the corpus.
Many commonsensewords take their particular meaning only in a con-text of domain categories and this can be expres-sed by means of lexico-semantic patterns.5.1 Co l tocatorThe collocator or multi-word term extraction mo-dule finds in the corpus significant co-occurrenceof lexical items (words and phrases) which con-stitute terminology.
Identified by the robust par-tial parser noun and verb groups which includedomain semantic ategories elicited at the preca-tegorization phase are collected together with fre-quencies of their appearance in the corpus.
Phra-ses are filtered through a list of general purposewords which is constructed separately for everynew domain.
Phrases which occur more oftenthan a threshold computed using Zipf-Mandelbrotlaw are saved for post-analysis.
Other phrases aredecomposed into constituents for recalculation ofsaved phrase weights as described in Mikheev 91.Many terms include other terms as their compo-nents.
This surface lexical structure correspondsto semantic relations between concepts represen-ted by these terms.
To uncover term inclusionthe system scans the term bank and replaces eachentry of a term which currently in focus with itsnumber.
Figure 4 displays an excerpt from collo-cations extracted from PDS corpus in the originalform and after term inclusion checking.5.2 Inner Context CategorizationThe major part of the terminology is usually re-presented by nouns or nominalizations.
Such197morphine~ indomethacinepinephrinepropranolol.
nifedipine_ verapamil_ diltiazem_.
halothane-.
isofluranebupivacainefentanyllidocainedexamethasone_~ amiodaronemethotrexatet-41disease~is ndrome orderdysfunctioninfectionfailureinjuryinfarctionobstructionfibrosistraumaillnessdeficiencysclerosismellitusrenalpulmonarycardiacmyocardialcerebralI ventricularcoronaryaorticvasculargastricarterialI venousrespiratoryI gastrointestinal IS----tacutechronicprimarylong-termnewmajormultiplevarioussinglesmalllar\[geearlylateFigure 2: This figure shows four sub-clusters of our hierarchical cluster analysis of the 3,000 mostfrequent words in the OHSUMED corpus (Hersh 1994).
It shows a subcluster of drugs (top left), disease-based nouns (top right), body-part adjectives (lower left), and condition modifying adjectives (lowerright).He had sufferedHe hadShe had hadMr.Mcdool sustainedShe developedan acute myocardial infarctiona true posterior myocardial infarctionan interior infarctiona small anterior myocardial infarctionan extensive myocardial infarctionin 1992.on 5th of November 1992...in 1985.in October 92.$PERSON <V head = {suffer, have, sustain, develop} > <NC head = "infarction"> {"on", "in"} $DATE?Figure 3: This figure shows a correspondence of many phrases to one lexico-semantic pattern.Num Freq$136 373$234 475$467 550$1109 17$1154 48$2574 21$2974 23$2980 46$3004 79Annotated Phrasemyocardial//BODY-PART infarction//DISEASEanterior myocardial//BODY-PART infarction//DISEASEinferior myoeardial//BODY-PART infarction//DISEASEestablished inferior myocardial//BODY-PART infarction//DISEASEhistory//INFORMATION of ischwemic heart//BODY-PART disease//DISEASEhistory//INFOR.MATION of an anterior myocardial//BODY-PART infarction//DISEASEmoderately severe stenosis//DISEASEaortic//BODY-PART valve//BODY-PART stenosis//DISEASEstenosis//DISEASE in the right coronary//BODY-PART artery//BODY-PARTFigure 4: This figure shows an excerpt from collocations extracted from PDS corpus and the result ofterm inclusion checking.198terms usually have a particular set of modifierswhich represent different properties.
The innercontext categorization is started with extractionof compound nouns from collected by the colloca-tor noun phrases.
Semantic ategories for manyadjectieval modifiers extracted at the word clu-stering phase are too general if any, but collectedcollocations and external lexical sources as, for ex-ample, WordNet can be used.First, we can sort terms with the same head-wordby length.
For example, for the type INFARCTIONthe systems orts terms as follows:myocardial infarction, old infarction, acute infarctionacute myocardial infarction, anterior myocardial inf-arction...further anterior myocardial infarction...Then we separate pure adjectival modifiers fromadjectivized nouns:infarction : inferior, old, acute, post, further, antero-lateral, lateral, infero-posterior, antero-septal, repea-ted, significant, large, limited / /  myocardial, dia-phragmatic, subendocardialmyocardial infarction : anterior, first, extensive,minor, small, previous, posterior, suspected.Next we cluster pure adjectival modifiers intogroups using synonym-antonym information avai-lable in WordNet.
However, it is not necessarilythe case that related adjectives are stated togetherin one WordNet entry.
Sometimes there is an in-direct link between adjectives.
Also, since quiteoften WordNet gives semantically unrelated (in agiven domain) adjectives together we use a heu-ristic rule which says that if two adjectives areused together in one phrase they don't hold syno-nymy//antonymy relation.The system assumes that if there is at least oneword in common in WordNet entries for two dif-ferent adjectives they can be clustered together.In our example for the type INFARCTION the fol-lowing clusters were automatically obtained:duster  1: chronic vs. acute;duster  2: major, extensive, significant, large, old vs.minor, small, limited;cluster 3: post vs. previous, ensuing;cluster 4: anterior vs. posterior;duster  5: inferior vs. superior;rest: suspected; lateral; recent; further; repeated;As we see all clusters look fairly plausible exceptthe single adjective "old" which was misclassified;it stands for a temporal property of an infarctionrather than its spreading at a myocardium.This algorithm is gradually applied to al\]~ entriesfrom the term bank and the knowledge ngineer ispresented with the results.
This method was fairlysuccessfully used in our experiment, however, alarge-scale valuation of sense discrimination forconstituent words is still needed to be done.5.3 Outer  Context  GeneralizerThe lexico-semantic generalizer is a tool whichextracts general lexico-semantic patterns in anempirical, corpus-sensitive manner analogous tothat used to automatically extract word classdendrograms.
From the multi-word term bankcollected by the collocation tool, we derive se-mantic frames by replacing each content word ineach phrase by its semantic ategory, derived eit-her empirically from the word-level dendrogramin the case of frequent words, or derived fromWordNet in the case of less frequent words (asdescribed above).
We also part of speech tagevery word in the phrase.
Therefore, the term"myocardial infarction" might become "BODY-PART<adj> DISEASE<noun/s>", as might "ga-strointestinal obstruction" or "respiratory fai-lure".
Another example might be the assignmentof "DISEASE<noun/s> ofBODYPART<noun/pl>"to "obstruction of arteries" (function words suchas "of" are usually not further subcategorized,since they convey structural information in them-selves).
Thus we map the term bank to a set ofparadigms, and we choose the set of paradigmswhich appear most frequently for clustering.Clustering proceeds by mapping words in the cor-pus to their semantic ategory (augmented withpart-of-speech information), and clustering in thesame way as we did for words, except that thecontext vectors are recorded for the set of frequentsemantic paradigms.
For infrequent words wherethe empirical method for finding semantic classcan't be applied, the WordNet technique descri-bed above is used.
When this is done, we get aclustering of short lexico-semantic paradigms.Once this is achieved, we can again apply the samemethodology to find patterns of higher level whichinclude patterns themselves.
In our notation, werefer to singe word semantic ategories as upper-case labels (which we choose as being descriptiveof the class which has been discovered), simple se-quences of semantic ategories by a preceding "$",and a sequence of sequences by a preceding "$$".These higher level patterns can be clustered in thesame way to yield longer semantic sequence para-digms.
Figure 5 illustrates generalizations for thetypes $BODY-PART and $$DISEASE.5.4 Analysis Support  ToolType oriented analysis is facilitated with genericconceptual structures which are different for diffe-199Pattern Structure for $BODY-PARTBODY-PART< adj > BODY-PART< noun/s >LOCATION< adj > BODY-PART< noun/s >LOCATION< adj > LOCATION< adj > BODY-PART< noun/s >Examplesaortic valveleft heartleft descending arteryI Pattern Structure for $$DISEASE$BODY-PART DISEASE< noun/s >DISEASE< noun/s > "in" $DATE$BODY-PART DISEASE< noun/s > "in" SDATEDISEASE< noun/s > "of" $BODY-PARTantero-septal myocardial infarctioninfarction in December 1987myocardial infarction in December 1987occlusion of arteryFigure 5: This figure shows results of generalization for the types SBODY-PART and $$DISEASE.rent conceptual types (as more fully described inMikheev & Moens 1994).
For example, a type ori-ented structure for eventualities includes their the-matic roles (agent, theme ...), temporal links andproperties while a type-oriented structure for ob-jects includes their components, parts, areas andproperties.
The system recognizes which structureshould be used and presents it to the knowledgeengineer with optional explanations or a questionguided strategy for filling it up.6 Hypotheses  Ref inementA fuzzy matcher is a tool which uses a sophisti-cated pattern-matching language to extract textfragments at various levels of exactness.
It mat-ches in the text patterns which represent hypo-theses of the knowledge ngineer, groups togetherand generalizes cases which have been discoveredand presents them to the knowledge ngineer fora final decision.Patterns themselves can be quite complex con-structions which can include strings, words, ty-pes, precedence relations and distance specifiers.In the simpliest case the knowledge ngineer canexamine a context for occurrences for a word ora type provided that the type exists in the termbank as represented in figure 6.More complex patterns can be used for thedescription of complex groups.
For instance,there a request can be made to find all co-occurrences of the type DISEASE with the typeBODY-COMPONENT when they are at the samestructural group (noun phrase or verb phrase) andthe disease is a head of the group:{ \[disease\].
< >\[body-component\] }curly brackets impose a context of a structuralgroup, the "."
means that the words can be dis-tributed in the group, <> means that the compo-nent can be both to the left and to the right, andsince the DISEASE is the first element of the pat-tern it is assumed to be the head.
The programmatches this pattern into the following entries:myocardial infarction, infarction of myocardium, ste-nosis at the origin of left coronary artery...To be powerful enough for our purposes this pat-tern language should be quite complex and it isimportant to provide an easy way for specificationof such patterns with a question-guided process.7 Externa l  Sources  AccessAlready existing lexical databases are an im-portant source of information about constituentwords of domain texts.
KAWB provides genericfacilities for access to such linguistic sources.
Foreach source a converter which transforms ourceinformation into SGML marked data, which thencan be used in the workbench, should be written.For some domains there already exist terminolo-gical banks available on-line.
These banks varyin their linguistic coverage- some list all possi-ble forms (singular, plural etc.)
for terms whileothers just a canonical one, and in a conceptualcoverage - some provide an extensive set of diffe-rent relations among terms (concepts) others justa subsumption hierarchical inclusion.
In ourimplementation we used Unified Medical Langu-age System (UMLS) and WordNet (Beckwith et al1990) - a publicly available lexical database, ho-wever we haven't provided the generic support foran abstract hesaurus yet.8 Conc lus ionThe workbench outlined in this paper encompas-ses a number of tools which facilitate different sta-ges of knowledge xtraction, analysis and refine-ment based on corpus processing paradigm.
Thesetools are integrated into a coherent workbenchwith a common inter-module data flow interfacebased on SGML.
Thus the workbench can easilyintegrate new tools and upgrade xisting ones.The general approach to knowledge acquisitionsupported by the workbench is a combination ofmethods used in knowledge ngineering, informa-200developed an anterior myocardialan established inferior myocardialan acute inferior myocardialsubsequent episodes of unstablehe has experienced unstableinfarction from whichinfarction .
Theinfarction with CHBangina including anangina and wasFigure 6: This figure shows an excerpt from a search for the type DISEASE with a distance four to theleft and two to the right.tion retrieval and computational linguistics.ReferencesBeckwith, R., C. Fellbaum, D. Gross and G.A.
Miller (1990) WordNet: A lexical data-base organized on psycholinguistic principles.CSL Report 42, Cognitive Science Labora-tory, Princeton University, Princeton.Cutting, D., J. Kupiec, J. Pedersen and P. Sibun(1993) Beta test version of the Xerox tagger.Xerox Palo Alto Reseach Center, Palo Alto,Ca.Finch, S. and N. Chater (1991) A hybrid approchto learning syntactic ategories.
AISB Quart-erly 8(4), 35-41.Finch, S. P. (1993) Finding Structure in Langu-age.
PhD thesis, Centre for Cognitive Science,University of Edinburgh, Edinburgh.Goldfarb, C. F. (1990) The SGML Handbook.
Ox-ford: Clarendon Press.Health, U S. D.of (1993) UMLS Knowledge Sour-ces.
Washington: National Library of Medi-cine.Hersh, W. (1994) An interactive retrieval evalua-tion and a new large test collection for rese-arch.
In W. B. Croft and C. J. van Rijsbergen,eds., Proceedings of the 17th Annual Interna-tional Conference onResearch and Develop-ment in Information Retrieval, pp.
192-202.Hirschman, L. (1986) Discovering sublanguagestructures.
In R. Grishman and R.
Kittredge,eds., Analyzing Language in Restricted Do-mains: Sublanguage Description and Proces-sing, pp.
211-234.
Hillsdale, N.J.: LawrenceErlbaum Associates.Mikheev, A. and M. Moens (1994) Acquiring andRepresenting Background Knowledge for aNLP System.
In Proceedings of the AAAI FallSymposium.Mikheev, A.
(1991) A cognitive system .for con-ceptual knowledge xtraction from NL texts.PhD thesis, Computer Science, Moscow Insti-tute for Radio-Engineering and Automation,Moscow.Sokal, R. R. and P. H. A. Sneath (1963) Principlesof Numerical Taxonomy.
San Fransisco: W.H.
Freeman.Whittemore, G. and J. Hicks (1994) ESK: Eventand state knowledge base.
In AAAI Fall Sym-posium.201
