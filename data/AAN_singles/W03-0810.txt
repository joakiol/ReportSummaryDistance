Accelerating Corporate Research in the Development, Applicationand Deployment of Human Language TechnologiesDavid FerrucciIBM T.J. Watson Research CenterYorktown Heights,  NY 10598ferrucci@us.ibm.comAdam LallyIBM T.J. Watson Research CenterYorktown Heights,  NY 10598alally@us.ibm.comAbstractIBM Research has over 200 people workingon Unstructured Information Management(UIM) technologies with a strong focus onHLT.
Spread out over the globe they are en-gaged in activities ranging from natural lan-guage dialog to machine translation tobioinformatics to open-domain question an-swering.
An analysis of these activitiesstrongly suggested that improving the organi-zation?s ability to quickly discover eachother's results and rapidly combine differenttechnologies and approaches would acceleratescientific advance.
Furthermore, the ability toreuse and combine results through a commonarchitecture and a robust software frameworkwould accelerate the transfer of research re-sults in HLT into IBM?s product platforms.Market analyses indicating a growing need toprocess unstructured information, specificallymulti-lingual, natural language text, coupledwith IBM Research?s investment in HLT, ledto the development of middleware architecturefor processing unstructured informationdubbed UIMA.
At the heart of UIMA arepowerful search capabilities and a data-drivenframework for the development, compositionand distributed deployment of analysis en-gines.
In this paper we give a general intro-duction to UIMA focusing on the designpoints of its analysis engine architecture andwe discuss how UIMA is helping to accelerateresearch and technology transfer.1 Architecture GoalsIn six major labs spread out over the globe, IBM Re-search has over 200 people working on UnstructuredInformation Management (UIM) technologies with asignificant focus on Human Language Technologies(HLT).
These researchers are engaged in activities rang-ing from natural language dialog to machine translationto bioinformatics to open-domain question answering.Each group is developing different technical and engi-neering approaches to process unstructured information(e.g., natural language text, voice, audio and video) inpursuit of specific research objectives and their applica-tions.The high-level objectives of IBM?s Unstructured In-formation Management Architecture (UIMA) are twofold:1) Accelerate scientific advances by enabling therapid combination UIM technologies (e.g., natu-ral language processing, video analysis, infor-mation retrieval, etc.
).2) Accelerate transfer of UIM technologies toproduct by providing a robust software frame-work that promotes reuse and supports flexibledeployment options.UIMA is a software architecture for developing ap-plications which integrate search and analytics over acombination of structured and unstructured information.We define structured information as information whoseintended meaning is unambiguous and explicitly repre-sented in the structure or format of the data.
The ca-nonical example is a database table.
We defineunstructured information as information whose intendedmeaning is only implied by its form.
The canonical ex-ample is a natural language document.The UIMA high-level architecture, illustrated inFigure 1, defines the roles, interfaces and communica-tions of large-grained components essential for UIMapplications.
These include components capable of ana-lyzing unstructured artifacts, integrating and accessingstructured sources and storing, indexing and searchingfor artifacts based on discovered semantic content.As part of the UIMA project, IBM is developing dif-ferent implementations of the architecture suitable fordifferent classes of deployment.
These range from light-weight and embeddable implementations to highlyscaleable implementations that are meant to exploitclusters of machines and provide high throughput andhigh availability.While the architecture extends to a variety of un-structured artifacts including voice, audio and video, aprimary analytic focus of current UIMA implementa-tions is squarely on human language technologies.In this paper we will refer to elements of unstruc-tured information processing as documents admitting,however, that an element may represent for the applica-tion, a whole text document, a text document fragmentor even multiple documents.2 Generalized Application Scenario andthe High-Level ArchitectureIn this section we provide a high-level overview of theUIMA architecture by describing its component roles ina generalized application scenario.The generalized scenario includes both analysis andaccess functions.
Analysis functions are divided intotwo classes, namely document-level and collection-levelanalysis.
Access functions are divided into semanticsearch and structured knowledge access.We refer to the software program that employsUIMA components to implement some end-user capa-bility as the application or application program.2.1 Document-Level AnalysisDocument-level analysis is performed by componentprocessing elements named Text Analysis Engines(TAEs).
These are extensions of the generic analysisengine, specialized for text.
They are analogous, forexample, to Processing Resources in the GATE archi-tecture (Cunningham et al, 2000).
In UIMA, a TAE is arecursive structure which may be composed of sub orcomponent engines each performing a different stage ofthe application?s analysis.ApplicationLogicSemantic Search EngineDocument, Collection& Meta-data Store(Text) Analysis Engines (TAEs)Structured Knowledge AccessQuery key words and conceptsToken and concept IndexingKnowledge Source Adapters (KSAs)integrate and deliver content frommany structured knowledge sourcesaccording  to central ontologiesCollectionProcessingManager(CPM)StructuredInformationUnstructuredInformationAnalysis engines employing a varietyof analytical techniques andstrategies for detecting semanticcontentRelevantKnowledgeCollectionAnalysisEngine(s)Figure 1: UIMA High-Level ArchitectureExamples of Text Analysis Engines include lan-guage translators, document summarizers, documentclassifiers, and named-entity detectors.
Each TAE spe-cializes in discovering specific concepts (or "semanticentities") otherwise unidentified or implicit in thedocument text.A TAE takes in a document and produces an analy-sis.
The original document and its analysis are repre-sented in a common structure called the CommonAnalysis System or CAS.
The CAS is conceptuallyanalogous to the annotations in other architectures, be-ginning with TIPSTER (Grishman, 1996).In general, annotations associate some meta-datawith a region in the original artifact.
Where the artifactis a text document, for example, the annotation associ-ates meta-data (e.g., a label) with a span of text in thedocument by giving the span?s start and end positions.Annotations in the CAS are stand-off, meaning that theannotations are maintained separately from the docu-ment itself; this is more flexible than inline markup(Mardis and Burger, 2002).
In UIMA, annotations arenot the only type of information stored in the CAS.
TheCAS may be used to represent any class of meta-dataelement associated with analysis of a document regard-less of whether it is explicitly linked to some sub com-ponent of the original document.
The CAS also allowsfor multiple definitions of this linkage, as is necessaryfor the analysis of images, video or other modalities.The analysis represented in the CAS may be thoughtof as a collection of meta-data that is enriched as itpasses through successive stages of analysis.
At a spe-cific stage of analysis, for example, the CAS may in-clude a deep parse.
A named-entity detector receivingthis CAS may consider the deep parse to identify namedentities.
The named entities may be input to an analysisengine that produces summaries or classifications of thedocument.The UIMA CAS object provides general object-based representation with a hierarchical type systemsupporting single inheritance.
It includes data creation,access and serialization methods designed for the effi-cient representation, access and transport of analysisresults among TAEs and between TAEs and otherUIMA components or applications.
Elements in theCAS may be indexed for fast access (Goetz et al,2001).
The CAS has been implemented in C++ andJava with serialization methods for binary as well asXML formats for managing the tradeoff between effi-ciency and interoperability.2.2 Collection-Level AnalysisDocuments are gathered by the application and organ-ized into collections.
The architecture defines a Collec-tion Reader interface.
Implementations of the CollectionReader provide access to collection elements, collectionmeta-data and element meta-data.
UIMA implementa-tions include a document, collection and meta-data storethat implements the Collection Reader interface andmanages multiple collections and their elements.
How-ever, applications that need to manage their own collec-tions can provide an implementation of a CollectionReader to UIMA components that require access to col-lection data.Collections are analyzed to produce collection levelanalysis results.
These results represent aggregate infer-ences computed over all or some subset of the docu-ments in a collection.
The component of an applicationthat analyzes an entire collection is considered a Collec-tion Analysis Engine.
These engines typically applyelement-level, or more specifically document-levelanalysis, to elements of a collection and then consider-ing the element analyses in performing aggregate com-putations.Examples of collection level analysis results includesub collections where elements contain certain features,glossaries of terms with their variants and frequencies,taxonomies, feature vectors for statistical categorizers,databases of extracted relations, and master indices oftokens and other detected entities.In support of Collection Analysis Engines, UIMAdefines the Collection Processing Manager (CPM)component.
The CPM?s primary responsibility is tomanage the application of a designated TAE to eachdocument accessible through a Collection Reader.
ACollection Analysis Engine may provide, as input to theCPM, a TAE and a Collection Reader.
The CPM appliesthe TAE and returns the analysis, represented by a CAS,for each element in the collection.
To control the proc-ess, the CPM provides administrative functions thatinclude failure reporting, pausing and restarting.At the request of the application?s collection analy-sis engine, the CPM may be optionally configured toperform functions typical of UIM application scenarios.Examples of these include:1) Filtering - ensures that only certain elementsare processed based on meta-data constraints.2) Persistence - stores element-level analysis re-sults in a provided Collection Writer.3) Indexing - indexes documents using a desig-nated search engine indexing interface basedon meta-data extracted from the analysis.4) Parallelization - manages the creation and exe-cution of multiple instances of a TAE for proc-essing multiple documents simultaneouslyutilizing available computing resources.2.3 Semantic SearchTo support the concept of ?semantic search?
?
the capa-bility to find documents based on semantic content dis-covered by document or collection level analysis andrepresented as annotations ?
UIMA specifies searchengine indexing and query interfaces.A key feature of the indexing interface is that it sup-ports the indexing of tokens as well as annotations andparticularly cross-over annotations.
Two or more anno-tations cross-over one another if they are linked to inter-secting regions of the document.The key feature of the query interface is that it sup-ports queries that may be predicated on nested structuresof annotations and tokens in addition to Boolean combi-nations of tokens and annotations.2.4 Structured Knowledge AccessAs analysis engines do their job they may consult awide variety of structured knowledge sources.
To in-crease reusability and facilitate integration, UIMAspecifies the Knowledge Source Adapter (KSA) inter-face.KSA objects provide a layer of uniform access todisparate knowledge sources.
They manage the techni-cal communication, representation language and ontol-ogy mapping necessary to deliver knowledge encodedin databases, dictionaries, knowledge bases and otherstructured sources in a uniform way.
The primary inter-face to a KSA presents structured knowledge as instan-tiated predicates using the Knowledge InterchangeFormat (KIF) encoded in XML.A key aspect of the KSA architecture is the KSAmeta-data and related services supporting KSA registra-tion and search.
These services include the descriptionand registration of named ontologies.
Ontologies aredescribed by the concepts and predicates they include.The KSA is self-descriptive and among other meta-dataincludes the predicate signatures belonging to registeredontologies that the KSA can instantiate and the knowl-edge sources it consults.Application or analysis engine developers can con-sult human browseable KSA directory services to searchfor and find KSAs that instantiate predicates of a regis-tered ontology.
The service will deliver a handle to aweb service or an embeddable KSA component.3 Analysis Engine FrameworkThis section takes a closer look at the analysis engineframework.UIMA specifies an interface for an analysis engine;roughly speaking it is ?CAS in?
and ?CAS out?.
Thereare other operations used for filtering, administrativeand self-descriptive functions, but the main interfacetakes a CAS as input and delivers a CAS as output.Any program that implements this interface may beplugged in as an analysis engine component in an im-plementation of UIMA.
However, as part of UIMAtooling we have developed an analysis engine frame-work to support the creation, composition and flexibledeployment of primitive and aggregate analysis engineson a variety of different system middleware platforms.The underlying design philosophy for the AnalysisEngine framework was driven by three primary princi-ples:1) Encourage and enable component reuse.2) Support distinct development roles insulatingthe algorithm developer from system anddeployment details.3) Support a flexible variety of deployment op-tions by insulating lower-level system middle-ware APIs.3.1 Encourage and Enable Component ReuseWith many HLT components being developed through-out IBM Research by independent groups, encouragingand enabling reuse is a critical design objective toachieve expected efficiencies and cross-group collabo-rations.
Three characteristics of the analysis engineframework address this objective:1) Recursive Structure2) Data-Driven3) Self-DescriptiveAnnotator(Annotator DeveloperImplements)CAS(part of framework)Controller(part of framework)process(Result Spec.
)Primitive Analysis Engineprocess(Result Spec.
)process(Result Spec.
)getCAS()getCAS()reads/writesanalysis dataFigure 2: Primitive Analysis EngineRecursive Structure.
A primitive analysis engine, illus-trated in Figure 2, is composed of an Annotator and aCAS.
The annotator is the object that implements theanalysis logic (e.g.
tokenization, grammatical parsing,entity detection).
It reads the original document contentand meta-data from the CAS.
It then computes andwrites new meta-data to the CAS.
An aggregate analy-sis engine, illustrated in Figure 3, is composed of two ormore component analysis engines, but implements ex-actly the same external interface as the primitive engine.At run-time an aggregate analysis engine is given a se-quence in which to execute its component engines.
Acomponent called the Analysis Structure Broker ensuresthat each component engine has access to the CAS ac-cording to the specified sequence.
Like any nested pro-gramming model, this recursive structure ensures thatcomponents may be easily reused in combination withone another while insulating their internal structure.Data-Driven.
An analysis engine?s processingmodel is strictly data-driven.
This means that an anno-tator?s analysis logic may be predicated only on the con-tent of its input and not on the specific analysis enginesit may be combined with or the control sequence inwhich it may be embedded.
This restriction ensures thatan analysis engine may be successfully reused in differ-ent aggregate structures and different control environ-ments as long as its input requirements are met.The Analysis Sequencer is a component in theframework responsible for dynamically determining thenext analysis engine to receive access to the CAS.
TheAnalysis Sequencer is distinct from the Analysis Struc-ture Broker, whose responsibility is to deliver the CASto the next analysis engine whichever it is wherever itmay be located.
The Analysis Sequencer?s control logicis separate from the analysis logic embedded in an An-notator and separate from the Analysis Structure Bro-ker?s concerns related to ensuring and/or optimizing theCAS transport.
This separation of concerns allows forthe plug-n-play of different Analysis Sequencers.
TheAnalysis Sequencer is a pluggable range from providesimple iteration over a declaratively specified staticflow to complex planning algorithms.
Current imple-mentations have been limited to simple linear flowsbetween analysis engines; however more advanced ap-plications are generating requirements for dynamic andadaptive sequencing.
How much of the control specifi-cation ends up in a declarative representation and howmuch is implemented in the sequencer for these ad-vanced requirements is currently being explored.Self-Descriptive.
Ensuring that analysis enginesmay be easily composed to form aggregates and may bereused in different control sequences is necessary fortechnical reusability but not sufficient for enabling andvalidating reuse within a broad community of develop-ers.
To promote reuse, analysis engine developers mustbe able to discover which analysis engines are availablein terms of what they do ?
their capabilities.Each analysis engine's data model is declared inXML and then dynamically realized in the CAS at run-time, an approach similar to MAIA (Laprun et al,2002).
In UIMA, however, analysis engines publishtheir input requirements and output specifications rela-tive to this declared data model, and this information isused to register the analysis engine in an analysis enginedirectory service.
This service includes a human-oriented interface that allows application developers tobrowse and/or search for analysis engines that meettheir needs.While self-description and related directory serviceswill promote reuse, their value is still dependent on es-tablishing common data models (or fragments thereof)to which analysis engine capability descriptions sub-scribe.3.2 Support Distinct Development RolesLanguage technology researchers that specialize in, forexample, multi-lingual machine translation, may not beAnalysisEngine 1CASAnalysis Structure BrokerAggregate Analysis Engineprocess(RS)AnalysisEngine 2process(RS) process(RS)Analysis SequencerProcess(Result Spec.
)init (Result Spec.
)read/write analysis dataControllerprocess(Result Spec.
)AnalysisEngine 3getCAS()next()Figure 3: Aggregate Analysis Enginehighly trained software engineers nor be skilled in thesystem technologies required for flexible and scaleabledeployments.
Yet one of the primary objectives of theUIMA project is to ensure that their work can be effi-ciently deployed in robust and scaleable system archi-tecture.Along the same lines, researchers with ideas abouthow to combine and orchestrate different componentsmay not themselves be algorithm developers or systemsengineers, yet we need to enable them to rapidly createand validate ideas through combining existing compo-nents.Finally, deploying analysis engines as distributed,highly available services or as collocated objects in anaggregate system requires yet another skill.As a result we have identified the following devel-opment roles and have designed the architecture withindependent sets of interfaces in support of each ofthese different skill sets.
Our separation of developmentroles is analogous to the separation of roles in Sun'sJ2EE platform (Sun Microsystems, 2001).Annotator Developer.
The annotator developer roleis focused on developing core algorithms ranging fromstatistical language recognizers to rule-based named-entity detectors to document classifiers.The framework design ensures that the annotatordeveloper need NOT develop code to address aggregatesystem behavior or systems issues like interoperability,recovery, remote communications, distributed deploy-ment, etc., but instead allow them to focus squarely onthe algorithmic logic and the logical representation oftheir results.This was achieved through the analysis engineframework by requiring the annotator developer to un-derstand only three interfaces, namely the Annotator,AnnotatorContext, and CAS interfaces.
The annotatordeveloper performs the following steps:1) Implement Annotator interface2) Encode analysis algorithm using the CAS inter-face to read input and write results and the An-notatorContext interface to access resources3) Write Analysis Engine Descriptor4) Call Analysis Engine FactoryTo embed an analysis algorithm in the framework,the annotator developer implements the Annotator inter-face.
This interface is simple and requires the imple-mentation of only two methods: one for initializationand one to analyze a document.It is only through the CAS that the annotator devel-oper accesses input data and registers analysis results.The CAS contains the original document (the subject ofanalysis) plus the meta-data contributed by any analysisengines that have run previously.
This meta-data mayinclude annotations over elements of the original docu-ment.
The CAS input to an analysis engine may residein memory, be managed remotely, or shared by othercomponents.
These issues are of concern to the analysisengine deployer role, but the annotator developer is in-sulated from these issues.All external resources, such as dictionaries, that anannotator needs to consult are accessed through the An-notator Context interface.
The exact physical manifes-tation of the data can therefore be determined by thedeployer, as can decisions about whether and how tocache the resource data.The annotator developer completes an XML descrip-tor that identifies the input requirements, output specifi-cations, and external resource dependencies.
Given theannotator object and the descriptor, the framework?sAnalysis Engine Factory returns a complete analysisengine.Analysis Engine Assembler.
The analysis engineassembler creates aggregate analysis engines throughthe declarative coordination of component engines.
Thedesign objective is to allow the assembler to build anaggregate engine without writing any code.The analysis engine assembler considers availableengines in terms of their capabilities and declarativelydescribes flow constraints.
These constraints are cap-tured in the aggregate engine?s XML descriptor alongwith the identities of selected component engines.
Theassembler inputs this descriptor in the framework?sanalysis engine factory object and an aggregate analysisengine is created and returned.Analysis Engine Deployer.
The analysis engine de-ployer decides how analysis engines and the resourcesthey require are deployed on particular hardware andsystem middleware.
UIMA does not provide its ownspecification for how components are deployed, nordoes it mandate the use of a particular type of middle-ware or middleware product.
Instead, UIMA aims togive deployers the flexibility to choose the middlewarethat meets their needs.3.3 Insulate Lower-Level System MiddlewareHLT applications can share many requirements withother types of applications ?
for example, they mayneed scalability, security, and transactions.
Existingmiddleware such as application servers can meet manyof these needs.
On the other hand, HLT applicationsmay need to have a small footprint so they can be de-ployed on a desktop computer or PDA or they may needto be embeddable within other applications that use theirown middleware.One design goal of UIMA is to support deploymentof analysis engines on any type of middleware, and toinsulate the annotator developer and analysis engineassembler from these concerns.
This is done throughthe use of Service Wrappers and the Analysis StructureBroker.
The analysis engine interface specifies thatinput and output are done via a CAS, but it does notspecify how that CAS is transported between compo-nent analysis engines.
A service wrapper implementsthe CAS serialization and deserialization necessary for aparticular deployment.
Within an aggregate AnalysisEngine, components may be deployed using differentservice wrappers.
The Analysis Structure Broker is thecomponent that transports the CAS between these com-ponents regardless of how they are deployed.To support a new type of middleware, a new servicewrapper and an extension to the Analysis Structure Bro-ker must be developed and plugged into the framework.The Analysis Engine itself does not need to be modifiedin any way.For example, we have implemented Service Wrap-pers and Analysis Structure Broker on top of both aweb-services and a message queuing infrastructure.Each implementation has different pros and cons fordeployment scenarios.4 Measuring SuccessWe have considered four ways to evaluate UIMA inmeeting its intended objectives:1) Combination Experiments2) Compliant Components and their Reuse3) System Performance Improvements4) Product Integration4.1 Combination ExperimentsOur UIMA implementations and associated tooling haveled to the design of several combination experimentsthat were previously unimagined or considered toocumbersome to implement.
This work has only just be-gun but includes collaborative efforts combining rule-based parsers with statistical machine translation en-gines, statistical named-entity detectors in rule-basedquestion answering systems (Chu-Carroll et al, 2003),and a host of independently developed analysis capabili-ties in bioinformatics applications.4.2 Compliant Components and their ReuseAnother way to measure the impact of UIMA is to lookat its adoption by the target community.
This may bemeasured by the number of compliant components andthe frequency of their reuse by different projects/groups.We have developed a registry and an integration testbed where contributed components are tested, certified,registered and made available to the community fordemonstration and download.The integration test bed includes a web-based facil-ity where users can select from a collection of corpora, acollection of certified analysis engines and run theanalysis engine on the corpus.
Performance statisticsbreaking down the time spent in analysis, communica-tions between components, and framework overhead arecomputed and presented.
The system generates analysisresults and stores them.
Analysis results may be viewedusing any of a variety of CAS viewers.
The results mayalso be indexed by a search engine, which may then beused to process queries.While we are still instrumenting this site and gather-ing reuse data, within six months of an internal distribu-tion we will have over 15 UIMA-compliant analysisengines, many of which are based on code developed aspart of multi-year HLT research efforts.
Engines werecontributed from six independent and geographicallydispersed groups.
They include several named-entitydetectors of both rule-based and statistical varieties,several classifiers, a summarizer, deep and shallowparsers, a semantic class detector, an Arabic to Englishtranslator, an Arabic person detector, and several bio-logical entity and relationship detectors.
Several of theseengines have been assembled using component enginesthat were previously inaccessible for reuse due to engi-neering incompatibilities.4.3 System Performance ImprovementsWe expect the architecture?s support for modularizationand for the insulation of system deployment issues fromalgorithm development to result in opportunities toquickly deploy more robust and scaleable solutions.For example, IBM's Question Answering system,now under development for over three years, includes ahome-grown answer type detector to analyze large cor-pora of millions of documents (Prager et al, 2002).With a half day of training, an algorithm developer castthe answer type detector as a UIMA Annotator and em-bedded it in the UIMA Analysis Engine framework.
Theframework provided the infrastructure necessary forconfiguring an aggregate analysis engine with the an-swer type detector down-stream of a tokenizer andnamed-entity detector without additional programming.Within a day, the framework was used to build the ag-gregate analysis engine and to deploy multiple instancesof it on a host of machines.
The result was a dramaticimprovement in overall throughput.4.4 Product IntegrationIBM develops a variety of information management,information integration, data mining, knowledge man-agement and search-related products and services.
Un-structured information processing and languagetechnologies in particular represent an increasingly im-portant capability that can enhance all of these products.UIMA will be a business success for IBM if it playsan important role in technology transfers.
IBM Researchhas engaged a number of product groups which are real-izing the benefits of adopting a standard architecturalapproach for integrating HLT that does not constrainalgorithm invention and that allows for easy extensionand integration and support for a wide variety of systemdeployment options.5 ConclusionThe UIMA project at IBM has encouraged many groupsin six of the Research division?s labs to understand andadopt the UIMA architecture as a common conceptualfoundation for classifying, describing, developing andcombining HLT components in aggregate applicationsthat integrate search and analytical functions.While our measurements are only just beginning, theadoption of UIMA has clearly improved knowledgetransfer throughout the organization.
Implementationsof the architecture are advancing and are beginning todemonstrate that HLT components developed withinResearch can be quickly combined to explore hybridapproaches as well as to rapidly transfer results intoIBM product and service offerings.IBM product groups are encouraged by Research?seffort and are committed to leverage UIMA as a vehicleto embed HLT components.
They are realizing thebenefits of having Research adopt a standard architec-tural approach that does not constrain algorithm inven-tion while allowing for a wide variety of systemdeployment options.
Product and service groups areseeing an easier path to combine, integrate and deliverthese technologies into information integration, datamining, knowledge management and search-relatedproducts and services.AcknowledgementsWe acknowledge the contributions of Dan Gruhl and theWF project to the development of UIMA.
In additionwe acknowledge David Johnson, Thomas Hampp, ThiloGoetz and Oliver Suhre in the development of IBM?sText Analysis Framework and the work of Roy Byrdand Mary Neff in the design of the Talent system.
Theirwork continues to influence the UIMA CAS and analy-sis engine framework.This work was supported in part by the AdvancedResearch and Development Activity (ARDA)?s Ad-vanced Question Answering for Intelligence(AQUAINT) Program under contract number MDA904-01-C-0988.ReferencesKaling Bontcheva, Hamish Cunningham, Valentin Tab-lan, Diana Maynard, Horacio Saggion.
2002.
?De-veloping Reusable and Robust Language ProcessingComponents for Information Systems using GATE.
?3rd International Workshop on Natural Language andInformation Systems (NLIS'2002), IEEE ComputerSociety Press.Jennifer Chu-Carroll, David Ferrucci, John Prager, andChristopher Welty.
2003.
"Hybridization in QuestionAnswering Systems."
Working Notes of the AAAISpring Symposium on New Directions in QuestionAnswering, to appear.Hamish Cunningham, Kaling Bontcheva, Valentin Tab-lan and Yorick Wilks.
2000.
?Software Infrastructurefor Language Resources: a Taxonomy of PreviousWork and a Requirements Analysis.?
Proceedings ofthe Second Conference on Language ResourcesEvaluation.Ralph Grishman.
1996.
?Tipster architecture designdocument version 2.2.?
Technical report, DARPATIPSTER.Thilo Goetz, Robin Lougee-Heimer and NicolasNicolov.
2001.
?Efficient Indexing for Typed Fea-ture Structures.?
Proceedings of Recent Advances inNatural Language Processing, Tzigov Chark, Bul-garia.Christophe Laprun, Johnathan Fiscus, John Garofolo,and Sylvain Pajot.
2002.
?A Practical Introduction toATLAS.?
Proceedings of the Third InternationalConference on Language Resources and Evaluation(LREC).John Prager, Jennifer Chu-Carroll, Eric Brown, andKrzysztof Czuba.
2003.
"Question Answering UsingPredictive Annotation."
Advances in Open-DomainQuestion Answering, T. Strzalkowski & S. Hara-bagiu (eds.
), Kluwer Academic Publishers, to appear.John Prager, Eric Brown, Anni Coden and DragomirRadev.
2000.
?Question-answering by PredictiveAnnotation.?
Proceedings of ACMSIGIR.Scott Mardis and John Burger.
2002.
?Qanda and theCatalyst Architecture.?
AAAI Spring Symposium onMining Answers from Text and Knowledge Bases.Sun Microsystems, Inc.  2001.
?Java?
2 PlatformEnterprise Edition Specification, v1.3.
?http://java.sun.com/j2ee/1.3/docs/.
