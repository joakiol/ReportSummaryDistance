Customiz ing  a Lex icon  to Bet ter  Sui t  aComputat iona l  TaskMarti A. Hearst Hinrich SchfitzeComputer Science Division571 Evans Hall, UC BerkeleyBerkeley, CA 94720 USAma rti@cs, be rkele y. eduCSLIVentura Hall, Stanford UniversityStanford, CA 94305 USAschuetze @csli.
stanford, eduAbst ractWe discuss a method for augmenting and rearranging a structured lexicon in orderto make it more suitable for a topic labefing task, by making use of lexical associationinformation from a large text corpus.
We first describe an algorithm for convertingthe hierarchical structure of WordNet \[13\] into a set of flat categories.
We thenuse lexical cooccurrence statistics in combination with these categories to classifyproper names, assign more specific senses to broadly defined terms, and classify newwords into existing categories.
We also describe how to use these statistics to assignschema-like information to the categories and show how the new categories improve atext-labeling algorithm.
In effect, we provide a mechanism for successfully combininga hand-built lexicon with knowledge-free, statistically-derived information.1 In t roduct ionMuch effort is being applied to the creation of lexicons and the acquisition of semanticand syntactic attr ibutes of the lexical items that comprise them, e.g, \[1\], \[4\],\[7\],\[8\], 11\],\[16\], \[18\], \[20\].
However, a lexicon as given may not suit the requirements of a part icularcomputat ional  task.
Because lexicons are expensive to build, rather than create new onesfrom scratch, it is preferable to adjust existing ones to meet an appl ication's needs.
Inthis paper we describe such an effort: we add associational information to a hierarchicallystructured lexicon in order to better serve a text labeling task.An algorithm for part it ioning a full-length expository text into a sequence of subtopiealdiscussions is described in \[9\].
Once the partit ioning is done, we need to assign labels 1indicating what the subtopical discussions are about, for the purposes of informationretrieval and hypertext navigation.
One way to label texts, when working within a l imiteddomain of discourse, is to start  with a pre-defined set of topics and specify the wordcontexts that indicate the topics of interest (e.g., \[10\]).
Another way, assuming that alarge collection of pre-labeled texts exists, is to use statistics to automatical ly  infer whichlexical items indicate which labels (e.g., \[12\]).
In contrast, we are interested in assigninglabels to general, domain- independent text, without benefit of pre-classified texts.
Inall three cases, a lexicon that specifies which lexical items correspond to which topics isrequired.
The topic labeling method we use is statistical and thus requires a large numberof representative l xical items for each category.The start ing point for our lexicon is WordNet \[13\], which is readily available onlineand provides a large repository of English lexical items.
WordNet 2 is composed of synsets,1 The terms "label" and "topic" are used interchangeably in this paper.2 All work described here pertains to Version 1.3 of WordNet.55structures containing sets of terms with synonymous meanings, thus allowing a distinctionto be made between different senses of homographs.
Associated with each synset is a listof relations that the synset participates in.
One of these, in the noun dataset, is thehyponymy relation (and its inverse, hypernymy), roughly glossed as the "ISA" relation.This relation imposes a hierarchical structure on the synsets, indicating how to generalizefrom a subordinate term to a superordinate one, and vice versa.
3 This is a very usefulkind of information for many tasks, such as reasoning with generalizations and assigningprobabilities to grammatical relations \[17\].We would like to adjust this lexicon in two ways in order to facilitate the label as-signment ask.
The first is to collapse the fine-grained hierarchical structure into a setof coarse but semantically-related categories.
These categories will provide the lexicalevidence for the topic labels.
(After the label is assigned, the hierarchical structure canbe reintroduced.)
Once the hierarchy has been converted into categories, we can augmentthe categories with new lexical items culled from free :text corpora, in order to furtherimprove the labeling task.The second way we would like to adjust the lexicon is to combine categories fromdistant parts of the hierarchy.
In particular, we are interested in finding groupings ofterms that contribute to a frame or schema-like representation \[14\]; this can be achievedby finding associational lexical relations among the existing taxonymic relations.
Forexample, WordNet has the following synsets: "athletic game" (hyponyms: baseball, ten-nis), "sports implement" (hyponyms: bat, racquet), and "tract, piece of land" (hyponyms:baseball_diamond, court), none of which are closely related in the hierarchy.
We would liketo automatically find relations among categories headed by synsets like these.
(In Version1.3, the WordNet encoders have placed some associational links among these categories,but still only some of the desired connections appear.
)In other words, we would like to derive links among schematically related parts of thehierarchy, where these links reflect the text genre on which text processing is to be done.\[19\] describes a method called WordSpace that represents lexical items according to howsemantically close they are to one another, based on evidence from a large text corpus.We propose combining this term-similarity information with the hierarchical informationalready available in WordNet to create structured associational information.In the next section we describe the algorithm for compressing the WordNet hierarchyinto a set of categories.
This is followed by a discussion of how these categories are tobe used and why they need to be improved.
Section 4 describes the first improvementtechnique: including new, related terms from a corpus, and Section 5 describes the sec-ond improvement technique: bringing disparate categories together to form schematicgroupings while retaining the given hierarchical structure.
Section 6 concludes the paper.2 Creating Categories from WordNetWe would like to decompose the WordNet noun hierarchy into a set of disjoint categories,each consisting of a relatively large number of synsets.
(This is necessary for the text-labeling task, because ach topic must be represented by many different erms.)
The goalof creating categories of a particular average size with as small a variance as possible.There is some limit as to how small this variance can be because there are several synsets3Actually, the hyponomy relation is a directed acycllc graph, in that  a minor i ty of the nodes arechildren of more than one parent.
We will at t imes refer to it as a hierarchy nonetheless.56fo r  each synset  N in  the  noun h ie rarchya_cat(N)a_cat (N) :i f  N has  not  been entered i n  a categoryT <- #descendents (N)i f  ((T >= LOWER_BRACKET)?
~ (T <- UPPER_BRACKET))mark(N,NeeCatNumber)e l se  i f  (T > UPPER_BRACKET)for each (direct) child C of NCT <- #descendents(C)i f  ((CT >- LOWER_BRACKET)(CT <-UPPER_BRACKET))mark(C,NewCatNumber)else if (CT > UPPER_BRACKET)a_cat(C)T <- #descendents(N)if (T >- LOWER_BRACKET)lark(N,NewCatNumber)Figure 1: Algorithm for creating categories from WordNet's noun hierarchy.that have a very large number of children (there are sixteen nodes with a branchingfactor greater than 100).
This primarily occurs with synsets of a taxonymic flavor, i.e.,mushroom species and languages of the world.
There are two other reasons why it is notstraightforward to find uniformly sized, meaningful categories:(i) There is no explicit measure of semantic distance among the children of a synset.
(ii) The hierarchy is not balanced, i.e., the depth from root to leaf varies dramaticallythroughout the hierarchy, as does the branching factor.
(The hierarchy has ten rootnodes; on average their maximum depth is 10.5 and their minimum depth is 2.
)Reason (ii) rules out a strategy of traveling down a uniform depth from the root or upa uniform height from the leaves in order to achieve uniform category sizes.The algorithm used here is controlled by two parameters: upper and lower boundson the category size (see Figure 1).
For example, the result of setting the lower boundto 25 and the upper bound to 60 yields categories with an average size of 58 members.An arbitrary node N in the hierarchy is chosen, and if it has not yet been registered asa member of a category, the algorithm checks to see how many unregistered escendantsit has.
In every case, if the number of descendants i too small, the assignment to acategory is deferred until a node higher in the hierarchy is examined (unless the node hasno parents).
This helps avoid extremely small categories, which are especially undesirable.If the number of descendants of N falls within the boundaries, the node and its unreg-istered descendants are bundled into a new category, marked, and assigned a label which57is derived from the synset at N. If N has too many descendants, that is, the count of itsunmarked escendants exceeds the upper bound, then each of its immediate children ischecked in turn: if the child's descendant count falls between the boundaries, then thechild and its descendants are bundled into a category.
If the child and its unmarked e-scendants exceed the upper bound, then the procedure is called recursively on the child.Otherwise, the child is too small and is left alone.
After all of N's children have beenprocessed, the category that N will participate in has been made as small as the algo-rithm will allow.
There is a chance that N and its unmarked escendants will now makea category that is too small, and if this is the case, N is left alone, and a higher-up nodewill eventually subsume it (unless N has no parents remaining).
Otherwise, N and itsremaining unmarked escendants are bundled into a category.If N has more than one parent, N can end up assigned to the category of any of itsparents (or none), depending on which parent was accessed first and how many unmarkedchildren it had at any time, but each synset is assigned to only one category.The function "mark" places the synset and all its descendents hat have not yet beenentered into a category into a new category.
Note that #descendents is recalculated inthe third-to-last line in case any of the children of N have been entered into categories.In the end there may be isolated small pieces of hierarchy that aren't stored in anycategory, but this can be fixed by a cleanup pass, if desired.3 A Topic LabelerWe are using a version of the disambiguation algorithm described in \[21\] to assign topiclabels to coherent passages of text.
Yarowsky defines word senses as the categories listedfor a word in Roger's Thesaurus (Fourth Edition), where a category is something likeTOOLS/MACHINERY.
For each category, the algorithm?
Collects contexts that are representative of the category.?
Identifies alient words in the collective contexts and determines the weight for eachword.?
Uses the resulting weights to predict the appropriate category for a word occurringin a novel context.The proper use of this algorithm is to choose among the categories to which a particularambiguous word can belong, based on the lexical context that surrounds a particularinstance of the word.In our implementation f the algorithm, the 726 categories derived from WordNet, asdescribed in the previous ection, are used instead of Rogel's categories, because these arenot available publically online.
Training is performed on Grolier's American AcademicEncyclopedia (~ 8.7M words).The labeling is done as follows: Instead of using the algorithm in the intended way, weare placing probes in the text at evenly-spaced intervals and accumulating the scores foreach category all the way through the text.
The intention is that at the end the highestscoring categories correspond to the main topics of the text.
Below we show the output ofthe labeler on two well-known texts (made available online by Project Gutenberg).
Thefirst column indicates the rank of the category, the second column indicates the score for58comparison purposes, and the third column shows the words in the synset at the top-most node of the category (these are not always entirely descriptive, so some glosses areprovided in parentheses).United States Constitution0 16300 assembly (court, legislature)Genesis29424 deity divinity god1 14286 due_process_of.law 28949 relative relation (mother, aunt)2 13313 legal_document legal_instrument 28934 worship3 11764 administrative_unit 28603 man adult_male4 11566 body (legislative) 28321 professional5 11481 charge (taxes) 28263 happiness gladness felicity6 11468 administrator decision_maker 28005 woman adult_female7 10442 document written_document 27643 evildoing transgression8 10250 approval (sanction, pass) 27514 literary_composition9 9428 power powerfulness 27203 religionist religious_personNote that although most of the categories are appropriate (with the glaring exceptionof "professional" in Genesis), there is some redundancy among them, and in some casesthey are too fine-level to indicate main topic information.In an earlier implementation f this algorithm, the categories were in general argerbut less coherent than in the current set.
The larger categories resulted in better-trainedclassifications, but the classes often conflated quite disparate terms.
The current imple-mentation produces maller, more coherent categories.
The advantage is that a moredistinct meaning can be associated with a particular label, but the disadvantage is thatin many cases so few of the words in the category appear in the training data that a weakmodel is formed.
Then the categories with little distinguishing training data dominatethe labeling scores inappropriately.In the category-derivation algorithm described above, in order to increase the sizeof a given category, terms must be taken from nodes adjacent in the hierarchy (eitherdescendants or siblings).
However, adjacent erms are not necessarily closely relatedsemantically, and so after a point, expanding the category via adjacent erms introducesnoise.
To remedy this problem, we have experimented with increasing the size of thecategories in two different ways:(1) The first approach is to retain the categories in their current form and add semanti-cally similar terms, extracted from corpora independent ofWordNet, thus improvingthe training of the labeling algorithm.
(2) The second approach is to determine which categories are semantically related to oneanother, despite the fact that they come from quite different parts of the hierarchy,and combine them so that they form schema-like associations.These are described in the next two sections, respectively.4 Augmenting Categories with Relevant TermsAs mentioned above, one way to improve the categories i to expand them with relatedrelevant erms.
In this section we show how comparing WordSpace vectors to the de-rived categories allows us to expand the categories.
The first subsection describes the59WordSpace algorithm, and the subsequent subsections show how it can be used to aug-ment the derived categories.4.1 Creating WordSpace from Free TextWordSpace \[19\] is a corpus-based method for inducing semantic representations for alarge number of words (50,000) from lexical cooccurrence statistics.
The representationsare derived from free text, and therefore are highly specific to the text type in question.The medium of representation is a multi-dimensional, real-valued vector space.
The cosineof the angle between two vectors in the space is a continuous measure of their semanticrelatedness.Lexical coocurrence, which is the basis for creating the word space vectors, can be easilymeasured.
However, for a vocabulary of 50,000 words, there are 2,500,000,000 possiblecoocurrence counts, a number too high to be computationally tractable.
Therefore, letterfourgrams are used here to bootstrap the representations.
Cooccurrence statistics arecollected for 5,000 selected fourgrams.
The 5000-by-5000 matrix used for this purpose ismanageable.
A vector for a lexical item is then computed as the sum of fourgram vectorsthat occur close to it in the text.The first step of the creation of WordSpace consists of deriving fourgram vectorsthat reflect semantic similarity in the sense of being used to describe the same contexts.Consequently, one needs to be able to pairwise compare fourgrams' contexts.
For thispurpose, a collocation matrix for fourgrams was collected such that the entry ai j  countsthe number of times that fourgram i occurs at most 200 fourgrams to the left of fourgramj.
Two columns in this matrix are similar if the contexts the corresponding fourgrams areused in are similar.
The counts were determined using five months of the New York Times(June - October 1990).
The resulting collocation matrix is dense: only 2% of entries arezeros, because almost any two fourgrams cooccur.
Only 10% of entries are smaller than 10,so that culling small counts would not increase the sparseness ofthe matrix.
Consequently,any computation that employs the fourgram vectors directly would be inefficient.
For thisreason, a singular value decomposition was performed and 97 singular values extracted(cf.
\[5\]) using an algorithm from SVDPACK \[3\].
Each fourgram can then be representedby a vector of 97 real values.
Since the singular value decomposition finds the best least-square approximation of the original space in 97 dimensions, two fourgram vectors will besimilar if their original vectors in the collocation matrix are similar.
The reduced fourgramvectors can be efficiently used in the following computations.Cooccurrence information was used for a second time to compute word representationsfrom the fourgram vectors: in this case coocurrence of a target word with any of the 5000fourgrams.
50,000 words that occurred at least 20 times in 50,000,000 words of the NewYork Times newswire were selected.
For each of the words, a context vector was computedfor every position at which it occurred in the text.
A context vector was defined as thesum of all defined fourgram vectors in a window of 1001 fourgrams centered around thetarget word.
The context vectors were then normalized and summed.
This sum of vectorsis the vector representation f the target word.
It is the confusion of all its uses in thecorpus.
More formally, if C(w) is the set of positions in the corpus at which w occurs andif 90(f) is the vector representation for fourgram f, then the vector representation 7"(w)of w is defined as: (the dot stands for normalization)50wordburglardisabledisenchantmentdomesticallyDourgruntskidS.O.B:.Ste.workforcenearest neighborsburglars thief rob mugging stray robbing lookout chase crate thievesdeter intercept repel halting surveillance shield maneuversdomestic auto/-s importers/-ed threefold inventories drastically carsheap into ragged goose neatly pulls buzzing rake odd roughConfessions Jill Julie biography Judith Novak Lois Learned Pulitzerdry oyster whisky hot filling rolls lean float bottle icejobs employ/-s/-ed/-ing attrition workers clerical labor hourlyTable 1: Ten random words and their nearest neighbors.#Z(  Zice(w) !
close to iTable 1 shows a random sample of 10 words and their nearest neighbors in WordSpace.
As can be seen from the table, proximity in the space corresponds closely tosemantic similarity in the corpus.
(N'Dour is a Senegalese jazz musician.
In the 1989/90New York Times, S.O.B.
mainly occurs in the book title "Confessions of an S.O.B.
", andSte.
in the name "Ste.-Marguerite" a Quebec river that is popular for salmon fishing.
)4.2 Augmenting WordNet Categories using WordSpaceWe chose the following simple mapping from the derived WordNet categories to WordSpace:?
for each category i from Section 2:?
collect the vectors of all the words in i that are covered by WordSpace?
compute the centroid of these vectors"rhis centroid defines an area in WordSpace thai, corresponds 1,o tile WordNet category.Ilsing these eentroids we can now assign a word ill WordSl)ace to a derived category I)yexamining the nearest neighbors of the word.
The assignment algorithm we use is:?
for each word w in WordSpace?
collect the 20 nearest neighbors of w in the space?
compute the score si of category i for w as the number of nearest neighbors thatare in i?
assign w to the highest scoring category or categoriesIn order to test this algorithm, we selected 1000 words from the medium frequencywords in WordSpace.
4 These turned out to be the medium-frequency words from defor-estation to downed.
The following subsections describe the application of the assignment4 WordSpace has three parts: high-frequency, medium-frequency, and low-frequency words.
The wordsin the test set have the internal identification numbers 26,000 through 26,999.61algorithm to classifying proper names, reassigning words in the categories, and assigningwords that are not covered by the categories.4.2.1 Semant ic  classif ication of  proper  namesA deficiency of WordNet for our text labeling task and for many other applications i thatit omits many proper names (and since the set of important proper names changes overtime, it cannot be expected to contain an exhaustive list).
We tested the performanceof our assignment algorithm by searching for proper names that had high scores for thecategories in Table 2.
For each category on the left-hand side we show all of the propernames that assigned high scores those categories.
The proper names assigned to "artist"are painters, the proper names assigned to "European country" are European politicians,"performer" contains actors, dancers and roles, writers and titles of movies, "music" hasmusicians and titles of musical performances (the Pasadena Doo Dah Parade, Purcell's"Dido and Aeneas"), "athlete jock" players of various ports, and "process of law" lawyers,judges and defendants.
We checked the referents of all proper names in Table 2 in the NewYork Times and found only one possible rror (although a few names like "DePalma" and"Delancey" had several referents only one of whom pertained to the assigned category):The President of Michigan State University, John DiBiaggio, was assigned to the "athlete"category because his name is mainly mentioned in articles dealing with a conflict he hadwith his athletic department.category highest scoring proper namesartist creative_person degas delacroixEuropean_country European_nation delors dienstbier diestelperformer performing_artist; deniro dennehy depalma delancey depardieudramatic omposition dern desi devito dewhurst dey diaghilevdoogie dourifmusical_organization musical_group; depeche(mode) deville diddley dido dire(straits)musician player; music doo doobie (N')Dourathlete jock dehere delpino demarco deleon deshaies detmerdibiaggio dinah doleman doughty doran dowisdue_process due_process_of.law degeorge depetris devita dichiaradicicco diles dilorenzo douganTable 2: Assigning proper names to WordNet categories.4.2.2 F ine - tun ing  WordNet  te rmsThe assignment algorithm can also be employed to adjust the assignments of individualwords in the WordNet hierarchy by matching against he derived categories.
Two kindsof adjustments are possible: specializing senses and adding senses that are not covered.Two examples of each case from the 1000 word test set are given in Table 3.52worddosagedissertationDerbiesdlhighest scoring categorymedidne medication medicamentscience scientific_disciplinehorse Equus_caballusathlete jockTable 3: Detecting misassignments in WordNet.word WordNet definitiondosage dose, dosage - (the quantity of an active agent (substance or radiation)taken in or absorbed at any one time)dissertation dissertation, thesis => treatise - (a formal exposition)derby bowler hat, bowler, derby, plug hat - (round and black and hardwith a narror brim; worn by some British businessmen)dl deciliter, decilitre, dlTable 4: Synonym sets in WordNet for the words in Table 3.worddegradabledemagoguerydeprenyldesktopdeuterium(pas de) deuxdideoxyinosine(per) diemdietersdinnerwaredioxinsdispersantsdisservicedissidencedisunitydiureticdiureticsdoctrinaldogfightdoggiedoggoneDomainedomesticitydopaminedossierdoubleheadersdownbeateva\].+0+++++++++0+++++++000highest scoring categoriescompound chemical_compoundfeeling emotioninfectious_disease; diseasememory_device storage_devicechemical_element element; substance matterdancing dance terpsichoremedicine medication medicament; infectious_diseasecommercial_document/instrument; occupation business linefoodstufftablewarechemical_element elementchange alteration modificationcognitive_state state_oLmindleader; social_groupspeech_actsymptomdisease; liquid_body_substance body_fluidreligion faith churchhappening occurrence; conflict struggleunpleasant_person persona_non_grataunit_oLmeasurement unit; integer whole_numberwine vinoperson individual man mortal human soul; feeling emotionmedicine medication medicament; roomstatement; message content subject_matter substancetime_period period period_oLtime amount_oLtime; athlete jockmessage content subject_matter substance; feeling emotionTable 5: Assigning unknown words63dosage and dissertation are defined in a very general way in Wordnet (see Table 4).While they can be used with the general sense given in WordNet, almost all uses ofdissertation i  the New York Times are for doctoral dissertations that report on scientificwork.
Similarly, non-medical contexts are conceivable for dosage, but the dosages that theNew York Times mentions are exclusively dosages of radiation or medicine in a medicalcontext.
The automatically found labelings in Table 3 indicate the need for specializationand can be used as the basis for reassignment.In some cases, the WordNet hierarchy is also incomplete.
The two senses "horse race"and "Disabled List" for derby and dl are missing from WordNet, although they are thedominant uses in the New York Times.
Again the classification algorithm finds the righttopic area for the two words which can be used as the basis for reassignment.Unfortunately, the algorithm also labels some correctly assigned words with incorrectcategories.
We are working on an improved version that will not give "false positives" inthe detection of misassignments.4.2 .3  Assigning unknown wordsWe would like to be able to handle unknown words since they are often highly specific andexcellent indicators for the topical structure of a document.
Table 5 shows the automaticassignments for all words in the 1000 word test set that were not found in WordNet.The results are mixed.
63% (17/27) of the words are assigned to a correct opic (+), anadditional 19% (5/27) are assigned to topics they are related to (0), 19% are misassigned (-).
We are considering several ways of improving the assignment algorithm.
For instance,there are "diluted" categories such as "speech_act" and "trait character feature" whosemembers are mostly words that are poorly characterized collocationally.
If we ignore themin assigning categories (hoping that most unknown words will be topic-specific specialterms) we can correct some of the errors, e.g.
disunity would be assigned to "group_actioninteraction social_activity" which seems correct.
We expect that we can improve theresults in Table 5 as we gain more experience in combining WordSpace and WordNet.These results are encouraging; we have not yet tested to see if they improve theparticular task of interest.5 Combining Distant Categories5.1 The  A lgor i thmTo find which categories should be considered closest o one another, we first determinedhow close they are in WordSpace and then group categories together that mutually rankedone another highly.To compute the first-degree closeness of two categories cl and cj we used the formula:1 1- 2 ledlcjl ~E?i ~Ecjwhere d is the Euclidean distance:64The primary rank of category i for category j indicates how closely related i is to j.For instance rank 1 means that i is the closest category to j, and rank 3 means there areonly two closer categories to j than i.The second-degree closeness is computed from the rank of the primary ranks.
Todetermine that close association is mutual between two categories, we check for mutualhigh ranking.
Thus category i and j are grouped together if and only if i ranks j highlyand j ranks i highly (where "highly" was determined by a cutoff value - i and j had tobe ranked k or above with respect o each other, for a threshold k).
Secondary ranking isneeded because some categories are especially "popular," attracting many other categoriesto them; the secondary rank enables the popular categories to retain only those categoriesthat they mutually rank highly.The results of this algorithm were difficult to interpret until we displayed them graph-ically.
The graph layout problem is notoriously difficult, but \[2\] describes a presentationtool based on theoretical work by \[6\] which uses a force-directed placement model to layoutcomplex networks (edges are modeled as springs; nodes linked by edges are attracted toeach other, but all other pairs of nodes are repelled from one another).
Figure 2 shows apiece of the network.
In these networks only connectivity has meaning; distance betweennodes does not connote semantic distance.Looking at Figure 2 in more detail, we see that categories associated with the notion"sports", such as "athletic_game", "race", "sports_equipment', and "sports_implement",have been grouped together.
The network also shows that categories that are specified tobe near one another in WordNet, such as the categories related to "bread", are found to beclosely interrelated.
This is useful in case we would like to begin with smaller categories,in order to eliminate some of the large, broad categories that we are currently workingwith.The connectivity of the network is interesting also because it indicates the intercon-nectivity between categories.
Athletics is linked to vehicle and competition categories;these in turn link to military vehicles and weaponry categories, which then lead in to legalcategories.Most of the connectivity information suggested by the network was used to createthe new categories.
However, many of the desireable relationships do not appear in thenetwork, perhaps because of the requirement for highly mutual co-ranking.
If we wereto relax this assumption we may find better coverage, but perhaps at the cost of moremisleading links.
The remaining associations where determined by hand, so that theoriginal 726 categories were combined into 106 new super-categories.5.2 Improv ing  the  Top ic  Labe lerThe super-categories are intended to group together related categories in order to eliminatetopical redundancy in the labeler and to help eliminate inappropriate labels (since thecategories are larger and so have more lexical items serving as evidence).
Thus the topfour or five super-categories should suffice to indicate the main topics of documents.
Wehave not yet rigorously analyzed the performance ofthe labeler with the original categoriesor with the super-categories.
In future we plan to obtain reader judgements about whichcategories are the best labels for various texts.
Here we show some example output anddiscuss its characteristics.The table below compares the results of the labeler using the original categories againstthe super-categories.
The numbers beside the category names are the scores assigned by65the algorithm; the scores in both cases are roughly similar.
It is important to realize thatonly the top four or five labels are to be used from the super-categories; since each super-category subsumes many categories, only a few super-categories should be expected tocontain the most relevant information.
The first article is a 31-sentence magazine article,published in 1987, taken from \[15\].
It describes how Soviet women have little politicalpower, discusses their role as working women, and describes the benefits of college life.The second article is a 77-sentence popular science magazine article about the Magellanspace probe exploring Venus.
When using the super-categories, the labeler avoids grosslyinappropriate labels such as "mollusk.genus" and "goddess" in the Magellan article, andcombines categories such as "layer", "natural_depression", and "rock stone" into the onesuper-category "land terra_firma".Raisa Gorbachev articleOriginal Categor ies  Super-Categories0 696 woman adult_female 637 social_standing1 676 status social.state 592 education2 666 man adult_male 577 politics3 654 political_orientation ideology 567 legal_system4 628 force personnel 561 people5 626 charge 547 psychological_state6 621 relationship 531 socializing7 608 fear 521 social_group8 603 attitude 512 personal_relationship9 600 educator pedagogue 506 governmentMagellan space probe articleOriginal Categories Super-Categories0 2770 celestial_body heavenly_body 2480 outer_space1 2760 mollusk_genus 2246 light_and_energy2 2588 electromagnetic.radiation 2056 atmosphere3 2349 layer (surface) 1908 land terra_firma4 2266 atmospheric_phenomenon 1778 physics5 2139 physical_phenomenon 1484 arrangement6 2122 goddess 1448 shapes7 2095 natural_depression depression 1413 water_and_liquids8 2032 rock stone 1406 properties9 1961 space (hole) 1388 amountsLooking again at the longer texts of the United States Constitution and Genesis we seethat the super-categories are more general and less redundant than the categories shownin Section 2.
(Alhough the high scores for the "breads" category seems incorrect, eventhough the term "bread" occurs 25 times.)
In some cases the user might desire morespecific categories; this experiment suggests that the labeler can generate topic labels atmultiple levels of granularity.55United States Constitution Genesis0 12200 legal_system 26459 religion1 11782 government 25062 breads2 7859 politics 24356 mythology3 7565 conflict 23377 people4 7354 crime 21810 social_outcasts5 6814 finance 21790 social.group6 6566 social_standing 21600 psychological_state7 6458 honesty 73 20614 personality8 6349 communication 20514 literature6 Conc lus ionsWe have discussed two approaches to augmenting and rearranging the components of alexicon, in effect adding new features to its members, by making use of lexical associationinformation from a large corpus.
We've used lexical cooecurrence statistics in combinationwith a modified lexicon to classify proper names, associate more specific senses to broadlydefined terms, and classify new words into existing categories with some degree of success.We've also used these statistics to suggest how to rearrange a lexicon with a taxonymicstructure into more frame-like categories, and assigned more general main-topic labels totexts based on these categories.One conclusion that may be drawn from this work, especially the results in Section 4,is that we have provided a mechanism for successfully combining hand-built lexicon infor-mation with knowledge-free, statistically-derived information.
The combined informationfrom the categories derived from WordNet provided the clusters from which WordSpacecentroids could be created, and these centroids in turn provided candidate words to im-prove the categories.In future, in addition to expanding the evaluation of the results described here, wewould like to try reversing the experiment; hat is, starting with WordSpace vectors, seewhich parts of WordNet should be interlinked into schematic ategories.Acknowledgments  The authors would like to thank Jan Pedersen for his help and en-couragement.
We are also indebted to Mike Berry for SVDPACK.
The first author's researchwas sponsored in part by the Advanced Research Projects Agency under Grant No.
MDA972-92-J-1029 with the Corporation for National Research Initiatives (CNRI), in part by an internshipat Xerox Palo Alto Research Center; and this material is based in part upon work supportedby the National Science Foundation under Infrastructure Grant No.
CDA-8722788.
The secondauthor was supported in part by the National Center for Supercomputing Applications undergrant BNS930000N.References\[1\] Hiyan Alshawi.
Processing dictionary definitions with phrasal pattern hierarchies.
AmericanJournal o\] Computational Linguistics, 13(3):195-202, 1987.\[2\] Elan Amir.
Carta: A network topology presentation tool.
Project Report, UC Berkeley,1993.\[3\] Michael W. Berry.
Large-scale sparse singular value computations.
The International Journalof Supercomputer Applications, 6(1):13-49, 1992.67\[4\] Nicoletta Calzolari and Remo Bindi.
Acquisition of lexical information from a large textualitalian corpus.
In Proceedings o/the Thirteenth International Conference on ComputationalLinguistics, Helsinki, 1990.\[5\] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and RichardHarshman.
Indexing by latent semantic analysis.
Journal o\] the American Society/orInformation Science, 41(6):391-407, 1990.\[6\] T. Fruchtermann and E. Rheingold.
Graph drawing by force-directed placement.
Techni-cal Report UIUCDCS-R-90-1609, Department of Computer Science, University of Illinois,Urbana-Champagne, Ill, June 1990.\[7\] G. Grefenstette.
A new knowledge-poor technique for knowledge xtraction from largecorpora.
In Proceedings of SIGIR'92, Copenhagen, Denmark, June 21-24 1992.
ACM.\[8\] Marti A. Hearst.
Automatic acquisition of hyponyms from large text corpora.
In Proceedingsol the Fourteenth International Conference on Computational Linguistics, pages 539-545,Nantes, France, July 1992.\[9\] Marti A. Hearst.
TextTiling: A quantitative approach to discourse segmentation.
TechnicalReport 93/24, Sequoia 2000, University of California, Berkeley, 1993.\[10\] Paul Jacobs and Lisa Rau.
SCISOR: Extracting information from On-Line News.
Commu-nications of the ACM, 33(11):88-97, 1990.\[11\] Judith Markowitz, Thomas Ahlswede, and Martha Evens.
Semantically significant patternsin dictionary definitions.
Proceedings of the 24th Annual Meeting of the Association forComputational Linguistics, pages 112-119, 1986.\[12\] Brij Masand, Gordon Linoff, and David Waltz.
Classifying news stories using memory basedreasoning.
In Proceedings of SIGIR 92, pages 59-65, 1992.\[13\] George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J.Miller.
Introduction to WordNet: An on-line lexical database.
Journal off Lexicography,3(4):235-244, 1990.\[14\] Marvin Minsky.
A framework for representing knowledge.
In Patrick Winston, editor, Thepsychology o\] computer vision.
McGraw-Hill, 1975.\[15\] Jane Morris.
Lexical cohesion, the thesaurus, and the structure of text.
Technical ReportCSRI-219, Computer Systems Research Institute, University of Toronto, 1988.\[16\] James Pustejovsky.
On the acquisition of lexical entries: The perceptual origin of thematicrelations.
Proceedings o\] the 25th Annual Meeting o\] the Association \]or ComputationalLinguistics, 1987.\[17\] Philip Resnik.
WordNet and Distributional Analysis: A Class-based Approach to LexicalDiscovery.
In Carl Weir, editor, Statistically-Based Natural Language Programming Tech-niques: Papers from the 1992 Workshop.
AAAI Press,Technical Report W-92-01, MenloPark, CA, 1992.\[18\] Hinrich Sch~tze.
Part-of-speech induction from scratch.
In Proceedings of ACL 31, OhioState University, 1993.\[19\] Hinrich Sch~tze.
Word space.
In Stephen J. Hanson, Jack D. Cowan, and C. Lee Giles,editors, Advances in Neural ln\]ormation Processing Systems 5.
Morgan Kaufmann, SanMateo CA, 1993.\[20\] Yorick A. Wilks, Dan C. Fass, Cheng ming Guo, James E. McDonald, Tony Plate, andBrian M. Slator.
Providing machine tractable dictionary tools.
Journal o/Computers andTranslation, 2, 1990.\[21\] David Yarowsky.
Word sense disambiguation using statistical models of roget's categoriestrained on large corpora.
In Proceedings of the Fourteenth International Conference onComputational Linguistics, pages 454-460, Nantes, France, July 1992.68aliment seafog:alllfl aceolls_lblvIgadmlnlstrco u I11peratlo~aln .=.
"~'~ d u e_ p r o c e s s7I  o' ,:)~Ig D ap~alsalIsehoodternal body_parteat \ [~t lme_per iod j~\ ]  ratetime E\],,~mon keyAscom~realol/lilhicleattackskilledworkersports_imrlstrokebridge \] travelrqa~lcev/bllFigure 2: A piece of the category network.
The grouping algorithm finds relatednessbetween categories that are near one another in WordNet (e.g., the food terms) as well ascategories that are far apart (e.g., "sports equipment" with "athlete").69
