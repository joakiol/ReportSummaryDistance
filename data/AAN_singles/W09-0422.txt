Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsEnglish-Czech MT in 2008 ?Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?Charles University, Institute of Formal and Applied LinguisticsMalostranske?
na?m.
25, Praha 1, CZ-118 00, Czech Republic{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz{popel,jan.rous}@matfyz.czAbstractWe describe two systems for English-to-Czech machine translation that took partin the WMT09 translation task.
One ofthe systems is a tuned phrase-based systemand the other one is based on a linguisti-cally motivated analysis-transfer-synthesisapproach.1 IntroductionWe participated in WMT09 with two very dif-ferent systems: (1) a phrase-based MT basedon Moses (Koehn et al, 2007) and tuned forEnglish?Czech translation, and (2) a complexsystem in the TectoMT platform ( ?Zabokrtsky?
etal., 2008).2 Data2.1 Monolingual DataOur Czech monolingual data consist of (1)the Czech National Corpus (CNC, versionsSYN200[056], 72.6%, Kocek et al (2000)), (2)a collection of web pages downloaded by PavelPecina (Web, 17.1%), and (3) the Czech mono-lingual data provided by WMT09 organizers(10.3%).
Table 1 lists sentence and token counts(see Section 2.3 for the explanation of a- and t-layer).Sentences 52 Mwith nonempty t-layer 51 Ma-nodes (i.e.
tokens) 0.9 Gt-nodes 0.6 GTable 1: Czech monolingual training data.?
The work on this project was supported by the grantsMSM0021620838, 1ET201120505, 1ET101120503, GAUK52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP(EuroMatrix).2.2 Parallel DataAs the source of parallel data we use an internalrelease of Czech-English parallel corpus CzEng(Bojar et al, 2008) extended with some additionaltexts.
One of the added sections was gatheredfrom two major websites containing Czech sub-titles to movies and TV series1.
The matching ofthe Czech and English movies is rather straight-forward thanks to the naming conventions.
How-ever, we were unable to reliably determine the se-ries number and the episode number from the filenames.
We employed a two-step procedure to au-tomatically pair the TV series subtitle files.
Forevery TV series:1.
We clustered the files on both sides to removeduplicates2.
We found the best matching using a provi-sional translation dictionary.
This proved tobe a successful technique on a small sampleof manually paired test data.
The process wasfacilitated by the fact that the correct pairs ofepisodes usually share some named entitieswhich the human translator chose to keep inthe original English form.Table 2 lists parallel corpus sizes and the distri-bution of text domains.English CzechSentences 6.91 Mwith nonempty t-layer 6.89 Ma-nodes (i.e.
tokens) 61 M 50 Mt-nodes 41 M 33 MDistribution: [%] [%]Subtitles 68.2 Novels 3.3Software Docs 17.0 Commentaries/News 1.5EU (Legal) Texts 9.5 Volunteer-supplied 0.4Table 2: Czech-English data sizes and sources.1www.opensubtitles.org and titulky.com1252.3 Data Preprocessing using TectoMTplatform: Analysis and AlignmentAs we believe that various kinds of linguisticallyrelevant information might be helpful in MT, weperformed automatic analysis of the data.
Thedata were analyzed using the layered annotationscheme of the Prague Dependency Treebank 2.0(PDT 2.0, Hajic?
and others (2006)), i.e.
we usedthree layers of sentence representation: morpho-logical layer, surface-syntax layer (called analyti-cal (a-) layer), and deep-syntax layer (called tec-togrammatical (t-) layer).The analysis was implemented using TectoMT,( ?Zabokrtsky?
et al, 2008).
TectoMT is a highlymodular software framework aimed at creatingMT systems (focused, but by far not limited totranslation using tectogrammatical transfer) andother NLP applications.
Numerous existing NLPtools such as taggers, parsers, and named entityrecognizers are already integrated in TectoMT, es-pecially for (but again, not limited to) English andCzech.During the analysis of the large Czech mono-lingual data, we used Jan Hajic?
?s Czech taggershipped with PDT 2.0, Maximum Spanning Treeparser (McDonald et al, 2005) with optimized setof features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-tic roles) from Klimes?
(2006), and numerous othercomponents of our own (e.g.
for conversion of an-alytical trees into tectogrammatical ones).In the parallel data, we analyzed the Czech sideusing more or less the same scenario as used forthe monolingual data.
English sentences were an-alyzed using (among other tools) Morce taggerSpoustova?
et al (2007) and Maximum SpanningTree parser.2The resulting deep syntactic (tectogrammatical)Czech and English trees are then aligned using T-aligner?a feature based greedy algorithm imple-mented for this purpose (Marec?ek et al, 2008).
T-aligner finds corresponding nodes between the twogiven trees and links them.
For deciding whetherto link two nodes or not, T-aligner makes use ofa bilingual lexicon of tectogrammatical lemmas,morphosyntactic similarities between the two can-didate nodes, their positions in the trees and othersimilarities between their parent/child nodes.
It2In some previous experiments (e.g.
?Zabokrtsky?
et al(2008)), we used phrase-structure parser Collins (1999) withsubsequent constituency-dependency conversion.also uses word alignment generated from surfaceshapes of sentences by GIZA++ tool, Och and Ney(2003).
We use acquired aligned tectogrammaticaltrees for training some models for the transfer.As analysis of such amounts of data is obvi-ously computationally very demanding, we run itin parallel using Sun Grid Engine3 cluster of 404-CPU computers.
For this purpose, we imple-mented a rather generic tool that submits any Tec-toMT pipeline to the cluster.3 Factored Phrase-Based MTWe essentially repeat our experiments from lastyear (Bojar and Hajic?, 2008): GIZA++ align-ments4 on a-layer lemmas (a-layer nodes corre-spond 1-1 to surface tokens), symmetrized usinggrow-diag-final (no -and) heuristic5 .Probably due to the domain difference (the testset is news), including Subtitles in the parallel dataand Web in the monolingual data did not bring anyimprovement that would justify the additional per-formance costs.
For most of the phrase-based ex-periments, we thus used only 2.2M parallel sen-tences (27M Czech and 32M English tokens) and43M Czech sentences (694 M tokens).In Table 3 below, we report the scores for thefollowing setups selected from about 50 experi-ments we ran in total:Moses T is a simple phrase-based translation (T)with no additional factors.
The translation isperformed on truecased word forms (i.e.
sen-tence capitalization removed unless the firstword seems to be a name).
The 4-gram lan-guage model is based on the 43M sentences.Moses T+C is a factored setup with form-to-formtranslation (T) and target-side morphologicalcoherence check following Bojar and Hajic?(2008).
The setup uses two language mod-els: 4-grams of word forms and 7-grams ofmorphological tags.Moses T+C+C&T+T+G 84k is a setup desirablefrom the linguistic point of view.
Two in-dependent translation paths are used: (1)form?form translation with two target-sidechecks (lemma and tag generated from thetarget-side form) as a fine-grained baseline3http://gridengine.sunsource.net/4Default settings, IBM models and iterations: 153343.5Later, we found out that the grow-diag-final-and heuris-tic provides insignificantly superior results.126with the option to resort to (2) an independenttranslation of lemma?lemma and tag?tagfinished by a generation step that combinestarget-side lemma and tag to produce the fi-nal target-side form.We use three language models in this setup(3-grams of forms, 3-grams of lemmas, and10-grams of tags).Due to the increased complexity of the setup,we were able to train this model on 84k par-allel sentences only (the Commentaries sec-tion) and we use the target-side of this smalltraining data for language models, too.For all the setups we perform standard MERTtraining on the provided development set.64 Translation Setup Based onTectogrammatical TransferIn this translation experiment, we follow the tradi-tional analysis-transfer-synthesis approach, usingthe set of PDT 2.0 layers: we analyze the inputEnglish sentence up to the tectogrammatical layer(through the morphological and analytical ones),then perform the tectogrammatical transfer, andthen synthesize the target Czech sentence from itstectogrammatical representation.
The whole pro-cedure consists of about 80 steps, so the followingdescription is necessarily very high level.4.1 AnalysisEach sentence is tokenized (roughly according tothe Penn Treebank conventions), tagged by the En-glish version of the Morce tagger Spoustova?
et al(2007), and lemmatized by our lemmatizer.
Thenthe dependency parser (McDonald et al, 2005) isapplied.
Then the analytical trees resulting fromthe parser are converted to the tectogrammaticalones (i.e.
functional words are removed, onlymorphologically indispensable categories are leftwith the nodes using a sequence of heuristic proce-dures).
Unlike in PDT 2.0, the information aboutthe original syntactic form is stored with each t-node (values such as v:inf for an infinitive verbform, v:since+fin for the head of a subor-dinate clause of a certain type, adj:attr foran adjective in attribute position, n:for+X for agiven prepositional group are distinguished).6We used the full development set of 2k sentences for?Moses T?
and a subset of 1k sentences for the other twosetups due to time constraints.One of the steps in the analysis of English isnamed entity recognition using Stanford NamedEntity Recognizer (Finkel et al, 2005).
The nodesin the English t-layer are grouped according to thedetected named entities and they are assigned thetype of entity (location, person, or organization).This information is preserved in the transfer of thedeep English trees to the deep Czech trees to al-low for the appropriate capitalization of the Czechtranslation.4.2 TransferThe transfer phase consists of the following steps:?
Initiate the target-side (Czech) t-trees sim-ply by ?cloning?
the source-side (English) t-trees.
Subsequent steps usually iterate overall t-nodes.
In the following, we denote asource-side t-node as S and the correspond-ing target-side node as T.?
Translate formemes usingtwo probabilistic dictionaries(p(T.formeme|S.formeme, S.parent.lemma)and p(T.formeme|S.formeme)) and a fewmanual rules.
The formeme translationprobability estimates were extracted from apart of the parallel data mentioned above.?
Translate lemmas using a probabilistic dictio-nary (p(T.lemma|S.lemma)) and a few rulesthat ensure compatibility with the previouslychosen formeme.
Again, this probabilisticdictionary was obtained using the alignedtectogrammatical trees from the parallel cor-pus.?
Fill the grammatemes (deep-syntactic equiv-alent of morphological categories) gender(for denotative nouns) and aspect (for verbs)according to the chosen lemma.
We alsofix grammateme values where the English-Czech grammateme correspondence is non-trivial (e.g.
if an English gerund expression istranslated to Czech as a subordinating clause,the tense grammateme has to be filled).
How-ever, the transfer of grammatemes is defi-nitely much easier task than the transfer offormemes and lemmas.4.3 SynthesisThe transfer step yields an abstract deepsyntactico-semantical tree structure.
Firstly,127we derive surface morphological categoriesfrom their deep counterparts taking care of theiragreement where appropriate and we also removepersonal pronouns in subject positions (becauseCzech is a pro-drop language).To arrive at the surface tree structure, auxil-iary nodes of several types are added, including(1) reflexive particles, (2) prepositions, (3) subor-dinating conjunctions, (4) modal verbs, (5) ver-bal auxiliaries, and (6) punctuation nodes.
Also,grammar-based node ordering changes (imple-mented by rules) are performed: e.g.
if an Englishpossessive attribute is translated using Czech gen-itive, it is shifted into post-modification position.After finishing the inflection of nouns, verbs,adjectives and adverbs (according to the values ofmorphological categories derived from agreementetc.
), prepositions may need to be vocalized: thevowel -e or -u is attached to the preposition if thepronunciation of prepositional group would be dif-ficult otherwise.After the capitalization of the beginning of eachsentence (and each named entity instance), we ob-tain the final translation by flattening the surfacetree.4.4 Preliminary Error AnalysisAccording to our observations most errors happenduring the transfer of lemmas and formemes.Usually, there are acceptable translations oflemma and formeme in respective n-best listsbut we fail to choose the best one.
The sce-nario described in Section 4.2 uses quite aprimitive transfer algorithm where formemesand lemmas are translated separately in twosteps.
We hope that big improvements couldbe achieved with more sophisticated algo-rithms (optimizing the probability of the wholetree) and smoothed probabilistic models (suchas p(T.lemma|S.lemma, T.parent.lemma) andp(T.formeme|S.formeme, T.lemma, T.parent.lemma)).Other common errors include:?
Analysis: parsing (especially coordinationsare problematic with McDonald?s parser).?
Transfer: the translation of idioms and col-locations, including named entities.
In thesecases, the classical transfer at the t-layeris not appropriate and utilization of somephrase-based MT would help.?
Synthesis: reflexive particles, word order.5 Experimental Results and DiscussionTable 3 reports lowercase BLEU and NIST scoresand preliminary manual ranks of our submissionsin contrast with other systems participating inEnglish?Czech translation, as evaluated on theofficial WMT09 unseen test set.
Note that auto-matic metrics are known to correlate quite poorlywith human judgements, see the best ranking but?lower scoring?
PC Translator this year and alsoin Callison-Burch et al (2008).System BLEU NIST RankMoses T 14.24 5.175 -3.02 (4)Moses T+C 13.86 5.110 ?Google 13.59 4.964 -2.82 (3)U. of Edinburgh 13.55 5.039 -3.24 (5)Moses T+C+C&T+T+G 84k 10.01 4.360 -Eurotran XP 09.51 4.381 -2.81 (2)PC Translator 09.42 4.335 -2.77 (1)TectoMT 07.29 4.173 -3.35 (6)Table 3: Automatic scores and preliminary humanrank for English?Czech translation.
Systems initalics are provided for comparison only.
Best re-sults in bold.Unfortunately, this preliminary evaluation sug-gests that simpler models perform better, partlybecause it is easier to tune them properly bothfrom computational point of view (e.g.
MERTnot stable and prone to overfitting with more fea-tures7), as well as from software engineering pointof view (debugging of complex pipelines of toolsis demanding).
Moreover, simpler models runfaster: ?Moses T?
with 12 sents/minute is 4.6times faster than ?Moses T+C?.
(Note that we havenot tuned either of the models for speed.
)While ?Moses T?
is probably nearly identicalsetup as Google and Univ.
of Edinburgh use,the knowledge of correct language-dependent to-kenization and the use of relatively high qualitylarge language model data seems to bring moder-ate improvements.6 ConclusionWe described our experiments with a complex lin-guistically motivated translation system and vari-ous (again linguistically-motivated) setups of fac-tored phrase-based translation.
An automatic eval-uation seems to suggest that simpler is better, butwe are well aware that a reliable judgement comesonly from human annotators.7For ?Moses T+C+C&T+T+G?, we observed BLEUscores on the test set varying by up to five points absolutefor various weight settings yielding nearly identical dev setscores.128ReferencesOndr?ej Bojar and Jan Hajic?.
2008.
Phrase-Based andDeep Syntactic English-to-Czech Statistical Ma-chine Translation.
In Proceedings of the ThirdWorkshop on Statistical Machine Translation, pages143?146, Columbus, Ohio, June.
Association forComputational Linguistics.Ondr?ej Bojar, Miroslav Jan?
?c?ek, Zdene?k ?Zabokrtsky?,Pavel ?Ces?ka, and Peter Ben?a.
2008.
CzEng 0.7:Parallel Corpus with Community-Supplied Transla-tions.
In Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08), Mar-rakech, Morocco, May.
ELRA.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on Statisti-cal Machine Translation, pages 70?106, Columbus,Ohio, June.
Association for Computational Linguis-tics.Michael Collins.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL ?05: Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 363?370, Morristown, NJ, USA.Association for Computational Linguistics.Jan Hajic?
et al 2006.
Prague Dependency Treebank2.0.
CD-ROM, Linguistic Data Consortium, LDCCatalog No.
: LDC2006T0 1, Philadelphia.Va?clav Klimes?.
2006.
Analytical and Tectogrammat-ical Analysis of a Natural Language.
Ph.D. thesis,Faculty of Mathematics and Physics, Charles Uni-versity, Prague, Czech Rep.Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-tors.
2000.
?Cesky?
na?rodn??
korpus - u?vod a pr???ruc?kauz?ivatele.
FF UK - ?U ?CNK, Praha.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In ACL 2007, Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics Companion Volume Proceedings of the Demoand Poster Sessions, pages 177?180, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clavNova?k.
2008.
Automatic Alignment of Czech andEnglish Deep Syntactic Dependency Trees.
In Pro-ceedings of European Machine Translation Confer-ence (EAMT 08), pages 102?111, Hamburg, Ger-many.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependencyparsing using spanning tree algorithms.
In HLT?05: Proceedings of the conference on Human Lan-guage Technology and Empirical Methods in Natu-ral Language Processing, pages 523?530, Vancou-ver, British Columbia, Canada.Va?clav Nova?k and Zdene?k ?Zabokrtsky?.
2007.
Featureengineering in maximum spanning tree dependencyparser.
In Va?clav Matous?ek and Pavel Mautner, ed-itors, Lecture Notes in Artificial Intelligence, Pro-ceedings of the 10th I nternational Conference onText, Speech and Dialogue, Lecture Notes in Com-puter Science, pages 92?98, Pilsen, Czech Repub-lic.
Springer Science+Business Media DeutschlandGmbH.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Drahom?
?ra Spoustova?, Jan Hajic?, Jan Votrubec, PavelKrbec, and Pavel Kve?ton?.
2007.
The best of twoworlds: Cooperation of statistical and rule-basedtaggers for czech.
In Proceedings of the Work-shop on Balto-Slavonic Natural Language Process-ing, ACL 2007, pages 67?74, Praha.Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas.
2008.TectoMT: Highly Modular Hybrid MT Systemwith Tectogrammatics Used as Transfer Layer.
InProc.
of the ACL Workshop on Statistical MachineTranslation, pages 167?170, Columbus, Ohio, USA.129
