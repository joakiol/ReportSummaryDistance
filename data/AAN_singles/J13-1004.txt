AbstractMorphological and Syntactic Case inStatistical Dependency ParsingWolfgang Seeker?University of StuttgartJonas Kuhn?
?University of StuttgartMost morphologically rich languages with free word order use case systems to mark the gram-matical function of nominal elements, especially for the core argument functions of a verb.
Thestandard pipeline approach in syntactic dependency parsing assumes a complete disambiguationof morphological (case) information prior to automatic syntactic analysis.
Parsing experimentson Czech, German, and Hungarian show that this approach is susceptible to propagatingmorphological annotation errors when parsing languages displaying syncretism in their mor-phological case paradigms.
We develop a different architecture where we use case as a possiblyunderspecified filtering device restricting the options for syntactic analysis.
Carefully designedmorpho-syntactic constraints can delimit the search space of a statistical dependency parser andexclude solutions that would violate the restrictions overtly marked in the morphology of thewords in a given sentence.
The constrained system outperforms a state-of-the-art data-drivenpipeline architecture, as we show experimentally, and, in addition, the parser output comes withguarantees about local and global morpho-syntactic wellformedness, which can be useful fordownstream applications.1.
IntroductionIn statistical parsing, many of the first models were developed and optimized forEnglish.
This is not surprising, given that English is the predominant language forresearch in both computational linguistics and linguistics proper.
By design, thestatistical parsing approach avoids language-specific decisions built into the modelarchitecture; models should in principle be trainable on any data following the generaltreebank representation scheme.
At the same time, it is well known from theoreticaland typological work in linguistics that there is a broad multi-dimensional spectrumof language types, and that English is in a rather ?extreme?
area in that it marksgrammatical relations (subject, object, etc.)
strictly with phrase-structural configura-tions.
There are only residues of an inflectional morphology left.
In other words, one?
Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart,Germany.
E-mail: seeker@ims.uni-stuttgart.de.??
Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart,Germany.
E-mail: jonas@ims.uni-stuttgart.de.Submission received: 30 September 2011; revised submission received: 20 May 2012; accepted for publication:3 August 2012.?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 1cannot exclude that architectural or representational modeling decisions establishedas empirically useful on English data may be favoring the specific language typeof English.
Indeed, carrying over successful model architectures from English totypologically different languages mostly leads to a substantial drop in parsingaccuracy.
Linguistically aware representational adjustments can help reduce theproblem significantly, as Collins et al) showed in their pivotal study adjusting astatistical (constituent) parsing model to a highly inflectional language with free wordorder, Czech in that case, pushing the results more than seven percentage points upto a final 80% dependency accuracy (as compared with 91% accuracy for the English?source?
parser on the Wall Street Journal).
Even in recent years, however, a clear gaphas remained between the top parsing architecture for English and morphologicallyrich(er) languages.1 The relative hardness of the parsing task, compared with English,cuts across statistical parsing approaches (constituent or dependency parsing) andacross morphological subtypes, such as languages with a moderately sized remaininginflectional system (like German), highly inflected languages (like Czech), andlanguages in which interactions with derivational morphology make the segmentationquestion non-trivial (such as Turkish or Arabic, compare, for example, Eryig?it, Nivre,and Oflazer [2008]).Still, it remains hard to pinpoint systematic architectural or representational factorsthat explain the empirical picture, although there is a collection of ?recipes?
one cantry to tune an approach to a ?hard language.?
Of course, there are good reasonsfor adjusting a well-proven system rather than developing a more general one fromscratch?given that part of the success of statistical parsing in general lies in subtleways of exploiting statistical patterns that reflect inaccessible levels of information in anindirect way.This article attempts to do justice to the special status of mature data-driven systemsand still contribute to a systematic clarification, by (1) focusing on a clear-cut aspectof morphological marking relevant to syntactic parsing (namely, case marking of corearguments); (2) comparing a selection of languages covering part of the typologicalspectrum (Czech, German, and Hungarian); (3) using a state-of-the-art data-drivenparser (Bohnet 2009, 2010) to establish how far the technique of representational ad-justments may take us; and (4) performing a problem-oriented comparison with analternative architecture, which allows us to add constraints motivated from linguisticconsiderations.In a first experiment, we vary the morphological information available to the parserand examine the errors of the parser with respect to the case-related functions.
Itturns out that although the parser is indeed able to learn the case-function mappingfor all three languages, it is susceptible to errors that are propagated through thepipeline model when parsing languages that show syncretism2 in their morphologicalparadigms, in our case Czech and German (e. g., for neuter nouns, nominative andaccusative case have the same surface form).
In contrast, due to its mostly unambiguouscase system, we find a much smaller effect for Hungarian.
Although the parser itselfprofits much from morphological information as our experiments with gold standardmorphology show, errors in automatically predicted morphological information fre-quently cause errors in the syntactic analysis.1 Compare, for example, the various Shared Tasks on parsing multiple languages, such as the CoNLLShared Tasks 2006, 2007, 2009 (Buchholz and Marsi 2006; Nivre et al; Hajic?
et al, or the PaGeShared Task on parsing German (Ku?bler 2008).2 Two or more different feature values are signaled by the same form.24Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency ParsingIn order to better handle syncretism in the morphological description, we thenpropose a different way of integrating morphology into the parsing process.
We developan alternative architecture that circumvents the strict separation of morphological andsyntactic analysis in the pipeline model.
We adopt the integer linear programming(henceforth ILP) approach by Martins, Smith, and Xing (2009), which we augmentwith a set of linguistically motivated constraints modeling the morpho-syntactic depen-dencies in the languages.
Case is herein interpreted as an underspecified filtering devicethat guides a statistical model by restricting the search space of the parser.
Due to theconstraints, the output of the ILP parser is guaranteed to obey all syntactic restrictionsthat are marked overtly in the morphological form of the words.
Although the restric-tions are implemented as symbolic constraints, they are applied to the parser duringthe search for the best tree, which is driven by a statistical model.
We show in a secondexperiment that restricting the search space in this way improves the performance onargument functions (indicated by case morphology) considerably on all three languageswhile the performance on all other functions stays stable.We proceed by first discussing the role of case morphology in syntax (Section 2),followed by a presentation of the parsing architecture of the Bohnet parser with adiscussion of the relevant aspects for our first experiment (Section 3).
Next, we comparethe morphological annotation quality of automatic tools with the gold standard acrosslanguages (Section 4).
We then turn to the first experiment in this article where weexamine the performance of the parser with respect to core argument functions onthe three languages (Section 5).
In the second experiment (Section 6), we apply anILP parser to the data sets augmented with a set of linguistic constraints that integratemorphological information in an underspecified way into the parsing architecture.
Weconclude in Section 7.2.
Challenges of Parsing Morphologically Rich LanguagesA characteristic property of most languages commonly referred to as morphologically richis that they use morphological means at the word level to encode grammatical relationswithin the sentence rather than using the phrase-structural configuration.
Whereas inEnglish or Chinese, placement of a word (or phrase) in a particular position relativeto the verbal head marks its function (e. g., as the subject or object), morphologicallyrich languages encode grammatical relations largely by changing the morphologicalform of the dependent word, the head word, or both.
A correlated phenomenon is thefree word order for which many of these languages allow.
Because information aboutgrammatical relations is marked on the words themselves, it stays available regardlessof their relative position, so word order can be used to mark other information suchas topic-focus structure.
The richer the morphological system, the freer the word ordertends to be, or, as Bresnan (2001) puts it, morphology competes with syntax.
We thus see thattypologically, morphological and syntactic systems are interdependent and influenceeach other.
Most languages are located somewhere along a continuum between purelyconfigurational and purely morphological marking.In principle, data-driven parsing models with word form sensitive features have thepotential to not only pick up configurational patterns for grammatical relation marking,but also systematic patterns in the observed variation and co-variation of morphologicalword forms.
It is, however, not only the interaction between syntax and morphol-ogy that adds challenges?the marking patterns are also non-trivial to pick up fromsurface data.25Computational Linguistics Volume 39, Number 1One of the linguistic challenges is that there are different, overlapping regimes formorphological marking.
One can distinguish head-marking and dependent-marking ofa grammatical relation, depending on where the inflection occurs.
In addition, Nichols(1986, page 58) identifies four ways in which inflection markers may play a role insignaling syntactic dependency:Example 1Hebrew, taken from Nichols (1986, page 58)be?thouse-ofseferbook?school?, lit.
?book house?First, the morphological marker simply registers the presence of a syntactic depen-dency.
In Example (1), the form of the word be?t signals the presence of a dependent,without specifying the nature of the relation.Second, the affix marks not only the presence but also the type of the dependency.A typical example of the dependent-marking kind is nominal case: Accusative caseon a noun marks it not only as a dependent of a verb, but it also marks the type ofrelation, namely, direct object.
Verb agreement markers in Indo-European languagesare a head-marking kind of example: They indicate that a noun stands specificallyin the subject relation.
Third, a morphological marker may, in addition, index certainlexical or inflectional categories of the dependent on the head (or vice versa).
Subjectagreement often indexes the dependent subject?s gender and number properties onthe head verb; attributive adjectives in Czech, for instance, agree with their nounheads in case, number, and gender.
Fourth, for some affixes, there is a paradigm forindexing internal properties of the head on the head itself (e. g., tense or mood ofa verb) or properties of the dependent on the dependent (e. g., gender marking onnouns).An additional linguistic challenge in learning the patterns from data, which we willdiscuss in detail in Section 2.2, comes from the fact that the inflectional paradigms maycontain syncretism.
This may interfere with the learning of the previously discussedpatterns.
Further challenges we do not address in this article include interactions be-tween syntax and derivational morphology, which for some languages like Turkish andArabic can go along with segmentation issues.The first Workshop on Statistical Parsing of Morphologically Rich Languages hasset the agenda for developing effective systems by identifying three main types oftechnical challenges (Tsarfaty et al page 2), which we rephrase here from oursystem perspective:Architectural challenges.
Should data-driven syntactic parsing be split into subtasks,and how should they interact?
Specifically, should morphological analysis (and likewisetokenization, part-of-speech tagging, etc.)
be performed in a separate (data-driven?
)module and how can error propagation through the pipeline be minimized?
Can a jointmodel be trained on data that captures two-way interactions between several levelsof representation?
Should the same system modularization be used in training anddecoding, or can decoding combine locally trained models, taking into account moreglobal structural and representational constraints?Representational challenges.
At what technical level should morphological distinc-tions be represented?
Should they (or some of them) be included at the part-of-speech(POS) level, or at a higher level in the structure?
Can some type of representation help26Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsingavoid confusions due to syncretisms?
What is the most effective set of dependencylabels for capturing morphological marking of grammatical relations?Lexical challenges.
How can lexical probabilities be estimated reliably?
The mainproblem for morphologically rich languages is the many different forms for one lexeme,which is amplified by the often limited amount of training data.
How can a parseranalyze unseen word forms and use the information profitably?2.1 Previous Work and Our ApproachThe first two types of technical challenges often go hand in hand, as a change in architec-ture effectively means a change in the interface representations, and vice versa.
Collinset al) reduce the tag set for the Czech treebank, which consists of a combination ofPOS tags and a detailed morphological specification, in order to tackle data sparseness.A combination of POS and case features turns out to be best for their parsing models.In statistical constituent parsing, many investigations devise treebank transformationsthat allow the parsing models to access morphological information higher in the tree(Schiehlen 2004; Versley 2005; Versley and Rehbein 2009).
These transformations applycategory splits by decorating category symbols with morphological information likecase.
Whereas these approaches change traditional models to cope with morphologicalinformation, others approach the problem by devising new models tailored to thespecial requirements of morphologically rich languages.
Tsarfaty and Sima?an (2008,2010) introduce an additional layer into the parsing process that directly models the sub-categorization of a non-terminal symbol without taking word order into consideration.The parser thus separates the functional subcategorization of a word from its surfacerealization, which is not a one-to-one relation in morphologically rich languages withfree word order.
In statistical dependency parsing, morphological information is mostlyused as features in the statistical classifier that guides the search for the most probabletree (Bohnet 2009; Goldberg and Elhadad 2010; Marton, Habash, and Rambow 2010).The standard way established in the CoNLL Shared Tasks (Nivre et al; Hajic?
et al2009) is a pipeline approach where POS and morphological information is predicted asa preprocessing step to the actual parsing.
Although Goldberg and Elhadad (2010) andMarton, Habash, and Rambow (2010) find improvements for hand-annotated (gold)morphological features, automatically predicted morphological information has noneor even negative effects on their parsing models.
Goldberg and Elhadad (2010) alsoshow that linguistically grounded, carefully designed features (here agreement be-tween adjectives and nouns in Hebrew) can contribute a considerable improvement,however.
Finally, the pipeline approach itself can be questioned.
Cohen and Smith(2007), Goldberg and Tsarfaty (2008), and Lee, Naradowsky, and Smith (2011) presentjoint models where the processes of predicting morphological information and syn-tactic information are performed at the same time.
All three approaches acknowl-edge the fact that syntax and morphology are heavily intertwined and interact witheach other.Our attempt at tackling the technical and linguistic challenges can be characterizedas follows: In Section 6, we propose a system architecture that at the basic level followsa pipeline approach, where local data-driven models are used to predict the highestscoring output in each step.
But this pipeline is complemented with a knowledge-based component modeling grammatical knowledge about inflectional paradigms andmorphological marking of grammatical relations.
Both parts are combined using a setof global constraints that model the language-specific morpho-syntactic dependenciesto which a syntactic structure in that language has to adhere.
These constraints are27Computational Linguistics Volume 39, Number 1used to weed out linguistically implausible structures among the candidate outputsof the parser.
Our architecture thus resides between a strict pipeline approach whereno step can influence previous results, and a full joint model, where several subtasksare predicted simultaneously.
Using the global constraints we can precisely define theparts of the structure where an interaction between the morphology and the syntax isallowed to take place.The key design tasks are of a representational nature: What are the linguistic unitsfor which hard constraints can (and should) be enforced in a language?
(For example,within Czech and German nominal phrases, indexing of case, number, and genderfollows a strict regime?the values have to co-vary.)
What underspecified interfacerepresentation is appropriate to negotiate between the potentially ambiguous outputof one local component and the assumed input of another component?
How can werestrict them as much as possible without sacrificing the correct solution?
As it turns out,the explicit enforcement of conservative linguistic constraints over morphological andsyntactic structures in decoding leads to significantly improved parsing performanceon case-bearing dependents, and also to improved overall performance over a state-of-the-art data-driven pipeline approach.2.2 Case Between Morphology and SyntaxIn this article, we concentrate on the case feature, which resides at the interface be-tween morphology and syntax.
The case feature overtly marks (when unambiguous)the syntactic function of a nominal element in a language.
Languages show differentsophistication in their case systems.
Where German has four different case values,Hungarian uses a complex system of about 20 different values.
In all languages witha case system, it is used to distinguish and mark the function of the different argumentsof verbs (Blake 2001).
Correctly recognizing the argument structure of verbs is one of themost important tasks in automatic syntactic analysis because verbs and their argumentsencode the core meaning of a sentence and are therefore essential to every subsequentsemantic analysis step.The three languages investigated in this article, German, Czech, and Hungarian,belong to the broad category of morphologically rich languages.
Syntactically, they alluse a case system to mark the function of the arguments of a verb (and a preposi-tion).
The morphological realization of these case systems show important differences,however, which have a direct influence on syntactic analysis.
Czech and German areboth Indo-European languages, Czech from the Slavonic branch and German fromthe Germanic branch.
Hungarian, on the other hand, is a Finno-Ugric language ofthe Ugric branch.
Czech and German both are fusional languages, where nominalinflection suffixes signal gender, number, and case values simultaneously.
Hungar-ian is an agglutinating language, namely, every morphological feature is signaledby its own morpheme, which is appended to the word.
Whereas Hungarian has amostly unambiguous case system, Czech and (more so) German show a considerableamount of syncretism in their nominal inflection.
It is this syncretism that makes it somuch harder for a statistical system to learn the morphological marking patterns of alanguage.Table 1 shows examples of declension paradigms for the fusional languages Czechand German.
Note that these are only examples and cannot represent the entire com-plexity of the systems.
We use them here to exemplify the widespread morphologicalsyncretism in these two languages.
In the masculine animate noun of Czech bratr(?brother?
), ACC/GEN SG, DAT/LOC SG, and ACC/INS PL use the same word forms28Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency ParsingTable 1Examples of nominal declension paradigms in Czech and German.
German never distinguishesgender in plural.Czech, masc.
animate noun brother Czech, neuter noun cityMASC ANI SG PL NEUT SG PLNOM bratr bratr?i NOM me?sto me?staACC bratra bratry ACC me?sto me?staDAT bratrovi/u bratru?m DAT me?stu me?stu?mGEN bratra bratru?
GEN me?sta me?stVOC bratr?e ?
VOC me?sto ?LOC bratrovi/u bratrech LOC me?ste?/u me?stechINS bratrem bratry INS me?stem me?styGerman, definite determiner German, masculine noun German, feminine nounthe dog womanMASC NEUT FEM PL MASC PL FEM PLNOM der das die die NOM Hund Hunde NOM Frau FrauenACC den das die die ACC Hund Hunde ACC Frau FrauenDAT dem dem der den DAT Hund Hunden DAT Frau FrauenGEN des des der der GEN Hundes Hunde GEN Frau Frauenrespectively.3 In the neuter noun me?sto (?city?)
we find syncretism in NOM/ACC SG andPL, and DAT/LOC SG.
The NOM/ACC syncretism in neuter nouns is a typical propertyof Indo-European languages (Blake 2001).
Note also that some inflection morphemesfill different paradigm cells, for instance bratra is ACC SG, me?sta is NOM PL.
To resolvethe ambiguity, gender and number features need to be considered.Unlike Czech, German has determiners, which are also marked for case and agreewith their head noun in the so-called phi-features (gender, number, case).
The declen-sion patterns of determiners and nouns in German have developed in different ways,leading to highly case-ambiguous forms for nouns.
We see in Table 1 two Germannouns, a masculine one and a feminine one.
Although the declension paradigm of themasculine noun has kept some residual formal marking of case in the GEN SG and theDAT PL, the declension pattern of the feminine noun does not show case distinction atall.
Both nouns, however, mark the number feature overtly.
The paradigm of the de-terminer is much less ambiguous in the case dimension, but shows syncretism betweendifferent number and gender features.
Eisenberg (2006) calls the distribution of differentkinds of syncretism over different parts of the German noun phrase Funktionsteilung(?function sharing?).
It makes the morphological agreement between German nounsand their dependents extremely important because only by agreement can a mutualdisambiguation take place and reduce the morpho-syntactic ambiguity for the nounphrase.
We will show that for the fusional languages Czech and German, automaticmorphological analyzers have problems predicting the correct case, number, and gen-der values, whereas for the agglutinating language Hungarian, the unambiguous caseparadigm makes case prediction extremely easy.3 NOM: nominative, GEN: genitive, DAT: dative, ACC: accusative, LOC: locative, INS: instrumental, SG:singular, PL: plural, M/MASC: masculine, F/FEM: feminine, N/NEUT: neuter.29Computational Linguistics Volume 39, Number 1??
???
?Jako?aspr?edkapelanomsupport bandseaccthemselvespr?edstav???presentkapelanombandAmbivalency?
?AuxYObj4 SbAtrAtvThe band Ambivalency performs as support bandFigure 1A dependency tree from the Czech treebank.
Sentence no.
3,159 in the CoNLL 2009 data set.In order to see the influence of morphology on today?s data-driven systems forsyntactic analysis, we investigate the performance of a state-of-the-art dependencyparser (Bohnet 2009, 2010) on the three languages just described paying special attentionto the handling of the core grammatical functions (i. e., the argument functions of verbs).Dependency syntax (Hudson 1984; Mel?c?uk 1988) models the syntactic structure of asentence by directed labeled links between the words (tokens) of a sentence.
Figure 1shows an example tree for a Czech sentence.
Every word of the tree is attached to exactlyone other word (its head) by a labeled arc whose label specifies the nature of the relation.For instance, kapela is labeled as subject (Sb) of the sentence.
Morphologically, the subjectis marked with nominative case (nom) whereas the direct object (Obj4) is marked withaccusative case (acc).
We see that the object can precede the verb.
Syntactically, Czechallows for all permutations of subject, object, and verb (Janda and Townsend 2000,page 86).
It is thus a free word order language.
Another property of free word orderlanguages is the higher amount of non-projective structures (compared with English).Non-projective structures are indicated by crossing branches in the tree structure, asbetween kapela and pr?edkapela in Figure 1.3.
Parsing ArchitectureIn this section, we give a brief description of the parser that we use in the first exper-iment, where we analyze the performance of the parser with respect to morphologicalinformation.
The parser is the state-of-the-art data-driven second-order graph-baseddependency parser presented in Bohnet (2010).4 It is an improved version of the parserdescribed in Bohnet (2009), which ranked first for German and second for Czech forsyntactic labeled attachment score in the CoNLL 2009 Shared Task (Hajic?
et al.The parser follows the standard pipeline approach.
Information about lemma,POS, and morphology is automatically predicted and fully disambiguated prior tothe parsing step.
The CoNLL 2009 Shared Task used a tabbed format where everytoken in a sentence is represented by a line of tabulator-separated fields holdinggold standard and predicted information about word position, word form, lemma,POS, morphology, attachment, and function label.
Figure 2 gives an example for theword se in the sentence in Figure 1.
Note that for every type of information, the4 http://code.google.com/p/mate-tools, version: anna-2.30Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing3 se se se P P SubPOS=7|Num=X|Cas=4 SubPOS=7|Num=X|Cas=4 4 4 Obj4 Obj4Figure 2Example of the CoNLL 2009 dependency format for se.
Columns are from left to right:Position, word form, gold lemma, predicted lemma, gold POS, predicted POS, goldmorphology, predicted morphology, gold head position, predicted head position, goldfunction label, predicted function label.
Semantic information is not displayed.
The goldstandard columns are used for evaluation purposes.human-annotated gold standard and the predicted value by an automatic tool is rep-resented.
The morphology columns contain several morphological features separatedby a vertical bar.The parser itself consists of two main modules, the decoder and the feature model.It is a maximum-spanning-tree5 parser (McDonald et al McDonald and Pereira2006) that searches for the best-scoring tree using a chart-based dynamic programmingapproach similar to the one proposed by Eisner (1997).
The substructures are scored bya statistical feature model that has been trained on treebank data; the best-scoring tree isthe tree with the highest sum over the scores of all substructures in the tree.
The actualimplementation is derived from the decoder by Carreras (2007), which was shown to beefficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008;Bohnet 2009).The features used in the statistical model are combinations of basic features, namely,word form, lemma, POS, and morphological features.
In addition, the distance betweentwo nodes, the direction of the edge, and the words between head and dependent areincluded.
Every feature is combined with the function label on the edge.
A detaileddescription of the feature model is beyond the scope of this article, but the interestedreader can find it in Bohnet (2009, 2010).Because we are interested in the way the parser handles morphological information,we will briefly discuss the inclusion of morphological features as described in Bohnet(2009, page 3).
The parser computes morphological features by combining the part-of-speech tags (pos) of the head and the dependent with the cross-product of theirmorphological feature values.
For this, the morphological information (see Figure 2:columns 7 and 8) is split at the vertical bar and every single morphological featurevalue is treated as one morphological feature in the statistical model.
The cross-productthen pairs the single feature values of dependent and head creating all combinations.One single feature computed for the edge between an adjective and a noun in Czechmay then look like (A,N,acc,acc), which states the information that both words havethe accusative case.
Other features are created as well, however, that might look like(A,N,sg,masc), which states that the adjective has singular number and the noun hasmasculine gender.
So the algorithm does not pay attention to category classes.
Further-more, the whole cross-product is computed for every edge in the tree.
All features areadditionally combined with the function label between the head and the dependent,so in the parsing features, a morphological feature like case is directly combined withthe function label with which it appears together in the treebank.
Because of this, theparser should have direct access to the information about which case value signalsa particular grammatical function.
Intuitively, the statistical model should learn thatcertain dependent head configurations often occur with certain morphological feature5 Or graph-based as opposed to transition-based (Nivre et al; Bohnet 2011).31Computational Linguistics Volume 39, Number 1combinations.
For example, a subject edge between a noun and a verb should very oftenoccur together with morphological features involving nominative case, and a dativeobject edge should often occur with a dative feature.The statistical model is a linear multi-class classifier, trained using an on-line learn-ing procedure (MIRA [Crammer et al with a hash kernel [Bohnet 2010]).
Learningis an iterative process where the parser repeatedly tries to recreate the training corpussentence by sentence.
If the parser makes no mistakes, it proceeds to the next traininginstance.
Otherwise, the feature weights for the tree that would have been correct andthe feature weights for the tree produced by the parser are compared and the weightsin the feature model are adjusted to favor the correct tree and disfavor the incorrect one.The parser repeatedly parses the treebank, adjusting its feature model to produce treesthat match the trees in the training data.
Because the decoder can only derive projectivetrees (without crossing edges), the parser reattaches individual edges in the tree ina post-processing step to allow for non-projective trees (crossing edges, see Figure 1)using the algorithm in McDonald and Pereira (2006).4.
DataBefore we turn to our first experiment and its analysis, we briefly describe the datasets that we used in the experiments and discuss the quality of the morphologicalannotation.
In a pipeline architecture, where morphological features are fully disam-biguated prior to parsing, low quality in the predicted morphological information willhave considerable impact on the ability of the parser to learn the mapping betweencase and grammatical functions that we want it to learn.
Furthermore, the errors madein the morphological preprocessing are the first observable difference between the twofusional languages and the agglutinating language and directly reflect this typologicaldifference.
We will thus show that whereas the morphological preprocessing for Czechand German makes mistakes because of the syncretism in the morphological paradigms,the morphological preprocessing for Hungarian suffers from a different problem.All the data sets come from the newspaper domain.
The Czech data set is theCoNLL 2009 Shared Task data set consisting of 38,727 sentences from the PragueDependency Treebank (Bo?hmova?
et al Hajic?
et al.
The German data set(Seeker and Kuhn 2012) is a semi-automatically corrected recreation of the data set thatwas used in the CoNLL 2009 Shared Task (36,017 sentences).
It uses the exact6 same raw(surface) data but contains a different syntactic annotation.
It was semi-automaticallyderived from the original TIGER treebank (Brants et al and some time was spenton manually correcting incorrect function labels and POS tags.
The Hungarian dataconsist of the general newspaper subcorpus (10,188 sentences) of the Szeged Treebank(Csendes, Csirik, and Gyimo?thy 2004), which was converted from the original con-stituent structure annotation to dependency annotation and manually checked by fourtrained linguists (Vincze et al.
For the experiments in the following sections, weuse the training splits for Czech and German, and the whole set for Hungarian.For the Czech and the Hungarian data, we kept the predicted information forlemmata, POS, and morphology that was already provided with the data.
For both6 Except for three sentences that for some reason were missing in the 2006 version of the TiGer treebank,from which this corpus was derived.
The original data set in the CoNLL 2009 Shared Task was derivedfrom the 2005 version, which still contains these three sentences.
The 2005 version also containedspelling errors in the raw data that had been removed in the 2006 version.
These errors were manuallyreintroduced in order to recreate the data set as exact as possible.32Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsinglanguages, this information is predicted in a two-step process where a finite-stateanalyzer produces a set of possible annotations for a given verb form, which is thendisambiguated by a statistical model trained on gold-standard data (for Czech, seeSpoustova?
et al]; for Hungarian, see Zsibrita, Vincze, and Farkas [2010]).
TheGerman data set was cross-annotated by applying statistical tools7 trained on the goldstandard annotation.
Contrary to the Czech and Hungarian data sets, lemma, POS,and morphological information were annotated in three steps, each building upon thepreceding one.In preparation for the experiments, we made two changes to the annotation in theCzech and the Hungarian treebanks in order to allow for a more fine-grained analysis.First, we copied the SubPOS feature value8 over to the respective POS column (goldto gold, predicted to predicted).
This helps us in doing a more fine-grained evaluation,which is based on certain POS tags, but it also allows us to formulate linguistic con-straints in the ILP parser more precisely, as we will see in Section 6.1.
The German POStag set is already rather specific.
We also changed the object labels (Obj) in the Czechdata set by combining it with the case value in the gold standard morphology (creatingObj1-7).
This gives us a more fine-grained object distinction for our analysis and it alsoseparates the case-marked objects from the clausal objects, which do not have a casefeature and therefore keep the original Obj label.9In order to learn the mapping between case and grammatical functions, the parserrelies on the automatically predicted morphological information in the data sets.
Whenthe parser is trained on predicted morphology, in principle, it has the chance to adaptto the errors of an automatic morphological analyzer.
We will see in Section 5, however,that this does not seem to happen very often.
Therefore, if we want the parser to performwell, we need to predict morphological information with high quality.
Table 2 showsthe prediction quality of the automatic morphological analyzers in the three data sets.On the left-hand side, precision and recall are shown for the phi-features for the wholedata set; on the right-hand side, only those words were evaluated where the predictedPOS tag matched the gold standard one.
We see that Czech and Hungarian achievehigh scores on all three features, with Czech achieving over 95% for each feature, andHungarian over 94% recall and almost 98% precision.
In contrast, we find a rathermediocre annotation in the German data set, where only the number feature can bepredicted with comparable quality,10 and gender and case prediction is rather bad.
To acertain extent, the lower performance for German compared to Czech can be explainedby the more informed annotation tool for Czech.
The German data set was annotatedby purely statistical tools whereas the Czech annotation tool uses a finite-state lexiconto support the statistical disambiguator.Hungarian shows a big gap between precision and recall (97.83% and 94.11% forcase) when evaluating all words, but the performance on the words with the correctPOS tag is almost perfect (99.22% for case!).
The reason lies in the POS recognition.The Hungarian POS tag set uses a category X as a kind of a catch-all category whereannotators would put tokens they could not assign anywhere else.
The precision for thisclass is below 10%, because the tool is classifying a considerable amount of proper nouns(Np) as X.
The class X, however, does not get a morphological specification so that about7 Mate-tools by Bernd Bohnet: http://code.google.com/p/mate-tools.8 The SubPOS feature distinguishes subcategories inside the main POS categories and is part of themorphological description (see Figure 2).9 Prepositional objects headed by prepositions (pos: RR, RF, RV) were also excluded.10 There are only two values to predict though.33Computational Linguistics Volume 39, Number 1Table 2Annotation quality of the phi-features (case, gender, and number) for all words and for thosewords with a correctly predicted POS tag.all correct POSprecision recall precision recallCzech case 95.73 95.63 96.06 96.06gender 97.59 97.45 98.03 98.03number 98.18 98.08 98.47 98.47German case 88.69 88.51 89.26 89.06gender 90.16 89.99 90.95 90.74number 96.18 95.63 96.92 96.61Hungarian case 97.83 94.11 99.22 99.22number 98.64 95.91 99.88 99.883,500 out of 12,500 proper nouns do not receive a case and a number value at all.
Thereason for the poor morphological annotation in Hungarian is apparently not a problemof an ambiguous morphology, it is simply a problem of the POS recognition.
We alreadyknow that Hungarian is an agglutinating language.
The case paradigm of Hungarian,although comprising about 20 different case values, does not show syncretic forms withthe exception of a regular genitive-dative syncretism.
Whereas in Hungarian, gettingthe POS correct effectively means getting case and number correct, the results in Table 2for Czech and German11 are not much better for words with correctly predicted POStags than for all words.
In Czech and German, this is a problem of the syncretism in themorphological paradigms.The low syncretism in the Hungarian case paradigm is due to the agglutinatingnature of its morphological system.
Because every feature (e.g., case) is signaled byits own morpheme, a syncretism in the system would erase the distinction betweenthe syncretic forms.
Because Hungarian uses the same case paradigm for all words,a regular syncretism would mean that a certain distinction can no longer be made inthe language.12 In fusional languages, an inflection morpheme signals more than onefeature value.
Many syncretisms can thus be disambiguated by the other feature valuesor by agreement with dependents, as is done in the German noun phrase.
We learntwo things from these findings: First, we may need different approaches for handlingmorphology in fusional languages like Czech and German than we do for agglutinatinglanguages like Hungarian.
And second, the category morphologically rich encompasses11 The fact that for German, precision and recall differ is due to the independency of the POS taggerand the morphological analyzer.
In the German data, the morphological analyzer is not boundto a certain feature template determined by the POS of the word, so that, in principle, it canassign case to verbs and tense to nouns.
This is not the case for the Czech and Hungarian analyzers.Precision, recall, and F-score measured over all possible values amount to simple accuracy in thoselanguages.12 One of the reviewers pointed out to us that Turkish as an agglutinating language also shows muchmorphological ambiguity.
That is correct and this also holds for Hungarian.
The case paradigm itselfseems to have no syncretism in Turkish, however.
The ambiguity rather comes from interaction withvowel harmony and definiteness marking.
The syncretism between genitive and dative case in theHungarian case system is more of a puzzle.
Our best guess is that the distribution of these cases isso different that the context can disambiguate them relatively easily.34Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsinglanguages that are not only different from English but also show important differencesamong each other that we should take into account when devising parsing technology.5.
Experiment 1Having examined the quality of the predicted morphological information in the datasets, we can now investigate how the parser deals with this information.
We proceedas follows: We train three different models for each language, one using gold standardmorphology, one using predicted morphology, and one using no morphological infor-mation (henceforth GOLD-M, PRED-M, and NO-M).
Comparing the performance of thesethree models allows us to see the effect that the morphological information has on theparsing performance.
The model using gold morphology serves as an upper boundwhere we can observe the behavior of the parser when it is not disturbed by errorscoming from the automatic morphological analyzers.
Note that this model is very unre-alistic in the sense that syncretisms are fully resolved in the morphological information.The model using predicted morphology serves as a realistic scenario where we canobserve the problems introduced by imperfect preprocessing and propagated errorsin the pipeline (e.g., due to syncretism).
And finally, the model using no morphologyshows us how much non-morphological information contributes to the parsing perfor-mance.
In comparison with the other two models, we can then see the contribution ofmorphological information13 to the parsing process.
All models use the same predictedlemma and POS information as discussed in the previous section.5.1 Experimental Set-upWe performed a five-fold cross annotation14 on the training portions of the data sets ofCzech and German, and on the whole subcorpus of Hungarian, varying the morpho-logical annotation as described.
The overall parsing performance is shown in Table 3,where the German and the Hungarian scores exclude punctuation and the Czech scoresinclude them.15Table 3 gives us the usual picture that has been noticed in several shared taskson dependency parsing for multiple languages (e.g., CoNLL-ST 2006, 2007, 2009).
Theperformance on German is pretty high, although not as high as it would be for English,and the performance on Czech is rather low.
Note the extreme divergence betweenlabeled (LAS) and unlabeled attachment score (UAS) for Czech.16 For Hungarian, theperformance is comparable to Czech in terms of UAS but the LAS for Hungarian isbetter.
We also see the expected ordering in performance for the models using dif-ferent kinds of morphological information.
The gold models always outperform themodels using predicted morphology, which in turn outperform the models using nomorphological information.
Note, however, that whereas the performance on Germandoes not degrade very much when using no morphological information, it is very13 It should be noted that by morphological information we always mean the complete annotation availablein the treebanks.
Although we concentrate in the analysis on the phi-features (gender, number, case), themodels using morphological information always use the whole set, including also, for example, verbalmorphology.14 The number of iterations during training was set to 10.15 Punctuation in the Czech data set is sometimes used as the head in coordination.16 This is due to the way the Czech data label certain phenomena, which makes it difficult for the parser todecide on the correct label.
See Boyd, Dickinson, and Meurers (2008, pages 8?9) for examples.35Computational Linguistics Volume 39, Number 1Table 3Overall performance of the Bohnet parser on the five-fold cross annotation for every languageand different kind of morphological annotation.
All results in percent.
LAS = labeled attachmentscore; UAS = unlabeled attachment score.
Results for German and Hungarian are withoutpunctuation.
Best score for Czech on the CoNLL 2009 Shared Task was by Gesmundo et al(2009), best score for German was by Bohnet (2009), best score for Hungarian on the CoNLL2007 Shared Task was by Nivre et ala).
Best CoNLL 09/07 results were obtained ondifferent data sets.Czech German HungarianLAS UAS LAS UAS LAS UASGOLD-M 82.49 88.61 91.26 93.20 86.70 89.70PRED-M 81.41 88.13 89.61 92.18 84.33 88.02NO-M 79.00 86.89 89.18 91.97 78.04 86.02best on CoNLL 09/07 80.38 ?
87.48 ?
80.27 83.55harmful for Hungarian to do so (78.04% LAS for NO-M in comparison with 84.33%LAS for PRED-M).
The Czech results lie in between.
To give a general impression ofthe performance of the parser, the last row shows parsing results for the three languagesreported in the literature.
The results have been obtained on different data sets, however,so a direct comparison would be invalid.5.2 AnalysisAlthough the scores in Table 3 reflect the quality of the parser on the complete testdata, we would not expect case morphology to influence all of the functions.
We willtherefore go into more detail and concentrate on nominal elements (nouns, pronouns,adjectives, etc.
)17 and core grammatical functions (subjects, objects, nominal predicates,etc.)
because in our three languages, nominal elements carry case morphology to marktheir syntactic function.
Core grammatical functions are vital to the interpretation ofa sentence because they mark the participants of a situation.
We exclude clausal andprepositional arguments, which can fill the argument slot of a verb but would notbe marked by case morphology.
Table 4 shows the encoding of the core grammaticalfunctions in the three treebanks.Table 5 shows the performance of the parsing models for each of the three languageson the core grammatical functions.
As described in Section 4, we split the object functionfor Czech according to its associated case value.
The results are shown for each of thethree models with GOLD-M on the left, PRED-M in the middle, and NO-M on the right.The results shown for the NO-M models indicate again that morphology plays abigger role in Czech and Hungarian for determining the core grammatical functionsthan it does for German.
The performance on all grammatical functions except therather rare genitive object is generally higher for German, showing that to a large17 We determine a nominal element by its gold standard POS tag:Czech: AA, AG, AM, AU, C?, Ca, Cd, Ch, Cl, Cn, Cr, Cw, Cy, NN, P1, P4, P5, P6, P7, P8, P9, PD, PE, PH,PJ, PK, PL, PP, PQ, PS, PW, PZ.German: ADJA, ART, NE, NN, PDAT, PDS, PIAT, PIS, PPER, PPOSAT, PPOSS, PRELAT, PRELS, PRF,PWS, PWAT.Hungarian: Oe, Oi, Md, Py, Oh, Ps, On, Px, Pq, Mf, Pp, Pg, Mo, Pi, Pr, Pd, Mc, Np, Af, Nc.36Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency ParsingTable 4Core argument functions and their encoding in the different treebanks.
The different objectlabels for Czech have been introduced by us.
The original function is simply Obj.PDT 2 (Czech) TiGer (German) HunDep (Hungarian)subject Sb SB SUBJnominal predicate Pnom PD PREDobject Obj1-7 OA, OA2, DA, OG OBJ, DATTable 5Precision, recall, and F-score (LAS) for core grammatical functions marked by case.
We omitlocative objects in Czech, and second accusative objects in German, due to their low frequency.GOLD-M PRED-M NO-MCzech freq prec rec f prec rec f prec rec fsubject 38,742 89.29 91.18 90.22 83.96 87.01 85.46 74.10 78.82 76.39obj (acc) 21,137 92.50 93.35 92.93 85.25 83.01 84.12 73.42 72.02 72.71predicate 6,478 89.07 87.14 88.09 88.24 86.00 87.11 82.34 78.19 80.21obj (dat) 3,896 83.18 85.68 84.41 80.21 78.88 79.54 74.29 48.05 58.35obj (instr) 1,579 71.38 66.50 68.85 67.74 62.51 65.02 58.93 35.53 44.33obj (gen) 1,053 86.69 77.30 81.73 80.42 62.39 70.26 74.60 48.81 59.01obj (nom) 167 57.63 40.72 47.72 56.97 29.34 38.74 48.67 32.93 39.29GOLD-M PRED-M NO-MGerman freq prec rec f prec rec f prec rec fsubject 45,670 95.11 96.05 95.58 89.95 91.23 90.59 88.32 89.86 89.08obj (acc) 23,830 93.93 94.80 94.36 84.83 84.89 84.86 82.20 83.35 82.77obj (dat) 3,864 89.56 87.73 88.64 79.17 64.44 71.05 77.09 50.78 61.23predicate 2,732 78.07 73.35 75.64 75.80 72.91 74.33 76.20 71.01 73.51obj (gen) 155 80.25 41.93 55.08 60.66 23.87 34.26 52.94 17.42 26.21GOLD-M PRED-M NO-MHungarian freq prec rec f prec rec f prec rec fsubject 11,816 88.34 91.57 89.93 84.96 88.15 86.53 64.58 66.44 65.50obj (acc) 9,326 93.63 94.22 93.92 92.36 92.70 92.53 66.23 63.86 65.03obj (dat) 1,254 80.55 76.95 78.71 75.57 71.53 73.49 58.36 30.62 40.17predicate 941 81.05 75.45 78.15 77.39 72.37 74.79 72.49 71.41 71.95extent the parser is able to use information from lexicalization and configurationalinformation (Seeker and Kuhn 2011).
Results for Czech and Hungarian are lower inthe NO-M models.
They improve by large margins when switching to predicted mor-phology.
Czech accusative objects improve from 72.71% F-score to 84.12% F-score inthe PRED-M model.
In Hungarian, the F-scores for dative objects improve by over 33percentage points to 73.49% F-score when switching to the PRED-M model.
In contrast,although all the scores improve for German, improvements are generally low whenswitching from the NO-M to the PRED-M model.
The biggest improvement happens37Computational Linguistics Volume 39, Number 1for dative objects, which increase by about 10 percentage points, but for subjects, theimprovement is just over one percentage point.
This is in line with the general idea thatGerman is a borderline case between morphologically poor configurational languageslike English and morphologically rich non-configurational languages like Czech orHungarian.
We already saw this general trend in Table 3, but the effect is much largerif we consider those functions that are directly marked by morphological means inthe language.If we now turn to the GOLD-M models, we see that in general, German and Czechbenefit more from the gold standard morphological annotation than Hungarian.
Know-ing that Hungarian does not have much form syncretism in its inflectional paradigms,this is not really surprising.
There is, however, still a gain of information becausethe effect of the wrong POS tags in Hungarian is eliminated in the GOLD-M model.An effect that comes out very clearly is the improvement for subjects and accusativeobjects for Czech and German when moving from predicted to gold morphology,because the typical syncretism between nominative and accusative in the neuter gen-der in Indo-European languages (cf.
Table 1) is correctly disambiguated: Comparingthe performance on subjects (marked by nominative case) and accusative objects, wesee a considerable improvement between 5 percentage points for Czech subjects andalmost 10 percentage points for German accusative objects when switching to goldmorphology.
This improvement does not happen for Hungarian, where there is no suchsyncretism.
The gold morphology acts as an oracle here and circumvents the ambiguityproblem that a pipeline approach to predicting morphological information prior toparsing has.Another interesting observation related to the way the parser works is that for alllanguages, predictions are less accurate for the less frequent functions.
The generalorder for all three languages from most frequent to least frequent is subjects > accusativeobjects > predicates/dative objects > instrumental/genitive objects.
For all languages, theparser?s quality of annotation follows this ordering.
This effect comes from the statisticalnature of the parsing system, which will in case of doubt resort to the more frequentfunction.
A clear sign is that for rare objects, the precision is always higher than therecall.
As an example, notice the performance of the parsing models on dative andgenitive objects.
The parser annotates genitive objects if it has strong evidence, hencethe high precision, but it frequently fails to find it in the first place, hence the low recall.Because the NO-M models do not have morphological information, they can only relyon lexicalization and contextual information to determine the correct grammatical func-tion.
We can see this ranking in all the models regardless of the amount of morphologicalinformation available, although the differences are much smaller for the more informedmodels.Finally, we see that the benefit from morphological information is comparativelylow for nominal predicates.
It seems that the non-morphological context already pro-vides much useful information (e.g., the copular verbs).5.3 Analysis of Confusion ErrorsWe now ask ourselves if the parser utilizes the morphological information, in our casethe case morphology, correctly.
In principle, there are two possible scenarios: (1) thefeature model of the parser does not integrate the morphological annotation in a usefulway, so that the parser has difficulties learning the association between case values andthe grammatical functions; (2) There is nothing wrong with the feature model, but the38Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsingmorphological annotation is not good enough and causes problems because the parsergets incorrect information in the features.To answer this question, we examine the confusion errors made by the parser.If the parser uses morphological information correctly, we expect it to confuse labelsthat can all be signaled by the same case value.
For example, if the parser learns theassociation between nominative and subject/predicate properly, we would still expectit to make errors in confusing these two functions.
Because the mapping between caseand grammatical function is one-to-many, knowing the case value reduces the numberof possible functions but the final decision between these functions must be made bynon-morphological information.
The effect should be strongest in the GOLD-M modelsbecause the morphological information is correctly disambiguated.
Consequently, weexpect the same results for the PRED-M models blurred by additional errors introducedby an imperfect morphological prediction.
If, however, the parser does not learn themapping or has no access to morphological information, we expect confusion errors allacross the case paradigms.To start with the last hypothesis, we examine the confusion errors with subjectsmade by the parser using the NO-M models.
Subjects are marked by nominative case inall three languages, with Czech allowing for dative and genitive subjects under specialcircumstances.
The NO-M models do not have access to morphological informationand should therefore mix up functions regardless of the case value that would usuallydistinguish them.
Table 6 shows the top five confusion errors made by the NO-M modelson the subject function.
The values are split for correct and incorrect head selection totease apart simple label classification errors from errors involving label classificationand attachment.Table 6Top five functions with which subjects were confused when parsing with the NO-M models.M marks a coordinated function in Czech.Czech GermanNO-M correct head wrong head NO-M correct head wrong headrank label freq label freq rank label freq label freq1 Obj4 4,996 Atr 2,644 1 OA 2,680 OA 1,4982 Pnom 1,261 Obj4 981 2 PD 776 NK 9063 Adv 811 Sb M 948 3 DA 458 DA 4314 Obj3 752 Adv 273 4 EP 301 AG 3135 Obj7 380 Obj M 245 5 MO 219 CJ 296HungarianNO-M correct head wrong headrank label freq label freq1 OBL 3,029 ATT 1,1162 OBJ 1,505 Exd 5743 PRED 250 COORD 3134 ATT 185 OBL 3115 DAT 152 OBJ 13939Computational Linguistics Volume 39, Number 1The results in Table 6 confirm the expectation that confusion errors appear regard-less of the case value involved, which is no surprise given that the models do not haveaccess to morphological information: For Czech, when the head was chosen correctly,Obj4, Obj3, and Obj7 (accusative, dative, and instrumental objects, respectively) are allsignaled by a different case value and their confusion rates follow their frequency inthe data.
Pnom (nominal predicates) are expected because they are also signaled bynominative case as are subjects.
If the head was chosen incorrectly, the parser assignsObj4 and coordinated subjects and objects (Sb M, Obj M).
Adverbial (Adv) and attribu-tive functions (Atr) are expected as they mark adjunct functions that can be filled bynominal elements.
For German, we see confusions with the object functions (accusativeOA and dative objects DA), predicates (PD), and the EP function marking expletivepronouns in subject position.
Both are marked by nominative case.
Furthermore, theparser makes confusion errors with MO, NK, and AG, which are the three adjunctfunctions that can be filled by nominal elements (e.g., AG marks genitive adjuncts).CJ finally marks coordinated elements, which is an expected error if the head waschosen incorrectly, but, unlike in the Czech treebank, we cannot tell by the coordinationlabel the particular function the element would have if it were not coordinated.
InHungarian, we also have errors across the board, with argument functions not markedby nominative case (accusative objects OBJ, dative objects DAT), the predicate functionPRED, and all types of adjuncts (ATT [attributives] and OBL [obliques]).
Obliques areespecially interesting in Hungarian because the language has only a small number ofprepositions.
Most oblique adjunct functions are realized by a particular case (hencethe about 20 different case values), which for a parsing model using no morphologi-cal information makes it rather difficult to distinguish them from the core argumentfunctions.
In summary, we find the expected picture of confusion errors across the caseparadigms.Turning now to the GOLD-M models, we can test whether the parser is able tolearn the mapping between case and its associated functions.
If so, we expect confusionerrors with functions that are all compatible with the case value of the correct function.Table 7 shows the top five confusion errors that the GOLD-M models made on the subjectfunction.
Here, we see a completely different picture compared with the NO-M modelerrors in Table 6.
In all three languages, we find?regardless if the head is correct ornot?confusions only with functions that are compatible with the nominative case.
InCzech, subjects are mostly confused with predicates (Pnom) and coordinated subjects(Sb M).
ExD marks suspended nodes moved because of an elliptical constructions.
Thelabel does not tell whether the node would be a subject with regard to the emptynode but it may be, so it is compatible with nominative case.
Atr between nominalelements may mark close appositions like the one in Figure 1, which would be markedas nominative by default.
ObjX marks objects with no annotated case value (mostly forforeign words).
Of all the functions, only Obj4 cannot be signaled by nominative case.If one checks those 69 cases, only 22 are annotated with accusative case in the goldstandard, the rest consist mostly of various, high-frequent numerals in neuter genderand quantifiers, most of which are ambiguous between nominative and accusative.
Inthese cases, lexicalization seems to overrule the case feature.
We get the same picturefor German and Hungarian, both models making errors that are compatible with thenominative case value.
Of the 112 errors with accusative objects (OA) in German, only 36have the correct case value in the gold standard.
Unlike in the Czech and the Hungariantreebank, the morphological annotation in TiGer contains a considerable number oferrors.
We then conclude that for subjects, the parser indeed has no problem learningthat subjects are marked by nominative case.40Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency ParsingTable 7Top five functions with which subjects were confused when parsing with the GOLD-M models.M marks a coordinated function in Czech.Czech GermanGOLD-M correct head wrong head GOLD-M correct head wrong headrank label freq label freq rank label freq label freq1 Pnom 583 Sb M 1,142 1 PD 773 NK 5552 ObjX 102 Atr 711 2 EP 323 CJ 2453 Adv 102 ExD M 162 3 MO 117 PNC 1394 Obj4 69 ExD 145 4 OA 112 PD 1295 ExD 45 Pnom 65 5 PH 96 APP 127HungarianGOLD-M correct head wrong headrank label freq label freq1 PRED 264 ATT 6782 Exd 102 Exd 4943 OBL 94 COORD 2494 ATT 90 NE 325 OBJ 50 DET 22Next, we examine the accusative objects and compare the performance of theGOLD-M models with their respective PRED-M counterparts to assess the effect ofpredicted morphological information.
Table 8 shows the confusion errors for the ac-cusative objects.
On the left, the GOLD-M errors are shown; on the right we see thePRED-M errors.
For the GOLD-M models, the picture is basically the same as with thesubjects, with the small exception that all three languages show confusion with subjectsunder the top five.18 Although the effect is not strong, it shows that the statisticalmodel can sometimes overrule the morphological features even for the gold standardmorphology.The most interesting effect, however, happens when switching to predicted mor-phological information.
The overall number of errors increases, but the biggest in-crease occurs for subjects in German (SB) and in Czech (Sb), although the same is notobservable in Hungarian (SUBJ).
Of the 2,945 confusion errors in Czech, where thePRED-M model incorrectly predicts an accusative object, 891 have been classified asaccusative despite being nominative in the gold standard and 1,505 have been classifiedas nominative although being accusative.
If we check the gender of these instances, wefind the overwhelming majority to be neuter, feminine, or masculine inanimate, exactlythose genders whose inflection paradigms show syncretism between nominative andaccusative forms.
We find the same effect in the German errors.
The syncretism inthe two languages causes the automatic morphological analyzers to confuse these case18 The AuxT label in the Czech errors is used to mark certain kinds of reflexive pronouns, which can be inaccusative or dative case.
The criterion for deciding whether a reflexive pronoun is labeled AuxT or Obj4(i.e., accusative object) is whether the governing verb denotes a conscious or unconscious action.
This is avery tough criterion to learn for a dependency parser.
In any case, however, AuxT is perfectly compatiblewith accusative case.41Computational Linguistics Volume 39, Number 1Table 8Top five functions with which accusative objects were confused when parsing with the gold(left) and predicted (right) morphology models.
M marks a coordinated function in Czech.CzechGOLD-M correct head wrong head PRED-M correct head wrong headrank label freq label freq rank label freq label freq1 Adv 274 Obj M 750 1 Sb 2,354 Atr 6872 AuxT 270 Atr 172 2 Adv 262 Obj M 6603 Sb 69 ExD M 67 3 AuxT 256 Sb 5944 ExD 34 Adv 65 4 Obj3 137 Sb M 1085 AuxR 28 Atv 53 5 Obj2 109 ExD M 94GermanGOLD-M correct head wrong head PRED-M correct head wrong headrank label freq label freq rank label freq label freq1 MO 283 NK 357 1 SB 2,176 SB 1,3292 SB 112 CJ 191 2 DA 610 NK 6063 DA 55 SB 121 3 MO 308 CJ 3654 CJ 43 APP 97 4 CJ 46 AG 1375 EP 25 MO 55 5 EP 40 APP 136HungarianGOLD-M correct head wrong head PRED-M correct head wrong headrank label freq label freq rank label freq label freq1 OBL 90 COORD 119 1 OBL 119 COORD 1402 ATT 60 Exd 81 2 SUBJ 86 Exd 1113 SUBJ 50 ATT 44 3 ATT 65 ATT 784 Exd 23 OBL 18 4 Exd 19 ROOT 185 MODE 14 ROOT 13 5 MODE 16 OBL 15values more often, which subsequently leads to errors in the parser due to the pipelinearchitecture.
That the parser so frequently falls for incorrect annotation is more proofthat it has learned the mapping between case and its associated grammatical functions.As expected, we do not find this effect for Hungarian.
As we discussed in Section 4, thereis almost no syncretism in the Hungarian case paradigm, which therefore does not leadto this kind of error propagation.
The slight increase in errors in Hungarian is insteadrelated to the POS errors and their influence on missing morphological information thanthe quality of the predicted morphology itself.For reasons of space and because it would not contribute anything new to thepicture, we will not go into detail for the errors for the remaining grammatical functions.We conclude that learning the morphological dependencies that hold for a language(cf.
the four types by Nichols [1986]) can be facilitated by a statistical model.
Whenpresented with gold standard morphological information, the parser performance im-proves considerably over the model without morphological information for all three42Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsinglanguages.
The error analysis shows that the parser learns the mapping between caseand grammatical function, which also shows that the feature model of the parserintegrates the information in a useful way.
In the more realistic scenario using pre-dicted morphology, however, the parser starts making more mistakes for Czech andGerman that are caused by errors of the automatic morphological predictors, whichare propagated through the pipeline model.
This effect does not occur for Hungarian.The syncretism in the inflectional paradigms in Czech and German makes the task oflearning the morpho-syntactic rules of a language much more difficult for a statisticalparser in a pipeline architecture.
With a high amount of syncretism, it is simply notsensible to fully disambiguate certain morphological properties of a word (e.g., case)without taking the syntactic context into account.6.
Case as a FilterFrom Experiment 1 we learned that one of the problems when parsing morphologi-cally rich languages like Czech and German is the propagation of annotation errorsin the processing pipeline and the unreliable morphological information.
The problemis that the parser learns a mapping between case values and grammatical functionsbut the predicted morphology delivers the wrong case value.
As a solution to thisproblem, Lee, Naradowsky, and Smith (2011) have proposed a joint architecture wherethe morphological information is predicted simultaneously with the syntactic structure,so that both processes can inform and influence each other.
This puts morphologicalprediction and syntactic analysis on the same level.
We choose a different approachhere: We keep the basic pipeline architecture, because it works very efficiently.
Wesupport the parser, however, with constraints that model the possibly underspecifiedmorphological restrictions grounded in the surface forms of the words.
Especially forthe core argument functions, a morphological feature like case first and foremost servesas a morpho-syntactic means to support the syntactic analysis by overtly markingsyntactic relations and thus reducing the choice for the parser.
For example, if a wordform morphologically cannot be accusative, the parser should not consider grammaticalfunctions that are signaled by accusative in the language.
Case acts here as a filter onthe available functions for the morphologically marked element.
Interpreting the roleof case as a filter, we can use the case feature as a formal device to restrict the searchspace of the parser.
This is different from the joint model, where morphology and syntaxare predicted at the same time, because the parser will not fully disambiguate a tokenwith respect to its morphology if the syntactic context does not provide the necessaryinformation.
Another thing that we learned from the first experiment is that althoughthe predicted morphology is not completely reliable, it is still much better than usingnone at all, especially for Czech and Hungarian (see difference between PRED-M andNO-M models in Table 5).
In the following, we will therefore still use the predictedmorphology as features in the statistical model in combination with the filter.
In thisarchitecture, the parser gets statistical information from the feature model to prefer aparticular analysis, but the constraints will block this option if it does not comply withthe morphological specification of the words.
The parser then needs to choose a differentoption.In order to implement the constrained parser, we use a parsing approach byMartins, Smith, and Xing (2009) using integer linear programming.
It is related tothe Bohnet parser in the sense that it is also a graph-based approach, but it allowsus to elegantly augment the basic decoder with linguistically motivated constraints43Computational Linguistics Volume 39, Number 1(Klenner 2007; Seeker et al.
ILP is a mathematical tool for optimizing linearfunctions and was first used in dependency parsing by Riedel and Clarke (2006), whoperformed experiments on Dutch using linguistically motivated constraints as we willdo.
Martins, Smith, and Xing improved the formulation considerably so that the parserwould output well-formed dependency trees without the need for iterative solving.
Inour ILP parser, we use the formulation by Martins, Smith, and Xing extended to labeleddependency parsing.
Like the Bohnet parser, the ILP parser consists of a decoder anda statistical feature model.
Whereas the feature model remains basically the same, thedecoder is implemented using ILP.
The formulation represents every possible arc thatmight appear in the parse tree as a binary variable (arc indicator), where 1 signalsthe presence of the arc in the tree, and 0 signals its absence (see also Figure 3).
Eachsuch arc indicator variable is weighted by a score assigned by the statistical modelthat is learned from a treebank.
During decoding,19 the parser searches for the highestscoring combination of arcs that also fulfills the global tree constraints as well as anyother global constraints that may be added to the equations to model, for instance,linguistic knowledge.
The tree constraints ensure that every word in the tree has exactlyone head and that there are no cycles in the tree.
Martins, Smith, and Xing use thesingle commodity flow formulation by Magnanti and Wolsey (1995) to enforce the treestructure.
The idea is that the root node sends N units of flow through the tree (with Nbeing the number of words in the sentence) and every node in the tree consumes oneunit.
If every node consumes exactly one unit of flow and every node can have only oneparent node, then the tree must be connected and acyclic.max?h?H?d?N?l?L?ldhaldh (1)?h?H?l?Laldh = 1 ?d ?
N (2)|N|?l?Laldh ?
fdh ?d ?
N,?h ?
H (3)?h?Hfdh ?
?g?Nfgd = 1 ?d ?
N (4)?d?NfdRoot = |N| (5)a ?
{0, 1}, f ?
Z (6)Let N be the set of words in a sentence, H = N ?
{Root} is the set of words plus anartificial root node, and L is the set of function labels.
For every sentence, Equations (1)?
(6) constitute the equation system that the constraint solver has to solve in order to findthe highest scoring dependency tree.
Equation (1) shows the objective function, whichis simply the sum over all binary arc indicator variables a ?
A = N ?
H ?
L weightedby their respective score ?.
Equation (2) restricts for every dependent d the numberof incoming arcs to exactly one.
It thus makes sure that every word will end up with19 We use the GUROBI constraint solver: www.gurobi.com, version 4.0.44Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsingd\h 0 1 2 3123a1,0a2,0a3,0a1,1a2,1a3,1a1,2a2,2a3,2a1,3a2,3a3,3d\h 0 1 2 3123f1,0f2,0f3,0f1,1f2,1f3,1f1,2f2,2f3,2f1,3f2,3f3,3arc indicators flow variablesRoot John loves Mary0 1 2 3Head candidatesfor Johna1,0 a1,2a1,3Equation (3): flow link (for each pair <a,f>)= 1Equation (2):single head= 1Equation (4):flow consumptionEquation (5): root flowf1,0 + f2,0 + f3,0 = 3Figure 3Schematic description of the unlabeled first-order model for the example sentence John lovesMary.
The constraints are shown for the dependent (d) John.
There are three head (h) candidates,from which the decoder needs to choose one because of the single head constraint (Equation (2)).Equations (3), (4), and (5) show as an example how the flow constraints are applied to ensure atree structure.
Equation (3) links each arc indicator to one flow variable making sure that onlyactive arcs (those that are set to 1) carry flow > 0.
Equation (5) sends three units of flow from theroot, one for each other token in the tree.
Equation (4) finally forces the flow difference betweenthe incoming arc (horizontal part) of each node (except root) and the flow on all outgoing arcs(vertical part) to be exactly 1, thus making sure that each node consumes one unit of flow.
To findthe optimal tree, the sum over the weights of all arc indicators that are set to one is maximized.exactly one head.
Equations (3)?
(5) model the single commodity flow.
A set of integervariables F = N ?
H is introduced to represent the flow on each arc.
Equation (3) linksevery flow variable that represents the flow between two nodes to the set of arc indicatorvariables that can connect these two nodes.
If there is no arc between the two nodes (allindicator variables are 0), the flow must be 0 as well.
If one arc indicator is 1, thenthe flow variable can take any integer value between 0 and |N|.
Equation (4) enforcesthe consumption of one unit of flow at each node by requiring the difference betweenincoming and outgoing arcs to be exactly one.
Equation (5) finally sets the amount offlow that is sent by the artificial root node to the number of words in the sentence.
Notethat this does not force the tree structure to be single-rooted, because the artificial rootnode can have multiple dependents.
It can be done by an additional constraint that setsthe number of dependents for the root node to one.
Figure 3 shows an example for thebasic formulation.Martins, Smith, and Xing (2009) propose several extensions to the basic model; forexample, second-order features, which introduce new variables for each combination45Computational Linguistics Volume 39, Number 1of two arc indicator variables into the ILP model.
For our parser, we implementedthe second-order features that they call all grandchildren and all siblings.
They also statethat the use of second-order features in the decoder renders exact decoding intractable,and they propose several techniques to reduce the complexity, which we also applyto our parser: (1) Before parsing, the trees are pruned by choosing for each token theten most probable heads using a linear classifier that is not restricted by structuralrequirements, and (2) The integer constraint is dropped, such that the variables cannow take values between 0 and 1 instead of either 0 or 1.
The dropping of the integerconstraint can lead to inexact solutions with fractional values.
To arrive at a well-formeddependency tree, we then use the first-order model in Equations (1)?
(6) to get themaximum spanning tree, this time using the fractional values from the actual solutionas arc weights.
Two other techniques that we apply are related to the arc labels: (1) Weuse an arc filter (Johansson and Nugues 2008) like the Bohnet parser, which blocksedges that did not appear in the training data based on the POS tags of the dependentand the head, and the label, and (2) We do not include labels in the second-ordervariables.The feature set of the ILP parser is similar to but not identical to one in the Bohnetparser.
The ILP parser uses loss-augmented MIRA for training (Taskar et al,which is similar to the MIRA used in the Bohnet parser.
We set the number of trainingiterations to 10 as well.6.1 Morpho-Syntax as ConstraintsUsing case as a filter for the decoder requires an underspecified symbolic representa-tion of morphological information that we can use to define constraints.
This allowsus to have an exact representation of syncretism controlling the search space of theparser.
The case features of a word are represented in the ILP decoder as a set ofbinary variables M for which 1 signals the presence of a particular value and 0 signalsits absence.
For Hungarian, we only model the different case values, which leads toone binary variable for each of the values.
For Czech and German, we also includethe gender and the number features which then gives, for each case marked word,a binary variable for every combination of the case, number, and gender values.
Thevalues of the morphological indicator variables are specified by annotating the datasets with underspecified morphological descriptions that are obtained from finite-statemorphological analyzers.20 If a certain feature value is excluded by the analyzers, thevalue of the indicator variable for this feature is fixed at 0, which then means that thedecoder cannot set it to 1.
This way, all morphological values that cannot be markedby the form of the token (according to the morphological analyzer) are blocked andthereby also all parser solutions that depend on them.
Words unknown to the analyzersare left completely underspecified so that each of the possible values is allowed (noneof the variables are fixed at 0).
The symbolic, grammar-based pre-annotations thus setsome of the morphological indicator variables to 0 where the word form gives enoughinformation while leaving other variables open to be set by the parser, which can usesyntactic context to make a more informed decision.We now present three types of constraints that model the morpho-syntactic inter-actions in the three languages.
Their purpose is to help the parser during decoding20 Czech: http://ufal.mff.cuni.cz/pdt/Morphology and Tagging/Morphology/index.html; German:Schiller (1994); Hungarian: Tro?n et al).46Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsingto find a linguistically plausible solution.
They are inspired by the types of morpho-syntactic interaction that Nichols (1986) describes and guide the parser by enforcingthem globally in the final structure.
It is important to emphasize that these constraintsdo not interact with or influence the statistical feature model of the parser.
They areapplied during decoding when the parser is searching for the highest-scoring tree andprevent solutions that violate the constraints.The first type of constraints that we apply explicitly formulates the mapping be-tween a function label and the case value that it requires.
Equation (7) shows an exampleof a case licensing constraint for the DAT label in Hungarian.
A dependent d cannot beattached to a head with label DAT if its morphological indicator variable for dative case(mdatd ) is zero.
?d :?h?HaDATdh ?
mdatd (7)The second type of constraint models the morphological agreement between de-pendents and their heads in noun phrases (Equations (8)?
(9)), for instance, determinersand adjectives with their head noun in the noun phrases in Czech and German.
In thetreebanks, the relation is marked by NK for German and Atr for Czech.21 The constraintsset the morphological indicators for an adjective and a noun in the following relation:As long as there is no arc (aNKdh is 0) between the adjective (d) and the noun (h), the twoconstraints allow for any value in the morphological indicator variables of both words.If the arc is established (aNKdh is set to 1), the two constraints form an equivalence forcingall the morphological indicators to agree on their value (i.e., to be both 1 or both 0).
Weadditionally require every word to have at least one morphological indicator variableset to 1.
Thus, if there is no solution to the equivalence the arc between the adjective andthe noun cannot be established with this function label.mdat?pl?femh ?
mdat?pl?femd + 1 ?
aNKdh (8)mdat?pl?femh ?
mdat?pl?femd ?
1 + aNKdh (9)For the third type, Equation (10) shows a constraint that was already proposedby Riedel and Clarke (2006).
It models label uniqueness by forcing label l to appearat most once on all the dependents of a head (h).
Due to the design of the decoderfollowing Carreras (2007), the Bohnet parser has no means of making sure that aparticular function label is annotated at most once per head.
Table 9 shows the numberof times a grammatical function occurs more than once per head in the treebank (TRBK)and how often it was annotated by the models in the previous experiment.
Althoughdoubly annotated argument functions almost never appear in the treebank, the parser21 In German and mostly also in Czech, if an adjective is attached to a noun by NK (or Atr), they stand inan agreement relation.
This fortunate circumstance allows us to bind the agreement constraint to thesefunction labels (and to the involved POS tags).
In a (very) small number of cases in the Czech treebank,however, an adjective is attached to a noun by Atr but there is no agreement.
This happens, for example,if the adjective is actually the head of another noun phrase that stands in attributive relation (Atr) to thenoun.
The Atr label was not meant to mark agreement relations, it just happens to coincide for most ofthe cases.
But it might be worth considering whether morpho-syntactic relations like agreement shouldbe represented explicitly in syntactic treebanks.47Computational Linguistics Volume 39, Number 1frequently annotates them because it has no way of checking whether the function hasalready been annotated (see also Khmylko, Foth, and Menzel [2009]).
?h?l :?d?Naldh ?
1 (10)The global constraint in Equation (10) allows us to restrict the number of argumentfunctions and thus implements a very conservative version of subcategorization framewith which we do not risk coverage problems caused by too restrictive verb frames.For each language, we automatically counted the number of times a function labeloccurred on the direct dependents of each node in the treebank.
Labels that occurredmore than once per head with a very low frequency were still counted as appearingat most once if our linguistic intuition would predict that (see, e.g., German subjectsin Table 9).
For each function label l in these lists, the constraint in Equation (10) wasapplied.Table 9Number of times a core grammatical function was annotated more than once in the treebank(TRBK) by the model using gold morphology (GOLD-M), and by the model using predictedmorphology (PRED-M).Czech German HungarianTRBK GOLD-M PRED-M TRBK GOLD-M PRED-M TRBK GOLD-M PRED-Msubjects 0 772 1,723 44 1,170 2,403 0 586 670predicates 7 174 190 6 92 108 1 17 19obj (dat.)
0 28 46 0 33 46 0 9 5obj (acc.)
22 284 602 2 364 912 0 182 189Each individual constraint already reduces the choices that the parser has availablefor the syntactic structure.
They exclude additional incorrect analyses, however, byinteraction.
Figure 4 illustrates the interaction between the three constraints for theGerman sentence den Ma?dchen helfen Frauen meaning women help the girls.
Each individ-ual word displays a high degree of syncretism.
But when the syntactic structure is de-cided, many options mutually exclude each other.
Constraints (8) and (9) disambiguateden Ma?dchen for dative plural feminine.
The case licensing (Constraint (7)) then restrictsthe labels for Ma?dchen to dative object (DA), and Constraint (10) ensures uniquenessby restricting the choice for Frauen.
The parser now has to decide whether Frauen issubject, accusative object, or something else completely.
The constraints are applied on-line during the decoding process.
If the statistical model would strongly prefer Ma?dchento be accusative object, the parser could label it with OA.
In that case, however, it wouldnot be able to establish the NK label between den and Ma?dchen, because the agreementconstraint would be violated.
So, the constraints filter out incorrect solutions but thedecoder is still driven by the statistical model.6.2 Experiment 2In the second experiment, we now apply the ILP parser to the same data sets that weused in the first experiment, again with a five-fold cross-annotation.
We trained two48Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing??
??denthe??????acc-sg-masc??????dat-pl-mascdat-pl-fem?????dat-pl-neutMa?dchengirls??????nom-pl-fem?????acc-pl-femdat-pl-fem?????gen-pl-femhelfenhelp----Frauenwomennom-pl-femacc-pl-femdat-pl-femgen-pl-femSB/?DA/OA/...
?SB/DA/?OANK?Women help the girls?Figure 4Constraint interaction for the German sentence den Ma?dchen helfen Frauen meaning women helpthe girls.Table 10Overall performance of the Bohnet parser and the ILP parser on the five-fold cross annotationfor every language.
All results in percent.
LAS = labeled attachment score, UAS = unlabeledattachment score.
Results for German and Hungarian are without punctuation.Czech German Hungarianmodel LAS UAS LAS UAS LAS UASGOLD-M 82.49 88.61 91.26 93.20 86.70 89.70PRED-M 81.41 88.13 89.61 92.18 84.33 88.02NO-M 79.00 86.89 89.18 91.97 78.04 86.02ILP NO-C 81.69 88.09 89.30 91.98 84.01 87.12ILP C 81.91 88.18 89.93 92.25 84.35 87.39models for each language, one using the constraints (c) and one without the constraints(no-c).
In both cases, we used the predicted morphology in the feature set.
Table 10shows the parsing results for the ILP parsing models in terms of LAS and UAS incomparison to the results of the Bohnet parser (repeated from Table 3).
Both ILP modelsshould be compared to the PRED-M model because they have the most similar featuresets.
As can be seen from the results, the ILP parser without constraints performs overallslightly worse than the Bohnet parser and the ILP parser using constraints performsoverall slightly better or equal.
This shows that both parsers perform on a similarlevel.
The differences between the Czech and the German models (ILP C vs. PRED-M)are statistically significant.22 The interesting results, however, occur for the argumentfunctions.Table 11 shows the performance of the unconstrained (no-c) and constrained (c) ILPmodels and the PRED-M models of the Bohnet parser on the argument functions.
Again,22 According to a two-tailed t-test for related samples with ?
= 0.05.49Computational Linguistics Volume 39, Number 1Table 11Parsing results for the unconstrained (NO-C) and the constrained (C) ILP models, and theBohnet parser in terms of F-score (LAS) for core grammatical functions marked by case.We omit locative objects in Czech, and second accusative objects in German because of theirextremely low frequency.
?
Statistically significant when comparing the performance on agrammatical function for the C model to the PRED-M model (?
= 0.05, two-tailed t-test forrelated samples).Czech German HungarianNO-C C PRED-M NO-C C PRED-M NO-C C PRED-Msubject 85.41 87.23* 85.46 90.02 92.91* 90.59 85.05 87.67* 86.53predicate 87.13 90.09* 87.11 72.86 80.70* 74.33 74.16 78.88* 74.79obj (nom) 47.48 53.19* 38.74 ?
?
?
?
?
?obj (gen) 70.15 72.54 70.27 31.41 42.98 34.26 ?
?
?obj (dat) 79.99 80.42 79.54 65.21 77.78* 71.05 75.33 77.92* 73.49obj (acc) 84.27 86.79* 84.12 83.74 87.96* 84.86 91.96 93.21* 92.53obj (instr) 67.36 68.76 65.02 ?
?
?
?
?
?all arg funcs 84.33 86.37* 84.21 86.27 90.11* 87.24 86.87 89.04* 87.78all other 81.37 81.37 81.05 89.79 89.88 89.98 82.73 82.86 83.43we only evaluated those tokens that actually carry case morphology, as we did in thefirst experiment.
For each language, the best results are in boldface.
In addition to theresults for the different argument functions, a total score is computed over all argumentfunctions (all arg funcs) and another is computed over all tokens that are not includedin the first score (all other).
The latter illustrates the performance of the parsing modelson the functions that are not marked by case morphology.For each language, we get the same basic picture: Although the unconstrained ILPmodel performs slightly worse than (German, Hungarian) or equally well as (Czech) thePRED-M model of the Bohnet parser, the constrained ILP model clearly outperforms bothon the argument functions.
On each of them, the constrained ILP model improves overthe other two models, raising the score by 1 percentage point for (for example) subjectsin Hungarian up to 7 percentage points on dative objects in German (compared withthe PRED-M model).
What we can see is that, in general, the improvements seem to behigher on the more infrequent arguments like dative objects and predicates than on thefrequent arguments like subject or accusative object.
It is not the case, however, that theperformance of one of the infrequent functions suddenly surpasses the performance ofa more frequent function.
Those two effects are to be expected because the ILP parser isstill a data-driven parser.
The constraints support it by excluding morpho-syntacticallyincorrect analyses but they do not resolve ambiguous cases, which are still decided bythe statistical model.The main work done by the constraints is to establish interactions between partsof the parse graph that are not represented in the statistical model.
Because the graph-based approach (in both parsers) factors the graph into first- (and some second-) orderarcs, and because both decoders do not use second-order features with more thanone label, a constraint like label uniqueness (Equation (10)), which is not even directlyrelated to morphology, is impossible to learn for the statistical model.
This is becauseit never sees two sister dependents and their labels together and thus does not know ifit has already annotated the current function label.
Applying the constraints duringthe search makes it impossible for the parser to produce an output that does not50Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsingobey label uniqueness even though the statistical model does not have access to thisinformation.It should be stressed that the ILP models in their statistical model still use the samepredicted and fully disambiguated morphological information from the pipeline archi-tecture as the Bohnet parser.
As we saw in the first experiment, using no morphologicalinformation in the statistical model is very harmful to the performance on Czech andHungarian, though not so much for German.One advantage of the proposed architecture is the fact that the ILP parser is stillmainly driven by the statistical model.
Krivanek and Meurers (2011) compared a data-driven, transition-based dependency parser (Nivre et al) and a constraint-baseddependency parser (Foth and Menzel 2006) on learner and newspaper corpora andfound that whereas the former is better on modifier functions (e.g., PP-attachment),the latter performs better on argument functions.
Their explanation is that wherethe data-driven parser has access to lots of data and can pick up statistical effectsin the data like semantic or selectional preferences, the constraint-based parser hasaccess to deep lexical and grammatical information and is thus able to model argu-ment structure in a better way.
In the ILP parser, we can combine both strengths,letting the statistical model learn preferences but forcing it via constraints to obey hardgrammatical information.
The last row in Table 11 shows that compared to the Bohnetparser, the ILP models perform comparably well on non-argument functions (maybewith the exception of Hungarian, where the difference is a bit more distinct).
At thesame time, they perform clearly better on the argument functions due to the linguisticconstraints.Foth and Menzel (2006) (see also Khmylko, Foth, and Menzel 2009) are furtherrelevant to this work in the sense that our architecture mirrors their approach.
Intheir work, they use a highly sophisticated rule-based parser, which they equip withstatistical components that model various subtasks like pos tagging, supertagging, orPP-attachment.
They demonstrate that a rule-based parser can benefit from statisticalmodels that model preferences rather than hard constraints.
Our approach comes fromthe other side: We equip a statistical parser with hard rules that ensure the linguisticplausibility of the output.
Both approaches prove that proper statistical models andlinguistically motivated rules can work well together to produce syntactic structures ofhigh quality.One advantage of applying constraints over the argument structure is that we cangive a guarantee that certain ill-formed trees will not be produced by the parser.
Forexample, the constraints make sure that there will not be any parser output where thereare two subjects annotated for the same verb.
Although this does not mean that thesubject will be the correct one, the formal requirement of not having two subjects is met,which we believe can be helpful for subsequent semantic analysis/interpretation or, forexample, relation extraction.
In the same sense, the constraints will also ensure that mor-phological agreement and case licensing is correct to the degree that the morphologicalanalyzer was correct.
This feature thus implements a tentative notion of grammaticalityfor the statistical model.7.
ConclusionIn this article, we investigated the performance of the state-of-the-art statistical de-pendency parser by Bohnet (2010) on three morphologically rich languages?Czech,German, and Hungarian.
We concentrated on the core grammatical functions (subject,51Computational Linguistics Volume 39, Number 1object, etc.)
that are marked by case morphology in each of the three languages.
Our firstexperiment shows that apart from small frequency effects due to the statistical nature ofthe parser, learning the mapping between a case value and the grammatical functionssignaled by it is not a problem for the parser.
We also see, however, that the pipelineapproach, where morphological information is fully disambiguated before being usedby the parser as features in the statistical model, is susceptible to error propagation forlanguages that show syncretism in their morphological paradigms.
Although we canshow that parsing Hungarian, an agglutinating language without major syncretism inthe case paradigm, is not affected by these problems, parsing the fusional languagesCzech and German frequently suffers from propagated errors due to ambiguous casemorphology.
Furthermore, although the predicted morphological information doesnot help very much in German, it contributes very much when parsing Czech andHungarian, even if it is not completely reliable.Handling syncretism requires changes in the processing architecture and the rep-resentation of morphological information.
We proposed an augmented pipeline wherethe parsing model is restricted by possibly underspecified, morpho-syntactic constraintsexploiting grammatical knowledge about the morphological marking regimes and theinflectional paradigms.
Although the statistical parsing model provides scores for localsubstructures during decoding, the symbolic constraints are applied globally to theentire output structure.
A morpho-syntactic feature like case is interpreted as a filteron the parser output.
By modeling phenomena like case-function mapping, agreement,and function uniqueness as constraints in an ILP decoder for dependency parsing,we showed in a second experiment that supporting a statistical model with theseconstraints helps avoiding parsing errors due to incorrect morphological preprocess-ing.
The advantage of this approach is the combination of local statistical modelsand globally enforced hard grammatical knowledge.
Whereas some key aspects of thegrammatical structure are ensured by the linguistic knowledge (e.g., overtly markedcase morphology) the underlying data-driven model can still exploit statistical effectsto resolve the remaining ambiguity and model semantic preferences, which are difficultto model with hard rules.Morphologically rich languages pose various challenges to the standard parsingapproaches because of their different linguistic properties.
As one of them, case systemsare a key device in these languages to encode argument structure and reside at thebrink between morphology and syntax.
Paying attention to the role of case in statisticalparsing results in more appropriate models.
Morphologically rich, however, is a wide cat-egory and covers a wide range of languages.
Taking the idea of linguistically informedrestrictions over data-driven system components may lead to further improvements onother phenomena and for other languages.AcknowledgmentsThe research reported in this article wassupported by the German ResearchFoundation (DFG) in project D8 of SFB 732Incremental Specification in Context.
We wouldlike to thank Richa?rd Farkas and VeronikaVincze at the University of Szeged for theirhelp with the Hungarian corpus and language;Bernd Bohnet for the help with his parser;and Anders Bjo?rkelund, Anett Diesner, andKyle Richardson for their comments onearlier drafts of this work.ReferencesBlake, Barry J.
2001.
Case.
CambridgeUniversity Press, Cambridge, MA,2nd edition.Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,and Barbora Hladka?.
2000.
The PragueDependency Treebank: A three-levelannotation scenario.
In A.
Abeille?, editor,Treebanks: Building and Using SyntacticallyAnnotated Corpora.
Kluwer AcademicPublishers, Amsterdam, chapter 1,pages 103?127.52Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency ParsingBohnet, Bernd.
2009.
Efficient parsing ofsyntactic and semantic dependencystructures.
In Proceedings of the 13thConference on Computational NaturalLanguage Learning: Shared Task,volume 2007, pages 67?72, Boulder, CO.Bohnet, Bernd.
2010.
Very high accuracyand fast dependency parsing is nota contradiction.
In Proceedings of the23rd International Conference onComputational Linguistics, pages 89?97,Beijing.Bohnet, Bernd.
2011.
Comparing advancedgraph-based and transition-baseddependency.
In Proceedings of theInternational Conference on DependencyLinguistics, pages 282?289, Barcelona.Boyd, Adriane, Markus Dickinson, andW.
Detmar Meurers.
2008.
On detectingerrors in dependency treebanks.
Researchon Language and Computation, 6(2):113?137.Brants, Sabine, Stefanie Dipper, SilviaHansen-Shirra, Wolfgang Lezius, andGeorge Smith.
2002.
The TIGER treebank.In Proceedings of the 1st Workshop onTreebanks and Linguistic Theories,20?21 September 2002, Sozopol,Bulgaria, pages 24?41.Bresnan, Joan.
2001.
Lexical-Functional Syntax.Blackwell Publishers, Oxford.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings of the10th Conference on Computational NaturalLanguage Learning, pages 149?164,New York, NY.Carreras, Xavier.
2007.
Experiments with ahigher-order projective dependency parser.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning, pages 957?961, Prague.Cohen, Shay B. and Noah A. Smith.
2007.Joint morphological and syntacticdisambiguation.
In Proceedings of the2007 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning, pages 208?217, Prague.Collins, Michael, Jan Hajic?, Lance Ramshaw,and Christoph Tillmann.
1999.
A statisticalparser for Czech.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics, pages 505?512,College Park, MD.Crammer, Koby, Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
2003.Online passive-aggressive algorithms.In Proceedings of the 16th AnnualConference on Neural Information ProcessingSystems, volume 7, pages 1217?1224,Cambridge, MA.Csendes, Do?ra, Ja?nos Csirik, and TiborGyimo?thy.
2004.
The Szeged Corpus:A POS tagged and syntactically annotatedHungarian natural language corpus.In Proceedings of the 5th InternationalWorkshop on Linguistically InterpretedCorpora, pages 19?23, Geneva.Eisenberg, Peter.
2006.
Grundriss der deutschenGrammatik: Der Satz.
J.B. Metzler, Stuttgart,3rd edition.Eisner, Jason.
1997.
Bilexical grammarsand a cubic-time probabilistic parser.In Proceedings of the 5th InternationalConference on Parsing Technologies,pages 54?65, Cambridge, MA.Eryig?it, Gu?ls?en, Joakim Nivre, and KemalOflazer.
2008.
Dependency parsing ofTurkish.
Computational Linguistics,34(3):357?389.Foth, Kilian A. and Wolfgang Menzel.2006.
Hybrid parsing: Using probabilisticmodels as predictors for a symbolic parser.In Proceedings of the 21st InternationalConference on Computational Linguisticsand the 44th annual meeting of the ACL,pages 321?328, Sidney.Gesmundo, Andrea, James Henderson,Paola Merlo, and Ivan Titov.
2009.A latent variable model of synchronoussyntactic-semantic parsing for multiplelanguages.
In Proceedings of the 13thConference on Computational NaturalLanguage Learning: Shared Task,pages 37?42, Boulder, CO.Goldberg, Yoav and Michael Elhadad.
2010.Easy first dependency parsing of modernHebrew.
In Proceedings of the NAACL HLT2010 First Workshop on Statistical Parsingof Morphologically-Rich Languages,pages 103?107, Los Angeles, CA.Goldberg, Yoav and Reut Tsarfaty.
2008.A single generative model for jointmorphological segmentation and syntacticparsing.
In Proceedings of the 46th AnnualMeeting of the Association for ComputationalLinguistics, pages 371?379, Columbus, OH.Hajic?, Jan, Massimiliano Ciaramita, RichardJohansson, Daisuke Kawahara,Maria Anto`nia Mart?
?, Llu?
?s Ma`rquez,Adam Meyers, Joakim Nivre, SebastianPado?, Jan Stepa?nek, Pavel Strana?k, MihaiSurdeanu, Nianwen Xue, and Yi Zhang.2009.
The CoNLL-2009 shared task:Syntactic and semantic dependencies inmultiple languages.
In Proceedings of the13th Conference on Computational Natural53Computational Linguistics Volume 39, Number 1Language Learning: Shared Task, pages 1?18,Boulder, CO.Hajic?, Jan, Jarmila Panevova?, Eva Hajic?ova?,Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??
?Havelka, and Marie Mikulova?.
2006.
PragueDependency Treebank 2.0, Linguistic DataConsortium, Philadelphia, PA.Hudson, Richard A.
1984.
Word Grammar.Basil Blackwell, Oxford.Janda, Laura A. and Charles E. Townsend.2000.
Czech.
Lincom Europa, Munich.Johansson, Richard and Pierre Nugues.
2008.Dependency-based syntactic-semanticanalysis with PropBank and NomBank.In Proceedings of the 12th Conference onComputational Natural Language Learning,pages 183?187, Manchester.Khmylko, Lidia, Kilian A. Foth, andWolfgang Menzel.
2009.
Co-parsing withcompetitive Models.
In Proceedings of the11th International Conference on ParsingTechnologies, pages 99?107, Paris.Klenner, Manfred.
2007.
Shallow dependencylabeling.
In Proceedings of the ACL 2007 Demoand Poster Sessions, pages 201?204, Prague.Krivanek, Julia and W. Detmar Meurers.2011.
Comparing rule-based anddatadriven dependency parsing of learnerlanguage.
In Proceedings of the InternationalConference on Dependency Linguistics,pages 310?318, Barcelona.Ku?bler, Sandra.
2008.
The PaGe 2008 sharedtask on parsing German.
In Proceedingsof the Workshop on Parsing German,pages 55?63, Morristown, NJ.Lee, John, Jason Naradowsky, and David A.Smith.
2011.
A discriminative model forjoint morphological disambiguation anddependency parsing.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics, pages 885?894,Portland, OR.Magnanti, Thomas and Laurence Wolsey.1995.
Optimal trees.
Handbooks inOperations Research and ManagementScience, 7(April):503?615.Martins, Andre?
F. T., Noah A. Smith,and Eric P. Xing.
2009.
Concise integerlinear programming formulations fordependency parsing.
In Proceedings of theJoint Conference of the 47th Annual Meetingof the ACL and the 4th International JointConference on Natural Language Processingof the AFNLP, pages 342?350, Suntec.Marton, Yuval, Nizar Habash, andOwen Rambow.
2010.
Improving Arabicdependency parsing with lexical andinflectional morphological features.In Proceedings of the NAACL HLT 2010First Workshop on Statistical Parsing ofMorphologically-Rich Languages,pages 13?21, Los Angeles, CA.McDonald, Ryan and Fernando Pereira.2006.
Online learning of approximatedependency parsing algorithms.
InProceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 81?88,Trento.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic?.
2005.Non-projective dependency parsingusing spanning tree algorithms.
InProceedings of the 2005 Conference onHuman Language Technology and EmpiricalMethods in Natural Language Processing,pages 523?530, Morristown, NJ.Mel?c?uk, Igor.
1988.
Dependency Syntax:Theory and Practice.
SUNY Series inLinguistics.
State University Press ofNew York.Nichols, Joanna.
1986.
Head-marking anddependent-marking grammar.
Language,62(1):56?119.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007a.
TheCoNLL 2007 shared task on dependencyparsing.
In Proceedings of the 2007 JointConference on Empirical Methods in NaturalLanguage Processing and ComputationalNatural Language Learning, pages 915?932,Prague.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gu?ls?en Eryig?it,Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007b.
MaltParser:A language-independent system fordata-driven dependency parsing.Natural Language Engineering,13(2):95?135.Riedel, Sebastian and James Clarke.
2006.Incremental integer linear programmingfor non-projective dependency parsing.In Proceedings of the 2006 Conference onEmpirical Methods in Natural LanguageProcessing, pages 129?137, Sydney.Schiehlen, Michael.
2004.
Annotationstrategies for probabilistic parsingin German.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics, pages 390?397, Geneva.Schiller, Anne.
1994.
Dmor - user?s guide.Technical report, University of Stuttgart.Seeker, Wolfgang and Jonas Kuhn.
2012.Making ellipses explicit in dependencyconversion for a German treebank.In Proceedings of the 8th International54Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency ParsingConference on Language Resources andEvaluation, pages 3132?3139, Istanbul.Seeker, Wolfgang and Jonas Kuhn.
2011.On the role of explicit morphologicalfeature representation in syntacticdependency parsing for German.
InProceedings of the 12th InternationalConference on Parsing Technologies,pages 58?62, Dublin.Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,and Josef Van Genabith.
2010.
Hardconstraints for grammatical functionlabelling.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics, pages 1087?1097, Uppsala.Spoustova?, Drahom?
?ra ?Johanka,?
Jan Hajic?,Jan Raab, and Miroslav Spousta.
2009.Semi-supervised training for the averagedperceptron POS tagger.
In Proceedings of the12th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 763?771, Athens.Taskar, Ben, Vassil Chatalbashev, DaphneKoller, and Carlos Guestrin.
2005.Learning structured prediction models:A large margin approach.
In Proceedingsof the 22th Annual International Conferenceon Machine Learning, pages 896?903,Bonn.Tro?n, Viktor, Pe?ter Hala?csy, Pe?ter Rebrus,Andra?s Rung, Pe?ter Vajda, and EszterSimon.
2006.
Morphdb.hu: Hungarianlexical database and morphologicalgrammar.
In Proceedings of the 5thInternational Conference on LanguageResources and Evaluation, pages 1670?1673,Genoa, Italy.Tsarfaty, Reut, Djame?
Seddah, YoavGoldberg, Sandra Ku?bler, Marie Candito,Jennifer Foster, Yannick Versley, InesRehbein, and Lamia Tounsi.
2010.Statistical parsing of morphologicallyrich languages (SPMRL): What, how andwhither.
In Proceedings of the NAACL HLT2010 First Workshop on Statistical Parsingof Morphologically-Rich Languages,pages 1?12, Los Angeles, CA.Tsarfaty, Reut and Khalil Sima?an.
2008.Relational-realizational parsing.
InProceedings of the 22nd InternationalConference on Computational Linguistics,pages 889?896, Manchester.Tsarfaty, Reut and Khalil Sima?an.
2010.Modeling morphosyntactic agreement inconstituency-based parsing of ModernHebrew.
In Proceedings of the NAACL HLT2010 First Workshop on Statistical Parsingof Morphologically-Rich Languages,pages 40?48, Los Angeles, CA.Versley, Yannick.
2005.
Parser evaluationacross text types.
In Proceedings of the 4thWorkshop on Treebanks and LinguisticTheories, pages 209?220, Barcelona.Versley, Yannick and Ines Rehbein.
2009.Scalable discriminative parsing forGerman.
In Proceedings of the 11thInternational Conference on ParsingTechnologies, pages 134?137, Paris.Vincze, Veronika, Do?ra Szauter, AttilaAlma?si, Gyo?rgy Mo?ra, Zolta?n Alexin,and Ja?nos Csirik.
2010.
HungarianDependency Treebank.
In Proceedingsof the 7th Conference on InternationalLanguage Resources and Evaluation,pages 1855?1862, Valletta.Zsibrita, Ja?nos, Veronika Vincze, andRicha?rd Farkas.
2010.
Ismeretlenkifejeze?sek e?s a szo?faji egye?rtelmu?s?
?te?s.In VII.
Magyar Sza?m?
?to?ge?pes Nyelve?szetiKonferencia, pages 275?283, Szeged.55
