TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 89?92,Rochester, April 2007 c?2007 Association for Computational LinguisticsVertex Degree Distribution for the Graph of Word Co-Occurrencesin RussianVictor KapustinFaculty of PhilologySaint-Petersburg State UniversitySaint-Petersburg, Russia 199178vak@icape.nw.ruAnna JamsenFaculty of PhilologySaint-Petersburg State UniversitySaint-Petersburg, Russia 199178anna_zheleznova@mail.ruAbstractDegree distributions for word forms co-occurrences for large Russian text collec-tions are obtained.
Two power laws fit thedistributions pretty good, thus supportingDorogovtsev-Mendes model for Russian.Few different Russian text collectionswere studied, and statistical errors areshown to be negligible.
The model expo-nents for Russian are found to differ fromthose for English, the difference probablybeing due to the difference in the collec-tions structure.
On the contrary, the esti-mated size of the supposed kernel lexiconappeared to be almost the same for theboth languages, thus supporting the ideaof importance of word forms for a percep-tual lexicon of a human.1 IntroductionFew years ago Ferrer and Sol?
(2001a) draw theattention of researchers to the fact that the lexicon ofa big corpus (British National Corpus ?
BNC ?inthe case) most probably consists of two major com-ponents: a compact kernel lexicon of about 103 ?104 words, and a cloud of all other words.
Ferrerand Sol?
studied word co-occurrence in BNC in(2001b).
Two word forms1 in BNC were consideredas ?interacting?
when they appeared in the samesentence and the words?
distance didn?t exceed 2.Ferrer and Sol?
(2001b) treated also some other no-1 Strictly speaking, word forms, not words.tions of word interaction, but the results obtaineddon?t differ qualitatively.
The interacting wordsform a graph, where the vertices are the wordsthemselves, and the edges are the words?
co-occurrences.
The fact of the collocation consideredto be important, not the number of collocations ofthe same pair of words.
Ferrer and Sol?
(2001b)studied vertices degree distribution and found twopower laws for that distribution with a crossover at adegree approximately corresponding to the previ-ously found size of the supposed kernel lexicon ofabout 103 ?
104 words.
In (Sol?
et al 2005) wordco-occurrence networks were studied for small(about 104 lines of text) corpora of English, Basque,and Russian.
The authors claim the same two-regime word degree distribution behavior for all thelanguages.Dorogovtsev and Mendes (2001, 2003: 151-156)offered an abstract model of language evolution,which provides for two power laws for word degreedistribution with almost no fitting, and also explainsthat the volume of the region of large degrees (thekernel lexicon) is almost independent of the corpusvolume.
Difference between word (lemma) andword form for an analytic language (e.g.
English)seems to be small.
Dorogovtsev-Mendes model cer-tainly treats word forms, not lemmas, as vertices ina corpus graph.
Is it really true for inflecting lan-guages like Russian?
Many researchers consider aword form, not a word (lemma) be a perceptuallexicon unit (Zasorina, 1977; Ventsov and Kas-sevich, 1998; Verbitskaya et.
al., 2003; Ventsov et.al., 2003).
So a hypothesis that word forms in a cor-pus of  an inflecting language should exhibit degreedistribution similar to that of BNC looks appealing.An attempt to investigate word frequency rank sta-89tistics for Russian was made by Gelbukh and Si-dorov (2001), but they studied only Zipf law on toosmall texts to reveal the kernel lexicon effects.
Tostudy the hypothesis one needs a corpus or a collec-tion2 of texts comparable in volume with the BNCpart that was examined in (Ferrer and Sol?, 2001b),i.e.
about 4.107 word occurrences.
Certainly, textsthat were analyzed in (Sol?
et al 2005) were muchsmaller.Recently Kapustin and Jamsen (2006) and Ka-pustin (2006) studied a big (~5.107 word occur-rences) collection of Russian.
The collectionexhibited power law behavior similar to that ofBNC except that the vertex degree at the crossoverpoint and the average degree were about 4-5 timesless than that of BNC.
These differences could beassigned either to a collection nature (legislationtexts specifics) or to the properties of the (Russian)language itself.
We shall reference the collectionstudied in (Kapustin and Jamsen, 2006; Kapustin,2006) as ?RuLegal?.In this paper we present a study of another bigcollection of Russian texts.
We have found thatdegree distributions (for different big sub-collec-tions) are similar to those of BNC and of RuLegal.While the exponents and the kernel lexicon size arealso similar to those of BNC, the average degreefor these collections are almost twice less than theaverage degree of BNC, and the nature of this dif-ference is unclear still.The rest of the paper has the following struc-ture.
Technology section briefly describes the col-lection and the procedures of building of co-occurrence graph and of calculation of exponentsof power laws.
In Discussion section we comparethe results obtained with those of Kapustin andJamsen (2006), Kapustin (2006), and (Ferrer andSol?, 2001b).
In Conclusion some considerationsfor future research are discussed.2 TechnologyAt present Russian National Corpus is unavailablefor bulk statistical research due to copyright con-siderations.
So we bought a CD (?World Literaturein Russian?)
in a bookstore ?
a collection of fictiontranslations to Russian.
We?ll call the collection2 We consider a corpus to be a special type of a text collection,which comprises text samples chosen for language researchpurposes, while a more general term ?collection ?
refers to aset of full texts brought together for some other purpose.WLR.
The size of the whole collection is morethan 108 word occurrences.
The source format ofthe collection is HTML, but its files contain essen-tially no formatting, just plain paragraphs.
Wemade three non-overlapping samples from WLR(WLR1?3).
The samples were approximately ofthe same size.
Each sample was processed thesame way.
The idea behind using more than onesample was to estimate statistical errors.We used open source Russian grapheme analy-sis module (Sokirko, 2001) to strip HTML and tosplit the texts into words and sentences.
Word co-occurrences were defined as in (Ferrer and Sol?,2001b): two words are ?interacting?
if and only ifthey: (a) appear in the same sentence, and (b) theword distance is either 1 (adjacent words) or 2 (oneword or a number or a date in-between).
A foundco-occurred pair of words was tried out againstMySQL database of recorded word pairs, and if itwasn?t found in the database, it was put there.Then we use a simple SQL query to get a table ofcount of vertices p(k) vs. vertex degree k.Figure 1.
Raw degree distribution for WLR1.The raw results for one of the samples areshown on Fig.
1.
For the two other samples thedistributions are similar.
All distributions are al-most linear (in log-log coordinates, that means thatthey obey power law), but fitting is impossible dueto high fluctuations.
As noted by Dorogovtsev andMendes (2003: 222-223), cumulative distribution-6-5-4-3-2-100 1 2 3 4 5 6log(k)log(p(k))90P(k) = ?K?k p(K) fluctuates much less, so wecalculated the cumulative degree distributions(Fig.2).
Cumulative degree distributions for allthree WLR samples are very similar.Figure 2.
Cumulative degree distributions forWLR1 (lower curve) and RuLegal (upper curve).3 DiscussionTo estimate statistical errors we have normalizedthe distributions to make them comparable: thedegree ranges were reduced to the largest one, thenthe cumulative degree distribution was sampledwith the step of 1/3, as in (Ferrer and Sol?, 2001a,Dorogovtsev and Mendes, 2003: 222-223).
Whenwe use WLR samples only, the statistical errors areless than 7% in the middle of the curves and reacha margin of 77% in small degrees region.
With theinclusion of RuLegal sample, difference betweensamples becomes larger ?
up to 13% in the middleof the curves), but are still small enough.In both cases (with and without RuLegal) weattempted to fit either a single power law (astraight line in log-log coordinates) or two/threepower laws with one/two crossover points.
Strongchanges and large statistical errors of the distribu-tions in the low degree region prevent meaningfulusage of these points for fitting.
We have madeattempts to fit all three approximations for allpoints, and omitting one or two points with thelowest degrees.
To choose between the hypotheseswe minimized Schwarz information criterion(Schwarz, 1978):SIC=N*ln(?i(pi-p?i)2/N)-m*ln(N)where pi ?
cumulative distribution at i-th point;p?i ?
fitting law at the same point;N ?
number of sampling points (13?15,depending on the number ofomitted points);m ?
number of fitting parameters (2, 4 or 6)SIC (1/2/3 power laws) Omittedpoints WLR1?3 WLR1?3 + RuLegal0 ?44 / ?85 / ?68 ?42 / ?93 / ?771 ?46 / ?85 / ?65 ?43 / ?93 / ?732 ?47 / ?80 / ?60 ?44 / ?86 / ?67Table 1.
Fitting power laws to averaged degreedistributions ?
Schwarz information criterionWLR1?3 WLR1?3 + RuLegal RuLegal BNC?1 ?0.95 ?0.95 ?0.95 ?0.5?2 ?1.44 ?1.46 ?1.75 ?1.7kcross 670 670 510 2000Vkernel 4.103 4.103 4.103 5.103kaverage 36 31 15 72Collectionsize3.107 14.107 5.107 4.107Table 2.
Parameters of the best fit two power lawsfor the cumulative distributionsClearly two power laws fit the curves better.The exponents, the crossover degree and estimatedsize of the kernel lexicon (number of vertices withhigh degrees above the crossover) for the best fits(two powers, zero/one omitted point) are shown inTable 2.
The exponents for the raw distributionsare ?1 and ?2 minus 1.Disagreement between English and Russianseems to exist.
Probably, the differences are stilldue to the collections?
nature (the difference be-tween different Russian collections is noticeable).4 ConclusionWe found that ergodic hypothesis for word formdegree distribution seems to work for large textcollections ?
differences between the distributions-6-5-4-3-2-100 1 2 3 4 5 6log(k)log(P(k))91are small (except for the few smallest degrees).
Atleast, a single big enough sample permits reliablecalculation of degree distribution parameters.Dorogovtsev-Mendes model, which yields twopower laws for the degree distribution for the wordforms graph, gives pretty good explanation bothfor an analytic language (English) and for an in-flecting one (Russian), though numeric parametersfor both languages differ.
The estimated sizes ofthe supposed kernel lexicons for the both lan-guages are almost the same, the fact supports thepoint that word form is a perceptual lexicon unit.To make more rigorous statements concerningstatistical properties of various languages, we planto calculate other important characteristics of theco-occurrence graph for Russian: clustering coeffi-cient and average shortest path.
Also we hope thatlegal obstacles to Russian National Corpus usagewill have been overcome.
Other statistical lan-guage graph studies are also interesting; amongthem are investigation of networks of lemmas, andstatistical research of agglutinated languages.AcknowledgementsThe authors are grateful to the anonymous review-ers, the comments of whom were of much help.The work is supported in part by Russian Foun-dation for Basic Research, grants 06-06-80434 and06-06-80251.ReferencesSergey N.
Dorogovtsev., Jos?
F. Mendes, 2001.
Lan-guage as an evolving word web.
Proceedings of theRoyal Society of London B, 268(1485): 2603-2606Sergey N.
Dorogovtsev., Jos?
F. Mendes, 2003.
Evolu-tion of Networks: From Biological Nets to the Inter-net and WWW.
Oxford University Press, Oxford.Ramon Ferrer and Ricard V. Sol?.
2001a.
Two regimes inthe frequency of words and the origin of complex lexi-cons.
Journal of Quantitative Linguistics 8: 165-173.Ramon Ferrer and Ricard V. Sol?.
2001b.
The Small-World of Human Language.
Proceedings of theRoyal Society of London B, 268(1485): 2261-2266Victor Kapustin, 2006.
????????
?.?.
????????
???-???????
??????????
?????????????
?????????
????????
????????????????
?????????.
????????????
?????????????
???????????
???????-???
???????????
?, 11?13 ???????
2006 ?., ?
???.:???-??
?.
??????.
??-?
?, 2006. ?
?.
135-142 (RankStatistics of Word Co-Occurrences in a Big Mono-thematic Collection.
Proc.
3rd International Conf.
?Corpus Linguistics?, Oct. 11-13, 2006.
Saint-Petersburg State Publishing: 135-142).Alexander Gelbukh and Grigory Sidorov, 2001.
Zipfand Heaps Laws?
Coefficients Depend on Language..Proc.
CICLing-2001, Conference on Intelligent TextProcessing and Computational Linguistics (February18?24, 2001, Mexico City), Lecture Notes in Com-puter Science, Springer-Verlag.
(2004): 332-335.
(ISSN 0302-9743, ISBN 3-540-41687-0)Victor Kapustin and Anna Jamsen.
2006.
????????
???-???????
?????????????
????
?
???????
??????????????????.
?????
8??
?????????????
??????????????????
????????????
??????????
: ???-??????????
??????
?
?????????
?, ?????????????????????
?
RCDL?2006, ??????
?, ?????
?, 2006.?
C. 245?251 (Rank Statistics of Word Occurrence ina Big Text Collection.
Proc.
8th National Russian Re-search Conference ?Digital libraries: advanced meth-ods and technologies, digital collections?, Oct. 17-19,2006: 245-251).Alexey Sokirko, 2001.
A short description of DialingProject.http://www.aot.ru/docs/sokirko/sokirko-candid-eng.htmlRicard V.
Sol?, Bernat Corominas, Sergi Valverde andLuc Steels.
2005.
Language Networks: their struc-ture, function and evolution.
SFI-WP 05-12-042, SFIWorking PapersGideon Schwarz, 1978.
Estimating the dimension of amodel.
Annals of Statistics 6(2): 461-464.Anatoly V. Ventsov and Vadim B. Kassevich, 1998.??????
?.
?., ???????
?.?.
???????
???
????????????????
????.
???????
?????-?????????????????????????
?, ???.
2, ???.
3, ?.
32-39 (A Dictionaryfor a Speech Perception Model.
Vestnik Sankt-Peterbugskogo Universiteta, 2(3): 32-39).Anatoly V. Ventsov, Vadim B. Kassevich and Elena V.Yagounova, 2003.
??????
?.
?., ???????
?.
?., ???-????
?.?.
??????
????????
?????
?
??????????
??-??.
??????-???????????
??????????.?
?????
2, ?6, ?.25-32 (Russian Corpus and Speech Perception.Research and Technical Information, 2(6): 25-32).Liudmila A. Verbitskaya, Nikolay N. Kazansky andVadim B. Kassevich, 2003.
?????????
?.
?., ??????-???
?.
?., ???????
?.?.
?????????
????????
?????-???
?????????????
???????
????????
?????.??????-???????????
??????????.?
?????
2, ?
5,?.2-8 (On Some Problems of Russian National CorpusDevelopment.
Research and Technical Information,2(5): 2-8).Lidia N. Zasorina, ed., 1977.
?????????
???????
???-?????
?????.
???
???.
?.?.
?????????.
?.
: ?????.??.
(Frequency Dictionary of Russian.
Russian Lan-guage, Moscow, 1977).92
