Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409?419,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsSpectral Learning for Non-Deterministic Dependency ParsingFranco M. LuqueUniversidad Nacional de Co?rdobaand CONICETCo?rdoba X5000HUA, Argentinafrancolq@famaf.unc.edu.arAriadna Quattoni and Borja Balle and Xavier CarrerasUniversitat Polite`cnica de CatalunyaBarcelona E-08034{aquattoni,bballe,carreras}@lsi.upc.eduAbstractIn this paper we study spectral learningmethods for non-deterministic split head-automata grammars, a powerful hidden-state formalism for dependency parsing.We present a learning algorithm that, likeother spectral methods, is efficient and non-susceptible to local minima.
We showhow this algorithm can be formulated asa technique for inducing hidden structurefrom distributions computed by forward-backward recursions.
Furthermore, wealso present an inside-outside algorithmfor the parsing model that runs in cubictime, hence maintaining the standard pars-ing costs for context-free grammars.1 IntroductionDependency structures of natural language sen-tences exhibit a significant amount of non-localphenomena.
Historically, there have been twomain approaches to model non-locality: (1) in-creasing the order of the factors of a dependencymodel (e.g.
with sibling and grandparent relations(Eisner, 2000; McDonald and Pereira, 2006; Car-reras, 2007; Martins et al 2009; Koo and Collins,2010)), and (2) using hidden states to pass in-formation across factors (Matsuzaki et al 2005;Petrov et al 2006; Musillo and Merlo, 2008).Higher-order models have the advantage thatthey are relatively easy to train, because estimat-ing the parameters of the model can be expressedas a convex optimization.
However, they havetwo main drawbacks.
(1) The number of param-eters grows significantly with the size of the fac-tors, leading to potential data-sparsity problems.A solution to address the data-sparsity problemis to explicitly tell the model what properties ofhigher-order factors need to be remembered.
Thiscan be achieved by means of feature engineering,but compressing such information into a state ofbounded size will typically be labor intensive, andwill not generalize across languages.
(2) Increas-ing the size of the factors generally results in poly-nomial increases in the parsing cost.In principle, hidden variable models couldsolve some of the problems of feature engineeringin higher-order factorizations, since they couldautomatically induce the information in a deriva-tion history that should be passed across factors.Potentially, they would require less feature engi-neering since they can learn from an annotatedcorpus an optimal way to compress derivationsinto hidden states.
For example, one line of workhas added hidden annotations to the non-terminalsof a phrase-structure grammar (Matsuzaki et al2005; Petrov et al 2006; Musillo and Merlo,2008), resulting in compact grammars that ob-tain parsing accuracies comparable to lexicalizedgrammars.
A second line of work has modeledhidden sequential structure, like in our case, butusing PDFA (Infante-Lopez and de Rijke, 2004).Finally, a third line of work has induced hiddenstructure from the history of actions of a parser(Titov and Henderson, 2007).However, the main drawback of the hiddenvariable approach to parsing is that, to the bestof our knowledge, there has not been any convexformulation of the learning problem.
As a result,training a hidden-variable model is both expen-sive and prone to local minima issues.In this paper we present a learning algorithmfor hidden-state split head-automata grammars(SHAG) (Eisner and Satta, 1999).
In this for-409malism, head-modifier sequences are generatedby a collection of finite-state automata.
In ourcase, the underlying machines are probabilisticnon-deterministic finite state automata (PNFA),which we parameterize using the operator modelrepresentation.
This representation allows the useof simple spectral algorithms for estimating themodel parameters from data (Hsu et al 2009;Bailly, 2011; Balle et al 2012).
In all previouswork, the algorithms used to induce hidden struc-ture require running repeated inference on train-ing data?e.g.
Expectation-Maximization (Demp-ster et al 1977), or split-merge algorithms.
Incontrast, spectral methods are simple and very ef-ficient ?parameter estimation is reduced to com-puting some data statistics, performing SVD, andinverting matrices.The main contributions of this paper are:?
We present a spectral learning algorithm forinducing PNFA with applications to head-automata dependency grammars.
Our for-mulation is based on thinking about the dis-tribution generated by a PNFA in terms ofthe forward-backward recursions.?
Spectral learning algorithms in previouswork only use statistics of prefixes of se-quences.
In contrast, our algorithm is ableto learn from substring statistics.?
We derive an inside-outside algorithm fornon-deterministic SHAG that runs in cubictime, keeping the costs of CFG parsing.?
In experiments we show that adding non-determinism improves the accuracy of sev-eral baselines.
When we compare our algo-rithm to EM we observe a reduction of twoorders of magnitude in training time.The paper is organized as follows.
Next sectiondescribes the necessary background on SHAGand operator models.
Section 3 introduces Op-erator SHAG for parsing, and presents a spectrallearning algorithm.
Section 4 presents a parsingalgorithm.
Section 5 presents experiments andanalysis of results, and section 6 concludes.2 Preliminaries2.1 Head-Automata Dependency GrammarsIn this work we use split head-automata gram-mars (SHAG) (Eisner and Satta, 1999; Eis-ner, 2000), a context-free grammatical formal-ism whose derivations are projective dependencytrees.
We will use xi:j = xixi+1 ?
?
?xj to de-note a sequence of symbols xt with i ?
t ?
j.A SHAG generates sentences s0:N , where sym-bols st ?
X with 1 ?
t ?
N are regular wordsand s0 = ?
6?
X is a special root symbol.
LetX?
= X ?
{?}.
A derivation y, i.e.
a depen-dency tree, is a collection of head-modifier se-quences ?h, d, x1:T ?, where h ?
X?
is a word,d ?
{LEFT, RIGHT} is a direction, and x1:T isa sequence of T words, where each xt ?
X isa modifier of h in direction d. We say that h isthe head of each xt.
Modifier sequences x1:T areordered head-outwards, i.e.
among x1:T , x1 is theword closest to h in the derived sentence, and xTis the furthest.
A derivation y of a sentence s0:Nconsists of a LEFT and a RIGHT head-modifier se-quence for each st. As special cases, the LEFT se-quence of the root symbol is always empty, whilethe RIGHT one consists of a single word corre-sponding to the head of the sentence.
We denoteby Y the set of all valid derivations.Assume a derivation y contains ?h, LEFT, x1:T ?and ?h, RIGHT, x?1:T ??.
Let L(y, h) be the derivedsentence headed by h, which can be expressed asL(y, xT ) ?
?
?
L(y, x1) h L(y, x?1) ?
?
?
L(y, x?T ?
).1The language generated by a SHAG are thestrings L(y, ?)
for any y ?
Y .In this paper we use probabilistic versions ofSHAG where probabilities of head-modifier se-quences in a derivation are independent of eachother:P(y) =?
?h,d,x1:T ?
?yP(x1:T |h, d) .
(1)In the literature, standard arc-factored models fur-ther assume thatP(x1:T |h, d) =T+1?t=1P(xt|h, d, ?t) , (2)where xT+1 is always a special STOP word, and ?tis the state of a deterministic automaton generat-ing x1:T+1.
For example, setting ?1 = FIRST and?t>1 = REST corresponds to first-order models,while setting ?1 = NULL and ?t>1 = xt?1 corre-sponds to sibling models (Eisner, 2000; McDon-ald et al 2005; McDonald and Pereira, 2006).1Throughout the paper we assume we can distinguish thewords in a derivation, irrespective of whether two words atdifferent positions correspond to the same symbol.4102.2 Operator ModelsAn operator model A with n states is a tuple?
?1, ?>?, {Aa}a?X ?, where Aa ?
Rn?n is an op-erator matrix and ?1, ??
?
Rn are vectors.
Acomputes a function f : X ?
?
R as follows:f(x1:T ) = ?>?
AxT ?
?
?
Ax1 ?1 .
(3)One intuitive way of understanding operatormodels is to consider the case where f computesa probability distribution over strings.
Such a dis-tribution can be described in two equivalent ways:by making some independence assumptions andproviding the corresponding parameters, or by ex-plaining the process used to compute f .
This isakin to describing the distribution defined by anHMM in terms of a factorization and its corre-sponding transition and emission parameters, orusing the inductive equations of the forward al-gorithm.
The operator model representation takesthe latter approach.Operator models have had numerous applica-tions.
For example, they can be used as an alter-native parameterization of the function computedby an HMM (Hsu et al 2009).
Consider an HMMwith n hidden states and initial-state probabilitiespi ?
Rn, transition probabilities T ?
Rn?n, andobservation probabilities Oa ?
Rn?n for eacha ?
X , with the following meaning:?
pi(i) is the probability of starting at state i,?
T (i, j) is the probability of transitioningfrom state j to state i,?
Oa is a diagonal matrix, such that Oa(i, i) isthe probability of generating symbol a fromstate i.Given an HMM, an equivalent operator modelcan be defined by setting ?1 = pi, Aa = TOaand ??
= ~1.
To see this, let us show that the for-ward algorithm computes the expression in equa-tion (3).
Let ?t denote the state of the HMMat time t. Consider a state-distribution vector?t ?
Rn, where ?t(i) = P(x1:t?1, ?t = i).
Ini-tially ?1 = pi.
At each step in the chain of prod-ucts (3), ?t+1 = Axt ?t updates the state dis-tribution from positions t to t + 1 by applyingthe appropriate operator, i.e.
by emitting symbolxt and transitioning to the new state distribution.The probability of x1:T is given by?i ?T+1(i).Hence, Aa(i, j) is the probability of generatingsymbol a and moving to state i given that we areat state j.HMM are only one example of distributionsthat can be parameterized by operator models.In general, operator models can parameterize anyPNFA, where the parameters of the model corre-spond to probabilities of emitting a symbol froma state and moving to the next state.The advantage of working with operator mod-els is that, under certain mild assumptions on theoperator parameters, there exist algorithms thatcan estimate the operators from observable statis-tics of the input sequences.
These algorithms areextremely efficient and are not susceptible to localminima issues.
See (Hsu et al 2009) for theoret-ical proofs of the learnability of HMM under theoperator model representation.In the following, we write x = xi:j ?
X ?
todenote sequences of symbols, and use Axi:j as ashorthand for Axj ?
?
?Axi .
Also, for conveniencewe assume X = {1, .
.
.
, l}, so that we can indexvectors and matrices by symbols in X .3 Learning Operator SHAGWe will define a SHAG using a collection of op-erator models to compute probabilities.
Assumethat for each possible head h in the vocabulary X?and each direction d ?
{LEFT, RIGHT} we havean operator model that computes probabilities ofmodifier sequences as follows:P(x1:T |h, d) = (?h,d?
)> Ah,dxT ?
?
?
Ah,dx1 ?h,d1 .Then, this collection of operator models definesan operator SHAG that assigns a probability toeach y ?
Y according to (1).
To learn the modelparameters, namely ?
?h,d1 , ?h,d?
, {Ah,da }a?X ?
forh ?
X?
and d ?
{LEFT, RIGHT}, we use spec-tral learning methods based on the works of Hsuet al(2009), Bailly (2011) and Balle et al(2012).The main challenge of learning an operatormodel is to infer a hidden-state space from ob-servable quantities, i.e.
quantities that can be com-puted from the distribution of sequences that weobserve.
As it turns out, we cannot recover theactual hidden-state space used by the operatorswe wish to learn.
The key insight of the spectrallearning method is that we can recover a hidden-state space that corresponds to a projection of theoriginal hidden space.
Such projected space isequivalent to the original one in the sense that we411can find operators in the projected space that pa-rameterize the same probability distribution oversequences.In the rest of this section we describe an algo-rithm for learning an operator model.
We will as-sume a fixed head word and direction, and drop hand d from all terms.
Hence, our goal is to learnthe following distribution, parameterized by oper-ators ?1, {Aa}a?X , and ??
:P(x1:T ) = ?>?
AxT ?
?
?
Ax1 ?1 .
(4)Our algorithm shares many features with theprevious spectral algorithms of Hsu et al(2009)and Bailly (2011), though the derivation givenhere is based upon the general formulation ofBalle et al(2012).
The main difference is thatour algorithm is able to learn operator modelsfrom substring statistics, while algorithms in pre-vious works were restricted to statistics on pre-fixes.
In principle, our algorithm should extractmuch more information from a sample.3.1 Preliminary DefinitionsThe spectral learning algorithm will use statisticsestimated from samples of the target distribution.More specifically, consider the function that com-putes the expected number of occurrences of asubstring x in a random string x?
drawn from P:f(x) = E(x v] x?)=?x?
?X ?
(x v] x?)P(x?
)=?p,s?X ?P(pxs) , (5)where x v] x?
denotes the number of times x ap-pears in x?.
Here we assume that the true valuesof f(x) for bigrams are known, though in practicethe algorithm will work with empirical estimatesof these.The information about f known by the algo-rithm is organized in matrix form as follows.
LetP ?
Rl?l be a matrix containing the value of f(x)for all strings of length two, i.e.
bigrams.2.
Thatis, each entry in P ?
Rl?l contains the expectednumber of occurrences of a given bigram:P (b, a) = E(ab v] x) .
(6)2In fact, while we restrict ourselves to strings of lengthtwo, an analogous algorithm can be derived that considerslonger strings to define P .
See (Balle et al 2012) for details.Furthermore, for each b ?
X let Pb ?
Rl?l denotethe matrix whose entries are given byPb(c, a) = E(abc v] x) , (7)the expected number of occurrences of trigrams.Finally, we define vectors p1 ?
Rl and p?
?
Rlas follows: p1(a) =?s?X ?
P(as), the probabil-ity that a string begins with a particular symbol;and p?
(a) =?p?X ?
P(pa), the probability thata string ends with a particular symbol.Now we show a particularly useful way to ex-press the quantities defined above in terms of theoperators ?
?1, ?>?, {Aa}a?X ?
of P. First, notethat each entry of P can be written in this form:P (b, a) =?p,s?X ?P(pabs) (8)=?p,s?X ??>?
As Ab Aa Ap ?1= (?>?
?s?X ?As) Ab Aa(?p?X ?Ap ?1) .It is not hard to see that, since P is a probabilitydistribution over X ?, actually ?>?
?s?X ?
As =~1>.
Furthermore, since?p?X ?
Ap =?k?0(?a?X Aa)k = (I ?
?a?X Aa)?1,we write ?
?1 = (I ?
?a?X Aa)?1?1.
From (8) itis natural to define a forward matrix F ?
Rn?lwhose ath column contains the sum of all hidden-state vectors obtained after generating all prefixesended in a:F (:, a) = Aa?p?X ?Ap ?1 = Aa ?
?1 .
(9)Conversely, we also define a backward matrixB ?
Rl?n whose ath row contains the probabilityof generating a from any possible state:B(a, :) = ?>?
?s?X ?AsAa = ~1>Aa .
(10)By plugging the forward and backward matri-ces into (8) one obtains the factorization P =BF .
With similar arguments it is easy to seethat one also has Pb = BAbF , p1 = B ?1, andp>?
= ?>?
F .
Hence, ifB and F were known, onecould in principle invert these expressions in orderto recover the operators of the model from em-pirical estimations computed from a sample.
Inthe next section we show that in fact one does notneed to know B and F to learn an operator modelfor P, but rather that having a ?good?
factorizationof P is enough.4123.2 Inducing a Hidden-State SpaceWe have shown that an operator model A com-puting P induces a factorization of the matrix P ,namely P = BF .
More generally, it turns out thatwhen the rank of P equals the minimal number ofstates of an operator model that computes P, thenone can prove a duality relation between opera-tors and factorizations of P .
In particular, one canshow that, for any rank factorization P = QR, theoperators given by ?
?1 = Q+p1, ??>?
= p>?R+,and A?a = Q+PaR+, yield an operator model forP.
A key fact in proving this result is that the func-tion P is invariant to the basis chosen to representoperator matrices.
See (Balle et al 2012) for fur-ther details.Thus, we can recover an operator model for Pfrom any rank factorization of P , provided a rankassumption on P holds (which hereafter we as-sume to be the case).
Since we only have accessto an approximation of P , it seems reasonable tochoose a factorization which is robust to estima-tion errors.
A natural such choice is the thin SVDdecomposition of P (i.e.
using top n singular vec-tors), given by: P = U(?V >) = U(U>P ).Intuitively, we can think of U and U>P as pro-jected backward and forward matrices.
Now thatwe have a factorization of P we can construct anoperator model for P as follows: 3?
?1 = U>p1 , (11)??>?
= p>?
(U>P )+ , (12)A?a = U>Pa(U>P )+ .
(13)Algorithm 1 presents pseudo-code for an algo-rithm learning operators of a SHAG from train-ing head-modifier sequences using this spectralmethod.
Note that each operator model in the3To see that equations (11-13) define a model for P, onemust first see that the matrix M = F (?V >)+ is invertiblewith inverse M?1 = U>B.
Using this and recalling thatp1 = B?1, Pa = BAaF , p>?
= ?>?F , one obtains that:?
?1 = U>B?1 = M?1?1 ,??>?
= ?>?F (U>BF )+ = ?>?M ,A?a = U>BAaF (U>BF )+ = M?1AaM .Finally:P(x1:T ) = ?>?
AxT ?
?
?Ax1 ?1= ?>?MM?1AxTM ?
?
?M?1Ax1MM?1?1= ?
?>?A?xT ?
?
?
A?x1 ?
?1Algorithm 1 Learn Operator SHAGinputs:?
An alphabet X?
A training set TRAIN = {?hi, di, xi1:T ?}Mi=1?
The number of hidden states n1: for each h ?
X?
and d ?
{LEFT, RIGHT} do2: Compute an empirical estimate from TRAIN ofstatistics matrices p?1, p?
?, P?
, and {P?a}a?X3: Compute the SVD of P?
and let U?
be the matrixof top n left singular vectors of P?4: Compute the observable operators for h and d:5: ?
?h,d1 = U?>p?16: (??h,d?
)> = p?>?(U?>P?
)+7: A?h,da = U?>P?a(U?>P?
)+ for each a ?
X8: end for9: return Operators ??
?h,d1 , ??h,d?
, A?h,da ?for each h ?
X?
, d ?
{LEFT, RIGHT}, a ?
XSHAG is learned separately.
The running timeof the algorithm is dominated by two computa-tions.
First, a pass over the training sequences tocompute statistics over unigrams, bigrams and tri-grams.
Second, SVD and matrix operations forcomputing the operators, which run in time cubicin the number of symbols l. However, note thatwhen dealing with sparse matrices many of theseoperations can be performed more efficiently.4 Parsing AlgorithmsGiven a sentence s0:N we would liketo find its most likely derivation, y?
=argmaxy?Y(s0:N ) P(y).
This problem, known asMAP inference, is known to be intractable forhidden-state structure prediction models, as itinvolves finding the most likely tree structurewhile summing out over hidden states.
We usea common approximation to MAP based on firstcomputing posterior marginals of tree edges (i.e.dependencies) and then maximizing over thetree structure (see (Park and Darwiche, 2004)for complexity of general MAP inference andapproximations).
For parsing, this strategy issometimes known as MBR decoding; previouswork has shown that empirically it gives goodperformance (Goodman, 1996; Clark and Cur-ran, 2004; Titov and Henderson, 2006; Petrovand Klein, 2007).
In our case, we use thenon-deterministic SHAG to compute posteriormarginals of dependencies.
We first explain thegeneral strategy of MBR decoding, and thenpresent an algorithm to compute marginals.413Let (si, sj) denote a dependency between headword i and modifier word j.
The posterioror marginal probability of a dependency (si, sj)given a sentence s0:N is defined as?i,j = P((si, sj) | s0:N ) =?y?Y(s0:N ) : (si,sj)?yP(y) .To compute marginals, the sum over derivationscan be decomposed into a product of inside andoutside quantities (Baker, 1979).
Below we de-scribe an inside-outside algorithm for our gram-mars.
Given a sentence s0:N and marginal scores?i,j , we compute the parse tree for s0:N asy?
= argmaxy?Y(s0:N )?
(si,sj)?ylog?i,j (14)using the standard projective parsing algorithmfor arc-factored models (Eisner, 2000).
Overallwe use a two-pass parsing process, first to com-pute marginals and then to compute the best tree.4.1 An Inside-Outside AlgorithmIn this section we sketch an algorithm to com-pute marginal probabilities of dependencies.
Ouralgorithm is an adaptation of the parsing algo-rithm for SHAG by Eisner and Satta (1999) tothe case of non-deterministic head-automata, andhas a runtime cost of O(n2N3), where n is thenumber of states of the model, and N is thelength of the input sentence.
Hence the algorithmmaintains the standard cubic cost on the sentencelength, while the quadratic cost on n is inher-ent to the computations defined by our model inEq.
(3).
The main insight behind our extensionis that, because the computations of our model in-volve state-distribution vectors, we need to extendthe standard inside/outside quantities to be in theform of such state-distribution quantities.4Throughout this section we assume a fixed sen-tence s0:N .
Let Y(xi:j) be the set of derivationsthat yield a subsequence xi:j .
For a derivation y,we use root(y) to indicate the root word of it,and use (xi, xj) ?
y to refer a dependency in yfrom head xi to modifier xj .
Following Eisner4Technically, when working with the projected operatorsthe state-distribution vectors will not be distributions in theformal sense.
However, they correspond to a projection of astate distribution, for some projection that we do not recoverfrom data (namely M?1 in footnote 3).
This projection hasno effect on the computations because it cancels out.and Satta (1999), we use decoding structures re-lated to complete half-constituents (or ?triangles?,denoted C) and incomplete half-constituents (or?trapezoids?, denoted I), each decorated with a di-rection (denoted L and R).
We assume familiaritywith their algorithm.We define ?I,Ri,j ?
Rn as the inside score-vectorof a right trapezoid dominated by dependency(si, sj),?I,Ri,j =?y?Y(si:j) : (si,sj)?y ,y={?si,R,x1:t?}
?
y?
, xt=sjP(y?
)?si,R(x1:t) .
(15)The term P(y?)
is the probability of head-modifiersequences in the range si:j that do not involvesi.
The term ?si,R(x1:t) is a forward state-distribution vector ?the qth coordinate of thevector is the probability that si generates rightmodifiers x1:t and remains at state q. Similarly,we define ?I,Ri,j ?
Rn as the outside score-vectorof a right trapezoid, as?I,Ri,j =?y?Y(s0:isj:n) : root(y)=s0,y={?si,R,xt:T ?}
?
y?
, xt=sjP(y?
)?si,R(xt+1:T ) , (16)where ?si,R(xt+1:T ) ?
Rn is a backward state-distribution vector ?the qth coordinate is theprobability of being at state q of the right au-tomaton of si and generating xt+1:T .
Analogousinside-outside expressions can be defined for therest of structures (left/right triangles and trape-zoids).
With these quantities, we can computemarginals as?i,j ={(?I,Ri,j )> ?I,Ri,j Z?1 if i < j ,(?I,Li,j)> ?I,Li,j Z?1 if j < i ,(17)where Z=?y?Y(s0:N)P(y) = (??,R?
)> ?C,R0,N .Finally, we sketch the equations for computinginside scores in O(N3) time.
The outside equa-tions can be derived analogously (see (Paskin,2001)).
For 0 ?
i < j ?
N :?C,Ri,i = ?si,R1 (18)?C,Ri,j =j?k=i+1?I,Ri,k((?sk,R?
)> ?C,Rk,j)(19)?I,Ri,j =j?k=iAsi,Rsj ?C,Ri,k((?sj ,L?
)> ?C,Lk+1,j)(20)4145 ExperimentsThe goal of our experiments is to show that in-corporating hidden states in a SHAG using oper-ator models can consistently improve parsing ac-curacy.
A second goal is to compare the spec-tral learning algorithm to EM, a standard learningmethod that also induces hidden states.The first set of experiments involve fully unlex-icalized models, i.e.
parsing part-of-speech tag se-quences.
While this setting falls behind the state-of-the-art, it is nonetheless valid to analyze empir-ically the effect of incorporating hidden states viaoperator models, which results in large improve-ments.
In a second set of experiments, we com-bine the unlexicalized hidden-state models withsimple lexicalized models.
Finally, we presentsome analysis of the automaton learned by thespectral algorithm to see the information that iscaptured in the hidden state space.5.1 Fully Unlexicalized GrammarsWe trained fully unlexicalized dependency gram-mars from dependency treebanks, that is, X arePoS tags and we parse PoS tag sequences.
Inall cases, our modifier sequences include specialSTART and STOP symbols at the boundaries.
5 6We compare the following SHAG models:?
DET: a baseline deterministic grammar witha single state.?
DET+F: a deterministic grammar with twostates, one emitting the first modifier of asequence, and another emitting the rest (see(Eisner and Smith, 2010) for a similar deter-ministic baseline).?
SPECTRAL: a non-deterministic grammarwith n hidden states trained with the spectralalgorithm.
n is a parameter of the model.?
EM: a non-deterministic grammar with nstates trained with EM.
Here, we estimateoperators ??
?1, ??
?, A?h,da ?
using forward-backward for the E step.
To initialize, wemimicked an HMM initialization: (1) we set?
?1 and ???
randomly; (2) we created a ran-dom transition matrix T ?
Rn?n; (3) we5Even though the operators ?1 and ??
of a PNFA ac-count for start and stop probabilities, in preliminary experi-ments we found that having explicit START and STOP sym-bols results in more accurate models.6Note that, for parsing, the operators for the START andSTOP symbols can be packed into ?1 and ??
respectively.One just defines ?
?1 = ASTART ?1 and ??>?
= ?>?
ASTOP.68707274767880822  4  6  8  10  12  14unlabeled attachmentscorenumber of statesDetDet+FSpectralEM (5)EM (10)EM (25)EM (100)Figure 1: Accuracy curve on English development setfor fully unlexicalized models.created a diagonal matrix Oh,da ?
Rn?n,where Oh,da (i, i) is the probability of gener-ating symbol a from h and d (estimated fromtraining); (4) we set A?h,da = TOh,da .We trained SHAG models using the standardWSJ sections of the English Penn Treebank (Mar-cus et al 1994).
Figure 1 shows the UnlabeledAttachment Score (UAS) curve on the develop-ment set, in terms of the number of hidden statesfor the spectral and EM models.
We can seethat DET+F largely outperforms DET7, while thehidden-state models obtain much larger improve-ments.
For the EM model, we show the accuracycurve after 5, 10, 25 and 100 iterations.8In terms of peak accuracies, EM gives a slightlybetter result than the spectral method (80.51% forEM with 15 states versus 79.75% for the spectralmethod with 9 states).
However, the spectral al-gorithm is much faster to train.
With our Matlabimplementation, it took about 30 seconds, whileeach iteration of EM took from 2 to 3 minutes,depending on the number of states.
To give a con-crete example, to reach an accuracy close to 80%,there is a factor of 150 between the training timesof the spectral method and EM (where we com-pare the peak performance of the spectral methodversus EM at 25 iterations with 13 states).7For parsing with deterministic SHAG we employ MBRinference, even though Viterbi inference can be performedexactly.
In experiments on development data DET improvedfrom 62.65% using Viterbi to 68.52% using MBR, andDET+F improved from 72.72% to 74.80%.8We ran EM 10 times under different initial conditionsand selected the run that gave the best absolute accuracy after100 iterations.
We did not observe significant differencesbetween the runs.415DET DET+F SPECTRAL EMWSJ 69.45% 75.91% 80.44% 81.68%Table 1: Unlabeled Attachment Score of fully unlexi-calized models on the WSJ test set.Table 1 shows results on WSJ test data, se-lecting the models that obtain peak performancesin development.
We observe the same behavior:hidden-states largely improve over deterministicbaselines, and EM obtains a slight improvementover the spectral algorithm.
Comparing to previ-ous work on parsing WSJ PoS sequences, Eisnerand Smith (2010) obtained an accuracy of 75.6%using a deterministic SHAG that uses informa-tion about dependency lengths.
However, theyused Viterbi inference, which we found to per-form worse than MBR inference (see footnote 7).5.2 Experiments with LexicalizedGrammarsWe now turn to combining lexicalized determinis-tic grammars with the unlexicalized grammars ob-tained in the previous experiment using the spec-tral algorithm.
The goal behind this experimentis to show that the information captured in hiddenstates is complimentary to head-modifier lexicalpreferences.In this case X consists of lexical items, and weassume access to the PoS tag of each lexical item.We will denote as ta and wa the PoS tag and wordof a symbol a ?
X?
.
We will estimate condi-tional distributions P(a | h, d, ?
), where a ?
Xis a modifier, h ?
X?
is a head, d is a direction,and ?
is a deterministic state.
Following Collins(1999), we use three configurations of determin-istic states:?
LEX: a single state.?
LEX+F: two distinct states for first modifierand rest of modifiers.?
LEX+FCP: four distinct states, encoding:first modifier, previous modifier was a coor-dination, previous modifier was punctuation,and previous modifier was some other word.To estimate P we use a back-off strategy:P(a|h, d, ?)
= PA(ta|h, d, ?
)PB(wa|ta, h, d, ?
)To estimate PA we use two back-off levels,the fine level conditions on {wh, d, ?}
and the72747678808284862  3  4  5  6  7  8  9  10unlabeled attachmentscorenumber of statesLexLex+FLex+FCPLex + SpectralLex+F + SpectralLex+FCP + SpectralFigure 2: Accuracy curve on English development setfor lexicalized models.coarse level conditions on {th, d, ?}.
For PB weuse three levels, which from fine to coarse are{ta, wh, d, ?
}, {ta, th, d, ?}
and {ta}.
We followCollins (1999) to estimate PA and PB from a tree-bank using a back-off strategy.We use a simple approach to combine lexicalmodels with the unlexical hidden-state models weobtained in the previous experiment.
Namely, weuse a log-linear model that computes scores forhead-modifier sequences ass(?h, d, x1:T ?)
= logPsp(x1:T |h, d) (21)+ logPdet(x1:T |h, d) ,where Psp and Pdet are respectively spectral anddeterministic probabilistic models.
We testedcombinations of each deterministic model withthe spectral unlexicalized model using differentnumber of states.
Figure 2 shows the accuracies ofsingle deterministic models, together with combi-nations using different number of states.
In allcases, the combinations largely improve over thepurely deterministic lexical counterparts, suggest-ing that the information encoded in hidden statesis complementary to lexical preferences.5.3 Results AnalysisWe conclude the experiments by analyzing thestate space learned by the spectral algorithm.Consider the space Rn where the forward-statevectors lie.
Generating a modifier sequence corre-sponds to a path through the n-dimensional statespace.
We clustered sets of forward-state vectorsin order to create a DFA that we can use to visu-alize the phenomena captured by the state space.416ccjj dt nnpprp$ vbg jjsrb vbn posjj in dt cd1 57I203ccnnscd,$ nnpcd nnsSTOP,prp$ rb posjj dt nnp9$ nnjjr nnpSTOPSTOPccnnSTOPccnn,prp$ nn posFigure 3: DFA approximation for the generation of NNleft modifier sequences.To build a DFA, we computed the forward vec-tors corresponding to frequent prefixes of modi-fier sequences of the development set.
Then, weclustered these vectors using a Group AverageAgglomerative algorithm using the cosine simi-larity measure (Manning et al 2008).
This simi-larity measure is appropriate because it comparesthe angle between vectors, and is not affected bytheir magnitude (the magnitude of forward vec-tors decreases with the number of modifiers gen-erated).
Each cluster i defines a state in the DFA,and we say that a sequence x1:t is in state i if itscorresponding forward vector at time t is in clus-ter i.
Then, transitions in the DFA are defined us-ing a procedure that looks at how sequences tra-verse the states.
If a sequence x1:t is at state i attime t ?
1, and goes to state j at time t, then wedefine a transition from state i to state j with la-bel xt.
This procedure may require merging statesto give a consistent DFA, because different se-quences may define different transitions for thesame states and modifiers.
After doing a merge,new merges may be required, so the proceduremust be repeated until a DFA is obtained.For this analysis, we took the spectral modelwith 9 states, and built DFA from the non-deterministic automata corresponding to headsand directions where we saw largest improve-ments in accuracy with respect to the baselines.A DFA for the automaton (NN, LEFT) is shownin Figure 3.
The vectors were originally dividedin ten clusters, but the DFA construction requiredtwo state mergings, leading to a eight state au-tomaton.
The state named I is the initial state.Clearly, we can see that there are special statesfor punctuation (state 9) and coordination (states1 and 5).
States 0 and 2 are harder to interpret.To understand them better, we computed an esti-mation of the probabilities of the transitions, bycounting the number of times each of them isused.
We found that our estimation of generatingSTOP from state 0 is 0.67, and from state 2 it is0.15.
Interestingly, state 2 can transition to state 0generating prp$, POS or DT, that are usual end-ings of modifier sequences for nouns (recall thatmodifiers are generated head-outwards, so for aleft automaton the final modifier is the left-mostmodifier in the sentence).6 ConclusionOur main contribution is a basic tool for inducingsequential hidden structure in dependency gram-mars.
Most of the recent work in dependencyparsing has explored explicit feature engineering.In part, this may be attributed to the high cost ofusing tools such as EM to induce representations.Our experiments have shown that adding hidden-structure improves parsing accuracy, and that ourspectral algorithm is highly scalable.Our methods may be used to enrich the rep-resentational power of more sophisticated depen-dency models.
For example, future work shouldconsider enhancing lexicalized dependency gram-mars with hidden states that summarize lexicaldependencies.
Another line for future researchshould extend the learning algorithm to be ableto capture vertical hidden relations in the depen-dency tree, in addition to sequential relations.Acknowledgements We are grateful to GabrieleMusillo and the anonymous reviewers for providing uswith helpful comments.
This work was supported bya Google Research Award and by the European Com-mission (PASCAL2 NoE FP7-216886, XLike STREPFP7-288342).
Borja Balle was supported by an FPUfellowship (AP2008-02064) of the Spanish Ministryof Education.
The Spanish Ministry of Science andInnovation supported Ariadna Quattoni (JCI-2009-04240) and Xavier Carreras (RYC-2008-02223 and?KNOW2?
TIN2009-14715-C04-04).417ReferencesRaphael Bailly.
2011.
Quadratic weighted automata:Spectral algorithm and likelihood maximization.JMLR Workshop and Conference Proceedings ?ACML.James K. Baker.
1979.
Trainable grammars for speechrecognition.
In D. H. Klatt and J. J. Wolf, editors,Speech Communication Papers for the 97th Meetingof the Acoustical Society of America, pages 547?550.Borja Balle, Ariadna Quattoni, and Xavier Carreras.2012.
Local loss optimization in operator models:A new insight into spectral learning.
Technical Re-port LSI-12-5-R, Departament de Llenguatges i Sis-temes Informa`tics (LSI), Universitat Polite`cnica deCatalunya (UPC).Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957?961, Prague, Czech Re-public, June.
Association for Computational Lin-guistics.Stephen Clark and James R. Curran.
2004.
Parsingthe wsj using ccg and log-linear models.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 103?110, Barcelona, Spain, July.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the em algorithm.
Journal of the royal sta-tistical society, Series B, 39(1):1?38.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head-automaton grammars.
In Proceedings of the 37thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), pages 457?464, Univer-sity of Maryland, June.Jason Eisner and Noah A. Smith.
2010.
Favorshort dependencies: Parsing with soft and hard con-straints on dependency length.
In Harry Bunt, PaolaMerlo, and Joakim Nivre, editors, Trends in ParsingTechnology: Dependency Parsing, Domain Adapta-tion, and Deep Parsing, chapter 8, pages 121?150.Springer.Jason Eisner.
2000.
Bilexical grammars and theircubic-time parsing algorithms.
In Harry Bunt andAnton Nijholt, editors, Advances in Probabilis-tic and Other Parsing Technologies, pages 29?62.Kluwer Academic Publishers, October.Joshua Goodman.
1996.
Parsing algorithms and met-rics.
In Proceedings of the 34th Annual Meetingof the Association for Computational Linguistics,pages 177?183, Santa Cruz, California, USA, June.Association for Computational Linguistics.Daniel Hsu, Sham M. Kakade, and Tong Zhang.
2009.A spectral algorithm for learning hidden markovmodels.
In COLT 2009 - The 22nd Conference onLearning Theory.Gabriel Infante-Lopez and Maarten de Rijke.
2004.Alternative approaches for generating bodies ofgrammar rules.
In Proceedings of the 42nd Meet-ing of the Association for Computational Lin-guistics (ACL?04), Main Volume, pages 454?461,Barcelona, Spain, July.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1?11, Uppsala, Sweden,July.
Association for Computational Linguistics.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, Cambridge,first edition, July.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19.Andre Martins, Noah Smith, and Eric Xing.
2009.Concise integer linear programming formulationsfor dependency parsing.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 342?350, Suntec, Singapore, August.
Association forComputational Linguistics.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL?05),pages 75?82, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 81?88.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 523?530, Vancouver,British Columbia, Canada, October.
Association forComputational Linguistics.Gabriele Antonio Musillo and Paola Merlo.
2008.
Un-lexicalised hidden variable models of split depen-dency grammars.
In Proceedings of ACL-08: HLT,Short Papers, pages 213?216, Columbus, Ohio,June.
Association for Computational Linguistics.James D. Park and Adnan Darwiche.
2004.
Com-plexity results and approximation strategies for map418explanations.
Journal of Artificial Intelligence Re-search, 21:101?133.Mark Paskin.
2001.
Cubic-time parsing and learningalgorithms for grammatical bigram models.
Techni-cal Report UCB/CSD-01-1148, University of Cali-fornia, Berkeley.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Association for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 433?440, Sydney, Australia, July.
Association for Com-putational Linguistics.Ivan Titov and James Henderson.
2006.
Loss mini-mization in parse reranking.
In Proceedings of the2006 Conference on Empirical Methods in Natu-ral Language Processing, pages 560?567, Sydney,Australia, July.
Association for Computational Lin-guistics.Ivan Titov and James Henderson.
2007.
A latent vari-able model for generative dependency parsing.
InProceedings of the Tenth International Conferenceon Parsing Technologies, pages 144?155, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.419
