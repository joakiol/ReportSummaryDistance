One-Level Phonology: AutosegmentalRepresentations and Rules as FiniteAutomataSteven Bird*University of EdinburghT.
Mark Ellison*University of EdinburghWhen phonological rules are regarded as declarative descriptions, itis possible to construct a modelof phonology in which rules and representations are no longer distinguished and such proceduraldevices as rule-ordering are absent.
In this paper we present afinite-state model of phonologyin which automata re the descriptions and tapes (or strings) are the objects being described.This provides the formal semantics for an autosegmental phonology without structure-changingrules.
Logical operations on the phonological domain--such as conjunction, disjunction, andnegation--make s nse since the phonological domain consists of descriptions rather than objects.These operations as applied to automata are the straightforward operations of intersection, union,and complement.
If the arrow in a rewrite rule is viewed as logical implication, then a phonologicalrule can also be represented as an automaton, albeit a less restrictive automaton than would berequired for a lexical representation.
The model is then compared with the transducer models forautosegmental phonology of Kay (1987), Kornai (1991), and Wiebe (1992).
We conclude that thedeclarative approach to phonology presents an attractive way of extending finite-state t chniquesto autosegmental phonology while remaining within the confines of regular grammar.1.
IntroductionThe decade since the publication of Koskenniemi's dissertation (1983) and since thedevelopment of the KIMMO system (Karttunen 1983) has witnessed a spectacular flurryof activity as the linguistic and computational consequences of this work have beenfleshed out.
A considerable body of literature has grown up around TWO-LEVEL MOR-PHOLOGY, along with texts 1and implementations.
2 The existence of a rule compiler(Koskenniemi 1985) has made it possible for the linguist to work at a convenientlyabstract level, and analyses of several languages now exemplify the approach.
Today,two-level morphology encompasses much of traditional segmental generative phonol-ogy of the SPE variety (Chomsky and Halle 1968).
3Although further development and application of this model is set to continue forsome time, there is now a clear need to integrate it more closely with computational* University of Edinburgh, Centre for Cognitive Science, 2 Buccleuch Place, Edinburgh EH8 9LW,Scotland, U.K. E-mail: {steven,marke}@cogsci.ed.ac.uk1 (Antworth 1990; Ritchie, Russell, Black, and Pulman 1992; Sproat 1992)2 (Bear 1986; Antworth 1990; Schiller and Steffens 1991; Pulman and Hepple 1993)3 Two caveats are necessary here.
SPE rules must be restricted so as not to apply to their own output(Johnson 1972) and there is no guarantee that the transducer encoding an SPE rule can be expressedusing the two-level rule notation (Ritchie 1992).
(~) 1994 Association for Computational LinguisticsComputational Linguistics Volume 20, Number 1grammar frameworks on the one hand and modern onlinear phonology on the other.The primary goal of this article is to show how the central tenets of autosegmentalphonology translate into an implemented finite state model.The model is named ONE-LEVEL PHONOLOGY for two reasons.
First, the modelis monostratal, in that there is only one level of linguistic description.
Second, thename is intended to contrast with models employing two levels (such as the FSTmodel mentioned above) or three levels (Goldsmith 1991; Touretzky and Wheeler1990), or an unbounded number of levels (Chomsky and Halle 1968).
The one-levelmodel represents he outgrowth of three independent s rands of research: (i) the finite-state modeling of phonology, (ii) the declarative approach to phonology, 4 and (iii) theautomatic learning of phonological generalizations (Ellison 1992, 1993).The paper is organized as follows.
Section 2 presents an overview of autosegmen-tal phonology and the temporal semantics of Bird and Klein (1990).
Then we definestate-labeled automata (Section 3.1), show their equivalence to finite state automata(Section 3.2), define the operations of concatenation, union, intersection, and com-plement (Section 3.3), and further define state-labeled transducers (Section 3.4).
Thecentral proposals of the paper are contained in Section 4.
We show how autosegmentalassociation can be interpreted in terms of the synchronization f two automata, whereeach automaton specifies an autosegmental ier.
We now give a brief foretaste of thisprocedure.
Suppose that we have the autosegmental diagram in (1), encoding high(hi) and round (rnd) autosegments.. +hi -h iI / I- rnd  +rndThis diagram is encoded as the following expression, where each numeral indicatesthe number of association lines incident with its corresponding autosegment.+hi:l -h i :2- rnd:  2 +rnd: 1From this encoding, we can write down the following regular expression.
Althoughsuch expressions will be opaque at this early stage of the exposition, it suffices to notehere that each line of the expression represents a tier and the tiers are combined usingthe intersection operation (m).
Moreover, the ls act as synchronization marks betweenthe operands of the intersection operation.
(+hi, 0)* (+hi, 1) (+hi, 0)* (-hi, 0)* (-hi, 1)(-hi, 0)* (-hi, 1)(-hi, 0)*m (-rnd, 0)* (-rnd, 1) (-rnd, 0)* (-rnd, 1) (-rnd, 0)* (+rnd, 0)* (+rnd, 1) (+rnd, 0)*The final step is to compute the intersection and project he first element of each tuple(ignoring the ls and 0s).
This produces the expression:(+hi A -rnd) + (-hi  N -rnd) + ( -h i  n +rnd) +.4 Wheeler 1981; Bird 1990; Coleman 1991; Scobbie 1991; Broe 1993; Russell 1993; Mastroianni 1993.56Steven Bird and T. Mark Ellison One-Level PhonologyGiven plausible interpretations of the high and round features, this last expressionsimplifies to i+a+o +, which describes an automaton tape (or a string) divided intothree nonempty intervals, the first containing \[i\], the second containing \[a\], and thethird containing \[o\].
This, we shall claim, is the intended interpretation of (1).After a detailed discussion of this procedure, the remainder of Section 4 is givenover to generalizing the procedure to an arbitrary number of autosegmental charts(Section 4.4), an evaluation of the encoding with respect o Kornai's desiderata (Sec-tion 4.5), and a presentation of the encoding of autosegmental rules (Section 4.6).Finally, Section 5 compares our proposals with those of Kay (1987), Kornai (1991),and Wiebe (1992).
While our model has regular grammar power and is fully imple-mented, these three models go beyond regular grammar power and to our knowledgehave never been implemented.2.
BackgroundIt has long been recognized that the SPE model lacks explanatory adequacy, a factnoted in SPE itself (Chomsky and Halle 1968, pp.
400ff).
For example, it is unable toexplain why a final devoicing rule like that in (2a) is commonplace in the languagesof the world, whereas the rule in (2b) is unattested (Kaye 1989, p. 61).2.
(a)(b)(c)(d)\[-sonorant\] --* \[-voice\] / - -#\[-sonorant\] --+\[+nasal\] / - -#\[\] --* \[anasal\] / \[anasal\] - -#\[\] ~ \[around\] / \ [anasa l \ ] -  #Similarly, the nasal harmony rule in (2c) occurs frequently, while the generalizationexpressed in (2d) is as unlikely as (2b).
Both SPE and the two-level model are unableto express the fact that some rules are commonplace while others are highly unnatural.Perhaps the SPE model could be rescued from these problems with additionalstipulations.
However, a more fundamental problem for the model was raised by thefollowing tone language data (Leben 1973; Goldsmith 1976).
At first blush, Mendevowels appear to manifest five tone patterns, namely high (ko), low (kph), falling(mbfi), rising (mb~) and rise-fall (mbS).
The SPE model would predict 25 tonal patternsfor two-syllable morphemes, but instead we find only 5.
These are high-high, low-low,high-low, low-high, and low-falling.
Similarly, for three-syllable morphemes we get 5patterns, not 125.
Leben noticed that the one-, two-, and three-syllable morphemescould be put into correspondence as shown in (3), from Leben (1978).. H: k5 war p~l~ house hhw~m~ waistlineL: kpa debt b~l~ trousers kpak/tll tripod chairHL: mbfi owl ngflh dog f61amh junctionLH: mbh rice fhnd6 cotton nd/~vfil~i slingLHL: rob8 companion nyhh~ woman nikili groundnutGoldsmith (1976) devised a graphical notation that made the above correspon-dence clearer still.
We display several examples of his notation in (4).
Here, the Hindicates high tone, while L indicates low tone.
(The association lines are assumed tobe incident with the vowels.
)57Computational Linguistics Volume 20, Number 1. k6 1 ~ 1?
h~i w~i mfiI/ \ /H H Hkpa b~ lP kpa kh liE/ \ l /L L Lmbfi ngi 13 f6 1~ mh\ I I I I/H L H L H Lmb~ fh nd6 ndh vti lfiL H L H L HLmb~ nyh ha ni kl Ii/1 \  I\ I IH L L H L L H LObserve that each row of the table has the same tone pattern.
Only the synchro-nization varies.
In diagrams like the ones in (4), units such as H and L are termedAUTONOMOUS SEGMENTS, or AUTOSEGMENTS, and a linear sequence of autoseg-ments is called a TIER.
The synchronization markers are called ASSOCIATION LINES.A pair of tiers linked by some association lines is called a CHART.
A chart is calledWELL-FORMED if the following conditions hold (Goldsmith 1976, p. 27).5.
Well-Formedness Condition:(a)(b)All vowels are associated with at least one tone;all tones are associated with at least one vowel.Association lines do not cross.The reader can ascertain that the above charts are wel l - formed according to (5).
How-ever, (5) is insufficiently restrictive on its own, and a further stipulation is required., Associat ion Convention: Only the r ightmost member  of a tier can beassociated to more than one member  of another tier.58Steven Bird and T. Mark Ellison One-Level PhonologyWhen (5) and (6) are combined, we achieve the effect of one-to-one left-to-rightassociation, where multiple association (or SPREADING) occurs only at the right-handend.
Observe also that the charts in (4) do not contain adjacent identical tones.
Forexample, there is no HH tone melody.
This is expressed by a principle attributable toLeben (1973)..
Obligatory Contour Principle: At the melodic level of the grammar, anytwo adjacent \[autosegments\] must be distinct.
Thus HHL is not apossible melodic pattern; it automatically simplifies to HL.Although nonlinear models like autosegmental phonology represent a major ad-vance on the linear model of SPE in the area of explanatory adequacy, it has sometimesbeen pointed out (e.g., Bird and Ladd 199l) that the formal explicitness of the SPEmodel has not been matched by these more recent proposals.
Before we can beginto compute with autosegmental representations and rules, they need to be given aformal semantics.
Our starting point here is the temporal semantics of Bird and Klein(1990), based on Sagey's (1988) model, which has gained widespread acceptance in au-tosegmental phonology.
Under this temporal semantics, phonological properties areattached to intervals that are related using precedence (an asymmetric, transitive re-lation) and overlap (a reflexive, symmetric relation).Bird (1990) showed how a phonological description language can be modeled bysuch event structures, where the precedence relation models the linear ordering oftiers and the overlap relation models association lines.
In this paper, we shall pro-vide an automaton-based semantics for precedence and overlap, thus arriving at acomputational semantics for the autosegmental notation.3.
State-Labeled AutomataIn this section we give definitions for a new device called a state-labeled finite automa-ton, and then we define various useful operations on these automata.
(Some readersmay prefer to skip Section 3 on a first reading.
)3.1 DefinitionsDefinition 1A STATE-LABELED NONDETERMINISTIC FINITE AUTOMATON (SFA) is a septuple(V~ ~, ~, 6~ S, F, e) whereV is a finite set, the set of STATES,~.
is a finite set, the ALPHABET,C V x ~, is the LABELING RELATION (states are labeled with subsets of thealphabet), 56 c V x V is the TRANSITION RELATION,S C V is the set of START STATES, andF c V is the set of FINAL STATES.e is a Boolean flag that is true iff the null string A is accepted, and false otherwise.5 Without loss of generality we have chosen to label states with segments ( ubsets of~), rather thanwith strings (subsets ofG*).59Computational Linguistics Volume 20, Number 1Before describing the execution of an SFA, we need to define COMPATIBILITY.Def in i t ion 2We say that a state v is COMPATIBLE with an input cr E G if (v, c~ / E )~.At each step in the execution of an SFA, a subset of the states is active while theremainder are inactive.
An SFA begins operation with those start states active that arecompatible with the first symbol in the input string.
If, at a certain step in processing, asubset T of states is active, then at the next step, the subset T t of states reachable from Tand compatible with the next input become active.
This operation is formalized below,following the approach taken by Partee et al (1990).
First we define a SITUATION tobe the processing status of an SFA.Def in i t ion 3A SITUATION of an  SFA, A, is a triple (x, T,y I where T c V is the set of currentlyactive states, and x and y are the portions of the input string to the left and right ofthe reading head, respectively.As an SFA operates, it moves through a sequence of these situations.
Now F-A is definedas a successor relation on situations for an automaton A.Def in i t ion 4Let Ix, T, y I and Ix', T', y' I be two situations.
Then Ix, T, y I }-"A IX'~ T', y'I iff there is aE G such that(i)(ii)(iii)y = cry ~ and x ~ = xGfor each Y E T I there is a t E T such that (t, t' I E 6, and(t', cr / E )~ for each t' E T'.The first condition in the definition concerns or, the tape symbol being scanned.
Thissymbol is the first in the string y and the last in the string x'.
The second conditionconcerns the transition relation, requiring that the new situation must be reachablefrom the previous situation.
The third condition is a check that ?
is in the label set ofeach currently active state.
We define t-~ to be the transitive closure of ~-a.
Now wecan specify the conditions under which an SFA accepts a string, where A is the emptystring.Def in i t ion 5Let A = (V,G,A,6, S,F,e) be an SFA and let w E G*.
A ACCEPTS w iff either e is trueand w = A, or (or, {s}, a) t-~ (era, F', A/, for some (s, cr / E ;~, s E S, a E ~* and FMF' ~ O,and where w = era.If no string can cause an SFA to have more than one active state at any processing step,then we say that the SFA is DETERMINISTIC.
In order to signify that an SFA acceptsthe empty string A (i.e., if e is true), we shall include a special state, labeled with adistinguished symbol 0, which is marked both initial and final.60Steven Bird and T. Mark Ellison One-Level Phonology3.2 Relationship to FSAsThe equivalence of SFAs and arc-labeled finite-state automata (FSAs) follows from theequivalence of Mealy and Moore machines.
6 Although SFAs are no more expressivethan FSAs, there are good linguistic reasons for wishing to use them.
The primarydifference between the two devices lies in the relative ease with which particular gen-eralizations can be expressed.
As an illustration, we shall consider the automaton thatprohibits two adjacent occurrences of any given symbol in a string, a constraint knownin autosegmental phonology as the Obligatory Contour Principle.
Here is the FSA ver-sion, using the graphical conventions for representing FSAs adopted by Hopcroft andUllman (1979).
Bullet marks a state, circled states are final, and states with incomingarrow heads are initial.d a - -The notation we use for state-labeled automata is different.
Because states carry labels,there is no need to use the contentless bullet symbol to mark a state.
Instead, the labelitself marks the state.
Initial and final states are indicated by arrowheads and circles,as for FSAs.
Arcs are unlabeled.The SFA that does not admit adjacent occurrences of any symbol in the alphabetis:Notice that the number of labels required by the SFA is considerably smaller (5 labels)than the number required in the FSA (12 labels), while the number of transitions isthe same for both.
On the other hand, the SFA has an extra state, labelled with 0 toshow that the automaton accepts the null string (see the final clause of Definition 1).The difference between the two devices becomes even clearer when we considerthe representations for a* (zero or more as) and a + (one or more as).
First, here are a*and a + expressed as FSAs.4c aa* : a + : >'e a ~"Observe that the specification of the Kleene star requires one state, while Kleene plusrequires two.
If we use SFAs instead, we find the reverse: Kleene star requires two6 See Hopcroft and Ullman (1979, p. 44) for a discussion of this equivalence.
An FSA is a Mealy machinethat ignores its input, while an SFA is a Moore machine that ignores its input.61Computational Linguistics Volume 20, Number 1states, while Kleene plus only requires one.a*: >(~ > ~  a+: ~~-~As we shall see in Section 4, the semantics of phonological representations requiresfrequent use of the Kleene plus and little use of the Kleene star.
The intuition behindthis is simple.
Recall from Section 2 that phonological entities uch as distinctive fea-tures are considered to be descriptions of phonetic events that may extend over aninterval of time.
As we saw in the case of tone, the defining feature of an autosegmentmay be spread across several segments.
Crucially, however, an autosegment must bepresent at at least one point, and so it makes sense to view phonological entities--suchas segments and autosegments--in terms of the Kleene plus rather than the Kleenestar.It might be reasoned that our interval interpretation of segments i better picturedwith arc-labeled evices.
After all, the states resemble points while the arcs resem-ble extended intervals.
Furthermore, it may be tempting to use a single arc betweentwo temporally distant points to show the spreading of an autosegment coarticulatedwith two consecutive autosegments on another tier.
For example, (8) shows a labialautosegment, L, bridging two instants also bridged by a nasal segment, N, and stop, S..>"o- N>o S >However, this representation is flawed: the semantics assigned to coterminous pathscontradicts the standard interpretation of FSAs.
The automaton pictured above wouldnormally be interpreted as either a nasal followed by a stop or by a single labialarticulation.
Crucially, it could not be interpreted as necessarily both a nasal followedby a stop and a labial articulation.While viewing states as instants of time and arcs as intervals offers some iconicity,it is also misleading.
It is just as natural--and more in keeping with the logical founda-tions presented in Section 2--to employ the states for temporally extended intervals,and the arcs for the relationship of immediate precedence.3.3 Bas ic  Operat ionsIn this section we define the operations of concatenation, union, intersection, andcomplement on SFAs.
These operations correspond naturally to the operations on thelanguages accepted by the automata, as indicated in the following table.operationconcatenationunionintersectioncomplementKleene plusKleene starautomata languagesABAUBA \ [TB-dA +A*(ab\]a L(A), b E L(B)}UL(A) n r(B)L(A)L(A) +L(A)*62Steven Bird and T. Mark Ellison One-Level PhonologyThe Kleene plus operation, which, when applied to a language L gives another L+,contains the concatenation of one or more strings from L. The Kleene star operationtakes L to {A} U L + and is written L*.Recall that the structure (~*; U, N, --, 0, G*), containing languages over an alphabetY.
together with the standard set operations, is a Boolean algebra (Partee et al 1990,p.
297ff).
Similarly, if A is the set of SFAs, then (A; U, , 4, 3_, T) is also a Booleanalgebra, where 3_ is the empty automaton (i.e., L(3_) = 0) and T is the automaton thataccepts G* (i.e., L(T) = y~$k).7The concatenation of two SFAs A and B, written AB, has an arrow linking eachfinal state of the first SFA to each initial state of the second.
The states that are initialor final in AB depend on whether A or B accepts the empty string A, as specified inthe following table.AA (t L(A)A E L(A)A ~ L(A)A C L(A)BA EL(B)A L(B)A L(B)A C L(B)ABA (d L(AB)A ~ L(AB)A (d L(AB)A E L(AB)AB initialsA initialsA & B initialsA initialsA & B initialsAB finalsB finalsB finalsA & B finalsA & B finalsSuppose we wished to recognize the language AB where A -- (12+3) + and B =(45+6) +.
The SFAs describing A and B are the following.~- 2 ~ ~- A:  1 ~ B: 4 \!
?Linking final states of A to the initial states of B gives the concatenation.AB : 1 4 \l \l3" ?Since neither A nor B accepts A, the initial state of AB is the initial state of A, and thefinal state of AB is the final state of B.The union (or disjunction) on SFAs is similar, in many ways, to the concatena-tion operation.
The difference is that rather than executing in sequence, the automataoperate in parallel.
The union of two automata A and B, written A U B, accepts thestring s iff either A or B, or both, accept s. The union of A and B is expressed ia-7 of course, these algebras are different, for there are many languages that cannot be defined by SFAs.63Computational Linguistics Volume 20, Number 1grammatically by placing the diagram for A alongside the diagram for B.
The union(12+3) + U (45+6) + is drawn as:AUB:  1?~ 4 ~5 ~?The intersection of two SFAs A and B, written A n B, accepts a string s iff both Aand B accept s. Consider the intersection of the automata that recognize the languages(12+3) + and (1+23+) +.
These two automata re:A:  1 ~ B: >2Two states are compatible if and only if their label sets have a nonempty intersection.The intersection of these automata is formed by taking all pairs of states that arecompatible and linking these with arcs whenever both projections of the pairs arelinked.
The intersection of the above automata is shown below.AnB:  1 72?This automaton recognizes the language (123) + .The complement of an automaton A, written A, accepts a string s iff A rejects s.One way of forming the complement involves the following steps.
First, the automatonmust be DETERMINIZED (Hopcroft and Ullman 1979, pp.
22ff).
The next step is to formthe COMPLETION.
A complete automaton is one that has a transition from every statefor each element of G. The final step is to mark all final states nonfinal, and all nonfinalstates final.
So if S is the set of states and F is the set of final states, then S\F is the setof final states in the complement.3.4 TransducersIn the previous three sections, we defined state-labeled automata nd some operationsthat can be used to combine them.
We also saw that these automata re equivalent64Steven Bird and T. Mark Ellison One-Level Phonologyto traditional, arcqabeled automata.
Just as we can define arc-labeled automata calledfinite-state transducers (FSTs), we can define state-labeled transducers (SFFs).An (epsilon free) state-labeled transducer is just an SFA with a special alphabet.Instead of labeling each state with a subset of a single alphabet, we label them withsubsets of the product of two alphabets.
The strings accepted are sequences of pairsconsisting of one letter from each alphabet.
A transducer can be used as a translator:it takes as input one half of the label on a state, and simultaneously writes as outputthe other half.
All output strings generated by a path from initial to final states aretranslations of the input string recognized by the same path.
Since the SFT is also anSFA, intersection is defined for SFTs.The reader may wonder whether there is any distinction between one-level phonol-ogy and two-level phonology if SFAs and SFTs are formally identical.
There is an im-portant distinction to be drawn, however.
First, most two-level models employ FSTswith epsilons, which are more powerful devices than FSAs.
Second, in the one-levelmodel, representations and rules are interpreted as automata.
In contrast, the two-level model employs strings for representations and automata for rules.
Finally, inone-level phonology surface forms and generalizations about them are stated directlyin a hierarchical lexicon akin to that of head-driven phrase structure grammar (HPSG)(Pollard and Sag 1987), rather than being mediated through a transducer (Bird andKlein, in press).4.
Association and SynchronizationIn this section we present he automaton-based semantics for autosegmental phonol-ogy.4.1 The Representation of Autosegments and TiersRecall that an autosegment denotes a possibly extended interval.
In terms of automata,this means that an autosegment must allow multiple copies of its defining property.This is expressed as follows.This state of affairs fits well with our intuitive understanding that a pair of adjacentintervals in which some property holds is indistinguishable from a single interval---theunion of the first two intervals--during which that same property holds.
Furthermore,such a claim connects with the Obligatory Contour Principle (7).
8Unfortunately, however, this definition of autosegment is inadequate.
Suppose wehave a nasal segment N that is lexically unspecified for its place of articulation.
In alanguage with the nasals m and n, the intention is that this segment denotes intervalssuch as the following:Imlm\ ]m\ [mlml  In \ [n \ ]n \ ]n~However, there is nothing to stop N from denoting the following interval:Imrnrnlm\[nJmf8 A consequence of this approach is that it circumvents some potential problems caused by our notemploying epsilons (cf.
Section 3.1).
If an autosegment al ernates with zero, we do not employ ?
for thezero alternant but permit surrounding autosegments to extend to 'fill in the gap.
'65Computational Linguistics Volume 20, Number 1Here, it is clear that N is behaving like a variable.
If N is instantiated, then theentire interval must remain homogeneous.
So the representation f this N is not (9a)but (9b).9.
(a)(b)Some autosegments, however, appear to lack this homogeneity property.
For ex-ample, the Turkish word pe~eleri contains everal front vowels.
An analysis of suchwords that employs the principles of vowel harmony posits a +front autosegment thatis associated with every vowel of the word.
Observe that this autosegment is heteroge-neous, as it includes in its temporal extent both e and i.
Accordingly, its interpretationwill follow the scheme of (9a) above.
Briefly continuing in this vein, we can conceiveof a range of different kinds of segment, using varying numbers of states and varyingnumbers of labels per state.
Four options are described below:Simple Segments.
These capture the ordinary kind of segment and consist of asingle state labeled with a singleton set.
In general, when we employ asymbol ike b it will be interpreted as a simple segment.Homogeneous Segments.
These represent slots (like N) and members of tem-plates (like CVCCVC), and consist of more than one state.
Each state islabeled with a singleton set.
An example of a homogeneous segment isfound in (9b).Heterogeneous Segments.
These represent spreading autosegments, like +high(and b in Section 5.4).
The automata have a single state which is labeledwith a nonsingleton set.
An example of a heterogeneous segment is foundin (9a).Hybrid Segments.
These represent spreading autosegments that have greek lettervariables, like c~place or ~high.
An example of a hybrid segment for odrontis given in example (10).10.Recall that an autosegmental tier is just a linear ordering of autosegments.
There-fore, if P, Q and R are segments (of any of the four kinds specified above), then a tierP-Q-R is formed by simply concatenating P, Q, and R together.Now we have seen the interpretation ofautosegments and tiers.
The synchronizationof tiers is controlled by association lines.
The next section discusses the interpretationof these lines.66Steven Bird and T. Mark Ellison One-Level Phonology4.2  The  In terpretat ion  o f  Assoc ia t ionIn Section 2 we presented an interpretation of association based on temporal overlap.Now we must find a way of simulating this temporal structure using automata.
Letus begin by considering the simplest possible autosegmental diagram.11 .
.
.
.
A ..
.?
?
?
B ?
?
?Since each autosegment denotes an interval and the two intervals must overlap, wewould like to interpret he above diagram as describing any of the following strings,among others: 9IA  A A " " {A IA IA  A IA I  " " A?
?
B B B ?
?
B B ?
B B BWhat kind of automaton will give us the required behavior?
The clue is that a pairof intervals overlap if and only if they share a point in common (Bird and Klein 1990,p?
36)?
Note that in each of the above diagrams, the third interval contains an instanceof both A and B, and the existence of this interval was both a necessary and sufficientrequirement for the association line to have its overlap interpretation.
So we need anautomaton for each autosegment that captures the two required properties: (i) deno-tation of an extended period, and (ii) existence of a special point.
The "automaton"required for the autosegment A is given in (12).12.This automaton accepts any string of one or more As, and requires that there is someA that is coincident with an autosegment somewhere lse.
The line extending fromthe middle state informally indicates that this state is simultaneous with a state inanother automaton.
Now we must create a similar automaton for the autosegment B.Recall from Section 4.1 that we need to construct iers for A and B by concatenatingthe autosegments o the left and right (elided in (11))?
Finally, the automata re puttogether as shown below:?
?
?
~ -  + ?..o .
?
-} -  + ?..9 Note that in using such diagrams we are attempting toisolate the effects of two automata: those cellscontaining both A and B should be understood as containing A N B.67Computational Linguistics Volume 20, Number 1This unusual kind of automaton represents a halfway house between autosegmentaldiagrams and SFAs; we shall call it a SYNCHRONIZED SFA.
Such automata have asimple interpretation: both component automata run in parallel, but the second stateof the A automaton is active iff the second state of the B automaton is simultaneouslyactive.
In a sense, we have converted an autosegmental model involving intervalsand overlap into a simpler model involving atomic periods and simultaneity.
Nowconsider autosegmental diagram (13).13.
A B\ /CNote that there is no additional material on either side of the group of three autoseg-ments (cf.
(11)).
Therefore, we assume that both tiers are descriptions of a completeutterance and so must begin and end simultaneously.
1?
In this diagram, C has twoassociations.
The automaton we need for C is more complex than before, since theinterval that C denotes must have two special points, one for overlap with A and theother for overlap with B.
The required automaton for C is given in (14).14.IAutomaton (14) is the concatenation of two copies of an automaton like (12).
Puttingthe automata for A, B, and C together gives the synchronized automaton in (15).15.A ~ A  ~ A~-~ ~ ~ - - - - - - - - ~The behavior of synchronized SFAs can be simulated by an ordinary SFA, as we showin the next section.4.3 Simulating Synchronized AutomataTo do this simulation, we need to do away with the synchronization lines connectingthe automata.
Starting with (15), we add indices to each state: a 0 to unsynchronized10 It is a trivial matter to do away with this restriction, since we can always add a completely unspecifiedautosegment to the start and end of each tier, thereby permitting slippage between the substantivematerial on each tier.68Steven Bird and T. Mark Ellison One-Level Phonologystates and a 1 to synchronized states, and then erase the lines.
(>(A~>{A, 1} , (A~/BObserve that each of these state labels is actually a pair.
These automata re definedover the product alphabet G x {0, 1}.
The intersection of these automata is as follows:~ANC'0) 7 \[ANC' 1} 4.. ~" (ANC'0} 7 (BN~' 4,.The function of the indices was to rule out certain states in the intersection.
Now thatthe indices have served their purpose, we can erase them and further simplify theautomaton:This, then, is the semantics assigned to (13).
Since we no longer require the graphicalnotation for synchronization, it will be convenient to represent SFAs using the notationof regular expressions over ordered pairs.
In order to do this it is useful to employsome macros.
An autosegment A is represented as s(A) =def /A~ "/q-.
Bullet (e) is usedhere as a context-dependent wildcard, indicating an alphabet.
(In this definition ofs(A), the bullet indicates the alphabet {0,1}; in the definition of a(n) below it indicatesthe alphabet G.) The n association lines incident to an autosegment are expressed asfollows:a(n) =def /'~ 0/* (/'~ 11 / ' ,  0/*) nObserve that a(m + n) = a(m) +a(n).
Finally, we combine these two macros into a thirdmacro thus:A:n =def s(a) ha(n)This states that A is an autosegment with n associations.
Now we shall illustrate theworkings of these definitions.
Consider diagram (13) again, reproduced below:69Computational Linguistics Volume 20, Number 113.
A B\ /CA singly associated autosegment, such as A, is written down as the following formula:A: I  = s(A) Ma(1)-- (A,.)
+ M (.
,0)*(- ,1)(- ,0)*= (A, 0)* (A, 1) (A,0)*We can now simply write down a formula for (13): 11 (A : I+ B:I) M C:2.
This expressionevaluates to the following:(ANC,0/* (AnC,1)  (A nC,0)* (B n C,0)* (B n C,1) (B n C,0)*Now that the indices have served their purpose, we would like to ignore them by pro-jecting the ordered pairs onto their first element.
The result of evaluating this projectionfor our current example is (An C)+(B n C) +, which is the intended interpretation ofdiagram (13).
We shall adopt the following notational convention: if D is the encodingof a diagram then \[D\] is the projection of the encoding that ignores the indices.As another example, consider an autosegmental diagram consisting of two tiers,each with two autosegments, and two association lines between the tiers, as shownin (16).16.
A BC DThe expression for (16) is (A : 1 B : 1) M (C : 1 D : 1).
This evaluates to the follow-ing expression under the projection: (A N C)+((A N D) + t3 (B n C) + u ~)(B N D) +.
Thecorresponding automaton for (16) is given in (17).17.Cc\, /11 It is important to notice that the numerals inthis expression are not the same as the indices that occuras the second member of pairs like (A, 1).
The former epresent degree of association, while the latterfunction as synchronization marks in an automaton.70Steven Bird and T. Mark Ellison One-Level PhonologyAutomaton (17) will accept he following sequences, among others:A B IAIBrBI IArArB r C D C C D C D DNote that all of these examples have A overlapping C and B overlapping D. Thesecond and third examples have an extra cell, for B overlapping C and A overlappingD, respectively.
This range of possibilities i compatible with (16), as it requires an A-Coverlap and a B-D overlap and optionally permits an A-D overlap or a B-C overlap(but not both).We have now seen an automaton-based interpretation f an autosegmental chart.Next we consider how the above regular expressions defined over ordered pairs canbe generalized to representations consisting of more than one chart.4.4 Multiple ChartsIt is straightforward to generalize the interpretation procedure for single charts to onefor representations with an arbitrary number of charts.
Recall that for one chart weneeded to employ ordered pairs.
In general, for n charts we must employ orderedn + 1-tuples.
The construction will be demonstrated using a diagram attributable toPulleyblank (1986, p. 13) involving three tiers and three charts.
1218.
A B C2 E FNow, since there are three charts in (18) we must employ 4-tuples.
We adopt thefollowing abbreviatory conventions:s(a) 7--de f (/:/, , ,  , ,  , )+a12(n) =def ( ' ,0 , ' , ' ) *  ((,, 1 , , , , ) ( , ,0 , , , , ) * )  na23(n) =def ( ' , ' ,0 , ' ) *  (( , , , ,  1,,)<',',O,o)*) na13(H) =def (-,-, .,0)* ((', ", ", 15 ( ' , - , .
,  0)*)"A:p:q:r =def s(A) nat2(p)na23(q)na13(r)We can now write down the expression for (18) as follows:19.
(A:I:0:0 B:0:0:I C:1:0:0)~(E:2:0:0 F:0:1:0)~D:0:1:1The first three terms of this expression correspond to tier 1 of (18).
The first term ofthe expression concerns the autosegment A and its association line on chart 1-2.
Thesecond term concerns B and its line on chart 1-3.
Notice that lines AE and BD are in12 Pulleyblank observes that the temporal interpretation f this diagram is ill-defined if association isassumed to be transitive, However, since overlap is not a transitive relation we do not have thisproblem.71Computational Linguistics Volume 20, Number 1different charts in (18) and so the count of association lines is in a different position inthe 4-tuple for A and B.
The third term concerns C and its line in chart 1-2.
The fourthand fifth terms of the expression concern the E-F tier.
Since E is doubly associated inchart 1-2, a 2 is used for E's degree of association.
D also has two associations, but theyare in different charts.
Expanding and projecting this expression gives the following:(AnDnE)+(BnDnE)+(CnDnE)+(CnDnF) +So the shortest string that satisfies the constraints expressed in diagram (18) is thefollowing:A B C CD D D DE E E FRecall that the encoding of the three-tier diagram (18) was given as expression(19).
In such expressions it is difficult to identify tiers.
Therefore, in the remainder ofthis paper we shall write expressions like (19) in the following format:20. tier1 A:I :0:0 B:0:0:I C:1:0:0tier2 E:2:0:0 F:0:I :0tier 3 D:0:1:1In general, if D is a diagram, then C(D) is its encoding in the format exemplifiedin (20).4.5 Evaluating the EncodingNow, we evaluate our encoding with respect o Kornai's desiderata: computability,compositionality, invertibility, and iconicity (Kornai 1991).Computability: The number of terms in the encoding is equal to the number ofautosegments, and each term has a fixed size} 3 Therefore, the encodingcan be computed in linear time.Compositionality: If D1 and D 2 are  two autosegmental diagrams then E(D1D2) =g(D1)E(D2), where concatenation f encodings i done in a tier-wise man-nen Thus the encoding is compositional.Invertibility: A representation can be reconstructed from its encoding.Iconicity: If an autosegment i  a diagram is changed, the effect on the encodingis local, since only one term is altered.
However, if an association lineis added or removed, two terms must be altered.
Although these termsmay not be adjacent, we believe the encoding is more iconic than Kornai'striple code (see Section 5.2), in which changes can affect an unboundedamount of material.In addition to these properties, our encoding can be used directly as a finite-staterecognizer of surface forms, simply by forming the intersection of the n tier encodings13 Observe that when we expand these macros the resulting expression has s + 2a terms, where s is thenumber  of autosegments and a is the number  of associations.72Steven Bird and T. Mark Ellison One-Level Phonologyand projecting the first elements of the tuples.
Note that if we revert o the encoding in(19), where the tier encodings are combined into a linear expression using intersection,then compositionality is lost.
Thus, the encoding is either linear or compositional, butnot both.
Unfortunately, this is the best that we can hope for; Wiebe (1992) has shownthat a compositional linear encoding does not exist.4.6 Phonological RulesPhonologists typically encode their descriptive generalizations in terms of RULES.Often these rules are interpreted as processes that manipulate representations.
In theone-level approach they are interpreted as a logical implication between two descrip-tions, which simplifies to a single description given the Boolean operations presentedin Section 3.3.As our first example, consider the phenomenon of homorganic nasal assimilation,whereby nasals agree in place of articulation with the following consonant.
An SPE-style rule for this is given in (21a), the corresponding logical implication in (21b).21.
(a)(b)\[+nasal\] --* \[c~place\] / __ \[+cons,~place\]\[+nasal\]\[+cons\] --+ \[c~place\]\[~place\]Thus, the sequences mb and nd are allowed, while md and nb are ruled out.
LetN -- {m,n}, S = {b,d}, L = {m,b}, and A = {n,d}.
The required constraint can beexpressed as NS --* LL U AA.
However, in order to make this rule apply to a wholestring (rather than just the first NS sequence it comes across), we must express it inthe following format.
1422.
-~(o*(NS F1LL U AA)o*)This states that it is not possible to find anywhere a nasal-stop cluster (NS) that isnot made up of two labials (LL) or two alveolars (AA).
We can simplify the aboveexpression to .*(mA).
* f\] .
*(nL),*.Now consider a general rule of the form SD ~ SC.
Since SD and SC pertain toparts of a string rather than a whole string, we have to ensure that the rule applies toall substrings of an 'input' string S. We do this as follows:23.
Vs c s, SD(s) ~ SC(s)Vs C_ S, ~SD(s) V SC(s)~s c s, SD(s) A -~SC(s)-4,*(SD n SC),*)This is how we arrived at (22) from (21b).Autosegmental rules can also be expressed in this framework.
Consider again thecase of assimilation.
The following diagram is the autosegmental rule correspondingto the SPE rule in (21).
Here, ~place is a hybrid autosegment (see Section 4.1) rangingover places of articulation.14 Note that we employ a '9" sign or an overline to represent complement, depending upon which ismost convenient.73Computational Linguistics Volume 20, Number 124.
N C\\c~placeThis rule states that wherever an NC sequence can be found, if the C is associatedwith an L, then the N is also associated with L. We can express this rule in the morefamiliar ewrite notation:N C --* N C\~place c~placeWe can give an automaton-based semantics to this rule.
In order to do this, we mustemploy two independent charts between the two tiers, one for the structural descriptionand one for the structural change.
We can represent this as follows, where 'sd' refers tolines in the structural description chart, and 'sc' refers to lines in the structural changechart.N Csd~placeNow we can write down the formulas for the structural description and the structuralchange independently and combine them into the rule format of (23).25 N0 c .0*  .00" N01 c10.00"1 / .
.e:0* ~place:l e:0* U-~ e:0:0* c~place:l:l e:0:0* sc sdIn (25) the 'sd' and 'sc' subscripts on the brackets refer to projection functions thatignore the structural description and structural change charts, respectively.
So, in eval-uating (25) we intersect the two tiers of the structural change part of the rule, and thendelete the second index of each tuple.
The complement of this automaton is then inter-sected with the structural description part of the rule and the first index of each tupleis then deleted.
The final step is to add the e* wildcards and form the complementagain.
The result is an automaton that rejects any nonhomorganic NC clusters.A more complex example of a phonological rule, this time concerning vowel har-mony, will now be discussed.
Since the advent of autosegmental phonology, vowelharmony has been analyzed as the spreading of autosegments from left to right.
Herewe show how such a rule can be translated into a regular constraint on surface forms.Turkish exhibits two orthogonal types of vowel harmony: one requiring that con-secutive vowels agree in fronting, the other requiring that consecutive vowels agreein rounding unless the second vowel is low.
As the rounding harmony is the morecomplex of the two, we will take it as our example.
To avoid the complications offronting harmony, we will only consider examples involving back vowels.74Steven Bird and T. Mark Ellison One-Level  PhonologyTurkish has eight vowels.
Four of them (a e i i) are unrounded, and four (o 6 uii) are rounded.
Turkish is an agglutinating language, and the vowels in many affixesdepend on the final vowel in the root of the word.
Compare, for instance, three of thecases of the words son end and adam man displayed in (26).26. casenominativeaccusative 15dativeson adamson adamsonu adam1sona adamaWhen the rounded root vowel is followed by a high vowel in a suffix, this vowel mustagree with the root in rounding.
Low vowels in harmonising suffixes are, however,always unrounded.The notation of autosegmental phonology makes it easy to state this generalizationas a rule.
The rule spreads a rounding autosegment onto the next vowel if (and onlyif) the next vowel is high.27.
+rnd\\V V\ [+ hi\]This rule only applies on the vowel tier, skipping over consonants, and the in-terpretation given to autosegments on this tier must reflect this.
We use the idea ofheterogeneous autosegments ( ee Section 4.1) to differentiate ordinary segments fromautosegments on restricted tiers such as the vowel tier.
A vowel on this tier may denotenot only a vowel but also consonants interspersed within this vowel as well.
Whereasa segmental a corresponds to an automaton with a single state having a singleton label(this is what we have called a SIMPLE segment; see Section 4.1), an a on the voweltier corresponds to a single state automaton whose state accepts not only a but anyconsonant as well (a HETEROGENEOUS segment).Given that the vowel tier uses this kind of heterogeneous representation, we cancombine the charts of our rule into the regular expression in (28).28  rndl "?
* "??
*  rndll "??
* J J /.
*e:0* V:0 +h i : l  e:0* e:0:0* V:0:I +hi: l :0 e:0:0* sc sdUsing the procedure of Section 4.6, we can translate this into a regular expression that15 More precisely, the accusative case is used only for definite objects.75Computational Linguistics Volume 20, Number 1implements rounding harmony as a constraint on surface forms.
After simplification,this automaton is:@Each of the states accepts the specified class of vowels and any consonant.
Informally,this automaton will accept any sequence of vowels except hose in which a roundvowel is followed by an unrounded high vowel.
This is precisely the intended effectof the autosegmental version of the harmony rule (27).
This concludes our discussionof vowel harmony.A comment is in order here about why two charts were required for the encodingof autosegmental ru es.
After all, our use of an input and an output chart appears to bea procedural device, and we have eschewed these from the outset.
However, note thatit is not possible for our structural description and structural change to be encodedon the same chart: the structural change has an association line not present in thestructural description, and so they are incompatible.
Of course, the overlaps describedby the structural description and structural change are mutually compatible; it is onlythe associations that are not.
Once the structural change has been computed, we canthrow away the 'sc' chart, and once the structural description and structural changehave been combined, we can also throw away the 'sd' chart.
In this way, the rulefunctions only as a filter on surface forms, and there is no way for two separate rulesto communicate via these rule-internal charts.This approach permits us to interpret any nondestructive autosegmental rule.
16Those rules involving deletion of autosegments or association lines must be approachedin a completely different way.
Rather than deleting an element in a particular context,we set up an alternation with zero, following Bloomfield (1926).
See Bird and Klein(in press) and Bird (in press) for detailed examples of this approach to deletion.This concludes our discussion of the automaton-based semantics for autosegmen-tal representations and rules.
In the next section we review some other attempts totreat autosegmental phonology using finite-state techniques.5.
Other Finite-State Approaches to Nonlinear Phonology5.1 Kay (1987)The earliest treatment ofautosegmental phonology in a finite-state setting is attributableto Kay (1987).
It is tailored to the framework of nonconcatenative morphology de-veloped by McCarthy (1981) in which consonants and vowels are segregated ontodifferent iers, as shown for the Arabic verb stem kattab in (29).16 Applied to a destructive autosegmental rule, the interpretation determines the restriction imposed onsurface forms by the rule, were it the last rule in a derivation.76Steven Bird and T. Mark Ellison One-Level Phonology29.
a perfective active/ \C V C C V C causativeI/k t b writeThis verb stem contains three morphemes.
Together, they mean 'caused to write.'
Thechallenge posed by Arabic morphology is to come up with a simple account of theinterleaving of the morphemes, relating the forms a, CVCCVC, and ktb to the stemkattab.Kay's solution, which we sketch here, involves the use of a kind of transducerthat reads four-tuples (rather than pairs like a normal FST).
This transducer scansfour strings, one for each of the three tiers in (29) and one for the correspondingsurface form.
In this way, Kay has identified tiers with tapes.
The association relationis encoded in the way the transducer scans these four tapes.Kay specifies a transducer by providing a set of FRAMES.
In effect, each framespecifies an association between a CV-tier slot and a melodic unit, such as a k or ana.
This melodic unit appears on the current surface tape cell.
Each frame is a four-tuple whose components correspond to (i) the consonantal root, (ii) the CV-tier, (iii)the vocalic melody, and (iv) the surface tape.
A simple frame is the following:k : C :c:  kThis frame specifies that when a k is being read from the consonantal tape and a C isbeing read from the CV tape, then there is an empty transition on the vocalism tapeand the surface tape must have a k. There is a similar frame for each consonant.
Aframe for the vowel a is c:V:a:a.More work is necessary in order to capture the idea of autosegmental spreading.Kay modifies the transducer model so that tape symbols can be inspected withoutthe reading head being advanced.
Three notational devices manipulate the way thisrevised model behaves.
Square brackets around one of the components of a framecauses the corresponding tape cell to be scanned without advancing the read head.Braces around a frame component behave in the same way, but only if they are scan-ning the final symbol on a tape.
(This is required to prevent he spreading of i, whichwill not be discussed here.)
Finally, the symbol G is used instead of C for geminateconsonants.
We shall see how these devices operate using a worked example, in whichthe surface form kattab is derived from the three lexical forms of diagram (29).Display (30) gives the initial configuration.
The box shows the collection of symbolscurrently being scanned on the four tapes.
To the far right is the appropriate frame.An empty pair of brackets is equivalent to ~.30.t b kV G C V C C\[\]k77Computational Linguistics Volume 20, Number 1The first CV tape symbol is a C and so the current consonant k is written onto thesurface tape and the read heads are advanced.
Notice that the read head for the voweltape is left on the first cell.
This is because the frame specified that there be no transitionon this tape.31.k b IIC Va~  G C V C V{a}k aIn (31), the CV tape symbol is a V, and so the a is copied to the surface tape.
Sincethis a is given in braces in the frame, there is no movement of the read head.32./"-"xk I t I b It\]C V a~ t C V C G\[\]k a tHaving reached configuration (32), we read a G, which indicates a geminate.
The tis written to the surface tape and the read head for the consonant tape stays whereit is.
The brackets around t in the frame mean that there is a nondeterministic choicebetween moving the read head and leaving it in the same position.
However, the Gsymbol requires that the head stays put.
Next we get a C on the CV tape.33./ '-- 'xk / t / b It\]C V G a~ t V C C\[\]k a t tAfter (33), the consonant tape read head advances 17and the CV tape head moves onto deal with another vowel (34).17 Note that there is nondeterminism hidden here again.
As it happens, if the consonant tape does notadvance then the b will never be used (i.e., we will get *kattat) and the transducer will fail, because ofthe requirement that all read heads be at the end of their input for a successful completion.78Steven Bird and T. Mark Ellison One-Level Phonology34.k t \[JC V G C ~a  C V{a}k a t t aFinally, the consonant b is transferred to the surface tape.35.k t /b \ ]  \[b\]C V G C V ~ C\[\]k a t t a bThe result on the surface tape is kattab as required.
Of course, the transducer alsoworks for recognition.
We could specify a surface tape and leave one or more of thelexical tapes unspecified.Kay's system is ingenious and we have not been able to demonstrate all of itscapabilities in this small space.
Nevertheless, we believe it suffers from a number ofproblems.
The first concerns the form katab (form I) and the (corresponding) reflexiveform ktatab (form VIII) with an infixed t. Indeed, more than half of the forms thatKay cites have infixes, although these are not explicitly analyzed in the model.
Twoconservative extensions to the model that encompass this infixation are (i) to insertinfixes into the CV tape directly (so form VIII is CtVCVC) or (ii) to introduce anotherCV tape symbol A (for affix), which directs the transducer to read from a fifth tape.A second set of problems concerns the appropriateness of Kay's model for non-linear phonology more generally.
First, notational devices like G move the frame 'lan-guage' away from what it is supposed to represent, namely autosegmental structures.No longer is gemination represented by association (perhaps derived by a spread-ing rule), but by a special marking on the skeleton.
Second, the model builds in theassumption that each morpheme appears on a separate autosegmental ier.
However,most applications of autosegmental phonology employ morphemes with phonologicalinformation arrayed on more than one tier (e.g., Clements and Ford 1979).
Similarly,the modeling of subsegmental feature geometry of the kind advocated by Clements(1985) and others also involves a single morpheme having material on several tiers.Third, the model breaks down in the area of MORPHEMIC SEGREGATION.
Since thereis no principled upper bound on the number of morphemes that may be overlaidin the way McCarthy advocates for Arabic, there is similarly no principled upperbound on the number of tapes Kay's transducer would require.
The assumption thateach morpheme defines its own set of tiers, implicit in early work (McCarthy 1981)but explicit in more recent work (McCarthy 1989), is incompatible with a fixed upperbound on the number of tapes.
Finally, using Kay's model for recognition would leadto much nondeterminism in positing G symbols, brackets, and braces.
For example,in processing kattab the lexical tapes kttb, CVCCVC, and aa could be generated.
Themodel generates all possible violations of the Obligatory Contour Principle.79Computational Linguistics Volume 20, Number 15.2 Kornai (1991)Kornai (1991) has developed a linear encoding of autosegmental representations thatallows the two-level transducer model to be applied to autosegmental phonology.We shall present Kornai's central innovation and describe a few of its strengths andweaknesses.As we saw in Section 4.5, Kornai presents four criteria under which an autoseg-mental encoding should be judged.
An encoding should be easily computable; ideallyby finite automata.
It should also be invertible.
An encoding should be iconic; minimallychanging the input should minimally change the output.
Finally, it should be compo-sitional, in the sense that the concatenation f the encodings of A1 and A2 ought to bethe same as the encoding of the concatenation f A1 and A2.
Kornai demonstrates thatan optimal encoding under these criteria does not exist, and he sets about defining anencoding that is claimed to be near-optimal.We shall only cover one of the codes he considers, namely, the TRIPLE CODE.This code represents wo tiers of autosegments and a chart between them as a lineardescription.
There are four keywords in the code that are interpreted as instructions toa device that is scanning two tiers (left to right) and drawing association lines betweencertain pairs of segments.
These keywords are as follows:0 : leave the current segments unassociated and advance the read head on eachtier;1 : draw an association line between the current segments on each tier and ad-vance the read heads on each tier;t : override the advance instruction on the bottom tier, i.e., only advance the readhead on the top tier; andb : only advance the read head on the bottom tier; retain the same segment onthe top tier.Each 0 or 1 is flanked by a statement of the current segments on the two tiers.
Anumber flanked by two segments forms the TRIPLE that gives the code its name.Where this code could give a number of different representations of the sameautosegmental structure, Kornai (1991) restricts the encoding to operating in the samemanner as the association convention of Goldsmith (1976).
Association, or the lackof association, is marked left-to-right in a one-to-one fashion until one tier is devoidof new autosegments.
From that point, only one tier advances until all remainingautosegments are represented in the linearization.As examples, let us encode two charts, the first completely devoid of associations.To show the pairs of current autosegments through the steps of the encoding, we linkthem with dotted lines indexed by the count of the step in the derivation.AIaB C2 3b cDi -.'-.
6 4 i5  ?d eThe first four code steps give the following encoding (separating triples with fullstops):AOa.BOb.
COc.DOd80Steven Bird and T. Mark Ellison One-Level PhonologyFor the remaining two steps, we must spread (in virtual associations, not real ones) thefinal segment of the top tier, remembering to record the fact that only the read headon the bottom tier advances.
The total encoding of this chart is thus the following:AOa.BOb.COc.DOd.b.DOe.b.DOfAs a second example, let us fill the same chart with some associations.A B C D/ /  J\a b c d e fThe encoding for this chart is the following:Ala.t.Bla.Clb.b.Clc.Dld.b.Dle.b.DOfThis code can be measured against he four criteria given above.
The code is definitelycomputable; we have just given an algorithm for constructing the code for an arbitrarychart.
Likewise' the code is invertible: given any code, it is clearly possible to reproducethe original chart.
Similarly, given the chart decoded from any encoding, it is likewisepossible to reproduce the encoding.The triple code is, however, neither iconic nor compositional.
Consider the twoautosegmental representations given below.A B C A B C\a b c a b cThe triple code for the first representation is A0a.
BOb.
C0c.
For the second representa-tion, the triple code is the sequence:AOa.b.AOb.b.A1c.t.BOc.t.COcAs we can see, a minor change in one part of the autosegmental representation hasresulted in major changes in much of the triple-coded representation.
Thus the codeis not iconic.Similarly, if we concatenate he two representations shown belowA B C Dwe geta b c dA B C Da b c d81Computational Linguistics Volume 20, Number 1If we concatenate the two encodings, we get AOa.b.AOb.b.AOc.BOd.t.COd.t.DOd, which isnot the same as the encoding of the concatenation: Ala.Blb.Clc.Dld.
So the triple codesatisfies only two of the desiderata Kornai gives for linearizations of autosegmentalrepresentations.
A  mentioned above, Wiebe (1992) has proven that linear encodingscannot get much better than this: no linear encoding can be both invertible and com-positional.Under the triple code, a lot of phonological processes are not finite-state.
Forexample, consider the process of putting morphemic tiers together.
In the case of theArabic example we saw in (29), we need to combine the root morpheme (with emptytemplate and vocalic tier) ktb and the template (with empty vocalic and consonantaltiers) CVCCVC together in such a way that we end up with the encoding in (38).In autosegmental representations, this is achieved by concatenation, and the "before"and "after" diagrams appear in (36) and (37).36.
C V C C V C37.k t bC V C C V Ck t bConcatenation f codes does not bring the same result.
Display (38) shows the twoconcatenands, written with 0 as place holder for the empty tier.
TM38.
C00.V00.C00.C00.V00.C00 + 00k.00t.00bC0k.V0t.C0b.t.C0b.t.V0b.t.C0bTo obtain the correct result, we need a more complex operation than concatenation--one that cannot be performed by any finite-state device.If the ktb is read before the CVCCVC is read, it is necessary to have sufficient stateinformation to be able to store k, t, and b, which in an alphabet of about 28 consonantsinvolves at least log2(283) ~ 15 extra bits of information in each state, compared toan otherwise quivalent transducer.
Suppose now that we wish to apply the well-formedness condition (5), the association convention (6), and some other rules to (38)to achieve the pattern of association between the two tiers given in (29), repeatedbelow.
We are not interested here in the detail of how a transducer might perform thisoperation, only whether a transducer can perform it.39.
C V C C V C/ Ik t b18 As far as we have been able to find, Kornai (1991) does not discuss how to code a representation thatcontains an empty tier.
Here, we have filled empty  tiers with a "place holder'  segment, and then usedthe triple code.82Steven Bird and T. Mark Ellison One-Level PhonologyThe encoding of (39) is Clk.V0t.t.Clt.t.Clt.V0b.t.Clb.
Observe that the t is met in thesecond triple of the right-hand side of (38) and it must be 'stored' until the fourthtriple in the output.
In a form like tada.hraj, two consonants must be stored in thisway, leading again to an explosion of state information.
19If the template consistedof n CV syllables, and the consonant tier consisted of n consonants, then the asso-ciation mechanism would need to store at least L(n - 1)/2J autosegments.
This is,in principle, unbounded, and so the association mechanism for coded autosegmentalrepresentations is not finite state.In general there is no limit on the amount of information that needs to be pre-served as state information in the transducer.
Consequently, using the triple code,certain phonological processes cannot be modeled using a finite state transducer.
In-deed, this is true of any linear code (Wiebe 1992).
It is true that adding each individualassociation--that is, finding the next position to associate, making the association andshuffling the lower tier along--can be performed by a finite-state transducer.
But noprincipled bound on the length of the derivation can be made, and the quantity ofmemory required is an increasing function of the length of the derivation.
Conse-quently, the step from unassociated to associated form cannot be made by a singleFST.This type of problem with manipulating coded representations is not limited toArabic morphology.
In fact, the problem arises whenever there is an autosegment thathas a restricted set of 'landing sites' (such as the consonants or the high vowels), a verycommon occurrence.
Another example is that of accent systems: Kornai and K~lman(1988, p. 185), citing Goldsmith (1982), state the Basic Tone Melody Association rulefor Ci-Ruri, a Bantu language of Tanzania, as follows:40.
Basic Tone Melody Association Rule:Associate the accented element of the Basic Tone Melody to the accentedelement of a word.In general, the accented element of a word may be an unbounded istance from thestart and end of a word.
Goldsmith (1982, p. 53) gives the following example:41. na a kamir* e I milkedL H* LAs before, performing the association using the triple code requires a multiplicationin state information, which in the general case cannot be bounded.
True, this exampleonly requires the automaton to store H; but Goldsmith (1982, p. 55) gives exampleswhere there are two stresses in the word, and, therefore, the association automatonneeds to store more tones (see Wiebe 1992, pp.
109-110 for the details).Next, consider the problem" of generalizing from one chart to an arbitrary number19 Wiebe (1992, p. 112) points out that he final consonant does not need to be stored, as it is alreadyassociated with the final C position.83Computational Linguistics Volume 20, Number 1of charts.
Kornai (1991, p. 70) gives the following example:d e fI/g h iJ k 1Each chart is encoded as a string and then these two strings are linked by associationlines.
Each line indicates that the segments it connects are actually the same segment(in the original diagram).
Here is the first step of the encoding of the above diagram: 2?dlgtelgfOhbfli\ I  I I \g13 thlkiOkb?
i1Next, this encoding can itself be encoded into a string 77 characters long.
Kornaihimself admits (p. 72) that his encoding is impractical for autosegmental  structureshaving more than two charts, n Kornai discusses another encoding, which involvestransducers that can read three (or more) tapes simultaneously.
However,  this comes infor the same criticism leveled at Kay's system above, namely that there is no principledupper  bound on the number  of tiers due to such considerations as complex featuregeometry and morphemic  segregation.5.3 Wiebe {1992)Wiebe (1992) has recently devised an encoding of autosegmental  diagrams that over-comes many of the problems of Kornai's triple code.
It is called the MULTI-LINEARCODE.
Consider again d iagram (29), reproduced below.29.
a perfective active/ \C V C C V C causativer~k t b writeThis d iagram contains two charts.
The upper  chart, which connects the vowel  to twoV slots will be referred to as chart 1, and the other chart as chart 2.
Since there is a totalordering on the associations in a given chart (Goldsmith 1976, p. 29), it is sufficient orecord how many associations each autosegment has on each chart, without specifyingwhere these associations lead.
The multi-l inear code for (29) is displayed in (42).20 Here the dots separating the triples have been omitted to enable more convenient location ofassociation lines.21 The encoding procedure would not terminate for Pulleyblank's example (18).
This is because the firstapplication of the encoding would take the three-tiered structure and produce another (morecomplicated) three-tiered structure.
In fact, this is just a special case of a more general problem, for theencoding fails for any tier structure containing acycle, such as the one proposed by Clements (1991).84Steven Bird and T. Mark Ellison One-Level Phonology42.
a l lC2VlC2C2VlC2k2t22b2Here, a numeral n following an autosegment A indicates that A has an associationon chart n. These numerals can be stacked up; the first line specifies that a has twoassociations on chart 1.
A given tier can participate in two charts; the second line of(42) has Cs associated on chart 2 and Vs associated on chart 1.
Wiebe shows howthe multi-linear code satisfies the criteria for computability, invertibility, iconicity, andcompositionality.Wiebe's encoding has some similarities to our revised encoding presented in Sec-tion 4.5.
Here is our encoding of (29):43. tier 1 a:2:0:0t ier2 C:0:1:0 V: I :0 :0  C:0:1:0 C:0:1:0 V : I :0 :0  C:0:1:0t ier3 k :0 : l :0  t :0:2:0 b :0 : l :0Suppose that a:p:q:r is an arbitrary 4-tuple of the kind in (43).
We use position in thetuple to specify which chart an association line is in, and use numerals to specify thenumber of association lines.
However, Wiebe uses numerals to specify the chart andrepetitions to specify the number of lines.
There is a mapping between terms like al lin Wiebe's encoding and a:2:0:0 in our encoding:f (a:p:q:r)=alP2q3 rIf we apply f to each term in (43), the result is as follows.a l lC2VIC2C2VIC2k2t22b2Now it should be clear that there is an isomorphism between the two encodings.Wiebe goes on to show how his encodings can be processed by new devicescalled MULTI-TAPE SFAs and SFTs, 22 where each tape corresponds to a row in themulti-linear code.
The devices are used for checking well-formedness constraints andapplying (possibly destructive) autosegmental rules.
Wiebe also demonstrates thatthese devices are more powerful than FSTs without epsilon transitions, claiming thatthey can recognize some (strictly) context-sensitive languages.
He argues that this extracomputational power is crucially required for processing autosegmental analyses withfeature- or structure-modifying rules.The read heads can scan n-tuples separated by arbitrary distances,and each head reads one co-ordinate of the n-tuple under it .
.
.
.
Itis precisely this ability to scan different parts of an input word atthe same time that is so important in modell ing autosegmental rules.Association lines can associate segments in any part of one tier tosegments in any part of the facing tier.
In order for any computational22 Wiebe borrows the terms SFA and SFT from an earlier version of this paper (Bird and Ellison 1992).85Computational Linguistics Volume 20, Number 1device to efficiently process autosegmental representations, it must beable to scan two associated segments from widely separated parts of therepresentation at the same time (Wiebe 1992, pp.
95-96, emphasis added).While this is a reasonable statement regarding any model like Kornai's, it doesnot apply to our one-level model since the notion 'widely separated' is meaninglessin this context.
Two terms in a multi-linear code are widely separated if they differsignificantly as to their distance from the left- or right-hand end of the encoding.
Thus,the longer and more slanted the line that associates the autosegments, the greater theirseparation.
Although this is true of the ink on a page, our semantics pays no attentionto the angle and length of association lines.
If a pair of autosegments is associatedthen they are ipso facto proximate.
The temporal extent of autosegments expands orshrinks to accommodate these temporal constraints imposed by association lines.
Theintensional character of our approach gives rise to a flexibility of interpretation thatobviates the need for more powerful devices of the kind advocated by Wiebe.Now that we have reviewed some details of other finite-state approaches to au-tosegmental phonology, we present a one-level analysis of an aspect of Arabic verbmorphology.5.4 A One-Level AnalysisAs an alternative to either the elaboration of the transducer or the reduction of thenonlinear representation to a code, we present a partial analysis of the Arabic verb interms of SEAs.
23 Rather than trying to generate one representation from another, weconstruct a description of the individual Arabic word by taking the intersection of theSFAs describing each of the morphemes.
The forms we analyze here, like those givenin McCarthy (1981), do not include inflections and also assume that the followingsuffix is consonant-initial} 4 We shall derive the stem kattab from three morphemes,specifying the form, the root, and the voice/mood.The form I SFA (44) generalizes over all verbs: it accepts/generates all correctverbs of this form, as well as a number of nonsense verbs, and factors out informationspecific to the individual words.44.
C:1 V:0 C:1 V:0 C:1The indices mark associations that will link this tier to the root.Another tier is the consonantal tier.
It contains heterogeneous autosegments cor-responding to consonants.
These symbols are heterogeneous because they allow notonly the corresponding consonant, but also any of the vowels.
So the f autosegment onthe consonant-tier denotes any sequence of the segments f, a, i, or u.
The two uses of fare disambiguated byreference to the tiers on which they occur.
Under this definition,autosegments on the consonant tier can spread over vowels to the next consonantalposition.The root SFA generalizes over all stems constructed from the same root.
The en-coding on the consonant tier describing the root fql, to do, appears in (45).23 The reader interested in other finite-state models of Arabic phonology is directed to the work ofNarayanan and Hashem (1993), Beesley, Buckwater, and Newton (1989), and Beesley (1990).
These havenot been discussed at length here, because they do not seek to implement autosegmental phonology.24 Biliteral roots in forms I, III, IV, V, VI, VII, VIII, X, XI, XII, and XIV, triliteral roots in forms IX and XI,and quadriliteral roots in form QIV reorder the consonant-vowel sequence in the final syllable of theroot if followed by a vowel-initial inflection.
Where possible, the Arabic verb stem will metathesize ordelete short vowels to create a geminate with root consonants (McCarthy 1981, pp.
197f).86Steven Bird and T. Mark Ellison One-Level Phonology45.
.
:0" f:l (.
:0" ~:1) + 1:1The wildcards are necessary for affix consonants, uch as the n- prefix of form VII orthe -t- infix of form VIII.It might be argued that fixing the associations (by the indices) in the specificationof the morpheme is redundant--that the associations should be supplied by rule.However, 3 of the 15 triliteral forms of the Arabic verb, namely forms II, V and XII,violate the association convention, multiply associating central autosegments ratherthan peripheral ones.
For these forms there seems little choice but to lexically specifythe associations orelse introduce an ad hoc, morphologically triggered, rule (McCarthy1981, p. 392, rule 24).
In our notation, we can specify these forms as:46.
II C:1 V:0 C:1 C:1 V:0 C:1V t:l V:0 C:1 V:0 C:1 C:1 V:0 C:1XII C:1 C:1 V:0 w:0 C:1 V:0 C:1The intersections of the three forms in (46) with the root automaton (45) are:47.
II f:l V:0 ~:1 ~:1 V:0 1:1V t:l V:0 f:l V:0 ~:1 ~:1 V:0 1:1XII f:l ~:1 V:0 w:0 ~:1 V:0 1:1The vocalism for the active perfect verb (except form I) is very simple to express: itis a sequence of as interleaved with consonants.
On a vowel plane, where each au-tosegment is heterogeneous, accepting a vowel and any of the consonants, this aspectmay be expressed by the single autosegment a, which corresponds to the automatonin (48).48.Forming the intersection of this SFA with the intersections of roots and formsshown in (47) gives the following forms.49.
II f:l a:0 q:l ~:1 a:0 1:1V t:l a:0 f:l a:0 ~:1 ~:1 a:0 1:1XII f:l ~:1 a:0 w:0 q:l a:0 1:1This brief analysis hows the ease with which a computable analysis of nonconcate-native morphology can be constructed in the SFA formalism.
While it may not coverall possible generalizations about Arabic verbal structure, the important point here isthat it captures generalizations that are always surface-true of the phonology of theseparate morphemes that come together to build up the verb.5.5 Automata versus LinearizationsWe have seen that Kornai (1991) finds it necessary to choose between the impositionof restrictions on autosegmental phonology and the loss of finite stateness in the trans-duction relationship.
As it turns out, the one-level approach does not suffer from thisproblem.
In this section we explain why.87Computational Linguistics Volume 20, Number 1Note that the natural processes by which finite-state automata re combined, andtherefore by which regular languages are manipulated, are not themselves regular.
Tosee why this is so, suppose we have two regular expressions describing the first formand the root of the Arabic verb to write:50.
CVCVCk (e* t) + e* bThe intersection is the following regular expression:51. kVtVbThe associations fixing the incidence of k with the first consonant slot, t with the third,and b with the final, are made by the intersection operation.
The question arises as tohow we can construct the associations if the same operation for Kornai's system is notregular.
The operation we have applied here--intersection---cannot be performed by aregular transducer.
This does not invalidate our claim to regularity.
What is regular inour theory is each individual description and generalization about phonological data.That is, the descriptions we use are all regular descriptions of phonological objects.What is not regular in one-level phonology is the relationship between differentformats of the same description.
There is no finite-state transducer that will form theproduct of two regular expressions.
Multilevel analyses necessarily seek to capturerelationships between different descriptions, and like the product operation, theserelationships often cannot be captured by finite-state transducers.6.
ConclusionsThe starting point of this paper was the distinction between descriptions and objects.Multidimensional phonological structures were taken to be descriptions of classesof phonetic objects, following Wheeler (1981), Bird and Klein (1990), Pierrehumbert(1990), Bird (1990), and Coleman (1992).
Multiple tiers could be put together not bya clever encoding but by the simple operation of intersection, which corresponds tological conjunction.
Furthermore, this move of intensionalizing phonology enabled usto provide a straightforward formal basis for adding logical negation and disjunctionto our representations.One important consequence of this work is that there are now good prospects forthe incorporation of nonlinear phonology into constraint-based grammar formalismssuch as HPSG (Pollard and Sag 1987).
Such a move gives rise to a novel view of therelationship between phonology and the other modules of grammar, as some initialinvestigation has already demonstrated (Bird 1992; Bird and Klein in press).
Mak-ing surface generalizations the only goal of analysis makes the machine learning ofanalyses impler (Ellison forthcoming).
The automaton semantics for autosegmentalrepresentations and rules gives us a mechanical way of comparing the empirical claimsmade by a range of autosegmental and segmental accounts of natural anguage phe-nomena.
Finally, to the extent hat phonologists are becoming increasingly committedto a declarative, constraint-based view of their domain, we believe that the modelproposed here is well suited to their computational needs.AcknowledgmentsThis research is funded by the U.K. Scienceand Engineering Research Council, undergrant GR/G-22084.
Computational Phonology:88Steven Bird and T. Mark Ellison One-Level PhonologyA Constraint-Based Approach.
We are gratefulto John Coleman, Mark Johnson, Andr~isKornai, Ewan Klein, Henry Thompson,Markus Walther, Bruce Wiebe, and twoanonymous reviewers for comments on thiswork.
We are also grateful to the studentswho attended our course at the FifthEuropean Summer School on Logic, Languageand Information (Lisbon, August 1993), andgave valuable feedback on this work.
Theauthors take equal responsibility for thematerial presented here.ReferencesAntworth, E. (1990).
PC-KIMMO: ATwo-Level Processor for MorphologicalAnalysis.
Summer Institute of Linguistics.Bear, J.
(1986).
"A morphological recognizerwith syntactic and phonological rules."
InProceedings, the 11th International Conferenceon Computational Linguistics, 272-276.Beesley, K. (1990).
"Finite-state descriptionsof Arabic morphology."
In Proceedings,Second Conference on Bilingual Computing inArabic and English.Beesley, K.; Buckwater, T.; and Newton, S.(1989).
"Two-level finite state analysis ofArabic."
In Proceedings, First Conference onBilingual Computing in Arabic and English.Bird, S. (1990).
Constraint-based phonology.Doctoral dissertation, University ofEdinburgh.Bird, S. (1992).
"Finite-state phonology inHPSG."
In Proceedings, FifteenthInternational Conference on ComputationalLinguistics, 74-80.Bird, S. (in press).
Computational Phonology:A Constraint-Based Approach.
Studies inNatural Language Processing.
CambridgeUniversity Press.Bird, S., and Ellison, T. M. (1992).
"Onelevel phonology: autosegmentalrepresentations and rules as finite-stateautomata."
RP 51, University ofEdinburgh, Centre for Cognitive Science.Bird, S., and Klein, E. (1990).
"Phonologicalevents."
Journal of Linguistics 26, 33-56.Bird, S., and Klein, E. (in press).'.
'Phonological nalysis in typed featuresystems."
Computational Linguistics, 20.Bird, S., and Ladd, D. R. (1991).
"Presentingautosegmental phonology."
Journal ofLinguistics, 27, 193-210.Bloomfield, L. (1926).
"A set of postulatesfor the science of language."
Language, 2,153-164.
Reprinted in Readings inLinguistics I: The Development of DescriptiveLinguistics in America 1925-56, edited byMartin Joos, 26-31.Broe, M. (1993).
Specification Theory: Thetreatment ofredundancy ingenerativephonology.
Doctoral dissertation,University of Edinburgh.Chomsky, N., and Halle, M. (1968).
TheSound Pattern of English.
Harper and Row.Clements, G. N. (1985).
"The geometry ofphonological features."
In PhonologyYearbook 2, edited by C. Ewen andJ.
Anderson, 225-252.
CambridgeUniversity Press.Clements, G. N. (1991).
"Place of articulationin consonants and vowels: a unifiedtheory."
In Working Papers of the CornellPhonetics Laboratory, Number 5, edited byG.
N. Clements and E. Hume, 77-123.Phonetics Laboratory, CorneU University.Clements, G. N., and Ford, K. C.
(1979).
"Kikuyu tone shift and its synchronicconsequences."
Linguistic Inquiry, 10,179-210.Coleman, J. S. (1991).
Phonologicalrepresentations--their names, forms andpowers.
Doctoral dissertation, Universityof York.Coleman, J. S. (1992).
"The phoneticinterpretation f headed phonologicalstructures containing overlappingconstituents."
Phonology, 9, 1-44.Ellison, T. M. (1992).
"Discovering vowelharmony."
In Background and Experimentsin Machine Learning of Natural Language,edited by W. Daelemans and D. Powers,131-136.
ITK.Ellison, T. M. (1993).
The machine l arning ofphonological structure.
Doctoraldissertation, University of WesternAustralia.EUison, T. M. (forthcoming).
"The iterativelearning of phonological constraints.
"Computational Linguistics.Goldsmith, J.
A.
(1976).
Autosegmentalphonology.
Doctoral dissertation,Massachusetts Institute of Technology.Goldsmith, J.
(1982).
"Accent systems."
InThe Structure of Phonological Representations,edited by H. van der Hulst and N. Smith,47--63.
Foris.Goldsmith, J.
(1991).
"Phonology as anintelligent system."
In Bridges BetweenPsychology and Linguistics, edited by D. J.Napoli and J. Kegl, 247-267.
LawrenceErlbaum.Hopcroft, J., and Ullman, J. D. (1979).Introduction to Automata Theory, Languagesand Computation.
Addison-Wesley.Johnson, C. D. (1972).
Formal Aspects ofPhonological Description.
Mouton.Karttunen, L. (1983).
"KIMMO: A generalmorphological processor."
Texas LinguisticForum, 22, 163-186.Kay, M. (1987).
"Nonconcatenative89Computational Linguistics Volume 20, Number 1finite-state morphology."
In Proceedings,Third Meeting of the European Chapter of theAssociation for Computational Linguistics,2-10.Kaye, J.
(1989).
Phonology: A Cognitive View.Erlbaum.Kornai, A.
(1991).
Formal Phonology.
Doctoraldissertation, Stanford University.Kornai, A., and K~ilman, L.
(1988).
"Hungarian sentence intonation."
InAutosegmental Studies on Pitch Accent,edited by H. van der Hulst and N. Smith,183-195.
Foris.Koskenniemi, K. (1983).
Two-level morphology:A general computational model for word-formrecognition and production.
Doctoraldissertation, University of Helsinki.Koskenniemi, K. (1985).
"Compilation ofautomata from morphological two-levelrules."
In Papers from the Fifth ScandinavianConference ofComputational Linguistics,University of Helsinki, 143-149.Leben, W. R. (1973).
Suprasegmentalphonology.
Doctoral dissertation,Massachusetts Institute of Technology.Leben, W. R. (1978).
"The representation ftone."
In Tone--A Linguistic Survey, editedby V. A. Fromkin, 177-219.
AcademicPress.Mastroianni, M. (1993).
"Attribute logicphonology."
Technical Report CMU-LCL93-4, Carnegie Mellon University.McCarthy, J.
(1981).
"A prosodic theory ofnon-concatenative morphology.
"Linguistic Inquiry, 12, 373-418.McCarthy, J.
(1989).
"Linear order inphonological representation."
LinguisticInquiry, 20(1), 71-99.Narayanan, A., and Hashem, L. (1993).
"Onabstract finite-state morphology."
InProceedings, Sixth Conference ofthe EuropeanChapter of the Association for ComputationalLinguistics, 297-304.Partee, B.; ter Meulen, A.; and Wall, R. E.(1990).
Mathematical Methods in Linguistics.Studies in Linguistics and Philosophy.Kluwer.Pierrehumbert, J.
(1990).
"Phonological ndphonetic representation."
Journal ofPhonetics, 18, 375-394.Pollard, C., and Sag, I.
(1987).Information-Based Syntax and Semantics,Volume 13 of CSLI Lecture Notes.
Stanford:Center for the Study of Language andInformation.Pulleyblank, D. (1986).
Tone in LexicalPhonology.
Studies in Natural Languageand Linguistic Theory.
Reidel.Pulman, S. G., and Hepple, M. R. (1993).
"Afeature-based formalism for two levelphonology: a description andimplementation."
Computer Speech andLanguage, 7, 333-358.Ritchie, G. D. (1992).
"Languages generatedby two-level morphological rules.
"Computational Linguistics, 18(1), 41-59.Ritchie, G. D.; Russell, G. J.; Black, A. W.;and Pulman, S. G. (1992).
ComputationalMorphology: Practical Mechanisms for theEnglish Lexicon.
MIT Press.Russell, K. (1993).
A constraint-based approachto phonology.
Doctoral dissertation,University of Southern California.Sagey, E. (1988).
"On the ill-formedness ofcrossing association lines."
LinguisticInquiry, 19, 109-118.Schiller, A., and Steffens, P.
(1991).
"Morphological processing in thetwo-level paradigm."
In TextUnderstanding in LILOG, edited byO.
Herzog and C.-R. Rollinger, 112-126.Springer-Verlag.Scobbie, J.
(1991).
Attribute-value phonology.Doctoral dissertation, University ofEdinburgh.Sproat, R. (1992).
Morphology andComputation.
Natural LanguageProcessing.
MIT Press.Touretzky, D. S., and Wheeler, D. W.
(1990).
"A computational basis for phonology.
"In Advances in Neural Information ProcessingSystems 2: The Collected Papers of the 1989IEEE Conference on Neural InformationProcessing Systems, edited byD.
S. Touretzky.
Morgan Kaufmann.Wheeler, D. (1981).
Aspects of a categorialtheory of phonology.
Doctoral dissertation,Unversity of Massachusetts, Amherst,MA.Wiebe, B.
(1992).
Modelling autosegmentalphonology with multi-tape finite statetransducers.
Master's dissertation, SimohFraser University.90
