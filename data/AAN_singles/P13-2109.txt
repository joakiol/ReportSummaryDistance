Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617?622,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTurning on the Turbo: Fast Third-Order Non-Projective Turbo ParsersAndre?
F. T.
Martins??
Miguel B.
Almeida??
Noah A. Smith#?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal#School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA{atm,mba}@priberam.pt, nasmith@cs.cmu.eduAbstractWe present fast, accurate, direct non-projective dependency parsers with third-order features.
Our approach uses AD3,an accelerated dual decomposition algo-rithm which we extend to handle special-ized head automata and sequential headbigram models.
Experiments in fourteenlanguages yield parsing speeds competi-tive to projective parsers, with state-of-the-art accuracies for the largest datasets(English, Czech, and German).1 IntroductionDependency parsing has become a prominent ap-proach to syntax in the last few years, with in-creasingly fast and accurate models being devised(Ku?bler et al, 2009; Huang and Sagae, 2010;Zhang and Nivre, 2011; Rush and Petrov, 2012).In projective parsing, the arcs in the dependencytree are constrained to be nested, and the problemof finding the best tree can be addressed with dy-namic programming.
This results in cubic-timedecoders for arc-factored and sibling second-ordermodels (Eisner, 1996; McDonald and Pereira,2006), and quartic-time for grandparent models(Carreras, 2007) and third-order models (Koo andCollins, 2010).
Recently, Rush and Petrov (2012)trained third-order parsers with vine pruning cas-cades, achieving runtimes only a small factorslower than first-order systems.
Third-order fea-tures have also been included in transition systems(Zhang and Nivre, 2011) and graph-based parserswith cube-pruning (Zhang and McDonald, 2012).Unfortunately, non-projective dependencyparsers (appropriate for languages with a moreflexible word order, such as Czech, Dutch, andGerman) lag behind these recent advances.
Themain obstacle is that non-projective parsing isNP-hard beyond arc-factored models (McDonaldand Satta, 2007).
Approximate parsers have there-fore been introduced, based on belief propagation(Smith and Eisner, 2008), dual decomposition(Koo et al, 2010), or multi-commodity flows(Martins et al, 2009, 2011).
These are all in-stances of turbo parsers, as shown by Martins etal.
(2010): the underlying approximations comefrom the fact that they run global inference infactor graphs ignoring loop effects.
While thisline of research has led to accuracy gains, none ofthese parsers use third-order contexts, and theirspeeds are well behind those of projective parsers.This paper bridges the gap above by presentingthe following contributions:?
We apply the third-order feature models of Kooand Collins (2010) to non-projective parsing.?
This extension is non-trivial since exact dy-namic programming is not applicable.
Instead,we adapt AD3, the dual decomposition algo-rithm proposed by Martins et al (2011), to han-dle third-order features, by introducing special-ized head automata.?
We make our parser substantially faster than themany-components approach of Martins et al(2011).
While AD3 requires solving quadraticsubproblems as an intermediate step, recent re-sults (Martins et al, 2012) show that they can beaddressed with the same oracles used in the sub-gradient method (Koo et al, 2010).
This enablesAD3 to exploit combinatorial subproblems likethe the head automata above.Along with this paper, we provide a free distribu-tion of our parsers, including training code.12 Dependency Parsing with AD3Dual decomposition is a class of optimizationtechniques that tackle the dual of combinatorial1Released as TurboParser 2.1, and publicly available athttp://www.ark.cs.cmu.edu/TurboParser.617Figure 1: Parts considered in this paper.
First-order models factor over arcs (Eisner, 1996; Mc-Donald et al, 2005), and second-order models in-clude also consecutive siblings and grandparents(Carreras, 2007).
Our parsers add also arbitrarysiblings (not necessarily consecutive) and head bi-grams, as in Martins et al (2011), in additionto third-order features for grand- and tri-siblings(Koo and Collins, 2010).problems in a modular and extensible manner (Ko-modakis et al, 2007; Rush et al, 2010).
In thispaper, we employ alternating directions dual de-composition (AD3; Martins et al, 2011).
Likethe subgradient algorithm of Rush et al (2010),AD3 splits the original problem into local sub-problems, and seeks an agreement on the over-lapping variables.
The difference is that the AD3subproblems have an additional quadratic term toaccelerate consensus.
Recent analysis (Martins etal., 2012) has shown that: (i) AD3 converges ata faster rate,2 and (ii) the quadratic subproblemscan be solved using the same combinatorial ma-chinery that is used in the subgradient algorithm.This opens the door for larger subproblems (suchas the combination of trees and head automata inKoo et al, 2010) instead of a many-componentsapproach (Martins et al, 2011), while still enjoy-ing faster convergence.2.1 Our SetupGiven a sentence with L words, to which weprepend a root symbol $, let A := {?h,m?
| h ?
{0, .
.
.
, L}, m ?
{1, .
.
.
, L}, h 6= m} be theset of possible dependency arcs.
We parame-terize a dependency tree via an indicator vectoru := ?ua?a?A, where ua is 1 if the arc a is in thetree, and 0 otherwise, and we denote by Y ?
R|A|the set of such vectors that are indicators of well-2Concretely, AD3 needs O(1/) iterations to converge toa -accurate solution, while subgradient needs O(1/2).formed trees.
Let {As}Ss=1 be a cover of A, whereeach As ?
A.
We assume that the score of a parsetree u ?
Y decomposes as f(u) :=?Ss=1 fs(zs),where each zs := ?zs,a?a?As is a ?partial view?
ofu, and each local score function fs comes from afeature-based linear model.Past work in dependency parsing considered ei-ther (i) a few ?large?
components, such as treesand head automata (Smith and Eisner, 2008; Kooet al, 2010), or (ii) many ?small?
components,coming from a multi-commodity flow formulation(Martins et al, 2009, 2011).
Let Ys ?
R|As| de-note the set of feasible realizations of zs, i.e., thosethat are partial views of an actual parse tree.
A tu-ple of views ?z1, .
.
.
,zS?
?
?Ss=1 Ys is said to beglobally consistent if zs,a = zs?,a holds for everya, s and s?
such that a ?
As?As?
.
We assume eachparse u ?
Y corresponds uniquely to a globallyconsistent tuple of views, and vice-versa.
Follow-ing Martins et al (2011), the problem of obtainingthe best-scored tree can be written as follows:maximize ?Ss=1 fs(zs)w.r.t.
u ?
R|A|, zs ?
Ys, ?ss.t.
zs,a = ua, ?s, ?a ?
As, (1)where the equality constraint ensures that the par-tial views ?glue?
together to form a coherent parsetree.32.2 Dual Decomposition and AD3Dual decomposition methods dualize out theequality constraint in Eq.
1 by introducing La-grange multipliers ?s,a.
In doing so, they solve arelaxation where the combinatorial sets Ys are re-placed by their convex hulls Zs := conv(Ys).4 Allthat is necessary is the following assumption:Assumption 1 (Local-Max Oracle).
Every s ?
{1, .
.
.
, S} has an oracle that solves efficiently anyinstance of the following subproblem:maximize fs(zs) +?a?As ?s,azs,aw.r.t.
zs ?
Ys.
(2)Typically, Assumption 1 is met whenever the max-imization of fs over Ys is tractable, since the ob-jective in Eq.
2 just adds a linear function to fs.3Note that any tuple ?z1, .
.
.
, zS?
?
?Ss=1 Ys satisfyingthe equality constraints will be globally consistent; this fact,due the assumptions above, will imply u ?
Y.4Let ?|Ys| := {?
?
R|Ys| |?
?
0, ?ys?Ys ?ys = 1}be the probability simplex.
The convex hull of Ys is the setconv(Ys) := {?ys?Ys ?ysys | ?
?
?|Ys|}.
Its membersrepresent marginal probabilities over the arcs in As.618The AD3 algorithm (Martins et al, 2011) alter-nates among the following iterative updates:?
z-updates, which decouple over s = 1, .
.
.
, S,and solve a penalized version of Eq.
2:z(t+1)s := argmaxzs?Zsfs(zs) +?a?As ?(t)s,azs,a?
?2?a?As(zs,a ?
u(t)a )2.
(3)Above, ?
is a constant and the quadratic termpenalizes deviations from the current global so-lution (stored in u(t)).5 We will see (Prop.
2)that this problem can be solved iteratively usingonly the Local-Max Oracle (Eq.
2).?
u-updates, a simple averaging operation:u(t+1)a := 1|{s : a?As}|?s : a?As z(t+1)s,a .
(4)?
?-updates, where the Lagrange multipliers areadjusted to penalize disagreements:?
(t+1)s,a := ?
(t)s,a ?
?
(z(t+1)s,a ?
u(t+1)a ).
(5)In sum, the only difference between AD3 andthe subgradient method is in the z-updates, whichin AD3 require solving a quadratic problem.While closed-form solutions have been developedfor some specialized components (Martins et al,2011), this problem is in general more difficultthan the one arising in the subgradient algorithm.However, the following result, proved in Martinset al (2012), allows to expand the scope of AD3to any problem which satisfies Assumption 1.Proposition 2.
The problem in Eq.
3 admits asolution z?s which is spanned by a sparse basisW ?
Ys with cardinality at most |W| ?
O(|As|).In other words, there is a distribution ?
with sup-port in W such that z?s =?ys?W ?ysys.6Prop.
2 has motivated an active set alorithm(Martins et al, 2012) that maintains an estimateof W by iteratively adding and removing elementscomputed through the oracle in Eq.
2.7 Typically,very few iterations are necessary and great speed-ups are achieved by warm-starting W with the ac-tive set computed in the previous AD3 iteration.This has a huge impact in practice and is crucial toobtain the fast runtimes in ?4 (see Fig.
2).5In our experiments (?4), we set ?
= 0.05.6Note that |Ys| = O(2|As|) in general.
What Prop.
2tells us is that the solution of Eq.
3 can be represented as adistribution over Ys with a very sparse support.7The algorithm is a specialization of Nocedal and Wright(1999), ?16.4, which effectively exploits the sparse represen-tation of z?s .
For details, see Martins et al (2012).0 10 20 30 40 50sentence length (words)0.000.100.20averageruntime(sec.)
AD3Subgrad.Figure 2: Comparison between AD3 and subgra-dient.
We show averaged runtimes in PTB ?22 asa function of the sentence length.
For subgradi-ent, we chose for each sentence the most favorablestepsize in {0.001, 0.01, 0.1, 1}.3 Solving the SubproblemsWe next describe the actual components used inour third-order parsers.Tree component.
We use an arc-factored scorefunction (McDonald et al, 2005): f TREE(z) =?Lm=1 ?ARC(pi(m),m), where pi(m) is the parentof the mth word according to the parse tree z,and ?ARC(h,m) is the score of an individual arc.The parse tree that maximizes this function can befound in time O(L3) via the Chu-Liu-Edmonds?algorithm (Chu and Liu, 1965; Edmonds, 1967).8Grand-sibling head automata.
Let Ainh andAouth denote respectively the sets of incoming andoutgoing candidate arcs for the hth word, wherethe latter subdivides into arcs pointing to the right,Aouth,?, and to the left, Aouth,?.
Define the setsAGSIBh,?
= Ainh ?Aouth,?
andAGSIBh,?
= Ainh ?Aouth,?.
Wedescribe right-side grand-sibling head automata;their left-side counterparts are analogous.
Foreach head word h in the parse tree z, defineg := pi(h), and let ?m0,m1, .
.
.
,mp+1?
be the se-quence of right modifiers of h, with m0 = STARTand mp+1 = END.
Then, we have the followinggrand-sibling component:fGSIBh,?
(z|AGSIBh,?)
=?p+1k=1(?SIB(h,mk?1,mk)?GP(g, h,mk) + ?GSIB(g, h,mk?1,mk)),where we use the shorthand z|B to denote thesubvector of z indexed by the arcs in B ?
A.Note that this score function absorbs grandparentand consecutive sibling scores, in addition to thegrand-sibling scores.9 For each h, fGSIBh,?
can be8In fact, there is an asymptotically fasterO(L2) algorithm(Tarjan, 1977).
Moreover, if the set of possible arcs is reducedto a subset B ?
A (via pruning), then the fastest known al-gorithm (Gabow et al, 1986) runs in O(|B|+L logL) time.9Koo et al (2010) used an identical automaton for theirsecond-order model, but leaving out the grand-sibling scores.619No pruning |Ainm| ?
K same, + |Aouth | ?
JTREE O(L2) O(KL+ L logL) O(KL+ L logL)GSIB O(L4) O(K2L2) O(JK2L)TSIB O(L4) O(KL3) O(J2KL)SEQ O(L3) O(K2L) O(K2L)ASIB O(L3) O(KL2) O(JKL)Table 1: Theoretical runtimes of each subproblemwithout pruning, limiting the number of candidateheads, and limiting (in addition) the number ofmodifiers.
Note the O(L logL) total runtime perAD3 iteration in the latter case.maximized in time O(L3) with dynamic program-ming, yielding O(L4) total runtime.Tri-sibling head automata.
In addition, we de-fine left and right-side tri-sibling head automatathat remember the previous two modifiers of ahead word.
This corresponds to the followingcomponent function (for the right-side case):f TSIBh,?
(z|Aouth,?)
=?p+1k=2 ?TSIB(h,mk?2,mk?1,mk).Again, each of these functions can be maximizedin time O(L3), yielding O(L4) runtime.Sequential head bigram model.
Head bigramscan be captured with a simple sequence model:f SEQ(z) =?Lm=2 ?HB(m,pi(m), pi(m?
1)).Each score ?HB(m,h, h?)
is obtained via featuresthat look at the heads of consecutive words (as inMartins et al (2011)).
This function can be maxi-mized in time O(L3) with the Viterbi algorithm.Arbitrary siblings.
We handle arbitrary siblingsas in Martins et al (2011), definingO(L3) compo-nent functions of the form fASIBh,m,s(z?h,m?, z?h,s?)
=?ASIB(h,m, s).
In this case, the quadratic problemin Eq.
3 can be solved directly in constant time.Tab.
1 details the time complexities of each sub-problem.
Without pruning, each iteration of AD3has O(L4) runtime.
With a simple strategy thatlimits the number of candidate heads per word toa constant K, this drops to cubic time.10 Furtherspeed-ups are possible with more pruning: by lim-iting the number of possible modifiers to a con-stant J , the runtime would reduce to O(L logL).10In our experiments, we employed this strategy withK =10, by pruning with a first-order probabilistic model.
Fol-lowing Koo and Collins (2010), for each word m, we alsopruned away incoming arcs ?h,m?
with posterior probabilityless than 0.0001 times the probability of the most likely head.UAS Tok/secPTB-YM ?22, 1st ord 91.38 4,063PTB-YM ?22, 2nd ord 93.15 1,338PTB-YM ?22, 2nd ord, +ASIB, +HB 93.28 1,018PTB-YM ?22, 3rd ord 93.29 709PTB-YM ?22, 3rd ord, gold tags 94.01 722This work (PTB-YM ?23, 3rd ord) 93.07 735Koo et al (2010) 92.46 112?Huang and Sagae (2010) 92.1?
587?Zhang and Nivre (2011) 92.9?
680?Martins et al (2011) 92.53 66?Zhang and McDonald (2012) 93.06 220This work (PTB-S ?23, 3rd ord) 92.82 604Rush and Petrov (2012) 92.7?
4,460Table 2: Results for the projective English dataset.We report unlabeled attachment scores (UAS) ig-noring punctuation, and parsing speeds in tokensper second.
Our speeds include the time necessaryfor pruning, evaluating features, and decoding, asmeasured on a Intel Core i7 processor @3.4 GHz.The others are speeds reported in the cited papers;those marked with ?
were converted from times persentence.4 ExperimentsWe first evaluated our non-projective parser in aprojective English dataset, to see how its speed andaccuracy compares with recent projective parsers,which can take advantage of dynamic program-ming.
To this end, we converted the Penn Tree-bank to dependencies through (i) the head rulesof Yamada and Matsumoto (2003) (PTB-YM) and(ii) basic dependencies from the Stanford parser2.0.5 (PTB-S).11 We trained by running 10 epochsof cost-augmented MIRA (Crammer et al, 2006).To ensure valid parse trees at test time, we roundedfractional solutions as in Martins et al (2009)?yet, solutions were integral ?
95% of the time.Tab.
2 shows the results in the dev-set (topblock) and in the test-set (two bottom blocks).
Inthe dev-set, we see consistent gains when more ex-pressive features are added, the best accuracies be-ing achieved with the full third-order model; thiscomes at the cost of a 6-fold drop in runtime com-pared with a first-order model.
By looking at thetwo bottom blocks, we observe that our parserhas slightly better accuracies than recent projec-tive parsers, with comparable speed levels (withthe exception of the highly optimized vine cascadeapproach of Rush and Petrov, 2012).11We train on sections ?02?21, use ?22 as validation data,and test on ?23.
We trained a simple 2nd-order tagger with10-fold jackknifing to obtain automatic part-of-speech tagsfor ?22?23, with accuracies 97.2% and 96.9%, respectively.620First Ord.
Sec.
Ord.
Third Ord.
Best published UAS RP12 ZM12UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UASArabic 77.23 2,481 78.50 388 79.64 197 81.12 - Ma11 - - -Bulgarian 91.76 5,678 92.82 2,049 93.10 1,273 93.50 - Ma11 91.9 3,980 93.08Chinese 88.49 18,094 90.14 4,284 89.98 2,592 91.89 - Ma10 90.9 7,800 -Czech 87.66 1,840 90.00 751 90.32 501 89.46 - Ma11 - - -Danish 89.42 4,110 91.20 1,053 91.48 650 91.86 - Ma11 - -Dutch 83.61 3,884 86.37 1,294 86.19 599 85.81 121 Ko10 - - -German 90.52 5,331 91.85 1,788 92.41 965 91.89 - Ma11 90.8 2,880 91.35English 91.21 3,127 93.03 1,317 93.22 785 92.68 - Ma11 - - -Japanese 92.78 23,895 93.14 5,660 93.52 2,996 93.72 - Ma11 92.3 8,600 93.24Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - -Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - -Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008.
?Best Published UAS?
includes the most accurate parsers among Nivre et al (2006), McDonald et al(2006), Martins et al (2010, 2011), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald(2012).
The last two are shown separately in the rightmost columns.In our second experiment (Tab.
3), we used 14datasets, most of which are non-projective, fromthe CoNLL 2006 and 2008 shared tasks (Buch-holz and Marsi, 2006; Surdeanu et al, 2008).Our third-order model achieved the best reportedscores for English, Czech, German, and Dutch?which includes the three largest datasets and theones with the most non-projective dependencies?and is on par with the state of the art for theremaining languages.
To our knowledge, thespeeds are the highest reported among higher-order non-projective parsers, and only about 3?4 times slower than the vine parser of Rush andPetrov (2012), which has lower accuracies.5 ConclusionsWe presented new third-order non-projectiveparsers which are both fast and accurate.
We de-coded with AD3, an accelerated dual decomposi-tion algorithm which we adapted to handle largecomponents, including specialized head automatafor the third-order features, and a sequence modelfor head bigrams.
Results are above the state ofthe art for large datasets and non-projective lan-guages.
In the hope that other researchers may findour implementation useful or are willing to con-tribute with further improvements, we made ourparsers publicly available as open source software.AcknowledgmentsWe thank all reviewers for their insightful com-ments and Lingpeng Kong for help in convertingthe Penn Treebank to Stanford dependencies.
Thiswork was partially supported by the EU/FEDERprogramme, QREN/POR Lisboa (Portugal), underthe Intelligo project (contract 2012/24803), by aFCT grant PTDC/EEI-SII/2312/2012, and by NSFgrant IIS-1054319.ReferencesS.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Inter-national Conference on Natural Language Learn-ing.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In International Con-ference on Natural Language Learning.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:551?585.J.
Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,71B:233?240.J.
M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proc.of International Conference on Computational Lin-guistics, pages 340?345.H.
N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan.1986.
Efficient algorithms for finding minimumspanning trees in undirected and directed graphs.Combinatorica, 6(2):109?122.621L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proc.
of An-nual Meeting of the Association for ComputationalLinguistics, pages 1077?1086.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition:Message-passing revisited.
In Proc.
of InternationalConference on Computer Vision.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of Annual Meeting of theAssociation for Computational Linguistics, pages 1?11.T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, andD.
Sontag.
2010.
Dual decomposition for parsingwith non-projective head automata.
In Proc.
of Em-pirical Methods for Natural Language Processing.S.
Ku?bler, R. McDonald, and J. Nivre.
2009.
Depen-dency parsing.
Morgan & Claypool Publishers.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009.Concise integer linear programming formulationsfor dependency parsing.
In Proc.
of Annual Meetingof the Association for Computational Linguistics.A.
F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.Figueiredo, and P. M. Q. Aguiar.
2010.
Turboparsers: Dependency parsing by approximate vari-ational inference.
In Proc.
of Empirical Methods forNatural Language Processing.A.
F. T. Martins, N. A. Smith, P. M. Q. Aguiar, andM.
A. T. Figueiredo.
2011.
Dual decompositionwith many overlapping components.
In Proc.
of Em-pirical Methods for Natural Language Processing.A.
F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,N.
A. Smith, and E. P. Xing.
2012.
Alternat-ing directions dual decomposition.
Arxiv preprintarXiv:1212.6550.R.
T. McDonald and F. C. N. Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proc.
of Annual Meeting of the EuropeanChapter of the Association for Computational Lin-guistics.R.
McDonald and G. Satta.
2007.
On the complex-ity of non-projective data-driven dependency pars-ing.
In Proc.
of International Conference on ParsingTechnologies.R.
T. McDonald, F. Pereira, K. Ribarov, and J. Ha-jic.
2005.
Non-projective dependency parsing us-ing spanning tree algorithms.
In Proc.
of EmpiricalMethods for Natural Language Processing.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Mul-tilingual dependency analysis with a two-stage dis-criminative parser.
In Proc.
of International Confer-ence on Natural Language Learning.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.2006.
Labeled pseudo-projective dependency pars-ing with support vector machines.
In Procs.
of In-ternational Conference on Natural Language Learn-ing.J.
Nocedal and S. J. Wright.
1999.
Numerical opti-mization.
Springer-Verlag.Alexander M Rush and Slav Petrov.
2012.
Vine prun-ing for efficient multi-pass dependency parsing.
InProc.
of Conference of the North American Chapterof the Association for Computational Linguistics.A.
Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010.On dual decomposition and linear programming re-laxations for natural language processing.
In Proc.of Empirical Methods for Natural Language Pro-cessing.D.
Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
of Empirical Methodsfor Natural Language Processing.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,and J. Nivre.
2008.
The CoNLL-2008 shared taskon joint parsing of syntactic and semantic dependen-cies.
Proc.
of International Conference on NaturalLanguage Learning.R.E.
Tarjan.
1977.
Finding optimum branchings.
Net-works, 7(1):25?36.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProc.
of International Conference on Parsing Tech-nologies.H.
Zhang and R. McDonald.
2012.
Generalizedhigher-order dependency parsing with cube pruning.In Proc.
of Empirical Methods in Natural LanguageProcessing.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proc.of the Annual Meeting of the Association for Com-putational Linguistics.622
