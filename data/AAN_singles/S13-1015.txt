Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 109?118, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUMCC_DLSI: Textual Similarity based on Lexical-Semantic featuresAlexander Ch?vez, Antonio Fern?ndez Orqu?n,H?ctor D?vila, Yoan Guti?rrez, ArmandoCollazo, Jos?
I. AbreuDI, University of MatanzasAutopista a Varadero km 3 ?Matanzas, Cuba.
{alexander.chavez, tony,hector.davila, yoan.gutierrez,armando.collazo, jose.abreu}@umcc.cuAndr?s Montoyo, Rafael Mu?ozDLSI, University of Alicante Carretera deSan Vicente S/N Alicante, Spain.
{montoyo,rafael}@dlsi.ua.esAbstractThis paper describes the specifications andresults of UMCC_DLSI system, whichparticipated in the Semantic TextualSimilarity task (STS) of SemEval-2013.
Oursupervised system uses different types oflexical and semantic features to train aBagging classifier used to decide the correctoption.
Related to the different features wecan highlight the resource ISR-WN used toextract semantic relations among words andthe use of different algorithms to establishsemantic and lexical similarities.
In order toestablish which features are the mostappropriate to improve STS results weparticipated with three runs using differentset of features.
Our best run reached theposition 44 in the official ranking, obtaininga general correlation coefficient of 0.61.1 IntroductionSemEval-2013 (Agirre et al 2013) presents thetask Semantic Textual Similarity (STS) again.
InSTS, the participating systems must examine thedegree of semantic equivalence between twosentences.
The goal of this task is to create aunified framework for the evaluation of semantictextual similarity modules and to characterizetheir impact on NLP applications.STS is related to Textual Entailment (TE) andParaphrase tasks.
The main difference is thatSTS assumes bidirectional graded equivalencebetween the pair of textual snippets.In case of TE, the equivalence is directional(e.g.
a student is a person, but a person is notnecessarily a student).
In addition, STS differsfrom TE and Paraphrase in that, rather thanbeing a binary yes/no decision, STS is asimilarity-graded notion (e.g.
a student is moresimilar to a person than a dog to a person).This graded bidirectional is useful for NLPtasks such as Machine Translation (MT),Information Extraction (IE), QuestionAnswering (QA), and Summarization.
Severalsemantic tasks could be added as modules in theSTS framework, ?such as Word SenseDisambiguation and Induction, LexicalSubstitution, Semantic Role Labeling, MultiwordExpression detection and handling, Anaphoraand Co-reference resolution, Time and Dateresolution and Named Entity, among others?11.1 Description of 2013 pilot taskThis edition of SemEval-2013 remain with thesame classification approaches that in their firstversion in 2012.
The output of different systemswas compared to the reference scores providedby SemEval-2013 gold standard file, whichrange from five to zero according to the nextcriterions2: (5) ?The two sentences areequivalent, as they mean the same thing?.
(4)?The two sentences are mostly equivalent, butsome unimportant details differ?.
(3) ?The twosentences are roughly equivalent, but someimportant information differs/missing?.
(2) ?Thetwo sentences are not equivalent, but share somedetails?.
(1) ?The two sentences are not1 http://www.cs.york.ac.uk/semeval-2012/task6/2 http://www.cs.york.ac.uk/semeval-2012/task6/data/uploads/datasets/train-readme.txt109equivalent, but are on the same topic?.
(0) ?Thetwo sentences are on different topics?.After this introduction, the rest of the paper isorganized as follows.
Section 3 shows theRelated Works.
Section 4 presents our systemarchitecture and description of the different runs.In section 4 we describe the different featuresused in our system.
Results and a discussion areprovided in Section 5 and finally we conclude inSection 6.2 Related WorksThere are more extensive literature on measuringthe similarity between documents than tobetween sentences.
Perhaps the most recentlyscenario is constituted by the competition ofSemEval-2012 task 6: A Pilot on SemanticTextual Similarity (Aguirre and Cerd, 2012).
InSemEval-2012, there were used different toolsand resources like stop word list, multilingualcorpora, dictionaries, acronyms, and tables ofparaphrases, ?but WordNet was the most usedresource, followed by monolingual corpora andWikipedia?
(Aguirre and Cerd, 2012).According to Aguirre, Generic NLP tools werewidely used.
Among those that stand out weretools for lemmatization and POS-tagging(Aguirre and Cerd, 2012).
On a smaller scaleword sense disambiguation, semantic rolelabeling and time and date resolution.
Inaddition, Knowledge-based and distributionalmethods were highly used.
Aguirre and Cerdremarked on (Aguirre and Cerd, 2012) thatalignment and/or statistical machine translationsoftware, lexical substitution, string similarity,textual entailment and machine translationevaluation software were used to a lesser extent.It can be noted that machine learning was widelyused to combine and tune components.Most of the knowledge-based methods ?obtaina measure of relatedness by utilizing lexicalresources and ontologies such as WordNet(Miller et al 1990b) to measure definitionaloverlap, term distance within a graphicaltaxonomy, or term depth in the taxonomy as ameasure of specificity?
(Banea et al 2012).Some scholars as in (Corley and Mihalcea,June 2005) have argue ?the fact that acomprehensive metric of text semantic similarityshould take into account the relations betweenwords, as well as the role played by the variousentities involved in the interactions described byeach of the two sentences?.
This idea is resumedin the Principle of Compositionality, thisprinciple posits that the meaning of a complexexpression is determined by the meanings of itsconstituent expressions and the rules used tocombine them (Werning et al 2005).
Corleyand Mihalcea in this article combined metrics ofword-to-word similarity, and language modelsinto a formula and they pose that this is apotentially good indicator of the semanticsimilarity of the two input texts sentences.
Theymodeled the semantic similarity of a sentence asa function of the semantic similarity of thecomponent words (Corley and Mihalcea, June2005).One of the top scoring systems at SemEval-2012 (?ari?
et al 2012) tended to use most ofthe aforementioned resources and tools.
Theypredict the human ratings of sentence similarityusing a support-vector regression model withmultiple features measuring word-overlapsimilarity and syntax similarity.
They alsocompute the similarity between sentences usingthe semantic alignment of lemmas.
First, theycompute the word similarity between all pairs oflemmas from first to second sentence, usingeither the knowledge-based or the corpus-basedsemantic similarity.
They named this methodGreedy Lemma Aligning Overlap.Daniel B?r presented the UKP system, whichperformed best in the Semantic TextualSimilarity (STS) task at SemEval-2012 in twoout of three metrics.
It uses a simple log-linearregression model, trained on the training data, tocombine multiple text similarity measures ofvarying complexity.3 System architecture and descriptionof the runsAs we can see in Figure 1, our three runs beginwith the pre-processing of SemEval-2013?straining set.
Every sentence pair is tokenized,lemmatized and POS-tagged using Freeling 2.2tool (Atserias et al 2006).
Afterwards, severalmethods and algorithms are applied in order toextract all features for our Machine LearningSystem (MLS).
Each run uses a particular groupof features.110Figure 1.
System Architecture.The Run 1 (named MultiSemLex) is our mainrun.
This takes into account all extracted featuresand trains a model with a Bagging classifier(Breiman, 1996) (using REPTree).
The trainingcorpus has been provided by SemEval-2013competition, in concrete by the Semantic TextualSimilarity task.The Run 2 (named MultiLex) and Run 3(named MultiSem) use the same classifier, butincluding different features.
Run 2 uses (seeFigure 1) features extracted from Lexical-Semantic Metrics (LS-M) described in section4.1, and Lexical-Semantic Alignment (LS-A)described in section 4.2.On the other hand, Run 3 uses featuresextracted only from Semantic Alignment (SA)described in section 4.3.As a result, we obtain three trained modelscapable to estimate the similarity value betweentwo phrases.Finally, we test our system with the SemEval-2013 test set (see Table 14 with the results of ourthree runs).
The following section describes thefeatures extraction process.4 Description of the features used in theMachine Learning SystemMany times when two phrases are very similar,one sentence is in a high degree lexicallyoverlapped by the other.
Inspired in this fact wedeveloped various algorithms, which measurethe level of overlapping by computing a quantityof matching words in a pair of phrases.
In oursystem, we used as features for a MLS lexicaland semantic similarity measures.
Other featureswere extracted from a lexical-semantic sentencesalignment and a variant using only a semanticalignment.4.1 Similarity measuresWe have used well-known string basedsimilarity measures like: Needleman-Wunch(sequence alignment), Smith-Waterman(sequence alignment), Smith-Waterman-Gotoh,Smith-Waterman-Gotoh-Windowed-Affine,Jaro, Jaro-Winkler, Chapman-Length-Deviation,Chapman-Mean-Length, QGram-Distance,Block-Distance, Cosine Similarity, DiceSimilarity, Euclidean Distance, JaccardSimilarity, Matching Coefficient, Monge-Elkanand Overlap-Coefficient.
These algorithms havebeen obtained from an API (ApplicationProgram Interface) SimMetrics library v1.5 for.NET 2.03.
We obtained 17 features for our MLSfrom these similarity measures.Using Levenshtein?s edit distance (LED), wecomputed also two different algorithms in orderto obtain the alignment of the phrases.
In the firstone, we considered a value of the alignment asthe LED between two sentences.
Contrary to(Tatu et al 2006), we do not remove thepunctuation or stop words from the sentences,3 Copyright (c) 2006 by Chris Parkinson, available inhttp://sourceforge.net/projects/simmetrics/Run1.Bagging ClassifierTraining set fromSemEval 2013Pre-Processing (using Freeling)Run 3 BaggingclassifierRun 2 BaggingclassifierSimilarity ScoresFeature extractionLexical-Semantic MetricsLexical-semanticalignmentSemanticalignmentJaro QGramRel.Concept.
.
.Tokenizing Lemmatizing POS taggingSemEval2013 TestsetTraining Process (using Weka)111neither consider different cost for transformationoperation, and we used all the operations(deletion, insertion and substitution).The second one is a variant that we namedDouble Levenshtein?s Edit Distance (DLED)(see Table 9 for detail).
For this algorithm, weused LED to measure the distance between thephrases, but in order to compare the words, weused LED again (Fern?ndez et al 2012;Fern?ndez Orqu?n et al 2009).Another distance we used is an extension ofLED named Extended Distance (in spanishdistancia extendida (DEx)) (see (Fern?ndez etal., 2012; Fern?ndez Orqu?n et al 2009) fordetails).
This algorithm is an extension of theLevenshtein?s algorithm, with which penaltiesare applied by considering what kind oftransformation (insertion, deletion, substitution,or non-operation) and the position it was carriedout, along with the character involved in theoperation.
In addition to the cost matrixes usedby Levenshtein?s algorithm, DEx also obtainsthe Longest Common Subsequence (LCS)(Hirschberg, 1977) and other helpful attributesfor determining similarity between strings in asingle iteration.
It is worth noting that theinclusion of all these penalizations makes theDEx algorithm a good candidate for ourapproach.In our previous work (Fern?ndez Orqu?n et al2009), DEx demonstrated excellent results whenit was compared with other distances as(Levenshtein, 1965), (Neeedleman and Wunsch,1970), (Winkler, 1999).
We also used as afeature the Minimal Semantic Distances(Breadth First Search (BFS)) obtained betweenthe most relevant concepts of both sentences.The relevant concepts pertain to semanticresources ISR-WN (Guti?rrez et al 2011;2010a), as WordNet (Miller et al 1990a),WordNet Affect (Strapparava and Valitutti,2004), SUMO (Niles and Pease, 2001) andSemantic Classes (Izquierdo et al 2007).
Thoseconcepts were obtained after having applied theAssociation Ratio (AR) measure betweenconcepts and words over each sentence.
(Werefer reader to (Guti?rrez et al 2010b) for afurther description).Another attribute obtained by the system was avalue corresponding with the sum of the smallerdistances (using QGram-Distance) between thewords or the lemmas of the phrase one with eachwords of the phrase two.As part of the attributes extracted by thesystem, was also the value of the sum of thesmaller distances (using Levenshtein) amongstems, chunks and entities of both phrases.4.2 Lexical-Semantic alignmentAnother algorithm that we created is the Lexical-Semantic Alignment.
In this algorithm, we triedto align the phrases by its lemmas.
If the lemmascoincide we look for coincidences among parts-of-speech4 (POS), and then the phrase isrealigned using both.
If the words do not sharethe same POS, they will not be aligned.
To thispoint, we only have taken into account a lexicalalignment.
From now on, we are going to applya semantic variant.
After all the process, the non-aligned words will be analyzed taking intoaccount its WordNet?s relations (synonymy,hyponymy, hyperonymy, derivationally-related-form, similar-to, verbal group, entailment andcause-to relation); and a set of equivalences likeabbreviations of months, countries, capitals, daysand currency.
In case of hyperonymy andhyponymy relation, words are going to bealigned if there is a word in the first sentencethat is in the same relation (hyperonymy orhyponymy) with another one in the secondsentence.
For the relations ?cause-to?
and?implication?
the words will be aligned if thereis a word in the first sentence that causes orimplicates another one in the second sentence.All the other types of relations will be carriedout in bidirectional way, that is, there is analignment if a word of the first sentence is asynonymous of another one belonging to thesecond one or vice versa.Finally, we obtain a value we called alignmentrelation.
This value is calculated as ???
=???
/ ????.
Where ???
is the finalalignment value, ???
is the number of alignedwords, and ????
is the number of words of theshorter phrase.
The  ???
value is also anotherfeature for our system.
Other extracted attributesthey are the quantity of aligned words and thequantity of not aligned words.
The core of thealignment is carried out in different ways, which4 (noun, verb, adjective, adverbs, prepositions,conjunctions, pronouns, determinants, modifiers, etc.
)112are obtained from several attributes.
Each waycan be compared by:?
the part-of-speech.?
the morphology and the part-of-speech.?
the lemma and the part-of-speech.?
the morphology, part-of-speech, andrelationships of WordNet.?
the lemma, part-of-speech, andrelationships of WordNet.4.3 Semantic AlignmentThis alignment method depends on calculatingthe semantic similarity between sentences basedon an analysis of the relations, in ISR-WN, ofthe words that fix them.First, the two sentences are pre-processed withFreeling and the words are classified accordingto their POS, creating different groups.The distance between two words will be thedistance, based on WordNet, of the mostprobable sense of each word in the pair, on thecontrary of our previously system in SemEval2012.
In that version, we assumed the selectedsense after apply a double Hungarian Algorithm(Kuhn, 1955), for more details  please refer to(Fern?ndez et al 2012).
The distance iscomputed according to the equation (1):?
(?, ?)
= ?
?
?
?(?[?
], ?[?
+ 1])?=?
?=0 ; (1)Where ?
is the collection of synsetscorresponding to the minimum path betweennodes ?
and ?, ?
is the length of ?
subtractingone, ?
is a function that search the relationconnecting ?
and ?
nodes, ?
is a weightassociated to the relation searched by ?
(seeTable 1).Relation WeightHyponym, Hypernym 2Member_Holonym, Member_Meronym,Cause, Entailment5Similar_To 10Antonym 200Other relation different to Synonymy 60Table 1.
Weights applied to WordNet relations.Table 1 shows the weights associated toWordNet relations between two synsets.Let us see the following example:?
We could take the pair 99 of corpusMSRvid (from training set of SemEval-2013) with a littler transformation inorder to a better explanation of ourmethod.Original pairA: A polar bear is running towards a group ofwalruses.B: A polar bear is chasing a group of walruses.Transformed pair:A1: A polar bear runs towards a group of cats.B1: A wale chases a group of dogs.Later on, using equation (1), a matrix with thedistances between all groups of both phrases iscreated (see Table 2).GROUPS polar bear runs towards group catswale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3group     Dist:=0dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1Table 2.
Distances between groups.Using the Hungarian Algorithm (Kuhn, 1955)for Minimum Cost Assignment, each group ofthe first sentence is checked with each elementof the second sentence, and the rest is marked aswords that were not aligned.In the previous example the words ?toward?and ?polar?
are the words that were not aligned,so the number of non-aligned words is two.There is only one perfect match: ?group-group?
(match with cost=0).
The length of the shortestsentence is four.
The Table 3 shows the resultsof this analysis.Number of exactcoincidenceTotal Distances ofoptimal MatchingNumber ofnon-alignedWords1 5 2Table 3.
Features from the analyzed sentences.This process has to be repeated for nouns (seeTable 4), verbs, adjective, adverbs, prepositions,conjunctions, pronouns, determinants, modifiers,digits and date times.
On the contrary, the tableshave to be created only with the similar groupsof the sentences.
Table 4 shows featuresextracted from the analysis of nouns.GROUPS bear group catswale Dist := 2  Dist := 2group  Dist := 0dogs Dist := 1  Dist := 1Table 4.
Distances between groups of nouns.113Number ofexactcoincidenceTotal Distancesof optimalMatchingNumber of non-alignedWords1 3 0Table 5.
Feature extracted from analysis of nouns.Several attributes are extracted from the pair ofsentences (see Table 3 and Table 5).
Threeattributes considering only verbs, only nouns,only adjectives, only adverbs, only prepositions,only conjunctions, only pronouns, onlydeterminants, only modifiers, only digits, andonly date times.
These attributes are:?
Number of exact coincidences?
Total distance of matching?
Number of words that do not matchMany groups have particular featuresaccording to their parts-of-speech.
The group ofthe nouns has one more feature that indicates ifthe two phrases have the same number (plural orsingular).
For this feature, we take the average ofthe number of each noun in the phrase like anumber of the phrase.For the group of adjectives we added a featureindicating the distance between the nouns thatmodify it from the aligned adjectives,respectively.For the verbs, we search the nouns that precedeit, and the nouns that are next of the verb, andwe define two groups.
We calculated thedistance to align each group with every pair ofaligned verbs.
The verbs have other feature thatspecifies if all verbs are in the same verbal time.With the adverbs, we search the verb that ismodified by it, and we calculate their distancefrom all alignment pairs.With the determinants and the adverbs wedetect if any of the alignment pairs areexpressing negations (like don?t, or do not) inboth cases or not.
Finally, we determine if thetwo phrases have the same principal action.
Forall this new features, we aid with Freeling tool.As a result, we finally obtain 42 attributes fromthis alignment method.
It is important to remarkthat this alignment process searches to solve, foreach word from the rows (see Table 4) it has arespectively word from the columns.4.4 Description of the alignment featureFrom the alignment process, we extract differentfeatures that help us a better result of our MLS.Table 6 shows the group of features with lexicaland semantic support, based on WordNetrelation (named F1).
Each of they were namedwith a prefix, a hyphen and a suffix.
Table 7describes the meaning of every prefix, and Table8 shows the meaning of the suffixes.FeaturesCPA_FCG, CPNA_FCG, SIM_FCG, CPA_LCG,CPNA_LCG, SIM_LCG, CPA_FCGR,CPNA_FCGR, SIM_FCGR, CPA_LCGR,CPNA_LCGR, SIM_LCGRTable 6.
F1.
Semantic feature group.Prefixes DescriptionsCPA Number of aligned words.CPNA Number of non-aligned words.SIM SimilarityTable 7.
Meaning of each prefixes.Prefixes Compared words for?FCG Morphology and POSLCG Lemma and POSFCGR Morphology, POS and WordNet relation.LCGR Lemma, POS and WordNet relation.Table 8.
Suffixes for describe each type of alignment.Features DescriptionsLevForma Levenshtein Distance between twophrases comparing words bymorphologyLevLema The same as above, but nowcomparing by lemma.LevDoble Idem, but comparing again byLevenshtein and accepting wordsmatch if the distance is ?
2.DEx Extended DistanceNormLevF,NormLevLNormalized forms of LevForma andLevLema.Table 9.
F2.
Lexical alignment measures.FeaturesNWunch, SWaterman, SWGotoh, SWGAffine, Jaro,JaroW, CLDeviation, CMLength, QGramD, BlockD,CosineS, DiceS, EuclideanD, JaccardS, MaCoef,MongeElkan, OverlapCoef.Table 10.
Lexical Measure from SimMetrics library.Features DescriptionsAxAQGD_L All against all applying QGramDand comparing by lemmas of thewords.AxAQGD_F Same as above, but applyingQGramD and comparing bymorphology.AxAQGD_LF Idem, not only comparing by lemmabut also by morphology.AxALev_LF All against all applying Levenhstein114comparing by morphology andlemmas.AxA_Stems Idem, but applying Levenhsteincomparing by the stems of thewords.Table 11.
Aligning all against all.Other features we extracted were obtainedfrom the following similarity measures (namedF2) (see Table 9 for detail).We used another group named F3, with lexicalmeasure extracted from SimMetric library (seeTable 10 for detail).Finally we used a group of five feature (namedF4), extracted from all against all alignment (seeTable 11 for detail).4.5 Description of the training phaseFor the training process, we used a supervisedlearning framework, including all the training setas a training corpus.
Using ten-fold crossvalidation with the classifier mentioned insection 3 (experimentally selected).As we can see in Table 12, the attributescorresponding with the Test 1 (only lexicalattributes) obtain 0.7534 of correlation.
On theother side, the attributes of the Test 2 (lexicalfeatures with semantic support) obtain 0.7549 ofcorrelation, and all features obtain 0.7987.
Beingdemonstrated the necessity to tackle the problemof the similarity from a multidimensional pointof view (see Test 3 in the Table 12).FeaturesCorrelation on the training data of SemEval-2013Test 1 Test 2 Test 3F10.75490.7987F2F3 0.7534F4Table 12.
Features influence.
Gray cells meanfeatures are not taking into account.5 Result and discussionSemantic Textual Similarity task of SemEval-2013 offered two official measures to rank thesystems5: Mean- the main evaluation value,Rank- gives the rank of the submission asordered by the "mean" result.5http://ixa2.si.ehu.es/sts/index.php?option=com_content&view=article&id=53&Itemid=61Test data for the core test datasets, comingfrom the following:Corpus DescriptionHeadlineas: news headlines mined from several newssources by European Media Monitorusing the RSS feed.OnWN: mapping of lexical resources OnWN.
Thesentences are sense definitions fromWordNet and OntoNotes.FNWN: the sentences are sense definitions fromWordNet and FrameNet.SMT: SMT dataset comes from DARPA GALEHTER and HyTER.
One sentence is aMT output and the other is a referencetranslation where a reference is generatedbased on human post editing.Table 13.
Test Core Datasets.Using these measures, our second run (Run 2)obtained the best results (see Table 14).
As wecan see in Table 14, our lexical run has obtainedour best result, given at the same time worthresult in our other runs.
This demonstrates thattackling this problem with combining multiplelexical similarity measure produce better resultsin concordance to this specific test corpora.To explain Table 14 we present followingdescriptions: caption in top row mean: 1-Headlines, 2- OnWN, 3- FNWN, 4- SMT and 5-mean.Run 1 R 2 R 3 R 4 R 5 R1 0.5841 60 0.4847 54 0.2917 52 0.2855 66 0.4352 582 0.6168 55 0.5557 39 0.3045 50 0.3407 28 0.4833 443 0.3846 85 0.1342 88 -0.0065 85 0.2736 72 0.2523 87Table 14.
Official SemEval-2013 results over testdatasets.
Ranking (R).The Run 1 is our main run, which contains thejunction of all attributes (lexical and semanticattributes).
Table 14 shows the results of all theruns for a different corpus from test phase.
Aswe can see, Run 1 did not obtain the best resultsamong our runs.Otherwise, Run 3 uses more semantic analysisthan Run 2, from this; Run 3 should get betterresults than reached over the corpus of FNWN,because this corpus is extracted from FrameNetcorpus (Baker et al 1998) (a semantic network).FNWN provides examples with high semanticcontent than lexical.Run 3 obtained a correlation coefficient of0.8137 for all training corpus of SemEval 2013,115while Run 2 and Run 1 obtained 0.7976 and0.8345 respectively with the same classifier(Bagging using REPTree, and cross validationwith ten-folds).
These results present acontradiction between test and train evaluation.We think it is consequence of some obstaclespresent in test corpora, for example:In headlines corpus there are great quantity ofentities, acronyms and gentilics that we not takeinto account in our system.The corpus FNWN presents a non-balanceaccording to the length of the phrases.In OnWN -test corpus-, we believe that someevaluations are not adequate in correspondencewith the training corpus.
For example, in line 7the goal proposed was 0.6, however both phrasesare semantically similar.
The phrases are:?
the act of lifting something?
the act of climbing something.We think that 0.6 are not a correct evaluationfor this example.
Our system result, for thisparticular case, was 4.794 for Run 3, and 3.814for Run 2, finally 3.695 for Run 1.6 Conclusion and future worksThis paper have introduced a new framework forrecognizing Semantic Textual Similarity, whichdepends on the extraction of several features thatcan be inferred from a conventionalinterpretation of a text.As mentioned in section 3 we have conductedthree different runs, these runs only differ in thetype of attributes used.
We can see in Table 14that all runs obtained encouraging results.
Ourbest run was situated at 44th position of 90 runsof the ranking of SemEval-2013.
Table 12 andTable 14 show the reached positions for the threedifferent runs and the ranking according to therest of the teams.In our participation, we used a MLS that workswith features extracted from five differentstrategies: String Based Similarity Measures,Semantic Similarity Measures, Lexical-SemanticAlignment and Semantic Alignment.We have conducted the semantic featuresextraction in a multidimensional context usingthe resource ISR-WN, the one that allowed us tonavigate across several semantic resources(WordNet, WordNet Domains, WordNet Affect,SUMO, SentiWordNet and Semantic Classes).Finally, we can conclude that our systemperforms quite well.
In our current work, weshow that this approach can be used to correctlyclassify several examples from the STS task ofSemEval-2013.
Compared with the best run ofthe ranking (UMBC_EBIQUITY- ParingWords)(see Table 15) our main run has very closeresults in headlines (1), and SMT (4) core testdatasets.Run 1 2 3 4 5 6(First) 0.7642 0.7529 0.5818 0.3804 0.6181 1(Our)RUN 20.6168 0.5557 0.3045 0.3407 0.4833 44Table 15.
Comparison with best run (SemEval 2013).As future work we are planning to enrich oursemantic alignment method with ExtendedWordNet (Moldovan and Rus, 2001), we thinkthat with this improvement we can increase theresults obtained with texts like those in OnWNtest set.6.1 Team CollaborationIs important to remark that our team has beenworking up in collaboration with INAOE(Instituto Nacional de Astrof?sica, ?ptica yElectr?nica) and LIPN (Laboratoired'Informatique de Paris-Nord), Universit?
Paris13 universities, in order to encourage theknowledge interchange and open sharedtechnology.
Supporting this collaboration,INAOE-UPV (Instituto Nacional de Astrof?sica,?ptica y Electr?nica and Universitat Polit?cnicade Val?ncia) team, in concrete in INAOE-UPV-run 3 has used our semantic distances for nouns,adjectives, verbs and adverbs, as well as lexicalattributes like LevDoble, NormLevF, NormLevLand Ext (see influence of these attributes inTable 12).AcknowledgmentsThis research work has been partially funded bythe Spanish Government through the projectTEXT-MESS 2.0 (TIN2009-13391-C04),"An?lisis de Tendencias Mediante T?cnicas deOpini?n Sem?ntica" (TIN2012-38536-C03-03)and ?T?cnicas de Deconstrucci?n en laTecnolog?as del Lenguaje Humano?
(TIN2012-31224); and by the Valencian Governmentthrough the project PROMETEO(PROMETEO/2009/199).116ReferenceAgirre, E.; D. Cer; M. Diab and W. Guo.
*SEM 2013Shared Task: Semantic Textual Similarityincluding a Pilot on Typed-Similarity.
*SEM2013: The Second Joint Conference on Lexical andComputational Semantics, Association forComputational Linguistics, 2013.Aguirre, E. and D. Cerd.
SemEval 2012 Task 6:APilot on Semantic Textual Similarity.
First JoinConference on Lexical and ComputationalSemantic (*SEM), Montr?al, Canada, Associationfor Computational Linguistics., 2012.
385-393 p.Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L.Padr?
and M. Padr?.
FreeLing 1.3: Syntactic andsemantic services in an opensource NLP library.Proceedings of LREC'06, Genoa, Italy, 2006.Baker, C. F.; C. J. Fillmore and J.
B. Lowe.
Theberkeley framenet project.
Proceedings of the 17thinternational conference on Computationallinguistics-Volume 1, Association forComputational Linguistics, 1998.
86-90 p.Banea, C.; S. Hassan; M. Mohler and R. Mihalcea.UNT:A Supervised Synergistic Approach toSemanticText Similarity.
First Joint Conference onLexical and Computational Semantics (*SEM),Montr?al.
Canada, Association for ComputationalLinguistics, 2012.
635?642 p.Breiman, L. Bagging predictors Machine learning,1996, 24(2): 123-140.Corley, C. and R. Mihalcea.
Measuring the SemanticSimilarity of Texts, Association for ComputationalLinguistic.
Proceedings of the ACL Work shop onEmpirical Modeling of Semantic Equivalence andEntailment, pages 13?18, June 2005.Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez;A. Gonz?lez; R. Estrada; Y. Casta?eda; S.V?zquez; A. Montoyo and R. Mu?oz.UMCC_DLSI: Multidimensional Lexical-Semantic Textual Similarity.
{*SEM 2012}: TheFirst Joint Conference on Lexical andComputational Semantics -- Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the SixthInternational Workshop on Semantic Evaluation{(SemEval 2012)}, Montreal, Canada, Associationfor Computational Linguistics, 2012.
608--616 p.Fern?ndez Orqu?n, A. C.; J.
D?az Blanco; A. FundoraRolo and R. Mu?oz Guillena.
Un algoritmo para laextracci?n de caracter?sticas lexicogr?ficas en lacomparaci?n de palabras.
IV Convenci?nCient?fica Internacional CIUM, Matanzas, Cuba,2009.Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S.V?zquez.
Integration of semantic resources basedon WordNet.
XXVI Congreso de la SociedadEspa?ola para el Procesamiento del LenguajeNatural, Universidad Polit?cnica de Valencia,Valencia, SEPLN 2010, 2010a.
161-168 p. 1135-5948.Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S.V?zquez.
UMCC-DLSI: Integrative resource fordisambiguation task.
Proceedings of the 5thInternational Workshop on Semantic Evaluation,Uppsala, Sweden, Association for ComputationalLinguistics, 2010b.
427-432 p.Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S.V?zquez Enriching the Integration of SemanticResources based on WordNet Procesamiento delLenguaje Natural, 2011, 47: 249-257.Hirschberg, D. S. Algorithms for the longest commonsubsequence problem J. ACM, 1977, 24: 664?675.Izquierdo, R.; A. Su?rez and G. Rigau A Proposal ofAutomatic Selection of Coarse-grained SemanticClasses for WSD Procesamiento del LenguajeNatural, 2007, 39: 189-196.Kuhn, H. W. The Hungarian Method for theassignment problem Naval Research LogisticsQuarterly, 1955, 2: 83?97.Levenshtein, V. I. Binary codes capable of correctingspurious insertions and deletions of ones.
Problemsof information Transmission.
1965. pp.
8-17 p.Miller, G. A.; R. Beckwith; C. Fellbaum; D. Grossand K. Miller.
Five papers on WordNet.Princenton University, Cognositive ScienceLaboratory, 1990a.Miller, G. A.; R. Beckwith; C. Fellbaum; D. Grossand K. Miller Introduction to WordNet: An On-line Lexical Database International Journal ofLexicography, 3(4):235-244., 1990b.Moldovan, D. I. and V. Rus Explaining Answers withExtended WordNet ACL, 2001.Neeedleman, S. and C. Wunsch A general methodapplicable to the search for similarities in theamino acid sequence of two proteins Mol.
Biol,1970, 48(443): 453.Niles, I. and A. Pease.
Origins of the IEEE StandardUpper Ontology.
Working Notes of the IJCAI-2001 Workshop on the IEEE Standard UpperOntology, Seattle, Washington, USA., 2001.117?ari?, F.; G.
Glava?
; Mladenkaran; J.
?najder and B.D.
Basi?.
TakeLab: Systems for MeasuringSemantic Text Similarity.
Montr?al, Canada, FirstJoin Conference on Lexical and ComputationalSemantic (*SEM), pages 385-393.
Association forComputational Linguistics., 2012.Strapparava, C. and A. Valitutti.
WordNet-Affect: anaffective extension of WordNet.
Proceedings ofthe 4th International Conference on LanguageResources and Evaluation (LREC 2004), Lisbon,2004.
1083-1086 p.Tatu, M.; B. Iles; J. Slavick; N. Adrian and D.Moldovan.
COGEX at the Second RecognizingTextual Entailment Challenge.
Proceedings of theSecond PASCAL Recognising Textual EntailmentChallenge Workshop, Venice, Italy, 2006.
104-109p.Werning, M.; E. Machery and G. Schurz.
TheCompositionality of Meaning and Content,Volume 1: Foundational issues.
ontos verlag[Distributed in] North and South America byTransaction Books, 2005. p. Linguistics &philosophy, Bd.
1.
3-937202-52-8.Winkler, W. The state of record linkage and currentresearch problems.
Technical Report, StatisticalResearch Division, U.S, Census Bureau, 1999.118
