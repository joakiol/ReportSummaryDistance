Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774?1784,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsConnotation Lexicon:A Dash of Sentiment Beneath the Surface MeaningSong Feng Jun Seok Kang Polina Kuznetsova Yejin ChoiDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400songfeng, junkang, pkuznetsova, ychoi@cs.stonybrook.eduAbstractUnderstanding the connotation of wordsplays an important role in interpreting sub-tle shades of sentiment beyond denotativeor surface meaning of text, as seeminglyobjective statements often allude nuancedsentiment of the writer, and even purpose-fully conjure emotion from the readers?minds.
The focus of this paper is draw-ing nuanced, connotative sentiments fromeven those words that are objective on thesurface, such as ?intelligence?, ?human?,and ?cheesecake?.
We propose inductionalgorithms encoding a diverse set of lin-guistic insights (semantic prosody, distri-butional similarity, semantic parallelism ofcoordination) and prior knowledge drawnfrom lexical resources, resulting in the firstbroad-coverage connotation lexicon.1 IntroductionThere has been a substantial body of researchin sentiment analysis over the last decade (Pangand Lee, 2008), where a considerable amount ofwork has focused on recognizing sentiment that isgenerally explicit and pronounced rather than im-plied and subdued.
However in many real-worldtexts, even seemingly objective statements can beopinion-laden in that they often allude nuancedsentiment of the writer (Greene and Resnik, 2009),or purposefully conjure emotion from the readers?minds (Mohammad and Turney, 2010).
Althoughsome researchers have explored formal and statis-tical treatments of those implicit and implied sen-timents (e.g.
Wiebe et al (2005), Esuli and Sebas-tiani (2006), Greene and Resnik (2009), Davidovet al (2010)), automatic analysis of them largelyremains as a big challenge.In this paper, we concentrate on understandingthe connotative sentiments of words, as they playan important role in interpreting subtle shades ofsentiment beyond denotative or surface meaningof text.
For instance, consider the following:Geothermal replaces oil-heating; it helps re-ducing greenhouse emissions.1Although this sentence could be considered as afactual statement from the general standpoint, thesubtle effect of this sentence may not be entirelyobjective: this sentence is likely to have an influ-ence on readers?
minds in regard to their opiniontoward ?geothermal?.
In order to sense the subtleovertone of sentiments, one needs to know that theword ?emissions?
has generally negative connota-tion, which geothermal reduces.
In fact, depend-ing on the pragmatic contexts, it could be preciselythe intention of the author to transfer his opinioninto the readers?
minds.The main contribution of this paper is a broad-coverage connotation lexicon that determines theconnotative polarity of even those words with everso subtle connotation beneath their surface mean-ing, such as ?Literature?, ?Mediterranean?, and?wine?.
Although there has been a number ofprevious work that constructed sentiment lexicons(e.g., Esuli and Sebastiani (2006), Wilson et al(2005a), Kaji and Kitsuregawa (2007), Qiu etal.
(2009)), which seem to be increasingly andinevitably expanding over words with (strongly)connotative sentiments rather than explicit senti-ments alone (e.g., ?gun?
), little prior work has di-rectly tackled this problem of learning connota-tion,2 and much of the subtle connotation of manyseemingly objective words is yet to be determined.1Our learned lexicon correctly assigns negative polarity toemission.2A notable exception would be the work of Feng et al1774POSITIVE NEGATIVEFEMA, Mandela, Intel, Google, Python, Sony, Pulitzer,Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney,Hoover, Goldman, Swarovski, Hawaii, YellowstoneKatrina, Monsanto, Halliburton, Enron, Teflon, Hi-roshima, Holocaust, Afghanistan, Mugabe, Hutu, Sad-dam, Osama, Qaeda, Kosovo, Helicobacter, HIVTable 1: Example Named Entities (Proper Nouns) with Polar Connotation.A central premise to our approach is that it iscollocational statistics of words that affect andshape the polarity of connotation.
Indeed, the ety-mology of ?connotation?
is from the Latin ?com-?
(?together or with?)
and ?notare?
(?to mark?
).It is important to clarify, however, that we do notsimply assume that words that collocate share thesame polarity of connotation.
Although such anassumption played a key role in previous work forthe analogous task of learning sentiment lexicon(Velikovich et al, 2010), we expect that the sameassumption would be less reliable in drawing sub-tle connotative sentiments of words.
As one ex-ample, the predicate ?cure?, which has a positiveconnotation typically takes arguments with nega-tive connotation, e.g., ?disease?, when used as the?relieve?
sense.3Therefore, in order to attain a broad cover-age lexicon while maintaining good precision, weguide the induction algorithm with multiple, care-fully selected linguistic insights: [1] distributionalsimilarity, [2] semantic parallelism of coordina-tion, [3] selectional preference, and [4] seman-tic prosody (e.g., Sinclair (1991), Louw (1993),Stubbs (1995), Stefanowitsch and Gries (2003))),and also exploit existing lexical resources as an ad-ditional inductive bias.We cast the connotation lexicon induction taskas a collective inference problem, and consider ap-proaches based on three distinct types of algorith-mic framework that have been shown successfulfor conventional sentiment lexicon induction:Random walk based on HITS/PageRank (e.g.,Kleinberg (1999), Page et al (1999), Fenget al (2011) Heerschop et al (2011),Montejo-Ra?ez et al (2012))Label/Graph propagation (e.g., Zhu and Ghahra-(2011) but with practical limitations.
See ?3 for detailed dis-cussion.3Note that when ?cure?
is used as the ?preserve?
sense, itexpects objects with non-negative connotation.
Hence word-sense-disambiguation (WSD) presents a challenge, thoughnot unexpectedly.
In this work, we assume the general conno-tation of each word over statistically prevailing senses, leav-ing a more cautious handling of WSD as future work.mani (2002), Velikovich et al (2010))Constraint optimization (e.g., Roth and Yih(2004), Choi and Cardie (2009), Lu et al(2011)).We provide comparative empirical results overseveral variants of these approaches with compre-hensive evaluations including lexicon-based, hu-man judgments, and extrinsic evaluations.It is worthwhile to note that not all words haveconnotative meanings that are distinct from deno-tational meanings, and in some cases, it can be dif-ficult to determine whether the overall sentiment isdrawn from denotational or connotative meaningsexclusively, or both.
Therefore, we encompass anysentiment from either type of meanings into thelexicon, where non-neutral polarity prevails overneutral one if some meanings lead to neutral whileothers to non-neutral.4Our work results in the first broad-coverageconnotation lexicon,5 significantly improving boththe coverage and the precision of Feng et al(2011).
As an interesting by-product, our algo-rithm can be also used as a proxy to measure thegeneral connotation of real-world named entitiesbased on their collocational statistics.
Table 1highlights some example proper nouns included inthe final lexicon.The rest of the paper is structured as follows.In ?2 we describe three types of induction algo-rithms followed by evaluation in ?3.
Then we re-visit the induction algorithms based on constraintoptimization in ?4 to enhance quality and scala-bility.
?5 presents comprehensive evaluation withhuman judges and extrinsic evaluations.
Relatedwork and conclusion are in ?6 and ?7.4In general, polysemous words do not seem to have con-flicting non-neutral polarities over different senses, thoughthere are many exceptions, e.g., ?heat?, or ?fine?.
We treateach word in each part-of-speech as a separate word to reducesuch cases, otherwise aim to learn the most prevalent polar-ity in the corpus with respect to each part-of-speech of eachword.5Available at http://www.cs.stonybrook.edu/?ychoi/connotation.1775?Pred-ArgArg-Argpred-arg distr simenjoythankwritingprofithelpinvestmentaidreadingFigure 1: Graph for Graph Propagation (?2.2).?
?synonyms antonymspreventsufferenjoythanktaxlosswritingprofitpreventinggaininvestmentbonuspred-arg distr simflucoldFigure 2: Graph for ILP/LP (?2.3, ?4.2).2 Connotation Induction AlgorithmsWe develop induction algorithms based on threedistinct types of algorithmic framework that havebeen shown successful for the analogous task ofsentiment lexicon induction: HITS & PageRank(?2.1), Label/Graph Propagation (?2.2), and Con-straint Optimization via Integer Linear Program-ming (?2.3).
As will be shown, each of these ap-proaches will incorporate additional, more diverselinguistic insights.2.1 HITS & PageRankThe work of Feng et al (2011) explored the useof HITS (Kleinberg, 1999) and PageRank (Pageet al, 1999) to induce the general connotationof words hinging on the linguistic phenomena ofselectional preference and semantic prosody, i.e.,connotative predicates influencing the connotationof their arguments.
For example, the object ofa negative connotative predicate ?cure?
is likelyto have negative connotation, e.g., ?disease?
or?cancer?.
The bipartite graph structure for thisapproach corresponds to the left-most box (labeledas ?pred-arg?)
in Figure 1.2.2 Label PropagationWith the goal of obtaining a broad-coverage lexi-con in mind, we find that relying only on the struc-ture of semantic prosody is limiting, due to rel-atively small sets of connotative predicates avail-able.6 Therefore, we extend the graph structureas an overlay of two sub-graphs (Figure 1) as de-scribed below:6For connotative predicates, we use the seed predicate setof Feng et al (2011), which comprises of 20 positive and 20negative predicates.Sub-graph #1: Predicate?Argument GraphThis sub-graph is the bipartite graph that encodesthe selectional preference of connotative predi-cates over their arguments.
In this graph, conno-tative predicates p reside on one side of the graphand their co-occurring arguments a reside on theother side of the graph based on Google Web 1Tcorpus.7 The weight on the edges between thepredicates p and arguments a are defined usingPoint-wise Mutual Information (PMI) as follows:w(p?
a) := PMI(p, a) = log2P (p, a)P (p)P (a)PMI scores have been widely used in previousstudies to measure association between words(e.g., Turney (2001), Church and Hanks (1990)).Sub-graph #2: Argument?Argument GraphThe second sub-graph is based on the distribu-tional similarities among the arguments.
One pos-sible way of constructing such a graph is simplyconnecting all nodes and assign edge weights pro-portionate to the word association scores, such asPMI, or distributional similarity.
However, such acompletely connected graph can be susceptible topropagating noise, and does not scale well over avery large set of vocabulary.We therefore reduce the graph connectivity byexploiting semantic parallelism of coordination(Bock (1986), Hatzivassiloglou and McKeown7We restrict predicte-argument pairs to verb-object pairsin this study.
Note that Google Web 1T dataset consists ofn-grams upto n = 5.
Since n-gram sequences are too shortto apply a parser, we extract verb-object pairs approximatelyby matching part-of-speech tags.
Empirically, when overlaidwith the second sub-graph, we found that it is better to keepthe connectivity of this sub-graph as uni-directional.
That is,we only allow edges to go from a predicate to an argument.1776POSITIVE NEGATIVE NEUTRALn.
avatar, adrenaline, keynote, debut,stakeholder, sunshine, cooperationunbeliever, delay, shortfall, gun-shot, misdemeanor, mutiny, rigorheader, mark, clothing, outline,grid, gasoline, course, previewv.
handcraft, volunteer, party, ac-credit, personalize, nurse, googlesentence, cough, trap, scratch, de-bunk, rip, misspell, overchargestate, edit, send, put, arrive, type,drill, name, stay, echo, registera.
floral, vegetarian, prepared, age-less, funded, contemporarydebilitating, impaired, swollen,intentional, jarring, unearnedsame, cerebral, west, uncut, auto-matic, hydrated, unheated, routineTable 2: Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a).
(1997), Pickering and Branigan (1998)).
In par-ticular, we consider an undirected edge between apair of arguments a1 and a2 only if they occurredtogether in the ?a1 and a2?
or ?a2 and a1?
coor-dination, and assign edge weights as:w(a1 ?
a2) = CosineSim(??a1,?
?a2) =?
?a1 ?
??a2||?
?a1|| ||?
?a2||where ?
?a1 and ?
?a2 are co-occurrence vectors for a1and a2 respectively.
The co-occurrence vector foreach word is computed using PMI scores with re-spect to the top n co-occurring words.8 n (=50)is selected empirically.
The edge weights in twosub-graphs are normalized so that they are in thecomparable range.9Limitations of Graph-based AlgorithmsAlthough graph-based algorithms (?2.1, ?2.2) pro-vide an intuitive framework to incorporate variouslexical relations, limitations include:1.
They allow only non-negative edge weights.Therefore, we can encode only positive (sup-portive) relations among words (e.g., distri-butionally similar words will endorse eachother with the same polarity), while miss-ing on exploiting negative relations (e.g.,antonyms may drive each other into the op-posite polarity).2.
They induce positive and negative polaritiesin isolation via separate graphs.
However, weexpect that a more effective algorithm shouldinduce both polarities simultaneously.3.
The framework does not readily allow incor-porating a diverse set of soft and hard con-straints.8We discard edges with cosine similarity ?
0, as thoseindicate either independence or the opposite of similarity.9Note that cosine similarity does not make sense for thefirst sub-graph as there is no reason why a predicate and an ar-gument should be distributionally similar.
We experimentedwith many different variations on the graph structure andedge weights, including ones that include any word pairs thatoccurred frequently enough together.
For brevity, we presentthe version that achieved the best results here.2.3 Constraint OptimizationAddressing limitations of graph-based algorithms(?2.2), we propose an induction algorithm basedon Integer Linear Programming (ILP).
Figure 2provides the pictorial overview.
In comparison toFigure 1, two new components are: (1) dictionary-driven relations targeting enhanced precision, and(2) dictionary-driven words (i.e., unseen wordswith respect to those relations explored in Figure1) targeting enhanced coverage.
We formulate in-sights in Figure 2 using ILP as follows:Definition of sets of words:1.
P+: the set of positive seed predicates.P?
: the set of negative seed predicates.2.
S: the set of seed sentiment words.3.
Rsyn: word pairs in synonyms relation.Rant: word pairs in antonyms relation.Rcoord: word pairs in coordination relation.Rpred: word pairs in pred-arg relation.Rpred+(?)
: Rpred based on P+ (P?
).Definition of variables: For each word i, wedefine binary variables xi, yi, zi ?
{0, 1}, wherexi = 1 (yi = 1, zi = 1) if and only if i has a pos-itive (negative, neutral) connotation respectively.For every pair of word i and j, we define binaryvariables dpqij where p, q ?
{+,?, 0} and dpqij = 1if and only if the polarity of i and j are p and qrespectively.Objective function: We aim to maximize:F = ?prosody + ?coord + ?neuwhere ?prosody is the scores based on semanticprosody, ?coord captures the distributional similar-ity over coordination, and ?neu controls the sen-sitivity of connotation detection between positive(negative) and neutral.
In particular,?prosody =Rpred?i,jwpredi,j (d++i,j + d?
?i,j ?
d+?i,j ?
d?+i,j )?coord =Rcoord?i,jwcoordi,j (d++i,j + d?
?i,j + d00i,j)1777?neu = ?Rpred?i,jwpredi,j ?
zjSoft constraints (edge weights): The weights inthe objective function are set as follows:wpred(p, a) = freq(p, a)?
(p,x)?Rpredfreq(p, x)wcoord(a1, a2) = CosSim(??a1,?
?a2) =?
?a1 ?
??a2||?
?a1|| ||?
?a2||Note that the same wcoord(a1, a2) has been usedin graph propagation described in Section 2.2.
?controls the sensitivity of connotation detectionsuch that higher value of ?
will promote neutralconnotation over polar ones.Hard constrains for variable consistency:1.
Each word i has one of {+,?, ?}
as polarity:?i, xi + yi + zi = 12.
Variable consistency between dpqij andxi, yi, zi:xi + xj ?
1 ?
2d++i,j ?
xi + xjyi + yj ?
1 ?
2d?
?i,j ?
yi + yjzi + zj ?
1 ?
2d00i,j ?
zi + zjxi + yj ?
1 ?
2d+?i,j ?
xi + yjyi + xj ?
1 ?
2d?+i,j ?
yi + xjHard constrains for WordNet relations:1.
Cant: Antonym pairs will not have the samepositive or negative polarity:?
(i, j) ?
Rant, xi + xj ?
1, yi + yj ?
1For this constraint, we only considerantonym pairs that share the same root, e.g.,?sufficient?
and ?insufficient?, as those pairsare more likely to have the opposite polaritiesthan pairs without sharing the same root, e.g.,?east?
and ?west?.2.
Csyn: Synonym pairs will not have the oppo-site polarity:?
(i, j) ?
Rsyn, xi + yj ?
1, xj + yi ?
13 Experimental Result IWe provide comprehensive comparisons over vari-ants of three types of algorithms proposed in ?2.We use the Google Web 1T data (Brants and Franz(2006)), and POS-tagged ngrams using StanfordPOS Tagger (Toutanova and Manning (2000)).
Wefilter out the ngrams with punctuations and otherspecial characters to reduce the noise.3.1 Comparison against ConventionalSentiment LexiconNote that we consider the connotation lexicon tobe inclusive of a sentiment lexicon for two prac-tical reasons: first, it is highly unlikely that anyword with non-neutral sentiment (i.e., positive ornegative) would carry connotation of the oppo-site, i.e., conflicting10 polarity.
Second, for somewords with distinct sentiment or strong connota-tion, it can be difficult or even unnatural to draw aprecise distinction between connotation and senti-ment, e.g., ?efficient?.
Therefore, sentiment lexi-cons can serve as a surrogate to measure a subsetof connotation words induced by the algorithms,as shown in Table 3 with respect to General In-quirer (Stone and Hunt (1963)) and MPQA (Wil-son et al (2005b)).11Discussion Table 3 shows the agreement statis-tics with respect to two conventional sentimentlexicons.
We find that the use of label propaga-tion alone [PRED-ARG (CP)] improves the per-formance substantially over the comparable graphconstruction with different graph analysis algo-rithms, in particular, HITS and PageRank ap-proaches of Feng et al (2011).
The two com-pletely connected variants of the graph propa-gation on the Pred-Arg graph, [?
PRED-ARG(PMI)] and [?
PRED-ARG (CP)], do not neces-sarily improve the performance over the simplerand computationally lighter alternative, [PRED-ARG (CP)].
The [OVERLAY], which is basedon both Pred-Arg and Arg-Arg subgraphs (?2.2),achieves the best performance among graph-basedalgorithms, significantly improving the precisionover all other baselines.
This result suggests:1 The sub-graph #2, based on the semantic par-allelism of coordination, is simple and yetvery powerful as an inductive bias.2 The performance of graph propagation variessignificantly depending on the graph topol-ogy and the corresponding edge weights.Note that a direct comparison against ILP for topN words is tricky, as ILP does not rank results.Only for comparison purposes however, we assign10We consider ?positive?
and ?negative?
polarities conflict,but ?neutral?
polarity does not conflict with any.11In the case of General Inquirer, we use words in POSITIVand NEGATIV sets as words with positive and negative labelsrespectively.1778GENINQ EVAL MPQA EVAL100 1,000 5,000 10,000 ALL 100 1,000 5,000 10,000 ALLILP 97.6 94.5 84.5 80.8 80.4 98.0 89.7 84.6 81.2 78.4OVERLAY 97.0 95.1 78.8 (78.3) 78.3 98.0 93.4 82.1 77.7 77.7?
PRED-ARG (PMI) 91.0 91.4 76.1 (76.1) 76.1 88.0 89.1 78.8 75.1 75.1?PRED-ARG (CP) 88.0 85.4 76.2 (76.2) 76.2 87.0 82.6 78.0 76.3 76.3PRED-ARG (CP) 91.0 91.0 81.0 (81.0) 81.0 88.0 91.5 80.0 78.3 78.3HITS-ASYMT 77.0 68.8 - - 66.5 86.3 81.3 - - 72.2PAGERANK-ASYMF 77.0 68.5 - - 65.7 87.2 80.3 - - 72.3Table 3: Evaluation of Induction Algorithms (?2) with respect to Sentiment Lexicons (precision%).ranks based on the frequency of words for ILP.
Be-cause of this issue, the performance of top ?1kwords of ILP should be considered only as a con-servative measure.
Importantly, when evaluatedover more than top 5k words, ILP is overall thetop performer considering both precision (shownin Table 3) and coverage (omitted for brevity).124 Precision, Coverage, and EfficiencyIn this section, we address three important aspectsof an ideal induction algorithm: precision, cover-age, and efficiency.
For brevity, the remainder ofthe paper will focus on the algorithms based onconstraint optimization, as it turned out to be themost effective one from the empirical results in ?3.Precision In order to see the effectiveness of theinduction algorithms more sharply, we had used alimited set of seed words in ?3.
However to build alexicon with substantially enhanced precision, wewill use as a large seed set as possible, e.g., entiresentiment lexicons13.Broad coverage Although statistics in Google1T corpus represent a very large amount of text,words that appear in pred-arg and coordination re-lations are still limited.
To substantially increasethe coverage, we will leverage dictionary words(that are not in the corpus) as described in ?2.3and Figure 2.Efficiency One practical problem with ILP is ef-ficiency and scalability.
In particular, we foundthat it becomes nearly impractical to run the ILPformulation including all words in WordNet plusall words in the argument position in Google Web1T.
We therefore explore an alternative approachbased on Linear Programming in what follows.12In fact, the performance of PRED-ARG variants for top10K w.r.t.
GENINQ is not meaningful as no additional wordwas matched beyond top 5k words.13Note that doing so will prevent us from evaluatingagainst the same sentiment lexicon used as a seed set.4.1 Induction using Linear ProgrammingOne straightforward option for Linear Program-ming formulation may seem like using the sameInteger Linear Programming formulation intro-duced in ?2.3, only changing the variable defini-tions to be real values ?
[0, 1] rather than integers.However, because the hard constraints in ?2.3 aredefined based on the assumption that all the vari-ables are binary integers, those constraints are notas meaningful when considered for real numbers.Therefore we revise those hard constraints to en-code various semantic relations (WordNet and se-mantic coordination) more directly.Definition of variables: For each word i, we de-fine variables xi, yi, zi ?
[0, 1].
i has a positive(negative) connotation if and only if the xi (yi) isassigned the greatest value among the three vari-ables; otherwise, i is neutral.Objective function: We aim to maximize:F = ?prosody + ?coord + ?syn + ?ant + ?neu?prosody =Rpred+?i,jwpred+i,j ?
xj +Rpred?
?i,jwpred?i,j ?
yj?coord =Rcoord?i,jwcoordi,j ?
(dc++i,j + dc?
?i,j )?syn = W synRsyn?i,j(ds++i,j + ds?
?i,j )?ant = W antRant?i,j(da++i,j + da?
?i,j )?neu = ?Rpred?i,jwpredi,j ?
zjHard constraints We add penalties to theobjective function if the polarity of a pair of wordsis not consistent with its corresponding semanticrelations.
For example, for synonyms i and j, weintroduce a penalty W syn (a positive constant) fords++i,j , ds?
?i,j ?
[?1, 0], where we set the upperbound of ds++i,j (ds?
?i,j ) as the signed distance of1779FORMULA POSITIVE NEGATIVE ALLR P F R P F R P FILP ?prosody + Csyn + Cant 51.4 85.7 64.3 44.7 87.9 59.3 48.0 86.8 61.8?prosody + Csyn + Cant + CS 61.2 93.3 73.9 52.4 92.2 66.8 56.8 92.8 70.5?prosody + ?coord + Csyn + Cant 67.3 75.0 70.9 53.7 84.4 65.6 60.5 79.7 68.8?prosody + ?coord + Csyn + Cant + CS 62.2 96.0 75.5 51.5 89.5 65.4 56.9 92.8 70.5LP ?prosody + ?syn + ?ant 24.4 76.0 36.9 23.6 78.8 36.3 24.0 77.4 36.6?prosody + ?syn + ?ant + ?S 71.6 87.8 78.9 68.8 84.6 75.9 70.2 86.2 77.4?prosody + ?coord + ?syn + ?ant 67.9 92.6 78.3 64.6 89.1 74.9 66.3 90.8 76.6?prosody + ?coord + ?syn + ?ant + ?S 78.6 90.5 84.1 73.3 87.1 79.6 75.9 88.8 81.8Table 4: ILP/LP Comparison on MQPA?
(%).xi and xj (yi and yj) as shown below:For (i, j) ?
Rsyn,ds++i,j ?
xi ?
xj , ds++i,j ?
xj ?
xids?
?i,j ?
yi ?
yj , ds?
?i,j ?
yj ?
yiNotice that ds++i,j , ds?
?i,j satisfying above inequal-ities will be always of negative values, hence inorder to maximize the objective function, the LPsolver will try to minimize the absolute values ofds++i,j , ds?
?i,j , effectively pushing i and j towardthe same polarity.
Constraints for semantic coor-dination Rcoord can be defined similarly.
Lastly,following constraints encode antonym relations:For (i, j) ?
Rant ,da++i,j ?
xi ?
(1?
xj), da++i,j ?
(1?
xj)?
xida?
?i,j ?
yi ?
(1?
yj), da?
?i,j ?
(1?
yj)?
yiInterpretation Unlike ILP, some of the vari-ables result in fractional values.
We consider aword has positive or negative polarity only if theassignment indicates 1 for the corresponding po-larity and 0 for the rest.
In other words, we treatall words with fractional assignments over differ-ent polarities as neutral.
Because the optimal so-lutions of LP correspond to extreme points in theconvex polytope formed by the constraints, we ob-tain a large portion of words with non-fractionalassignments toward non-neutral polarities.
Alter-natively, one can round up fractional values.4.2 Empirical Comparisons: ILP v.s.
LPTo solve the ILP/LP, we run ILOG CPLEX Opti-mizer (CPLEX, 2009)) on a 3.5GHz 6 core CPUmachine with 96GB RAM.
Efficiency-wise, LPruns within 10 minutes while ILP takes severalhours.
Table 4 shows the results evaluated againstMPQA for different variations of ILP and LP.We find that LP variants much better recall andF-score, while maintaining comparable precision.Therefore, we choose the connotation lexicon byLP (C-LP) in the following evaluations in ?5.5 Experimental Results IIIn this section, we present comprehensive intrin-sic ?5.1 and extrinsic ?5.2 evaluations comparingthree representative lexicons from ?2 & ?4: C-LP, OVERLAY, PRED-ARG (CP), and two popularsentiment lexicons: SentiWordNet (Baccianella etal., 2010) and GI+MPQA.14 Note that C-LP is thelargest among all connotation lexicons, including?70,000 polar words.155.1 Intrinsic Evaluation: Human JudgementsWe evaluate 4000 words16 using Amazon Me-chanical Turk (AMT).
Because we expect thatjudging a connotation can be dependent on one?scultural background, personality and value sys-tems, we gather judgements from 5 people foreach word, from which we hope to draw a moregeneral judgement of connotative polarity.
About300 unique Turkers participated the evaluationtasks.
We gather gold standard only for thosewords for which more than half of the judgesagreed on the same polarity.
Otherwise we treatthem as ambiguous cases.17 Figure 3 shows a partof the AMT task, where Turkers are presented withquestions that help judges to determine the subtleconnotative polarity of each word, then asked torate the degree of connotation on a scale from -5 (most negative) and 5 (most positive).
To draw14GI+MPQA is the union of General Inquirer and MPQA.The GI, we use words in the ?Positiv?
& ?Negativ?
set.
ForSentiWordNet, to retrieve the polarity of a given word, wesum over the polarity scores over all senses, where positive(negative) values correspond to positive (negative) polarity.15?13k adj, ?6k verbs, ?28k nouns, ?22k proper nouns.16We choose words that are not already in GI+MPQA andobtain most frequent 10,000 words based on the unigram fre-quency in Google-Ngram, then randomly select 4000 words.17We allow Turkers to mark words that can be used withboth positive and negative connotation, which results in about7% of words that are excluded from the gold standard set.1780Figure 3: A Part of AMT Task Design.YES NOQUESTION % Avg % Avg?Enjoyable or pleasant?
43.3 2.9 16.3 -2.4?Of a good quality?
56.7 2.5 6.1 -2.7?Respectable / honourable?
21.0 3.3 14.0 -1.1?Would like to do or have?
52.5 2.8 11.5 -2.4Table 5: Distribution of Answers from AMT.the gold standard, we consider two different votingschemes:?
?V ote: The judgement of each Turker ismapped to neutral for ?1 ?
score ?
1, pos-itive for score ?
2, negative for score ?
2,then we take the majority vote.?
?Score: Let ?
(i) be the sum (weighted vote)of the scores given by 5 judges for word i.Then we determine the polarity label l(i) of ias:l(i) =??
?positive if ?
(i) > 1negative if ?
(i) < ?1neutral if ?1 ?
?
(i) ?
1The resulting distribution of judgements is shownin Table 5 & 6.
Interestingly, we observethat among the relatively frequently used Englishwords, there are overwhelmingly more positivelyconnotative words than negative ones.In Table 7, we show the percentage of wordswith the same label over the mutual words by thetwo lexicon.
The highest agreement is 77% byC-LP and the gold standard by AMTV ote.
Howgood is this?
It depends on what is the natural de-gree of agreement over subtle connotation amongpeople.
Therefore, we also report the degree ofagreement among human judges in Table 7, wherewe compute the agreement of one Turker with re-spect to the gold standard drawn from the rest ofthe Turkers, and take the average across over allfive Turkers18.
Interestingly, the performance of18In order to draw the gold standard from the 4 remainingTurkers, we consider adjusted versions of ?V ote and ?Scoreschemes described above.POS NEG NEU UNDETERMINED?V ote 50.4 14.6 24.1 10.9?Score 67.9 20.6 11.5 n/aTable 6: Distribution of Connotative Polarity fromAMT.C-LP SENTIWN HUMAN JUDGES?V ote 77.0 71.5 66.0?Score 73.0 69.0 69.0Table 7: Agreement (Accuracy) against AMT-driven Gold Standard.Turkers is not as good as that of C-LP lexicon.
Weconjecture that this could be due to generally vary-ing perception of different people on the connota-tive polarity,19 while the corpus-driven inductionalgorithms focus on the general connotative po-larity corresponding to the most prevalent sensesof words in the corpus.5.2 Extrinsic EvaluationWe conduct lexicon-based binary sentiment clas-sification on the following two corpora.SemEval From the SemEval task, we obtain aset of news headlines with annotated scores (rang-ing from -100 to 87).
The positive/negative scoresindicate the degree of positive/negative polarityorientation.
We construct several sets of the posi-tive and negative texts by setting thresholds on thescores as shown in Table 8.
??
n?
indicates thatthe positive set consists of the texts with scores?
n and the negative set consists of the texts withscores ?
?n.Emoticon tweets The sentiment Twitter data20consists of tweets containing either a smileyemoticon (positive sentiment) or a frowny emoti-con (negative sentiment).
We filter out the tweetswith question marks or more than 30 words, andkeep the ones with at least two words in the unionof all polar words in the five lexicons in Table 8,and then randomly select 10000 per class.We denote the short text (e.g., content of tweetsor headline texts from SemEval) by t. w repre-sents the word in t. W+/W?
is the set of posi-19Pearson correlation coefficient among turkers is 0.28,which corresponds to a positive small to medium correlation.Note that when the annotation of turkers is aggregated, weobserve agreement as high as 77% with respect to the learnedconnotation lexicon.20http://www.stanford.edu/?alecmgo/cs224n/twitterdata.2009.05.25.c.zip1781DATALEXICON TWEET SEMEVAL?20 ?40 ?60 ?80C-LP 70.1 70.8 74.6 80.8 93.5OVERLAY 68.5 70.0 72.9 76.8 89.6PRED-ARG (CP) 60.5 64.2 69.3 70.3 79.2SENTIWN 67.4 61.0 64.5 70.5 79.0GI+MPQA 65.0 64.5 69.0 74.0 80.5Table 8: Accuracy on Sentiment Classification(%).tive/negative words of the lexicon.
We define theweight of w as s(w).
If w is adjective, s(w) = 2;otherwise s(w) = 1.
Then the polarity of each textis determined as follows:pol(t) =????????
?positive ifW+?w?ts(w) ?W?
?w?ts(w)negative ifW+?w?ts(w) <W?
?w?ts(w)As shown in Table 8, C-LP generally performsbetter than the other lexicons on both corpora.Considering that only very simple classificationstrategy is applied, the result by the connotationlexicon is quite promising.Finally, Table 1 highlights interesting exam-ples of proper nouns with connotative polarity,e.g., ?Mandela?, ?Google?, ?Hawaii?
with pos-itive connotation, and ?Monsanto?, ?Hallibur-ton?, ?Enron?
with negative connotation, sug-gesting that our algorithms could potentially serveas a proxy to track the general connotation of realworld entities.
Table 2 shows example commonnouns with connotative polarity.5.3 Practical Remarks on WSD and MWEsIn this work we aim to find the polarity of mostprevalent senses of each word, in part because itis not easy to perform unsupervised word sensedisambiguation (WSD) on a large corpus in a reli-able way, especially when the corpus consists pri-marily of short n-grams.
Although the resultinglexicon loses on some of the polysemous wordswith potentially opposite polarities, per-word con-notation (rather than per-sense connotation) doeshave a practical value: it provides a convenientoption for users who wish to avoid the burden ofWSD before utilizing the lexicon.
Future work in-cludes handling of WSD and multi-word expres-sions (MWEs), e.g., ?Great Leader?
(for KimJong-Il), ?Inglourious Basterds?
(a movie title).2121These examples credit to an anonymous reviewer.6 Related WorkA very interesting work of Mohammad and Tur-ney (2010) uses Mechanical Turk in order to buildthe lexicon of emotions evoked by words.
In con-trast, we present an automatic approach that in-fers the general connotation of words.
Velikovichet al (2010) use graph propagation algorithms forconstructing a web-scale polarity lexicon for sen-timent analysis.
Although we employ the samegraph propagation algorithm, our graph construc-tion is fundamentally different in that we integratestronger inductive biases into the graph topologyand the corresponding edge weights.
As shownin our experimental results, we find that judiciousconstruction of graph structure, exploiting multi-ple complementing linguistic phenomena can en-hance both the performance and the efficiency ofthe algorithm substantially.
Other interesting ap-proaches include one based on min-cut (Dong etal., 2012) or LDA (Xie and Li, 2012).
Our pro-posed approaches are more suitable for encodinga much diverse set of linguistic phenomena how-ever.
But our work use a few seed predicates withselectional preference instead of relying on wordsimilarity.
Some recent work explored the useof constraint optimization framework for inducingdomain-dependent sentiment lexicon (Choi andCardie (2009), Lu et al (2011)).
Our work dif-fers in that we provide comprehensive insights intodifferent formulations of ILP and LP, aiming tolearn the much different task of learning the gen-eral connotation of words.7 ConclusionWe presented a broad-coverage connotation lexi-con that determines the subtle nuanced sentimentof even those words that are objective on the sur-face, including the general connotation of real-world named entities.
Via a comprehensive eval-uation, we provided empirical insights into threedifferent types of induction algorithms, and pro-posed one with good precision, coverage, and effi-ciency.AcknowledgmentsThis research was supported in part by the StonyBrook University Office of the Vice President forResearch.
We thank reviewers for many insightfulcomments and suggestions, and for providing uswith several very inspiring examples to work with.1782ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).J.
Kathryn Bock.
1986.
Syntactic persistencein language production.
Cognitive psychology,18(3):355?387.Thorsten Brants and Alex Franz.
2006.
{Web 1T 5-gram Version 1}.Yejin Choi and Claire Cardie.
2009.
Adapting a po-larity lexicon using integer linear programming fordomain-specific sentiment classification.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 2 -Volume 2, EMNLP ?09, pages 590?598, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Comput.
Linguist., 16:22?29, March.ILOG CPLEX.
2009.
High-performance software formathematical programming and optimization.
U RLhttp://www.ilog.com/products/cplex.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcastic sentencesin twitter and amazon.
In Proceedings of theFourteenth Conference on Computational NaturalLanguage Learning, CoNLL ?10, pages 107?116,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Xishuang Dong, Qibo Zou, and Yi Guan.
2012.
Set-similarity joins based semi-supervised sentimentanalysis.
In Neural Information Processing, pages176?183.
Springer.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A publicly available lexical resourcefor opinion mining.
In In Proceedings of the 5thConference on Language Resources and Evaluation(LREC06), pages 417?422.Song Feng, Ritwik Bose, and Yejin Choi.
2011.
Learn-ing general connotation of words using graph-basedalgorithms.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 1092?1103.
Association for Computa-tional Linguistics.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 503?511, Boulder, Colorado, June.Association for Computational Linguistics.Vasileios Hatzivassiloglou and Kathleen R McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the eighth conference onEuropean chapter of the Association for Computa-tional Linguistics, pages 174?181.
Association forComputational Linguistics.Bas Heerschop, Alexander Hogenboom, and FlaviusFrasincar.
2011.
Sentiment lexicon creation fromlexical resources.
In Business Information Systems,pages 185?196.
Springer.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massive col-lection of html documents.
In Proceedings of the2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).Jon M. Kleinberg.
1999.
Authoritative sources in a hy-perlinked environment.
JOURNAL OF THE ACM,46(5):604?632.Bill Louw.
1993.
Irony in the text or insincerity inthe writer.
Text and technology: In honour of JohnSinclair, pages 157?176.Yue Lu, Malu Castellanos, Umeshwar Dayal, andChengXiang Zhai.
2011.
Automatic constructionof a context-aware sentiment lexicon: an optimiza-tion approach.
In Proceedings of the 20th interna-tional conference on World wide web, pages 347?356.
ACM.Saif Mohammad and Peter Turney.
2010.
Emotionsevoked by common words and phrases: Using me-chanical turk to create an emotion lexicon.
In Pro-ceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 26?34, Los Angeles,CA, June.
Association for Computational Linguis-tics.Arturo Montejo-Ra?ez, Eugenio Mart??nez-Ca?mara,M.
Teresa Mart?
?n-Valdivia, and L. Alfonso Uren?aLo?pez.
2012.
Random walk weighting over sen-tiwordnet for sentiment polarity detection on twit-ter.
In Proceedings of the 3rd Workshop in Com-putational Approaches to Subjectivity and SentimentAnalysis, pages 3?10, Jeju, Korea, July.
Associationfor Computational Linguistics.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Technical Report1999-66, Stanford InfoLab, November.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135.Martin J Pickering and Holly P Branigan.
1998.
Therepresentation of verbs: Evidence from syntacticpriming in language production.
Journal of Mem-ory and Language, 39(4):633?651.1783Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding domain sentiment lexicon throughdouble propagation.
In Proceedings of the 21st in-ternational jont conference on Artifical intelligence,IJCAI?09, pages 1199?1204, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
Defense Technical Information Center.John Sinclair.
1991.
Corpus, concordance, colloca-tion.
Describing English language.
Oxford Univer-sity Press.Anatol Stefanowitsch and Stefan Th Gries.
2003.
Col-lostructions: Investigating the interaction of wordsand constructions.
International journal of corpuslinguistics, 8(2):209?243.Philip J.
Stone and Earl B.
Hunt.
1963.
A computerapproach to content analysis: studies using the gen-eral inquirer system.
In Proceedings of the May 21-23, 1963, spring joint computer conference, AFIPS?63 (Spring), pages 241?256, New York, NY, USA.ACM.Michael Stubbs.
1995.
Collocations and semantic pro-files: on the cause of the trouble with quantitativestudies.
Functions of language, 2(1):23?55.Kristina Toutanova and Christopher D. Manning.2000.
Enriching the knowledge sources used ina maximum entropy part-of-speech tagger.
In InEMNLP/VLC 2000, pages 63?70.Peter Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.Leonid Velikovich, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The via-bility of web-derived polarity lexicons.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics.
Association forComputational Linguistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Eval-uation (formerly Computers and the Humanities),39(2/3):164?210.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005a.
Opinionfinder: a system for subjec-tivity analysis.
In Proceedings of HLT/EMNLP onInteractive Demonstrations, pages 34?35, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT ?05: Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 347?354, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Rui Xie and Chunping Li.
2012.
Lexicon construc-tion: A topic model approach.
In Systems and Infor-matics (ICSAI), 2012 International Conference on,pages 2299?2303.
IEEE.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learn-ing from labeled and unlabeled data with label prop-agation.
In Technical Report CMU-CALD-02-107.CarnegieMellon University.1784
