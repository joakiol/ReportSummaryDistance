Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 348?357,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsEmpirically-motivated Generalizations of CCG Semantic ParsingLearning AlgorithmsJesse GlassTemple University1801 N Broad StreetPhiladelphia, PA 19122, USAjglassemc2@gmail.comAlexander YatesTemple University1801 N Broad StreetPhiladelphia, PA 19122, USAayates@gmail.comAbstractLearning algorithms for semantic parsinghave improved drastically over the pastdecade, as steady improvements on bench-mark datasets have shown.
In this pa-per we investigate whether they can gen-eralize to a novel biomedical dataset thatdiffers in important respects from the tra-ditional geography and air travel bench-mark datasets.
Empirical results for twostate-of-the-art PCCG semantic parsers in-dicates that learning algorithms are sensi-tive to the kinds of semantic and syntac-tic constructions used in a domain.
In re-sponse, we develop a novel learning algo-rithm that can produce an effective seman-tic parser for geography, as well as a muchbetter semantic parser for the biomedicaldataset.1 IntroductionSemantic parsing is the task of converting nat-ural language utterances into formal representa-tions of their meaning.
In this paper, we considerin particular a grounded form of semantic pars-ing, in which the meaning representation languagetakes its logical constants from a given, fixed on-tology.
Several recent systems have demonstratedthe ability to learn semantic parsers for domainslike the GeoQuery database containing geographyrelations, or the ATIS database of air travel infor-mation.
In these settings, existing systems canproduce correct meaning representations with F1scores approaching 0.9 (Wong and Mooney, 2007;Kwiatkowski et al., 2011).These benchmark datasets have supported a di-verse and influential line of research into semanticparsing learning algorithms for sophisticated se-mantic constructions, with continuing advances inaccuracy.
However, the focus on these datasetsleads to a natural question ?
do other naturaldatasets have similar syntax and semantics, and ifnot, can existing algorithms handle the variabilityin syntax and semantics?In an effort to investigate and improve thegeneralization capacity of existing learning algo-rithms for semantic parsing, we develop a novel,natural experimental setting, and we test whethercurrent semantic parsers generalize to the new set-ting.
For our datset, we use descriptions of clin-ical trials of experimental drugs in the UnitedStates, available from the U.S. National Insti-tutes of Health1.
Much of the text in the de-scription of these clinical trials can be mappedneatly onto biomedical ontologies, thus permittinggrounded semantic analysis.
Crucially, the datasetwas not designed specifically with semantic pars-ing or question-answering in mind, and as a re-sult, it provides a natural source for the varietyand complexity of utterances that humans use inthis domain.
As an added benefit, a successfulsemantic parser in this domain could yield a va-riety of useful bioinformatics applications by per-mitting comparisons between and across clinicaltrials using structured representations of the data,rather than unstructured text.In this initial investigation of semantic parsingin this context, we ask:?
Can existing semantic parsing learning al-gorithms handle the variety and complexityof the clinical trials dataset?
We show thattwo representative learning algorithms farepoorly on the clinical trials data: the best oneachieves a 0.41 F1 in our tests.?
What types of constructions are the majorcause of errors on the clinical trials dataset,1clinicaltrials.gov348and can semantic parsers be extended to han-dle them?
While this initial investigationdoes not cover all types of constructions, weidentify three important types of construc-tions that existing learning algorithms do nothandle.
We propose a new learning algorithmthat can handle these types of constructions,and we demonstrate empirically that the newalgorithm produces a semantic parser that im-proves by over 23 points in F1 on the clinicaltrials dataset compared with existing parsers.The rest of this paper is organized as follows.The next section provides background informationon CCG and semantic parsing.
Section 3 describesthe text and ontology that form the new clinicaltrials dataset for semantic parsing, as well as someof the problems that exising approaches have onthis dataset.
Sections 4 describes our semanticparsing model, and learning and inference algo-rithms.
Section 5 presents our experiments and re-sults, and Section 6 concludes.2 Background on Semantic Parsing withCCGOur approach to learning a semantic parserfalls into the general framework of context-freeProbabilistic Combinatory Categorial Grammars(PCCG) (Zettlemoyer and Collins, 2005) withtyped lambda calculus expressions for the seman-tics.
PCCG grammars involve lexical entries,which are weighted unary rewrite rules of the formSyntax : Semantics?
Phrase.
For example:Example Lexical EntriesNP : melanoma?
skin cancerS\NP : ?p?d.has condition(p, d)?patients withIn addition to lexical rules, PCCG grammars in-volve weighted binary rewrite rules like the fol-lowing:Example CCG Grammar RulesX : f(g)?
X/Y : f Y : g (function application)X : f(g)?
Y : g X\Y : f (backward application)These rules apply for any syntactic categories Xand Y , and any logical forms f and g. The rulesspecify mechanisms for deducing syntactic cate-gories for whole phrases based on their constituentparts.
They also specify mechanisms for identify-ing semantics (logical forms) for phrases and sen-tences based on combinations of the semantics forthe constituent parts.
Besides function application,other ways to combine the semantics of childrentypically include conjunction, disjunction, func-tion composition, and substitution, among others.Inference algorithms for PCCG can identify thebest parse and logical form for a given sentence us-ing standard dynamic programming algorithms forcontext-free grammars (Clark and Curran, 2007).As a baseline in our experiments, we use alearning algorithm for semantic parsing known asUnification Based Learning (UBL) (Kwiatkowskiet al., 2010).
Source code for UBL is freelyavailable.
Its authors found that the semanticparsers it learns achieve results competitive withthe state-of-the-art on a variety of standard se-mantic parsing data sets, including GeoQuery(0.882 F1).
UBL uses a log-linear probabilis-tic model P (L, T |S) over logical forms L andparse tree derivations T , given sentences S. Dur-ing training, only S and L are observed, andUBL?s gradient-based parameter estimation algo-rithm tries to maximize?TP (L, T |S) over thetraining dataset.
To learn lexicon entries, it adoptsa search procedure that involves unification inhigher-order logic.
The objective of the searchprocedure is to identify lexical entries for thewords in a sentence that, when combined with thelexical entries for other words in the sentence, willproduce the observed logical form in the trainingdata.
For each training sentence, UBL heuristi-cally explores the space of all possible lexical en-tries to produce a set of promising candidates, andadds them to the lexicon.Our second baseline is an extension of thiswork, called Factored Unification Based Learning(FUBL) (Kwiatkowski et al., 2011).
Again, sourcecode is freely available.
FUBL factors the lexiconinto a set of base lexical entries, and a set of tem-plates that can construct more complex lexical en-tries from the base entries.
This allows for a signif-icantly more compact lexicon, as well as the abil-ity to handle certain linguistic constructions, likeellipsis, that appear frequently in the ATIS datasetand which UBL struggles with.
FUBL achieves anF1 of 0.82 on ATIS (compared with 66.3 for UBL),and an F1 of 0.886 on GeoQuery; both results areat or very near the best-reported results for thosedatasets.2.1 Previous WorkMany supervised learning frameworks have beenapplied to the task of learning a semantic parser,including inductive logic programming (Zelle andMooney, 1996; Thompson and Mooney, 1999;Thompson and Mooney, 2003), support vec-349tor machine-based kernel approaches (Kate etal., 2005; Kate and Mooney, 2006; Kate andMooney, 2007), machine translation-style syn-chronous grammars (Wong and Mooney, 2007),and context-free grammar-based approaches likeprobabilistic Combinatory Categorial Grammar(Zettlemoyer and Collins, 2005; Zettlemoyer andCollins, 2007; Zettlemoyer and Collins, 2009;Kwiatkowski et al., 2010; Kwiatkowski et al.,2011; Lu et al., 2008) and discriminative reranking(Ge and Mooney, 2006; Ge and Mooney, 2009).These approaches have yielded steady improve-ments on standard test sets like GeoQuery.
As faras we are aware, such systems have not been testedon domains besides ATIS and GeoQuery.Because of the complexity involved in build-ing a training dataset for a supervised semanticparser, there has been a recent push towards de-veloping techniques which reduce the annotationcost or the data complexity of the models.
Mod-els have been developed which can handle someambiguity in terms of which logical form is thecorrect label for each training sentence (Chen etal., 2010; Liang et al., 2009).
Another set of ap-proaches have investigated the case where no log-ical forms are provided, but instead some form offeedback or response from the world is used as ev-idence for what the correct logical form must havebeen (Clarke et al., 2010; Liang et al., 2011; Artziand Zettlemoyer, 2011).
Several projects have in-vestigated unsupervised (Goldwasser et al., 2011;Poon, 2013; Krishnamurthy and Mitchell, 2012)and semi-supervised (Yahya et al., 2012; Cai andYates, 2013) approaches.
These techniques tendto handle either the same benchmark domains, orsimpler questions over larger ontologies.
Whilesuch techniques are important, their (unlabeledand labeled) sample complexity is higher than itcould be, because the underlying grammars in-volved are not as general as they could be.
Ourwork investigates techniques that will reduce thissample complexity.3 The Clinical Trials DatasetClinical trials are scientific experiments that mea-sure the effects of a medical procedure, instru-ment, or product on humans.
Since September2009 in the United States, any clinical trial thatis funded by the federal government must makeits results publicly available online at clinicaltri-als.gov.
This site provides a wealth of biomedicaltext and structured data, which we use to producea novel test set for semantic parsing.3.1 The text and ontologyWe collected our utterances from a set of 47 ran-dom documents from clinicaltrials.gov.
Many as-pects of each study are reported in structured for-mat; for example, the number of participants whowere given a placebo and the number of partici-pants who were given the intervention under con-sideration are both reported in a table in a stan-dard format.
However, certain crucial aspects ofeach study are reported only in text.
Perhaps themost critical aspect of each study that is describedonly in text is the set of criteria for deciding whowill be admitted to the study and who cannot be;these criteria are called inclusion criteria and ex-clusion criteria.
We focus our semantic parsingtests on these criteria because they often form thelongest portion of unstructured text for a givenclinical trial report; because their meaning can berepresented using a concise set of logical constantsfrom a biomedical ontology; and because the cri-teria have a great deal of significance in the clin-ical trials domain.
For example, these criteria arecrucial for understanding why the results of tworelated studies about the same intervention mightdiffer.The criteria for a study can be logically repre-sented as a function of candidate test subjects thatreturns true if they match the study criteria, andfalse otherwise.
We use a variant of lambda calcu-lus over a typed ontology to represent each inclu-sion and exclusion criterion in our dataset.
We ran-domly collected 803 utterances and manually la-beled each using our representation language.
401were used for training, 109 for development, and293 for our final tests.To keep our semantic parsing study simple, weeschewed existing ontologies like UMLS (Boden-reider, 2004) that are large and overly-complex forthis problem.
We instead developed an ontologyof 10 types, 38 relations and functions, and a dic-tionary of 591 named-entities to build the logicalforms.
The five most common types and relationsin our dataset are listed in Table 1.
On average,the logical forms in our dataset involved 3.7 rela-tions per logical form, typically joined with con-junction, implication, or disjunction.
If accepted,both the full ontology and dataset will be madepublicly available.3.2 Problems with semantic parsing theclinical trials dataWe applied two state-of-the-art learning algo-rithms for learning PCCG semantic parsers ?UBL and its extension, FUBL?
to our training350Example Types Example Functions(p)erson t has condition(p, d)(d)isease t complication(d, d)(t)est i result(p, t)(tr)eatment t treated with(p, tr, date)(bo)dy-part t located(d, bo)Table 1: Common types and functions in our on-tology.
In the example functions, t indicatesboolean type, i indicates real values, p indicatesperson, d disease, and so on.patients with acute lymphoma?p .
has condition(p, acute(lymphoma))hypertension(i.e., include patients with hypertension)?p .
has condition(p, hypertension)AST > 3 mg(i.e., include patients with a level of the AST en-zyme in the blood of greater than 3 mg)?p .
> (result(p, AST), unit(3, mg))Table 2: Example utterances from the clinical tri-als dataset, and their logical forms.
Paraphrases inparentheses do not appear in the actual data.data and tested the resulting parsers on develop-ment data.
Results indicate that both systems havedifficulty with the clinical trials datasets: FUBLachieves an F1 of 0.413, and UBL of just 0.281.To help understand why state-of-the-art sys-tems?
performance differs so much from perfor-mance on benchmark datasets like GeoQuery, weperformed an error analysis.
Table 3 describes themost common errors we observed.
The most com-mon errors occurred on sentences containing co-ordination constructions, nested function applica-tions, and for UBL, ellipsis, although a long tailof less common errors exists.
FUBL manages tohandle the elliptical constructions in Clinical Tri-als well, but not coordination or nested functions.Both systems tend to learn many, overly-specificlexical entries that include too much of the logicalform in one lexical entry.
For instance, from thecoordination example in Table 3, UBL learns a lex-ical entry for the word ?or?
that includes the log-ical form ?p?d1?d2 .
or(has condition(p, d1),has condition(p, d2)).
While this entry workswell when coordinating two diseases or conditionsthat patients must have, it will not work for coor-dinations between treatments or dates, or coordi-nations between diseases that patients should nothave.
UBL learns over 250 lexical entries for theword ?or?
from our training dataset of 401 sen-tences, each one with limited applicability to de-velopment sentences.Based on these observed error types, we nextdevelop novel learning procedures that properlyhandle coordination, nested function construc-tions, and ellipsis.4 Learning to Handle ComplexConstructions in Clinical Trials Data4.1 Model and InferenceWe introduce the GLL system for learning a se-mantic parser that generalizes to both GeoQueryand Clinical Trials data.
The semantic parsingmodel involves a grammar that consists of a fixedset of binary CCG rewrite rules, a learned lexicon?, and a new set T of learned templates for con-structing unary type-raising rules.
We call thesetemplates for type-raising rules T-rules; these aredescribed below in Section 4.4.Following Kwiatkowsi et al.
(2010), we as-sign a probability P (L, T |S,~?,G) to a logicalform and parse tree for a sentence licensed bygrammar G using a log-linear model with pa-rameters~?.
We use a set of feature functions~F (L, T, S) = (f1(L, T, S), .
.
.
, fK(L, T, S)),where each ficounts the number of times that theith grammar rule is used in the derivation of Tand S. The probability of a particular logical formgiven a sentence and~?
is given by:P (L|S,~?,G) =?Texp(~?
?~F (L, T, S))?T?,L?exp(~?
?~F (L?, T?, S))(1)where the trees T (and T?)
are restricted to thosethat are licensed by G and which produce L (L?)
asthe logical form for the parent node of the tree.
In-ference is performed using standard dynamic pro-gramming algorithms for context-free parsing.4.2 LearningThe input for the task of learning a semantic parseris a set of sentences~S, where each Si?~S hasbeen labeled with a logical form L(Si).
We as-sume a fixed set of binary grammar productions,and use the training data to learn lexical entries,T-rules, and parameters.
The training objective isto maximize the likelihood of the observed logical351Error Type Freq.
Example DescriptionNested Funcs.
27% patients > 18 years of age?p .
> (result(age, p), unit(18, year))Many logical forms involve functions asarguments to other functions or relations.Ellipsis 26% diabetes?p .
has condition(p, diabetes)Many examples in the inclusion (exclu-sion) criteria simply list a disease or treat-ment, with the understanding that a patientp should be included (excluded) if p hasthe disease or is undergoing the treatment.Coordination 16% patient is pregnant or lactating?p .
or(has condition(p, pregnant),has condition(p, lactating))Clinical trials data has more coordina-tion, especially noun phrase and adjectivephrase coordination, than GeoQuery.Table 3: Three common kinds of utterances in the clinical trials development set that caused UBL andFUBL to make errors.
Frequency indicates the percentage of all development examples that exhibitedthat type of construction.Input: set of labeled sentences {(Si, L(Si))}, ini-tial grammar G0, number of iterations MAX ,learning rate ???
?
?i : ??
?
?
{S : L(Si)?
Si}G?
G0?
?~?
?~0For iteration := 1 to MAX:TR?
TRLEARN(G)Add dimension ?tto~?
for t ?
TR?GG?
G ?
TRFor each sentence Si:??
LEXENTLEARN(Si, L(Si), G)Add dimension ??to~?
for all ?
?
??GG?
G ?
?~?
?~?
+ ?
?iCLLReturn G,~?Figure 1: The GLL Learning Algorithm.
?iCLLindicates the local gradient of the conditional loglikelihood at sentence Si.forms, or to find G?and~?
?such that:G?,~?
?= arg maxG,~?
?iP (L(Si)|Si,~?,G)This is a non-convex optimization problem.
Weuse a greedy optimization procedure that iter-atively updates G and~?.
Figure 1 shows anoverview of the full algorithm.We use stochastic gradient updates to estimateparameters (LeCun et al., 1998).
For each exam-ple sentence Siin training, we compute the localgradient of the conditional log likelihood functionCLL = logP (L(Si)|Si,~?,G), and update~?
bya step in the direction of this local gradient.
Thepartial derivatives for this local gradient are:?CLL?
?j= EP (T |L(Si),Si,~?,G)fj(L(Si), T, Si)?EP (T |Si,~?,G)fj(L(Si), T, Si)4.3 Learning Lexical Entries with InverseFunction CompositionWe adopt a greedy approach to learning new lex-ical entries.
We first identify in our current parseany high-scoring lexical entries that cover multiplewords, and then look for new lexical rules for thesub-phrases covered by these lexical entries thatcould combine to create the current parse chart en-try using the existing grammar rules.
This requiressearching through the grammar rules to find chil-dren nodes that the nonterminal could be the par-ent of.
In general, this produces an intractablylarge set, because it requires taking the inverseof function application and function compositionfor forming the semantics of the nonterminal, andthose inverses are intractably large.Figure 2 shows our algorithm for learning lex-ical entries, and Figure 3 shows the details of thecritical component that generates the semantics ofnew potential lexical entries.
For brevity, we omitthe details of how we learn the syntax and map-pings from semantics to words or phrases for newlexical entries, but these are borrowed from the ex-isting techniques in UBL.
The crucial differencefrom existing techniques is that the SPLITLEARNalgorithm focuses on inverse function composi-tion, while existing techniques focus on inverse352Input: training sentence Sent, its logical form L,current grammar GInitialize:PC ?
parse chart from parsing Sent with Gsplits?
?For len := length(Sent) to 1:For pos := 0 to length(Sent)?
len:e = arg maxentry?PC[len][pos]entry.scoreif e?s only derivation is a lexical rule in G:(score,?)?
SPLITLEARN(e, PC)splits?
splits ?
{(score,?)}split??
arg maxsplit?splitssplit.scoreReturn split?.
?Figure 2: LEXENTLEARN Algorithm for learninglexical entriesfunction application.
While a priori both tech-niques are reasonable choices (and both work wellon GeoQuery), our empirical results show that in-verse function composition can learn the same se-mantic forms as inverse function application, butin addition can handle nested functions (whichare function compositions) and coordination ?
aform of function composition if one views logicalconnectives like or as boolean functions.The SPLITLEARN algorithm uses a GET-SUBEXPR subroutine to heuristically select onlycertain subexpressions of the input logical formfor computing inverse composition.
This is toavoid a combinatorial explosion in the numberof learned splits of the input semantics.
Mostlywe consider any subexpression that forms an ar-gument to some function in le.sem, but we takecare to also include abstracted versions of thesesubexpressions, in which some of their argumentsare in turn replaced by variables.
The subrou-tine FREEVARS identifies all variables in a logicalform that have no quantifier; REPEATVARS iden-tifies all variables that appear at least twice.
PC-SCORE looks for any entry in the parse chart thathas a matching semantics and returns the score ofthat entry, or 0 if no matches are found.
We usePCSCORE to measure the improvement (delta) inthe score of the parse if it uses the two new lexicalentries, rather than the previous single lexical en-try.
SPLITLEARN returns the set of lexical entriesthat tie for the largest improvement in the score ofthe parse.Figures 4 and 5 illustrate the difference be-Input: lexical entry le, parse chart PCEntries?
?For s ?GETSUBEXPR(le.sem):t?
copy of ssem??
copy of le.semApply[t]?
?For v ?
FREEVARS(s)?
REPEATVARS(sem?
):Create variable v?, t?
tv?sub for vConcatenate ??v??
onto front of tApply[t]?
Apply[t] ?
{v}For v ?
FREEVARS(t):Remove ??v?
from front of sem?Concatenate ??v?
onto front of tCreate new variable wsub?
?(w?
+ each a ?
Apply[t] + ?)?sem??
sem?sub sub for sConcatenate ??w?
onto front of sem?Entries?
Entries ?
{t, sem?
}delta[t], delta[sem?]?
PCSCORE(t) +PCSCORE(sem?)
- PCSCORE(le)max?
maxxdelta[x]Returnmax, {s ?
Entries | delta[s] = max}Figure 3: SPLITLEARN Algorithm for generating(the semantics of) new lexical entries.tween SPLITLEARN and lexical entry learning forUBL and FUBL.
For both example sentences,there is a point in the learning process wherea logical form must be split using inverse func-tion composition in order for useful lexical en-tries to be learned.
At those points, UBL andFUBL split the logical forms using inverse func-tion application, resulting in splits where the se-mantics of different lexemes are mixed togetherin the two resulting subexpressions.
In Figure 4,all three systems take the logical form ?u.?p.
>(result(p, bilirubin), unit(1.5, u)) and splitit by removing some aspect of the final argu-ment, unit(1.5, u), from the full expression.
InUBL and FUBL, the term that is left behind inthe full expression is something that unifies with?u.unit(1.5, u).
In GLL, however, only a vari-able is left behind, since that variable can be re-placed by ?u.
unit(1.5, u) through function com-position to obtain the original expression.
ThusGLL?s split yields one significantly simpler subex-pression, which in the end yields simpler lexicalentries.
In both figures, and in general for mostparses we have observed, inverse function compo-sition yields simpler and cleaner subexpressions.353total > 1.5 mg/dLmg/dl:ubilirubin:q?p.>(result(p,bilirubin),unit(1.5,mg/dl))?p.result(p,bilirubin)bilirubin?u.unit(1.5,u)F: u ?
iG: p ?
i?i.?u.?p.>(result(p,bilirubin),unit(i,u))?F.?u.?p.>(result(p,bilirubin),F(u))?G.?F.?u.?p.>(G(p),F(u))?i.?p.>(result(p,bilirubin),i)1.5:i?q.?i.?u.?p.>(result(p,q),unit(i,u))?i'.?i.>(i',i)UBLFUBLGLL?u.
?p.>(result(p,bilirubin),unit(1.5,u))Figure 4: An example of a sentence with nested-function semantics.
GLL?s lexical entry learningprocedure correctly identifies the most general se-mantics for the lexeme >, while UBL and FUBLlearn more specific and complex semantics.4.4 Learning T-rulesWe use T-rules to handle elliptical constructions.They are essentially a simplification of the fac-tored lexicon used in FUBL that yields very sim-ilar results.
Each T-rule ?
?
T is a function of theform ?e .
if type(e) then return Syn : f(e)?Syn?
: e, where type is a type from our ontology,Syn and Syn?are two syntactic CCG categoriesor variables, and f is an arbitrary lambda calcu-lus expression.
For example, consider the T-rule?
= (?e .
if disease(e) then return S\N :?p .
has condition(p, e) ?
N : e).When applied to the entity diabetes, this T-rule results in an ordinary CCG rule: S\N :?p .
has condition(p, diabetes) ?
N :diabetes.
Thus each T-rule is a template for con-structing unary (type-raising) CCG grammar rulesfrom an entity of the appropriate type.TRLEARN works by first identifying a setof entity symbols E that appear in multiplelexical entries in the input grammar G. Letthe lexical entries for entity e ?
E be denotedby ?
(e); thus, E consists of all entities where|?
(e)| ?
2.
TRLEARN then looks for patternsin each of these sets of lexical entries.
If oneof the lexical entries in ?
(e) has a semanticsthat consists of just e (for example, the lexicalentry N : diabetes ?
diabetes), we createcandidate T-rules from every other lexical entryl??
?
(e) that has the same child, such asS\N : ?p .
has condition(p, diabetes) ?diabetes.
From this lexical entry,we create the candidate T-rule ?
=toal>1a l. t5>m1o1a g5 /odaoal1mLu:Lq:bo.idg1rlalg1uqnu?t5>m1o1a(u /odaoal1m(uLq:g5ubo.idg1rlalg1uqnt5>m1o1a?nibo.idg1rlalg1uqn/odaoal1m??Lue:Lp:Lu:Lq:g5ubo.idg1rlalg1uqnuF?npuqnu??Lp:Lu:Lq:g5ubo.idg1rlalg1uqnt5>m1o1a?npuqnu???ip.?'
.
?i .>(r' ripes' ri eep(iuqnu?isi iG(iuisi i?i .??
.ult,b>in)G)>ir ?
si eu,?)
?L?
?Lu:Lq:g5ubo.idg1rlalg1uqnt5>m1o1a?nibo.idg1rlalg1uqn ???'
.
?i .>(r' r?(?
?iliGes ' ri eeFigure 5: An example of a sentence with coor-dination semantics.
GLL?s lexical entry learningprocedure correctly identifies the semantics for thelexeme or, while UBL and FUBL learn incorrectsemantics.
(?x .
if disease(x) then return S\N :?p .
has condition(p, x) ?
N : x).
In general,the test in the if statement in the T-rule containsa check for the type of entity e. The right-handside of the implication contains a unary grammarrule whose parent matches the parent of the rulein l?, except that entity e has been replaced by avariable x.
The child of the grammar rule matchesthe parent of the basic lexical entry N : e, exceptagain that the entity e has been replaced by thevariable x.Having constructed a set of candidate T-rulesfrom this process, TRLEARN must select the onesthat will actually be added to the grammar.
Weuse a test of selecting T-rules that cover at leastMIN existing grammar rules in the input gram-mar G. In our implementation, we set MIN = 2.When parsing a sentence, the parser checks anyparse chart entry for semantics that consist solelyof an entity; for any such entry, it looks in a hash-based index for applicable T-rules, applies them tothe entity to construct new unary grammar rules,and then applies the unary grammar rules to theparse chart entry to create new nonterminal nodes.5 ExperimentsIn our experiments, we test the generality of ourlearning algorithm by testing its ability to handleboth GeoQuery and the Clinical Trials datasets.5.1 Experimental setupThe clinical trials dataset is described above inSection 3.
GeoQuery consists of a database of354System Precision Recall F1UBL 87.9 88.5 88.2FUBL 88.6 88.6 88.6GLL 84.6 86.1 85.5Table 4: GLL performs comparably to two state-of-the-art learning algorithms for PCCG semanticparsing on the benchmark GeoQuery dataset.System Precision Recall F1UBL 20.3 19.9 20.1FUBL 42.3 39.7 40.8GLL 65.3 63.2 64.1Table 5: On the clinical trials dataset, GLL outper-forms UBL and FUBL by more than 23 points inF1, for a reduction in error (i.e., 1-F1) of nearly40% over FUBL.2400 geographical entities, such as nations, rivers,and mountains, as well as 8 geography relations,such as the location of a mountain, and whetherone state borders another.
The text for semanticparsing consists of a set of 880 geography ques-tions, labeled with a lambda-calculus representa-tion of the sentence?s meaning.
We follow the pro-cedure described by Kwiatkowski et al.. (2010)in splitting these sentences into training, develop-ment, and test sentences.
This dataset allows us toprovide a comparison with other semantic parserson a well-known dataset.
We measured perfor-mance based on exact-match of the full logicalform, modulo re-ordering of arguments to sym-metric relations (like conjunction and disjunction).5.2 Results and DiscussionTables 4 and 5 show the results of semantic parserslearned by the UBL, FUBL, and GLL learningalgorithms on the GeoQuery and clinical trialsdatasets, respectively.
On the GeoQuery dataset,all three parsers perform very similarly, althoughGLL?s performance is slightly worse.
However, onthe clinical trials dataset, GLL significantly out-performs both UBL and FUBL in terms of preci-sion, recall, and F1.
Of course, there clearly re-main many syntactic and semantic constructionsthat none of these algorithms can currently han-dle, as all systems perform significantly worse onclinical trials than on GeoQuery.Tables 6 shows the overall size of UBL?s andGLL?s learned lexicons, and Table 7 shows thenumber of learned entries for selected lexicalLexicon SizeSystem GeoQuery Clinical TrialsUBL 5,149 49,635GLL 4,528 36,112Table 6: GLL learns a lexicon that is 27% smallerthan UBL?s lexicon on clinical trials data.Lexeme UBL meanings GLL meanings> 36 2< 28 2= 35 2and 6 4or 254 9Table 7: For certain common and critical lexicalitems in the clinical trials dataset, GLL learns farfewer (but more general) lexical entries; for theword ?or?, GLL learns only 3.5% of the entriesthat UBL learns.items that appear frequently in the clinical trialscorpus.
FUBL uses a factored lexicon in whichthe semantics of a logical form is split across twodata structures.
As a result, FUBL?s lexicon is notdirectly comparable to the other systems, so forthese comparisons we restrict our attention to UBLand GLL.
UBL tends to learn far more lexical en-tries than GLL, particularly for words that appearin multiple sentences.
Yet the poorer performanceof UBL on clinical trials is an indication that theselexical entries are overly specific.6 ConclusionWe have introduced the clinical trials dataset,a naturally-occurring set of text where existinglearning algorithms for semantic parsing struggle.Our new GLL algorithm uses a novel inverse func-tion composition algorithm to handle coordina-tion and nested function constructions, and patternlearning to handle elliptical constructions.
Theseinnovations allow GLL to handle GeoQuery andimprove on clinical trials.
Many sources of er-ror on clinical trials remain for future research,including long-distance dependencies, attachmentambiguities, and coreference.
In addition, furtherinvestigation is necessary to test how these algo-rithms handle additional domains and other typesof natural linguistic constructions.355AcknowledgmentsThis work was supported by National ScienceFoundation grant 1218692.
The authors appreciatethe help that Anjan Nepal, Qingqing Cai, AvirupSil, and Fei Huang provided.ReferencesYoav Artzi and Luke Zettlemoyer.
2011.
Bootstrap-ping Semantic Parsers from Conversations.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP).Olivier Bodenreider.
2004.
The Unified Medical Lan-guage System (UMLS): integrating biomedical ter-minology.
Nucleic Acids Research, 32:D267?D270.Qingqing Cai and Alexander Yates.
2013.
Large-scaleSemantic Parsing via Schema Matching and Lexi-con Extension.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).David L. Chen, Joohyun Kim, and Raymond J.Mooney.
2010.
Training a MultilingualSportscaster: Using Perceptual Context to LearnLanguage.
Journal of Artificial Intelligence Re-search, 37:397?435.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with ccg andlog-linear models.
Computational Linguistics,33(4):493?552.J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Computational Natural Language Learn-ing (CoNLL).Ruifang Ge and Raymond J. Mooney.
2006.
Discrim-inative Reranking for Semantic Parsing.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING/ACL-06).Ruifang Ge and Raymond J. Mooney.
2009.
Learninga Compositional Semantic Parser using an ExistingSyntactic Parser.
In Joint Conference of the 47th An-nual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Confer-ence on Natural Language Processing of the AsianFederation of Natural Language Processing (ACL-IJCNLP 2009).D.
Goldwasser, R. Reichart, J. Clarke, and D. Roth.2011.
Confidence driven unsupervised semanticparsing.
In Association for Computational Linguis-tics (ACL).Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing String-Kernels for Learning Semantic Parsers.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the ACL.Rohit J. Kate and Raymond J. Mooney.
2007.
Semi-Supervised Learning for Semantic Parsing usingSupport Vector Machines.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, Short Papers (NAACL/HLT-2007).Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to Transform Natural toFormal Languages.
In Proceedings of the Twen-tieth National Conference on Artificial Intelligence(AAAI-05).Jayant Krishnamurthy and Tom Mitchell.
2012.Weakly Supervised Training of Semantic Parsers.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing Probabilis-tic CCG Grammars from Logical Form with Higher-order Unification.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP).Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-water, and Mark Steedman.
2011.
Lexical Gen-eralization in CCG Grammar Induction for Seman-tic Parsing.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Y.
LeCun, L. Bottou, Y. Bengio, and P. Haffner.
1998.Gradient-based learning applied to document recog-nition.
In Proceedings of the IEEE.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InAssociation for Computational Linguistics and In-ternational Joint Conference on Natural LanguageProcessing (ACL-IJCNLP).P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InAssociation for Computational Linguistics (ACL).Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A Generative Model for ParsingNatural Language to Meaning Representations.
InProceedings of The Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Hoifung Poon.
2013.
Grounded Unsupervised Se-mantic Parsing.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).C.A.
Thompson and R.J. Mooney.
1999.
Automaticconstruction of semantic lexicons for learning natu-ral language interfaces.
In Proc.
16th National Con-ference on Artificial Intelligence (AAAI-99), pages487?493.356Cynthia A. Thompson and Raymond J. Mooney.
2003.Acquiring Word-Meaning Mappings for NaturalLanguage Interfaces.
Journal of Artificial Intelli-gence Research (JAIR), 18:1?44.Yuk Wah Wong and Raymond J. Mooney.
2007.Learning Synchronous Grammars for SemanticParsing with Lambda Calculus.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics (ACL-2007).Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural Language Questions for theWeb of Data.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to Parse Database Queries using Inductive LogicProgramming.
In AAAI/IAAI, pages 1050?1055.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to Map Sentences to Logical Form: Struc-tured Classification with Probabilistic CategorialGrammars.
In Proceedings of the Twenty FirstConference on Uncertainty in Artificial Intelligence(UAI).Luke S. Zettlemoyer and Michael Collins.
2007.
On-line Learning of Relaxed CCG Grammars for Pars-ing to Logical Form.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).Luke S. Zettlemoyer and Michael Collins.
2009.Learning Context-dependent Mappings from Sen-tences to Logical Form.
In Proceedings of the JointConference of the Association for ComputationalLinguistics and International Joint Conference onNatural Language Processing (ACL-IJCNLP).357
