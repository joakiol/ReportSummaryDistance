Proceedings of NAACL HLT 2007, Companion Volume, pages 77?80,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsILR-Based MT Comprehension Test with Multi-Level QuestionsDouglas Jones, Martha Herzog, Hussny Ibrahim, Arvind Jairam, Wade Shen,Edward Gibson and Michael EmontsMIT Lincoln LaboratoryLexington, MA 02420{DAJ,Arvind,SWade}@LL.MIT.EDUMHerzog2005@comcast.netDLI Foreign Language CenterMonterey, CA 93944{Hussny.Ibrahim,Michael.Emonts}@monterey.army.milMIT Brain and CognitiveSciences DepartmentCambridge MA, 02139EGibson@MIT.EDUAbstractWe present results from a new InteragencyLanguage Roundtable (ILR) based compre-hension test.
This new test design presentsquestions at multiple ILR difficulty levelswithin each document.
We incorporatedArabic machine translation (MT) outputfrom three independent research sites, arbi-trarily merging these materials into one MTcondition.
We contrast the MT condition,for both text and audio data types, with highquality human reference Gold Standard(GS) translations.
Overall, subjectsachieved 95% comprehension for GS and74% for MT, across 4 genres and 3 diffi-culty levels.
Surprisingly, comprehensionrates do not correlate highly with translationerror rates, suggesting that we are measur-ing an additional dimension of MT quality.We observed that it takes 15% more timeoverall to read MT than GS.1 IntroductionThe official Defense Language Proficiency Test(DLPT) is constructed according to rigorous andwell-established principles that have been devel-oped to measure the foreign language proficiencyof human language learners in U.S. Department ofDefense settings.
In 2004, a variant of that testtype was constructed, following the general DLPTdesign principles, but modified to measure thequality of machine translation.
This test, known asthe DLPTstar (Jones et al 2005),  was based onauthentic Arabic materials at ILR  text difficultylevels 1, 2, and 3, accompanied by constructed-response questions at matching levels.
The ILRlevel descriptors, used throughout the U.S. gov-ernment, can be found at the website cited in thelist of references.
The text documents were pre-sented in two conditions in English translation: (1)professionally translated into English, and (2) ma-chine translated with state-of-the art MT systems,often quite garbled.
Results showed that nativereaders of English could generally pass the Levels1 and 2 questions on the test, but not those at Level3.
Also, Level 1 comprehension was less than ex-pected, given the low level of the original material.It was not known whether the weak Level 1 per-formance was due to systematic deficits in MTperformance at Level 1, or whether the materialswere simply mismatched to the MT capabilities.In this paper, we present a new variant of thetest, using materials specifically created to test thecapabilities of the MT systems.
To guarantee thatthe MT systems were up to the task of processingthe documents, we used the DARPA GALE 2006evaluation data sets, against which several researchsites were testing MT algorithms.
We arbitrarilymerged the MT output from three sites.
The ILRdifficulty of the documents ranged from Level 2 toLevel 3, but the test did not contain any true Level1 documents.
To compensate for this lack, weconstructed questions about Level 1 elements (e.g.,personal and place names) in Level 2 and 3 docu-ments.
A standard DLPT would have more varia-tion at Level 1.2 Related and Previous WorkEarlier work in MT evaluation incorporated an in-formativeness measure, based on comprehensiontest answers, in addition to fluency, a measure ofoutput readability without reference to a gold stan-dard, and adequacy, a measure of accuracy withreference to a gold standard translation (White andO'Connell, 1994).
Later MT evaluation found flu-ency and adequacy to correlate well enough withautomatic measures (BLEU), and since compre-hension tests are relatively more expensive to cre-ate, the informativeness test was not used in later77MT evaluations, such as the ones performed byNIST from 2001-2006.
In other work, task-basedevaluation has been used for MT evaluation (Vossand Tate, 2006), which measures human perform-ance on exhaustively extracting ?who?, ?when?, and?where?
type elements in MT output.
The DLPT-star also uses this type of factual question, particu-larly for Level 2 documents, but not exhaustively.Instead, the test focuses on text elements mostcharacteristic of the levels as defined in the ILRscale.
At Level 3, for example, questions mayconcern abstract concepts or hypotheses found inthe documents.
Applying the ILR construct pro-vides Defense Department decision makers withtest scores that are readily interpretable.3 Test Construction and AdministrationIn this paper, we present a new test, based entirelyon the DARPA GALE 2006 evaluation data, se-lecting approximately half of the material for ourtest.
We selected twenty-four test documents, withbalanced coverage across four genres: newswire,newsgroups, broadcast news and talk radio.
Ourtarget was to have at least 2500 words for eachgenre, which we exceeded slightly with approxi-mately 12,200 words in total for the test.
We be-gan with a random selection of documents andadjusted it for better topic coverage.
We con-structed an exhaustive set of questions for eachdocument, approximately 200 questions in total.The questions ranged in ILR difficulty, from "0+,1,1+, 2, 2+ and 3, with Levels 0+, 1 and 1+ com-bined to a pseudo-level we called L1~, providingfour levels of difficulty to be measured.
We di-vided the questions into two sets, and each indi-vidual subject answered questions for one of thesets.
The test itself was constructed by a DLPTtesting expert and a senior native-speaking Arabiclanguage instructor, using only the original Arabicdocuments and the Gold Standard translations.They had no access to any machine translationoutput during the test construction or scoring.In August 2006, we administered the test at MITto 49 test subjects who responded to announce-ments for paid experimental subjects.
The subjectsread the documents in a Latin square design, mean-ing that each subject saw each document, but onlyin one of the two conditions, randomly assigned.Subjects were allowed 5 hours to complete the test.Since the questions were divided into two sets foreach document, the actual set of 49 subjectsyielded approximately 25 ?virtual subjects?
read-ing the full list of 228 questions.
The mean timespent on testing, not counting breaks or subjectorientation, was 2.5 hours; fastest was 1.1 hours,slowest was 3.4 hours.The subject responses were hand-graded by thetwo testing experts, following the pre-establishedanswers in the test protocol.
There was no pre-assessment of whether information was preservedor garbled in the MT when designing questions orresponses in the test protocol.
The testing expertswere provided the reference translations and theoriginal Arabic documents, but not the MT duringscoring.
Moreover, test conditions were masked inorder to provide a blind assessment.
The two test-ing experts provided both preliminary and finalscores; multiple passes provided an opportunity toclarify the correct answers and to normalize scor-ing.
The scoring agreement rate was 96% for thefinal scores.4 Overall ResultsThe overall result for comprehension accuracy was95% for subjects reading the Gold Standard trans-lation and 74% for reading Machine Translation,across each of the genres and difficulty levels.
Thecomprehension accuracy for each genre is shownin Figure 1.
The two text genres score better thanthe audio genres, which is to be expected becausethe audio MT condition has more opportunities forerror.
Within each modality, the more standard,more structured genre fares better: newswire re-sults are better than newsgroup results, and themore structured genre of broadcast news scoresbetter than the less constrained, less structuredconversations present in the talk radio shows.Figure 1.
Comprehension Accuracy per Genre97% 93% 94% 94%80% 77%72% 66%0%20%40%60%80%100%Newswire Broadcast News Talk RadioGSMTNewsgroupsOverall Comprehension Accuracy78The break-down by ILR level of difficulty for eachquestion is shown in Figure 2.
The general trend isconsistent with what has been observed previously(Jones et al 2005).
The best results are at Level 2;Level 1 does well but not as well as expected.Thus the test has provided a key finding, which isthat MT systems perform more poorly on Level 1,even when the data is matched to their capabilities.Level 3 is very challenging for the MT condition,and also more difficult in the GS condition.
Usinga standard 70 percent passing threshold, responsesto questions on all MT documents, except forLevel 3, received a passing grade.Figure 2.
Comprehension Accuracy per Level.To provide a snapshot of the ILR levels: L1 in-dicates sentence-level comprehensibility, and mayinclude factual local announcements, etc.
; L2 indi-cates paragraph-level comprehensibility; factual/concrete, covering a wide spectrum of topics (poli-tics, economy, society, culture, security, science);L3 involves extended discourse comprehensibility;the ability to understand hypotheses, supportedopinion, implications, and abstract linguistic for-mulations, etc.It was not possible to balance Level 3 documentsacross genres within the GALE evaluation data;except for those taken from Talk Radio, mostdocuments did not reach that level of complexity.Hence, genre and difficulty level were not com-pletely independent in this test.5 Comprehension and Translation ErrorWe expect to see a relationship between compre-hension rates and translation error.
In an idealizedcase, we may expect a precise inverse correlation.We then compared comprehension rates with Hu-man Translation Error Rate (HTER), an errormeasure for machine translation that counts thenumber of human edits required to change systemMT output so that it contains all and only the in-formation present in a Gold Standard reference(NIST, 2006).
The linear regression line in Figure3 shows the kind of inverse correlation we mightexpect.
Subjects lose about 12% in comprehensionfor every 10% of translation error.
The R2 value is33%.
The low correlation suggests that the com-prehension results are measuring a somewhat inde-pendent aspect of MT quality, which we feel isimportant.
HTER does not directly address thefacts that not all MT errors are equally importantand that the texts contain inherent redundancy thatthe readers use to answer the questions.
For ex-ploratory purposes, we divide the graph of Figure 3into four quadrants.
Quadrant I and IV containexpected behavior: 122 data points of good transla-tions and good comprehension results versus 43points of bad translations and poor comprehension.Q-II has 24 robust points: the translations havehigh error, but somehow managed to containenough well-translated words that people can an-swer the questions.
Q-III has 28 fragile points: thefew translation errors impaired comprehension.Figure 3.
Comprehension vs.
Translation Error.We point out that there is a 1-to-1 mapping be-tween comprehension questions and individualsub-passages of the documents in the data.
Eachpoint in Figure 3 plots the HTER of a single seg-ment versus the average comprehension score onthe corresponding question.
The good and baditems are essentially a sanity-check on the experi-mental design.
We expect to see good comprehen-sion when translations are good, and we expect tosee poor comprehension when translations are bad.Next we will examine the two other types: fragileand robust translations.Overall Comprehension Accuracy97% 96% 91% 88%77% 82% 76%51%0%20%40%60%80%100%L1~ L2 L2+ L3GSMTQ-I (Good)                        Q-II (Robust)122 points (57%)               24 points (10%)(All Levels and Genres)0%20%40%60%80%100%0% 20% 40% 60% 80% 100%x = Translation Error (HTER)y = Comprehension (DLPT*)Q-III (Fragile)                      Q-IV (Bad)28 points (13%)                 43 points (20%)79A fragile translation is one that has a goodHTER score but a bad comprehension score.
Asample fragile translation is one from a broadcastnews which asks for a particular name:  the HTERwas a respectable 24%, but the MT comprehensionaccuracy was a flat 0%, since the name was miss-ing.
Everyone reading GS answered correctly.A robust translation is one that has a bad HTERscore but still manages to get a good comprehen-sion score.
A sample robust translation is onedrawn from a posting providing instructions forfoot massage.
The text was quite garbled, with anHTER score of 48%, but the MT comprehensionaccuracy was a perfect 100%.
Everyone readingthe GS condition also answered the question cor-rectly, which was that one should start a foot mas-sage with oil.
We note in passing that the highesterror rate for a question with 100% comprehensionis about 50%, shown with the up-arrow in Figure3.
We should be surprised to see any items with100% comprehension for HTER rates above 50%,considering Shannon?s estimate that written Eng-lish is about 50% redundant.
We expect that MTreaders are making use of their general worldknowledge to interpret the garbled MT output.
Achallenge is to identify robust translations, whichare useful despite their high translation error rate.6 Detailed DiscussionIn this section we will discuss several aspects ofthe test in more detail: the scoring methodology,including a discussion of partial credit and inter-rater agreement; timing information; questionsabout personal names.Each correct answer was assigned a score of 1,and each incorrect answer was assigned a score of0.
Partial credit was assigned on an ad-hoc basis,but normalized for scoring by assigning all non-integer scores to 0.5.
This method yielded scoresthat were generally at the midpoint between binaryscoring, in which non-integer scored were uni-formly mapped either harshly to 0 or leniently to 1,the average difference between harsh and lenientscoring being approximately 11%.
Inter-rateragreement was 96%.The testing infrastructure we used recorded theamount of time spent on each document.
The gen-eral trend is that people spend longer on MT thanon GS.
The mean percentage of time spent on MTcompared with GS is 115% per item, meaning thatit takes 15% more time to read MT than GS.
Thestandard error was 4%.
The median is 111%;minimum is 89% and maximum is 159%.
In futureanalysis and experimentation we will conduct morefine-grained temporal estimates.As we have seen in previous experiments, theperformance for personal names is lower than fornon-names.
We observed that the name questionshave 71% comprehension accuracy, compared withthe 83% for questions about things other than per-sonal names.7 Conclusions and Future WorkWe have long felt that Level 2 is the natural andsuccessful level for machine translation.
The abil-ity to present concrete factual information that canbe retrieved by the reader, without requirementsfor understanding the style, tone, or organizationalpattern used by the writer seemed to be present inthe previous work.
It is worth pointing out thatthough we have many Level 1 questions, we arestill not really testing Level 1 because the test doesnot contain true Level 1 documents.
In future testswe wish to include Level 1 documents and ques-tions.Continuing along these lines, we are currentlycreating two new tests.
We are constructing a newArabic DLPT-star test, tailoring the document se-lection more specifically for comprehension testingand ensuring texts and tasks are at the intendedILR levels.
We are also constructing a MandarinChinese test with similar design specifications.We intend for both of these tests to be available fora public machine translation evaluation to be con-ducted in 2007.ReferencesDoddington, G. 2002.
Automatic Evaluation of MachineTranslation Quality Using N-gram Co-OccurrenceStatistics.
Proceedings of HLT 2002.NIST 2006.
GALE Go/No-Go Eval Plan; www.nist.gov/speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdfJones, D. A., W. Shen, et al 2005a.
Measuring Transla-tion Quality by Testing English Speakers with a NewDLPT for Arabic.
Int?l Conf.
on Intel.
Analysis.Interagency Language Roundtable Website.
2005.
ILRSkill Level Descriptions: http://www.govtilr.orgVoss, Clare and Calandra Tate.
2006.
Task-basedEvaluation of MT Engines.
European Association forMachine Translation conference.White, JS and TA O'Connell.
1994.
Evaluation in theARPA machine translation program: 1993 method-ology.
Proceedings of the HLT workshop.80
