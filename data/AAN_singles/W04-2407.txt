Memory-Based Dependency ParsingJoakim Nivre, Johan Hall and Jens NilssonSchool of Mathematics and Systems EngineeringVa?xjo?
UniversitySE-35195 Va?xjo?Swedenfirstname.lastname@msi.vxu.seAbstractThis paper reports the results of experimentsusing memory-based learning to guide a de-terministic dependency parser for unrestrictednatural language text.
Using data from a smalltreebank of Swedish, memory-based classifiersfor predicting the next action of the parser areconstructed.
The accuracy of a classifier assuch is evaluated on held-out data derived fromthe treebank, and its performance as a parserguide is evaluated by parsing the held-out por-tion of the treebank.
The evaluation shows thatmemory-based learning gives a signficant im-provement over a previous probabilistic modelbased on maximum conditional likelihood esti-mation and that the inclusion of lexical featuresimproves the accuracy even further.1 IntroductionDeterministic dependency parsing has recently been pro-posed as a robust and efficient method for syntactic pars-ing of unrestricted natural language text (Yamada andMatsumoto, 2003; Nivre, 2003).
Dependency parsingmeans that the goal of the parsing process is to constructa dependency graph, of the kind depicted in Figure 1.
De-terministic parsing means that we always derive a singleanalysis for each input string.
Moreover, this single anal-ysis is derived in a monotonic fashion with no redundancyor backtracking, which makes it possible to parse naturallanguage sentences in linear time (Nivre, 2003).In this paper, we report experiments using memory-based learning (Daelemans, 1999) to guide the parser de-scribed in Nivre (2003), using data from a small tree-bank of Swedish (Einarsson, 1976).
Unlike most pre-vious work on data-driven dependency parsing (Eisner,1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la-beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen-tations.The paper is structured as follows.
Section 2 givesthe necessary background definitions and introduces theidea of guided parsing as well as memory-based learning.Section 3 describes the data used in the experiments, theevaluation metrics, and the models and algorithms usedin the learning process.
Results from the experiments aregiven in section 4, while conclusions and suggestions forfurther research are presented in section 5.2 Background2.1 Dependency GraphsThe linguistic tradition of dependency grammar com-prises a large and fairly diverse family of theories and for-malisms that share certain basic assumptions about syn-tactic structure, in particular the assumption that syntacticstructure consists of lexical nodes linked by binary re-lations called dependencies (see, e.g., Tesnie`re (1959),Sgall (1986), Mel?c?uk (1988), Hudson (1990)).
Thus,the common formal property of dependency structures,as compared to the representations based on constituency(or phrase structure), is the lack of nonterminal nodes.In a dependency structure, every word token is depen-dent on at most one other word token, usually called itshead or regent, which means that the structure can berepresented as a directed graph, with nodes representingword tokens and arcs representing dependency relations.In addition, arcs may be labeled with specific dependencytypes.
Figure 1 shows a labeled dependency graph for asimple Swedish sentence, where each word of the sen-tence is labeled with its part of speech and each arc la-beled with a grammatical function.Formally, we define dependency graphs in the follow-ing way:PPPa?
(In ?ADVNN60-taletthe-60?s ?PRVBma?ladepaintedPNhanhe ?SUBJJdja?rvabold ?ATTNNtavlorpictures ?OBJHPsomwhich ?ATTVBretadeannoyed? SUBPMNikitaNikita ?OBJPMChrusjtjov.Chrustjev.
) ?IDFigure 1: Dependency graph for Swedish sentence1.
Let R = {r1, .
.
.
, rm} be the set of permissible de-pendency types (arc labels).2.
A dependency graph for a string of words W =w1?
?
?wn is a labeled directed graph D = (W,A),where(a) W is the set of nodes, i.e.
word tokens in theinput string,(b) A is a set of labeled arcs (wi, r, wj) (wherewi, wj ?
W and r ?
R).We write wi < wj to express that wi precedes wjin the string W (i.e., i < j); we write wi r?
wj tosay that there is an arc from wi to wj labeled r, andwi ?
wj to say that there is an arc from wi to wj(regardless of the label); we use ??
to denote thereflexive and transitive closure of the unlabeled arcrelation; and we use ?
and ??
for the correspond-ing undirected relations, i.e.
wi ?
wj iff wi ?
wjor wj ?
wi.3.
A dependency graph D = (W,A) is well-formed iffthe five conditions given in Figure 2 are satisfied.For a more detailed discussion of dependency graphsand well-formedness conditions, the reader is referred toNivre (2003).2.2 Parsing AlgorithmThe parsing algorithm presented in Nivre (2003) is inmany ways similar to the basic shift-reduce algorithm forcontext-free grammars (Aho et al, 1986), although theparse actions are different given that no nonterminal sym-bols are used.
Moreover, unlike the algorithm of Yamadaand Matsumoto (2003), the algorithm considered here ac-tually uses a blend of bottom-up and top-down process-ing, constructing left-dependencies bottom-up and right-dependencies top-down, in order to achieve incremental-ity.
For a similar but nondeterministic approach to depen-dency parsing, see Obrebski (2003).Parser configurations are represented by triples?S, I, A?, where S is the stack (represented as a list), I isthe list of (remaining) input tokens, and A is the (current)arc relation for the dependency graph.
Given an inputstring W , the parser is initialized to ?nil,W, ??
and termi-nates when it reaches a configuration ?S,nil, A?
(for anylist S and set of arcs A).
The input string W is accepted ifthe dependency graph D = (W,A) given at terminationis well-formed; otherwise W is rejected.
The behavior ofthe parser is defined by the transitions defined in Figure3 (where wi, wj and wk are arbitrary word tokens, and rand r?
are arbitrary dependency relations):1.
The transition Left-Arc (LA) adds an arc wj r?wifrom the next input token wj to the token wi on topof the stack and reduces (pops) wi from the stack.2.
The transition Right-Arc (RA) adds an arc wi r?wjfrom the token wi on top of the stack to the next in-put token wj , and shifts (pushes) wj onto the stack.3.
The transition Reduce (RE) reduces (pops) the to-ken wi on top of the stack.4.
The transition Shift (SH) shifts (pushes) the next in-put token wi onto the stack.The transitions Left-Arc and Right-Arc are subject toconditions that ensure that the graph conditions Uniquelabel and Single head are satisfied.
By contrast, the Re-duce transition can only be applied if the token on top ofthe stack already has a head.
For Shift, the only conditionis that the input list is non-empty.As it stands, this transition system is nondeterminis-tic, since several transitions can often be applied to thesame configuration.
Thus, in order to get a deterministicparser, we need to introduce a mechanism for resolvingtransition conflicts.
Regardless of which mechanism isused, the parser is guaranteed to terminate after at most2n transitions, given an input string of length n (Nivre,2003).
This means that as long as transitions can be per-formed in constant time, the running time of the parserwill be linear in the length of the input.
Moreover, theparser is guaranteed to produce a dependency graph thatis acyclic and projective (and satisfies the unique-labeland single-head constraints).
This means that the depen-dency graph given at termination is well-formed if andonly if it is connected (Nivre, 2003).Unique label (wi r?wj ?
wi r?
?wj) ?
r = r?Single head (wi?wj ?
wk?wj) ?
wi = wkAcyclic ?
(wi?wj ?
wj?
?wi)Connected wi?
?wjProjective (wi?wk ?
wi<wj<wk) ?
(wi?
?wj ?
wk?
?wj)Figure 2: Well-formedness conditions on dependency graphsInitialization ?nil,W, ?
?Termination ?S,nil, A?Left-Arc ?wi|S,wj |I, A?
?
?S,wj |I, A ?
{(wj , r, wi)}?
??wk?r?
(wk, r?, wi) ?
ARight-Arc ?wi|S,wj |I, A?
?
?wj |wi|S, I, A ?
{(wi, r, wj)}?
??wk?r?
(wk, r?, wj) ?
AReduce ?wi|S, I, A?
?
?S, I, A?
?wj?r(wj , r, wi) ?
AShift ?S,wi|I, A?
?
?wi|S, I, A?Figure 3: Parser transitions2.3 Guided ParsingOne way of turning a nondeterministic parser into a deter-ministic one is to use a guide (or oracle) that can informthe parser at each nondeterministic choice point; cf.
Kay(2000), Boullier (2003).
Guided parsing is normally usedto improve the efficiency of a nondeterministic parser,e.g.
by letting a simpler (but more efficient) parser con-struct a first analysis that can be used to guide the choiceof the more complex (but less efficient) parser.
This is theapproach taken, for example, in Boullier (2003).In our case, we rather want to use the guide to im-prove the accuracy of a deterministic parser, starting froma baseline of randomized choice.
One way of doing thisis to use a treebank, i.e.
a corpus of analyzed sentences, totrain a classifier that can predict the next transition (anddependency type) given the current configuration of theparser.
However, in order to maintain the efficiency of theparser, the classifier must also be implemented in such away that each transition can still be performed in constanttime.Previous work in this area includes the use of memory-based learning to guide a standard shift-reduce parser(Veenstra and Daelemans, 2000) and the use of sup-port vector machines to guide a deterministic depen-dency parser (Yamada and Matsumoto, 2003).
In theexperiments reported in this paper, we apply memory-based learning within a deterministic dependency parsingframework.2.4 Memory-Based LearningMemory-based learning and problem solving is based ontwo fundamental principles: learning is the simple stor-age of experiences in memory, and solving a new problemis achieved by reusing solutions from similar previouslysolved problems (Daelemans, 1999).
It is inspired by thenearest neighbor approach in statistical pattern recogni-tion and artificial intelligence (Fix and Hodges, 1952), aswell as the analogical modeling approach in linguistics(Skousen, 1989; Skousen, 1992).
In machine learningterms, it can be characterized as a lazy learning method,since it defers processing of input until needed and pro-cesses input by combining stored data (Aha, 1997).Memory-based learning has been successfully appliedto a number of problems in natural language process-ing, such as grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, andbase noun phrase chunking (Daelemans et al, 2002).Most relevant in the present context is the use of memory-based learning to predict the actions of a shift-reduceparser, with promising results reported in Veenstra andDaelemans (2000).The main reason for using memory-based learning inthe present context is the flexibility offered by similarity-based extrapolation when classifying previously unseenconfigurations, since previous experiments with a proba-bilistic model has shown that a fixed back-off sequencedoes not work well in this case (Nivre, 2004).
Moreover,the memory-based approach can easily handle multi-classclassification, unlike the support vector machines used byYamada and Matsumoto (2003).For the experiments reported in this paper, we haveused the software package TiMBL (Tilburg MemoryBased Learner), which provides a variety of metrics, al-gorithms, and extra functions on top of the classical knearest neighbor classification kernel, such as value dis-tance metrics and distance weighted class voting (Daele-mans et al, 2003).3 Method3.1 Target Function and ApproximationThe function we want to approximate is a mapping ffrom parser configurations to parser actions, where eachaction consists of a transition and (unless the transition isShift or Reduce) a dependency type:f : Config ?
{LA,RA,RE, SH} ?
(R ?
{nil})Here Config is the set of all possible parser configura-tions and R is the set of dependency types as before.However, in order to make the problem tractable, we tryto learn a function f?
whose domain is a finite space ofparser states, which are abstractions over configurations.For this purpose we define a number of features that canbe used to define different models of parser state.
Thefeatures used in this study are listed in Table 1.The first five features (TOP?TOP.RIGHT) deal withproperties of the token on top of the stack.
In addition tothe word form itself (TOP), we consider its part-of-speech(as assigned by an automatic part-of-speech tagger in apreprocessing phase), the dependency type by which it isrelated to its head (which may or may not be available ina given configuration depending on whether the head isto the left or to the right of the token in question), andthe dependency types by which it is related to its leftmostand rightmost dependent, respectively (where the currentrightmost dependent may or may not be the rightmost de-pendent in the complete dependency tree).The following three features (NEXT?NEXT.LEFT) referto properties of the next input token.
In this case, there areno features corresponding to TOP.DEP and TOP.RIGHT,since the relevant dependencies can never be present atdecision time.
The final feature (LOOK) is a simple looka-head, using the part-of-speech of the next plus one inputtoken.In the experiments reported below, we have usedtwo different parser state models, one called the lexicalmodel, which includes all nine features, and one calledthe non-lexical model, where the two lexical featuresTOP and NEXT are omitted.
For both these models, wehave used memory-based learning with different parame-ter settings, as implemented TiMBL.For comparison, we have included an earlier classifierthat uses the same features as the non-lexical model, butwhere prediction is based on maximum conditional likeli-hood estimation.
This classifier always predicts the mostprobable transition given the state and the most probabledependency type given the transition and the state, withconditional probabilities being estimated by the empiri-cal distribution in the training data.
Smoothing is per-formed only for zero frequency events, in which case theclassifier backs off to more general models by omittingfirst the features TOP.LEFT and LOOK and then the fea-tures TOP.RIGHT and NEXT.LEFT; if even this does nothelp, the classifier predicts Reduce if permissible andShift otherwise.
This model, which we will refer to as theMCLE model, is described in more detail in Nivre (2004).3.2 DataIt is standard practice in data-driven approaches to nat-ural language parsing to use treebanks both for trainingand evaluation.
Thus, the Penn Treebank of AmericanEnglish (Marcus et al, 1993) has been used to train andevaluate the best available parsers of unrestricted Englishtext (Collins, 1999; Charniak, 2000).
One problem whendeveloping a parser for Swedish is that there is no com-parable large-scale treebank available for Swedish.For the experiments reported in this paper we haveused a manually annotated corpus of written Swedish,created at Lund University in the 1970?s and consistingmainly of informative texts from official sources (Einars-son, 1976).
Although the original annotation scheme isan eclectic combination of constituent structure, depen-dency structure, and topological fields (Teleman, 1974),it has proven possible to convert the annotated sentencesto dependency graphs with fairly high accuracy.In the conversion process, we have reduced the orig-inal fine-grained classification of grammatical functionsto a more restricted set of 16 dependency types, whichare listed in Table 2.
We have also replaced the origi-nal (manual) part-of-speech annotation by using the sameautomatic tagger that is used for preprocessing in theparser.
This is a standard probabilistic tagger trained onthe Stockholm-Umea?
Corpus of written Swedish (SUC,1997) and found to have an accuracy of 95?96% whentested on held-out data.Since the function we want to learn is a mapping fromparser states to transitions (and dependency types), thetreebank data cannot be used directly as training and testFeature DescriptionTOP The token on top of the stackTOP.POS The part-of-speech of TOPTOP.DEP The dependency type of TOP (if any)TOP.LEFT The dependency type of TOP?s leftmost dependent (if any)TOP.RIGHT The dependency type of TOP?s rightmost dependent (if any)NEXT The next input tokenNEXT.POS The part-of-speech of NEXTNEXT.LEFT The dependency type of NEXT?s leftmost dependent (if any)LOOK.POS The part-of-speech of the next plus one input tokenTable 1: Parser state featuresdata.
Instead, we have to simulate the parser on the tree-bank in order to derive, for each sentence, the transitionsequence corresponding to the correct dependency tree.Given the result of this simulation, we can construct adata set consisting of pairs ?s, t?, where s is a parser stateand t is the correct transition from that state (includinga dependency type if applicable).
Unlike standard shift-reduce parsing, the simulation of the current algorithm isalmost deterministic and is guaranteed to be correct if theinput dependency tree is well-formed.The complete converted treebank contains 6316 sen-tences and 97623 word tokens, which gives a mean sen-tence length of 15.5 words.
The treebank has been di-vided into three non-overlapping data sets: 80% for train-ing 10% for development/validation, and 10% for finaltesting (random samples).
The results presented beloware all from the validation set.
(The final test set has notbeen used at all in the experiments reported in this paper.
)When talking about test and validation data, we makea distinction between the sentence data, which refers tothe original annotated sentences in the treebank, and thetransition data, which refers to the transitions derived bysimulating the parser on these sentences.
While the sen-tence data for validation consists of 631 sentences, thecorresponding transition data contains 15913 instances.For training, only transition data is relevant and the train-ing data set contains 371977 instances.3.3 EvaluationThe output of the memory-based learner is a classifier thatpredicts the next transition (including dependency type),given the current state of the parser.
The quality of thisclassifier has been evaluated with respect to both predic-tion accuracy and parsing accuracy.Prediction accuracy refers to the quality of the clas-sifier as such, i.e.
how well it predicts the next transitiongiven the correct parser state, and is measured by the clas-sification accuracy on unseen transition data (using a 0-1loss function).
We use McNemar?s test for statistical sig-nificance.Parsing accuracy refers to the quality of the classifieras a guide for the deterministic parser and is measuredby the accuracy obtained when parsing unseen sentencedata.
More precisely, parsing accuracy is measured bythe attachment score, which is a standard measure usedin studies of dependency parsing (Eisner, 1996; Collinset al, 1999).
The attachment score is computed as theproportion of tokens (excluding punctuation) that are as-signed the correct head (or no head if the token is a root).Since parsing is a sentence-level task, we believe thatthe overall attachment score should be computed as themean attachment score per sentence, which gives an es-timate of the expected attachment score for an arbitrarysentence.
However, since most previous studies insteaduse the mean attachment score per word (Eisner, 1996;Collins et al, 1999), we will give this measure as well.In order to measure label accuracy, we also define a la-beled attachment score, where both the head and the labelmust be correct, but which is otherwise computed in thesame way as the ordinary (unlabeled) attachment score.For parsing accuracy, we use a paired t-test for statisticalsignificance.4 ResultsTable 3 shows the prediction accuracy achieved withmemory-based learning for the lexical and non-lexicalmodel, with two different parameter settings for thelearner.
The results in the first column were obtained withthe default settings of the TiMBL package, in particular:?
The IB1 classification algorithm (Aha et al, 1991).?
The overlap distance metric.?
Features weighted by Gain Ratio (Quinlan, 1993).?
k = 1, i.e.
classification based on a single nearestneighbor.11In TiMBL, the value of k in fact refers to k nearest dis-tances rather than k nearest neighbors, which means that, evenwith k = 1, the nearest neighbor set can contain several in-Label Dependency TypeADV Adverbial modifierAPP AppositionATT AttributeCC Coordination (conjunction or second conjunct)DET DeterminerID Non-first element of multi-word expressionIM Infinitive dependent on infinitive markerIP Punctuation mark dependent on lexical headINF Infinitival complementOBJ ObjectPR Complement of prepositionPRD Predicative complementSUB SubjectUK Main verb of subordinate clause dependent on complementizerVC Verb chain (nonfinite verb dependent on other verb)XX Unclassifiable dependentTable 2: Dependency types in Swedish treebankModel Default MaximumNon-lexical 86.8 87.4Lexical 88.4 89.7Table 3: Prediction accuracy for MBL modelsThe second column shows the accuracy for the best pa-rameter settings found in the experiments (averaged overboth models), which differ from the default in the follow-ing respects:?
Overlap metric replaced by the modified value dis-tance metric (MVDM) (Stanfill and Waltz, 1986;Cost and Salzberg, 1993).?
No weighting of features.?
k = 5, i.e.
classification based on 5 nearest neigh-bors.?
Distance weighted class voting with inverse distanceweighting (Dudani, 1976).For more information about the different parameters andsettings, the reader is referred to Daelemans et al (2003).The results show that the lexical model performs con-sistently better than the non-lexical model, and that thedifference increases with the optimization of the learningalgorithm (all differences being significant at the .0001level according to McNemar?s test).
This confirms pre-vious results from statistical parsing indicating that lex-ical information is crucial for disambiguation (Collins,stances that are equally distant to the test instance.
This is dif-ferent from the original IB1 algorithm, as described in Aha etal.
(1991).1999; Charniak, 2000).
As regards optimization, we maynote that although there is a significant improvement forboth models, the magnitude of the difference is relativelysmall.Table 4 shows the parsing accuracy obtained with theoptimized versions of the MBL models (lexical and non-lexical), compared to the MCLE model described in sec-tion 3.
We see that MBL outperforms the MCLE modeleven when limited to the same features (all differencesagain being significant at the .0001 level according toa paired t-test).
This can probably be explained by thefact that the similarity-based smoothing built into thememory-based approach gives a better extrapolation thanthe fixed back-off sequence in the MCLE model.
Wealso see that the lexical MBL model outperforms boththe other models.
If we compare the labeled attachmentscore to the prediction accuracy (which also takes depen-dency types into account), we observe a substantial drop(from 89.7 to 81.7 for the lexical model, from 87.4 to76.5 for the non-lexical model), which is of course onlyto be expected.
The unlabeled attachment score is natu-rally higher, and it is worth noting that the relative differ-ence between the MBL lexical model and the other twomodels is much smaller.
This indicates that the advan-tage of the lexical model mainly concerns the accuracy inpredicting dependency type in addition to transition.Model Labeled UnlabeledMCLE 74.7 (72.3) 81.5 (79.7)MBL non-lexical 76.5 (74.7) 82.9 (81.7)MBL lexical 81.7 (80.6) 85.7 (84.7)Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses)If we compare the results concerning parsing accuracyto those obtained for other languages (given that thereare no comparable results available for Swedish), we notethat the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach-ment score per word) (Collins et al, 1999; Yamada andMatsumoto, 2003), but higher than for Czech (Collins etal., 1999).
This is encouraging, given that the size ofthe training set in our experiments is fairly small, onlyabout 10% of the standard training set for the Penn Tree-bank.
One reason why our results nevertheless comparereasonably well with those obtained with the much largertraining set is probably that the conversion to dependencytrees is more accurate for the Swedish treebank, given theexplicit annotatation of grammatical functions.
More-over, the fact that our parser uses labeled dependenciesis probably also significant, since the possibility of us-ing information from previously assigned (labeled) de-pendencies during parsing seems to have a positive effecton accuracy (Nivre, 2004).Finally, it may be interesting to consider the accuracyfor individual dependency types.
Table 5 gives labeledprecision, labeled recall and unlabeled attachment scorefor four of the most important types with the MBL lex-ical model.
The results indicate that subjects have thehighest accuracy, especially when labels are taken intoaccount.
Objects and predicative complements have com-parable attachment accuracy, but are more often misclas-sified with respect to dependency type.
For adverbialmodifiers, finally, attachment accuracy is lower than forthe other dependency types, which is largely due to thenotorious PP-attachment problem.5 ConclusionIn this paper we have shown that a combination ofmemory-based learning and deterministic dependencyparsing can be used to construct a robust and efficientparser for unrestricted natural language text, achieving aparsing accuracy which is close to the state of the art evenwith relatively limited amounts of training data.
Clas-sifiers based on memory-based learning achieve higherparsing accuracy than previous probabilistic models, andthe improvement increases if lexical information is addedto the model.Suggestions for further research includes the furtherexploration of alternative models and parameter settings,but also the combination of inductive and analyticallearning to impose high-level linguistic constraints, andthe development of new parsing methods (e.g.
involvingmultiple passes over the data).
In addition, it is importantto evaluate the approach with respect to other languagesand corpora in order to increase the comparability withother approaches.AcknowledgementsThe work presented in this paper was supported by agrant from the Swedish Research Council (621-2002-4207).
The memory-based classifiers used in the experi-ments were constructed using the Tilburg Memory-BasedLearner (TiMBL) (Daelemans et al, 2003).
We are grate-ful to three anonymous reviewers for constructive com-ments on the preliminary version of the paper.ReferencesD.
W. Aha, D. Kibler and M. Albert.
1991.
Instance-based Learning Algorithms.
Machine Learning 6, 37?66.D.
Aha.
1997.
Lazy Learning.
Dordrecht: Kluwer.A.
V. Aho, R. Sethi and J. D. Ullman.
1986.
Compilers:Principles Techniques, and Tools.
Addison Wesley.P.
Boullier.
2003.
Guided Earley Parsing.
In G. van No-ord (ed.)
Proceedings of the 8th International Work-shop on Parsing Technologies (IWPT 03), Nancy,France, pp.
43?54.E.
Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proceedings NAACL-2000.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
PhD Thesis, University ofPennsylvania.M.
Collins, J. Hajic, E. Brill, L. Ramshaw and C. Till-mann.
1999.
A Statistical Parser of Czech.
In Pro-ceedings of 37th ACL Conference, University of Mary-land, College Park, USA, pp.
505?512.S.
Cost and S. Salzberg.
1993.
A Weighted NearestNeighbor Algorithm for Learning with Symbolic Fea-tures.
Machine Learning 10, 57?78.Dependency type Precision Recall AttachmentSUB 84.3 82.7 89.2OBJ 74.7 78.8 87.0PRD 75.1 71.4 84.2ADV 76.2 74.6 78.3Table 5: Dependency type accuracy, MBL lexical model; labeled precision, labeled recall, unlabeled attachment scoreW.
Daelemans.
1999.
Memory-Based Language Pro-cessing.
Introduction to the Special Issue.
Journalof Experimental and Theoretical Artificial Intelligence11(3), 287?292.W.
Daelemans, A. van den Bosch, J. Zavrel.
2002.
For-getting Exceptions is Harmful in Language Learning.Machine Learning 34, 11?43.W.
Daelemans, J. Zavrel, K. van der Sloot andA.
van den Bosch, .
2003.
TiMBL: Tilburg MemoryBased Learner, version 5.0, Reference Guide.
Techni-cal Report ILK 03-10, Tilburg University.S.
A. Dudani.
1976.
The Distance-Weighted K-nearestNeighbor Rule.
IEEE Transactions on Systems, Man,and Cybernetics SMC-6, 325?327.J.
Einarsson.
1976.
Talbankens skriftsprkskonkordans.Lund University.J.
M. Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
In Proceedingsof COLING-96, Copenhagen.E.
Fix and J. Hodges.
1952.
Discriminatory Analy-sis: Nonparametric Discrimination: Consistency Prop-erties.
Technical Report 21-49-004-11, USAF Schoolof Aviation Medicine, Randolph Field, Texas.R.
A. Hudson.
1990.
English Word Grammar.
Black-well.M.
Kay.
2000.
Guides and Oracles for Linear-Time Pars-ing.
In Proceedings of the 6th International Workshopon Parsing Technologies (IWPT 00), Trento, Italy, pp.6?9.M.
P. Marcus, B. Santorini and M. A. Marcinkiewics.1993.
Building a Large Annotated Corpus of English:The Penn Treebank.
Computational Linguistics 19,313?330.I.
Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.J.
Nivre.
2003.
An Efficient Algorithm for Projective De-pendency Parsing.
In G. van Noord (ed.)
Proceedingsof the 8th International Workshop on Parsing Tech-nologies (IWPT 03), Nancy, France, pp.
149?160.J.
Nivre.
2004.
Inductive Dependency Parsing.
Techni-cal Report, Va?xjo?
University.T.
Obrebski.
2003.
Dependency Parsing Using Depen-dency Graph.
In G. van Noord (ed.)
Proceedings ofthe 8th International Workshop on Parsing Technolo-gies (IWPT 03), Nancy, France, pp.
217?218.J.
R. Quinlan.
1993.
C4.5: Programs for MachineLearning.
San Mateo, CA: Morgan Kaufmann.P.
Sgall, E. Hajicova?
and J. Panevova?.
1986.
The Mean-ing of the Sentence in Its Pragmatic Aspects.
Reidel.R.
Skousen.
1989.
Analogical Modeling of Language.Dordrecht: Kluwer.R.
Skousen.
1992.
Analogy and Structure.
Dordrecht:Kluwer.C.
Stanfill and D. Waltz.
1986.
Toward Memory-BasedReasoning.
Communications of the ACM 29(12),1213?1228.SUC 1997.
Stockholm Umea?
Corpus.
Version 1.0.
Pro-duced by Department of Linguistics, Umea?
Universityand Department of Linguistics, Stockholm University.U.
Teleman.
1974.
Manual fo?r grammatisk beskrivningav talad och skriven svenska.
Studentlitteratur.L.
Tesnie`re.
1959.
Ele?ments de syntaxe structurale.
Edi-tions KlincksieckJ.
Veenstra and W. Daelemans.
2000.
A Memory-BasedAlternative for Connectionist Shift-Reduce Parsing.Technical Report ILK-0012, University of Tilburg.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.
InG.
van Noord (ed.)
Proceedings of the 8th Interna-tional Workshop on Parsing Technologies (IWPT 03),Nancy, France, pp.
195?206.
