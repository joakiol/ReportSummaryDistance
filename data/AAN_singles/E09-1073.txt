Proceedings of the 12th Conference of the European Chapter of the ACL, pages 639?647,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsOutclassing Wikipedia in Open-Domain Information Extraction:Weakly-Supervised Acquisition of Attributes over Conceptual HierarchiesMarius Pas?caGoogle Inc.Mountain View, California 94043mars@google.comAbstractA set of labeled classes of instances is ex-tracted from text and linked into an exist-ing conceptual hierarchy.
Besides a signif-icant increase in the coverage of the classlabels assigned to individual instances, theresulting resource of labeled classes ismore effective than similar data derivedfrom the manually-created Wikipedia, inthe task of attribute extraction over con-ceptual hierarchies.1 IntroductionMotivation: Sharing basic intuitions and long-term goals with other tasks within the area of Web-based information extraction (Banko and Etzioni,2008; Davidov and Rappoport, 2008), the taskof acquiring class attributes relies on unstructuredtext available on the Web, as a data source for ex-tracting generally-useful knowledge.
In the caseof attribute extraction, the knowledge to be ex-tracted consists in quantifiable properties of var-ious classes (e.g., top speed, body style and gasmileage for the class of sports cars).Existing work on large-scale attribute extractionfocuses on producing ranked lists of attributes, fortarget classes of instances available in the formof flat sets of instances (e.g., ferrari modena,porsche carrera gt) sharing the same class label(e.g., sports cars).
Independently of how the inputtarget classes are populated with instances (man-ually (Pas?ca, 2007) or automatically (Pas?ca andVan Durme, 2008)), and what type of textual datasource is used for extracting attributes (Web docu-ments or query logs), the extraction of attributesoperates at a lexical rather than semantic level.Indeed, the class labels of the target classes maybe not more than text surface strings (e.g., sportscars) or even artificially-created labels (e.g., Car-toonChar in lieu of cartoon characters).
More-over, although it is commonly accepted that sportscars are also cars, which in turn are also motor ve-hicles, the presence of sports cars among the inputtarget classes does not lead to any attributes beingextracted for cars and motor vehicles, unless thelatter two class labels are also present explicitlyamong the input target classes.Contributions: The contributions of this paperare threefold.
First, we investigate the role ofclasses of instances acquired automatically fromunstructured text, in the task of attribute extrac-tion over concepts from existing conceptual hi-erarchies.
For this purpose, ranked lists of at-tributes are acquired from query logs for variousconcepts, after linking a set of more than 4,500open-domain, automatically-acquired classes con-taining a total of around 250,000 instances intoconceptual hierarchies available in WordNet (Fell-baum, 1998).
In comparison, previous workextracts attributes for either manually-specifiedclasses of instances (Pas?ca, 2007), or for classes ofinstances derived automatically but considered asflat rather than hierarchical classes, and manuallyassociated to existing semantic concepts (Pas?caand Van Durme, 2008).
Second, we expand theset of classes of instances acquired from text, thusincreasing their usefulness in attribute extractionin particular and information extraction in general.To this effect, additional class labels (e.g., mo-tor vehicles) are identified for existing instances(e.g., ferrari modena) of existing class labels (e.g.,sports cars), by exploiting IsA relations availablewithin the conceptual hierarchy (e.g., sports carsare also motor vehicles).
Third, we show thatlarge-scale, automatically-derived classes of in-639stances can have as much as, or even bigger, prac-tical impact in open-domain information extrac-tion tasks than similar data from large-scale, high-coverage, manually-compiled resources.
Specif-ically, evaluation results indicate that the accu-racy of the extracted lists of attributes is higherby 8% at rank 10, 13% at rank 30 and 18% atrank 50, when using the automatically-extractedclasses of instances rather than the comparativelymore numerous and a-priori more reliable, human-generated, collaboratively-vetted classes of in-stances available within Wikipedia (Remy, 2002).2 Attribute Extraction over HierarchiesExtraction of Flat Labeled Classes: Unstruc-tured text from a combination of Web documentsand query logs represents the source for derivinga flat set of labeled classes of instances, which arenecessary as input for attribute extraction experi-ments.
The labeled classes are acquired in threestages:1) extraction of a noisy pool of pairs of aclass label and a potential class instance, by ap-plying a few Is-A extraction patterns, selectedfrom (Hearst, 1992), to Web documents:(fruits, apple), (fruits, corn), (fruits, mango),(fruits, orange), (foods, broccoli), (crops, lettuce),(flowers, rose);2) extraction of unlabeled clusters of distribu-tionally similar phrases, by clustering vectors ofcontextual features collected around the occur-rences of the phrases within Web documents (Linand Pantel, 2002):{lettuce, broccoli, corn, ..},{carrot, mango, apple, orange, rose, ..};3) merging and filtering of the raw pairs and un-labeled clusters into smaller, more accurate sets ofclass instances associated with class labels, in anattempt to use unlabeled clusters to filter noisy rawpairs instead of merely using clusters to general-ize class labels across raw pairs (Pas?ca and VanDurme, 2008):fruits={apple, mango, orange, ..}.To increase precision, the vocabulary of classinstances is confined to the set of queries that aremost frequently submitted to a general-purposeWeb search engine.
After merging, the resultingpairs of an instance and a class label are arrangedinto instance sets (e.g., {ferrari modena, porschecarrera gt}), each associated with a class label(e.g., sports cars).Linking Labeled Classes into Hierarchies:Manually-constructed language resources such asWordNet provide reliable, wide-coverage upper-level conceptual hierarchies, by grouping togetherphrases with the same meaning (e.g., {analgesic,painkiller, pain pill}) into sets of synonyms(synsets), and organizing the synsets into concep-tual hierarchies (e.g., painkillers are a subconcept,or a hyponym, of drugs) (Fellbaum, 1998).
To de-termine the points of insertion of automatically-extracted labeled classes into hand-built Word-Net hierarchies, the class labels are looked up inWordNet using built-in morphological normaliza-tion routines.
When a class label (e.g., age-relateddiseases) is not found in WordNet, it is looked upagain after iteratively removing its leading words(e.g., related diseases, and diseases) until a poten-tial point of insertion is found where one or moresenses exist in WordNet for the class label.An efficient heuristic for sense selection is touniformly choose the first (that is, most frequent)sense of the class label in WordNet, as point ofinsertion.
Due to its simplicity, the heuristic isbound to make errors whenever the correct sense isnot the first one, thus incorrectly linking academicjournals under the sense of journals as personaldiaries rather than periodicals, and active volca-noes under the sense of volcanoes as fissures inthe earth, rather than mountains formed by vol-canic material.
Nevertheless, choosing the firstsense is attractive for three reasons.
First, Word-Net senses are often too fine-grained, making thetask of choosing the correct sense difficult evenfor humans (Palmer et al, 2007).
Second, choos-ing the first sense from WordNet is sometimesbetter than more intelligent disambiguation tech-niques (Pradhan et al, 2007).
Third, previous ex-perimental results on linking Wikipedia classes toWordNet concepts confirm that first-sense selec-tion is more effective in practice than other tech-niques (Suchanek et al, 2007).
Thus, a class la-bel and its associated instances are inserted underthe first WordNet sense available for the class la-bel.
For example, silicon valley companies and itsassociated instances (apple, hewlett packard etc.
)are inserted under the first of the 9 senses of com-panies in WordNet, which corresponds to compa-nies as institutions created to conduct business.In order to trade off coverage for higher preci-sion, the heuristic can be restricted to link a classlabel under the first WordNet sense available, as640before, but only when no other senses are avail-able at the point of insertion beyond the first sense.With the modified heuristic, the class label internetsearch engines is linked under the first and onlysense of search engines in WordNet, but siliconvalley companies is no longer linked under the firstof the 9 senses of companies.Extraction of Attributes for Hierarchy Con-cepts: The labeled classes of instances linked toconceptual hierarchies constitute the input to theacquisition of attributes of hierarchy concepts, bymining a collection of Web search queries.
The at-tributes capture properties that are relevant to theconcept.
The extraction of attributes exploits thesets of class instances rather than the associatedclass labels.
More precisely, for each hierarchyconcept for which attributes must be extracted, theinstances associated to all class labels linked un-der the subhierarchy rooted at the concept are col-lected as a union set of instances, thus exploitingthe transitivity of IsA relations.
This step is equiv-alent to propagating the instances upwards, fromtheir class labels to higher-level WordNet conceptsunder which the class labels are linked, up to theroot of the hierarchy.
The resulting sets of in-stances constitute the input to the acquisition ofattributes, which consists of four stages:1) identification of a noisy pool of candidate at-tributes, as remainders of queries that also con-tain one of the class instances.
In the case of theconcept movies, whose instances include jay andsilent bob strike back and kill bill, the query ?castjay and silent bob strike back?
produces the can-didate attribute cast;2) construction of internal vector representa-tions for each candidate attribute, based on queries(e.g., ?cast selection for kill bill?)
that contain acandidate attribute (cast) and a class instance (killbill).
These vectors consist of counts tied to thefrequency with which an attribute occurs with agiven ?templatized?
query.
The latter replaces spe-cific attributes and instances from the query withcommon placeholders, e.g., ?X for Y?
;3) construction of a reference internal vectorrepresentation for a small set of seed attributesprovided as input.
A reference vector is the nor-malized sum of the individual vectors correspond-ing to the seed attributes;4) ranking of candidate attributes with respectto each concept, by computing the similarity be-tween their individual vector representations andthe reference vector of the seed attributes.The result of the four stages, which are de-scribed in more detail in (Pas?ca, 2007), is a rankedlist of attributes (e.g., [opening song, cast, charac-ters,...]) for each concept (e.g., movies).3 Experimental SettingTextual Data Sources: The acquisition of open-domain knowledge relies on unstructured textavailable within a combination of Web documentsmaintained by, and search queries submitted to theGoogle search engine.
The textual data sourcefor extracting labeled classes of instances con-sists of around 100 million documents in En-glish, as available in a Web repository snapshotfrom 2006.
Conversely, the acquisition of open-domain attributes relies on a random sample offully-anonymized queries in English submitted byWeb users in 2006.
The sample contains about 50million unique queries.
Each query is accompa-nied by its frequency of occurrence in the logs.Other sources of similar data are available publiclyfor research purposes (Gao et al, 2007).Parameters for Extracting Labeled Classes:When applied to the available document col-lection, the method for extracting open-domainclasses of instances from unstructured text intro-duced in (Pas?ca and Van Durme, 2008) produces4,583 class labels associated to 258,699 uniqueinstances, for a total of 869,118 pairs of a classinstance and an associated class label.
All col-lected instances occur among to the top five mil-lion queries with the highest frequency within theinput query logs.
The data is further filtered bydiscarding labeled classes with fewer than 25 in-stances.
The classes, examples of which are shownin Table 1, are linked under conceptual hierarchiesavailable within WordNet 3.0, which contains a to-tal of 117,798 English noun phrases grouped in82,115 concepts (or synsets).Parameters for Extracting Attributes: For eachtarget concept from the hierarchy, given the unionof all instances associated to class labels linked tothe target concept or one of its subconcepts, andgiven a set of five seed attributes (e.g., {quality,speed, number of users, market share, reliabil-ity} for search engines), the method describedin (Pas?ca, 2007) extracts ranked lists of attributesfrom the input query logs.
Internally, the rank-ing of attributes uses Jensen-Shannon (Lee, 1999)to compute similarity scores between internal rep-641Class Label Class Size Class Instancesaccounting systems 40 flexcube, myob, oracle financials, peachtree accounting, sybizantimicrobials 97 azithromycin, chloramphenicol, fusidic acid, quinolones, sulfa drugscivilizations 197 ancient greece, chaldeans, etruscans, inca, indians, roman republicelementary particles 33 axions, electrons, gravitons, leptons, muons, neutrons, positronsfarm animals 61 angora goats, burros, cattle, cows, donkeys, draft horses, mule, oxenforages 27 alsike clover, rye grass, tall fescue, sericea lespedeza, birdsfoot trefoilideologies 179 egalitarianism, laissez-faire capitalism, participatory democracysocial events 436 academic conferences, afternoon teas, block parties, masquerade ballsTable 1: Examples of instances within labeled classes extracted from unstructured text, used as input forattribute extraction experimentsresentations of seed attributes, on one hand, andeach of the newly acquired attributes, on the otherhand.
Depending on the experiments, the amountof supervision is thus limited to either 5 seed at-tributes for each target concept, or to 5 seed at-tributes (population, area, president, flag and cli-mate) provided for only one of the extracted la-beled classes, namely european countries.Experimental Runs: The experiments consist offour different runs, which correspond to differentchoices for the source of conceptual hierarchiesand class instances linked to those hierarchies, asillustrated in Table 2.
In the first run, denoted N,the class instances are those available within thelatest version of WordNet (3.0) itself via HasIn-stance relations.
The second run, Y, corresponds toan extension of WordNet based on the manually-compiled classes of instances from categories inWikipedia, as available in the 2007-w50-5 versionof Yago (Suchanek et al, 2007).
Therefore, run Yhas the advantage of the fact that Wikipedia cat-egories are a rich source of useful and accurateknowledge (Nastase and Strube, 2008), which ex-plains their previous use as a source for evaluationgold standards (Blohm et al, 2007).
The last tworuns from Table 2, Es and Ea, correspond to theset of open-domain labeled classes acquired fromunstructured text.
In both Es and Ea, class labelsare linked to the first sense available at the pointof insertion in WordNet.
In Es, the class labelsare linked only if no other senses are available atthe point of insertion beyond the first sense, thuspromoting higher linkage precision at the expenseof fewer links.
For example, since the phrases im-pressionists, sports cars and painters have 1, 1 and4 senses available in WordNet respectively, theclass labels french impressionists and sports carsare linked to the respective WordNet concepts,whereas the class label painters is not.
Compar-atively, in Ea, the class labels are uniformly linkedDescription Source of Hierarchy and InstancesN Y Es EaInclude instances?
?- -from WordNet?Include instances -?
?
?from elsewhere?#Instances (?103) 14.3 1,296.5 108.0 257.0#Class labels 945 30,338 1,315 4,517#Pairs of a class label 17.4 2,839.8 191.0 859.0and instance (?103)Table 2: Source of class instances for various ex-perimental runsto the first sense available in WordNet, regardlessof whether other senses may or may not be avail-able.
Thus, Ea trades off potentially lower preci-sion for the benefit of higher linkage recall, andresults in more of the class labels and their asso-ciated instances extracted from text to be linked toWordNet than in the case of run Es.4 Evaluation4.1 Evaluation of Labeled ClassesCoverage of Class Instances: In run N, the in-put class instances are the component phrases ofsynsets encoded via HasInstance relations underother synsets in WordNet.
For example, the synsetcorresponding to {search engine}, defined as ?acomputer program that retrieves documents orfiles or data from a database or from a computernetwork?, has 3 HasInstance instances in Word-Net, namely Ask Jeeves, Google and Yahoo.
Ta-ble 3 illustrates the coverage of the class instancesextracted from unstructured text and linked toWordNet in runs Es and Ea respectively, relative toall 945 WordNet synsets that contain HasInstanceinstances.
Note that the coverage scores are con-servative assessments of actual coverage, since arun (i.e., Es or Ea) receives credit for a WordNetinstance only if the run contains an instance thatis a full-length, case-insensitive match (e.g., ask642Concept HasInstance Instances within WordNet CvgSynset Offset Examples Count Es Ea{existentialist, existentialist, 10071557 Albert Camus, Beauvoir, Camus, 8 1.00 1.00philosopher, existential philosopher} Heidegger, Jean-Paul Sartre{search engine} 06578654 Ask Jeeves, Google, Yahoo 3 1.00 1.00{university} 04511002 Brown, Brown University, 44 0.61 0.77Carnegie Mellon University{continent} 09254614 Africa, Antarctic continent, Europe, 13 0.54 0.54Eurasia, Gondwanaland, Laurasia{microscopist} 10313872 Anton van Leeuwenhoek, Anton 6 0.00 0.00van Leuwenhoek, SwammerdamAverage over all 945 WordNet concepts that have HasInstance instance(s) 18.71 0.21 0.40Table 3: Coverage of class instances extracted from text and linked to WordNet (used as input in runs Esand Ea respectively), measured as the fraction of WordNet HasInstance instances (used as input in runN) that occur among the class instances (Cvg=coverage)jeeves) of the WordNet instance.
On average, thecoverage scores for class instances of runs Es andEa relative to run N are 0.21 and 0.40 respectively,as shown in the last row in Table 3.
Comparatively,the equivalent instance coverage for run Y, whichalready includes most of the WordNet instances bydesign (cf.
(Suchanek et al, 2007)), is 0.59.Relative Coverage of Class Labels: The link-ing of class labels to WordNet concepts allows forthe expansion of the set of classes of instances ac-quired from text, thus increasing its usefulness inattribute extraction in particular and informationextraction in general.
To this effect, additionalclass labels are identified for existing instances,in the form of component phrases of the synsetsthat are superconcepts (or hypernyms, in WordNetterminology) of the synset under which the classlabel of the instance is linked in WordNet.
For ex-ample, since the class label sports cars is linkedunder the WordNet synset {sports car, sport car},and the latter has the synset {motor vehicle, auto-motive vehicle} among its hypernyms, the phrasesmotor vehicles and automotive vehicles are col-lected as new class labels 1 and associated to ex-isting instances of sports cars from the originalset, such as ferrari modena.
No phrases are col-lected from a selected set of 10 top-level Word-Net synsets, including {entity} and {object, phys-ical object}, which are deemed too general to beuseful as class labels.
As illustrated in Table 4,a collected pair of a new class label and an exist-ing instance either does not have any impact, if thepair already occurs in the original set of labeled1For consistency with the original labeled classes, newclass labels collected from WordNet are converted from sin-gular (e.g., motor vehicle) to plural (e.g., motor vehicles).Already in original labeled classes:painters alfred sisleyeuropean countries austriaExpansion of existing labeled classes:animals avocetanimals northern oriolescientists howard gardnerscientists phil zimbardoCreation of new labeled classes:automotive vehicles acura nsxautomotive vehicles detomaso panteracreative persons aaron coplandcreative persons yoshitomo naraTable 4: Examples of additional class labels col-lected from WordNet, for existing instances of theoriginal labeled classes extracted from textclasses; or expands existing classes, if the classlabel already occurs in the original set of labeledclasses but not in association to the instance; orcreates new classes of instances, if the class labelis not part of the original set.
The latter two casesaggregate to increases in coverage, relative to thepairs from the original sets of labeled classes, of53% for Es and 304% for Ea.4.2 Evaluation of AttributesTarget Hierarchy Concepts: The performance ofattribute extraction is assessed over a set of 25 tar-get concepts also used for evaluation in (Pas?ca,2008).
The set of 25 target concepts includes: Ac-tor, Award, Battle, CelestialBody, ChemicalEle-ment, City, Company, Country, Currency, Dig-italCamera, Disease, Drug, FictionalCharacter,Flower, Food, Holiday, Mountain, Movie, Nation-alPark, Painter, Religion, River, SearchEngine,Treaty, Wine.
Each target concept represents ex-actly one WordNet concept (synset).
For instance,643one of the target concepts, denoted Country, cor-responds to a synset situated at the internal off-set 08544813 in WordNet 3.0, which groups to-gether the synonymous phrases country, state andland and associates them with the definition ?theterritory occupied by a nation?.
The target con-cepts exhibit variation with respect to their depthswithin WordNet conceptual hierarchies, rangingfrom a minimum of 5 (e.g., for Food) to a maxi-mum of 11 (for Flower), with a mean depth of 8over the 25 concepts.Evaluation Procedure: The measurement of re-call requires knowledge of the complete set ofitems (in our case, attributes) to be extracted.
Un-fortunately, this number is often unavailable in in-formation extraction tasks in general (Hasegawaet al, 2004), and attribute extraction in particular.Indeed, the manual enumeration of all attributesof each target concept, to measure recall, is un-feasible.
Therefore, the evaluation focuses on theassessment of attribute accuracy.To remove any bias towards higher-ranked at-tributes during the assessment of class attributes,the ranked lists of attributes produced by each runto be evaluated are sorted alphabetically into amerged list.
Each attribute of the merged list ismanually assigned a correctness label within itsrespective class.
In accordance with previouslyintroduced methodology, an attribute is vital if itmust be present in an ideal list of attributes ofthe class (e.g., side effects for Drug); okay if itprovides useful but non-essential information; andwrong if it is incorrect (Pas?ca, 2007).To compute the precision score over a rankedlist of attributes, the correctness labels are con-verted to numeric values (vital to 1, okay to 0.5and wrong to 0).
Precision at some rank N in thelist is thus measured as the sum of the assignedvalues of the first N attributes, divided by N .Attribute Accuracy: Figure 1 plots the precisionat ranks 1 through 50 for the ranked lists of at-tributes extracted by various runs as an averageover the 25 target concepts, along two dimensions.In the leftmost graphs, each of the 25 target con-cepts counts towards the computation of precisionscores of a given run, regardless of whether anyattributes were extracted or not for the target con-cept.
In the rightmost graphs, only target con-cepts for which some attributes were extracted areincluded in the precision scores of a given run.Thus, the leftmost graphs properly penalize a run0.20.40.60.810  10  20  30  40  50PrecisionRankClass: Average-ClassNYEsEa0.20.40.60.810  10  20  30  40  50PrecisionRankClass: Average-ClassNYEsEa0.20.40.60.810  10  20  30  40  50PrecisionRankClass: Average-ClassNYEsEa0.20.40.60.810  10  20  30  40  50PrecisionRankClass: Average-ClassNYEsEaFigure 1: Accuracy of the attributes extracted forvarious runs, as an average over the entire set of25 target concepts (left graphs) and as an averageover (variable) subsets of the 25 target conceptsfor which some attributes were extracted in eachrun (right graphs).
Seed attributes are provided asinput for only one target concept (top graphs), orfor each target concept (bottom graphs)for failing to extract any attributes for some tar-get concepts, whereas the rightmost graphs do notinclude any such penalties.
On the other dimen-sion, in the graphs at the top of Figure 1, seed at-tributes are provided only for one class (namely,european countries), for a total of 5 attributes overall classes.
In the graphs at the bottom of the fig-ure, there are 5 seed attributes for each of the 25target concepts in the graphs at the bottom of Fig-ure 1, for a total of 5?25=125 attributes.Several conclusions can be drawn after inspect-ing the results.
First, providing more supervi-sion, in the form of seed attributes for all conceptsrather than for only one concept, translates intohigher attribute accuracy for all runs, as shownby the graphs at the top vs. graphs at the bot-tom of Figure 1.
Second, in the leftmost graphs,run N has the lowest precision scores, which is inline with the relatively small number of instancesavailable in the original WordNet, as confirmed bythe counts from Table 2.
Third, in the leftmostgraphs, the more restrictive run Es has lower pre-cision scores across all ranks than its less restric-tive counterpart Ea.
In other words, adding more644Class Precision@10 @30 @50N Y Es Ea N Y Es Ea N Y Es EaActor 1.00 1.00 1.00 1.00 0.78 0.85 0.98 0.95 0.62 0.84 0.95 0.96Award 0.00 0.50 0.95 0.85 0.00 0.35 0.80 0.73 0.00 0.29 0.70 0.69Battle 0.80 0.90 0.00 0.90 0.76 0.80 0.00 0.80 0.74 0.72 0.00 0.73CelestialBody 1.00 1.00 1.00 0.40 1.00 1.00 0.93 0.16 0.98 0.89 0.91 0.12ChemicalElement 0.00 0.65 0.80 0.80 0.00 0.45 0.83 0.63 0.00 0.48 0.84 0.51City 1.00 1.00 0.00 1.00 0.86 0.80 0.00 0.83 0.78 0.70 0.00 0.76Company 0.00 1.00 0.90 1.00 0.00 0.90 0.93 0.88 0.00 0.77 0.82 0.80Country 1.00 0.90 1.00 1.00 0.98 0.81 0.96 0.96 0.97 0.76 0.98 0.97Currency 0.00 0.90 0.00 0.90 0.00 0.53 0.00 0.83 0.00 0.36 0.00 0.87DigitalCamera 0.00 0.20 0.85 0.85 0.00 0.10 0.85 0.85 0.00 0.10 0.82 0.82Disease 0.00 0.60 0.75 0.75 0.00 0.76 0.83 0.83 0.00 0.63 0.87 0.86Drug 0.00 1.00 1.00 1.00 0.00 0.91 1.00 1.00 0.00 0.88 0.96 0.96FictionalCharacter 0.80 0.70 0.00 0.55 0.65 0.48 0.00 0.38 0.42 0.41 0.00 0.34Flower 0.00 0.65 0.00 0.70 0.00 0.26 0.00 0.55 0.00 0.16 0.00 0.53Food 0.00 0.80 0.90 1.00 0.00 0.65 0.71 0.96 0.00 0.53 0.59 0.96Holiday 0.00 0.60 0.80 0.80 0.00 0.50 0.48 0.48 0.00 0.37 0.41 0.41Mountain 1.00 0.75 0.00 0.90 0.96 0.61 0.00 0.86 0.77 0.58 0.00 0.74Movie 0.00 1.00 1.00 1.00 0.00 0.90 0.80 0.78 0.00 0.85 0.75 0.74NationalPark 0.90 0.80 0.00 0.00 0.85 0.76 0.00 0.00 0.82 0.75 0.00 0.00Painter 1.00 1.00 1.00 1.00 0.96 0.93 0.88 0.96 0.92 0.89 0.76 0.93Religion 0.00 0.00 1.00 1.00 0.00 0.00 1.00 1.00 0.00 0.00 0.92 0.97River 1.00 0.80 0.00 0.00 0.70 0.60 0.00 0.00 0.61 0.58 0.00 0.00SearchEngine 0.40 0.00 0.25 0.25 0.23 0.00 0.35 0.35 0.32 0.00 0.43 0.43Treaty 0.50 0.90 0.80 0.80 0.33 0.65 0.53 0.53 0.26 0.59 0.42 0.42Wine 0.00 0.30 0.80 0.80 0.00 0.26 0.43 0.45 0.00 0.20 0.28 0.29Average (over 25) 0.41 0.71 0.59 0.77 0.36 0.59 0.53 0.67 0.32 0.53 0.49 0.63Average (over non-empty) 0.86 0.78 0.87 0.83 0.75 0.64 0.78 0.73 0.68 0.57 0.73 0.68Table 5: Comparative accuracy of the attributes extracted by various runs, for individual concepts, as anaverage over the entire set of 25 target concepts, and as an average over (variable) subsets of the 25 targetconcepts for which some attributes were extracted in each run.
Seed attributes are provided as input foreach target conceptrestrictions may improve precision but hurts recallof class instances, which results in lower averageprecision scores for the attributes.
Fourth, in theleftmost graphs, the runs using the automatically-extracted labeled classes (Es and Ea) not only out-perform N, but one of them (Ea) also outperformsY.
This is the most important result.
It showsthat large-scale, automatically-derived classes ofinstances can have as much as, or even bigger,practical impact in attribute extraction than similardata from larger (cf.
Table 2), manually-compiled,collaboratively created and maintained resourcessuch as Wikipedia.
Concretely, in the graph onthe bottom left of Figure 1, the precision scores atranks 10, 30 and 50 are 0.71, 0.59 and 0.53 for runY, but 0.77, 0.67 and 0.63 for run Ea.
The scorescorrespond to attribute accuracy improvements of8% at rank 10, 13% at rank 30, and 18% at rank50 for run Ea over run Y.
In fact, in the rightmostgraphs, that is, without taking into account targetconcepts without any extracted attributes, the pre-cision scores of both Es and Ea are higher than forrun Y across most, if not all, ranks from 1 through50.
In this case, it is E1 that produces the mostaccurate attributes, in a task-based demonstrationthat the more cautious linking of class labels toWordNet concepts in Es vs. Ea leads to less cov-erage but higher precision of the linked labeledclasses, which translates into extracted attributesof higher accuracy but for fewer target concepts.Analysis: The curves plotted in the two graphsat the bottom of Figure 1 are computed as av-erages over precision scores for individual targetconcepts, which are shown in detail in Table 5.Precision scores of 0.00 correspond to runs forwhich no attributes are acquired from query logs,because no instances are available in the subhier-archy rooted at the respective concepts.
For exam-ple, precision scores for run N are 0.00 for Awardand DigitalCamera, among others concepts in Ta-ble 5, due to the lack of any HasInstance instancesin WordNet for the respective concepts.
The num-ber of target concepts for which some attributesare extracted is 12 for run N, 23 for Y, 17 for Es645and 23 for Ea.
Thus, both run N and run Es exhibitrather binary behavior across individual classes, inthat they tend to either not retrieve any attributes orretrieve attributes of relatively higher quality thanthe other runs, causing Es and N to have the worstprecision scores in the last but one row of Table 5,but the best precision scores in the last row of Ta-ble 5.The individual scores shown for Es and Ea inTable 5 concur with the conclusion drawn earlierfrom the graphs in Figure 1, that Run Es has lowerprecision than Ea as an average over all target con-cepts.
Notable exceptions are the scores obtainedfor the concepts CelestialBody and ChemicalEle-ment, where Es significantly outperforms Ea in Ta-ble 5.
This is due to confusing instances (e.g., kobebryant) being associated with class labels (e.g.,nba stars) that are incorrectly linked under the tar-get concepts (e.g., Star, which is a subconcept ofCelestialBody in WordNet) in Ea, but not linked atall and thus not causing confusion in Es.Run Y performs better than Ea for 5 of the 25individual concepts, including NationalPark, forwhich no instances of national parks or relatedclass labels are available in run Ea; and River, forwhich relevant instances in the labeled classes inEa, but they are associated to the class label riversystems, which is incorrectly linked to the Word-Net concept systems rather than to rivers.
How-ever, run Ea outperforms Y on 12 individual con-cepts (e.g., Award, DigitalCamera and Disease),and also as an average over all classes (last tworows in Table 5).5 Related WorkPrevious work on the automatic acquisition of at-tributes for open-domain classes from text requiresthe manual enumeration of sets of instances andseed attributes, for each class for which attributesare to be extracted.
In contrast, the current methodoperates on automatically-extracted classes.
Theexperiments reported in (Pas?ca and Van Durme,2008) also exploit automatically-extracted classesfor the purpose of attribute extraction.
However,they operate on flat classes, as opposed to conceptsorganized hierarchically.
Furthermore, they re-quire manual mappings from extracted class labelsinto a selected set of evaluation classes (e.g., bymapping river systems to River, football clubs toSoccerClub, and parks to NationalPark), whereasthe current method maps class labels to conceptsautomatically, by linking class labels and their as-sociated instances to concepts.
Manually-encodedattributes available within Wikipedia articles areused in (Wu and Weld, 2008) in order to deriveother attributes from unstructured text within Webdocuments.
Comparatively, the current methodextracts attributes from query logs rather thanWeb documents, using labeled classes extractedautomatically rather than available in manually-created resources, and requiring minimal supervi-sion in the form of only 5 seed attributes providedfor only one concept, rather than thousands of at-tributes available in millions of manually-createdWikipedia articles.
To our knowledge, there isonly one previous study (Pas?ca, 2008) that directlyaddresses the problem of extracting attributes overconceptual hierarchies.
However, that study useslabeled classes extracted from text with a differentmethod; extracts attributes for labeled classes andpropagates them upwards in the hierarchy, in orderto compute attributes of hierarchy concepts fromattributes of their subconcepts; and does not con-sider resources similar to Wikipedia, as sources ofinput labeled classes for attribute extraction.6 ConclusionThis paper introduces an extraction frameworkfor exploiting labeled classes of instances to ac-quire open-domain attributes from unstructuredtext available within search query logs.
The link-ing of the labeled classes into existing conceptualhierarchies allows for the extraction of attributesover hierarchy concepts, without a-priori restric-tions to specific domains of interest and with littlesupervision.
Experimental results show that theextracted attributes are more accurate when us-ing automatically-derived labeled classes, ratherthan classes of instances derived from manually-created resources such as Wikipedia.
Currentwork investigates the impact of the semantic dis-tribution of the classes of instances on the overallaccuracy of attributes; the potential benefits of us-ing more compact conceptual hierarchies (Snowet al, 2007) on attribute accuracy; and the orga-nization of labeled classes of instances into con-ceptual hierarchies, as an alternative to insertingthem into existing conceptual hierarchies createdmanually from scratch or automatically by filter-ing manually-generated relations among classesfrom Wikipedia (Ponzetto and Strube, 2007).646ReferencesM.
Banko and O. Etzioni.
2008.
The tradeoffs between openand traditional relation extraction.
In Proceedings of the46th Annual Meeting of the Association for ComputationalLinguistics (ACL-08), pages 28?36, Columbus, Ohio.S.
Blohm, P. Cimiano, and E. Stemle.
2007.
Harvesting re-lations from the web - quantifiying the impact of filter-ing functions.
In Proceedings of the 22nd National Con-ference on Artificial Intelligence (AAAI-07), pages 1316?1321, Vancouver, British Columbia.D.
Davidov and A. Rappoport.
2008.
Classification of se-mantic relationships between nominals using pattern clus-ters.
In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL-08), pages227?235, Columbus, Ohio.C.
Fellbaum, editor.
1998.
WordNet: An Electronic LexicalDatabase and Some of its Applications.
MIT Press.W.
Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and H. Hon.2007.
Cross-lingual query suggestion using query logsof different languages.
In Proceedings of the 30th ACMConference on Research and Development in InformationRetrieval (SIGIR-07), pages 463?470, Amsterdam, TheNetherlands.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Discover-ing relations among named entities from large corpora.
InProceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL-04), pages 415?422, Barcelona, Spain.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14thInternational Conference on Computational Linguistics(COLING-92), pages 539?545, Nantes, France.L.
Lee.
1999.
Measures of distributional similarity.
In Pro-ceedings of the 37th Annual Meeting of the Association ofComputational Linguistics (ACL-99), pages 25?32, Col-lege Park, Maryland.D.
Lin and P. Pantel.
2002.
Concept discovery from text.In Proceedings of the 19th International Conference onComputational linguistics (COLING-02), pages 1?7.V.
Nastase and M. Strube.
2008.
Decoding Wikipedia cat-egories for knowledge acquisition.
In Proceedings ofthe 23rd National Conference on Artificial Intelligence(AAAI-08), pages 1219?1224, Chicago, Illinois.M.
Pas?ca and B.
Van Durme.
2008.
Weakly-supervised ac-quisition of open-domain classes and class attributes fromweb documents and query logs.
In Proceedings of the 46thAnnual Meeting of the Association for Computational Lin-guistics (ACL-08), pages 19?27, Columbus, Ohio.M.
Pas?ca.
2007.
Organizing and searching the World WideWeb of facts - step two: Harnessing the wisdom of thecrowds.
In Proceedings of the 16th World Wide Web Con-ference (WWW-07), pages 101?110, Banff, Canada.M.
Pas?ca.
2008.
Turning Web text and search queries intofactual knowledge: Hierarchical class attribute extraction.In Proceedings of the 23rd National Conference on Arti-ficial Intelligence (AAAI-08), pages 1225?1230, Chicago,Illinois.M.
Palmer, H. Dang, and C. Fellbaum.
2007.
Making fine-grained and coarse-grained sense distinctions, both man-ually and automatically.
Natural Language Engineering,13(2):137?163.S.
Ponzetto and M. Strube.
2007.
Deriving a large scaletaxonomy from Wikipedia.
In Proceedings of the 22ndNational Conference on Artificial Intelligence (AAAI-07),pages 1440?1447, Vancouver, British Columbia.S.
Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007.SemEval-2007 Task-17: English lexical sample, SRL andall words.
In Proceedings of the 4th Workshop on Se-mantic Evaluations (SemEval-07), pages 87?92, Prague,Czech Republic.M.
Remy.
2002.
Wikipedia: The free encyclopedia.
OnlineInformation Review, 26(6):434.R.
Snow, S. Prakash, D. Jurafsky, and A. Ng.
2007.
Learningto merge word senses.
In Proceedings of the 2007 Con-ference on Empirical Methods in Natural Language Pro-cessing (EMNLP-07), pages 1005?1014, Prague, CzechRepublic.F.
Suchanek, G. Kasneci, and G. Weikum.
2007.
Yago:a core of semantic knowledge unifying WordNet andWikipedia.
In Proceedings of the 16th World Wide WebConference (WWW-07), pages 697?706, Banff, Canada.F.
Wu and D. Weld.
2008.
Automatically refining theWikipedia infobox ontology.
In Proceedings of the 17thWorld Wide Web Conference (WWW-08), pages 635?644,Beijing, China.647
