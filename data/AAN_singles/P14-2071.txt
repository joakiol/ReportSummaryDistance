Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434?439,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproving Twitter Sentiment Analysis with Topic-Based MixtureModeling and Semi-Supervised TrainingBing Xiang *IBM Watson1101 Kitchawan RdYorktown Heights, NY 10598, USAbingxia@us.ibm.comLiang ZhouThomson Reuters3 Times SquareNew York, NY 10036, USAl.zhou@thomsonreuters.comAbstractIn this paper, we present multiple ap-proaches to improve sentiment analysison Twitter data.
We first establish astate-of-the-art baseline with a rich fea-ture set.
Then we build a topic-based sen-timent mixture model with topic-specificdata in a semi-supervised training frame-work.
The topic information is generatedthrough topic modeling based on an ef-ficient implementation of Latent Dirich-let Allocation (LDA).
The proposed sen-timent model outperforms the top systemin the task of Sentiment Analysis in Twit-ter in SemEval-2013 in terms of averagedF scores.1 IntroductionSocial media, such as Twitter and Facebook, hasattracted significant attention in recent years.
Thevast amount of data available online provides aunique opportunity to the people working on nat-ural language processing (NLP) and related fields.Sentiment analysis is one of the areas that haslarge potential in real-world applications.
For ex-ample, monitoring the trend of sentiment for a spe-cific company or product mentioned in social me-dia can be useful in stock prediction and productmarketing.In this paper, we focus on sentiment analysis ofTwitter data (tweets).
It is one of the challengingtasks in NLP given the length limit on each tweet(up to 140 characters) and also the informal con-versation.
Many approaches have been proposedpreviously to improve sentiment analysis on Twit-ter data.
For example, Nakov et al (2013) providean overview on the systems submitted to one of theSemEval-2013 tasks, Sentiment Analysis in Twit-ter.
A variety of features have been utilized for* This work was done when the author was with Thom-son Reuters.sentiment classification on tweets.
They includelexical features (e.g.
word lexicon), syntactic fea-tures (e.g.
Part-of-Speech), Twitter-specific fea-tures (e.g.
emoticons), etc.
However, all of thesefeatures only capture local information in the dataand do not take into account of the global higher-level information, such as topic information.Two example tweets are given below, with theword ?offensive?
appearing in both of them.?
Im gonna post something that might be offen-sive to people in Singapore.?
#FSU offensive coordinator Randy Sanderscoached for Tennessee in 1st #BCS titlegame.Generally ?offensive?
is used as a negative word(as in the first tweet), but it bears no sentiment inthe second tweet when people are talking about afootball game.
Even though some local contextualfeatures could be helpful to distinguish the twocases above, they still may not be enough to get thesentiment on the whole message correct.
Also, thelocal features often suffer from the sparsity prob-lem.
This motivates us to explore topic informa-tion explicitly in the task of sentiment analysis onTwitter data.There exists some work on applying topic in-formation in sentiment analysis, such as (Mei etal., 2007), (Branavan et al, 2008), (Jo and Oh,2011) and (He et al, 2012).
All these work aresignificantly different from what we propose inthis work.
Also they are conducted in a domainother than Twitter.
Most recently, Si et al (2013)propose a continuous Dirichlet Process Mixturemodel for Twitter sentiment, for the purpose ofstock prediction.
Unfortunately there is no eval-uation on the accuracy of sentiment classificationalone in that work.
Furthermore, no standard train-ing or test corpus is used, which makes compari-son with other approaches difficult.Our work is organized in the following way:434?
We first propose a universal sentiment modelthat utilizes various features and resources.The universal model outperforms the topsystem submitted to the SemEval-2013 task(Mohammad et al, 2013), which was trainedand tested on the same data.
The universalmodel serves as a strong baseline and alsoprovides an option for smoothing later.?
We introduce a topic-based mixture modelfor Twitter sentiment.
The model is inte-grated in the framework of semi-supervisedtraining that takes advantage of large amountof un-annotated Twitter data.
Such a mixturemodel results in further improvement on thesentiment classification accuracy.?
We propose a smoothing technique throughinterpolation between universal model andtopic-based mixture model.?
We also compare different approaches fortopic modeling, such as cross-domain topicidentification by utilizing data from newswiredomain.2 Universal Sentiment ClassifierIn this section we present a universal topic-independent sentiment classifier to establish astate-of-the-art baseline.
The sentiment labels areeither positive, neutral or negative.2.1 SVM ClassifierSupport Vector Machine (SVM) is an effec-tive classifier that can achieve good performancein high-dimensional feature space.
An SVMmodel represents the examples as points in space,mapped so that the examples of the different cate-gories are separated by a clear margin as wide aspossible.
In this work an SVM classifier is trainedwith LibSVM (Chang and Lin, 2011), a widelyused toolkit.
The linear kernel is found to achievehigher accuracy than other kernels in our initial ex-periments.
The option of probability estimation inLibSVM is turned on so that it can produce theprobability of sentiment class c given tweet x atthe classification time, i.e.
P (c|x).2.2 FeaturesThe training and testing data are run throughtweet-specific tokenization, similar to that used inthe CMU Twitter NLP tool (Gimpel et al, 2011).It is shown in Section 5 that such customized tok-enization is helpful.
Here are the features that weuse for classification:?
Word N-grams: if certain N-gram (unigram,bigram, trigram or 4-gram) appears in thetweet, the corresponding feature is set to 1,otherwise 0.
These features are collectedfrom training data, with a count cutoff toavoid overtraining.?
Manual lexicons: it has been shown in otherwork (Nakov et al, 2013) that lexicons withpositive and negative words are important tosentiment classification.
In this work, weadopt the lexicon from Bing Liu (Hu andLiu, 2004) which includes about 2000 posi-tive words and 4700 negative words.
We alsoexperimented with the popular MPQA (Wil-son et al, 2005) lexicon but found no extraimprovement on accuracies.
A short list ofTwitter-specific positive/negative words arealso added to enhance the lexicons.
We gen-erate two features based on the lexicons: totalnumber of positive words or negative wordsfound in each tweet.?
Emoticons: it is known that people use emoti-cons in social media data to express theiremotions.
A set of popular emoticons are col-lected from the Twitter data we have.
Twofeatures are created to represent the presenceor absence of any positive/negative emoti-cons.?
Last sentiment word: a ?sentiment word?
isany word in the positive/negative lexiconsmentioned above.
If the last sentiment wordfound in the tweet is positive (or negative),this feature is set to 1 (or -1).
If none of thewords in the tweet is sentiment word, it is setto 0 by default.?
PMI unigram lexicons: in (Mohammad etal., 2013) two lexicons were automaticallygenerated based on pointwise mutual infor-mation (PMI).
One is NRC Hashtag Senti-ment Lexicon with 54K unigrams, and theother is Sentiment140 Lexicon with 62K un-igrams.
Each word in the lexicon has an as-sociated sentiment score.
We compute 7 fea-tures based on each of the two lexicons: (1)sum of sentiment score; (2) total number of435positive words (with score s > 1); (3) to-tal number of negative words (s < ?1); (4)maximal positive score; (5) minimal negativescore; (6) score of the last positive words; (7)score of the last negative words.
Note that forthe second and third features, we ignore thosewith sentiment scores between -1 and 1, sincewe found that inclusion of those weak subjec-tive words results in unstable performance.?
PMI bigram lexicon: there are also 316K bi-grams in the NRC Hashtag Sentiment Lexi-con.
For bigrams, we did not find the sen-timent scores useful.
Instead, we only com-pute two features based on counts only: totalnumber of positive bigrams; total number ofnegative bigrams.?
Punctuations: if there exists exclamationmark or question mark in the tweet, the fea-ture is set to 1, otherwise set to 0.?
Hashtag count: the number of hashtags ineach tweet.?
Negation: we collect a list of negation words,including some informal words frequentlyobserved in online conversations, such as?dunno?
(?don?t know?
), ?nvr?
(?never?),etc.
For any sentiment words within a win-dow following a negation word and not af-ter punctuations ?.
?, ?,?, ?
;?, ??
?, or ?!
?, we re-verse its sentiment from positive to negative,or vice versa, before computing the lexicon-based features mentioned earlier.
The win-dow size was set to 4 in this work.?
Elongated words: the number of words in thetweet that have letters repeated by at least 3times in a row, e.g.
the word ?gooood?.3 Topic-Based Sentiment Mixture3.1 Topic ModelingLatent Dirichlet Allocation (LDA) (Blei et al,2003) is one of the widely adopted generativemodels for topic modeling.
The fundamental ideais that a document is a mixture of topics.
For eachdocument there is a multinomial distribution overtopics, and a Dirichlet prior Dir(?)
is introducedon such distribution.
For each topic, there is an-other multinomial distribution over words.
One ofthe popular algorithms for LDA model parameterestimation and inference is Gibbs sampling (Grif-fiths and Steyvers, 2004), a form of Markov ChainMonte Carlo.
We adopt the efficient implementa-tion of Gibbs sampling as proposed in (Yao et al,2009) in this work.Each tweet is regarded as one document.
Weconduct pre-processing by removing stop wordsand some of the frequent words found in Twitterdata.
Suppose that there are T topics in total in thetraining data, i.e.
t1, t2, ..., tT.
The posterior prob-ability of each topic given tweet xiis computed asin Eq.
1:Pt(tj|xi) =Cij+ ?j?Tk=1Cik+ T?j(1)where Cijis the number of times that topic tjisassigned to some word in tweet xi, usually aver-aged over multiple iterations of Gibbs sampling.
?jis the j-th dimension of the hyperparameter ofDirichlet distribution that can be optimized duringmodel estimation.3.2 Sentiment Mixture ModelOnce we identify the topics for tweets in the train-ing data, we can split the data into multiple sub-sets based on topic distributions.
For each subset,a separate sentiment model can be trained.
Thereare many ways of splitting the data.
For example,K-means clustering can be conducted based onthe similarity between the topic distribution vec-tors or their transformed versions.
In this work,we assign tweet xito cluster j if Pt(tj|xi) > ?or Pt(tj|xi) = maxkPt(tk|xi).
Note that this isa soft clustering, with some tweets possibily as-signed to multiple topic-specific clusters.
Similarto the universal model, we train T topic-specificsentiment models with LibSVM.During classification on test tweets, we runtopic inference and sentiment classification withmultiple sentiment models.
They jointly deter-mine the final probability of sentiment class cgiven tweet xias the following in a sentiment mix-ture model:P (c|xi) =T?j=1Pm(c|tj, xi)Pt(tj|xi) (2)where Pm(c|tj, xi) is the probability of sentimentc from topic-specific sentiment model trained ontopic tj.4363.3 SmoothingAdditionally, we also experiment with a smooth-ing technique through linear interpolation betweenthe universal sentiment model and topic-basedsentiment mixture model.P (c|xi) = ?
?
PU(c|xi) + (1?
?
)?T?j=1Pm(c|tj, xi)Pt(tj|xi) (3)where ?
is the interpolation parameter andPU(c|xi) is the probability of sentiment c giventweet xifrom the universal sentiment model.4 Semi-supervised TrainingIn this section we propose an integrated frame-work of semi-supervised training that containsboth topic modeling and sentiment classification.The idea of semi-supervised training is to takeadvantage of large amount low-cost un-annotateddata (tweets in this case) to further improve the ac-curacy of sentiment classification.
The algorithmis as follows:1.
Set training corpus D for sentiment classifi-cation to be the annotated training data Da;2.
Train a sentiment model with current trainingcorpus D;3.
Run sentiment classification on the un-annotated data Duwith the current sentimentmodel and generate probabilities of sentimentclasses for each tweet, P (c|xi);4.
Perform data selection.
For those tweets withP (c|xi) > p, add them to current trainingcorpus D. The rest is used to replace the un-annotated corpus Du;5.
Train a topic model on D, and store the topicinference model and topic distributions ofeach tweet;6.
Cluster data in D based on the topic distribu-tions from Step 5 and train a separate senti-ment model for each cluster.
Replace currentsentiment model with the new sentiment mix-ture model;7.
Repeat from Step 3 until finishing a pre-determined number of iterations or no moredata is added to D in Step 4.5 Experimental Results5.1 Data and EvaluationWe conduct experiments on the data from the taskB of Sentiment Analysis in Twitter in SemEval-2013.
The distribution of positive, neutral andnegative data is shown in Table 1.
The develop-ment set is used to tune parameters and features.The test set is for the blind evaluation.Set Pos Neu Neg TotalTraining 3640 4586 1458 9684Dev 575 739 340 1654Test 1572 1640 601 3813Table 1: Data from SemEval-2013.
Pos: positive;Neu: neutral; Neg: negative.For semi-supervised training experiments, weexplored two sets of additional data.
The firstone contains 2M tweets randomly sampled fromthe collection in January and February 2014.
Theother contains 74K news documents with 50Mwords collected during the first half year of 2013from online newswire.For evaluation, we use macro averaged F scoreas in (Nakov et al, 2013), i.e.
average of the Fscores computed on positive and negative classesonly.
Note that this does not make the task a binaryclassification problem.
Any errors related to neu-tral class (false positives or false negatives) willnegatively impact the F scores.5.2 Universal ModelIn Table 2, we show the incremental improvementin adding various features described in Section 2,measured on the test set.
In addition to the fea-tures, we also find SVM weighting on the trainingsamples is helpful.
Due to the skewness in classdistribution in the training set, it is observed dur-ing error analysis on the development set that sub-jective (positive/negative) tweets are more likelyto be classified as neutral tweets.
The weights forpositive, neutral and negative samples are set tobe (1, 0.4, 1) based on the results on the develop-ment set.
As shown in Table 2, weighting adds a2% improvement.
With all features combined, theuniversal sentiment model achieves 69.7 on aver-age F score.
The F score from the best system inSemEval-2013 (Mohammad et al, 2013) is alsolisted in the last row of Table 2 for a comparison.437Model Avg.
F scoreBaseline with word N-grams 55.0+ tweet tokenization 56.1+ manual lexicon features 62.4+ emoticons 62.8+ last sentiment word 63.7+ PMI unigram lexicons 64.5+ hashtag counts 65.0+ SVM weighting 67.0+ PMI bigram lexicons 68.2+ negations 69.0+ elongated words 69.7Mohammad et al, 2013 69.0Table 2: Results on the test set with universal sen-timent model.5.3 Topic-Based Mixture ModelFor the topic-based mixture model and semi-supervised training, based on the experiments onthe development set, we set the parameter ?
usedin soft clustering to 0.4, the data selection pa-rameter p to 0.96, and the interpolation parame-ter for smoothing ?
to 0.3.
We found no morenoticeable benefits after two iterations of semi-supervised training.
The number of topics is setto 100.The results on the test set are shown Table 3,with the topic information inferred from eitherTwitter data (second column) or newswire data(third column).
The first row shows the per-formance of the universal sentiment model asa baseline.
The second row shows the resultsfrom re-training the universal model by simplyadding tweets selected from two iterations ofsemi-supervised training (about 100K).
It servesas another baseline with more training data, fora fair comparison with the topic-based mixturemodeling that uses the same amount of trainingdata.We also conduct an experiment by only consid-ering the most likely topic for each tweet whencomputing the sentiment probabilities.
The resultsshow that the topic-based mixture model outper-forms both the baseline and the one that considersthe top topics only.
Smoothing with the universalmodel adds further improvement in addition to theun-smoothed mixture model.
With the topic in-formation inferred from Twitter data, the F scoreis 2 points higher than the baseline without semi-Model Tweet-topic News-topicBaseline 69.7 69.7+ semi-supervised 70.3 70.2top topic only 70.6 70.4mixture 71.2 70.8+ smoothing 71.7 71.1Table 3: Results of topic-based sentiment mixturemodel on SemEval test set.supervised training and 1.4 higher than the base-line with semi-supervised data.As shown in the third column in Table 3, sur-prisingly, the model with topic information in-ferred from the newswire data works well on theTwitter domain.
A 1.4 points of improvement canbe obtained compared to the baseline.
This pro-vides an opportunity for cross-domain topic iden-tification when data from certain domain is moredifficult to obtain than others.In Table 4, we provide some examples from thetopics identified in tweets as well as the newswiredata.
The most frequent words in each topic arelisted in the table.
We can clearly see that the top-ics are about phones, sports, sales and politics, re-spectively.Tweet-1 Tweet-2 News-1 News-2phone game sales partycall great stores governmentanswer play online electionquestion team retail ministerservice win store politicaltext tonight retailer primetexting super business stateTable 4: The most frequent words in example top-ics from tweets and newswire data.6 ConclusionsIn this paper, we presented multiple approachesfor advanced Twitter sentiment analysis.
We es-tablished a state-of-the-art baseline that utilizes avariety of features, and built a topic-based sen-timent mixture model with topic-specific Twitterdata, all integrated in a semi-supervised trainingframework.
The proposed model outperforms thetop system in SemEval-2013.
Further research isneeded to continue to improve the accuracy in thisdifficult domain.438ReferencesDavid Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
In Journal of Ma-chine Learning Research.
3(2003), 993?1022.S.
R. K. Branavan, Harr Chen, Jacob Eisenstein, andRegina Barzilay.
2008.
Learning document-levelsemantic properties from free-text annotations.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL-2008).Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
InACM Transactions on Intelligent Systems and Tech-nology.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: annotation, features, and experiments.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
In Proceedings of the NationalAcademy of Science.
101, 5228?5235.Yulan He, Chenghua Lin, Wei Gao, and Kam-FaiWong.
2012.
Tracking sentiment and topic dynam-ics from social media.
In Proceedings of the 6th In-ternational AAAI Conference on Weblogs and SocialMedia (ICWSM-2012).Mingqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of theTenth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining.Yohan Jo and Alice Oh.
2011.
Aspect and senti-ment unification model for online review analysis.In Proceedings of ACM Conference in Web Searchand Data Mining (WSDM-2011).Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of International Conference on WorldWide Web (WWW-2007).Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013).
312-320, At-lanta, Georgia, June 14-15, 2013.Preslav Nakov, Zornitsa Kozareva, Alan Ritter, SaraRosenthal, Veselin Stoyanov, and Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment Analysis inTwitter.
In Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013).312-320, Atlanta, Georgia, June 14-15, 2013.Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,Huayi Li, and Xiaotie Deng.
2013.
Exploiting topicbased Twitter sentiment for stock prediction.
In Pro-ceedings of the 51st Annual Meeting of the Associ-ation for Computational Linguistics.
24-29, Sofia,Bulgaria, August 4-9,2013.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT 05.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inferenceon streaming document collections.
KDD?09.439
