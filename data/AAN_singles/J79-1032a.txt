Ame6~tU1 Journal of Computationd Lu~gUitti~ Hi ~icr~fi che 32P R O C E E D l N G S1 3 T H  A N N U A L  M E E T I N GASSOCIATION FOR COMPUTATIONK LINGUI ST1 CSTimothy C. Diller, EditorSperry-UnivacS-t. Paul, Minnesota 56101Copyright @ 1975 by the Association for romputational Lf nguf $tic@PREFACEThe 13th annual ACL meeting was held at Boston.
Massa-chusetts, October 30 - November 1, 1975, in conjunction withthe 38th meeting of the American Society for InformationScience.
The ACL thanks the ASIS  for i t s  assistance in pub-licizing the conference and in handling registration.This and the fallowing four microfiehe8 contain 27 ofthe 30 papers presented at the meeting.
The breadth o f  theoonference is evident in (a) the modes of communication in-vestFgated (speech, sign language, and written text), (b) thestyles of communication (monologues, dialogues, and notemaking) , and (c) the uses envisioned for @he processing oflanguage data ( e .
g .
,  theoretical modeling, data collection andretrieval, game playing, story generation, idiolect charac-terization, and automatic indexing).Topics considered include the development of languageunderstanding systems, the integration and utilization ofspecific components of language, specifically syntax andsemantics, the representation and use of discourse structureand general world knowledge, and the construction of textprocessing eystems.The program committee was so le ly  responsible for select-ing the t a l k s  to be given, and hence the papers to be pub-lished hereln.
(Reg~etfully, nearly half of those submittedcould not be accepted for lack of program t i m e  . )
Members ofthe program committee w e r e  Jonathan Allen, Joyce Friedman,..Bonnie Nash-Webber, and Chuck Rieger.
A special  word of ap-preciation is due Jonathan Allen, who a l so  served as LocalArrangements Chairman.
Working with h i m  were B e t t y  Brocinerand Skip McAfee of the M I S .
Aravind Joshi, president ofACL, provided guidance in all areas of preparation.The AJCL kindly provided advance publication of theaccepted abstracts and now makes possible the publication ofthe entire proceedings.
David Hays, e d i t o r  of AJCL, providedguidance in publication format and each author provided finalcopy in accordance with requested s p e c i f i c a t i o n s .
The Centerfor Appl ied  Ltnguistics (in p a r t i c u l a r ,  David Hoffman andNancy J~kovich with guidance from Hood Roberts)  contributedin a variety of ways, most notab ly  in the preparation ofmeeting handbooks.Tkis microfiche contains the papers as submitted bytheir authors for ffve of the s ix  talkb touching on LanguageUnderstanding Systems.
The paper detailing "Conceptua1Grammartt by William Mattin was too long f o r  inclusion in&baa microfiche and will appear elsewhere.
My thanks toYorick Wilks for chairing the session.--Timothy C. DillerProgram Committee ChairmanTABLE O f ,  CONTENTSProgram SchedulePEDAGLDT and Unlderstanding Natural Language Processing.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
William Fa.bens 9A General System f o r  Semantic Analysis of English andilta Use in Drawing Maps from Directions ~ e r r y , ~ .
H Q ~ S  .
21Arl Adaptive Natural Language Parser P a r r y  L. Miller .
, .
42Conceptual Gramar (abstract only) W i l l i a m  A .
Martin .
, 57Semantic-based Parsing and a Natural-language In ter facef o r  Iaterac tive Data Management Ja'm F ,  Burger ,  AntonioLeal, and Arie Shoshani .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
58PHLIQA 1: Multilevel Semantics in Question AnsweringP.
Medema, W ,  J. Bromenberg, H. C .
Bunt, S. P. J ,  Landsbergen,R .
J. H I  ScM, W. J. Schoenmakers, and E l  P. C. van Utteren .
.
.
72THIRTEENTH ANNUAL MEETINGTHE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICSSheraton Boston HotelBoston, Massachusef t sOctober $0-November 1, 1975Thursday, October 30, 197.5S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf SSession Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh990 A.M. Greetings and Irrtroductory Rernarks9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t singWilltarn Fabens - Rutgers University9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  UseI n  Drawirt g A l  apa from DircctiottsJerry R. t.lobbs - The C ~ t y  College of CUNYtO:OS A.M An Adaprivo Nutural Lartguago ParserFerry t. M~ller - M I T.1030 AM.
COFFEE & D O N U St 1 :30 A.M. Semantic-Based Parsing Arzd A Natural-Langun /ar, Irttr,r farr,Far Intcrraciittc!
Data Af artngctnrrltJohn F. Burger, Antonio Leal, and A r ~ e  Shoshanl -System Development CarporationL2a0 NOaN Pl iL lQA I :  Mulrilrucl Srrmaniic~ i n  Qrrrlrtiort Ancu:crinaP.
hkderna, st. a1 - Phil~ps Research Laboratorres,The Netheviand$1290 P.M.
LUNCHEON BREAKS E S S I O N  2: LANGUAGE G E N E R A T I O N  S Y S T E h I SSess~on Chairman: Martin K a y  - Xerox Corporation2:OO P.M. A Framework for W r i t i f t g  Ga~~erat io tr  Crurnrnnrsfor Ints tact ivc  Cornputr?t ProgrnrnnDav~d McDonald - M I T.2:30 P.M.3:OO P.M,3:30 P.M.4:OO P.M.4:30 P.M.5:30 P.M.8:00 P.M.Incrarn~ntnl Sonlenco P r o c ~ s ~ i ~ t gRodger Knaus - Bureau of the CensusA IJoricnl Proces~ Model of IVomit~crl CompoundingIn EnglishJ.R.
Rhyne - University of HoustonCOFFEE & OONUTSGsnerat in~r ns Parsing horn A Natzrtork irtto aIdheat Str ingStuart C Shap~ro - Indiana UniversitySpeech Gcnrrntior~ frdm Scmnr~i ir NctrJonathan Slocum - Stanford Research lnst i luteUsing Plnrming Structurra to C~rzcratc StoriesJim Meehan - Yale UniversityDINNER BREAKWINE, CHEESE & COMPUTER DEMONSTRATIONSSESSION 3 :  P A R S I N G ,  S Y N T A X ,  flND SEhl  ANTICSSession Chairman: Joyce Friedrnan - Stanford Research Institute9:00 AM.
Synrucric Procanrirta in tho B R N  Spcaclr Urtdcrstnrtdi~tjySystemMadeline Bates- Bolt, Beranek & Newman, lnc9:30 A.M. Sygtnrn latonration and Coi~iml for $prcr  h Uttdrrrrnrzdin/rWllimrn H, Paxtoln and Ann E. RobinsonStanford Research I ~ s t i t u t e10a0 A.M. A Tuneahlo Porfarrnancc GrnrnmnrJane J'.
P~binson - Stanford Rosearch Institute10:30 A.M. COFFEE & DONUTSlla0 A.M. Scmdrriic Processing f ~ r  Speech UnderxiandingGary G Hendrix - Stanford Research inst i tute11:30 A.M.. SPS: A Fortnalim f o r *  Scmani ic  Itrterlrrr~ation artdIts Use irr Processing P r c p s s i t i o n ~  that Rcj'rrettcc SpaccNorman K Sondhetrner - Ohro State University12:OO  NOON Tho Nature and Computational Use of n f i lcnningReproscrrlntion Tor Pard ConccprzNick Cercone - University of Alberta1 2:30 P.M LUNCHEON BREAKSESSION 4: MODELING DISCOURSB A M  EBOR1,D KN111171,1CIIGE ISession Chairman: Carl H e w ~ t t  - MIT2a0 P.M. Ertabliahirrg Conrcxt irt  Task-Oricnicd DinlogsBarbara G, Oeutsch - Stanford Research Institute290 P.M. Dircoursc Modclx and Language Cotnpr~lzerl cionBertram C Bruce - Bolt, Bersnek & Newman, Inc300 P.M. Judging ihc Coherertcy o/ Disrourse (and SomeObservations About Frtzrnc?s/Scrip t s)Brian Ph~il~ps - University of Illinois atChicago Circle3:30 P.M. COFFEE & DONUTS4f i0 P.M. Art Approach to r hc Orgariizatiort of Murtdnrtc 117orZdK r t o ~ l e d ~ a :  tho Carterarioir and fifariaacrrterrt of ScriptsR.E.
Cuflingford - Yale University490 P.M. Tho &ncaptuaZ II~h;cTi~t  on of P hysicnl Ar l i .
t i i t i~sNorman Badler - University of Pennsylvan~a5:OO P.M. f i  Frarno Artalysit 01 Arn~rican S i g n  l,nr~gunaeh d y  Keg1 (MIT) and Nancy Ch~ncho r  (U. of Mass )5:30 P.M. ACL BUSINESS MEETING AND ELECTION OF OFFICERSDlNNER: ACL BANQUETSaturday, November 1, 1975S E S S I O N  $A: AIOl)Ef,liVG DISCOURSE & WORI,D KNOlVI,ItIIGlI: / ISession Chairman: Georgette Silva - System Development Corporation9:00 A.M. Cross-Sct~t~tztinE R ~ f i r ~ n r c  Rr~olutiorzDavid Klappholz and Abe Lockman - Colutnbia Uhiversity9:30 A.M. Ilaia Doc$ a Systcrn Knou~ TVhclz to Stop l ~ t f ~ r c r ~ r i n g ?Stan Rosensche~n - University of Pennsylvahla10:00 A.M. COFFEE Rt DONUTSS E S S I O N  5i?
: TI5XT AiYflLYSIS1 1 :00 A.M.1 1 :30 A.M.D;c?ucZoping n Cornputcr Syatrm for Ilanrlling IltizcrclrttlyVtzrinhlc I , i i ~ ~ u i s ~ i c  DataD a v ~ d  8eckles, Lawrence Carrrngton, andGemma Warner - The unrversity of the West lndiesA Nururul I1a~tguagc Proecs.sirtg Pnckrcgc,David Brill and Beairlee T Oshika -Speech Communications Reseatch LaboratoryOn the K o l c  of W o r d s  and Phrases irt Autornntir T e r tAnaly,& and Cornru~niiortGerard Salton - Cornell University12:OQ NOON Crnmrnnlicul Comprrssiolz in  N o t ~ s  and Rcrards:A~talysib and Cornl~v tntioltBarbara Anderson (University of New Brunswick),Irwin Bross (Roswell Park Memorial Institute),a ~ d  Naomi Sager (New 'fork University)American Journal of Computational Linguistics H i c r o f i c h e  32 : 9CGmputer Scf ence DepartmentR u t g e r s  Uni versi tyFew B r m i c k ,  New Jersey 08903ABSTRACTPEDAGLOT is a programmable parser, a 'meta-parser . '
To program i t ,  onedescribes not  j u s t  syntax and some semantics, but also-- independent ly-- i tsmodes of behavior.
The PEDAGLOT formulation of  such modes o f  behavior followsa ca tegor iza t ion  of pars ing processes  i n t o  a t t e n t i o n - c o n t r o l ,  d iscovery,  pre-d i c t i o n  and cons t ruc t ion .
Within these  o v e r a l l  types o f  - a c t i v i t i e s ,  cont ro lcan be s p e c i f l e d  covering a number of  syntax-processing and semantics-process-ing operat ions .
While it i s  not t h e  only p o s s i b l e  way of programing a meta-parse r ,  t h e  PEDAGLOT mode-specification technique i s  suggest ive i n  i t s e l f  ofvar ious  new approaches t o  modeling and understanding same language processinga c t i v i t i e s  besides parsing, such as generation and inference ,7% i s  wotk was sponsored by through NIH Grant #RR643.I t  i s  well  known t h a t  t o  process n a t u r a l  language, one needs both asyntactic desc r ip t ion  of poss ib le  sentences,  blended i n  some way with a semanticdesc r ip t ion  bf a c e r t a i n  domain of discourse,  and a r a t h e r  d e t a i l e d  desc r ip t ionof t h e  ac tua l  processes used i n  hearing o r  producing sentences.An augmented t r a n s i t i o n  network (Woods, 1970) i s  qn example of t h e  blendingof s y n t a c t i c  and quasi-semantic desc r ip t ions ,  Here r e g i s t e r s  would be repos i -tor ies  o f ,  o r  po in te r s  t o ,  semantics.
When used i n  conjunction with a semanticnqtwork, an ATN can be used 60 parse  o r  t o  generate (Simmons and Slocum, 1912)sentences.
The i s s u e  o f  changing the  des'cription of t h e  actual  processes usedi n  such systems has been touched on by Woods ( i n  using a 'generation modet), t osome extent  by Gimmons and Slo~um (usi~g decis ion  funct ions t o  control  s t y l e  ofgenerat ion) ,  and t o  a l a r g e r  ex tent  by Kaplari (19751, i n  h i s  General Syntac t icProcdssor, GSP.
GSP indeed is  one example of  a system i n  which syntax, semanticsand t o  some extent  processes can each be u s e f u l l y  defined.If we look at syntax, semantics and processes a s  t h r e e  descr ibable  components,these  systems j u s t  mentioned i l l u s t r a t e  how thoroughly intertwined they can become--t o  the  extent  t h a t  t h e o r i s t s  from time t o  time deny the  exis tence o r  a t  l e a s t  t h eimportance of some one of  them.
Ignoring t h a t  d ispute ,  I would- l ike  t o  concentrateon the quest ion of  being a b l e  t o  comprehensively descr ibe one ' s  theory of languagei n  terms of its syntax, semantics and processes i n  a way t h a t  allows fo r  t h e i rnecessary and extensive in ter twining  connections, bu t  a t  t h e  sane time allows onet o  describe them independently.I came t a  t h e  need for  doing t h i s  while designing a Tre laxa t ion  p a r s e r , '  aparser which can make grammatical r e l axa t ions  i f  i t  i s  given an Ill-formed s t r i n g ,so as t o  arrive a t  a k l o s e s t t  poss ib le  parse  f o r  the' s t r i n g .
This probl-em involveddescr ib ing  a k o r r e c t t  grammar and then (in some way) descr ibing a space of  deviat ionsazthat night be allowed by the paxser.
Thus the syntax would be fixed and the waythe  parser uses it would separately have to  be described.
I t  was soon noticedthat efficiency could be greatly enhanced i f  some rudimentary notion of semanticp laus ib i l i ty  could also be used.
I t  would have t o  be described i n  a way relatedt o  the cbrrect syntax but st i l l  be usable by the parser.
Thus, for  my purposes,the descriptions had t o  be independent of one another.One feature of a relaxation parser i s  tha t  it can ' f i l l  i n  the gaps' of astring tha t  is missing various words.
If one could, which my re laxat ion parserdid not, specify the semantic context of a sentence, the  generated sentence mightbe semantically rather plausible.
In any case, the relaxation parser operates i nvarious respects l ike  an actual parser or like a generator, and it was t h i s  re la -tionship between parsing and generating that became of in teres t ,Out of  the design of the relaxation parser,  the  notation (independent of syn-tax) which t o  some extent describes various processes and choices of  al ternate waysof processing was developed.
Thus, one may take a s e t  of syntax and semantic de-scriptions and then through describing the processing 'modest involved, define aprocessor which uses the par t icular  algorithm t h a t  the  individual processes togetherdefine, One may c a l l  the parser that  i s  programmabLe i n  i t s  processes a meta-parser,of which various existing qarse r s  and generators appear t o  be special  cases,A closer examination of the  parser I have developed (called PEDAGLOT*) may showsome such aspects of meta-parsing, especially as regards the  relat ionship betweenparsing and generating.
I will describe the syntactic and semantic parts o f  t h eparser first: by noting i t s  resemblances to  t h e  parser of J .
Earley (1970) and theATN system of Woods.
Then I w i l l  describe t he  process-type specifications t h a t  areavailable, and the  use of meta-parsers as a basis f o r  defining general language be-haviors.
Purther detail can be found in the PEDAGMT manual (Fabens, 1972 and 1973) .
*for pe&~ogic polyglot1.
The Core of the Parqer-1The fundamental operation of t h e  parser  i s  very s imi lar  t o  the  operationof Earleyvs parser ,  with augmentations f o r  recording t h e  r e s u l t s  of parses(e ,g , ,  their t re  s t ruc tu re ,  and various of t h e i r  a t t r i b u t e s ,  which I c a l lftags').
It is given a grammar a s  a s e t  of context-free rules with variousextensions, most i m p ~ r t a n t  of which a r e  t h a t  LISP functions may be used aspredicates instead of terminals, and thay each rule may be followed by opera-t ions t h a t  are defined i n  tbnns of t h e  syntac t ic  elements a f  the  r u l e  i n  question,An example of t h i s  notation i s  as follows:S -+ NP VP=> [AGREE [REF NP] [VB VP] ][SUM = [REF NP] ] [OW = [REF VP] [VB = [VB VP] ]S -* NP [BE] [VPASS] BY NP=> [AGREE [REF NP] [VB [BE] ]I[SUM = [REF NP I ]  ] [OBJ = REF NP] ] [VB = [VB [VPASS] ] ]NP -+ [DET] [N]=> [REF = [N]]W + [VINP=> [VB = [V]] [REF = [REF NP]]Here, each bracketed symbol i s  the name of a recognition predicate ( e  .g.,IN] recognizes nouns, [BE] recognizes f o n s  o f  h t o  b e 1 ) ,  Following t h e  => arethe  post -recognit ion functions.
For instance [AGREE [REF NP] [VB VP] ] specifiesa ca l l  t o  the  AGREE function which i s  given, as arguments, the  REF a t t r i b u t e  (tag)of the sub-parse involved in tha t  rule and the  VB a t t r i b u t e  of t he  VP part oft h e  rule.Following i s  a parse tree fo r  'The Man Bites t h e D o g l  and values o f  tagsafter the parse.The DogThe general flow of the  parser is from top-down, and as  the  lowest compo-nents (symbols i n  the s t r ing)  are found, the post-recognition functions tha t  areassociated with t h e  ru le  tha t  recognized them a r e  applied.
Tags become associatedwith sub-parses when the post-recognition operation uses the  form [x = y ]  ( i n  whichthe value referenced by y i s  stored as the x t ag  of t h e  sub-parse).
In the  example,[DET] and [N] recognize 'The Manf and 'Manf i s  used as  the REF a t t r i b u t e  ofi t h efirst NP.
In t h e  second S ru le ,  t h e  operation of [SUM = [REF NP']] would be t oretrieve the REF tag of  the second NP (thus the  prime), and t o  s tore  t h a t  as t h eSUM tag  of the  final p a n e .As i n  most top-down parses, t h i s  parser begins with S and i ts two ru les ,s ince S i s  non-terminal.
S is expanded into t h e  two sequences of matches it shouldperform.
This expansion r e s u l t s  i n  various (in t h i s  case, two) predict isns  of whatt o  f ind next, When t h e  i n i t i a l  symbol i n  some r u l e  i s  a terminal o r  a predicate,a discovery is  cal led fo r  (in which a match is pexformed, possibly involving theknown values of the  tags).
When some complete sequence of elements is found (here,for instance, when NP -+ [DET] IN] has matched t h e  [N] ) .
Construction invokes thepost-reoognit ion operat ions and then usual lyt completes some e a r l i e r  part of a r u l e(here, the 'NPi ~f S + NP VP) So fur ther  predictions (involving VP) or discoveriesare then specified.1 have broken up the parsing process into t\he$e three parts so as to  simi4arlycatalpg t he  'parsing modes,' turn ing  this parser into a meta-parser.
Before doingso, f should note tbat th i s  parser stores each zesult under construction in a'chart' as is done by Kaplan i n  his GSP, so that, for instance, the NP ' testtw i l l  only have to  be evaluated once for each place one i s  wanted i n  the string.
[ N l  [;I 1 [ N l5 The T Man Bites The $ DogI1 lustrat ion of PEDAGLOT ' s Parsing ChartSimple Arrows indicate 'Predictions.
'Double Head Arrows indicate iDiscoveries,Dotted Arrows indicate tlConstruction.Also, for various well known reasons of efficiency, Earley's concept ofindependent processing of syntactic events i s  used (combined conceptually withthe chart), SO that a main controller can evaluate the individual syntactic ' t e s t s 1i n  almost any order, and not just in a backtracking sense (cf.
Woods, 1975).
Thbefficiency i s  realized here since many 'partial parses (partially recognized forns)15are effectively abandoned if other results can complete t h e  parse, o r  a sub-parse, first .2 .
Meta-Parsing ModesOne can see that, except f o r  the nota t iona l  i ne f f i c i enc i e s  o f  the  context,free formalism (as opposed to the augmented t r a n s i t i o n  network form), t h i s  parseris very much like other standard parsers (especially ATN s) .
I t  differs i n  t h a tthere is a waytof specifying how t o  proceed.
Currently, this system has approxi-mately a dozen toodesr and I will present some of them here.
Each mode spec i f i e show t o  handle a certain part of the parsing process.
They can be classified i n t ofour categories: attention control, prediction, discovery and construct ion.a, Attention Control W e s :Since the parser operates on a chart of independent events ( 'parsingquestions1), one must give t h e  parse r  a method of sequencing through them.Thus, one may specify 'breadth-first1 or 'depth-first1 and the  appropriate~echanism will be invoked {this merely involves t h e  way the processor stacksi t s  jobs).
A 'best-first ' option i s  -under development, which, when given anevaluation function to be applied to the set of currently a c t i v e  part ialparses, allows the system to operate  on the 'best1 problem next ,  Experi-Bents with this mode have so far been inconclusive.One also can speci fy  when t o  stop (i.e., at the first complete parse,or t o  wait  until a l l  other ambiguous parses have been discovered).
The d i s -it~gbiguation routine (which i s  described as a part o f  t h e  construct ion modes)defines which parse is %est l ,  Further, one may specify a left-to-right orright-to-left mode of how to progress along the  s t r i n g .b.
Discovery Modes:The starting point of building a relaxation parser is to specify whatt o  do when an exact match i s  not made.
If the parser i s  expecting one wordand finds another it can look arowd the indicated place in the s t r i n g  t o  f indwhat- it i s  looking f o r ,  o r  it can i n  c e r t a i n  other  circumstances simplyi n s e r t  t h e  expected word i n t o  t'he s t r i n g .
Thus, under discovery.modes,there are vaxious options:  e i t h e r  t h e  parser  i s  allowed t o  attempt matchesin out-of-sequence p a r t s  of t he  string, or n a t ,  And i f  not ,  or  i f  no suchmatch i s  found, t h e  parser  may or may not be allowed to make an inser t ion .So i n  PEDAGLOT, t h e r e  i s  an INSERT mode (and various r e s t r i c t e d  versionso f  f t )  and a  'where t o  look1 mode which i s  used t o  control  t h e  degree t o  whichthe  parser can t r y  t o  f i n d  out-of-place matches, There a re  tags  associatedwi th- these  two spec i f ica t ions ,  t he  INSERT t a g  and t h e  OMIT t a g ,  which a r eassociated with the parses involving inser t ions  and omissions t b a t  containthe number of insertions made and the number of input  symbols omitted i nbuilding t h e  parse.There i s  also a rearrangenient mode.
mus, given ce r t a in  cons t ra in ts ,the parser could be givep 'The Bites M a n  Dogt and produce a parse f o r  *TheMan Bites t h e  Dogt s ince  it would have found 'Man,' by temporarily omitting'Bites,' but then it looks f o r  and finds 'Bitest and f i n a l l y ,  f inding no se-cond lthe,fthe,l i n s e r t s  one [or some other  determiner because of t h e  [DET] func-tion]) and f inds 'Dog.
In  a similar way it would t r y  t o  produce a passiveform [i.e., the Man Is Bitten By the Dog) but since t h i s  involves more inser-t i o n s ,  etc .
it would not be chosen.These h e u r i s t i c s  a r e  control led by recording numerical summary t a g swith each sub-parse that p a r t i c i p a t e  in ,  and are  judged1 by the  disambiguatic;~~rout ines .
Similar ideas are used by Lyon (1974).c.
Predict ion Modes:As Woods (1975) has pointed out ,  t he  extent t o  which a parser ' s  predictionincreases efficiency varies with the quality of the expected input.
This f a c taffects greatly our discavBry procedures, since, if inser t ions  are to be made,one aught t o  be rather s u m  of  one's p ~ e d i c t i o n s ,  o r  risk a combinatorial ex-plosion.
In PEDAGLOT, t h e r e  i s  a programmable choice ' funct ion tha t -  con-t r o l s  predlc t ions .
Spec i f i ca l ly ,  when the  parser encounters a non-terminalsymbol, t h a t  symbol is t h e  left-hand s i d e  of various r u l e s .
An uncontrolledpkediction (used by a canonical top-down parser )  i s  t o  s e l e c t  each such r u l eas the expansion.
I n t u i t i v e l y ,  however, people do n o t  seem t o  do t h i s .
In-stead, as i n  an A'I?
?, they t r y  one and only i f  t h a t  f a i l s ,  go In to  the  next .In  PEDAGLOT, t h e  choice of which r u l e  t o  t r y  can be defined as  the  r e s u l t  ofthe c a l l  t o  a 'choosef funct ion (or it can be l e f t  uncontrolled] , We havedes&ned various approaches t o  such predic t ions  (e.g., a l imi ted  key-wordscan of the incoming s t r i n g ,  and the use of 'language s t a t i s t i c s  such as thes e t  of rules which can generate the next symbol i n  the  s t r i n g  as t h e i r  l e f tmost symbol).The predic t ion  is cur ren t ly  made once f o r  any given choice poin t ;  itsoutcomes are expected t o  be an ordered s e t  of r u l e s  t o  t r y  next.d .
Construct ion Modes :The phase of parsing i n  which t h e  p a r t s  of t h e  parse  t r e e  and associatedtag values are formed, is  a p lace  where most of t h e  non-syntactic information(tags] about the s t r i n g  being parsed can come i n t o  play.In t h e  first place,  new t a g s  can be formed as funct ions of lower l eve lparse tags tbough a process called melding, Thus, 'nonsense1 can be discoveredd pronoun references can sometimes be t i e d  down, In t h e  second place,  it i sa r e s u l t  of construct ion t h a t  ambiguity i s  discovered and dea l t  w i t h ,Since these fea tures  of parsing deal  pr imari ly  with semantics (and s ince ,i f  anyrcthere, sttsntantic representa t ions  of the s t r i n g  r e s i d e  i n  the  t a g s ) ,  mostof t b  PEDAGLOT construct ion modes involve tags .One play e x p l i c i t l y  meld t a g  values by using post-recognit ion operators ,  o rone nay def ine  an 'implicit' melding rou t ine  that i s  associated with t h e  tagnames themselves ins tead  of with indiv idual  rules.
I n  our example we usethis device t o  i m p l i c i t l y  form a simple l i s t  of  t h e  two REF t a g s  t h a t  be-come associa ted  with t h e  S ru le .
This implicit melding operat ion can a l s oinclude a blocking function, o r  some reference t o  a d a t a  base.
The t agst h a t  contain INSERT and OMIT information are used i n  t h i s  way t o  keep runningt o t a l s  o f ,  and t o  minimize the  munber o f  such h e u r i s t i c s  i n  t h e  r e l axa t ionpars ing  modes.
One may a l s o  a s s o c i a t e  a LIFT funct ion which, when t h e  par-t i a l  parse becomes complete, s p e c i f i e s  a transformation of t h a t  t a g  t o  beused as The tag of  the next higher  level  parse.Ambiguity i s  discovered when two parses from the same symbol, cbveringt he  same s t r i ng  segment axe found.
For t h i s  case,  an AMBIG funct ion i s  asso-c i a t e d  with t ag  names, and it makes a 'value judgement1 of  which t a g  i s  ' b e t t e r ,hence which i n t e r p r e t a t i o n  t o  use.
(Other types of  c r i t e r i a  can a l s o  come i n t op l a y  here such as u s e r  in te rac t ion ,  (cf.
Kay, 1973).3.
The Uses of Meta-ParsersI ha-re just catalogued some of  t h e  parsing modes ava i l ab le  i n  PEDAGLOT.
Others,such as Bottom-Up ( instead of Top-Down) o r  Inside-Out ( ins tead  of  Left-to-Right, e tc . )
,are envisionedlbut not  implemented.
Since PEDAGLOT is an i n t e r a c t i v e  program, theu s e r  can change modes a t  w i l l ,  j u s t  a s  he can change syntax o r  introduce new t ags ,Thus, the obvious first use a f  meta-parsers i s  t ha t  one may use them t o  des isnlanguage processors without having t o  t i e  oneself  down from t h e  s t a r t  t o  say, ad e p t h - f i r s t  pa r se r ,Meta-parsers a l s o  have a c e r t a i n  amount of t r a c t i b i l i t y  t h a t  parsers t h a tblend a l l  .
a c t i v i t i e s  i n t o  one huge network may not .
Ono may sea a t  a r a the r  high l eve lwhat is  going t o  be happening ( i .
e , ,  a l l  t a g s  of a c e r t a i n  name w i l l  meld togetheri n  a c e r t a i n  way, unless  t h e  grammar s p e c i f i e s  otherwise) ,  If one, however, wantsc e r t a i n  foms of local behavior, one may use predica tes  o r  funct ions on individuaQr u l e s .
Further,  i f  one wants t o  change t h e  order  i n  which predict ions a r e  evaluated,one can program a tchoosel function which w i l l  make t h a t  global change.
To alarge extent, the language designer may specify mch of t h e  processor in broadternas and s t i l l  be able t o  cont ro l  local events where necessary.In a more general sense, a meta-parser allows one t o  understand and buildhigher order theories about how people might represent and process language.For instance, while it may be true that  generating is  t h e  inverse of  parsing,there is more than one way t o  do such inve r t i ng .
One could s tart  from a senanticnetwork, using the choose function along with t h e  INSERT mode t o  restrict means o fexpression consistent with the intendea message, and using AMBIG functions to weedout a l l  but reasonable messages from m n g  the many the parser may produce o r  onemight simply t a k e  from the  semantic network a simple str ing o f  meaningful words,and then we a less t i g h t l y  programmed 'relaxation parser' t o  rearrange these wordsto be syntactically correct.
We are now considering using a crude 'backwardsT modewhich begins with the operati~n part of a ru l e  and, by using predicates (e .g .
,  AGREE)to yield inverses, specifies what the context-free pat tern must produce.
Thus thereare many variations of how t o  generate using a meta-parser.In the area of language inference, t o  take another example of language processing,PEDAGLOT suggests various differing ways of approaching the  problem.
First, ofie mayuse it a5 a 'relaxation-parser, the 'parse t ree1  can be pattern-matched aga ins tthe new sentence, and hypotheses can be famed.
Or, one could place a more rudimentaryinference systw on the 'prediction' part of the processor i t se l f ,  and using othercontrols, the predictions that  are successful could be rewritten as a new gramar.These two learning paradigms could each be strengthened by way of  t h e  use of  tagst o  contain (in a sense) t h e  meaning of t h e  sentelzces t o  be learned, Each of theseparadips can be modeled using a meta-parser like PEDAGLM.
Thus, a meta-parser canraise [and be prepared to answer) a nlrmbor of interesting questions.ReferencesEarley, J.
(1970), llAn Efficient Context-Free Parsing Algorithm,I1 Comm.
ACM 13,number 2, (February 1970)) pp, 94-102.Fabens, W, (1972), PEDAGLOT Users Manual, Rutgers University CBM-TR-12,kt.
19722,Fabens, W. (1973), PEDAGLOT Users Manual : Part 11, Rutgers University CBM-TR-23,Nov.
1973.Kaplan, R.M.
(1973), "A General Syntactic Proce~sor,~~ in R .
Rustin (ed.
)Natural Language Processing, New York: Algorithmics Press, (1973), pp.
193-242.Kay, M. (1973), llThe MIND Systemjfl in R. Rustin (ed.)
Natural Language Processing,New York: Algorithmics Press, (1973).
pp.
155-188.Lyon, G. (1974)) "Syntax-Directed Least-Errors Analysis for Context-FreeLanguages: A Practical Approach.lr Comm.
ACM 17, number 1, (January 1974),pp.
3-13.Simmons, R. and Slocum, J.
(1972), "Generating English Discourse from SemanticNetworks,''l Comm.
ACM 15, number 10, (October 1972), pp.
891-905,Woods, W.A.
(1970), "Transition Network Grammars for Natural Language Analysis,"Comm, ACkl 13, number 10, (October 1970)) pp.
591-606,Woods, W.A., [1975), Syntax, Semantics, and Speech, BBN Report No.
3067, A.I .Report No, 27.
Bolt Beranek and Newman Inc , , t o  appear in D, R Reddy (ed ,)- -~ S e c h  Recognition, Academic Press (1975) .American Journal of Compatationd Linguistics Microfiche 32 : 2 1Department of Computer ScienceThe C i t y  College of theC i t y  University of New YorkConvent Avenue at 140th StreetHew York, N e w  York 10031ABSTRACTWe describe a semantic processor we are constructing which isi n t e n d e d  to be of general applicability.
It is designed aroundsemantic operations which work on a s t r u c t u r e d  data base of worldknowledge to draw the appropriate i n f e r e n c e s  and to identify thesame entities i n  d i f f e r e n t  parts of t h e  t e x t .
The semantic oper-ations capitalize on the high degree of redundancy e x h i b i t e d  byall texts.
Described are the operations for interpreting higherpredicates, f o r  de t ec t ing  some intersententialqrelations, and inparticular detail, for f i n d i n g  the  an tece6en t s  of definite nounphrases.
The processor is applied to the problem of drawing mapsfrom direct ions .
We describe a l a t t i c e - l i k e  representationintermediate between the linguistic representation of directionsand the visual representation of maps.OVERVIEW 1,2We are trying to cons t ruc t  a semantic processor of some7A This research was supported by the Research Foundation of theCity University of New York under  F a c u l t y  G r a n t  No.
11233.The author would like to express h i s  indebtedness to Harry  Elamfor  many insights i n t o  the problems discussed here.2 2generality.
We are using as our data base a set of f a c t s  involv-i n g  spat ia l  terms i n  English.
To test the  processor and to s t u d ythe interfacing of semantic and task components, we are buildinga system which takes as i n p u t  directions in E n g l i s h  of how to getfrom one place to another and outputs a map, a map such as onemight sketch for an unfamiliar region, hearing the directionsover the phone.A typical input might be the text"Upon leaving thi,s building, turn right and followWashington Street three blocks.
Make a left, Thel ib ra ry  is an t h e  r i g h t  side of the s t ree t  beforethe next coxner.
"The ou tpu t  would be t h e  mapIL i b r a r yITo bypass syntactic problems, we are us ing  a s  our input theo u t p u t  of t h e  Linguistic String Project's transformational pro-A IWashington Streetgram (Grishman et al 1973, Hobbs & Grishman), which is very.close to a predicate-like natation.
The semantic component is.
1This Buildingdesigned around general semantic operations which work on ars t r u c t u r e d  data base of world knowledge to draw the appropriateNinferences and to identify phrases in different p a r t s  of the t e x twhich refer to t h e  same e p t i t y .
The text, augmented and i n t e r -related in t h i s  way, is then passed over to the task component,which makes arbitrary decisions when the map requires informationnot given by the directions and produces the map.ORGANIZATION OF TEXT AND WORLD KNOWLEDGEThe kwp problems of semantic analysis are to f i n d ,  o u t  of ap o t e n t i a l l y  enormous collection of inferences, the appropriatei n f e r ences ,  and t o  f i n d  them quickly .
Our s o l u t i o n  t o  t h e  firstis i n  our semantic o p e r a t i o n s  described below.
Our approach t othe second problem is in the organization of the data base.The d a t a  i n  the semantic coptponent is of two sorts:1.
The Text: the information which is explicitly in t h et e x t ,  I n  the course of  semantic processing t h i s  is augmented byi n fo rma t ion  which is only implicit i n  the text.
The text con-sists of the set of entities X1,X2, ..., e x p l i c i t l y  and i m p l i c i t l yreferred to in the text, and s t r u c t u r e s  of $he form p (X1,X2) rep-resenting the statements m#de or implied about t h e s e  e n t i t i e s , e .
g .walk (XI) = X1 walks,building (XZ) = X is a building, 2door ( X 3 ,  X2) = X is a &or of X2.
32 .
The World Knowledge or the Lexicon: the system's knowl-edge of words and the world.
Words are the boundary between theText and the LexPcon.
A word is viewed as  a key indexing a largebody of facts (Holzman, 1 9 7 1 ) .Associated with each word are a number of facts  or i n f e r e n c e swhich can be drawn from the occurrknce of p(X1, ..., X,) in theText.
The facts are expressed in terms of p ' s  s e t  of parametersY l f  ,Ykt and a s e t  of other l ex ica l  variables z l , .
.
, , z  m'stanaing for entities whose existence i s  also implied.
A factconsists of enabling c o n d i t i o n s  and conclusions.
When p ( X 1 ,  ... X,)occurs i n  t h e  Text and the semantic operations determine a24particular inference appropriate, its enabling conditions arechecked.
If they hold, the conclusions are instantiated byc r e a t i n g  a copy of them in t he  Text with the lexical variablesrep laced  by Text entities.Clusters.
One way td state the "frames" problem (Minsky1974) is "How should the data base be organized to guide, confine,and make e f f i c i e n t  t h e  searches which the semantic opera t ionsrequire?"
W e  approach this by dividing the sets of inferencesi n t o  clusters according to topic and salience in the particularapplication.
In the searches, the clusters are probed in orderof their salience.
In our application, the top-level clusterconcerns the one-dimensional aspects of objects and actions.
Forexample, the fact about a block that it is the distance betweentwo intersections i s  in the cluster.
If "around the block" isencountered, less salient clusters will have to be accessed tof i n d  i n fo rma t io ,~  about  the two-dimensional nature of blocks, Themast important fact about an apartment building is that it is abuilding, to be represented by a square on the map.
But if thed i r e c t i o n s  take us inside the building, up the elevator, andalong the hallway, the cluster of facts about the interiors ofbuildings must be accessed,A self-organizing list (Knath 1973) of the clusters is main-tained--when a fact in a cluster i s  used,  it becqmes t h e  top-level cluster--on the ,assumption that t h e  t e x t  will continue totalk about the same thing.The ''<Truth Status"  of Inferences.
In natural language,unlike mathematics, one is no t  always free to draw cer ta ininferehces.
We t a g  our i n f e r e n c e s  always, normally,  o r  sometimes.These notions are d e f i n e d  o p e r a t i o n a l l y .
An a lways  i n f e r e n c e  i sone we are always f r e e  t o  draw, such as that a street i s  a p a t hthrough space.
A normal ly  i n f e r e n c e  i s  one w e  c an  draw if it isnot explicitly c o n t r a d i c t e d  e l sewhere ,  such  as that b u i l d i n g shave windows.
A sometimes inference may be drawn i f  r e i n f o r c e delsewhere, such as the f a c t  used below t h a t  a b u i l d i n g  i s  by astreet.
This  c l a s s i f i c a t i o n  of i n f e r e n c e s c u t s  across t h e  clusterstructure of the Lexicon.Lattices.
A large number of statements i n  any natural lan-guage t e x t ,  especially t h e  texts this system analyzes, involve atransitive relation, or e q u i v a l e n t l y ,  say something about anunderlying scale.
For example, the word "walk" i n d i c a t e s  achange of location along a p a t h  through space, o r  a distancescale; " tu rn"  indicates a change along a scale of a n g u l a r  orie,n--t a t i o n .I n  any p a r t i c u l a r  t y p e  of t e x t  there are scales o r  t r a n s i t i v erelations which are important enough t o  deserve a more economicalr e p r e d e n t a t i o n  than predicate n o t a t i o n .
I n  this particulak task,the impor tan t  scales are a distance scale, a s u b s c a l e  of t h b i sindicating the path "you" $ill travel, and a scale representingangular orientation.
This is the principal information used inconstructing the map.
For these scales w e  t r a n s l a t e  i n t o  adirected graph or  l a t t i c e - l i k e  representation (Hobbs 1 9 7 4 ) .Some of the things which can be said about t h e  structure ofa scale are mat some p o i n t  i s  on t h e  scale, t h a t  of t w o  p o i n t s-on the scale one is closer t o  t h e  positive end tHan the  o t h e r ,26and t h a t  a scale i s  a part of another s c a l e .
If a point  B i scloser to the  positive end of the s c a l e  than point  A ,  this *factis  represented byA-BIf po in t  C l i e s  i n  t h e  interval  from A t o  B the representat ion  i sThe  diagrammean& the scale from C to D is part of the scale from A to B, Itis possible to represent incompleteness of information.
For exam-ple, if it i s  known that points  A and B both lie in a region Rof a scale bu t  their re la t ive  positions are n o t  known and if itis known about C only  thati,tprecedes B t h i s  i s  represented byThe lattice for  the distance  scale for t e x t  (1) is as follows:Washington St.
The Second St.thecrossst.LibraryThe lattices are intermediate between the linguistic repre-s e n t a t i o n  of the directions and t h e  v i s u a l  representation of themaps.
They are used at several po in t s  in the semantic and t a s k27processes.
They can be constructed f o r  any transitive relation,and could be very u s e f u l ,  f o r  example, in representing causal andenabling r e l a t i o n s  in a system translating descriptions of algo-rithms into flowcharts OE programs.SEMANTIC OPERATIONSBasic Principle of Semantic Analysis.
We bedieve the key tot=he first problem of semantic a n a l y s i s ,  that of finding whichinferences are appropriate, is  Joos '  Semantic A x i o m  N u m b e r  One(Joos 1972), or what I w i l l  call the Principle of  knitting.Restated, this is, "The important facts in a text w i l l  be repeat-ed, explicitly or implicity."
That is, we capitalize on the veryhigh degree of redundancy that characterizes a11 texts.
Consi i fer ,for example, the simple sentenced "Walk out the door of thisbuilding."
"Walk" implies motion from one pLace to another.
"Out" implies motion from inside something to the ou t s ide .
"Door"i s  something which permits motion from inside something to theoutside or from the outside to the inside, or if closed, preventsthis motion.
"Building" is something whose, purpose is for peopleto be in.
Thus, all four c o n t e n t  words of t h e  sen tence  repeated-ly key the same facts.
Those inferences  which should be drawnare those which are keyed by more than one element in t h e  text.This p r i n c i p l e  i s  used both formally and informally by thesemantic operations.
It is used formally in the interpretation.of higher predicates and in finding antecedents.
It is used moreinformally for deciding among competing p l a u s i b l e  an t eceden t s ,resolving ambiguities, d e t e c t i n g  intersentential relations, andknitting the text together in some minimal way.
Here it isdp r i m a r i l y  the  formal  uses that w i l l  be desc r ibed .X n t e r p r e t a t i o n .
o f  Higher P r e d i c a t e s .
I n  "walk o u t " ,  "walks lwoly" ,  and "pleasant walk" ,  t h e  h i g h e r  p r e d i c a t e s  "out", " s l o w "and ' ' p leasant"  a11 apply  t o  "walk", b u t  t hey  narrow i n  on d i f f e r -e n t  aspects of  walking.
That  is ,  each demands t h a t  a  d i f f e r e n tinference be drawn from t h e  s t a t e m e n t  t h a t  "X walks".
"Out" and"slow" demand t h e i r  arguments be motion from one place t oanother., f o r c i n g  us t o  infe ' r  f r o m  " X  walks'' t h a t  "X  goes from At o  B " .
"Out" then adds in format ion  about  t h e  l o c a t i o n s  s f  A andB, whi le  "slow" says something abou t  t h e  speed of t h i s  motion.
"Pleasant", on the other hand, r e q u i r e s  i t s  argument t o  be anawareness,  so we must i n f e r  from "X walks" t h a t  "X engages i n  ab o d i l y  a c t i v i t y  he i s  aware of" .Stored i n  t h e  Lexicon w i t h  each h i g h e r  predicate is t h ei n f e r e n c e  which must be drawn from i t s  argument and t h e  informa-11 t i o n  it adds t o  t h i s  i n f e r e n c e .
For  example, go (z l , z2 , z3 )"  mustbe inferred from t h e  argument of "out" .
When t h e  s ta tement"out(waDk(X1))" i s  encountered i n  t h e  Text, t h e  higher predicateo p e r a t i o n  makes e f f o r t s  t o  f i n d  a proof of 1 1 g o ( z l , ~ 1 , ~ 3 )  I1 from" w a l k ( X L ) " .
The search for t h i s  i n f e r e n c e  is s i m i l a r  t d  t h esearch procedure  described below f o r  f i n d i n g  antecefienes.
T h ef a c t s  in the  resulting c h a i n  of  inference are i n s t a n t i a t e dt o g e t h e r  w i t h  the  in fo rma t ion  added by the h ighe r  p r e d i c a t e ,  andt h e y  are subsequent ly  treated as though p a r t  of- the e x p l i c i t  Text .I t  i s  u s u a l  for them t o  be u s e f u l  in f u r t h e r  p rocess ing ,  u n l e s sthe  mod i f i e r  i s  simply g r a t u i t o u s  in format ion .Note t h a t  t h i s  o p e r a t i o n  a l lows  c o n s i d e r a b l e  compression i n29the number of senses that must be s tored  for each word* Itellows us, f o r  example, to define "slow" a s  something like "Findthe most salient associated motion.
Find t h e  most specific speedScale for the object X of this motion.
X ' s  speed i s  on t h e  lowerend of t h i s  scale".
This definition is adequate for such  phrasesas "walk slowlyn (the most salient motion is the forward motionof the walk ing ) ,  "slow race" [the forward motion of the competi-tors), "slow horsew (its running at f u l l  speed, usually in arace), and "slow personw.
This last case is highly dependent oncontext, and could mean the person's physical acts in general,h i s  mental processes, o r  the  act h e  is engaged in at the moment.This operation has a default f e a tu r e ,  If a proof of t h erequired inference can't be found, it is assumed anyway.
Thisallows a t e x t  to be understood even if all the words aren'tknown.
Suppose, for example, "veer rightw is encountered, andthe word "veern isn't known, i .
e .
no inferences can be drawn f r o mit.
Since "rightn requires a change i n  angular o r i e n t a t i o n  a sits argument, it is assumed this is w h a t  "veer" means.
Only theinformation that the change is  small is lost.FIND ANTECEDENTS OF DEFINITE NOUN PHRASES~ n t i t i e s  referred to in a text may be arranged in a hierarchyaccording to t h e i r  degree of specification:1. proper names, including "you" and "I"2 .
other noun phrases,  inc luding  those w i t h  definite,indefinite, and demofistrative articles3 .
khird person pronouns4 .
zeroed arguments am5 implied entities.3 0So far  our work has  concerned p r i m a r i l y  definite noun phrases ,but it is expected that many f e a t u r e s  of t h e  d e f i n i t e  noun phrasealgorithm w i l l  carry over t o  other cases,The d e f i n i t e  noun phrase a lgor i thm consists of fou r  steps.First, "uniquent2~s cond i t i onsn  are checked t o  determine whetheran antecedent i s  requ i red .
If so, t h e  Text and Lexicon aresearched for p l a u s i b l e  anteceaents .
Third, cons i s t ency  checksare made on these.
F i n a l l y  i f  more than  one p l a u s i b l e  antecedentremains the  Principle of Kn i t t i ng  is  app l i ed  t o  decide betweenthem.Vniqueness Condi t ions ,  I n  t h e  phrase "the end of the block",we know we must look back i n  the t e x t  for an e x p l i c i t l y  o r  impl i -citly mentioned "block" ( the  search case), b u t  we do fiat neqes-s a r i l y  look for a previously meptioned "end" (the no-search case) .Given a d e f i n i t e  noun phrase t he  a lgor i thm first tries t o  deter-mine whether it b e l o n g s t o t h e  search or no-search case.
This i sdone by checking two broad cr i ter ia .
(These criteria were moti-vated by a large number of examples no t  only  from s e t s  of direc-tions but a l s o  from t e c h n i c a l  and news ar t ic les , )These criteria are checked by sea rch ing  t h e  Lexicon forc e r t a i n  f e a t u r e s .
However these searches are generally veryshallow, i n  c o n t r a s t  t o  the p o t e n t i a l l y  much deeper searches inthe riext s t e p  of the algorithm.
S i n c s  by far the majority ofd e f i n i t e  noun phrases  are i n  t h e  no-search case, checking unique-nes s  cond i t i ons  can r e s u l t  i n  g r e a t  savings .A caveat is in order.
W e  state the  c r i t e r i a  at a very highlevel of abstraction, We feel i n  f a c t  t h a t  t h e  a lgor i thm canwork at that level of abstraction if the   ex icon is proper lyconstructed.
But how to construct a large  exi icon properly isa problem we have not yet tackled in detail.
In any event, wegive examples f o r  each case, and the  examples themselves form areasonably exhaustive classification.1.
A d e f i n i t e  entity is in the no-search case i f  it can belocated precisely w i t h  respect to some framework.
n his includesme following conditions.a.
Objects which are located with r e s p e c t  to some identi-f i e d  point  in space: "the building on the corner".b, Plurals and mass nouns which are restricted to someidentified region sf space: "the trees in the park", " the  waterin the swimming pool".
Here "the" indicates a l l  such objects orsubstance.c..
Points and intervals in time khich are fixed withrespect to some identified event: "the minute you arrive", "thehour since you left".d.
Events in which at least some of the participants areidentified and which can be recognized as occurring at a specifictime: nthe ride you took through the park yesterday1';e, P o i n t s  or intervqls on more abstract scales: "the endof the block", "the size of  t h e  bui ld ing".
The end is a specificpoin t  on the distance scale defined by the block.
The size ofthe building is a specific point on the general s i z e  scale forobjects , i .
e.  the volume scale.f.
Superlatives, ordinals, and related terms: " the  largesthouse on the block", "the second house on the block", " the  onlyhouse on the block".
If the set of comparison is identified,the superlative or ordinal indicates the scale oE comparison andthe place on that scale of t h e  e n t i t y  it describes.
This is asubcase of (e) .A l l  of these c o n d i t i o n s  can  be checked in one operation ifthe facts in the Lexicon are expressed in terms of suitablyabstract operators relating entities t o  scales.
We simply ask ifthe definite entity is on or part of a scale or  a t  a p o i n t  on or- -along an +interval of a scale, where the scale can be identified.However this r e q u i r e s  that w e  t a k e  very seriously m y  suggestionin Hobbs (1974) t h a t  the lexicon for the entire language be built,insofar as possible, along the lines of a spa t i a l  metaphor.
Wehave no t  yet had to f a c e  these problems since our only scales arephysica l  -- our " a t "  and "on" are the locative " a t "  and "on".Also checking this c r i t e r i o n  presupposes a very sophisticateds y n t a c t i c  and semantic analysis.
For example, [d) assumes thatthe times of events mentioned in tenseless constructions can berecovered.2.
A definite entity is in the no-search case i f  it i s  thedominant entity of t h a t  description.
This d i v i d e s  i n t o  two sub-cr i t e r i a :a ,  Those e n t i t i e s  which are unique  or dominant by virtueof the properties which describe them: " t h e  sun1',  "the wind".
Ift h e  p roper t ies  p1 (X) ,pZ (X), ..., are known about the d e f i n i t eentity X, the definitions o f  p1,p2, ..., are probed f o r  the f ac tthat the entity does not normally occur in the plural.
Includedunder this heading are proper names beginning with "the", like"the Empire State Buildingff, and appositives, like "the city ofBos tonr' .b.
Those entities which are unique by virtue of t h e  prop-erties of an entity with which they are grammatically related:"the door of the building", "the Hudson River valley".
"The doorof the buildingn is represented in t h e  Text a s  "xl 1  door'(^^,^^ 1building{X2))' i.e.
"the  Xl such that  XI i s  t h e  door of X2 whichis a building".
The uniqueness or dominance of XI is not a prop-e r t y  of "door" but  of "building".
Stored w i t h  "building" is thefact  that  a building has in its front surface a main door whichdoes not normally occur i n  t h e  p l u r a l .
"The door of t h e  bu i ld ing r 'is interpreted as this dominant dosr.If the tvliqueness conditions succeed, a poin te r  is s e t  fromt h e  dominant lexical  variable to the corresponding e n t i t y .
Ifsubsequently the same definite noun phrase occurs, the uniquenesscheck will discover t h i s  pointer  and correctly identify the ante-cedent.
Thus, we can handle the example"Walk up to the door of t h e  building.
Go throughthe door of the building.
"Here the uniqueness check gives us a s h o r t c u t  around the n e x tstep in the algorithm.The Search for Plausible Antecedents.
To illustrate thesearch for an antecedent, consider"Walk out the door of this bu i l8 ing .
Turn right.Walk to the end of the block.
"What block?
From "block" W e  follow a back p o i n t e r t o  the f a c tstored with "streetn *that "streets consist of blocks", and from34"street1' the fact with "buildingt' that "Buildings are by streets"Since a building is mentioned, we assume it is "the block of thestreet the b u i l d i n g  is on".
T h e  facts in the chain of inferenceleading to this are instantiated, An entity is introduced i n t othe t e x t  fo r  t h e  "street" and the Text is augmented by the state-ments that "the bu i ld ing  is  on the street" and "the block is partof the street".
This information turns out to be required forthe map.
Note that t he  Eact that a building is on a street is asometimes f a c t  and that we are free to d'raw it only because "theblockn occurs*To conduct the search of the Lexicon, ideally we would liketo send out a pulse from the word "block" which travels fasterover more salient paths, and look for the first entity which theptXlse reaches.
The saliency is simulated by the clusterstructure descrihea above, The parallel process of the spreadingsignal is simulated by interleafing deeper pfobes from salientclusters with shallower probes from less salient clusters.
Forexample, i f  "streets consist of blocks" is  a c l u s t e r  1 fac t ,  t h e nwe might probe for a cluster 1 fac t  involving syreets and acluster 2 Eact involving blocks at roughly the same time, Afterone plaus ib le  antecedent is found in this way, t h e  search iscontinued for possible antecedents which are n e a r l y  as plausible.If after a time no plausible antecedents are found, the searchis discontinued.Searches for  antecedents are conducted not only for entitiesbut also for definite noun phrases that the nominalization trans-formations of t h e  syntactic component have turned into statements3 5- -e .g .
"The walk was t i r i n g " .
Here we look back for a statementwhose predicate is "walk" or from which a statement involving"walkn can be i n f e r r e d .
There are  cases in which the requiredinference  is in f a c t  a summary o f  an entire paragraph--e.g.
"These actions surprised.
, . "
--although of course we cannothandle these cases.Consistencv.
Each of the plausible antecedents is checkedfor consistency.
Suppose X1 is the definite entity which prompt-ed the search and its properties areand X2 is the proposed antecedent with propertiesWe must cycle through the q ' s  and the r ' s  to ensure they are con-sistent properties.
Of course, to prove t w o  properties q(X) andr(X) inconsistent can be an indefinitely long process with noassurance of termination.
One admittedly ad hoc way we getaround this is by placing into a special cluster  those f a c t s  wefeel are likely to lead quickly to a contradiction.
The secondtool we use for  deriving inconsistencies may t u r n  out to bequi te  significant.In the course of processing, the lattice described abave isconstructed for several predicates.
They c o n t a i n  in fo rma t ionwhich can be useful i n  deriving a n  inconsistency.
Suppose wehave a t ex t  in which "the block" occurs explicitly several times.Toward the end of it, we encounter"Turn right on to  Adarnii Street.
The libraryfs at the end of t h e  block" .The search algorithm looks first for explicit mentions of "blockl"and finds them.
Yet none of these entities is the one we want.Intuitively, the reason we know this is our almost visual feelingthat we are already beyond those points.The lattice consistency check corresponds precisely to thisfeeling.
If a definite entity X1 is a point or interval in alattice or at a point or along an interval, we ask if the propos-ed antecedent X2 is or can be related to a portion of the lattice.If so, then s i n c e  the lattice represents a transitive relation,we need only ask i f  there is a path in the lattice from X2 to XI.If there is, they cannot be the same entity.Many cases which pass for applications of the supposedrecency principle--"Pick the most recent plausible antecedentn--are in reality examples of this consistency check.
The earlierplausible antecedent is rejected because of lattice considera-tions.As the text is processed, the whole structure of thediscourse is built up.
When a definite noun phrase is encounter-ed, this discourse structure is known and it is this knowledgethat is used to determine the antecedent rather than the linearordering of the words on the page.Competition among Remaining Plausible Antecedents.
Evenafter the consistency checks, several plausible antecedents mayremain, forcing us to decide among them on less certain criteria.To do this, we appeal to the Principle of Knitting again and makethe choice that will maximize the redundancy in the simplestpossible way.A probe is s e n t  out f r o m  the definite entity and from eachplausible antecedent.
Each plausible antecedent i s  searched forproperties it has in comon with the definite entity.
Commonproperties Count most if they are already in the Text, an8 with-in the Lexicon, comon properties count more if they are withinmore salient clusters or they result from shorter chains ofinference.Default.
Like the higher predicate algorithm, the definitenoun phrase algorithm has a default feature.
If the uniquenessconditions fail and the search turns up no antecedent, we simplyintroduce a new e n t i t y .
In fact, in the direct ians  texts thereare a disproportionately large number of default cases, for "theobject" may simply be the object you will see when you reachthat point in following the directions.Other Anaphora.
We have not y e t  implemented rou t ines  forhandling other anaphora.
However, we believe they a re  verysimilar to the definite noun phrase rou t ine ,  w i t h  c e r t a i n  d i f f e r -ences.
For entities tagged with demonstrative a r t i c l e s ,  we  donot check uniqueness conditions, and the search will be narrowersince the antecedent must be an entity or statement actuallyoccurr ing  in t he  text.
For pronouns also, no uniqueness cond i -t i o n s  are checked.
The search will turn up more consistentplausible  antecedents, and a correspondingly greater burden  willbe placed on the competition routine.INTERSENTENTIAL CONNECTIVESWe de tec t  unstated inter-sentence connectives by matching twosuccessive sentences S1 S2 with a small number of common38patterns.
In the directions texts the patterns are usually fewand simple.
The most common are1.
S1 asserts a change whose final state is asserted orpresupposed by S 2 .2.
S1 asserts or presupposes a state which is the initialstate of a change asserted by S2.
(These are likely very common patterns in all narratives ,)  Forexample, in the text"Walk out the door of this building.
Turn right.Walk to the end of the black",pattern(1) j o i n s  the first two sentences, where the state is"You at X", Pattern(2') joins the last two sentences, whereagain the state is "You at X-".
Note moreover that the sentencesaxe interlocked by n second application of the two pa t te rns :  Thefirst sentence assumes an angular orientation which is theinitial state of the change asserted in the second sentence.The final state of this change is assumed by the third sentence.In addition to providing the discourse with structure, thisoperation i s  one of t h e - p r i n c l i p a l  means by which implied entitiesin one sentence, like X above, are identified with those inanother.When pqttern (2) is applied, we delete the independent occur-rence of the s t a t e  in the Text, so that subsequently it ex i s t sonly as one intermediate state ih a la rger  event.
Changes acrosstime are handled in this way.TASK PERF-ORMANCE COMPONENTArbitrary Decisians, The semantic operations are quite39g e n e r a l  and can be used for any application.
The augmented andi n t e r r e l a t e d  Text i s  t h e n  handed aver to the task performancecomponent, which of course is specific to the a p p l i c a t i o n .Our task component first makes arbitrary decisions r e q u i r e dby the map but not given in the text.
Both natural languaged i r e c t i o n s  and ske tched  maps allow information to be incompleteand imprecise, but in different ways.
Far example, innTurn right at the third street or the second stoplight".we must decide whether to put the first stoplight at the firstor second street,The l a t t i ce  representing the p a t h  "your' take must be completei n  the sense t h a t  it i s  continuous, begins at the initial loca-tion, and ends at the desired goal, and that the relative loca-tions of all points on the path are known.
The lattide iscomplete if and only if there is a directed path passing throughevery point in the lattice at least once.
If it is not complete,it is completed by supplying t h e  fewest possible new links.Gsometr-izing the Lattices.
The second task operation is toc o n v e r t  the topological lattice representation into the geometricr e p r e s e n t a t i o n  required by the maps.
First we assign d i r e c t i o n sto all t h e  points in the angular orientation lattice.
In thesimplest case we may have something likewhere "a - b" means direction b results from a clockwiserotation of d i r e c t i o n  a.
If no explicit directional information4 (0is present, we simply assume a, c, and e are the same direction,and b and d are the same, and then assume the two directions areat right angles, Then in the distance lattice, contiguous oroverlapping paths which share the same orientation are assumedto be parts of the same path and are mapped into a straight line.Information about names is accessed and assigned to the streetsand buildings and the map is drawn,Specific Systems with a General Semantic Component.
We areaiming not so much at the construction o f  a general naturallanguage processing system, which still seems reasonably f a r  o f fb u t  a t  an easier way of constructing specific systems.
The caseof syntax is instructive.
It would be foolish for one who isbuilding a natural language processing system to build hissyntactic component from scratch.
Large general grammars andparsers for them exist (e.g.
Grishman et al 1973, Sager &Grishrnan 1975).
It is easier by several orders of magnitude tobegin with a genera l  grammar and specialize it, by weeding outthe rules for constructions that don't occur in the texts one isdealing with, and by adding a few rules f o r  constructions andconstraints peculiar to orre's application.We are trying to make a similar facility available for themost common kinds of semantic processing.
Specializing thegeneral semantic component would consist of several relativelyeasy steps.
First the Lexicon would be organized into acluster structure appropriate to the task.
At worst, this wouldmean specifying the necessary knowledge in a fairly simple format.If a very large Lexicon were available, this could mean no morethan designating for each fact the cluster it should appear i n .Cer ta in  inferences could be made obligatory while others whichare irrelevant t o  the task  could be l e f t  out of the special  Lexi-con altogether.
Second a Task Component would be built whichwould take, as ours does, the semantically processed Text, anduse it t o  perform t h e  task.
W e  are demonstrating the usefulnessof this approach in performing a task involv ing  a v i s u a l  repre-sentation.
It is likely to be useful in other sorts of tasks also.BIBLIOGRAPHYGrishman, R., Sager, N., Raze, C., & Bookchin,B.,"~he ~inguisticString Parser," Proc.
NCC, M I P S  Press, Montvale, N .
J .
1973.Hobbs, J e t  "A Model for Natural Language Semantics, Part I: TheModel," Yale Univ.
Dept.
Comp.
Sci.
Res.
Rep. 36, Nov. 1 9 7 4 .Hobbs, J., and Grishman, R., "The Automatic TransformationalAnalysis of Engl jsh Sentences: An Implementation,"Submitted to International Journal of Computer Mathematics.-- -Holzman, M., "Ellipsis in Discourse: Implications for LinguisticAnalysis by Computer, The C h i l d ' s  Acquisition of Language, andSemantic ~heory," Language and Speech (1971, 86-98.Joos, M., "Semantic Axiom Number One," Language (1972) 257-265 .Hnuth, D. The Art of Computer Programming, - 3 ,  Addison-Wesley,Reading, Mass., 1973.Minsky, M., "A Framework for  Representing Knowledge," MIT A1 Memo306, June 1974.Sager, N., and Grishman-, R., "The Restriction Language for Compu-ter Grammars of Natural Language," CACM 18, 7 (7/75) 390-400,-American Journal of Computational Linguistics Microfiche 32: 42PERRY t. MILLERMassachusetts Inst i tute of TechnologyCambridge,  Massachusetts 02139ABSTRACT\Jheh a user interacts w i t h  a natural language system, he may welluse words and expressions which were not anticipated by the systemdesigners.
This paper describes a system which can play TIC-TAC-TOE, anddiscuss  the game while it is in progress.
I f  the system encounters newwords, new expressions, or inadvertent  ungrammaticalities, it attempts tounderstand what was meant, through contextual inference, and by askingi h t e l i i g e n t  c larifying questions of the user.
The system then recordsthe meaning of any ne9 words or expressions, thus augmenting its1inguist;lic knowledge i n  the course of user interaction,A number of  systems tire being developed which communicate withusers i n  a natural  language such as  English.
The u l t imate  purpose ofsuch systems is t o  provide easy computer access to a technicallyOnsophisticated pepon.
When such a person interacts with a naturallanguage systemr, however, he is quite l ikely  t o  use words and expressionswhich were not  anticipated.
To provide truly natural interaction, thesystem should be able t o  respond intell igently when this happens.Most current systems, such as those of Winograd [ l o ]  and WoodsI l l ] ,  are not designed t o  ;ope wi th  such "l i igu is t i c  i n p u t  uncertainty.
"Their parsers f a i l  completely i f  an i n p u t  sentence does not use as p e c i f i c ,  b u i l t - i n  syntax and vocabulary.
A t  the other extreme, systemsl i k e  ELIZB [93 and PARRY [ Z ]  allow the user to type anything, but make noattempt t o  fully understand the sentence.
The present work explores thetnlddle ground between these extremes: developing a sys.t;em which has agreat deal of knowledge about a particular subject area, and which canuse this knowledge to  make language interaction a flexible, adaptive,learning medium.In pursuing t h i s  goal, the present work is  most closely relatedt o  work being dona i n  the various speech recognition efforts [5 ,  7, 8,121 which ara studying how l ingu i s t i c  and semantic constraints can h e l pdeal w i t h  the ACOUSTIC error and uncertainty of speech.
The adaptivesystem, however, is designed t o  deal with a much mors LINGUISTIC type ofuncertainty.When people use unfamiliar words or expressions in conversation,we can usually deduce from context what is meant, and i f  not, we can a tleast ask inte l l igent  clarifying q u ~ s t i o n s .
To allow the machine to  dothe same, there must be a very flexible interaction of syntax and$emantics i n  the parsing/understanding process, There must be adif ferent  parser organization, and a more f l e x i b l e  use oP l i n g u i s t i c  andsemantic constraints ,  than i s  p f e s e n  t in current natural languagesys tern .The adaptive system is a step towards t h i s  goal  Tha cu r ren timplementation i s  a prototype, design'ed to  i l l u s t r a t e  many o f  theseideas, and t o  t i e  them together in a restricted system t h a t  is completebut not t o o  complex.
The system's domain o f  discourse is TIC-TAC-TOE: it.plays a game and discusses the game in progress.
It has a ( c u r r e n t l ylimited) se t  o f  semantic concepts, and attempts t o  interpret the u s e r ' ssentences ih terms of these.
The set of semantic concepts is FIXED.
Thesystem does not attempt t o  learn new concepts, merely new ways ofdescribing concepts that  are already famil iar.When t h e  system encounters a new word or a new phrasing, it usescontextual information t o  detername the p o s s i b l e  meaning, and to askclarifying questions which a re  as i n t e l l i g e n t  as p o s s i b l e .
It thenremembers the meaning of the new e&pressfon, and w i l l  use it i tse lF *enit talks.2.
SAHPLE SESSIONThe best way t o  introduce the system is t o  show i t  in o p e r a t i a n .In the sample session that follows, user input is pteced~rd by 'U:",machine output  by and comments added by the author arg in smalltype, enclosed in brackets,(HACHINE: I PLACE AM X IN TClE TOP LEFT SQUARE)[The machine makes its opening move.
](USER: PLAY A ZERO IN THE CENTER WARE)(H: 1s ZERO A NOW HEANING 0 )(U: YES)(H: OK)[The new word *zeroa is added t o  the machine's vocabu1ary.l(R: I PLACE AN X IN THE BOTTOM RIGHT WARE)(O: SLAP Ab4 0 IN THE BOTTOV!
LEFT CORNER)(PI: IS (TO SLAP SOMETHING SOREWHERE) A WAY OF SAYING (70  PLACE SOMEmINGSOMEWHERE))(U: YES)(19: OK)[ A  new surface framer (as described in section 4 )  I s  added to the set  sfsurface b r a e s  which express the concept of making a move.
]( m :  1 SLAP AN X IN TWE TOP RIGHT SQUARE)f n t h i s  sample sass ion ,  the machine uses i t s  newly acquired vocabularya t  every opportunity.
)HOU CORE YOU PLACED AN X IN THE TOP RIGHT SQUARE)(H: IS (HOW CQHE MMEONE DOES SOHETHING) AN EXPRESSION ISEANIMG ( l!lOESSOMEONE 00 SQMETHING))(U: YES)(M: OR)(H: TO-BEAT YOU)[ A  new surface frme i s  added t o  those sxpssssing the concept ' ISyntactic featuras are associated w i t h  each such frauge i n d i c a t i n g  suchthings as Wether subject-verb invsrsion is ds be DerBomed.1(U: CAN YOU CURRENTLY BEAT M E )m: 1s CURRENTLY AN ADVERB REFERRING TO THE TInE OF A RWE)[These are the only  adverbs the system knows abou t .
In f ac t ,  a t  present,the system ignores adverbs in its sewantic processing.
I %  asks thequestion because it wants t o  account for all t h e  words in the sentence.
(U: YES)'(M: OK)(11: YES)( M : r s WEST AN ADJECTIVE ~ E A N I N GEITHER(1: CORNER)( 2 :  LEFT)(U: 2) (n:  or;^(H: NO)[Notice that t w o  top squares contain an x .
However, only the l e f t  onelwas played while the bottom r ight  square was Free.
Therefore the systcsznconcluded t h a t  the top l e f t  square was being described.
This examplei l lustrates  how contextual information can be used t o  ask as intelligenta cBarifying question as p a s s i b l a .
](U: *yOW COME YOU PLACED AJ4 X IN THE TOP EAST SQUARE)(!I: IS EAST AN l@JECTLVE NEAWXNG)EITHER(1: RIGHT)(2: WEST)( 0 :  l p(U: on)M IS (TO LAY SONETHING SOHEWHERE) A WAY OF SAYING (TO SLAP SOflETHINGS0UEWERE) )(U: Y E S )(M: OK)(H: IS (TO POLISH SOMEONE OFF) A WAY OF SAYING)E I THER( 1 : ( TO BEAT SOMEONE) )(2: (TO LOSE TO SOMEONE))( 3 :  (TO TIE SOMEONE))(U: I )(?'?
: OK)(R: BY LAYING AN X ON WE TOP CENTER CELL)[ I f  t h i s  same session were subsequently repeated,  sf course, the systemwould not  need t o  ask any questions of t h e  ushJr.
12.1 Comments on C u r r e n t  LirnitatiansThere are a number of l i m r t a t i o n s  to the a d a p t i v e  system as itnow s tands .
Some of these may be apparent in the smple session, bud anin t roduc t ian  t o  the system is not complete without discuss ing  themexplicitly.
(1) The number of concepts ava i lab le  t o  the system a t  present is verysmall.
T h i s ,  in fact, is why the system's first guess is usually thecorrect one.
I f  the sentence is at a l l  with in  the systea'scomprehension, t h e  options as to its meaning a re  currently q u i t e  limited.
( 2 )  The range of expressive devices  presently recognized is q u i t slimited as well.
For instance, the  system does n o t  recognaze relativeclauses, con junctions, o r  pronouns (except f o r  1 and you).
( 3 )  The system currently d e a l s  only  wi th  TOTALLY U N F M I L I A R  words andexpressions in this adaptive fashion, It w i l l  not correctly handlefamiliar words which are used in new ways (such as a noun used eas a varb,as i n  wzero the center  squaren) .
( 4 )  The system tr ies  to map the meaning o f  new wards and expressiunsinto i t s  speci f ied  s e t  of underlying concepts.
It then displays itshypotheses t o  the user, g iv ing  h i m  only the option of saying yas or nu.The user cann-ot say "no, not qui te ,  it meahs .
.
.".
(Thus concepts likeV h e  'northeast1 square" o r  "the 'topmost' squarew would ba confusing andnot correctly understood.
)The present simple system h a s  been developed w i t h  two goals inmind: (1) to explore the techniques required t o  achieve adaptivebehavior,  and ( 2 )  t o  h e l p  fornulate the issues which will have t o  befaced when incorporating these techniques in to  a much broader naturallanguage system.3 .
OVERVIEWFig.
1 shows ths various stages that  the Adaptive System geesthrough in understanding a sentence.
In this sectian, we s h a l l  watchwhile t h e  system processes the sentence "Mow came you placed an x in thetop right ~ q u a r e .
~( 1 )  Local Syntact ic  Processing:In this f i r s t  stage, the system scans the entire sentence look ing  f o rlocal  cons t i tuents .
These i n c l u d e  Hsimplem noun phrases (NPs) andprepositional phrases (PPs), ("simplen meaning 'up to the head noun butnot including any modifying clauses or phrases"),  and verb groups (VGs)consisting o f  verbs together with any adjoining rnodals, auxilliaries, andadverbs.
In t h i s  instance, the  system Finds the t w o  N P s ,  "youe and "anxm,  the PP "in the top r ight  squarem, and the VG nplacedw.
( 2 )  Semantic Clustering:A t  t h i s  s tage ,  the c lause- leve l  processing s tar t s .
U n l i k e  most systems,this clause- level  processing is driven by SEMANTIC r s la t i onsh ig s ,  rath-erthan by syntactic form.
It uses a semantics-first kclustssinsg*, with asscondary use of syntax for cormnents and confirmation+ In t h i s  example,a l l  t h e  l o c a l  constituents found can be clustered i n t o  s description of esingle concept: t h a t  o f  making a nave, Section 4 describes the mechanicsof this stage in more detail.
( 3 )  Cluster Expansion and Connection:During t h i s  stage an attempt I s  mada t o  account Psr each word in t h esentence by expanding the concept c lus ters ,  and i f  there i s  more thawone, by j o i n i n g  them together t o  form an e n t i r e  multicXausa1 sentence-In t h i s  case, ths concept c luster  rnlght b s  axpanded I n  two ways.a )  One possiblllty night  be t h a t  I t  i s  a "MOW" type q u e s t i o n ,  and t h a twcornc.tn is some sort of adverb,  However this possibility v io la t s f  asemantic constraiet, since the system is not s e t  up t o  answer haw a moveis made; only how t o  win,  how t o  prevent sorneons From winning, e t c .There fore  this possibility is ignored.b) The other p o s s i b i l i t y  f r; t h a t  "how come" i s  a new way of  describingsoma other clause f u n e t t o n .
(4) Contextual Inference; Clarification; and Response:During t h i s  f i n a l  staga, any c o n t e x t u a l  inf~rrnatfsn avai lable  is broughtt o  bear on araas of  uncertainty, any necessary clarifying questions areasked, and the system responds t o  the sentencs.
In this example, theonly uncertainty is the meaning of "how comew.
Since t h i s  i s  the mainsentence1Xocal constituentsconceptclusterscompletesentencehypothesf ssystem respondst o  sentenceFig.
1: Adaptive System Overviewclause of the sentence, the possibility of  its b e i n g  an Wn or *aftsraclause are discarded.
The remaining p o s s i b i l i t i e s  are n i m p e r a t i v s w ,"hown, m ~ h y n ,  and "canw.
The system does n o t  answer %own and "canwquest ions i n  relation t o  making moves.
Similarly, "imperativen does n o tmake sense since the action described is  a previously made move.Therefore the system asks i f  "How come someone does somethingw means Vhydoes someone do somethingn.
The user answers "yesn, so  the system storest h i s  new way of asking "whyn, and proceeds t o  answer the question.4 .
SEMANTICS-FIRST CLAUSE-LEVEL PROCESSINGOne of  the major differences between t h i s  approach t o  parsing andtha t  of a top-down, syntax-driven system (such a s  Moods' or Winograd's)is the order i n  which s y n t a c t i c  and semantic processing is done a t  theclause level.In  a top-dom system, a sentence must exactly match t h e  b u i l t - i nsyntax before semantics can even be cal led and given the variousconst i tuents  o f  a clause, T h i s  IS clearly undesirable when one i sdealing with  i n p u t  uncertainty, since one cannot be sure exact ly  how theuser will phrase his sentence.
One would prefer to Bet semantics opera%@First on any local consituents present, so that i t  can make a reasonablegrgss as to what is being discussed.As semantically-rslated clusters of local constf  tuents are found,syntax can be consulted and asked to comment.
on the rslativegrmmaticality of the various c lus ters .
If there are two competingsemantlc inte~pretations of one part of  a sentence, and syntax l i k e s  onemuch better than the other ,  then the "syntactically pleasing"interpretation can be pursued f i r s t .
Later, i f  this does not pan out,the syntactically irregular possibility can be looked at  as wsP1.
Int h i s  way, syntax can he lp  guide the system, but is not placed in atotally controlling p o s i t i o n .A by-product advantage o f  t h i s  s e m a n t i c s - f i r s t  approach i s  thatthe system can handle mildly ungrammatical input without any ex t ra  work,In addit ion,  t h e  semantics-first  c lus tar ing  approach lends i t s e l f  q u i t enaturally t o  handling sentence fragments.I n  the remainder of t h k s  s e c t i o n ,  we describe how the adaptivesystem organizes i d s  linguistic knowledge t o  implement this semantics-f i r s t  approach.
As we s h a l l  s e e ,  there are three componeflts o f  thisknowledge.
( a )  Ths local racognizars which initially find local constituents.recognizers are represented t n  Augmented Transition Network [ I l l  f o m ,are q u i t s  s i m p l e ,  and are not described further i n  t h i s  paper .
(b) Clause-level knowledge sf how actions and clause-functions aredescribed.
This  knowledge is expressed i n  a descriptiva fash ion  whichmakes it msily manipulabla, and easy to add to.
( c )  Clause-level syntac t ic  knowladge which is sxprssred ira a domain-indebpendent fom.4 .
1  Knowledge of how A c t i o n s  are DescribedFigure 2 i l l u s t r a t e s  how t h e  system s t o r e s  i t s  knowledge sf howact ions  ( o r  events) are described.
This knowledge is stored a t  twol eve l s  : the conceptual  l e v e l ,  and t h e  surface (or expressive) l e v e lAs shown in F i g .
2, the  concept PLACE represents  the a c t  o fmaking a TIC-TAC-TOE wove.
( a )  On the  CONCEPTUAL l e v e l ,  there are three "conceptual s lo t s 'i n d i c a t i n g  the  actors which are involved  in the ac t l on :  a player, a @ark,and a square.
(b) On the SURFACE, or expressive, level there is a list sf surfaceframes each indicating one poss ib l e  way t h a t  t h e  concept  can beexpressed.
Each surface frame conslsts  o f  a verb p l u s  a set  of s y n t a c t i scase frames to be f i l l e d  by t h e  ac tors .
(Notice that neither the conceptual slots nar  the sur face  frames i n d i c a t eexplicitly t h e  order in which the varlous constituents are to appear Fw asentence.
)When the system processes a sentence, it fills t h e  concsp tua lshots w i t h  local  constituents found rn the sentence I f  i t  h a s  f o u n d  af m i l i a r  verb, then i t  a l s o  gets  any surface e ( s )  associated w i t hthat  verb.
A t  this p o i n t  i t  c a l l s  syntax,  a s k i n g  for c s m e n t s .For instance,  i f  the input sentence is "1  place an x in thecorner", t h e n  all the conceptual slots of #PLACE would be f i l l e d ,  and thesystem would pass the following string to syntax wagen% verb o b j  ppw .
Asa result, clause-level syntax does not see t h e  a c t u a l  constituents of  thesentence, only t h e  l a b e l s  specifled I n  the surface case frame, plusinformation indicating number, tense, etc .An interest ing aspect o f  this approach is t h a t  t h e  clause-levelsyntax is entirely domain-independent.
I t  knows no thing about TIC-TAC-TOE, o r  even about the words used t o  talk about TIC-TAC-TOE.
Tke surfaceframes allow semantics t o  t a l k  t o  syntax purely in t e rms  o f  syntact iclabels.
As a result, one could write a single syntact ic  module, and t h a ninsert i t  unchanged in to  many domains.4.1 .1  Using t h i s  InformationIn t h i s  s e c t i o n ,  we descr ibe i n  more d e t a i l  how this  knowledgecan be used when processing a sentence.
(1) I f  the verb and constituents a re  familiar:I f  t h e r e  i s  no uncertainty i n  a c lause ,  then each const i tuent  canbe put  into one of Ghe conceptual s lots ,  and any surface framesassociated w i t h  the verb can be examined The frame ~ n d i c a t e s  the csse(agent, object, etc. )
associated with each c o n s t i t u e n t  whon that  verb isused.
The frame is used t a  create a string of case l a b e l s  t h a t  a r e  s e n tt o  syntax for  coments .For instance, iF the sentence is "1 place an x i n  the centerCONCEPT: PLACECONCEPTUAL SLOTS:P: playerH: markS: squareSURFACE FRAMES:VERB: place (as in:AGENT: P m I  place an x i n  the centera)OW: PIin: SVERB: play (as in:AGENT: P sf play an x in the centers)ow: HIn:  SVERB: play (as in:AGENT: P w X  play the center")00J: SFLg .
2 : Linguis t i c  KnowleMge about Actionssquare", the string passed to syntax is "agent verb obj  pp".
Syntaxreplies t h a t  t h e  sentence follows normal order.
Had the  string been"verb obj  pp" syntax would reply t h a t  the subjec t  had been deleted .
I fthe s t r i n g  was @'do agent verb obj ppn, syntax would reply that subject-verb inversion had taken p l a c e .
Given "gent obj verb ppn,  syntax wouldreply that  t h e  object was out of position.Thus syntax i s  se t  up  to notice both g~irnmcatical andu n g r m a t f  cal permutations i n  constituent order, and t o  commentappropriately.
The system must then decide how t o  interpret thesecomments.For instance, if syntax repl ies  t h a t  the object is out ofposition i n  the clause,  or t h a t  there is incorrect agreement in numberbetween subject and verb, the  system may decide that t h e  user has made aminor grammatical error, and allow the sentence t o  be processed anyway,especially if there i s  no better  interpretation of the sentence.
In thisway, clause-level syntax plays an a s s i s t i n g  role rather than acastrolling r o l e  i n  t h e  analysis of  a sentence.
( 2 )  If a constituent is unknown:If an unknown constituent is p r e s e n t ,  then both the frame andslot information can be used to h e l p  resolve its meaning.
For ins tance ,suppose the sentence is " I  place a c r o s s  in the  canter squarew, and the,word ~ c r o s s u  is unfamiliar,Here, during t h e  semantic clustering, t h e  conceptual s l o t s  for aplayer and a square can bs f i l l e d  by "Iu and "in the center square", b u tthe slot for a mark is u n f i l l e d .
I n  a d d i t i q ,  there is the unknownconstituent "a crossg.A natural hypothesis,  therefore, is t h a t  the unknown constituentrefers t o  a type of  mark.
Since the verb is familia~, a surface frme isavaflable.
Next, assumtag the unknown constituent is a mark, the s t r ing"agent verb ob j  ppw can be passed to syntax.
Men syntax approves, thisoffers addi t iona l  confirmation t h a t  the hypothesis is probably right.Subsequent evaluation of this hypothesis indicates t h a t  thesentence makes sense only if the mark referred to is Etn x ,  so the systemasks i f  "crossu is a noun meaning( 3 )  I f  the verb is unknown:I f  an unfamiliar verb is used, then  there i s  no sur face  fsmeavailabls t o  h e l p  guide the analysis.
Instead, syntax must ba used in adifferent mode t o  propose what the surface frame should be.Suppose the sentence is "I  p lunk an x in the center squareM.Here, a l l  the constituants can be clustered into the concept #PLACE, butt b r e  is an unknown word, and no verb.
Ths loglcrrl hypothasis is t h a tt h e  new word i s  a verb.
A special syntactic module i s  therefore passadt h e  followfag s t r i n g  "NP(P) verb(p1unk) NP(M) PP(in,S)# This moduleexamines the  string and produces tn new Frame:VERB: plunkAGENT: POW: Rin: 8The system can then ask if "to plunk something somewherew means" t o  place something somewheren, and upon getting an affirmative reply,can add t h e  new frame to those associated w i t h  the concept PLACE.Since the system uses the surface frames to generate its o mreplies, it can now-use this new frame i t se l f  when it talks.
When thesystem wants to generate a c lause ,  it passes a selected frame, theconstituents, and a list of syntactic features to a clause generatorwhich o u t p u t s  the specified form.
(Thus, c l aus s - l eve l  syntax can be usedby the  system i n  three different  modes: (1) to comment on theg r m a t i c a l i t y  of a s t r i n g  of case markers, (2) t o  constrbct  a newsurface frame, and ( 3 )  t o  generate clauscas when t h a  system itselfreplies .
)4.2 Knowledge of' how Clause-Functions are DescribedAs i l lustrated i n  Fig.
3,  knowledge of  how clause-functionconcepts are described i s  also expressed as two Lexals.CONCEPT: #WHYCONCEPTUAL SLOTS:ACTION: #PLACESURFACE FWhy ACTIQN(SV1NV) ( a s  in:*Why does someone do somsthkng")flow come ACTION() ( as  i n :"Now come someone does something")Fig.
3 : Linguist ic  h o w l  edge about Clause FunctionsEach clause function has a conceptual s l o t  indicat ing what typesof action can be used w i t h  t ha t  clause type ( i n  t h i s  case, the  ac t ion#PLACE), and a list of surface frames ind ica t ing  di f ferent  ways i n  whicht h e  cancspt can be expressed.A clause-type frame currently includes any special  words whichintroduce the c lause  ( i e .
"whyn or "how comen),  together w i t h  a list sfsyntactic proparties which should be present in the clauss.
This list ofsyntactic properties might include SVIMV, nsubjec$-verb inversionw (as in"why does someone do something"), ar 9 u b  ject  deletionH, 'ING fomm, and"use of a particular preposition* (as  i n  "from doing somethingw).These syntactic features, however, need not bs inflexible rules.Sentence understanding can still psocaed wen  i f  tha  syntact ic featuresfound by syntax do not exactly match those spec i f ied  by the clause-function frame.
Thus, an inadvertent ungrammaticality cam readily berecognized as such, and processing can cont inue .4.2 .1  Using the Clause Function KnowledgeIn this section we examine how this clause function knowledge canbe used.
(1) With no uncertainty:I f  the i n p u t  sentence is "Why d l d  you place an x in the centersquarew, then during the semantic clustering the s tr ing  Rdo agent verbobj ppu i s  passed t o  syntax, which repl ies  t h a t  subject-verb inversionhas taken place.When exarninlng t h e  whole clause, the system sees t h a t  it e x a c t l ymatches one of the surface frames for  a #WHY-type question, since itstarts  with the word n ~ h y V i n d  contams subject-verb inverslbon,Suppose, however, the sentence had been "Why you place an x IRthe center squaren, or "How come d i d  you place an x i n  the centersquare*.
Each o f  these sentences matches a surface frame for  a MY-typequestion, except that i n  both cases subject-verb inversion i s  incorrect.In such a case, the system can, if it chooses, decide t h a t  the user hasmade a minor error, and allow the sentence t o  be processed anway.
Thelocally-driven semantics-first approach Lets this happen i n  a naturalway.
( 2 )  A new surface frame:Another problem arises  when a new clause introducer isencountered, as i n :  "Wherefore d i d  you place an x i n  the center squareM.Here, as described i n  section 3 ,  the system hypothesizes that  this may bea new way of  asking a #WHY-type question.
Since syntax reports thatsubject-verb inversion has taken place,  the system can therefore create anew surface frame:Wherefore ACTIOM(SV1NV)t o  be added t o  the frames associated w i t h  #WHY.B In summary, the adaptive -5ys tern stores i t s  l inguis t i c  knowledgei n  a very accessible form.
I t  is not embedded in the parsing l o g i c .howledge of how actions and clause-functions are described isrepresented i n  a descriptive,  manipulable format.
Syntax is domainindependent, and is used only t o  make cornants, with semantics playingthe guiding role.
This organization allows the parsinglunderstandingprocess t o  proceed kn a f lexible  fashion,5 .
CONCLUSIONLanguage communication is an i n h e r e n t l y  a d a p t i v e  medium.
Onesees t h i s  c l ea r ly  ~f one takes  a problem t o  a lawyer and spends timetrying t o  assimilate t h e  r e l a t e d  " l e g a l e s e n .
One a l s o  sees i t  i n  anyconversation where a persron is t r y i n g  t o  convey a complicated idea ,expressed i n  his own mental te rms,  t o  someone else.
The l i s t e n e r  mustt r y  t o  r e l a t e  t h e  words he Rears  to h i s  own set of concepts .
Languagehas ,  presumably, evolved t o  f a c i l i t a t e  t h i s  s o r t  of i n t e r a c t i o n .Therefore it is reasonable t o  expect  t h a t  a good deal  of the  structure oflanguage is i n  some s e n s e  s e t  u p  t o  assist i n  this adap t ive  process.
Byt h e  same t o k e n ,  studying language from an adap t ive  standpoint shou ldp r o v i d e  a f resh p e r s p e c t i v e  on how t h e  va r ious  levsls of l i n g u i s t i cstructure i n t e r a c t .REFERENCES[ l ]  D a v i e s ,  Q.J.M., and Isard, S.D., 'Ut terances as Programs, "resenteda t  t h e  7 t h  I n t e r n a t i o n a l  Machine I n t e l l i g e n c e  Workshop, Edinburg, J u n e1972.
[2] Enea, H .
,  and Colby, K , M .
,  ' Ideolectic Language Analysis f o rUnderstanding Doctor -Pa t i en t  D i a l o g s ' ,  Proceedings o f  t h e  3rd IJCAI,Stanford, August 1973.
[3 ]  Fillmore, C.J.
,  'The Case for Case' ,  i n  'Universals i n  L i n g u i s t i cTheory', Bach and Warms (Eds.
), Wolt, Rinehar t ,  and Winston, I n c .
,Chicago 1968.
[ 4 ]  Joshi, A .
K .
,  and Weischedel,  R.M., 'Some F r i l l s  far  the Hodaf TIC-TAC-TOE of Isard and Davies: Semantics of Predicate ComplementConstructions,' Proceedings  of  t h e  3 rd  IJCAI, Stanford, August 1973.5 ] e ,  P .L., 'A Locally Organized P a r s e r  f o r  Spoken I n p u t ' ,  Corn.ACM 17, 11 -(Nov, 19741, 621-63@.163 Miller, P.L., 'An Adaptive  System: f o r  Natura l  Language Understandingand Assimilation', RLE Na tu ra l  Language memo No.
25, H I T ,  February  1974.
[ 7 ]  Reddy, D .
R .
,  Erman, L .
D .
,  Fenne l l ,  R.B., and Nealey, R .
B .
,  'TheHEARSAY Speech Understanding Systemt, Proceedings of' the 3rd HJCAZ,Stanford, August 1973.
[ a ]  Walker, D.E., 'Speech Understanding through Syntactic and SemanticAnalysis', Proceed ings  of t h e  3 rd  IJCAI, Stanford, August 1973.
[ 9 3  Weizenbaum, J .
,  'Eliza- a Computer Program f o r  the  S tudy  of NaturalComunicatian between Man and Machine', CACM 9 ,  1972.
[ l o ]  Winograd, T.  Procedures  as a Representation of Knowledge Fw aComputer Program Tqr Understanding Natural Language, MAC-TR-84, P r o j e c tMAC, MIT, Cambridge, Mass., February 1971.
[ l l ]  Woods, W.A., and Kaplan, R .
N .
,  'The Lunar  Sciences Natural LanguageInformation System" BBN Report No.
2265, Bol t ,  Beranek, and Neman Xnc.September 1971,[12] Woods-, W.A.,  and MakhsuP, J .
,  'Ovlechanical In fe rence  Problems i nContinuous Speech Understanding , Proceedings of t h e  3rd HJCAB, Stanford,1973.1575 ACL McetlngCONCEPTUAL GRAMMARW I L L I A M  A ,  M A R T I NKassachusetts Insti t u t e  of Tech~ologyIn OWL, an implementation o f  conceptual  grammar, t h e  twotypes o f  data items are symbols and concepts and the  two bas icdata composition operat ions  are specialization and restriction.A symbol is an alphanumeric s t r i n g  headed by ".
Symbolscorrespond to words, suffixes, pre f ixe s ,  and word stens inZnglish and the programer can introduce them a t  willmOWL concepts correspond t o  t h e  meanings of EEglish wordsand phrases.
They are constructed using the  specialization ope-ration, comparable t o  CONS i n  LISP* (A B) is t he  specializationof A ,  a concept, by B, a concept o r  symbol.
OWL f o r m  a branch-ing tree under specialization, with SOMETHING a t  the  t o p .Concepts are given properties by restriction, which puts aconcept on the reference list of another  concept (compare proper tyl ists and S-expressions in LISP).
A / B  is the r e s t r i c t i on  of  Aby B.The categories in the specialization tree are semantic, butwe use them also f o r  the purposes usually assigned to syntact icdategories.A predication is a double specification of 2 model such aspresent tense or can.
Examples areThe pool is full of  water.
((PRES-TNS (BE (FULL 94TER)) J POOL/THE)The cookie can be in t h e  j a f .
( (CAN (BE (IN JAR/TIIE))) COOKIE/THE)aob is the fa ther  o f  Sam.
( (PRES -TKS (BE (FATHE: SAM) ITHE) ) BOB)3ob hits the b a l l .
((PRES-TNS (HIT BALLITHE)) Boa)Bob is hitting the b a l l .
((PRES-TNS (BE (-ING (HIT BALL/THE))))BOB)Starting from t h i s  base we will discuss a number of issuesbuch as n~minalization incorporat ion,  and deep vs surface cases.American Journal of Computational Linguistics ~ i c r o f f c h e  32 : 58JOHN F.  BURGER, ANTONIO LEAL, AND A R I E  SHOSHANISystem Development CorporationSanta Monica, California 90406m c TWe describe a natural-language recognition system having both applied andtheoretical relevance.
A t  the applications level, the prwram w i l l  give anatural ccmmunications interface facility to users of existing interact ivedata management systems.
A t  the theoretical  level,  our work shows that  theuseful infoxmation i n  a natural-language expression (its "meaning") can beobtained by an algorithm tha t  uses no formal description of synt-.
Theconstruction of the parsing tree is cont ro l led  primarily by semantics i n  theform of an abstraction of the nmicxo-world" of the DMS's func t iona l  capabil-ities and the organizat~on and semantic relations of the  data base contentmaterial.
A prototype is current ly  implemented in LTSP 1.5 on tho IBM370/145 computsr at System Development Corporation.In a recent article in Scient i f ic ,  American, Dr. Alphonse Chapanis says, "Tft r u l y  interactive computer ( ; y s t m  are ever to be created, they will ~omehowhave to cope w i t h  the... errors and vio la t ions  of format tha t  are  the rulerather than the exception in normal human ccmmunication" [1] .
An exampledialogue produced by t w a  persons interacting w i t h  each other by teletype-writer to solve a problem as~igned to them by experimenters showed that :notone grernaaatfcally correct sentence appears in the  entire protocol.
tlMany existing language pmcessors (woods ,  Kellogg , Thcmpson , etc.  )
[ 2,3,4)are limited to what Chapanis calls "Irmnaculate prose," that i s ,  "the sen-tences that are fed into the computer are parsed in one way or another sothat the m e a n i n g  of the ensemble can be inferred frm conventional rules ofsyntax," which are a ?0- descr ip t ion  of the language.
In effect, usersare required to in teract  w i t h  these s y s t e m  in sme  formal language, or atleast  i n  a language that has a formal representation i n  the computer systemthat  a user's expression must conform to (we are t h i n k i n g ,  in t he  latterinstance, of Vhampsonls REL, which has an extensible formal representationfacility).
In addi t ion ,  most natural-language question-answering systems,including all referenced above, require that a user's data be restruct-wedland reorganized acwraing t o  the pa r t i cu la r  data base requirements of thenatural-language system to be used.A t  the level of a r t i f i c i a l  in te l l igence research [ti ,6 ,?
'I , Mere is sameinterest in systems that recognize meaning i n  natural-language expressionsby methods that dd not m i r e  compiler-like syntactic analysi~ of anexpression prior to asmantic interpretation.
We believe it is possible,practical, and feasible, using new lingufstic processing strategies, todesign a natural-language interface system that will permit flexible, intu-itive coaansmicatiba w i t h  information management systems and other computerprograms already in existence.
This interface is open-ended in that it hasno prejudice about t h e  user's system funckians and can be joined to almostany such system with relatively l i t t l e  effort.
I t  i s ,  i n  addition, able toinfer t h e  meaning of free-form English expressions, as they pertain to thehost system, without  requiring any formal description or representation ofEnglish.THE SEMANTIC INTEREACE ALTERNATIVEThe syntactic inflexibiiity of existing natural-language processors limitstheir usefulness i n  interactive man-madine tasks.
O u r  approach does notuse a collection of syntax rules or equations as they are normally defined.Instead, we  construct a dictionary in which w e  define words in terms of theirpossible meanings with respect to the particular data base and data manage-ment system (DMS) we want to use and according to the possible relationstha t  can exist between data-base and I3MS elements ( e .
g .
,  an averaging func-t i o n  on a group CKE numbers) i n  the limited "micro-world" of this preciselyorganized data collection.
Words appearing in a user's expression t ha t  arenot explicitly defined are ignored by the system i n  processing the expres-sion; an example would be the  word "the," which is usually not meaningful ina data management environment.
Wa thus avoid the expressive rigidity thatformal syntactic methods hposa on tha user and the excesaivcs time andresource consumption tha t  results from the catibinatorial explosions usuallyproduced by such rnethade.We distinguish in their def ini t ions  beween two types of words: contentwords m d  function w o r b  (or "operatore").
Content words are wads whoae'meaningsw are the objects, events, and concepts that make up the subjectsbeing referred t o  by users, More precise ly ,  for data axetnagernent systems,these meanings (or "concepts") are the f i e l d  names and entz'y i d e n t i f i e r s  f o r*e data b-e and the names for available IHS operations such as averaging,s d n g ,  sorting,  comparing, etc.
Function words serve as connectors ofcontent words.
Their use i n  natural language i s  to indicate khe manner inwhich neighboring conltent words ar'e intended to relate to one another.
Inthe example "the salary of the secretary ," used belaw, "salary" and"secretary," are content  words, and "of" is a function word used to connecttheta.Many cmntent wor& are context sensitive, In a particular data base, fo rbtmcm, the ward "salary" may refer t o  the data-base f i e l d  name SECSAL ifthe saXW frs "of a secretary," but may also  indicate the f i e l d  name CLKSALif it is a *salary of a clerk."
In recpgnition of this we therefore def ineeaah aontent word by a set of one or more pairs of the form( ( X I  Y l )  (X2 Y2) .
.
.
(Xn Yn))where the Xi a d  Y i  are " o o n c e p ~ "  (that is, f i e l d  names, etc.)
as describedabove.
This expression may be interpreted as, "if the word so defined i r j tcontactually related in a sehtance to Xl, its particular meaning in thiscentact  is Y1,  if it i s r  eo related b X2, it meme Y 2 ,  m d  ao forth."
Thisparticular oontextual mnaranfng af the word is callad its sense.
Two contentwarm are consrid=& to  bls artmantically related i f  the in te rsec t ion  of  theX i ' a  fmtn the definition of one wort!
w i t h  the Yi's from the d e f i n i t i o n  ofU1Q other ira not empty.To get a more i n t u i t i v e  understanding of this process, suppose, again, t ha ta data base contains ent r ies  for both secretaries and clerks w i t h  salariesfox each.
Suppose "Suzi&' is an instance of a secretary and  om" is aninstance of a clerk.
We then have three words defined as follms:Suzie ( (SUZIE SECY) )Torn ( (TOM C-LK) )Salary ( ( sECY SECSAL) (CLK CLKSAL) )Processing me phrase "Suzie ' s salary" would i n t e r s e c t  the Y i  ( "  (SECY) " )from t h e  def in i t ion  of "Suzie" w i t h  t h e  Xi's ("SECY" and "CLK") from t h edefinition of "salary."
The intersection is nan-empty ("(SECY)") , and, i ndiscovering the semantic relationship the sense "SECSALI-' is assigned t o  theword "salary."
Similarly, "Tan's salary" assigns the sense "CLKSAL" t o"salary.
!IA particular bplmentation of the natural-language interface processoroperates for a par t i cu la r  DMS/data-base t a r g e t  system.
It contains aparticular &&ion- created for t h a t  t a r g e t  system.
For a par t icu lar  dic-tionary, the s e t  of a21 l is ts  05 pa i r s  as described above, therefore,consti tutes the equivalent of a ~ a n c c p t  q ~ a p h  ox network for the part iculardata b a a  malogous to those U R Q ~  hy many of the  more conventj-onall, parsersPox semantic analysis folluwing (or during) the syntactic phase of parsing.In the analysis of a particular input by our system, two words i n  contextare t e ~ t e d  using t h e  "intersection" method described abave and, if they arefound to be semantically r e l a t e d ,  they are considered candidates fo r"connection" as descrrLbed below.
Two words so connected ?
o m  a phrase.Function words are defined as operators or processors t h a t  perform thissemantic test .
The defini t ion of one function word  dif fers  fm that ofanother according to its slope (see belaw) and also in that  t h e  operationaldefinition of a function word can reject a connection even though t h e  twowords may be samntically related.
In the operational def in i t ion  of t h efunction word may be a list of acceptable concepts or a rejection list ofunacceptable concepts.
In most conceivable data bases, the phrase "salaryin the secretary" would be thus rejected by the  function word "in.
nAs the analysis of an input  expression proceeds, a "clumpifig" of word andphr as e meanings more and more explicitly normally,processing of the entire sentence r e s u l t s  in a tree structure  made up of theconnected senses of a l l  the content words fran the sentence.
This  result weterm the sentence qraph even though the input expression may not be agrammatically cmplete sentence.
This sentence graph will be t ransla tedin to  statement.We recognize t ha t  the linear ordering of the words in an input expressionis not entirely randm and t h a t  certain aspects of me function of syntaxmust be taken into accorunt.
This is done by means of a new and pwerfulazgorithm b k d  on what we cal l  the syntactic-semantic slope.
Linguistsgenerally recognize that whenever two units of meaning are combined, one issemantically domfnant and t h e  other subordinate, as a modifier is sub-ordinate to the modified word.
A f t e r  coenbinatfon, the d d n a n t  word may bewed in m o s t  cases to refar to the canjoined pair.
Thus, a "red herring"18 a "herring" (not a "red") , and the "salary of  t h e  secretary" is a"salary."
If this relationship of dominance i s  represented vertically on altrectangular graph (i.e., dominance on the Y-axis),  and if t&e l i n e a r  order-ing of the words in the expression is represented on the X-axis in n o w 1left---right: order, then the connection of an adjacent pair of contentwords or phrases will describe a linear slope on the graph.
The slope ispositive eir negative as the dominating sub-unit is, respectively, to t h eright or to the left  of the subordinate sub-unit.
For example, the phrase"red herring" makes a positive slope, thus:HERRING/REDand "the salary of the secre=" makes a negative slope:S;71LARYThus, the ~ p e r a ~ o n a l  meanings of fqnctian words operate on the meanings ofnearby content words.
Dominance is assigned, semantic relationships areverified, and the relationships so discovered are accepted or rejected.
Ifaccepted, the two word-meanings are connected, and the acceptable sense isassigned to the  dumllnant word.Eunction words may connect content words in "positive," "negative ," or"peak" connections.
me follming are examples of each mannax of connection:1.
"Of" is a negative operator, as in " the  salary of theSALARY2. "
' 8 "  is a positive operator, as in "the secretary 's  salary":3.
"And" is a peak operator, as in "Atlantic and Pacific.  "
Incontrast w i t h  positive and negative operators, peak operators adda representation of their m semantics i n t o  the structures theybuild ;AND\A-IC PACIFIC4.
Between any two adjacent content words there is an implicit "empty"operator t h a t  is a positive operator, as in "red herring":REDIn general, all prepositions are defined as negative operators.
This isequivalent Go the ruleused by syntactic processors.
The positive empty operator is equivalent tothe ruleN P + A x x r P 3 Pand athew, while vexbe and conjunctions are  defined as peak operators,giving our atatemcnt o f  rules such errss+NPvE'NPMP + NP CONJ NP.Each operator has the faci l i ty  to accept or reject  any semantic rejlationaccordin9 to the precise def in i t ion  of the function word for the host  datamanagement system.Progressive connection of word meanings and previously connected groups or"phrase meanings" results in a tree graph t h a t  we ca l l  the sentence qraph.For example, the question "What is ;t;he surface displacement of U S .
dieselsubmarines?"
could, f o r  a particular data base, produce from the dictionarya string of content-word and funeion-word definitions that might be rep-resented typographically l i k e  this:( (SUB SURE-DISC) ) <OF> ( (U .
S. LOC) ( (DIESEL TYPE) ) ( (LOC SUBS)(TYPE SUBS)As a xesult of processing, these will assemble into a tree structured (usingthe senseg of the words) l i k e  this:WHAT/ sUm-D=sPLOC AsuBs TYPEU , S .
DIESELEven though this tree,  or  sentence graph, i s  created as a result o f  semanticrelationships instead of Eonnal r u l e s  of grammar, it still.
closely resemblesthe "parse t ree"  produced by m o ~ t  conventional syntactic language processors.With respect t o  the user's target data management system, t h e  sentence graphis preci~e and unambiguous and contains enough information for astraightforward translation into the formal query language of the EMS.
InSDCrs DS/3 lanwage, f o r  example, the above question would be expressed asPRINT SURF-DISP WHERE TYPE EQ DIESEL AND lXXl EQ U.S.The response to the usex's question will thus be the response frclrn h i s  DMSt o  the formal query statement.The user's input in this hypothetical example i s  proper i n  fom and grammar.However ,  it need not have been.
The requestOBTAIN SURFACE DISP FOR US SUBS SUCH AS HAS TYPE EQ DIE=.would produce exactly the same sentence graph and thexefore, exactly t h esame f o m l  query statement with the same response f r o m  the DMS.It is not l ike ly  t ha t  a syntax-based parser would have anticipated the  oddlaxxguage-use and grammar of this last request.
Without a syntax rule t h a twould alluw for the phrase "such as has" such a parser would not look at thesemantics involved and would be unable t o  interpret the request.
Our syntaxalgorithm gets the same results that would be expected f m m  the applicationof syntax rules without the need t o  anticipate each grammatical constructexpected from the user.In overview, the  parsing algorithm makes a series of positive, negative, andpeak connections based on the operational meanings of the function wards(including the "empty" aperator) and on the relations between meanings of thecontent wort%?.
The algoridt-Xlm adheres to the following rules:e 1 Connections between content words are possible only ifthe result of the intez'sectfon t e s t  described & m e  is non-emptyand i f  this result i s  not rejected by the operation of the functionword p e r f o d n g  t h e  test.
The function word d e f i n i t i o n  also deter-m i n e s  which w o r d  supplies its X ' s  and which its Y's for the t e s t ,It  thus controls which w o r d  has its sense d e t e d n e d  if t h e  t e s tia successful.
Most of ten (though there are exceptions) , posit iveoperators use the X's f r o m  the  w o r d  to the r i g h t  and the  Y ' s  fromthe word to the left of .
b e  operator.
Positive operators, these-fore, determine the sense of the word t o  the  right.
This isi l lustrated using, again, the secretaxy and her salary, Considerthe defini t ion of "Suzie" and "salary" as shown on page 5 ,  Thephrase "Suzie's salazy" has two content w o r d s ,  "Suzie" and"salary, " separated by the function word , " s , " This functionword is  a positive operator and, hence, applies the  intersectiont e s t  t o  the X i  from the definition of "salary" w i t h  the  Yi fromthe definition of " ~ u z i e . "
These values are, xespactively,'I (SECY CLK) " and " (km) . "
The intersection yields " (SECY) , "which is acceptable to the " ' s "  operator, and the connection ismade with "salary" as the dominant word.
The sense of "salary"is the Y i  associated with "SECY" in t h e  def in i t ion  of "salary,"hence, "SECSAL."
T h i s  selection process is reversed f o r  negativeaperators, while peak operators employ both kinds of t e s t s ,  oneon each s i d e  of the peak.Rule 2: N o  node i n  a sentence graph may have m o r e  .than one dominatingnode.
That is to say, a l l  connections m u s t  r e s u l t  i n  trees, ThisI s  a canmon asswnptLon consistent with conventional syntax-drivenparsers.Rule 3: Given a subtree, a const i tuent  on its left has the poss ib i l i tyof conneation only to nodes of the subtree's positive adjacentslope, and a const i tuent  on the r i g h t  can connect onLy t o  the nodesi n  the adjacent negative slope.
In tu i t ive ly ,  this means that ifthe nodes of a subtree are connected by "lines" that are "opaqueb a r i e r s r n  then a constituent on either side of t h e  subtree  mayconnect to it only on those nodes that it can rlsee.r' I t  may notconnect t o  nodes on the "inside" or the "fax s ide" of the subtree.This i s  a powerful h e u r i s t i c  rule that eliminates t h e  need t o  t ryconnections to many syntactically impossible portions of the  sub-tree.
In effect this one rule, together w i t h  the definitions ofthe function words, replaces all the syntax rules used by mostconventional parsers.Rule  4: In  order t o  minimize disconnection of existing subtreestructures (badcup) and s t i l l  consider a l l  possible connections,the system should, whenever possible, constrztct,subtrees s t a r t i n gfrom the top and make new connections from belaw.
This rule leadsto the following algorithm: Scan the consUtuents from left t oright making negative connections, then scan from right to leftmaking positive connections.
S c a n  thus back and forth unti l  nomore connections can be made.
Then make any poasible peak aonnec-t ions  and repeat the algorithm.
Continue t h i s  process u n t i l  a l lconst i tuents  have been connected i n t o  a single tree,We have observed t h a t  if ambiguities exist under these conditions, they w i l lbe semantic and, in all probability.
not resolvable by any further processingor analysis of the expression.
Therefore.
there is no need to carry alongtemporary multiple construction poss ibi l i t ies ,  The algorithm may eirherquery the  user at this point for disambiguation or W d w t  the pxocesging andinf o m  reason,I.
Chapanis, Alphonse.
Interactive human cammunlcation, ScientificAmerican, May, 1975.2.
Woods, W. A, Trahsition network gr-ars for natural language analysis.Cozmnunications of the ACM, October 13, 1970,3.
Kellogg, C. H,, et al,  The CONVEXGE natural language data managementsystem: current status and plans.
ACM Sym~osium on Information Storaqeand Ratrieval, University of Maryland, 1971.4, Thompson, F, B .
;  'Lockman, P. C.; Dostert, B.; Deverill, R, S. REL:a rapidly extensible language.
Proceedings of 24th National Conference,ACM, New York, 1969, 399-417,-Riesbeck, C, K. Computational understanding.
Theoretical Issues i nNatural Langu~ge Processinq: Proceedinqs of an InterdisciplinaryWorkshop in Canputat icmal ~inguist&cs, Psychology, Linguistics andArtificial Intelligence.
Cambridge, Massachuastts, June 10-13, l975.6, Waltz, D. L. On understanding poetry, Theoretical Issues i n  NaturalLangtmgs Processing, Proceedings of an Interdisciplinary Workshop inCamputational Linguistics, Psychology, Limuistics and ~rtificialIntelligence.
Cambridge, Massachuset-, June 10-13, 1975,7 .
Sdhank, Roger, and Tesler, L. G. A Conceptual Parser for N a t u r a l .Language.
Stanford Artificial InteUigence Project.
Memo No.
AI-76,Januaq, 1969.American Journal of Computational Linguis ties Microfiche 32 : 7 2P.
MEDEMA, W .
J. BRONNENBERG, H. C. BUNT.
5.
P.  J.  LANDSBERGEN,R ,  J. H. SCHA, W .
J. SCHOENMAKERS, AND E .
P .
c. V A N  UTTERENPhilips Research  L a b o r a t o r i e sE indhoven ,  The NetherlandsABSTRACTThis paper outlinee a recently implemented que~tion answering system , calledPHLIQA 1 , which answers English questions about a data base .Unlike other existing aysteme , that directly tramlate a syntactic deep structureinto a program to be executed, PHLIQA 1 leads a question through severalintermediate etages of semantic analysis .
In every stage the question is repre-sented a0 an expression of a formal language, The paper describes aome featuresof the Languages that are &uc~essivelg used during the analyeis process : theEnglish-oriented Formal Language , the World Model Language and the Data BaseLanguage .
Next ,  we ahow the separate conversion steps that can be distinguishedin the process.
We indicate the problems that are handled by these conversions ,and that are often neglected in other systems.1.
IntroductionPHLIQA 1 is an experimental ~ y e t e m  for answering isolated English questionsabout a data base .
We have singled this out as the central problem of queationanawerlng , and therefore postponed the treatment of declaratives and imperrttives , as well aa the analyak of discourse untll a later vereion of the system .The data baee is about computer installations in Europe and their users .
Atthe moment, it is small and resides in core- but its structure and contentare those of a realistic Codagyl format data base on disk ( CODASYL DataBase Task Group [ 1971 'J )Only one module of the system , the wevaluation componenVT , would have to bechmqpd in order to handle a lha l t f  data base .2, PELIQA 1 ' e top level designLike other recent QA systems ( e,g, Petrick 1 1973 ] , Plath 1 1973 ] ,Winograd 1 1972 ] , Woo& [ 1972 ] ) , the PHLIQA 1 system can , on themost global level , be divided into 3 parts ( aee fig.
1 ) :-- Underetandtng the question : Translating the question into a formal expree-sion which represents its meaning with respect to the world model of the- Computing the answer : Elaborating this expreseion , thereby finding theanswer, it is repreeented in the system' s internal formalism.-- Formulating the answer : Translating this answer into a form that can bemore readily under8 toad .questlon in EnglishIformal expression , representingthe meaning of the questionIAnswerComputationIanswer In internal formatAnswerFormulationanswer in external formatFig .
1.
Global subdivision of PHLIQA 1,The interface between the Question understanding component and the AnswerComputation component 1s a formal language , called the World Model Language( WML) .
Expressions of this language represent the meaning of questions withrespect to the world model of th@ system.
Its conrrtants correspond to the conceptsthat canstitute the universe of discourse .
The language is independent of the inputlanguage that ie udled ( in this case English) , and also independent of the storagestructure of the data base.If we now look at a further subdivierion of the component& , the difference betweenPHLIQA 1 and other systems becornea apparent .
Both above and below the WorldModel level, there is an intermediate stage of analysis , characterized by aformal language , resp r- The Engliaboriented Formal Language ( EFL) , which containa  constant^ thatcorrespond to the terms of English, This language is wed to represent thesemantic deep structure of the question , That divides the Question U n d e ~standing component into two succes~ive subcomponents Ia.
Constructing an EFL expression .
using only linguistic knowledge .b, Translating the EFL expression into a WML expression, by takingknowledge about the structuf.e of the world into account.- The Data Base Language ( DBL ) , which contains conatants that correspondto data base primitives .
( The World Model constants do not correspond todaW base primitives , because we want to handle a realfs tic " data base :one that was designed to be stored efficiently , rather than to reflect neatly thestructure of the world .
)This splits the Answer Computation component into two successive subcomp*nenta :a. Translating a WML expression into a DBL expression taking knowledgeabut  the data base structure into account,b.
Evaluating the DBL expre~sion .The aebup of the system that one arrives at in this way,  is shown in fig, 2.In section 3 , we gay eamething more about PHLIQAq s formal languagqs ingeneral .
How the three succeesive translation modules are further divided intosmaller modules , c a U d  ftconvertorsw , is dfscu~sed fn the sections 4 , 5 and 6,Section 7 treats the evaluation component .
The Answer Formulation componentis very primitive , and will not be considered further .question in EnglishIQuestionUnder0 tandingAnswerComputationexpreabion of Englisboriented Formal Langua$teI ( Semantic Deep Structure )EFL- WML - - -  - - owledge oftsanslation - - - - -  World Structureexpre $ sion of World Model LanguageI- WML- DBL - - t - -translation f - - -[ expredsion of Data Base Language 1Ianswer in internal formatFormulationanrswer in external formatFie 2, PHLIQA 1 main components .3.
PHLIQA 1' B formal laxlguages3.
1, sylitaxThe three PHLIQA languages ( the English-oriented Formal Language , theWorld Model Language and the Data Base Language) have largely identfcalsyntactic definitions .
A s  pointed out already, their moat important differenceis in the constants they contain .
T h y  share most , but not all , syntacticCOIlJ3 t~C!tf~Ils  ,PHLIQA expresgions are rt trees TT that conaists of terminal nodes ( conetantsand variables) and syntactic constructions .
A syntact'ic construction is anunordered collection of labeled branches , departing from one node .The branches of a PHLIQA fl tree " can converge to a common subtree .Using a system of semantic types , the syntax of a PHLIQA language defineshow expressions c m  be combined to form a larger expressfan.
For everysyntactic conetruetion, there ie a rule which specffies :- What the semantic types of it8 Immediate sub-expressions are allowed to be .
( There is never a restriction on the syntactic form of the sub-expressions , )- How the semantic type of the remitting expression is derived from thesemantic types of the immediate sub-expressions .Given the types of the elementary expressions ( the constants and variables ) ,this def'lnes the language, ( Sources of inspiration f o r  the syntax of our formallanguages were the Vienna Definition Language- ( Wegner [ 1972 ] ) , and aformulation of Higher - Order Lo@c by J.A.
Robinson [ 1969 ].
)Some ~imple xamples of semantic types are the foXlowing :A comtant reprersenting a single object has a simple type .
E.g, , 6 hasthe type " integer " , A c6nstant representing a collection of objedta of type ochas a type of the form <d> .
E,g.
, companies has the type "(company)" intagera has the type "(integer) .A constant representing a function that can have arguments of type andvalues of type ('3 has the type + .
E.g.
, the functionTt IL-cornpany-sites TI has the type ??
company* &il%y: the function &sum "has the type t v  (integer) integerw.The syntactic rule for the construction function - application t' could statethat the emreasionis well -- formed if T is a well-formed expre~lsion of type and T i s  a 2 1well - formed expression of type 6 -+ /3 , where oC and may beany type ; the whole expression then has the type PThe PHLIQA languages contaln a wide variety of syntactic constructions , e,g.constructions for different kinds of quantification , for selecting elements froma list, for reordering a list, etc ,3.
2, SemanticsThe PaIQA language8 have a formal semantics which recursively defines thevalues of the expressions, This definition assumes as primitive nations thedenotatian~ of the conetants of the language : function - constants denoteprocedures , and the other canstants denoh value - expressions , This meansthat if we know the denotations of the constants occurring in an expreesion , thevalue of the expression fs defined by the semantic rules of the language , Fort b  Data Base Language , we indeed know the denotations of the constants ; whatwe call the data base is nothing but the implementation of the " primitiveprocedure8 ", t e. : the procedures corresponding to DBL functions , andthe procedures for finding the value - expres~ions of the other DBL constants .Therefore , the DBL expressione are actually evaluable .For  the World Model Language and the English-orientad Formal Language , sucha data base does not exiat , but one could be imagined .
We express thls by sayingt4&t the WML and EFL expressions are * evaluable with respect to a virtual database4, Constraction of the semantic deep structure of a question.A s  we have seen, the EnglfsMriented Formal L m a g e  differ8 from the othertfttu, languagee in two respect8 :1, It has different constants , of'whieh the most important are ta names of sets corresponding to noune ( e.g.
* computers ") , to verbs( " buy - sitrtatiane * ) and to ssme of the prepoeitions( in - place - situations ) .b.
grammatical functions t subject, object, etc .2, It Borne different constructione .
Here the most striking difference is thatEFL conekuctinns contain eemantic and syntactic featurea .
The semanticfeatures influence the formal semagtfca of the constructlorn ( e,g, the definite-nees or indefiniteness of a noun phrase influences the choice of the kfnd ofquantification for that noun phrase ) .
The syntactic features only play a roleduring the tranaiormatian process from English to EFL .T t  should be noted that Ln general two eynonymoue eenteqes need not be representedby tho same semantic deep structure in EFL .
For example , the synonymy ofA buys B from C and C sells B to A is not accounted for at tbia level .Hwever ,at the level of the World Model Language synonymous sentences aremapped onto equivalent ( not necesaarilg identical ) WML emrerssr iom .The construction of the semantic deep structure in EFL consists of three mainphanes rphase 1: a lexicon , providing for each word one o r  more interpretations ,represented by pairs ( CATi, SEM \ , where CAT I s  a syntactic category i iand SEM an EFL expression .
iphase 2: a set of rules that enables to combine the sequence of pairs ( CAT SEM1) , i tcorresponding to the original sequence of words , into higher level categories andmore complex structures , until we have ultimately the pair ( SENTENCE , SEM ) ,Swhere SEM is the EFL expression for the bomplete sentence .SA rule of phase 2 is a combination of a context free rule and a set of rules on EFLexpressions , that show when and how a sequence of pairscan be reduced fo a pair ( CAT , SEMR) .
RThe  general  format of theae rules i s  :- context free reduction rule :........ CATl +.
+ CATk -> CAT R- EFL rules :The C O N D ~ ' s  are conditions on the EFL expressions SEM .
.
, , , 1' SEMk .The ACTION ' s ahow how a new EFL expression SEM can be constructed with the i RhelpofSEM .....
I' SEMk .
The rule i s  applicable if at least one of theconditions COND is true .
Then SEM ia constructed according to ACTION and I a ithe aequence of pairs i s  reduced to ( CAT SEM ) .
If more than one of theR' RCOND is true , we have a local ambiguity.
iphase 3: transformation rules that transform the semantic surface structure intoan EFL expression that I s  called the semantic deep structure .
~ h e e e  t r & m f ~ rmation rules handle aspecte of meaning that could not be resolved locally , duringphase 2.
This applies for Instance to anaphoric references and elllptic clausesin comparative cons-ctlons .A ~impler example is the specification of the subject in a clauae like ' to uee acomputer ', The eemantic surface structure of this clause means: there is ausesituation , with ~ a m e  computer as its object , and an unspecified subject .Phase 2 can be said to ' disambiguate ' thi@ expression in a context like' when did Shell start to q e  a computer 3 .A transformation specifies the subject of the use-situation as Shell '.
Thistransformation would not apply if we had the verb propose instead of start ' .The condition8 of phase 2 and phase 3 contain a rkhortcuV' to the world model1the semantic types of the world model interpretations of the EFL congtants areinspected in order to avoid the construction of semantic deep e tructures thathave no interpretation in the world model .
This blocks many unfruitful parsingpaths.5 .
Translation from semantic deep structure to unambiguous World ModelLanguage expressionThe translation from a semantic deep structure ( EFL expraseion ) into an un-arnbiguoua World Model Language expmsarion proceeds in 3 phases1phase 1s Translation from EFL expression Into ambiguous WML expression.b tbls phase , traneformations are applied which replace expressions containingEFL conetants by expreiseiolu containing WML canatants .
Their most conspipuow effect is the elimination of "situations" and rTgrarnrnatical functionst1.
It isimportant to note that the resulting expreseion often contains several "ambig-uous constantsW, These ariae from polyeemous brms in English r words thathave a "range1?
of posaible meanings .
Such terms lead now to expressions withambiguous constants8 constants that stand for a whole class of possible "insta*cesT' .
An expression containing such constants , stands for the class of wellrformed expressions that can be generated by 'Ymtantlating" the ambiguous c o wstants .phase 2% Disambiguation of  quantification^ .Many sentences are ambiguous with respect to quantification ,E .g .
Were the largest 3 computers bought by 2 French companies ?
can eitherask whether there are 2 French companies such that they both bought each ofthese computers , o r ,  perhaps more plausibly , it can ask whether there are 2French companies such that together they bought these computers .Until thie stage in the process , the representation of such questions containsconstructions which stand for both interpretatiow at once .
But now that thesystem' 8 assumptions about the structure s f  the world are reflected In the ex-pression, some such interpretations may be ruled out as implausible , becausethey would lead to the same answer , independent of what the atate of affairs inthe world is  .
E ,g ., the first interpretation of the above example questionhas the value 'YalseW , independently of the values of the constants in the ex-preaeion .
( Because the assumption that a computer can only be bought by onecompany wapJ Introduced by a previous traneformatfon ) .
Therefore , the secondinterpretation is chosen,phase 32 Di~arnbiguation of WML conestants .The ambiguous WML constants can be instantiated in a very efficient manner byusing the semantic type system: The possible interpretations of an ambiguouscomtant are severely restricted by the semantic types of the other constantsthat appear in it8 context,6.
Tramlation from World Model L a n w g e  expression to Data BaseLaqpage expression-In the World Model Language , constants correspond to the concepts of the universeof discourse, In the Data Base Language, conatants correspond to primitivelogical and arithmetical procedures and to primitives of the data base .
The choiceof these primitives was governed by coneiderations of efficiency, rather than bythe wish to represent neatly the structure of the univeree of discourse.
Therefore ,WML and DB conb fn different conatants .The translation from a WML expression to the DBL expression that will be evalu-ated, proceeb in three stages :1, Paraphrase of the WML expression, in order to eliminate * infinite notions ".WML contains conrrtanb representing infinite sets or infinite continua , likeinteger8 * , * moaey~amounts and ?'
time ' l .
Such comtants can not bedirectly or hidirectly represented in the data base , and hence have no D B btramlation.
By paraphrasing the expression, the infinite notions can of*nbe elirntnated .2, Translation of expressions conklning WML constants into expressions con-&ining DBL cow tanh ,This tranalatlon is required by phenomena like the following :- it Ls poasible that a class of objects is not represented explicitly in the databaee , while propertlee of ib elementa are represented indirectly,  asproperties of other , related objects , ( E.g.
, cities do not occur in thePHLIC&Il data base , but their names are represented as the ciwnarnesof sites .
)A special case of this phenomenon ie the representation of a continuum by aclass of diacrete objects ( E.g.
, core ie represented by rr corememories ") t-- objects may be represented more than once in the data base.
E.g.
, in thePHLIQA 1 database, the flle of computer users and the file of manufacturerscan contain records that represent one and the same f i rm.-- the data baee is more limited than the world model .
Some questions thatcan be expreased in WML can be answered only partially or  not a t  all rthe WML expresrition has no DBL translation.
The present convertor detectssuch expressions and can generate a message which specifies what informa-tion ia lacking .Examples of this caae are r the se t  '' integers '* ( if the attempt of the previousconvertor to eliminate it has been umuccesr~ful ) , and the date-ottaking-o u t - - o w e  ?
* of a computer ( which happens to be not in the data base ) .3.
Paraphrase of the DBL exprenr~ion , in order to improve the efficiency of itsevaluation .The DBL expression produced by the previous convertor can already be evalu-ated, but i t  may be possible to paraphrase it in such a way, that the evaluaii~nof the paraphrase expression is more efficient, This conversion is worthwhilebecause , even with our small data base , the evaluation is often the mosttime-consuming part of the whole process ; compared to thie , the time thattransformations take is negligible .7.
The evaluation of a Data Base Language expressionThe value of a Data Base Language expression is completely defined by the sernaxl-tic rules of the Data Base Language ( see section 3 .
2 . )
, and one could cohceiveof an algorithm that corresponds exactly to these rules .
For reasons of efficiency,the actual algorithm differs from such an qlgorithm in some major respects r- in evaluating quantlficatiom over sets , it does not evaluate more element0 ofthe sat than ie necessary for determining the value of the quantification .- if ( e-g. during the evaluation of a quantification) , a variable assumes a newvalue , this doe8 not cause the, re-evaluation of any subexpressions that don* tcontain this variable .Currently , evaluation occurs with respeet to a small data base in Core , To handlea real data base on dierk , only the evaluation of constantn would have to change .8, PELIQA I ' s Control Smckrrc3The sections 4 thmugh 7 sketched what the basic modulea of the system ( theconvertors ") do .
W e  shall now make some very general rernarh about theway they were implemented .
These r e m a r k  apply to all convertors except theparser, whioh is described in some detail by Medema [ 1975 ] .The convertors can be viewed as functiong which map an input expression into a setof zero or more output expressions .
Such a function fa defined by a collection, oftransformations , acting on subexpresslons of the input expression .
Each tr&aa-formation wnrrists of a condition and an action , The action ie applied to a sub-expression if the condition holde for it .
The action can either be a proceduretransformfngra subexpression to its * lower level equivalent '' or it can be thedecbian this subexpressfon cannot be translated to the next lower level '' ,"I1 convertore are implemented as procedures which operate on the tree thatrepregents the whole f~uestion .
The procedures cooperate in a " deptb-first ?
'm m r  : a conversion procedure finds suc~es s ive ly  all interpretations that the inputexpression haa on the next lower level .
Far each of theae Interpretations , as soonas it is found, the next convertbr ie called.
If no interpretation can be found, amessage Bving the reason for this dead end is buffered , and control fe returnedto the calling convertor ,If the answer fs found, it is displayed.
If requested, the ayatem can continue itssearch for more interpretatlorn .
If the answer level is not reached , it displaysthe buffered message from the " lowest " convertor that was reached ,ColophonThe PHLIQA 1 program was written in SPL ( a PL/1 dialect) , and runs under theMDS time sharing system on the Philips Pl.400 computer of the Philips ResearchLaboratories a t  Eindhoven .The quantfflcatio~i~lambiguation ghaae of the EFG-WML translation, the effi-ciency-conVersion ( step 3 ) in the WML-DBL translation , as well a s  some partsof the grammar , are  not yet part  of the running system , though the convertorsare complekly coded and the grammar is elaborately specified.During the design of PHLIQA 1 , the PHLIQA project was coordinated by PietMedema .
He and Eric van Utteren deaigned the algorithmic structure of the aye-tern and made decisions about many general aspectxi of implsrnentatlon .The formal languages and related transformation rules were designed by HarryBunt .
Jan Landabergen and Remko Scha .
Wijnand Schoenmakera deaigned the evalu-ation component.
Jan Landsbergen wrote a grammar for an extensive subset of EnglishA l l  author6 were involved in the implementation of the system .During the design of PHLIQA 1 , exteneiva discussione with members of the SRISpeech Understanding team have helped us in making our ideasl more explicit,ReferencesCODASYL Data Base Task GroupApril 71 report.
A C M ,  New York, 1971 .P.
Medema A control structure for a question answering sys  tern .Proceedings of the 4th Inte~national Joint C~nferen~ce onArtificial Intelligence .
Tbilisi , USSR , 1975.
Vol.
2 .S,RPetrick SemanticInterpretaticmintheREQUESTsystem.Proceedings of the International Conference on ComputationalLinguistice , VoL 1 , Pisa , 1973 .W, J. Plath Transformational Grammar and Transformational Pars fng inthe REQUEST system,Proceedings of the International.
Conference on ComputationalLinguistics , Vol.
2 , Pisa , 1973 .J.
A. Robinson Mechanizing HighexLQrdelr Logic ,In : B, Meltzer and D. Michie ( eds. )
,Machine Intelligence 4 , Edinburgh University Pres~l , 1969.P.
Wegner The Vienna Definition Language .Computing Surveys , Vol, 4 , no.
1 , 1972 .T, Winograd Understanding Natural Language .Cognitive Psychology , VoL 3 , no.
1 , 1972 ,W. A, Woode , R. M. Kaplan and B. Nash-WebberThe Lunar Sciences Natural Language Information System :Final Report .
BBN , Cambridge , Masa, 1972 .
