POS Tagging Us ing Re laxat ion  Label l ingLlufs Padr6Depar tamenl ;  de L lenguatges  i S i s temes  In fo rmgt icsUn ivers i ta t  Po l i t6cn ica  de CatahmyaPan  Garga l lo ,  5.
08071 Barce lona ,  Spa inpadro~lsi ,  upc.
esAbstractRelaxation labelling is an optimizationtechnique used in many fields to solveconstraint satisfael,ion problems.
The al-gorithm finds a combination of valuesfor a set of variables such that satis-fies -to the maximum possible degree- aset of given constraints.
This paper de-scribes some experiments performed ap-plying it to POS tagging, and the resultsobtained, it also ponders the possibil-ity of applying it to Word Sense Disam-biguation.1 Introduction and MotivationRelaxation is a well-known technique used to solveconsistent labelling problems.
Actually, relax-ation is a family of energy-function-minimizing al-gorithms closely related to Boltzmann machines,gradient step, and Hopfield nets.A consistent labelling problem consists of, giwma set of variables, assigning to each variable a la-bc'l compatible with the labels of the other ones,according to a set of compatibility constraints.Many problems can be stated as a labellingproblem: the travelling salesman problen 4 n-queens, corner and edge recognition, imagesmoothing, etc.In this paper we will try to make a first, insightinto applying relaxation labelling to natural lan-guage processing.
The main idea of the work isthat NLP problems uch as POS tagging or WSDcan be stated as constraint satisfaction problems,thus, they could be addressed with the usual tech-niques of that field, such as relaxation labelling.It seems reasonable to consider POS tagging orWSD as combinatorial problenrs in which we havea set of variables (words in a sentence) a set, ofpossible labels for each one (POS tags or senses),and a set of constraints for these labels.
We mightalso coinbine both problems in only one, and ex-press constraints between the two types of tags,using semantic information to disambiguate POStags and visceversa.
This is not the point; in thispaper, but it will be addressed in fllrther work.2 Relaxation Labell ing AlgorithmRelaxation labelling is a generic name for a familyof iterative algorittuns which perform function op-timization, based (m local infi~rmation.
See (Tor-ras 89) for a clear exposition.Let V = {vl ,  v2 , .
.
.
,  v,,~} be a set of variablesLet t = , ,,,~ } be the set of possilflelabels for variable vi.Let Cb' be a set: of constraints between the la-bels of the variables.
Each constraint C C CSstates a "compatibility value" C,.
ibr a colnbina-lion of pairs variable-label.
Constraints can be ofany order (that is, any number of variables maybe involved in a constraint).The aim of the algorithm is to find a weightedlabelling such that "global consistency" is maxi-mized.
A weighted labelling is a weight assigna-tion for each possibh', label of each variable.
Max-infizing "Global consistency" is defined as maxi-)i )i is the weight mizing ~ j  t j x Sij , Vvi.
Where I jfor label j in wtriable vi and Si j  the support re-ceived by the same combination.
The support fora pair w~riable-label xpresses how compatible isthat pair with the labels of neighbouring variables,according to the constraint set.The relaxation algorithm consists of:?
start in a randoln weighted labelling.?
fbr each variable, compute the "support" thateach label receives froln the current .weightsfor the labels of the other variabh;s.?
Update the weight of each variable label ac-(:ording to the support obtained.?
iterate the process until a convergence crite-rion is met.The support computing and label weight chang-ing must be perfornmd in parallel, to avoid thatchanging the a variable weights would affect t;hesupport colnputation of the others.The algorithm requires a way to compute whichis the support for a wn'iable label given the others877and the constraints.
This is called the "supportfunction".Several support, functions are used in tire liter-ature to define the support received by label j ofvariable i (Sij).Being:1"1 ?
'd R~j = {," I r -- \[(v,,, tk~),..., (~, *}) , .
.
.
,  (v,.,, t.k,,)\]tile set of constraints on label j for variable i,i.e.
the constraints formed by any coinbination ofpairs variable-label that includes the pair (vi, t}).r l  l)k, (m) the weight assigned to label t~.~ for variablev,,~ at time m.TO(V) the set of all possible subsets of variables inV.R~ (for G E T?
(V)) the set of constraints on tagi ieor word j in which the involved variables areexactly those of G.Usual support flnmtions are based on coinput-ing, for each constraint r involving (vi,t}), tile"constraint influence", I n f ( r )  = C,.
x p~'(m) x.. .
x p~Z., (m), which is the product of tile currentweights for the labels appearing the constraintexcept (vi,t}) (representing how applicable is tileconstraint in the current context) multiplied by C.,.which is the constraint compatibility value (stat-ing how compatible is the pair with the context).The first formula combines influences justadding them:(1.1) Sij = ~ In f ( r )rGR i jThe next fornmla adds the constraint influencesgrouped according to the variables they involve,then multiplies the results of each group to getthe final value:(1.2) &- -  11The last formula is tile same than the previousone, but instead of adding the constraint influ-ences in the same group, just picks tile maximum.
(1.3) Sij = I I  max { In f ( r )}The algorithm also needs art "updating func-tion" to compute at each iteration which is tilenew weight for a variable label, arrd this compu-tation must be done in such a way that it can beproven to meet a certain convergence criterkm, atleast under appropriate conditions 1Several formulas have been proposed and someof them have been proven to be approximations ofa gradient step algorithin.Usual updating flmctions are the following.~Convergence has been proven under certain con-ditions, but in a complex application such as POSgagging we will lind cases where it is not necessarilyachieved.
Alternative stopping criterions will requirefurther attention.Tile first formula increases weights for labelswith support greater than 1, and decreases thosewith support smaller than 1.
The denonfinatorexpression is a normalization factor.
(2.1) p}(m + 1) = ~;~ where S,ij > 0ik IThe second formula increases weight for labelswith support greater than 0 and decreases weight,for those with support smaller than 0.~ (~,,) x (1 + &j)  (2.2) + 1) =k=lwhere- l<S i j  <_ +1Advantages of the algorithm are:?
Its irighly local character (only the stateat, previous time step is needed to computeeach new weight).
This makes the algorithmhighly parallelizable.?
Its expressivity, since we state the problem interms of constraints between labels.?
Its flexibility, we don't have to check absolutecoherence of constraints.?
Its robustness, sin(:(,' it can give an answer toproblenls without an exact solution (incom-patible constraints, insufficient data...)?
Its ability to find local-optima solutions toNP problems in a non-exponential time.
(Only if we have an upper bound for the nun>ber of iterations, i.e.
convergence is fast, orthe algorithm is stopped after a fixed numberof iterations.
See section 4 for further details)Drawbacks of tire algorithm are:?
Its cost.
Being n the number of variables,v the average number of possible labels pervariable, c the average number of constraintsper label, and I tire average number of iter-ations until convergence, tile average cost isn x v x c x i, an expression in which the inulgi~plying terms ,night; be much bigger than n ifwe deal with probh',ms with many values andconstraints, or if convergence is not quicklyachieved.?
Since it acts as an approximation of gradi-ent step algorithms, it has similar weakness:Found optima are local, and convergence isnot always guaranteed.?
In ge, ne, ral, constraints must be written mann-ally, since they at(', the modelling of the prob-lem.
This is good for easily modelable orreduced constraint-set problems, but in thecase of POS tagging or WSD constraints aretoo many and too complicated l;o be writtenby hand.8 '7 8?
The diificulty to state which is the "(:omt)at-ibility value" for each constraint.?
The, difficulty to choose the support and up-dating fun('tions more suitable for ea(:h l)ar-t itular prol)lem.3 App l i ca t ion  to  POS Tagg ingIn this section we expose our application of relax-ation labelling to assign 1);u't of speech tags to thewords in a sentenc, e.Addressing tagging problems through ot)timiza-tion methods has been done in (Schmid 94) (POStagging using neural networks) and in (Cowie etal.
92) (WSD using sinmlated annealing).
(Pelillo& I{efice 94) use a toy POS tagging l)i'oblenl to ex-t)eriment heir methods to improve the quality ofeoInt)atibility coeflh:ients for the constraints usedby a relaxation labelling algorithm.The model used is l i e  tblh)wing: each word illthe text is a variable and may take several hfl)els,which are its POS tags.Since the  number of variabh~s lind word po-sition will vary from one senten(:e to another,constraints are expressed in relative terms (e.g.\[(vi, Determiner)(v.i , , ,  Adjective)(vi ,2, Nou'r0\]).The Conshnint Setl{elaxation labelling is a.bh~ to deal wil;h con-straints 1)etween any subset of wn'ial)les.Any rehttionship between any subset of wordsand tags may 1)e expressed as constraint and usedl;o feed th(: algorithm.
So, linguisl;s are fre(, to ex-press ;my kind of constraint an(l are not restrictedI:o previously decided patl;erns like in (Brill 92).Constraints for subsets of two and three vari-ables are automati(:ally acquired, and any othersubsets are left, to the linguists' criterion.
That is,we are establishing two classes of constraints: theautoinatically acquired, and the mmmally writ-ten.
This means that we ha.ve a great model flex-ibility: we can choose among a completely handwritten model, where, a linguist has written alll;he constraint;s, a comph~tely mm)mat, ically lie-rived model, or ally interinediate (:olnl)ination of(',onstrailfl;s fl'om ea, ch (;ype.We can use the same information than HMMtaggers to ot)tain automatic (:onstraints: the1)robability 2. of transition fl'om one tag to an-other (bigram -or binary constraint- probabil ity)will give us an idea of how eomt)atible they are inthe posit ions i and i + 1, ;rod the same for l;rigrain-or ternary cbnstraint- probabilities.
Extending~Esl;imated fi'om occurrences in tagged (:ort)or~t.W(: prefer tll(: use of supervis(:d training (sin(:e largeenough corpora arc available) because of the diffi-culty of using an unsut)ervised method (such as Bmm>Welch re-estimation) when dealing, as in our case,with heterogeneous constraints.this to higher order constraints is possil)le, but;would result in prohibitive comtmt;ational costs.l)ealing with han(l-written constraints will notbe so easy, since it; is not obvious \]low to com-pute "transition probabilities" for a comph:x con-stra intAlthough accurate-but  costly- methods to esti-mate comt)al;ibility values have been proposed in(Pelillo & Hetice 94), we will choose a simpler an(tmuch (:heaptw (:Olntmtationally solution: (JOHll)llt-ing the compatibi l ity degree fl)r the manually writ-ten constraints using the number of occurr('neesof the consl;raint pattern in the training (:orIms tocomtmte the prol)ability of the restricted word-tagpair given the contexl; defined by the constraint aII.elaxation doesn't need -as HMMs (h)- the priorprot)at)ility of a certain tag for a word, since it isnot a constraint, but il; Call \])e llSCd to  SOt; theinitial st;at(; to a 11ot templet;ely rall( lol\[I  OllC.
hfi-tially we will assign to each word il;s most I)ro/)abletag, so we start optimization in a biassed point.Alternative Support l,%nctionsThe sut)port functions described in section 2are traditionally used in relaxation algorithnts, itseems better for our purt)ose to choose an addi-tive one, since the multiplicative flm(:tions mightyiehl zero or tiny values when -as in Ollr cose- for ,q(:crtain val'iable or tag no constraints are availablefor a given subsel; of vm'ial)les.Since that fllnt:tions are general, we may try tolind ;~ suI)I)ort f lmctkm more speciiic tbr our t)rol)-h:m. Sin(:e I IMMs lind the maxinmm sequ(:n(:eprobat)ility and relaxation is a maximizing algo-rii;hm, we (:an make relaxation maximize th(,' se-(lllenc(?
t)robability an(l we should gel; tile sameresults.
To a(:hieve this we define a new Sul)portflmc, l;ion, which is the sequence i)robability:Being:t k tile tag for varial)h: 'vk with highest weight valuea~ the current tilne step.7r(Vt, t 1) \[;he probal)ility for t~he sequence to sl;artin tag t I.P(v,t)  the lexical probabil ity for the word repre-se\]tted by v to have t;ag t.T(tl ,  I2) the probabil ity of tag t2 given that I;heprevious one is tl.~itj the set of all ternm'y constrainl;s on tag j forword i.I I  ,q ?
H... the :(:t of all hand-written constraints On (;ag3 k)r word i.We define:= ?
t})?N !k- - l .
, k / iaThis is an issue that will require fitrtl,er ati:en-lion, since as constraints can be expressed in severaldegrees of g(merality, l;he estimated probabilities mayvary greatly del)ending on how t;he constraint wasexpressed.879To obtain the new support function:(3.1)Compatibility ValuesIdentifying compatibility values with transitionprobabilities may be good for n-gram models, butit is dubious whether it can be generalized tohigher degree constraints.
In addition we canquestion the appropriateness of using probabilityvalues to express compatibilities, and try to findanother set of values that fits better our needs.We tried several candidates to represent com-patibility: Mutual Information, Association Ratioand Relative Entropy.This new compatibility measures are not lim-ited to \[0, 1\] as probabilities.
Since relaxation up-dating functions (2.2) and (2.1) need support val-ues to be normalized, we must choose some func-tion to normalize compatibility values.Although the most intuitive and direct scal-ing would be the linear function, we will test aswell some sigmoid-shaped hmctions widely usedin neural networks and in signal theory to scalefree-ranging values in a finite interval.All this possibilities together with all the pos-sibilities of the relaxation algorithm, give a largeamount of combinations and each one of them isa possible tagging algorithm.4 Exper imentsTo this extent, we have presented the relaxationlabelling algorithm family, and stated soine con-siderations to apply them to POS tagging.In this section we will describe the experimentsperformed on applying this technique to our par-tieular problem.Our experiments will consist of tagging a corpuswith all logical combinations of the following pa-rameters: Support function, Updating function,Compatibility values, Normalization function andConstraints degree, which can be binary, ternary,or hand-written constraints, we will experimentwith any combination of them, as well as witha particular combination consisting of a back-offtechnique described below.In order to have a comparison reference we willevaluate the pertbrmance of two tuggers: A blindmost-likely-tag tagger and a HMM tagger (Elwor-thy 93) performing Viterbi algorithm.
The train-ing and test corpora will be the same for all tag-germAll results are given as prec is ion percentagesover ambiguous words.4.1 ResultsWe performed the same experiments on three dif-ferent corpora:Corpus SN (Spanish Novel) train: 15Kw, test:2Kw, tag set size: 70.
This corpus waschosen to test the algorithm in a languagedistinct than English, and because previouswork (Moreno-Torres 94) on it provides uswith a good test bench and with linguist writ-ten constraints.Corpus Sus (Susanne) train: 141Kw, test: 6Kw,tag set, size: 150.
The interest of this corpusis to test the algorithm with a large tag set.Corpus WSJ  (Wall Street Journal)train: 1055Kw, test: 6Kw, tag set size: 45The interest of this corpus is obviously itssize, which gives a good statistical evidencefor automatic onstraints acquisition.Baseline results.Results obtained by the baseline tuggers arefound in table 1.SNMost-likely\[MM 94.62%Sus WSJ86.01% 88.52%93.20% 93.63%Table 1: Results achieved by conventional tuggers.First; row of table 2 shows the best results ob-tained by relaxation when using only binary con-straints (B).
That is, in the same conditions thanHMM taggers.
In this conditions, relaxation onlyperforms better than HMM for the small corpusSN, and tile bigger the corpus is, tile worse resultsrelaxation obtains.Adding hand-written constraints (C).Relaxation can deal with more constraints, sowe added between 30 and 70 hand-written con-straints depending on the corpus.
The constraintswere derived ~malyzing the most frequent errorscommitted by tile HMM tagger, except for SNwhere we adapted the context constraints pro-posed by (Moreno-Torres 94).The constraints do not intend to be a generallanguage model, they cover only some common er-ror cases.
So, experiments with only hand-writtenconstraints are not performed.The compatibility value for these constraints icoinputed from their occurrences in the corpus,and may be positive (compatible) or negative (in-compatible).Second row of table 2 shows the results obtainedwhen using binary plus hand-written constraints.In all corpora results improve when addinghand-written constraints, except in WSJ .
Thisis because the constraints used in this case arefew (about 30) and only cover a few specific er-ror cases (mainly tile distinction past/participlefollowing verbs to have or to be).Using trigram information (T).We have also available ternary constraints, ex-tracted from trigram occurrences.
Results ob-880I _ _~_  S N 19"-5.77%_cJ 96.54%Sus91.65%WSJ~79.34V7/092.50% 89.24%88.6ooof8-97~3 3 ~-89.83%~.y8  0/~0,Table 2: Best relaxation results using every combina-tion of constraint kinds.tained using ternary constraints in combinationwith other kinds of information are shown in rowsT, BT, TC and BTC in table 2.There seem to be two tendencies in this table:First, using trigrmns is only helpflfl in WSJ .This is becmme the training cortms for WSJ  ismuch bigger than in the other cases, and so the tri-grmn model obtained is good, while, for the ()tilerc<)rpora, the training set; seems to t)e too small toprovide a good tr igram iniormation.Secondly, we can observe that there is a generaltendency to "the more information, the better re-suits", that ix, when using BTC we get l)etter re-suits that with B~, which is in turn better thanT alone.Stopping before eonve~yenee.All above results at'(; obtaine.d stopt)ing the re-laxation ;algorithm whim it reaches convergence(no significant cbmges are l)rodu(:ed fl'om one it-eration to the next), but relaxation algorithms notnecessarily give their l)est results at convergence 4,or not always need to achieve convergence to knowwhat the result will be (Zucker et al 81).
So theyare often stoplmd after a few iterations.
Actually,what we arc (loing is changing our convergen('e cri-terion to one more sophisticated than "sto 1) whendlere are no Inore changes".The results l)resented in table 3 are tit(; bestoverall results dmt we wouM obtain if we had acriterion which stopped tit(; iteration f)rocess whenthe result obtained was an optimum.
The numberin parenthesis is the iteration at, which the algo-rithm should be stopped.
Finding such a criterionis ~ point that will require fllrther research.
(12)\] 93.78% (6)Table 3: Best results stopping before conw.~rgence.4This is due to two main reasons: (1)2}t,('.
optimumof tit(*, supI)ort function doesn't correspond ea;actly tothe best solution for the problem, that is, the chosenflmction is only a,n approximation of the desired one.And (2) performing too much iterations can producea more probable solution, which will not necessarilybe the correct one.These results are clearly better than those ob-tained at; relaxation convergence, and they alsooutperform HMM taggers.Searching a more specific support flLnction.We have t)een using support fimctions that aretraditionally used in relaxation, but we might tryto st)ecialize relaxation labelling to POS tagging.Results obtained with this specific sut)t)ort fun(:-tion (3.1) are sumntarize.d in table 4SN SusTable 4: Best results using a specific support fun<:-tkm.Using this new supt)ort fun(:tion we obtain re-suits slightly below those of the I IMM tagger,Our sut)i)ort fun(:tion is tim sequence 1)robal)il-ity, which is what Viterbi maxinfizes, 1)ut we getworse, results.
Tlmrc are two main reasons forthat.
The first one is that relaxation does notmaximize the sui)t)ort; flln('tion but the weigh, tedsupport for each variable, so we' are not doingexactly the same than a HMM tagger.
Secondreason is that relaxation is not an algorithm thatfinds global opt ima an(1 can be trapl)ed in localmaxima.Combining information in a llack-off h, ierarchy.Wh can confl)ine bigram and ti'igranl infi'oma-tion in a. back-off mechanism: Use trigrams ifavailable and bigrmns when not.Results o})tained with that technique at'(', shownin table 5Sus WSJ\[92.31% (3'-~)_ t 93.66% (4)t94.29% (4)\]Table 5: Best; results using ~* back-off' technique.The results he, re point to the same conclusionsthan the use of trigrams: il!
we have a good trigrmnmodel (as in WSJ )  then the back-off" techniqueis usefifl, and we get here the best overall resultfor tiffs corlms.
If the tr igram model ix not sogood, results are not better than the obtained withl)igrams ahme.5 Appl icat ion to Word SenseD isambiguat ionWe can apply the same algorithm to the task ofdisambiguating tile sense of a word in a certaincontext.
All we need is to state tile <',onslxaintsbetween senses of neighbour words.
We can coin-bine this task with POS tagging, since t, here~ arealso constraints between the POS tag of a wordattd its sense, or the sense of a neighbour word.881Preliminary experiments have been performedon SemCor (Miller et al 93).
The problem con-sists in assigning to each word its correct POS tagand the WordNet file code for its right sense.A most-likely algorithm got 62% (over nounsapperaring in WN).
We obtained 78% correct,only adding a constraint stating that the sensechosen for a word must be compatible with itsPOS tag.Next steps should be adding more constraints(either hand written or automatically derived) onword senses to improve performance and taggingeach word with its sense in WordNet instead of itsfile code.6 ConclusionsWe have applied relaxation labelling algorithm tothe task of POS tagging.
Results obtained showthat the algorithm not only can equal markoviantaggers, but also outperform them when givenenough constraints or a good enough model.The main advantages of relaxation over Marko-vian taggers are the following: First of all, relax-ation can deal with more information (constraintsof any degree), secondly, we can decide whetherwe want to use only automatically acquired con-straints, only linguist-written constraints, or anycombination of both, and third, we can tune themodel (,~dding or changing constraints or compat-ibility coefficients).We can state that in all experiments, the re-finement of the model with hand written con-straints led to an improvement in performance.We improved performance adding few constraintswhich were not linguistically motiwtted.
Probablyadding more "linguistic" constraints would yieldmore significant improvements.Several parametrizations for relaxation havebeen tested, and results seem to indicate that:?
support function (1.2) produces clearly worseresults than the others.
Support flmction(1.1) is slightly ahead (1.3).?
using mutual information as compatibilityvalues gives better results.?
waiting for convergence is not a good policy,and so alternative stopping criterions must bestudied.?
the back-off technique, as well as the trigrammodel, requires a really big training corpus.7 Future  workThe experiments reported and the conclusionsstated in this paper seem to provide a solid back-ground for further work.
We intend to follow sev-eral lines of research:?
Applying relaxation to WSD and to WSDp!us POS-tagging.?
Experiment with different stopt)ing criteri-ons.?
Consider automatically extracted constraints(Mhrquez & Rodrlguez 95).?
Investigate alternative ways to computecompatibility degrees for hand-written con-straints.?
Study back-off techniques that take into ac-count all classes and degrees of constraints.?
Experiment stochastic relaxation (Sinmlatedannealing).?
Compare with other optimization or con-straint satisfaction teehlfiques applied toNLP tasks.AcknowledgementsI thank Horacio Rodfguez for his help, supportand valuable comments on this paper.
I also thankKiku Ribas, German Rigau and Pedro Meseguerfor their interesting suggestions.ReferencesBrill, E.; A simple rule-based part-of-speech tag-ger.
ANLP 1992Cowie, J.; Guthrie, J.; Guthrie, L.; Lexical Disam-biguatio'n using Simulated Annealing DARPASpeech and Natural Language; Feb. 1992Elworthy, D.; Part of Speech and Phrasal Tagging.ESPRIT BRA-7315 Acquilex iI, Working Paper10, 1993Mhrquez, L.; Rodrfguez, H.; Towards Learning aConstraint Grammar from Annotated Cool, ovaUsing Decision Trees.
ESPRIT BRA-7315 Ac-quilex II, Working Paper, 1995Miller, G.A.
; Leacock, C.; Tengi, R.; Bunker,R.T.
; A semantic concordance ARPA Wks onHuman Language Technology, 1993Moreno-Torres, I.; A morphological disambigua~tion tool (MDS).
An application to Spanish.
ES-PRIT BRA-7315 Acquilex II, Working Paper24, 1994Pelillo, M.; Refice M.; Learning CompatibilityCoefficients for Relaxation Labeling Processes.IEEE Trans.
on Patt.
An.
& Maeh.
Int.
16, n.9 (1994)Schmid, It.
; Part of Speech lhgging with NeuralNetworks COLING 1994Torras, C.; Relaxation and Neural Learning:Points of Convergence and Divergence.
Jour-nal of Parallel and Distributed Computing 6,pp.217-244 (1989)Zucker, S.W.
; Leclerc, Y.G.
; Mohammed, J.L.
;Continuous Relaxation and local maxima selec-tion: Conditions for equivalence.
IEEE Trans.on Patt.
An.
&Mach.
Int.
3, n. 2 (1981)882
