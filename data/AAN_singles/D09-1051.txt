Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487?495,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPCollocation Extraction Using Monolingual Word Alignment MethodZhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Sheng Li11Harbin Institute of Technology, Harbin, China2Toshiba (China) Research and Development Center, Beijing, China{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cnlisheng@hit.edu.cnAbstractStatistical bilingual word alignment has beenwell studied in the context of machine trans-lation.
This paper adapts the bilingual wordalignment algorithm to monolingual scenarioto extract collocations from monolingual cor-pus.
The monolingual corpus is first repli-cated to generate a parallel corpus, whereeach sentence pair consists of two identicalsentences in the same language.
Then themonolingual word alignment algorithm isemployed to align the potentially collocatedwords in the monolingual sentences.
Finallythe aligned word pairs are ranked accordingto refined alignment probabilities and thosewith higher scores are extracted as colloca-tions.
We conducted experiments using Chi-nese and English corpora individually.
Com-pared with previous approaches, which useassociation measures to extract collocationsfrom the co-occurring word pairs within agiven window, our method achieves higherprecision and recall.
According to humanevaluation in terms of precision, our methodachieves absolute improvements of 27.9% onthe Chinese corpus and 23.6% on the Englishcorpus, respectively.
Especially, we can ex-tract collocations with longer spans, achiev-ing a high precision of 69% on the long-span(>6) Chinese collocations.1 IntroductionCollocation is generally defined as a group ofwords that occur together more often than bychance (McKeown and Radev, 2000).
In this pa-per, a collocation is composed of two words oc-curring as either a consecutive word sequence oran interrupted word sequence in sentences, suchas "by accident" or "take ?
advice".
The collo-cations in this paper include phrasal verbs (e.g.
"put on"), proper nouns (e.g.
"New York"), idi-oms (e.g.
"dry run"), compound nouns (e.g.
"icecream"), correlative conjunctions (e.g.
"either ?or"), and the other commonly used combinationsin following types: verb+noun, adjective+noun,adverb+verb, adverb+adjective and adjec-tive+preposition (e.g.
"break rules", "strong tea","softly whisper", "fully aware", and "fond of").Many studies on collocation extraction arecarried out based on co-occurring frequencies ofthe word pairs in texts (Choueka et al, 1983;Church and Hanks, 1990; Smadja, 1993; Dun-ning, 1993; Pearce, 2002; Evert, 2004).
Theseapproaches use association measures to discovercollocations from the word pairs in a given win-dow.
To avoid explosion, these approaches gen-erally limit the window size to a small number.As a result, long-span collocations can not beextracted1.
In addition, since the word pairs inthe given window are regarded as potential col-locations, lots of false collocations exist.
Al-though these approaches used different associa-tion measures to filter those false collocations,the precision of the extracted collocations is nothigh.
The above problems could be partiallysolved by introducing more resources into collo-cation extraction, such as chunker (Wermter andHahn, 2004), parser (Lin, 1998; Seretan and We-hrli, 2006) and WordNet (Pearce, 2001).This paper proposes a novel monolingualword alignment (MWA) method to extract collo-cation of higher quality and with longer spansonly from monolingual corpus, without usingany additional resources.
The difference betweenMWA and bilingual word alignment (Brown etal., 1993) is that the MWA method works onmonolingual parallel corpus instead of bilingualcorpus used by bilingual word alignment.
The1  Here, "span of collocation" means the distance of twowords in a collocation.
For example, if the span of the col-location (w1, w2) is 6, it means there are 5 words interrupt-ing between w1 and w2 in a sentence.487monolingual corpus is replicated to generate aparallel corpus, where each sentence pair con-sists of two identical sentences in the same lan-guage, instead of a sentence in one language andits translation in another language.
We adapt thebilingual word alignment algorithm to the mono-lingual scenario to align the potentially collo-cated word pairs in the monolingual sentences,with the constraint that a word is not allowed tobe aligned with itself in a sentence.
In addition,we propose a ranking method to finally extractthe collocations from the aligned word pairs.This method assigns scores to the aligned wordpairs by using alignment probabilities multipliedby a factor derived from the exponential functionon the frequencies of the aligned word pairs.
Thepairs with higher scores are selected as colloca-tions.The main contribution of this paper is that thewell studied bilingual statistical word alignmentmethod is successfully adapted to monolingualscenario for collocation extraction.
Comparedwith the previous approaches, which use associa-tion measures to extract collocations, our methodachieves much higher precision and slightlyhigher recall.
The MWA method has the follow-ing three advantages.
First, it explicitly modelsthe co-occurring frequencies and position infor-mation of word pairs, which are integrated into amodel to search for the potentially collocatedword pairs in a sentence.
Second, a new feature,fertility, is employed to model the number ofwords that a word can collocate with in a sen-tence.
Finally, our method can obtain the long-span collocations.
Human evaluations on the ex-tracted Chinese collocations show that 69% ofthe long-span (>6) collocations are correct.
Al-though the previous methods could also extractlong-span collocations by setting the larger win-dow size, the precision is very low.In the remainder of this paper, Section 2 de-scribes the MWA model for collocation extrac-tion.
Section 3 describes the initial experimentalresults.
In Section 4, we propose a method toimprove the MWA models.
Further experimentsare shown in Sections 5 and 6, followed by a dis-cussion in Section 7.
Finally, the conclusions arepresented in Section 8.2 Collocation Extraction With Mono-lingual Word Alignment Method2.1 Monolingual Word AlignmentGiven a bilingual sentence pair, a source lan-guage word can be aligned with its correspond-Figure 1.
Bilingual word alignmenting target language word.
Figure 1 shows an ex-ample of Chinese-to-English word alignment.In Figure 1, a word in one language is alignedwith its counterpart in the other language.
Forexamples, the Chinese word "?
?/tuan-dui" isaligned with its English translation "team", whilethe Chinese word "??
?/fu-ze-ren" is alignedwith its English translation "leader".In the Chinese sentence in Figure 1, there aresome Chinese collocations, such as (?
?/tuan-dui, ???/fu-ze-ren).
There are also some Eng-lish collocations in the English sentence, such as(team, leader).
We separately illustrate the collo-cations in the Chinese sentence and the Englishsentence in Figure 2, where the collocated wordsare aligned with each other.
(a) Collocations in the Chinese sentence(b) Collocations in the English sentenceFigure 2.
Word alignments of collocations insentenceComparing the alignments in Figures 1 and 2,we can see that the task of monolingual colloca-tions construction is similar to that of bilingualword alignment.
In a bilingual sentence pair, asource word is aligned with its correspondingtarget word, while in a monolingual sentence, aword is aligned with its collocates.
Therefore, itis reasonable to regard collocation constructionas a task of aligning the collocated words inmonolingual sentences.??
???
?
??
??
?
?
??
??
?tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .The team leader plays a key role in the project undertaking .The team leader plays a key role in the project undertaking.The team leader plays a key role in the project undertaking.??
???
?
??
??
?
?
??
??
?tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .??
???
?
??
??
?
?
??
??
?tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .488Statistical bilingual word alignment method,which has been well studied in the context ofmachine translation, can extract the aligned bi-lingual word pairs from a bilingual corpus.
Thispaper adapts the bilingual word alignment algo-rithm to monolingual scenario to align the collo-cated words in a monolingual corpus.Given a sentence with l words },...,{ 1 lwwS = ,the word alignments ]},1[|),{( liaiA i ?=  can beobtained by maximizing the word alignmentprobability of the sentence, according to Eq.
(1).
)|(maxarg SApAA?=??
(1)Where Aai i ?
),(  means that the word iw  isaligned with the wordiaw .In a monolingual sentence, a word never col-locates with itself.
Thus the alignment set is de-noted as }&],1[|),{( ialiaiA ii ?
?= .We adapt the bilingual word alignment model,IBM Model 3 (Brown et al, 1993), to monolin-gual word alignment.
The probability of thealignment sequence is calculated using Eq.
(2).??
?==ljjajliii lajdwwtwnSAp j11),|()|()|()|( ?
(2)Where i?
denotes the number of words that arealigned with iw .
Three kinds of probabilities areinvolved:- Word collocation probability )|(jajwwt ,which describes the possibility of wj collo-cating withjaw ;- Position collocation probability d(j, aj, l),which describes the probability of a wordin position aj collocating with anotherword in position j;- Fertility probability )|( ii wn ?
, which de-scribes the probability of the number ofwords that a word wi can collocate with(refer to subsection 7.1 for further discus-sion).Figure 3 shows an example of word alignmenton the English sentence in Figure 2 (b) with theMWA method.
In the sentence, the 7th word"role" collocates with both the 4th word "play"and the 6th word "key".
Thus, )|( 74 wwt  and)|( 76 wwt  describe the probabilities that theword "role" collocates with "play" and "key",Figure 3.
Results of MWA methodrespectively.
)12,7|4(d  and )12,7|6(d  describethe probabilities that the word in position 7 col-locates with the words in position 4 and 6 in asentence with 12 words.
For the word "role", 7?is 2, which indicates that the word "role" collo-cates with two words in the sentence.To train the MWA model, we implement aMWA tool for collocation extraction, which usessimilar training methods for bilingual wordalignment, except that a word can not be alignedto itself.2.2 Collocation ExtractionGiven a monolingual corpus, we use the trainedMWA model to align the collocated words ineach sentence.
As a result, we can generate a setof aligned word pairs on the corpus.
Accordingto the alignment results, we calculate the fre-quency for two words aligned in the corpus, de-noted as ),( ji wwfreq .
In our method, we filteredthose aligned word pairs whose frequencies arelower than 5.
Based on the alignment frequency,we estimate the alignment probabilities for eachaligned word pair as shown in Eq.
(3) and (4).?
?=?w jjiji wwfreqwwfreqwwp),(),()|(  (3)?
?=?w ijiij wwfreqwwfreqwwp),(),()|(  (4)With alignment probabilities, we assign scoresto the aligned word pairs and those with higherscores are selected as collocations, which areestimated as shown in Eq.
(5).2)|()|(),( ijjijiwwpwwpwwp+=      (5)3 Initial ExperimentsIn this experiment, we used the method as de-scribed in Section 2 for collocation extraction.Since our method does not use any linguistic in-formation, we compared our method with theThe team leader plays a key role in the project undertaking .
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)               (12)The team leader plays a key role in the project undertaking .
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)              (12)4890246810120 20 40 60 80 100 120 140 160 180 200Top-N collocations (K)Precision(%)Our method (Probability)Log-likelihood ratioFigure 4.
Precision of collocationsbaseline methods without using linguistic knowl-edge.
These baseline methods take all co-occurring word pairs within a given window ascollocation candidates, and then use associationmeasures to rank the candidates.
Those candi-dates with higher association scores are extractedas collocations.
In this paper, the window size isset to [-6, +6].3.1 DataThe experiments were carried out on a Chinesecorpus, which consists of one year (2004) of theXinhua news corpus from LDC 2 , containingabout 28 millions of Chinese words.
Since punc-tuations are rarely used to construct collocations,they were removed from the corpora.
To auto-matically estimate the precision of extracted col-locations on the Chinese corpus, we built a goldset by collecting Chinese collocations fromhandcrafted collocation dictionaries, containing56,888 collocations.3.2 ResultsThe precision is automatically calculated againstthe gold set according to Eq.
(6).)(#)(#TopgoldTopNNCCCprecision?
?= I            (6)Where CTop-N and Cgold denote the top colloca-tions in the N-best list and the collocations in thegold set, respectively.We compared our method with several base-line methods using different association meas-ures3: co-occurring frequency, log-likelihood2 Available at: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T033 The definitions of these measures can be found in Man-ning and Sch?tze (1999).0204060801000.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9log(frequency)(%)PrecisionAlignment ProbabilityFigure 5.
Frequency vs. precision/alignmentprobabilityratio, chi-square test, mutual information, and t-test.
Among them, the log-likelihood ratio meas-ure achieves the best performance.
Thus, in thispaper, we only show the performance of the log-likelihood ratio measure.Figure 4 shows the precisions of the top N col-locations as N steadily increases with an incre-ment of 1K, which are extracted by our methodand the baseline method using log-likelihoodratio as the association measure.The absolute precision of collocations is nothigh in the figure.
For example, among the top200K collocations, about 4% of the collocationsare correct.
This is because our gold set containsonly about 57K collocations.
Even if all colloca-tions in the gold set are included in the 200K-best list, the precision is only 28%.
Thus, it ismore useful to compare precision curves for col-locations in the N-best lists extracted by differentmethods.
In addition, since this gold set only in-cludes a small number of collocations, the preci-sion curves of our method and the baselinemethod are getting closer, as N increases.
Forexample, when N is set to 200K, our method andthe baseline method achieved precisions of4.09% and 3.12%, respectively.
And when N isset to 400K, they achieved 2.78% and 2.26%,respectively.
For convenience of comparison, weset N up to 200K in the experiments.From the results, it can also be seen that,among the N-best lists with N less than 20K, theprecision of the collocations extracted by ourmethod is lower than that of the collocations ex-tracted by the baseline, and became higher whenN is larger than 20K.In order to analyze the possible reasons, weinvestigated the relationships among the fre-quencies of the aligned word pairs, the alignment490xyb =4b =2Figure 6.  xbey /?=probabilities, and precisions of collocations,which are shown in Figure 5.
From the figure,we can see (1) that the lower the frequencies ofthe aligned word pairs are, the higher the align-ment probabilities are; and (2) that the precisionsof the aligned word pairs with lower frequenciesis lower.
According to the above observations,we conclude that it is the word pairs with lowerfrequencies but higher probabilities that causedthe lower precision of the top 20K collocationsextracted by our method.4 Improved MWA MethodAccording to the analysis in subsection 3.2, weneed to penalize the aligned word pairs withlower frequencies.
In order to achieve the abovegoal, we need to refine the alignment probabili-ties by using a penalization factor derived from afunction on the frequencies of the aligned wordpairs.
This function )(xfy =  should satisfy thefollowing two conditions, where x  representsthe log function of frequencies.
(1) The function is monotonic.
When x  is set toa smaller number, y  is also small.
This re-sults in the penalization on the aligned wordpairs with lower frequencies.
(2) When ?
?x , y  is set to 1.
This means thatwe don?t penalize the aligned word pairswith higher frequencies.According to the above descriptions, we pro-pose to use the exponential function in Eq.
(7).xbey /?=  (7)Figure 6 describes this function.
The constantb in the function is used to adjust the shape of theline.
The line is sharp with b set to a small num-ber, while the line is flat with b set to a largernumber.
In our case, if b is set to a larger number,05101520250 20 40 60 80 100 120 140 160 180 200Top-N collocations (K)Precision(%)Refined probabilityProbabilityBaseline (Log-likelihood ratio)Figure 7.
Precision of collocations extracted bythe improved methodwe assign a larger penalization weight to thosealigned word pairs with lower frequencies.According to the above discussion, we can usethe following measure to assign scores to thealigned words pairs generated by the MWAmethod.
)),(log(2)|()|(),(ji wwfreqbijjijirewwpwwpwwp?
?+=(8)Where wi and wj are two aligned words.
p(wi|wj)and p(wj|wi) are alignment probabilities as shownin Eq.
(3) and (4).
)),(log( ji wwfreq  is the logfunction of the frequencies of the aligned wordpairs (wi, wj).5 Evaluation on Chinese corpusWe used the same Chinese corpus described inSection 3 to evaluate the improved method asshown in Section 4.
In the experiments, b  wastuned by using a development set and set to 25.5.1 PrecisionIn this section, we evaluated the extracted collo-cations in terms of precision using both auto-matic evaluation and human evaluation.Automatic EvaluationFigure 7 shows the precisions of the colloca-tions in the N-best lists extracted by our methodand the baseline method against the gold set inSection 3.
For our methods, we used two differ-ent measures to rank the aligned word pairs:alignment probabilities in Eq.
(5) and refined491Our method BaselineTrue 569 290A 25 16B 5 4C 240 251FalseD 161 439Table 1.
Manual evaluation of the top 1K Chi-nese collocations.
The precisions of our methodand the baseline method are 56.9% and 29.0%,respectively.alignment probabilities in Eq.
(8).
From the re-sults, it can be seen that with the refined align-ment probabilities, our method achieved thehighest precision on the N-best lists, whichgreatly outperforms the best baseline method.For example, in the top 1K list, our methodachieves a precision of 20.6%, which is muchhigher than the precision of the baseline method(11.7%).
This indicates that the exponential func-tion used to penalize the alignment probabilitiesplays a key role in demoting most of the alignedword pairs with low frequencies.Human EvaluationIn automatic evaluation, the gold set only con-tains collocations in the existing dictionaries.Some collocations related to specific corpora arenot included in the set.
Therefore, we selectedthe top 1K collocations extracted by our im-proved method to manually estimate the preci-sion.
During human evaluation, the true colloca-tions are denoted as "True" in our experiments.The false collocations were further classified intothe following classes.A: The candidate consists of two words thatare semantically related, such as (??
doctor,??
nurse).B: The candidate is a part of the multi-word(?
3) collocation.
For example, (??
self, ?
?mechanism) is a part of the three-word colloca-tion (??
self, ??
regulating, ??
mecha-nism).C: The candidates consist of the adjacentwords that frequently occur together, such as (?he, ?
say) and (?
very, ?
good).D: Two words in the candidates have no rela-tionship with each other, but occur together fre-quently, such as (??
Beijing, ?
month) and(?
and, ?
for).Table 1 shows the evaluation results.
Ourmethod extracted 569 true collocations, which0246810120 1 2 3 4 5 6 7 8 9 10 11 12Training corpus (Months)Precision(%)Our methodBaselineFigure 8.
Corpus size vs. precisionare much more than those extracted by the base-line method.
Further analysis shows that, in addi-tion to extracting short-span collocations, ourmethod extracted collocations with longer spansas compared with the baseline method.
For ex-ample, (??
in, ??
state) and (??
because,??
so) are two long-span collocations.
Amongthe 1K collocations, there are 48 collocation can-didates whose spans are larger than 6, which arenot covered by the baseline method since thewindow size is set to 6.
And 33 of them are truecollocations, with a higher precision of 69%.Classes C and D account for the most part ofthe false collocations.
Although the words inthese two classes co-occur frequently, they cannot be regarded as collocations.
And we alsofound out that the errors in class D produced bythe baseline method are much more than that ofthose produced by our method.
This indicatesthat our MWA method can remove much morenoise from the frequently occurring word pairs.In Class A, the two words are semantically re-lated and occur together in the corpus.
Thesekinds of collocations can not be distinguishedfrom the true collocations by our method withoutadditional resources.Since only bigram collocations were extractedby our method, the multi-word (?
3) collocationswere split into bigram collocations, which causedthe error collocations in Class B4.Corpus size vs. precisionHere, we investigated the effect of the corpussize on the precision of the extracted collocations.We evaluated the precision against the gold setas shown in the automatic evaluation.
First, thewhole corpus (one year of newspaper) was splitinto 12 parts according to the published months.Then we calculated the precisions as the training4 Since only a very small faction of collocations containmore than two words, a few error collocations belong toClass B.4920204060801000 20 40 60 80 100 120 140 160 180 200Top-N collocations (K)Recall (%)Our methodBaselineFigure 9.
Recall on the Chinese corpuscorpus increases part by part.
The top 20K collo-cations were selected for evaluation.Figure 8 shows the experimental results.
Theprecision of collocations extracted by our methodis obviously higher than that of collocations ex-tracted by the baseline method.
When the size ofthe training corpus became larger, the differencebetween our method and the baseline methodalso became bigger.
When the training corpuscontains more than 9 months of corpora, the pre-cision of collocations extracted by the baselinemethod did not increase anymore.
However, theprecision of collocations extracted by our methodkept on increasing.
This indicates the MWAmethod can extract more true collocations ofhigher quality when it is trained with larger sizeof training data.5.2 RecallRecall was evaluated on a manually labeled sub-set of the training corpus.
The subset contains100 sentences that were randomly selected fromthe whole corpus.
The sentence average length is24.
All true collocations (660) were labeledmanually.
The recall was calculated according toEq.
(9).
)(#)(#subsetsubsetTopCCCrecall NI?=               (9)Here, CTop-N denotes the top collocations in theN-best list and Csubset denotes the true colloca-tions in the subset.Figure 9 shows the recalls of collocations ex-tracted by our method and the baseline methodon the labeled subset.
The results show that ourmethod can extract more true collocations thanthe baseline method.0204060801000 20 40 60 80 100 120 140 160 180 200Top-N collocations (K)Recall (%)Our methodBaselineFigure 10.
Recall on the English corpusOur method BaselineTrue 591 355A 11 4B 19 20C 200 136FalseD 179 485Table 2.
Manual evaluation of the top 1K Eng-lish collocations.
The precisions of our methodand the baseline method are 59.1% and 35.5%,respectively.In our experiments, the baseline method ex-tracts about 20 millions of collocation candidates,while our method only extracts about 3 millionsof collocation candidates5.
Although the colloca-tions of our method are much less than that of thebaseline, the experiments show that the recall ofour method is higher.
This again proved that ourmethod has the stronger ability to distinguishtrue collocations from false collocations.6 Evaluation on English corpusWe also manually evaluated the proposedmethod on an English corpus, which is a subsetrandomly extracted from the British NationalCorpus6.
The English corpus contains about 20millions of words.6.1 PrecisionWe estimated the precision of the top 1K collo-cations.
Table 2 shows the results.
The classifica-tion of the false collocations is the same as thatin Table 1.
The results show that our methodsoutperformed the baseline method using log-5 We set the threshold to 7.88 with a confidence level  of005.0=?
(cf.
page 174 of Chapter 5 in (McKeown andRadev, 2000) for more details).6 Available at: http://www.hcu.ox.ac.uk/BNC/493051015200 20 40 60 80 100 120 140 160 180Top-N collocation (K)Precision(%)Figure 11.
Fertility vs. precisionlikelihood ratio.
And the distribution of the falsecollocations is similar to that on the Chinese cor-pus.6.2 RecallWe used the method described in subsection 5.2to calculate the recall.
100 English sentenceswere labeled manually, obtaining 205 true collo-cations.
Figure 10 shows the recall of the collo-cations in the N-best lists.
From the figure, it canbe seen that the trend on the English corpus issimilar to that on the Chinese corpus, which in-dicates that our method is language-independent.7 Discussion7.1 The Effect of FertilityIn the MWA model as described in subsection2.1, i?
denotes the number of words that canalign with iw .
Since a word only collocates witha few other words in a sentence, we should set amaximum number for ?
, denote as max?
.In order to set max?
, we examined the true col-locations in the manually labeled set described insubsection 5.2.
We found that 78% of words col-locate with only one word, and 17% of wordscollocate with two words.
In sum, 95% of wordsin the corpus can only collocate with at most twowords.
According to the above observation, weset max?
to 2.In order to further examine the effect of max?on collocation extraction, we used several differ-ent max?
in our experiments.
The comparison0123456780 20 40 60 80 100Span of collocationlog(#(alignedwordpairs))Figure 12.
Distribution of spansresults are shown in Figure 11.
The highest pre-cision is achieved when max?
is set to 2.
Thisresult verifies our observation on the corpus.7.2 Span of CollocationOne of the advantages of our method is thatlong-span collocations can be reliably extracted.In this subsection, we investigate the distributionof the span of the aligned word pairs.
For thealigned word pairs occurring more than once, wecalculated the average span as shown in Eq.
(10).
),();,(),(jicorpussjiji wwfreqswwSpanwwAveSpan?= ?
(10)Where, );,( swwSpan ji  is the span of the wordswi and wj in the sentence s; ),( ji wwAveSpan  isthe average span.The distribution is shown in Figure 12.
It canbe seen that the number of the aligned word pairsdecreased exponentially as the average span in-creased.
About 17% of the aligned word pairshave spans longer than 6.
According to the hu-man evaluation result for precision in subsection5.1, the precision of the long-span collocations iseven higher than that of the short-span colloca-tions.
This indicates that our method can extractreliable collocations with long spans.8 ConclusionWe have presented a monolingual word align-ment method to extract collocations from mono-lingual corpus.
We first replicated the monolin-gual corpus to generate a parallel corpus, inwhich each sentence pair consists of the twoidentical sentences in the same language.
Thenwe adapted the bilingual word alignment algo-rithm to the monolingual scenario to align the10321maxmaxmaxmax====???
?494potentially collocated word pairs in the monolin-gual sentences.
In addition, a ranking methodwas proposed to finally extract the collocationsfrom the aligned word pairs.
It scores collocationcandidates by using alignment probabilities mul-tiplied by a factor derived from the exponentialfunction on the frequencies.
Those with higherscores are selected as collocations.
Both Chineseand English collocation extraction experimentsindicate that our method outperforms previousapproaches in terms of both precision and recall.For example, according to the human evaluationson the Chinese corpus, our method achieved aprecision of 56.9%, which is much higher thanthat of the baseline method (29.0%).
Moreover,we can extract collocations with longer span.Human evaluation on the extracted Chinese col-locations shows that 69% of the long-span (>6)collocations are correct.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2): 263-311.Yaacov Choueka, S.T.
Klein, and E. Neuwitz.
1983.Automatic Retrieval of Frequent Idiomatic andCollocational Expressions in a Large Corpus.Journal for Literary and Linguistic computing,4(1):34-38.Kenneth Church and Patrick Hanks.
1990.
Word As-sociation Norms, Mutual Information, and Lexi-cography.
Computational Linguistics, 16(1):22-29.Ted Dunning.
1993.
Accurate Methods for the Statis-tics of Surprise and Coincidence.
ComputationalLinguistics, 19(1): 61-74.Stefan Evert.
2004.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis,University of Stuttgart.Dekang Lin.
1998.
Extracting Collocations from TextCorpora.
In Proceedings of the 1st Workshop onComputational Terminology, pp.
57-63.Christopher D. Manning and Hinrich Sch?tze.
1999.Foundations of Statistical Natural Language Proc-essing, Cambridge, MA; London, U.K.: BradfordBook & MIT Press.Kathleen R. McKeown and Dragomir R. Radev.
2000.Collocations.
In Robert Dale, Hermann Moisl, andHarold Somers (Ed.
), A Handbook of Natural Lan-guage Processing, pp.
507-523.Darren Pearce.
2001.
Synonymy in Collocation Ex-traction.
In Proceedings of NAACL-2001 Workshopon Wordnet and Other Lexical Resources: Applica-tions, Extensions and Customizations, pp.
41-46.Darren Pearce.
2002.
A Comparative Evaluation ofCollocation Extraction Techniques.
In Proceedingsof the 3rd International Conference on LanguageResources and Evaluation, pp.
651-658.Violeta Seretan and Eric Wehrli.
2006.
Accurate Col-location Extraction Using a Multilingual Parser.
InProceedings of the 21st International Conferenceon Computational Linguistics and 44th AnnualMeeting of the Association for Computational Lin-guistics (COLING/ACL-2006), pp.
953-960Frank Smadja.
1993.
Retrieving Collocations fromText: Xtract.
Computational Linguistics, 19(1):143-177.Joachim Wermter and Udo Hahn.
2004.
CollocationExtraction Based on Modifiability Statistics.
InProceedings of the 20th International Conferenceon Computational Linguistics (COLING-2004), pp.980-986.495
