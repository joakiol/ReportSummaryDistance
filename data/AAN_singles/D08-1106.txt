Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011?1020,Honolulu, October 2008. c?2008 Association for Computational LinguisticsGraph-based Analysis of Semantic Drift in Espresso-likeBootstrapping AlgorithmsMamoru KomachiNAIST, Japanmamoru-k@is.naist.jpTaku KudoGoogle Inc.taku@google.comMasashi ShimboNAIST, Japanshimbo@is.naist.jpYuji MatsumotoNAIST, Japanmatsu@is.naist.jpAbstractBootstrapping has a tendency, called seman-tic drift, to select instances unrelated to theseed instances as the iteration proceeds.
Wedemonstrate the semantic drift of bootstrap-ping has the same root as the topic drift ofKleinberg?s HITS, using a simplified graph-based reformulation of bootstrapping.
Weconfirm that two graph-based algorithms, thevon Neumann kernels and the regularizedLaplacian, can reduce semantic drift in thetask of word sense disambiguation (WSD)on Senseval-3 English Lexical Sample Task.Proposed algorithms achieve superior perfor-mance to Espresso and previous graph-basedWSD methods, even though the proposed al-gorithms have less parameters and are easy tocalibrate.1 IntroductionIn recent years machine learning techniques be-come widely used in natural language processing(NLP).
These techniques offer various ways to ex-ploit large corpora and are known to perform wellin many tasks.
However, these techniques often re-quire tagged corpora, which are not readily availableto many languages.
So far, reducing the cost of hu-man annotation is one of the important problems forbuilding NLP systems.To mitigate the problem of hand-tagging re-sources, semi(or minimally)-supervised and unsu-pervised techniques have been actively studied.Hearst (1992) first presented a bootstrapping methodwhich requires only a small amount of instances(seed instances) to start with, but can easily mul-tiply the number of tagged instances with mini-mal human annotation cost, by iteratively apply-ing the following phases: pattern induction, patternranking/selection, and instance extraction.
Boot-strapping has been widely adopted in NLP applica-tions such as word sense disambiguation (Yarowsky,1995), named entity recognition (Collins and Singer,1999) and relation extraction (Riloff and Jones,1999; Pantel and Pennacchiotti, 2006).However, it is known that bootstrapping often ac-quires instances not related to seed instances.
Forexample, consider the task of collecting the namesof common tourist sites from web corpora.
Givenwords like ?Geneva?
and ?Bali?
as seed instances,bootstrapping would eventually learn generic pat-terns such as ?pictures?
and ?photos,?
which alsoco-occur with many other unrelated instances.
Thesubsequent iterations would likely acquire frequentwords that co-occur with these generic patterns,such as ?Britney Spears.?
This phenomenon iscalled semantic drift (Curran et al, 2007).A straightforward approach to avoid semanticdrift is to terminate iterations before hitting genericpatterns, but the optimal number of iterations is taskdependent and is hard to come by.
The recently pro-posed Espresso (Pantel and Pennacchiotti, 2006) al-gorithm incorporates sophisticated scoring functionsto cope with generic patterns, but as Komachi andSuzuki (2008) pointed out, Espresso still shows se-mantic drift unless iterations are terminated appro-priately.Another deficiency in bootstrapping is its sensi-tivity to many parameters such as the number of1011seed instances, the stopping criterion of iteration, thenumber of instances and patterns selected on each it-eration, and so forth.
These parameters also need tobe calibrated for each task.In this paper, we present a graph-theoretic anal-ysis of Espresso-like bootstrapping algorithms.
Weargue that semantic drift is inherent in these algo-rithms, and propose to use two graph-based algo-rithms that are theoretically less prone to semanticdrift, as an alternative to bootstrapping.After a brief review of related work in Section 2,we analyze in Section 3 a bootstrapping algorithm(Simplified Espresso) which can be thought of as adegenerate version of Espresso.
Simplified Espressois simple enough to allow an algebraic treatment,and its equivalence to Kleinberg?s HITS algorithm(Kleinberg, 1999) is shown.
An implication of thisequivalence is that semantic drift in this bootstrap-ping algorithm is essentially the same phenomenonas topic drift observed in link analysis.
Another im-plication is that semantic drift is inevitable in Sim-plified Espresso as it converges to the same scorevector regardless of seed instances.The original Espresso also suffers from the sameproblem as its simplified version does.
It incorpo-rates heuristics not present in Simplified Espresso toreduce semantic drift, but these heuristics have lim-ited effect as we demonstrate in Section 3.3.In Section 4, we propose two graph-based algo-rithms to reduce semantic drift.
These algorithmsare used in link analysis community to reduce theeffect of topic drift.
In Section 5 we apply them tothe task of word sense disambiguation on Senseval-3Lexical Sample Task and verify that they indeed re-duce semantic drift.
Finally, we conclude our workin Section 6.2 Related Work2.1 Overview of BootstrappingBootstrapping (or self-training) is a general frame-work for reducing the requirement of manual an-notation.
Hearst (1992) described a bootstrappingprocedure for extracting words in hyponym (is-a)relation, starting with three manually given lexico-syntactic patterns.The idea of learning with a bootstrapping methodwas adopted for many tasks.
Yarowsky (1995) pre-sented an unsupervised WSD system which rivalssupervised techniques.
Abney (2004) presented athorough discussion on the Yarowsky algorithm.
Heextended the original Yarowsky algorithm to a newfamily of bootstrapping algorithms that are mathe-matically well understood.Li and Li (2004) proposed a method called Bilin-gual Bootstrapping.
It makes use of a translationdictionary and a comparable corpus to help disam-biguate word senses in the source language, by ex-ploiting the asymmetric many-to-many sense map-ping relationship between words in two languages.Curran et al (2007) presented an algorithm calledMutual Exclusion Bootstrapping, which minimizessemantic drift using mutual exclusion between se-mantic classes of learned instances.
They prepareda list of so-called stop classes similar to a stop wordlist used in information retrieval to help bound thesemantic classes.
Stop classes are sets of termsknown to cause semantic drift in particular seman-tic classes.
However, stop classes vary from task totask and domain to domain, and human interventionis essential to create an effective list of stop classes.A major drawback of bootstrapping is the lackof principled method for selecting optimal param-eter values (Ng and Cardie, 2003; Banko and Brill,2001).
Also, there is an issue of generic patternswhich deteriorates the quality of acquired instances.Previously proposed bootstrapping algorithms differin how they deal with the problem of semantic drift.We will take recently proposed Espresso algorithmas the example to explain common configuration forbootstrapping in detail.2.2 The Espresso AlgorithmPantel and Pennachiotti (2006) proposed a boot-strapping algorithm called Espresso to learn binarysemantic relations such as is-a and part-of froma corpus.
What distinguishes Espresso from otherbootstrapping algorithms is that it benefits fromgeneric patterns by using a principled measure ofinstance and pattern reliability.
The key idea ofEspresso is recursive definition of pattern-instancescoring metrics.
The reliability scores of pattern pand instance i, denoted respectively as rpi(p) and1012r?
(i), are given as follows:rpi(p) =?i?Ipmi(i,p)max pmir?(i)|I|(1)r?
(i) =?p?Ppmi(i,p)max pmirpi(p)|P |(2)wherepmi(i, p) = log2|i, p||i, ?||?, p|(3)is pointwise mutual information between i and p, Pand I are sets of patterns and instances, and |P | and|I| are the numbers of patterns and instances, respec-tively.
|i, ?| and |?, p| are the frequencies of patternp and instance i in a given corpus, respectively, and|i, p| is the frequency of pattern p which co-occurswith instance i. max pmi is a maximum value ofthe pointwise mutual information over all instancesand patterns.
The intuition behind these definitionsis that a reliable pattern co-occurs with many reli-able instances, and a reliable instance co-occurs withmany reliable patterns.Espresso and other bootstrapping methods iteratethe following three phases: pattern induction, pat-tern ranking/selection, and instance extraction.We describe these phases below, along with theparameters that controls each phase.Phase 1.
Pattern Induction Induce patterns froma corpus given seed instances.
Patterns may be sur-face text patterns, lexico-syntactic patterns, and/orjust features.Phase 2.
Pattern Ranking/Selection Create apattern ranker from a corpus using instances as fea-tures and select patterns which co-occur with seedinstances for the next instance extraction phase.
Themain issue here is to avoid ranking generic patternshigh and to choose patterns with high relatedness tothe seed instances.
Parameters and configurations:(a) a pattern scoring metrics and (b) the number ofpatterns to use for extraction of instances.Phase 3.
Instance Extraction Select high-confidence instances to the seed instance set.
It isdesirable to keep only high-confidence instances atthis phase, as they are used as seed instances for theinput:seed vector i0pattern-instance co-occurrence matrix Moutput:instance and pattern score vectors i and p1: i = i02: loop3: p ?
M i4: Normalize p5: i ?
MTp6: Normalize i7: if i and p have both converged then8: return i and p9: end if10: end loopFigure 1: A simple bootstrapping algorithmnext iteration.
Optionally, instances can be cumula-tively obtained on each iteration to retain highly rel-evant instances learned in early iterations.
Parame-ters and configurations: (c) instance scoring metrics,(d) whether to retain extracted instances on each it-eration or not, and (e) the number of instances topass to the next iteration.Bootstrapping iterates the above three phases sev-eral times until stopping criteria are met.
Acquiredinstances tend to become noisy as the iteration pro-ceeds, so it is important to terminate before semanticdrift occurs.
Thus, we have another configuration:(f) stopping criterion.Espresso uses Equations (1) for (a) and (2) for (c)respectively, whereas other parameters rely on thetasks and need calibration.
Even though Espressogreatly improves recall while keeping high precisionby using these pattern and instance scoring metrics,Komachi and Suzuki (2008) observed that extractedinstances matched against generic patterns may be-come erroneous after tens of iterations, showing thedifficulty of applying bootstrapping methods to dif-ferent domains.3 Analysis of an Espresso-likeBootstrapping Algorithm3.1 Simplified EspressoLet us consider a simple bootstrapping algorithmillustrated in Figure 1, in order to elucidate the cause1013of semantic drift.As before, let |I| and |P | be the numbers ofinstances and patterns, respectively.
The algo-rithm takes a seed vector i0, and a pattern-instanceco-occurrence matrix M as input.
i0 is a |I|-dimensional vector with 1 at the position of seed in-stances, and 0 elsewhere.
M is a |P | ?
|I|-matrixwhose (p, i)-element [M ]pi holds the (possibly re-weighted) number of co-occurrence of pattern p andinstance i in the corpus.
If both i and p have con-verged, the algorithm returns the pair of i and p asoutput.This algorithm, though simple, can encodeEspresso?s update formulae (1) and (2) as Steps 3through 6 if we pose[M ]pi =pmi(i, p)max pmi, (4)and normalize p and i in Steps 4 and 6 byp ?
p/|I| and i ?
i/|P |, (5)respectively.This specific instance of the algorithm of Fig-ure 1, obtained by specialization through Equations(4) and (5), will be henceforth referred to as Simpli-fied Espresso.
Indeed, it is an instance of the origi-nal Espresso in which the iteration is not terminateduntil convergence, all instances are carried over tothe next iteration, and instances are not cumulativelylearned.3.2 Simplified Espresso as Link AnalysisLet n denote the number of times Steps 2?10 areiterated.
Plugging (4) and (5) into Steps 3?6, wesee that the score vector of instances after the nthiteration isin = Ani0 (6)whereA = 1|I||P |MTM.
(7)Suppose matrix A is irreducible; i.e., the graphinduced by taking A as the adjacency matrix is con-nected.
If n is increased and in is normalized oneach iteration, in tends to the principal eigenvec-tor of A.
This implies that no matter what seed in-stances are input, the algorithm will end up with thesame ranking of instances, if it is run until conver-gence.
Because A = MTM|I||P | , the principal eigen-vector of A is identical to the authority vector ofHITS (Kleinberg, 1999) algorithm run on the graphinduced by M .
1 This similarity of Equations (1),(2) and HITS is not discussed in (Pantel and Pen-nacchiotti, 2006).As a consequence of the above discussion, se-mantic drift in simplified Espresso seems to be in-evitable as the iteration proceeds, since the principaleigenvector of A need not resemble seed vector i0.A similar phenomenon is reported for HITS and isknown as topic drift, in which pages of the dominanttopic are ranked high regardless of the given query.
(Bharat and Henzinger, 1998)Unlike HITS and Simplified Espresso, how-ever, Espresso and other bootstrapping algo-rithms (Yarowsky, 1995; Riloff and Jones, 1999),incorporate heuristics so that only patterns and in-stances with high confidence score are carried overto the next iteration.3.3 Convergence Process of EspressoTo investigate the effect of semantic drift onEspresso with and without the heuristics of selectingthe most confident instances on each iteration (i.e.,the original Espresso and Simplified Espresso ofSection 3.2), we apply them to the task of word sensedisambiguation of word ?bank?
in the Senseval-3Lexical Sample (S3LS) Task data.2 There are 394instances of word ?bank?
and their occurring con-text in this dataset, and each of them is annotatedwith its true sense.
Of the ten senses of bank, themost frequent is the bank as in ?bank of the river.
?We use the standard training-test split provided withthe data set.We henceforth denote Espresso with the follow-ing filtering strategy as Filtered Espresso to stressthe distinction from Simplified Espresso.
For Fil-tered Espresso, we cleared all but the 100 top-scoring instances in the instance vector on each iter-ation, and the number of non-zeroed instance scores1As long as the relative magnitude of the components of vec-tor in is preserved, the vector can be normalized in any way oneach iteration.
Hence HITS and Simplified Espresso use differ-ent normalization but both converge to the principal eigenvectorof A.2http://www.senseval.org/senseval3/data.html1014grows by 100 on each iteration.
On the other hand,we cleared all but the 20 top-scoring patterns in thepattern vector on each iteration, and the number ofnon-zeroed pattern scores grows by 1 on each iter-ation following (Pantel and Pennacchiotti, 2006).3The values of other parameters (b), (d), (e) and (f)remains the same as those for simplified Espresso inSection 3.1.The task of WSD is to correctly predict the sensesof test instances whose true sense is hidden from thesystem, using training data and their true senses.
Topredict the sense of a given instance i, we apply k-nearest neighbor algorithm.Given a test instance i, its sense is predicted withthe following procedure:1.
Compute the instance-pattern matrix M fromthe entire set of instances.
We defer the detailsof this step to Section 5.2.2.
Run Simplified- and Filtered Espresso usingthe given instance i as the only seed instance.3.
After the termination of the algorithm, select ktraining instances with the highest scores in thescore vector i output by the algorithm.4.
Since the selected k instances are traininginstances, their true senses are accessible.Choose the majority sense s from these k in-stances, and output s as the prediction for thegiven instance i.
When there is a tie, output thesense of the instance with the highest score ini.
Note that only Step 4 uses sense information.Figure 2 shows the convergence process ofSimplified- and Filtered Espresso.
X-axis indicatesthe number of bootstrapping iterations and Y-axisindicates the recall, which in this case equals pre-cision, as the coverage is 100% in all cases.3We conducted preliminary experiment to find these param-eters to maximize the performance of Filtered Espresso.
(Thesenumbers are different from the original Espresso (Pantel andPennacchiotti, 2006).)
The number of initial patterns is rel-atively large because of a data sparseness problem in WSD,unlike relation extraction and named entity recognition.
Also,WSD basically uses more features than relation extraction andthus it is hard to determine the stopping criterion based on thenumber and scores of patterns, as (Pantel and Pennacchiotti,2006) does.0.40.50.60.70.80.915  10  15  20  25  30recall of "bank"iterationSimplified EspressoFiltered Espressomost frequent sense (baseline)Figure 2: Recall of Simplified- and Filtered EspressoSimplified Espresso tends to select the most fre-quent sense as the iteration proceeds, and after nineiterations it selects the most frequent sense (?thebank of the river?)
regardless of the seed instances.As expected from the discussion in Section 3.2,generic patterns gradually got more weight and se-mantic drift occurred in later iterations.
Indeed, theranking of the instances after convergence was iden-tical to the HITS authority ranking computed frominstance-pattern matrix M (i.e., the ranking inducedby the dominant eigenvector of MTM ).On the other hand, Filtered Espresso suffers lessfrom semantic drift.
The final recall achievedwas 0.773 after convergence on the 20th iteration,outperforming the most-frequent sense baseline by0.10.
However, a closer look reveals that the filter-ing heuristics is limited in effectiveness.Figure 3 plots the learning curve of FilteredEspresso on the set of test instances.
We show re-call ( |correct instances||total true instances| ) of each sense to see howFiltered Espresso tends to select the most frequentsense.
If semantic drift takes place, the numberof instances predicted as the most frequent senseshould increase as the iteration proceeds, resultingin increased recall on the most frequent sense anddecreased recall on other senses.
Figure 3 exactlyexhibit this trend, meaning that Filtered Espresso isnot completely free from semantic drift.
Figure 2also shows that the recall of Filtered Espresso startsto decay after the seventh iteration.10150.30.40.50.60.70.80.915  10  15  20  25  30recalliterationmost frequent senseother sensesFigure 3: Recall of Filtered Espresso on the instanceshaving ?bank of the river?
and other senses4 Two Graph-based Algorithms forExploiting Generic PatternsWe explore two graph-based methods which havethe advantage of Espresso to harness the property ofgeneric patterns by the mutual recursive definitionof instance and pattern scores.
They also have lessparameters than bootstrapping, and are less prone tosemantic drift.4.1 Von Neumann KernelKandola et al (2002) proposed the von Neumannkernels for measuring similarity of documents us-ing words.
If we apply the von Neumann kernels tothe pattern-instance co-occurrence matrix instead ofthe document-word matrix, the relative importanceof an instance to seed instances can be estimated.Let A = MTM be the instance similarity matrixobtained from pattern-instance matrix M , and ?
bethe principal eigenvalue of A.
The von Neumannkernel matrix K?
with diffusion factor ?
(0 ?
?
<?
?1) is defined as follows:K?
= A?
?n=0?nAn = A(I ?
?A)?1.
(8)The similarity between two instances i, j is given bythe (i, j) element of K?
.
Hence, the i-th columnvector can be used as the score vector for seed in-stance i.Ito et al (2005) showed that the von Neumannkernels represent a mixture of the co-citation re-latedness and Kleinberg?s HITS importance.
Theycompute the weighted sum of all paths between twonodes in the co-citation graph induced by A =MTM .
The (MTM)n term of smaller n corre-sponds to the relatedness to the seed instances, andthe (MTM)n term of larger n corresponds to HITSimportance.
The von Neumann kernels calculate theweighted sum of (MTM)n from n = 1 to ?, andtherefore smaller diffusion factor ?
results in rank-ing by relatedness, and larger ?
returns ranking byHITS importance.In NLP literature, Schu?tze (1998) introduced thenotion of first- and second-order co-occurrence.First-order co-occurrence is a context which directlyco-occurs with a word, whereas second-order co-occurrence is a context which occurs with the (con-textual) words that co-occur with a word.
Higher-order co-occurrence information is less sparse andmore robust than lower-order co-occurrence, andthus is useful for a proximity measure.Given these definitions, we see that the (MTM)nterm of smaller n corresponds to lower-order co-occurrence, which is accurate but sparse, and the(MTM)n term of larger n corresponds to higher-order co-occurrence, which is dense but possiblygiving too much weight on unrelated instances ex-tracted by generic patterns.As a result, it is expected that setting diffusionfactor ?
to a small value prevents semantic drift andalso takes higher order pattern vectors into account.We verify this claim in Section 5.3.4.2 Regularized Laplacian KernelThe von Neumann kernels can be regarded as a mix-ture of relatedness and importance, and diffusionfactor ?
controls the trade-off between relatednessand importance.
In practice, however, setting theright parameter value becomes an issue.
We solvethis problem by the regularized Laplacian (Smolaand Kondor, 2003; Chebotarev and Shamis, 1998),which are stable across diffusion factors and cansafely benefit from generic patterns.LetG be a weighted undirected graph whose adja-cency (weight) matrix is a symmetric matrix A.
The(combinatorial) graph Laplacian L of a graph G isdefined as follows:L = D ?A (9)where D is a diagonal matrix, and the ith diagonal1016Table 1: Recall of predicted labels of bankalgorithm MFS othersSimplified Espresso 100.0 0.0Filtered Espresso 100.0 30.2Filtered Espresso (optimal stopping) 94.4 67.4von Neumann kernels 92.1 65.1regularized Laplacian 92.1 62.8element [D]ii is given by[D]ii =?j[A]ij .
(10)Here, [A]ij stands for the (i, j) element of A.
By re-placing A with ?L in Equation (8) and deleting thefirst A, we obtain a regularized Laplacian kernel 4.R?
=?
?n=0?n(?L)n = (I + ?L)?1 (11)Again, ?(?
0) is called the diffusion factor.Both the regularized Laplacian and the von Neu-mann kernels compute all the possible paths in agraph, and consequently they can calculate influencebetween nodes in a long distance in the graph.
Also,Equations (9) and (10) show that the negative Lapla-cian ?L can be regarded as a modification to thegraph G with the weight of self-loops re-weightedto negative values.
In this modified graph, if an in-stance co-occurs with a pattern which also co-occurswith a large number of other instances, a self-loopof a node in the instance similarity graph inducedby MTM will receive a higher negative weight.In other words, instances co-occurring with genericpatterns will get less weight in the regularized Lapla-cian than in the von Neumann kernels.5 Experiments and Results5.1 Experiment 1: Reducing Semantic DriftWe test the von Neumann kernels and the regular-ized Laplacian on the same task as we used in Sec-tion 3.3; i.e., word sense disambiguation of word4It has been reported that normalization of A improves per-formance in application (Johnson and Zhang, 2007), so we nor-malize L by L = I ?D?12AD?12 .?bank.?
During the training phase, a pattern-instancematrix M was constructed using the training andtesting data from Senseval-3 Lexical Sample (S3LS)Task.
The (i, j) element of M of both kernels is setto pointwise mutual information of a pattern i andan instance j, just the same as in Espresso.
Recall isused in evaluation.5 The diffusion parameter ?
is setto 10?5 and 10?2 for the von Neumann kernels andthe regularized Laplacian, respectively.Table 1 illustrates how well the proposed meth-ods reduce semantic drift, just the same as the ex-periment of Figure 3 in Section 3.3.
We evalu-ate the recall on predicting the most frequent sense(MFS) and the recall on predicting other less fre-quent senses (others).
For Filtered Espresso, tworesults are shown: the result on the seventh iter-ation, which maximizes the performance (FilteredEspresso (optimal stopping)), and the one after con-vergence.
As in Section 3.3, if semantic drift oc-curs, recall of prediction on the most frequent senseincreases while recall of prediction on other sensesdeclines.
Even Filtered Espresso was affected by se-mantic drift, which is again a consequence of theinherent graphical nature of Espresso-like bootstrap-ping algorithms.
On the other hand, both proposedmethods succeeded to balance the most frequentsense and other senses.
Filtered Espresso at the op-timal number of iterations achieved the best perfor-mance.
Nevertheless, the number of iterations has tobe estimated separately.5.2 Experiment 2: WSD Benchmark DataWe conducted experiments on the task of word sensedisambiguation of S3LS data, this time not just onthe word ?bank?
but on all target nouns in the data,following (Agirre et al, 2006).
We used two typesof patterns.Unordered single words (bag-of-words) Weused all single words (unigrams) in the providedcontext from S3LS data sets.
Each word in the con-text constructs one pattern.
The pattern correspond-ing to a word w is set to 1 if it appears in the con-text of instance i.
Words were lowercased and pre-processed with the Porter Stemmer6.5Again, recall equals precision in this case as the coverageis 100% in all cases.6http://tartarus.org/?martin/PorterStemmer/def.txt1017Table 2: Comparison of WSD algorithmsalgorithm Recallmost frequent sense 54.5HyperLex (Ve?ronis, 2004) 64.6PageRank (Agirre et al, 2006) 64.5Simplified Espresso 44.1Filtered Espresso 46.9Filtered Espresso (optimal stopping) 66.5von Neumann kernels (?
= 10?5) 67.2regularized Laplacian (?
= 10?2) 67.1Local collocations A local collocation refers tothe ordered sequence of tokens in the local, narrowcontext of the target word.
We allowed a pattern tohave wildcard expressions like ?sale of * interest in* *?
for the target word interest.
We set the windowsize to ?3 by a preliminary experiment.We report the results of Filtered Espresso both af-ter convergence, and with its optimal number of iter-ations to show the upper bound of its performance.Table 2 compares proposed methods withEspresso with various configurations.
The proposedmethods outperform by a large margin the most fre-quent sense baseline and both Simplified- and Fil-tered Espresso.
This means that the proposed meth-ods effectively prevent semantic drift.Also, Filtered Espresso without early stoppingshows more or less identical performance to Sim-plified Espresso.
It is implied that the heuristics offiltering and early stopping is a crucial step not toselect generic patterns in Espresso, and the result isconsistent with the experiment of convergence pro-cess of Espresso in Section 3.3.Filtered Espresso halted after the seventh itera-tion (Filtered Espresso (optimal stopping)) is com-parable to the proposed methods.
However, in boot-strapping, not only the number of iterations but alsoa large number of parameters must be adjusted foreach task and domain.
This shortcoming makes ithard to adapt bootstrapping in practical cases.
Oneof the main advantages of the proposed methods isthat they have only one parameter ?
and are mucheasier to tune.It is suggested in Sections 3.3 and 4.1 thatEspresso and the von Neumann kernel with large ?40455055606570751e-07  1e-06  1e-05  0.0001  0.001recalldiffusion factor  von Neumann kernelSimplified Espressomost frequent senseFigure 4: Recall of the von Neumann kernels with a dif-ferent diffusion factor ?
on S3LS WSD taskconverge to the principal eigenvector of A, thoughthe result does not seem to support this claim (bothSimplified- and Filtered Espresso are 10 pointslower than the most frequent sense baseline).
Thereason seems to be because Espresso and the vonNeumann kernels use pointwise mutual informationas a weighting factor so that the principal eigenvec-tor of A may not always represent the most frequentsense.7We also show the results of previous graph-basedmethods (Agirre et al, 2006), based on Hyper-Lex (Ve?ronis, 2004) and PageRank (Brin and Page,1998).
The experimental set-up is the same as oursin that they do not use the sense tags of training cor-pus to construct a co-occurrence graph, and they usethe sense tags of all the S3LS training corpus formapping senses to clusters.
However, these meth-ods have seven parameters to tune in order to achievethe best performance, and hence are difficult to opti-mize.5.3 Experiment 3: Sensitivity to a DifferentDiffusion FactorFigure 4 shows the performance of the von Neu-mann kernels with a diffusion factor ?.
As ex-pected, smaller ?
leads to relatedness to seed in-stances, and larger ?
asymptotically converges to theHITS authority ranking (or equivalently, Simplified7A similar but more extreme case is described in (Ito et al,2005) in which the use of a normalized weight matrixM resultsin an unintuitive principal eigenvector.101840455055606570750.001  0.01  0.1  1  10  100  1000recalldiffusion factor  regularized Laplacianmost frequent senseFigure 5: Recall of the regularized Laplacian with a dif-ferent diffusion factor ?
on S3LS WSD taskEspresso).One of the disadvantages of the von Neumannkernels over the regularized Laplacian is their sen-sitivity to parameter ?.
Figure 5 illustrates the per-formance of the regularized Laplacian with a diffu-sion factor ?.
The regularized Laplacian is stable forvarious values of ?, while the von Neumann kernelschange their behavior drastically depending on thevalue of ?.
However, ?
in the von Neumann kernelsis upper-bounded by the reciprocal 1/?
of the prin-cipal eigenvalue of A, and the derivatives of kernelmatrices with respect to ?
can be used to guide sys-tematic calibration of ?
(see (Ito et al, 2005) fordetail).6 Conclusion and Future WorkThis paper gives a graph-based analysis of seman-tic drift in Espresso-like bootstrapping algorithms.We indicate that semantic drift in bootstrapping is aparallel to topic drift in HITS.
We confirm that thevon Neumann kernels and the regularized Laplacianreduce semantic drift in the Senseval-3 Lexical Sam-ple task.
Our proposed methods have only one pa-rameters and are easy to calibrate.Beside the regularized Laplacian, many other ker-nels based on the eigenvalue regularization of theLaplacian matrix have been proposed in machinelearning community (Kondor and Lafferty, 2002;Nadler et al, 2006; Saerens et al, 2004).
One suchkernel is the commute-time kernel (Saerens et al,2004) defined as the pseudo-inverse of Laplacian.Despite having no parameters at all, it has been re-ported to perform well in many collaborative filter-ing tasks (Fouss et al, 2007).
We plan to test thesekernels in our task as well.Another research topic is to investigate othersemi-supervised learning techniques such as co-training (Blum and Mitchell, 1998).
As we havedescribed in this paper, self-training can be thoughtof a graph-based algorithm.
It is also interesting toanalyze how co-training is related to the proposedalgorithm.Bootstrapping algorithms have been used in manyNLP applications.
Two major tasks of bootstrap-ping are word sense disambiguation and named en-tity recognition.
In named entity recognition task,instances are usually retained on each iteration andadded to seed instance set.
This seems to be be-cause named entity recognition suffers from seman-tic drift more severely than word sense disambigua-tion.
Even though this problem setting is differentfrom ours, it needs to be verified that the graph-based approaches presented in this paper are also ef-fective in named entity recognition.AcknowledgementsWe thank anonymous reviewers for helpful com-ments and for making us aware of Abney?s work.The first author is partially supported by the JapanSociety for Promotion of Science (JSPS), Grant-in-Aid for JSPS Fellows.ReferencesSteven Abney.
2004.
Understanding the Yarowsky Al-gorithm.
Computational Linguistics, 30(3):365?395.Eneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalle,and Aitor Soroa.
2006.
Two graph-based algorithmsfor state-of-the-art WSD.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing, pages 585?593.Michele Banko and Eric Brill.
2001.
Scaling to VeryVery Large Corpora for Natural Language Disam-biguation.
In Proceedings of the 39th Annual Meetingon Association for Computational Linguistics, pages26?33.Krishna Bharat and Monika R. Henzinger.
1998.
Im-proved algorithms for topic distillation in a hyper-linked environment.
In Proceedings of the 21st ACMSIGIR Conference.1019Avrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In Pro-ceedings of the Workshop on Computational LearningTheory (COLT), pages 92?100.
Morgan Kaufmann.Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual Web search engine.
Com-puter Networks and ISDN Systems, 30(1?7):107?117.Pavel Yu Chebotarev and Elena V. Shamis.
1998.
Onproximity measures for graph vertices.
Automationand Remote Control, 59(10):1443?1459.Michael Collins and Yoram Singer.
1999.
Unsuper-vised Models for Named Entity Classification.
In Pro-ceedings of the Joint SIGDAT Conference on Empiri-cal Methods in Natural Language Processing and VeryLarge Corpora, pages 100?110.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with Mutual Exclu-sion Bootstrapping.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 172?180.Franc?ois Fouss, Luh Yen, Pierr Dupont, and MarcoSaerens.
2007.
Random-walk computation of simi-larities between nodes of a graph with application tocollaborative recommendation.
IEEE Transactions onKnowledge and Data Engineering, 19(3):355?369.Marti Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In Proceedings ofthe Fourteenth International Conference on Computa-tional Linguistics, pages 539?545.Takahiko Ito, Masashi Shimbo, Taku Kudo, and YujiMatsumoto.
2005.
Application of Kernels toLink Analysis.
In Proceedings of the EleventhACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 586?592.Rie Johnson and Tong Zhang.
2007.
On the Effec-tiveness of Laplacian Normalization for Graph Semi-supervised Learning.
Journal of Machine LearningResearch, 8:1489?1517.Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.2002.
Learning Semantic Similarity.
In Advancesin Neural Information Processing Systems 15, pages657?664.Jon Kleinberg.
1999.
Authoritative Sources in a Hyper-linked Environment.
Journal of the ACM, 46(5):604?632.Mamoru Komachi and Hisami Suzuki.
2008.
Mini-mally Supervised Learning of Semantic Knowledgefrom Query Logs.
In Proceedings of the 3rd Inter-national Joint Conference on Natural Language Pro-cessing, pages 358?365.Risi Imre Kondor and John Lafferty.
2002.
Diffusionkernels on graphs and other discrete input spaces.
InProceedings of the 19th International Conference onMachine Learning (ICML-2002).Hang Li and Cong Li.
2004.
Word Translation Disam-biguation Using Bilingual Bootstrapping.
Computa-tional Linguistics, 30(1):1?22.Boaz Nadler, Stephane Lafon, Ronald Coifman, andIoannis Kevrekidis.
2006.
Diffusion maps, spectralclustering and eigenfunctions of fokker-planck opera-tors.
Advances in Neural Information Processing Sys-tems 18, pages 955?962.Vincent Ng and Claire Cardie.
2003.
Weakly Su-pervised Natural Language Learning Without Redun-dant Views.
In Proceedings of the HLT-NAACL 2003,pages 94?101.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging Generic Patterns for Automatically Har-vesting Semantic Relations.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the ACL, pages113?120.Ellen Riloff and Rosie Jones.
1999.
Learning Dic-tionaries for Information Extraction by Multi-LevelBootstrapping.
In Proceedings of the Sixteenth Na-tional Conference on Artificial Intellligence (AAAI-99), pages 474?479.Marco Saerens, Franc?ois Fouss, Luh Yen, and PierreDupont.
2004.
The principal component analysisof a graph, and its relationship to spectral clustering.In Proceedings of European Conference on MachineLearning (ECML 2004), pages 371?383.
Springer.Heinrich Schu?tze.
1998.
Automatic Word Sense Dis-crimination.
Computational Linguistics, 24(1):97?123.Alex J. Smola and Risi Imre Kondor.
2003.
Kernels andRegularization of Graphs.
In Proceedings of the 16thAnnual Conference on Learning Theory, pages 144?158.Jean Ve?ronis.
2004.
HyperLex: Lexical Cartography forInformation Retrieval.
Computer Speech & Language,18(3):223?252.David Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In Pro-ceedings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 189?196.1020
