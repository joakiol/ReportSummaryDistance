Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 468?477,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPMulti-Word Expression Identification Using Sentence Surface FeaturesRam BoukobzaSchool of Computer ScienceHebrew University of Jerusalemram.boukobza@mail.huji.ac.ilAri RappoportSchool of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractMuch NLP research on Multi-Word Ex-pressions (MWEs) focuses on the discov-ery of new expressions, as opposed to theidentification in texts of known expres-sions.
However, MWE identification isnot trivial because many expressions al-low variation in form and differ in therange of variations they allow.
We showthat simple rule-based baselines do notperform identification satisfactorily, andpresent a supervised learning method foridentification that uses sentence surfacefeatures based on expressions?
canonicalform.
To evaluate the method, we haveannotated 3350 sentences from the BritishNational Corpus, containing potential usesof 24 verbal MWEs.
The method achievesan F-score of 94.86%, compared with80.70% for the leading rule-based base-line.
Our method is easily applicable toany expression type.
Experiments in pre-vious research have been limited to thecompositional/non-compositional distinc-tion, while we also test on sentences inwhich the words comprising the MWE ap-pear but not as an expression.1 IntroductionMulti-Word Expressions (MWEs) such as ?pullstrings?, ?make a face?
and ?get on one?s nerves?are very common in language.
Such MWEs canbe characterized as being non-compositional: themeaning of the expression does not transparentlyfollow from the meaning of the words that com-prise it.
Much of the work on MWEs in NLP hasbeen in MWE extraction ?
the discovery of newMWEs from a corpus, using statistical and othermethods.
Identification of known MWEs in texthas received less attention, but is necessary formany NLP applications, for example in machinetranslation.
The current work deals with the MWEidentification task: deciding if a sentence containsa use of a known expression.MWE identification is not as simple as may ini-tially appear, as will be shown by the performanceof two rule-based baselines in our experiments.One source of difficulty is variations in expres-sions?
usage in text.
Although MWEs generallyshow less variation than single words, they showenough that it cannot be ignored.
In a study onV+NP idioms, Riehemann (2001) found that theidioms?
canonical form accounted for 75% of theirappearances in a corpus.
Additionally, expressionsdiffer considerably in the types of variations theyallow, which include passivization, nominalizationand addition of modifying words (Moon, 1998).A second source of difficulty is that expressionsconsisting of very frequent words will often co-occur in sentences in a non-MWE usage and insimilar but distinct expressions.MWE identification can be modeled as a twostep process.
Given a sentence and a known ex-pression, step (1) is to decide if the sentence con-tains a potential use of the expression.
This is arelatively simple step based on the appearance inthe sentence of the words comprising the MWE.Step (2) is to decide if the potential use is indeednon-compositional.
Consider the following sen-tences with regard to the expression hit the road,meaning ?to leave on a journey?
:(a) ?At the time, the road was long and difficultwith few travelers daring to take it.?
(b) ?The headlights of the taxi-van behind us468flashed as it hit bumps in the road.?
(c) ?The bullets were hitting the road and I couldsee them coming towards me a lot faster thanI was able to reverse.?
(d) ?Lorry trailers which would have been hittingthe road tomorrow now stand idle.
?Sentence (a) does not contain a potential use ofthe expression due to the missing component ?hit?.Each of (b)-(d) does contain a potential use of theexpression.
In (b) all of the expression compo-nents are present, but they do not form an expres-sion.
In (c), the words form an expression, butwith a compositional (literal) meaning.
Only (d)contains a non-compositional use of hit the road.The task we address in this paper is to identifywhether or not we are in case (d), for a given ex-pression in a given sentence.To date, most work in MWE identification hasfocused on manually encoding rules that identifyexpressions in text.
The encodings, usually con-sisting of regular expressions and syntactic struc-tures, are intended to contain all the necessary in-formation for processing the MWE in text.
Beingmanual, this is time-consuming work and requiresexpert knowledge of individual expressions.
Interms of the above model, such encodings handleboth MWE identification steps.A second approach is to use machine learningmethods to learn an expression?s behavior from acorpus.
Studies taking this approach have focusedon distinguishing between compositional and non-compositional uses of an expression (cases (c) and(d) above).
As will be detailed in Section 2, exist-ing methods are tailored to an expression?s type,and experiment with a single MWE pattern.
In ad-dition, the training and test sets they used did notcontain non-expression uses as in case (b), whichcan be quite common in practice.Our approach is more general.
Given a set ofsentences with potential MWE uses, we use sen-tence surface features to create a Support Vec-tor Machine (SVM) classifier for each expres-sion.
The classifier is binary and differentiates be-tween non-compositional uses of the expression((d) above) on the one hand, and compositionaland non-expression uses ((b) and (c)) on the other.The experiments and results presented below fo-cus on verbal MWEs, since verbal MWEs arequite common in language use and have also beeninvestigated in related MWE research (e.g., (Cooket al, 2007)).
However, the developed features arenot specific to a particular type of expression.The supervised method is compared with twosimple rule-based baselines in order to testwhether a simple approach is sufficient.
In addi-tion, the use of surface features is compared withthe use of syntactic features (based on dependencyparse trees of the sentences).
Averaged over ex-pressions in an independent test set, the super-vised classifiers outperform the rule-based base-lines, with F-scores of 94.86% (surface features)and 87.77% (syntactic features), compared with80.70% for the best baseline.Section 2 reviews previous work.
Section 3 dis-cusses the features used for the supervised classi-fier.
Section 4 explains the experimental setting.The results and a discussion are given in sections5 and 6.2 Previous Work2.1 MWE Lexical EncodingThe approach to handling MWEs in early systemswas to employ a list of expressions, each witha quasi regular expression that encodes morpho-syntactic variations.
One example is Leech etal.
(1994) who used this method for automaticpart-of-speech tagging for the BNC.
Another is aformalism called IDAREX (IDioms And RegularEXpressions) (Breidt et al, 1996).More recent research emphasizes the integra-tion of MWE lexical entries into existing singleword lexicons and grammar systems (Villavicen-cio et al, 2004; Alegria et al, 2004).
There isalso an attempt to take advantage of regularities inmorpho-syntactic properties across MWE groups,which allows encoding the behavior of the groupinstead of individual expressions (Villavicencio etal., 2004; Gr?egoire, 2007).
Fellbaum (1998) dis-cusses some difficulties in representing idioms,which are largely figurative in meaning, in Word-Net.
More recent work (Fellbaum et al, 2006) fo-cuses on German VP idioms.As already mentioned, one issue with lexi-cal encoding is that it is done manually, mak-ing lexicons difficult to create, maintain and ex-tend.
The use of regularities among different typesof MWEs is one way of reducing the amountof work required.
A second issue is that im-plementations tend to ignore the likelihood andeven the possibility of compositional and otherinterpretations of expressions in text, which can469be common for some expressions.
For exam-ple, in an MWE identification study, Hashimotoet al (2006) built an identification system us-ing hand crafted rules for some 100 Japanese id-ioms.
The results showed near perfect perfor-mance on expressions without compositional/non-compositional ambiguity but significantly poorerperformance on expressions with ambiguity.2.2 MWE Identification by MLKatz and Giesbrecht (2006) used a supervisedlearning method to distinguish between composi-tional and non-compositional uses of an expres-sion (in German text) by using contextual infor-mation in the form of Latent Semantic Analy-sis (LSA) vectors.
LSA vectors of compositionaland non-compositional meaning were built from atraining set of example sentences and then a near-est neighbor algorithm was applied on the LSAvector of one tested MWE.
The technique wastested more thoroughly in Cook et al (2007).Cook et al (2007) devised two unsupervisedmethods to distinguish between compositional (lit-eral) and non-compositional (idiomatic) tokens ofverb-object expressions.
The first method is basedon an expression?s canonical form.
In a previ-ous study (Fazly and Stevenson, 2006), the authorscame up with a dozen possible syntactic forms forverb-object pairs (based on passivization, deter-miner, and object pluralization) and used a corpus-based statistical measure to determine the canoni-cal form(s).
The method classifies new tokens asidiomatic if they use a canonical form, and literalotherwise.The second method uses context as well asform.
Co-occurrence vectors representing the id-iomatic and literal meaning of each expressionwere computed based on corpus data.
Idiomatic-meaning vectors were based on examples match-ing the expressions?
canonical form.
Literal mean-ing vectors were based on examples that did notmatch the canonical form.
New tokens wereclassified as literal/idiomatic based on their (co-occurrence) vector?s cosine similarity to the id-iomatic and literal vectors.
(Sporleder and Li, 2009) also attempted to dis-tinguish compositional from non-compositionaluses of expressions in text.
Their assumption wasthat if an expression is used literally, but not id-iomatically, its component words will be relatedsemantically to several words in the surroundingdiscourse.
For example, when the expression ?playwith fire?
is used literally, words such as ?smoke,?burn?, ?fire department?, and ?alarm?
tend to alsobe used nearby; when it is used idiomatically, theyaren?t (indeed, other words, e.g., ?danger?
or ?risk?appear nearby but they are not close semanticallyto ?play?
or to ?fire?).
This property was usedto distinguish literal and non-literal instances bymeasuring the semantic relatedness of an expres-sion?s component words to nearby words in thetext.
If one or more of the expression?s compo-nents were sufficiently related to enough nearbywords, forming a ?lexical chain?, the usage wasclassified as literal.
Otherwise it was idiomatic.Two classifiers based on lexical chains were de-vised.
These were compared with a supervisedmethod that trains a classifier for each expressionbased on surrounding context.
The results showedthat the supervised classifier method did much bet-ter (90% F-score on literal uses) than the lexicalchain classifier methods (60% F-score).In the above studies the focus is on thecompositional/non-compositional expression dis-tinction.
The sentence data used contains exam-ples of either one or the other.
In (Sporleder andLi, 2009) the experimental data included only sen-tences in which the expressions were in canoni-cal form (allowing for verb inflection).
In (Cooket al, 2007) a syntactic parser was used to col-lect sentences containing the MWEs in the activeand passive voice using heuristics.
Thus, exam-ples such as the following (from the BNC) wouldnot be included in their sample:1. take a chance: ?While he still had a chanceof being near Maisie, he would take it?.2.
face the consequences: ?.
.
.
she did not haveto face, it appears, the possible serious oreven fatal consequences of her decision?.3.
make a distinction: ?Logically, the distinc-tion between the two aspects of the theorycan and should be made?.4.
break the ice: ?The ice, if not broken, wasbeginning to soften a little?.5.
settle a score: ?Morrissey had another scoreto settle?.This means that their experiments have not in-cluded all types of sentences that might be encoun-tered in practice when attempting MWE identifi-470cation.
Specifically, they would miss many ex-amples in which the MWE words are present butare not used as an expression (case (b) in Sec-tion 1).
Moreover, their heuristics are tailoredto the Verb-Direct Object MWE type.
Differentheuristics would need to be employed for differentMWE types.In our approach there is no pre-processing stagerequiring type-specific knowledge.
Specifically,the above examples are used as training sentencesin our experiments.2.3 MWE ExtractionThere exists an extensive body of research onMWE extraction (see Wermter and Hahn (2004)for a review), where the only input is a corpus,and the output is a list of MWEs found in it.
Mostmethods collect MWE candidates from the corpus,score them according to some association measurebetween their components, and accept candidateswith scores passing some threshold.
The focus ofresearch has been on developing association mea-sures, including statistical, information-theoreticand linguistically motivated measures (e.g., Juste-son and Katz (1995), Wermter and Hahn (2006),and Deane (2005)).3 MWE Identification MethodOur method decides if a potential use of aknown expression in a given sentence is non-compositional.
The input to the method, for eachMWE, is a labeled training set of sentences con-taining one or more potentially non-compositionaluses of the MWE.
The output, for each MWE, is abinary classifier, trained on those sentences.
Thus,we target step (2) of MWE identification, which isthe difficult one.The learning algorithm used is Support VectorMachine (SVM), which outputs a binary classifier,using Sequential Minimal Optimization (Platt,1998)1in the Weka toolkit2(Witten and Frank,2000).For training, sentences are converted into fea-ture vectors.
Features depend on the assignmentof the lexical components of the expression to spe-cific tokens in the sentence.
In some cases, thereare several tokens in the sentence that match a sin-gle component in the expression, and this leads to1Using the PUK kernel (The Pearson VII function-based Universal Kernel), with parameters omega=1.0 andsigma=1.0.2Weka version 3.5.6; www.cs.waikato.ac.nz/ ml/ weka/multiple (potential) assignments.
So in the gen-eral case a sentence is converted to a set of featurevectors, each corresponding to a single assignmentof the MWE?s lexical components to sentence to-kens.Training sentences are labeled positive if theycontain a non-compositional use of the expressionand negative if they do not (i.e., literal and otheruses).
If the sentence is positive, at least one ofthe assignments is the true assignment (there maybe more than one, e.g., when an expression is usedtwice in the same sentence).
The vector matchingthe true assignment is labeled positive.
The othersare labeled negative.
If the sentence is negative,all of the vectors are labeled negative.As mentioned, the output of the method is adistinct binary classifier for each MWE.
Althoughhaving a single classifier for all expressions wouldseem advantageous, the wide variation exhibitedby MWEs (e.g., for some the passive is common,for other not at all) precludes this option and re-quires having a separate classifier for each expres-sion.3.1 FeaturesSurface features include order and distance, part-of-speech and inflection of an expression?s wordsin a sentence.Use of surface features is intuitive and relativelycheap.
In addition, many studies have shown theimportance of order and distance in MWE extrac-tion in English (two recent examples are (Dias,2003; Deane, 2005)).
Thus, we develop a super-vised classifier based on surface features.Many of the surface features make use of anexpression?s Canonical Form (CF), thus the learn-ing algorithm assumes that it is given such a form.Formally defining the CF is difficult.
Indeed, someresearchers have concluded that some expressionsdo not have a CF (Moon, 1998).
For our purposes,CF can be informally defined as the most frequentform in which the expression appears.
In practice,an approximation of this definition, explained inSection 4, is used.3.1.1 Surface Features1.
Word Distance: The number of words be-tween the leftmost and rightmost MWE to-kens in the sentence.2.
Ordered Gap List: A list of gaps, measuredin number of words, between each pair of the471expression?s tokens in their canonical formorder.
For example, if the token locations (incanonical form order) are 10, 7 and 3, the or-dered gap list would be (10 ?
7 = 2, 10 ?3 = 6, 7?
3 = 3).3.
Word Order: A boolean value indicatingwhether the expression?s word order in thesentence matches the canonical form wordorder.4.
Word Order Permutation: The permutationof word order relative to the canonical form.For example, the permutation (1,0,2) indi-cates that component words 1 and 0 haveswitched order in the sentence.5.
Inflection Ratio: The fraction of words in theexpression that have undergone inflection rel-ative to the canonical form.6.
Lexical Values: A list of the tokens in thesentence matching the expression?s compo-nent words, ordered according to canonicalform.
For example, if the expression is ?makea distinction?, a possible lexical values listis (made,no,distinction) in the sentence ?Nopossible distinction can be made between thetwo?.7.
POS Pattern: A boolean value indicatingwhether the expression?s use in the sentencehas the same part-of-speech pattern as thecanonical form.Two combinations of surface features are usedin the experiments below.
The first, named R1,uses all of the above features.
The second, R2,uses only Word Distance, Ordered Gap List andWord Order Permutation.
Using R2 the learnerhas only word order and distance information fromwhich to create a classifier.3.1.2 Syntactic FeaturesAn expression?s words may appear unrelated ina sentence, because of distance, order, part-of-speech and other surface variations.
However, thewords will still be closely related syntactically.Syntactic analysis of the sentence in the form ofa dependency parse tree directly gives the syntac-tic relationships between the expression?s compo-nents.
Thus, we also develop a classifier based onsyntactic features.Dependency Parsing.
A dependency parse treeis a directed acyclic graph in which the nodes rep-resent tokens in the sentence and the edges rep-resent syntactic dependencies between the words(e.g., direct-object, prepositional-object, noun-subject etc.).
The Stanford Parser3(Marneffe etal., 2006) was used.Minimal Sub-Tree.
To compute a syntactic fea-ture, the dependency tree is computed and then theminimal sub-tree containing the expression?s to-kens is extracted.The features are:1.
Sub-Tree Distance Sum: The number ofedges in the minimal sub-tree.
A large num-ber of edges suggests a weaker dependency.2.
Sub-Tree Distance List: A list of the dis-tances of the MWE component nodes fromthe root of their sub-tree.3.
Descendant Relations List: A list of descen-dant relations between each pair of MWEcomponent nodes.A descendant relation between two nodes ex-ists if there is a directed path from one node(the ancestor) to the other (the descendant).Descendant relations are either direct (parent-child) or indirect.
The list consists of the lev-els of descendant relations between the MWEcomponent nodes, which can be none, indi-rect or direct.4.
Descendant Direction List: A list of the di-rections of the descendant relations betweeneach pair of MWE component nodes.If there are descendant relations between apair of nodes, the direction of the depen-dency, indicating which is the modifying andwhich the modified node, is important.5.
Sibling Relations List: A list of sibling rela-tions between each pair of MWE componentnodes.Two nodes are first degree siblings if theyshare the same parent (which usually meansthey modify the same word).
Two nodes aresecond degree siblings if they share a com-mon ancestor no more than two edges away,and so on.
The list consists of the level ofsibling relations for each pair of component3http://www-nlp.stanford.edu/software/lex-parser.shtml472nodes, which can be first, second and thirddegree.6.
Descendant Type List: A list of the depen-dency types (e.g., subject, direct object etc.
)between each pair of component nodes.
If thecomponent nodes are not direct descendantstheir dependency type is null.7.
Sibling Type List: A list of pairs of depen-dency types corresponding to the dependen-cies between a pair of component nodes andtheir common parent.
If the component nodesare not first degree siblings, the type is null.In the experiments reported below, the classifierusing only the syntactic features is denoted by S,and the one using all surface and all syntactic fea-tures is denoted by C. We have experimented withadditional feature combinations, with no improve-ment in results.4 Experimental MethodCanonical form.
As described, an expression?scanonical form (CF) is used in many of the learn-ing algorithm?s features.
The CF is taken fromCollins COBUILD Advanced Learner?s EnglishDictionary (2003) which is also used as our sourcefor MWEs.
COBUILD is an English-English dic-tionary based on the Bank of English (BOE) cor-pus (over 520 million words) with approximately34,000 entries.Traditional single-word dictionaries are a goodsource for expressions because they usually list, aspart of single-word entries, expressions in whichthe word is a component.
The CF is not explic-itly given in COBUILD, so an approximation isthe form which appears in the expression?s defini-tion.
This is a reasonable approximation since theCOBUILD authors claim to have selected typicaluses of the expressions in their definitions.Each CF also has a matching part-of-speech(POS) pattern, which is a list of the parts-of-speech of the components in the CF.
Forexample, ?walking on air?
has the pattern(V erb, Preposition,Noun).
COBUILD doesnot include part-of-speech information for expres-sions so this information was determined using theBritish National Corpus (BNC) (BNC, 2001), a(mostly) automatically POS tagged corpus (usingthe CLAWS tagger).
For each MWE, the POS pat-terns of all instances of the CF in the corpus werecounted.
The most frequent pattern is the expres-sion?s POS pattern.The expressions.
A set of 17 verbal MWEs, thedevelopment set, was used for development of thesurface and syntactic features described above.
Allof the development set MWEs had the POS pattern(V erb,Determiner,Noun).
Another set of 24verbal MWEs, the training/test set4, was then usedto test the method.
Because the method is not spe-cific to the (V erb,Determiner,Noun) pattern,new POS patterns are included in the training/testset.
The training/test set consists of 8 MWEsof the POS pattern (V erb,Determiner,Noun),7 (V erb, Preposition,Noun) MWEs and and 9(V erb,Noun, Preposition) MWEs.
The list ofMWEs was selected randomly from the corre-sponding POS pattern types.
MWEs with a pos-itive or negative percentage of under 5% in theirdata set were discarded5.
The MWEs, in theircanonical form, are:Development set:(V erb,Determiner,Noun) [17]: break the ice,calls the shots, catch a cold, clear the air, facethe consequences, fits the bill, hit the road, makea face, make a distinction, makes an impression,raise the alarm, set an example, sound the alarm,stay the course, take a chance, take the initiative,tie the knot.Training/test set:(V erb,Determiner,Noun) [8]: changes thesubject, get a grip, get the picture, lead the way,makes the grade, sets the scene, take a seat, takethe plunge;(V erb, Preposition,Noun) [7]: fall into place,goes to extremes, brought to justice, take to heart,gets on nerves, keep up appearances, comes tolight;(V erb,Noun, Preposition) [9]: take aim at,make allowances for, takes advantage of, keephands off, lay claim to, take care of, make contactwith, gives rise to, wash hands of.The sentences.
As mentioned, the first step ofMWE identification is to identify if the sentencecontains a potential non-compositional use of theexpression.
In order to test our method, which tar-gets step (2), a set of such sentences (for each ex-pression) was collected from the BNC corpus and4Using 10-fold cross validation.5Initially there were 20 MWEs in the development set and30 (10 per group) in the training/test set.473then labeled for use as training/test sentences6.The collection method was intended to allowa wide range of variations in expression use.
Inpractice, for each expression sentences contain-ing all of the expression?s CF components, in anyof their inflections, were collected, but excludingcommon auxiliary words.
So for example, whentargeting the MWE ?make an impression?
we al-lowed inflections of ?make?
and ?impression?
anddid not require ?an?, to allow for variations suchas ?make no impression?
and ?make some impres-sion?.
For some expressions, sentences were lim-ited to those with a distance of up to 8 words be-tween each expression component.
Very long sen-tences (above 80 words) were discarded.
The finalset of sentences was then randomly selected.Given this method, training/test sentences al-low non-lexical variations: inflection, word or-der, part-of-speech, syntactic structure and othernon-syntactic transformations.
Lexical variationswhich involve a change in one of the expression?scomponents are not allowed, except for commonauxiliary words.For the development set an average of 97 (40-137) sentences were collected per MWE, giving atotal of 1663 sentences, with a micro average of49% positive labels.
For the training/test set therewere 139 (73-150) sentences per MWE on aver-age, totaling 3350, with a 40% average positiveratio.The sentences were manually labeled as posi-tive if they contained a non-compositional use ofthe MWE and negative if they contained a compo-sitional or non-expression usage.
Judgment wasbased on a single sentence, without wider context.Baseline methods.
Two baseline methods areused to test the intuitive notion that simple rule-based methods are sufficient for MWE identifica-tion as well as for comparison with the supervisedlearning methods.The first method, CanonicalForm (CF), acceptsa sentence use as a non-compositional MWE useif and only if the MWE is in canonical form (thereare no intervening words between the MWE com-ponents, their order matches canonical-form order,and there is an inflection in at most one componentword).The second method, DistanceOrder (DO), ac-6The PyLucene software package, http://pylucene.
os-afoundation.
org/, was used for building an index to the BNCand for searching.CF DO R1 R2 S CVerb-Det-Noun: All (17)A 73.53 82.27 89.48 90.83 88.58 87.02P 97.09 89.29 82.71 87.18 83.89 78.54R 58.81 76.83 92.29 90.35 92.97 97.19F 67.39 79.68 86.92 88.56 87.78 86.00Verb-Det-Noun: Best (8)A 84.51 91.56 95.33 95.48 92.52 93.27P 95.90 85.70 92.50 95.63 91.12 87.63R 73.50 89.80 97.25 95.25 95.83 98.50F 78.63 86.29 94.70 95.36 93.44 92.25Table 1: Development set: Average performance over allMWEs and best 8.
Supervised classifiers outperform base-lines.
A: Accuracy; P: Positive Precision; R: Positive Recall;F: F-Score.cepts a sentence use if and only if the number ofwords between the leftmost and rightmost MWEcomponents is less than or equal to 2 (not count-ing the middle MWE component), and if the ordermatches the canonical form order.5 ResultsThe baseline methods (CF and DO) and the super-vised methods (R1,R2,S,C) were run on the devel-opment and training/test sets.
For the supervisedmethods, for each MWE we used 10-fold cross-validation7.Tables 1 and 2 summarize the results for the de-velopment and test sets, respectively.
For the de-velopment set, average results over all 17 MWEsand over the best 8 MWEs (on R1), a group sizecomparable to the test set, are shown.
For the testset, results over all 24 MWEs and the three MWEtypes tested are shown.The tables show average overall accuracy andaverage precision, recall and F-score on posi-tive instances, where the averages are taken overthe results of the individual MWEs (i.e., micro-averaged).Baselines.
Baseline accuracy, (for DO) 82.27%on the development set and 87.2% on the test set(over all groups), is probably insufficient for manyNLP applications.The baselines perform similarly in terms of av-erage accuracy.
CF does this with very high preci-sion and low recall, while for DO recall improvesat the expense of precision.
Looking at individ-ual MWEs reveals that for expressions which al-low more variation in terms of intervening words7I.e., we ran 10 experiments where in each experiment wedivided the corresponding annotated sentence sets into 90%training sentences and 10% test sentences, and the results re-ported are the average of the 10 experiments.474CF DO R1 R2 S CAll (24)A 86.16 87.15 93.50 91.61 89.73 91.50P 94.16 80.38 93.08 93.16 89.86 89.26R 68.86 86.88 93.00 89.74 88.94 93.33F 75.53 80.70 94.86 93.09 87.77 92.80Verb-Det-Noun (8)A 89.08 89.08 93.83 93.65 90.07 91.33P 95.44 84.13 92.88 94.00 91.04 89.25R 73.30 88.53 97.50 95.50 91.57 97.63F 80.97 84.91 95.09 94.71 91.21 93.08Verb-Prep-Noun (7)A 85.53 91.15 93.64 92.62 88.75 92.10P 97.13 81.40 96.81 97.20 92.48 94.33R 64.36 92.67 84.73 82.79 82.71 85.00F 74.08 86.03 97.81 96.87 83.13 96.65Verb-Noun-Prep (9)A 84.06 82.32 93.11 88.99 90.18 91.18P 90.72 76.26 90.78 89.73 86.78 85.89R 68.41 80.90 95.44 90.03 91.44 96.00F 71.82 72.82 92.69 89.14 88.33 89.99Table 2: Test set: Average performance over all MWEs andby group.
The best supervised classifier outperforms base-lines in all groups.
A: Accuracy; P: Precision; R: Recall; F:F-Score.and lexical change, DO outperforms CF.
To namea few, make an impression, raise the alarm, takea chance and make allowances for.
For example,for take a chance intervening words are quite com-mon, as in: ?I?m taking a real chance on you.
?, ora change in determiner as in: ?I preferred to takemy chances?.
Indeed, CF showed poor precisiononly for MWEs with a common literal usage.
Twosuch MWEs were present in the development set(break the ice and tie the knot ) and two in the testset (wash hands of and keep hands off).Baselines versus supervised classifiers.
Asshown in the tables, R1 outperforms the best base-line in terms of accuracy in both test and devel-opment.
Moreover, the supervised classifiers aremore stable in their accuracy.
For the develop-ment set the standard deviation of accuracy scoresaverages 22.58 for CF and DO, and 6.68 for R1,R2, S, and C. For the test set the baselines av-erage 9.07 (Verb-Det-Noun), 11.11 (Verb-Prep-Noun) and 14.26 (Verb-Noun-Prep), and the su-pervised methods average 4.97 (Verb-Det-Noun),7.66 (Verb-Prep-Noun) and 7.97.
This stabilitymeans that the supervised classifiers are able toperform well on MWEs with different behavior.For example, R1 is able to perform well on ex-pressions where order is strict, as DO does (e.g.,make a face), while also performing well on thosewhere order varies (e.g., make a distinction).Supervised classifiers.
R1 and R2, based onsurface features, show similar accuracy values,with R1 doing somewhat better in the Verb-Prep-Noun and Verb-Noun-Prep groups.
This is dueto the Lexical Values feature, which accounts fora change in preposition.
A change in preposi-tion (as in ?wash hands of some matter?
versus?wash hands in the sink?)
is more significant thana change in determiner in the Verb-Determiner-Noun group.
This improves precision on negativeinstances, which are rejected more precisely basedon the preposition value.
Nevertheless, the rela-tively simple features in R2, essentially order anddistance, perform quite well.The F-score result for R1, 94.86, is an improve-ment over the F-score result of the supervised clas-sifier used in (Sporleder and Li, 2009), 90.15.Although the sentence data is different (our dataincludes sentences with non-expression uses) thenumber of sentences used is similar.S, based on syntactic features, performs worsethan R1/2.
It shows better accuracy than the base-lines in all but the (Verb-Prep-Noun) group andis also more stable.
C, a combination of surfaceand syntactic features, performs better than S andslightly worse than R1/2.Why do the syntactic features perform worsethan surface features?
An analysis of the S clas-sifier errors reveals two important causes.
First,there is substantial variation in the dependencytree structures of the non-compositional uses ofthe expressions as output by the parser.
Thus,the syntactic feature classifier was more difficultto learn than the surface feature one, requiring alarger training set.
This is not surprising, giventhat many MWEs exhibit an irregular syntactic be-havior that might even seem strange at times.
Forexample, in the sentence fragment ?and then hecame to.
?, ?came to?
is an MWE.
A parser mightfind it difficult to parse the sentence correctly, ex-pecting a noun phrase to follow the ?to?.Second, as described above, the syntactic fea-tures consist of general syntactic relations ex-tracted from the parse tree and not type-specificknowledge.
As a result, literal or non-expressionuses of the MWE?s components, which have aclose syntactic relation in a given sentence, appearas non-compositional uses of the expression to theclassifier.4756 DiscussionThis study has addressed MWE identification: de-ciding if a potential use of an expression is a non-compositional one.
Despite its importance in ba-sic NLP tasks, the problem has been largely over-looked in NLP research, probably due to it pre-sumed simplicity.
However, as we have shown,simple methods for MWE identification, such asour baselines, do not perform consistently wellacross MWEs.
This study serves to highlight thispoint and the need for more sophisticated methodsfor MWE identification.We have shown that using a supervised learningmethod employing surface sentence features basedon canonical form, it is possible to improve perfor-mance significantly.
Unlike previous research, ourmethod is not tailored to specific MWE types, andwe did not ignore non-expression uses in our ex-periments.Future research should experiment with non-verbal MWEs, since our features are not spe-cific to verbal MWE types.
Another direction isa more sophisticated corpus sampling algorithm.The current work ignored MWEs which had an un-balanced training set (usually too few positives).Methods for gathering enough positive instancesof such MWEs will be useful for testing the meth-ods proposed here, as well as for general MWEresearch.ReferencesI?nki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,Koldo Gojenola and Ruben Urizar.
2004.
Repre-sentation and treatment of multiword expressions inBasque.
ACL ?04 Workshop on Multiword Expres-sions.The British National Corpus.
2001.
The BritishNational Corpus, version 2 (BNC World).
Dis-tributed by Oxford University Computing Ser-vices on behalf of the BNC Consortium.
URL:http://www.natcorp.ox.ac.uk/Elisabeth Breidt, Frederique Segond, and GiuseppenValetto.
1996.
Local grammars for the descriptionof multi-word lexemes and their automatic recogni-tion in texts.
COMPLEX ?96.
Budapest.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntacticforms for the automatic identification of idiomaticexpressions in context.
ACL ?07 Workshop on ABroader Perspective on Multiword Expressions.Collins COBUILD.
2003.
Collins COBUILD Ad-vanced Learner?s English Dictionary.
Harper-Collins Publishers, 4th edition.Paul Deane.
2005.
A nonparametric method for ex-traction of candidate phrasal terms.
ACL ?05.Gael Dias.
2003.
Multiword unit hybrid extrac-tion.
ACL ?03 Workshop on Multiword Expressions:Analysis, Acquisition and Treatment.Afsaneh Fazly and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
EACL ?06.Christiane Fellbaum, Alexander Geyken, Axel Herold,Fabian Koerner, and Gerald Neumann.
2006.Corpus-based studies of German idioms and lightverbs.
International Journal of Lexicography,19(4):349?361.Christiane Fellbaum.
1998.
Towards a representationof idioms in WordNet.
COLING-ACL ?98 Workshopon the Use of WordNet in Natural Language Pro-cessing Systems.Nicole Gr?egoire.
2007.
Design and implementation ofa lexicon of Dutch multiword expressions.
ACL ?07Workshop on A Broader Perspective on MultiwordExpressions.Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.2006.
Japanese idiom recognition: Drawing a linebetween literal and idiomatic meanings.
COLING-ACL ?06, Poster Sessions.John S. Justeson and Slava M. Katz.
1995.
Technicalterminology: some linguistic properties and an al-gorithm for identification in text.
Natural LanguageEngineering, 1:9?27.Graham Katz and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.COLING-ACL ?06 Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties..Geoffrey Leech, Roger Garside and Michael Bryant.1994.
CLAWS4: The tagging of the British Na-tional Corpus.
COLING ?94.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.LREC ?06.Rosamund Moon.
1998.
Fixed Expressions and Id-ioms in English.
Oxford: Clarendon Press.John Platt.
1998.
Machines using sequential minimaloptimization.
In In B. Schoelkopf and C. Burges andA.
Smola, editors, Advances in Kernel Methods ?Support Vector Learning.476Susanne Z. Riehemann.
2001.
A Constructional Ap-proach to Idioms and Word Formation.
Ph.D. Thesis.Stanford.Caroline Sporleder and Linlin Li.
2009.
Unsupervisedrecognition of literal and non-literal use of idiomaticexpressions.
EACL ?09.Aline Villavicencio, Ann Copestake, Benjamin Wal-dron, and Fabre Lambeau.
2004.
Lexical encodingof MWE.
ACL ?04 Workshop on Multiword Expres-sions.Joachim Wermter and Udo Hahn.
2004.
Collocationextraction based on modifiability statistics.
COL-ING ?04.Joachim Wermter and Udo Hahn.
2006.
You can?t beatfrequency (unless you use linguistic knowledge) ?a qualitative evaluation of association measures forcollocation and term extraction.
COLING-ACL ?06.Ian H. Witten amd Eibe Frank.
2000.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann.477
