Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 224?231,Sydney, July 2006. c?2006 Association for Computational LinguisticsEfficient Search for Inversion Transduction GrammarHao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractWe develop admissible A* search heuris-tics for synchronous parsing with Inver-sion Transduction Grammar, and presentresults both for bitext alignment and formachine translation decoding.
We alsocombine the dynamic programming hooktrick with A* search for decoding.
Thesetechniques make it possible to find opti-mal alignments much more quickly, andmake it possible to find optimal transla-tions for the first time.
Even in the pres-ence of pruning, we are able to achievehigher BLEU scores with the same amountof computation.1 IntroductionThe Inversion Transduction Grammar (ITG) ofWu (1997) is a syntactically motivated algorithmfor producing word-level alignments of pairs oftranslationally equivalent sentences in two lan-guages.
The algorithm builds a synchronous parsetree for both sentences, and assumes that the treeshave the same underlying structure but that the or-dering of constituents may differ in the two lan-guages.
ITG imposes constraints on which align-ments are possible, and these constraints havebeen shown to be a good match for real bitext data(Zens and Ney, 2003).A major motivation for the introduction of ITGwas the existence of polynomial-time algorithmsboth for alignment and translation.
Alignment,whether for training a translation model using EMor for finding the Viterbi alignment of test data,is O(n6) (Wu, 1997), while translation (decod-ing) is O(n7) using a bigram language model, andO(n11) with trigrams.
While polynomial-time al-gorithms are a major improvement over the NP-complete problems posed by the alignment modelsof Brown et al (1993), the degree of these polyno-mials is high, making both alignment and decod-ing infeasible for realistic sentences without verysignificant pruning.
In this paper, we explore useof the ?hook trick?
(Eisner and Satta, 1999; Huanget al, 2005) to reduce the asymptotic complexityof decoding, and the use of heuristics to guide thesearch.Our search heuristics are a conservative esti-mate of the outside probability of a bitext cell inthe complete synchronous parse.
Some estimateof this outside probability is a common elementof modern statistical (monolingual) parsers (Char-niak et al, 1998; Collins, 1999), and recent workhas developed heuristics that are admissible for A*search, guaranteeing that the optimal parse willbe found (Klein and Manning, 2003).
We extendthis type of outside probability estimate to includeboth word translation and n-gram language modelprobabilities.
These measures have been used toguide search in word- or phrase-based MT sys-tems (Wang and Waibel, 1997; Och et al, 2001),but in such models optimal search is generally notpractical even with good heuristics.
In this paper,we show that the same assumptions that make ITGpolynomial-time can be used to efficiently com-pute heuristics which guarantee us that we willfind the optimal alignment or translation, whilesignificantly speeding the search.2 Inversion Transduction GrammarAn Inversion Transduction Grammar can generatepairs of sentences in two languages by recursivelyapplying context-free bilingual production rules.Most work on ITG has focused on the 2-normalform, which consists of unary production rulesthat are responsible for generating word pairs:X ?
e/f224and binary production rules in two forms that areresponsible for generating syntactic subtree pairs:X ?
[Y Z]andX ?
?Y Z?The rules with square brackets enclosing theright hand side expand the left hand side symbolinto the two symbols on the right hand side in thesame order in the two languages, whereas the ruleswith pointed brackets expand the left hand sidesymbol into the two right hand side symbols in re-verse order in the two languages.3 A* Viterbi Alignment SelectionA* parsing is a special case of agenda-based chartparsing, where the priority of a node X[i, j] on theagenda, corresponding to nonterminal X spanningpositions i through j, is the product of the node?scurrent inside probability with an estimate of theoutside probability.
By the current inside proba-bility, we mean the probability of the so-far-most-probable subtree rooted on the node X[i, j], withleaves being iwj , while the outside probability isthe highest probability for a parse with the rootbeing S[0, N ] and the sequence 0wiXjwn formingthe leaves.
The node with the highest priority is re-moved from the agenda and added to the chart, andthen explored by combining with all of its neigh-boring nodes in the chart to update the prioritiesof the resulting nodes on the agenda.
By usingestimates close to the actual outside probabilities,A* parsing can effectively reduce the number ofnodes to be explored before putting the root nodeonto the chart.
When the outside estimate is bothadmissible and monotonic, whenever a node is putonto the chart, its current best inside parse is theViterbi inside parse.To relate A* parsing with A* search for find-ing the lowest cost path from a certain sourcenode to a certain destination node in a graph, weview the forest of all parse trees as a hypergraph.The source node in the hypergraph fans out intothe nodes of unit spans that cover the individualwords.
From each group of children to their par-ent in the forest, there is a hyperedge.
The destina-tion node is the common root node for all the parsetrees in the forest.
Under the mapping, a parse is ahyperpath from the source node to the destinationnode.
The Viterbi parse selection problem thus be-comes finding the lowest-cost hyperpath from thesource node to the destination node.
The cost inthis scenario is thus the negative of log probabil-ity.
The inside estimate and outside estimate natu-rally correspond to the g?
and h?
for A* searching,respectively.A stochastic ITG can be thought of as a stochas-tic CFG extended to the space of bitext.
A node inthe ITG chart is a bitext cell that covers a sourcesubstring and a target substring.
We use the no-tion of X[l, m, i, j] to represent a tree node in ITGparse.
It can potentially be combined with anybitext cells at the four corners, as shown in Fig-ure 1(a).Unlike CFG parsing where the leaves are fixed,the Viterbi ITG parse selection involves findingthe Viterbi alignment under ITG constraint.
Goodoutside estimates have to bound the outside ITGViterbi alignment probability tightly.3.1 A* Estimates for AlignmentUnder the ITG constraints, each source languageword can be aligned with at most one target lan-guage word and vice versa.
An ITG constituentX[l, m, i, j] implies that the words in the sourcesubstring in the span [l, m] are aligned with thewords in the target substring [i, j].
It further im-plies that the words outside the span [l, m] in thesource are aligned with the words outside the span[i, j] in the target language.
Figure 1(b) displaysthe tic-tac-toe pattern for the inside and outsidecomponents of a particular cell.
To estimate theupper bound of the ITG Viterbi alignment proba-bility for the outside component with acceptablecomplexity, we need to relax the ITG constraint.Instead of ensuring one-to-one in both directions,we use a many-to-one constraint in one direction,and we relax all constraints on reordering withinthe outside component.The many-to-one constraint has the same dy-namic programming structure as IBM Model 1,where each target word is supposed to be trans-lated from any of the source words or the NULLsymbol.
In the Model 1 estimate of the outsideprobability, source and target words can align us-ing any combination of points from the four out-side corners of the tic-tac-toe pattern.
Thus inFigure 1(b), there is one solid cell (correspond-ing to the Model 1 Viterbi alignment) in each col-umn, falling either in the upper or lower outsideshaded corner.
This can be also be thought of assqueezing together the four outside corners, creat-225lm0 i j Nlm0 i j Nlmn0 i j k N(a) (b) (c)Figure 1: (a) A bitext cell X[l, m, i, j] (shaded) for ITG parsing.
The inside cell can be combined withadjacent cells in the four outside corners (lighter shading) to expand into larger cells.
One possibleexpansion to the lower left corner is displayed.
(b) The tic-tac-toe pattern of alignments consistent witha given cell.
If the inner box is used in the final synchronous parse, all other alignments must comefrom the four outside corners.
(c) Combination of two adjacent cells shown with region for new outsideheuristic.ing a new cell whose probability is estimated usingIBM Model 1.
In contrast, the inside Viterbi align-ment satisfies the ITG constraint, implying onlyone solid cell in each column and each row.
Math-ematically, our Model 1 estimate for the outsidecomponent is:hM1(l, m, i, j) =?t<i,t>jmaxs<l,s>mP (ft, es)This Model 1 estimate is admissible.
Maximiz-ing over each column ensures that the translationprobability for each target word is greater than orequal to the corresponding word translation prob-ability under the ITG constraint.
Model 1 virtuallyassigns a probability of 1 for deleting any sourceword.
As a product of word-to-word translationprobabilities including deletions and insertions,the ITG Viterbi alignment probability cannot behigher than the product of maximal word-to-wordtranslation probabilities using the Model 1 esti-mate.The Model 1 estimate is also monotonic, a prop-erty which is best understood geometrically.
Asuccessor state to cell (l, m, i, j) in the search isformed by combining the cell with a cell whichis adjacent at one of the four corners, as shownin Figure 1(c).
Of the four outside corner regionsused in calculating the search heuristic, one willbe the same for the successor state, and three willbe a subset of the old corner region.
Withoutloss of generality, assume we are combining a cell(m, n, j, k) that is adjacent to (l, m, i, j) to the up-per right.
We defineHM1(l, m, i, j) = ?
log hM1(l, m, i, j)as the negative log of the heuristic in order to cor-respond to an estimated cost or distance in searchterminology.
Similarly, we speak of the cost of achart entry c(X[l, m, i, j]) as its negative log prob-ability, and the cost of a cell c(l, m, i, j) as thecost of the best chart entry with the boundaries(l, m, i, j).
The cost of the cell (m, n, j, k) whichis being combined with the old cell is guaranteedto be greater than the contribution of the columnsj through k to the heuristic HM1(l, m, i, j).
Thecontribution of the columns k through N to thenew heuristic HM1(l, n, i, k) is guaranteed to begreater in cost than their contribution to the oldheuristic.
Thus,HM1(l, m, i, j) ?
c(m, n, j, k) + c(X ?
Y Z)+ HM1(l, n, i, k)meaning that the heuristic is monotonic or consis-tent.The Model 1 estimate can be applied in bothtranslation directions.
The estimates from bothdirections are an upper bound of the actual ITGViterbi probability.
By taking the minimum of thetwo, we can get a tighter upper bound.We can precompute the Model 1 outside esti-mate for all bitext cells before parsing starts.
Ana?
?ve implementation would take O(n6) steps ofcomputation, because there are O(n4) cells, eachof which takes O(n2) steps to compute its Model 1probability.
Fortunately, exploiting the recursive226ju v</S><S>iFigure 2: The region within the dashed lines is the translation hypothesis X[i, j, u, v].
The word sequenceon the top is the Viterbi translation of the sentence on the bottom.
Wide range word order change mayhappen.nature of the cells, we can compute values for theinside and outside components of each cell usingdynamic programming in O(n4) time (Zhang andGildea, 2005).4 A* DecodingThe of ITG decoding algorithm of Wu (1996) canbe viewed as a variant of the Viterbi parsing al-gorithm for alignment selection.
The task of stan-dard alignment is to find word level links betweentwo fixed-order strings.
In the decoding situation,while the input side is a fixed sequence of words,the output side is a bag of words to be linked withthe input words and then reordered.
Under the ITGconstraint, if the target language substring [i, j] istranslated into s1 in the source language and thetarget substring [j, k] is translated into s2, then s1and s2 must be consecutive in the source languageas well and two possible orderings, s1s2 and s2s1,are allowed.
Finding the best translation of thesubstring of [i, k] involves searching over all pos-sible split points j and two possible reorderingsfor each split.
In theory, the inversion probabilitiesassociated with the ITG rules can do the job of re-ordering.
However, a language model as simple asbigram is generally stronger.
Using an n-gram lan-guage model implies keeping at least n?1 bound-ary words in the dynamic programming table for ahypothetical translation of a source language sub-string.
In the case of a bigram ITG decoder, atranslation hypothesis for the source language sub-string [i, j] is denoted as X[i, j, u, v], where u andv are the left boundary word and right boundaryword of the target language counterpart.As indicated by the similarity of parsing itemnotation, the dynamic programming property ofthe Viterbi decoder is essentially the same as thebitext parsing for finding the underlying Viterbialignment.
By permitting translation from the nulltarget string of [i, i] into source language words asmany times as necessary, the decoder can translatean input sentence into a longer output sentence.When there is the null symbol in the bag of candi-date words, the decoder can choose to translate aword into null to decrease the output length.
Bothinsertions and deletions are special cases of the bi-text parsing items.Given the similarity of the dynamic program-ming framework to the alignment problem, it isnot surprising that A* search can also be ap-plied in a similar way.
The initial parsing itemson the agenda are the basic translation units:X[i, i + 1, u, u], for normal word-for-word trans-lations and deletions (translations into nothing),and also X[i, i, u, u], for insertions (translationsfrom nothing).
The goal item is S[0, N, ?s?, ?/s?
],where ?s?
stands for the beginning-of-sentencesymbol and ?/s?
stands for the end-of-sentencesymbol.
The exploration step of the A* searchis to expand the translation hypothesis of a sub-string by combining with neighboring translationhypotheses.
When the outside estimate is admis-sible and monotonic, the exploration is optimalin the sense that whenever a hypothesis is takenfrom the top of the agenda, it is a Viterbi transla-tion of the corresponding target substring.
Thus,when S[0, N, ?s?, ?/s?]
is added to the chart, wehave found the Viterbi translation for the entiresentence.227?
(X[i, j, u, v]) = max{???
(X[i, j, u, v]), ?
[](X[i, j, u, v])}?
[](X[i, j, u, v]) = maxk,v1,u2,Y,Z[?
(Y [i, k, u, v1]) ?
?
(Z[k, j, u2, v]) ?
P (X ?
[Y Z]) ?
Plm(u2 | v1)]= maxk,u2,Y,Z[maxv1[?
(Y [i, k, u, v1]) ?
Plm(u2 | v1)]?
P (X ?
[Y Z]) ?
?
(Z[k, j, u2, v])]Figure 3: Top: An ITG decoding constituent can be built with either a straight or an inverted rule.Bottom: An efficient factorization for straight rules.4.1 A* Estimates for TranslationThe key to the success of A* decoding is an out-side estimate that combines word-for-word trans-lation probabilities and n-gram probabilities.
Fig-ure 2 is the picture of the outside translationsand bigrams of a particular translation hypothesisX[i, j, u, v].Our heuristic involves precomputing two val-ues for each word in the input string, involvingforward- and backward-looking language modelprobabilities.
For the forward looking value hf atinput position n, we take a maximum over the setof words Sn that the input word tn can be trans-lated as:hf (n) = maxs?Sn[Pt(s | tn) maxs??SPlm(s?
| s)]where:S =?nSnis the set of all possible translations for all wordsin the input string.
While hf considers lan-guage model probabilities for words following s,the backward-looking value hb considers languagemodel probabilities for s given possible precedingwords:hb(n) = maxs?Sn[Pt(s | tn) maxs?
?SPlm(s | s?
)]Our overall heuristic for a partial translationhypothesis X[i, j, u, v] combines language modelprobabilities at the boundaries of the input sub-string with backward-looking values for the pre-ceding words, and forward-looking values for thefollowing words:h(i, j, u, v) =[maxs?SPlm(u | s)] [maxs?SPlm(s | v)]?
?n<i,n>jmax [hb(n), hf (n)]Because we don?t know whether a given inputword will appear before or after the partial hypoth-esis in the final translation, we take the maximumof the forward and backward values for words out-side the span [i, j].4.2 Combining the Hook Trick with A*The hook trick is a factorization technique for dy-namic programming.
For bilexical parsing, Eis-ner and Satta (1999) pointed out we can reducethe complexity of parsing from O(n5) to O(n4)by combining the non-head constituents with thebilexical rules first, and then combining the resul-tant hook constituents with the head constituents.By doing so, the maximal number of interactivevariables ranging over n is reduced from 5 to 4.For ITG decoding, we can apply a similar factor-ization trick.
We describe the bigram-integrateddecoding case here, and refer to Huang et al(2005) for more detailed discussion.
Figure 3shows how to decompose the expression for thecase of straight rules; the same method applies toinverted rules.
The number of free variables on theright hand side of the second equation is 7: i, j, k,u, v, v1, and u2.1 After factorization, counting thefree variables enclosed in the innermost max oper-ator, we get five: i, k, u, v1, and u2.
The decompo-sition eliminates one free variable, v1.
In the out-ermost level, there are six free variables left.
Themaximum number of interacting variables is sixoverall.
So, we reduced the complexity of ITG de-coding using bigram language model from O(n7)to O(n6).
If we visualize an ITG decoding con-stituent Y extending from source language posi-tion i to k and target language boundary words uand v1 with a diagram:Yi ku v11X , Y , and Z range over grammar nonterminals, of whichthere are a constant number.2280501001502002503003504004505000  10  20  30  40  50  60  70secondssentence lengthfulluniformibm1encnibm1sym01e+062e+063e+064e+065e+066e+060  10  20  30  40  50  60  70#arcsmax sentence lengthfulluniformibm1encnibm1symFigure 4: Speed of various techniques for finding the optimal alignment.the hook corresponding to the innermost max op-erator in the equation can be visualized as follows:Yi ku u2with the expected language model state u2 ?hang-ing?
outside the target language string.The trick is generic to the control strategies ofactual parsing, because the hooks can be treatedas just another type of constituent.
Building hooksis like applying special unary rules on top of non-hooks.
In terms of of outside heuristic for hooks,there is a slight difference from that for non-hooks:h(i, j, u, v) =[maxs?SPlm(s | v)]?
?n<i,n>jmax [hb(n), hf (n)]That is, we do not need the backward-looking es-timate for the left boundary word u.5 ExperimentsWe tested the performance of our heuristics foralignment on a Chinese-English newswire corpus.Probabilities for the ITG model were trained usingExpectation Maximization on a corpus of 18,773sentence pairs with a total of 276,113 Chinesewords and 315,415 English words.
For EM train-ing, we limited the data to sentences of no morethan 25 words in either language.
Here we presenttiming results for finding the Viterbi alignment oflonger sentences using this fixed translation modelwith different heuristics.
We compute alignmentson a total of 117 test sentences, which are brokendown by length as shown in Table 1.Length # sentences0-9 510?19 2620?29 2930?39 2240?49 2450?59 1060 1Table 1: Length of longer sentence in each pairfrom test data.method time speedupfull 815s ?uniform 547s 1.4ibm1encn 269s 3.0ibm1sym 205s 3.9Table 2: Total time for each alignment method.Results are presented both in terms of time andthe number of arcs added to the chart before theoptimal parse is found.
Full refers to exhaus-tive parsing, that is, building a complete chartwith all n4 arcs.
Uniform refers to a best-firstparsing strategy that expands the arcs with thehighest inside probability at each step, but doesnot incorporate an estimate of the outside proba-bility.
Ibm1encn denotes our heuristic based onIBM model 1, applied to translations from Englishto Chinese, while ibm1sym applies the Model 1heuristic in both translation directions and takesthe minimum.
The factor by which times were de-creased was found to be roughly constant acrossdifferent length sentences.
The alignment timesfor the entire test set are shown in Table 2, thebest heuristic is 3.9 times faster than exhaustive22902e+064e+066e+068e+061e+071.2e+071.4e+070  5  10  15  20#hyperedgesinput sentence lengthBI-UNIFORMBI-HOOK-UNIFORMBI-HOOK-A*BI-HOOK-A*-BEAM13.713.813.91414.114.214.314.414.50  100  200  300  400  500  600  700bleuaverage number of arcs (unit is 1k)BI-HOOK-A*+BEAMBI-CYK-BEAMFigure 5: On the left, we compare decoding speed for uniform outside estimate best-first decoding withand without the hook trick, as well as results using our heuristic (labeled A*) and with beam pruning(which no longer produces optimal results).
On the right, we show the relationship between computationtime and BLEU scores as the pruning threshold is varied for both A* search and bottom-up CYK parsing.dynamic programming.We did our ITG decoding experiments on theLDC 2002 MT evaluation data set for translationof Chinese newswire sentences into English.
Theevaluation data set has 10 human translation refer-ences for each sentence.
There are a total of 371Chinese sentences of no more than 20 words inthe data set.
These sentences are the test set forour different versions of ITG decoders using botha bigram language model and a trigram languagemodel.
We evaluate the translation results by com-paring them against the reference translations us-ing the BLEU metric.
The word-for-word transla-tion probabilities are from the translation modelof IBM Model 4 trained on a 160-million-wordEnglish-Chinese parallel corpus using GIZA++.The language model is trained on a 30-million-word English corpus.
The rule probabilities forITG are from the same training as in the alignmentexperiments described above.We compared the BLEU scores of the A* de-coder and the ITG decoder that uses beam ratiopruning at each stage of bottom-up parsing.
In thecase of bigram-integrated decoding, for each inputword, the best 2 translations are put into the bag ofoutput words.
In the case of trigram-integrated de-coding, top 5 candidate words are chosen.
The A*decoder is guaranteed to find the Viterbi transla-tion that maximizes the product of n-grams prob-abilities, translation probabilities (including inser-tions and deletions) and inversion rule probabili-ties by choosing the right words and the right wordorder subject to the ITG constraint.Figure 5 (left) demonstrates the speedup ob-Decoder Combinations BLEUBI-UNIFORM 8.02M 14.26BI-HOOK-A* 2.10M 14.26BI-HOOK-A*-BEAM 0.40M 14.43BI-CYK-BEAM 0.20M 14.14Table 3: Decoder speed and BLEU scores for bi-gram decoding.Decoder Cbns BLEUTRI-A*-BEAM(10?3) 213.4M 17.83TRI-A*-BEAM(10?2) 20.7M 17.09TRI-CYK-BEAM(10?3) 21.2M 16.86Table 4: Results for trigram decoding.tained through the hook trick, the heuristic, andpruning, all based on A* search.
Table 3 shows theimprovement of BLEU score after applying the A*algorithm to find the optimal translation under themodel.
Figure 5 (right) investigates the relation-ship between the search effort and BLEU score forA* and bottom-up CYK parsing, both with prun-ing.
Pruning for A* works in such a way that wenever explore a low probability hypothesis fallingout of a certain beam ratio of the best hypothesiswithin the bucket of X[i, j, ?, ?
], where ?
meansany word.
Table 4 shows results for trigram-integrated decoding.
However, due to time con-straint, we have not explored time/performancetradeoff as we did for bigram decoding.The number of combinations in the table isthe average number of hyperedges to be exploredin searching, proportional to the total number of230computation steps.6 ConclusionA* search for Viterbi alignment selection underITG is efficient using IBM Model 1 as an outsideestimate.
The experimental results indicate thatdespite being a more relaxed word-for-word align-ment model than ITG, IBM Model 1 can serveas an efficient and reliable approximation of ITGin terms of Viterbi alignment probability.
This ismore true when we apply Model 1 to both trans-lation directions and take the minimum of both.We have also tried to incorporate estimates of bi-nary rule probabilities to make the outside esti-mate even sharper.
However, the further improve-ment was marginal.We are able to find the ITG Viterbi translationusing our A* decoding algorithm with an outsideestimate that combines outside bigrams and trans-lation probabilities for outside words.
The hooktrick gave us a significant further speedup; we be-lieve this to be the first demonstrated practical ap-plication of this technique.Interestingly, the BLEU score for the opti-mal translations under the probabilistic model islower than we achieve with our best bigram-based system using pruning.
However, this sys-tem makes use of the A* heuristic, and ourspeed/performance curve shows that the heuris-tic allows us to achieve higher BLEU scores withthe same amount of computation.
In the case oftrigram integrated decoding, there is 1 point ofBLEU score improvement by moving from a typ-ical CYK plus beam search decoder to a decoderusing A* plus beam search.However, without knowing what words will ap-pear in the output language, a very sharp outsideestimate to further bring down the number of com-binations is difficult to achieve.The brighter side of the move towards optimaldecoding is that the A* search strategy leads usto the region of the search space that is close tothe optimal result, where we can more easily findgood translations.Acknowledgments This work was supportedby NSF ITR IIS-09325646 and NSF ITR IIS-0428020.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Eugene Charniak, Sharon Goldwater, and Mark John-son.
1998.
Edge-based best-first chart parsing.
InProceedings of the Sixth Workshop on Very LargeCorpora.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In 37th Annual Meeting of theAssociation for Computational Linguistics.Liang Huang, Hao Zhang, and Daniel Gildea.
2005.Machine translation as lexicalized parsing withhooks.
In International Workshop on Parsing Tech-nologies (IWPT05), Vancouver, BC.Dan Klein and Christopher D. Manning.
2003.
A*parsing: Fast exact viterbi parse selection.
In Pro-ceedings of the 2003 Meeting of the North Americanchapter of the Association for Computational Lin-guistics (NAACL-03).Franz Josef Och, Nicola Ueffing, and Herman Ney.2001.
An efficient a* search algorithm for statis-tical machine translation.
In Proceedings of theACL Workshop on Data-Driven Machine Transla-tion, pages 55?62, Toulouse, France.Ye-Yi Wang and Alex Waibel.
1997.
Decoding algo-rithm in statistical machine translation.
In 35th An-nual Meeting of the Association for ComputationalLinguistics.Dekai Wu.
1996.
A polynomial-time algorithm for sta-tistical machine translation.
In 34th Annual Meetingof the Association for Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Richard Zens and Hermann Ney.
2003.
A compara-tive study on reordering constraints in statistical ma-chine translation.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics, Sapporo, Japan.Hao Zhang and Daniel Gildea.
2005.
Stochastic lex-icalized inversion transduction grammar for align-ment.
In Proceedings of the 43rd Annual Confer-ence of the Association for Computational Linguis-tics (ACL-05), Ann Arbor, MI.231
