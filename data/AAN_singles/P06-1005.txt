Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 33?40,Sydney, July 2006. c?2006 Association for Computational LinguisticsBootstrapping Path-Based Pronoun ResolutionShane BergsmaDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8bergsma@cs.ualberta.caDekang LinGoogle, Inc.1600 Amphitheatre Parkway,Mountain View, California, 94301lindek@google.comAbstractWe present an approach to pronoun reso-lution based on syntactic paths.
Through asimple bootstrapping procedure, we learnthe likelihood of coreference between apronoun and a candidate noun based on thepath in the parse tree between the two en-tities.
This path information enables us tohandle previously challenging resolutioninstances, and also robustly addresses tra-ditional syntactic coreference constraints.Highly coreferent paths also allow miningof precise probabilistic gender/number in-formation.
We combine statistical knowl-edge with well known features in a Sup-port Vector Machine pronoun resolutionclassifier.
Significant gains in performanceare observed on several datasets.1 IntroductionPronoun resolution is a difficult but vital part of theoverall coreference resolution task.
In each of thefollowing sentences, a pronoun resolution systemmust determine what the pronoun his refers to:(1) John needs his friend.
(2) John needs his support.In (1), John and his corefer.
In (2), his refersto some other, perhaps previously evoked entity.Traditional pronoun resolution systems are not de-signed to distinguish between these cases.
Theylack the specific world knowledge required in thesecond instance ?
the knowledge that a persondoes not usually explicitly need his own support.We collect statistical path-coreference informa-tion from a large, automatically-parsed corpus toaddress this limitation.
A dependency path is de-fined as the sequence of dependency links betweentwo potentially coreferent entities in a parse tree.A path does not include the terminal entities; forexample, ?John needs his support?
and ?He needstheir support?
have the same syntactic path.
Ouralgorithm determines that the dependency pathlinking the Noun and pronoun is very likely to con-nect coreferent entities for the path ?Noun needspronoun?s friend,?
while it is rarely coreferent forthe path ?Noun needs pronoun?s support.
?This likelihood can be learned by simply count-ing how often we see a given path in text withan initial Noun and a final pronoun that are fromthe same/different gender/number classes.
Casessuch as ?John needs her support?
or ?They needhis support?
are much more frequent in text thancases where the subject noun and pronoun termi-nals agree in gender/number.
When there is agree-ment, the terminal nouns are likely to be corefer-ent.
When they disagree, they refer to different en-tities.
After a sufficient number of occurrences ofagreement or disagreement, there is a strong sta-tistical indication of whether the path is coreferent(terminal nouns tend to refer to the same entity) ornon-coreferent (nouns refer to different entities).We show that including path coreference in-formation enables significant performance gainson three third-person pronoun resolution experi-ments.
We also show that coreferent paths can pro-vide the seed information for bootstrapping other,even more important information, such as the gen-der/number of noun phrases.2 Related WorkCoreference resolution is generally conducted asa pairwise classification task, using various con-straints and preferences to determine whether two33expressions corefer.
Coreference is typically onlyallowed between nouns matching in gender andnumber, and not violating any intrasentential syn-tactic principles.
Constraints can be applied as apreprocessing step to scoring candidates based ondistance, grammatical role, etc., with scores devel-oped either manually (Lappin and Leass, 1994), orthrough a machine-learning algorithm (Kehler etal., 2004).
Constraints and preferences have alsobeen applied together as decision nodes on a deci-sion tree (Aone and Bennett, 1995).When previous resolution systems handle caseslike (1) and (2), where no disagreement or syntac-tic violation occurs, coreference is therefore de-termined by the weighting of features or learneddecisions of the resolution classifier.
Withoutpath coreference knowledge, a resolution processwould resolve the pronouns in (1) and (2) thesame way.
Indeed, coreference resolution researchhas focused on the importance of the strategyfor combining well known constraints and prefer-ences (Mitkov, 1997; Ng and Cardie, 2002), devot-ing little attention to the development of new fea-tures for these difficult cases.
The application ofworld knowledge to pronoun resolution has beenlimited to the semantic compatibility between acandidate noun and the pronoun?s context (Yanget al, 2005).
We show semantic compatibility canbe effectively combined with path coreference in-formation in our experiments below.Our method for determining path coreferenceis similar to an algorithm for discovering para-phrases in text (Lin and Pantel, 2001).
In thatwork, the beginning and end nodes in the pathsare collected, and two paths are said to be similar(and thus likely paraphrases of each other) if theyhave similar terminals (i.e.
the paths occur with asimilar distribution).
Our work does not need tostore the terminals themselves, only whether theyare from the same pronoun group.
Different pathsare not compared in any way; each path is individ-ually assigned a coreference likelihood.3 Path CoreferenceWe define a dependency path as the sequence ofnodes and dependency labels between two poten-tially coreferent entities in a dependency parsetree.
We use the structure induced by the minimal-ist parser Minipar (Lin, 1998) on sentences fromthe news corpus described in Section 4.
Figure 1gives the parse tree of (2).
As a short-form, weJohnneedshissupportsubjgenobjFigure 1: Example dependency tree.write the dependency path in this case as ?Nounneeds pronoun?s support.?
The path itself does notinclude the terminal nouns ?John?
and ?his.
?Our algorithm finds the likelihood of coref-erence along dependency paths by counting thenumber of times they occur with terminals thatare either likely coreferent or non-coreferent.
Inthe simplest version, we count paths with termi-nals that are both pronouns.
We partition pronounsinto seven groups of matching gender, number,and person; for example, the first person singulargroup contains I, me, my, mine, and myself.
If thetwo terminal pronouns are from the same group,coreference along the path is likely.
If they arefrom different groups, like I and his, then they arenon-coreferent.
Let NS(p) be the number of timesthe two terminal pronouns of a path, p, are fromthe same pronoun group, and let ND(p) be thenumber of times they are from different groups.We define the coreference of p as:C(p) = NS(p)NS(p) + ND(p)Our statistics indicate the example path, ?Nounneeds pronoun?s support,?
has a low C(p) value.We could use this fact to prevent us from resolv-ing ?his?
to ?John?
when ?John needs his support?is presented to a pronoun resolution system.To mitigate data sparsity, we represent the pathwith the root form of the verbs and nouns.
Also,we use Minipar?s named-entity recognition to re-place named-entity nouns by the semantic cate-gory of their named-entity, when available.
Allmodifiers not on the direct path, such as adjectives,determiners and adverbs, are not considered.
Welimit the maximum path length to eight nodes.Tables 1 and 2 give examples of coreferent andnon-coreferent paths learned by our algorithm andidentified in our test sets.
Coreferent paths aredefined as paths with a C(p) value (and overallnumber of occurrences) above a certain threshold,indicating the terminal entities are highly likely34Table 1: Example coreferent paths: Italicized entities generally corefer.Pattern Example1.
Noun left ... to pronoun?s wife Buffett will leave the stock to his wife.2.
Noun says pronoun intends...
The newspaper says it intends to file a lawsuit.3.
Noun was punished for pronoun?s crime.
The criminal was punished for his crime.4.
... left Noun to fend for pronoun-self They left Jane to fend for herself.5.
Noun lost pronoun?s job.
Dick lost his job.6.
... created Noun and populated pronoun.
Nzame created the earth and populated it7.
Noun consolidated pronoun?s power.
The revolutionaries consolidated their power.8.
Noun suffered ... in pronoun?s knee ligament.
The leopard suffered pain in its knee ligament.to corefer.
Non-coreferent paths have a C(p) be-low a certain cutoff; the terminals are highly un-likely to corefer.
Especially note the challenge ofresolving most of the examples in Table 2 with-out path coreference information.
Although thesepaths encompass some cases previously coveredby Binding Theory (e.g.
?Mary suspended her,?her cannot refer to Mary by Principle B (Haege-man, 1994)), most have no syntactic justificationfor non-coreference per se.
Likewise, althoughBinding Theory (Principle A) could identify thereflexive pronominal relationship of Example 4 inTable 1, most cases cannot be resolved throughsyntax alone.
Our analysis shows that successfullyhandling cases that may have been handled withBinding Theory constitutes only a small portion ofthe total performance gain using path coreference.In any case, Binding Theory remains a chal-lenge with a noisy parser.
Consider: ?Alex gaveher money.?
Minipar parses her as a possessive,when it is more likely an object, ?Alex gave moneyto her.?
Without a correct parse, we cannot ruleout the link between her and Alex through Bind-ing Theory.
Our algorithm, however, learns thatthe path ?Noun gave pronoun?s money,?
is non-coreferent.
In a sense, it corrects for parser errorsby learning when coreference should be blocked,given any consistent parse of the sentence.We obtain path coreference for millions of pathsfrom our parsed news corpus (Section 4).
WhileTables 1 and 2 give test set examples, many otherinteresting paths are obtained.
We learn corefer-ence is unlikely between the nouns in ?Bob mar-ried his mother,?
or ?Sue wrote her obituary.?
Thefact you don?t marry your own mother or writeyour own obituary is perhaps obvious, but thisis the first time this kind of knowledge has beenmade available computationally.
Naturally, ex-ceptions to the coreference or non-coreference ofsome of these paths can be found; our patternsrepresent general trends only.
And, as mentionedabove, reliable path coreference is somewhat de-pendent on consistent parsing.Paths connecting pronouns to pronouns are dif-ferent than paths connecting both nouns and pro-nouns to pronouns ?
the case we are ultimately in-terested in resolving.
Consider ?Company A gaveits data on its website.?
The pronoun-pronounpath coreference algorithm described above wouldlearn the terminals in ?Noun?s data on pronoun?swebsite?
are often coreferent.
But if we see thephrase ?Company A gave Company B?s data onits website,?
then ?its?
is not likely to refer to?Company B,?
even though we identified this asa coreferent path!
We address this problem with atwo-stage extraction procedure.
We first bootstrapgender/number information using the pronoun-pronoun paths as described in Section 4.1.
Wethen use this gender/number information to countpaths where an initial noun (with probabilistically-assigned gender/number) and following pronounare connected by the dependency path, record-ing the agreement or disagreement of their gen-der/number category.1 These superior paths arethen used to re-bootstrap our final gender/numberinformation used in the evaluation (Section 6).We also bootstrap paths where the nodes inthe path are replaced by their grammatical cate-gory.
This allows us to learn general syntactic con-straints not dependent on the surface forms of thewords (including, but not limited to, the BindingTheory principles).
A separate set of these non-coreferent paths is also used as a feature in our sys-1As desired, this modification allows the first example toprovide two instances of noun-pronoun paths with terminalsfrom the same gender/number group, linking each ?its?
to thesubject noun ?Company A?, rather than to each other.35Table 2: Example non-coreferent paths: Italicized entities do not generally coreferPattern Example1.
Noun thanked ... for pronoun?s assistance John thanked him for his assistance.2.
Noun wanted pronoun to lie.
The president wanted her to lie.3.
... Noun into pronoun?s pool Max put the floaties into their pool.4.
... use Noun to pronoun?s advantage The company used the delay to its advantage.5.
Noun suspended pronoun Mary suspended her.6.
Noun was pronoun?s relative.
The Smiths were their relatives.7.
Noun met pronoun?s demands The players?
association met its demands.8.
... put Noun at the top of pronoun?s list.
The government put safety at the top of its list.tem.
We also tried expanding our coverage by us-ing paths similar to paths with known path coref-erence (based on distributionally similar words),but this did not generally increase performance.4 Bootstrapping in Pronoun ResolutionOur determination of path coreference can be con-sidered a bootstrapping procedure.
Furthermore,the coreferent paths themselves can serve as theseed for bootstrapping additional coreference in-formation.
In this section, we sketch previous ap-proaches to bootstrapping in coreference resolu-tion and explain our new ideas.Coreference bootstrapping works by assumingresolutions in unlabelled text, acquiring informa-tion from the putative resolutions, and then mak-ing inferences from the aggregate statistical data.For example, we assumed two pronouns from thesame pronoun group were coreferent, and deducedpath coreference from the accumulated counts.The potential of the bootstrapping approach canbest be appreciated by imagining millions of doc-uments with coreference annotations.
With such aset, we could extract fine-grained features, perhapstied to individual words or paths.
For example, wecould estimate the likelihood each noun belongs toa particular gender/number class by the proportionof times this noun was labelled as the antecedentfor a pronoun of this particular gender/number.Since no such corpus exists, researchers haveused coarser features learned from smaller setsthrough supervised learning (Soon et al, 2001;Ng and Cardie, 2002), manually-defined corefer-ence patterns to mine specific kinds of data (Beanand Riloff, 2004; Bergsma, 2005), or accepted thenoise inherent in unsupervised schemes (Ge et al,1998; Cherry and Bergsma, 2005).We address the drawbacks of these approachesTable 3: Gender classification performance (%)Classifier F-ScoreBergsma (2005) Corpus-based 85.4Bergsma (2005) Web-based 90.4Bergsma (2005) Combined 92.2Duplicated Corpus-based 88.0Coreferent Path-based 90.3by using coreferent paths as the assumed resolu-tions in the bootstrapping.
Because we can varythe threshold for defining a coreferent path, we cantrade-off coverage for precision.
We now outlinetwo potential uses of bootstrapping with coref-erent paths: learning gender/number information(Section 4.1) and augmenting a semantic compat-ibility model (Section 4.2).
We bootstrap this dataon our automatically-parsed news corpus.
Thecorpus comprises 85 GB of news articles takenfrom the world wide web over a 1-year period.4.1 Probabilistic Gender/NumberBergsma (2005) learns noun gender (and num-ber) from two principal sources: 1) mining itfrom manually-defined lexico-syntactic patterns inparsed corpora, and 2) acquiring it on the fly bycounting the number of pages returned for variousgender-indicating patterns by the Google searchengine.
The web-based approach outperformedthe corpus-based approach, while a system thatcombined the two sets of information resulted inthe highest performance (Table 3).
The combinedgender-classifying system is a machine-learnedclassifier with 20 features.The time delay of using an Internet search en-gine within a large-scale anaphora resolution ef-fort is currently impractical.
Thus we attempted36Table 4: Example gender/number probability (%)Word masc fem neut plurcompany 0.6 0.1 98.1 1.2condoleeza rice 4.0 92.7 0.0 3.2pat 58.3 30.6 6.2 4.9president 94.1 3.0 1.5 1.4wife 9.9 83.3 0.8 6.1to duplicate Bergsma?s corpus-based extraction ofgender and number, where the information can bestored in advance in a table, but using a muchlarger data set.
Bergsma ran his extraction onroughly 6 GB of text; we used roughly 85 GB.Using the test set from Bergsma (2005), wewere only able to boost performance from an F-Score of 85.4% to one of 88.0% (Table 3).
Thisresult led us to re-examine the high performanceof Bergsma?s web-based approach.
We realizedthat the corpus-based and web-based approachesare not exactly symmetric.
The corpus-based ap-proaches, for example, would not pick out genderfrom a pattern such as ?John and his friends...?
be-cause ?Noun and pronoun?s NP?
is not one of themanually-defined gender extraction patterns.
Theweb-based approach, however, would catch thisinstance with the ?John * his/her/its/their?
tem-plate, where ?*?
is the Google wild-card opera-tor.
Clearly, there are patterns useful for capturinggender and number information beyond the pre-defined set used in the corpus-based extraction.We thus decided to capture gender/number in-formation from coreferent paths.
If a noun is con-nected to a pronoun of a particular gender along acoreferent path, we count this as an instance of thatnoun being that gender.
In the end, the probabilitythat the noun is a particular gender is the propor-tion of times it was connected to a pronoun of thatgender along a coreferent path.
Gender informa-tion becomes a single intuitive, accessible feature(i.e.
the probability of the noun being that gender)rather than Bergsma?s 20-dimensional feature vec-tor requiring search-engine queries to instantiate.We acquire gender and number data for over 3million nouns.
We use add-one smoothing for datasparsity.
Some example gender/number probabil-ities are given in Table 4 (cf.
(Ge et al, 1998;Cherry and Bergsma, 2005)).
We get a perfor-mance of 90.3% (Table 3), again meeting our re-quirements of high performance and allowing fora fast, practical implementation.
This is lowerthan Bergsma?s top score of 92.2% (Figure 3),but again, Bergsma?s top system relies on Googlesearch queries for each new word, while ours areall pre-stored in a table for fast access.We are pleased to be able to share our genderand number data with the NLP community.2 InSection 6, we show the benefit of this data as aprobabilistic feature in our pronoun resolution sys-tem.
Probabilistic data is useful because it allowsus to rapidly prototype resolution systems with-out incurring the overhead of large-scale lexicaldatabases such as WordNet (Miller et al, 1990).4.2 Semantic CompatibilityResearchers since Dagan and Itai (1990) have var-iously argued for and against the utility of col-location statistics between nouns and parents forimproving the performance of pronoun resolution.For example, can the verb parent of a pronoun beused to select antecedents that satisfy the verb?s se-lectional restrictions?
If the verb phrase was shat-ter it, we would expect it to refer to some kindof brittle entity.
Like path coreference, semanticcompatibility can be considered a form of worldknowledge needed for more challenging pronounresolution instances.We encode the semantic compatibility betweena noun and its parse tree parent (and grammaticalrelationship with the parent) using mutual infor-mation (MI) (Church and Hanks, 1989).
Supposewe are determining whether ham is a suitable an-tecedent for the pronoun it in eat it.
We calculatethe MI as:MI(eat:obj, ham) = log Pr(eat:obj:ham)Pr(eat:obj)Pr(ham)Although semantic compatibility is usually onlycomputed for possessive-noun, subject-verb, andverb-object relationships, we include 121 differ-ent kinds of syntactic relationships as parsed inour news corpus.3 We collected 4.88 billion par-ent:rel:node triples, including over 327 millionpossessive-noun values, 1.29 billion subject-verband 877 million verb-direct object.
We use smallprobability values for unseen Pr(parent:rel:node),Pr(parent:rel), and Pr(node) cases, as well as a de-fault MI when no relationship is parsed, roughlyoptimized for performance on the training set.
We2Available at http://www.cs.ualberta.ca/?bergsma/Gender/3We convert prepositions to relationships to enhance ourmodel?s semantics, e.g.
Joan:of:Arc rather than Joan:prep:of37include both the MI between the noun and the pro-noun?s parent as well as the MI between the pro-noun and the noun?s parent as features in our pro-noun resolution classifier.Kehler et al (2004) saw no apparent gain fromusing semantic compatibility information, whileYang et al (2005) saw about a 3% improvementwith compatibility data acquired by searching onthe world wide web.
Section 6 analyzes the con-tribution of MI to our system.Bean and Riloff (2004) used bootstrapping toextend their semantic compatibility model, whichthey called contextual-role knowledge, by identi-fying certain cases of easily-resolved anaphors andantecedents.
They give the example ?Mr.
Bushdisclosed the policy by reading it.?
Once we iden-tify that it and policy are coreferent, we includeread:obj:policy as part of the compatibility model.Rather than using manually-defined heuristicsto bootstrap additional semantic compatibility in-formation, we wanted to enhance our MI statisticsautomatically with coreferent paths.
Consider thephrase, ?Saddam?s wife got a Jordanian lawyer forher husband.?
It is unlikely we would see ?wife?shusband?
in text; in other words, we would notknow that husband:gen:wife is, in fact, semanti-cally compatible and thereby we would discour-age selection of ?wife?
as the antecedent at res-olution time.
However, because ?Noun gets ...for pronoun?s husband?
is a coreferent path, wecould capture the above relationship by adding aparent:rel:node for every pronoun connected to anoun phrase along a coreferent path in text.We developed context models with and with-out these path enhancements, but ultimately wecould find no subset of coreferent paths that im-prove the semantic compatibility?s contribution totraining set accuracy.
A mutual information modeltrained on 85 GB of text is fairly robust on its own,and any kind of bootstrapped extension seems tocause more damage by increased noise than can becompensated by increased coverage.
Although welike knowing audiences have noses, e.g.
?the audi-ence turned up its nose at the performance,?
suchphrases are apparently quite rare in actual test sets.5 Experimental DesignThe noun-pronoun path coreference can be useddirectly as a feature in a pronoun resolution sys-tem.
However, path coreference is undefined forcases where there is no path between the pro-noun and the candidate noun ?
for example, whenthe candidate is in the previous sentence.
There-fore, rather than using path coreference directly,we have features that are true if C(p) is above orbelow certain thresholds.
The features are thus setwhen coreference between the pronoun and candi-date noun is likely (a coreferent path) or unlikely(a non-coreferent path).We now evaluate the utility of path coreferencewithin a state-of-the-art machine-learned resolu-tion system for third-person pronouns with nom-inal antecedents.
A standard set of features is usedalong with the bootstrapped gender/number, se-mantic compatibility, and path coreference infor-mation.
We refer to these features as our ?proba-bilistic features?
(Prob.
Features) and run experi-ments using the full system trained and tested witheach absent, in turn (Table 5).
We have 29 featuresin total, including measures of candidate distance,frequency, grammatical role, and different kindsof parallelism between the pronoun and the can-didate noun.
Several reliable features are used ashard constraints, removing candidates before con-sideration by the scoring algorithm.All of the parsing, noun-phrase identification,and named-entity recognition are done automat-ically with Minipar.
Candidate antecedents areconsidered in the current and previous sentenceonly.
We use SVMlight (Joachims, 1999) to learna linear-kernel classifier on pairwise examples inthe training set.
When resolving pronouns, weselect the candidate with the farthest positive dis-tance from the SVM classification hyperplane.Our training set is the anaphora-annotated por-tion of the American National Corpus (ANC) usedin Bergsma (2005), containing 1270 anaphoricpronouns4 .
We test on the ANC Test set (1291 in-stances) also used in Bergsma (2005) (highest res-olution accuracy reported: 73.3%), the anaphora-labelled portion of AQUAINT used in Cherry andBergsma (2005) (1078 instances, highest accu-racy: 71.4%), and the anaphoric pronoun subsetof the MUC7 (1997) coreference evaluation for-mal test set (169 instances, highest precision of62.1 reported on all pronouns in (Ng and Cardie,2002)).
These particular corpora were chosen sowe could test our approach using the same dataas comparable machine-learned systems exploit-ing probabilistic information sources.
Parameters4See http://www.cs.ualberta.ca/?bergsma/CorefTags/ forinstructions on acquiring annotations38Table 5: Resolution accuracy (%)Dataset ANC AQT MUC1 Previous noun 36.7 34.5 30.82 No Prob.
Features 58.1 60.9 49.73 No Prob.
Gender 65.8 71.0 68.64 No MI 71.3 73.5 69.25 No C(p) 72.3 73.7 69.86 Full System 73.9 75.0 71.67 Upper Bound 93.2 92.3 91.1were set using cross-validation on the training set;test sets were used only once to obtain the finalperformance values.Evaluation Metric: We report results in terms ofaccuracy: Of all the anaphoric pronouns in the testset, the proportion we resolve correctly.6 Results and DiscussionWe compare the accuracy of various configura-tions of our system on the ANC, AQT and MUCdatasets (Table 5).
We include the score from pick-ing the noun immediately preceding the pronoun(after our hard filters are applied).
Due to the hardfilters and limited search window, it is not possi-ble for our system to resolve every noun to a cor-rect antecedent.
We thus provide the performanceupper bound (i.e.
the proportion of cases with acorrect answer in the filtered candidate list).
OnANC and AQT, each of the probabilistic featuresresults in a statistically significant gain in perfor-mance over a model trained and tested with thatfeature absent.5 On the smaller MUC set, none ofthe differences in 3-6 are statistically significant,however, the relative contribution of the variousfeatures remains reassuringly constant.Aside from missing antecedents due to the hardfilters, the main sources of error include inaccuratestatistical data and a classifier bias toward preced-ing pronouns of the same gender/number.
It wouldbe interesting to see whether performance could beimproved by adding WordNet and web-mined fea-tures.
Path coreference itself could conceivably bedetermined with a search engine.Gender is our most powerful probabilistic fea-ture.
In fact, inspecting our system?s decisions,gender often rules out coreference regardless ofpath coreference.
This is not surprising, since webased the acquisition of C(p) on gender.
That is,5We calculate significance with McNemar?s test, p=0.05.0.70.750.80.850.90.9510  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1RecallPrecisionTop-1PrecisionTop-2PrecisionTop-3Figure 2: ANC pronoun resolution accuracy forvarying SVM-thresholds.our bootstrapping assumption was that the major-ity of times these paths occur, gender indicatescoreference or lack thereof.
Thus when they oc-cur in our test sets, gender should often sufficientlyindicate coreference.
Improving the orthogonalityof our features remains a future challenge.Nevertheless, note the decrease in performanceon each of the datasets when C(p) is excluded(#5).
This is compelling evidence that path coref-erence is valuable in its own right, beyond its abil-ity to bootstrap extensive and reliable gender data.Finally, we can add ourselves to the camp ofpeople claiming semantic compatibility is usefulfor pronoun resolution.
Both the MI from the pro-noun in the antecedent?s context and vice-versaresult in improvement.
Building a model fromenough text may be the key.The primary goal of our evaluation was to as-sess the benefit of path coreference within a com-petitive pronoun resolution system.
Our systemdoes, however, outperform previously publishedresults on these datasets.
Direct comparison ofour scoring system to other current top approachesis made difficult by differences in preprocessing.Ideally we would assess the benefit of our prob-abilistic features using the same state-of-the-artpreprocessing modules employed by others suchas (Yang et al, 2005) (who additionally use asearch engine for compatibility scoring).
Clearly,promoting competitive evaluation of pronoun res-olution scoring systems by giving competitorsequivalent real-world preprocessing output alongthe lines of (Barbu and Mitkov, 2001) remains thebest way to isolate areas for system improvement.Our pronoun resolution system is part of a largerinformation retrieval project where resolution ac-39curacy is not necessarily the most pertinent mea-sure of classifier performance.
More than one can-didate can be useful in ambiguous cases, and notevery resolution need be used.
Since the SVMranks antecedent candidates, we can test this rank-ing by selecting more than the top candidate (Top-n) and evaluating coverage of the true antecedents.We can also resolve only those instances where themost likely candidate is above a certain distancefrom the SVM threshold.
Varying this distancevaries the precision-recall (PR) of the overall res-olution.
A representative PR curve for the Top-nclassifiers is provided (Figure 2).
The correspond-ing information retrieval performance can now beevaluated along the Top-n / PR configurations.7 ConclusionWe have introduced a novel feature for pronounresolution called path coreference, and demon-strated its significant contribution to a state-of-the-art pronoun resolution system.
This feature aidscoreference decisions in many situations not han-dled by traditional coreference systems.
Also, bybootstrapping with the coreferent paths, we areable to build the most complete and accurate ta-ble of probabilistic gender information yet avail-able.
Preliminary experiments show path coref-erence bootstrapping can also provide a means ofidentifying pleonastic pronouns, where pleonasticneutral pronouns are often followed in a depen-dency path by a terminal noun of different gender,and cataphoric constructions, where the pronounsare often followed by nouns of matching gender.ReferencesChinatsu Aone and Scott William Bennett.
1995.
Evaluatingautomated and manual acquisition of anaphora resolutionstrategies.
In Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics, pages 122?129.Catalina Barbu and Ruslan Mitkov.
2001.
Evaluation tool forrule-based anaphora resolution methods.
In Proceedingsof the 39th Annual Meeting of the Association for Compu-tational Linguistics, pages 34?41.David L. Bean and Ellen Riloff.
2004.
Unsupervised learn-ing of contextual role knowledge for coreference resolu-tion.
In HLT-NAACL, pages 297?304.Shane Bergsma.
2005.
Automatic acquisition of gender in-formation for anaphora resolution.
In Proceedings of theEighteenth Canadian Conference on Artificial Intelligence(Canadian AI?2005), pages 342?353.Colin Cherry and Shane Bergsma.
2005.
An expectationmaximization approach to pronoun resolution.
In Pro-ceedings of the Ninth Conference on Natural LanguageLearning (CoNLL-2005), pages 88?95.Kenneth Ward Church and Patrick Hanks.
1989.
Word asso-ciation norms, mutual information, and lexicography.
InProceedings of the 27th Annual Meeting of the Associationfor Computational Linguistics (ACL?89), pages 76?83.Ido Dagan and Alan Itai.
1990.
Automatic processingof large corpora for the resolution of anaphora refer-ences.
In Proceedings of the 13th International Con-ference on Computational Linguistics (COLING-90), vol-ume 3, pages 330?332, Helsinki, Finland.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A statisti-cal approach to anaphora resolution.
In Proceedings of theSixth Workshop on Very Large Corpora, pages 161?171.Liliane Haegeman.
1994.
Introduction to Government &Binding theory: Second Edition.
Basil Blackwell, Cam-bridge, UK.Thorsten Joachims.
1999.
Making large-scale SVM learn-ing practical.
In B. Scho?lkopf and C. Burges, editors, Ad-vances in Kernel Methods.
MIT-Press.Andrew Kehler, Douglas Appelt, Lara Taylor, and AleksandrSimma.
2004.
The (non)utility of predicate-argument fre-quencies for pronoun interpretation.
In Proceedings ofHLT/NAACL-04, pages 289?296.Shalom Lappin and Herbert J. Leass.
1994.
An algorithm forpronominal anaphora resolution.
Computational Linguis-tics, 20(4):535?561.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural Language En-gineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation of MINI-PAR.
In Proceedings of the Workshop on the Evalua-tion of Parsing Systems, First International Conference onLanguage Resources and Evaluation.George A. Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine J. Miller.
1990.
Introductionto WordNet: an on-line lexical database.
InternationalJournal of Lexicography, 3(4):235?244.Ruslan Mitkov.
1997.
Factors in anaphora resolution: theyare not the only things that matter.
a case study based ontwo different approaches.
In Proceedings of the ACL ?97 /EACL ?97 Workshop on Operational Factors in Practical,Robust Anaphora Resolution, pages 14?21.MUC-7.
1997.
Coreference task definition (v3.0, 13 Jul97).
In Proceedings of the Seventh Message Understand-ing Conference (MUC-7).Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 104?111.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguistics,27(4):521?544.Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2005.
Im-proving pronoun resolution using statistics-based seman-tic compatibility information.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 165?172, June.40
