R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
292 ?
301, 2005.?
Springer-Verlag Berlin Heidelberg 2005Using Multiple Discriminant Analysis Approachfor Linear Text SegmentationZhu  Jingbo1, Ye Na 1, Chang Xinzhi 1, Chen Wenliang 1, and Benjamin K Tsou21 Natural Language Processing Laboratory,Institute of Computer Software and Theory, Northeastern University, Shenyang, P.R.
China{zhujingbo, chenwl}@mail.neu.edu.cn{yena, changxz}@ics.neu.edu.cn2 Language Information Sciences Research Centre,City University of Hong Kong, HKrlbtsou@cityu.edu.hkAbstract.
Research on linear text segmentation has been an on-going focus inNLP for the last decade, and it has great potential for a wide range ofapplications such as document summarization, information retrieval and textunderstanding.
However, for linear text segmentation, there are two criticalproblems involving automatic boundary detection and automatic determinationof the number of segments in a document.
In this paper, we propose a newdomain-independent statistical model for linear text segmentation.
In ourmodel, Multiple Discriminant Analysis (MDA) criterion function is used toachieve global optimization in finding the best segmentation by means of thelargest word similarity within a segment and the smallest word similaritybetween segments.
To alleviate the high computational complexity problemintroduced by the model, genetic algorithms (GAs) are used.
Comparativeexperimental results show that our method based on MDA criterion functionshas achieved higher Pk measure (Beeferman) than that of the baseline systemusing TextTiling algorithm.1   IntroductionTypically a document is concerned with more than one subject, and most texts consistof long sequences of paragraphs with very little structural demarcation.
The goal oflinear text segmentation is to divide a document into topically-coherent sections, eachcorresponding to a relevant subject.
Linear text segmentation has been applied indocument summarization, information retrieval, and text understanding.
For example,in recent years, passage-retrieval techniques based on linear text segmentation, arebecoming increasingly popular in information retrieval as relevant text passages oftenprovide better answers than complete document texts in response to user queries[1].In recent years, many techniques have been applied to linear text segmentation.Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuationmarks, prosodic features, reference, and new words occurrence.
Others have usedstatistical methods[7,8,10,11,12,13,14,15] such as those based on word co-occurrence, lexical cohesion relations, semantic network, similarity between adjacentparts of texts, similarity between all parts of a text, dynamic programming algorithm,and HMM model.Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293In linear text segmentation study, there are two critical problems involvingautomatic boundary detection and automatic determination of the number of segmentsin a document.
Some efforts have focused on using similarity between adjacent partsof a text to solve topic boundary detection.
In fact, the similarity threshold is veryhard to set, and it is very difficult to identify exactly topic boundaries only accordingto similarity between adjacent parts of a text.
Other works have focused on thesimilarity between all parts of a text.
Reynar[7] and Choi[13] used dotplots techniqueto perform linear text segmentation which can be seen as a form of approximate andlocal optimization.
Yaari[16] has used agglomerative clustering to performhierarchical segmentation.
Others[10,17,18,19] used dynamic programming toperform exact and global optimization in which some prior parameters are needed.These parameters can be obtained via uninformative prior probabilities[18], orestimated from training data[19].In this paper, we propose a new statistical model for linear text segmentation,which uses Multiple Discriminant Analysis (MDA) method to define a globalcriterion function for document segmentation.
Our method focuses on within-segmentword similarity and between-segment word similarity.
This process can achieveglobal optimization in addressing the two aforementioned problems of linear textsegmentation.
Our method is domain-independent and does not use any training data.In section 2, we introduce Multiple Discriminant Analysis (MDA) criterionfunctions in detail.
In section 3, our statistical model of linear text segmentation isproposed.
A new MDA criterion function revised by adding penalty factor is furtherdiscussed in section 4.
Comparative experimental results are given in Section 5.
Atlast, we address conclusions and future work in section 6.2   MDA Criterion FunctionIn statistical pattern classification, MDA approach is commonly used to find effectivelinear transformations[20,21].
The MDA approach seeks a projection that bestseparates the data in a least-squares sense.
As shown in Figure 1, using MDA methodwe could get the greatest separation over data space when average within-classdistance is the smallest, and average between-class distance is the largest.Similarly, if we consider a document as data space, and a segment as a class, thebasic idea of our approach for linear text segmentation is to find best segmentation ofa document(greatest separation over data space) by focusing on within-segment wordsimilarity and between-segment word similarity.
It is clear that the smaller theaverage within-class distance or the average between-class distance, the larger thewithin-segment word similarity or the between-segment word similarity, and viceversa.
In other words, we want to find the best segmentation of a document in whichwithin-segment word similarity is the largest, and between-segment word similarity isthe smallest.
To achieve this goal, we introduce a criterion function to evaluate thesegmentation of a document and assign a score to it.
In this paper, we adopt the MDAapproach to define a global criterion function of document segmentation, and calledas MDA criterion function, which is described below.294 J. Zhu et alFig.
1.
When average within-class distance is the smallest, and average between-class distanceis the largest, the greatest separation over data space is shownLet W=w1w2?wt be a text consisting of t words, and let S=s1s2?sc be asegmentation of W consisting of c segments.
We define W as data space, S assegmentation distribution over data space W. Because the lengths of paragraphs orsentences can be highly irregular, unbalanced comparisons can result in textsegmentation process.
Thus we adopt the block method that is used in the TextTilingalgorithm[2,3], but we replace lexical word with block.
In our model, we groupblocksize words into a block which can be represented by a d-dimensional vector.
Inpractice, we find that the value of blocksize=100 works well for many Chinesedocuments.
Then W =w1w2?wt can be redefined as B=b1b2?bk.
As illustrated inFigure 1, a cross point can be defined as a d-dimensional block vector.In this paper, we introduce MDA criterion function Jd in the following form[20]( )( ) ( )BdWtr SJ str S=  (1)Where tr(A) is the trace of matrix A. SW and SB are within-segment scatter matrix andbetween-segment scatter matrix, respectively.
SW is defined by11 ( )( )ictW i i ii b siS P b m b mn= ?= ?
??
?
(2)Where b stands for blocks belonging to segment si, Pi is the a priori probability ofsegment si, and is defined to be the ratio of blocks in segment si divided by the totalnumber of blocks of the document, ni is the number of blocks in the segment si, mi isthe d-dimensional block mean of the segment si given by1.iib sim bn ?= ?
(3)D1 labeled class ?1D2 labeled class ?2D3 labeled class ?3Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 295Suppose that a total mean vector m is defined by11 1 ci iB im b n mn n== =?
?
(4)In equation (1), between-segment scatter matrix SB is defined by1( )( )ctB i i iiS P m m m m== ?
??
(5)3   Statistical Model for Linear Text SegmentationUsing the same definitions of text W, segmentation S and blocks B in section 2, wefirst discuss the statistical model for linear text segmentation.
The key of statisticalmodel for text segmentation is to find the segmentation with maximum-probability.This can be turned into another task of finding segmentation with highest Jd scoreequally.
The most likely segmentation is given by$ arg max P( | ) arg max ( , )def dS SS S W J W S= =  (6)As mentioned above, because paragraph or sentence length can be highly irregular,it leads to unbalanced comparisons in text segmentation process.
So W =w1w2?wncould be redefined as B=b1b2?bk, and the most likely segmentation is given by?
arg max P( | ) arg max ( , )def dS SS S B J B S= =  (7)The computational complexity for achieving the above solution is O(2k), where kis the number of blocks in a document.
To alleviate the high computationalcomplexity problem, we adopt the genetic algorithms (GAs)[22].
GAs provides alearning method motivated by an analogy to biological evolution.
Rather thansearching from general-to-specific hypotheses, or from simple-to-complex, GAsgenerate successor hypotheses by repeatedly mutating and recombining parts of thebest currently known hypotheses.
GAs have most commonly been applied tooptimization problems outside machine learning, and are especially suited to tasksin which hypotheses are complex.By adopting this methodology, we derive the following text segmentationalgorithm, as illustrated in Figure 2.
In this paper, we focus our study on paragraph-level linear text segmentation, in which the potential boundary mark betweensegments can be placed only between adjacent paragraphs.296 J. Zhu et alGiven a text W and blocks B, Kmax is the total number of paragraphs in the text.Initialization: Sbest = {}, Jd(B,Sbest)=0.0Segmentation:For k = 2 to KmaxBegin1) Use genetic algorithms and equation (7) to find the best segmentation Sof k segments.2)  If Jd(B,Sbest) < Jd(B,S) ThenBeginSbest = S and Jd(B,Sbest) = Jd(B,S).EndifEndforOutput the best segmentation Sbest.Fig.
2.
MDA-based text segmentation algorithm4   Penalty FactorIn the text segmentation process, adjacent boundary adjustment should beconsidered in cases when there are some very close adjacent but incorrect segmentboundaries.
In experiments we find that in these cases some single-sentenceparagraphs are wrongly recognized as isolated segments.
To solve the problem, wepropose a penalty factor (PF) to prevent assignment of very short segmentboundaries (such as a single-sentence segment) by adjusting very close adjacentboundaries, and therefore improve the performance of linear text segmentationsystem.Suppose that we get a segmentation S=s1s2?sc of the input document, let L be thelength of the document, Li be the length of the segment si.
We know L=L1+L2+?+Lc.We define penalty factor as1ciiLPFL== ?
(8)As can be seen, short-length segments would result in smaller penalty factor.
Weuse penalty factor to revise the Jd scores of segmentations.
To incorporate the penaltyfactor PF, our MDA criterion function Jd can be rewritten as1( )( ) ( ) ( )ci Bd PF di WL tr SJ x PF J xL tr S?== ?
= ??
(9)In the following experiments, we will evaluate effectiveness of using the two MDAcriterion functions Jd and Jd-PF for linear text segmentation.Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 2975   Experimental Results5.1   Evaluation MethodsPrecision and recall statistics are conventional means of evaluating the performanceof classification algorithms.
For the segmentation task, recall measures the fraction ofactual boundaries that an automatic segmenter correctly identifies, and precisionmeasures the fraction of boundaries identified by an automatic segmenter that areactual boundaries.
The shortcoming is that every inaccurately estimated segmentboundary is penalized equally whether it is near or far from a true segment boundary.To overcome the shortcoming of precision and recall, we use a measure called Pk,proposed by Beeferman et al[8].
Pk method measures the proportion of sentenceswhich are wrongly predicted to belong in the same segment or sentences which arewrongly predicted to belong in different segments.
More formally, given twosegmentations ref(true segmentation) and hyp(hypothetical segmentation) for adocument of n sentences, Pk is formally defined by1( , ) ( , )( ( , ) ( , ))k ref hypi j nP ref hyp D i j i j i j?
?
??
?
?= ??
(10)Where ?ref(i,j) is an indicator function whose value is 1 if sentences i and j belong inthe same segment in the true segmentation, and 0 otherwise.
Similarly, ?hyp(i,j) is anindicator function which evaluates to 1 if sentences i and j belong in the samesegment in the hypothetical segmentation, and 0 otherwise.
The operator between?ref(i,j) and ?hyp(i,j) in the above formula is the XNOR function on its two operands.The function D?is a distance probability distribution over the set of possible distancesbetween sentences chosen randomly from the document, and will in general dependon certain parameters ?
such as the average spacing between sentences.
In equation(10), D?was defined as an exponential distribution with mean 1/?, a parameter that wefix at the approximate mean document length for the domain[8].
( , ) i jD i j e ??
??
?
?=  (11)Where ?
?is a normalization chosen so that D?is a probability distribution over therange of distance it can accept.
From the above formulation, we could find oneweakness of the metric: there is no principled way of specifying the distancedistribution D?.
In the following experiments, we use Pk as performance measure,where the mean segment length in the test data was 1/?=11 sentences.5.2   Quantitative ResultsWe mainly focus our work on paragraph-level linear text segmentation techniques.The Hearst?s TextTiling algorithm[2,3] is a simple and domain-independent techniquefor linear text segmentation, which segments at the paragraph level.
Topic boundariesare determined by changes in the sequence of similarity scores.
This algorithm uses asimple cutoff function to determine automatically the number of boundaries.298 J. Zhu et alIn our experiments, we use the TextTiling algorithm to provide the baselinesystem, and use the Pk measure to evaluate and compare the performance of theTextTiling and our method.
Our data set - NEU_TS, is collected manually, and itconsists of 100 Chinese documents, all from 2004-2005 Chinese People?s Dailynewspaper.
The number of segments per document varies from five to eight.
Theaverage number of paragraphs per document is 25.8 paragraphs.
To build the groundtruth for NEU_TS data set, five trained graduate students in our laboratory who areworking on the analysis of Chinese document are asked to provide judgment on thesegmentation of every Chinese document.
We first use the toolkit CipSegSDK[23] fordocument preprocessing, including word segmentation, but with the removal ofstopwords from all documents.1)   Experiment 1In the first experiment, we assume the number of segments of an input document isknown in advance.
We use the NEU_TS data set and the Pk measure to evaluate andcompare the performance of TextTiling and our method.
The purpose of thisexperiment is to compare the performance of boundary detection techniques ofTextTiling algorithm and our model using MDA criterion functions.Table 1.
Pk value with known number of document segmentsMeasure TextTiling algorithm MDA methodusing JdMDA methodusing Jd-PFPk value 0.825 0.869 0.905In the TextTiling algorithm, topic boundaries are determined by changes in thesequence of similarity scores.
The boundaries are determined by locating thelowermost portions of valleys in the resulting plot.
Therefore, it is not a globalevaluation method.
However, in our model, MDA criterion function provides a globalevaluation method to text segmentation; it selects the best segmentation with thelargest within-segment word similarity and the smallest between-segment wordsimilarity.
Results shown in Table 1 indicated that our boundary detection techniquesbased on two MDA criterion functions perform better than the TextTiling algorithm,and MDA criterion function Jd-PF works the best.Table 2.
Pk value with unknown number of document segmentsMeasure TextTiling algorithm MDA methodusing JdMDA methodusing Jd-PFPk value 0.808 0.831 0.872)   Experiment 2In this experiment, we assume the number of segments of a document is unknown inadvance.
In other words, Texttiling algorithm and our model should determine thenumber of segments of a document automatically.
Similar to Experiment 1, the sameUsing Multiple Discriminant Analysis Approach for Linear Text Segmentation 299data set is used and the Pk measure is calculated for both TextTiling and our methodusing MDA criterion functions Jd and Jd-PF.
The comparative results are shownin Table 2.As mentioned above, how to determine the number of segments to be assigned to adocument is a difficult problem.
Texttiling algorithm uses a simple cutoff functionmethod to determine the number of segments and it is sensitive to the patterns ofsimilarity scores[2,3].
The cutoff function is defined as a function of the average andstandard deviations of the depth scores for the text under analysis.
A boundary is drawnonly if the depth score exceeds the cutoff value.
We think that the simple cutoff functionmethod is hard to achieve global optimization when solving these two key problems oflinear text segmentation process.
In our model, two MDA criterion functions Jd and Jd-PFare used to determine the number of segments and boundary detection by maximizing Jdscore of segmentations.
Once the maximum-score segmentation is found, the number ofsegments of the document is produced automatically.
Experimental results show thatour MDA criterion functions are superior to the TextTiling?s cutoff function in terms ofautomatic determination of the number of segments.
It is also shown that the MDAcriterion function Jd-PF revised with Penalty Factor works better than Jd.
Inimplementation, we have adopted genetic algorithms (GAs) to alleviate thecomputational complexity of MDA, and have obtained good results.6   Conclusions and Future WorkIn this paper, we studied and proposed a new domain-independent statistical modelfor linear text segmentation in which multiple discriminant analysis(MDA) approachis used as global criterion function for document segmentation.
We attempted toachieve global optimization in solving the two fundamental problems of textsegmentation involving automatic boundary detection and automatic determination ofnumber of segments of a document, by focusing on within-segment word similarityand between-segment word similarity.
We also applied genetic algorithms(GAs) toreduce the high computational complexity of MDA based method.
Experimentalresults show that our method based on MDA criterion functions outperforms theTextTiling algorithm.The solution to the high computational complexity problem will continue to bestudied by using other effective optimization algorithm or near optimal solutions.
In thenext stage we plan to combine MDA criterion functions with other algorithms such asclustering to improve the performance of our text segmentation system, and apply thetext segmentation technique to other text processing task, such as information retrievaland document summarization.AcknowledgementsWe thank Keh-Yih Su and Matthew Ma for discussions related to this work.
Thisresearch was supported in part by the National Natural Science Foundation of China& Microsoft Asia Research Centre(No.
60203019), the Key Project of ChineseMinistry of Education(No.
104065), and the National Natural Science Foundation ofChina(No.
60473140).300 J. Zhu et alReferences1.
Gerard Salton, Amit Singhal, Chris Buckley, and Mandar Mitra.
: Automatic textdecomposition using text segments and text themes.
In proceedings of the seventh ACMconference on Hypertext, Bethesda, Maryland, United States (1996) 53-652.
Hearst, M.A.
: Multi-paragraph segmentation of expository text.
In proceedings of the 32thAnnual Meeting of the Association for Computational Linguistics, Las Cruces, NewMexico (1994) 9-163.
Hearst, M.A.
: TextTiling: segmenting text into multi-paragraph subtopic passages.Computational Linguistics, Vol.23, No.1 (1997) 33-644.
Youmans, G.: A new tool for discourse analysis: The vocabulary management profile.Language, Vol.67, No.4 (1991) 763-7895.
Morris, J. and Hirst, G.: Lexical cohesion computed by thesauri relations as an indicator ofthe structure of text.
Computational Linguistics, Vol.17, No.1 (1991) 21-426.
Kozima, H.: Text segmentation based on similarity between words.
In proceedings of the31th Annual Meeting of the Association for Computational Linguistics, Student Session(1993) 286-2887.
Reynar, J.C.: An automatic method of finding topic boundaries.
In proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Student Session, LasCruces, New Mexico (1994) 331-3338.
Beeferman, D., Berger, A., and Lafferty, J.: Text segmentation using exponential models.In proceedings of the Second Conference on Empirical Methods in Natural LanguageProcessing, pages, Providence, Rhode Island (1997) 35-469.
Passoneau, R. and Litman, D.J.
: Intention-based segmentation: Human reliability andcorrelation with linguistic cues.
In proceedings of the 31st Meeting of the Association forComputational Linguistics (1993) 148-15510.
Jay M. Ponte and Bruce W.
Croft.
: Text segmentation by topic.
In proceeding of the firstEuropean conference on research and advanced technology for digital libraries.
U.Mass.Computer Science Technical Report TR97-18 (1997)11.
Reynar, J.C.: Statistical models for topic segmentation.
In proceedings of the 37th AnnualMeeting of the Association for Computational Linguistics (1999) 357-36412.
Hirschberg, J. and Grosz, B.: Intentional features of local and global discourse.
Inproceedings of the Workshop on Spoken Language Systems (1992) 441-44613.
Freddy Y. Y.
Choi.
: Advances in domain independent linear text segmentation.
In Proc.
ofNAACL-2000 (2000)14.
Choi, F.Y.Y., Wiemer-Hastings, P. & Moore, J.: Latent semantic analysis for textsegmentation.
In proceedings of the 6th Conference on Empirical Methods in NaturalLanguage Processing (2001) 109-117.15.
Blei, D.M.
and Moreno, P.J.
: Topic segmentation with an aspect hidden Markov model.Tech.
Rep. CRL 2001-07, COMPAQ Cambridge Research Lab (2001)16.
Yaari, Y.: Segmentation of expository texts by hierarchical agglomerative clustering.
Inproceedings of the conference on recent advances in natural language processing (1997)59-6517.
Heinonen, O.: Optimal multi-paragraph text segmentation by dynamic programming.
Inproceedings of 17th international conference on computational linguistics (1998) 1484-1486.18.
Utiyama, M., and Isahara, H.: A statistical model for domain-independent textsegmentation.
In proceedings of the 9th conference of the European chapter of theassociation for computational linguistics (2001) 491-498Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 30119.
A Kehagias, P Fragkou, V Petridis.
: Linear Text Segmentation using a DynamicProgramming Algorithm.
In proceedings of 10th Conference of European chapter of theassociation for computational linguistics (2003)20.
R. Duda, P. Hart, and D.
Stork.
: Pattern Classification.
Second Edition, John Wiley &Sons (2001)21.
Julius T.Tol and Rafael C.
Gonzaiez.
: Pattern recognition principles.
Addison-WesleyPublishing Company (1974)22.
Tom M.Mitchell.
: Machine Learning.
McGraw-Hill (1997)23.
Yao Tianshun, Zhu Jingbo, Zhang li, and Yang Ying.
: Natural language processing-research on making computers understand human languages.
Tsinghua universitypress (2002)
