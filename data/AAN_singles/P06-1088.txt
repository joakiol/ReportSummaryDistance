Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 697?704,Sydney, July 2006. c?2006 Association for Computational LinguisticsMulti-Tagging for Lexicalized-Grammar ParsingJames R. CurranSchool of ITUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auStephen ClarkComputing LaboratoryOxford UniversityWolfson BuildingParks RoadOxford, OX1 3QD, UKsclark@comlab.ox.ac.ukDavid VadasSchool of ITUniversity of SydneyNSW 2006, Australiadvadas1@it.usyd.edu.auAbstractWith performance above 97% accuracy fornewspaper text, part of speech (POS) tag-ging might be considered a solved prob-lem.
Previous studies have shown thatallowing the parser to resolve POS tagambiguity does not improve performance.However, for grammar formalisms whichuse more fine-grained grammatical cate-gories, for example TAG and CCG, taggingaccuracy is much lower.
In fact, for theseformalisms, premature ambiguity resolu-tion makes parsing infeasible.We describe a multi-tagging approachwhich maintains a suitable level of lexicalcategory ambiguity for accurate and effi-cient CCG parsing.
We extend this multi-tagging approach to the POS level to over-come errors introduced by automaticallyassigned POS tags.
Although POS taggingaccuracy seems high, maintaining somePOS tag ambiguity in the language pro-cessing pipeline results in more accurateCCG supertagging.1 IntroductionState-of-the-art part of speech (POS) tagging ac-curacy is now above 97% for newspaper text(Collins, 2002; Toutanova et al, 2003).
One pos-sible conclusion from the POS tagging literatureis that accuracy is approaching the limit, and anyremaining improvement is within the noise of thePenn Treebank training data (Ratnaparkhi, 1996;Toutanova et al, 2003).So why should we continue to work on the POStagging problem?
Here we give two reasons.
First,for lexicalized grammar formalisms such as TAGand CCG, the tagging problem is much harder.Second, any errors in POS tagger output, even at97% acuracy, can have a significant impact oncomponents further down the language processingpipeline.
In previous work we have shown that us-ing automatically assigned, rather than gold stan-dard, POS tags reduces the accuracy of our CCGparser by almost 2% in dependency F-score (Clarkand Curran, 2004b).CCG supertagging is much harder than POS tag-ging because the CCG tag set consists of fine-grained lexical categories, resulting in a larger tagset ?
over 400 CCG lexical categories comparedwith 45 Penn Treebank POS tags.
In fact, usinga state-of-the-art tagger as a front end to a CCGparser makes accurate parsing infeasible becauseof the high supertagging error rate.Our solution is to use multi-tagging, in whicha CCG supertagger can potentially assign morethan one lexical category to a word.
In thispaper we significantly improve our earlier ap-proach (Clark and Curran, 2004a) by adapting theforward-backward algorithm to a Maximum En-tropy tagger, which is used to calculate a proba-bility distribution over lexical categories for eachword.
This distribution is used to assign one ormore categories to each word (Charniak et al,1996).
We report large increases in accuracy oversingle-tagging at only a small cost in increasedambiguity.A further contribution of the paper is to alsouse multi-tagging for the POS tags, and to main-tain some POS ambiguity in the language process-ing pipeline.
In particular, since POS tags are im-portant features for the supertagger, we investigatehow supertagging accuracy can be improved bynot prematurely committing to a POS tag decision.Our results first demonstrate that a surprising in-697crease in POS tagging accuracy can be achievedwith only a tiny increase in ambiguity; and secondthat maintaining some POS ambiguity can signifi-cantly improve the accuracy of the supertagger.The parser uses the CCG lexical categories tobuild syntactic structure, and the POS tags areused by the supertagger and parser as part of theirstatisical models.
We show that using a multi-tagger for supertagging results in an effective pre-processor for CCG parsing, and that using a multi-tagger for POS tagging results in more accurateCCG supertagging.2 Maximum Entropy TaggingThe tagger uses conditional probabilities of theform P (y|x) where y is a tag and x is a localcontext containing y.
The conditional probabili-ties have the following log-linear form:P (y|x) =1Z(x)e?i?ifi(x,y) (1)where Z(x) is a normalisation constant which en-sures a proper probability distribution for eachcontext x.The feature functions fi(x, y) are binary-valued, returning either 0 or 1 depending on thetag y and the value of a particular contextual pred-icate given the context x. Contextual predicatesidentify elements of the context which might beuseful for predicting the tag.
For example, the fol-lowing feature returns 1 if the current word is theand the tag is DT; otherwise it returns 0:fi(x, y) ={1 if word(x) = the & y = DT0 otherwise(2)word(x) = the is an example of a contextualpredicate.
The POS tagger uses the same con-textual predicates as Ratnaparkhi (1996); the su-pertagger adds contextual predicates correspond-ing to POS tags and bigram combinations of POStags (Curran and Clark, 2003).Each feature fi has an associated weight ?iwhich is determined during training.
The trainingprocess aims to maximise the entropy of the modelsubject to the constraints that the expectation ofeach feature according to the model matches theempirical expectation from the training data.
Thiscan be also thought of in terms of maximum like-lihood estimation (MLE) for a log-linear model(Della Pietra et al, 1997).
We use the L-BFGS op-timisation algorithm (Nocedal and Wright, 1999;Malouf, 2002) to perform the estimation.MLE has a tendency to overfit the training data.We adopt the standard approach of Chen andRosenfeld (1999) by introducing a Gaussian priorterm to the objective function which penalises fea-ture weights with large absolute values.
A param-eter defined in terms of the standard deviation ofthe Gaussian determines the degree of smoothing.The conditional probability of a sequence oftags, y1, .
.
.
, yn, given a sentence, w1, .
.
.
, wn, isdefined as the product of the individual probabili-ties for each tag:P (y1, .
.
.
, yn|w1, .
.
.
, wn) =n?i=1P (yi|xi) (3)where xi is the context for word wi.
We use thestandard approach of Viterbi decoding to find thehighest probability sequence.2.1 Multi-taggingMulti-tagging ?
assigning one or more tags to aword ?
is used here in two ways: first, to retainambiguity in the CCG lexical category sequencefor the purpose of building parse structure; andsecond, to retain ambiguity in the POS tag se-quence.
We retain ambiguity in the lexical cate-gory sequence since a single-tagger is not accurateenough to serve as a front-end to a CCG parser, andwe retain some POS ambiguity since POS tags areused as features in the statistical models of the su-pertagger and parser.Charniak et al (1996) investigated multi-POStagging in the context of PCFG parsing.
It wasfound that multi-tagging provides only a minorimprovement in accuracy, with a significant lossin efficiency; hence it was concluded that, giventhe particular parser and tagger used, a single-tagPOS tagger is preferable to a multi-tagger.
Morerecently, Watson (2006) has revisited this questionin the context of the RASP parser (Briscoe and Car-roll, 2002) and found that, similar to Charniak etal.
(1996), multi-tagging at the POS level results ina small increase in parsing accuracy but at somecost in efficiency.For lexicalized grammars, such as CCG andTAG, the motivation for using a multi-tagger to as-sign the elementary structures (supertags) is morecompelling.
Since the set of supertags is typ-ically much larger than a standard POS tag set,the tagging problem becomes much harder.
In698fact, when using a state-of-the-art single-tagger,the per-word accuracy for CCG supertagging is solow (around 92%) that wide coverage, high ac-curacy parsing becomes infeasible (Clark, 2002;Clark and Curran, 2004a).
Similar results havebeen found for a highly lexicalized HPSG grammar(Prins and van Noord, 2003), and also for TAG.As far as we are aware, the only approach to suc-cessfully integrate a TAG supertagger and parser isthe Lightweight Dependency Analyser of Banga-lore (2000).
Hence, in order to perform effectivefull parsing with these lexicalized grammars, thetagger front-end must be a multi-tagger (given thecurrent state-of-the-art).The simplest approach to CCG supertagging isto assign all categories to a word which the wordwas seen with in the data.
This leaves the parserthe task of managing the very large parse space re-sulting from the high degree of lexical categoryambiguity (Hockenmaier and Steedman, 2002;Hockenmaier, 2003).
However, one of the orig-inal motivations for supertagging was to signifi-cantly reduce the syntactic ambiguity before fullparsing begins (Bangalore and Joshi, 1999).
Clarkand Curran (2004a) found that performing CCGsupertagging prior to parsing can significantly in-crease parsing efficiency with no loss in accuracy.Our multi-tagging approach follows that ofClark and Curran (2004a) and Charniak et al(1996): assign all categories to a word whoseprobabilities are within a factor, ?, of the proba-bility of the most probable category for that word:Ci = {c | P (Ci = c|S) > ?
P (Ci = cmax|S)}Ci is the set of categories assigned to the ith word;Ci is the random variable corresponding to the cat-egory of the ith word; cmax is the category with thehighest probability of being the category of the ithword; and S is the sentence.
One advantage of thisadaptive approach is that, when the probability ofthe highest scoring category is much greater thanthe rest, no extra categories will be added.Clark and Curran (2004a) propose a simplemethod for calculating P (Ci = c|S): use theword and POS features in the local context to cal-culate the probability and ignore the previouslyassigned categories (the history).
However, it ispossible to incorporate the history in the calcula-tion of the tag probabilities.
A greedy approach isto use the locally highest probability history as afeature, which avoids any summing over alterna-tive histories.
Alternatively, there is a well-knowndynamic programming algorithm ?
the forwardbackward algorithm ?
which efficiently calcu-lates P (Ci = c|S) (Charniak et al, 1996).The multitagger uses the following conditionalprobabilities:P (yi|w1,n) =?y1,i?1,yi+1,nP (yi, y1,i?1, yi+1,n|w1,n)where xi,j = xi, .
.
.
xj .
Here yi is to be thought ofas a fixed category, whereas yj (j 6= i) varies overthe possible categories for word j.
In words, theprobability of category yi, given the sentence, isthe sum of the probabilities of all sequences con-taining yi.
This sum is calculated efficiently usingthe forward-backward algorithm:P (Ci = c|S) = ?i(c)?i(c) (4)where ?i(c) is the total probability of all the cate-gory sub-sequences that end at position i with cat-egory c; and ?i(c) is the total probability of all thecategory sub-sequences through to the end whichstart at position i with category c.The standard description of the forward-backward algorithm, for example Manning andSchutze (1999), is usually given for an HMM-styletagger.
However, it is straightforward to adapt thealgorithm to the Maximum Entropy models usedhere.
The forward-backward algorithm we use issimilar to that for a Maximum Entropy MarkovModel (Lafferty et al, 2001).POS tags are very informative features for thesupertagger, which suggests that using a multi-POS tagger may benefit the supertagger (and ulti-mately the parser).
However, it is unclear whethermulti-POS tagging will be useful in this context,since our single-tagger POS tagger is highly accu-rate: over 97% for WSJ text (Curran and Clark,2003).
In fact, in Clark and Curran (2004b) we re-port that using automatically assigned, as opposedto gold-standard, POS tags as features results in a2% loss in parsing accuracy.
This suggests that re-taining some ambiguity in the POS sequence maybe beneficial for supertagging and parsing accu-racy.
In Section 4 we show this is the case forsupertagging.3 CCG Supertagging and ParsingParsing using CCG can be viewed as a two-stageprocess: first assign lexical categories to the wordsin the sentence, and then combine the categories699The WSJ is a paper that I enjoy readingNP/N N (S [dcl ]\NP)/NP NP/N N (NP\NP)/(S [dcl ]/NP) NP (S [dcl ]\NP)/(S [ng ]\NP) (S [ng ]\NP)/NPFigure 1: Example sentence with CCG lexical categories.together using CCG?s combinatory rules.1 We per-form stage one using a supertagger.The set of lexical categories used by the su-pertagger is obtained from CCGbank (Hocken-maier, 2003), a corpus of CCG normal-formderivations derived semi-automatically from thePenn Treebank.
Following our earlier work, weapply a frequency cutoff to the training set, onlyusing those categories which appear at least 10times in sections 02-21, which results in a set of425 categories.
We have shown that the resultingset has very high coverage on unseen data (Clarkand Curran, 2004a).
Figure 1 gives an examplesentence with the CCG lexical categories.The parser is described in Clark and Curran(2004b).
It takes POS tagged sentences as inputwith each word assigned a set of lexical categories.A packed chart is used to efficiently representall the possible analyses for a sentence, and theCKY chart parsing algorithm described in Steed-man (2000) is used to build the chart.
A log-linearmodel is used to score the alternative analyses.In Clark and Curran (2004a) we described anovel approach to integrating the supertagger andparser: start with a very restrictive supertagger set-ting, so that only a small number of lexical cate-gories is assigned to each word, and only assignmore categories if the parser cannot find a span-ning analysis.
This strategy results in an efficientand accurate parser, with speeds up to 35 sen-tences per second.
Accurate supertagging at lowlevels of lexical category ambiguity is thereforeparticularly important when using this strategy.We found in Clark and Curran (2004b) that alarge drop in parsing accuracy occurs if automat-ically assigned POS tags are used throughout theparsing process, rather than gold standard POStags (almost 2% F-score over labelled dependen-cies).
This is due to the drop in accuracy of thesupertagger (see Table 3) and also the fact thatthe log-linear parsing model uses POS tags as fea-tures.
The large drop in parsing accuracy demon-strates that improving the performance of POS tag-1See Steedman (2000) for an introduction to CCG, andsee Hockenmaier (2003) for an introduction to wide-coverageparsing using CCG.TAGS/WORD ?
WORD ACC SENT ACC1.00 1 96.7 51.81.01 0.8125 97.1 55.41.05 0.2969 98.3 70.71.10 0.1172 99.0 80.91.20 0.0293 99.5 89.31.30 0.0111 99.6 91.71.40 0.0053 99.7 93.24.23 0 99.8 94.8Table 1: POS tagging accuracy on Section 00 fordifferent levels of ambiguity.gers is still an important research problem.
In thispaper we aim to reduce the performance drop ofthe supertagger by maintaing some POS ambiguitythrough to the supertagging phase.
Future workwill investigate maintaining some POS ambiguitythrough to the parsing phase also.4 Multi-tagging ExperimentsWe performed several sets of experiments forPOS tagging and CCG supertagging to explore thetrade-off between ambiguity and tagging accuracy.For both POS tagging and supertagging we variedthe average number of tags assigned to each word,to see whether it is possible to significantly in-crease tagging accuracy with only a small increasein ambiguity.
For CCG supertagging, we also com-pared multi-tagging approaches, with a fixed cate-gory ambiguity of 1.4 categories per word.All of the experiments used Section 02-21 ofCCGbank as training data, Section 00 as develop-ment data and Section 23 as final test data.
Weevaluate both per-word tag accuracy and sentenceaccuracy, which is the percentage of sentences forwhich every word is tagged correctly.
For themulti-tagging results we consider the word to betagged correctly if the correct tag appears in theset of tags assigned to the word.4.1 ResultsTable 1 shows the results for multi-POS taggingfor different levels of ambiguity.
The row corre-sponding to 1.01 tags per word shows that adding700METHOD GOLD POS AUTO POSWORD SENT WORD SENTsingle 92.6 36.8 91.5 32.7noseq 96.2 51.9 95.2 46.1best hist 97.2 63.8 96.3 57.2fwdbwd 97.9 72.1 96.9 64.8Table 2: Supertagging accuracy on Section 00 us-ing different approaches with multi-tagger ambi-guity fixed at 1.4 categories per word.TAGS/ GOLD POS AUTO POSWORD ?
WORD SENT WORD SENT1.0 1 92.6 36.8 91.5 32.71.2 0.1201 96.8 63.4 95.8 56.51.4 0.0337 97.9 72.1 96.9 64.81.6 0.0142 98.3 76.4 97.5 69.31.8 0.0074 98.4 78.3 97.7 71.02.0 0.0048 98.5 79.4 97.9 72.52.5 0.0019 98.7 80.6 98.1 74.33.0 0.0009 98.7 81.4 98.3 75.612.5 0 98.9 82.3 98.8 80.1Table 3: Supertagging accuracy on Section 00 fordifferent levels of ambiguity.even a tiny amount of ambiguity (1 extra tag in ev-ery 100 words) gives a reasonable improvement,whilst adding 1 tag in 20 words, or approximatelyone extra tag per sentence on the WSJ, gives a sig-nificant boost of 1.6% word accuracy and almost20% sentence accuracy.The bottom row of Table 1 gives an upper boundon accuracy if the maximum ambiguity is allowed.This involves setting the ?
value to 0, so all feasi-ble tags are assigned.
Note that the performancegain is only 1.6% in sentence accuracy, comparedwith the previous row, at the cost of a large in-crease in ambiguity.Our first set of CCG supertagging experimentscompared the performance of several approaches.In Table 2 we give the accuracies when using goldstandard POS tags, and also POS tags automaticallyassigned by our POS tagger described above.
SincePOS tags are important features for the supertaggermaximum entropy model, erroneous tags have asignificant impact on supertagging accuracy.The singlemethod is the single-tagger supertag-ger, which at 91.5% per-word accuracy is too inac-curate for use with the CCG parser.
The remainingrows in the table give multi-tagger results for a cat-egory ambiguity of 1.4 categories per word.
Thenoseq method, which performs significantly betterthan single, does not take into account the previ-ously assigned categories.
The best hist methodgains roughly another 1% in accuracy over noseqby taking the greedy approach of using only thetwo most probable previously assigned categories.Finally, the full forward-backward approach de-scribed in Section 2.1 gains roughly another 0.6%by considering all possible category histories.
Wesee the largest jump in accuracy just by returningmultiple categories.
The other more modest gainscome from producing progressively better modelsof the category sequence.The final set of supertagging experiments in Ta-ble 3 demonstrates the trade-off between ambigu-ity and accuracy.
Note that the ambiguity levelsneed to be much higher to produce similar perfor-mance to the POS tagger and that the upper boundcase (?
= 0) has a very high average ambiguity.This is to be expected given the much larger CCGtag set.5 Tag uncertainty thoughout the pipelineTables 2 and 3 show that supertagger accuracywhen using gold-standard POS tags is typically1% higher than when using automatically assignedPOS tags.
Clearly, correct POS tags are importantfeatures for the supertagger.Errors made by the supertagger can multiplyout when incorrect lexical categories are passedto the parser, so a 1% increase in lexical categoryerror can become much more significant in theparser evaluation.
For example, when using thedependency-based evaluation in Clark and Curran(2004b), getting the lexical category wrong for aditransitive verb automatically leads to three de-pendencies in the output being incorrect.We have shown that multi-tagging can signif-icantly increase the accuracy of the POS taggerwith only a small increase in ambiguity.
Whatwe would like to do is maintain some degree ofPOS tag ambiguity and pass multiple POS tagsthrough to the supertagging stage (and eventuallythe parser).
There are several ways to encode mul-tiple POS tags as features.
The simplest approachis to treat all of the POS tags as binary features,but this does not take into account the uncertaintyin each of the alternative tags.
What we need is away of incorporating probability information intothe Maximum Entropy supertagger.7016 Real-values in ME modelsMaximum Entropy (ME) models, in the NLP lit-erature, are typically defined with binary features,although they do allow real-valued features.
Theonly constraint comes from the optimisation algo-rithm; for example, GIS only allows non-negativevalues.
Real-valued features are commonly usedwith other machine learning algorithms.Binary features suffer from certain limitationsof the representation, which make them unsuitablefor modelling some properties.
For example, POStaggers have difficulty determining if capitalised,sentence initial words are proper nouns.
A usefulway to model this property is to determine the ra-tio of capitalised and non-capitalised instances ofa particular word in a large corpus and use a real-valued feature which encodes this ratio (Vadas andCurran, 2005).
The only way to include this fea-ture in a binary representation is to discretize (orbin) the feature values.
For this type of feature,choosing appropriate bins is difficult and it may behard to find a discretization scheme that performsoptimally.Another problem with discretizing feature val-ues is that it imposes artificial boundaries to definethe bins.
For the example above, we may choosethe bins 0 ?
x < 1 and 1 ?
x < 2, which sepa-rate the values 0.99 and 1.01 even though they areclose in value.
At the same time, the model doesnot distinguish between 0.01 and 0.99 even thoughthey are much further apart.Further, if we have not seen cases for the bin2 ?
x < 3, then the discretized model has no evi-dence to determine the contribution of this feature.But for the real-valued model, evidence support-ing 1 ?
x < 2 and 3 ?
x < 4 provides evidencefor the missing bin.
Thus the real-valued modelgeneralises more effectively.One issue that is not addressed here is the inter-action between the Gaussian smoothing parameterand real-valued features.
Using the same smooth-ing parameter for real-valued features with vastlydifferent distributions is unlikely to be optimal.However, for these experiments we have used thesame value for the smoothing parameter on allreal-valued features.
This is the same value wehave used for the binary features.7 Multi-POS Supertagging ExperimentsWe have experimented with four different ap-proaches to passing multiple POS tags as featuresthrough to the supertagger.
For the later exper-iments, this required the existing binary-valuedframework to be extended to support real values.The level of POS tag ambiguity was varied be-tween 1.05 and 1.3 POS tags per word on average.These results are shown in Table 4.The first approach is to treat the multiple POStags as binary features (bin).
This simply involvesadding the multiple POS tags for each word inboth the training and test data.
Every assignedPOS tag is treated as a separate feature and con-sidered equally important regardless of its uncer-tainty.
Here we see a minor increase in perfor-mance over the original supertagger at the lowerlevels of POS ambiguity.
However, as the POSambiguity is increased, the performance of thebinary-valued features decreases and is eventuallyworse than the original supertagger.
This is be-cause at the lowest levels of ambiguity the extraPOS tags can be treated as being of similar reli-ability.
However, at higher levels of ambiguitymany POS tags are added which are unreliable andshould not be trusted equally.The second approach (split) uses real-valuedfeatures to model some degree of uncertainty inthe POS tags, dividing the POS tag probability massevenly among the alternatives.
This has the ef-fect of giving smaller feature values to tags wheremany alternative tags have been assigned.
Thisproduces similar results to the binary-valued fea-tures, again performing best at low levels of ambi-guity.The third approach (invrank) is to use the in-verse rank of each POS tag as a real-valued feature.The inverse rank is the reciprocal of the tag?s rankordered by decreasing probability.
This methodassumes the POS tagger correctly orders the alter-native tags, but does not rely on the probabilityassigned to each tag.
Overall, invrank performsworse than split.The final and best approach is to use the prob-abilities assigned to each alternative tag as real-valued features:fi(x, y) ={p(POS(x) = NN) if y = NP0 otherwise(5)This model gives the best performance at 1.1 POStags per-word average ambiguity.
Note that, evenwhen using the probabilities as features, only asmall amount of additional POS ambiguity is re-quired to significantly improve performance.702METHOD POS AMB WORD SENTorig 1.00 96.9 64.8bin 1.05 97.3 67.71.10 97.3 66.31.20 97.0 63.51.30 96.8 62.1split 1.05 97.4 68.51.10 97.4 67.91.20 97.3 67.01.30 97.2 65.1prob 1.05 97.5 68.71.10 97.5 69.11.20 97.5 68.71.30 97.5 68.7invrank 1.05 97.3 68.01.10 97.4 68.01.20 97.3 67.11.30 97.3 67.1gold - 97.9 72.1Table 4: Multi-POS supertagging on Section 00with different levels of POS ambiguity and usingdifferent approaches to POS feature encoding.Table 5 shows our best performance figures forthe multi-POS supertagger, against the previouslydescribed method using both gold standard and au-tomatically assigned POS tags.Table 6 uses the Section 23 test data todemonstrate the improvement in supertaggingwhen moving from single-tagging (single) to sim-ple multi-tagging (noseq); from simple multi-tagging to the full forward-backward algorithm(fwdbwd); and finally when using the probabilitiesof multiply-assigned POS tags as features (MULTI-POS column).
All of these multi-tagging experi-ments use an ambiguity level of 1.4 categories perword and the last result uses POS tag ambiguity of1.1 tags per word.8 ConclusionThe NLP community may consider POS tagging tobe a solved problem.
In this paper, we have sug-gested two reasons why this is not the case.
First,tagging for lexicalized-grammar formalisms, suchas CCG and TAG, is far from solved.
Second,even modest improvements in POS tagging accu-racy can have a large impact on the performance ofdownstream components in a language processingpipeline.TAGS/ AUTO POS MULTI POS GOLD POSWORD WORD SENT WORD SENT WORD SENT1.0 91.5 32.7 91.9 34.3 92.6 36.81.2 95.8 56.5 96.3 59.2 96.8 63.41.4 96.9 64.8 97.5 67.0 97.9 72.11.6 97.5 69.3 97.9 73.3 98.3 76.41.8 97.7 71.0 98.2 76.1 98.4 78.32.0 97.9 72.5 98.4 77.4 98.5 79.42.5 98.1 74.3 98.5 78.7 98.7 80.63.0 98.3 75.6 98.6 79.7 98.7 81.4Table 5: Best multi-POS supertagging accuracy onSection 00 using POS ambiguity of 1.1 and theprobability real-valued features.METHOD AUTO POS MULTI POS GOLD POSsingle 92.0 - 93.3noseq 95.4 - 96.6fwdbwd 97.1 97.7 98.2Table 6: Final supertagging results on Section 23.We have developed a novel approach to main-taining tag ambiguity in language processingpipelines which avoids premature ambiguity res-olution.
The tag ambiguity is maintained by usingthe forward-backward algorithm to calculate indi-vidual tag probabilities.
These probabilities canthen be used to select multiple tags and can alsobe encoded as real-valued features in subsequentstatistical models.With this new approach we have increased POStagging accuracy significantly with only a tiny am-biguity penalty and also significantly improved onprevious CCG supertagging results.
Finally, us-ing POS tag probabilities as real-valued features inthe supertagging model, we demonstrated perfor-mance close to that obtained with gold-standardPOS tags.
This will significantly improve the ro-bustness of the parser on unseen text.In future work we will investigate maintainingtag ambiguity further down the language process-ing pipeline and exploiting the uncertainty fromprevious stages.
In particular, we will incorporatereal-valued POS tag and lexical category featuresin the statistical parsing model.
Another possibil-ity is to investigate whether similar techniques canimprove other tagging tasks, such as Named EntityRecognition.This work can be seen as part of the largergoal of maintaining ambiguity and exploiting un-703certainty throughout language processing systems(Roth and Yih, 2004), which is important for cop-ing with the compounding of errors that is a sig-nificant problem in language processing pipelines.AcknowledgementsWe would like to thank the anonymous reviewersfor their helpful feedback.
This work has beensupported by the Australian Research Council un-der Discovery Project DP0453131.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertagging:An approach to almost parsing.
Computational Linguis-tics, 25(2):237?265.Srinivas Bangalore.
2000.
A lightweight dependency anal-yser for partial parsing.
Natural Language Engineering,6(2):113?138.Ted Briscoe and John Carroll.
2002.
Robust accurate statis-tical annotation of general tex.
In Proceedings of the 3rdLREC Conference, pages 1499?1504, Las Palmas, GranCanaria.Eugene Charniak, Glenn Carroll, John Adcock, AnthonyCassandra, Yoshihiko Gotoh, Jeremy Katz, MichaelLittman, and John McCann.
1996.
Taggers for parsers.Artificial Intelligence, 85:45?57.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussian priorfor smoothing maximum entropy models.
Technical re-port, Carnegie Mellon University, Pittsburgh, PA.Stephen Clark and James R. Curran.
2004a.
The impor-tance of supertagging for wide-coverage CCG parsing.In Proceedings of COLING-04, pages 282?288, Geneva,Switzerland.Stephen Clark and James R. Curran.
2004b.
Parsing theWSJ using CCG and log-linear models.
In Proceedings ofthe 42nd Meeting of the ACL, pages 104?111, Barcelona,Spain.Stephen Clark.
2002.
A supertagger for Combinatory Cate-gorial Grammar.
In Proceedings of the TAG+ Workshop,pages 19?24, Venice, Italy.Michael Collins.
2002.
Discriminative training methods forHidden Markov Models: Theory and experiments withperceptron algorithms.
In Proceedings of the EMNLPConference, pages 1?8, Philadelphia, PA.James R. Curran and Stephen Clark.
2003.
Investigating GISand smoothing for maximum entropy taggers.
In Proceed-ings of the 10th Meeting of the EACL, pages 91?98, Bu-dapest, Hungary.Stephen Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing features of random fields.
IEEETransactions Pattern Analysis and Machine Intelligence,19(4):380?393.Julia Hockenmaier and Mark Steedman.
2002.
Generativemodels for statistical parsing with Combinatory CategorialGrammar.
In Proceedings of the 40th Meeting of the ACL,pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D.thesis, University of Edinburgh.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proceedings ofthe 18th International Conference on Machine Learning,pages 282?289, Williams College, MA.Robert Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedings ofthe Sixth Workshop on Natural Language Learning, pages49?55, Taipei, Taiwan.Christopher Manning and Hinrich Schutze.
1999.
Foun-dations of Statistical Natural Language Processing.
TheMIT Press, Cambridge, Massachusetts.Jorge Nocedal and Stephen J. Wright.
1999.
Numerical Op-timization.
Springer, New York, USA.Robbert Prins and Gertjan van Noord.
2003.
Reinforcingparser preferences through tagging.
Traitement Automa-tique des Langues, 44(3):121?139.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLP Conference,pages 133?142, Philadelphia, PA.D.
Roth and W. Yih.
2004.
A linear programming for-mulation for global inference in natural language tasks.In Hwee Tou Ng and Ellen Riloff, editors, Proc.
of theAnnual Conference on Computational Natural LanguageLearning (CoNLL), pages 1?8.
Association for Computa-tional Linguistics.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.Kristina Toutanova, Dan Klein, Christopher Manning, andYoram Singer.
2003.
Feature-rich part-of-speech tag-ging with a cyclic dependency network.
In Proceedingsof the HLT/NAACL conference, pages 252?259, Edmon-ton, Canada.David Vadas and James R. Curran.
2005.
Tagging un-known words with raw text features.
In Proceedings of theAustralasian Language Technology Workshop 2005, pages32?39, Sydney, Australia.Rebecca Watson.
2006.
Part-of-speech tagging models forparsing.
In Proceedings of the Computaional Linguisticsin the UK Conference (CLUK-06), Open University, Mil-ton Keynes, UK.704
