Coling 2010: Poster Volume, pages 851?859,Beijing, August 2010Imbalanced Classification Using Dictionary-based Prototypes andHierarchical Decision Rules for Entity Sense DisambiguationTingting MuNational Centre for Text MiningUniversity of Manchestertingting.mu@man.ac.ukXinglong WangNational Centre for Text MiningUniversity of Manchesterxinglong.wang@man.ac.ukJun?ichi TsujiiDepartment of Computer ScienceUniversity of Tokyotsujii@is.s.u-tokyo.ac.jpSophia AnaniadouNational Centre for Text MiningUniversity of ManchesterSophia.Ananiadou@man.ac.ukAbstractEntity sense disambiguation becomes dif-ficult with few or even zero training in-stances available, which is known as im-balanced learning problem in machinelearning.
To overcome the problem, wecreate a new set of reliable training in-stances from dictionary, called dictionary-based prototypes.
A hierarchical classifi-cation system with a tree-like structure isdesigned to learn from both the prototypesand training instances, and three differenttypes of classifiers are employed.
In addi-tion, supervised dimensionality reductionis conducted in a similarity-based space.Experimental results show our system out-performs three baseline systems by at least8.3% as measured by macro F1 score.1 IntroductionAmbiguities in terms and named entities are achallenge for automatic information extraction(IE) systems.
The problem is particularly acutefor IE systems targeting the biomedical domain,where unambigiously identifying terms is of fun-damental importance.
In biomedical text, a term(or its abbreviation (Okazaki et al, 2010)) maybelong to a wide variety of semantic categories(e.g., gene, disease, etc.).
For example, ER maydenote protein estrogen receptor in one context,but cell subunit endoplasmic reticulum in another,not to mention it can also mean emergency room.In addition, same terms (e.g., protein) may be-long to many model organisms, due to the nomen-clature of gene and gene products, where genesin model organisms other than human are given,whenever possible, the same names as their hu-man orthologs (Wain et al, 2002).
On the otherhand, public biological databases keep species-specific records for the same protein or gene,making species disambiguation an inevitable stepfor assigning unique database identifiers to entitynames in text (Hakenberg et al, 2008; Krallingeret al, 2008).One way to entity disambiguation is classify-ing an entity into pre-defined semantic categories,based on its context (e.g., (Bunescu and Pas?ca,2006)).
Existing classifiers, such as maximumentropy model, achieved satisfactory results onthe ?majority?
classes with abundant training in-stances, but failed on the ?minority?
ones with fewor even zero training instances, i.e., the knowl-edge acquisition bottleneck (Agirre and Martinez,2004).
Furthermore, it is often infeasible to cre-ate enough training data for all existing semanticclasses.
In addition, too many training instancesfor certain majority classes lead to increased com-putational complexity for training, and a biasedsystem ignoring the minority ones.
These corre-spond to two previously addressed difficulties inimbalanced learning: ?...
either (i) you have farmore data than your algorithms can deal with,851and you have to select a sample, or (ii) you haveno data at all and you have to go through an in-volved process to create them?
(Provost, 2000).Given an entity disambiguation task with imbal-anced data, this paper explores how to create moreinformative training instances for minority classesand how to improve the large-scale training formajority classes.Previous research has shown that words denot-ing class information in the surrounding context ofan entity can be an informative indicator for dis-ambiguation (Krallinger et al, 2008; Wang et al,2010).
Such words are refered to as ?cue words?throughout this paper.
For example, to disam-biguate the type of an entity, that is, whether itis a protein, gene, or RNA, looking at words suchas protein, gene and RNA are very helpful (Hatzi-vassiloglou et al, 2001).
Similarly, for the taskof species disambiguation (Wang et al, 2010),the occurrence of mouse p53 strongly suggeststhat p53 is a mouse protein.
In many cases, cuewords are readily available in dictionaries.
Thus,for the minority classes, instead of creating arti-ficial training instances by commonly used sam-pling methods (Haibo and Garcia, 2009), we pro-pose to create a new set of real training instancesby modelling cue words from a dictionary, calleddictionary-based prototypes.
To learn from boththe original training instances and the dictionary-based prototypes, a hierarchical classification sys-tem with a tree-like structure is designed.
Further-more, to cope with the large number of featuresrepresenting each instance, supervised orthogo-nal locality preserving projection (SOLPP) is con-ducted for dimensionality reduction, by simulta-neously preserving the intrinsic structures con-structed from both the features and labels.
A newset of lower-dimensional embeddings with betterdiscriminating power is obtained and used as in-put to the classifier.
To cope with the large num-ber of training instances in some majority classes,we propose a committee machine scheme to ac-celerate training speed without sacrificing classi-fication accuracy.
The proposed method is evalu-ated on a species disambiguation task, and the em-pirical results are encouraging, showing at least8.3% improvement over three different baselinesystems.2 Related WorkConstruction of a classification model using su-pervised learning algorithms is popular for entitydisambiguation.
A number of researchers havetackled entity disambiguation in general text us-ing wikipedia as a resource to learn classifica-tion models (Bunescu and Pas?ca, 2006).
Hatzi-vassiloglou et al (2001) studied disambiguatingproteins, genes, and RNA in text by training var-ious classifiers using entities with class informa-tion provided by adjacent cue words.
Wang etal.
(2010) proposed a ?hybird?
system for speciesdisambiguation, which heuristically combines re-sults obtained from classifying the context, andthose from modeling relations between cue wordsand entities.
Although satisfactory performancewas reported, their system incurs higher computa-tional cost due to syntactic parsing and the binaryrelation classifier.Many imbalanced learning techniques, as re-viewed by Haibo and Garcia (2009), can also beused to achieve the same purpose.
However, toour knowledge, there is little research in apply-ing these machine learning (ML) techniques to en-tity disambiguation.
It is worth mentioning thatalthough these ML techniques can improve thelearning performance to some extent, they onlyconsider the information contained in the origi-nal training instances.
The created instances donot add new information, but instead utilize theoriginal training information in a more sophisti-cated way.
This motivates us to pursue a differ-ent method of creating new training instances byusing information from a related and easily ob-tained source (e.g., a dictionary), similar to trans-fer learning (Pan and Yang, 2009).3 Task and CorpusIn this work, we develop an entity disambiguationtechnique with the use of cue words, as well as ageneral ML algorithm for imbalanced classifica-tion using a set of newly created dictionary-basedprototypes.
These prototypes are represented withdifferent features from those used by the originaltraining instances.
The proposed method is eval-uated on a species disambiguation task: given atext, in which mentions of biomedical named en-852tities are annotated, we assign a species identi-fier to every entity mention.
The types of entitiesstudied in this work are genes and gene products(e.g., proteins), and we use the NCBI Taxonomy1(taxon) IDs as species tags and to build the proto-types.
Note that this paper focuses on the task ofspecies disambiguation and makes the assumptionthat the named entities are already recognised.Consider the following sentence as an exam-ple: if one searches the proteins (i.e., the under-lined term) in a protein database, he/she will findthey belong to many model organisms.
However,in this particular context, CD200R-CD4d3+4 ishuman and mouse protein, while rCD4d3+4 isa rat one.2 We call such a task of assigningspecies identifiers to entities, according to context,as species disambiguation.The amounts of human and mouseCD200R-CD4d3+4 and rCD4d3+4protein on the microarray spots weresimilar as visualized by the red fluo-rescence of OX68 mAb recognisingthe CD4 tag present in each of therecombinant proteins.The informative cue words (e.g., mouse) usedto help species disambiguation are called specieswords.
In this work, species words are defined asany word that indicates a model organism and alsoappears in the organism dictionaries we use.
Theymay have various parts-of-speech, and may alsocontain multiple tokens (despite the name speciesword).
For example, ?human?, ?mice?, ?bovine?and ?E.
Coli?
are all species words.
We detectthese words by automatic dictionary lookup: aword is annotated as a species word if it matchesan entry in a list of organism names.
Each entry inthe list contains a species word and its correspond-ing taxon ID, and the list is merged from two dic-tionaries: the NCBI Taxonomy and the UniProtcontrolled vocabulary of species.3 The NCBI por-tion is a flattened NCBI Taxonomy (i.e., withouthierarchy) including only the identifiers of genusand species ranks.
In total, the merged list con-1http://www.ncbi.nlm.nih.gov/sites/entrez?db= taxon-omy2Prefix ?r?
in ?rCD4d3+4?
indicates that it is a rat protein.3http://www.expasy.ch/cgi-bin/speclisttains 356,387 unique species words and 272,991unique species IDs.
The ambiguity in specieswords is low: 3.86% of species words map to mul-tiple IDs, and on average each word maps to 1.043IDs.The proposed method was evaluated on thecorpus developed in (Wang et al, 2010), con-taining 6, 223 genes and gene products, each ofwhich was manually assigned with either a taxonID or an ?Other?
tag, with human being themost frequent at 50.30%.
With the extractedfeatures and the species ID tagged by domainexperts, each occurrence of named entities canbe represented as a d-dimensional vector witha label.
Species disambiguation can be mod-elled as a multi-classification task: Given n train-ing instances {xi}ni=1, their n ?
d feature ma-trix X = [xij ] and n-dimensional label vectory = [y1, y2, .
.
.
, yn]T are used to train a clas-sifier C(?
), where xi = [xi1, xi2, .
.
.
, xid]T , yi ?
{1, 2, .
.
.
, c}, and c denotes the number of ex-isting species in total.
Given m different queryinstances {x?i}mi=1, their m ?
d feature matrixX?
= [x?ij ] are used as the input to the trainedclassifier, so that their labels can be predicted by{C(x?i)}mi=1.We used relatively simple contextual featuresbecause this work was focused on developing aML framework.
In more detail, we used the fol-lowing features: 1) 200 words surrounding the en-tity in question; 2) two nouns and two adjectivesat the entity?s left and right; 3) 5 species wordsat the entity?s left and right.
In addition, functionwords and words that consist of only digits andpunctuations are filtered out.
The final numeri-cal dataset consists of 6,227 instances, each rep-resented by 16,851 binary features and belongingto one of the 13 classes.
The dataset is highly im-balanced: among the 13 classes, the numbers ofinstances in the four majority classes vary from449 to 3,220, while no more than 20 instances arecontained in the eight minority classes (see Table1).8534 Proposed Method4.1 Dictionary-based PrototypesFor each existing species, we create a b-dimensional binary vector, given as pi =[pi1, pi2, .
.
.
, pib]T , using b different specieswords listed in the dictionary as features, whichis called dictionary-based prototype.
The binaryvalue pij denotes whether the jth species wordbelongs to the ith species in the dictionary.
Thisleads to a c ?
b feature matrix P = [pij ] for cspecies.Considering that the species words precedingand appearing in the same sentence as an en-tity can be informative indicators for the possiblespecies of this entity, we create two morem?b bi-nary feature matrices for the query instances withthe same b species words as features: X?1 = [x?
(1)ij ]and X?2 = [x?
(2)ij ], where x?
(1)ij denotes whether thejth species word is the preceding word of the ithentity, and x?
(2)ij denotes whether the jth speciesword appears in the same sentence as the ith en-tity but is not preceding word.
Thus, the similar-ity between each query entity and existing speciescan be simply evaluated by calculating the inner-product between the entity instance and the cor-responding prototype.
This leads to the followingm?
c similarity matrix S?
= [s?ij ]:S?
= ?X?1PT + (1?
?
)X?2PT , (1)where 0 ?
?
?
1 is a user-defined parameter con-trolling the degree of indicating reliability of thepreceding word and the same-sentence word.
Then?c similarity matrix S = [sij ] between the train-ing instances and the species can be constructed inexactly the same way.
Based on empirical expe-rience, the preceding word indicates the entity?sspecies more accurately than the same-sentenceword.
Thus, ?
is preferred to be set as greaterthan 0.5.
The obtained similarity matrix will beused in the nearest neighbour classifier (see Sec-tion 4.2.1).Both the original training instances X and thenewly created prototypes P are used to train theproposed hierarchical classification system.
Sub-ject to the nature of the classifier employed, it isconvenient to construct one single feature matrixNearest Neighbor Classifier(IT1)Minority ClassesMajority ClassesSOLPP-FLDA Classifier(IT2)Small-scale Majority ClassesLarge-scale Majority ClassesCommittee Classifier(END)YesNo YesNoOutput: Instances with predicted labels belonging to MI Output: Instances with predicted labels belonging to SMAOutput: Instances with predicted labels belonging to LMANote: Definition of the minority,majority, small-scale majority, large-scale majority classes, as well as theIF-THEN rule 1 (IT1) and IF-THEN rule2 (IT2) are provided in the paper.Figure 1: Structure of the proposed hierarchicalclassification systeminstead of using X and P individually.
Aiming atkeeping the same similarity values between eachentity instance and the species prototype, we con-struct the following (n+c)?
(d+b) feature matrixfor both the training instances and prototypes:F =[ X ?X1 + (1?
?
)X20 P], (2)where X1 and X2 are constructed in the same wayas X?1 and X?2 but for training instances.
Their cor-responding label vector is l = [yT , 1, 2, .
.
.
, c]T .4.2 Hierarchical ClassificationMulti-stage or hierarchical classification (Giustiet al, 2002; Podolak, 2007; Kurzyn?ski, 1988)is widely used in many complex multi-categoryclassification tasks.
Existing research shows suchtechniques can potentially achieve right trade-offbetween accuracy and resource allocation (Giustiet al, 2002; Podolak, 2007).
Our proposed hier-archical system has a tree-like structure with threedifferent types of classifier at nodes (see Figure 1).Different classes are organized in a hierarchicalorder to be classified based on the correspondingnumbers of available training instances.
Lettingni denote the number of training instances avail-able in the ithe class excluding the created proto-types, we categorize the classes as follows:?
Minority Classes (MI): Classes with lesstraining instances than the threshold: MI ={i : nin < ?1, i ?
{1, 2 .
.
.
, c}}.854?
Majority Classes (MA): Classes with moretraining instances than the threshold: MA ={i : nin ?
?1, i ?
{1, 2 .
.
.
, c}}.?
Small-scale Majority Classes (SMA): Ma-jority Classes with less training instancesthan the threshold: SMA = {i : nin <?2, i ?
MA}.?
Large-scale Majority Classes (LMA): Ma-jority Classes with more training instancesthan the threshold: LMA = {i : nin ?
?2, i ?
MA}.Here, 0 < ?1 < 1 and 0 < ?2 < 1 are sizethresholds set by users.
We have MI ?MA = ?,SMA ?
LMA = ?, and SMA ?
LMA = MA.The tree-like hierarchical structure of our sys-tem is determined by MI, MA, SMA, and LMA.We propose two IF-THEN rules to control the sys-tem: Given a query instance x?i, the level 1 clas-sifier C1 is used to predict whether x?i belongs toMA or a specific class in MI, which wer call IF-THEN rule 1 (IT1).
If x?i belongs to MA, the level2 classifier C2 is used to predict whether x?i be-longs to LMA or a specific class in SMA, calledIF-THEN rule 2 (IT2).
If x?i belongs to LMA, thelevel 3 classifier C3 finally predicts the specificclass in LMA x?i belongs to.
We explain in thefollowing sections how the classifiers C1, C2, andC3 work in detail.4.2.1 Nearest Neighbour ClassifierThe goal of the nearest neighbour classifier, de-noted by C1, is to decide whether the nearest-neighbour prototype of the query instance be-longs to MI.
The only used training instances areour created dictionary-based prototypes {pi}ci=1with the label vector [1, 2, .
.
.
, c]T .
The nearest-neighbour prototype of the query instance x?i pos-sesses the maximum similarity to x?i:NN(x?i) = arg maxj=1, 2, ..., c s?ij , (3)where s?ij is obtained by Eq.
(1).
Consequently,the output of the classifier C1 is given asC1(x?i) ={NN(x?i), If NN(x?i) ?
MI,0, Otherwise.
(4)The IF-THEN rule 1 can then be expressed asAction(IT1) ={ Go to C2, If C1(x?i) = 0,Stop, Otherwise.4.2.2 SOLPP-FLDA ClassifierThe goal of the SOLPP-FLDA classifier, de-noted by C2, is to predict whether the query in-stance belongs to LMA or a specific class in SMA.In this classifier, the used training instances arethe original training entities and the dictionary-based prototypes, both belonging to MA.
The fea-ture matrix F and the label vector l defined in Sec-tion 4.1 are used, but with instances from MI re-moved (we use n?
to denote the number of remain-ing training instances, and the same symbol F forfeature matrix).
The used label vector l?
to train C2should be re-defined as l?i = li if li ?
SMA, and 0otherwise.First, we propose to implement orthog-onal locality preserving projection (OLPP)(Kokiopoulou and Saad, 2007) in a supervisedmanner, leading to SOLPP, to obtain a smaller setof more powerful features for classification.
Also,we conduct SOLPP in a similarity-based featurespace computed from (d + 2b) original featuresby employing dot-product based similarity, givenby FFT .
As explained later, to compute thenew features from FFT instead of the originalfeatures F achieves reduced computational cost.An n?
?k projection matrix V = [vij ] is optimizedin this n-dimensional similarity-based featurespace.
The optimal projections are obtained byminimizing the weighted distances between thelower-dimensional embeddings so that ?similar?instances are mapped together in the projectedfeature space.
Mathematically, this leads to thefollowing constrained optimization problem:minV?Rn?
?k,VT V=Ik?ktr[VTFTF(D?W)FFTV], (5)where W = [wij ] denotes the n ?
n weight ma-trix with wij defining the degree of ?closeness?
or?similarity?
between the ith and jth instances, Dis a diagonal matrix with {di =?n?j=1wij}n?i=1 asthe diagonal elements.Usually, the weight matrix W is defined byan adjacency graph constructed from the original855data, e.g.
for OLPP.
One common way to definethe adjacency is by including the K-nearest neigh-bors (KNN) of a given node to its adjacency list,which is also called the KNN-graph (Kokiopoulouand Saad, 2007).
There are two common ways todefine the weight matrix: constant value, wherewij = 1 if the ith and jth samples are adjacent,while wij = 0 otherwise, and Gaussian kernel.We will denote in the rest of the paper such aweight matrix computed only from the featuresas WX .
Ideally, if the features can accuratelydescribe all the discriminating characteristics, thesamples that are close or similar enough to eachother should have the same label vectors.
How-ever, when processing real dataset, what may hap-pen is that, in the d-dimensional feature space,the data points that are close to each other maybelong to different categories, while on the con-trary, the data points that are in a distant to eachother may belong to the same category.
In the k-dimensional projected feature space obtained byOLPP, one may have the same problem.
BecauseOLPP solves the constrained optimization prob-lem in Eq.
(5) using WX : if two instances areclose or similar to each other in the original fea-ture space, they will be the same close or simi-lar to each other in the projected space.
To solvethis problem, we decide to modify the ?closeness?or ?similarity?
between instances in the projectedfeature space by considering the label informa-tion.
The following computation of a supervisedweight matrix is used for our SOLPP:W = (1?
?
)WX + ?LLT , (6)where 0 ?
?
?
1 is a user-defined parametercontrolling the tradeoff between the label-basedand feature-based neighborhood structures, andL = [lij ] is an n?
?
c binary label matrix withlij = 1 if the ith instance belongs to the jth class,and lij = 0 otherwise.The optimal solution of Eq.
(5) is the top(k + 1)th eigenvectors of the n?
?
n?
symmetricmatrix FTF(D ?
W)FFT , corresponding to thek + 1 smallest eigenvalues, but with the top oneeigenvector removed, denoted by V?.
It is worthto mention that if the original feature matrix F isused as the input of SOLPP, one needs to com-pute the eigen-decomposition of the (d + b) ?
(d+ b) symmetric matrix FT (D?W)F. The cor-responding computation complexity increases inO((d + b)3), which is unacceptable in practicalwhen d + b  n?.
The projected features for thetraining instances are computed byZ = FFTV?.
(7)Given a different set of m query instances with anm?
(d+ b) feature matrix,F?
= [X?, ?X?1 + (1?
?
)X?2], (8)their embeddings can be easily obtained byZ?
= F?F?TV?.
(9)Then, the projected feature matrix Z and labelvector l?
are used to train a multi-class classifier.By employing the one-against-all scheme, differ-ent binary classifiers {C(2)i }i?SMA?
{0} with labelspace {+1, ?1} are trained.
For the ith class(i ?
SMA?
{0}), the training instances belongingto it are labeled as positive, otherwise negative.
Ineach binary classifier C(2)i , a separating functionf (2)i (x) = xTw(2)i + b(2)i (10)is constructed, of which the optimal values of theweight vector w(2)i and bias b(2)i are computed us-ing Fisher?s linear discriminant analysis (FLDA)(Fisher, 1936; Mu, 2008).
Finally, the output ofthe classifier C2 can be obtained by assigning themost confident class label to the query instance x?i,with the confidence value indicated by the value ofseparating function:C2(x?i) = arg maxj?SMA?
{0}f (2)j (x?i).
(11)The IF-THEN rule 2 can then be expressed asAction(IT2) ={ Go to C3, If C2(x?i) = 0,Stop, Otherwise.4.2.3 Committee ClassifierThe goal of the committee classifier, denotedby C3, is to predict the specific class in LMAthe query instance belongs to.
The used training856instances are entities and dictionary-based proto-types only belonging to LMA.
With the same one-against-all scheme, there are large number of pos-itive and negative training instances to train a bi-nary classifier for a class in LMA.
To acceleratethe training procedure without sacrificing the ac-curacy, the following scheme is designed.Letting ne denote the number of experts incommittee, all the training instances are averagelydivided into ne+1 groups each containing similarnumbers of training instances from the same class.The instances in the ith and the (i+1)th groups areused to train the ith expert classifier.
This achievesoverlapped training instances between expert clas-sifiers.
The output value of C(3)i is not the class in-dex as used in C2, but the value of the separatingfunction of the most confident class, denoted byf (3)i .
Different from the commonly used majorityvoting rule, we only trust the most confident ex-pert.
Thus, the output of C3 for a query instancex?i can be obtained byC3(x?i) = arg maxj=1, 2, ..., ne f(3)j (x?i).
(12)By using C3, different expert classifiers can betrained in parallel.
The total training time is equalto that of the slowest expert classifier.
The moreexpert classifiers are used, the faster the system is,however, the less accurate the system may becomedue to the decrease of used training instances foreach expert, especially the positive instances inthe case of imbalanced classification.
This is alsothe reason we do not apply the committee schemeto SMA classes.5 Experiments5.1 System Evaluation and BaselineWe evaluate the proposed method using 5-foldcross validation, with around 4,980 instances fortraining, and 1,245 instances for test in each trial.We compute the F1 score for each species, andemploy macro- and micro- average scheme tocompute performance for all species.
Three base-lines for comparison include:?
Baseline 1 (B1) : A maximum entropymodel trained with training data only.?
Baseline 1 (B2) : Combination of B1 andthe species dictionary using rules employedin Wang et al (2010).?
Baseline 2 (B3): The ?hybrid?
system com-bining B1, the dictionary and a relationmodel 4 using rules (Wang et al, 2010).Our hierarchical classification system were imple-mented in two ways:?
HC: Only the training data on its own is usedto train the system.?
HC/D: Both the training data and thedictionary-based prototypes are used to trainthe system.5.2 Results and AnalysisThe proposed system was implemented with ?
=0.8, ?
= 0.8, ne = 4, and k = 1000.
The species9606, 10090, 7227, and 4932 were categorized asLMA, the species 10116 as SMA, and the rest sep-cies as MI.
To compute the supervised weight ma-trix, the percentage of the used KNN in the KNN-graph was 0.6.
Parameters were not fine tuned, butset based on our empirical experience on previousclassification research.
As shown in Table 1: HCand B1 were trained with the same instances andfeatures, and HC outperformed B1 in both macroand micro F1.
Both HC and B1 obtained zero F1scores for most minority species, showing that it isnearly impossible to correctly label the query in-stances of minority classes, due to lack of trainingdata.
By learning from a related resource, HC/D,B2, and B3 yielded better macro performance.
Inparticular, while HC/D and B2 learned from thesame dictionary and training data, HC/D outper-formed B2 by 19.1% in macro and 2.5% in mi-cro F1.
B3 aimed at improving the macro perfor-mance by employing computationally expensivesyntactic parsers and also by training an extra re-lation classifier.
With the same goal, HC/D inte-grated the cue word information into the ML clas-sifier in a more general way, and yielded an 8.3%improvement over B3, as measured by macro-F1.4This is an SVM model predicting relations between en-tities and nearby species words with positive output indicatesspecies words bear the semantic label of entities.857Species Name Cat.
No.
HC HC/D B1 B2 B3Homo sapiens (9606) LMA 3220 87.39 87.48 86.06 85.43 86.48Mus musculus (10090) LMA 1709 79.99 79.98 79.59 80.00 80.41Drosophila melanogaster (7227) LMA 641 86.62 86.35 87.96 87.02 87.37Saccharomyces cerevisiae (4932) LMA 499 90.24 90.24 83.35 81.64 84.64Rattus norvegicus (10116) SMA 50 55.07 69.23 48.42 64.41 59.41Escherichia coli K-12 (83333) MI 18 0.00 0.00 0.00 0.00 0.00Xenopus tropicalis (8364) MI 8 0.00 40.00 0.00 41.67 36.36Caenorhabditis elegans (6239) MI 7 0.00 22.22 0.00 28.57 22.22Oryctolagus cuniculus (9986) MI 3 0.00 0.00 0.00 20.00 0.00Bos taurus (9913) MI 3 0.00 50.00 0.00 0.00 100.00Arabidopsis thaliana (3702) MI 2 0.00 0.00 0.00 0.00 66.67Arthropoda (6656) MI 1 0.00 100.00 0.00 50.00 0.00Martes zibellina (36722) MI 1 0.00 50.00 0.00 28.57 0.00Micro-average N/A N/A 85.03 85.13 83.59 83.04 83.80Macro-average N/A N/A 30.72 51.96 29.42 43.64 47.97Table 1: Performance is compared in F1 (%), where ?No.?
denotes the number of training instancesand ?Cat.?
denotes the category of species class as defined in Section 4.2.6 Conclusions and Future WorkDisambiguating bio-entities presents a challengefor traditional supervised learning methods, dueto the high number of semantic classes and lack oftraining instances for some classes.
We have pro-posed a hierarchical framework for imbalancedlearning, and evaluated it on the species disam-biguation task.
Our method automatically buildstraining instances for the minority or missingclasses from a cue word dictionary, under the as-sumption that cue words in the surrounding con-text of an entity strongly indicate its semantic cat-egory.
Compared with previous work (Wanget al, 2010; Hatzivassiloglou et al, 2001), ourmethod provides a more general way to integratethe cue word information into a ML frameworkwithout using deep linguistic information.Although the species disambiguation task isspecific to bio-text, the difficulties caused by im-balanced frequency of different senses are com-mon in real application of sense disambiguation.The proposed technique can also be applied toother domains, providing the availability of a cueword dictionary that encodes semantic informa-tion regarding the target semantic classes.
Build-ing such a dictionary from scratch can be chal-lenging, but may be easier compared to manualannotation.
In addition, such dictionaries may al-ready exist in specialised domains.AcknowledgmentThe authors would like to thank the biologists whoannotated the species corpus, and National Cen-tre for Text Mining.
Funding: Pfizer Ltd.; JointInformation Systems Committee (to UK NationalCentre for Text Mining)ReferencesAgirre, E. and D. Martinez.
2004.
Unsupervised WSDbased on automatically retrieved examples: The im-portance of bias.
In Proceedings of EMNLP.Bunescu, R. and M. Pas?ca.
2006.
Using encyclope-dic knowledge for named entity disambiguation.
InProceedings of EACL.Fisher, R. A.
1936.
The use of multiple measure-ments in taxonomic problems.
Annals of Eugenics,7(2):179?188.Giusti, N., F. Masulli, and A. Sperduti.
2002.
Theoret-ical and experimental analysis of a two-stage systemfor classification.
IEEE Trans.
on Pattern Analysisand Machine Intelligence, 24(7):893?904.Haibo, H. and E. A. Garcia.
2009.
Learning fromimbalanced data.
IEEE Trans.
on Knowledge andData Engineering, 21(9):1263?1284.858Hakenberg, J., C. Plake, R. Leaman, M. Schroeder, andG.
Gonzalez.
2008.
Inter-species normalization ofgene mentions with GNAT.
Bioinformatics, 24(16).Hatzivassiloglou, V., PA Duboue?, and A. Rzhetsky.2001.
Disambiguating proteins, genes, and RNA intext: a machine learning approach.
Bioinformatics,17(Suppl 1).Kokiopoulou, E. and Y. Saad.
2007.
Orthogonalneighborhood preserving projections: A projection-based dimensionality reduction technique.
IEEETrans.
on Pattern Analysis and Machine Intelli-gence, 29(12):2143?2156.Krallinger, M., A. Morgan, L. Smith, F. Leitner,L.
Tanabe, J. Wilbur, L. Hirschman, and A. Valen-cia.
2008.
Evaluation of text-mining systems forbiology: overview of the second biocreative com-munity challenge.
Genome Biology, 9(Suppl 2).Kurzyn?ski, M. W. 1988.
On the multistage bayes clas-sifier.
Pattern Recognition, 21(4):355?365.Mu, T. 2008.
Design of machine learning algorithmswith applications to breast cancer detection.
Ph.D.thesis, University of Liverpool.Okazaki, N., S. Ananiadou, and J. Tsujii.
2010.Building a high quality sense inventory for im-proved abbreviation disambiguation.
Bioinformat-ics, doi:10.1093/bioinformatics/btq129.Pan, S. J. and Q. Yang.
2009.
A survey on transferlearning.
IEEE Trans.
on Knowledge and Data En-gineering.Podolak, I. T. 2007.
Hierarchical rules for a hierarchi-cal classifier.
Lecture Notes in Computer Science,4431:749?757.Provost, F. 2000.
Machine learning from imbalanceddata sets 101.
In Proc.
of Learning from ImbalancedData Sets: Papers from the Am.
Assoc.
for ArtificialIntelligence Workshop.
(Technical Report WS-00-05).Wain, H., E. Bruford, R. Lovering, M. Lush,M.
Wright, and S. Povey.
2002.
Guidelines forhuman gene nomenclature.
Genomics, 79(4):464?470.Wang, X., J. Tsujii, and S. Ananiadou.
2010.
Dis-ambiguating the species of biomedical named enti-ties using natural language parsers.
Bioinformatics,26(5):661667.859
