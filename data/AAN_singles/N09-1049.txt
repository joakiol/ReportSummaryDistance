Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433?441,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHierarchical Phrase-Based Translation withWeighted Finite State TransducersGonzalo Iglesias?
Adria` de Gispert??
University of Vigo.
Dept.
of Signal Processing and Communications.
Vigo, Spain{giglesia,erbanga}@gts.tsc.uvigo.es?
University of Cambridge.
Dept.
of Engineering.
CB2 1PZ Cambridge, U.K.{ad465,wjb31}@eng.cam.ac.ukEduardo R. Banga?
William Byrne?AbstractThis paper describes a lattice-based decoderfor hierarchical phrase-based translation.
Thedecoder is implemented with standard WFSToperations as an alternative to the well-knowncube pruning procedure.
We find that theuse of WFSTs rather than k-best lists requiresless pruning in translation search, resultingin fewer search errors, direct generation oftranslation lattices in the target language,better parameter optimization, and improvedtranslation performance when rescoring withlong-span language models and MBR decod-ing.
We report translation experiments forthe Arabic-to-English and Chinese-to-EnglishNIST translation tasks and contrast the WFST-based hierarchical decoder with hierarchicaltranslation under cube pruning.1 IntroductionHierarchical phrase-based translation generatestranslation hypotheses via the application of hierar-chical rules in CYK parsing (Chiang, 2005).
Cubepruning is used to apply language models at eachcell of the CYK grid as part of the search for ak-best list of translation candidates (Chiang, 2005;Chiang, 2007).
While this approach is very effectiveand has been shown to produce very good qualitytranslation, the reliance on k-best lists is a limita-tion.
We take an alternative approach and describe alattice-based hierarchical decoder implemented withWeighted Finite State Transducers (WFSTs).
In ev-ery CYK cell we build a single, minimal word latticecontaining all possible translations of the source sen-tence span covered by that cell.
When derivationscontain non-terminals, we use pointers to lower-level lattices for memory efficiency.
The pointersare only expanded to the actual translations if prun-ing is required during search; expansion is otherwiseonly carried out at the upper-most cell, after the fullCYK grid has been traversed.We describe how this decoder can be easily im-plemented with WFSTs.
For this we employ theOpenFST libraries (Allauzen et al, 2007).
Usingstandard FST operations such as composition, ep-silon removal, determinization, minimization andshortest-path, we find this search procedure to besimpler to implement than cube pruning.
The mainmodeling advantages are a significant reduction insearch errors, a simpler implementation, direct gen-eration of target language word lattices, and betterintegration with other statistical MT procedures.
Wereport translation results in Arabic-to-English andChinese-to-English translation and contrast the per-formance of lattice-based and cube pruning hierar-chical decoding.1.1 Related WorkHierarchical phrase-based translation has emergedas one of the dominant current approaches to statis-tical machine translation.
Hiero translation systemsincorporate many of the strengths of phrase-basedtranslation systems, such as feature-based transla-tion and strong target language models, while alsoallowing flexible translation and movement basedon hierarchical rules extracted from aligned paral-lel text.
We summarize some extensions to the basicapproach to put our work in context.433Hiero Search Refinements Huang and Chiang(2007) offer several refinements to cube pruning toimprove translation speed.
Venugopal et al (2007)introduce a Hiero variant with relaxed constraintsfor hypothesis recombination during parsing; speedand results are comparable to those of cube prun-ing, as described by Chiang (2007).
Li and Khudan-pur (2008) report significant improvements in trans-lation speed by taking unseen n-grams into accountwithin cube pruning to minimize language model re-quests.
Dyer et al (2008) extend the translation ofsource sentences to translation of input lattices fol-lowing Chappelier et al (1999).Extensions to Hiero Several authors describe ex-tensions to Hiero, to incorporate additional syntacticinformation (Zollmann and Venugopal, 2006; Zhangand Gildea, 2006; Shen et al, 2008; Marton andResnik, 2008), or to combine it with discriminativelatent models (Blunsom et al, 2008).Analysis and Contrastive Experiments Zollman etal.
(2008) compare phrase-based, hierarchical andsyntax-augmented decoders for translation of Ara-bic, Chinese, and Urdu into English.
Lopez (2008)explores whether lexical reordering or the phrasediscontiguity inherent in hierarchical rules explainsimprovements over phrase-based systems.
Hierar-chical translation has also been used to great effectin combination with other translation architectures,e.g.
(Sim et al, 2007; Rosti et al, 2007).WFSTs for Translation There is extensive work inusing Weighted Finite State Transducer for machinetranslation (Bangalore and Riccardi, 2001; Casacu-berta, 2001; Kumar and Byrne, 2005; Mathias andByrne, 2006; Graehl et al, 2008).To our knowledge, this paper presents the first de-scription of hierarchical phrase-based translation interms of lattices rather than k-best lists.
The nextsection describes hierarchical phrase-based transla-tion with WFSTs, including the lattice constructionover the CYK grid and pruning strategies.
Sec-tion 3 reports translation experiments for Arabic-to-English and Chinese-to-English, and Section 4 con-cludes.2 Hierarchical Translation with WFSTsThe translation system is based on a variant of theCYK algorithm closely related to CYK+ (Chappe-lier and Rajman, 1998).
Parsing follows the de-scription of Chiang (2005; 2007), maintaining back-pointers and employing hypothesis recombinationwithout pruning.
The underlying model is a syn-chronous context-free grammar consisting of a setR = {Rr} of rules Rr : N ?
??r,?r?
/ pr, with?glue?
rules, S ?
?X,X?
and S ?
?S X,S X?.
If arule has probability pr, it is transformed to a cost cr;here we use the tropical semiring, so cr = ?
log pr.N denotes a non-terminal; in this paper, N can beeither S, X, or V (see section 3.2).
T denotes theterminals (words), and the grammar builds parsesbased on strings ?, ?
?
{{S,X, V } ?
T}+.
Eachcell in the CYK grid is specified by a non-terminalsymbol and position in the CYK grid: (N,x, y),which spans sx+y?1x on the source sentence.In effect, the source language sentence is parsedusing a context-free grammar with rules N ?
?.The generation of translations is a second step thatfollows parsing.
For this second step, we describea method to construct word lattices with all possibletranslations that can be produced by the hierarchicalrules.
Construction proceeds by traversing the CYKgrid along the backpointers established in parsing.In each cell (N,x, y) in the CYK grid, we build atarget language word lattice L(N,x, y).
This lat-tice contains every translation of sx+y?1x from everyderivation headed by N .
These lattices also containthe translation scores on their arc weights.The ultimate objective is the word latticeL(S, 1, J) which corresponds to all the analyses thatcover the source sentence sJ1 .
Once this is built,we can apply a target language model to L(S, 1, J)to obtain the final target language translation lattice(Allauzen et al, 2003).We use the approach of Mohri (2002) in applyingWFSTs to statistical NLP.
This fits well with the useof the OpenFST toolkit (Allauzen et al, 2007) toimplement our decoder.2.1 Lattice Construction Over the CYK GridIn each cell (N,x, y), the set of rule indices usedby the parser is denoted R(N,x, y), i.e.
for r ?R(N,x, y), N ?
??r,?r?
was used in at least onederivation involving that cell.For each rule Rr, r ?
R(N,x, y), we build a lat-tice L(N,x, y, r).
This lattice is derived from thetarget side of the rule ?r by concatenating lattices434R1: X ?
?s1 s2 s3,t1 t2?R2: X ?
?s1 s2,t7 t8?R3: X ?
?s3,t9?R4: S ?
?X,X?R5: S ?
?S X,S X?L(S, 1, 3) = L(S, 1, 3, 4) ?
L(S, 1, 3, 5)L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) == A(t1)?A(t2)L(S, 1, 3, 5) = L(S, 1, 2)?
L(X, 3, 1)L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) == L(X, 1, 2, 2) = A(t7)?A(t8)L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)L(S, 1, 3, 5) = A(t7)?A(t8)?A(t9)L(S, 1, 3) = (A(t1)?A(t2))?
(A(t7)?A(t8)?A(t9))Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.
The gridis represented here in two dimensions (x, y).
In practice only the first column accepts both non-terminals (S,X).
Forthis reason it is divided in two subcolumns.corresponding to the elements of ?r = ?r1...?r|?r |.If an ?ri is a terminal, creating its lattice is straight-forward.
If ?ri is a non-terminal, it refers to a cell(N ?, x?, y?)
lower in the grid identified by the back-pointer BP (N,x, y, r, i); in this case, the latticeused is L(N ?, x?, y?).
Taken together,L(N,x, y, r) = ?i=1..|?r|L(N,x, y, r, i) (1)L(N,x, y, r, i) ={A(?i) if ?i ?
TL(N ?, x?, y?)
else(2)where A(t), t ?
T returns a single-arc accep-tor which accepts only the symbol t. The latticeL(N,x, y) is then built as the union of lattices cor-responding to the rules in R(N,x, y):L(N,x, y) = ?r?R(N,x,y)L(N,x, y, r) (3)Lattice union and concatenation are performedusing the ?
and ?
WFST operations respectively, asdescribed by Allauzen et al(2007).
If a rule Rr hasa cost cr, it is applied to the exit state of the latticeL(N,x, y, r) prior to the operation of Equation 3.2.1.1 An Example of Phrase-based TranslationFigure 1 illustrates this process for a three wordsource sentence s1s2s3 under monotone phrase-based translation.
The left-hand side shows the stateof the CYK grid after parsing using the rules R1 toR5.
These include 3 rules with only terminals (R1,R2, R3) and the glue rules (R4, R5).
Arrows repre-sent backpointers to lower-level cells.
We are inter-ested in the upper-most S cell (S, 1, 3), as it repre-sents the search space of translation hypotheses cov-ering the whole source sentence.
Two rules (R4, R5)are in this cell, so the lattice L(S, 1, 3) will be ob-tained by the union of the two lattices found by thebackpointers of these two rules.
This process is ex-plicitly derived in the right-hand side of Figure 1.2.1.2 An Example of Hierarchical TranslationFigure 2 shows a hierarchical scenario for thesame sentence.
Three rules, R6, R7, R8, are addedto the example of Figure 1, thus providing two ad-ditional derivations.
This makes use of sublatticesalready produced in the creation of L(S, 1, 3, 5) andL(X, 1, 3, 1) in Figure 1; these are within {}.2.2 A Procedure for Lattice ConstructionFigure 3 presents an algorithm to build the latticefor every cell.
The algorithm uses memoization: ifa lattice for a requested cell already exists, it is re-turned (line 2); otherwise it is constructed via equa-tions 1,2,3.
For every rule, each element of the tar-get side (lines 3,4) is checked as terminal or non-terminal (equation 2).
If it is a terminal element(line 5), a simple acceptor is built.
If it is a non-terminal (line 6), the lattice associated to its back-pointer is returned (lines 7 and 8).
The completelattice L(N,x, y, r) for each rule is built by equa-tion 1 (line 9).
The lattice L(N,x, y) for this cellis then found by union of all the component rules(line 10, equation 3); this lattice is then reduced by435R6: X ?
?s1,t20?R7: X ?
?X1 s2 X2,X1 t10 X2?R8: X ?
?X1 s2 X2,X2 t10 X1?L(S, 1, 3) = L(S, 1, 3, 4) ?
{L(S, 1, 3, 5)}L(S, 1, 3, 4) = L(X, 1, 3) =={L(X, 1, 3, 1)} ?L(X, 1, 3, 7)?
L(X, 1, 3, 8)L(X, 1, 3, 7) = L(X, 1, 1, 6)?A(t10)?L(X, 3, 1, 3) == A(t20)?A(t10)?A(t9)L(X, 1, 3, 8) = A(t9)?A(t10)?A(t20)L(S, 1, 3) = {(A(t1)?A(t2))} ??(A(t20)?A(t10)?A(t9))?
(A(t9)?A(t10)?A(t20))??
{(A(t7)?A(t8)?A(t9))}Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8.
Lattices previously derived appear within {}.standard WFST operations (lines 11,12,13).
It isimportant at this point to remove any epsilon arcswhich may have been introduced by the variousWFST union, concatenation, and replacement oper-ations (Allauzen et al, 2007).1 function buildFst(N,x,y)2 if ?
L(N,x, y) return L(N,x, y)3 for r ?
R(N,x, y), Rr : N ?
??,?
?4 for i = 1...|?|5 if ?i ?
T, L(N,x, y, r, i) = A(?i)6 else7 (N ?, x?, y?)
= BP (?i)8 L(N,x, y, r, i) = buildFst(N ?, x?, y?
)9 L(N,x, y, r)=?i=1..|?| L(N,x, y, r, i)10 L(N,x, y) =?r?R(N,x,y) L(N,x, y, r)11 fstRmEpsilon L(N,x, y)12 fstDeterminize L(N,x, y)13 fstMinimize L(N,x, y)14 return L(N,x, y)Figure 3: Recursive Lattice Construction.2.3 Delayed TranslationEquation 2 leads to the recursive construction of lat-tices in upper-levels of the grid through the unionand concatenation of lattices from lower levels.
Ifequations 1 and 3 are actually carried out over fullyexpanded word lattices, the memory required by theupper lattices will increase exponentially.To avoid this, we use special arcs that serve aspointers to the low-level lattices.
This effectivelybuilds a skeleton of the desired lattice and delaysthe creation of the final word lattice until a singlereplacement operation is carried out in the top cell(S, 1, J).
To make this exact, we define a functiong(N,x, y) which returns a unique tag for each latticein each cell, and use it to redefine equation 2.
Withthe backpointer (N ?, x?, y?)
= BP (N,x, y, r, i),these special arcs are introduced as:L(N,x, y, r, i) ={A(?i) if ?i ?
TA(g(N ?, x?, y?))
else(4)The resulting lattices L(N,x, y) are a mix of tar-get language words and lattice pointers (Figure 4,top).
However each still represents the entire searchspace of all translation hypotheses covering thespan.
Importantly, operations on these lattices ?such as lossless size reduction via determinizationand minimization ?
can still be performed.
Owingto the existence of multiple hierarchical rules whichshare the same low-level dependencies, these opera-tions can greatly reduce the size of the skeleton lat-tice; Figure 4 shows the effect on the translation ex-ample.
This process is carried out for the lattice atevery cell, even at the lowest level where there areonly sequences of word terminals.
As stated, sizereductions can be significant.
However not all redu-dancy is removed, since duplicate paths may arisethrough the concatenation and union of sublatticeswith different spans.At the upper-most cell, the lattice L(S, 1, J) con-tains pointers to lower-level lattices.
A single FSTreplace operation (Allauzen et al, 2007) recursivelysubstitutes all pointers by their lower-level latticesuntil no pointers are left, thus producing the com-plete target word lattice for the whole source sen-tence.
The use of the lattice pointer arc was in-spired by the ?lazy evaluation?
techniques developedby Mohri et al(2000).
Its implementation uses theinfrastructure provided by the OpenFST libraries for43601t12g(X,1,2)3g(X,1,1)5g(X,3,1)7t2g(X,3,1)4t106t10g(X,3,1)g(X,1,1)03g(X,1,1)2g(X,1,2)1t14g(X,3,1)t106g(X,3,1)t25t10g(X,1,1)Figure 4: Delayed translation WFST with derivationsfrom Figure 1 and Figure 2 before [t] and after minimiza-tion [b].delayed composition, etc.2.4 Pruning in Lattice ConstructionThe final translation lattice L(S, 1, J) can grow verylarge after the pointer arcs are expanded.
We there-fore apply a word-based language model, via WFSTcomposition, and perform likelihood-based prun-ing (Allauzen et al, 2007) based on the combinedtranslation and language model scores.Pruning can also be performed on sublatticesduring search.
One simple strategy is to monitorthe number of states in the determinized latticesL(N,x, y).
If this number is above a threshold, weexpand any pointer arcs and apply a word-based lan-guage model via composition.
The resulting latticeis then reduced by likelihood-based pruning, afterwhich the LM scores are removed.
This search prun-ing can be very selective.
For example, the pruningthreshold can depend on the height of the cell in thegrid.
In this way the risk of search errors can becontrolled.3 Translation ExperimentsWe report experiments on the NIST MT08 Arabic-to-English and Chinese-to-English translation tasks.We contrast two hierarchical phrase-based decoders.The first decoder, Hiero Cube Pruning (HCP), is a k-best decoder using cube pruning implemented as de-scribed by Chiang (2007).
In our implementation, k-best lists contain unique hypotheses.
The second de-coder, Hiero FST (HiFST), is a lattice-based decoderimplemented with Weighted Finite State Transduc-ers as described in the previous section.
Hypothesesare generated after determinization under the trop-ical semiring so that scores assigned to hypothesesarise from single minimum cost / maximum likeli-hood derivations.
We also use a variant of the k-bestdecoder which works in alignment mode: given aninput k-best list, it outputs the feature scores of eachhypothesis in the list without applying any pruning.This is used for Minimum Error Training (MET)with the HiFST system.These two language pairs pose very differenttranslation challenges.
For example, Chinese-to-English translation requires much greater wordmovement than Arabic-to-English.
In the frame-work of hierarchical translation systems, we havefound that shallow decoding (see section 3.2) isas good as full hierarchical decoding in Arabic-to-English (Iglesias et al, 2009).
In Chinese-to-English, we have not found this to be the case.Therefore, we contrast the performance of HiFSTand HCP under shallow hierarchical decoding forArabic-to-English, while for Chinese-to-English weperform full hierarchical decoding.Both hierarchical translation systems share acommon architecture.
For both language pairs,alignments are generated over the parallel data.
Thefollowing features are extracted and used in trans-lation: target language model, source-to-target andtarget-to-source phrase translation models, word andrule penalties, number of usages of the glue rule,source-to-target and target-to-source lexical models,and three rule count features inspired by Bender etal.
(2007).
The initial English language model isa 4-gram estimated over the parallel text and a 965million word subset of monolingual data from theEnglish Gigaword Third Edition.
Details of the par-allel corpus and development sets used for each lan-guage pair are given in their respective section.Standard MET (Och, 2003) iterative parameterestimation under IBM BLEU (Papineni et al, 2001)is performed on the corresponding development set.For the HCP system, MET is done following Chi-ang (2007).
For the HiFST system, we obtain a k-437best list from the translation lattice and extract eachfeature score with the aligner variant of the k-bestdecoder.
After translation with optimized featureweights, we carry out the two following rescoringsteps.?
Large-LM rescoring.
We build sentence-specific zero-cutoff stupid-backoff (Brants etal., 2007) 5-gram language models, estimatedusing ?4.7B words of English newswire text,and apply them to rescore either 10000-bestlists generated by HCP or word lattices gener-ated by HiFST.
Lattices provide a vast searchspace relative to k-best lists, with translationlattice sizes of 1081 hypotheses reported in theliterature (Tromble et al, 2008).?
Minimum Bayes Risk (MBR).
We rescore thefirst 1000-best hypotheses with MBR, takingthe negative sentence level BLEU score as theloss function (Kumar and Byrne, 2004).3.1 Building the Rule SetsWe extract hierarchical phrases from word align-ments, applying the same restrictions as introducedby Chiang (2005).
Additionally, following Iglesiaset al (2009) we carry out two rule filtering strate-gies:?
we exclude rules with two non-terminals withthe same order on the source and target side?
we consider only the 20 most frequent transla-tions for each ruleFor each development set, this produces approx-imately 4.3M rules in Arabic-to-English and 2.0Mrules in Chinese-to-English.3.2 Arabic-to-English TranslationWe translate Arabic-to-English with shallow hierar-chical decoding, i.e.
only phrases are allowed to besubstituted into non-terminals.
The rules used in thiscase are, in addition to the glue rules:X ?
?
?s,?s?X ?
?V ,V ?V ?
?s,t?s, t ?
T+; ?s, ?s ?
({V } ?T)+For translation model training, we use all allowedparallel corpora in the NIST MT08 Arabic track(?150M words per language).
In addition to theMT08 set itself, we use a development set mt02-05-tune formed from the odd numbered sentences of theNIST MT02 through MT05 evaluation sets; the evennumbered sentences form the validation set mt02-05-test.
The mt02-05-tune set has 2,075 sentences.The cube pruning decoder, HCP, employs k-bestlists of depth k=10000 (unique).
Using deeper listsresults in excessive memory and time requirements.In contrast, the WFST-based decoder, HiFST, re-quires no local pruning during lattice constructionfor this task and the language model is not applieduntil the lattice is fully built at the upper-most cell ofthe CYK grid.Table 1 shows results for mt02-05-tune, mt02-05-test and mt08, as measured by lowercased IBMBLEU and TER (Snover et al, 2006).
MET param-eters are optimized for the HCP decoder.
As shownin rows ?a?
and ?b?, results after MET are compara-ble.Search Errors Since both decoders use exactly thesame features, we can measure their search errors ona sentence-by-sentence basis.
A search error is as-signed to one of the decoders if the other has founda hypothesis with lower cost.
For mt02-05-tune, wefind that in 18.5% of the sentences HiFST finds a hy-pothesis with lower cost than HCP.
In contrast, HCPnever finds any hypothesis with lower cost for anysentence.
This is as expected: the HiFST decoderrequires no pruning prior to applying the languagemodel, so search is exact.Lattice/k-best Quality Rescoring results are dif-ferent for cube pruning and WFST-based decoders.Whereas HCP improves by 0.9 BLEU, HiFST im-proves over 1.5 BLEU.
Clearly, search errors in HCPnot only affect the 1-best output but also the qualityof the resulting k-best lists.
For HCP, this limits thepossible gain from subsequent rescoring steps suchas large LMs and MBR.Translation Speed HCP requires an average of 1.1seconds per input word.
HiFST cuts this time byhalf, producing output at a rate of 0.5 seconds perword.
It proves much more efficient to process com-pact lattices contaning many hypotheses rather thanto independently processing each one of them in k-best form.438decoder mt02-05-tune mt02-05-test mt08BLEU TER BLEU TER BLEU TERa HCP 52.2 41.6 51.5 42.2 42.5 48.6+5gram 53.1 41.0 52.5 41.5 43.3 48.3+MBR 53.2 40.8 52.6 41.4 43.4 48.1b HiFST 52.2 41.5 51.6 42.1 42.4 48.7+5gram 53.3 40.6 52.7 41.3 43.7 48.1+MBR 53.7 40.4 53.3 40.9 44.0 48.0Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequentrescoring steps.
Decoding time reported for mt02-05-tune.The mixed case NIST BLEU-4 for the HiFST sys-tem on mt08 is 42.9.
This is directly comparable tothe official MT08 Constrained Training Track eval-uation results1.3.3 Chinese-to-English TranslationWe translate Chinese-to-English with full hierarchi-cal decoding, i.e.
hierarchical rules are allowed to besubstituted into non-terminals.
We consider a maxi-mum span of 10 words for the application of hierar-chical rules and only glue rules are allowed at upperlevels of the CYK grid.For translation model training, we use all avail-able data for the GALE 2008 evaluation2, approx.250M words per language.
In addition to the MT08set itself, we use a development set tune-nw anda validation set test-nw.
These contain a mix ofthe newswire portions of MT02 through MT05 andadditional developments sets created by translationwithin the GALE program.
The tune-nw set has1,755 sentences.Again, the HCP decoder employs k-best lists ofdepth k=10000.
The HiFST decoder applies prun-ing in search as described in Section 2.4, so that anylattice in the CYK grid is pruned if it covers at least3 source words and contains more than 10k states.The likelihood pruning threshold relative to the bestpath in the lattice is 9.
This is a very broad thresholdso that very few paths are discarded.1Full MT08 results are available at http://www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html.
It isworth noting that many of the top entries make use of systemcombination; the results reported here are for single systemtranslation.2See http://projects.ldc.upenn.edu/gale/data/catalog.html.Improved Optimization Table 2 shows results fortune-nw, test-nw and mt08, as measured by lower-cased IBM BLEU and TER.
The first two rows showresults for HCP when using MET parameters opti-mized over k-best lists produced by HCP (row ?a?
)and by HiFST (row ?b?).
We find that using the k-best list obtained by the HiFST decoder yields bet-ter parameters during optimization.
Tuning on theHiFST k-best lists improves the HCP BLEU score,as well.
We find consistent improvements in BLEU;TER also improves overall, although less consis-tently.Search Errors Measured over the tune-nw devel-opment set, HiFST finds a hypothesis with lowercost in 48.4% of the sentences.
In contrast, HCPnever finds any hypothesis with a lower cost for anysentence, indicating that the described pruning strat-egy for HiFST is much broader than that of HCP.Note that HCP search errors are more frequent forthis language pair.
This is due to the larger searchspace required in fully hierarchical translation; thelarger the search space, the more search errors willbe produced by the cube pruning k-best implemen-tation.Lattice/k-best Quality The lattices produced byHiFST yield greater gains in LM rescoring than thek-best lists produced by HCP.
Including the subse-quent MBR rescoring, translation improves as muchas 1.2 BLEU, compared to 0.7 BLEU with HCP.The mixed case NIST BLEU-4 for the HiFST sys-tem on mt08 is 27.8, comparable to official resultsin the UnConstrained Training Track of the NIST2008 evaluation.439decoder MET k-best tune-nw test-nw mt08BLEU TER BLEU TER BLEU TERa HCP HCP 31.6 59.7 31.9 59.7 ?
?b HCP 31.7 60.0 32.2 59.9 27.2 60.2+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3+MBR 32.4 59.2 32.7 59.4 28.1 59.3c HiFST 32.0 60.1 32.2 60.0 27.1 60.5+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1+MBR 32.9 58.4 33.4 58.5 28.9 58.9Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequentrescoring steps.
The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.4 ConclusionsThe lattice-based decoder for hierarchical phrase-based translation described in this paper can be eas-ily implemented using Weighted Finite State Trans-ducers.
We find many benefits in this approachto translation.
From a practical perspective, thecomputational operations required can be easily car-ried out using standard operations already imple-mented in general purpose libraries.
From a model-ing perspective, the compact representation of mul-tiple translation hypotheses in lattice form requiresless pruning in hierarchical search.
The result isfewer search errors and reduced overall memory userelative to cube pruning over k-best lists.
We alsofind improved performance of subsequent rescor-ing procedures which rely on the translation scores.In direct comparison to k-best lists generated un-der cube pruning, we find that MET parameter opti-mization, rescoring with large language models, andMBR decoding, are all improved when applied totranslations generated by the lattice-based hierarchi-cal decoder.AcknowledgmentsThis work was supported in part by the GALE pro-gram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011- 06-C-0022.
G.Iglesias supported by Spanish Government researchgrant BES-2007-15956 (project TEC2006-13694-C03-03).ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proceedings of ACL, pages 557?564.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of CIAA, pages 11?23.Srinivas Bangalore and Giuseppe Riccardi.
2001.
Afinite-state approach to machine translation.
In Pro-ceedings of NAACL.Oliver Bender, Evgeny Matusov, Stefan Hahn, SasaHasan, Shahram Khadivi, and Hermann Ney.
2007.The RWTH Arabic-to-English spoken language trans-lation system.
In Proceedings of ASRU, pages 396?401.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proceedings of ACL-HLT,pages 200?208.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of EMNLP-ACL,pages 858?867.Francisco Casacuberta.
2001.
Finite-state transducersfor speech-input translation.
In Proceedings of ASRU.Jean-Ce?dric Chappelier and Martin Rajman.
1998.A generalized CYK algorithm for parsing stochasticCFG.
In Proceedings of TAPD, pages 133?137.Jean-Ce?dric Chappelier, Martin Rajman, Ramo?nAragu?e?s, and Antoine Rozenknop.
1999.
Latticeparsing for speech recognition.
In Proceedings ofTALN, pages 95?104.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270.440David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Christopher Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of ACL-HLT, pages 1012?1020.Jonathan Graehl, Kevin Knight, and Jonathan May.
2008.Training tree transducers.
Computational Linguistics,34(3):391?427.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of ACL, pages 144?151.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedings ofEACL.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of HLT-NAACL, pages 169?176.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of HLT-EMNLP, pages 161?168.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In Pro-ceedings of the ACL-HLT Second Workshop on Syntaxand Structure in Statistical Translation, pages 10?18.Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proceedings of COLING, pages505?512.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proceedings of ACL-HLT, pages 1003?1011.Lambert Mathias and William Byrne.
2006.
Statisti-cal phrase-based speech translation.
In Proceedingsof ICASSP.Mehryar Mohri, Fernando Pereira, and Michael Riley.2000.
The design principles of a weighted finite-state transducer library.
Theoretical Computer Sci-ence, 231:17?32.Mehryar Mohri, Fernando Pereira, and Michael Riley.2002.
Weighted finite-state transducers in speechrecognition.
In Computer Speech and Language, vol-ume 16, pages 69?88.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL,pages 311?318.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie Dorr.2007.
Combining outputs from multiple machinetranslation systems.
In Proceedings of HLT-NAACL,pages 228?235.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-HLT, pages 577?585.Khe Chai Sim, William Byrne, Mark Gales, HichemSahbi, and Phil Woodland.
2007.
Consensus net-work decoding for statistical machine translation sys-tem combination.
In Proceedings of ICASSP, vol-ume 4, pages 105?108.Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA, pages 223?231.Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Riskdecoding for statistical machine translation.
In Pro-ceedings of EMNLP, pages 620?629.Ashish Venugopal, Andreas Zollmann, and VogelStephan.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In Proceed-ings of HLT-NAACL, pages 500?507.Hao Zhang and Daniel Gildea.
2006.
Synchronous bi-narization for machine translation.
In Proceedings ofHLT-NAACL, pages 256?263.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of NAACL Workshop on Statistical Ma-chine Translation, pages 138?141.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalMT.
In Proceedings of COLING, pages 1145?1152.441
