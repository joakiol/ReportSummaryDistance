Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 60?68,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsText-level Discourse Parsing with Rich Linguistic FeaturesVanessa Wei FengDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadaweifeng@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadagh@cs.toronto.eduAbstractIn this paper, we develop an RST-style text-level discourse parser, based on the HILDAdiscourse parser (Hernault et al, 2010b).
Wesignificantly improve its tree-building step byincorporating our own rich linguistic features.We also analyze the difficulty of extendingtraditional sentence-level discourse parsing totext-level parsing by comparing discourse-parsing performance under different discourseconditions.1 IntroductionIn a well-written text, no unit of the text is com-pletely isolated; interpretation requires understand-ing the unit?s relation with the context.
Research indiscourse parsing aims to unmask such relations intext, which is helpful for many downstream applica-tions such as summarization, information retrieval,and question answering.However, most existing discourse parsers oper-ate on individual sentences alone, whereas discourseparsing is more powerful for text-level analysis.Therefore, in this work, we aim to develop a text-level discourse parser.
We follow the framework ofRhetorical Structure Theory (Mann and Thompson,1988) and we take the HILDA discourse parser (Her-nault et al, 2010b) as the basis of our work, becauseit is the first fully implemented text-level discourseparser with state-of-the-art performance.
We signif-icantly improve the performance of HILDA?s tree-building step (introduced in Section 5.1 below) byincorporating rich linguistic features (Section 5.3).In our experiments (Section 6), we also analyze thedifficulty with extending traditional sentence-leveldiscourse parsing to text-level parsing, by compar-ing discourse parsing performance under differentdiscourse conditions.2 Discourse-annotated corpora2.1 The RST Discourse TreebankRhetorical Structure Theory (Mann and Thompson,1988) is one of the most widely accepted frame-works for discourse analysis.
In the framework ofRST, a coherent text can be represented as a dis-course tree whose leaves are non-overlapping textspans called elementary discourse units (EDUs);these are the minimal text units of discourse trees.Adjacent nodes can be related through particular dis-course relations to form a discourse subtree, whichcan then be related to other adjacent nodes in the treestructure.
According to RST, there are two types ofdiscourse relations, hypotactic (?mononuclear?)
andparatactic (?multi-nuclear?).
In mononuclear rela-tions, one of the text spans, the nucleus, is moresalient than the other, the satellite, while in multi-nuclear relations, all text spans are equally importantfor interpretation.The example text fragment shown in Figure 1consists of four EDUs (e1-e4), segmented by squarebrackets.
Its discourse tree representation is shownbelow in the figure, following the notational conven-tion of RST.
The two EDUs e1 and e2 are related by amononuclear relation ATTRIBUTION, where e1 is themore salient span; the span (e1-e2) and the EDU e3are related by a multi-nuclear relation SAME-UNIT,where they are equally salient.60[Catching up with commercial competitors in retail bankingand financial services,]e1 [they argue,]e2 [will be difficult,]e3[particularly if market conditions turn sour.
]e4(e1) (e2)attribution(e1-e3)same-unit(e3)(e4)condition(e1-e4)(e1-e2)Figure 1: An example text fragment (wsj 0616) com-posed of four EDUs, and its RST discourse tree repre-sentation.The RST Discourse Treebank (RST-DT) (Carlsonet al, 2001), is a corpus annotated in the frameworkof RST.
It consists of 385 documents (347 for train-ing and 38 for testing) from the Wall Street Jour-nal.
In RST-DT, the original 24 discourse relationsdefined by Mann and Thompson (1988) are furtherdivided into a set of 18 relation classes with 78 finer-grained rhetorical relations in total, which providesa high level of expressivity.2.2 The Penn Discourse TreebankThe Penn Discourse Treebank (PDTB) (Prasad etal., 2008) is another annotated discourse corpus.
Itstext is a superset of that of RST-DT (2159 WallStreet Journal articles).
Unlike RST-DT, PDTB doesnot follow the framework of RST; rather, it followsa lexically grounded, predicate-argument approachwith a different set of predefined discourse relations,as proposed by Webber (2004).
In this framework, adiscourse connective (e.g., because) is considered tobe a predicate that takes two text spans as its argu-ments.
The argument that the discourse connectivestructurally attaches to is called Arg2, and the otherargument is called Arg1 ?
unlike in RST, the twoarguments are not distinguished by their saliencyfor interpretation.
Another important difference be-tween PDTB and RST-DT is that in PDTB, theredoes not necessarily exist a tree structure coveringthe full text, i.e., PDTB-styled discourse relationsexist only in a very local contextual window.
InPDTB, relation types are organized hierarchically:there are 4 classes, which can be further divided into16 types and 23 subtypes.3 Related workDiscourse parsing was first brought to prominenceby Marcu (1997).
Since then, many different algo-rithms and systems (Soricut and Marcu, 2003; Reit-ter, 2003; LeThanh et al, 2004; Baldridge and Las-carides, 2005; Subba and Di Eugenio, 2009; Sagae,2009; Hernault et al, 2010b) have been proposed,which extracted different textual information andadopted various approaches for discourse tree build-ing.
Here we briefly review two fully implementedtext-level discourse parsers with the state-of-the-artperformance.The HILDA discourse parser of Hernault and hiscolleagues (duVerle and Prendinger, 2009; Hernaultet al, 2010b) is the first fully-implemented feature-based discourse parser that works at the full textlevel.
Hernault et al extracted a variety of lexi-cal and syntactic features from the input text, andtrained their system on RST-DT.
While some of theirfeatures were inspired by the previous work of oth-ers, e.g., lexico-syntactic features borrowed fromSoricut and Marcu (2003), Hernault et al also pro-posed the novel idea of discourse tree building byusing two classifiers in cascade ?
a binary struc-ture classifier to determine whether two adjacent textunits should be merged to form a new subtree, anda multi-class classifier to determine which discourserelation label should be assigned to the new subtree?
instead of the more-usual single multi-class clas-sifier with the additional label NO-REL.
Hernaultet al obtained 93.8% F-score for EDU segmenta-tion, 85.0% accuracy for structure classification, and66.8% accuracy for 18-class relation classification.Lin et al (2009) attempted to recognize implicitdiscourse relations (discourse relations which arenot signaled by explicit connectives) in PDTB by us-ing four classes of features ?
contextual features,constituent parse features, dependency parse fea-tures, and lexical features ?
and explored their indi-vidual influence on performance.
They showed thatthe production rules extracted from constituent parsetrees are the most effective features, while contex-tual features are the weakest.
Subsequently, theyfully implemented an end-to-end PDTB-style dis-course parser (Lin et al, 2010).Recently, Hernault et al (2010a) argued that moreeffort should be focused on improving performance61on certain infrequent relations presented in the dis-course corpora, since due to the imbalanced distribu-tion of different discourse relations in both RST-DTand PDTB, the overall accuracy score can be over-whelmed by good performance on the small sub-set of frequent relations, even though the algorithmsperform poorly on all other relations.
However, be-cause of infrequent relations for which we do nothave sufficient instances for training, many unseenfeatures occur in the test data, resulting in poor testperformance.
Therefore, Hernault et al proposeda semi-supervised method that exploits abundant,freely-available unlabeled data as a basis for featurevector extension to alleviate such issues.4 Text-level discourse parsingNot until recently has discourse parsing for full textsbeen a research focus ?
previously, discourse pars-ing was only performed on the sentence level1.
Inthis section, we explain why we believe text-leveldiscourse parsing is crucial.Unlike syntactic parsing, where we are almostnever interested in parsing above sentence level,sentence-level parsing is not sufficient for discourseparsing.
While a sequence of local (sentence-level)grammaticality can be considered to be global gram-maticality, a sequence of local discourse coherencedoes not necessarily form a globally coherent text.For example, the text shown in Figure 2 containstwo sentences, each of which is coherent and sen-sible itself.
However, there is no reasonable contenttransition between these two sentences, so the com-bination of the two sentences does not make muchsense.
If we attempt to represent the text as an RSTdiscourse tree like the one shown in Figure 1, wefind that no discourse relation can be assigned to re-late the spans (e1-e2) and (e3-e4) and the text cannotbe represented by a valid discourse tree structure.In order to rule out such unreasonable transitionsbetween sentences, we have to expand the text unitsupon which discourse parsing is performed: fromsentences to paragraphs, and finally paragraphs to1Strictly speaking, for PDTB-style discourse parsing (e.g.,Lin et al (2009; 2010)), there is no absolute distinction betweensentence-level and text-level parsing, since in PDTB, discourserelations are annotated at a level no higher than that of adjacentsentences.
Here we are concerned with RST-style discourseparsing.
[No wonder he got an A for his English class,]e1 [he wasstudying so hard.
]e2 [He avoids eating chocolates,]e3 [since heis really worried about gaining weight.
]e4(e1) (e2)cause(e1-e2)(e3) (e4)cause(e3-e4)?Figure 2: An example of incoherent text fragment com-posed of two sentences.
The two EDUs associated witheach sentence are coherent themselves, whereas the com-bination of the two sentences is not coherent at the sen-tence boundary.
No discourse relation can be associatedwith the spans (e1-e2) and (e3-e4).the full text.Text-level discourse parsing imposes more con-straints on the global coherence than sentence-leveldiscourse parsing.
However, if, technically speak-ing, text-level discourse parsing were no more diffi-cult than sentence-level parsing, any sentence-leveldiscourse parser could be easily upgraded to a text-level discourse parser just by applying it to fulltexts.
In our experiments (Section 6), we showthat when applied above the sentence level, the per-formance of discourse parsing is consistently infe-rior to that within individual sentences, and we willbriefly discuss what the key difficulties with extend-ing sentence-level to text-level discourse parsing are.5 MethodWe use the HILDA discourse parser of Hernault etal.
(2010b) as the basis of our work.
We refine Her-nault et al?s original feature set by incorporating ourown features as well as some adapted from Lin et al(2009).
We choose HILDA because it is a fully im-plemented text-level discourse parser with the bestreported performance up to now.
On the other hand,we also follow the work of Lin et al (2009), becausetheir features can be good supplements to those usedby HILDA, even though Lin et al?s work was basedon PDTB.
More importantly, Lin et al?s strategy ofperforming feature selection prior to classificationproves to be effective in reducing the total featuredimensions, which is favorable since we wish to in-corporate rich linguistic features into our discourseparser.625.1 Bottom-up approach and two-stagelabeling stepFollowing the methodology of HILDA, an input textis first segmented into EDUs.
Then, from the EDUs,a bottom-up approach is applied to build a discoursetree for the full text.
Initially, a binary Structure clas-sifier evaluates whether a discourse relation is likelyto hold between consecutive EDUs.
The two EDUswhich are most probably connected by a discourserelation are merged into a discourse subtree of twoEDUs.
A multi-class Relation classifier evaluateswhich discourse relation label should be assigned tothis new subtree.
Next, the Structure classifier andthe Relation classifier are employed in cascade to re-evaluate which relations are the most likely to holdbetween adjacent spans (discourse subtrees of anysize, including atomic EDUs).
This procedure is re-peated until all spans are merged, and a discoursetree covering the full text is therefore produced.Since EDU boundaries are highly correlated withthe syntactic structures embedded in the sentences,EDU segmentation is a relatively trivial step ?
us-ing machine-generated syntactic parse trees, HILDAachieves an F-score of 93.8% for EDU segmenta-tion.
Therefore, our work is focused on the tree-building step, i.e., the Structure and the Relationclassifiers.
In our experiments, we improve the over-all performance of these two classifiers by incorpo-rating rich linguistic features, together with appro-priate feature selection.
We also explore how thesetwo classifiers perform differently under differentdiscourse conditions.5.2 Instance extractionBecause HILDA adopts a bottom-up approach fordiscourse tree building, errors produced on lowerlevels will certainly propagate to upper levels, usu-ally causing the final discourse tree to be very dis-similar to the gold standard.
While appropriate post-processing may be employed to fix these errors andhelp global discourse tree recovery, we feel that itmight be more effective to directly improve the rawinstance performance of the Structure and Relationclassifiers.
Therefore, in our experiments, all classi-fications are conducted and evaluated on the basis ofindividual instances.Each instance is of the form (SL,SR), which is apair of adjacent text spans SL (left span) and SR (rightspan), extracted from the discourse tree representa-tion in RST-DT.
From each discourse tree, we ex-tract positive instances as those pairs of text spansthat are siblings of the same parent node, and neg-ative examples as those pairs of adjacent text spansthat are not siblings in the tree structure.
In all in-stances, both SL and SR must correspond to a con-stituent in the discourse tree, which can be either anatomic EDU or a concatenation of multiple consec-utive EDUs.5.3 Feature extractionGiven a pair of text spans (SL,SR), we extract thefollowing seven types of features.HILDA?s features: We incorporate the origi-nal features used in the HILDA discourse parserwith slight modification, which include the follow-ing four types of features occurring in SL, SR, orboth: (1) N-gram prefixes and suffixes; (2) syntac-tic tag prefixes and suffixes; (3) lexical heads in theconstituent parse tree; and (4) POS tag of the domi-nating nodes.Lin et al?s features: Following Lin et al (2009),we extract the following three types of features: (1)pairs of words, one from SL and one from SR, asoriginally proposed by Marcu and Echihabi (2002);(2) dependency parse features in SL, SR, or both; and(3) syntactic production rules in SL, SR, or both.Contextual features: For a globally coherenttext, there exist particular sequential patterns in thelocal usage of different discourse relations.
Given(SL,SR), the pair of text spans of interest, contextualfeatures attempt to encode the discourse relations as-signed to the preceding and the following text spanpairs.
Lin et al (2009) also incorporated contextualfeatures in their feature set.
However, their workwas based on PDTB, which has a very different an-notation framework from RST-DT (see Section 2):in PDTB, annotated discourse relations can form achain-like structure such that contextual features canbe more readily extracted.
However, in RST-DT, afull text is represented as a discourse tree structure,so the previous and the next discourse relations arenot well-defined.We resolve this problem as follows.
Suppose SL =(ei-e j) and SR = (e j+1-ek), where i?
j < k. To findthe previous discourse relation RELprev that immedi-63ately precedes (SL,SR), we look for the largest spanSprev = (eh-ei?1),h < i, such that it ends right beforeSL and all its leaves belong to a single subtree whichneither SL nor SR is a part of.
If SL and SR belongto the same sentence, Sprev must also be a within-sentence span, and it must be a cross-sentence spanif SL and SR are a cross-sentence span pair.
RELprevis then the discourse relation which covers Sprev.
Thenext discourse relation RELnext that immediately fol-lows (SL,SR) is found in the analogous way.However, when building a discourse tree usinga greedy bottom-up approach, as adopted by theHILDA discourse parser, RELprev and RELnext arenot always available; therefore these contextual fea-tures represent an idealized situation.
In our ex-periments we wish to explore whether incorporatingperfect contextual features can help better recognizediscourse relations, and if so, set an upper bound ofperformance in more realistic situations.Discourse production rules: Inspired by Lin etal.
(2009)?s syntactic production rules as features,we develop another set of production rules, namelydiscourse production rules, derived directly from thetree structure representation in RST-DT.For example, with respect to the RST discoursetree shown in Figure 1, we extract the followingdiscourse production rules: ATTRIBUTION ?
NO-REL NO-REL, SAME-UNIT ?
ATTRIBUTION NO-REL, CONDITION ?
SAME-UNIT NO-REL, whereNO-REL denotes a leaf node in the discourse subtree.The intuition behind using discourse productionrules is that the discourse tree structure is able to re-flect the relatedness of different discourse relations?
discourse relations on the lower level of the treecan determine the relation of their direct parent tosome degree.
Hernault et al (2010b) attempt tocapture such relatedness by traversing a discoursesubtree and encoding its traversal path as features,but since they used a depth-first traversal order, theinformation encoded in a node?s direct children istoo distant; whereas most useful information can begained from the relations covering these direct chil-dren.Semantic similarities: Semantic similarities areuseful for recognizing relations such as COMPARI-SON, when there are no explicit syntactic structuresor lexical features signaling such relations.We use two subsets of similarity features for verbsand nouns separately.
For each verb in either SL orSR, we look up its most frequent verb class ID inVerbNet2, and specify whether that verb class ID ap-pears in SL, SR, or both.
For nouns, we extract allpairs of nouns from (SL,SR), and compute the aver-age similarity among these pairs.
In particular, weuse path similarity, lch similarity, wup similarity,res similarity, jcn similarity, and lin similarity pro-vided in the nltk.wordnet.similarity package (Bird etal., 2009) for computing WordNet-based similarity,and always choose the most frequent sense for eachnoun.Cue phrases: We compile a list of cue phrases,the majority of which are connectives collected byKnott and Dale (1994).
For each cue phrase in thislist, we determine whether it appears in SL or SR. Ifa cue phrase appears in a span, we also determinewhether its appearance is in the beginning, the end,or the middle of that span.5.4 Feature selectionIf we consider all possible combinations of the fea-tures listed in Section 5.3, the resulting data spacecan be horribly high dimensional and extremelysparse.
Therefore, prior to training, we first conductfeature selection to effectively reduce the dimensionof the data space.We employ the same feature selection method asLin et al (2009).
Feature selection is done for eachfeature type separately.
Among all features belong-ing to the feature type to be selected, we first ex-tract all possible features that have been seen in thetraining data, e.g., when applying feature selectionfor word pairs, we find all word pairs that appearin some text span pair that have a discourse relationbetween them.
Then for each extracted feature, wecompute its mutual information with all 18 discourserelation classes defined in RST-DT, and use the high-est mutual information to evaluate the effectivenessof that feature.
All extracted features are sorted toform a ranked list by effectiveness.
After that, weuse a threshold to select the top features from thatranked list.
The total number of selected featuresused in our experiments is 21,410.2http://verbs.colorado.edu/?mpalmer/projects/verbnet646 ExperimentsAs discussed in Section 5.1, our research focus inthis paper is the tree-building step of the HILDAdiscourse parser, which consists of two classifica-tions: Structure and Relation classification.
The bi-nary Structure classifier decides whether a discourserelation is likely to hold between consecutive textspans, and the multi-class Relation classifier decideswhich discourse relation label holds between thesetwo text spans if the Structure classifier predicts theexistence of such a relation.Although HILDA?s bottom-up approach is aimedat building a discourse tree for the full text, it doesnot explicitly employ different strategies for within-sentence text spans and cross-sentence text spans.However, we believe that discourse parsing is signif-icantly more difficult for text spans at higher levelsof the discourse tree structure.
Therefore, we con-duct the following three sub-experiments to explorewhether the two classifiers behave differently underdifferent discourse conditions.Within-sentence: Trained and tested on text spanpairs belonging to the same sentence.Cross-sentence: Trained and tested on text spanpairs belonging to different sentences.Hybrid: Trained and tested on all text span pairs.In particular, we split the training set and the test-ing set following the convention of RST-DT, andconduct Structure and Relation classification by in-corporating our rich linguistic features, as listed inSection 5.3 above.
To rule out all confounding fac-tors, all classifiers are trained and tested on the basisof individual text span pairs, by assuming the dis-course subtree structure (if any) covering each indi-vidual text span has been already correctly identified(no error propagation).6.1 Structure classificationThe number of training and testing instances used inthis experiment for different discourse conditions islisted in Table 1.
Instances are extracted in the man-ner described in Section 5.2.
We observe that thedistribution of positive and negative instances is ex-tremely skewed for cross-sentence instances, whilefor all conditions, the distribution is similar in thetraining and the testing set.In this experiment, classifiers are trained usingDataset Pos # Neg # Total #WithinTraining 11,087 10,188 21,275Testing 1,340 1,181 2,521CrossTraining 6,646 49,467 56,113Testing 882 6,357 7,239HybridTraining 17,733 59,655 77,388Testing 2,222 7,539 9,761Table 1: Number of training and testing instances used inStructure classification.the SVMperf classifier (Joachims, 2005) with a lin-ear kernel.Structure classification performance for all threediscourse conditions is shown in Table 2.
Thecolumns Full and NC (No Context) denote the per-formance of using all features listed in Section 5.3and all features except for contextual features re-spectively.
As discussed in Section 5.3, contex-tual features represent an ideal situation which isnot always available in real applications; therefore,we wish to see how they affect the overall per-formance by comparing the performance obtainedwith them and without them as features.
The col-umn HILDA lists the performance of using Hernaultet al (2010b)?s original features, and Baseline de-notes the performance obtained by always pickingthe more frequent class.
Performance is measuredby four metrics: accuracy, precision, recall, and F1score on the test set, shown in the first section ineach sub-table.Under the within-sentence condition, we observethat, surprisingly, incorporating contextual featuresboosts the overall performance by a large margin,even though it requires only 38 additional features.Under the cross-sentence condition, our features re-sult in lower accuracy and precision than HILDA?sfeatures.
However, under this discourse condition,the distribution of positive and negative instancesin both training and test sets is extremely skewed,which makes it more sensible to compare the recalland F1 scores for evaluation.
In fact, our featuresachieve much higher recall and F1 score despite amuch lower precision and a slightly lower accuracy.In the second section of each sub-table, we alsolist the F1 score on the training data.
This allows65us to compare the model-fitting capacity of differ-ent feature sets from another perspective, especiallywhen the training data is not sufficiently well fittedby the model.
For example, looking at the trainingF1 score under the cross-sentence condition, we cansee that classification using full features and clas-sification without contextual features both performsignificantly better on the training data than HILDAdoes.
At the same time, such superior performanceis not due to possible over-fitting on the trainingdata, because we are using significantly fewer fea-tures (21,410 for Full and 21,372 for NC) than Her-nault et al (2010b)?s 136,987; rather, it suggeststhat using carefully selected rich linguistic featuresis able to better model the problem itself.Comparing the results obtained under the firsttwo conditions, we see that the binary classificationproblem of whether a discourse relation is likely tohold between two adjacent text spans is much moredifficult under the cross-sentence condition.
Onemajor reason is that many features that are predictivefor within-sentence instances are no longer applica-ble (e.g., Dependency parse features).
In addition,given the extremely imbalanced nature of the datasetunder this discourse condition, we might need toemploy special approaches to deal with this needle-in-a-haystack problem.
This difficulty can also beperceived from the training performance.
Comparedto the within-sentence condition, all features fit thetraining data much more poorly under the cross-sentence condition.
This suggests that sophisticatedfeatures or models in addition to our rich linguis-tic features must be incorporated in order to fit theproblem sufficiently well.
Unfortunately, this under-fitting issue cannot be resolved by exploiting anyabundant linguistic resources for feature vector ex-tension (e.g., Hernault et al (2010a)), because thepoor training performance is no longer caused by theunknown features found in test vectors.Turning to the hybrid condition, the performanceof Full features is surprisingly good, probably be-cause we have more available training data than theother two conditions.
However, with contextual fea-tures removed, our features perform quite similarlyto those of Hernault et al (2010b), but still witha marginal, but nonetheless statistically significant,improvement on recall and F1 score.Full NC HILDA BaselineWithin-sentenceAccuracy 91.04* 85.17* 83.74 53.15Precision 92.71* 85.36* 84.81 53.15Recall 90.22* 87.01* 84.55 100.00F1 91.45* 86.18* 84.68 69.41Train F1 97.87* 96.23* 95.42 68.52Cross-sentenceAccuracy 87.69 86.68 89.13 87.82Precision 49.60 44.73 61.90 ?Recall 63.95* 39.46* 28.00 0.00F1 55.87* 41.93* 38.56 ?Train F1 87.25* 71.93* 49.03 ?HybridAccuracy 95.64* 87.03 87.04 77.24Precision 94.77* 74.19 79.41 ?Recall 85.92* 65.98* 58.15 0.00F1 89.51* 69.84* 67.13 ?Train F1 93.15* 80.79* 72.09 ?Table 2: Structure classification performance (in percent-age) on text spans of within-sentence, cross-sentence, andall level.
Performance that is significantly superior to thatof HILDA (p < .01, using the Wilcoxon sign-rank test forsignificance) is denoted by *.6.2 Relation classificationThe Relation classifier has 18 possible output la-bels, which are the coarse-grained relation classesdefined in RST-DT.
We do not consider nuclearitywhen classifying different discourse relations, i.e.,ATTRIBUTION[N][S] and ATTRIBUTION[S][N] aretreated as the same label.
The training and test in-stances in this experiment are from the positive sub-set used in Structure classification.In this experiment, classifiers are trained usingLibSVM classifier (Chang and Lin, 2011) with a lin-ear kernel and probability estimation.Relation classification performance under threediscourse conditions is shown in Table 3.
We listthe performance achieved by Full, NC, and HILDAfeatures, as well as the majority baseline, which isobtained by always picking the most frequent classlabel (ELABORATION in all cases).66Full NC HILDA BaselineWithin-sentenceMAFS 0.490 0.485 0.446 ?WAFS 0.763 0.762 0.740 ?Acc (%) 78.06 78.13 76.42 31.42TAcc (%) 99.90 99.93 99.26 33.38Cross-sentenceMAFS 0.194 0.184 0.127 ?WAFS 0.334 0.329 0.316 ?Acc (%) 46.83 46.71 45.69 42.52TAcc (%) 78.30 67.30 57.70 47.79HybridMAFS 0.440 0.428 0.379 ?WAFS 0.607 0.604 0.588 ?Acc (%) 65.30 65.12 64.18 35.82TAcc (%) 99.96 99.95 90.11 38.78Table 3: Relation classification performance on textspans of within-sentence, cross-sentence, and all levels.Following Hernault et al (2010a), we use Macro-averaged F-scores (MAFS) to evaluate the perfor-mance of each classifier.
Macro-averaged F-scoreis not influenced by the number of instances thatexist in each relation class, by equally weightingthe performance of each relation class3.
Therefore,the evaluation is not biased by the performance onthose prevalent classes such as ATTRIBUTION andELABORATION.
For reasons of space, we do notshow the class-wise F-scores, but in our results,we find that using our features consistently providessuperior performance for most class relations overHILDA?s features, and therefore results in higheroverall MAFS under all conditions.
We also list twoother metrics for performance on the test data ?Weight-averaged F-score (WAFS), which weightsthe performance of each relation class by the num-ber of its existing instances, and the testing accuracy(Acc) ?
but these metrics are relatively more bi-3No significance test is reported for relation classification,because we are comparing MAFS, which equally weights theperformance of each relation.
Therefore, traditional signifi-cance tests which operate on individual instances rather thanindividual relation classes are not applicable.ased evaluation metrics in this task.
Similar to Struc-ture classification, the accuracy on the training data(TAcc)4 is listed in the second section of each sub-table.
It demonstrates that our carefully selected richlinguistic features are able to better fit the classifi-cation problem, especially under the cross-sentencecondition.Similar to our observation in Structure classifica-tion, the performance of Relation classification forcross-sentence instances is also much poorer thanthat on within-sentence instances, which again re-veals the difficulty of text-level discourse parsing.7 ConclusionsIn this paper, we aimed to develop an RST-styletext-level discourse parser.
We chose the HILDAdiscourse parser (Hernault et al, 2010b) as the ba-sis of our work, and significantly improved its tree-building step by incorporating our own rich linguis-tic features, together with features suggested by Linet al (2009).
We analyzed the difficulty of extendingtraditional sentence-level discourse parsing to text-level parsing by showing that using exactly the sameset of features, the performance of Structure and Re-lation classification on cross-sentence instances isconsistently inferior to that on within-sentence in-stances.
We also explored the effect of contextualfeatures on the overall performance.
We showedthat contextual features are highly effective for bothStructure and Relation classification under all dis-course conditions.
Although perfect contextual fea-tures are available only in idealized situations, whenthey are correct, together with other features, theycan almost correctly predict the tree structure andbetter predict the relation labels.
Therefore, an it-erative updating approach, which progressively up-dates the tree structure and the labeling based on thecurrent estimation, may push the final results towardthis idealized end.Our future work will be to fully implement anend-to-end discourse parser using our rich linguis-tic features, and focus on improving performance oncross-sentence instances.4We use accuracy instead of MAFS as the evaluation metricon the training data because it is the metric that the trainingprocedure is optimized toward.67AcknowledgmentsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada and by the University of Toronto.ReferencesJason Baldridge and Alex Lascarides.
2005.
Probabilis-tic head-driven parsing for discourse structure.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning, pages 96?103.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python ?
AnalyzingText with the Natural Language Toolkit.
O?Reilly.Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.2001.
Building a discourse-tagged corpus in theframework of Rhetorical Structure Theory.
In Pro-ceedings of Second SIGdial Workshop on Discourseand Dialogue, pages 1?10.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:1?27.David A. duVerle and Helmut Prendinger.
2009.
Anovel discourse parser based on Support Vector Ma-chine classification.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, Volume 2, ACL ?09,pages 665?673, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2010a.
A semi-supervised approach to im-prove classification of infrequent discourse relationsusing feature vector extension.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 399?409, Cambridge,MA, October.
Association for Computational Linguis-tics.Hugo Hernault, Helmut Prendinger, David A. duVerle,and Mitsuru Ishizuka.
2010b.
HILDA: A discourseparser using support vector machine classification.
Di-alogue and Discourse, 1(3):1?33.Thorsten Joachims.
2005.
A support vector method formultivariate performance measures.
In InternationalConference on Machine Learning (ICML), pages 377?384.Alistair Knott and Robert Dale.
1994.
Using linguisticphenomena to motivate a set of coherence relations.Discourse Processes, 18(1).Huong LeThanh, Geetha Abeysinghe, and ChristianHuyck.
2004.
Generating discourse structures forwritten texts.
In Proceedings of the 20th InternationalConference on Computational Linguistics, pages 329?335.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, Volume 1, EMNLP ?09, pages 343?351.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
APDTB-styled end-to-end discourse parser.
Technicalreport, School of Computing, National University ofSingapore.William Mann and Sandra Thompson.
1988.
Rhetoricalstructure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse re-lations.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages368?375, Philadelphia, Pennsylvania, USA, July.
As-sociation for Computational Linguistics.Daniel Marcu.
1997.
The rhetorical parsing of natu-ral language texts.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguis-tics, pages 96?103.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.In Proceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).David Reitter.
2003.
Simple signals for complexrhetorics: On rhetorical analysis with rich-feature sup-port vector models.
LDV Forum, 18(1/2):38?52.Kenji Sagae.
2009.
Analysis of discourse structure withsyntactic dependencies and data-driven shift-reduceparsing.
In Proceedings of the 11th International Con-ference on Parsing Technologies, pages 81?84.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical informa-tion.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy, Volume 1, pages 149?156.Rajen Subba and Barbara Di Eugenio.
2009.
An effec-tive discourse parser that uses rich linguistic informa-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 566?574.Bonnie Webber.
2004.
D-LTAG: Extending lexicalizedTAG to discourse.
Cognitive Science, 28(5):751?779.68
