Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?88,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsExperimental Results on the Native Language Identification Shared TaskAmjad Abu-Jbara, Rahul Jha, Eric Morley, Dragomir RadevDepartment of EECSUniversity of MichiganAnn Arbor, MI, USA[amjbara, rahuljha, eamorley, radev]@umich.eduAbstractWe present a system for automatically iden-tifying the native language of a writer.
Weexperiment with a large set of features andtrain them on a corpus of 9,900 essays writ-ten in English by speakers of 11 different lan-guages.
our system achieved an accuracy of43% on the test data, improved to 63% withimproved feature normalization.
In this paper,we present the features used in our system, de-scribe our experiments and provide an analysisof our results.1 IntroductionThe task of Native Language Identification (NLI)is the task of identifying the native language of awriter or a speaker by analyzing their writing inEnglish.
Previous work in this area shows thatthere are several linguistic cues that can be usedto do such identification.
Based on their nativelanguage, different speakers tend to make differentkinds of errors pertaining to spelling, punctuation,and grammar (Garfield, 1964; Wong and Dras, 2009;Kochmar, 2011).
We describe the complete set offeatures we considered in Section 4.
We evaluatedifferent combinations of these features, and differ-ent ways of normalizing them in Section 5.There are many possible applications for an NLIsystem, as noted by Kochmar (2011): finding theorigins of anonymous text; error correction in var-ious tasks including speech recognition, part-of-speech tagging, and parsing; and in the field of sec-ond language acquisition for identifying learner dif-ficulties.
We are most interested in statistical ap-proaches to this problem because it may point to-wards fruitful avenues of research in language andsound transfer, which are how people apply knowl-edge of their native language, and its phonologyand orthography, respectively, to a second language.For example, Tsur and Rappoport (2007) found thatcharacter bigrams are quite useful for NLI, whichled them to suggest that second language learners?word choice may in part be driven by their nativelanguages.
Analysis of such language and soundtranslation patterns might be useful in understand-ing the process of language acquisition in humans.2 Previous WorkThe work presented in this paper was done as partof the NLI shared task (Tetreault et al 2013), whichis the first time this problem has been the subjectof a shared task.
However, several researchers haveinvestigated NLI and similar problems.
Authorshipattribution, a related problem, has been well stud-ied in the literature, starting from the seminal workon disputed Federalist Papers by Mosteller and Wal-lace (1964).
The goal of authorship attribution isto assign a text to one author from a candidate set82of authors.
This technique has many applications,and has recently been used to investigate terroristcommunication (Abbasi and Chen, 2005) and dig-ital crime (Chaski, 2005).
The goal of NLI some-what similar to authorship attribution, in that NLIattempts to distinguish between candidate commu-nities of people who share a common cultural andlinguistic background, while authorship attributiondistinguishes between candidate individuals.In the earliest treatment of this problem, Koppelet al(2005) used stylistic text features to identifythe native language of an author.
They used featuresbased on function words, character n-grams and er-rors and idiosyncrasies such as spelling errors andnon-standard syntactic constructions.
They exper-imented on a dataset with essays written by non-native English speakers from five countries, Russia,Czech Republic, Bulgaria, France and Spain, with258 instances from each dataset.
They trained amulti-class SVM model using the above features andreported 10-fold cross validation accuracy of 80.2%.Tsur and Rappoport (2007) studied the problemof NLI with a focus on language transfer, i.e.
howa seaker?s native language affects the way in whichthey acquire a second language, an important area inSecond Language Acquisition research.
Their fea-ture analysis showed that character bigrams alonecan lead to a classification accuracy of about 66%in a 5-class task.
They concluded that the choice ofwords people make when writing in a second lan-guage is highly influenced by the phonology of theirnative language.Wong and Dras (2009) studied syntactic errors de-rived from contrastive analysis as features for NLI.They used the five languages from Koppel et al(2008) along with Chinese and Japanese, but did notfind an improvement in classification accuracy byadding error features based on contrastive analysis.Later, Wong and Dras (2011) studied a more gen-eral set of syntactic features and showed that addingthese features improved the accuracy significantly.They also investigated classification models basedon LDA (Wong et al 2011), but did not find themto be useful overall.
They did, however, notice thatsome of the topics were capturing information thatwould be useful for identifying particular native lan-guages.
They also proposed the use of adaptor gram-mars (Johnson et al 2007), which are a generaliza-tion of probabilistic context-free grammars, to cap-ture collocational pairings.
In a later paper, Wonget alexplored the use of adapter grammars in de-tail (Wong et al 2012) and showed that an exten-sion of adaptor grammars to discover collocationsbeyond lexical words can produce features useful forthe NLI task.3 DatasetThe experiments for this paper were performed us-ing the TOEFL11 dataset (Blanchard et al 2013)provided as part of the shared task.
The dataset con-tains essays written in English from native speakersof 11 languages (Arabic, Chinese, French, German,Hindi, Italian, Japanese, Korean, Spanish, Telugu,and Turkish).
The corpus contains 12,099 essays perlanguage sampled evenly from 8 prompts or topics.This dataset was designed specifically to support thetask of NLI and addresses some of the shortcom-ings of earlier datasets used for research in this area.Specifically, the dataset has been carefully selectedin order to maintain consistency in topic distribu-tions, character encodings and annotations acrossthe essays from different native languages.
The datawas split into three data sets: a training set com-prising 9,900 essays, a development set comprising1,100 essays, and a test set comprising 1,100 essays.4 ApproachWe addressed the problem as a supervised, multi-class classification task.
We trained a Support VectorMachine (SVM) classifier on a set of lexical, syntac-tic and dependency features extracted from the train-ing data.
We computed the minimum and maximumvalues for each of the features and normalized thevalues by the range (max - min).
Here we describethe features in turn.83Character and Word N-grams Tsur and Rap-poport (2007) found that character bigrams wereuseful for NLI, and they suggested that this may bedue to the writer?s native language influencing theirchoice of words.
To reflect this, we compute featuresusing both characters and word N-grams.
For char-acters, we consider 2,3 and 4-grams, with paddingcharacters at the beginning and end of each sentence.The features are generated over the entire trainingdata, i.e., every n-gram occurring in the training datais used as a feature.
Similarly, we create featureswith 1,2 and 3-grams of words.
Each word n-gramis used as a separate feature.
We explore both binaryfeatures for each character or word n-gram, as wellas normalized count features.Part-Of-Speech N-grams Several investigations,for example those conducted by Kochmar (2011)and Wong and Dras (2011), have found that part-of-speech tags can be useful for NLI.
Therefore we in-clude part-of-speech (POS) n-grams as features.
Weparse the sentences with the Stanford Parser (Kleinand Manning, 2003) and extract the POS tags.
Weuse binary features describing the presence or ab-sence of POS bigrams in a document, as well as nu-merical features describing their relative frequencyin a document.Function Words Koppel et al(2005) found thatfunction words can help identify someone?s nativelanguage.
To this end, we include a categorical fea-ture for the presence of function words that are in-cluded in list of 321 function words.Use of punctuation Based on our experiencewith speakers of native languages, as well asKochmar?s (2011) observations of written Englishproduced by Germanic and Romance languagespeakers, we suspect that speakers of different nativelanguages use punctuation in different ways, pre-sumably based on the punctuation patterns in theirnative language.
For example, comma placementdiffers between German and English, and neitherChinese nor Japanese requires a full stop at the endof every sentence.
To capture these kinds of patterns,we create two features for each essay: the number ofpunctuation marks used per sentence, and the num-ber of punctuation marks used per word.Number of Unique Stems Speakers of differentnative languages might differ in the amount of vo-cabulary they use when communicating in English.We capture this by counting the number of uniquestems in each essay and using this as an additionalfeature.
The hypothesis here is that depending on thesimilarity of the native language with English, thepresence of common words, and other cultural cues,people with different native language might have ac-cess to different amounts of vocabulary.Misuse of Articles We count instances in whichthe number of an article is inconsistent with the as-sociated noun.
To do so, we fist identify all the detdependency relations in the essay.
We then com-pute the ratio of det relations between ?a?
or ?an?and a plural noun (NNS), to all det relations.
Wealso count the ratio of det relations between ?a?
or?an?
and an uncountable noun, to all det relations.We do this using a list of 288 uncountable nouns.1Capitalization The writing systems of some lan-guages in the data set, for example Telugu, do notinclude capitalization.
Furthermore, other languagesmay use capitalization quite differently from En-glish, for example German, in which all nouns arecapitalized, and French, in which nationalities arenot.
Character capitalization mistakes may be com-mon in the text written by the speakers of such lan-guages.
We compute the ratio of words with at leasttwo letters that are written in all caps to identify ex-cessive capitalization.
We also count the relative fre-quency of capitalized words that appear in the mid-dle of a sentence that are not tagged as proper nounsby the part of speech tagger.Tense and Aspect Frequency Verbal tense andaspect systems vary widely between languages.
En-glish has obligatory tense (past, present, future) and1http://www.englishclub.com/vocabulary/nouns-uncountable-list.htm84aspect (imperfect, perfect, progressive) marking onverbs.
Other languages, for example French, mayrequire verbs to be marked for tense, but not as-pect.
Still other languages, for example Chinese,may use adverbials and temporal phrases to com-municate temporal and aspectual information.
Toattempt to capture some of the ways learners of En-glish may be influenced by their native language?ssystem of tense and aspect, we compute two fea-tures.
First, we compute the relative frequency ofeach tense and aspect in the article from the countsof each verb POS tags (ex.
VB, VBD, VBG).
Wealso compute the percentage of sentences that con-tain verbs of different tenses or aspect, again usingthe verb POS tags.Missing Punctuation We compute the relativefrequency of sentences that include an introductoryphrase (e.g.
however, furthermore, moreover) that isnot followed by a comma.
We also count the relativefrequency of sentences that start with a subordinat-ing conjunction (e.g.
sentences starting with if, after,before, when, even though, etc.
), but do not containa comma.Average Number of Syllables We count the num-ber of syllables per word and the ratio of words withthree or more syllables.
To count the number of syl-lables in a word, we used a perl module that esti-mates the number of syllables by applying a set ofhand-crafted rules.2.Arc Length We calculate several features pertain-ing to dependency arc length and direction.
Weparse each sentence separately, using the StanfordDependency Parser, and then compute a single valuefor each of these features for each document.
First,we simply compute the percentage of arcs that pointleft or right (PCTARCL, PCTARCR).
We also com-pute the minumum, maximum, and mean depen-dency arc length, ignoring arc direction.
We alsocompute similar features for typed dependencies:the minimum, maximum, and mean dependency arc2http://search.cpan.org/dist/Lingua-EN-Syllable/Syllable.pmlength for each typed dependency; and the percent-age of arcs for each typed dependency that go to theleft or right.Downtoners and Intensifiers We compute threefeatures to describe the use of downtoners, and threefor intensifiers in each document.
First, we count thenumber of downtoners or intensifiers in a given doc-ument.3 We normalize this count by the number oftokens, types, and sentences in the document to yieldthe three features capturing the use of downtoners orintensifiers.Production Rules We compute a set of features todescribe the relative frequency of production rulesin a given document.
First, we parse each sentenceusing the Stanford Parser, using the default EnglishPCFG (Klein and Manning, 2003).
We then countall non-terminal production rules in a given docu-ment, and report the relative frequency of each pro-duction rule in that document.Subject Agreement We count the number of sen-tences in which there appears to be a mistake in sub-ject agreement.
To do this, we first identify nsubjand nsubjpass dependency relationships.
Of thesedependencies, we count ones meeting the followingcriteria as mistakes: a third person singular presenttense verb with a nominal that is not third personsingular, and a third person singular subject with apresent tense verb not marked as third person sin-gular.
We then normalize the count of errors by thetotal number of nsubj and nsubj pass dependenciesin the document, and the number of sentences in thedocument to produce two features.Words per Sentence We compute both the num-ber of tokens per line and the number of types per3The words we count as downtoners are: ?almost?, ?alot?,?a lot?, ?barely?, ?a bit?, ?fairly?, ?hardly?, ?just?, ?kind of?,?least?, ?less?, ?merely?, ?mildly?, ?nearly?, ?only?, ?partially?,?partly?, ?practically?, ?rather?, ?scarcely?, ?sort of?, ?slightly?,and ?somewhat?.
The intensifiers are: ?a good deal?, ?a greatdeal?, ?absolutely?, ?altogether?, ?completely?,?enormously?,?entirely?, ?extremely?, ?fully?, ?greatly?, ?highly?, ?intensely?,?more?, ?most?, ?perfectly?, ?quite?, ?really?, ?so?, ?strongly?,?super?, ?thoroughly?, ?too?, ?totally?, ?utterly?, and ?very?.85line.Topic Scores We construct an unsupervised topicmodel for all of the documents using Mallet (Mc-Callum, 2002) with 100 topics, dirichlet hyperpa-rameter reestimation every 10 rounds, and all otheroptions set to default values.
We then use the topicweights as features.Passive Constructions We count the number oftimes an author uses passive constructions by count-ing the number of nsubjpass dependencies in eachdocument.
We normalize this count in two ways toproduce two different features: dividing by the num-ber of sentences, and dividing by the total number ofnsubj and nsubjpass dependencies.5 Experiments and ResultsWe used weka (Hall et al 2009) and libsvm (Changand Lin, 2011) to run the experiments.
The classi-fication was done using an SVM classifier.
We ex-perimented with different SVM kernels and differentvalues for the cost parameter.
The best performancewas achieved with a linear kernel and cost = 0.001.We trained the model using the combination of thetraining and the development sets.
We submitted theoutput of the system to the NLI shared task work-shop.
Our system achieved 43.3% accuracy.
Table 1shows the confusion matrix and the precision, recall,and F-measure for each language.
After the NLIsubmission deadline, we noticed that we our systemwas not handling the normalization of the featuresproperly which resulted in the poor performance.After fixing the problem, our system achieved 63%accuracy on both test data and 10-fold cross valida-tion on the entire data.6 AnalysisWe did feature analysis on the training and devel-opment data sets using the Chi-squared test.
Ourfeature analysis shows that the most important fea-tures for the classifier were topic models, charac-ter n-grams of all orders, word unigrams and bi-grams, POS bigrams, capitalization features, func-tion words, production rules, and arc length.
Theseresults are consistent with those presented in previ-ous work done on this task.Looking at the confusion matrix in Figure 1, wesee that Korean and Japanese were the most com-monly confused pair of languages.
Hindi and Tel-ugu, two languages from the Indian subcontinent,were also often confused.
To analyze this further,we did another experiment by training just a binaryclassifier on Korean and Japanese using the exactsame feature set as earlier.
We achieved a 10-foldcross validation accuracy of 83.3% on this classifi-cation task.
Thus, given just these two languages,we were able to obtain high classification accuracy.This suggests that a potentially fruitful strategy forNLI systems might be to fuse often-confused pairs,such as Korean/Japanese and Hindi/Telugu, into sin-gleton classes for the initial run, and then run a sec-ond classifier to do a more fine grained classificationwithin these higher level classes.When doing feature analysis for these two lan-guages, we found that the character bigrams rep-resenting the country names were some of the topfeatures used for classification.
For example ?Kor?occurred as a trigram frequently in essays from na-tive language speakers of Korean.
Based on this, wedesigned a small experiment where we created fea-tures corresponding to the country name associatedwith each native language, e.g., ?Korea?, ?China?,?India?, ?France?, etc.
For Arabic, we used a list of22 countries where Arabic is spoken.
Just using thisfeature, we obtained a 10-fold cross validation accu-racy of 21.3% on the development set.
This suggeststhat in certain genres, one may be able to leverage in-formation conveying geographical and demographicattributes for NLI.7 ConclusionIn this paper, we presented a supervised system forthe task of Native Language Identification.
We de-scribe and motivate several features for this taskand report results of supervised classification usingthese features on a test data set consisting of 11 lan-86ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 41 7 8 3 6 2 3 5 10 7 8 44.6% 41.0% 42.7%CHI 6 38 5 2 2 8 15 8 3 3 10 40.0% 38.0% 39.0%FRE 8 6 43 8 1 14 2 4 6 1 7 39.1% 43.0% 41.0%GER 3 3 10 49 4 9 1 7 6 0 8 54.4% 49.0% 51.6%HIN 5 2 6 9 34 0 3 1 3 32 5 47.9% 34.0% 39.8%ITA 5 3 10 5 1 52 2 1 17 0 4 46.0% 52.0% 48.8%JPN 3 11 0 1 1 3 49 26 1 1 4 37.4% 49.0% 42.4%KOR 2 6 6 1 1 2 35 40 1 1 5 38.1% 40.0% 39.0%SPA 4 6 14 1 1 17 6 2 38 0 11 40.9% 38.0% 39.4%TEL 9 7 3 4 18 2 2 2 2 48 3 51.1% 48.0% 49.5%TUR 6 6 5 7 2 4 13 9 6 1 41 38.7% 41.0% 39.8%Accuracy = 43.0%Table 1: The results of our original submission to the NLI shared task on the test set.
These results reflect theperformance of the system that does not normalize the features properlyguages provided as part of the NLI shared task.
Wefound that our classifier often confused two pairsof languages that are spoken near one another, butare linguistically unrelated: Hindi/Telugu and Ko-rean/Japanese.
We found that we could obtain highclassification accuracy on these two pairs of lan-guages using a binary classifier trained on just thesepairs.
During our feature analysis, we also foundthat certain features that happened to convey geo-graphical and demographic information were alsoinformative for this task.ReferencesAhmed Abbasi and Hsinchun Chen.
2005.
Apply-ing authorship analysis to extremist-group web fo-rum messages.
IEEE Intelligent Systems, 20(5):67?75,September.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.Carole E. Chaski.
2005. Who?s at the keyboard: Au-thorship attribution in digital evidence investigations.International Journal of Digital Evidence, 4:2005.Eugene Garfield.
1964.
Can citation indexing be auto-mated?Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor grammars: A framework for speci-fying compositional nonparametric bayesian models.Advances in neural information processing systems,19:641.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Ekaterina Kochmar.
2011.
Identification of a Writer?sNative Langauge by Error Analysis.
Ph.D. thesis.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining, pages 624?628, Chicago, IL.ACM.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2008.
Computational methods in authorship attribu-tion.
Journal of the American Society for informationScience and Technology, 60(1):9?26.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Frederick Mosteller and David L. Wallace.
1964.
Infer-ence and Disputed Authorship: The Federalist Papers.Addison-Wesley, Reading, Mass.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshop87on Innovative Use of NLP for Building EducationalApplications, Atlanta, GA, USA, June.
Association forComputational Linguistics.Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Com-putational Language Acquisition, CACLA ?07, pages9?16, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Sze-Meng Jojo Wong and Mark Dras.
2009.
ContrastiveAnalysis and Native Language Identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop 2009, pages 53?61, Sydney, Aus-tralia, December.Sze-Meng Jojo Wong and Mark Dras.
2011.
ExploitingParse Structures for Native Language Identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2011.
Topic Modeling for Native Language Identifi-cation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 115?124, Canberra, Australia, December.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring Adaptor Grammars for Native Lan-guage Identification.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 699?709, Jeju Island, Korea,July.
Association for Computational Linguistics.88
