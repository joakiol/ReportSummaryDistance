Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076?1085,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPRe-Ranking Models Based-on Small Training Data for Spoken LanguageUnderstandingMarco DinarelliUniversity of TrentoItalydinarelli@disi.unitn.itAlessandro MoschittiUniversity of TrentoItalymoschitti@disi.unitn.itGiuseppe RiccardiUniversity of TrentoItalyriccardi@disi.unitn.itAbstractThe design of practical language applica-tions by means of statistical approachesrequires annotated data, which is one ofthe most critical constraint.
This is par-ticularly true for Spoken Dialog Systemssince considerably domain-specific con-ceptual annotation is needed to obtain ac-curate Language Understanding models.Since data annotation is usually costly,methods to reduce the amount of data areneeded.
In this paper, we show that bet-ter feature representations serve the abovepurpose and that structure kernels pro-vide the needed improved representation.Given the relatively high computationalcost of kernel methods, we apply them tojust re-rank the list of hypotheses providedby a fast generative model.
Experimentswith Support Vector Machines and differ-ent kernels on two different dialog cor-pora show that our re-ranking models canachieve better results than state-of-the-artapproaches when small data is available.1 IntroductionSpoken Dialog Systems carry out automaticspeech recognition and shallow natural languageunderstanding by heavily relying on statisticalmodels.
These in turn need annotated data de-scribing the application domain.
Such annotationis far the most expensive part of the system de-sign.
Therefore, methods reducing the amount oflabeled data can speed up and lower the overallamount of work.Among others, Spoken Language Understand-ing (SLU) is an important component of the sys-tems above, which requires training data to trans-late a spoken sentence into its meaning repre-sentation based on semantic constituents.
Theseare conceptual units instantiated by sequences ofwords.In the last decade two major approaches havebeen proposed to automatically map words in con-cepts: (i) generative models, whose parameters re-fer to the joint probability of concepts and con-stituents; and (ii) discriminative models, whichlearn a classification function based on conditionalprobabilities of concepts given words.A simple but effective generative model is theone based on Finite State Transducers.
It performsSLU as a translation process from words to con-cepts using Finite State Transducers (FST).
An ex-ample of discriminative model used for SLU is theone based on Support Vector Machines (SVMs)(Vapnik, 1995), as shown in (Raymond and Ric-cardi, 2007).
In this approach, data is mapped intoa vector space and SLU is performed as a clas-sification problem using Maximal Margin Clas-sifiers (Vapnik, 1995).
A relatively more recentapproach for SLU is based on Conditional Ran-dom Fields (CRF) (Lafferty et al, 2001).
CRFsare undirected graphical and discriminative mod-els.
They use conditional probabilities to accountfor many feature dependencies without the need ofexplicitly representing such dependencies.Generative models have the advantage to bemore robust to overfitting on training data, whilediscriminative models are more robust to irrele-vant features.
Both approaches, used separately,have shown good accuracy (Raymond and Ric-cardi, 2007), but they have very different charac-teristics and the way they encode prior knowledgeis very different, thus designing models that takeinto account characteristics of both approaches areparticularly promising.In this paper, we propose a method for SLUbased on generative and discriminative models:the former uses FSTs to generate a list of SLUhypotheses, which are re-ranked by SVMs.
Toeffectively design our re-ranker, we use all pos-1076sible word/concept subsequences with gaps of thespoken sentence as features (i.e.
all possible n-grams).
Gaps allow for encoding long distance de-pendencies between words in relatively small se-quences.
Since the space of such features is huge,we adopted kernel methods, i.e.
sequence kernels(Shawe-Taylor and Cristianini, 2004) and tree ker-nels (Collins and Duffy, 2002; Moschitti, 2006a)to implicitly encode them along with other struc-tural information in SVMs.We experimented with different approaches fortraining the discriminative models and two differ-ent corpora: the french MEDIA corpus (Bonneau-Maynard et al, 2005) and a corpus made availableby the European project LUNA1(Dinarelli et al,2009b).
In particular, the new contents with re-spect to our previous work (Dinarelli et al, 2009a)are:?
We designed a new sequential structure(SK2) and two new hierarchical tree struc-tures (MULTILEVEL and FEATURES) forre-ranking models (see Section 4.2).
The lat-ter combined with two different tree kernelsoriginate four new different models.?
We experimented with automatic speechtranscriptions thus assessing the robustness tonoise of our models.?
We compare our models against ConditionalRandom Field (CRF) approaches describedin (Hahn et al, 2008), which are the cur-rent state-of-the-art in SLU.
Learning curvesclearly show that our models improve CRF,especially when small data sets are used.The remainder of the paper is organized as fol-lows: Section 2 introduces kernel methods forstructured data, Section 3 describes the generativemodel producing the initial hypotheses whereasSection 4 presents the discriminative models forre-ranking them.
The experiments and resultsare reported in Section 5 and the conclusions aredrawn in Section 6.2 Feature Engineering via StructureKernelsKernel methods are viable approaches to engi-neer features for text processing, e.g.
(Collins andDuffy, 2002; Kudo and Matsumoto, 2003; Cumby1Contract n. 33549and Roth, 2003; Cancedda et al, 2003; Culottaand Sorensen, 2004; Toutanova et al, 2004; Kudoet al, 2005; Moschitti, 2006a; Moschitti et al,2007; Moschitti, 2008; Moschitti et al, 2008;Moschitti and Quarteroni, 2008).
In the follow-ing, we describe structure kernels, which will beused to engineer features for our discriminative re-ranker.2.1 String KernelsThe String Kernels that we consider count thenumber of substrings containing gaps shared bytwo sequences, i.e.
some of the symbols of theoriginal string are skipped.
We adopted the ef-ficient algorithm described in (Shawe-Taylor andCristianini, 2004; Lodhi et al, 2000).
Morespecifically, we used words and markers as sym-bols in a style similar to (Cancedda et al, 2003;Moschitti, 2008).
For example, given the sen-tence: How may I help you ?
sample substrings,extracted by the Sequence Kernel (SK), are: Howhelp you ?, How help ?, help you, may help you,etc.2.2 Tree kernelsTree kernels represent trees in terms of their sub-structures (fragments).
The kernel function detectsif a tree subpart (common to both trees) belongs tothe feature space that we intend to generate.
Forsuch purpose, the desired fragments need to be de-scribed.
We consider two important characteriza-tions: the syntactic tree (STF) and the partial tree(PTF) fragments.2.2.1 Tree Fragment TypesAn STF is a general subtree whose leaves canbe non-terminal symbols (also called SubSet Tree(SST) in (Moschitti, 2006a)).
For example, Fig-ure 1(a) shows 10 STFs (out of 17) of the sub-tree rooted in VP (of the left tree).
The STFs sat-isfy the constraint that grammatical rules cannotbe broken.
For example, [VP [V NP]] is anSTF, which has two non-terminal symbols, V andNP, as leaves whereas [VP [V]] is not an STF.If we relax the constraint over the STFs, we ob-tain more general substructures called partial treesfragments (PTFs).
These can be generated by theapplication of partial production rules of the gram-mar, consequently [VP [V]] and [VP [NP]]are valid PTFs.
Figure 1(b) shows that the num-ber of PTFs derived from the same tree as beforeis still higher (i.e.
30 PTs).1077(a) Syntactic Tree fragments (STF) (b) Partial Tree fragments (PTF)Figure 1: Examples of different classes of tree fragments.2.3 Counting Shared SubtreesThe main idea of tree kernels is to compute thenumber of common substructures between twotrees T1and T2without explicitly considering thewhole fragment space.
To evaluate the above ker-nels between two T1and T2, we need to define aset F = {f1, f2, .
.
.
, f|F|}, i.e.
a tree fragmentspace and an indicator function Ii(n), equal to 1if the target fiis rooted at node n and equal to 0otherwise.
A tree-kernel function over T1and T2is TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2),where NT1and NT2are the sets of the T1?sand T2?s nodes, respectively and ?
(n1, n2) =?|F|i=1Ii(n1)Ii(n2).
The latter is equal to the num-ber of common fragments rooted in the n1and n2nodes.The algorithm for the efficient evaluation of ?for the syntactic tree kernel (STK) has been widelydiscussed in (Collins and Duffy, 2002) whereas itsfast evaluation is proposed in (Moschitti, 2006b),so we only describe the equations of the partialtree kernel (PTK).2.4 The Partial Tree Kernel (PTK)PTFs have been defined in (Moschitti, 2006a).Their computation is carried out by the following?
function:1. if the node labels of n1and n2are differentthen ?
(n1, n2) = 0;2. else ?
(n1, n2) =1 +?~I1,~I2,l(~I1)=l(~I2)?l(~I1)j=1?
(cn1(~I1j), cn2(~I2j))where~I1= ?h1, h2, h3, ..?
and~I2=?k1, k2, k3, ..?
are index sequences associated withthe ordered child sequences cn1of n1and cn2ofn2, respectively,~I1jand~I2jpoint to the j-th childin the corresponding sequence, and, again, l(?)
re-turns the sequence length, i.e.
the number of chil-dren.Furthermore, we add two decay factors: ?
forthe depth of the tree and ?
for the length of thechild subsequences with respect to the original se-quence, i.e.
we account for gaps.
It follows that?
(n1, n2) =?(?2+?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1?
(cn1(~I1j), cn2(~I2j))),(1)where d(~I1) =~I1l(~I1)?~I11and d(~I2) =~I2l(~I2)?~I21.
This way, we penalize both larger trees andchild subsequences with gaps.
Eq.
1 is more gen-eral than the ?
equation for STK.
Indeed, if weonly consider the contribution of the longest childsequence from node pairs that have the same chil-dren, we implement STK.3 Generative Model: StochasticConceptual Language Model (SCLM)The first step of our approach is to produce a listof SLU hypotheses using a Stochastic ConceptualLanguage Model.
This is the same described in(Raymond and Riccardi, 2007) with the only dif-ference that we train the language model using theSRILM toolkit (Stolcke, 2002) and we then con-vert it into a Stochastic Finite State Transducer(SFST).
Such method allows us to use a widegroup of language models, backed-off or inter-polated with many kind of smoothing techniques(Chen and Goodman, 1998).To exemplify our SCLM let us consider thefollowing input italian sentence taken from theLUNA corpus along with its English translation:Ho un problema col monitor.
(I have a problem with my screen).A possible semantic annotation is:null{ho} PROBLEM{un problema} HARD-WARE{col monitor},where PROBLEM and HARDWARE are twodomain concepts and null is the label used forwords not meaningful for the task.
To associateword sequences with concepts, we use begin1078(B) and inside (I) markers after each word of asequence, e.g.
:null{ho} PROBLEM-B{un} PROBLEM-I{problema} HARDWARE-B{col} HARD-WARE-I{monitor}This annotation is automatically performedby a model based on a combination of threetransducers:?SLU= ?W?
?W2C?
?SLM,where ?Wis the transducer representation of theinput sentence, ?W2Cis the transducer mappingwords to concepts and ?SLMis the StochasticConceptual Language Model trained with SRILMtoolkit and converted in FST.
The SCLM repre-sents joint probability of word and concept se-quences by using the joint probability:P (W,C) =k?i=1P (wi, ci|hi),where W = w1..wk, C = c1..ckand hi=wi?1ci?1..w1c1.4 Discriminative re-rankingOur discriminative re-ranking is based on SVMstrained with pairs of conceptually annotated sen-tences produced by the FST-based generativemodel described in the previous section.
An SVMlearn to classify which annotation has an error ratelower than the others so that it can be used to sortthe m-best annotations based on their correctness.While for SVMs details we remaind to the wideliterature available, for example (Vapnik, 1995) or(Shawe-Taylor and Cristianini, 2004), in this sec-tion we focus on hypotheses generation and on thekernels used to implement our re-ranking model.4.1 Generation of m-best concept labelingUsing the FST-based model described in Section3, we can generate the list of m best hypothesesranked by the joint probability of the StochasticConceptual Language Model (SCLM).
The Re-ranking model proposed in this paper re-rankssuch list.After an analysis of the m-best hypothesis list,we noticed that many times the first hypothesisranked by SCLM is not the most accurate, i.e.the error rate evaluated with its Levenshtein dis-tance from the manual annotation is not the low-est among the m hypotheses.
This means that re-ranking hypotheses could improve the SLU ac-curacy.
Intuitively, to achieve satisfactory re-sults, different features from those used by SCLMshould be considered to exploit in a different waythe information encoded in the training data.4.2 Structural features for re-rankingThe kernels described in previous sections pro-vide a powerful technology for exploiting featuresof structured data.
These kernels were originallydesigned for data annotated with syntactic parsetrees.
In Spoken Language Understanding the dataavailable are text sentences with their semanticannotation based on basic semantic constituents.This kind of data has a rather flat structure withrespect to syntactic parse trees.
Thus, to exploitthe power of kernels, a careful design of the struc-tures used to represent data must be carried out,where the goal is to build tree-like annotation fromthe semantic annotation.
For this purpose, wenote that the latter is made upon sentence chunks,which implicitly define syntactic structures as longas the annotation is consistent in the corpus.We took into account the characteristics of thepresented kernels and the structure of semantic an-notated data.
As a result we designed the treestructures shown in figures 2(a), 2(b) and 3 forSTK and PTK and sequential structures for SKdefined in the following (where all the structuresrefer to the same example presented in Section 3,i.e.
Ho un problema col monitor).
The structuresused with SK are:(SK1) NULL ho PROBLEM-B unPROBLEM-I problema HARDWARE-B colHARDWARE-I monitor(SK2) NULL ho PROBLEM B un PROB-LEM I problema HARDWARE B col HARD-WARE I monitor,For simplicity, from now on, the two structureswill be referred as SK1 and SK2 (String Kernel 1and 2).
They differer in the use of chunk mark-ers B and I.
In SK1, markers are part of the con-cept, thus they increase the number of semantictags in the data whereas in SK2 markers are putapart as separated words so that they can mark ef-fectively the beginning and the end of a concept,but for the same reason they can add noise in thesentence.
Notice that the order of words and con-cepts is meaningful since each word is precededby its corresponding concepts.The structures shown in Figure 2(a), 2(b) and 31079have been designed for STK and PTK.
They pro-vide trees with increasing structure complexity asdescribed in the following.The first structure (FLAT) is a simple treeproviding direct dependency between words andchunked concepts.
From it, STK and PTK can ex-tract relevant features (tree fragments).The second structure (MULTILEVEL) has onemore level of nodes and yields the same separationof concepts and markers shown in SK1.
Noticethat the same separation can be carried out puttingthe markers B and I as features at the same level ofthe words.
This would increase exponentially (inthe number of leaves) the number of subtrees takeninto account by the STK computation.
Since STKdoesn?t separate children, as described in Section2.3, the structure we chose is lighter but also morerigid.The third structure (FEATURES) is a morecomplex structure.
It allows to use a wide num-ber of features (like Word categories, POS tags,morpho-syntactic features), which are commonlyused in this kind of task.
As described above, theuse of features exponentially increases the num-ber of subtrees taken into account by kernel com-putations but they also increase the robustness ofthe model.
In this work we only used Word Cate-gories as features.
They are domain independent,e.g.
?Months?, ?Dates?, ?Number?
etc.
or POStags, which are useful to generalize target words.Note also that the features in common betweentwo trees must appear in the same child-position,hence we sort them based on their indices, e.g.
?F0?for words and ?F1?
for word categories.4.3 Re-ranking models using sequencesThe FST generates the m most likely concept an-notations.
These are used to build annotationpairs,?si, sj?, which are positive instances if sihas a lower concept annotation error than sj, withrespect to the manual annotation.
Thus, a trainedbinary classifier can decide if siis more accuratethan sj.
Each candidate annotation siis describedby a word sequence with its concept annotation.Considering the example in the previous section, apair of annotations?si, sj?could besi: NULL ho PROBLEM-B un PROBLEM-I problema HARDWARE-B col HARDWARE-Imonitorsj: NULL ho ACTION-B un ACTION-I prob-lema HARDWARE-B col HARDWARE-B moni-torwhere NULL, ACTION and HARDWARE arethe assigned concepts.
The second annotation isless accurate than the first since problema is erro-neously annotated as ACTION and ?col monitor?is split in two different concepts.Given the above data, the sequence kernelis used to evaluate the number of common n-grams between siand sj.
Since the string ker-nel skips some elements of the target sequences,the counted n-grams include: concept sequences,word sequences and any subsequence of wordsand concepts at any distance in the sentence.Such counts are used in our re-ranking functionas follows: let ekbe the pair?s1k, s2k?we evaluatethe kernel:KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (2)?
SK(s11, s22)?
SK(s21, s12)This schema, consisting in summing four differentkernels, has been already applied in (Collins andDuffy, 2002; Shen et al, 2003) for syntactic pars-ing re-ranking, where the basic kernel was a treekernel instead of SK.
It was also used also in (Shenet al, 2004) to re-rank different candidates of thesame hypothesis for machine translation.
Noticethat our goal is different from the one tackled insuch paper and, in general, it is more difficult: wetry to learn which is the best annotation of a giveninput sentence, while in (Shen et al, 2004), theylearn to distinguish between ?good?
and ?bad?translations of a sentence.
Even if our goal is moredifficult, our approach is very effective, as shownin (Dinarelli et al, 2009a).
It is more appropriatesince in parse re-ranking there is only one best hy-pothesis, while in machine translation a sentencecan have more than one correct translations.Additionally, in (Moschitti et al, 2006; Mos-chitti et al, 2008) a tree kernel was applied to se-mantic trees similar to the one introduced in thenext section to re-rank Semantic Role Labeling an-notations.4.4 Re-ranking models using treesSince the aim of concept annotation re-ranking isto exploit innovative and effective source of infor-mation, we can use, in addition to sequence ker-nels, the power of tree kernels to generate correla-tion between concepts and word structures.Figures 2(a), 2(b) and 3 describe the struc-tural association between the concept and the word1080(a) FLAT Tree (b) MULTILEVEL TreeFigure 2: Examples of structures used for STK and PTKFigure 3: The FEATURES semantic tree used for STK or PTKCorpus Train set Test setLUNA words concepts words conceptsDialogs 183 67Turns 1.019 373Tokens 8.512 2.887 2.888 984Vocab.
1.172 34 - -OOV rate - - 3.2% 0.1%Table 1: Statistics on the LUNA corpusCorpus Train set Test setMedia words concepts words conceptsTurns 12,922 3,518# of tokens 94,912 43,078 26,676 12,022Vocabulary 5,307 80 - -OOV rate - - 0.01% 0.0%Table 2: Statistics on the MEDIA corpuslevel.
This kind of trees allows us to engineer newkernels and consequently new features (Moschittiet al, 2008), e.g.
their subparts extracted by STKor PTK, like the tree fragments in figures 1(a) and1(b).
These can be used in SVMs to learn the clas-sification of words in concepts.More specifically, in our approach, we use treefragments to establish the order of correctnessbetween two alternative annotations.
Therefore,given two trees associated with two annotations, are-ranker based on tree kernel can be built in thesame way of the sequence-based kernel by substi-tuting SK in Eq.
2 with STK or PTK.
The majoradvantage of using trees is the hierarchical depen-dencies between its nodes, allowing for the use ofricher n-grams with back-off models.5 ExperimentsIn this section, we describe the corpora, parame-ters, models and results of our experiments on re-ranking for SLU.
Our baseline is constituted by theerror rate of systems solely based on either FSTor SVMs.
The re-ranking models are built on theFST output, which in turn is applied to both man-ual or automatic transcriptions.5.1 CorporaWe used two different speech corpora:The LUNA corpus, produced in the homony-mous European project, is the first Italian datasetof spontaneous speech on spoken dialogs.
It isbased on help-desk conversations in a domainof software/hardware repairing (Dinarelli et al,2009b).
The data is organized in transcriptionsand annotations of speech based on a new multi-level protocol.
Although data acquisition is still inprogress, 250 dialogs have been already acquiredwith a WOZ approach and other 180 Human-Human (HH) dialogs have been annotated.
In thiswork, we only use WOZ dialogs, whose statisticsare reported in Table 1.The corpus MEDIA was collected withinthe French project MEDIA-EVALDA (Bonneau-Maynard et al, 2005) for development and evalu-ation of spoken understanding models and linguis-tic studies.
The corpus is composed of 1257 di-alogs (from 250 different speakers) acquired witha Wizard of Oz (WOZ) approach in the contextof hotel room reservations and tourist information.1081Statistics on transcribed and conceptually anno-tated data are reported in Table 2.5.2 Experimental setupGiven the small size of LUNA corpus, we did notcarried out any parameterization thus we used de-fault or a priori parameters.
We experimented withLUNA and three different re-rankers obtained withthe combination of SVMs with STK, PTK and SK,described in Section 4.
The initial annotation to bere-ranked is the list of the ten best hypotheses out-put by an FST model.We point out that, on the large Media dataset theprocessing time is considerably high2so we couldnot run all the models.We trained all the SCLMs used in our experi-ments with the SRILM toolkit (Stolcke, 2002) andwe used an interpolated model for probability es-timation with the Kneser-Ney discount (Chen andGoodman, 1998).
We then converted the model inan FST again with SRILM toolkit.The model used to obtain the SVM baseline forconcept classification was trained using YamCHA(Kudo and Matsumoto, 2001).
As re-rankingmodels based on structure kernels and SVMs,we used the SVM-Light-TK toolkit (available atdisi.unitn.it/moschitti).
For ?
(see Section 3), cost-factor and trade-off parameters, we used, 0.4, 1and 1, respectively (i.e.
the default parameters).The number m of hypotheses was always set to 10.The CRF model we compare with wastrained with the CRF++ tool, available athttp://crfpp.sourceforge.net/.
The model is equiva-lent to the one described in (Hahn et al, 2008).
Asfeatures, we used word and morpho-syntactic cat-egories in a window of [-2, +2] with respect to thecurrent token, plus bigrams of concept tags (see(Hahn et al, 2008) and the CRF++ web site formore details).Such model is very effective for SLU.
In (Hahnet al, 2008), it is compared with other four models(Stochastic Finite State Transducers, Support Vec-tor Machines, Machine Translation, Positional-Based Log-linear model) and it is by far the beston MEDIA.
Additionally, in (Raymond and Ric-cardi, 2007), a similar CRF model was comparedwith FST and SVMs on ATIS and on a different2The number of parameters of the models and the numberof training approaches make the exhaustive experimentationvery expensive in terms of processing time, which would beroughly between 2 and 3 months of a typical workstation.Structure STK PTK SKFLAT 18.5 19.3 -MULTILEVEL 20.6 19.1 -FEATURES 19.9 18.4 -SK1 - - 16.2SK2 - - 18.5Table 3: CER of SVMs using STK, PTK and SKon LUNA (manual transcriptions).
The Baselines,FST and SVMs alone, show a CER of 23.2% and26.3%, respectively.Model MEDIA (CER) LUNA (CER)FST 13.7% 23.2%CRF 11.5% 20.4%SVM-RR (PTK) 12.1% 18.4%Table 4: Results of SLU experiments on MEDIAand LUNA test set (manual transcriptions).version of MEDIA, showing again to be very ef-fective.We ran SLU experiments on manual and auto-matic transcriptions.
The latter are produced bya speech recognizer with a WER of 41.0% and31.4% on the LUNA and the MEDIA test sets, re-spectively.5.3 Training approachesThe FST model generates the 10-best annotations,i.e.
the data used to train the re-ranker based onSVMs.
Different training approaches can be car-ried out based on the use of the data.
We dividedthe training set in two parts.
We train FSTs onpart 1 and generate the 10-best hypotheses usingpart 2, thus providing the first chunk of re-rankingdata.
Then, we re-apply these steps inverting part1 with part 2 to provide the second data chunk.Finally, we train the re-ranker on the merged data.For classification, we generate the 10-best hy-potheses of the whole test set using the FSTtrained on all training data.5.4 Re-ranking resultsIn Tables 3, 4 and 5 and Figures 4(a) and 4(b) wereport the results of our experiments, expressed interms of concept error rate (CER).
CER is a stan-dard measure based on the Levensthein alignmentof sentences and it is computed as the ratio be-tween inserted, deleted and confused concepts andthe number of concepts in the reference sentence.Table 3 shows the results on the LUNA cor-pus using the different training approaches, ker-nels and structures described in this paper.
The108215202530354045505560100 500 1000 2000 3000 4000 5000 6000CERTraining SentencesFST CRF RR(a) Learning Curve on MEDIA corpus using the RR modelbased on SVMs and STK152025303540455055100 300 500 700 1000CERTraining SentencesFST CRF RR(b) Learning Curve on LUNA corpus using the RR modelbased on SVMs and SKFigure 4: Learning curves on MEDIA and LUNA corpora using FST, CRF and RR on the FST hypothesesModel MEDIA (CER) LUNA (CER)FST 28.6% 42.7%CRF 24.0% 41.8%SVM-RR (PTK) 25.0% 38.9%Table 5: Results of SLU experiments on MEDIAand LUNA test set (automatic transcriptions witha WER 31.4% on MEDIA and 41% on LUNA)dash symbol means that the structure cannot beapplied to the corresponding kernel.
We note thatour re-rankers significantly improve our baselines,i.e.
23.2% CER for FST and 26.3% CER for SVMconcept classifiers.
For example, SVM re-rankerusing SK, in the best case, improves FST conceptclassifier of 23.2-16.2 = 7 points.Note also that the structures designed for treesyield quite different results depending on whichkernel is used.
We can see in Table 3 that thebest result using STK is obtained with the simpleststructure (FLAT), while with PTK the best resultis achieved with the most complex structure (FEA-TURES).
This is due to the fact that STK doesnot split the children of each node, as explained inSection 2.2, and so structures like MULTILEVELand FEATURES are too rigid and prevent the STKto be effective.For lack of space we do not report all the resultsusing different kernels and structures on MEDIA,but we underline that as MEDIA is a more com-plex task (34 concepts in LUNA, 80 in MEDIA),the more complex structures are more effective tocapture word-concept dependencies and the bestresults were obtained using the FEATURES tree.Table 4 shows the results of the SLU exper-iments on the MEDIA and LUNA test sets us-ing the manual transcriptions of spoken sentencesand a re-ranker based on PTK and the FEATURESstructure (already reported in the previous table).We used PTK since it is enough efficient to carryout the computation on the much larger Media cor-pus although as previously shown it is less accu-rate than SK.We note that on a big corpus like MEDIA, thebaseline models (FST and CRF) can be accuratelylearned thus less errors can be ?corrected?.
Asa consequence, our re-ranking approach does notimprove CRF but it still improves the FSTs base-line of 1.6% points (11.7% of relative improve-ment).The same behavior is reproduced for the SLUexperiments on automatic transcriptions, shown inTable 5.
We note that, on the LUNA corpus, CRFsare more accurate than FSTs (0.9% points), butthey are significantly improved by the re-rankingmodel (2.9% points), which also improves theFSTs baseline by 3.8% points.
On the MEDIAcorpus, the re-ranking model is again very accu-rate improving the FSTs baseline of 3.6% points(12.6% relative improvement) on attribute anno-tation, but the most accurate model is again CRF(1% points better than the re-ranking model).5.5 DiscussionThe different behavior of the re-ranking model inthe LUNA and MEDIA corpora is due partially tothe task complexity, but it is mainly due to the factthat CRFs have been deeply studied and experi-mented (see (Hahn et al, 2008)) on MEDIA.
ThusCRF parameters and features have been largelyoptimized.
We believe that the re-ranking modelcan be relevantly improved by carrying out param-eter optimization and new structural feature de-1083sign.Moreover, our re-ranking models achieve thehighest accuracy for automatic concept annota-tion when small data sets are available.
To showthis, we report in Figure 4(a) and 4(b) the learningcurves according to an increasing number of train-ing sentences on the MEDIA and LUNA corpora,respectively.
To draw the first plot, we used a re-ranker based on STK (and the FLAT tree), whichis less accurate than the other kernels but also themost efficient in terms of training time.
In the sec-ond plot, we report the re-ranker accuracy usingSK applied to SK1 structure.In these figures, the FST baseline performanceis compared with our re-ranking (RR) and a Con-ditional Random Field (CRF) model.
The abovecurves clearly shows that for small datasets ourRR model is better than CRF whereas when thedata increases, CRF accuracy approaches the oneof the RR.Regarding the use of kernels two main findingscan be derived:?
Kernels producing a high number of features,e.g.
SK, produce accuracy higher than ker-nels less rich in terms of features, i.e.
STK.
Inparticular STK is improved by 18.5-16.2=2.3points (Table 3).
This is an interesting re-sult since it shows that (a) a kernel producingmore features also produces better re-rankingmodels and (b) kernel methods give a remark-able help in feature design.?
Although the training data is small, the re-rankers based on kernels appear to be veryeffective.
This may also alleviate the burdenof annotating large amount of data.6 ConclusionsIn this paper, we propose discriminative re-ranking of concept annotation to jointly exploitgenerative and discriminative models.
We im-prove the FST-based generative approach, whichis a state-of-the-art model in LUNA, by 7 points,where the more limited availability of annotateddata leaves a larger room for improvement.
Ourre-ranking model also improves FST and CRF onMEDIA when small data sets are used.Kernel methods show that combinations of fea-ture vectors, sequence kernels and other structuralkernels, e.g.
on shallow or deep syntactic parsetrees, appear to be a promising future researchline3.
Finally, the experimentation with automaticspeech transcriptions revealed that to test the ro-bustness of our models to transcription errors.In the future we would like to extend this re-search by focusing on advanced shallow semanticapproaches such as predicate argument structures,e.g.
(Giuglea and Moschitti, 2004; Moschitti andCosmin, 2004; Moschitti et al, 2008).
Addition-ally, term similarity kernels, e.g.
(Basili et al,2005; Bloehdorn et al, 2006), will be likely im-prove our models, especially when combined syn-tactic and semantic kernels are used, i.e.
(Bloe-hdorn and Moschitti, 2007a; Bloehdorn and Mos-chitti, 2007b).ReferencesRoberto Basili, Alessandro Moschitti, andMaria Teresa Pazienza.
1999.
A text classifierbased on linguistic processing.
In Proceedingsof IJCAI 99, Machine Learning for InformationFiltering.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005.
Effective use of WordNet seman-tics via kernel-based learning.
In Proceedings ofCoNLL-2005, Ann Arbor, Michigan.Stephan Bloehdorn and Alessandro Moschitti.
2007a.Combined syntactic and semantic kernels for textclassification.
In Proceedings of ECIR 2007, Rome,Italy.Stephan Bloehdorn and Alessandro Moschitti.
2007b.Structure and semantics for expressive text kernels.In In proceedings of CIKM ?07.Stephan Bloehdorn, Roberto Basili, Marco Cammisa,and Alessandro Moschitti.
2006.
Semantic kernelsfor text classification based on topological measuresof feature similarity.
In Proceedings of ICDM 06,Hong Kong, 2006.H.
Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,and D. Mostefa.
2005.
Semantic annotation of thefrench media dialog corpus.
In Proceedings of In-terspeech2005, Lisbon, Portugal.N.
Cancedda, E. Gaussier, C. Goutte, and J. M. Ren-ders.
2003.
Word sequence kernels.
J. Mach.Learn.
Res., 3.S.
F. Chen and J. Goodman.
1998.
An empirical studyof smoothing techniques for language modeling.
InTechnical Report of Computer Science Group, Har-vard, USA.3A basic approach is the use of part-of-speech tags like forexample in text categorization (Basili et al, 1999) but giventhe high efficiency of modern syntactic parsers we can use thecomplete parse tree.1084M.
Collins and N. Duffy.
2002.
New Ranking Al-gorithms for Parsing and Tagging: Kernels overDiscrete structures, and the voted perceptron.
InACL02, pages 263?270.Aron Culotta and Jeffrey Sorensen.
2004.
DependencyTree Kernels for Relation Extraction.
In Proceed-ings of ACL?04.Chad Cumby and Dan Roth.
2003.
Kernel Methods forRelational Learning.
In Proceedings of ICML 2003.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2009a.
Re-ranking models for spoken lan-guage understanding.
In Proceedings of EACL2009,Athens, Greece.Marco Dinarelli, Silvia Quarteroni, Sara Tonelli,Alessandro Moschitti, and Giuseppe Riccardi.2009b.
Annotating spoken dialogs: from speechsegments to dialog acts and frame semantics.
InProceedings of SRSL 2009 Workshop of EACL,Athens, Greece.Ana-Maria Giuglea and Alessandro Moschitti.
2004.Knowledge Discovery using Framenet, Verbnet andPropbank.
In A. Meyers, editor, Workshop on On-tology and Knowledge Discovering at ECML 2004,Pisa, Italy.Stefan Hahn, Patrick Lehnen, Christian Raymond, andHermann Ney.
2008.
A comparison of variousmethods for concept tagging for spoken languageunderstanding.
In Proceedings of LREC, Mar-rakech, Morocco.T.
Kudo and Y. Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings ofNAACL2001, Pittsburg, USA.Taku Kudo and Yuji Matsumoto.
2003.
Fast meth-ods for kernel-based text analysis.
In Proceedingsof ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree fea-tures.
In Proceedings of ACL?05.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML2001, US.Huma Lodhi, John S. Taylor, Nello Cristianini, andChristopher J. C. H. Watkins.
2000.
Text classifi-cation using string kernels.
In NIPS.Alessandro Moschitti and Adrian Bejan Cosmin.
2004.A semantic kernel for predicate argument classifica-tion.
In CoNLL-2004, Boston, MA, USA.Alessandro Moschitti and Silvia Quarteroni.
2008.Kernels on linguistic structures for answer extrac-tion.
In Proceedings of ACL-08: HLT, Short Papers,Columbus, Ohio.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Semantic role labeling via tree kerneljoint inference.
In Proceedings of CoNLL-X, NewYork City.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploit-ing syntactic and shallow semantic kernels forquestion/answer classification.
In Proceedings ofACL?07, Prague, Czech Republic.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role label-ing.
Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006a.
Efficient ConvolutionKernels for Dependency and Constituent SyntacticTrees.
In Proceedings of ECML 2006, pages 318?329, Berlin, Germany.Alessandro Moschitti.
2006b.
Making Tree KernelsPractical for Natural Language Learning.
In Pro-ceedings of EACL2006.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InProceeding of CIKM ?08, NY, USA.C.
Raymond and G. Riccardi.
2007.
Generative anddiscriminative algorithms for spoken language un-derstanding.
In Proceedings of Interspeech2007,Antwerp,Belgium.J.
Shawe-Taylor and N. Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge Univer-sity Press.Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
2003.Using LTAG Based Features in Parse Reranking.
InProceedings of EMNLP?06.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InHLT-NAACL, pages 177?184.A.
Stolcke.
2002.
Srilm: an extensible language mod-eling toolkit.
In Proceedings of SLP2002, Denver,USA.Kristina Toutanova, Penka Markova, and ChristopherManning.
2004.
The Leaf Path Projection Viewof Parse Trees: Exploring String Kernels for HPSGParse Selection.
In Proceedings of EMNLP 2004.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.1085
