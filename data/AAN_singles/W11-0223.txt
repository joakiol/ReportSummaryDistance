Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 182?183,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsSelf-training and co-training in biomedical word sense disambiguationAntonio Jimeno-YepesNational Library of Medicine8600 Rockville PikeBethesda, 20894, MD, USAantonio.jimeno@gmail.comAlan R. AronsonNational Library of Medicine8600 Rockville PikeBethesda, 20894, MD, USAalan@nlm.nih.govAbstractWord sense disambiguation (WSD) is an inter-mediate task within information retrieval andinformation extraction, attempting to selectthe proper sense of ambiguous words.
Due tothe scarcity of training data, semi-supervisedlearning, which profits from seed annotatedexamples and a large set of unlabeled data,are worth researching.
We present preliminaryresults of two semi-supervised learning algo-rithms on biomedical word sense disambigua-tion.
Both methods add relevant unlabeled ex-amples to the training set, and optimal param-eters are similar for each ambiguous word.1 IntroductionWord sense disambiguation (WSD) is an interme-diate task within information retrieval and informa-tion extraction, attempting to select the proper senseof ambiguous words.
Supervised learning achievesbetter performance compared to other WSD ap-proaches (Jimeno-Yepes et al, 2011).
Manual anno-tation requires a large level of human effort whereasthere is a large quantity of unlabeled data.
Ourwork follows (Mihalcea, 2004) but is applied to thebiomedical domain; it relies on two semi-supervisedlearning algorithms.We have performed experiments of semi-supervised learning for word sense disambiguationin the biomedical domain.
In the following section,we present the evaluated algorithms.
Then, wepresent preliminary results for self-training andco-training, which show a modest improvementwith a common set-up of the algorithms for theevaluated ambiguous words.2 MethodsFor self-training we use the definition by (Clark etal., 2003): ?a tagger that is retrained on its ownlabeled cache on each round?.
The classifier istrained on the available training data which is thenused to label the unlabeled examples from whichthe ones with enough prediction confidence are se-lected and added to the training set.
The processis repeated for a number of predefined iterations.Co-training (Blum and Mitchell, 1998) uses severalclassifiers trained on independent views of the sameinstances.
These classifiers are then used to label theunlabeled set, and from this newly annotated dataset the annotations with higher prediction probabil-ity are selected.
These newly labeled examples areadded to the training set and the process is repeatedfor a number of iterations.
Both bootstrapping algo-rithms produce an enlarged training data set.Co-training requires two independent views onthe same data set.
As first view, we use the contextaround the ambiguous word.
As second view, weuse the MEDLINE MeSH indexing available fromPubMed which is obtained by human assignment ofMeSH heading based on their full-text articles.Methods are evaluated with the accuracy mea-sure on the MSH WSD set built automatically usingMeSH indexing from MEDLINE (Jimeno-Yepes etal., 2011) 1 in which senses are denoted by UMLSconcept identifiers.
To avoid any bias derived from1Available from: http://wsd.nlm.nih.gov/collaboration.shtml182the indexing of the UMLS concept related to the am-biguous word, the concept has been removed fromthe MeSH indexing of the recovered citations.10-fold cross validation using Na?
?ve Bayes (NB)has been used to compare both views which achievesimilar accuracy (0.9386 context text, 0.9317 MeSHindexing) while the combined view achieves evenbetter accuracy (0.9491).In both algorithms a set of parameters is used: thenumber of iterations (1-10), the size of the pool ofunlabeled examples (100, 500, 1000) and the growthrate or number of unlabeled examples which are se-lected to be added to the training set (1, 10, 20, 50,100).3 Results and discussionResults shown in Table 1 have been obtained from21 ambiguous words which achieved lower perfor-mance in a preliminary cross-validation study.
Eachambiguous word has around 2 candidate senses with100 examples for each sense.
We have split the ex-amples for each ambiguous word into 2/3 for train-ing and 1/3 for test.The baseline is NB trained and tested using thissplit.
Semi-supervised algorithms use this split, butthe training data is enlarged with selected unlabeledexamples.
Self-training and the baseline use thecombined views while co-training relies on two NBclassifiers, each trained on one view of the train-ing data.
Even though we are willing to evalu-ate other classifiers, NB was selected for this ex-ploratory work since it is fast and space efficient.Unlabeled examples are MEDLINE citations whichcontain the ambiguous word and MeSH headingterms.
Any mention of MeSH heading related to theambiguous word has been removed.
Optimal param-eters were selected, and average accuracy is shownin Table 1.Method AccuracyBaseline 0.8594Self-training 0.8763 (1.93%)Co-training 0.8759 (1.88%)Table 1: Accuracy for the baseline, self-training and co-trainingBoth semi-supervised algorithms show a modestimprovement on the baseline which is a bit higherfor self-training.
Best results are achieved with asmall number of iterations (< 5), a small growthrate (1-10) and a pool of unlabeled data over 100 in-stances.
Noise affects the performance with a largernumber of iterations, which after an initial increase,shows a steep decrease in accuracy.
Small growthrate ensures a smoothed increase in accuracy.
Alarger growth rate adds more noise after each iter-ation.
A larger pool of unlabeled data offers a largerset of candidate unlabeled examples to choose fromat a higher computational cost.4 Conclusions and Future workPreliminary results show a modest improvement onthe baseline classifier.
This means that the semi-supervised algorithms have identified relevant dis-ambiguated instances to be added to the training set.We plan to evaluate the performance of these al-gorithms on all the ambiguous words available in theMSH WSD set.
In addition, since the results haveshown that performance decreases rapidly after fewiterations, we would like to further explore smooth-ing techniques applied to bootstrapping algorithmsand the effect on classifiers other than NB.AcknowledgmentsThis work was supported by the Intramural ResearchProgram of the NIH, National Library of Medicine,administered by ORISE.ReferencesA.
Blum and T. Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In Proceedings of theeleventh annual conference on Computational learn-ing theory, pages 92?100.
ACM.S.
Clark, J.R. Curran, and M. Osborne.
2003.
Bootstrap-ping POS taggers using unlabelled data.
In Proceed-ings of the seventh conference on Natural languagelearning at HLT-NAACL 2003-Volume 4, pages 49?55.Association for Computational Linguistics.A.
Jimeno-Yepes, B.T.
McInnes, and A.R.
Aronson.2011.
Exploiting MeSH indexing in MEDLINEto generate a data set for word sense disambigua-tion(accepted).
BMC bioinformatics.R.
Mihalcea.
2004.
Co-training and self-trainingfor word sense disambiguation.
In Proceedings ofthe Conference on Computational Natural LanguageLearning (CoNLL-2004).183
