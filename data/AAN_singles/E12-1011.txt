Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 99?108,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCan Click Patterns across User?s Query Logs Predict Answers toDefinition Questions?Alejandro FigueroaYahoo!
Research Latin AmericaBlanco Encalada 2120, Santiago, Chileafiguero@yahoo-inc.comAbstractIn this paper, we examined click patternsproduced by users of Yahoo!
search enginewhen prompting definition questions.
Reg-ularities across these click patterns are thenutilized for constructing a large and hetero-geneous training corpus for answer rank-ing.
In a nutshell, answers are extractedfrom clicked web-snippets originating fromany class of web-site, including KnowledgeBases (KBs).
On the other hand, non-answers are acquired from redundant piecesof text across web-snippets.The effectiveness of this corpus was as-sessed via training two state-of-the-artmodels, wherewith answers to unseenqueries were distinguished.
These test-ing queries were also submitted by searchengine users, and their answer candidateswere taken from their respective returnedweb-snippets.
This corpus helped bothtechniques to finish with an accuracy higherthan 70%, and to predict over 85% of theanswers clicked by users.
In particular, ourresults underline the importance of non-KBtraining data.1 IntroductionIt is a well-known fact that definition queries arevery popular across users of commercial searchengines (Rose and Levinson, 2004).
The essen-tial characteristic of definition questions is theiraim for discovering as much as possible descrip-tive information about the concept being defined(a.k.a.
definiendum, pl.
definienda).
Some exam-ples of this kind of query include ?Who is Ben-jamin Millepied??
and ?Tell me about Bank ofAmerica?.It is a standard practice of definition ques-tion answering (QA) systems to mine KBs (e.g.,online encyclopedias and dictionaries) for reli-able descriptive information on the definiendum(Sacaleanu et al 2008).
Normally, these pieces ofinformation (i.e., nuggets) explain different facetsof the definiendum (e.g., ?ballet choreographer?and ?born in Bordeaux?
), and the main idea con-sists in projecting the acquired nuggets into theset of answer candidates afterwards.
However,the performance of this category of method fallsinto sharp decline whenever few or no coverageis found across KBs (Zhang et al 2005; Han etal., 2006).
Put differently, this technique usuallysucceeds in discovering the most relevant factsabout the most promiment sense of the definien-dum.
But it often misses many pertinent nuggets,especially those that can be paraphrased in severalways; and/or those regarding ancillary senses ofthe definiendum, which are hardly found in KBs.As a means of dealing with this, current strate-gies try to construct general definition modelsinferred from a collection of definitions com-ing from the Internet or KBs (Androutsopoulosand Galanis, 2005; Xu et al 2005; Han et al2006).
To a great extent, models exploiting non-KB sources demand considerable annotation ef-forts, or when the data is obtained automatically,they benefit from empirical thresholds that ensurea certain degree of similarity to an array of KBarticles.
These thesholds attempt to trade-off thecleanness of the training material against its cov-erage.
Moreover, gathering negative samples isalso hard as it is not easy to find wide-coverageauthoritative sources of non-descriptive informa-tion about a particular definiendum.Our approach has different innovative aspects99compared to other research in the area of defini-tion extraction.
It is at the crossroads of querylog analysis and QA systems.
We study the clickbehavior of search engines?
users with regard todefinition questions.
Based on this study, we pro-pose a novel way of acquiring large-scale and het-erogeneous training material for this task, whichconsists of:?
automatically obtaining positive samples inaccordance with click patterns of search en-gine users.
This aids in harvesting a hostof descriptions from non-KB sources in con-junction with descriptive information fromKBs.?
automatically acquiring negative data in con-sonance with redundancy patterns acrosssnippets displayed within search engine re-sults when processing definition queries.In brief, our experiments reveal that these pat-terns can be effectively exploited for devising ef-ficient models.Given the huge amount of amassed data, weadditionally contrast the performance of systemsbuilt on top of samples originated solely fromKB, non-KB, and both combined.
Our compar-ison corroborates that KBs yield massive trust-worthy descriptive knowledge, but they do notbear enough diversity to discriminate all answer-ing nuggets within any kind of text.
Essentially,our experiments unveil that non-KB data is richerand therefore it is useful for discovering more de-scriptive nuggets than KB material.
But its usagerelies on its cleanness and on a negative set.
Manypeople had these intuitions before, but to the bestof our knowledge, we provide the first empiricalconfirmation and quantification.The road-map of this paper is as follows: sec-tion 2 touches on related works; section 3 digsdeeper into click patterns for definition questions,subsequently section 4 explains our corpus con-struction strategy; section 5 describes our experi-ments, and section 6 draws final conclusions.2 Related WorkIn recent years, definition QA systems haveshown a trend towards the utilization of severaldiscriminant and statistical learning techniques(Androutsopoulos and Galanis, 2005; Chen et al2006; Han et al 2006; Fahmi and Bouma, 2006;Katz et al 2007; Westerhout, 2009; Navigli andVelardi, 2010).
Due to training, there is a press-ing necessity for large-scale authoritative sourcesof descriptive and non-descriptive nuggets.
In thesame manner, there is a growing importance ofstrategies capable of extracting trustworthy andnegative/positive samples from any type of text.Conventionally, these methods interpret descrip-tions as positive examples, whereas contexts pro-viding non-descriptive information as negative el-ements.
Four representative techniques are:?
centroid vector (Xu et al 2003; Cui etal., 2004) collects an array of articles aboutthe definiendum from a battery of pre-determined KBs.
These articles are thenused to learn a vector of word frequencies,wherewith answer candidates are rated af-terwards.
Sometimes web-snippets togetherwith a query reformulation method are ex-ploited instead of pre-defined KBs (Chen etal., 2006).?
(Androutsopoulos and Galanis, 2005) gath-ered articles from KBs to score 250-characters windows carrying the definien-dum.
These windows were taken fromthe Internet, and accordingly, highly sim-ilar windows were interpreted as positiveexamples, while highly dissimilar as nega-tive samples.
For this purpose, two thresh-olds are used, which ensure the trustwor-thiness of both sets.
However, they alsocause the sets to be less diverse as not alldefinienda are widely covered across KBs.Indeed, many facets outlined within the 250-characters windows will not be detected.?
(Xu et al 2005) manually labeled samplestaken from an Intranet.
Manual annotationsare constrained to a small amount of exam-ples, because it requires substantial humanefforts to tag a large corpus, and disagree-ments between annotators are not uncom-mon.?
(Figueroa and Atkinson, 2009) capitalizedon abstracts supplied by Wikipedia for build-ing language models (LMs), thus there wasno need for a negative set.Our contribution is a novel technique for ob-taining heterogeneous training material for defi-100nitional QA, that is to say, massive examples har-vested from KBs and non-KBs.
Fundamentally,positive examples are extracted from web snippetsgrounded on click patterns of users of a search en-gine, whereas the negative collection is acquiredvia redundancy patterns across web-snippets dis-played to the user by the search engine.
This datais capitalized by two state-of-the-art definition ex-tractors, which are different in nature.
In addition,our paper discusses the effect on the performanceof different sorts (KBs and non-KBs) and amountof training data.As for user clicks, they provide valuable rele-vance feedback for a variety of tasks, cf.
(Radlin-ski et al 2010).
For instance, (Ji et al 2009)extracted relevance information from clicked andnon-clicked documents within aggregated searchsessions.
They modelled sequences of clicks asa means of learning to globally rank the relativerelevance of all documents with respect to a givenquery.
(Xu et al 2010) improved the quality oftraining material for learning to rank approachesvia predicting labels using clickthrough data.
Inour work, we combine click patterns across Ya-hoo!
search query logs with QA techniques tobuild one-sided and two-sided classifiers for rec-ognizing answers to definition questions.3 User Click Analysis for Definition QAIn this section, we examine a collection of queriessubmitted to Yahoo!
search engine during the pe-riod from December 2010 to March 2011.
Morespecifically, for this analysis, we considered alog encompassing a random sample of 69,845,262(23,360,089 distinct) queries.
Basically, this logcomprises the query sent by the user in conjunc-tion with the displayed URLs and the informationabout the sequence of their clicks.In the first place, we associate each query witha category in the taxonomy proposed by (Roseand Levinson, 2004), and in this way definitionqueries are selected.
Secondly, we investigateuser click patterns observed across these filtereddefinition questions.3.1 Finding Definition QueriesAccording to (Broder, 2002; Lee et al 2005;Dupret and Piwowarski, 2008), the intention ofthe user falls into at least two categories: navi-gational (e.g., ?google?)
and informational (e.g.,?maximum entropy models?).
The former entailsthe desire of going to a specific site that the userhas in mind, and the latter regards the goal oflearning something by reading or viewing somecontent (Rose and Levinson, 2004).
Navigationalqueries are hence of less relevance to definitionquestions, and for this reason, these were removedin congruence with the next three criteria:?
(Lee et al 2005) pointed out that users willonly visit the web site they bear in mind,when prompting navigational queries.
Thus,these queries are characterized by clickingthe same URL almost all the time (Lee et al2005).
More precisely, we discarded queriesthat: a) appear more than four times in thequery log; and which at the same time b) itsmost clicked URL represents more than 98%of all its clicks.
Following the same idea, weadditionally eliminated prompted URLs andqueries where the clicked URL is of the form?www.search-query-without-spaces.??
By the same token, queries containing key-words such as ?homepage?, ?on-line?, and?sign in?
were also removed.?
After the previous steps, many navigationalqueries (e.g., ?facebook?)
still remained inthe query log.
We noticed that a substantialportion was signaled by several frequentlyand indistinctly clicked URLs.
Take forinstance ?facebook?
: ?www.facebook.com?and ?www.facebook.com/login.php?.With this in mind, we discarded entries em-bodied in a manually compiled black list.This list contains the 600 highest frequentcases.A third category in (Rose and Levinson, 2004)regards resource queries, which we distinguishedvia keywords like ?image?, ?lyrics?
and ?maps?.Altogether, an amount of (35.67%) 24,916,610(3,576,817 distinct) queries were seen as navi-gational and resource.
Note that in (Rose andLevinson, 2004) both classes encompassed be-tween 37%-38% of their query set.Subsequently, we profited from the remaining44,928,652 (informational) entries for detectingqueries where the intention of the user is find-ing descriptive information about a topic (i.e.,definiendum).
In the taxonomy delineated by101(Rose and Levinson, 2004), informational queriesare sub-categorized into five groups including list,locate, and definitional (directed and undirected).In practice, we filtered definition questions as fol-lows:1.
We exploited an array of expressions thatare commonly utilized in query analysis forclassifying definition questions (Figueroa,2010).
E.g., ?Who is/was...?, ?What is/wasa/an...?, ?define...?, and ?describe...?.
Over-all, these rules assisted in selecting 332,227entries.2.
As stated in (Dupret and Piwowarski, 2008),informational queries are typified by the userclicking several documents.
In light of that,we say that some definitional queries arecharacterized by multiple clicks, where atleast one belongs to a KB.
This aids in cap-turing the intention of the user when look-ing for descriptive knowledge and only en-tering noun phrases like ?thoracic outlet syn-drome?
:www.medicinenet.comen.wikipedia.orghealth.yahoo.netwww.livestrong.comhealth.yahoo.neten.wikipedia.orgwww.medicinenet.comwww.mayoclinic.comen.wikipedia.orgwww.nismat.orgen.wikipedia.orgTable 1: Four distinct sequences of hosts clicked byusers given the search query: ?thoracic outlet syn-drome?.In so doing, we manually compiled a listof 36 frequently clicked KB hosts (e.g.,Wikipedia and Britannica encyclopedia).This filter produced 567,986 queries.Unfortunately, since query logs stored bysearch engines are not publicly available due toprivacy and legal concerns, there is no accessibletraining material to build models on top of anno-tated data.
Thus, we exploited the aforementionedhand-crafted rules to connect queries to their re-spective category in this taxonomy.3.2 User Click PatternsIn substance, the first filter recognizes the inten-tion of the user by means of the formulation givenby the user (e.g., ?What is a/the/an...?).
With re-gard to this filter, some interesting observationsare as follows:?
In 40.27% of the entries, users did not visitany of the displayed web-sites.
Conse-quently, we concluded that the informationconveyed within the multiple snippets wasoften enough to answer the respective def-inition question.
In other words, a signifi-cant fraction of the users were satisfied witha small set of brief, but quickly generated de-scriptions.?
In 2.18% of these cases, the search engine re-turned no results, and a few times users triedanother paraphrase or query, due to uselessresults or misspellings.?
We also noticed that definition questionsmatched by these expressions are seldom re-lated to more than one click, although infor-mational queries produce several clicks, ingeneral.
In 46.44% of the cases, the userclicked a sole document, and more surpris-ingly, we observed that users are likely toclick sources different from KBs, in con-trast to the widespread belief in definitionQA research.
Users pick hits originatingfrom small but domain-specific web-sites asa result of at least two effects: a) they arelooking for minor or ancillary senses of thedefiniendum (e.g., ?ETA?
in ?www.travel-industry-dictionary.com?
); and more perti-nent b) the user does not trust the informationyielded by KBs and chooses more authorita-tive resources, for instance, when looking forreliable medical information (e.g., ?What ishypothyroidism?
?, and ?What is mrsa infec-tion??
).While the first filter infers the intention of theuser from the query itself, the second deduces itfrom the origin of the clicked documents.
Withregard to this second filter, clicking patterns aremore disperse.
Here, the first two clicks normallycorrespond to the top two/three ranked hits re-turned by the search engine, see also (Ji et al2009).
Also, sequences of clicks signal that users102normally visit only one site belonging to a KB,and at least one coming from a non-KB (see Ta-ble 1).All in all, the insight gained in this analysis al-lows the construction of an heterogeneous corpusfor definition question answering.
Put differently,these user click patterns offer a way to obtain hugeamounts of heterogeneous training material.
Inthis way the heavy dependence of open-domaindescription identifiers on KB data can be allevi-ated.4 Click-Based Corpus AcquisitionSince queries obtained by the previous two filtersare not associated with the actual snippets seenby the users (due to storage limitations), snip-pets were recovered by means of submitting thequeries to Yahoo!
search engine.After retrieval, we benefited from OpenNLP1for detecting sentence boundaries, tokenizationand part-of-speech (POS) information.
Here, weadditionally interpreted truncations (?.
.
.?)
as sen-tence delimiters.
POS tags were used to recognizeand replace numbers with a placeholder (#CD#)as a means of creating sentence templates.
Wemodified numbers as their value is just as of-ten confusing as useful (Baeza-Yates and Ribeiro-Neto, 1999).Along with numbers, sequences of fulland partial matches of the definiendum werealso substituted with placeholders, ?#Q#?
and?#QT#?, respectively.
To exemplify, considerthis pre-processed snippet regarding ?BenjaminMillepied?
from ?www.mashceleb.com?
:#Q# / News &amp; Biography - MashCelebLatest news coverage of #Q##Q# ( born #CD# ) is a principal dancerat New York City Ballet and a balletchoreographer...We benefit from these templates for buildingboth a positive and a negative training set.4.1 Negative SetThe negative set comprised templates appearingacross all (clicked and unclicked) web-snippets,which at the same time, are related to morethan five distinct queries.
We hypothesize thatthese prominent elements correspond to non-informative, and thus non-descriptive, content as1http://opennlp.sourceforge.netthey appear within snippets across several ques-tions.
In other words: ?If it seems to answer everyquestion, it will probably answer no question?.Take for instance:Information about #Q# in the ColumbiaEncyclopedia , Computer DesktopEncyclopedia , computing dictionaryConversely, templates that are more plausibleto be answers are strongly related to their specificdefinition questions, and consequently, they arelow in frequency and unlikely to be in the resultset of a large number of queries.
This negative setwas expanded with templates coming from titlesof snippets, which at the same time, have a fre-quency higher than four across all snippets (inde-pendent on which queries they appear).
This pro-cess cooperated on gathering 1,021,571 differentnegative examples.
In order to measure the pre-cision of this process, we randomly selected andchecked 1,000 elements, and we found an error of1.3%.4.2 Positive SetAs for the positive set, this was constructedonly from the summary section of web-snippetsclicked by the users.
We constrained these snip-pets to bear a title template associated with at leasttwo web-snippets clicked for two distinct queries.Some good examples are:What is #Q# ?
Choices and Consequences.Biology question : What is an #Q# ?Since clicks are linked with entire snippets,it is uncertain which sentences are genuine de-scriptions (see the previous example).
There-fore, we removed those templates already con-tained in the negative set, along with those sam-ples that matched an array of well-known hand-crafted rules.
This set included:a. sentences containing words such as ?ask?,?report?, ?say?, and ?unless?
(Kil et al2005; Schlaefer et al 2007);b. sentences bearing several named entities(Schlaefer et al 2006; Schlaefer et al2007), which were recognized by the numberof tokens starting with a capital letter versusthose starting with a lowercase letter;c. statements of persons (Schlaefer et al2007); and103d.
we also profited from about five hundredcommon expressions across web snippets in-cluding ?Picture of ?, and ?Jump to : naviga-tion , search?, as well as ?Recent posts?.This process assisted in acquiring 881,726 dif-ferent examples, where 673,548 came from KBs.Here, we also randomly selected 1,000 instancesand manually checked if they were actual descrip-tions.
The error of this set was 12.2%.To put things into perspective, in contrast toother corpus acquisition approaches, the presentmethod generated more than 1,800,000 positiveand negative training samples combined, whilethe open-domain strategy of (Miliaraki and An-droutsopoulos, 2004; Androutsopoulos and Gala-nis, 2005) ca.
20,000 examples, the close-domaintechnique of (Xu et al 2005) about 3,000 and(Fahmi and Bouma, 2006) ca.
2,000.5 Answering New Definition QueriesIn our experiments, we checked the effectivenessof our user click-based corpus acquisition tech-nique by studying its impact on two state-of-the-art systems.
The first one is based on the bi-termLMs proposed by (Chen et al 2006).
This sys-tem requires only positive samples as training ma-terial.
Conversely, our second system capitalizeson both positive and negative examples, and it isbased on the Maximum Entropy (ME) modelspresented by (Fahmi and Bouma, 2006).
TheseME2 models amalgamated bigrams and unigramsas well as two additional syntactic features, whichwere not applicable to our task (i.e, sentence posi-tion).
We added to this model the sentence lengthas a feature in order to homologate the attributesused by both systems, therefore offering a goodframework to assess the impact of our negativeset.
Note that (Fahmi and Bouma, 2006), unlikeus, applied their models only to sentences observ-ing some specific syntactic patterns.With regard to the test set, this was constructedby manually annotating 113,184 sentence tem-plates corresponding to 3,162 unseen definienda.In total, this array of unseen testing instancesencompassed 11,566 different positive samples.In order to build a balanced testing collection,the same number of negative examples were ran-domly selected.
Overall, our testing set contains2http://maxent.sourceforge.net/about.html23,132 elements, and some illustrative annota-tions are shown in Table 2.
It is worth highlight-ing that these examples signal that our modelsare considering pattern-free descriptions, that isto say, unlike other systems (Xu et al 2003; Katzet al 2004; Fernandes, 2004; Feng et al 2006;Figueroa and Atkinson, 2009; Westerhout, 2009)which consider definitions aligning an array ofwell-known patterns (e.g., ?is a?
and ?also knownas?
), our models disregard any class of syntacticconstraint.As to a baseline system, we accounted for thecentroid vector (Xu et al 2003; Cui et al 2004).When implementing, we followed the blueprintin (Chen et al 2006), and it was built for eachdefiniendum from a maximum of 330 web snip-pets fetched by means of Bing Search.
This base-line achieved a modest performance as it correctlyclassified 43.75% of the testing examples.
In de-tail, 47.75% out of the 56.25% of the misclas-sified elements were a result of data-sparseness.This baseline has been widely used as a startingpoint for comparison purposes, however it is hardfor this technique to discover diverse descriptivenuggets.
This problem stems from the narrow-coverage of the centroid vector learned for the re-spective definienda (Zhang et al 2005).
In short,these figures support the necessity for more robustmethods based on massive training material.Experiments.
We trained both models by sys-tematically increasing the size of the training ma-terial by 1%.
For this, we randomly split the train-ing data into 100 equally sized packs, and system-atically added one to the previously selected sets(i. e., 1%, 2%, 3%, .
.
., 99%, 100%).
We also ex-perimented with: 1) positive examples originatedsolely from KBs; 2) positive samples harvestedonly from non-KBs; and eventually 3) all positiveexamples combined.Figure 1 juxtaposes the outcomes accom-plished by both techniques under the differentconfigurations.
These figures, compared with re-sults obtained by the baseline, indicate the im-portant contribution of our corpus to tackle data-sparseness.
This contrast substantiates our claimthat click patterns can be utilized as indicators ofanswers to definition questions.
Since our modelsignore definition patterns, they have the potentialof detecting a wide diversity of descriptive infor-mation.Further, the improvement of about 9%-10% by104Label Example/Template+ Propylene #Q# is a type of alcohol made from fermented yeast and carbohydrates andis commonly used in a wide variety of products .+ #Q# is aggressive behavior intended to achieve a goal .+ In Hispanic culture , when a girl turns #CD# , a celebration is held called the #Q#,symbolizing the girl ?s passage to womanhood .+ Kirschwasser , German for ?
cherry water ?
and often shortened to #Q# in English-speakingcountries , is a colorless brandy made from black ...+ From the Gaelic ?dubhglas ?
meaning #Q#, #QT# stream , or from the #QT# river .+ Council Bluffs Orthopedic Surgeon Doctors physician directory - Read about #Q#, damageto any of the #CD# tendons that stabilize the shoulder joint .+ It also occurs naturally in our bodies in fact , an average size adult manufactures up to#CD# grams of #Q# daily during normal metabolism .- Sterling Silver #Q# Hoop Earrings Overstockjeweler.com- I know V is the rate of reaction and the #Q# is hal ...- As sad and mean as that sounds , there is some truth to it , as #QT# as age their bodies donot function as well as they used to ( in all respects ) so there is a ...- If you ?re new to the idea of Christian #Q#, what I call ?
the wild things of God ,- A look at the Biblical doctrine of the #QT# , showing the biblical basis for the teaching andincluding a discussion of some of the common objections .- #QT# is Users Choice ( application need to be run at #QT# , but is not system critical ) ,this page shows you how it affects your Windows operating system .- Your doctor may recommend that you use certain drugs to help you control your #Q# .- Find out what is the full meaning of #Q# on Abbreviations.com !Table 2: Samples of manual annotations (testing set).means of exploiting our negative set makes itspositive contribution clear.
In particular, this sup-ports our hypothesis that redundancy across web-snippets pertaining to several definition questionscan be exploited as negative evidence.
On thewhole, this enhancement also suggests that MEmodels are a better option than LMs.Furthermore, in the case of ME models, puttingtogether evidence from KB and non-KBs bet-ters the performance.
Conversely, in the case ofLMs, we do not observe a noticeable improve-ment when unifying both sources.
We attributethis difference to the fact that non-KB data is nois-ier, and thus negative examples are necessary tocushion this noise.
By and large, the outcomesshow that the usage of descriptive information de-rived exclusively from KBs is not the best, but acost-efficient solution.Incidentally, Figure 1 reveals that more trainingdata does not always imply better results.
Overall,the best performance (ME-combined?
80.72%)was reaped when considering solely 32% of thetraining material.
Hence, ME-KB finished withthe best performance when accounting for about215,500 positive examples (see Table 3).
Addingmore examples brought about a decline in accu-Best True PositiveConf.
of Accuracy positives examplesME-combined 80.72% 88% 881,726ME-KB 80.33% 89.37% 673,548ME-N-KB 78.99% 93.38% 208,178Table 3: Comparison of performance, the total amountand origin of training data, and the number of recog-nized descriptions.racy.
Nevertheless, this fraction (32%) is stilllarger than the data-sets considered by other open-domain Machine Learning approaches (Miliarakiand Androutsopoulos, 2004; Androutsopoulosand Galanis, 2005).In detail, when contrasting the confusion ma-trices of the best configurations accomplishedby ME-combined (80.72%), ME-KB (80.33%)and ME-N-KB (78.99%), one can find that ME-combined correctly identified 88% of the answers(true positives), while ME-KB 89.37% and ME-N-KB 93.38% (see Table 3).Interestingly enough, non-KB data only em-bodies 23.61% of all positive training material,but it still has the ability to recognize more an-swers.
Despite of that, the other two strate-gies outperform ME-N-KB, because they are able105Figure 1: Results for each configuration (accuracy).to correctly label more negative test examples.Given these figures, we can conclude that this isachieved by mitigating the impact of the noise inthe training corpus by means of cleaner (KB) data.We verified this synergy by inspecting the num-ber of answers from non-KBs detected by thethree top configurations in Table 3: ME-combined(9,086), ME-KB (9,230) and ME-N-KB (9,677).In like manner, we examined the confusion ma-trix for the best configuration (ME-combined ?80.72%): 1,388 (6%) positive examples were mis-labeled as negative, while 3,071 (13.28%) nega-tive samples were mistagged as positive.In addition, we performed significance tests uti-lizing two-tailed paired t-test at 95% confidenceinterval on twenty samples.
For this, we usedonly the top three configurations in Table 3 andeach sample was determined by using boostrap-ping resampling.
Each sample has the same sizeof the original test corpus.
Overall, the tests im-plied that all pairs were statistically different fromeach other.In summary, the results show that both negativeexamples and combining positive examples fromheterogeneous sources are indispensable to tackleany class of text.
However, it is vital to lessen thenoise in non-KB data, since this causes a moreadverse effect on the performance.
Given the up-perbound in accuracy, our outcomes indicate thatcleanness and quality are more important than thesize of the corpus.
Our figures additionally sug-gest that more effort should go into increasing di-versity than the number of training instances.
Inlight of these observations, we also conjecture thata more reduced, but diverse and manually anno-tated, corpus might be more effective.
In partic-ular, a manually checked corpus distilled by in-specting click patterns across query logs of searchengines.Lastly, in order to evaluate how good a clickpredictor the three top ME-configurations are,we focused our attention only on the manu-ally labeled positive samples (answers) that wereclicked by the users.
Overall, 86.33% (ME-combined), 88.85% (ME-KB) and 92.45% (ME-N-KB) of these responses were correctly pre-dicted.
In light of that, one can conclude that(clicked and non-clicked) answers to definitionquestions can be identified/predicted on the basisof user?s click patterns across query logs.From the viewpoint of search engines, websnippets are computed off-line, in general.
Inso doing, some methods select the spans of textbearing query terms with the potential of puttingthe document on top of the rank (Turpin et al2007; Tsegay et al 2009).
This helps to create anabridged version of the document that can quicklyproduce the snippet.
This has to do with the trade-off between storage capacity, indexing, and re-trieval speed.
Ergo, our technique can help to de-106termine whether or not a span of text is worth ex-panding, or in some cases whether or not it shouldbe included in the snippet view of the document.In our instructive snippet, we now might have:Benjamin Millepied / News &amp;Biography - MashCelebBenjamin Millepied (born 1977) is aprincipal dancer at New York City Balletand a ballet choreographer ofinternational reputation.
Millepied wasborn in Bordeaux, France.
His...Improving the results of informational (e.g.,definition) queries, especially of less frequentones, is key for competing commercial searchengines as they are embodied in the non-navigational tail where these engines differ themost (Zaragoza et al 2010).6 ConclusionsThis work investigates into the click behavior ofcommercial search engine users regarding defi-nition questions.
These behaviour patterns arethen exploited as a corpus acquisition techniquefor definition QA, which offers the advantage ofencompassing positive samples from heterogo-neous sources.
In contrast, negative examplesare obtained in conformity to redundancy pat-terns across snippets, which are returned by thesearch engine when processing several definitionqueries.
The effectiveness of these patterns, andhence of the obtained corpus, was tested by meansof two models different in nature, where bothwere capable of achieving an accuracy higher than70%.As a future work, we envision that answers de-tected by our strategy can aid in determining somequery expansion terms, and thus to devise somerelevance feedback methods that can bring aboutan improvement in terms of the recall of answers.Along the same lines, it can cooperate on the vi-sualization of the results by highlighting and/orextending truncated answers, that is more infor-mative snippets, which is one of the holy grail ofsearch operators, especially when processing in-formational queries.NLP tools (e.g., parsers and name entity recog-nizers) can also be exploited for designing bettertraining data filters and more discriminative fea-tures for our models that can assist in enhanc-ing the performance, cf.
(Surdeanu et al 2008;Figueroa, 2010; Surdeanu et al 2011).
However,this implies that these tools have to be re-trainedto cope with web-snippets.AcknowledgementsThis work was partially supported by R&Dproject FONDEF D09I1185.
We also thank ourreviewers for their interesting comments, whichhelped us to make this work better.ReferencesI.
Androutsopoulos and D. Galanis.
2005.
A prac-tically Unsupervised Learning Method to IdentifySingle-Snippet Answers to Definition Questions onthe web.
In HLT/EMNLP, pages 323?330.R.
Baeza-Yates and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
Addison Wesley.A.
Broder.
2002.
A Taxonomy of Web Search.
SIGIRForum, 36:3?10, September.Y.
Chen, M. Zhon, and S. Wang.
2006.
Reranking An-swers for Definitional QA Using Language Model-ing.
In Coling/ACL-2006, pages 1081?1088.H.
Cui, K. Li, R. Sun, T.-S. Chua, and M.-Y.
Kan.2004.
National University of Singapore at theTREC 13 Question Answering Main Task.
In Pro-ceedings of TREC 2004.
NIST.Georges E. Dupret and Benjamin Piwowarski.
2008.A user browsing model to predict search engineclick data from past observations.
In SIGIR ?08,pages 331?338.Ismail Fahmi and Gosse Bouma.
2006.
Learning toIdentify Definitions using Syntactic Features.
InProceedings of the Workshop on Learning Struc-tured Information in Natural Language Applica-tions.Donghui Feng, Deepak Ravichandran, and Eduard H.Hovy.
2006.
Mining and Re-ranking for AnsweringBiographical Queries on the Web.
In AAAI.Aaron Fernandes.
2004.
Answering DefinitionalQuestions before they are Asked.
Master?s thesis,Massachusetts Institute of Technology.A.
Figueroa and J. Atkinson.
2009.
Using Depen-dency Paths For Answering Definition Questions onThe Web.
In WEBIST 2009, pages 643?650.Alejandro Figueroa.
2010.
Finding Answers to Defini-tion Questions on the Web.
Phd-thesis, Universitaetdes Saarlandes, 7.K.
Han, Y.
Song, and H. Rim.
2006.
Probabilis-tic Model for Definitional Question Answering.
InProceedings of SIGIR 2006, pages 212?219.Shihao Ji, Ke Zhou, Ciya Liao, Zhaohui Zheng, Gui-Rong Xue, Olivier Chapelle, Gordon Sun, andHongyuan Zha.
2009.
Global ranking by exploit-ing user clicks.
In Proceedings of the 32nd inter-national ACM SIGIR conference on Research and107development in information retrieval, SIGIR ?09,pages 35?42, New York, NY, USA.
ACM.B.
Katz, M. Bilotti, S. Felshin, A. Fernandes,W.
Hildebrandt, R. Katzir, J. Lin, D. Loreto,G.
Marton, F. Mora, and O. Uzuner.
2004.
An-swering multiple questions on a topic from hetero-geneous resources.
In Proceedings of TREC 2004.NIST.B.
Katz, S. Felshin, G. Marton, F. Mora, Y. K. Shen,G.
Zaccak, A. Ammar, E. Eisner, A. Turgut, andL.
Brown Westrick.
2007.
CSAIL at TREC 2007Question Answering.
In Proceedings of TREC2007.
NIST.Jae Hong Kil, Levon Lloyd, and Steven Skiena.
2005.Question Answering with Lydia (TREC 2005 QAtrack).
In Proceedings of TREC 2005.
NIST.U.
Lee, Z. Liu, and J. Cho.
2005.
Automatic Iden-tification of User Goals in Web Search.
In Pro-ceedings of the 14th WWW conference, WWW ?05,pages 391?400.S.
Miliaraki and I. Androutsopoulos.
2004.
Learn-ing to identify single-snippet answers to definitionquestions.
In COLING ?04, pages 1360?1366.Roberto Navigli and Paola Velardi.
2010.LearningWord-Class Lattices for Definitionand Hypernym Extraction.
In Proceedings ofthe 48th Annual Meeting of the Association forComputational Linguistics (ACL 2010).Filip Radlinski, Martin Szummer, and Nick Craswell.2010.
Inferring query intent from reformulationsand clicks.
In Proceedings of the 19th internationalconference on World wide web, WWW ?10, pages1171?1172, New York, NY, USA.
ACM.Daniel E. Rose and Danny Levinson.
2004.
Un-derstanding User Goals in Web Search.
In WWW,pages 13?19.B.
Sacaleanu, G. Neumann, and C. Spurk.
2008.DFKI-LT at QA@CLEF 2008.
In In Working Notesfor the CLEF 2008 Workshop.Nico Schlaefer, P. Gieselmann, and Guido Sautter.2006.
The Ephyra QA System at TREC 2006.
InProceedings of TREC 2006.
NIST.Nico Schlaefer, Jeongwoo Ko, Justin Betteridge,Guido Sautter, Manas Pathak, and Eric Nyberg.2007.
Semantic Extensions of the Ephyra QA Sys-tem for TREC 2007.
In Proceedings of TREC 2007.NIST.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to Rank Answers onLarge Online QA Collections.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics (ACL 2008), pages 719?727.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37:351?383.Yohannes Tsegay, Simon J. Puglisi, Andrew Turpin,and Justin Zobel.
2009.
Document compactionfor efficient query biased snippet generation.
InProceedings of the 31th European Conference onIR Research on Advances in Information Retrieval,ECIR ?09, pages 509?520, Berlin, Heidelberg.Springer-Verlag.Andrew Turpin, Yohannes Tsegay, David Hawking,and Hugh E. Williams.
2007.
Fast generation ofresult snippets in web search.
In Proceedings ofthe 30th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?07, pages 127?134, New York,NY, USA.
ACM.Eline Westerhout.
2009.
Extraction of definitions us-ing grammar-enhanced machine learning.
In Pro-ceedings of the EACL 2009 Student Research Work-shop, pages 88?96.Jinxi Xu, Ana Licuanan, and Ralph Weischedel.
2003.TREC2003 QA at BBN: Answering DefinitionalQuestions.
In Proceedings of TREC 2003, pages98?106.
NIST.J.
Xu, Y. Cao, H. Li, and M. Zhao.
2005.
RankingDefinitions with Supervised Learning Methods.
InWWW2005, pages 811?819.Jingfang Xu, Chuanliang Chen, Gu Xu, Hang Li, andElbio Renato Torres Abib.
2010.
Improving qual-ity of training data for learning to rank using click-through data.
In Proceedings of the third ACM in-ternational conference on Web search and data min-ing, WSDM ?10, pages 171?180, New York, NY,USA.
ACM.H.
Zaragoza, B. Barla Cambazoglu, and R. Baeza-Yates.
2010.
We Search Solved?
All Result Rank-ings the Same?
In Proceedings of CKIM?10, pages529?538.Zhushuo Zhang, Yaqian Zhou, Xuanjing Huang, andLide Wu.
2005.
Answering Definition QuestionsUsing Web Knowledge Bases.
In Proceedings ofIJCNLP 2005, pages 498?506.108
