Workshop on Monolingual Text-To-Text Generation, pages 64?73,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 64?73,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsFramework for Abstractive Summarization using Text-to-Text GenerationPierre-Etienne Genest, Guy LapalmeRALI-DIROUniversite?
de Montre?alP.O.
Box 6128, Succ.
Centre-VilleMontre?al, Que?becCanada, H3C 3J7{genestpe,lapalme}@iro.umontreal.caAbstractWe propose a new, ambitious framework forabstractive summarization, which aims at se-lecting the content of a summary not from sen-tences, but from an abstract representation ofthe source documents.
This abstract repre-sentation relies on the concept of InformationItems (INIT), which we define as the smallestelement of coherent information in a text or asentence.
Our framework differs from previ-ous abstractive summarization models in re-quiring a semantic analysis of the text.
Wepresent a first attempt made at developing asystem from this framework, along with eval-uation results for it from TAC 2010.
We alsopresent related work, both from within andoutside of the automatic summarization do-main.1 IntroductionSummarization approaches can generally be cate-gorized as extractive or abstractive (Mani, 2001).Most systems developped for the main internationalconference on text summarization, the Text Analy-sis Conference (TAC) (Owczarzak and Dang, 2010),predominantly use sentence extraction, including allthe top-ranked systems, which make only minorpost-editing of extracted sentences (Conroy et al,2010) (Gillick et al, 2009) (Genest et al, 2008)(Chen et al, 2008).Abstractive methods require a deeper analysis ofthe text and the ability to generate new sentences,which provide an obvious advantage in improvingthe focus of a summary, reducing its redundancyand keeping a good compression rate.
Accordingto a recent study (Genest et al, 2009b), there is anempirical limit intrinsic to pure extraction, as com-pared to abstraction.
For these reasons, as well as forthe technical and theoretical challenges involved, wewere motivated to come up with an abstractive sum-marization model.Recent abstractive approaches, such as sentencecompression (Knight and Marcu, 2000) (Cohn andLapata, 2009) and sentence fusion (Barzilay andMcKeown, 2005) or revision (Tanaka et al, 2009)have focused on rewriting techniques, without con-sideration for a complete model which would in-clude a transition to an abstract representation forcontent selection.
We believe that a ?fully abstrac-tive?
approach requires a separate process for theanalysis of the text that serves as an intermediatestep before the generation of sentences.
This way,content selection can be applied to an abstract repre-sentation rather than to original sentences or gener-ated sentences.We propose the concept of Information Items(INIT) to help define the abstract representation.
AnINIT is the smallest element of coherent informa-tion in a text or a sentence.
It can be something assimple as some entity?s property or as complex as awhole description of an event or action.
We believethat such a representation could eventually allow fordirectly answering queries or guided topic aspects,by generating sentences targeted to address specificinformation needs.Figure 1 compares the workflow of our approachwith other possibilities.
Extractive summarizationconsists of selecting sentences directly from the64SourceDocumentsSummaryInformation Items(       )ShortSentencesSummaryItemsCompressedSentencesThemesInIt SelectionSentenceSelectionCompressionInItRetrievalGenerationSentence Compression Sentence FusionAbstractiveSummarizationExtractiveSummarizationFusionN TN TInItN TFigure 1: Workflow diagram of our suggested approach for abstractive summarization, compared to pure extractivesummarization, sentence compression, and sentence fusion for summarization.
The dashed line represents the simpli-fied framework used in our first attempt at abstractive summarization (see section 2.4).source documents and generating a summary fromthem.
Sentence compression first compresses thesentences and chooses from those and the sourcedocuments?
sentences to form a summary; it mayalso be completed in the reverse order, which isto select sentences from the source documents andthen compress them for the summary.
Sentencefusion first identifies themes (clusters of similarsentences) from the source documents and selectswhich themes are important for the summary (a pro-cess similar to the sentence selection of centroid-based extractive summarization methods (Radev etal., 2004)) and then generates a representative sen-tence for each theme by sentence fusion.Our proposed abstractive summarization ap-proach is fundamentally different because the selec-tion of content is on Information Items rather than onsentences.
The text-to-text generation aspect is alsochanged.
Instead of purely going from whole sen-tences to generated sentences directly, there is nowa text planning phase that occurs at the conceptuallevel, like in Natural Language Generation (NLG).This approach has the advantage of generatingtypically short, information-focused sentences toproduce a coherent, information rich, and less re-dundant summary.
However, the difficulties aregreat: it is difficult for a machine to properly extractinformation from sentences at an abstract level, andtext generated from noisy data will often be flawed.Generating sentences that do not all sound similarand generic is an additional challenge that we havefor now circumvented by re-using the original sen-65tence structure to a large extent, which is a type oftext-to-text generation.
Even considering those diffi-culties, we believe that efforts in abstractive summa-rization constitute the future of summarization re-search, and thus that it is worthwhile to work to-wards that end.In this paper, we present our new abstractive sum-marization framework in section 2.
Section 3 de-scribes and analyses our first attempt at using thisframework, for the TAC 2010 multi-document newssummarization task, followed by the competition?sresults in section 4.
In this first attempt, we simpli-fied the framework of section 2 to obtain early re-sults which can help us as we move forward in thisproject.
Related work is discussed in section 5, andwe conclude in section 6.2 Abstractive Summarization FrameworkOur proposed framework for fully abstractive sum-marization is illustrated in figure 1.
This section dis-cusses how each step could be accomplished.2.1 INIT RetrievalAn Information Item is the smallest element of co-herent information in a text or a sentence.
This in-tentionally vague definition leaves the implementa-tion details to be decided based on resources avail-able.
The goal is to identify all entities in the text,their properties, predicates between them, and char-acteristics of the predicates.
This seemingly un-reachable goal, equivalent to machine reading, canbe limited to the extent that we only need INITs tobe precise and accurate enough to generate a sum-mary from them.The implementation of INITs is critical, as every-thing will depend on the abstract information avail-able.
Semantic Role Labeling (SRL) and predicate-logic analysis of text are two potential candidates fordeveloping INIT Retrieval.
Word-sense disambigua-tion, co-reference resolution and an analysis of wordsimilarity seem important as well to complement thesemantic analysis of the text.2.2 INIT SelectionGiven an analysis of the source documents that leadsto a list of INITs, we may now proceed to selectcontent for the summary.
Frequency-based mod-els, such as those used for extractive summarization,could be applied to INIT selection instead of sen-tence selection.
This would result in favoring themost frequently occurring entities, predicates, andproperties.INIT selection could also easily be applied totasks such as query-driven or guided summariza-tion, in which the user information need is knownand the summarization system attempts to address it.With smaller building blocks (INITs rather than sen-tences), it would be much easier to tailor summariesso that they include only relevant information.2.3 GenerationPlanning, summary planning in our case, providesthe structure of the generated text.
Most INITs do notlead to full sentences, and need to be combined intoa sentence structure before being realized as text.Global decisions of the INIT selection step now leadto local decisions as to how to present the informa-tion to the reader, and in what order.Text generation patterns can be used, based onsome knowledge about the topic or the informationneeds of the user.
One could use heuristic rules withdifferent priority levels or pre-generated summaryscenarios, to help decide how to structure sentencesand order the summary.
We believe that machinelearning could be used to learn good summary struc-tures as well.Once the detailed planning is completed, the sum-mary is realized with coherent syntax and punctu-ation.
This phase may involve text-to-text genera-tion, since the source documents?
sentences providea good starting point to generate sentences with var-ied and complex structures.
The work of (Barzilayand McKeown, 2005) on sentence fusion shows anexample of re-using the same syntactical structure ofa source sentence to create a new one with a slightlydifferent meaning.2.4 First Attempt at AbstractiveSummarizationThe three-step plan that we laid down is very hard,and instead of tackling it head on, we decided to fo-cus on certain aspects of it for now.
We followeda simplified version of our framework, illustratedby the dashed line in Figure 1.
It defers the con-tent selection step to the selection of generated shortsentences, rather than actually doing it abstractly as66Original Sentence The Cypriot airliner that crashed in Greece may have suffered a sudden loss of cabinpressure at high altitude, causing temperatures and oxygen levels to plummet and leaving everyoneaboard suffocating and freezing to death, experts said Monday.Information Items1.
airliner ?
crash ?
null (Greece, August 15, 2005)2. airliner ?
suffer ?
loss (Greece, August 15, 2005)3. loss ?
cause ?
null (Greece, August 15, 2005)4. loss ?
leave ?
null (Greece, August 15, 2005)Generated Sentences1.
A Cypriot airliner crashed.2.
A Cypriot airliner may have suffered a sudden loss of cabin pressure at high altitude.3.
A sudden loss of cabin pressure at high altitude caused temperatures and oxygen levels to plum-met.4.
A sudden loss of cabin pressure at high altitude left everyone aboard suffocating and freezing todeath.Selected Generated Sentence as it appears in the summary1.
On August 15, 2005, a Cypriot airliner crashed in Greece.Original Sentence At least 25 bears died in the greater Yellowstone area last year, including eight breeding-age females killed by people.Information Items1.
bear ?
die ?
null (greater Yellowstone area, last year)2. person ?
kill ?
female (greater Yellowstone area, last year)Generated Sentences1.
25 bears died.2.
Some people killed eight breeding-age females.Selected Generated Sentence as it appears in the summary1.
Last year, 25 bears died in greater Yellowstone area.Figure 2: Two example sentences and their processing by our 2010 system.
In the summary, the date and locationassociated with an INIT are added to its generated sentence.planned.
The summary planning has to occur aftergeneration and selection, in a Summary Generationstep not shown explicitly on the workflow.We have restricted our implementation of INITs todated and located subject?verb?object(SVO) triples,thus relying purely on syntactical knowledge, ratherthan including the semantics required for our frame-work.
Dates and locations receive a special treat-ment because we were interested in news summa-rization for this first attempt, and news articles arefactual and give a lot of importance to date and lo-cation.We did not try to combine more than one INIT inthe same sentence, relying instead on short, to-the-67point sentences, with one INIT each.
Figure 2 showstwo examples of sentences that were generated froma source document sentence using the simplified ab-stractive summarization framework.At first glance, the simplified version of our ap-proach for generating sentences may seem similarto sentence compression.
However, it differs inthree important ways from the definition of the taskof compression usually cited (Knight and Marcu,2000):?
Our generated sentences intend to cover onlyone item of information and not all the impor-tant information of the original sentence.?
An input sentence may have several generatedsentences associated to it, one for each of itsINITs, where it normally has only one com-pressed sentence.?
Generated sentences sometimes include wordsthat do not appear in the original sentence (like?some?
in the second example), whereas sen-tence compression is usually limited to worddeletion.3 Abstractive Summarization at TAC 2010Our first attempt at full abstractive summarizationtook place in the context of the TAC 2010 multi-document news summarization task.
This sectiondescribes briefly each module of our system, while(Genest and Lapalme, 2010) provides the implemen-tation details.3.1 INIT RetrievalAn INIT is defined as a dated and located subject?verb?object triple, relying mostly on syntacticalanalyses from the MINIPAR parser (Lin, 1998) andlinguistic annotations from the GATE informationextraction engine (Cunningham et al, 2002).Every verb encountered forms the basis of a can-didate INIT.
The verb?s subject and object are ex-tracted, if they exist, from the parse tree.
Each INITis also tagged with a date and a location, if appropri-ate.Many candidate INITs are rejected, for variousreasons: the difficulty of generating a grammaticaland meaningful sentence from them, the observedunreliability of parses that include them, or becauseit would lead to incorrect INITs most of the time.The rejection rules were created manually and covera number of syntactical situations.
Cases in whichbad sentences can be generated remain, of course,even though about half the candidates are rejected.Examples of rejected Inits include those with verbsin infinitive form and those that are part of a con-ditional clause.
Discarding a lot of available infor-mation is a significant limitation of this first attempt,which we will address as the first priority in the fu-ture.3.2 GenerationFrom each INIT retrieved, we directly generate anew sentence, instead of first selecting INITs andplanning the summary.
This is accomplished usingthe original parse tree of the sentence from whichthe INIT is taken, and the NLG realizer SimpleNLG(Gatt and Reiter, 2009) to generate an actual sen-tence.
Sample generated sentences are illustrated inFigure 2.This process ?
a type of text-to-text generation ?can be described as translating the parts that we wantto keep from the dependency tree provided by theparser, into a format that the realizer understands.This way we keep track of what words play whatrole in the generated sentence and we select directlywhich parts of a sentence appear in a generated sen-tence for the summary.
All of this is driven by theprevious identification of INITs.
We do not includeany words identified as a date or a location in thesentence generation process, they will be generatedif needed at the summary generation step, section3.4.Sentence generation follows the following steps:?
Generate a Noun Phrase (NP) to represent thesubject if present?
Generate a NP to represent the object if present?
Generate a NP to represent the indirect objectif present?
Generate a complement for the verb if one ispresent and only if there was no object?
Generate the Verb Phrase (VP) and link all thecomponents together, ignoring anything elsepresent in the original sentenceNP GenerationNoun phrase generation is based on the subtree ofits head word in the dependency parse tree.
The head68in the subtree becomes the head of the NP and chil-dren in its parse subtree are added based on manualrules that determine which children are realized andhow.Verb Complement GenerationWhen an INIT has no object, then we attempt tofind another complement instead, in case the verbwould have no interesting meaning without a com-plement.
The first verb modifier that follows it in thesentence order is used, including for example prepo-sitional phrases and infinitive clauses.VP GenerationFinally, the verb phrases are generated from eachverb and some of its children.
The NPs generated forthe subject, object and indirect object are added, aswell as the verb complement if it was generated.
Ifthere is an object but no subject, the VP is set to pas-sive, otherwise the active form is always used.
Thetense (past or present) of the VP is set to the tense ofthe verb in the original sentence, and most modifierslike auxiliaries and negation are conserved.3.3 Sentence SelectionTo determine which of the generated sentencesshould be used in the summary, we would have likedto choose from among the INITs directly.
For exam-ple, selecting the most frequent INIT, or INITs con-taining the most frequent subject-verb pair seem rea-sonable at first.
However, during development, nosuch naive implementation of selecting INITs pro-vided satisfactory results, because of the low fre-quency of those constructs, and the difficulty tocompare them semantically in our current level ofabstraction.
Thus this critical content selection stepoccurs after the sentence generation process.
Onlythe generated sentences are considered for the sen-tence selection process; original sentences from thesource documents are ignored.We compute a score based on the frequencies ofthe terms in the sentences generated from the INITsand select sentences that way.
Document frequency(DF) ?
the number of documents that include an en-tity in its original text ?
of the lemmas included inthe generated sentence is the main scoring criterion.This criterion is commonly used for summaries ofgroups of similar documents.
The generated sen-tences are ranked based on their average DF (thesum of the DF of all the unique lemmas in the sen-tence, divided by the total number of words in thesentence).
Lemmas in a stop list and lemmas that areincluded in a sentence already selected in the sum-mary have their DF reduced to 0, to avoid favoringfrequent empty words, and to diminish redundancyin the summary.3.4 Summary GenerationA final summary generation step is required in thisfirst attempt, to account for the planning stage andto incorporate dates and locations for the generatedsentences.Sentence selection provides a ranking of the gen-erated sentences and a number of sentences inten-tionally in excess of the size limit of the summaryis first selected.
Those sentences are ordered by thedate of their INIT when it can be determined.
Oth-erwise, the day before the date of publication of thearticle that included the INIT is used instead.
Allgenerated sentences with the same known date aregrouped in a single coordinated sentence.
The dateis included directly as a pre-modifier ?On date,?
atthe beginning of the coordination.Each INIT with a known location has its generatedsentence appended with a post-modifier ?in loca-tion?, except if that location has already been men-tioned in a previous INIT of the summary.At the end of this process, the size of the summaryis always above the size limit.
We remove the leastrelevant generated sentence and restart the summarygeneration process.
We keep taking away the leastrelevant generated sentence in a greedy way, untilthe length of the summary is under the size limit.This naive solution to never exceed the limit waschosen because we originally believed that our INITsalways lead to short generated sentences.
However,it turns out that some of the generated summariesare a bit too short because some sentences that wereremoved last were quite long.4 Results and DiscussionHere, we present and discuss the results obtained byour system in the TAC 2010 summarization systemevaluation.
We only show results for the evaluationof standard multi-document summaries; there was69also an update task, but we did not develop a spe-cific module for it.
After ranking at or near the topwith extractive approaches in past years (Genest etal., 2008) (Genest et al, 2009a), we expected a largedrop in our evaluation results with our first attemptat abstractive summarization.
In general, they areindeed on the low side, but mostly with regards tolinguistic quality.As shown in Table 1, the linguistic quality of oursummaries was very low, in the bottom 5 of 43 par-ticipating automatic systems.
This low linguisticscore is understandable, because this was our firsttry at text generation and abstractive summarization,whereas the other systems that year used sentenceextraction, with at most minor modifications madeto the extracted sentences.The cause of this low score is mostly our methodfor text generation, which still needs to be refinedin several ways.
The way we identify INITs, as wehave already discussed, is not yet developped fully.Even in the context of the methodology outlined insection 3, and specifically 3.2, many improvementscan still be made.
Errors specific to the current stateof our approach came from two major sources: in-correct parses, and insufficiently detailed and some-times inappropriate rules for ?translating?
a part ofa parse into generated text.
A better parser wouldbe helpful here and we will try other alternatives fordependency parsing in future work.Pyr.
Ling.
Q.
Overall R.AS 0.315 2.174 2.304Avg 0.309 2.820 2.576Best 0.425 3.457 3.174Models 0.785 4.910 4.760AS Rank 29 39 29Table 1: Scores of pyramid, linguistic quality and overallresponsiveness for our Abstractive Summarization (AS)system, the average of automatic systems (Avg), the bestscore of any automatic system (Best), and the averageof the human-written models (Models).
The rank is com-puted from amongst the 43 automatic summarization sys-tems that participated in TAC 2010.Although the linguistic quality was very low,our approach was given relatively good Pyramid(Nenkova et al, 2007) (a content metric) and overallresponsiveness scores, near the average of automatic!
"#"$%&'&()"* $%+,-'./"#"$%0'&0$%$$$$%'$$+%$$$+%'$$,%$$$,%'$$(%$$$(%'$$$%$$$ $%'$$ +%$$$ +%'$$ ,%$$$ ,%'$$ (%$$$ (%'$$ 1%$$$!
"#$%&&'(#)*+,)-"#,#)).-,/0-)1-2'30%&-14AS' AS!
"#"$%&''()"*"+%,'-./"#"0%-+-10%0000%,00+%000+%,001%0001%,00$%000$%,00(%0000%000 0%0,0 0%+00 0%+,0 0%100 0%1,0 0%$00 0%$,0 0%(00 0%(,0!
"#$%"&'"()*%+,"'-.-/+0"1AS AS'Figure 3: Scatter plots of overall responsiveness with re-spect to linguistic quality (top) and pyramid score withrespect to linguistic quality (bottom), for all the systemscompeting in TAC 2010.
The two runs identified withan arrow, AS and AS?, were two similar versions of ourabstractive summarization approach.systems.
This indicates that, even in a rough first trywhere content selection was not the main focus, ourmethod is capable of producing summaries with rea-sonably good content and of reasonably good over-all quality.
There is a correlation between linguis-tic quality and the other two manual scores for mostruns, but, as we can see in Figure 3, the two runsthat we submitted stand out, even though linguisticquality plays a large role in establishing the overallresponsiveness scores.
We believe this to be rep-resentative of the great difference of our approachcompared to extraction.
By extension, following thetrend, we hope that increasing the linguistic qualityof our approach to the level of the top systems wouldyield content and overall scores above their currentones.The type of summaries that our approach pro-duces might also explain why it receives good con-tent and overall scores, even with poor linguistic70quality.
The generated sentences tend to be short,and although some few may have bad grammar oreven little meaning, the fact that we can pack a lot ofthem shows that INITs give a lot more flexibility tothe content selection module than whole sentences,that only few can fit in a small size limit such as100 words.
Large improvements are to be expected,since this system was developped over only a fewmonths, and we haven?t implemented the full scaleof our framework described in section 2.5 Related WorkWe have already discussed alternative approaches toabstractive summarization in the introduction.
Thissection focuses on other work dealing with the tech-niques we used.Subject?Verb?Object (SVO) extraction is notnew.
Previous work by (Rusu et al, 2007) dealsspecifically with what the authors call triplet extrac-tion, which is the same as SVO extraction.
Theyhave tried a variety of parsers, including MINIPAR,and they build parse trees to extract SVOs simi-larly to us.
They applied this technique to extrac-tive summarization in (Rusu et al, 2009) by buildingwhat the authors call semantic graphs, derived fromtriplets, and then using said graphs to identify themost interesting sentences for the summary.
Thispurpose is not the same as ours, and triplet extrac-tion was conducted quite superficially (and thus in-cluded a lot of noise), whereas we used several rulesto clean up the SVOs that would serve as INITs.Rewriting sentences one idea at a time, as wehave done in this work, is also related to the fieldof text simplification.
Text simplification has beenassociated with techniques that deal not only withhelping readers with reading disabilities, but also tohelp NLP systems (Chandrasekar et al, 1996).
Thework of (Beigman Klebanov et al, 2004) simplifiessentences by using MINIPAR parses as a startingpoint, in a process similar to ours, for the purposeof helping information-seeking applications in theirown task.
(Vickrey and Koller, 2008) applies similartechniques, using a sequence of rule-based simpli-fications of sentences, to preprocess documents forSemantic Role Labeling.
(Siddharthan et al, 2004)uses shallow techniques for syntactical simplifica-tion of text by removing relative clauses and apposi-tives, before running a sentence clustering algorithmfor multi-document summarization.The kind of text-to-text generation involved in ourwork is related to approaches in paraphrasing (An-droutsopoulos and Malakasiotis, 2010).
Paraphrasegeneration produces sentences with similar mean-ings, but paraphrase extraction from texts requiresa certain level of analysis.
In our case, we are in-terested both in reformulating specific aspects of asentence, but also in identifying parts of sentences(INITs) with similar meanings, for content selection.We believe that there will be more and more similar-ities between our work and the field of paraphrasingas we improve on our model and techniques.6 ConclusionWe have proposed an ambitious new way of look-ing at abstractive summarization, with our proposedframework.
We believe that this framework aims atthe real goals of automatic summarization ?
control-ling the content and structure of the summary.
Thisrequires both an ability to correctly analyze text, andan ability to generate text.
We have described a firstattempt at fully abstractive summarization that relieson text-to-text generation.We find the early results of TAC 2010 quite sat-isfactory.
Receiving a low linguistic quality scorewas expected, and we are satisfied with average per-formance in content and in overall responsiveness.It means that our text-to-text generation was goodenough to produce understandable summaries.Our next step will be to go deeper into the analysisof sentences.
Generating sentences should rely lesson the original sentence structure and more on theinformation meant to be transmitted.
Thus, we wantto move away from the current way we generate sen-tences, which is too similar to rule-based sentencecompression.
At the core of moving toward full ab-straction, we need to redefine INITs so that they canbe manipulated (compared, grouped, realized as sen-tences, etc.)
more effectively.
We intend to use toolsand techniques that will enable us to find words andphrases of similar meanings, and to allow the gener-ation of a sentence that is an aggregate of informa-tion found in several source sentences.
In this way,we would be moving away from purely syntacticalanalysis and toward the use of semantics.71ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entailmentmethods.
J. Artif.
Int.
Res., 38:135?187, May.Regina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.Beata Beigman Klebanov, Kevin Knight, and DanielMarcu.
2004.
Text simplification for information-seeking applications.
In Robert Meersman and ZahirTari, editors, Proceedings of Ontologies, Dabases, andApplications of Semantics (ODBASE) InternationalConference, volume 3290 of Lecture Notes in Com-puter Science, pages 735?747, Agia Napa, Cyprus,October.
Springer.R.
Chandrasekar, Christine Doran, and B. Srinivas.
1996.Motivations and methods for text simplification.
InProceedings of the 16th conference on Computationallinguistics - Volume 2, COLING ?96, pages 1041?1044, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Shouyuan Chen, Yuanming Yu, Chong Long, Feng Jin,Lijing Qin, Minlie Huang, and Xiaoyan Zhu.
2008.Tsinghua University at the Summarization Track ofTAC 2008.
In Proceedings of the First Text AnalysisConference, Gaithersburg, Maryland, USA.
NationalInstitute of Standards and Technology.Trevor Cohn and Mirella Lapata.
2009.
Sentencecompression as tree transduction.
J. Artif.
Int.
Res.,34(1):637?674.John M. Conroy, Judith D. Schlesinger, Peter A. Rankel,and Dianne P. O?Leary.
2010.
CLASSY 2010: Sum-marization and metrics.
In Proceedings of the ThirdText Analysis Conference, Gaithersburg, Maryland,USA.
National Institute of Standards and Technology.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:A framework and graphical development environmentfor robust NLP tools and applications.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics, Philadelphia, PA, USA.Albert Gatt and Ehud Reiter.
2009.
SimpleNLG: a Re-alisation Engine for Practical Applications.
In ENLG?09: Proceedings of the 12th European Workshop onNatural Language Generation, pages 90?93, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Pierre-Etienne Genest and Guy Lapalme.
2010.
Textgeneration for abstractive summarization.
In Proceed-ings of the Third Text Analysis Conference, Gaithers-burg, Maryland, USA.
National Institute of Standardsand Technology.Pierre-Etienne Genest, Guy Lapalme, Luka Nerima, andEric Wehrli.
2008.
A Symbolic Summarizer for theUpdate Task of TAC 2008.
In Proceedings of theFirst Text Analysis Conference, Gaithersburg, Mary-land, USA.
National Institute of Standards and Tech-nology.Pierre-Etienne Genest, Guy Lapalme, Luka Nerima, andEric Wehrli.
2009a.
A symbolic summarizer with 2steps of sentence selection for TAC 2009.
In Proceed-ings of the Second Text Analysis Conference, Gaithers-burg, Maryland, USA.
National Institute of Standardsand Technology.Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-Monod.
2009b.
HexTac: the Creation of a ManualExtractive Run.
In Proceedings of the Second TextAnalysis Conference, Gaithersburg, Maryland, USA.National Institute of Standards and Technology.David Gillick, Benoit Favre, Dilek-Hakkani Tu?r, BerndtBohnet, Yang Liu, and Shasha Xie.
2009.
TheICSI/UTD Summarization System at TAC 2009.
InProceedings of the Second Text Analysis Conference,Gaithersburg, Maryland, USA.
National Institute ofStandards and Technology.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of the Seventeenth National Con-ference on Artificial Intelligence and Twelfth Confer-ence on Innovative Applications of Artificial Intelli-gence, pages 703?710.
AAAI Press.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proc.
Workshop on the Evaluation of Pars-ing Systems, Granada.Inderjeet Mani.
2001.
Automatic Summarization, vol-ume 3 of Natural Language Processing.
John Ben-jamins Publishing Company.Ani Nenkova, Rebecca Passonneau, and Kathleen McK-eown.
2007.
The pyramid method: Incorporating hu-man content selection variation in summarization eval-uation.
ACM Trans.
Speech Lang.
Process., 4, May.Karolina Owczarzak and Hoa Trang Dang.
2010.Overview of the TAC 2009 summarizationtrack.
In Proceedings of the Third Text Analy-sis Conference, Gaithersburg, Maryland, USA.National Institute of Standards and Technology.http://www.nist.gov/tac/publications/.Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, andDaniel Tam.
2004.
Centroid-based summarizationof multiple documents.
Information Processing andManagement, 40(6):919?938.Delia Rusu, Lorand Dali, Blaz Fortuna, Marko Gro-belnik, and Dunja Mladenic.
2007.
Triplet extrac-tion from sentences.
Proceedings of the 10th Inter-national Multiconference ?Information Society ?
IS2007?, A:218?222, October.72Delia Rusu, Blaz Fortuna, Marko Grobelnik, and DunjaMladenic.
2009.
Semantic graphs derived fromtriplets with application in document summarization.Informatica, 33, October.Advaith Siddharthan, Ani Nenkova, and Kathleen McK-eown.
2004.
Syntactic simplification for improvingcontent selection in multi-document summarization.In Proceedings of the 20th international conferenceon Computational Linguistics, COLING ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,Tadashi Kumano, and Naoto Kato.
2009.
Syntax-driven sentence revision for broadcast news summa-rization.
In Proceedings of the 2009 Workshop on Lan-guage Generation and Summarisation, UCNLG+Sum?09, pages 39?47, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.David Vickrey and Daphne Koller.
2008.
SentenceSimplification for Semantic Role Labeling.
In Pro-ceedings of ACL-08: HLT, pages 344?352, Columbus,Ohio, June.
Association for Computational Linguis-tics.73
