Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298?307,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSubcategorisation Acquisition from Raw Text for a Free Word-OrderLanguageWill Roberts and Markus Egg and Valia KordoniInstitute f?ur Anglistik und Amerikanistik, Humboldt University10099 Berlin, Germany{will.roberts,markus.egg,evangelia.kordoni}@anglistik.hu-berlin.deAbstractWe describe a state-of-the-art automaticsystem that can acquire subcategorisationframes from raw text for a free word-orderlanguage.
We use it to construct a subcate-gorisation lexicon of German verbs from alarge Web page corpus.
With an automaticverb classification paradigm we evaluateour subcategorisation lexicon against a pre-vious classification of German verbs; thelexicon produced by our system performsbetter than the best previous results.1 IntroductionWe introduce a state-of-the-art system for the ac-quisition of subcategorisation frames (SCFs) fromlarge corpora, which can deal with languages withvery free word order.
The concrete language wetreat is German; its word order variability is illus-trated in (1)?
(4), all of which express the sentenceThe man gave the old dog a chop:(1) Dem alten Hund gab der Mann ein Schnitzel.
(2) Ein Schnitzel gab dem alten Hund der Mann.
(3) Ein Schnitzel gab der Mann dem alten Hund.
(4) Der Mann gab dem alten Hund ein Schnitzel.On the basis of raw text, the system can beused to build extensive SCF lexicons for Germanverbs.
Subcategorisation means that lexical itemsrequire specific obligatory concomitants or argu-ments; we focus on verb subcategorisation.
E.g.,the verb geben ?give?
requires three arguments, thenominative subject der Mann ?the man?, the dativeindirect object dem alten Hund ?the old dog?, andthe accusative direct object ein Schnitzel ?a chop?.Other syntactic items may be subcategorised for,too, e.g.
both stellen and its English translationput subcategorise for subject, direct object, and aprepositional phrase (PP) like on the shelf :(5) [NPAl] put [NPthe book] [PPon the shelf].Subcategorisation frames describe a combina-tion of arguments required by a specific verb.
Theset of SCFs for a verb is called its subcategori-sation preference.
Our system follows much pre-vious work by counting PPs that accompany theverb among its complements, even though they arenot obligatory (so-called ?adjuncts?
), because PPadjuncts are excellent clues to a verb?s semantics(Sun et al., 2008).
However, nominal and clausaladjuncts do not count as verbal complements.SCF information can benefit all applicationsthat need information on predicate-argument struc-ture, e.g., parsing, verb clustering, semantic role la-belling, or machine translation.
Automatic acquisi-tion of SCF information with minimal supervisionis also crucial to construct useful resources quickly.The main innovation of the presented new sys-tem is to address two challenges simultaneously,viz., SCF acquisition from raw text and the focuson languages with a very free word order.
Withthis system, we create an SCF lexicon for Germanverbs and evaluate this lexicon against a previouslypublished manual verb classification, showing bet-ter performance than has been reported until now.After an overview of previous work on SCF ac-quisition in Section 2, Section 3 describes our sub-categorisation acquisition system, and Section 4the SCF lexicon that we build using it.
In Sec-tions 5 and 6 we evaluate the SCF lexicon on a verbclassification task and discuss our results; Section 7then concludes with directions for future work.2982 Previous workTo date, research on SCF acquisition from corporahas mostly targeted English.
Brent and Berwick(1991) detect five SCFs by looking for attestedcontexts where argument slots are filled by closed-class lexical items (pronouns or proper names).Briscoe and Carroll (1997) detect 163 SCFs witha system that builds an SCF lexicon whose en-tries include the relative frequency of SCF classes.Potential SCF patterns are extracted from a cor-pus parsed with a dependency-based parser, andthen filtered by hypothesis testing on binomial fre-quency data.
Korhonen (2002) refines Briscoe andCarroll (1997)?s system using back-off estimateson the WordNet semantic class of the verb?s pre-dominant sense, assuming that semantically similarverbs have similar SCFs, following Levin (1993).Some current statistical methods for Semantic RoleLabelling build models that also capture subcat-egorisation information, e.g., Grenager and Man-ning (2006).
Schulte im Walde (2009) offers a re-cent survey of the SCF acquisition literature.SCF acquisition is also an important step in theautomatic semantic role labelling (Grenager andManning, 2006; Lang and Lapata, 2010; Titov andKlementiev, 2012).
Semantic roles of a verb de-scribe the kind of involvement of entities in theevent introduced by the verb, e.g., as agent (active,often not affected by the event) or patient (passive,often affected).
On the basis of these SCFs, se-mantic roles can be assigned due to the interdepen-dence between semantic roles and their syntacticrealisations, called Argument Linking (Levin, 1993;Levin and Rappaport Hovav, 2005).Acquiring SCFs for languages with a very fixedword order like English needs only a simple syn-tactic analysis, which mainly relies on the prede-termined sequencing of arguments in the sentence,e.g., Grenager and Manning (2006).
When wordorder is freer, the analysis gets more complicated,and must include a full syntactic parse.What is more, German is a counterexample toManning?s (1993) expectation that freedom ofword order should be matched by an increase incase and/or agreement marking.
This is due to avery high degree of syncretism (identity of wordforms) in German paradigms for nouns, adjectives,and determiners.
E.g., the noun Auto ?car?
has onlytwo forms, Auto for nominative, dative, and ac-cusative singular, and Autos for genitive singularand all four plural forms.
This is in contrast to someother free word order languages for which SCFacquisition has been studied, like Modern Greek(Maragoudakis et al., 2000) and Czech (Sarkar andZeman, 2000).
A one-many relation between wordforms and case is also one of the problems for SCFacquisition in Urdu (Ghulam, 2011).For German, initial studies used semi-automatictechniques and manual evaluation (Eckle-Kohler,1999; Wauschkuhn, 1999).
The first automatic sub-categorisation acquisition system for German is de-scribed by Schulte im Walde (2002a), who definedan SCF inventory and manually wrote a grammarto analyse verb constructions according to theseframes.
A lexicalised PCFG parser using this gram-mar was trained on 18.7 million words of Germannewspaper text; the trained parser model containedexplicit subcategorisation frequencies, which couldthen be extracted to construct a subcategorisationlexicon for 14,229 German verbs.
This work wasevaluated against a German dictionary, the DudenStilw?orterbuch (Schulte im Walde, 2002b).Schulte im Walde and Brew (2002) used the sub-categorisation lexicon created by the system to au-tomatically induce a set of semantic verb classeswith an unsupervised clustering algorithm.
Thisclustering was evaluated against a small manuallycreated semantic verb classification.
Schulte imWalde (2006) continues this work using a largermanual verb classification.
The SCFs used in thisstudy are defined at three levels of granularity.
Thefirst level (38 different SCFs) lists only the comple-ments in the frame; the second one adds head andcase information for PP complements (183 SCFs).The third level examined the effect of adding selec-tional preferences, but results were inconclusive.A recent paper (Scheible et al., 2013) describes asystem similar to ours, built on a statistical depen-dency parser, and using some of the same kindsof rules as we describe in Section 3.1; this systemis evaluated in a task-based way (e.g., to improvethe performance of a SMT system) and cannot bedirectly compared to our system in this paper.3 The SCF acquisition systemThis section describes the first contribution of thispaper, a state-of-the-art subcategorisation acquisi-tion system for German.
Its core component is arule-based SCF tagger which operates on phrasestructure analyses, as delivered by a statisticalparser.
Given a parse of a sentence, the tagger as-signs each finite verb in the sentence an SCF type.299We use the SCF inventory of Schulte im Walde(2002a), which includes complements like n fornominative subject, a for accusative direct object,d for dative indirect object, r for reflexive pronoun,and x for expletive es (?it?)
subject.
Clausal com-plements can be infinite (i); finite ones can havethe verb in second position (S-2) or include thecomplementiser dass ?that?
(S-dass).
Comple-ments can be combined as in na (transitive verb);for PPs in SCFs, the head is specified, e.g., p:f?urfor PP complements headed by f?ur ?for?1.Due to the free word order, simple phrase struc-ture like that used for analysis of English is notenough to specify the syntax of German sentences.Therefore we use the annotation scheme in themanually constructed German treebanks NEGRAand TIGER (Skut et al., 1997; Brants et al., 2002),which decorate parse trees with edge labels specify-ing the syntactic roles of constituents.
We automat-ically annotate the parse trees from our statisticalparser using a simple machine learning model.In the next section, we illustrate the operation ofthe SCF tagger with reference to examples; then inSection 3.2 we describe our edge labeller.3.1 The SCF taggerThe SCF tagger begins by collecting complementsco-occurring with a verb instance using the phrasestructure of the sentence.
In our system, we obtainphrase structure information for unannotated textusing the Berkeley Parser (Petrov et al., 2006), astatistical unlexicalised parser trained on TIGER.Fig.
1 illustrates the phrase structure analysis andedge labels in the TIGER corpus for (6):(6) Das hielte ich f?ur moralisch au?erordentlichfragw?urdig.
?I?d consider that morally extremelyquestionable?.Its finite verb hielte (from halten ?hold?)
hasthree complements, the subject ich ?I?, edge-labelled with SB, the direct object das ?that?, la-belled with OA, and a PP headed by f?ur ?for?
(MOstands for ?modifier?).
After collecting comple-ments, the SCF tagger uses this edge label infor-mation to determine the complements?
syntacticroles, and assigns the verb the corresponding SCF;in the case of halten above, the SCF is nap:f?ur.1We digress from Schulte im Walde?s original SCF inven-tory in that we do not indicate case information in PPs.The rule-based SCF tagger handles auxiliary andmodal verb constructions, passive alternations, sep-arable verb prefixes, and raising and control con-structions.
E.g., the subject sie ?they?
of anfangen?begin?
in (7) doubles as the subject of its infiniteclausal complement; hence, it shows up in the SCFof the complement?s head geben ?give?, too:(7) Sie fingen an, mir Stromschl?age zu geben.
?They started to give me electric shocks.
?The tagger also handles involved cases withmany complements, including PPs and clauses asin (8).
As the SCF inventory allows at most threecomplements in an SCF, such cases call for pri-oritising of verbal complements (e.g., subjects, ob-jects, and clausal complements are preferred overPP complements).
Consequently, the main verbempfehlen ?recommend?
in (8), which has a subject,a dative object, a PP, and an infinitival clausal com-plement, is assigned the SCF ndi.
Another chal-lenging task which relies on edge label informationis filtering out clausal adjuncts (relative clauses andparentheticals) so as not to include them in SCFs.
(8) [PPAm Freitag] empfahl [NP:NomderAufsichtsrat] [NP:Datden Aktion?aren], [SdasAngebot abzulehnen].
?On Friday the board of directors advisedshareholders to turn down the offer.
?The 17 rules of the SCF tagger are simple; mostof them categorise the complements of a specificverb instance; e.g., if a nominal complement to theverb is edge-labelled as a nominative subject, add nto the verb?s SCF, unless the verb is in the passive,in which case add a to the SCF.Our system was optimised by progressively re-fining the SCF tagger?s rules through manual erroranalysis on sentences from TIGER.
The result isan automatic SCF tagger that is resilient to varia-tions in sentence structure and is firmly based onlinguistically motivated knowledge.
As a test casefor its linguistic soundness, we chose the perfectparses in the TIGER treebank and found that thetagger is very accurate in capturing subcategorisa-tion information inherent in these data.3.2 The edge labellerTo obtain edge label information for the parses de-livered by the Berkeley Parser, we built a novelmachine learning classifier to annotate parse trees300SPDSDasVVFINhieltePPERichPPAPPRf?urAPmoralisch au?erordentlich fragw?urdig$..OAHDSBMOACNKFigure 1: Edge labels in the TIGER corpus.with TIGER edge label information.
This edge la-beller is a maximum entropy (multiclass logisticregression) model built using the Stanford Classi-fier package2.
We include features such as:?
The part of speech of the complement;?
The first word of the complement;?
The lexical head of the complement;?
N-grams on the end of the lexical head of thecomplement;?
The kind of article of a complement;?
The presence or absence of specific articleforms in other complements to the same verb;?
Position of the complement with respect to areflexive pronoun in the sentence;?
The lemmatised form of the verb governingthe complement (i.e., the verb on which thecomplement depends syntactically);?
The clause type of the governing verb; and,?
Active or passive voice of the governing verb.We do no tuning and use the software?s defaulthyperparameters (L2 regularisation with ?
= 3).This classifier was trained from edge label dataextracted from the NEGRA and TIGER corpora;our training set contained 300,000 samples (ap-proximately 25% from NEGRA and 75% fromTIGER).
On a held-out test set of 10% (contain-ing 34,000 samples), the classifier achieves a finalF-score of 95.5% on the edge labelling task.The edge labeller makes the simplifying assump-tion that verbal complements can be labelled inde-pendently.
Consequently, it tends to annotate multi-ple complements as subject for each verb.
This hasto do with the numerical dominance of subjects,which make up about 40% of all verb complements,more than three times the number of the next mostcommon complement type (direct object).Therefore we first collect all possible labels withassociated probabilities that the edge labeller as-2http://nlp.stanford.edu/software/classifier.shtmlsigns to each complement of a verb.
We thenchoose the set of labels with the highest probabilitythat includes at most one subject and at most oneaccusative direct object for the verb, assuming thatthe joint probability of a set of labels is the productof the individual label probabilities.We use our edge labeller in this work for mor-phological disambiguation of nominals and foridentifying clausal adjuncts, but the edge labelleris a standalone reusable component, which mightbe equally well be used to mark up parse trees for,e.g., a semantic role labelling system.4 The subcategorisation lexiconWith the system described in Sec.
3, we build a Ger-man subcategorisation lexicon that collects countsof ?lemma,SCF?
on deWaC (Baroni et al., 2009),a corpus of text extracted from Web search re-sults, with 109words automatically POS-taggedand lemmatised by the TreeTagger (Schmid, 1994).A subset of this corpus, SdeWaC (Faa?
and Eckart,2013), has been preprocessed to include only sen-tences which are maximally parsable; this smallercorpus includes 880 million words in 45 millionsentences.
We parsed 3 million sentences (80 mil-lion words) of SdeWaC; after filtering out thoseverb lemmas seen only five times or fewer in thecorpus, we are left with statistics on 8 million verbinstances, representing 9,825 verb lemmas.As a concrete example for the resulting SCF lexi-con, consider the entry for sprechen ?talk?
in Fig.
2,which occurs 16,254 times in our SCF lexicon.Sprechen refers to a conversation with speaker,hearer, topic, message, and code: Speakers are ex-pressed by nominative NPs, hearers, by mit-, bei-or zu-PPs, topics, by von- and ?uber-PPs.
The codeis expressed in in-PPs, and the message, by ac-cusative NPs (einige Worte sprechen ?to say a fewwords?
), main-clause complements or subordinatedass (?that?)
sentences.
Other uses of the verb are301np:von (2715), n (2696), na (1380), np:mit(1247), np:in (1132), nS-2 (1064), np:?uber(853), np:f?ur (695), nS-dass (491), np:zu(307), nap:in (280), nap:von (275), ni (261),np:bei (212), np:gegen (192), np:an (186),naS-2 (172), np:aus (168), np:auf (112),nap:?uber (112)Figure 2: SCF lexicon for sprechenfigurative , e.g., sprechen gegen ?be a counterar-gument to?.
As the distinction between argumentsand adjuncts is gradual in our system, some adjunctpatterns appear in the lexicon, too, but only withlow frequency, e.g., np:auf, in which the auf -PPexpresses the setting of the conversation, as in aufder Tagung sprechen ?speak at the convention?.For reference, we also constructed an SCF lexi-con from the NEGRA and TIGER corpora, whichtogether comprise about 1.2 million words.
ThisSCF lexicon contains statistics on 133,897 verbinstances (5,316 verb lemmas).
While the manualannotations in NEGRA and TIGER mean that thisSCF lexicon has virtually no noise, the small sizeof the corpora results in problems with data spar-sity and negatively impacts the utility of this re-source (see discussion in Section 6.2).5 Automatic verb classificationThe remainder of the paper sets out to establish therelevance of our SCF acquisition system by com-parison to previous work.
As stated in Sec.
2, theonly prior automatic German SCF acquisition sys-tem is that of Schulte im Walde (2002a), which wasevaluated directly against an electronic version ofa large dictionary; as this is not an open accessresource, we cannot perform a similar evaluation.We opt therefore to use a task-based evaluationto compare our system directly with Schulte imWalde?s, and leave manual evaluation for futurework.
We refer back to the experiment set up bySchulte im Walde (2006) to automatically induceclassifications of German verbs by clustering themon the basis of their SCF preferences as listed inher SCF lexicon.
By casting this experiment as afixed task, we can compare our system directly tohers.
The link between subcategorisation and verbsemantics is linguistically sound, due to the inter-dependence between verb meanings and the num-ber and kinds of their syntactic arguments (Levin,1993; Levin and Rappaport Hovav, 2005).
E.g.,only transitive verbs that denote a change of statelike cut and break enter in the middle construction(The bread cuts easily.
), with the patient or themeargument appearing as the syntactic subject.
Thus,verbs whose SCF preferences show such an alter-nation can be predicted to denote a change of state.We adopt the automatic verb classificationparadigm to evaluate our system, replicatingSchulte im Walde?s (2006) experiment to the bestof our ability.
We argue that by evaluating ourSdeWaC SCF lexicon described in the previoussection, we simultaneously evaluate our subcate-gorisation acquisition system; this technique alsoallows us to demonstrate the semantic relevance ofour SCF lexicon.
Section 5.1 introduces the man-ual verb classification we use as a gold standardand Section 5.2 describes our unsupervised clus-tering technique.
Our evaluation of the clusteringagainst the gold standard then follows in Section 6.5.1 Manual verb classificationsThe semantic verb classification proposed bySchulte im Walde (2006, page 162ff.
), hereafterSiW2006, comprises 168 high- and low-frequencyverbs grouped into 43 semantic classes, with be-tween 2 and 7 verbs per class.
Examples of theseclasses are Aspect (e.g., anfangen ?begin?
), Propo-sitional Attitude (e.g., denken ?think?
), Transfer ofPossession (Obtaining) (e.g., bekommen ?get?
), andWeather (e.g., regnen ?rain?).
Some of the classesare subclassified3, e.g., Manner of Motion, withthe subclasses Locomotion (klettern ?climb?
), Ro-tation (rotieren ?rotate?
), Rush (eilen ?hurry?
), Ve-hicle (fliegen ?fly?
), and Flotation (gleiten ?glide?
).These classes are related to Levin classes in thatsome are roughly equivalent to a Levin class (e.g.,Aspect and Levin?s Begin class), others are sub-groups of Levin classes, e.g., Position is a sub-group of Levin?s Dangle class; finally, some classeslump together Levin classes, e.g., Transfer of Pos-session (Obtaining) combines Levin?s Get and Ob-tain classes.
This shows that these classes could beintegrated into a large-scale classification of Ger-man verbs in the style of Levin (1993).5.2 ClusteringFrom the counts of ?lemma,SCF?
in the SCF lexi-con, we can estimate the conditional probabilitythat a particular verb v appears with an SCF f :3For the purpose of our evaluation, we disregard class-subclass relations and consider subclasses as separate entities.302P (scf = f |lemma = v).
We smooth these con-ditional probability distributions by backing off tothe prior probability P (scf) (Katz, 1987).With these smoothed conditional probabilities,we cluster verbs with k-means clustering (Forgy,1965), a hard clustering technique, which partitionsa set of objects into k clusters.
The algorithm is ini-tialised with a starting set of k cluster centroids; itthen proceeds iteratively, first assigning each ob-ject to the cluster whose centroid is closest undersome distance measure, and then calculating newcentroids to represent the centres of the updatedclusters.
The algorithm terminates when the assign-ment of objects to clusters no longer changes.D(p?q) =?ipilogpiqi(9)irad(p, q) = D(p?p+ q2) +D(q?p+ q2) (10)skew(p, q) = D(p?
?q + (1?
?
)p) (11)In our experiments, verbs are represented bytheir conditional probability distributions overSCFs.
As distance measures, we use two variantsof the Kullback-Leibler divergence (9), a measureof the dissimilarity of two probability distributions.The KL divergence from p to q is undefined if atsome point q but not p is zero, so we use measuresbased on KL without this problem, viz., the in-formation radius (aka Jensen-Shannon divergence,a symmetric metric, (10)), as well as skew diver-gence (an asymmetric dissimilarity measure whichsmoothes q by interpolating it to a small degreewith p, (11)), where we set the interpolation param-eter to be ?
= 0.9, to make our results comparableto Schulte im Walde?s (2006)4.As mentioned, the k-means algorithm is ini-tialised with a set of cluster centroids; in this study,we initialise the centroids by random partitions(each of the n objects is randomly assigned to oneof k clusters, and the centroids are then computedas the means of these random partitions).
Becausethe random initial centroids influence the final clus-tering, we repeat the clustering a number of times.We also initialise the k-means cluster centroidsusing agglomerative hierarchical clustering, a de-terministic iterative bottom-up process.
Hierarchi-cal clustering initially assigns verbs to singletonclusters; the two clusters which are ?nearest?
to4Schulte im Walde (2006) takes ?
= 0.9 although Lee(1999) recommends ?
= 0.99 or higher values in her originaldescription of skew divergence.each other are then joined together, and this pro-cess is repeated until the desired number of clustersis obtained.
Hierarchical clustering is performedto group the verbs into k clusters; the centroidsof these clusters are then used to initialise the k-means algorithm.
While there exist several variantsof hierarchical clustering, we use Ward?s method(Ward, Jr, 1963) for merging clusters, which at-tempts to minimise the variance inside clusters;Ward?s criterion was previously found to be themost effective hierarchical clustering technique forverb classification (Schulte im Walde, 2006).6 EvaluationThis section presents the results of evaluating theunsupervised verb clustering based on our SCF lex-ica against the gold standard described in Sec.
5.1.6.1 ResultsWe use two cluster purity measures, defined inFig.
3; we intentionally target our numerical eval-uations to be directly comparable with previousresults in the literature.
As k-means is a hard clus-tering algorithm, we consider a clustering C to bean equivalence relation that partitions n verbs intok disjoint subsets C = {C1, .
.
.
, Ck}.The first of these purity measures, adjusted Randindex (Randain Eq.
(12)) judges clustering simi-larity using the notion of the overlap between acluster Ciin a given clustering C and a cluster Gjin a gold standard clustering G, this value beingdenoted by CGij= |Ci?
Gj|; values of Randarange between 0 for chance and 1 for perfect cor-relation.
The other metric, the pairwise F -score(PairF, Eq.
(13)), operates by constructing a con-tingency table on the(n2)pairs of verbs, the ideabeing that the gold standard provides binary judge-ments about whether two verbs should be clusteredtogether or not.
If a clustering agrees with the goldstandard in clustering a pair of verbs together orseparately, this is a ?correct?
answer; by extension,information retrieval measures such as precision(P ) and recall (R) can be computed.Table 1 shows the performance of our SCF lex-ica, evaluated against the SiW2006 gold standard.The random baseline is given by PairF = 2.08 andRanda= ?0.004 (calculated as the average of 50random partitions).
The optimal baseline is PairF= 95.81 and Randa= 0.909, calculated by evalu-ating the gold standard against itself.
As the goldstandard includes polysemous verbs, which belong303Randa(C,G) =?i,j(CGij2)?[?i(|Ci|2)?j(|Gj|2)]/(n2)12[?i(|Ci|2)+?j(|Gj|2)]?
[?i(|Ci|2)?j(|Gj|2)]/(n2)(12)PairF(C,G) =2P (C,G)R(C,G)P (C,G) +R(C,G)(13)Figure 3: Evaluation metrics used to compare clusterings to gold standards.Data Set Eval Distance Manual Random Best Random Mean WardSchulte im Walde PairF IRad 40.23 1.34?
16.15 13.37 17.86?
17.49Skew 47.28 2.41?
18.01 14.07 15.86?
15.23RandaIRad 0.358 0.001?
0.118 0.093 0.145?
0.142Skew 0.429 ?0.002?
0.142 0.102 0.158?
0.158NEGRA/TIGER PairF IRad 30.77 2.06?
14.67 12.39 16.13?
15.52Skew 40.19 3.47?
12.95 11.48 14.05?
14.31RandaIRad 0.281 0.000?
0.122 0.094 0.134?
0.129Skew 0.382 ?0.015?
0.102 0.089 0.112?
0.114SdeWaC PairF IRad 42.66 1.62?
20.36 18.26 26.94?
27.50Skew 50.38 2.99?
20.75 17.80 24.60?
24.94RandaIRad 0.387 ?0.006?
0.167 0.146 0.232?
0.238Skew 0.465 0.008?
0.170 0.143 0.208?
0.211Table 1: Evaluation of the NEGRA/TIGER and SdeWaC SCF lexica using the SiW2006 gold standard.to more than one cluster, the optimal baseline iscalculated by randomly picking one of their senses;the average is then taken over 50 such trials.We cluster using k = 43, matching the numberof clusters in the gold standard.
Of the 168 verbs inSiW2006, 159 are attested in NEGRA and TIGER(17,285 instances), and 167 are found in SdeWaC(1,047,042 instances)5.We report the results using k-means clusteringinitialised under a variety of conditions.
?Manual?shows the quality of the clustering achieved wheninitialising k-means with the gold standard classes.We also initialise clustering 10 times using ran-dom partitions.
For the best clustering6in these10, ?Random Best?
shows the evaluation of boththe starting random partition and the final cluster-ing found by k-means; ?Random Mean?
shows theaverage cluster purity of the 10 final clusterings.?Ward?
shows the evaluation of the clustering ini-tialised with centroids found by hierarchical clus-5Verbs missing from the clustering reduce the maximumachievable cluster purity score.6Specifically, we take the clustering result with the mini-mum intra-cluster distance (not the clustering result with thebest performance on the gold standard).tering of the verbs using Ward?s method.
Again,both the initial partition found by Ward?s methodand the k-means solution based on it are shown.For comparison, we list the results of Schulteim Walde (2006, p. 174, Table 7) for the secondlevel of SCF granularity, with PP head and caseinformation (see Sec.
2 for Schulte im Walde?sanalysis).
While this seems the most appropriatecomparison to draw, since we also collect statis-tics about PPs, it is ambitious because, as notedin Section 3, our SCF lexica lack case informa-tion about PPs.7Compared to Schulte im Walde?snumbers, the NEGRA/TIGER SCF lexicon scoressignificantly worse on the PairF evaluation metricunder all conditions, and also on the Randametricusing the skew divergence measure (Randa/IRadis not significantly different).
The SdeWaC SCFlexicon scores better on all metrics and conditions;these results are significant at the p < 0.001 level8.7PP case information is relevant for prepositions that cantake both locative and directional readings, as in in der Stadt(dative) ?in town?
und in die Stadt (accusative) ?to town?.8Statistical significance is calculated by running repeatedk-means clusterings with random partition initialisation andevaluating the results using the relevant purity metrics.
Theserepeated clustering scores represent a random variable (a func-3046.2 DiscussionSec.
6.1 compared the SCF lexicon created us-ing SdeWaC with the lexicon built by Schulte imWalde (2002a), showing that our lexicon achievessignificantly better results on the verb clusteringtask.
We interpret this to be indicative of a moreaccurate subcategorisation lexicon, and, by exten-sion, of a more accurate SCF acquisition system.We attribute this superior performance primar-ily to our use of a statistical parser as opposed toa hand-written grammar.
This design choice hasseveral advantages.
First, the parser delivers robustsyntactic analyses, which we can expect to be rel-atively domain-independent.
Second, we make noprior assumptions about the variety of subcategori-sation phenomena that might appear in text, decou-pling the identification of SCFs from the abilityto parse natural language.
Third, the fact that ourparser and edge labeller are trained on the 800,000word NEGRA/TIGER corpus means that we bene-fit from the linguistic expertise that went into build-ing that treebank.
Our use of off-the-shelf tools(the parser and our simple yet effective machinelearning model describing edge label information)makes our system considerably simpler and easierto implement than Schulte im Walde?s.
We see oursystem as more easily extensible to other languagesfor which there is a parser and an initial syntacti-cally annotated corpus to train the edge labeller on.The NEGRA/TIGER SCF lexicon performs notas well on the verb clustering evaluations, as fewerverbs are attested in NEGRA/TIGER compared tothe SdeWaC SCF lexicon and gold standard clus-terings.
Data sparsity can be a problem in SCF ac-quisition; all other factors being equal, using moredata to construct an SCF lexicon should make pat-terns in the language more readily visible and re-duce the chance of missing a particular lemma-SCF combination accidentally.
A secondary ef-fect is that models of verb subcategorisation prefer-ences like the ones used here can be more preciselyestimated as the counts of observed verb instancesincrease, particularly for low-frequency verbs.Error analysis of our SCF lexicon reveals lowcounts of expletive subjects.
The edge labeller issupposed to annotate semantically empty subjects(es, ?it?)
as expletive; for clusterings examined inSec.
5.1, this would affect weather verbs (e.g., estion of the random cluster centroids used to initialise the k-means clustering).
These samples are normally distributed, sowe determine statistical significance using a t-test against the?Random Mean?
results reported by Schulte im Walde (2006).regnet, ?it?s raining?).
However, in our SdeWaCSCF lexicon, expletive subjects are clearly under-represented.
Our SCF lexicon built on TIGER,where expletive subjects are systematically la-belled, has the SCF xa as the most common SCFfor the verb geben (in es gibt ?there is?).
In con-trast, in our SdeWaC SCF lexicon, the most com-mon SCF is the transitive na, with xa in seventhplace.
I.e., the edge labeller does not identify allexpletive subjects, which is due to the fact that ex-pletive subjects are syntactically indistinguishablefrom neuter pronominal subjects, so the edge la-beller does not have a rich feature set to inform itabout this category.
But since, statistically, exple-tive pronouns make up less than 1% of subjectsin TIGER, the prior probability of labelling a con-stituent as expletive is very low.
Due to these fig-ures, we do not expect this issue to seriously impactthe quality of our verb classification evaluations.7 Future workIn this paper we have presented a state-of-the-art subcategorisation acquisition system forfree-word order languages, and used it to cre-ate a large subcategorisation frame lexicon forGerman verbs.
Our SCF lexicon resource isavailable at http://amor.cms.hu-berlin.de/?robertsw/scflex.html.
We are per-forming a manual evaluation of the output of oursystem, which we will report soon.We plan to continue this work first by expandingour SCF lexicon with case information and selec-tional preferences, second by using our SCF clas-sifier and lexicon for verbal Multiword Expressionidentification in German, and last by comparingit to existing verb classifications, either by usingavailable resources for German like the SALSAcorpus (Burchardt et al., 2006), or by translatingparts of VerbNet into German to create a moreextensive gold standard for verb clustering in thespirit of Sun et al.
(2010) who found that Levin?sverb classification can be translated to French andstill usefully allow generalisation over verb classes.Finally, we plan to perform in vivo evaluationof our SCF lexicon, to determine what benefitit can deliver for NLP applications such as Se-mantic Role Labelling and Word Sense Disam-biguation.
Recent research has found that evenautomatically-acquired verb classifications can beuseful for NLP applications (Shutova et al., 2010;Guo et al., 2011).305ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In TLT, pages 24?41.Michael R. Brent and Robert C. Berwick.
1991.
Auto-matic acquisition of subcategorization frames fromtagged text.
In HLT, pages 342?345.
Morgan Kauf-mann.Ted Briscoe and John Carroll.
1997.
Automatic ex-traction of subcategorization from corpora.
CoRR,cmp-lg/9702002.Aljoscha Burchardt, Katrin Erk, Anette Frank, An-drea Kowalski, Sebastian Pado, and Manfred Pinkal.2006.
The SALSA corpus: A German corpus re-source for lexical semantics.
In LREC.Judith Eckle-Kohler.
1999.
Linguistic knowledge forautomatic lexicon acquisition from German text cor-pora.
Ph.D. thesis, Universit?at Stuttgart.Gertrud Faa?
and Kerstin Eckart.
2013.
SdeWaC - Acorpus of parsable sentences from the Web.
In Lan-guage processing and knowledge in the Web, pages61?68.
Springer, Berlin, Heidelberg.Edward W. Forgy.
1965.
Cluster analysis of multivari-ate data: Efficiency versus interpretability of classifi-cations.
Biometrics, 21:768?769.Raza Ghulam.
2011.
Subcategorization acquisitionand classes of predication in Urdu.
Ph.D. thesis,Universit?at Konstanz.Trond Grenager and Christopher D. Manning.
2006.Unsupervised discovery of a statistical verb lexicon.In EMNLP, pages 1?8.Yufan Guo, Anna Korhonen, and Thierry Poibeau.2011.
A weakly-supervised approach to argumen-tative zoning of scientific documents.
In EMNLP,pages 273?283.Slava Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3):400?401.Anna Korhonen.
2002.
Subcategorization acquisi-tion.
Technical report, University of Cambridge,Computer Laboratory.Joel Lang and Mirella Lapata.
2010.
Unsupervisedinduction of semantic roles.
In HLT, pages 939?947.Lillian Lee.
1999.
Measures of distributional similar-ity.
In ACL, pages 25?32.Beth Levin and Malka Rappaport Hovav.
2005.
Argu-ment realization.
Cambridge University Press, Cam-bridge.Beth Levin.
1993.
English verb classes and alter-nations: A preliminary investigation.
University ofChicago Press, Chicago.Christopher D. Manning.
1993.
Automatic acquisitionof a large subcategorization dictionary from corpora.In ACL, pages 235?242.Manolis Maragoudakis, Katia Lida Kermanidis, andGeorge Kokkinakis.
2000.
Learning subcategoriza-tion frames from corpora: A case study for modernGreek.
In Proceedings of COMLEX 2000, Work-shop on Computational Lexicography and Multime-dia Dictionaries, pages 19?22.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In ACL, pages 433?440.Anoop Sarkar and Daniel Zeman.
2000.
Automaticextraction of subcategorization frames for Czech.
InCOLING, pages 691?697.Silke Scheible, Sabine Schulte im Walde, MarionWeller, and Max Kisselew.
2013.
A compact but lin-guistically detailed database for German verb subcat-egorisation relying on dependency parses from Webcorpora: Tool, guidelines and resource.
In Web asCorpus Workshop.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In NeMLaP, vol-ume 12, pages 44?49.Sabine Schulte im Walde and Chris Brew.
2002.
Induc-ing German semantic verb classes from purely syn-tactic subcategorisation information.
In ACL, pages223?230.Sabine Schulte im Walde.
2002a.
A subcategorisationlexicon for German verbs induced from a lexicalisedPCFG.
In LREC, pages 1351?1357.Sabine Schulte im Walde.
2002b.
Evaluating verb sub-categorisation frames learned by a German statisti-cal grammar against manual definitions in the DudenDictionary.
In EURALEX, pages 187?197.Sabine Schulte im Walde.
2006.
Experiments onthe automatic induction of German semantic verbclasses.
Computational Linguistics, 32(2):159?194.Sabine Schulte im Walde.
2009.
The induction ofverb frames and verb classes from corpora.
In AnkeL?udeling and Merja Kyt?o, editors, Corpus linguis-tics: An international handbook, volume 2, chap-ter 44, pages 952?971.
Mouton de Gruyter, Berlin.Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In COLING, pages 1002?1010.306Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme forfree word order languages.
In ANLP, pages 88?95.Lin Sun, Anna Korhonen, and Yuval Krymolowski.2008.
Verb class discovery from rich syntactic data.In CICLing, pages 16?27, Haifa, Israel.Lin Sun, Anna Korhonen, Thierry Poibeau, and C?edricMessiant.
2010.
Investigating the cross-linguisticpotential of VerbNet-style classification.
In COL-ING, pages 1056?1064, Beijing, China.Ivan Titov and Alexandre Klementiev.
2012.
ABayesian approach to unsupervised semantic role in-duction.
In EACL, pages 12?22.Joe H. Ward, Jr. 1963.
Hierarchical grouping to opti-mize an objective function.
Journal of the AmericanStatistical Association, 58(301):236?244.Oliver Wauschkuhn.
1999.
Automatische Extrak-tion von Verbvalenzen aus deutschen Textkorpora.Shaker Verlag.307
