Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 36?41,Prague, June 2007. c?2007 Association for Computational LinguisticsRecognizing Textual Entailment Using Sentence Similarity based onDependency Tree SkeletonsRui Wang and G?nter NeumannLT-lab, DFKIStuhlsatzenhausweg 3, 66123 Saarbr?cken, Germany{wang.rui,Neumann}@dfki.deAbstractWe present a novel approach to RTE thatexploits a structure-oriented sentence rep-resentation followed by a similarity func-tion.
The structural features are automati-cally acquired from tree skeletons that areextracted and generalized from dependencytrees.
Our method makes use of a limitedsize of training data without any externalknowledge bases (e.g.
WordNet) or hand-crafted inference rules.
We have achievedan accuracy of 71.1% on the RTE-3 devel-opment set performing a 10-fold crossvalidation and 66.9% on the RTE-3 testdata.1 IntroductionTextual entailment has been introduced as a rela-tion between text expressions, capturing the factthat the meaning of one expression can be inferredfrom the other (Dagan and Glickman, 2004).
Moreprecisely, textual entailment is defined as ??
arelationship between a coherent text T and a lan-guage expression, which is considered as a hy-pothesis, H. We say that T entails H (H is a conse-quent of T), denoted by T ?
H, if the meaning ofH, as interpreted in the context of T, can be in-ferred from the meaning of T.?Table 1 displays several examples from theRTE-3 development set.
For the third pair (id=410)the key knowledge needed to decide whether theentailment relation holds is that ?
[PN1]?s wife,[PN2]?
entails ?The name of [PN1]?s wife is[PN2]?, although T contains much more (irrelevant)information.
On the other hand, the first pair (id=1)requires an understanding of concepts with oppo-site meanings (i.e.
?buy?
and ?sell?
), which is acase of semantic entailment.The different sources of possible entailmentsmotivated us to consider the development of spe-cialized entailment strategies for different NLPtasks.
In particular, we want to find out the poten-tial connections between entailment relations be-longing to different linguistic layers for differentapplications.In this paper, we propose a novel approach to-wards structure-oriented entailment based on ourempirical discoveries from the RTE corpora: 1) His usually textually shorter than T; 2) not all infor-mation in T is relevant to make decisions for theentailment; 3) the dissimilarity of relations amongthe same topics between T and H are of great im-portance.Based on the observations, our primary methodstarts from H to T (i.e.
in the opposite direction ofthe entailment relation) so as to exclude irrelevantinformation from T. Then corresponding key top-ics and predicates of both elements are extracted.We then represent the structural differences be-tween T and H by means of a set of Closed-ClassSymbols.
Finally, these acquired representations(named Entailment Patterns - EPs) are classified bymeans of a subsequence kernel.The Structure Similarity Function is combinedwith two robust backup strategies, which are re-sponsible for cases that are not handled by the EPs.One is a Triple Similarity Function applied on topof the local dependency relations of T and H; theother is a simple Bag-of-Words (BoW) approachthat calculates the overlapping ratio of H and T.Together, these three methods deal with differententailment cases in practice.362 Related WorkConventional methods for RTE define measures forthe similarity between T and H either by assumingan independence between words (Corley and Mi-halcea, 2005) in a BoW fashion or by exploitingsyntactic interpretations.
(Kouylekov and Magnini,2006) explore a syntactic tree editing distance todetect entailment relations.
Since they calculate thesimilarity between the two dependency trees of Tand H directly, the noisy information may decreaseaccuracy.
This observation actually motivated us tostart from H towards the most relevant informationin T.Logic rules (as proposed by (Bos and Markert,2005)) or sequences of allowed rewrite rules (as in(de Salvo Braz et al, 2005)) are another fashion oftackling RTE.
One the best two teams in RTE-2(Tatu et al, 2006) proposed a knowledge represen-tation model which achieved about 10% better per-formance than the third (Zanzotto and Moschitti,2006) based on their logic prover.
The other bestteam in RTE-2 (Hickl et al, 2006) automaticallyacquired extra training data, enabling them toachieve about 10% better accuracy than the third aswell.
Consequently, obtaining more training dataand embedding deeper knowledge were expectedto be the two main directions pointed out for futureresearch in the RTE-2 summary statement.
How-ever, except for the positive cases of SUM, T-Hpairs are normally not very easy to collect auto-matically.
Multi-annotator agreement is difficult toreach on most of the cases as well.
The knowledge-based approach also has its caveats since logicalrules are usually implemented manually and there-fore require a high amount of specialized humanexpertise in different NLP areas.Another group (Zanzotto and Moschitti, 2006)utilized a tree kernel method for cross-pair similar-ity, which showed an improvement, and this hasmotivated us to investigate kernel-based methods.The main difference in our method is that we applysubsequence kernels on patterns extracted from thedependency trees of T and H, instead of applyingtree kernels on complete parsing trees.
On the onehand, this allows us to discover essential parts in-dicating an entailment relationship, and on theother hand, computational complexity is reduced.3 An Overview of RTEFigure 1 shows the different processing techniquesand depths applied to the RTE task.
Our work fo-cuses on constructing a similarity function operat-ing between sentences.
In detail, it consists of sev-eral similarity scores with different domains oflocality on top of the dependency structure.
Figure2 gives out the workflow of our system.
The mainpart of the sentence similarity function is the Struc-ture Similarity Function; two other similarityscores are calculated by our backup strategies.
Thefirst backup strategy is a straightforward BoWmethod that we will not present in this paper (seemore details in (Corley and Mihalcea, 2005));Id Ta s k Te x t H y po th e s i s En ta i l s ?1 IETh e sa le wa s m a d e to  p a y Yu ko s' U S $  2 7 .5  b illio n  ta x b ill, Yu g a n skn efteg a zwa s o rig in a lly so ld  fo r U S $  9 .4  b illio n  to  a  little  kn o wn  co m p a n yB a ika lfin a n sg ro u p  wh ich  wa s la ter b o ught  b y th e R u ssia n  sta te-o wn ed  o ilB a ika lfin a n sg ro u pwa s so ld  toR o sn eft.Y E S3 9 0 IRT yp ho o n  X a n g sa n e la shed  th e P hilip p ine  ca p ita l o n  Th u rsd a y,g ro u n d in g  flig h ts, h a ltin g  vessels a n d  clo sin g  sch o o ls a n d  m a rkets a ftertrig g erin g  fa ta l fla sh  flo o d s in  th e cen tre o f th e  co u n try.A  typ ho o n  b a ttersth e  P hilip p ines .
Y E S4 1 0 Q A(S en ten ce 1  ...) .
A lo n g  with  th e first la d y's m o th er, Jen n a  Welch , th eweeken d  g a th erin g  in clu d es th e p resid en t's p a ren ts, fo rm er P resid en tG eo rge H .W .
Bush a nd  his wife, Ba rb a ra ;  h is sister D o ro  Ko ch  a n d  h erh u sb a n d , B o b b y;  a n d  h is b ro th er, M a rvin , a n d  h is wife , M a rg a ret.Th e n a m e o fG eo rg e H .W.B u sh 's wife isB a rb a ra .Y E S7 3 9 SU MTh e FD A  wo u ld  n o t sa y in  wh ich  sta tes  th e p ills  h a d  b een  so ld , b u tin stea d  reco m m en d ed  th a t cu sto m ers d eterm in e wh eth er p ro d u cts th eyb o u g h t a re b ein g  reca lled  b y ch eckin g  th e sto re list  o n  th e  FD A  Web  site ,a n d  th e b a tch  list .
Th e b a tch  n u m b ers a p p ea r o n  th e co n ta in er's la b el.Th e FD Ap ro vid ed  a  list  o fsta tes  in  wh ich  th ep ills  h a ve b eenN OTable 1 Examples from RTE-3Figure 1 Overview of RTE37while the second one is based on a triple set repre-sentation of sentences that expresses the local de-pendency relations found by a parser1.A dependency structure consists of a set of triplerelations (TRs).
A TR is of the form <node1, rela-tion, node2>, where node1 represents the head,node2 the modifier and relation the dependencyrelation.
Chief requirements for the backup systemare robustness and simplicity.
Accordingly, weconstruct a similarity function, the Triple Similar-ity Function (TSF), which operates on two triplesets and determines how many triples of H2 arecontained in T. The core assumption here is thatthe higher the number of matching triple elements,the more similar both sets are, and the more likelyit is that T entails H.TSF uses an approximate matching function.Different cases (i.e.
ignoring either the parent nodeor the child node, or the relation between nodes)might provide different indications for the similar-ity of T and H. In all cases, a successful match be-tween two nodes means that they have the samelemma and POS.
We then sum them up using dif-ferent weights and divide the result by the cardinal-ity of H for normalization.
The different weightslearned from the corpus indicate that the ?amountof missing linguistic information?
affect entailmentdecisions differently.4 Workflow of the Main ApproachOur Structure Similarity Function is based on thehypothesis that some particular differences be-tween T and H will block or change the entailmentrelationship.
Initially we assume when judging theentailment relation that it holds for each T-H pair1We are using Minipar (Lin, 1998) and Stanford Parser (Kleinand Manning, 2003) as preprocessors, see also sec.
5.2.2Note that henceforth T and H will represent either the origi-nal texts or the dependency structures.
(using the default value ?YES?).
The major stepsare as follows (see also Figure 2):4.1 Tree Skeleton ExtractorSince we assume that H indicates how to extractrelevant parts in T for the entailment relation, westart from the Tree Skeleton of H (TSH).
First, weconstruct a set of keyword pairs using all the nounsthat appear in both T and H. In order to increasethe hits of keyword pairs, we have applied a partialsearch using stemming and some word variationtechniques on the substring level.
For instance, thepair (id=390) in Table 1 has the following list ofkeyword pairs,<Typhoon_Xangsane ## typhoon,Philippine ## Philippines>Then we mark the keywords in the dependencytrees of T and H and extract the sub-trees by ignor-ing the inner yields.
Usually, the Root Node of H(RNH) is the main verb; all the keywords are con-tained in the two spines of TSH (see Figure 3).Note that in the Tree Skeleton of T (TST), 1) theRoot Node (RNT) can either be a verb, a noun oreven a dependency relation, and 2) if the two FootNodes (FNs) belong to two sentences, a dummynode is created that connects the two spines.Thus, the prerequisite for this algorithm is thatTSH has two spines containing all keywords in H,and T satisfies this as well.
For the RTE-3 devel-opment set, we successfully extracted tree skele-Figure 3 Example of a Tree SkeletonFigure 2 Workflow of the System38?=?=+?=?==><><||1|'|1')',(||1|'|1')',()',',,(HjHj jCCSjCCSCCSKTiTi iCCSiCCSCCSKHTHTesubsequencKtons from 254 pairs, i.e., 32% of the data is cov-ered by this step, see also sec.
5.2.Next, we collapse some of the dependency rela-tion names from the parsers to more generalizedtag names, e.g., collapsing <OBJ2> and <DESC>to <OBJ>.
We group together all nodes that haverelation labels like <CONJ> or <NN>, since theyare assumed to refer to the same entity or belong toone class of entities sharing some common charac-teristics.
Lemmas are removed except for the key-words.
Finally, we add all the tags to the CCS set.Since a tree skeleton TS consists of spines con-nected via the same root node, TS can be trans-formed into a sequence.
Figure 4 displays an ex-ample corresponding to the second pair (id=390) ofTable 1.
Thus, the general form of a sequential rep-resentation of a tree skeleton is:LSP #RN# RSPwhere LSP represents the Left Spine, RSP repre-sents the Right Spine, and RN is the Root Node.On basis of this representation, a comparison of thetwo tree skeletons is straightforward: 1) merge thetwo LSPs by excluding the longest common prefix,and 2) merge the two RSPs by excluding the long-est common suffix.
Then the Spine Difference (SD)is defined as the remaining infixes, which consistsof two parts, SDT and SDH.
Each part can be eitherempty (i.e.
?)
or a CCS sequence.
For instance, thetwo SDs of the example in Figure 4 (id=390) are(LSD ?
Left SD; RSD ?
Right SD; ## is a separa-tor sign):LSDT(N) ## LSDH(?)RSDT(?)
## RSDH(?
)We have observed that two neighboring depend-ency relations of the root node of a tree skeleton(<SUBJ> or <OBJ>) can play important roles inpredicting the entailment relation as well.
There-fore, we assign them two extra features namedVerb Consistence (VC) and Verb Relation Con-sistence (VRC).
The former indicates whether tworoot nodes have a similar meaning, and the latterindicates whether the relations are contradictive(e.g.
<SUBJ> and <OBJ> are contradictive).We represent the differences between TST andTSH by means of an Entailment Pattern (EP),which is a quadruple <LSD, RSD, VC, VRC>.
VCis either true or false, meaning that the two RNsare either consistent or not.
VRC has ternary value,whereby 1 means that both relations are consistent,-1 means at least one pair of corresponding rela-tions is inconsistent, and 0 means RNT is not averb.3 The set of EPs defines the feature space forthe subsequence kernels in our Structure SimilarityFunction.4.2 Structure Similarity FunctionWe define the function by constructing two basickernels to process the LSD and RSD part of an EP,and two trivial kernels for VC and VRC.
The fourkernels are combined linearly by a composite ker-nel that performs binary classification on them.Since all spine differences SDs are either emptyor CCS sequences, we can utilize subsequencekernel methods to represent features implicitly, cf.
(Bunescu and Mooney, 2006).
Our subsequencekernel function is:whereby T and H refers to all spine differencesSDs from T and H, and |T| and |H| represent thecardinalities of SDs.
The function KCCS(CCS,CCS?
)checks whether its arguments are equal.Since the RTE task checks the relationship be-tween T and H, we need to consider collocationsof some CCS subsequences between T and H aswell.
Essentially, this kernel evaluates the similar-ity of T and H by means of those CCS subse-quences appearing in both elements.
The kernelfunction is as follows:On top of the two simple kernels, KVC, and KVRC,we use a composite kernel to combine them line-arly with different weights:VRCVCncollocatioesubsequenccomposite KKKKK ????
+++= ,3Note that RNH is guaranteed to be a verb, because otherwisethe pair would have been delegated to the backup strategies.???
?= = = =?=><><||1'||1'||1'||1'''),(),()',',,(TiTiHjHjjjCCSiiCCSncollocatioCCSCCSKCCSCCSKHTHTKFigure 4 Spine Merging39where ?
and ?
are learned from the training corpus;?=?=1.5 EvaluationWe have evaluated four methods: the two backupsystems as baselines (BoW and TSM, the TripleSet Matcher) and the kernel method combined withthe backup strategies using different parsers, Mini-par (Mi+SK+BS) and the Stanford Parser(SP+SK+BS).
The experiments are based on RTE-3 Data 4 .
For the kernel-based classification, weused the classifier SMO from the WEKA toolkit(Witten and Frank, 1999).5.1 Experiment ResultsRTE-3 data include the Dev Data (800 T-H pairs,each task has 200 pairs) and the Test Data (samesize).
Experiment A performs a 10-fold cross-validation on Dev Data; Experiment B uses DevData for training and Test Data for testing cf.
Table2 (the numbers denote accuracies):Systems\Tasks IE IR QA SUM AllExp A: 10-fold Cross Validation on RTE-3 Dev DataBoW 54.5 70 76.5 68.5 67.4TSM 53.5 60 68 62.5 61.0Mi+SK+BS 63 74 79 68.5 71.1SP+SK+BS 60.5 70 81.5 68.5 70.1Exp B: Train: Dev Data; Test: Test DataBoW 54.5 66.5 76.5 56 63.4TSM 54.5 62.5 66 54.5 59.4Mi+SP+SK+BS 58.5 70.5 79.5 59 66.9Table 2 Results on RTE-3 DataFor the IE task, Mi+SK+BS obtained the highestimprovement over the baseline systems, suggestingthat the kernel method seems to be more appropri-ate if the underlying task conveys a more ?rela-tional nature.?
Improvements in the other tasks areless convincing as compared to the baselines.
Nev-ertheless, the overall result obtained in experimentB would have been among the top 3 of the RTE-2challenge.
We utilize the system description tableof (Bar-Haim et al, 2006) to compare our systemwith the best two systems of RTE-2 in Table 35:4See (Wang and Neumann, 2007) for details concerning theexperiments of our method on RTE-2 data.5Following the notation in  (Bar-Haim et al, 2006): Lx: Lexi-cal Relation DB; Ng: N-Gram / Subsequence overlap; Sy:Syntactic Matching / Alignment; Se: Semantic Role Labeling;LI: Logical Inference; C: Corpus/Web; M: ML Classification;B: Paraphrase Technology / Background Knowledge; L: Ac-quisition of Entailment Corpora.Systems Lx Ng Sy Se LI C M B LHickl et al X X X X  X X  XTatu et al X    X   XOurs  X X    XTable 3 Comparison with the top 2 systems inRTE-2.Note that the best system (Hickl et al, 2006) ap-plies both shallow and deep techniques, especiallyin acquiring extra entailment corpora.
The secondbest system (Tatu et al, 2006) contains manymanually designed logical inference rules andbackground knowledge.
On the contrary, we ex-ploit no additional knowledge sources besides thedependency trees computed by the parsers, nor anyextra training corpora.5.2 DiscussionsTable 4 shows how our method performs for thetask-specific pairs matched by our patterns:Tasks IE IR QA SUM ALLExpA:Matched 53 19 23.5 31.5 31.8ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8ExpB:Matched 58.5 16 27.5 42 36ExpB:Accuracy 57.2 81.5 90.9 65.5 68.8Table 4 Performances of our methodFor IE pairs, we find good coverage, whereasfor IR and QA pairs the coverage is low, though itachieves good accuracy.
According to the experi-ments, BoW has already achieved the best per-formance for SUM pairs cf.
Table 2.As a whole, developing task specific entailmentoperators is a promising direction.
As we men-tioned in the first section, the RTE task is neither aone-level nor a one-case task.
The experimentalresults uncovered differences among pairs of dif-ferent tasks with respect to accuracy and coverage.On the one hand, our method works successfullyon structure-oriented T-H pairs, most of which arefrom IE.
If both TST and TSH can be transformedinto CCS sequences, the comparison performs well,as in the case of the last example (id=410) in Table1.
Here, the relation between ?wife?, ?name?, and?Barbara?
is conveyed by the punctuation ?,?, theverb ?is?, and the preposition ?of?.
Other cases likethe ?work for?
relation of a person and a companyor the ?is located in?
relation between two locationnames are normally conveyed by the preposition?of?.
Based on these findings, taking into accountmore carefully the lexical semantics based on in-ference rules of functional words might be helpfulin improving RTE.40On the other hand, accuracy varies with T-Hpairs from different tasks.
Since our method ismainly structure-oriented, differences in modifiersmay change the results and would not be caughtunder the current version of our tree skeleton.
Forinstance, ?a commercial company?
will not entail?a military company?, even though they are struc-turally equivalent.Most IE pairs are constructed from a binary rela-tion, and so meet the prerequisite of our algorithm(see sec.
4.1).
However, our method still has ratherlow coverage.
T-H pairs from other tasks, for ex-ample like IR and SUM, usually contain more in-formation, i.e.
more nouns, the dependency trees ofwhich are more complex.
For instance, the pair(id=739) in Table 1 contains four keyword pairswhich we cannot handle by our current method.This is one reason why we have constructed extraT-H pairs from MUC, TREC, and news articlesfollowing the methods of (Bar-Haim et al, 2006).Still, the overall performance does not improve.All extra training data only serves to improve thematched pairs (about 32% of the data set) forwhich we already have high accuracy (see Table 4).Thus, extending coverage by machine learningmethods for lexical semantics will be the main fo-cus of our future work.6 Conclusions and Future WorkApplying different RTE strategies for differentNLP tasks is a reasonable solution.
We have util-ized a structure similarity function to deal with thestructure-oriented pairs, and applied backup strate-gies for the rest.
The results show the advantage ofour method and direct our future work as well.
Inparticular, we will extend the tree skeleton extrac-tion by integrating lexical semantics based on in-ference rules for functional words in order to getlarger domains of locality.AcknowledgementsThe work presented here was partially supportedby a research grant from BMBF to the DFKI pro-ject HyLaP (FKZ: 01 IW F02) and the EC-fundedproject QALL-ME.ReferencesBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-colo, D., Magnini, B. and Szpektor, I.
2006.
The Sec-ond PASCAL Recognising Textual Entailment Chal-lenge.
In Proc.
of the PASCAL RTE-2 Challenge.Bos, J. and Markert, K. 2005.
Combining Shallow andDeep NLP Methods for Recognizing Textual Entail-ment.
In Proc.
of the PASCAL RTE Challenge.Bunescu, R. and Mooney, R. 2006.
Subsequence Ker-nels for Relation Extraction.
In Advances in NeuralInformation Processing Systems 18.
MIT Press.Corley, C. and Mihalcea, R. 2005.
Measuring the Se-mantic Similarity of Texts.
In Proc.
of the ACLWorkshop on Empirical Modeling of SemanticEquivalence and Entailment.Dagan, R., Glickman, O.
2004.
Probabilistic textualentailment: Generic applied modelling of languagevariability.
In PASCAL Workshop on Text Under-standing and Mining.de Salvo Braz, R., Girju, R., Punyaka-nok, V., Roth, D.,and Sammons, M. 2005.
An Inference Model for Se-mantic Entailment in Natural Language.
In Proc.
ofthe PASCAL RTE Challenge.Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink,B.
and Shi, Y.
2006.
Recognizing Textual Entailmentwith LCC?s GROUNDHOG System.
In Proc.
of thePASCAL RTE-2 Challenge.Klein, D. and Manning, C. 2003.
Accurate Unlexical-ized Parsing.
In Proc.
of ACL 2003.Kouylekov, M. and Magnini, B.
2006.
Tree Edit Dis-tance for Recognizing Textual Entailment: Estimat-ing the Cost of Insertion.
In Proc.
of the PASCALRTE-2 Challenge.Lin, D. 1998.
Dependency-based Evaluation of MINI-PAR.
In Workshop on the Evaluation of Parsing Sys-tems.Tatu, M., Iles, B., Slavik, J., Novischi, A. and Moldo-van, D. 2006.
COGEX at the Second RecognizingTextual Entailment Challenge.
In Proc.
of the PAS-CAL RTE-2 Challenge.Wang, R. and Neumann, G. 2007.
Recognizing TextualEntailment Using a Subsequence Kernel Method.
InProc.
of AAAI 2007.Witten, I. H. and Frank, E. Weka: Practical MachineLearning Tools and Techniques with Java Implemen-tations.
Morgan Kaufmann, 1999.Zanzotto, F.M.
and Moschitti, A.
2006.
AutomaticLearning of Textual Entailments with Cross-pairSimilarities.
In Proc.
of ACL 2006.41
