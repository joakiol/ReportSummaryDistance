Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 790?799, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Sequence Labelling Approach to Quote AttributionTim O?Keefe?
Silvia Pareti James R. Curran?
Irena Koprinska?
Matthew Honnibal??
e-lab, School of IT School of Informatics ?Centre for Language TechnologyUniversity of Sydney University of Edinburgh Macquarie UniversityNSW 2006, Australia United Kingdom NSW 2109, Australia{tokeefe,james,irena}@it.usyd.edu.au S.Pareti@sms.ed.ac.uk matthew.honnibal@mq.edu.auAbstractQuote extraction and attribution is the task ofautomatically extracting quotes from text andattributing each quote to its correct speaker.The present state-of-the-art system uses goldstandard information from previous decisionsin its features, which, when removed, resultsin a large drop in performance.
We treat theproblem as a sequence labelling task, whichallows us to incorporate sequence featureswithout using gold standard information.
Wepresent results on two new corpora and an aug-mented version of a third, achieving a newstate-of-the-art for systems using only realis-tic features.1 IntroductionNews stories are often driven by the quotes madeby politicians, sports stars, musicians, and celebri-ties.
When these stories exit the news cycle, thequotes they contain are often forgotten by both read-ers and journalists.
A system that automatically ex-tracts quotes and attributes those quotes to the cor-rect speaker would enable readers and journalists toplace news in the context of all comments made bya person on a given topic.Though quote attribution may appear to be astraightforward task, the simple rule-based ap-proaches proposed thus far have produced disap-pointing results.
Going beyond these to machinelearning approaches presents several problems thatmake quote attribution surprisingly difficult.
Themain challenge is that while a large portion of quotescan be attributed to a speaker based on simple rules,the remainder have few or no contextual clues asto who the correct speaker is.
Additionally, manyquote sequences, such as dialogues, rely on thereader understanding that there is an alternating se-quence of speakers, which creates dependencies be-tween attribution decisions made by a classifier.Elson and McKeown (2010) is the only study thatdirectly uses machine learning in quote attribution,treating the task as a classification task, where eachquote is attributed independently of other quotes.
Tohandle conversations and similar constructs they usegold standard information about speakers of previ-ous quotes as features for their model.
This is anunrealistic assumption, since gold standard informa-tion is not available in practice.The primary contribution of this paper is that wereformulate quote attribution as a sequence labellingtask.
This allows us to use sequence features with-out having to use the unrealistic gold standard fea-tures that were used in Elson and McKeown (2010).We experiment with three sequence decoding mod-els including greedy, Viterbi and a linear chain Con-ditional Random Field (CRF).Furthermore we present results on two new cor-pora and an augmented version of a third.
The twonew corpora are from news articles from the WallStreet Journal and the Sydney Morning Herald re-spectively, while the third corpus is an extension tothe classic literature corpus from Elson and McK-eown (2010).
Our results show that a quote attri-bution system using only realistic features is highlyfeasible for the news domain, with accuracies of92.4% on the SMH corpus and 84.1% on the WSJcorpus.7902 BackgroundEarly work into quote attribution by Zhang et al(2003) focused on identifying when different char-acters were talking in children?s stories, so that aspeech synthesis system could read the quoted partsin different voices.
While they were able to ex-tract quotes with high precision and recall, their at-tribution accuracy was highly dependent on the doc-ument in question, ranging from 47.6% to 86.7%.Mamede and Chaleira (2004) conducted similar re-search on children?s stories written in Portuguese.Their system proved to be very good at extractingquotes through simple rules, but when using a hand-crafted decision tree to attribute those quotes to aspeaker, they achieved an accuracy of only 65.7%.In the news domain, both Pouliquen et al2007)and Sarmento and Nunes (2009) proposed rule-based systems that work over large volumes of text.Both systems aimed for high precision at the ex-pense of low recall, as their data contained many re-dundant quotes.
More recently, SAPIENS, a French-language quote extraction and attribution system,was developed by de La Clergerie et al2011).
Itconducts a full parse of the text, which allows it touse patterns to extract direct and indirect quotes, aswell as the speaker of each quote.
Their evaluationfound that 19 out of 40 quotes (47.5%) had a correctspan and author, while a further 19 had an incorrectauthor, and 4 had an incorrect span.
In related work,Sagot et al2010) built a lexicon of French reportedspeech verbs, and conducted some analysis of dif-ferent types of quotes.Glass and Bangay (2007) approached the taskwith a three stage method.
For each quote theyfirst find the nearest speech verb, they then find thegrammatical actor of that speech verb, and finallythey select the appropriate speaker for that actor.
Toachieve each of these subtasks they built a modelwith several manually weighted features that goodcandidates should possess.
For each subtask theythen choose the candidate with the largest weightedsum of features.
Their full approach yields an ac-curacy of 79.4% on a corpus of manually annotatedfiction books.Schneider et al2010) describe PICTOR, whichis principally a quote visualisation tool.
Their taskwas to find direct and indirect quotes, which theyattribute to a text span representing the speaker.To do this they constructed a specialised grammar,which was built with reference to a small develop-ment corpus.
With a permissive evaluation metrictheir grammar-based approach yielded 86% recalland 75% precision, however this dropped to 52% re-call and 56% precision when measured in terms ofcompletely correct quote-speaker pairs.The work most similar to ours is the work by El-son and McKeown (2010).
Their aim was to au-tomatically identify both quotes and speakers, andthen to attribute each quote to a speaker, in a corpusof classic literature that they compiled themselves.To identify potential speakers they used the StanfordNER tagger (Finkel et al2005) and a method out-lined in Davis et al2003) that allowed them to findnominal character references.
They then groupedname variants and pronominal mentions into a coref-erence chain.To attribute a quote to a speaker they first classi-fied the quotes into categories.
Several of the cat-egories have a speaker explicit in their structure,so they attribute quotes to those speakers with nofurther processing.
For the remaining categories,they cast the attribution problem as a binary clas-sification task, where each quote-speaker pair hasa ?speaker?
or ?not speaker?
label predicted by theclassifier.
They then reconciled these independentdecisions using various techniques to produce a sin-gle speaker prediction for each quote.
For the sim-ple category predictions they achieved 93-99% ac-curacy, while for the more complicated categoriesthey achieved 63-64%, with an overall result of 83%accuracy.
This compares favourably with their rule-based baseline, which achieved an accuracy of 52%.While the results of Elson and McKeown (2010)appear encouraging, they are misleading for two rea-sons.
First their corpus does not include quoteswhere all three annotators chose different speakers.While these quotes include some cases where theannotators chose coreferent spans, it also includescases of legitimate disagreement about the speaker.An automated system would likely find these caseschallenging.
Second both their category predictionsand machine learning predictions rely on gold stan-dard information from previous quotes, which is notavailable in practice.
In our study we address boththese issues.791Proportion (%) Accuracy (%)LIT WSJ SMH LIT WSJ SMHQuote-Said-Person 17.9 20.2 3.1 98.9 99.8 99.1Quote-Person-Said 2.8 6.1 16.6 97.7 97.0 98.5Other Trigram 0.1 2.3 0.3 66.7 56.2 54.5Quote-Said-Pronoun 1.9 0.1 0.0 38.6 100.0 0.0Quote-Pronoun-Said 5.9 8.8 13.5 36.5 92.2 93.9Other Anaphors 0.1 0.1 0.2 0.0 100.0 62.5Added* 24.6 28.3 23.9 89.7 76.3 97.5Backoff 11.0 33.9 32.3 - - -Alone 18.0 0.2 9.7 - - -Conversation* 17.7 0.2 0.3 85.2 0.0 8.3Total 100.0 100.0 100.0 60.5 57.2 55.8Table 1: The proportion of quotes in each category and the accuracy of the speaker prediction based on the category.The two categories marked with an asterisk (*) depend on previous decisions.3 CorporaWe evaluate our methods on two new corpora com-ing from the news domain, and an augmented ver-sion of an existing corpus, which covers classic lit-erature.
They are described below.3.1 Columbia Quoted Speech AttributionCorpus (LIT)The first corpus we use was originally created byElson and McKeown (2010).
It is a set of excerptsfrom 11 fictional 19th century works by six well-known authors, split into 18 documents.
In total itcontains 3,126 quotes annotated with their speakers.Elson and McKeown used an automated systemto find named entity spans and nominal mentions inthe text, with the named entities being linked to forma coreference chain (they did not link nominal men-tions).
The corpus was built using Amazon?s Me-chanical Turk, with three annotations per quote.
Toensure quality, all annotations from poorly perform-ing annotators were removed, as were quotes whereeach annotator chose a different speaker.
Thoughexcluding some quotes ensures quality annotations,it causes gaps in the quote chains, which is a prob-lem for sequence labelling.
Furthermore, the caseswhere annotators disagreed are likely to be challeng-ing, so removing them from the corpus could makeresults appear better than they would be in practice.To rectify this, we conducted additional annota-tion of the quotes that were excluded by the origi-nal authors.
Two postgraduates annotated 654 addi-tional quotes, with a raw agreement of 79% over 48double-annotated quotes.
Our annotators reportedseeing some errors in existing annotations, so wehad one annotator check 400 existing annotations forcorrectness.
This additional check found that 92.5%of the quotes were correctly annotated.3.2 PDTB Attribution Corpus Extension (WSJ)Our next corpus is an extension to the attributionannotations found in the Penn Discourse TreeBank(PDTB).
The original PDTB contains several formsof discourse, including assertions, beliefs, facts, andeventualities.
These can be attributed to named enti-ties or to unnamed, pronominal, or implicit sources.Recent work by Pareti (2012) conducted further an-notation of this corpus, including reconstructing at-tributions that were only partially annotated, and in-troducing additional information.
From this corpuswe use only direct quotes and the directly quotedportions of mixed quotes, giving us 4,923 quotes.For the set of potential speakers we use theBBN pronoun coreference and entity type cor-pus (Weischedel and Brunstein, 2005), with auto-matically coreferred pronouns.
We automaticallymatched BBN entities to PDTB extension speakers,and included the PDTB speaker where no matchingBBN entity could be found.
This means an automaticsystem has an opportunity to find the correct speakerfor all quotes in the corpus.7923.3 Sydney Morning Herald Corpus (SMH)We compiled the final corpus from a set of newsdocuments taken from the Sydney Morning Her-ald website1.
We randomly selected 965 documentspublished in 2009 that were not obituaries, opin-ion pages, advertisements or other non-news sto-ries.
To conduct the annotation we employed 11non-expert annotators via the outsourcing site Free-lancer2, as well as five expert annotators from ourresearch group.
A total of 400 news stories weredouble-annotated, with at least 33 double-annotatedstories per annotator.
Raw agreement on the speakerof each quote was high at 98.3%.
These documentshad already been annotated with named entities aspart of a separate research project (Hachey et al2012), which includes manually constructed coref-erence chains.
The resulting corpus contains 965documents, with 3,535 quotes.3.4 Corpus ComparisonsIn order to compare the corpora we categorise thequotes into the categories defined by Elson andMcKeown (2010), as shown in Table 1.
We assignedquotes to these categories by testing (after text pre-processing) whether the quote belonged to each cat-egory, in the order shown below:1.
Trigram ?
the quote appears consecutively witha mention of an entity, and a reported speechverb, in any order;2.
Anaphors ?
same as above, except that the men-tion is a pronoun;3.
Added ?
the quote is in the same paragraph asanother quote that precedes it;4.
Conversation ?
the quote appears in a para-graph on its own, and the two paragraphs pre-ceding the current paragraph each contain a sin-gle quote, with alternating speakers;5.
Alone ?
the quote is in a paragraph on its own;6.
Miscellaneous ?
the quote matches none of thepreceding categories.
This category is called?Backoff?
in Elson and McKeown (2010).1http://www.smh.com.au2http://www.freelancer.comUnsurprisingly, the two corpora from the news do-main share similar proportions of quotes in eachcategory.
The main differences are that the SMHuses a larger number of pronouns compared to theWSJ, which tends to use explicit attribution more fre-quently.
The SMH also has a significant proportionof quotes that appear alone in a paragraph, whilethe WSJ has almost none.
Finally, when attribut-ing a quote using a trigram pattern, the SMH mostlyuses the Quote-Person-Said pattern, while the WSJmostly uses the Quote-Said-Person pattern.
Thesedifferences probably reflect the editorial guidelinesof the two newspapers.The differences between the news corpora andthe literature corpus are more substantial.
Most no-tably the LIT corpus has a much higher proportionof quotes that fall into the Conversation and Alonecategories.
This is unsurprising as both monologuesand dialogues are common in fiction, but are rare innewswire.
The two news corpora have more quotesin the Trigram and Backoff categories.4 Quote ExtractionQuote extraction is the task of finding the spans thatrepresent quotes within a document.
There are threetypes of quotes that can appear:1.
Direct quotes appear entirely between quota-tion marks, and are used to indicate that thespeaker said precisely what is written;2.
Indirect quotes do not appear between or con-tain quotation marks, and are used to get thespeaker?s point across without implying thatthe speaker used the exact words of the quote;3.
Mixed quotes are indirect quotes that contain adirectly quoted portion.In this work, we limit ourselves to detecting directquotes and the direct portions of mixed quotes.To extract quotes we use a regular expression thatsearches for text between quotation marks.
We alsodeal with the special case of multi-paragraph quoteswhere one quotation mark opens the quote and everynew paragraph that forms part of the quote, with a fi-nal quotation mark only at the very end of the quote.This straightforward approach yields over 99% ac-curacy on all three corpora.7935 Quote AttributionGiven a document with a set of quotes and a setof entities, quote attribution is the task of findingthe entity that represents the speaker of each quote,based on the context provided by the document.Identifying the correct entity can involve choosingeither an entire coreference chain representing anentity, or identifying a specific span of text that rep-resents the entity.In practice, most applications only need to knowwhich coreference chain represents the speaker, notwhich particular span in the text.
Despite this, thebest evidence about which chain is the speaker isfound in the context of the individual text spans, andmost existing systems aim to get the particular entityspan correct.
This presents a problem for evaluation,as an incorrect entity span may be identified, but itmight still be part of the correct coreference chain.We chose to count attributions as correct if they at-tributed the quote to the correct coreference chainfor both the LIT and SMH corpora, while for the WSJcorpus, where the full coreference chains do not ex-ist, we evaluated an attribution as correct if it was tothe correct entity span in the text.5.1 Rule-based BaselineTo establish the effectiveness of our method we builta rule-based baseline system.
For each quote it pro-ceeds with the following steps:1.
Search backwards in the text from the end ofthe sentence the quote appears in for a reportedspeech verb2.
If the verb is found return the entity mentionnearest the verb (ignoring mentions in quotes),in the current sentence or any sentence preced-ing it3.
If not, return the mention of an entity near-est the end of the quote (ignoring mentions inquotes), in the current sentence or any sentencepreceding itThis forms a reasonable baseline as it is able to pickup the quotes that fall into the more simple cate-gories, such as the Trigram category and the Addedcategory.
It is also able to make a guess at the morecomplicated categories, without using gold standardinformation as the category predictions do.6 Experimental SetupWe use two classifiers: a logistic regression imple-mentation available in LIBLINEAR (Fan et al2008),and a Conditional Random Field (CRF) from CRF-Suite (Okazaki, 2007).
Both packages use maxi-mum likelihood estimation with L2 regularisation.We experimented with several values for the coef-ficient on a development set, but found that it hadlittle impact, so stuck with the default value.
All ofour machine learning experiments use the same textencoding, which is explained below, and all use thecategory predictions when they are available.6.1 Text EncodingWe encode our text similarly to Elson and McKeown(2010).
The major steps are:1.
Replace all quotes and speakers with specialsymbols;2.
Replace all reported speech verbs with a sym-bol.
Elson and McKeown (2010) provided uswith their list of reported speech verbs;3.
Part-of-Speech (POS) tag the text and removeadjectives, adverbs, and other parts of speechthat do not contribute useful information.
Weused the POS tagger from Curran and Clark(2003);4.
Remove any paragraphs or sentences where noquotes, pronouns or names occur.All features that will be discussed are calculatedwith respect to this encoding (e.g.
word distancewould be the number of words in the encoded text,rather than the number of words in the original text).6.2 FeaturesIn our experiments we use the feature set from Elsonand McKeown (2010).
The features for a particu-lar pair of target quote (q) and target speaker (s) aresummarised below.Distance features including number of words be-tween q and s, number of paragraphs betweenq and s, number of quotes between q and s, andnumber of entity mentions between q and s794CorpusSequence FeaturesGold Pred NoneLIT 74.7 49.0 49.6WSJ 87.3 74.1 82.9SMH 95.0 85.6 92.4Table 2: Accuracy results comparing the E&M approachwith gold standard, predicted or no sequence features.Paragraph features derived from the 10 para-graphs preceding the quote (including the para-graph the quote is in), includes number of men-tions of s, number of mentions of other speak-ers, number of words in each paragraph, andnumber of quotes in each paragraphNearby features relating to the two tokens eitherside of q and s, includes binary features foreach position indicating whether the position ispunctuation, s, q, a different speaker, a differ-ent quote, or a reported speech verbQuote features about q itself, including whether sis mentioned within it, whether other speakersare mentioned within it, how far the quote isfrom the start of its paragraph and the length inwords of qSequence features that depend on the speakerschosen for the previous quotes, includes num-ber of quotes in the 10 paragraphs precedingand including the paragraph where q appearsthat were attributed to s, and the number thatwere attributed to other speakers6.3 Elson and McKeown ReimplementationAs part of our study we reproduce the core resultsof Elson and McKeown (2010) (E&M ), as we be-lieve it is a state-of-the-art system.
This allows usto determine the effectiveness of our approach whencompared to a state-of-the-art approach, and it alsoallows us to determine how well the E&M approachperforms on other corpora.
In this section we willbriefly summarise the key elements needed to repro-duce their work.The E&M approach makes a binary classificationbetween ?speaker?
and ?not speaker?
for up to 15candidate speakers for each quote.
They then recon-cile these 15 classifications into one speaker predic-tion for the quote.
While E&M experimented withseveral different reconciliation methods, we simplychose the speaker with the highest probability at-tached to its ?speaker?
label.We conducted an experiment using our imple-mentation of the E&M method on the original,unaugmented E&M corpus, to see how our resultcompared with E&M ?s 83%.
On our test set weachieved 78.2%, however this rose to 82.3% whenperforming 10-fold cross validation across the wholecorpus.
Though this is a large difference, it is notnecessarily that surprising, as our test set containsdocuments by authors which are unseen, whereasboth the original E&M test set and all the cross val-idation test sets contain documents by authors thatthe learner has seen before.In their work, E&M make a simplifying assump-tion that all previous attribution decisions were cor-rect.
Due to this, their sequence features use goldstandard labels from previous quotes, which makestheir results unrealistic.
In Table 2 we show the ef-fect of replacing the gold standard sequence featureswith features based on the predicted labels, or withno sequence features at all.
All three corpora show asignificant drop in accuracy, with the LIT corpus inparticular suffering a drop of more than 25%.
Thismotivates our study into including sequence infor-mation without using gold standard labels.7 Class ModelsWe consider two class models for our experiments,which are described in detail below.
The binarymodel is able to take advantage of more data but hasless competition between decisions, while the n-waymodel has more competition with less data.
Bothmodels are used with all the decoding methods, withthe exception that the binary model is unsuitable forthe CRF experiments.7.1 BinaryWhen working with n previous speakers, a binaryclass model works by predicting n independent?speaker?
versus ?not speaker?
labels, one for eachquote-speaker pair.
As the classifications are inde-pendent the n decisions need to be reconciled, asmore than one speaker might be predicted.
We rec-oncile the n decisions by attributing the quote to the795speaker with the highest ?speaker?
probability.
Us-ing a binary class with reconciliation in a greedydecoding model is equivalent to the method in El-son and McKeown (2010), except that the gold stan-dard sequence features are replaced with predictedsequence features.7.2 n-wayA key advantage of the binary class model is thatwhen predicting ?speaker?
versus ?not speaker?
theclassifier only needs to predict one probability, andthus can take into account the evidence of all otherquote-speaker pairs.
The drawback to the binarymodel is that the probabilities assigned to the can-didate speakers do not need to directly competeagainst each other.
In other words when assigninga binary probability to a candidate speaker, the clas-sifier does not take into account how good the othercandidate speakers are.To rectify these issues we experiment with a sin-gle classification for each quote, where the classifierdirectly decides between up to n candidate speakersper quote.
As speaker-specific evidence is far toosparse, we encode the speakers with their ordinal po-sition backwards from the quote.
In other words, thecandidate speaker immediately preceding the quotewould be labelled ?speaker1?, the speaker preced-ing it would be ?speaker2?
and so on.
The classifierthen directly predicts these labels.
This representa-tion means that candidate speakers need to directlycompete for probability mass, although it has thedrawback that the evidence for the higher-numberedspeakers is quite sparse.The features we use for this representation aresimilar to the features used in the E&M binarymodel.
The key difference is that where there wereindividual features that were calculated with respectto the speaker, there are now n features, one for eachof the speaker candidates.
This allows the model toaccount for the strength of other candidates when as-signing a speaker label.8 Sequence DecodingWe noted in the previous section that the E&M re-sults are based on the unrealistic assumption thatall previous quotes were attributed correctly.
Inthis section we outline three sequence decoding ap-proaches that remove this unrealistic assumption,without removing all of the transition informationthat it provides.
We believe the transition infor-mation is important as many quotes have no ex-plicit attribution in the text, and instead rely on thereader understanding something about the sequenceof speakers.For these experiments we regard the set of speakerattributions in a document as the sequence that wewant to decode.
Each individual state therefore rep-resents a sequence of w previous attribution deci-sions, and a decision for the current quote.
Obtain-ing a probability for this state can be done in oneof two ways.
Either the transition probabilities fromstate to state can be learned explicitly, or the w pre-vious attribution decisions can be used to build thesequence features for the current state, which im-plicitly encodes the transition probabilities.8.1 Greedy DecodingIn sequence decoding the greedy algorithm calcu-lates the probability of each label at a decision pointbased on the predictions it has already made for pre-vious decisions.
More concretely this means we ap-ply a standard classifier at each step, with the se-quence features being calculated from the predic-tions made in previous steps.
Greedy decoding isefficient in that it only considers one possible historyat each decision point, but it is consequently unableto make trade-offs between good previous choicesand good current choices, which means that in gen-eral it will not return the optimum sequence of la-bels.
As greedy decoding is an efficient algorithmwe do not restrict w, the number of previous deci-sions, beyond the 10 paragraph restriction that is al-ready in place.8.2 Viterbi DecodingViterbi decoding finds the most probable paththrough a sequence of decisions.
It does this by de-termining the probabilities of each of the labels atthe current decision point, with each of the possi-ble histories of decisions within a given window w.These probabilities can be multiplied together withthe previous decisions to retrieve a joint probabilityfor the entire sequence.
The final decision for eachquote is then just the speaker which is predicted bythe sequence with the largest joint probability.796Although they do not come with probabilities,we chose to include the category predictions in ourViterbi model.
As we already know that they areaccurate indicators of the speaker we assign thema probability of 100%, which effectively forces theViterbi decoder to choose the category predictionswhen they are available.
It is worth noting thatquotes are only assigned to the Conversation cate-gory if the two prior quotes had alternating speakers.As such, during the Viterbi decoding the categori-sation of the quote actually needs to be recalculatedwith regard to the two previous attribution decisions.By forcing the Viterbi decoder to choose categorypredictions when they are available, we get the ad-vantage that quote sequences with no interveningtext may be forced into the Conversation category,which is typically under-represented otherwise.Both the sequences using the binary class andthe n-way class can be decoded using the Viterbialgorithm, so we experiment with both class mod-els.
We also experiment with varying window sizes(w), in order to gain insight into how many previousdecisions impact the current decision.
Though theViterbi algorithm is able to find the best sequenceof probabilities without the need for an exhaustivesearch, it can still take an impractical amount of timeto run.
As such we ignore all but the 10 most promis-ing sequences at each decision point.8.3 Conditional Random Field (CRF) DecodingThe key drawback with the logistic regression ex-periments described thus far is that the sequencefeatures are trained with gold standard information.This means that during the training phase the se-quence features have perfect information about pre-vious speakers and are thus unrealistically good pre-dictors of the final outcome.
When the resultingmodel is used with the less accurate predicted se-quence features, it is overconfident about the infor-mation those features provide.We account for this by using a first-order linearchain CRF model, which learns the probabilities ofprogressing from speaker to speaker more directly.During training the CRF is able to learn the asso-ciation between features and labels, as well as thechance of transitioning from one label to the next.It also has the advantage of avoiding the label biasproblem that would be present in the equivalent Hid-den Markov Model (Lafferty et al2001).Though the n-way class model can be used di-rectly in a CRF, the binary class model is more chal-lenging.
The main problem is that the ?speaker?versus ?not speaker?
output of the binary classifierdoes not directly form a meaningful sequence thatthe CRF can learn over.
If the reconciliation step isincluded it effectively adds an extra layer to the lin-ear chain, making learning more difficult.
Due tothese difficulties we only use the n-way class modelin our CRF experiments.9 ResultsThe main result of our experiments with the E&Mmethod is the large drop in accuracy that occurswhen the gold standard sequence features are re-moved, which can be seen in Table 3.
When usingthe binary class model this results in a drop of 25.1%for the LIT corpus, while for the WSJ and SMH cor-pora the drop is less substantial at 4.4% and 2.6%,respectively.
For the LIT corpus the drop is so severethat it actually performs worse than the simple rule-based system.
Even more surprisingly, when thepredictions from previous decisions are used with asimple greedy decoder, the accuracy drops even fur-ther for all three corpora.
This indicates that the clas-sifier is putting too much weight on the gold stan-dard sequence features during training, and is mis-led into making poor decisions when the predictedfeatures are used during test time.Table 4 shows the results for the n-way classmodel.
Compared to the binary model, the n-wayclass model generally produced lower results, al-though the results were more stable to changes inparameters and decoders.
The only corpus that pro-duced better results with the n-way class model wasthe WSJ corpus, which does not have full entitycoreference information.
This indicates that the n-way model may be helpful when there is more vari-ety in the choice of entities.The final results we would like to discuss here arethe CRF results.
On all three corpora the CRF resultsare underwhelming.
The major issue that we cansee when applying a CRF model to this task is thatthe sequences that it needs to learn over are entiredocuments.
This means that for the LIT corpus thetraining set consisted of only 12 sequences, while797Corpus E&M Rule No seq.
Greedy Viterbiw = 1 w = 2 w = 5LIT 74.7 53.3 49.6 49.0 46.0 49.8 45.9WSJ 87.3 77.9 82.9 74.1 82.3 83.1 83.1SMH 95.0 91.2 92.4 85.6 91.7 90.5 84.1Table 3: Accuracy on test set with the binary class model.
Italicised results indicate gold standard information is used.Bold results show the best realistic result for each corpus.Corpus Gold seq.
Rule No seq.
Greedy Viterbi CRFw = 1 w = 2 w = 5LIT 68.6 53.3 47.1 46.7 42.5 46.5 44.4 48.6WSJ 88.9 77.9 83.6 77.0 84.1 83.7 83.3 79.6SMH 94.4 91.2 90.0 89.6 89.5 90.1 90.4 91.0Table 4: Accuracy on test set with the n-way class model.
Italicised results indicate gold standard information is used.Bold results show the best realistic result for each corpus.the test set consisted of 6 sequences.
With so fewsequences it is unsurprising that the CRF model didnot perform well.
The limited range of the first orderlinear chain model could also have played a part inthe poor performance of the CRF models.
However,moving to a higher-order model is problematic asthe number of transition probabilities that need to becalculated increases exponentially with the order ofthe model.10 ConclusionIn this paper, we present the first large-scale evalua-tion of a quote attribution system on newswire fromthe 1989 Wall Street Journal (WSJ) and the 2009Sydney Morning Herald (SMH), as well as compar-ing against previous work (Elson and McKeown,2010) on 19th-century literature.We show that when Elson and McKeown?s unre-alistic use of gold-standard history information is re-moved, accuracy on all three corpora drops substan-tially.
We demonstrate that by treating quote attribu-tion as a sequence labelling task, we can achieve re-sults that are very close to their results on newswire,though not for literature.In future work, we intend to further explore thesequence features that have a large impact on accu-racy, and to find similar features or proxies for thesequence features that would be beneficial.
We willalso explore other approaches to representing quoteattribution with a CRF.
For the task more broadly,it would be beneficial to compare methods of find-ing indirect and mixed quotes, and to evaluate howwell quote attribution performs on those quotes asopposed to just direct quotes.Our newswire results, 92.4% for the SMH and84.1% for the WSJ corpus, demonstrate it is possibleto develop an accurate and practical quote extractionsystem.
On the LIT corpus our best result was fromthe simple rule-based system, which yielded 53.3%.It is clear that literature poses an ongoing researchchallenge.AcknowledgementsWe would like to thank David Elson for helpingus to reimplement his method and Bonnie Webberfor her feedback and assistance.
O?Keefe has beensupported by a University of Sydney Merit schol-arship and a Capital Markets CRC top-up scholar-ship; Pareti has been supported by a Scottish Infor-matics and Computer Science Alliance (SICSA) stu-dentship.
This work has been supported by ARCDiscovery grant DP1097291 and the Capital Mar-kets CRC Computable News project.ReferencesJames R. Curran and Stephen Clark.
2003.
Investi-gating GIS and smoothing for maximum entropytaggers.
In Proceedings of the tenth conference on798European chapter of the Association for Compu-tational Linguistics, pages 91?98.Peter T. Davis, David K. Elson, and Judith L. Kla-vans.
2003.
Methods for precise named entitymatching in digital collections.
In Proceedings ofthe 3rd ACM/IEEE-CS Joint Conference on Digi-tal libraries, pages 125?127.Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-cal Denis, Gaelle Recource, and Victor Mignot.2011.
Extracting and visualizing quotations fromnews wires.
Human Language Technology.
Chal-lenges for Computer Science and Linguistics,pages 522?532.David.
K Elson and Kathleen.
R McKeown.
2010.Automatic attribution of quoted speech in literarynarrative.
In Proceedings of AAAI, pages 1013?1019.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIB-LINEAR: A library for large linear classification.Journal of Machine Learning Research, 9:1871?1874.Jenny Rose Finkel, Trond Grenager, and Christo-pher Manning.
2005.
Incorporating non-local in-formation into information extraction systems bygibbs sampling.
In Proceedings of the 43rd An-nual Meeting on Association for ComputationalLinguistics, pages 363?370.Kevin Glass and Shaun Bangay.
2007.
A naivesalience-based method for speaker identificationin fiction books.
In Proceedings of the 18th An-nual Symposium of the Pattern Recognition Asso-ciation of South Africa (PRASA07), pages 1?6.Ben Hachey, Will Radford, Joel Nothman, MatthewHonnibal, and James R. Curran.
2012.
Evaluatingentity linking with Wikipedia.
Artificial Intelli-gence.
(in press).John Lafferty, Andrew McCallum, and FernandoC.N.
Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
International Conference on Ma-chine Learning, pages 282?289.Nuno Mamede and Pedro Chaleira.
2004.
Charac-ter identification in children stories.
Advances inNatural Language Processing, pages 82?90.Naoaki Okazaki.
2007.
CRFsuite: a fast im-plementation of Conditional Random Fields(CRFs).
URL http://www.chokkan.org/software/crfsuite/.Silvia Pareti.
2012.
A database of attribution rela-tions.
In Proceedings of the Eight InternationalConference on Language Resources and Evalua-tion (LREC?12), pages 3213?3217.Bruno Pouliquen, Ralf Steinberger, and Clive Best.2007.
Automatic detection of quotations in multi-lingual news.
In Proceedings of Recent Advancesin Natural Language Processing, pages 487?492.Beno?
?t Sagot, Laurence Danlos, and Rosa Stern.2010.
A lexicon of french quotation verbs for au-tomatic quotation extraction.
In 7th internationalconference on Language Resources and Evalua-tion - LREC 2010.Luis Sarmento and Sergio Nunes.
2009.
Automaticextraction of quotes and topics from news feeds.In 4th Doctoral Symposium on Informatics Engi-neering.Nathan Schneider, Rebecca Hwa, Philip Gianfor-toni, Dipanjan Das, Michael Heilman, Alan W.Black, Frederik L. Crabbe, and Noah A. Smith.2010.
Visualizing topical quotations over timeto understand news discourse.
Technical ReportCMU-LTI-01-013, Carnegie Mellon University.Ralph Weischedel and Ada Brunstein.
2005.
BBNpronoun coreference and entity type corpus.
Lin-guistic Data Consortium, Philadelphia.Jason Zhang, Alan Black, and Richard Sproat.2003.
Identifying speakers in children?s storiesfor speech synthesis.
In Proceedings of EU-ROSPEECH, pages 2041?2044.799
