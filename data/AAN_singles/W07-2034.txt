Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 165?168,Prague, June 2007. c?2007 Association for Computational LinguisticsHIT-IR-WSD: A WSD System for English Lexical Sample TaskYuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang and Ting LiuInformation Retrieval LabHarbin Institute of technologyHarbin, China, 150001{yhguo,wxche}@ir.hit.edu.cnAbstractHIT-IR-WSD is a word sense disambigua-tion (WSD) system developed for Englishlexical sample task (Task 11) of Semeval2007 by Information Retrieval Lab, HarbinInstitute of Technology.
The system isbased on a supervised method using anSVM classifier.
Multi-resources includingwords in the surrounding context, the part-of-speech of neighboring words, colloca-tions and syntactic relations are used.
Thefinal micro-avg raw score achieves 81.9%on the test set, the best one among partici-pating runs.1 IntroductionLexical sample task is a kind of WSD evaluationtask providing training and test data in which asmall pre-selected set of target words is chosen andthe target words are marked up.
In the training datathe target words?
senses are given, but in the testdata are not and need to be predicted by task par-ticipants.HIT-IR-WSD regards the lexical sample taskas a classification problem, and devotes to extracteffective features from the instances.
We didn?t useany additional training data besides the officialones the task organizers provided.
Section 2 givesthe architecture of this system.
As the task pro-vides correct word sense for each instance, a su-pervised learning approach is used.
In this system,we choose Support Vector Machine (SVM) asclassifier.
SVM is introduced in section 3.
Know-ledge sources are presented in section 4.
The lastsection discusses the experimental results andpresent the main conclusion of the work performed.2 The Architecture of the SystemHIT-IR-WSD system consists of 2 parts: featureextraction and classification.
Figure 1 portrays thearchitecture of the system.Figure?1:?The?architecture?of?HIT?IR?WSD?165Features are extracted from original instancesand are made into digitized features to feed theSVM classifier.
The classifier gets the features oftraining data to make a model of the target word.Then it uses the model to predict the sense of targetword in the test data.3 Learning AlgorithmSVM is an effective learning algorithm to WSD(Lee and Ng, 2002).
The SVM tries to find ahyperplane with the largest margin separating thetraining samples into two classes.
The instances inthe same side of the hyperplane have the sameclass label.
A test instance?s feature decides theposition where the sample is in the feature spaceand which side of the hyperplane it is.
In this way,it leads to get a prediction.
SVM could be extendedto tackle multi-classes problems by using one-against-one or one-against-rest strategy.In the WSD problem, input of SVM is the fea-ture vector of the instance.
Features that appear inall the training samples are arranged as a vectorspace.
Every instance is mapped to a feature vector.If the feature of a certain dimension exists in asample, assign this dimension 1 to this sample, elseassign it 0.
For example, assume the feature vectorspace is <x1, x2, x3, x4, x5, x6, x7>; the instance is?x2 x6 x5 x7?.
The feature vector of this sampleshould be <0, 1, 0, 0, 1, 1, 1>.The implementation of SVM here is libsvm 1(Chang and Lin, 2001) for multi-classes.4 Knowledge SourcesWe used 4 kinds of features of the target word andits context as shown in Table 1.Part of the original text of an example is ?
?This is the <head>age</head> of new media , theera of ?
?.Name Extraction Tools ExampleSurroundingwordsWordNet(morph)2?, this, be, age, new,medium, ,, era, ?Part-of-speech SVMTool3DT_0, VBZ_0, DT_0,NN_t, IN_1, JJ_1,NNS_11?http://www.csie.ntu.edu.tw/~cjlin/libsvm/?2?http://wordnet.princeton.edu/man/morph.3WN.html?3?http://www.lsi.upc.es/~nlp/SVMTool/?Collocationthis_0, be_0, the_0,age_t, of_1, new_1,medium_1, ,_1, the_1Syntacticrelation MaltParser4SYN_HEAD_isSYN_HEADPOS_VBZSYN_RELATION_PRDSYN_HEADRIGHTTable?1:?Features?the?system?extracted?The next 4 subsections elaborate these features.4.1 Words in the Surrounding ContextWe take the neighboring words in the context ofthe target word as a kind of features ignoring theirexact position information, which is called bag-of-words approach.Mostly, a certain sense of a word is tend to ap-pear in a certain kind of context, so the contextwords could contain some helpful information todisambiguate the sense of the target word.Because there would be too many context wordsto be added into the feature vector space, datasparseness problem is inevitable.
We need to re-duce the sparseness as possible as we can.
A sim-ple way is to use the words?
morphological rootforms.
In addition, we filter the tokens which con-tain no alphabet character (including punctuationsymbols) and stop words.
The stop words aretested separately, and only the effective oneswould be added into the stop words list.
All re-maining words in the instance are gathered, con-verted to lower case and replaced by their morpho-logical root forms.
The implementation for gettingthe morphological root forms is WordNet (morph).4.2 Part-of-Speechs of Neighboring WordsAs mentioned above, the data sparseness is a se-rious problem in WSD.
Besides changing tokens totheir morphological root forms, part-of-speech is agood choice too.
The size of POS tag set is muchsmaller than the size of surrounding words set.And the neighboring words?
part-of-speeches alsocontain useful information for WSD.
In this part,we use a POS tagger (Gim?nez and M?rquez, 2004)to assign POS tags to those tokens.We get the left and right 3 words?
POS tags to-gether with their position information in the targetwords?
sentence.For example, the word age is to be disambi-guated in the sentence of ??
This is the4?http://w3.msi.vxu.se/~nivre/research/MaltParser.html?166<head>age</head> of new media , the era of ?
?.The features then will be added to the feature vec-tor are ?DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1,NNS_1?, in which _0/_1 stands for the word withcurrent POS tag is in the left/right side of the targetword.
The POS tag set in use here is Penn Tree-bank Tagset5.4.3 CollocationsDifferent from bag-of-words, collocation featurecontains the position information of the targetwords?
neighboring words.
To make this feature inthe same form with the bag-of-words, we appendeda symbol to each of the neighboring words?
mor-phological root forms to mark whether this word isin the left or in the right of the target word.
LikePOS feature, collocation was extracted in the sen-tence where the target word belongs to.
The win-dow size of this feature is 5 to the left and 5 to theright of the target word, which is attained by em-pirical value.
In this part, punctuation symbol andstop words are not removed.Take the same instance last subsection has men-tioned as example.
The features we extracted are?this_0, be_0, the_0, age_t, of_1, new_1, me-dium_1?.
Like POS, _0/_1 stands for the word isin the left/right side of the target word.
Then thefeatures were added to the feature vector space.4.4 Syntactic RelationsMany effective context words are not in a shortdistance to the target word, but we shouldn?t en-large the window size too much in case of includ-ing too many noises.
A solution to this problem isto use the syntactic relations of the target word andits parent head word.We use Nivre et al, (2006)?s dependency parser.In this part, we get 4 features from every instance:head word of the target word, the head word?s POS,the head word?s dependency relation with the tar-get word and the relative position of the head wordto the target word.Still take the same instance which has beenmentioned in the las subsection as example.
Thefeatures we extracted are ?SYN_HEAD_is,SYN_HEADPOS_VBZ, SYN_RELATION_PRD,SYN_HEADRIGHT?, in which SYN_HEAD_isstands for is is the head word of age;SYN_HEADPOS_VBZ stands for the POS of the5?http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html?head word is is VBZ; SYN_RELATION_PRDstands for the relationship between the head wordis and target word age is PRD; andSYN_HEADRIGHT stands for the target word ageis in the right side of the head word is.5 Data Set and ResultsThis English lexical sample task: Semeval 2007task 116 provides two tracks of the data set for par-ticipants.
The first one is from LDC and the secondfrom web.We took part in this evaluation in the secondtrack.
The corpus is from web.
In this track the taskorganizers provide a training data and test data setfor 20 nouns and 20 adjectives.In order to develop our system, we divided thetraining data into 2 parts: training and developmentsets.
The size of the training set is about 2 times ofthe development set.
The development set contains1,781 instances.4 kinds of features were merged into 15 combi-nations.
Here we use a vector (V) to express whichfeatures are used.
The four dimensions stand forsyntactic relations, POS, surrounding words andcollocations, respectively.
For example, 1010means that the syntactic relations feature and thesurrounding words feature are used.V Precision V Precision0001 78.6% 1001 78.2%0010 80.3% 1010 81.9%0011 82.0% 1011 82.8%0100 70.4% 1100 73.3%0101 79.0% 1101 79.1%0110 82.1% 1110 82.5%0111 82.9% 1111 82.9%1000 72.6%Table?2:?Results?of?Combinations?of?Features?From Table 2, we can conclude that the sur-rounding words feature is the most useful kind offeatures.
It obtains much better performance thanother kinds of features individually.
In other words,without it, the performance drops a lot.
Amongthese features, syntactic relations feature is themost unstable one (the improvement with it is un-stable), partly because the performance of the de-pendency parser is not good enough.
As the oneswith the vector 0111 and 1111 get the best perfor-6http://nlp.cs.swarthmore.edu/semeval/tasks/task11/description.shtml?167mance, we chose all of these kinds of features forour final system.A trade-off parameter C in SVM is tuned, andthe result is shown in Figure 2.
We have also tried4 types of kernels of the SVM classifier (parame-ters are set by default).
The experimental resultsshow that the linear kernel is the most effective asTable 3 shows.Figure?2:?Accuracy?with?different?C?parameters?KernelFunctionTypeLinear Poly-nomial RBFSig-moidAccuracy 82.9% 68.3% 68.3% 68.3%Table?3:?Accuracy?with?different?kernel?function?types?Another experiment (as shown in Figure 3) alsovalidate that the linear kernel is the most suitableone.
We tried using polynomial function.
Unlikethe parameters set by default above (g=1/k, d=3),here we set its Gama parameter as 1 (g=1) but oth-er parameters excepting degree parameter are stillset by default.
The performance gets better whenthe degree parameter is tuned towards 1.
Thatmeans the closer the kernel function to linear func-tion the better the system performs.Figure?3:?Accuracy?with?different?degree?
in?po?lynomial?function?In order to get the relation between the systemperformance and the size of training data, we madeseveral groups of training-test data set from thetraining data the organizers provided.
Each of themhas the same test data but different size of trainingdata which are 2, 3, 4 and 5 times of the test datarespectively.
Figure 4 shows the performancecurve with the training data size.
Indicated in Fig-ure 4, the accuracy increases as the size of trainingdata enlarge, from which we can infer that wecould raise the performance by using more trainingdata potentially.Figure?4:?Accuracy?s?trend?with?the?training?da?ta?size?Feature extraction is the most time-consumingpart of the system, especially POS tagging andparsing which take 2 hours approximately on thetraining and test data.
The classification part (usinglibsvm) takes no more than 5 minutes on the train-ing and test data.
We did our experiment on a PCwith 2.0GHz CPU and 960 MB system memory.Our official result of HIT-IR-WSD is: micro-avg raw score 81.9% on the test set, the top oneamong the participating runs.AcknowledgementWe gratefully acknowledge the support for thisstudy provided by the National Natural ScienceFoundation of China (NSFC) via grant 60435020,60575042, 60575042 and 60675034.ReferencesLee, Y. K., and Ng, H. T. 2002.
An empirical evaluationof knowledge sources and learning algorithms forword sense disambiguation.
In Proceedings ofEMNLP02, 41?48.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.Jes?s Gim?nez and Llu?s M?rquez.
2004.
SVMTool: Ageneral POS tagger generator based on Support Vec-tor Machines.
Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC'04).
Lisbon, Portugal.Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S.2006.
Labeled Pseudo-Projective Dependency Pars-ing with Support Vector Machines.
In Proceedings ofthe Tenth Conference on Computational NaturalLanguage Learning (CoNLL).168
