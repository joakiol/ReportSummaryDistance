Learning to Identify Single-Snippet Answers to Definition QuestionsSpyridoula MILIARAKI and Ion ANDROUTSOPOULOSDepartment of Informatics, Athens University of Economics and BusinessPatission 76, GR-104 34, Athens, GreeceAbstractWe present a learning-based method to identifysingle-snippet answers to definition questions inquestion answering systems for documentcollections.
Our method combines and extends twoprevious techniques that were based mostly onmanually crafted lexical patterns and WordNethypernyms.
We train a Support Vector Machine(SVM) on vectors comprising the verdicts orattributes of the previous techniques, and additionalphrasal attributes that we acquire automatically.The SVM is then used to identify and rank single250-character snippets that contain answers todefinition questions.
Experimental results indicatethat our method clearly outperforms the techniquesit builds upon.1 IntroductionSince the introduction of the TREC QA track(Voorhees, 2001), question answering systems fordocument collections have attracted a lot ofattention.
The goal is to return from the collectiontext snippets (eg., 50 or 250 characters long) orexact answers (e.g., names, dates) that answernatural language questions submitted by users.A typical system first classifies the question intoone of several categories (questions asking forlocations, persons, etc.
), producing expectations oftypes of named entities that must be present in theanswer (location names, person names, etc.).
Usingthe question terms as a query, an informationretrieval (IR) system identifies possibly relevantpassages in the collection, often after queryexpansion (e.g., adding synonyms).
Snippets ofthese passages are then selected and ranked, basedon criteria such as whether or not they contain theexpected types of named entities, the percentage ofquestion words in each snippet, the percentage ofwords that also occur in other candidate snippets,etc.
The system reports the most highly-rankedsnippets, or, in the case of exact answers, namedentities of the required type therein.Unfortunately, the approach highlighted abovefalls short with questions that do not generateexpectations of particular types of named entitiesand contain very few non-stop-words.
Definitionquestions (e.g.
?What is a nanometer?
?, ?Who wasDuke Ellington??)
have both properties, and areparticularly common.
In TREC-2001, where thedistribution of question types reflected that of realuser logs, 27% of the questions were requests fordefinitions.
Hence, techniques to handle thiscategory of questions are very important.We propose a new method to answer definitionquestions, that combines and extends the techniqueof Prager et al (2001, 2002), which relied onWordNet hypernyms, and that of Joho et al (2001,2002), which relied on manually crafted lexicalpatterns, sentence position, and word co-occurrence across candidate answers.
We train anSVM (Sch?lkopf and Smola, 2002) on vectorswhose attributes include the verdict of Prager etal.
?s method, the attributes of Joho et al, andadditional phrasal attributes that we acquireautomatically.
The SVM is then used to identifyand rank 250-character snippets, each intended tocontain a stand-alone definition of a given term,much as in TREC QA tasks prior to 2003.In TREC-2003, the answers to definitionquestions had to be lists of complementarysnippets (Voorhees, 2003), as opposed to single-snippet definitions.
Here, we focus on the pre-2003task, for which TREC data were publicly availableduring our work.
We believe that this task is stillinteresting and of practical use.
For example, a listof single-snippet definitions accompanied by theirsource URLs can be a good starting point for usersof search engines wishing to find definitions.Single-snippet definitions can also be useful ininformation extraction, where the templates to befilled in often require short entity descriptions; seeRadev and McKeown (1997).
Experiments indicatethat our method clearly outperforms the techniquesit builds upon in the task we considered.
We sketchin section 6 how we plan to adapt our method tothe post-2003 TREC task.2 Previous techniquesPrager et al (2001, 2002) observe that definitionquestions can often be answered by hypernyms; forexample, ?schizophrenia?
is a ?mental illness?,where the latter is a hypernym of the former inWordNet.
Deciding which hypernym to report,however, is not trivial.
To use an example ofPrager et al, in ?What is a meerkat??
WordNetprovides hypernym synsets such as {?viverrine?,?viverrine mammal?}
(level 1), {?carnivore?
}(level 2), {?placental?, ?}
(level 3), {?mammal?
}(level 4), up to {?entity?, ?something?}
(level 9).In a neutral context, the most natural response isarguably ?mammal?
or ?animal?.
A hypernym onits own may also not be a satisfactory answer.Responding that an amphibian is an animal is lesssatisfactory than saying it is an animal that livesboth on land and in water.Prager et al identify the best hypernyms bycounting how often they co-occur with thedefinition term in two-sentence passages of thedocument collection.
They then short-list thepassages that contain both the term and any of thebest hypernyms, and rank them using measuressimilar to those of Radev et al (2000).
Moreprecisely, given a term to define, they compute thelevel-adapted count (LAC) of each of itshypernyms, defined as the number of two-sentencepassages where the hypernym co-occurs with theterm divided by the distance between the term andthe hypernym in WordNet?s hierarchy.
They thenretain the hypernym with the largest LAC, and allthe hypernyms whose LAC is within a 20% marginfrom the largest one.
To avoid very generalhypernyms (e.g., {?entity?, ?something?
}), Prageret al discard the hypernyms of the highest 1 or 2levels in WordNet?s trees, if the distance from thetop of the tree to the definition term is up to 3 or 5,respectively; if the distance is longer, they discardthe top three levels.
This ceiling is raised graduallyif no co-occurring hypernym is found.Prager et al?s method performed well with thedefinition questions and documents of TREC-9(Prager et al, 2001).
In 20 out of the 24 definitionquestions they considered (83.3%), it managed toreturn at least one correct response in the five mosthighly ranked two-sentence passages; in all 20questions, the correct response was actually thehighest ranked.
In TREC-2001, however, wherethere were more definition questions mirroringmore directly real user queries, this percentagedropped to 46% (Prager et al, 2002).
In 44 of the130 definition questions (33.8%) they considered,WordNet did not help at all, because it either didnot contain the term whose definition was sought,or none of its hypernyms were useful even aspartial definitions.
Even when WordNet containeda hypernym that constituted a self-containeddefinition, it was not always selected.In work that led to an alternative method, Hearst(1998) sketched a method to identify patterns thatsignal particular lexical semantic relations, ineffect a bootstrapping approach.
Applying thisprocess by hand, Hearst was able to identify fourhigh precision hyponym?hypernym patterns thatare common across text genres.
The patterns areshown below in the slightly modified form of Johoand Sanderson (2000).
Here, qn (query noun) anddp (descriptive phrase) are phrases containinghyponyms and hypernyms, respectively.
(1) (dp such | such dp) as qne.g., ?injuries such as broken bones?
(2) qn (and | or) other dpe.g., ?broken bones and other injuries?
(3) dp especially qne.g., ?injuries especially broken bones?
(4) dp including qne.g., ?European countries including England,France, and Spain?Compared to Prager et al?s method, Hearst?spatterns have the advantage that they may identifyhyponym?hypernym relations that are not presentin WordNet, which is often the case with domain-specific terminology and proper names.Joho and Sanderson (2000) observe that co-occurrences of hyponyms and hypernyms are oftenindicative of contexts that define the hyponyms.Instead of using WordNet, they identify hyponym?hypernym contexts using Hearst?s patterns, towhich they add the following ones.1 Here, qn is theterm to define, and dp is a phrase that contains adefinition of qn; dp no longer necessarily containsa hypernym of qn.
Each pattern is assigned aweight, which is its precision on a training set (thenumber of sentences it correctly identifies ascontaining a definition, over the number ofsentences it matches).
(5) qn ?(?
dp ?)?
| ?(?
dp ?)?
qne.g., ?MP (Member of Parliament)?
(6) qn (is | was | are | were) (a | an | the) dpe.g., ?Tony Blair is a politician?
(7) qn , (a | an | the) dpe.g., ?Tony Blair, the politician?
(8) qn , which (is | was | are | were) dpe.g., ?bronchitis, which is a disease of??
(9) qn  , dp , (is | was | are | were)e.g., ?Blair, Prime Minister of Britain, is?
?Joho and Sanderson first locate all the sentencesof the document collection that contain the term todefine, and then rank them using three attributes.The first one (KPW) is the weight of the pattern thesentence matched, if any.
The second attribute(SN) is the ordinal number of the sentence in itsdocument, ignoring sentences that do not containthe term; sentences that mention first the term in adocument are more likely to define it.
The thirdattribute (WC) shows how many of the words that1 We ignore a variant of (7) that ends in ?.
?, ???
or ?!
?.are common across candidate answers are presentin the sentence.
More precisely, to compute WC,Joho and Sanderson retrieve the first sentence ofeach document that contains the definition term,and retain the 20 most frequent words of thesesentences after applying a stop-list and alemmatizer.
WC is the percentage of these wordsthat are present in the sentence being ranked.
Thesentences are ranked using the weighted sum of thethree attributes, after hand-tuning the weights ofthe sum on a training set.
In Joho et al (2001), thismethod was evaluated with 50 definition questionsand the top 600 documents that Google returnedfor each definition term.
It returned a correctdefinition in the five top-ranked sentences in 66%of the questions.2 As with Prager et al?s method,then, there is scope for improvement.3 Our methodPrager et al?s approach is capable of answeringa large number of definition questions, though, asdiscussed above, it does not always manage tolocate the most appropriate hypernym, and theappropriate hyponym-hypernym relation may notbe present in WordNet.
Joho et al?s techniquedoes not rely on a predetermined ontology, whichmakes it less vulnerable to domain-specificterminology and proper names.
Its limited patternset, however, cannot capture definitions that arephrased in less typical ways, and the fact that theweights of the three attributes are hand-tunedraises doubts as to whether they are optimal.
Ourmethod overcomes these limitations by combiningthe two approaches in a common machine learningframework, and by using a larger attribute set.We assume that a question processing modulethat separates definition from other types ofquestions is available, and that in each definitionquestion it also identifies the term to be defined.When such a module is not available, the user canbe asked to specify explicitly the question type andthe term to be defined, via a form-based interfaceas in Buchholtz and Daelemans (2001).
Hence, theinput to our method is a (possibly multi-word)term, along with the most highly ranked documentsthat an IR engine returned for that term (in theexperiments of section 4, we used the top 50documents).
The goal is to identify five 250-character snippets in these documents, such that atleast one of the snippets contains an acceptabledefinition of the input term.
As an example, weshow below the two snippets that configuration 4of our method (to be discussed) considered most2 Joho et al report better results when using a less stringentform of evaluation that admits partial answers, but theiracceptance criteria in that case appear to be over-permissive.appropriate in the case of ?What are pathogens?
?.The first snippet defines pathogens as hazardousmicroorganisms.
The second one providesinstances of pathogens, but does not actually statewhat a pathogen is.
?considerations as nutrient availability.
In particular,the panel concluded that the fear of creating hazardousmicroorganisms, or pathogens, is overstated.
"It ishighly unlikely that moving one or a few genes from apathogen to?
?definite intraspecial physiological and morphologicaldiversity.
Ph.
helianthi thrives at higher temperaturesthan other sunflower pathogens (Sclerotiniasclerotiorum and Botrytis cinerea) do.
In variousnutrient media, Ph.
helianthi ?We select the five snippets to report usingalternatively a baseline and four differentconfigurations of our learning-based method.3.1 BaselineAs a baseline, we use a reimplementation ofPrager et al?s method that operates with 250-character snippets.3 Unlike Prager et al, ourreimplementation does not use a ranking function(Radev et al, 2000).
When more than five snippetscontain both the input term and one of its besthypernyms, we rank the snippets according to theranking (RK) of the documents they derive from,i.e., the ranking of the IR engine.
Our evaluationresults (section 4) indicate that the performance ofour implementation is still very similar to that ofPrager et al3.2 Learning-based methodIn all the configurations of our learning-basedmethod, we use a Support Vector Machine (SVM)with a simple inner product (polynomial of firstdegree) kernel (Sch?lkopf and Smola, 2002),which in effect learns an optimal linear classifierwithout moving to a higher-dimension space.
(Wehave experimented with higher degree polynomialkernels, but there was no sign of improvement.
)The SVM is trained as follows.
Given a training setof terms to be defined and the correspondingdocuments that the IR engine returned, we collectfrom the documents all the 250-character snippets3 Following Prager et al (2002), we considersynonyms of the input term as level-0 hypernyms, andinclude them in the search for best hypernyms; theirLAC is the number of times they co-occur with theinput term.
We did not implement the tests fororthographic variations and count ratios of Prager et alWhen an input term occurs in multiple synsets, whichproduces multiple paths towards hypernyms, we selectthe hypernyms with the best overall LAC scores, insteadof the best scores per path, unlike Prager et al (2001).that have the term at their center.
Each snippet isthen converted to a training vector, the attributes ofwhich differ across the configurations presentedbelow.
The training vectors are manually classifiedin two categories, depending on whether or not thecorresponding snippets contain acceptabledefinitions.
The SVM is trained on these vectors topredict the category of new, unseen vectors fromtheir attributes.The SVM implementation we used actuallyreturns confidence scores, showing how probable itis that a particular vector belongs to eachcategory.4 Using a classifier that returns confidencescores instead of binary decisions is crucial,because the training vectors that correspond todefinitions are much fewer than the vectors fornon-definitions (3004 vs. 15469 in our dataset ofsection 4).
As a result, the induced classifier isbiased towards non-definitions, and, hence, mostunseen vectors receive higher confidence scoresfor the category of non-definitions than for thecategory of definitions.
We do not compare thetwo scores.
We pick the five vectors whoseconfidence score for the category of definitions ishighest, and report the corresponding snippets; ineffect, we use the SVM as a ranker, rather than aclassifier; see also Ravichandran et al (2003).
Theimbalance between the two categories can bereduced by considering (during both training andclassification) only the first three snippets of eachdocument, which discards mostly non-definitions.3.2.1 Configuration 1: attributes of Joho at al.In the first configuration of our learning-basedapproach, the attributes of the vectors are roughlythose of Joho et al: two numeric attributes for SNand WC (section 2), and a binary attribute for eachone of patterns (1)?
(9) showing if the pattern issatisfied.
We have also added binary attributes forthe following manually crafted patterns, and anumeric attribute for RK (section 3.1).
(10) dp like qne.g., ?antibiotics like amoxicillin?
(11) qn or dpe.g., ?autism or some other type of disorder?
(12) qn (can | refer | have) dpe.g., ?amphibians can live on land and??
(13) dp (called | known as | defined) qne.g., ?the giant wave known as tsunami?This configuration can be seen as anapproximation of Joho et al?s method, although itis actually an improvement, because of the extraattributes and the fact that it uses an SVM learner4 We used Weka?s SVM implementation.
Weka isavailable from http://www.cs.waikato.ac.nz/ml/weka/ .instead of a weighted sum with hand-tuned weightsto combine the attributes.3.2.2 Configuration 2: adding WordNetThe second configuration is identical to the firstone, except that it adds an extra binary attributeshowing if the snippet contains one of its besthypernyms, as returned by the baseline.
This isessentially a combination of the approaches ofPrager et al and Joho et al Unlike simple voting(Chu-Carroll et al, 2003), the two methodscontribute attributes to the instance representations(vectors) of an overall learner (the SVM).
Thisallows the overall learner to assess their reliabilityand adjust their weights accordingly.3.2.3 Configuration 3: n-gram attributesThe third configuration adds m extra binaryattributes, each corresponding to an automaticallyacquired pattern.
(In the experiments of section 4,m ranges from 100 to 300.)
Observing that most ofthe previously proposed patterns are anchored atthe term to define, we collect from the documentsthat the IR engine returns for the training questionsall the n-grams (n ?
{1, 2, 3}) of tokens that occurimmediately before or after the definition term.The n-grams that are encountered at least 10 timesare considered candidate patterns.
From these, weselect the m patterns with the highest precisionscores, where precision is defined as in section 2,but for snippets instead of sentences.In our experiments, this pattern acquisitionprocess re-discovered several of the patterns (1)?
(13) or sub-expressions of them, namely [dp suchas qn], [qn and other dp], [qn ?(?
dp], [dp ?)?
qn],[qn is (a | an | the) dp], [qn (are | were) dp], [qn , (a| an | the) dp), [qn , which is dp], [qn , dp], [qn ordp], [qn can dp], [dp called qn], [dp known as qn].It also discovered some reasonable variations ofpatterns (1)?
(13); e.g, [qn is one dp], [dp , (a | an)qn] (as in ?A sudden health problem, a heart attackor ??
), [dp , qn], [qn , or dp].
We include dp, thephrase that defines qn, in the acquired patterns tomake them easier to compare to patterns (1)?
(13).The acquired patterns, however, do not predict theposition of dp in a snippet.Many of the acquired patterns at first look odd,but under a closer examination turn out to bereasonable.
For example, definition sentences oftenstart with the term they define, as in ?Amphibiansare?
?, ?An alligator is?
?, sometimes with theterm quoted, which is how patterns like [.
qn], [.An qn], [. '
' qn] arise.
Many of the questions inour data set were about diseases, and, hence,several of the acquired patterns were expressions(e.g,, ?people with?, ?symptoms of?)
that co-occurfrequently with definitions of diseases in snippets.This suggests that automatic pattern acquisitionwould allow domain-specific question answeringsystems (e.g., systems for medical documents) toexploit domain-specific indicators of definitions.The pattern acquisition process also producedmany patterns (e.g., ?the hallucinogenic drug?,?President Rafael?)
that do not seem to provideany useful information in general, although theyoccurred frequently in definition snippets of ourtraining data.
Since we did not filter manually theautomatically acquired patterns, patterns of thelatter kind gave rise to irrelevant attributes thatcarried noise.
This is a common problem inmachine learning, which most learning algorithmsare designed to cope with.
Our experimental resultsindicate that the SVM learner benefits from the n-gram attributes, despite the noise they introduce.We also experimented with keeping both highand low precision patterns, hoping to acquire bothn-grams that are indicative of definitions and n-grams that are indicative of contexts that do notconstitute definitions.
The experimental results,however, were inferior, and manual inspection ofthe low precision n-grams indicated that theycarried mostly noise, suggesting that it is difficultto identify frequent n-grams whose presence rulesout definitions reliably.The reader may wonder why we rely only onprecision, rather than selecting also attributes withhigh recall.
(Here, recall is the number of snippetsthe pattern correctly identifies as containing adefinition, over the total number of snippets thatcontain a definition.)
In multi-source documentcollections, like the Web or the TREC documents,one can expect to find several definitions of thesame term, phrased in different ways.
Unliketraditional document retrieval tasks, we are notinterested in obtaining all the definitions; a singlecorrect one suffices.
Furthermore, as the number ofsnippets that can be reported is limited, we need tobe confident that the snippets we return are indeeddefinitions.
Hence, we need to rely on high-precision indicators.
Preliminary experiments weperformed using the F-measure (with ?=1), acombination of recall and precision, instead ofprecision led to inferior results, confirming thatattribute recall is not helpful in this task.We have also experimented with informationgain.
In that case, one selects the n-grams with them highest IG(C,X) scores, defined below, where Cand X are random variables denoting the categoryof a snippet and the value of the n-gram?s binaryattribute, respectively, and H(C) and H(C|X) arethe entropy and conditional entropy of C. Byselecting the attributes with the highest informationgain scores, one selects the attributes that carrymost information about the value of C.)|()()(),(}1,0{xXCHxXPCHXCIGx=?=?= ?
?Although information gain is one of the bestattribute selection measures in text categorization(Yang and Pedersen, 1997), in our case it led tovery few attributes with non-zero IG(C,X) scores(around 90 attributes from the entire dataset ofsection 4).
This is because most of the n-grams arevery rare (i.e., P(X=0) is very large), and theirabsence (X=0) provides very little informationabout C (i.e., H(C) ?
H(C | X = 0)).
For example,not encountering [dp such as qn] provides verylittle information on whether or not the snippet is adefinition.
The experiments we performed with theresulting attributes led to inferior results, comparedto those that we got via precision.3.2.4 Configuration 4: discarding WordNetThe fourth configuration is identical to the thirdone, except that it does not use the attribute thatshows if the snippet contains one of the besthypernyms of Prager et al (The attribute is presentin configurations 2 and 3).
The intention is toexplore if the performance of configuration 3 canbe sustained without the use of WordNet.4 Experimental resultsWe evaluated the baseline and the machinelearning configurations of section 3 on thedefinition questions of TREC-9 (2000) and TREC-2001, the same data used by Prager et al For eachquestion, the TREC organizers provide the 50 mosthighly ranked documents that an IR enginereturned from the TREC documents.
The task is toreturn for each question five 250-character snippetsfrom the corresponding 50 documents, such that atleast one of the snippets contains an acceptabledefinition.
Following Prager et al, we count asnippet as containing an acceptable definition, if itsatisfies the Perl answer patterns that the TRECorganizers provide for the corresponding question(Voorhees, 2001).
The answer patterns incorporatethe correct responses of all the participants of thecorresponding TREC competition.
In TREC-9, thecorrelation between system rankings produced byanswer patterns and rankings produced by humanswas at the same level as the average correlationbetween rankings of different human assessors(Voorhees and Tice 2000).
In TREC-2001, thecorrelation between patterns and judges was lower,but still similar for 250-character responses(Voorhees, 2001).All the experiments with machine learningconfigurations were performed with 10-fold cross-validation.
That is, the question set was dividedinto 10 parts, and each experiment was repeated 10times.
At each iteration, the questions of a differentpart and the corresponding documents werereserved for testing, while the questions anddocuments of the remaining nine parts were usedfor training.
(In configurations 3 and 4, patternacquisition was repeated at each iteration.)
Table 1reports the total number of questions that eachmethod managed to handle successfully over theten iterations; i.e., questions with at least oneacceptable definition in the five returned snippets.When questions for which there was no answer inthe corresponding 50 documents and/or there wasno answer pattern are excluded, the results arethose shown in italics.
The second row contains theresults reported by Prager et al (2001, 2002),while the third one shows the results of ourreimplementation.
We include six questions thatPrager et al appear to have excluded, which iswhy the total number of questions is different;there are also some implementation differences, asdiscussed in section 3.Method % questions handled correctlyPrager et al 51.95 (80/154), 60.15 (80/133)baseline 50.00 (80/160), 58.39 (80/137)config.
1 61.88 (99/160), 72.26 (99/137)config.
2 63.13 (101/160), 73.72 (101/137)config.
3 72.50 (116/160), 84.67 (116/137)config.
4 71.88 (115/160), 83.94 (115/137)Table 1: Results on TREC-9 & TREC-2001 dataThe SVM learner with roughly Joho et al?sattributes (config.
1) clearly outperforms Prager etal.
?s WordNet-based method.
Adding Prager etal.
?s method as an extra attribute to the SVM(config.
2) leads to only a marginal improvement.Automatic pattern acquisition (config.
3) is muchmore beneficial.
Removing the attribute of theWordNet-based method (config.
4) caused thesystem to fail in only one of the questions thatconfiguration 3 handled successfully, which againsuggests that the WordNet-based method does notcontribute much to the performance ofconfiguration 3.
This is particularly interesting forlanguages with no WordNet-like resources.The results of configurations 3 and 4 in table 1were obtained using the 200 n-gram attributes withthe highest precision scores.
When using the 100n-gram attributes with the highest and the 100 n-gram attributes with the lowest precision scores,the results of configuration 3 were 70.63% and82.48%.
When using all the n-gram attributes withnon-zero information gain scores, the results ofconfiguration 3 were 66.25% and 77.42%.Configurations 2 and 3 achieved inferior resultswith 300 highest-precision n-gram attributes (table2), which may be a sign that low reliability n-grams are beginning to dominate the attribute set.n-grams config.
3 (%) config.
4 (%)100 68.13, 79.56 70.00, 81.75200 72.50, 84.67 71.88, 83.94300 68.75, 80.29 71.25, 83.21Table 2: Results for variable number of n-grams5 Related workNg et al (2000) use machine learning (C5 withboosting) to classify and rank candidate answers,but do not treat definition questions in any specialway, and use only four generic attributes across allquestion categories.
Some of their worst results arefor ?What ???
questions, that presumably includea large number of definition questions.Ittycheriah and Roukos (2002) employ amaximum entropy model to rank candidateanswers, which uses a very rich set of attributesthat includes 8,500 patterns.
The latter are n-gramsof words that occur frequently in answers of thetraining data, each associated with a two-wordquestion prefix (e.g., ?What is?)
that also has to bematched for the pattern to be satisfied.
Unlike ourwork, the n-grams have to be five or more wordslong, and, in the case of definition questions, theydo not need to be anchored at the term to define.Ittycheriah and Roukos (2002) do not provideseparate figures on the performance of their systemon definition questions.Blair-Goldensohn et al (2003) focus ondefinition questions, but aim at producing coherentmulti-sentence definitions, rather than identifyingsingle defining snippets.
At the heart of theirapproach is a component that uses machinelearning (Riper) to identify sentences that arecandidates for inclusion in the multi-sentencedefinition.
This component plays a role similar tothat of our SVM learner, but it is intended to admita larger range of sentences, and appears to employonly attributes conveying the position of thesentence in its document and the frequency of thedefinition term in the context of the sentence.Automatically acquired n-gram patterns can alsobe used for query expansion in informationretrieval, as in Agichtein et al (2001).6 Conclusions and future workWe have presented a new method to identifysingle-snippet definitions in question answeringsystems.
Our method combines previouslyproposed techniques as attributes of an SVMlearner, to which an automatic pattern acquisitionprocess contributes additional attributes.
We haveevaluated several configurations of our method onTREC data, with results indicating it outperformsprevious techniques.The performance of our method may improve ifn-grams that start or end within a margin of a fewtokens from the term to define are added.
This mayallow definitions like ?X, that Y defined as ??
tobe found.
Further improvements may be possibleby using a sentence splitter instead of windows offixed length, anaphora resolution, clustering ofsimilar snippets to avoid ranking them separately,and identifying additional n-gram attributes bybootstrapping (Ravichandran et al 2003).We believe that it is possible to address the post-2003 TREC task for definition questions with thesame approach, but training the SVM learner toidentify snippets that should be included in multi-snippet definitions.
With sufficient training, weexpect that n-grams indicative of informationcommonly included in multi-snippet definitions(e.g., dates of birth, important works for persons)will be discovered.
Larger amounts of trainingdata, however, will be required.
We are currentlyworking on a method to generate training examplesin an unsupervised manner from parallel texts.ReferencesE.
Agichtein, S. Lawrence, and L. Gravano.
2001.Learning Search Engine Specific QueryTransformations for Question Answering.
Pro-ceedings of WWW-10, Hong Kong, pp.
169?178.S.
Blair-Goldensohn, K.R.
McKeown, and A.H.Schlaikjer.
2003.
A Hybrid Approach forAnswering Definitional Questions.
TechnicalReport CUCS-006-03, Columbia University.S.
Buchholtz and W. Daelemans.
2001.
ComplexAnswers: A Case Study Using a WWWQuestion Answering System.
Natural LanguageEngineering, 7(4):301?323.J.
Chu-Carroll, K. Czuba, J. Prager, and A.Ittycheriah.
2003.
In Question Answering, TwoHeads are Better than One.
Proceedings of HLT-NAACL 2003, Edmonton, Canada, pp.
24?31.M.A.
Hearst.
1998.
Automated Discovery ofWordnet Relations.
In C. Fellbaum (Ed.
), Word-Net: An Electronic Lexical Database.
MIT Press.A.
Ittycheriah and S. Roukos.
2002.
IBM?sStatistical Question Answering System ?
TREC-11.
Proceedings of TREC-2002.H.
Joho and M. Sanderson.
2000.
RetrievingDescriptive Phrases from Large Amounts of FreeText.
Proceedings of the 9th ACM Conference onInformation and Knowledge Management,McLean, VA, pp.
180?186.H.
Joho and M. Sanderson.
2001.
Large ScaleTesting of a Descriptive Phrase Finder.Proceedings of the 1st Human LanguageTechnology Conference, San Diego, CA, pp.219?221.H.T.
Ng, J.L.P.
Kwan, and Y. Xia.
2001.
QuestionAnswering Using a Large Text Database: AMachine Learning Approach.
Proceedings ofEMNLP 2001, Pittsburgh, PA, pp.
67?73.J.
Prager, D. Radev, and K. Czuba.
2001.Answering What-Is Questions by VirtualAnnotation.
Proceedings of the 1st HumanLanguage Technology Conference, San Diego,CA, pp.
26?30.J.
Prager, J. Chu-Carroll, and K. Czuba.
2002.
Useof WordNet Hypernyms for Answering What-IsQuestions.
Proceedings of TREC-2001.D.R.
Radev and K.R.
McKeown.
1997.
Buliding aGeneration Knowledge Source using Internet-Accessible Newswire.
Proceedings of the 5thANLP, Washington, D.C., pp.
221?228.D.R.
Radev, J. Prager, and V. Samn.
2000.Ranking Suspected Answers to NaturalLanguage Questions Using PredictiveAnnotation.
Proceedings of NAACL/ANLP-2000,Seattle, WA, pp.
150?157.D.
Ravichandran, E. Hovy, and F.J. Och.
2003.Statistical QA ?
Classifier vs. Ranker: What?sthe Difference?
Proceedings of the ACLworkshop on Multilingual Summarization andQuestion Answering, Sapporo, Japan.D.
Ravichandran, A. Ittycheriah, and S. Roukos.2003.
Automatic Derivation of Surface TextPatterns for a Maximum Entropy Based QuestionAnswering System.
Proceedings of HLT-NAACL2003, Edmonton, Canada, pp.
85?87.B.
Sch?lkopf and A. Smola.
2002.
Learning withKernels.
MIT Press.E.M.
Voorhees and D.M.
Tice.
2000.
Building aQuestion Answering Test Collection.
Proc.
ofSIGIR-2000, Athens, Greece, pp.
200?207.E.M.
Voorhees.
2001.
The TREC QA Track.Natural Language Engineering, 7(4):361?378.E.M.
Voorhees.
2003.
Evaluating Answers toDefinition Questions.
Proceedings of HLT-NAACL 2003, Edmonton, Canada, pp.
109?111.Y.
Yang and J.O.
Pedersen.
1997.
A ComparativeStudy on Feature Selection in TextCategorization.
Proceedings of the 14thInternational Conference on Machine Learning,Nashville, TN, pp.
412?420.
