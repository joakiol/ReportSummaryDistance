Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 64?72,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsPlurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope DisambiguationMehdi Manshadi, Daniel Gildea, and James AllenUniversity of Rochester734 Computer Studies BuildingRochester, NY 14627mehdih,gildea,james@cs.rochester.eduAbstractRecent work on statistical quantifier scopedisambiguation (QSD) has improved uponearlier work by scoping an arbitrary num-ber and type of noun phrases.
No corpus-based method, however, has yet addressedQSD when incorporating the implicit uni-versal of plurals and/or operators such asnegation.
In this paper we report early,though promising, results for automaticQSD when handling both phenomena.
Wealso present a general model for learningto build partial orders from a set of pair-wise preferences.
We give an n log n algo-rithm for finding a guaranteed approxima-tion of the optimal solution, which worksvery well in practice.
Finally, we signifi-cantly improve the performance of the pre-vious model using a rich set of automati-cally generated features.1 IntroductionThe sentence there is one faculty member in ev-ery graduate committee is ambiguous with respectto quantifier scoping, since there are at least twopossible readings: If one has wide scope, there isa unique faculty member on every committee.
Ifevery has wide scope, there can be different fac-ulty members on each committee.
Over the pastdecade there has been some work on statisticalquantifier scope disambiguation (QSD) (Higginsand Sadock, 2003; Galen and MacCartney, 2004;Manshadi and Allen, 2011a).
However, the extentof the work has been quite limited for several rea-sons.
First, in the past two decades, the main focusof the NLP community has been on shallow textprocessing.
As a deep processing task, QSD is notessential for many NLP applications that do not re-quire deep understanding.
Second, there has beena lack of comprehensive scope-disambiguated cor-pora, resulting in the lack of work on extensivestatistical QSD.
Third, QSD has often been con-sidered only in the context of explicit quantifica-tion such as each and every versus some and a/an.These co-occurrences do not happen very often inreal-life data.
For example, Higgins and Sadock(2003) find fewer than 1000 sentences with two ormore explicit quantifiers in the Wall Street journalsection of Penn Treebank.
Furthermore, for morethan 60% of those sentences, the order of the quan-tifiers does not matter, either as a result of the logi-cal equivalence (as in two existentials), or becausethey do not have any scope interaction.Having said that, with deep language processingreceiving more attention in recent years, QSD isbecoming a real-life issue.1 At the same time, newscope-disambiguated corpora have become avail-able (Manshadi et al, 2011b).
In this paper, weaim at tackling the third issue mentioned above.We push statistical QSD beyond explicit quantifi-cation, and address an interesting, yet practicallyimportant, problem in QSD: plurality and quan-tification.
In spite of an extensive literature intheoretical semantics (Hamm and Hinrichs, 2010;Landmann, 2000), this topic has not been well in-vestigated in computational linguistics.
To illus-trate the phenomenon, consider (1):1.
Three words start with a capital letter.A deep understanding of this sentence, requiresdeciding whether each word in the set, referredto by Three words, starts with a potentially dis-tinct capital letter (as in Apple, Orange, Banana)or there is a unique capital letter which each wordstarts with (as in Apple, Adam, Athens).
By treat-ing the NP Three words as a single atomic entity,earlier work on automatic QSD has overlookedthis problem.
In general, every plural NP poten-tially introduces an implicit universal, ranging1For example, Liang et al (2011) in their state-of-the-artstatistical semantic parser within the domain of natural lan-guage queries to databases, explicitly devise quantifier scop-ing in the semantic model.64over the collection of entities introduced by theplural.2 Scoping this implicit universal is just asimportant.
While explicit universals may not oc-cur very often in natural language, the usage ofplurals is very common.
Plurals form 18% of theNPs in our corpus and 20% of the nouns in PennTreebank.
Explicit universals, on the other hand,form less than 1% of the determiners in Penn Tree-bank.
Quantifiers are also affected by negation.Previous work (e.g., Morante and Blanco, 2012)has investigated automatically detecting the scopeand focus of negation.
However, the scope ofnegation with respect to quantifiers is a differentphenomenon.
Consider the following sentence.2.
The word does not start with a capital letter.Transforming this sentence into a meaning repre-sentation language, for almost any practical pur-poses, requires deciding whether the NP a capitalletter lies in the scope of the negation or outsideof it.
The former describes the preferred readingwhere The word starts with a lowercase letter asin apple, orange, banana, but the latter gives theunlikely reading, according to which there exists aparticular capital letter, say A, that The word startswith, as in apple, Orange, Banana.
By not in-volving negation in quantifier scoping, a semanticparser may produce an unintended interpretation.Previous work on statistical QSD has been quiterestricted.
Higgins and Sadock (2003), whichwe refer to as HS03, developed the first statisti-cal QSD system for English.
Their system dis-ambiguates the scope of exactly two explicitlyquantified NPs in a sentence, ignoring indefinitea/an, definites and bare NPs.
Manshadi and Allen(2011a), hence MA11, go beyond those limita-tions and scope an arbitrary number of NPs in asentence with no restriction on the type of quantifi-cation.
However, although their corpus annotatesthe scope of negations and the implicit universal ofplurals, their QSD system does not handle those.As a step towards comprehensive automaticQSD, in this paper we present our work on auto-matic scoping of the implicit universal of pluralsand negations.
For data, we use a new revisionof MA11?s corpus, first introduced in Manshadi etal.
(2011b).
The new revision, called QuanText,carries a more detailed, fine-grained scope annota-tion (Manshadi et al, 2012).
The performance of2Although plurals carry different types of quantification(Herbelot and Copestake, 2010), almost always there existsan implicit universal.
The importance of scoping this univer-sal, however, may vary based on the type of quantification.our model defines a baseline for future efforts on(comprehensive) QSD over QuanText.
In additionto addressing plurality and negation, this work im-proves upon MA11?s in two directions.?
We theoretically justify MA11?s ternary-classification approach, formulating it as ageneral framework for learning to build par-tial orders.
An n log n algorithm is then givento find a guaranteed approximation within afixed ratio of the optimal solution from a setof pairwise preferences (Sect.
3.1).?
We replace MA11?s hand-annotated featureswith a set of automatically generated linguis-tic features.
Our rich set of features signifi-cantly improves the performance of the QSDmodel, even though we give up the gold-standard dependency features (Sect.
3.3).2 Task definitionIn QuanText, scope-bearing elements (or, as wecall them, scopal terms) of each sentence havebeen identified using labeled chunks, as in (3).3.
Replace [1/ every line] in [2/ the file] endingin [3/ punctuation] with [4/ a blank line] .NP chunks follow the definition of baseNP(Ramshaw and Marcus, 1995) and hence are flat.Outscoping relations are used to specify the rel-ative scope of scopal terms.
The relation i > jmeans that chunk i outscopes (or has wide scopeover) chunk j. Equivalently, chunk j is said tohave narrow scope with respect to i.
Each sen-tence is annotated with its most preferred scoping(according to the annotators?
judgement), repre-sented as a partial order:4.
SI : (2 > 1 > 4; 1 > 3)If neither i > j nor j > i is entailed from thescoping, i and j are incomparable.
This happensif both orders are equivalent (as in two existentials)or when the two chunks have no scope interaction.Since a partial order can be represented by a Di-rected Acyclic Graph (DAG), we use DAGs torepresent scopings.
For example, G1 in Figure 1represents the scoping in (4).2.1 Evaluation metricsGiven the gold standard DAG Gg = (V,Eg) andthe predicted DAG Gp = (V,Ep), a similaritymeasure may be defined based on the ratio of thenumber of pairs (of nodes) labeled correctly to the65213 4(a) G1213 4(b) G+121 43(c) G22 1 3 4(d) G3Figure 1: Scoping as DAGtotal number of pairs.
In order to take the transi-tivity of outscoping relations into account, we usethe transitive closure (TC) of DAGs.
Let G+ =(V,E+) represent the TC of a DAG G = (V,E).3G1 and G+1 in Figure 1 illustrate this concept.
Wenow define the similiarty metric S+ as follows:?+ =|E+p ?
E+g | ?
|E?+p ?
E?+g ||V |(|V | ?
1)/2 (1)in which G?
= (V, E?)
is the complement of theunderlying undirected version of G.HS03 and others have used such a similaritymeasure for evaluation purposes.
A disadvantageof this metric is that it gives the same weight tooutscoping and incomparability relations.
In prac-tice, if two scopal terms with equivalent ordering(and hence, no outscoping relation) are incorrectlylabeled with an outscoping, the logical form stillremains valid.
But if an outscoping relation is mis-labeled, it will change the interpretation of the sen-tence.
Therefore, in MA11, we suggest defining aprecision/recall based on the number of outscop-ing relations recovered correctly: 4P+ =|E+p ?
E+g ||E+p |, R+ =|E+p ?
E+g ||E+g |(2)3 (u, v) ?
G+ ??
((u, v)?G ?
?w1 .
.
.
wn?V, (u,w1) .
.
.
(wn, v) ?
E )4MA11 argues that TC-based metrics tend to producehigher numbers.
For example if G3 in Figure 1 is a gold-standard DAG andG1 is a candidate DAG, TC-based metricscount 2>3 as another match, even though it is entailed from2 > 1 and 1 > 3.
They give an alternative metric based ontransitive reduction (TR), obtained by removing all the re-dundant edges of a DAG.
TR-based metrics, however, havetheir own disadvantage.
For example, if G2 is another candi-date forG3, TR-based metrics produce the same numbers forboth G1 and G2, even though G1 is clearly closer to G3 thanG2.
Therefore, in this paper we stick to TC-based metrics.3 Our framework3.1 Learning to do QSDSince we defined QSD as a partial ordering, auto-matic QSD would become the problem of learn-ing to build partial orders.
The machine learningcommunity has studied the problem of learning to-tal orders (ranking) in depth (Cohen et al, 1999;Furnkranz and Hullermeier, 2003; Hullermeier etal., 2008).
Many ranking systems create partialorders as output when the confidence level for therelative order of two objects is below some thresh-old.
However, the target being a partial order isa fundamentally different problem.
While the lackof order between two elements is interpreted as thelack of confidence in the former, it should be inter-preted as incomparability in the latter.
Learningto build partial orders has not attracted much atten-tion in the learning community, although as seenshortly, the techniques developed for ranking canbe adopted for learning to build partial orders.As mentioned before, a partial order P can berepresented by a DAG G, with a preceding b in Pif and only if a reaches b in G by a directed path.Although there could be many DAGs representinga partial order P , only one of those is a transitiveDAG.5 Therefore, in order to have a one-to-one re-lationship between QSDs and DAGs, we only con-sider the class of transitive DAGs, or TDAG.
Ev-ery non-transitive DAG will be converted into itstransitive counterpart by taking its transitive clo-sure (as shown in Figure 1).Consider V , a set of nodes and a TDAG G =(V,E).
It would help to think of disconnectednodes u, v of G, as connected with a null edge .We define the labeling function ?G : V ?
V ??
{+,?, } assigning one of the three labels to eachpair of nodes in G:?G(u, v) =??
?+ (u, v) ?
G?
(v, u) ?
G otherwise(3)Given the true TDAG G?
= (V, E?
), and a candidateTDAG G, we define the Loss function to be thetotal number of incorrect edges:L(G, G?)
=?u?v?VI(?G(u, v) 6= ?G?
(u, v)) (4)in which ?
is an arbitrary total order over thenodes in V 6, and I(?)
is the indicator function.
We5G is transitive iff (u, v), (v, w) ?
G =?
(u,w) ?
G.6E.g., the left-to-right order of the corresponding chunksin the sentence.66adopt a minimum Bayes risk (MBR) approach,with the goal of finding the graph with the lowestexpected loss against the (unknown) target graph:G?
= argminG?TDAGEG?
[L(G, G?
)](5)Substituting in the definition of the loss functionand exchanging the order of the expectation andsummation, we get:G?
= argminG?TDAG?u?v?VEG?
[I(?G(u, v) 6= ?G?
(u, v)]= argminG?TDAG?u?v?VP (?G(u, v) 6= ?G?
(u, v)) (6)This means that in order to solve Eq.
(5), we needonly the probabilities of each of the three labels foreach of the C(n, 2) = n(n?
1)/2 pairs of nodes7in the graph, rather than a probability for eachof the superexponentially many possible graphs.We train a classifier to estimate these probabili-ties directly for a given pair.
Therefore, we havereduced the problem of predicting a partial orderto pairwise comparison, analogous to ranking bypairwise comparison or RPC (Hullermeier et al,2008; Furnkranz and Hullermeier, 2003), a popu-lar technique in learning total orders.
The differ-ence though is that in RPC, the comparison is a(soft) binary classification, while for partial orderswe have the case of incomparability (the label ),hence a (soft) ternary classification.A soft ternary classifier generates three proba-bilities, pu,v(+), pu,v(?
), and pu,v() for each pair(u, v),8 corresponding to the three labels.
Hence,equation Eq.
(6) can be rearranged as follows:G?
= argmaxG?TDAG?u?v?Vpu,v(?G(u, v)) (7)Let ?p be a graph like the one in Figure 2, contain-ing exactly three edges between every two nodes,weighted by the probabilities from the n(n?
1)/2classifiers.
We call ?p the preference graph.
In-tuitively speaking, the solution to Eq.
(7) is thetransitive directed acyclic subgraph of ?p that hasthe maximum sum of weights.
Unfortunately find-ing this subgraph is an NP-hard problem.97Throughout this subsection, unless otherwise specified,by a pair of nodes we mean a pair (u, v) with u?v.8pv,u for u?v is defined in the obvious way: pv,u(+) =pu,v(?
), pv,u(?)
= pu,v(+), and pv,u() = pu,v().9 The proof is beyond the scope of this paper, but the ideais similar to that of Cohen et al (1999), on finding total or-ders.
Although they don?t use an RPC technique, Cohen et320.510.10.80.20.30.10.30.10.6Figure 2: A preference graph over three nodes.1.
Let ?p be the preference graph andset G to ?.2.
?u ?
V , let pi(u) =?v pu,v(+)?
?v pu,v(?).3.
Let u?
= argmaxu pi(u),S?
=?v?G pv,u?(?)
& S =?v?G pv,u?().4.
Remove u?
and all its incident edgesfrom ?p.5.
Add u?
to G; also if S?
> S, forevery v ?
G?
u?, add (v, u?)
to G.6.
If ?p is empty, output G, otherwiserepeat steps 2-5.Figure 3: An approximation algorithm for Eq.
(7)Since it is very unlikely to find an efficient al-gorithm to solve Eq.
(7), instead, we propose thealgorithm in Figure 3 which finds an approximatesolution.
The idea of the algorithm is simple.
Byfinding u?
with the highest pi(u) in step 3, we forma topological order for the nodes in G in a greedyway (see Footnote 9).
We then add u?
to G. Adirected edge is added either from every node inG?u?
to u?
or from no node, depending on whichcase makes the sum of the weights in G higher.Theorem 1 The algorithm in Figure 3 is a 1/3-OPT approximation algorithm for Eq.
(7).Proof idea.
First of all, note that G is a TDAG,because edges are only added to the most recentlycreated node in step 5.
Let OPT be the optimumvalue of the right hand side of Eq.
(7).
The sum ofall the weights in ?p is an upper bound for OPT :?u?v?V???{+,?,}pu,v(?)
?
OPTStep 5 of the algorithm guarantees that the labels?G(u, v) satisfy:?u?v?Vpu,v(?G(u, v)) ??u?v?Vpu,v(?)
(8)al.
(1999) encounter a similar optimization problem.
Theypropose an approximation algorithm which finds the solution(a total order) in a greedy way.
Here we use the same greedytechnique to find a total order, but take it only as the topolog-ical order of the solution (Figure 3).67for any ?
?
{+,?, }.
Hence:?u?v?Vpu,v(?G(u, v))=13(3?u?v?Vpu,v(?G(u, v)))?
13?u?v?V???{+,?,}pu,v(?)?
13OPTIn practice, we improve the algorithm in Figure 3,while maintaining the approximation guarantee, asfollows.
When adding a node u?
to graph G, wedo not make a binary decision as to whether con-nect every node in G to u?
or none, but we usesome heuristics to choose a subset of nodes (pos-sibly empty) in G that if connected to u?
resultsin a TDAG whose sum of weights is at least asbig as the binary none-vs-all case.
As described inSec.
4, the algorithm works very well in our QSDsystem, finding the optimum solution in virtuallyall cases we examined.3.2 Dealing with plurality and negationConsider the following sentence with the pluralNP chunk the lines.5.
Merge [1p/ the lines], ending in [2/ a punctu-ation], with [3/ the next non-blank line].6.
SI : (1c > 1d > 2; 1d > 3) 10In QuanText, plural chunks are indexed with anumber followed by the lowercase letter ?p?.
Asseen in (6), the scoping looks different from beforein that the terms 1d and 1c are not the label of anychunk.
These two terms refer to the two quantifiedterms introduced by the plural chunk 1p: 1c (forcollection) represents the set (or in better wordscollection) of entities, defined by the plural, and 1d(for distribution) refers to the implicit universal,introduced by the plural.
In other words, for a plu-ral chunk ip, id represents the universally quanti-fied entity over the collection ic.
The outscopingrelation 1d > 2 in (6) states that every line in thecollection, denoted by 1c, starts with its own punc-tuation character.
Similarly, 1d > 3 indicates thatevery line has its own next non-blank line.
Fig-ure 4(a) shows a DAG for the scoping in (6).In (7) we have a sentence containing a negation.In QuanText, negation chunks are labeled with anuppercase ?N?
followed by a number.10This scoping corresponds to the logical formula:Dx1c, Collection(x1c) ?
?x1d, In(x1d, x1c)?(Line(x1d)?
(?x2, Punctuation(x2)?EndIn(x1d, x2))?
(Dx3,?blank(x3) ?
next(x1d, x3) ?merge(x1d, x3)))It is straightforward to write a formula for, say, 1c > 2 > 1d.
(a)1c 1d23(b)2 13N1 4Figure 4: DAGs for scopings in (6) and (8)7.
Extract [1/ every word] in [2/ file ?1.txt?
],which starts with [3/ a capital letter], butdoes [N1/ not] end with [4/ a capital letter].8.
SI : (2 > 1 > 3; 1 > N1 > 4)As seen here, a negation simply introduces achunk, which participates in outscoping relationslike an NP chunk.
Figure 4(b) represents the scop-ing in (8) as a DAG.From these examples, as long as we create twonodes in the DAG corresponding to each plu-ral chunk, and one node corresponding to eachnegation, there is no need to modify the under-lying model (defined in the previous section).However, when u (or v) is a negation (Ni) oran implicit universal (id) node, the probabilitiesp?u,v (?
?
{+,?, }) may come from a differentsource, e.g.
a different classification model or thesame model with a different set of features, as de-scribed in the following section.3.3 Feature selectionPrevious work has shown that the lexical itemof quantifiers and syntactic clues (often extractedfrom phrase structure trees) are good at predictingquantifier scoping.
Srinivasan and Yates (2009)use the semantics of the head noun in a quantifiedNP to predict the scoping.
MA11 also find the lex-ical item of the head noun to be a good predictor.In this paper, we introduce a new set of syntac-tic features which we found very informative: the?type?
dependency features of de Marneffe et al(2006).
Adopting this new set of features, we out-perform MA11?s system by a large margin.
An-other point to mention here is that the features thatare predictive of the relative scope of quantifiersare not necessarily as helpful when determiningthe scope of negation and vice versa.
Therefore wedo not use exactly the same set of features when68one of the scopal terms in the pair11 is a negation,although most of the features are quite similar.3.3.1 NP chunksWe first describe the set of features we haveadopted when both scopal terms in a pair are NP-chunks.
We have organized the features into dif-ferent categories listed below.Individual NP-chunk featuresFollowing features are extracted for both NPchunks in a pair.?
The part-of-speech (POS) tag of the head of chunk?
The lexical item of the head noun?
The lexical item of the determiner/quantifier?
The lexical item of the pre-determiner?
Does the chunk contain a constant (e.g.
?do?, ?x?)??
Is the NP-chunk a plural?Implicit universal of a pluralRemember that every plural chunk i introducestwo nodes in the DAG, ic and id.
Both nodesare introduced by the same chunk i, therefore theyuse the same set of features.
The only exceptionis a single additional binary feature for plural NPchunks, which determines whether the given noderefers to the implicit universal of the plural (i.e.
id)or to the collection itself (i.e.
ic).?
Does this node refer to an implicit universal?Syntactic features ?
phrase structure treeAs mentioned above, we have used two setsof syntactic features.
The first is motivated byHS03?s work and is based on the constituency (i.e.phrase structure) tree T of the sentence.
Sinceour model is based on pairwise comparison, thefollowing features are defined for each pair ofchunks.
In the following, by chunk we mean thedeepest phrase-level node in T dominating all thewords in the chunk.
If the constituency tree is cor-rect, this node is usually an NP node.
Also, Prefers to the undirected path in T connecting thetwo chunks.?
Syntactic category of the deepest common ancestor?
Does 1st/2nd chunk C-command 2nd/1st one??
Length of the path P?
Syntactic categories of nodes on P?
Is there a conjoined node on P ??
List of punctuation marks dominated by nodes on PSyntactic features ?
dependency treeAlthough regular ?untyped?
dependency relationsdo not seem to help our QSD system in the pres-ence of phrase-structure trees, we found the col-11Since our model is based on pairwise comparison, everysample is in fact a pair of nodes (u, v) of the DAG.lapsed typed dependencies (de Marneffe and Man-ning, 2008) very helpful, even when used on top ofthe phrase-structure features.
Below is the list offeatures we extract from the collapsed typed de-pendency tree Td of each sentence.
In the follow-ing, by noun we mean the node in Td which corre-sponds to the head of the chunk.
The choice of theword noun, however, may be sloppy, as the headof an NP chunk may not be a noun.?
Does 1st/2nd noun dominate 2nd/1st noun??
Does 1st/2nd noun immediately dominate 2nd/1st??
Type of incoming dependency relation of each noun?
Syntactic category of the deepest common ancestor?
Lexical item of the deepest common ancestor?
Length of the undirected path between the two3.3.2 NegationsThere are no sentences in our corpus with morethan one negation.
Therefore, for every pair ofnodes with one negation, the other node must re-fer to an NP chunk.
We use the following word-level, phrase-structure, and dependency featuresfor these pairs.?
Lexical item of the determiner for the NP chunk?
Does the NP chunk contain a constant??
Is the NP chunk a plural??
If so, does this node refer to its implicit universal??
Does the negation C-command the NP chunk in T ??
Does the NP chunk C-command the negation in T ??
What is the POS of the parent p of negation in Td??
Does p dominate the noun in Td??
Does the noun dominate p in Td??
Does p immediately dominate the noun in Td??
If so, what is the type of the dependency??
Does the noun immediately dominate p in Td??
If so, what is the type of the dependency??
Length of the undirected path between the two in Td4 ExperimentsQuanText contains 500 sentences with a total of1750 chunks, that is 3.5 chunks/sentence on av-erage.
Of those, 1700 chunks are NP chunks.The rest are scopal operators, mainly negation.
Ofall the NP chunks, 320 (more than 18%) are plu-ral, each introducing an implicit universal, that is,an additional node in the DAG.
Since we feedeach pair of elements to the classifiers indepen-dently, each (unordered) pair introduces one sam-ple.
Therefore, a sentence with n scopal elementscreates C(n, 2) = n(n ?
1)/2 samples for classi-fication.
When all the elements are taken into ac-count,12 the total number of samples in the corpuswill be:12Here by all elements we mean explicit chunks and theimplicit universals.
QuanText labels some other (implicit) el-ements, which we have not been handled in this work.
Inparticular, some nouns introduce two entities: a type and a69?iC(ni, 2) ?
4500 (9)Where ni is the number of scopal terms introducedby sentence i.
Out of the 4500 samples, around1800 involve at least one implicit universal (i.e.,id), but only 120 samples contain a negation.
Weevaluate the performance of the system for implicituniversals and negation both separately and in thecontext of full scope disambiguation.
We split thecorpus at random into three sets of 50, 100, and350 sentences, as development, test, and train setsrespectively.13To extract part-of-speech tags, phrase structuretrees, and typed dependencies, we use the Stan-ford parser (Klein and Manning, 2003; de Marn-effe et al, 2006) on both train and test sets.
Sincewe are using SVM, we have passed the confidencelevels through a softmax function to convert theminto probabilities P ?u,v before applying the algo-rithm of Section 3.
We take MA11?s system as thebaseline.
However, in order to have a fair com-parison, we have used the output of the Stanfordparser to automatically generate the same featuresthat MA11 have hand-annotated.14 In order to runthe baseline system on implicit universals, we takethe feature vector of a plural NP and add a fea-ture to indicate that this feature vector representsthe implicit universal of the corresponding chunk.Similarly, for negation we add a feature to showthat the chunk represents a negation.
As shown inSection 3.3.2, we have used a more compact setof features for negations.
Once again, in order tohave a fair comparison, we apply a similar modifi-cation to the baseline system.
We also use the ex-act same classifier as used in MA11.15 Figure 5(a)compares the performance of our model, which werefer to as RPC-SVM-13, with the baseline sys-tem, but only on explicit NP chunks.16 The goalfor running this experiment has been to comparethe performance of our model to the baseline sys-token, as described by Manshadi et al (2012).
In this work,we have only considered the token entity introduced by thosenouns and have ignored the type entity.13Since the percentage of sentences with negation is small,we made sure that those sentences are distributed uniformlybetween three sets.14MA11?s features are similar to part-of-speech tags anduntyped dependency relations.15SVMMulticlass from SVM-light (Joachims, 1999).16In all experiments, we ignore NP conjunctions.
Previouswork treats a conjunction of NPs as separate NPs.
However,similar to plurals, NP conjunctions (disjunctions) introducean extra scopal element: a universal (existential).
We areworking on an annotation scheme for NP conjunctions, sowe have left this for after the annotations become available.NP-Chunks only (no id or negation) ?+ P+ R+ F+ AR ABaseline (MA11) 0.762 0.638 0.484 0.550 0.59 0.47Our model (RPC-SVM-13) 0.827 0.743 0.677 0.709 0.68 0.55(a) Scoping explicit NP chunksOverall system (including negation and implicit universals) ?+ P+ R+ F+ AR ABaseline (MA11) 0.787 0.688 0.469 0.557 0.59 0.47Our model (RPC-SVM-13) 0.863 0.784 0.720 0.751 0.69 0.55(b) Scoping all elements (including id and Ni)Figure 5: Performance on QuanText datatem on the task that it was actually defined to per-form (that is scoping only explicit NP chunks).As seen in this table, by incorporating a richerset of features and a better learning algorithm, ourmodel outperforms the baseline by almost 15%.The measure A in these figures shows sentence-based accuracy.
A sentence counts as correct iffevery pair of scopal elements has been labeledcorrectly.
Therefore A is a tough measure.
Fur-thermore, it is sensitive to the length of the sen-tence.
Following MA11, we have computed an-other sentence-based accuracy measure, AR.
Incomputing AR, a sentence counts as correct iff allthe outscoping relations have been recovered cor-rectly ?
in other words, iff R = 100%, regardlessof the value of P. AR may be more practicallymeaningful, because if in the correct scoping ofthe sentence there is no outscoping between twoelements, inserting one does not affect the inter-pretation of the sentence.
In other words, precisionis less important for QSD in practice.Figure 5(b) gives the performance of the over-all model when all the elements including the im-plicit universals and the negations are taken intoaccount.
That the F-score of our model for thesecond experiment is 0.042 higher than F-score forthe first indicates that scoping implicit universalsand/or negations must be easier than scoping ex-plicit NP chunks.
In order to find how much one orboth of the two elements contribute to this gain, wehave run two more experiments, scoping only thepairs with at least one implicit universal and pairswith one negation, respectively.
Figure 6 reportsthe results.
As seen, the contribution in boostingthe overall performance comes from the implicituniversals while negations, in fact, lower the per-formance.
The performance for pairs with implicituniversal is higher because universals, in general,70Implicit universals only (pairs with at least one id) P+ R+ F+Baseline (MA11) 0.776 0.458 0.576Our model (RPC-SVM-13) 0.836 0.734 0.782(a) Pairs with at least one implicit universalNegation only (pairs with one negation) P+ R+ F+Baseline (MA11) 0.502 0.571 0.534Our model (RPC-SVM-13) 0.733 0.55 0.629(b) Pairs with at least one negationFigure 6: Implicit universals and negationsare easier to scope, even for the human annota-tors.17 There are several reasons for poor perfor-mance with negations as well.
First, the numberof negations in the corpus is small, therefore thedata is very sparse.
Second, the RPC model doesnot work well for negations.
Scoping a negationrelative to an NP chunk, with which it has a longdistance dependency, often depends on the scopeof the elements in between.
Third, scoping nega-tion usually requires a deep semantic analysis.In order to see how well our approximation al-gorithm is working, similar to the approach ofChambers and Jurafsky (2008), we tried an ILPsolver18 for DAGs with at most 8 nodes to find theoptimum solution, but we found the difference in-significant.
In fact, the approximation algorithmfinds the optimum solution in all but one case.195 Related workSince automatic QSD is in general challenging,traditionally quantifier scoping is left underspec-ified in deep linguistic processing systems (Al-shawi and Crouch, 1992; Bos, 1996; Copestake etal., 2001).
Some efforts have been made to moveunderspecification frameworks towards weightedconstraint-based graphs in order to produce themost preferred reading (Koller et al, 2008), butthe source of these types of constraint are oftendiscourse, pragmatics, world knowledge, etc., andhence, they are hard to obtain automatically.
In or-17Trivially, we have taken the relation outscoping ic > idfor granted and not counted it towards higher performance.18lpsolve: http://sourceforge.net/projects/lpsolve19To find the gain that can be obtained with gold-standardparses, we used MA11?s system with their hand-annotatedand the equivalent automatically generated features.
Theformer boost the performance by 0.04.
Incidentally, HS03lose almost 0.04 when switching to automatically generatedparses.der to evade scope disambiguation, yet be able toperform entailment, Koller and Thater (2010) pro-pose an algorithm to calculate the weakest read-ings20 from a scope-underspecified representation.Early efforts on automatic QSD (Moran, 1988;Hurum, 1988) were based on heuristics, manuallyformed into rules with manually assigned weightsfor resolving conflicts.
To the best of our knowl-edge, there have been four major efforts on statisti-cal QSD for English: Higgins and Sadock (2003),Galen and MacCartney (2004), Srinivasan andYates (2009), and Manshadi and Allen (2011a).The first three only scope two scopal terms in asentence, where the scopal term is an NP with anexplicit quantification.
MA11 is the first to scopeany number of NPs in a sentence with no restric-tion on the type of quantification.
Besides ignor-ing negation and implicit universals, their systemhas some other limitations too.
First, the learningmodel is not theoretically justified.
Second, hand-annotated features (e.g.
dependency relations) areused on both the train and the test data.6 Summary and future workWe develop the first statistical QSD model ad-dressing the interaction of quantifiers with nega-tion and the implicit universal of plurals, defininga baseline for this task on QuanText data (Man-shadi et al, 2012).
In addition, our work improvesupon Manshadi and Allen (2011a)?s work by (ap-proximately) optimizing a well justified criterion,by using automatically generated features insteadof hand-annotated dependencies, and by boostingthe performance by a large margin with the help ofa rich feature vector.This work can be improved in many directions,among which are scoping more elements such asother scopal operators and implicit entities, de-ploying more complex learning models, and de-veloping models which require less supervision.AcknowledgementWe need to thank William de Beaumont andJonathan Gordon for their comments on the pa-per and Omid Bakhshandeh for his assistance.This work was supported in part by NSF grant1012205, and ONR grant N000141110417.20Those which can be entailed from other readings but donot entail any other reading71ReferencesHiyan Alshawi and Richard Crouch.
1992.
Monotonicsemantic interpretation.
In Proceedings of Associa-tion for Computational Linguistics, pages 32?39.Johan Bos.
1996.
Predicate logic unplugged.
In Pro-ceedings of the 10th Amsterdam Colloquium, pages133?143.Nathanael Chambers and Dan Jurafsky.
2008.
Jointlycombining implicit constraints improves temporalordering.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?08, pages 698?706, Stroudsburg, PA.William W. Cohen, Robert E. Schapire, and YoramSinger.
1999.
Learning to order things.
Journalof Artificial Intelligence Research, 10:243?270.Ann Copestake, Alex Lascarides, and Dan Flickinger.2001.
An algebra for semantic construction inconstraint-based grammars.
In Proceedings of As-sociation for Computational Linguistics ?01, pages140?147.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, CrossParser ?08, pages 1?8.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure trees.
InProceedings of International Conference on Lan-guage Resources and Evaluation ?12.Johannes Furnkranz and Eyke Hullermeier.
2003.Pairwise preference learning and ranking.
In Pro-ceedings of the 14th European Conference on Ma-chine Learning, volume 2837, pages 145?156.Andrew Galen and Bill MacCartney.
2004.
Statisticalresolution of scope ambiguity in natural language.http://nlp.stanford.edu/nlkr/scoper.pdf.Fritz Hamm and Edward W. Hinrichs.
2010.
Pluralityand Quantification.
Studies in Linguistics and Phi-losophy.
Springer.Aurelie Herbelot and Ann Copestake.
2010.
Anno-tating underquantification.
In Proceedings of theFourth Linguistic Annotation Workshop, LAW IV?10, pages 73?81.Derrick Higgins and Jerrold M. Sadock.
2003.
A ma-chine learning approach to modeling scope prefer-ences.
Computational Linguistics, 29(1):73?96.Eyke Hullermeier, Johannes Furnkranz, WeiweiCheng, and Klaus Brinker.
2008.
Label rankingby learning pairwise preferences.
Artificial Intelli-gence, 172(1617):1897 ?
1916.Sven Hurum.
1988.
Handling scope ambiguities inEnglish.
In Proceedings of the second conferenceon Applied natural language processing, ANLC ?88,pages 58?65.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In BernhardScho?lkopf, Christopher J. C. Burges, and Alexan-der J. Smola, editors, Advances in kernel methods,pages 169?184.
MIT Press, Cambridge, MA, USA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430.Alexander Koller and Stefan Thater.
2010.
Comput-ing weakest readings.
In Proceedings of the 48thAnnual Meeting on Association for ComputationalLinguistics, Uppsala, Sweden.Alexander Koller, Michaela Regneri, and StefanThater.
2008.
Regular tree grammars as a formal-ism for scope underspecification.
In Proceedings ofAnnual Meeting on Association for ComputationalLinguistics and Human Language Technologies ?08.Fred Landmann.
2000.
Events and plurality.
KluwerAcademic Publishers, Dordrecht.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of Association for Computa-tional Linguistics (ACL).Mehdi Manshadi and James Allen.
2011a.
Unre-stricted quantifier scope disambiguation.
In Pro-ceedings of Association for Computational Linguis-tics ?11, Workshop on Graph-based Methods forNLP (TextGraph-6).Mehdi Manshadi, James Allen, and Mary Swift.2011b.
A corpus of scope-disambiguated Englishtext.
In Proceedings of Association for Computa-tional Linguistics and Human Language Technolo-gies ?11: short papers, pages 141?146.Mehdi Manshadi, James Allen, and Mary Swift.
2012.An annotation scheme for quantifier scope disam-biguation.
In Proceedings of International Confer-ence on Language Resources and Evaluation ?12.Douglas Moran.
1988.
Quantifier scoping in the SRIcore language engine.
In Proceedings of the 26thAnnual Meeting on Association for ComputationalLinguistics.Lance Ramshaw and Mitch Marcus.
1995.
TextChunking Using Transformation-Based Learning.In Proceedings of the Third Workshop on Very LargeCorpora, pages 82?94, Somerset, New Jersey.Prakash Srinivasan and Alexander Yates.
2009.
Quan-tifier scope disambiguation using extracted prag-matic knowledge: preliminary results.
In Proceed-ings of EMNLP ?09.72
