Generating Context-Appropriate Word Ordersin TurkishBeryl Hoffman*Department of Computer and Information SciencesUniversity of Pennsylvania(hoffman@linc.cis.upenn.edu)1 Int roduct ionTurkish, like Finnish, German, Hindi, Japanese, andKorean, has considerably freer word order than En-glish.
In these languages, word order variation is usedto convey distinctions in meaning that are not gener-ally captured in the semantic representations that havebeen developed for English, although these distinctionsare also present-in somewhat less obvious ways in En-glish.
In the next section, I present a summary of thelinguistic data on Turkish word order variations.
Sec-tion 3 describes the categorial formalism I propose tomodel the syntax, semantics, and pragmatic informa-tion in Turkish sentences.
To capture the syntax offree word order languages, I present an adaptation ofCombinatory Categorial Grammars, CCGs (Steedman-85; Steedman-91), called {}-CCGs (set-CCGs).
Then,I integrate a level of information structure, represent-ing pragmatic functions uch as topic and focus, with{}-CCGs to allow pragmatic distinctions in meaning toinfluence the word order of the sentence in a composi-tional way.
In Section 4, I discuss how this strategy isused within a generation system which produces Turk-ish sentences with word orders appropriate to the con-text, and include sample runs of the implementation.2 Free Word Order in TurkishThe most common word order used in simple transitivesentences in Turkish is SOV (Subject-Object-Verb), utall six permutations ofa transitive sentence can be usedin the proper discourse situation since the subject andobject are differentiated by case-marking.
1*I would like to thank Mark Steed.man and the anonymousreferees for their valuable advice.
This work was partially sup-ported by DARPA N00014-90-J-1863, ARO DAAL03-89-C-0031,NSF IRI 90-16592, Ben Franklin 91S.3078C-1.1 According to a language acquisition study in (Slobin-82), 52%of transitive sentences used by a sample of Turkish speakers werenot in the canonical SOV word order.
(1) a. Ay?e Fatma'yl anyor.Ay~e Fatma-Acc seek-Pres-(3Sg).
"Ay?e is looking for Fatma."b.
Fatma'yl Ay~e arlyor.c.
Ay?e arlyor Fatma'yl.d.
Farina'y1 anyor Ay~e.e.
Anyor Fatma'yl Ay~e.f.
Anyor Ay?e Fatma'yl.The propositional interpretation assigned to all sixof these sentences is seek'(Ay~e',Fatma').
However,each word order conveys a different discourse meaningonly appropriate to a specific discourse situation.
Theone propositional interpretation cannot capture the dis-tinctions in meaning necessary for effective translationand communication i Turkish.
The interpretationsof these different word orders rely on discourse-relatednotions such as theme/rheme, focus/presupposition,topic/comment, etc.
that describe how the sentencerelates to its context.There is little agreement on how to represent hediscourse-related functions of components in the sen-tence, i.e.
the information structure of the sentence.Among Turkish linguists, Erguvanh (Erguvanli-84) cap-tures the general use of word order by associating eachposition in a Turkish sentence with a specific pragmaticfunction.
Generally in Turkish, speakers first place theinformation that links the sentence to the previous con-text, then the important and/or new information im-mediately before the verb, and the information thatis not really needed but may help the hearer under-stand the sentence better, after the verb.
Erguvanllidentifies the sentence-initial position as the topic, theimmediately preverbal position as the focus, and thepostverbal positions as backgrounded information.
Thefollowing template that I will be using in the implemen-tation describes the general association between wordorder and information structure components (in boldfont) for Turkish sentences:(2) Topic Neutra l  Focus Verb Background1177th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994I will call the phrase formed by the topic and the neutralcomponents he theme of the sentence and the phraseformed by the focus and the verb, the rheme of thesentence.Using these information structure components, wecan now explain why certain word orders are appro-priate or inappropriate in a certain context.
For exam-ple, a speaker may use the SOV order in (3b) because inthat context, the speaker wants to focus the new object,Ahmet, and so places it in the immediately preverbalposition.
However, in (4)b, Ahmet is the topic or a linkto the previous context whereas the subject, Fatma, isthe focus, and thus the OSV word order is used.
Here,we translate these Turkish sentences to English usingdifferent "stylistic" constructions (e.g.
topicalization,it-clefts, phonological focusing etc.)
in order to pre-serve approximately the same meanings.
(3) a. Fatma kimi anyor?Fatma who seek-Pres?
"Who is Fatma looking for?"b.
Fatma Ahmet'i anyor.
SOVFatma Ahmet-Acc seek-Pres.
"Fatma is looking for AHMET.
"(4) a.b.Ahmet'i kim anyor?Ahmet-Dat who seek-Pres.
"Who is looking for Ahmet?
"Ahmet'i Fatma anyor.
OSVAhmet-Acc Fatma seek-Pres.
"As for Ahmet, it is FATMA who is looking for him.
"It is very common for Turkish speakers to putinformation already mentioned in the discourse, i.e.discourse-given, in the post-verbal positions, in thebackground component of the information structure.In fact, discourse-new elements cannot occur in thepostverbal positions.
In addition, referential status,' i.e.whether the speaker uses a full noun phrase, an overtpronoun, or a null pronoun to refer to an entity in thediscourse, can be used to signal the familiarity infor-mation to the hearer.
Thus, given information can befreely dropped (5)bl or placed in post-verbal positions(5)b2 in Turkish.
Although further esearch is requiredon the interaction between referential status and wordorder, I will not concentrate on this issue in this pa-per.
(5) a.bl.Fatma Ahmet'i aradl.Fatma Ahmet-Acc seek-Past.
"Fatma looked for Ahmet.
"Area 0 O bulama&.But 0 0 find-Neg-Past.
"But (she) could not find (him)."b2.
Ama bulamadl Fatma Ahmet'i.But find-Neg-Past Fatma Ahmet-Acc.
"But she, Fatma, could not find him, Ahmet.
"The same information structure components topic,focus, background can also explain the positioning ofadjuncts in Turkish sentences.
For example, placing alocative phrase in different positions in a sentence re-sults in different discourse meanings, much as in Englishsentences:(6) a. Fatma Ahmet'i Istanbul'da ra&.Fatma Ahmet-Acc Istanbul-loc seek-Past.
"Fatma looked for Ahmet in ISTANBUL."b.
Istanbul'da FatmaAhmet'i aradl.IstanbuMoc Fatma Ahmet-Acc seek-Past.
"In Istanbul, Fatma looked for Ahmet."c.
Fatma Ahmet'i aradl Istanbul'da.Fatma Ahmet-Acc seek-Past Istanbul-loc.
"Fatma looked for Ahmet, in Istanbul.
"Long distance scrambling, word order permutation i -volving more than one clause, is also possible out ofmost embedded clauses in Turkish; in complex sen-tences, elements of the embedded clauses can occurin matrix clause positions.
However, these word or-ders with long distance dependencies are only used byspeakers for specific pragmatic functions.
Generally, anelement from the embedded clause can occur in the sen-tence initiM topic position of the matrix clause, as in(7)b, or to the right of the matrix verb as backgroundedinformation, as in (7)c. 2(7) a.Fatma \[Ay~e'nin gitti~ini\] biliyor.Fatma \[Ay?e-Gen go-Ger-3sg-Acc\] know-Prog.
"Fatma knows that Ay~e left.
"b.Ay?e'nini Fatma \[tl gitti~ini\] biliyor.Ay?e-Gen~ Fatma \[ti go-Ger-3sg-Acc\] know-Prog.
"As for Ay~e, Fatma knows that she left.
"C.Fatma \[ti gitti~ini\] biliyor Ay~e'nini.Fatma Irk go-Ger-3sg-Acc\] know-Prog Ay~e-Geni.
"Fatma knows that she, Ay?e, left.
"3 The  Categor ia l  Formal i smMany different syntactic theories have been proposed todeal with free word order variation.
It has been widelydebated whether word order variation is the result ofstylistic rules, the result of syntactic movement, orbase-generated.
I adopt a categorial framework in whichthe word order variations in Turkish are pragmatically-2I have put-in coindexed traces and italicized the scrambledelements in these examples to help the reader; I am not makingthe syntactic laim that these traces actually exist.1187th International Generation Workshop * Kennebunkport, Maine * June 21-24, 1994driven; this lexicalist framework is not compatible withtransformational movement rules.My work is influenced by (Steedman-91) in whicha theory of prosody, closely related to a theory ofinformation structure, is integrated with Combina-tory Categorial Grammars (CCGs).
Often intona-tional phrase boundaries do not correspond to tradi-tional phrase structure boundaries.
However, by us-ing the CCG type-raising and composition rules, CCGformalisms can produce nontraditional syntactic con-stituents which may match the intonational phrasing.These intonational phrases often correspond to a unitof planning or presentation with a single discourse func-tion, much like the information structure componentsof topic, neutral, focus, and background in Turkish sen-tences.
Thus, the ambiguity that CCG rules produce isnot spurious, but in fact, necessary to capture prosodicand pragmatic phrasing.
The surface structure of a sen-tence in CCGs can directly reflect its information struc-ture, so that different derivations of the same sentencecorrespond to different information structures.In the previous ection, we saw that ordering of con-stituents in Turkish sentences is dependent on prag-matic functions, the information structure of the sen-tence, rather than on the argument structure of the sen-tence as in English.
Moreover, information structure isdistinct from argument structure in that adjuncts andelements from embedded clauses can serve a pragmaticfunction in the matrix sentence and thus be a compo-nent of the information structure without taking partin the argument structure of the matrix sentence.
Thissuggests an approach where the ordering informationwhich is dependent on the information structure is sep-arated from the the argument structure of the sentence.In section 3.1, I describe a version of CCGs adapted forfree word order languages in (Hoffman-92) to capturethe argument structure of Turkish, while producing aflexible surface structure and word order.
In addition,each CCG constituent is associated with a pragmaticcounterpart, described in section 3.2, that contains thecontext-dependent word order restrictions.3 .1  {}-CCGMulti-set Combinatory Categorial Grammars,{}-CCGs, (Hoffman-92) is a version of CCGs for freeword order languages in which the subcategorizationinformation associated with each verb does not spec-ify the order of the arguments.
Each verb is assigned afunction category in the lexicon which specifies a rnulli-set of arguments, so that it can combine with its argu-ments in any order.
For instance, a transitive verb hasthe following category S l{Nn , Na}  which defines afunction looking for a set of arguments, nominative casenoun phrase (Nn) and an accusative case noun phrase(Na), and resulting in the category S, a complete sen-tence, once it has found these arguments.
Some phrasestructure information is lost by representing a verb asa function with a set of arguments.
However, this cat-egory is also associated with a semantic interpretation.For instance, the verb "see" could have the followingcategory where the hierarchical information among thearguments is expressed within the semantic interpre-tation separated from the syntactic representation bya colon: S : see(X ,Y ) \ ]{Nr t  : X ,  Na  : Y} .
Thiscategory can easily be transformed into a DAG repre-sentation like the following where coindices,z and y, areindicated by italicized font.
sResultArts(8)'Syn :: SemSynSereSynSem\[Cat: S, Tense: Pres\] \]see(x,y) JCat : np \] \]\[ Case : nom ,:\ [Case  : acc: yWe can modify the CCG application rules for func-tions with sets as follows.
The sets indicated by bracesin these rules are order-free, i.e.
Y in the following rulescan be any element in the set.
Functions can specifya direction feature for each of their arguments, notatedin the rules as an arrow above the argument.
4 We as-sume that a category X\[{ } where { } is the empty setrewrites by a clean-up rule to just X.
(9) a.
Forward  App l i ca t ion '  (>):y Xl{...}b. Backward  App l i ca t ion '  (<):Y Xl{...}Using these new rules, a verb can apply to its argumentsin any order.
For example, the following is a derivationof a sentence with the word orderObject-Subject-Verb:(10) Gazeteyi Ay~e okuyor.Newspaper-acc Ay~e reads?Na Nn SI{Nn,Na }S\[{Na}<3To improve the efficiency of unification and parsing, the ar-guments in the set can be associated with feature labels whichindicate their category and case.4 Since Turkish is not strictly verb-final, most verbs will notspecify the direction features of their arguments.1197th International Generation Workshop * Kennebunkport, Maine * June 21-24, 1994Instead of using the set notation, we could imag-ine assigning Turkish verbs multiple lexical entries, onefor each possible word order permutation; for exam-ple, a transitive verb could be assigned the categoriesS\Nn\ga, S\Na\Nn, S\Na/gn, etc., instead of theone entry S\[ {Nn, Na}.
However, we will see below thatthe set notation is more than a shorthand represent-ing multiple entries because it allows us to handle longdistance scrambling, permutations involving more thanone clause, as well.The following composition rules are proposed to com-bine two functions with set-valued arguments, e.g.
twoverbs.
(11) a.
Forward  Compos i t ion '  (> S):YI{-..2} x1{...1,...2}b. Backward  Compos i t ion '  (< B):YI{...1}These composition rules allow two verb categories withsets of arguments to combine together.
For example,(12)go-gerund-acc knows.S~o : go(y)lUVg : y} S :  know(z,p)I{Nn: x, S. .
:  p}<BS: know(x, go(y))l{Ng : y, Nn : x}As the two verbs combine, their arguments collapse intoone argument set in the syntactic representation.
How-ever, the verbs' respective arguments are still distinctwithin the semantic representation f the sentence.
Thepredicate-argument structure of the subordinate clauseis embedded into the semantic representation of thematrix clause.
Long distance scrambling can easily behandled by first composing the verbs together to forma complex verbal function which can then apply to allof the arguments in any order.Certain coordination constructions ( uch as 'SO andSOV' seen in (13) as well as 'SOV and SO') can be han-dled in a CCG based formalism by type-raising NPs intofunctions over verbs.
Two type-raised noun phrasescan combine together using the composition rules toform a nontraditional constituent which can then coor-dinate.
(13)Ay?e kitabx, Fatma da gazeteyi okuyor.Ay~e book-acc, Fatma too newspaper-acc reads.
"Ay~e is reading the book and Fatma the newspaper.
"Order-preserving type-raising rules that are modifiedfor {}-CCGs are used to convert nouns in the gram-mar into functors over the verbs.
These rules can beobligatorily activated in the lexicon when case-markingmorphemes attach to the noun stems.>(14) a. N + case ~ .... }}<ub.
N + case ~ (SI{-.-})I {S l{Ncase, ...}}The first rule indicates that a noun in the presence ofa case morpheme becomes a functor looking for a verbon its right; this verb is also a functor looking for theoriginal noun with the appropriate case on its left.
Afterthe noun functor combines with the appropriate verb,the result is a functor which is looking for the remainingarguments of the verb.
The notation ... is a variablewhich can unify with one or more elements of a set.The second type-raising rule indicates that a case-marked noun is looking for a verb on its left.
{\]-CCGscan model a strictly verb-finM language like Korean byrestricting the noun phrases of that language to thefirst type-raising rule.
Since most, but not all, case-marked nouns in Turkish can occur behind the verb,certain pragmatic and semantic properties of a Turkishnoun determine whether it can type-raise to the cate-gory produced by the second rule.The {}-CCG for Turkish described above can be usedto parse and generate all word orders in Turkish sen-tences.
However, it does not capture the more interest-ing questions about word order variation: namely, whyspeakers choose a certain word order in a certain con-text and what additional meaning these different wordorders provide to the hearer.
Thus, we need to inte-grate the {}-CCG formalism with a level of informationstructure that represents pragmatic functions, such astopic and focus, of constituents in the sentence in acompositional way.3 .2  A Grammar  fo r  Word  OrderIn (Steedman-91; Prevost/Steedman-93), a theory ofprosody, closely related to a theory of information struc-ture, is integrated with CCGs by associating every CCGcategory encoding syntactic and semantic propertieswith a prosodic category.
Taking advantageof  the non-traditional constituents that CCGs can produce, twoCCG constituents are allowed to combine only if theirprosodic counterparts can also combine.Similarly, I adopt a simple interface between {}-CCGand ordering information by associating each syntac-tic/semantic category with an ordering category whichbears linear precedence information.
These two cat-egories are linked together by the features of the in-formation structure.
For example, the verb "arwor"(seeks) is assigned the lexical entry seen in the categoryfeature of the DAG in Figure 1.
The category featurecontains the argument structure in the features yn andsere as well as the information structure in the featureinfo.
This lexical entry is associated with an orderingcategory seen in the feature order of the DAG in Fig-ure 1.
This ordering feature is linked to the category1207th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994anyor (seek) :=" "Cat : S \]Syn : Tense : Pres J: \]Sem : LF : seek(x,y), Xlf, Ylf\]" \[Topic : N T \] Result : Theme : \[Neutral :\[Focus : F \]Info : I Rheme : \[Main-Prop : seekBackground : BSyn : \[Case : nom\] Syn : \ [Case :*rgs : LSeI \ [Ent i ty  : ?
\ [Sem Ent i ty :: \[ Props X Props :/ / ( \ [Background:  B\])\(\[Topic : T\])\(\[Neutral : N\])\(\[Focus : F\])Category :Order :?p\]acc,Figure 1: The Lexical Entry for a Transitive Verb, "anyor" (seeks).feature via the co-indices T, N, F, and B.The ordering categories are assigned to lexical entriesaccording to context-dependent word order restrictionsfound in the language.
All Turkish verbs are assignedthe ordering category seen in the orderfeature in Figure1; this is a function which can use the categorial appli-cation rules to first combine with a focused constituenton its left, then a neutral constituent on its left, thena topic constituent on its left, and then a backgroundconstituent on its right, finally resulting in a completeutterance.
This function represents the template men-tioned in example 2 for assigning discourse functions ac-cording to their positional relation to the verb following(Erguvanli-84).
However, it is more flexible than Ergu-vanh's approach in that it allows more than one possi-ble information structure.
The parentheses around thearguments of the ordering category indicate that theyare optional arguments.
The sentence may contain allor some or none of these information structure com-ponents.
It may turn out that we need to restrict theoptionality on these components.
For instance, if thereis no topic found in the sentence-initial position, thenwe may need to infer a topic or a link to the previouscontext.
In the current implementation, the focus is anobligatory constituent in order to ensure that the parserproduces the derivation with the most likely informa-tion structure first, and there is an additional orderingcategory possible where the verb itself is focused andwhere there are no pre-verbal elements in the sentence.Categories other than verbs, such as nouns, deter-miners, adjectives, and adverbs, are associated with anordering category that is just a basic element, not afunction.
In Turkish, the familiarity status of entitiesin the discourse model serves a role in determining theirdiscourse function.
For example, discourse-new entitiescannot occur in the post-verbal or sentence initial po-sitions in Turkish sentences.
Thus, discourse-new el-ements can be assigned ordering categories with thefeature-attribute focus or neutral with their semanticproperties as the feature-value, but they cannot be as-sociated with background or ?opic ordering categories.There are no such restrictions for discourse-old entities;thus they can be assigned a variable which can unifywith any of the information structure components.During a derivation in parsing or generation, two con-stituents can combine only if the categories in theircategory features can combine using the {}-CCG rulespresented in the previous section, and the categoriesin their order features can combine using the follow-ing rewriting rules.
A sample derivation involving theordering grammar can be seen in Figure 2.
(15) a.
Forward  App l i ca t ion  (>):X/Y  Y =~ X where Y is not a functor.b.
Backward  App l i ca t ion  (<):Y X \Y  ~ X where Y is not a functor.c.
Forward  Sk ip -Opt iona l  Ru le  (>~kip):X / (Y)  Z ~ X Zd.
Backward  Sk ip -Opt iona l  Ru le  (<skip):z x \ (Y )  x ze.
Ident i ty  (=):X X~XThe identity rule allows two constituents with thesame discourse function to combine.
The resulting1217th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994Q: Ay?e Fatma'yl ne zaman aradffAy?e Fatma-Acc when call-Past?
"When did Ay~e call Fatma?
"A: Bugiin aradlToday call-PastFocus:~oday' I/(Skgd:B)\(Wopic: T)\(Neutral:N)\(Focus:F)(<)I/(Bkgd:B)\(Topic: T)\(Neutral:N)//(Bkgd:B)\(Topic: T)//(Bkgd:B)Ay?eAy~eX:\[ayse\]IFatma'yl.Fatma-Acc.X:\[fatma\]X : \[ ayse ',fairaa \](>)(=)Figure 2: A Derivation involving just the Ordering Categories.constituent may not be a traditional syntactic con-stituent, however as argued in (Steedman-91), this iswhere we see the advantage of using a CCG based for-malism.
Through type-raising and composition, CCGformalisms can produce nontraditional syntactic on-stituents which may have a single discourse function.For example in Figure 2, the NPs Farina and Ay~eform a pragmatic onstituent using the identity rulein the ordering grammar; in order to form a syntac-tic constituent as well, they must be type-raised andcombine together using the {}-CCG composition rule.Type-raising in Turkish is needed for sentences involv-ing more than one NP in the neutral and backgroundpositions.The ordering rammar also allows adjuncts and ele-ments from other clauses (long distance scrambled) tobe components in the information structure.
This isbecause the information structure in a verb's lexicalentry does not specify that its components must be ar-guments of the verb in its argument structure.
Thus,adjuncts and elements from embedded clauses can beserve a purpose in the information structure of the ma-trix clause, although they are not subcategorized argu-ments of the matrix verb.
For long distance scrambling,the additional restriction (that Y is not a functor) onthe application rules given above ensures that a verb inthe embedded clause has already combined with all ofits obligatory arguments or skipped all of its optionalarguments before combining with the matrix verb.The ordering grammar presented above is similarto the template grammars in (Danlos-87), the syntaxspecialists in PAULINE (Hovy-88), and the realiza-tion classes in MUMBLE (McDonald/Pustejovsky-85)in that it allows certain pragmatic distinctions to in-fluence the syntactic onstruction of the sentence.
Theordering rammar does not make as fine-grained prag-matic distinctions as the generation systems above, butit represents language-specific and context-dependentword order restrictions that can be lexicalized into com-positional categories.
The categorial formalism pre-sented above captures the general discourse meaningof word order variation in languages such as Turkishwhile using a compositional method.4 The  Implementat ionI have implemented a simple data-base query task, dia-gramed in Figure 3, to demonstrate how the categorialformalism presented in the previous ection can gener-ate Turkish sentences with word orders appropriate tothe context.
The system simulates a Personal Assistantwho schedules meetings and phone calls with a numberof individuals.
The user issues queries to which theprogram responds, after consulting the data-base, insentences with the appropriate word order, while main-taining a model of the changing context.Since most of the information is lexicalized, the samegrammar and lexicon is used by the parser and thegenerator.
After the question is parsed, the discoursemodel is updated ~, and the question's representation issent to the planning component of the generator.
Theplanner at this point consists of simple plans for con-structing answers to certain wh-questions and yes/noquestions.
Certain predicates in the queries trigger thes As suggested by(Vallduvi-90), the information structure ofa sentence an provide cues on how to update and organize thediscourse model.1227th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994Grammar DataBaseQuestionParser1Planner1TacticalGeneratorAnswerFigure 3: The PersonalLexicon OrderingCategoriesy DiscourseModelAssistant Generation Systemplanner to look up schedules and make appointmentsfor the agents mentioned in the query.The planner creates a representation for the answerby copying much of the question representation a d byadding the appropriate new information found in thedatabase.
The information structure of the questioncan be used by the planner as well.
The topic of thequestion is copied to the answer in order to maintaintopic continuity, although in a less limited domain, aseparate algorithm isneeded to allow for shifts in topic.In addition, when a yes/no question is not validatedin the data-base, the planner eplaces the focus of thequestion with a variable and requests another search ofthe data-base to find a new focus which statisfies therest of the question.
For example, 6(16) a. Ahmet Fatma'yi gSrdii mii?Ahmet Fatma-Acc see-Past Quest?
"Did Ahmet see FATMA?"b.
Haylr, ama Ahmet Ay?e'yi gSrdii.No, but Ahmet Ay?e-Acc see-Past.
"No, but Ahmet saw AY~E.In all question types, the information found in thedatabase lookup is specified to be the focus of the an-swer.
The semantic properties of the focused entity areeither found in the database, or if it has already beenmentioned in the discourse, by consulting the discoursemodel.
The planner then passes the representation f6Particles uch as "yes" and "no" are are produced by a sep-arate call to the generator, before generating the answer.the answer to the realization component of the genera-tor described in the next section.4.1 Head-dr iven  Bot tom-up  Genera -t ionI adopt a head-driven bottom up generation algorithm(Calder/etal, 1989; Shieber/etal-89; vanNoord-90) thattakes advantage oflexical information as well as the top-down input provided by the planner.
This approach isparticularly useful for categorial grammars since mostof the information isstored in the lexical entries ratherthan the grammar rules.The planning component described above providesthe input for the algorithm, for example, to generate asentence with the syntactic, semantic, and informationstructure features hown in Figure 4.
7 The input doesnot have to fully specify the word order in the infor-mation structure.
For instance, since the descriptionin Figure 4 of the sentence to be generated oes notspecify the function of "the students" in the informa-tion structure, either of the following two word orderscan be generated:(17) a. Ay~e 5~rencileri ii~te gSrdii.Ay?e student-P1-Acc three-Loc see-Past.
"Ay?e saw the students at THREE.
"rNote that the semantic predicates of the sentence are  repre-sented using a list notation; actually, the DAG unification algo-r i thm has been extended to recognize the function format suchas s tudent (x )  as  features, so that this notat ion is a DAG as well.1237th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994Category : ResultSynSemInfo"Cat :Tense :?
Event :LF :ThemeRheme?\[time(e,3), see(e,Ayse,y),\[def(Ayse,+), one(Ayse)\],\[def(y,+), many(y), student(y)\]\]\[Topic : \[def(Ayse,+), one(Ayse)\] ]: \[ Neutral : J: Main-Prop : seeBackground :Figure 4: Input to the Generation Algorithm?b.
Ay?e ii~te ghrdfi 5~rencileri.Ay?e three-Loc see-Past student-P1-Acc.
"Ay~e saw the students at THREE.
"The algorithm for the head-driven bottom up gener-ator is seen below:generate(Input) :-find_lex_cat(Input,LexDag),bup_generate(Input,LexDag).bup_generate(Input,LexDag):- unify(Input,LexDag).bup_generate(Input, LexDag) :-combine(Arg, LexDag, ResDag, backward),generate(Arg),order(Arg, LexDag, ResDag),concat_phons(Arg, LexDag, ResDag),bup_generate(Input, ResDag).bup_generate(Input, LexDag) "-combine(LexDag, Arg, ResDag, forward),generate (Arg),order(LexDag, Arg, ResDag),concat_phons(LexDag, Arg, ResDag),bup_generate(Input, ResDag).This algorithm is very similar to the (Calder/etal,1989) algorithm for Unificational Categorial Grammar(UCG).
First, the function generate  finds a categoryin the lexicon which is the head of the sentence.
Then inbup-generate ,  we try to apply the combinatory gram-mar rules (i.e.
the forward and backward {}-CCG rules)to this lexical functor to generate its arguments in abottom-up fashion.
The order function applies the or-dering rules to the functor and argument to make surethat they form a constituent in the information struc-ture.
The bup-generate  function is called recursivelyon the result of applying the rules until it has found allof the head functor's (LezDag) arguments, eventuallyresulting in something which unifies with the Input.
sThe main difference between this CCG algorithm andthe UCG algorithm is that the CCG algorithm uses allof the information (syntactic, semantic, and informa-tion structure features) given in the input, instead ofusing only the semantic information, to find the headfunctor in the lexicon.
This is possible because of theformulation of the CCG rules.
We can assume there issome function in the lexicon whose result unifies withthe input, if this function is to take part in a CCGderivation that produces the input.
This assumptionis built into the CCG rules, since the head daughter ineach rule (shown in bold in the following {}-CCG rules)shares its function result (X) with the final result afterapplying the rule:(18) a. X I{Y , .
.
.}
Y =z X\]{..
.}b.
Y X \ [{Y , .
.
.}
~ XI{...}c. YI{...2} x1{.. .1, .
.
.2}d. Y\[{...1} X I{Y , .
.
.~}  ~ X\[{...1,...2}To make the algorithm more efficient, f ind - lex -catfirst finds a rough match in the lexicon using term-unification.
We associate ach item in the lexicon witha semantic key-predicate that is one of the propertiesin its semantic description.
A lexical entry roughlymatches the input if its semantic key-predicate is amember of the list of semantic properties given in the in-put.
After a rough match using term-unification, f ind-lex-cat  unifies the DAGs containing all of the knownsyntactic, semantic, and pragmatic information for themost embedded result of the lexical category and theresult of the input, e.g.
Figure 4, to find the lexical8Note that order and concat-phons must be called after wehave lexically instantiated both Arg and LexDag to avoid infiniteloops.
The UCG algorithm also freezes uch features until theargument is instantiated.1247th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994category which is the head functor.
9 Then, the rulescan be applied in a bottom up fashion assuming thatthe found lexical category is the head daughter in therules.In this section, I have shown how the head-drivenbottom-up generation algorithm can be adapted forthe CCG formalism.
The following sample runs of thegeneration system further demonstrate how context-appropriate word orders are generated in this formal-ism.4.2 Sample RunsThe sample runs below represent he following trans-lated dialogue:(19) a. Fatma Ay~e'yi g6rebilirmi?Fatma Ay?e-Acc see-abil-aor-quest?
"Can Fatma see Ay?e?"b.
Evet, Fatma Ay~e'yi ikide g6rebilir.Yes, Fatma Ay~e-Acc two-Loc see-abil-aor.
"Yes, Fatma can see Ay~e at TWO.
"(20) a. Ikide kimi gbrecek Fatma?Two-Loc who-Acc see-Future Fatma?
"Who will Fatma see at two?"b.
Ikide Ay?e'yi gbrecek Fatma.Two-loc Ay~e-Acc see-Pres-prog Fatma.
"At two, she, Fatma, will see AYtE.
"The questions are the user's input and the answers arethe Personal Assistant system's replies.
Each question-answer pair is followed by the DAG representation ofthe answer.
Note that the syntax and semantics of theanswers in each run are identical, but the difference intheir information structure is reflected in the word orderthat is generated.I: fatma ayseyi  goreb i l i rmi7Answer: ever, fatma ayseyi  ik ide gorebi l ir .Dag:syn :cat : svoice : act ivetense : aor istagr :number  : s ingperson  : 3compound : abi l i t ive9The function fmd-lex-cat can  Mso fund type-r~sed NPs in thelexicon whose resets unit  with the input.
However, to preventinfinite loops, type-rMsed categories cannot be written with justa variable ~ the reset of the ~nction, i.e.
v/(v\np) where vis a variable.
One solution to this is to represent the semantic~atures of the NP in the result of the type-raised function, usingthe DAG notation ~r  the semantics.sem :type : declIf : \[t ime(e6,2), see(e6, fatma,ayse) ,\ [one(fatma),def( fatma,+)\] ,\ [one(ayse),def(ayse,+)\] \ ]event : e6info :rheme :focus : \[t ime(e6,2)\]mainprop : see(e6, fatma,ayse)theme :topic : \[one(fatma), def( fatma,+)\]neutra l  : \[one(ayse), def(ayse,+)\]background : noneI: ik ide kimi gorecek fatma?Answer: ikide ayseyi  gorecek fatma.Dag:syn :cat : svoice : act ive$ense:  futureagr :number : singperson : 3sen :type : dec1if : \[t ime(e6,2), see(e6, fatma,ayse) ,\ [one(fatma),def( fatma,+)\] ,\ [one(ayse),def(ayse,+)\] \ ]event : e6info :rhemethemebackground :focus : \ [one(ayse) ,de f (ayse ,+) \ ]mainprop : see(e6 , fa tma,ayse)topic : \[t ime(e6,2)\]neutral  : none\ [one(fatma),def( fatma,+)\]5 Conc lus ionsIn this paper, I have presented a strategy for the re-alization component of a generation system to handleword order variation in Turkish sentences.
I integrateda level of information structure with a unification-basedversion of Combinatory Categorial Grammars,  adaptedfor free word order languages.
I discussed an imple-mentation of a database query task using a modifiedhead-driven bottom-up generation algorithm to demon-1257th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994strate how the categorial formalism generates Turkishsentences with word orders appropriate to the context.Further research is needed on processing the infor-mation found in the information structure after pars-ing a sentence, e.g.
inferences about how focused dis-course entities or topic entities are related to sets ofother discourse ntities in the discourse model.
In ad-dition, a separate algorithm, perhaps Centering Theory(Grosz/etal-83), is needed to keep track of the salienceof discourse ntities and resolve the reference of emptypronouns, or in the case of generation, to determinewhat must be realized and what can be dropped in theutterance.
In future research, I would also like to ex-tend this same approach to generate certain stylisticconstructions in English such as topicalization, it-clefts,and right dislocation.ReferencesJ.
Calder, M. Reape, and H. Zeevat.
An Algorithmfor Generatiola in Unification Categorial Grammars.In Proceedings of the ~th Conference of the EuropeanACL, 1989.Laurence Danlos, editor.
The Linguistic Basis of TextGeneration.
Studies in NL Processing.
CambridgeUniversity Press, 1987.Eser Emine Erguvanli.
The Function of Word Orderin Turkish Grammar.
University of California Press,1984.
UCLA PhD dissertation 1979.Barbara Grosz, Aravind K. Joshi, and Scott Weinstein.Providing a unified account of definite noun phrasesin discourse.
In Proceedings of the 21st Annual Meet-ing of the Association for Computational Linguistics,Cambridge, MA, 1983.Beryl Hoffman.
A CCG Approach to Free Word Or-der Languages.
In Proceedings of the 30th AnnualMeeting of the ACL, 1992.Eduard H. Hovy.
Generating Natural Language UnderPragmatic Constraints, Hillsdale, N J: Lawrence Erl-baum, 1988.David D. McDonald and James Pustejovsky.
TAGs asa Grammatical Formalism for Generation.
In Pro-ceedings of the 23rd Conference of ACL, 1985.Scott Prevost and Mark Steedman.
Generating Intona-t ion Form Context Using a Combinatory Grammar.to apppear in JACL, 1993.Dan I. Slobin and Thomas G. Bever.
Children useCanonical Sentence Schemas: A Cross-linguisticStudy of Word Order and Inflections.
Cognition,12:229-265, 1982.S.
Shieber, G. van Noord, R. Moore, and F. Pereira .A Semantic-Head-Driven Generation Algorithm forUnification Based Formalisms.
In Proceedings of the27th Conference of ACL, 1989.Mark Steedman.
Dependencies and Coordination i theGrammar of Dutch and English.
Language, 61:523-568, 1985.Mark Steedman.
Structure and Intonation.
Language,67:260-296, 1991.Enric Vallduvi.
The Informational Component.
PhDthesis, University of Pennsylvania, 1990.Gertjan van Noord.
An Overview of Head-drivenBottom-up Generation.
In Robert Dale, Chris Mel-lish, and Michael Zock, editors, Current Researchin Natural Language Generation, Cognitive ScienceSeries, pages 141-166.
Academic Press, New York,1990.126
