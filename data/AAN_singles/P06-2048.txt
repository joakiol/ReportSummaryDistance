Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 369?376,Sydney, July 2006. c?2006 Association for Computational LinguisticsExploring the Potential of Intractable ParsersMark HopkinsDept.
of Computational LinguisticsSaarland UniversitySaarbr?ucken, Germanymhopkins@coli.uni-sb.deJonas KuhnDept.
of Computational LinguisticsSaarland UniversitySaarbr?ucken, Germanyjonask@coli.uni-sb.deAbstractWe revisit the idea of history-based pars-ing, and present a history-based parsingframework that strives to be simple, gen-eral, and flexible.
We also provide a de-coder for this probability model that islinear-space, optimal, and anytime.
Aparser based on this framework, whenevaluated on Section 23 of the Penn Tree-bank, compares favorably with other state-of-the-art approaches, in terms of both ac-curacy and speed.1 IntroductionMuch of the current research into probabilis-tic parsing is founded on probabilistic context-free grammars (PCFGs) (Collins, 1996; Charniak,1997; Collins, 1999; Charniak, 2000; Charniak,2001; Klein and Manning, 2003).
For instance,consider the parse tree in Figure 1.
One way to de-compose this parse tree is to view it as a sequenceof applications of CFG rules.
For this particulartree, we could view it as the application of rule?NP ?
NP PP,?
followed by rule ?NP ?
DT NN,?followed by rule ?DT ?
that,?
and so forth.
Henceinstead of analyzing P (tree), we deal with themore modular:P(NP ?
NP PP, NP ?
DT NN,DT ?
that, NN ?
money, PP ?
IN NP,IN ?
in, NP ?
DT NN, DT ?
the,NN ?
market)Obviously this joint distribution is just as diffi-cult to assess and compute with as P (tree).
How-ever there exist cubic-time dynamic programmingalgorithms to find the most likely parse if we as-sume that all CFG rule applications are marginallyNPNPDTthatNNmoneyPPINinNPDTtheNNmarketFigure 1: Example parse tree.independent of one another.
The problem, ofcourse, with this simplification is that althoughit is computationally attractive, it is usually toostrong of an independence assumption.
To miti-gate this loss of context, without sacrificing algo-rithmic tractability, typically researchers annotatethe nodes of the parse tree with contextual infor-mation.
A simple example is the annotation ofnodes with their parent labels (Johnson, 1998).The choice of which annotations to use isone of the main features that distinguish parsersbased on this approach.
Generally, this approachhas proven quite effective in producing Englishphrase-structure grammar parsers that performwell on the Penn Treebank.One drawback of this approach is its inflexibil-ity.
Because we are adding probabilistic contextby changing the data itself, we make our data in-creasingly sparse as we add features.
Thus we areconstrained from adding too many features, be-cause at some point we will not have enough datato sustain them.
We must strike a delicate bal-ance between how much context we want to in-clude versus how much we dare to partition ourdata set.369The major alternative to PCFG-based ap-proaches are so-called history-based parsers(Black et al, 1993).
These parsers differ fromPCFG parsers in that they incorporate context byusing a more complex probability model, ratherthan by modifying the data itself.
The tradeoff tousing a more powerful probabilistic model is thatone can no longer employ dynamic programmingto find the most probable parse.
Thus one tradesassurances of polynomial running time for greatermodeling flexibility.There are two canonical parsers that fall intothis category: the decision-tree parser of (Mager-man, 1995), and the maximum-entropy parser of(Ratnaparkhi, 1997).
Both showed decent resultson parsing the Penn Treebank, but in the decadesince these papers were published, history-basedparsers have been largely ignored by the researchcommunity in favor of PCFG-based approaches.There are several reasons why this may be.
Firstis naturally the matter of time efficiency.
Mager-man reports decent parsing times, but for the pur-poses of efficiency, must restrict his results to sen-tences of length 40 or less.
Furthermore, his two-phase stack decoder is a bit complicated and is ac-knowledged to require too much memory to han-dle certain sentences.
Ratnaparkhi is vague aboutthe running time performance of his parser, stat-ing that it is ?observed linear-time,?
but in anyevent, provides only a heuristic, not a complete al-gorithm.Next is the matter of flexibility.
The main ad-vantage of abandoning PCFGs is the opportunityto have a more flexible and adaptable probabilis-tic parsing model.
Unfortunately, both Magermanand Ratnaparkhi?s models are rather specific andcomplicated.
Ratnaparkhi?s, for instance, consistsof the interleaved sequence of four different typesof tree construction operations.
Furthermore, bothare inextricably tied to the learning procedure thatthey employ (decision trees for Magerman, maxi-mum entropy for Ratnaparkhi).In this work, our goal is to revisit history-basedparsers, and provide a general-purpose frameworkthat is (a) simple, (b) fast, (c) space-efficient and(d) easily adaptable to new domains.
As a methodof evaluation, we use this framework with a verysimple set of features to see how well it performs(both in terms of accuracy and running time) onthe Penn Treebank.
The overarching goal is to de-velop a history-based hierarchical labeling frame-work that is viable not only for parsing, but forother application areas that current rely on dy-namic programming, like phrase-based machinetranslation.2 PreliminariesFor the following discussion, it will be useful toestablish some terminology and notational con-ventions.
Typically we will represent variableswith capital letters (e.g.
X , Y ) and sets of vari-ables with bold-faced capital letters (e.g.
X,Y).
The domain of a variable X will be denoteddom(X), and typically we will use the lower-casecorrespondent (in this case, x) to denote a value inthe domain of X .
A partial assignment (or simplyassignment) of a set X of variables is a functionw that maps a subset W of the variables of Xto values in their respective domains.
We definedom(w) = W. When W = X, then we say thatw is a full assignment of X.
The trivial assign-ment of X makes no variable assignments.Let w(X) denote the value that partial assign-ment w assigns to variable X .
For value x ?dom(X), let w[X = x] denote the assignmentidentical to w except that w[X = x](X) = x.For a set Y of variables, let w|Y denote the re-striction of partial assignment w to the variablesin dom(w) ?
Y.3 The Generative ModelThe goal of this section is to develop a probabilis-tic process that generates labeled trees in a mannerconsiderably different from PCFGs.
We will usethe tree in Figure 2 to motivate our model.
In thisexample, nodes of the tree are labeled with eitheran A or a B.
We can represent this tree using twocharts.
One chart labels each span with a booleanvalue, such that a span is labeled true iff it is aconstituent in the tree.
The other chart labels eachspan with a label from our labeling scheme (A orB) or with the value null (to represent that thespan is unlabeled).
We show these charts in Fig-ure 3.
Notice that we may want to have more thanone labeling scheme.
For instance, in the parsetree of Figure 1, there are three different types oflabels: word labels, preterminal labels, and nonter-minal labels.
Thus we would use four 5x5 chartsinstead of two 3x3 charts to represent that tree.We will pause here and generalize these con-cepts.
Define a labeling scheme as a set of symbolsincluding a special symbol null (this will desig-370ABA BBFigure 2: Example labeled tree.1 2 31 true true true2 - true false3 - - true1 2 31 A B A2 - B null3 - - BFigure 3: Chart representation of the example tree:the left chart tells us which spans are tree con-stituents, and the right chart tells us the labels ofthe spans (null means unlabeled).nate that a given span is unlabeled).
For instance,we can define L1 = {null, A,B} to be a labelingscheme for the example tree.Let L = {L1, L2, ...Lm} be a set of labelingschemes.
Define a model variable of L as a sym-bol of the form Sij or Lkij , for positive integers i,j, k, such that i ?
j and k ?
m. Model vari-ables of the form Sij indicate whether span (i, j)is a tree constituent, hence the domain of Sij is{true, false}.
Such variables correspond to en-tries in the left chart of Figure 3.
Model variablesof the form Lkij indicate which label from schemeLk is assigned to span (i, j), hence the domain ofmodel variable Lkij is Lk.
Such variables corre-spond to entries in the right chart of Figure 3.
Herewe have only one labeling scheme.Let VL be the (countably infinite) set of modelvariables of L. Usually we are interested in treesover a given sentence of finite length n. Let VnLdenote the finite subset of VL that includes pre-cisely the model variables of the form Sij or Lkij ,where j ?
n.Basically then, our model consists of two typesof decisions: (1) whether a span should be labeled,and (2) if so, what label(s) the span should have.Let us proceed with our example.
To generate thetree of Figure 2, the first decision we need to makeis how many leaves it will have (or equivalently,how large our tables will be).
We assume that wehave a probability distribution PN over the set ofpositive integers.
For our example tree, we drawthe value 3, with probability PN (3).Now that we know our tree will have threeleaves, we can now decide which spans will beconstituents and what labels they will have.
Inother words, we assign values to the variables inV3L.
First we need to choose the order in whichwe will make these assignments.
For our exam-ple, we will assign model variables in the follow-ing order: S11, L111, S22, L122, S33, L133, S12, L112,S23, L123, S13, L113.
A detailed look at this assign-ment process should help clarify the details of themodel.Assigning S11: The first model variable in ourorder is S11.
In other words, we need to decidewhether the span (1, 1) should be a constituent.We could let this decision be probabilistically de-termined, but recall that we are trying to gener-ate a well-formed tree, thus the leaves and the rootshould always be considered constituents.
To han-dle situations when we would like to make deter-ministic variable assignments, we supply an aux-illiary function A that tells us (given a model vari-able X and the history of decisions made so far)whether X should be automatically determined,and if so, what value it should be assigned.
In ourrunning example, we ask A whether S11 should beautomatically determined, given the previous as-signments made (so far only the value chosen forn, which was 3).
The so-called auto-assignmentfunction A responds (since S11 is a leaf span) thatS11 should be automatically assigned the valuetrue, making span (1, 1) a constituent.Assigning L111: Next we want to assign a la-bel to the first leaf of our tree.
There is no com-pelling reason to deterministically assign this la-bel.
Therefore, the auto-assignment function Adeclines to assign a value to L111, and we pro-ceed to assign its value probabilistically.
For thistask, we would like a probability distribution overthe labels of labeling scheme L1 = {null, A,B},conditioned on the decision history so far.
The dif-ficulty is that it is clearly impractical to learn con-ditional distributions over every conceivable his-tory of variable assignments.
So first we distillthe important features from an assignment history.For instance, one such feature (though possiblynot a good one) could be whether an odd or aneven number of nodes have so far been labeledwith an A.
Our conditional probability distribu-tion is conditioned on the values of these features,instead of the entire assignment history.
Considerspecifically model variable L111.
We compute itsfeatures (an even number of nodes ?
zero ?
haveso far been labeled with an A), and then we usethese feature values to access the relevant prob-371ability distribution over {null, A,B}.
Drawingfrom this conditional distribution, we probabilis-tically assign the value A to variable L111.Assigning S22, L122, S33, L133: We proceed inthis way to assign values to S22, L122, S33, L133 (theS-variables deterministically, and the L1-variablesprobabilistically).Assigning S12: Next comes model variableS12.
Here, there is no reason to deterministicallydictate whether span (1, 2) is a constituent or not.Both should be considered options.
Hence wetreat this situation the same as for the L1 variables.First we extract the relevant features from the as-signment history.
We then use these features toaccess the correct probability distribution over thedomain of S12 (namely {true, false}).
Drawingfrom this conditional distribution, we probabilis-tically assign the value true to S12, making span(1, 2) a constituent in our tree.Assigning L112: We proceed to probabilisti-cally assign the value B to L112, in the same man-ner as we did with the other L1 model variables.Assigning S23: Now we must determinewhether span (2, 3) is a constituent.
We couldagain probabilistically assign a value to S23 as wedid for S12, but this could result in a hierarchi-cal structure in which both spans (1, 2) and (2, 3)are constituents, which is not a tree.
For trees,we cannot allow two model variables Sij and Sklto both be assigned true if they properly over-lap, i.e.
their spans overlap and one is not a sub-span of the other.
Fortunately we have already es-tablished auto-assignment function A, and so wesimply need to ensure that it automatically assignsthe value false to model variable Skl if a prop-erly overlapping model variable Sij has previouslybeen assigned the value true.Assigning L123, S13, L113: In this manner, wecan complete our variable assignments: L123 is au-tomatically determined (since span (2, 3) is not aconstituent, it should not get a label), as is S13 (toensure a rooted tree), while the label of the root isprobabilistically assigned.We can summarize this generative process as ageneral modeling tool.
Define a hierarchical la-beling process (HLP) as a 5-tuple ?L, <,A,F ,P?where:?
L = {L1, L2, ..., Lm} is a finite set of label-ing schemes.?
< is a model order, defined as a total orderingof the model variables VL such that for allHLPGEN(HLP H = ?L, <,A,F ,P?):1.
Choose a positive integer n from distributionPN .
Let x be the trivial assignment of VL.2.
In the order defined by <, compute step 3 foreach model variable Y of VnL.3.
If A(Y,x, n) = ?true, y?
for some y in thedomain of model variable Y , then let x =x[Y = y].
Otherwise assign a value to Yfrom its domain:(a) If Y = Sij , then let x = x[Sij = sij ],where sij is a value drawn from distri-bution PS(s|FS(x, i, j, n)).
(b) If Y = Lkij , then let x = x[Lkij = lkij ],where lkij is a value drawn from distribu-tion Pk(lk|Fk(x, i, j, n)).4.
Return ?n,x?.Figure 4: Pseudocode for the generative process.i, j, k: Sij < Lkij (i.e.
we decide whethera span is a constituent before attempting tolabel it).?
A is an auto-assignment function.
Specifi-cally A takes three arguments: a model vari-able Y of VL, a partial assignment x of VL,and integer n. The function A maps this 3-tuple to false if the variable Y should not beautomatically assigned a value based on thecurrent history, or the pair ?true, y?, where yis the value in the domain of Y that should beautomatically assigned to Y .?
F = {FS ,F1,F2, ...,Fm} is a set of fea-ture functions.
Specifically, F k (resp., FS)takes four arguments: a partial assignmentx of VL, and integers i , j , n such that1 ?
i ?
j ?
n. It maps this 4-tuple to afull assignment f k (resp., fS) of some finiteset Fk (resp., FS) of feature variables.?
P = {PN , PS , P1, P2, ..., Pm} is a set ofprobability distributions.
PN is a marginalprobability distribution over the set of pos-itive integers, whereas {PS , P1, P2, ..., Pm}are conditional probability distributions.Specifically, Pk (respectively, PS) is a func-tion that takes as its argument a full assign-ment fk (resp., fS) of feature set Fk (resp.,372A(variable Y , assignment x, int n):1.
If Y = Sij , and there exists a properlyoverlapping model variable Skl such thatx(Skl) = true, then return ?true, false?.2.
If Y = Sii or Y = S1n, then return?true, true?.3.
If Y = Lkij , and x(Sij) = false, then return?true, null?.4.
Else return false.Figure 5: An example auto-assignment function.FS).
It maps this to a probability distributionover dom(Lk) (resp., {true, false}).An HLP probabilistically generates an assign-ment of its model variables using the generativeprocess shown in Figure 4.
Taking an HLP H =?L, <,A,F ,P?
as input, HLPGEN outputs an in-teger n, and an H-labeling x of length n, definedas a full assignment of VnL.Given the auto-assignment function in Figure 5,every H-labeling generated by HLPGEN can beviewed as a labeled tree using the interpretation:span (i, j) is a constituent iff Sij = true; span(i, j) has label lk ?
dom(Lk) iff Lkij = lk.4 LearningThe generative story from the previous section al-lows us to express the probability of a labeled treeas P (n,x), where x is an H-labeling of length n.For model variable X , define V<L (X) as the sub-set of VL appearing before X in model order <.With the help of this terminology, we can decom-pose P (n,x) into the following product:P0(n) ??Sij?YPS(x(Sij)|fSij)?
?Lkij?YPk(x(Lkij)|fkij)where fSij = FS(x|V<L (Sij), i, j, n) andfkij = Fk(x|V<L (Lkij), i, j, n) and Y is the sub-set of VnL that was not automatically assigned byHLPGEN.Usually in parsing, we are interested in comput-ing the most likely tree given a specific sentence.In our framework, this generalizes to computing:argmaxxP (x|n,w), where w is a subassignmentof an H-labeling x of length n. In natural lan-guage parsing, w could specify the constituencyand word labels of the leaf-level spans.
This wouldbe equivalent to asking: given a sentence, what isits most likely parse?Let W = dom(w) and suppose that we choosea model order < such that for every pair of modelvariables W ?
W, X ?
VL\W, either W < Xor W is always auto-assigned.
Then P (x|n,w)can be expressed as:?Sij?Y\WPS(x(Sij)|fSij)?
?Lkij?Y\WPk(x(Lkij)|fkij)Hence the distributions we need to learnare probability distributions PS(sij|fS) andPk(lkij |fk).
This is fairly straightforward.
Givena data bank consisting of labeled trees (such asthe Penn Treebank), we simply convert each treeinto its H-labeling and use the probabilisticallydetermined variable assignments to compile ourtraining instances.
In this way, we compile k + 1sets of training instances that we can use to inducePS , and the Pk distributions.
The choice of whichlearning technique to use is up to the personalpreference of the user.
The only requirementis that it must return a conditional probabilitydistribution, and not a hard classification.
Tech-niques that allow this include relative frequency,maximum entropy models, and decision trees.For our experiments, we used maximum entropylearning.
Specifics are deferred to Section 6.5 DecodingFor the PCFG parsing model, we can findargmaxtreeP (tree|sentence) using a cubic-timedynamic programming-based algorithm.
Byadopting a more flexible probabilistic model, wesacrifice polynomial-time guarantees.
The centralquestion driving this paper is whether we can jetti-son these guarantees and still obtain good perfor-mance in practice.
For the decoding of the prob-abilistic model of the previous section, we choosea depth-first branch-and-bound approach, specif-ically because of two advantages.
First, this ap-proach takes linear space.
Second, it is anytime,373HLPDECODE(HLP H, int n, assignment w):1.
Initialize stack S with the pair ?x?, 1?, wherex?
is the trivial assignment of VL.
Letxbest = x?
; let pbest = 0.
Until stack S isempty, repeat steps 2 to 4.2.
Pop topmost pair ?x, p?
from stack S.3.
If p > pbest and x is an H-labeling of lengthn, then: let xbest = x; let pbest = p.4.
If p > pbest and x is not yet a H-labeling oflength n, then:(a) Let Y be the earliest variable in VnL (ac-cording to model order <) unassignedby x.
(b) If Y ?
dom(w), then push pair ?x[Y =w(Y )], p?
onto stack S.(c) Else if A(Y,x, n) = ?true, y?
for somevalue y ?
dom(Y ), then push pair?x[Y = y], p?
onto stack S.(d) Otherwise for every value y ?
dom(Y ),push pair ?x[Y = y], p ?q(y)?
onto stackS in ascending order of the value ofq(y), where:q(y) ={PS(y|FS(x, i, j, n)) if Y = SijPk(y|Fk(x, i, j, n)) if Y = Lkij5.
Return xbest.Figure 6: Pseudocode for the decoder.i.e.
it finds a (typically good) solution early andimproves this solution as the search progresses.Thus if one does not wish the spend the time torun the search to completion (and ensure optimal-ity), one can use this algorithm easily as a heuristicby halting prematurely and taking the best solutionfound thus far.The search space is simple to define.
Given anHLP H, the search algorithm simply makes as-signments to the model variables (depth-first) inthe order defined by <.This search space can clearly grow to be quitelarge, however in practice the search speed isimproved drastically by using branch-and-boundbacktracking.
Namely, at any choice point in thesearch space, we first choose the least cost childto expand (i.e.
we make the most probable assign-ment).
In this way, we quickly obtain a greedysolution (in linear time).
After that point, we cancontinue to keep track of the best solution we havefound so far, and if at any point we reach an inter-nal node of our search tree with partial cost greaterthan the total cost of our best solution, we can dis-card this node and discontinue exploration of thatsubtree.
This technique can result in a significantaggregrate savings of computation time, depend-ing on the nature of the cost function.Figure 6 shows the pseudocode for the depth-first branch-and-bound decoder.
For an HLP H =?L, <,A,F ,P?, a positive integer n, and a partialassignment w of VnL, the call HLPDECODE(H, n,w) returns the H-labeling x of length n such thatP (x|n,w) is maximized.6 ExperimentsWe employed a familiar experimental set-up.
Fortraining, we used sections 2?21 of the WSJ sectionof the Penn treebank.
As a development set, weused the first 20 files of section 22, and then savedsection 23 for testing the final model.
One uncon-ventional preprocessing step was taken.
Namely,for the entire treebank, we compressed all unarychains into a single node, labeled with the label ofthe node furthest from the root.
We did so in or-der to simplify our experiments, since the frame-work outlined in this paper allows only one labelper labeling scheme per span.
Thus by avoidingunary chains, we avoid the need for many label-ing schemes or more complicated compound la-bels (labels like ?NP-NN?).
Since our goal herewas not to create a parsing tool but rather to ex-plore the viability of this approach, this seemed afair concession.
It should be noted that it is indeedpossible to create a fully general parser using ourframework (for instance, by using the above ideaof compound labels for unary chains).The main difficulty with this compromise is thatit renders the familiar metrics of labeled preci-sion and labeled recall incomparable with previ-ous work (i.e.
the LP of a set of candidate parseswith respect to the unmodified test set differs fromthe LP with respect to the preprocessed test set).This would be a major problem, were it not forthe existence of other metrics which measure onlythe quality of a parser?s recursive decompositionof a sentence.
Fortunately, such metrics do exist,thus we used cross-bracketing statistics as the ba-sic measure of quality for our parser.
The cross-bracketing score of a set of candidate parses with374word(i+k) = w word(j+k) = wpreterminal(i+k) = p preterminal(j+k) = plabel(i+k) = l label(j+k) = lcategory(i+k) = c category(j+k) = csignature(i,i+k) = sFigure 7: Basic feature templates used to deter-mine constituency and labeling of span (i, j).
k isan arbitrary integer.respect to the unmodified test set is identical to thecross-bracketing score with respect to the prepro-cessed test set, hence our preprocessing causes nocomparability problems as viewed by this metric.For our parsing model, we used an HLP H =?L, <,A,F ,P?
with the following parameters.
Lconsisted of three labeling schemes: the set Lwdof word labels, the set Lpt of preterminal labels,and the set Lnt of nonterminal labels.
The or-der < of the model variables was the unique or-der such that for all suitable integers i, j, k, l: (1)Sij < Lwdij < Lptij < Lntij , (2) Lntij < Skl iffspan (i, j) is strictly shorter than span (k, l) or theyhave the same length and integer i is less than inte-ger k. For auto-assignment function A, we essen-tially used the function in Figure 5, modified sothat it automatically assigned null to model vari-ables Lwdij and Lptij for i 6= j (i.e.
no preterminal orword tagging of internal nodes), and to model vari-ables Lntii (i.e.
no nonterminal tagging of leaves,rendered unnecessary by our preprocessing step).Rather than incorporate part-of-speech tagginginto the search process, we opted to pretag the sen-tences of our development and test sets with anoff-the-shelf tagger, namely the Brill tagger (Brill,1994).
Thus the object of our computation wasHLPDECODE(H, n, w), where n was the lengthof the sentence, and partial assignment w speci-fied the word and PT labels of the leaves.
Giventhis partial assignment, the job of HLPDECODEwas to find the most probable assignment of modelvariables Sij and Lntij for 1 ?
i < j ?
n.The two probability models, P S and P nt, weretrained in the manner described in Section 4.Two decisions needed to be made: which fea-tures to use and which learning technique to em-ploy.
As for the learning technique, we usedmaximum entropy models, specifically the imple-mentation called MegaM provided by Hal Daume(Daume?
III, 2004).
For P S , we needed features?
40 ?
100CB 0CB CB 0CBMagerman (1995) 1.26 56.6Collins (1996) 1.14 59.9Klein/Manning (2003) 1.10 60.3 1.31 57.2this paper 1.09 58.2 1.25 55.2Charniak (1997) 1.00 62.1Collins (1999) 0.90 67.1Figure 8: Cross-bracketing results for Section 23of the Penn Treebank.that would be relevant to deciding whether a givenspan (i, j) should be considered a constituent.
Thebasic building blocks we used are depicted in Fig-ure 7.
A few words of explanation are in or-der.
By label(k), we mean the highest nonter-minal label so far assigned that covers word k, orif such a label does not yet exist, then the preter-minal label of k (recall that our model order wasbottom-up).
By category(k), we mean the cat-egory of the preterminal label of word k (givena coarser, hand-made categorization of pretermi-nal labels that grouped all noun tags into onecategory, all verb tags into another, etc.).
Bysignature(k,m), where k ?
m, we mean thesequence ?label(k), label(k + 1), ..., label(m)?,from which all consecutive sequences of identi-cal labels are compressed into a single label.
Forinstance, ?IN,NP,NP, V P, V P ?
would become?IN,NP, V P ?.
Ad-hoc conjunctions of these ba-sic binary features were used as features for ourprobability model P S .
In total, approximately800,000 such conjunctions were used.For P nt, we needed features that would be rele-vant to deciding which nonterminal label to giveto a given constituent span.
For this somewhatsimpler task, we used a subset of the basic fea-tures used for P S , shown in bold in Figure 7.
Ad-hoc conjunctions of these boldface binary featureswere used as features for our probability modelP nt.
In total, approximately 100,000 such con-junctions were used.As mentioned earlier, we used cross-bracketingstatistics as our basis of comparision.
These re-sults as shown in Figure 8.
CB denotes the av-erage cross-bracketing, i.e.
the overall percent-age of candidate constituents that properly overlapwith a constituent in the gold parse.
0CB denotesthe percentage of sentences in the test set that ex-hibit no cross-bracketing.
With a simple featureset, we manage to obtain performance compara-ble to the unlexicalized PCFG parser of (Klein andManning, 2003) on the set of sentences of length37540 or less.
On the subset of Section 23 consist-ing of sentences of length 100 or less, our parserslightly outperforms their results in terms of av-erage cross-bracketing.
Interestingly, our parserhas a lower percentage of sentences exhibiting nocross bracketing.
To reconcile this result with thesuperior overall cross-bracketing score, it wouldappear that when our parser does make bracketingerrors, the errors tend to be less severe.The surprise was how quickly the parser per-formed.
Despite its exponential worst-case timebounds, the search space turned out to be quiteconducive to depth-first branch-and-bound prun-ing.
Using an unoptimized Java implementationon a 4x Opteron 848 with 16GB of RAM, theparser required (on average) less than 0.26 sec-onds per sentence to optimally parse the subset ofSection 23 comprised of sentences of 40 words orless.
It required an average of 0.48 seconds persentence to optimally parse the sentences of 100words or less (an average of less than 3.5 secondsper sentence for those sentences of length 41-100).As noted earlier, the parser requires space linear inthe size of the sentence.7 DiscussionThis project began with a question: can we de-velop a history-based parsing framework that issimple, general, and effective?
We sought toprovide a versatile probabilistic framework thatwould be free from the constraints that dynamicprogramming places on PCFG-based approaches.The work presented in this paper gives favorableevidence that more flexible (and worst-case in-tractable) probabilistic approaches can indeed per-form well in practice, both in terms of runningtime and parsing quality.We can extend this research in multiple direc-tions.
First, the set of features we selected werechosen with simplicity in mind, to see how well asimple and unadorned set of features would work,given our probabilistic model.
A next step wouldbe a more carefully considered feature set.
For in-stance, although lexical information was used, itwas employed in only a most basic sense.
Therewas no attempt to use head information, which hasbeen so successful in PCFG parsing methods.Another parameter to experiment with is themodel order, i.e.
the order in which the model vari-ables are assigned.
In this work, we explored onlyone specific order (the left-to-right, leaves-to-headassignment) but in principle there are many otherfeasible orders.
For instance, one could try a top-down approach, or a bottom-up approach in whichinternal nodes are assigned immediately after allof their descendants?
values have been determined.Throughout this paper, we strove to present themodel in a very general manner.
There is no rea-son why this framework cannot be tried in otherapplication areas that rely on dynamic program-ming techniques to perform hierarchical labeling,such as phrase-based machine translation.
Apply-ing this framework to such application areas, aswell as developing a general-purpose parser basedon HLPs, are the subject of our continuing work.ReferencesEzra Black, Fred Jelinek, John Lafferty, David M.Magerman, Robert Mercer, and Salim Roukos.1993.
Towards history-based grammars: usingricher models for probabilistic parsing.
In Proc.ACL.Eric Brill.
1994.
Some advances in rule-based part ofspeech tagging.
In Proc.
AAAI.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proc.AAAI.Eugene Charniak.
2000.
A maximum entropy-inspiredparser.
In Proc.
NAACL.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proc.
ACL.Michael Collins.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proc.
ACL.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Hal Daume?
III.
2004.
Notes on CG and LM-BFGS op-timization of logistic regression.
Paper available athttp://www.isi.edu/ hdaume/docs/daume04cg-bfgs.ps, implementation available athttp://www.isi.edu/ hdaume/megam/, August.Mark Johnson.
1998.
Pcfg models of linguistictree representations.
Computational Linguistics,24:613?632.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
ACL.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proc.
ACL.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.In Proc.
EMNLP.376
