Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802?810,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsHierarchical Phrase Table Combination for Machine TranslationConghui Zhu1 Taro Watanabe2 Eiichiro Sumita2 Tiejun Zhao11School of Computer Science and TechnologyHarbin Institute of Technology (HIT), Harbin, China2National Institute of Information and Communication Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan{chzhu,tjzhao}@mtlab.hit.edu.cn{taro.watanabe,Sumita}@nict.go.jpAbstractTypical statistical machine translation sys-tems are batch trained with a given train-ing data and their performances are large-ly influenced by the amount of data.
Withthe growth of the available data acrossdifferent domains, it is computationallydemanding to perform batch training ev-ery time when new data comes.
In faceof the problem, we propose an efficientphrase table combination method.
In par-ticular, we train a Bayesian phrasal inver-sion transduction grammars for each do-main separately.
The learned phrase ta-bles are hierarchically combined as if theyare drawn from a hierarchical Pitman-Yorprocess.
The performance measured byBLEU is at least as comparable to the tra-ditional batch training method.
Further-more, each phrase table is trained sepa-rately in each domain, and while compu-tational overhead is significantly reducedby training them in parallel.1 IntroductionStatistical machine translation (SMT) system-s usually achieve ?crowd-sourced?
improvementswith batch training.
Phrase pair extraction, thekey step to discover translation knowledge, heav-ily relies on the scale of training data.
Typi-cally, the more parallel corpora used, the morephrase pairs and more accurate parameters willbe learned, which can obviously be beneficial toimproving translation performances.
Today, moreparallel sentences are drawn from divergent do-mains, and the size keeps growing.
Consequent-ly, how to effectively use those data and improvetranslation performance becomes a challenging is-sue.This joint work was done while the first author visitedNICT.Batch retraining is not acceptable for this case,since it demands serious computational overheadwhen training on a large data set, and it requiresus to re-train every time new training data is avail-able.
Even if we can handle the large computationcost, improvement is not guaranteed every time weperform batch tuning on the newly updated train-ing data obtained from divergent domains.
Tradi-tional domain adaption methods for SMT are alsonot adequate in this scenario.
Most of them havebeen proposed in order to make translation sys-tems perform better for resource-scarce domain-s when most training data comes from resource-rich domains, and ignore performance on a moregeneric domain without domain bias (Wang et al,2012).
As an alternative, incremental learningmay resolve the gap by incrementally adding da-ta sentence-by-sentence into the training data.
S-ince SMT systems trend to employ very large scaletraining data for translation knowledge extraction,updating several sentence pairs each time will beannihilated in the existing corpus.This paper proposes a new phrase table combi-nation method.
First, phrase pairs are extractedfrom each domain without interfering with oth-er domains.
In particular, we employ the non-parametric Bayesian phrasal inversion transduc-tion grammar (ITG) of Neubig et al (2011) to per-form phrase table extraction.
Second, extractedphrase tables are combined as if they are drawnfrom a hierarchical Pitman-Yor process, in whichthe phrase tables represented as tables in the Chi-nese restaurant process (CRP) are hierarchicallychained by treating each of the previously learnedphrase tables as prior to the current one.
Thus, wecan easily update the chain of phrase tables by ap-pending the newly extracted phrase table and bytreating the chain of the previous ones as its prior.Experiment results indicate that our method canachieve better translation performance when thereexists a large divergence in domains, and can802achieve at least comparable results to batch train-ing methods, with a significantly less computa-tional overhead.The rest of the paper is organized as follows.In Section 2, we introduce related work.
In sec-tion 3, we briefly describe the translation mod-el with phrasal ITGs and Pitman-Yor process.
Insection 4, we explain our hierarchical combinationapproach and give experiment results in section 5.We conclude the paper in the last section.2 Related WorkBilingual phrases are cornerstones for phrase-based SMT systems (Och and Ney, 2004; Koehnet al, 2003; Chiang, 2005) and existing translationsystems often get ?crowd-sourced?
improvements(Levenberg et al, 2010).
A number of approacheshave been proposed to make use of the full poten-tial of the available parallel sentences from vari-ous domains, such as domain adaptation and in-cremental learning for SMT.The translation model and language modelare primary components in SMT.
Previous workproved successful in the use of large-scale data forlanguage models from diverse domains (Brants etal., 2007; Schwenk and Koehn, 2008).
Alterna-tively, the language model is incrementally up-dated by using a succinct data structure with ainterpolation technique (Levenberg and Osborne,2009; Levenberg et al, 2011).In the case of the previous work on translationmodeling, mixed methods have been investigat-ed for domain adaptation in SMT by adding do-main information as additional labels to the orig-inal phrase table (Foster and Kuhn, 2007).
Un-der this framework, the training data is first di-vided into several parts, and phase pairs are ex-tracted with some sub-domain features.
Then al-l the phrase pairs and features are tuned togetherwith different weights during decoding.
As a wayto choose the right domain for the domain adap-tion, a classifier-based method and a feature-basedmethod have been proposed.
Classification-basedmethods must at least add an explicit label to indi-cate which domain the current phrase pair comesfrom.
This is traditionally done with an automat-ic domain classifier, and each input sentence isclassified into its corresponding domain (Xu et al,2007).
As an alternative to the classification-basedapproach, Wang et al (2012) employed a feature-based approach, in which phrase pairs are enrichedby a feature set to potentially reflect the domain in-formation.
The similarity calculated by a informa-tion retrieval system between the training subsetand the test set is used as a feature for each paral-lel sentence (Lu et al, 2007).
Monolingual topicinformation is taken as a new feature for a domainadaptive translation model and tuned on the devel-opment set (Su et al, 2012).
Regardless of under-lying methods, either classifier-based or feature-based method, the performance of current domainadaptive phrase extraction methods is more sensi-tive to the development set selection.
Usually thedomain similar to a given development data is usu-ally assigned higher weights.Incremental learning in which new parallel sen-tences are incrementally updated to the trainingdata is employed for SMT.
Compared to tradi-tional frequent batch oriented methods, an onlineEM algorithm and active learning are applied tophrase pair extraction and achieves almost compa-rable translation performance with less computa-tional overhead (Levenberg et al, 2010; Gonza?lez-Rubio et al, 2011).
However, their methods usu-ally require numbers of hyperparameters, such asmini-batch size, step size, or human judgment todetermine the quality of phrases, and still rely on aheuristic phrase extraction method in each phrasetable update.3 Phrase Pair Extraction withUnsupervised Phrasal ITGsRecently, phrase alignment with ITGs (Cherryand Lin, 2007; Zhang et al, 2008; Blunsom etal., 2008) and parameter estimation with Gibb-s sampling (DeNero and Klein, 2008; Blunsomand Cohn, 2010) are popular.
Here, we em-ploy a method proposed by Neubig et al (2011),which uses parametric Bayesian inference with thephrasal ITGs (Wu, 1997).
It can achieve com-parable translation accuracy with a much small-er phrase table than the traditional GIZA++ andheuristic phrase extraction methods.
It has al-so been proved successful in adjusting the phraselength granularity by applying character-basedSMT with more sophisticated inference (Neubiget al, 2012).ITG is a synchronous grammar formalismwhich analyzes bilingual text by introducing in-verted rules, and each ITG derivation correspondsto the alignment of a sentence pair (Wu, 1997).Translation probabilities of ITG phrasal align-803ments can be estimated in polynomial time by s-lightly limiting word reordering (DeNero and K-lein, 2008).More formally, P (?e, f?
; ?x, ?t) are the proba-bility of phrase pairs ?e, f?, which is parameter-ized by a phrase pair distribution ?t and a symboldistribution ?x.
?x is a Dirichlet prior, and ?t is es-timated with the Pitman-Yor process (Pitman andYor, 1997; Teh, 2006), which is expressed as?t ?
PY(d, s, Pdac) (1)where d is the discount parameter, s is the strengthparameter, and , and Pdac is a prior probabilitywhich acts as a fallback probability when a phrasepair is not in the model.Under this model, the probability for a phrasepair found in a bilingual corpus ?E,F ?
can be rep-resented by the following equation using the Chi-nese restaurant process (Teh, 2006):P(?ei, fi?
; ?E,F ?
)= 1C + s(ci ?
d?
ti)+1C + s(s+ d?
T )?
Pdac(?ei, fi?)
(2)where1.
ci and ti are the customer and table count ofthe ith phrase pair ?ei, fi?
found in a bilingualcorpus ?E,F ?;2.
C and T are the total customer and table countin corpus ?E,F ?;3.
d and s are the discount and strengthen hyper-parameters.The prior probability Pdac is recursively definedby breaking a longer phrase pair into two throughthe recursive ITG?s generative story as follows(Neubig et al, 2011):1.
Generate symbol x from Px(x; ?x) with threepossible values: Base, REG, or INV .2.
Depending on the value of x take the followingactions.a.
If x = Base, generate a new phrase pairdirectly from Pbase.b.
If x = REG, generate ?e1, f1?
and?e2, f2?
from P(?e, f?
; ?x, ?t), and con-catenate them into a single phrase pair?e1e2, f1f2?.Figure 1: A word alignment (a), and its hierarchi-cal derivation (b).c.
If x = INV , follow a similar process as b,but concatenate f1 and f2 in reverse order?e1e2, f2f1?.Note that the Pdac is recursively defined throughthe binary branched P , which in turns employsPdac as a prior probability.
Pbase is a base measuredefined as a combination of the IBM Models in t-wo directions and the unigram language models inboth sides.
Inference is carried out by a heuristicbeam search based block sampling with an effi-cient look ahead for a faster convergence (Neubiget al, 2012).Compared to GIZA++ with heuristic phrase ex-traction, the Bayesian phrasal ITG can achievecompetitive accuracy under a smaller phrase ta-ble size.
Further, the fallback model can incor-porate phrases of all granularity by following theITG?s recursive definition.
Figure 1 (b) illustratesan example of the phrasal ITG derivation for wordalignment in Figure 1 (a) in which a bilingual sen-tence pair is recursively divided into two throughthe recursively defined generative story.4 Hierarchical Phrase TableCombinationWe propose a new phrase table combinationmethod, in which individually learned phrase ta-ble are hierarchically chained through a hierarchi-cal Pitman-Yor process.Firstly, we assume that the whole train-ing data ?E,F ?
can be split into J domains,{?E1, F 1?, .
.
.
, ?EJ , F J?}.
Then phrase pairs are804Figure 2: A hierarchical phrase table combination (a), and a basic unit of a Chinese restaurant processwith K tables and N customers.extracted from each domain j (1 ?
j ?
J) sepa-rately with the method introduced in Section 3.
Intraditional domain adaptation approaches, phrasepairs are extracted together with their probabili-ties and/or frequencies so that the extracted phrasepairs are merged uniformly or after scaling.In this work, we extract the table counts for eachphrase pair under the Chinese restaurant processgiven in Section 3.
In Figure 2 (b), a CRP is illus-trated which has K tables and N customers witheach chair representing a customer.
Meanwhilethere are two parameters, discount and strength foreach domain similar to the ones in Equation (1).Our proposed hierarchical phrase table combi-nation can be formally expressed as following:?1 ?
PY (d1, s1, P 2)?
?
?
?
?
?
?j ?
PY (dj , sj , P j+1)?
?
?
?
?
?
?J ?
PY(dJ , sJ , P Jbase) (3)Here the (j + 1)th layer hierarchical Pitman-Yorprocess is employed as a base measure for thejth layer hierarchical Pitman-Yor process.
Thehierarchical chain is terminated by the base mea-sure from the J th domain P Jbase.
The hierarchi-cal structure is illustrated in Figure 2 (a) in whichthe solid lines implies a fall back using the ta-ble counts from the subsequent domains, and thedotted lines means the final fallback to the basemeasure P Jbase.
When we query a probability ofa phrase pair ?e, f?, we first query the probabil-ity of the first layer P 1(?e, f?).
If ?e, f?
is notin the model, we will fallback to the next level ofP 2(?e, f?).
This process continues until we reachthe Jth base measure of P J(?e, f?).
Each fallbackcan be viewed as a translation knowledge integra-tion process between subsequent domains.For example in Figure 2 (a), the ith phrase pair?ei, fi?
appears only in the domain 1 and domain2, so its translation probability can be calculatedby substituting Equation (3) with Equation (2):P(?ei, fi?
; ?E,F ?
)= 1C1 + s1 (c1i ?
d1 ?
t1i )+ s1 + d1 ?
T 1(C1 + s1)?
(C2 + s2)(c2i ?
d2 ?
t2i )+J?j=1(sj + dj ?
T jCj + sj)?
P Jbase(?ei, fi?)
(4)where the superscript indicates the domain for thecorresponding counts, i.e.
cji for the customercount in the jth domain.
The first term in Equa-tion (4) is the phrase probability from the first do-main, and the second one comes from the seconddomain, but weighted by the fallback weight of the1st domain.
Since ?ei, fi?
does not appear in therest of the layers, the last term is taken from al-l the fallback weight from the second layer to theJ th layer with the final P Jbase.
All the parameter-s ?j and hyperparameters dj and sj , are obtainedby learning on the jth domain.
Returning the hy-perparameters again when cascading another do-main may improve the performance of the combi-nation weight, but we will leave it for future work.The hierarchical process can be viewed as an in-stance of adapted integration of translation knowl-edge from each sub-domain.805Algorithm 1 Translation Probabilities Estima-tionInput: cji , tji , P jbase, Cj , T j , dj and sjOutput: The translation probabilities for eachpair1: for all phrase pair ?ei, fi?
do2: Initialize the P (?ei, fi?)
= 0 and wi = 13: for all domain ?Ej , Fj?
such that 1 6 j 6J ?
1 do4: if ?ei, fi?
?
?Ej , Fj?
then5: P (?ei, fi?)
+= wi ?
(Cji ?
dj ?tji )/(Cj + sj)6: end if7: wi = wi ?
(sj + dj ?
T j)/(Cj + sj)8: end for9: P (?ei, fi?)
+= wi?
(CJi ?dJ ?
tJi + (sJ +dJ ?
T J)?
P Jbase(?ei, fi?
))/(CJ + sJ)10: end forOur approach has several advantages.
First,each phrase pair extraction can concentrate on a s-mall portion of domain-specific data without inter-fering with other domains.
Since no tuning stageis involved in the hierarchical combination, we caneasily include a new phrase table from a new do-main by simply chaining them together.
Second,phrase pair phrase extraction in each domain iscompletely independent, so it is easy to parallelizein a situation where the training data is too largeto fit into a small amount of memory.
Finally, newdomains can be integrated incrementally.
Whenwe encounter a new domain, and if a phrase pair iscompletely new in terms of the model, the phrasepair is simply appended to the current model, andcomputed without the fallback probabilities, sinceotherwise, the phrase pair would be boosted by thefallback probabilities.
Pitman-Yor process is alsoemployed in n-gram language models which arehierarchically represented through the hierarchi-cal Pitman-Yor process with switch priors to in-tegrate different domains in all the levels (Woodand Teh, 2009).
Our work incrementally combinesthe models from different domains by directly em-ploying the hierarchical process through the basemeasures.5 ExperimentWe evaluate the proposed approach on theChinese-to-English translation task with three datasets with different scales.Data set Corpus #sent.
pairsIWSLT HIT 52, 603BTEC 19, 975Domain 1 47, 993Domain 2 30, 272FBIS Domain 3 49, 509Domain 4 38, 228Domain 5 55, 913News 221, 915News 95, 593LDC Magazine 98, 335Magazine 254, 488Finance 86, 112Table 1: The sentence pairs used in each data set.5.1 Experiment SetupThe first data set comes from the IWSLT2012OLYMPICS task consisting of two training sets:the HIT corpus, which is closely related to the Bei-jing 2008 Olympic Games, and the BTEC corpus,which is a multilingual speech corpus containingtourism-related sentences.
The second data set,the FBIS corpus, is a collection of news articlesand does not have domain information itself, so aLatent Dirichlet Allocation (LDA) tool, PLDA1,is used to divide the whole corpus into 5 differentsub-domains according to the concatenation of thesource side and target side as a single sentence (Li-u et al, 2011).
The third data set is composed of 5corpora2 from LDC with various domains, includ-ing news, magazine, and finance.
The details areshown in Table 1.In order to evaluate our approach, four phrasepair extraction methods are performed:1.
GIZA-linear: Phase pairs are extracted in eachdomain by GIZA++ (Och and Ney, 2003) andthe ?grow-diag-final-and?
method with a max-imum length 7.
The phrase tables from vari-ous domains are linearly combined by averag-ing the feature values.2.
Pialign-linear: Similar to GIZA-linear, but weemployed the phrasal ITG method described inSection 3 using the pialign toolkit 3 (Neubig et1http://code.google.com/p/plda/2In particular, they come from LDC catalog number:LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47,LDC2006E26, in this order.3http://www.phontron.com/pialign/806Methods IWSLT FBIS LDCBLEU Size BLEU Size BLEU SizeGIZA-linear 19.222 1,200,877 29.342 15,369,028 30.67 77,927,347Pialign-linear 19.534 876,059 29.858 7,235,342 31.12 28,877,149GIZA-batch 19.616 1,185,255 31.38 13,737,258 32.06 63,606,056Pialign-batch 19.506 841,931 31.104 6,459,200Pialign-adaptive 19.624 841,931 30.926 6,459,200Hier-combin 20.32 876,059 31.29 7,235,342 32.03 28,877,149Table 2: BLEU scores and phrase table size by alignment method and probabilities estimation method.Pialign was run with five samples.
Because of computational overhead, the baseline Pialign-batch andPialign-adaptive were not run on the largest data set.al., 2011).
Extracted phrase pairs are linearlycombined by averaging the feature values.3.
GIZA-batch: Instead of splitting into each do-main, the data set is merged as a single corpusand then a heuristic GZA-based phrase extrac-tion is performed, similar as GIZA-linear.4.
Pialign-batch: Similar to the GIZA-batch, a s-ingle model is estimated from a single, mergedcorpus.
Since pialign cannot handle large data,we did not experiment on the largest LDC dataset.5.
Pialign-adaptive: Alignment and phrase pairsextraction are same to Pialign-batch, whiletranslation probabilities are estimated by theadaptive method with monolingual topic in-formation (Su et al, 2012).
The method es-tablished the relationship between the out-of-domain bilingual corpus and in-domain mono-lingual corpora via topic distribution to esti-mate the translation probability.?(e?|f?)
=?tf?
(e?, tf |f?)=?tf?
(e?|tf , f?)
?
P (tf |f?
)(5)where ?
(e?|tf , f?)
is the probability of translating f?into e?
given the source-side topic f?
, P (tf |f?)
isthe phrase-topic distribution of f.The method we proposed is named Hier-combin.
It extracts phrase pairs in the same way asthe Pialign-linear.
In the phrase table combinationprocess, the translation probability of each phrasepair is estimated by the Hier-combin and the otherfeatures are also linearly combined by averagingthe feature values.
Pialign is used with default pa-rameters.
The parameter ?samps?
is set to 5, whichindicates 5 samples are generated for a sentencepair.The IWSLT data consists of roughly 2, 000 sen-tences and 3, 000 sentences each from the HIT andBTEC for development purposes, and the test da-ta consists of 1, 000 sentences.
For the FBIS andLDC task, we used NIST MT 2002 and 2004 fordevelopment and testing purposes, consisting of878 and 1, 788 sentences respectively.
We em-ploy Moses, an open-source toolkit for our exper-iment (Koehn et al, 2007).
SRILM Toolkit (Stol-cke, 2002) is employed to train 4-gram languagemodels on the Xinhua portion of Gigaword cor-pus, while for the IWLST2012 data set, only itstraining set is used.
We use batch-MIRA (Cher-ry and Foster, 2012) to tune the weight for eachfeature and translation quality is evaluated by thecase-insensitive BLEU-4 metric (Papineni et al,2002).
The BLEU scores reported in this paperare the average of 5 independent runs of indepen-dent batch-MIRA weight training, as suggested by(Clark et al, 2011).5.2 Result and Analysis5.2.1 Performances of various extractionmethodsWe carry out a series of experiments to evaluatetranslation performance.
The results are listed inTable 2.
Our method significantly outperforms thebaseline Pialign-linear.
Except for the translationprobabilities, the phrase pairs of two methods areexactly same, so the number of phrase pairs areequal in the two methods.
Further more, the per-formance of the baseline Pialign-adaptive is alsohigher than the baseline Pialign-linear?s and lowerthan ours.
This proves that the adaptive method807Methods Task Time(minute)Batch Retraining 536.9Hierarchical Parallel Extraction 122.55Combination Integrating 1.5Total 124.05Table 3: Minutes used for alignment and phasepair extraction in the FBIS data set.with monolingual topic information is useful inthe tasks, but our approach with the hierarchicalPitman-Yor process can estimate more accuratetranslation probabilities based on all the data fromvarious domains.Compared with the GIZA-batch, our approachachieves competitive performance with a much s-maller phrase table.
The number of phase pairsgenerated by our method is only 73.9%, 52.7%,and 45.4% of the GIZA-batch?s respectively.
Inthe IWLST2012 data set, there is a huge differencegap between the HIT corpus and the BTEC corpus,and our method gains 0.814 BLEU improvement.While the FBIS data set is artificially divided andno clear human assigned differences among sub-domains, our method loses 0.09 BLEU.In the framework we proposed, phrase pairs areextracted from each domain completely indepen-dent of each other, so those tasks can be executedon different machines, at different times, and ofcourse in parallel when we assume that the do-mains are not incrementally added in the train-ing data.
The runtime of our approach and thebatch-based ITGs sampling method in the FBISdata set is listed in Table 3 measured on a 2.7 GHzE5-2680 CPU and 128 Gigabyte memory.
Whencomparing the hier-combin with the pialign-batch,the BLEU scores are a little higher while the timespent for training is much lower, almost one quar-ter of the pialign-batch.Even the performance of the pialign-linear isbetter than the Baseline GIZA-linear?s, whichmeans that phrase pair extraction with hierarchi-cal phrasal ITGs and sampling is more suitablefor domain adaptation tasks than the combinationGIZA++ and a heuristic method.Generally, the hierarchical combination methodexploits the nature of a hierarchical Pitman-Yorprocess and gains the advantage of its smoothingeffect, and our approach can incrementally gener-ate a succinct phrase table based on all the datafrom various domains with more accurate prob-abilities.
Traditional SMT phrase pair extractionis batch-based, while our method has no obviousshortcomings in translation accuracy, not to men-tion efficiency.5.2.2 Effect of Integration OrderHere, we evaluate whether our hierarchical com-bination is sensitive to the order of the domainswhen forming a hierarchical structure.
ThroughEquation (3), in our experiments, we chained thedomains in the order listed in Table 1, which isin almost chronological order.
Table 4 shows theBLEU scores for the three data sets, in which theorder of combining phrase tables from each do-main is alternated in the ascending and descendingof the similarity to the test data.
The similarity be-tween the data from each domain and the test datais calculated using the perplexity measure with 5-gram language model.
The model learned fromthe domain more similar to the test data is placedin the front so that it can largely influence theparameter computation with less backoff effects.There is a big difference between the two oppositeorder in IWSLT 2012 data set, in which more thanone point of decline in BLEU score when takingthe BTEC corpus as the first layer.
Note that theperplexity of BTEC was 344.589 while that of HITwas 107.788.
The result may indicate that our hi-erarchical phrase combination method is sensitiveto the integration order when the training data issmall and there exists large gap in the similarity.However, if most domains are similar (FBIS dataset) or if there are enough parallel sentence pairs(NIST data set) in each domain, then the transla-tion performances are almost similar even with theopposite integrating orders.IWSLT FBIS LDCDescending 20.154 30.491 31.268Ascending 19.066 30.388 31.254Difference 1.088 0.103 0.014Table 4: BLEU scores for the hierarchical modelwith different integrating orders.
Here Pialign wasrun without multi-samples.6 Conclusion and Future WorkIn this paper, we present a novel hierarchicalphrase table combination method for SMT, whichcan exploit more of the potential from all of da-ta coming from various fields and generate a suc-808cinct phrase table with more accurate translationprobabilities.
The method assumes that a com-bined model is derived from a hierarchical Pitman-Yor process with each prior learned separately ineach domain, and achieves BLEU scores competi-tive with traditional batch-based ones.
Meanwhile,the framework has natural characteristics for par-allel and incremental phrase pair extraction.
Theexperiment results on three different data sets in-dicate the effectiveness of our approach.In future work, we will also introduce incre-mental learning for phase pair extraction inside adomain, which means using the current translationprobabilities already obtained as the base measureof sampling parameters for the upcoming domain.Furthermore, we will investigate any tradeoffs be-tween the accuracy of the probability estimationand the coverage of phrase pairs.AcknowledgmentsWe would like to thank our colleagues in bothHIT and NICT for insightful discussions, andthree anonymous reviewers for many invaluablecomments and suggestions to improve our paper.This work is supported by National Natural Sci-ence Foundation of China (61100093, 61173073,61073130, 61272384), and the Key Project of theNational High Technology Research and Develop-ment Program of China (2011AA01A207).ReferencesPhil Blunsom and Trevor Cohn.
2010.
Inducing syn-chronous grammars with slice sampling.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 238?241,Los Angeles, California, June.
Association for Com-putational Linguistics.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisti-cal machine translation.
In Proceedings of ACL,pages 200?208, Columbus, Ohio, June.
Associationfor Computational Linguistics.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofthe 2007 Joint Conference on Empirical Methodsin Natural Language Processing and Computation-al Natural Language Learning (EMNLP-CoNLL),pages 858?867, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Montre?al, Canada, June.
Association forComputational Linguistics.Colin Cherry and Dekang Lin.
2007.
Inversiontransduction grammar for joint phrasal translationmodeling.
In Proceedings of SSST, NAACL-HLT2007/AMTA Workshop on Syntax and Structure inStatistical Translation, pages 17?24.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 263?270, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: controlling for opti-mizer instability.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: short pa-pers - Volume 2, HLT ?11, pages 176?181, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.John DeNero and Dan Klein.
2008.
The complexi-ty of phrase alignment problems.
In Proceedings ofACL-08: HLT, Short Papers, pages 25?28, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,pages 128?135.Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Martinez, andFrancisco Casacuberta.
2011.
Fast incremental ac-tive learning for statistical machine translation.
A-VANCES EN INTELIGENCIA ARTIFICIAL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL, pages 45?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for smt.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 2-Volume 2, pages 756?764.
Association for Compu-tational Linguistics.809Abby Levenberg, Chris Callison-Burch, and Miles Os-borne.
2010.
Stream-based translation modelsfor statistical machine translation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 394?402, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Abby Levenberg, Miles Osborne, and David Matthews.2011.
Multiple-stream language models for statisti-cal machine translation.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages177?186, Edinburgh, Scotland, July.
Association forComputational Linguistics.Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, andMaosong Sun.
2011.
Plda+: Parallel latent dirichletallocation with data placement and pipeline process-ing.
ACM Transactions on Intelligent Systems andTechnology (TIST), 2(3):1?18.Yajuan Lu, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by train-ing data selection and optimization.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and Computation-al Natural Language Learning (EMNLP-CoNLL),pages 343?350, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori, and Tatsuya Kawahara.
2011.
Anunsupervised model for joint phrase alignment andextraction.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistic-s: Human Language Technologies, pages 632?641,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.Graham Neubig, Taro Watanabe, Shinsuke Mori, andTatsuya Kawahara.
2012.
Machine translation with-out words through substring alignment.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 165?174, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A systemat-ic comparison of various statistical alignment mod-els.
Computational linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Comput.
Linguist., 30(4):417?449, Decem-ber.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic e-valuation of machine translation.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Jim Pitman and Marc Yor.
1997.
The two-parameterpoisson-dirichlet distribution derived from a stablesubordinator.
The Annals of Probability, 25(2):855?900.Holger Schwenk and Philipp Koehn.
2008.
Largeand diverse language models for statistical machinetranslation.
In International Joint Conference onNatural Language Processing, pages 661?668.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
of ICSLP.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, X-iaodong Shi, Huailin Dong, and Qun Liu.
2012.Translation model adaptation for statistical machinetranslation with monolingual topic information.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 459?468.Yee Whye Teh.
2006.
A hierarchical bayesian lan-guage model based on pitman-yor processes.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 985?992.
Association for Computa-tional Linguistics.Wei Wang, Klaus Macherey, Wolfgang Macherey,Franz Och, and Peng Xu.
2012.
Improved do-main adaptation for statistical machine translation.In Proceedings of the Conference of the Associationfor Machine translation, Americas.F.
Wood and Y. W. Teh.
2009.
A hierarchical non-parametric Bayesian approach to statistical languagemodel domain adaptation.
In Proceedings of the In-ternational Conference on Artificial Intelligence andStatistics, volume 12.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.Jia Xu, Yonggang Deng, Yuqing Gao, and HermannNey.
2007.
Domain dependent statistical machinetranslation.
In Proceedings of the MT Summit XI.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.In Proceedings of ACL-08: HLT, pages 97?105,Columbus, Ohio, June.
Association for Computa-tional Linguistics.810
