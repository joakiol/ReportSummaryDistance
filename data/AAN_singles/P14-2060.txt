Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 364?369,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsHippocratic Abbreviation ExpansionBrian Roark and Richard SproatGoogle, Inc, 79 Ninth Avenue, New York, NY 10011{roark,rws}@google.comAbstractIncorrect normalization of text can be par-ticularly damaging for applications liketext-to-speech synthesis (TTS) or typingauto-correction, where the resulting nor-malization is directly presented to the user,versus feeding downstream applications.In this paper, we focus on abbreviationexpansion for TTS, which requires a ?dono harm?, high precision approach yield-ing few expansion errors at the cost ofleaving relatively many abbreviations un-expanded.
In the context of a large-scale, real-world TTS scenario, we presentmethods for training classifiers to establishwhether a particular expansion is apt.
Weachieve a large increase in correct abbrevi-ation expansion when combined with thebaseline text normalization component ofthe TTS system, together with a substan-tial reduction in incorrect expansions.1 IntroductionText normalization (Sproat et al, 2001) is an im-portant initial phase for many natural language andspeech applications.
The basic task of text normal-ization is to convert non-standard words (NSWs)?
numbers, abbreviations, dates, etc.
?
into stan-dard words, though depending on the task and thedomain a greater or lesser number of these NSWsmay need to be normalized.
Perhaps the most de-manding such application is text-to-speech synthe-sis (TTS) since, while for parsing, machine trans-lation and information retrieval it may be accept-able to leave such things as numbers and abbre-viations unexpanded, for TTS all tokens need tobe read, and for that it is necessary to know howto pronounce them.
Which normalizations are re-quired depends very much on the application.What is also very application-dependent is thecost of errors in normalization.
For some applica-tions, where the normalized string is an interme-diate stage in a larger application such as trans-lation or information retrieval, overgeneration ofnormalized alternatives is often a beneficial strat-egy, to the extent that it may improve the accu-racy of what is eventually being presented to theuser.
In other applications, such as TTS or typingauto-correction, the resulting normalized string it-self is directly presented to the user; hence errorsin normalization can have a very high cost relativeto leaving tokens unnormalized.In this paper we concentrate on abbreviations,which we define as alphabetic NSWs that it wouldbe normal to pronounce as their expansion.
Thisclass of NSWs is particularly common in personalads, product reviews, and so forth.
For example:home health care svcs stat home health llcosceola aquatic ctr stars rating writeaudi vw repair ser quality and customerEach of the examples above contains an abbrevi-ation that, unlike, e.g., conventionalized state ab-breviations such as ca for California, is either onlyslightly standard (ctr for center) or not standard atall (ser for service).An important principle in text normalization forTTS is do no harm.
If a system is unable to re-liably predict the correct reading for a string, it isbetter to leave the string alone and have it defaultto, say, a character-by-character reading, than toexpand it to something wrong.
This is particularlytrue in accessibility applications for users who relyon TTS for most or all of their information needs.Ideally a navigation system should read turn on30N correctly as turn on thirty north; but if it can-not resolve the ambiguity in 30N, it is far better toread it as thirty N than as thirty Newtons, since lis-teners can more easily recover from the first kindof error than the second.We present methods for learning abbreviationexpansion models that favor high precision (incor-rect expansions < 2%).
Unannotated data is usedto collect evidence for contextual disambiguationand to train an abbreviation model.
Then a smallamount of annotated data is used to build modelsto determine whether to accept a candidate expan-364sion of an abbreviation based on these features.The data we report on are taken from GoogleMapsTMand web pages associated with its map en-tries, but the methods can be applied to any datasource that is relatively abbreviation rich.We note in passing that similar issues arisein automatic spelling correction work (Wilcox-O?Hearn et al, 2008), where it is better to leavea word alone than to ?correct?
it wrongly.2 Related workThere has been a lot of interest in recent years on?normalization?
of social media such as Twitter,but that work defines normalization much morebroadly than we do here (Xia et al, 2006; Choud-hury et al, 2007; Kobus et al, 2008; Beaufort etal., 2010; Kaufmann, 2010; Liu et al, 2011; Pen-nell and Liu, 2011; Aw and Lee, 2012; Liu et al,2012a; Liu et al, 2012b; Hassan and Menezes,2013; Yang and Eisenstein, 2013).
There is a goodreason for us to focus more narrowly.
For Twit-ter, much of the normalization task involves non-standard language such as ur website suxx brah(from Yang and Eisenstein (2013)).
Expanding thelatter to your website sucks, brother certainly nor-malizes it to standard English, but one could arguethat in so doing one is losing information that thewriter is trying to convey using an informal style.On the other hand, someone who writes svc ctrfor service center in a product review is probablymerely trying to save time and so expanding theabbreviations in that case is neutral with respect topreserving the intent of the original text.One other difference between the work we re-port from much of the recent work cited above isthat that work focuses on getting high F scores,whereas we are most concerned with getting highprecision.
While this may seem like a trivialtrade off between precision and recall, our goalmotivates developing measures that minimize the?risk?
of expanding a term, something that is im-portant in an application such as TTS, where onecannot correct a misexpansion after it is spoken.3 MethodsSince our target application is text-to-speech, wedefine the task in terms of an existing TTS lexi-con.
If a word is already in the lexicon, it is leftunprocessed, since there is an existing pronuncia-tion for it; if a word is out-of-vocabulary (OOV),we consider expanding it to a word in the lexicon.We consider a possible expansion for an abbrevi-ation to be any word in the lexicon from whichthe abbreviation can be derived by only deletion ofletters.1For present purposes we use the GoogleEnglish text-to-speech lexicon, consisting of over430 thousand words.
Given an OOV item (possi-ble abbreviation) in context, we make use of fea-tures of the context and of the OOV item itself toenumerate and score candidate expansions.Our data consists of 15.1 billion words of textdata from Google MapsTM, lower-cased and tok-enized to remove punctuation symbols.
We usedthis data in several ways.
First, we used it to boot-strap a model for assigning a probability of an ab-breviation/expansion pair.
Second, we used it toextract contextual n-gram features for predictingpossible expansions.
Finally, we sampled just over14 thousand OOV items in context and had themmanually labeled with a number of categories, in-cluding ?abbreviation?.
OOVs labeled as abbrevia-tions were also labeled with the correct expansion.We present each of these uses in turn.3.1 Abbreviation modelingWe collect potential abbreviation/full-word pairsby looking for terms that could be abbreviationsof full words that occur in the same context.
Thus:the svc/service centerheating clng/cooling systemdry clng/cleaning systemcontributes evidence that svc is an abbreviationof service.
Similarly instances of clng in con-texts that can contain cooling or cleaning are evi-dence that clng could be an abbreviation of eitherof these words.
(The same contextual informationof course is used later on to disambiguate whichof the expansions is appropriate for the context.
)To compute the initial guess as to what can be apossible abbreviation, a Thrax grammar (Roark etal., 2012) is used that, among other things, speci-fies that: the abbreviation must start with the sameletter as the full word; if a vowel is deleted, all ad-jacent vowels should also be deleted; consonantsmay be deleted in a cluster, but not the last one;and a (string) suffix may be deleted.2We counta pair of words as ?co-occurring?
if they are ob-served in the same context.
For a given context C,e.g., the center, letWCbe the set of words foundin that context.
Then, for any pair of words u, v,we can assign a pair count based on the count ofcontexts where both occur:c(u, v) = |{C : u ?WCand v ?WC}|1We do not deal here with phonetic spellings in abbrevia-tions such as 4get, or cases where letters have been transposeddue to typographical errors (scv).2This Thrax grammar can be found athttp://openfst.cs.nyu.edu/twiki/bin/view/Contrib/ThraxContrib365blvd boulevard rd road yrs yearsca california fl florida ctr centermins minutes def definitely ste suiteTable 1: Examples of automatically mined abbrevia-tion/expansion pairs.Let c(u) be defined as?vc(u, v).
From thesecounts, we can define a 2?2 table and calculatestatistics such as the log likelihood statistic (Dun-ning, 1993), which we use to rank possible abbre-viation/expansion pairs.
Scores derived from thesetype (rather than token) counts highly rank pairs ofin-vocabulary words and OOV possible abbrevia-tions that are substitutable in many contexts.We further filter the potential abbreviations byremoving ones that have a lot of potential expan-sions, where we set the cutoff at 10.
This removesmostly short abbreviations that are highly ambigu-ous.
The resulting ranked list of abbreviation ex-pansion pairs is then thresholded before buildingthe abbreviation model (see below) to provide asmaller but more confident training set.
For thispaper, we used 5-gram contexts (two words on ei-ther side) to extract abbreviations and their expan-sions.
See Table 1 for some examples.Our abbreviation model is a pair character lan-guage model (LM), also known as a joint multi-gram model (Bisani and Ney, 2008), wherebyaligned symbols are treated as a single token anda smoothed n-gram model is estimated.
This de-fines a joint distribution over input and outputsequences, and can be efficiently encoded as aweighted finite-state transducer.
The extractedabbreviation/expansion pairs are character-alignedand a 7-gram pair character LM is built overthe alignments using the OpenGrm n-gram library(Roark et al, 2012).
For example:c:c :e :n t:t :e r:rNote that, as we?ve defined it, the alignments fromabbreviation to expansion allow only identity andinsertion, no deletions or substitutions.
The costfrom this LM, normalized by the length of the ex-pansion, serves as a score for the quality of a pu-tative expansion for an abbreviation.For a small set of frequent, conventionalizedabbreviations (e.g., ca for California ?
63 pairsin total ?
mainly state abbreviations and similaritems), we assign an fixed pair LM score, sincethese examples are in effect irregular cases, wherethe regularities of the productive abbreviation pro-cess do not capture their true cost.3.2 Contextual featuresTo predict the expansion given the context, we ex-tract n-gram observations for full words in the TTSlexicon.
We do this in two ways.
First, we sim-ply train a smoothed n-gram LM from the data.Because of the size of the data set, this is heav-ily pruned using relative entropy pruning (Stolcke,1998).
Second, we use log likelihood and log oddsratios (this time using standardly defined n-gramcounts) to extract reliable bigram and trigram con-texts for words.
Space precludes a detailed treat-ment of these two statistics, but, briefly, both canbe derived from contingency table values calcu-lated from the frequencies of (1) the word in theparticular context; (2) the word in any context; (3)the context with any word; and (4) all words inthe corpus.
See Agresti (2002), Dunning (1993)and Monroe et al (2008) for useful overviews ofhow to calculate these and other statistics to de-rive reliable associations.
In our case, we use themto derive associations between contexts and wordsoccuring in those contexts.
The contexts includetrigrams with the target word in any of the threepositions, and bigrams with the target word in ei-ther position.
We filter the set of n-grams based onboth their log likelihood and log odds ratios, andprovide those scores as features.3.3 Manual annotationsWe randomly selected 14,434 OOVs in their fullcontext, and had them manually annotated asfalling within one of 8 categories, along with theexpansion if the category was ?abbreviation?.
Notethat these are relatively lightweight annotationsthat do not require extensive linguistics expertise.The abbreviation class is defined as cases wherepronouncing as the expansion would be normal.Other categories included letter sequence (expan-sion would not be normal, e.g., TV); partial let-ter sequence (e.g., PurePictureTV); misspelling;leave as is (part of a URL or pronounced as aword, e.g., NATO); foreign; don?t know; and junk.Abbreviations accounted for nearly 23% of thecases, and about 3/5 of these abbreviations wereinstances from the set of 63 conventional abbrevi-ation/expansion pairs mentioned in Section 3.1.3.4 Abbreviation expansion systemsWe have three base systems that we compare here.The first is the hand-built TTS normalization sys-tem.
This system includes some manually builtpatterns and an address parser to find common ab-breviations that occur in a recognizable context.For example, the grammar covers several hundredcity-state combinations, such as Fairbanks AK,yielding good performance on such cases.The other two systems were built using data ex-tracted as described above.
Both systems makeuse of the pair LM outlined in Section 3.1, butdiffer in how they model context.
The first sys-366tem, which we call ?N-gram?, uses a pruned Katz(1987) smoothed trigram model.
The second sys-tem, which we call ?SVM?, uses a Support Vec-tor Machine (Cortes and Vapnik, 1995) to classifycandidate expansions as being correct or not.
Forboth systems, for any given input OOV, the pos-sible expansion with the highest score is output,along with the decision of whether to expand.For the ?N-gram?
system, n-gram negative logprobabilities are extracted as follows.
Let wibethe position of the target expansion.
We extract thepart of the n-gram probability of the string that isnot constant across all competing expansions, andnormalize by the number of words in that window.Thus the score of the word is:S(wi) = ?1k + 1i+k?j=ilog P(wj| wj?1wj?2)In our experiments, k = 2 since we have a trigrammodel, though in cases where the target word is thelast word in the string, k = 1, because there onlythe end-of-string symbol must be predicted in ad-dition to the expansion.
We then take the Bayesianfusion of this model with the pair LM, by addingthem in the log space, to get prediction from boththe context and abbreviation model.For the ?SVM?
model, we extract features fromthe log likelihood and log odds scores associatedwith contextual n-grams, as well as from the pairLM probability and characteristics of the abbrevi-ation itself.
We train a linear model on a subset ofthe annotated data (see section 4).
Multiple con-textual n-grams may be observed, and we take themaximum log likelihood and log odds scores foreach candidate expansion in the observed context.We then quantize these scores down into 16 bins,using the histogram in the training data to definebin thresholds so as to partition the training in-stances evenly.
We also create 16 bins for the pairLM score.
A binary feature is defined for eachbin that is set to 1 if the current candidate?s scoreis less than the threshold of that bin, otherwise 0.Thus multiple bin features can be active for a givencandidate expansion of the abbreviation.We also have features that fire for each type ofcontextual feature (e.g., trigram with expansion asmiddle word, etc.
), including ?no context?, wherenone of the trigrams or bigrams from the currentexample that include the candidate expansion arepresent in our list.
Further, we have features forthe length of the abbreviation (shorter abbrevia-tions have more ambiguity, hence are more riskyto expand); membership in the list of frequent,conventionalized abbreviations mentioned earlier;and some combinations of these, along with biasfeatures.
We train the model using standard op-tions with Google internal SVM training tools.Note that the number of n-grams in the twomodels differs.
The N-gram system has around200M n-grams after pruning; while the SVMmodel uses around a quarter of that.
We also trieda more heavily pruned n-gram model, and the re-sults are only very slightly worse, certainly accept-able for a low-resource scenario.4 Experimental ResultsWe split the 3,209 labeled abbreviations into atraining set of 2,209 examples and a held aside de-velopment set of 1,000 examples.
We first evaluateon the development set, then perform a final 10-fold cross validation over the entire set of labeledexamples.
We evaluate in terms of the percent-age of abbreviations that were correctly expanded(true positives, TP) and that were incorrectly ex-panded (false positives, FP).Results are shown in Table 2.
The first two rowsshow the baseline TTS system and SVM model.On the development set, both systems have a falsepositive rate near 3%, i.e., three abbreviations areexpanded incorrectly for every 100 examples; andover 50% true positive rate, i.e., more than half ofthe abbreviations are expanded correctly.
To re-port true and false positive rates for the N-gramsystem we would need to select an arbitrary de-cision threshold operating point, unlike the deter-ministic TTS baseline and the SVM model withits decision threshold of 0.
Rather than tune such ameta-parameter to the development set, we insteadpresent an ROC curve comparison of the N-gramand SVM models, and then propose a methodfor ?intersecting?
their output without requiring atuned decision threshold.Figure 1 presents an ROC curve for the N-gramand SVM systems, and for the simple Bayesianfusion (sum in log space) of their scores.
We cansee that the SVM model has very high precisionfor its highest ranked examples, yielding nearly20% of the correct expansions without any in-correct expansions.
However the N-gram systemachieves higher true positive rates when the falsePercent of abbreviationsdev set full setSystem TP FP TP FPTTS baseline 55.0 3.1 40.0 3.0SVM model 52.6 3.3 53.3 2.6SVM ?
N-gram 50.6 1.1 50.3 0.9SVM ?
N-gram, then TTS 73.5 1.9 74.5 1.5Table 2: Results on held-out labeled data, and with final10-fold cross-validation over the entire labeled set.
Percent-age of abbreviations expanded correctly (TP) and percentageexpanded incorrectly (FP) are reported for each system.367Correctexpansionpercentage(TP)0 1 2 3 40102030405060N-?gram SVM SVM ?+ ?N-?gram SVM ?intersect ?N-?gramIncorrect ?expansion ?percentage ?(FP)Correct?expansion?percentage?(TP)Figure1:ROCcurveplottingtruepositive(correctexpan-sion)percentagesversusfalsepositive(incorrectexpansion)percentagesforseveralsystemsonthedevelopmentset.attheSVM?sdecisionthresholdcorrespondingtoaround3.3%falsepositiverate.Thesimplecom-binationoftheirscoresachievesstrongimprove-mentsovereithermodel,withanoperatingpointassociatedwiththeSVMdecisionboundarythatyieldsacoupleofpointsimprovementintruepos-itivesandafull1%reductioninfalsepositiverate.Onesimplewaytocombinethesetwosystemoutputsinawaythatdoesnotrequiretuningade-cisionthresholdistoexpandtheabbreviationifandonlyif(1)boththeSVMmodelandtheN-grammodelagreeonthebestexpansion;and(2)theSVMmodelscoreisgreaterthanzero.Inaslightabuseoftheterm?intersection?,wecallthiscombination?SVMintersectN-gram?(or?SVM\N-gram?inTable2).Usingthisapproach,ourtruepositiverateonthedevsetdeclinesabittojustover50%,butourfalsepositiveratedeclinesovertwofullpercentagepointsto1.1%,yieldingaveryhighprecisionsystem.Takingthisveryhighprecisionsystemcombi-nationoftheN-gramandSVMmodels,wethencombinewiththebaselineTTSsystemasfollows.Firstweapplyoursystem,andexpandtheitemifitscoresabovethreshold;forthoseitemsleftun-expanded,welettheTTSsystemprocessitinitsownway.Inthisway,weactuallyreducethefalsepositiverateonthedevsetoverthebaselineTTSsystembyover1%absolutetolessthan2%,whilealsoincreasingthetruepositiverateto73.5%,anincreaseof18.5%absolute.Ofcourse,attesttime,wewillnotknowwhetheranOOVisanabbreviationornot,sowealsolookedattheperformanceontherestofthecollecteddata,toseehowofteniterro-neouslysuggestsanexpansionfromthatset.Ofthe11,157examplesthatwerehand-labeledasnon-abbreviations,ourSVM\N-gramsystemex-panded45items,whichisafalsepositiverateof0.4%undertheassumptionthatnoneofthemshouldbeexpanded.Infact,manualinspectionfoundthat20%ofthesewerecorrectexpansionsofabbreviationsthathadbeenmis-labeled.Duringsystemdevelopment,wealsoexperi-mentedwithanumberofalternativehighpreci-sionapproachesthatspaceprecludesourpresent-ingindetailhere,including:pruningthenum-berofexpansioncandidatesbasedonthepairlan-guagemodelscore;onlyallowingabbreviationex-pansionwhenatleastoneextractedn-gramcon-textispresentforthatexpansioninthatcontext;andCARTtree(Breimanetal.,1984)trainingwithrealvaluedscores.Someoftheseyieldedveryhighprecisionsystems,thoughatthecostofleavingmanymoreabbreviationsunexpanded.Wefoundthat,foruseincombinationwiththebase-lineTTSsystem,largeoverallreductionsinfalsepositiveratewereachievedbyusinganinitialsys-temwithsubstantiallyhigherTPandsomewhathigherFPrates,sincefarfewerabbreviationswerethenpassedalongunexpandedtothebaselinesys-tem,withitsrelativelyhigh3%FPrate.Toensurethatwehadnotover-tunedoursys-temstothedevsetthroughexperimentation,weperformed10-foldcrossvalidationoverthefullsetofabbreviations,andtheresultsarepresentedinTable2.Mostnotably,theTTSbaselinesystemhasamuchlowertruepositiverate;yetwefindoursystemsachieveperformanceveryclosetothatforthedevelopmentset,sothatourfinalcombinationwiththeTTSbaselinewasactuallyslighlybetterthanthenumbersonthedevelopmentset.5ConclusionsNotesforinternalreviewers?Maybemoretechnicalexplanationofloglikelihoodandlogoddsscores?Revampedintro,background,conclusionandexpandedrefs.
?Anonymizationforsubmission.Incorrect expansion percentage (FP)Figure 1: ROC curve plotting true positive (correct expan-sion) percentages versus false positive (incorrect expansion)percentages for several systems on the development set.positive rate falls between 1 and 3 percent, thoughboth systems reach roughly the same performanceat the SVM?s decision threshold corresponding toaround 3.3% false positive rate.
The simple com-bination of their scores achieves strong improve-ments over either model, with an operating pointassociated with the SVM decision boundary thatyields a couple of points improvement in true pos-itives and a full 1% reduction in false positive rate.One simple way to combine these two systemoutputs in a way that does not require tuning a de-cision threshold is to expand the abbreviation ifand only if (1) both the SVM model and the N-gram model agree on the best expansion; and (2)the SVM model score is greater than zero.
In aslight abuse of the term ?intersection?, we call thiscombination ?SVM intersect N-gram?
(or ?SVM?
N-gram?
in Table 2).
Using this approach, ourtrue positive rate on the development set declinesa bit to just over 50%, but our false positive ratedeclines over two full percentage points to 1.1%,yielding a very high precision system.Taking this very high precision system combi-nation of the N-gram and SVM models, we thencombine with the baseline TTS system as follows.First we apply our system, and expand the item ifit scores above threshold; for those items left un-expanded, we let the TTS system process it in itsown way.
In this way, we actually reduce the falsepositive rate on the development set over the base-line TTS system by over 1% absolute to less than2%, while also increasing the true positive rate to73.5%, an increase of 18.5% absolute.Of course, at test time, we will not knowwhether an OOV is an abbreviation or not, sowe also looked at the performance on the restof the collected data, to see how often it erro-neously suggests an expansion from that set.
Ofthe 11,157 examples that were hand-labeled asnon-abbreviations, our SVM ?N-gram system ex-panded 45 i ems, which is a false positive rateof 0.4% under the assumption that none of themshould be expanded.
In fact, m nual inspectionfound that 20% of these were correct expansionsof abbreviations that had been mis-labeled.We also experimented with a umber of alter-native hig precision approaches tha space p e-cludes our presenting in detail here, including:pruning the number of expansion candidates basedon the pair LM score; only allowing abbreviationexpansion when at least one extracted n-gram con-text is present for that expansion in that context;and CART tree (Breiman et al, 1984) trainingwith real valued scores.
Some of these yieldedvery high precision systems, though at the costof leaving many more abbreviations unexpanded.We found that, for use in combination with thebaseline TTS system, large overall reductions inFP rate were achieved by using an initial systemwith substantially higher TP and somewhat higherFP rates, since far fewer abbreviations were thenpassed along unexpanded to the baseline system,with its relatively high 3% FP rate.To ensure that we did not overtune our systemsto the development set through experimentation,we performed 10-fold cross validation over the fullset of abbreviations.
These results are presentedin Table 2.
Most notably, the TTS baseline systemhas a much lower true positive rate; yet we find oursystems achieve performance very close to that forthe development set, so that our final combinationwith the TTS baseline was actually slighly betterthan the numbers on the development set.5 ConclusionsIn this paper we have presented methods for highprecision abbreviation expansion for a TTS appli-cation.
The methods are largely self-organizing,using in-domain unannotated data, and depend ononly a small amount of annotated data.
Since theSVM features relate to general properties of ab-breviations, expansions and contexts, the classi-fier parameters will likely carry over to new (En-glish) domains.
We demonstrate that in combi-nation with a hand-built TTS baseline, the meth-ods afford dramatic improvement in the TP rate(to about 74% from a starting point of about 40%)and a reduction of FP to below our goal of 2%.AcknowledgmentsWe would like to thank Daan van Esch and theGoogle Speech Data Operations team for theirwork on preparing the annotated data.
We alsothank the reviewers for their comments.368ReferencesAlan Agresti.
2002.
Categorical data analysis.
JohnWiley & Sons, 2nd edition.Ai Ti Aw and Lian Hau Lee.
2012.
Personalized nor-malization for a multilingual chat system.
In Pro-ceedings of the ACL 2012 System Demonstrations,pages 31?36, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Richard Beaufort, Sophie Roekhaut, Louise-Am?elieCougnon, and C?edrick Fairon.
2010.
A hybridrule/model-based finite-state framework for normal-izing SMS messages.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 770?779, Uppsala, Sweden, July.Association for Computational Linguistics.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451.Leo Breiman, Jerome H. Friedman, Richard A. Olshen,and Charles J.
Stone.
1984.
Classification and Re-gression Trees.
Wadsworth & Brooks, Pacific GroveCA.Monojit Choudhury, Rahul Saraf, Vijit Jain, SudeshaSarkar, and Anupam Basu.
2007.
Investigation andmodeling of the structure of texting language.
Int.
J.Doc.
Anal.
Recognit., 10:157?174.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Hany Hassan and Arul Menezes.
2013.
Social text nor-malization using contextual graph random walks.
InProceedings of the 51st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1577?1586.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recogniser.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3):400?401.Max Kaufmann.
2010.
Syntactic normalization ofTwitter messages.
In International Conference onNLP.Catherine Kobus, Franc?ois Yvon, and G?eraldineDamnati.
2008.
Normalizing SMS: are twometaphors better than one?
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008), pages 441?448, Manch-ester, UK, August.
Coling 2008 Organizing Com-mittee.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011.
Insertion, deletion, or substitution?
Nor-malizing text messages without pre-categorizationnor supervision.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, pages 71?76, Portland, Oregon, USA, June.
Association forComputational Linguistics.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012a.
Abroad-coverage normalization system for social me-dia language.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 1035?1044, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Xiaohua Liu, Ming Zhou, Xiangyang Zhou,Zhongyang Fu, and Furu Wei.
2012b.
Jointinference of named entity recognition and nor-malization for tweets.
In Proceedings of the 50thAnnual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages526?535, Jeju Island, Korea, July.
Association forComputational Linguistics.Burt L Monroe, Michael P Colaresi, and Kevin MQuinn.
2008.
Fightin?words: Lexical feature se-lection and evaluation for identifying the content ofpolitical conflict.
Political Analysis, 16(4):372?403.Deana Pennell and Yang Liu.
2011.
A character-levelmachine translation approach for normalization ofSMS abbreviations.
In IJCNLP.
Papers/pennell-liu3.pdf.Brian Roark, Michael Riley, Cyril Allauzen, Terry Tai,and Richard Sproat.
2012.
The OpenGrm open-source finite-state grammar software libraries.
InACL, Jeju Island, Korea.Richard Sproat, Alan Black, Stanley Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of non-standard words.
Com-puter Speech and Language, 15(3):287?333.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proc.
DARPA Broad-cast News Transcription and Understanding Work-shop, pages 270?274.Amber Wilcox-O?Hearn, Graeme Hirst, and AlexanderBudanitsky.
2008.
Real-word spelling correctionwith trigrams: A reconsideration of the Mays, Dam-erau, and Mercer model.
In CICLing 2008, volume4919 of LNCS, pages 605?616, Berlin.
Springer.Yunqing Xia, Kam-Fai Wong, and Wenjie Li.
2006.A phonetic-based approach to Chinese chat text nor-malization.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics, pages 993?1000, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Yi Yang and Jacob Eisenstein.
2013.
A log-linearmodel for unsupervised text normalization.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 61?72.369
