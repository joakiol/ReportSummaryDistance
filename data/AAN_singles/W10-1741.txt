Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 282?289,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsL1 Regularized Regression for Reranking and System Combination inMachine TranslationErgun Bic?iciKoc?
University34450 Sariyer, Istanbul, Turkeyebicici@ku.edu.trDeniz YuretKoc?
University34450 Sariyer, Istanbul, Turkeydyuret@ku.edu.trAbstractWe use L1 regularized transductive regres-sion to learn mappings between sourceand target features of the training setsderived for each test sentence and usethese mappings to rerank translation out-puts.
We compare the effectiveness of L1regularization techniques for regression tolearn mappings between features given ina sparse feature matrix.
The results showthe effectiveness of using L1 regulariza-tion versus L2 used in ridge regression.We show that regression mapping is ef-fective in reranking translation outputs andin selecting the best system combinationswith encouraging results on different lan-guage pairs.1 IntroductionRegression can be used to find mappings be-tween the source and target feature sets derivedfrom given parallel corpora.
Transduction learn-ing uses a subset of the training examples thatare closely related to the test set without usingthe model induced by the full training set.
Inthe context of SMT, we select a few training in-stances for each test instance to guide the transla-tion process.
This also gives us a computationaladvantage when considering the high dimension-ality of the problem.
The goal in transductiveregression based machine translation (TRegMT)is both reducing the computational burden of theregression approach by reducing the dimension-ality of the training set and the feature set andalso improving the translation quality by usingtransduction.
Transductive regression is shown toachieve higher accuracy than L2 regularized ridgeregression on some machine learning benchmarkdatasets (Chapelle et al, 1999).In an idealized feature mapping matrix wherefeatures are word sequences, we would like to ob-serve few target features for each source featurederived from a source sentence.
In this setting, wecan think of feature mappings being close to per-mutation matrices with one nonzero item for eachcolumn.
L1 regularization helps us achieve solu-tions close to the permutation matrices by increas-ing sparsity.We show that L1 regularized regression map-ping is effective in reranking translation outputsand present encouraging results on different lan-guage pairs in the translation task of WMT10.
Inthe system combination task, different translationoutputs of different translation systems are com-bined to find a better translation.
We model systemcombination task as a reranking problem amongthe competing translation models and present en-couraging results with the TRegMT system.Related Work: Regression techniques canbe used to model the relationship betweenstrings (Cortes et al, 2007).
Wang et al (2007)applies a string-to-string mapping approachto machine translation by using ordinary leastsquares regression and n-gram string kernels toa small dataset.
Later they use L2 regularizedleast squares regression (Wang and Shawe-Taylor,2008).
Although the translation quality theyachieve is not better than Moses (Koehn et al,2007), which is accepted to be the state-of-the-art,they show the feasibility of the approach.
Ser-rano et al (2009) use kernel regression to findtranslation mappings from source to target featurevectors and experiment with translating hotelfront desk requests.
Ueffing (2007) approachesthe transductive learning problem for SMT bybootstrapping the training using the translationsproduced by the SMT system that have a scoringperformance above some threshold as estimatedby the SMT system itself.282Outline: Section 2 gives an overview of regres-sion based machine translation, which is used tofind the mappings between the source and targetfeatures of the training set.
In section 3 we presentL1 regularized transductive regression for align-ment learning.
Section 4 presents our experiments,instance selection techniques, and results on thetranslation task for WMT10.
In section 5, wepresent the results on the system combination taskusing reranking.
The last section concludes.2 An Overview of Regression BasedMachine TranslationLet X and Y correspond to the token sets used torepresent source and target strings, then a train-ing sample of m inputs can be represented as(x1, y1), .
.
.
, (xm, ym) ?
X?
?
Y ?, where (xi, yi)corresponds to a pair of source and target languagetoken sequences.
Our goal is to find a mappingf : X?
?
Y ?
that can convert a given set ofsource tokens to a set of target tokens that sharethe same meaning in the target language.X?
Y ?-?
R ?-FX FYg?X ?Y6?
?1YfhFigure 1: String-to-string mapping.Figure 1 depicts the mappings between differentrepresentations.
?X : X?
?
FX = RNX and?Y : Y ?
?
FY = RNY map each string sequenceto a point in high dimensional real number spacewhere dim(FX) = NX and dim(FY ) = NY .Let MX ?
RNX?m and MY ?
RNY ?m suchthat MX = [?X(x1), .
.
.
,?X(xm)] and MY =[?Y (y1), .
.
.
,?Y (ym)].
The ridge regression so-lution using L2 regularization is found as:HL2 = arg minH?RNY ?NX?MY ?HMX ?2F +?
?H?2F .
(1)Proposition 1 Solution to the cost function givenin Equation 1 is found by the following identities:H = MY MTX(MXMTX + ?INX )?1 (primal)H = MY (KX + ?Im)?1MTX (dual)(2)where KX = MTXMX is the Gram matrix withKX(i, j) = kX(xi, xj) and kX(xi, xj) is the ker-nel function defined as kX(xi, xj) = ?(xi)T?
(xj).The primal solution involves the inversion of thecovariance matrix in the feature space (O(N3X))and the dual solution involves the inversion of thekernel matrix in the instance space (O(m3)) andL2 regularization term prevents the normal equa-tions to be singular.
We use the dual solution whencomputing HL2 .Two main challenges of the RegMT approachare learning the regression function, g : X?
?FY , and solving the pre-image problem, which,given the features of the estimated target string se-quence, g(x) = ?Y (y?
), attempts to find y ?
Y ?
:f(x) = arg miny?Y ?
||g(x)?
?Y (y)||2.
Pre-imagecalculation involves a search over possible transla-tions minimizing the cost function:f(x) = arg miny?Y ??
?Y (y)?H?X(x)?2= arg miny?Y ?kY (y, y)?
2(KyY )T (KX + ?Im)?1KxX ,(3)where KyY =[kY (y, y1), .
.
.
, kY (y, ym)]T ?
Rm?1and KxX ?
Rm?1 is defined similarly.We use n-spectrum weighted word ker-nel (Shawe-Taylor and Cristianini, 2004) as fea-ture mappers which consider all word sequencesup to order n:k(x, x?
)=nXp=1|x|?p+1Xi=1|x?|?p+1Xj=1p I(x[i : i+p?1]=x?
[j :j+p?1])(4)where x[i : j] denotes a substring of x with thewords in the range [i, j], I(.)
is the indicator func-tion, and p is the number of words in the feature.3 L1 Regularized RegressionIn statistical machine translation, parallel cor-pora, which contain translations of the same doc-uments in source and target languages, are usedto estimate a likely target translation for a givensource sentence based on the observed transla-tions.
String kernels lead to very sparse represen-tations of the feature space and we examine the ef-fectiveness of L1 regularized regression to find themappings between sparsely observed feature sets.3.1 Sparsity in Translation MappingsWe would like to observe only a few nonzero tar-get feature coefficients corresponding to a sourcefeature in the coefficient matrix.
An example solu-tion matrix representing a possible alignment be-tween unigram source and target features could bethe following:283H e1 e2 e3f1 1 1f2 1f3 1Here ei represents unigram source features and firepresent unigram target features.
e1 and e3 haveunambiguous translations whereas e2 is ambigu-ous.
Even if unigram features lead to ambiguity,we expect higher order features like bigrams andtrigrams to help us resolve the ambiguity.
TypicalH matrices have thousands of features.
L1 regu-larization helps us achieve solutions close to per-mutation matrices by increasing sparsity (Bishop,2006).
In contrast, L2 solutions give us dense ma-trices.3.2 L1 Regularized Regression for LearningHL2 does not give us a sparse solution and mostof the coefficients remain non-zero.
L1 norm be-haves both as a feature selection technique and amethod for reducing coefficient values.HL1 = arg minH?RNY ?NX?MY ?HMX ?2F +?
?H?1 .
(5)Equation 5 presents the lasso (least absoluteshrinkage and selection operator) (Tibshirani,1996) solution where the regularization term isnow the L1 matrix norm defined as ?
H ?1=?i,j |Hi,j |.
Since L1 regularization cost is notdifferentiable, HL1 is found by optimization or ap-proximation techniques.
We briefly describe threetechniques to obtain L1 regularized regression co-efficients.Forward Stagewise Regression (FSR): Weexperiment with forward stagewise regression(FSR) (Hastie et al, 2006), which approximatesthe lasso.
The incremental forward stagewise re-gression algorithm increases the weight of the pre-dictor variable that is most correlated with theresidual by a small amount, , multiplied withthe sign of the correlation at each step.
As ?
0, the profile of the coefficients resemble thelasso (Hastie et al, 2009).Quadratic Programming (QP): We also usequadratic programming (QP) to find HL1 .
We canpose lasso as a QP problem as follows (M?rupand Clemmensen, 2007).
We assume that therows of MY are independent and solve for eachrow i, Myi ?
R1?m, using non-negative variablesh+i ,h?i ?
RNX?1 such that hi = h+i ?
h?i :hi = arg minh?Myi ?
hMX?2F +?NXXk=1|hk|, (6)hi = arg minh?i12h?igMXgMXTh?iT?
h?i(gMXMTyi ?
?1), (7)s.t.
h?i > 0, gMX =?MX?MX?, h?i =?h+i h?i?.Linear Programming (LP): L1 minimizationcan also be posed as a linear programming (LP)problem by interpreting the error term as the con-straint (Chen et al, 1998) and solving for each rowi:hi = arg minh?h?1 subject to Myi = hMX , (8)which can again be solved using non-negativevariables.
This is a slightly different optimizationand the results can be different but linear program-ming solvers offer computational advantages.3.3 Transductive RegressionTransduction uses test instances, which can some-times be accessible at training time, to learn spe-cific models tailored towards the test set.
Trans-duction has computational advantages by not us-ing the full training set and by having to satisfy asmaller set of constraints.
For each test sentence,we pick a limited number of training instances de-signed to improve the coverage of correct featuresto build a regression model.
Section 4.2 details ourinstance selection methods.4 Translation ExperimentsWe perform experiments on the translation taskof the English-German, German-English, English-French, English-Spanish, and English-Czech lan-guage pairs using the training corpus provided inWMT10.4.1 Datasets and BaselineWe developed separate SMT models usingMoses (Koehn et al, 2007) with default settingswith maximum sentence length set to 80 using 5-gram language model and obtained distinct 100-best lists for the test sets.
All systems were tunedwith 2051 sentences and tested with 2525 sen-tences.
We have randomly picked 100 instancesfrom the development set to be used in tuning theregression experiments (dev.100).
The translationchallenge test set contains 2489 sentences.
Num-ber of sentences in the training set of each system284and baseline performances for uncased output (testset BLEU, challenge test set BLEU) are given inTable 1.Corpus # sent BLEU BLEU Challengeen-de 1609988 .1471 .1309de-en 1609988 .1943 .1556en-fr 1728965 .2281 .2049en-es 1715158 .2237 .2106en-cz 7320238 .1452 .1145Table 1: Initial uncased performances of the trans-lation systems.Feature mappers used are 3-spectrum countingword kernels, which consider all N -grams up toorder 3 weighted by the number of tokens in thefeature.
We segment sentences using some of thepunctuation for managing the feature set better anddo not consider N -grams that cross segments.We use BLEU (Papineni et al, 2001) andNIST (Doddington, 2002) evaluation metrics formeasuring the performance of translations auto-matically.4.2 Instance SelectionProper selection of training instances plays an im-portant role to learn feature mappings with limitedcomputational resources accurately.
In previouswork (Wang and Shawe-Taylor, 2008), sentencebased training instances were selected using tf-idfretrieval.
We transform test sentences to featuresets obtained by the kernel mapping before mea-suring their similarities and index the sentencesbased on the features.
Given a source sentenceof length 20, its feature representation would havea total of 57 uni/bi/tri-gram features.
If we selectclosest sentences from the training set, we may nothave translations for all the features in this repre-sentation.
But if we search for translations of eachfeature, then we have a higher chance of coveringall the features found in the sentence we are try-ing to translate.
The index acts as a dictionary ofsource phrases storing training set entries whosesource sentence match the given source phrase.The number of instances per feature is choseninversely proportional to the frequency of the fea-ture determined by the following formula:#instance(f) = n/ ln(1 + idfScore(f)/9.0), (9)where idfScore(f) sums the idf (inverse documentfrequency) of the tokens in feature f and n is asmall number.4.3 Addition of Brevity PenaltyDetailed analysis of the results shows TRegMTscore achieves better N -gram match percentagesthan Moses translation but suffers from the brevitypenalty due to selecting shorter translations.
Dueto using a cost function that minimizes the squaredloss, TRegMT score tends to select shorter trans-lations when the coverage is low.
We also observethat we are able to achieve higher scores for NIST,which suggests the addition of a brevity penalty tothe score.Precision based BLEU scoring divides N -grammatch counts toN -gram counts found in the trans-lation and this gives an advantage to shorter trans-lations.
Therefore, a brevity penalty (BP) is addedto penalize short translations:BP = min(1?ref-lengthtrans-length, 0) (10)BLEU = e(log(ngramprec)+BP) (11)where ngramprec represent the sum of n-gramprecisions.
Moses rarely incurs BP as it has a wordpenalty parameter optimized against BLEU whichpenalizes translations that are too long or too short.For instance, Moses 1-best translation for en-desystem achieves .1309 BLEU versus .1320 BLEUwithout BP.We handle short translations in two ways.
Weoptimize the ?
parameter of QP, which managesthe sparsity of the solution (larger ?
values cor-respond to sparser solutions) against BLEU scorerather than the squared loss.
Optimization yields?
= 20.744.
We alternatively add a BP cost to thesquared loss:BP = e?min(1?|?Y (y)||pH?X (x)+?BP q|,0)?
(12)f(x) = arg miny?Y ??
?Y (y)?H?X(x)?2 +?BPBP (13)where |.| denotes the length of the feature vector,p.q rounds feature weights to integers, ?BP is aconstant weight added to the estimation, and ?BPis the weight given for the BP cost.
|pH?X(x) +?BP q| represents an estimate of the length of thereference as found by the TRegMT system.
ThisBP cost estimate is similar to the cost used in (Ser-rano et al, 2009) normalized by the length of thereference.
We found ?BP = 0.1316 and ?BP =?13.68 when optimized on the en-de system.
Weadd a BP penalty to all of the reranking resultsgiven in the next section and QP results also useoptimized ?.285en-de de-en en-fr en-es en-czScore BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NISTBaseline .1309 5.1417 .1556 5.4164 .2049 6.3194 .2106 6.3611 .1145 4.5008Oracle .1811 6.0252 .2101 6.2103 .2683 7.2409 .2770 7.3190 .1628 5.4501L2 .1319 5.1680 .1555 5.4344 .2044 6.3370 .2132 6.4093 .1148 4.5187FSR .1317* 5.1639 .1559 5.4383 .2053 6.3458 .2144 6.4168 .1150 4.5172LP .1317 5.1695 .1561 5.4304 .2048 6.3245 .2109 6.4176 .1124 4.5143QP .1309 5.1664 .1550 5.4553 .2033 6.3354* .2121 6.4271 .1150 4.5264Table 2: Reranking results using TRegMT, TM, and LM scores.
We use approximate randomizationtest (Riezler and Maxwell, 2005) with 1000 repetitions to determine score difference significance: resultsin bold are significant with p ?
0.01 and italic results with (*) are significant with p ?
.05.
Thedifference of the remaining from the baseline are not statistically significant.4.4 Reranking ExperimentsWe rerank N -best lists by using linear combina-tions of the following scoring functions:1.
TRegMT: Transductive regression based ma-chine translation scores as found by Equa-tion 3.2.
TM: Translation model scores we obtainfrom the baseline SMT system that is usedto generate the N -best lists.3.
LM: 5-gram language model scores that thebaseline SMT system uses when calculatingthe translation model scores.The training set we obtain may not contain allof the features of the reference target due to lowcoverage.
Therefore, when performing reranking,we also add the cost coming from the features of?Y (y) that are not represented in the training setto the squared loss as in:?
?Y (y) \ FY ?2 + ?
?Y (y)?H?X(x)?2, (14)where ?Y (y) \ FY represent the features of y notrepresented in the training set.We note that TRegMT score only contains or-dering information as present in the bi/tri-gramfeatures in the training set.
Therefore, the ad-dition of a 5-gram LM score as well as the TMscore, which also incorporates the LM score initself, improves the performance.
We are notable to improve the BLEU score when we useTRegMT score by itself however we are able toachieve improvements in the NIST and 1-WERscores.
The performance increase is important fortwo reasons.
First of all, we are able to improvethe performance using blended spectrum 3-gramfeatures against translations obtained with 5-gramlanguage model and higher order features.
Out-performing higher order n-gram models is knownto be a difficult task (Galley and Manning, 2009).Secondly, increasing the performance with rerank-ing itself is a hard task since possible translationsare already constrained by the ones observed inN -best lists.
Therefore, an increase in the N -best listsize may increase the score gaps.Table 2 presents reranking results on all of thelanguage pairs we considered, using TRegMT,TM, and LM scores with the combination weightslearned in the development set.
We are able toachieve better BLEU and NIST scores on all of thelisted systems.
We are able to see up to .38 BLEUpoints increase for the en-es pair.
Oracle rerankingperformances are obtained by using BLEU scoringmetric.If we used only the TM and LM scores whenreranking with the en-de system, then we wouldobtain .1309 BLEU and 5.1472 NIST scores.
Weonly see a minor increase in the NIST score and nochange in the BLEU score with this setting whencompared with the baseline given in Table 2.Due to computational reasons, we do not usethe same number of instances to train differentmodels.
In our experiments, we used n = 3 forL2, n = 1.5 for FSR, and n = 1.2 for QP andLP solutions to select the number of instances inEquation 9.
The average number of instances usedper sentence in training corresponding to thesechoices are approximately 140, 74, and 61.
Evenwith these decreased number of training instances,L1 regularized regression techniques are able toachieve comparable scores to L2 regularized re-gression model in Table 2.5 System Combination ExperimentsWe perform experiments on the system com-bination task for the English-German, German-English, English-French, English-Spanish, andEnglish-Czech language pairs using the training286en-de de-en en-fr en-es en-czScore BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NISTRandom .1490 5.6555 .2088 6.4886 .2415 6.8948 .2648 7.2563 .1283 4.9238Best model .1658 5.9610 .2408 6.9861 .2864 7.5272 .3047 7.7559 .1576 5.4480L2 .1694 5.9974 .2336 6.9398 .2948 7.7037 .3036 7.8120 .1657 5.5654FSR .1689 5.9638 .2357 6.9254 .2947 7.7107 .3049 7.8156 .1657 5.5632LP .1694 5.9954 .2368 6.8850 .2928 7.7157 .3027 7.7838 .1659 5.5680QP .1692 5.9983 .2368 6.9172 .2913 7.6949 .3040 7.8086 .1662 5.5785Table 3: Reranking results using TRegMT, TM, and LM scores.
bold correspond to the best score ineach rectangle of scores.corpus provided in WMT10.5.1 DatasetsWe use the training set provided in WMT10 to in-dex and select transductive instances from.
Thechallenge split the test set for the translation taskof 2489 sentences into a tuning set of 455 sen-tences and a test set with the remaining 2034 sen-tences.
Translation outputs for each system isgiven in a separate file and the number of sys-tem outputs per translation pair varies.
We havetokenized and lowercased each of the system out-puts and combined these in a singleN -best file perlanguage pair.
We also segment sentences usingsome of the punctuation for managing the featureset better.
We use these N -best lists for TRegMTreranking to select the best translation model.
Fea-ture mappers used are 3-spectrum counting wordkernels, which consider all n-grams up to order 3weighted by the number of tokens in the feature.5.2 ExperimentsWe rerank N -best lists by using combinations ofthe following scoring functions:1.
TRegMT: Transductive regression based ma-chine translation scores as found by Equa-tion 3.2.
TM?
: Translation model scores are obtainedby measuring the average BLEU perfor-mance of each translation relative to the othertranslations in the N -best list.3.
LM: We calculate 5-gram language modelscores for each translation using the languagemodel trained over the target corpus providedin the translation task.Since we do not have access to the referencetranslations nor to the translation model scoreseach system obtained for each sentence, we es-timate translation model performance (TM?)
bymeasuring the average BLEU performance of eachtranslation relative to the other translations in theN -best list.
Thus, each possible translation in theN -best list is BLEU scored against other transla-tions and the average of these scores is selectedas the TM score for the sentence.
Sentence levelBLEU score calculation avoids singularities in n-gram precisions by taking the maximum of thematch count and 12|si| for |si| denoting the lengthof the source sentence si as used in (Macherey andOch, 2007).Table 3 presents reranking results on all of thelanguage pairs we considered, using TRegMT,TM, and LM scores with the same combinationweights as above.
Random model score lists therandom model performance selected among thecompeting translations randomly and it is used asa baseline.
Best model score lists the performanceof the best model performance.
We are able toachieve better BLEU and NIST scores in all of thelisted systems except for the de-en language pairwhen compared with the performance of the bestcompeting translation system.
The lower perfor-mance in the de-en language pair may be due tohaving a single best translation system that outper-forms others significantly.
The difference betweenthe best model performance and the mean as wellas the variance of the scores in the de-en languagepair is about twice their counterparts in en-de lan-guage pair.Due to computational reasons, we do not usethe same number of instances to train differentmodels.
In our experiments, we used n = 4 forL2, n = 1.5 for FSR, and n = 1.2 for QP andLP solutions to select the number of instances inEquation 9.
The average number of instances usedper sentence in training corresponding to thesechoices are approximately 189, 78, and 64.2876 ContributionsWe use transductive regression to learn mappingsbetween source and target features of given paral-lel corpora and use these mappings to rerank trans-lation outputs.
We compare the effectiveness ofL1regularization techniques for regression.
TRegMTscore has a tendency to select shorter transla-tions when the coverage is low.
We incorporate abrevity penalty to the squared loss and optimize ?parameter of QP to tackle this problem and furtherimprove the performance of the system.The results show the effectiveness of using L1regularization versus L2 used in ridge regression.Proper selection of training instances plays an im-portant role to learn correct feature mappings withlimited computational resources accurately.
Weplan to investigate better instance selection meth-ods for improving the translation performance.TRegMT score has a tendency to select shortertranslations when the coverage is low.
We incor-porate a brevity penalty to the score and optimizethe ?
parameter of QP to tackle this problem.AcknowledgmentsThe research reported here was supported inpart by the Scientific and Technological ResearchCouncil of Turkey (TUBITAK).ReferencesChristopher M. Bishop.
2006.
Pattern Recognitionand Machine Learning.
Springer.Olivier Chapelle, Vladimir Vapnik, and Jason Weston.1999.
Transductive inference for estimating valuesof functions.
In NIPS, pages 421?427.Scott Shaobing Chen, David L. Donoho, andMichael A. Saunders.
1998.
Atomic decompositionby basis pursuit.
SIAM Journal on Scientific Com-puting, 20(1):33?61.Corinna Cortes, Mehryar Mohri, and Jason Weston.2007.
A general regression framework for learn-ing string-to-string mappings.
In Gokhan H. Bakir,Thomas Hofmann, and Bernhard Sch editors, Pre-dicting Structured Data, pages 143?168.
The MITPress, September.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, pages 138?145, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Michel Galley and Christopher D. Manning.
2009.Quadratic-time dependency parsing for machinetranslation.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 773?781,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Trevor Hastie, Jonathan Taylor, Robert Tibshirani, andGuenther Walther.
2006.
Forward stagewise regres-sion and the monotone lasso.
Electronic Journal ofStatistics, 1.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2009.
The Elements of Statistical Learning:Data Mining, Inference and Prediction.
Springer-Verlag, 2nd edition.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of the Assoc.
for Compu-tational Linguistics, pages 177?180, Prague, CzechRepublic, June.Wolfgang Macherey and Franz J. Och.
2007.
Anempirical study on computing consensus transla-tions from multiple machine translation systems.
InEMNLP-CoNLL, pages 986?995.M.
M?rup and L. H. Clemmensen.
2007.
Multiplica-tive updates for the lasso.
In Machine Learning forSignal Processing MLSP, IEEE Workshop on, pages33 ?38, Aug.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 311?318,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures forMachine Translation and/or Summarization, pages57?64, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Nicolas Serrano, Jesus Andres-Ferrer, and FranciscoCasacuberta.
2009.
On a kernel regression approachto machine translation.
In Iberian Conference onPattern Recognition and Image Analysis, pages 394?401.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.288Robert J. Tibshirani.
1996.
Regression shrinkage andselection via the lasso.
Journal of the Royal Statisti-cal Society, Series B, 58(1):267?288.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Transductive learning for statisticalmachine translation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 25?32, Prague, Czech Republic,June.
The Association for Computer Linguistics.Zhuoran Wang and John Shawe-Taylor.
2008.
Kernelregression framework for machine translation: UCLsystem description for WMT 2008 shared transla-tion task.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 155?158,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-mak.
2007.
Kernel regression based machine trans-lation.
In Human Language Technologies 2007:The Conference of the North American Chapterof the Association for Computational Linguistics;Companion Volume, Short Papers, pages 185?188,Rochester, New York, April.
Association for Com-putational Linguistics.289
