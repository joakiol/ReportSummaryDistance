Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 443?447,Dublin, Ireland, August 23-24, 2014.NRC-Canada-2014: Recent Improvements inthe Sentiment Analysis of TweetsXiaodan Zhu, Svetlana Kiritchenko, and Saif M. MohammadNational Research Council CanadaOttawa, Ontario, Canada K1A 0R6{xiaodan.zhu,svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.caAbstractThis paper describes state-of-the-art statis-tical systems for automatic sentiment anal-ysis of tweets.
In a Semeval-2014 sharedtask (Task 9), our submissions obtainedhighest scores in the term-level sentimentclassification subtask on both the 2013 and2014 tweets test sets.
In the message-levelsentiment classification task, our submis-sions obtained highest scores on the Live-Journal blog posts test set, sarcastic tweetstest set, and the 2013 SMS test set.
Thesesystems build on our SemEval-2013 senti-ment analysis systems (Mohammad et al.,2013) which ranked first in both the term-and message-level subtasks in 2013.
Keyimprovements over the 2013 systems arein the handling of negation.
We createseparate tweet-specific sentiment lexiconsfor terms in affirmative contexts and innegated contexts.1 IntroductionAutomatically detecting sentiment of tweets (andother microblog posts) has attracted extensiveinterest from both the academia and industry.The Conference on Semantic Evaluation Exercises(SemEval) organizes a shared task on the senti-ment analysis of tweets with two subtasks.
In themessage-level task, the participating systems areto identify whether a tweet as a whole expressespositive, negative, or neutral sentiment.
In theterm-level task, the objective is to determine thesentiment of a marked target term (a single wordor a multi-word expression) within the tweet.
Oursubmissions stood first in both subtasks in 2013.This paper describes improvements over that sys-Evaluation Set Term-level Task Message-level TaskTwt14 1 4Twt13 1 2Sarc14 3 1LvJn14 2 1SMS13 2 1Table 1: Overall rank of NRC-Canada sentimentanalysis models in Semeval-2014 Task 9 under theconstrained condition.
The rows are five evalua-tion datasets and the columns are the two subtasks.tem and the subsequent submissions to the 2014shared task (Rosenthal et al., 2014).The training data for the SemEval-2014 sharedtask is same as that of SemEval-2013 (about10,000 tweets).
The 2014 test set has five sub-categories: a tweet set provided newly in 2014(Twt14), the tweet set used for testing in the 2013shared task (Twt13), a set of tweets that are sarcas-tic (Sarc14), a set of sentences from the bloggingwebsite LiveJournal (LvJn14), and the set of SMSmessages used for testing in the 2013 shared task(SMS13).
Instances from these categories were in-terspersed in the provided test set.
The partici-pants were not told about the source of the indi-vidual messages.
The objective was to determinehow well a system trained on tweets generalizes totexts from other domains.Our submissions to SemEval-2014 Task 9,ranked first in five out of the ten subtask?datasetcombinations.
In the other evaluation sets as well,our submissions performed competitively.
Theresults are summarized in Table 1.
As we willshow, automatically generated tweet-specific lexi-cons were especially helpful in all subtask?datasetcombinations.
The results also show that eventhough our models are trained only on tweets, theygeneralize well to data from other domains.443Our systems are based on supervised SVMs anda number of surface-form, semantic, and senti-ment features.
The major improvement in our2014 system over the 2013 system is in the way ithandles negation.
Morante and Sporleder (2012)define negation to be ?a grammatical category thatallows the changing of the truth value of a propo-sition?.
Negation is often expressed through theuse of negative signals or negators, words such asisnt and never, and it can significantly affect thesentiment of its scope.
We create separate tweet-specific sentiment lexicons for terms in affirmativecontexts and in negated contexts.
That is, we au-tomatically determine the average sentiment of aterm when occurring in an affirmative context, andseparately the average sentiment of a term whenoccurring in a negated context.2 Our SystemsOur SemEval-2014 systems are based on ourSemEval-2013 systems (Mohammad et al., 2013).For completeness, we briefly revisit our previ-ous approach, which uses support vector machine(SVM) as the classification algorithm and lever-ages the following features.Lexicon features These features are generated byusing three manually constructed sentiment lexi-cons and two automatically constructed lexicons.The manually constructed lexicons include theNRC Emotion Lexicon (Mohammad and Turney,2010; Mohammad and Yang, 2011), the MPQALexicon (Wilson et al., 2005), and the Bing LiuLexicon (Hu and Liu, 2004).
The two automati-cally constructed lexicons, the Hashtag SentimentLexicon and the Sentiment140 Lexicon, were cre-ated specifically for tweets (Mohammad et al.,2013).The sentiment score of each term (e.g., a wordor bigram) in the automatically constructed lexi-cons is computed by measuring the PMI (point-wise mutual information) between the term andthe positive or negative category of tweets usingthe formula:SenScore (w) = PMI(w, pos)?
PMI(w, neg)(1)where w is a term in the lexicons.
PMI(w, pos)is the PMI score between w and the positive class,and PMI(w, neg) is the PMI score between wand the negative class.
Therefore, a positive Sen-Score (w) suggests a stronger association of wordw with positive sentiment and vice versa.
Themagnitude indicates the strength of association.Note that the sentiment class of the tweets usedto construct the lexicons was automatically iden-tified either from hashtags or from emoticons asdescribed in (Mohammad et al., 2013).With these lexicons available, the following fea-tures were extracted for a text span.
Here a textspan can be a target term, its context, or an en-tire tweet, depending on the task.
The lexiconfeatures include: (1) the number of sentiment to-kens in a text span; sentiment tokens are wordtokens whose sentiment scores are not zero in alexicon; (2) the total sentiment score of the textspan:?w?textSpanSenScore (w); (3) the maxi-mal score: maxw?textSpanSenScore (w); (4) thetotal positive and negative sentiment scores of thetext span; (5) the sentiment score of the last tokenin the text span.
Note that all these features aregenerated, when applicable, by using each of thesentiment lexicons mentioned above.Ngrams We employed two types of ngram fea-tures: word ngrams and character ngrams.
Theformer reflect the presence or absence of contigu-ous or non-contiguous sequences of words, andthe latter are sequences of prefix/suffix charactersin each word.
These features are same as in ourlast year?s submission.Negation The number of negated contexts.
Ourdefinition of a negated context follows Pang et al.
(2002), which will be described in more details be-low in Section 2.1.POS The number of occurrences of each part-of-speech tag.
We tokenized and part-of-speechtagged the tweets with the Carnegie Mellon Uni-versity (CMU) Twitter NLP tool (Gimpel et al.,2011).Cluster features The CMU POS-tagging tool pro-vides the token clusters produced with the Brownclustering algorithm from 56 million English-language tweets.
These 1,000 clusters serve as analternative representation of tweet content, reduc-ing the sparsity of the token space.Encodings The encoding features are derivedfrom hashtags, punctuation marks, emoticons,elongated words, and uppercased words.For the term-level task, all the above featuresare extracted for target terms and their context,where a context is a window of words surround-ing a target term.
For the message-level task, thefeatures are extracted from the whole tweet.444In the term-level task, we used the LIB-SVM (Chang and Lin, 2011) tool with the follow-ing parameters: -t 0 -b 1 -m 1000.
The total num-ber of features is about 115,000.
In the message-level task, we used an in-house implementation ofSVM with a linear kernel.
The parameter C wasset to 0.005.
The total number of features wasabout 1.5 million.2.1 Improving Lexicons and NegationModelsAn important advantage of our SemEval-2013systems comes from the use of the two high-coverage tweet-specific sentiment lexicons.
Inthe SemEval-2014 submissions, we improve theselexicons by incorporating negation modeling intothe lexicon generation process.2.1.1 Improving Sentiment LexiconsA word in a negated context has a different eval-uative nature than the same word in an affirma-tive (non-negated) context.
We have proposed alexicon-based approach (Kiritchenko et al., 2014)to determining the sentiment of words in these twosituations by automatically creating separate senti-ment lexicons for the affirmative and negated con-texts.
In this way, we do not need to employ anyexplicit assumptions to model negation.To achieve this, a tweet corpus is split into twoparts: Affirmative Context Corpus and NegatedContext Corpus.
Following the work of Pang et al.
(2002), we define a negated context as a segmentof a tweet that starts with a negation word (e.g., no,shouldn?t) and ends with one of the punctuationmarks: ?,?, ?.
?, ?
:?, ?
;?, ?!
?, ???.
The list of negationwords was adopted from Christopher Potts?
senti-ment tutorial.1Thus, part of a tweet that is markedas negated is included into the negated context cor-pus while the rest of the tweet becomes part of theaffirmative context corpus.
The sentiment labelfor the tweet is kept unchanged in both corpora.Then, we generate an affirmative context lexiconfrom the affirmative context corpus and a negatedcontext lexicon from the negated context corpususing the technique described in (Kiritchenko etal., 2014).Furthermore, we refined the method of con-structing the negated context lexicons by split-ting a negated context into two parts: the imme-diate context consisting of a single token that di-rectly follows a negation word, and the distant1http://sentiment.christopherpotts.net/lingstruc.htmlcontext consisting of the rest of the tokens in thenegated context.
This has two benefits.
Intu-itively, negation affects words directly followingthe negation words more strongly than more dis-tant words.
Second, immediate-context scores areless noisy.
Our simple negation scope identifica-tion algorithm can at times fail and include partsof a tweet that are not actually negated (e.g., if apunctuation mark is missing).
Overall, a sentimentword can have up to three scores, one for affirma-tive context, one for immediate negated context,and one for distant negated context.We reconstructed the Hashtag Sentiment Lexi-con and the Sentiment140 Lexicon with this ap-proach and used them in our SemEval-2014 sys-tems.2.1.2 Discriminating Negation WordsDifferent negation words, e.g., never and didn?t,can have different effects on sentiment (Zhu et al.,2014; Taboada et al., 2011).
In our SemEval-2014submission, we discriminate negation words in theterm-level models.
For example, the word accept-able appearing in a sentence this is never accept-able is marked as acceptable beNever, while inthe sentence this is not acceptable, it is markedas acceptable beNot.
In this way, different nega-tors (e.g., be not and be never) are treated differ-ently.
Note that we do not differentiate the tenseand person of auxiliaries in order to reduce sparse-ness (e.g., was not and am not are treated in thesame way).
This new representation is used to ex-tract ngrams and lexicon-based features.3 ResultsOverall performance The evaluation metric usedin the competition is the macro-averaged F-measure calculated over the positive and negativecategories.
Table 2 presents the overall perfor-mance of our models.
NRC13 and NRC14 arethe systems we submitted to SemEval-2013 andSemEval-2014, respectively.
The integers in thebrackets are our official ranks in SemEval-2014under the constrained condition.In the term-level task, our submission rankedfirst on the two Tweet datasets among 14 teams.The results show that we achieved significant im-provements over our last year?s submission: the F-score improves from 85.19 to 86.63 on the Twt14data and from 89.10 to 90.14 on the Twt13 data.More specifically, on the Twt14 data, the approachdescribed in Section 2.1.1 improved our F-score445Term-level Message-levelNRC13 NRC14 NRC13 NRC14Twt14 85.19 86.63(1) 68.88 69.85(4)Twt13 89.10 90.14(1) 69.02 70.75(2)Sarc14 78.16 77.13(3) 47.64 58.16(1)LvJn14 84.96 85.49(2) 74.01 74.84(1)SMS13 88.34 88.03(2) 68.34 70.28(1)Table 2: Overall performance of the NRC-Canadasentiment analysis systems.from 85.19 to 86.37, and discriminating nega-tion words (discussed in Section 2.1.2) further im-proved the F-score from 86.37 to 86.63.Our system ranked second on the LvJn14 andSMS13 dataset.
Note that the term-level systemthat ranked first on LvJn14 performed worse thanour system on SMS13 and the system that rankedfirst on SMS13 showed worse results than ours onLvJn14, indicating that our term-level models ingeneral have good generalizability on these twoout-of-domain datasets.On the message-level task, again the NRC14system showed significant improvements over thelast year?s system on all five datasets.
It achievedthe second best result on the Twt13 data and thefourth result on the Twt14 data among 42 teams.It was also the best system to predict sentiment insarcastic tweets (Sarc14).
Furthermore, the systemproved to generalize well to other types of shortinformal texts; it placed first on the two out-of-domain datasets: SMS13 and LvJn14.
We observea major improvement of our message-level modelon Sarc14 over our last year?s model, but as thesize of Sarc14 is small (86 tweets), more data andanalysis would be desirable to help better under-stand this phenomenon.Contribution of features Table 3 presents the re-sults of ablation experiments on all five test sets forthe term-level task.
The features derived from themanual and automatic lexicons proved to be usefulon four datasets.
The only exception is the Sarc14data where removing lexicon features results in noperformance improvement.
Considering that thistest set is very small (only about 100 test terms),further investigation would be desirable if a largerdataset becomes available.
Also, in sarcasm thereal sentiment of a text span may be different fromits literal sentiment.
In such a situation, a systemthat correctly recognizes the literal sentiment mayactually make mistakes in capturing the real sen-timent.
The last two rows in Table 3 show the re-sults obtained when the features are extracted onlyfrom the target (and not from its context) and whenthey are extracted only from the context of the tar-get (and not from the target itself).
Observe thateven though the context may influence the polar-ity of the target, using target features alone is sub-stantially more useful than using context featuresalone.
Nonetheless, adding context features im-proves the F-scores in general.On the message-level task (Table 4), the fea-tures derived from the sentiment lexicons and, inparticular, from our large-coverage tweet-specificlexicons turned out to be the most influential.
Theuse of the lexicons provided consistent gains of 9?11 percentage points not only on tweet datasets,but also on out-of-domain SMS and LiveJournaldata.
Note that removing the features derived fromthe manual lexicons as well as removing the ngramfeatures improves the performance on the Twt14dataset.
However, this effect is not observed onthe Twt13 and the out-of-domain test sets.
Thepossible explanation of this phenomenon is minoroverfitting on the tweet data.4 ConclusionsWe presented supervised statistical systems formessage-level and term-level sentiment analysisof tweets.
They incorporate many surface-form,semantic, and sentiment features.
Among sub-missions from over 40 teams in the Semeval-2014 shared task ?Sentiment Analysis in Twit-ter?, our submissions ranked first in five out ofthe ten subtask-dataset combinations.
The sin-gle most useful set of features are those obtainedfrom automatically generated tweet-specific lexi-cons.
We obtained significant improvements overour previous system (which ranked first in the2013 shared task) notably by estimating the senti-ment of words in affirmative and negated contextsseparately.
Also, since different negation wordsimpact sentiment differently, we modeled differentnegation words separately in our term-level sys-tem.
This too led to an improvement in F-score.The results on different kinds of evaluation setsshow that even though our systems are trained onlyon tweets, they generalize well to text from otherdomains such as blog posts and SMS messages.Many of the resources we created and used aremade freely available.22www.purl.com/net/sentimentoftweets446Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13all features 86.63 90.14 77.13 85.49 88.03all - lexicons 81.98 86.25 80.74 80.00 83.91all - manu.
lex.
86.08 89.25 75.32 84.13 87.69all - auto.
lex.
86.05 88.32 80.38 83.96 86.18all - ngrams 83.31 86.67 72.95 81.58 82.41all - target 72.93 74.19 63.09 72.21 69.34all - context 84.40 88.83 77.22 82.99 87.97Table 3: Term-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the featuregroups removed.Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13all features 69.85 70.75 58.16 74.84 70.28all - lexicons 60.59 60.04 47.17 65.80 60.56all - manu.
lex.
71.84 69.84 53.34 73.41 66.60all - auto.
lex.
63.40 65.08 47.57 71.76 66.94all - ngrams 70.02 67.90 44.58 74.43 68.45Table 4: Message-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of thefeature groups removed.AcknowledgmentsWe thank Colin Cherry for providing his SVMcode and for helpful discussions.ReferencesChih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27:1?27:27.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: Annotation, features, and experiments.In Proceedings of ACL.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of KDD,pages 168?177, New York, NY, USA.
ACM.Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-mad.
2014.
Sentiment analysis of short informaltexts.
(To appear) Journal of Artificial IntelligenceResearch.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: UsingMechanical Turk to create an emotion lexicon.
InProceedings of the NAACL-HLT Workshop on Com-putational Approaches to Analysis and Generationof Emotion in Text, LA, California.Saif M. Mohammad and Tony (Wenda) Yang.
2011.Tracking sentiment in mail: How genders differ onemotional axes.
In Proceedings of the ACL Work-shop on Computational Approaches to Subjectivityand Sentiment Analysis, Portland, OR, USA.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the International Workshop on Semantic Evalua-tion, SemEval ?13, Atlanta, Georgia, USA, June.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special is-sue.
Computational linguistics, 38(2):223?260.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86, Philadelphia, PA.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Proceedings ofSemEval-2014, Dublin, Ireland.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional Linguistics, 37(2):267?307.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP, HLT ?05, pages 347?354, Stroudsburg, PA,USA.Xiaodan Zhu, Hongyu Guo, Saif Mohammad, andSvetlana Kiritchenko.
2014.
An empirical study onthe effect of negation words on sentiment.
In Pro-ceedings of ACL, Baltimore, Maryland, USA, June.447
