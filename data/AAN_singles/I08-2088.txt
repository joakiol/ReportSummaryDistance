Method of Selecting Training Data to Build a Compact and EfficientTranslation ModelKeiji Yasuda?,?, Ruiqiang Zhang?,?, Hirofumi Yamamoto?,?
and Eiichiro Sumita?,?
?National Institute of Communications Technology?ATR Spoken Language Translation Research Laboratories2?2?2, Hikaridai, ?Keihanna Science City?, Kyoto, 619?0288 Japan{keiji.yasuda,ruiqiang.zhang}@nict.go.jp{hirofumi.yamamoto,eiichiro.sumita}@nict.go.jpAbstractTarget task matched parallel corpora are re-quired for statistical translation model train-ing.
However, training corpora sometimesinclude both target task matched and un-matched sentences.
In such a case, train-ing set selection can reduce the size of thetranslation model.
In this paper, we proposea training set selection method for transla-tion model training using linear translationmodel interpolation and a language modeltechnique.
According to the experimentalresults, the proposed method reduces thetranslation model size by 50% and improvesBLEU score by 1.76% in comparison with abaseline training corpus usage.1 IntroductionParallel corpus is one of the most important compo-nents in statistical machine translation (SMT), andthere are two main factors contributing to its perfor-mance.
The first is the quality of the parallel corpus,and the second is its quantity.A parallel corpus that has similar statistical char-acteristics to the target domain should yield a moreefficient translation model.
However, domain-mismatched training data might reduce the transla-tion model?s performance.
A large training corpusobviously produces better quality than a small one.However, increasing the size of the training corpuscauses another problem, which is increased compu-tational processing load.
This problem not only af-fects the training of the translation model, but alsoits applications.
The reason for this is that a largeamount of training data tends to yield a large trans-lation model and applications then have to deal withthis model.We propose a method of selecting translationpairs as the training set from a training parallelcorpus to solve the problem of an expanded trans-lation model with increased training load.
Thismethod enables an adequate training set to be se-lected from a large parallel corpus by using a smallin-domain parallel corpus.
We can make the transla-tion model compact without degrading performancebecause this method effectively reduces the size ofthe set for training the translation model.
This com-pact translation model can outperform a translationmodel trained on the entire original corpus.This method is especially effective for domainswhere it is difficult to enlarge the corpus, such asin spoken language parallel corpora (Kikui et al,2006).
The main approach to recovering from an un-dersupply of the in-domain corpus has been to usea very large domain-close or out-of-domain paral-lel corpus for the translation model training (NIST,2006).
In such case, the proposed method effectivelyreduces the size of the training set and translationmodel.Section 2 describes the method of selecting thetraining set.
Section 3 details the experimental re-sults for selecting the training set and actual trans-lation from the International Workshop on SpokenLanguage Translation 2006 (IWSLT2006).
Section4 compares the results of the proposed method withthose of the conventional method.
Section 5 con-cludes the paper.655Target languageTargetlanguage SourcelanguageSource languageLarge out-of-domain parallel corpusSmall in-domain parallel corpus1.
Train translationmodel3.
Calculate perplexityLMTarget LMSource2.
Train languagemodelsTMin-domain4.
Select translation pairs based on the perplexity6.
Integrate TMs using linear interpolation5.
Train translationmodel TMselectedTMfinalFigure 1: Framework of method.2 MethodOur method use a small in-domain parallel corpusand a large out-of-domain parallel corpus, and itselects a number of appropriate training translationpairs from the out-of-domain parallel corpus.
Fig-ure 1 is a flow diagram of the method.
The proce-dure is as follows:1.
Train a translation model using the in-domainparallel corpus.2.
Train a language model using the source lan-guage side or/and target language side of thein-domain corpus.3.
Calculate the word perplexity for each sentence(in source language side or/and target languageside) in the out-of-domain corpus by using thefollowing formulas.PPe = Pe(Se)?1ne (1)where PPe is the target language side perplex-ity, and Pe is the probability given by the targetside language model.
Se is the target languagesentence in the parallel corpus, and ne is thenumber of words in the sentence.We can also calculate the perplexity in thesource language (PPf ) in the same way.PPf = Pf (Sf )?
1nf (2)If we use perplexities in both languages, we cancalculate average perplexity (PPe+f ) by usingthe following formula.PPe+f = (PPe ?
PPf )12 (3)656Table 1: Size of parallel corporaEnglish Chinese English ChineseIn-domainparallel corpus40 K 40 K 320 K 301 K Basic Travel Expressions CorpusOut-of-domainparallel corpus2.5 M 2.5 M 62 M 54 MLDC corpus (LDC 2002T01, LDC2003T17, LDC2004T07,LDC2004T08, LDC2005T06 and LDC2005T10)# of sentences # of wordsExplanation4.
Select translation pairs from the out-of-domainparallel corpus.
If the perplexity is smaller thanthe threshold, use translation pairs as the train-ing set.
Otherwise, discard the translation pairs.5.
Train a translation model by using the selectedtranslation pairs.6.
Integrate the translation model obtained in 1and 6 by linear interpolation.3 ExperimentsWe carried out statistical machine translation experi-ments using the translation models obtained with theproposed method.3.1 Framework of SMTWe employed a log-linear model as a phrase-basedstatistical machine translation framework.
Thismodel expresses the probability of a target-languageword sequence (e) of a given source language wordsequence (f ) given byP (e|f) =exp(?Mi=1 ?ihi(e, f))?e?
exp(?Mi=1 ?ihi(e?, f)) (4)where hi(e, f) is the feature function, ?i is the fea-ture function?s weight, and M is the number of fea-tures.
We can approximate Eq.
4 by regarding itsdenominator as constant.
The translation results (e?
)are then obtained bye?
(f, ?M1 ) = argmaxeM?i=1?ihi(e, f) (5)3.2 Experimental conditions3.2.1 CorpusWe used data from the Chinese-to-English trans-lation track of the IWSLT 2006(IWSLT, 2006) forthe experiments.
The small in-domain parallel cor-pus was from the IWSLT workshop.
This corpuswas part of the ATR Bilingual Travel ExpressionCorpus (ATR-BTEC) (Kikui et al, 2006).
The largeout-of-domain parallel corpus was from the LDCcorpus (LDC, 2007).
Details on the data are listedin Table 1.
We used the test set of the IWSLT2006workshop for the evaluation.
This test set consistedof 500 Chinese sentences with eight English refer-ence translations per Chinese sentence.For the statistical machine-translation experi-ments, we first aligned the bilingual sentencesfor preprocessing using the Champollion tool (Ma,2006).
We then segmented the Chinese words us-ing Achilles (Zhang et al, 2006).
After the seg-mentation, we removed all punctuation from bothEnglish and Chinese corpuses and decapitalized theEnglish corpus.
We used the preprocessed data totrain the phrase-based translation model by usingGIZA++ (Och and Ney, 2003) and the Pharaoh toolkit (Koehn et al, 2003).3.2.2 FeaturesWe used eight features (Och and Ney, 2003;Koehn et al, 2003) and their weights for the transla-tions.1.
Phrase translation probability from source lan-guage to target language (weight = 0.2)2.
Phrase translation probability from target lan-guage to source language (weight = 0.2)3.
Lexical weighting probability from source lan-guage to target language (weight = 0.2)4.
Lexical weighting probability from source tar-get to language weight = 0.2)5.
Phrase penalty (weight = 0.2)6576.
Word penalty (weight = ?1.0)7.
Distortion weight (weight = 0.5)8.
Target language model probability (weight =0.5)According to a previous study, the minimum er-ror rate training (MERT) (Och, 2003), which is theoptimization of feature weights by maximizing theBLEU score on the development set, can improvethe performance of a system.
However, the rangeof improvement is not stable because the MERT al-gorithm uses random numbers while searching forthe optimum weights.
As previously mentioned, weused fixed weights instead of weights optimized byMERT to remove its unstable effects and simplifythe evaluation.3.2.3 Linear interpolation of translationmodelsThe experiments used four features (Feature # 1to 4 in 3.2.2) as targets for integration.
For each fea-ture, we applied linear interpolation by using the fol-lowing formula.h(e, f) = ?outhout(e, f)+(1?
?out)hin(e, f) (6)Here, hin(e, f) and hout(e, f) are features trainedon the in-domain parallel corpus and out-of-domaincorpus, respectively.
?out is the weight for the fea-ture trained on the out-of-domain parallel corpus.3.2.4 Language modelWe used a Good-Turing (Good, 1953) 3-gram lan-guage model for data selection.For the actual translation, we used a modi-fied Kneser-Ney (Chen and Goodman, 1998) 3-gram language model because modified Kneser-Neysmoothing tended to perform better than the Good-Turing language model in this translation task.
Fortraining of the language model, only the English sideof the in-domain corpus was used.
We used thesame language model for the entire translation ex-periment.3.3 Experimental results3.3.1 Translation performanceFigure 2 and 3 plot the results of the experiments.The horizontal axis represents the weight for the out-of-domain translation model, and the vertical axis15%16%17%18%19%20%21%22%23%24%25%0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Weight for out-of-domain translation modelBLEUscore400 K800 K1.2 M1.6 M2.5 MFigure 2: Results of data selection and linear inter-polation (BLEU)represents the automatic metric of translation qual-ity (BLEU score (Papineni et al, 2002) in Fig.
2,and NIST score (NIST, 2002) in Fig.
3).
Thickstraight broken lines in the figures indicate auto-matic scores of a baseline system.
This base line sys-tem was trained on the in-domain and all of the out-of-domain corpus (2.5M sentence pairs).
These datawere concatenated before training; then one modelwas trained without linear interpolation.
The fivesymbols in the figures represent the sizes (# of sen-tence pairs) of the selected parallel corpus.
Here,the selection was carried out by using Eq.
1.
Forautomatic evaluation, we used the reference transla-tion with a case unsensitive and no-punctuation set-ting.
Hence, higher automatic scores indicate bettertranslations; the selected corpus size of 1.2M (?
)indicates the best translation quality in Fig.
2 at thepoint where the weight for the out-of-domain trans-lation model is 0.7.In contrast to Fig.
2, Fig.
3 shows no improve-ments to the NIST score by using the baseline out-of-domain usage.
The optimal weights for each cor-pus size are different from those in Fig.
2.
How-ever, there is no difference in optimal corpus size;i.e., the selected corpus size of 1.2M gives the bestNIST score.6585.45.55.65.75.85.966.16.26.36.46.56.66.76.86.90.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Weight for out-of-domain translation modelNISTscore400 K800 K1.2 M1.6 M2.5 MFigure 3: Results of data selection and linear inter-polation (BLEU)Table 2: Size of integrated phrase tablesIn-domain Out-of-domain40 K 0 14 M40 K 1.2 M 917 M40 K 2.5 M 1.8 GSize of phrase table(Bytes)Corpus size(Sentence pairs)3.3.2 Size of the translation modelsTable 2 lists the sizes of the translation modelsof the baseline and optimum-size training corpus.The size of the phrase table is the uncompressed filesize of the phrase table trained by the Pharaoh toolkit.
As the table indicates, our method reduced themodel sizes by 50%.This reduction had a positive effect on the com-putational load of decoding.3.3.3 Equations for the selectionThe experiments described above used only targetlanguage side information, i.e., Eq.
1, for the dataselection.
Here, we compare selection performancesof Eqs.
1, 2, and 3.
Table 3 shows the results.The first row shows the results of using only the in-domain parallel corpus.
The second row shows re-sults of the baseline.
The third row shows the resultsof using linear interpolation without data selection.Comparing the results for the three equations, wesee that Eq.
1 gives the best performance.
It out-performs not only the baseline but also the resultsobtained by using all of the (2.5M) out-of-domaindata and linear interpolation.The results of using source language side infor-mation (Eq.
2) and information from both languagesides (Eq.
3) still showed better performance thanthe baseline system did.4 Comparison with conventional methodThere are few studies on data selection for trans-lation model training.
Most successful and recentstudy was that of (Lu et al, 2007).
They appliedthe TF*IDF framework to translation model train-ing corpus selection.
According to their study, theyobtained a 28% translation model size reduction (A2.41G byte model was reduced to a 1.74G bytemodel) and 1% BLEU score improvement (BLEUscore increased from 23.63% to 24.63%).
Althoughthere results are not directly comparable to ours [??
]because of the differences in the experimental set-ting, our method outperforms theirs for both aspectsof model size reduction and translation performanceimprovement (50% model size reduction and 1.76%BLEU score improvement).5 ConclusionsWe proposed a method of selecting training sets fortraining translation models that dramatically reducesthe sizes of the training set and translation models.We carried out experiments using data from theChinese-to-English translation track of the IWSLTevaluation campaign.
The experimental results indi-cated that our method reduced the size of the trainingset by 48%.
The obtained translation models werehalf the size of the baseline.The proposed method also had good translationperformance.
Our experimental results demon-strated that an SMT system with a half-size transla-tion model obtained with our method improved theBLEU score by 1.76%.
(Linear interpolation im-proved BLEU score by 1.61% and data selection im-proved BLEU score by an additional 0.15%.
)659Table 3: Results of data selection by using Eqs.
1, 2, and 3In-domain Out-of-domain40 K 0 N/A N/A 21.68%40 K 2.5 M N/A N/A 23.16%40 K 2.5 M N/A 0.7 24.77%40 K 1.2 M Eq.
1 0.7 24.92%40 K 1.2 M Eq.
2 0.8 24.76%40 K 1.2 M Eq.
3 0.6 24.56%Optimal weight forout-of-domain modelBLEU scoreCorpus size (Sentence pairs)Selection methodWe also compared the selections using source lan-guage side information, target language side infor-mation and information from both language sides.The experimental results show that target languageside information gives the best performance in theexperimental setting.
However, there are no largedifferences among the different selection results.The results are encouraging because they show thatthe in-domain mono-lingual corpus is sufficient toselect training data from the out-of-domain parallelcorpus.ReferencesS.
F. Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
In Tech-nical report TR-10-98, Center for Research in Com-puting Technology (Harvard University).I.
J Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3):237?264.IWSLT.
2006.
IWSLT: International Work-shop on Spoken Language Translation.http://www.slc.atr.jp/IWSLT2006/.G.
Kikui, S. Yamamoto, T. Takezawa, and E. Sumita.2006.
Comparative study on corpora for speech trans-lation.
In IEEE Transactions on Audio, Speech andLanguage Processing, volume 14(5), pages 1674?1682.P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
Proc.
of Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL), pages 127?133.LDC.
2007.
Linguistic Data Consortium.http://www.ldc.upenn.edu/.Yajuan Lu, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by trainingdata selection and optimization.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 343?350.X Ma.
2006.
Champollion: A Robust Parallel Text Sen-tence Aligner.
In Proc.
of international conference onLanguage Resources and Evaluation (LREC), pages489?492.NIST.
2002.
Automatic Evaluation of Machine Trans-lation Quality Using N-gram Co-Occurence Statistics.http://www.nist.gov/speech/tests/mt/mt2001/resource/.NIST.
2006.
The 2006 NIST MachineTranslation Evaluation Plan (MT06).http://www.nist.gov/speech/tests/mt/doc/mt06 evalplan.v3.pdf.F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum Error Rate Training for Sta-tistical Machine Translation.
Proc.
of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318.R.
Zhang, G. Kikui, and E. Sumita.
2006.
Subword-based Tagging by Conditional Random Fields for Chi-nese Word Segmentation.
Proc.
of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL), Short Paper:193?196.660
