Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393?1398,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsBilingual Word Embeddings for Phrase-Based Machine TranslationWill Y.
Zou?, Richard Socher, Daniel Cer, Christopher D. ManningDepartment of Electrical Engineering?
and Computer Science DepartmentStanford University, Stanford, CA 94305, USA{wzou,danielcer,manning}@stanford.edu, richard@socher.orgAbstractWe introduce bilingual word embeddings: se-mantic embeddings associated across two lan-guages in the context of neural language mod-els.
We propose a method to learn bilingualembeddings from a large unlabeled corpus,while utilizing MT word alignments to con-strain translational equivalence.
The new em-beddings significantly out-perform baselinesin word semantic similarity.
A single semanticsimilarity feature induced with bilingual em-beddings adds near half a BLEU point to theresults of NIST08 Chinese-English machinetranslation task.1 IntroductionIt is difficult to recognize and quantify semantic sim-ilarities across languages.
The Fr-En phrase-pair{?un cas de force majeure?, ?case of absolute neces-sity?
}, Zh-En phrase pair {?????
?,?persist in astubborn manner?}
are similar in semantics.
If co-occurrences of exact word combinations are rare inthe training parallel text, it can be difficult for classi-cal statistical MT methods to identify this similarity,or produce a reasonable translation given the sourcephrase.We introduce an unsupervised neural modelto learn bilingual semantic embedding for wordsacross two languages.
As an extension to theirmonolingual counter-part (Turian et al 2010;Huang et al 2012; Bengio et al 2003), bilin-gual embeddings capture not only semantic infor-mation of monolingual words, but also semantic re-lationships across different languages.
This prop-erty allows them to define semantic similarity met-rics across phrase-pairs, making them perfect fea-tures for machine translation.To learn bilingual embeddings, we use a new ob-jective function which embodies both monolingualsemantics and bilingual translation equivalence.
Thelatter utilizes word alignments, a natural sub-taskin the machine translation pipeline.
Through large-scale curriculum training (Bengio et al 2009), weobtain bilingual distributed representations whichlie in the same feature space.
Embeddings of di-rect translations overlap, and semantic relationshipsacross bilingual embeddings were further improvedthrough unsupervised learning on a large unlabeledcorpus.Consequently, we produce for the research com-munity a first set of Mandarin Chinese word embed-dings with 100,000 words trained on the ChineseGigaword corpus.
We evaluate these embeddingon Chinese word semantic similarity from SemEval-2012 (Jin and Wu, 2012).
The embeddings sig-nificantly out-perform prior work and pruned tf-idfbase-lines.
In addition, the learned embeddingsgive rise to 0.11 F1 improvement in Named EntityRecognition on the OntoNotes dataset (Hovy et al2006) with a neural network model.We apply the bilingual embeddings in an end-to-end phrase-based MT system by computing seman-tic similarities between phrase pairs.
On NIST08Chinese-English translation task, we obtain an im-provement of 0.48 BLEU from a competitive base-line (30.01 BLEU to 30.49 BLEU) with the StanfordPhrasal MT system.13932 Review of prior workDistributed word representations are useful in NLPapplications such as information retrieval (Pas?ca etal., 2006; Manning et al 2008), search query ex-pansions (Jones et al 2006), or representing se-mantics of words (Reisinger et al 2010).
A num-ber of methods have been explored to train and ap-ply word embeddings using continuous models forlanguage.
Collobert et al(2008) learn embed-dings in an unsupervised manner through a con-trastive estimation technique.
Mnih and Hinton (2008), Morin and Bengio ( 2005) proposed efficienthierarchical continuous-space models.
To system-atically compare embeddings, Turian et al(2010)evaluated improvements they bring to state-of-the-art NLP benchmarks.
Huang et al(2012) intro-duced global document context and multiple wordprototypes.
Recently, morphology is explored tolearn better word representations through RecursiveNeural Networks (Luong et al 2013).Bilingual word representations have been ex-plored with hand-designed vector space mod-els (Peirsman and Pado?
, 2010; Sumita, 2000),and with unsupervised algorithms such as LDA andLSA (Boyd-Graber and Resnik, 2010; Tam et al2007; Zhao and Xing, 2006).
Only recently havecontinuous space models been applied to machinetranslation (Le et al 2012).
Despite growing in-terest in these models, little work has been donealong the same lines to train bilingual distributionedword represenations to improve machine translation.In this paper, we learn bilingual word embeddingswhich achieve competitive performance on seman-tic word similarity, and apply them in a practicalphrase-based MT system.3 Algorithm and methods3.1 Unsupervised training with global contextOur method starts with embedding learning formu-lations in Collobert et al(2008).
Given a contextwindow c in a document d, the optimization mini-mizes the following Context Objective for a word win the vocabulary:J (c,d)CO =?wr?VRmax(0, 1?
f(cw, d) + f(cwr , d))(1)Here f is a function defined by a neural network.wr is a word chosen in a random subset VR of thevocabulary, and cwr is the context window contain-ing word wr.
This unsupervised objective func-tion contrasts the score between when the correctword is placed in context with when a random wordis placed in the same context.
We incorporate theglobal context information as in Huang et al(2012),shown to improve performance of word embed-dings.3.2 Bilingual initialization and trainingIn the joint semantic space of words across two lan-guages, the Chinese word ????
is expected to beclose to its English translation ?government?.
At thesame time, when two words are not direct transla-tions, e.g.
?lake?
and the Chinese word ???
(deeppond), their semantic proximity could be correctlyquantified.We describe in the next sub-sections the methodsto intialize and train bilingual embeddings.
Thesemethods ensure that bilingual embeddings retaintheir translational equivalence while their distribu-tional semantics are improved during online trainingwith a monolingual corpus.3.2.1 Initialization by MT alignmentsFirst, we use MT Alignment counts as weightingto initialize Chinese word embeddings.
In our ex-periments, we use MT word alignments extractedwith the Berkeley Aligner (Liang et al 2006) 1.Specifically, we use the following equation to com-pute starting word embeddings:Wt-init =S?s=1Cts + 1Ct + SWs (2)In this equation, S is the number of possible tar-get language words that are aligned with the sourceword.
Cts denotes the number of times when word tin the target and word s in the source are aligned inthe training parallel text; Ct denotes the total num-ber of counts of word t that appeared in the targetlanguage.
Finally, Laplace smoothing is applied tothis weighting function.1On NIST08 Zh-En training data and data from GALE MTevaluation in the past 5 years1394Single-prototype English embeddings by Huanget al(2012) are used to initialize Chinese em-beddings.
The initialization readily provides a set(Align-Init) of benchmark embeddings in experi-ments (Section 4), and ensures translation equiva-lence in the embeddings at start of training.3.2.2 Bilingual trainingUsing the alignment counts, we form alignmentmatrices Aen?zh and Azh?en.
For Aen?zh, eachrow corresponds to a Chinese word, and each col-umn an English word.
An element aij is first as-signed the counts of when the ith Chinese word isaligned with the jth English word in parallel text.After assignments, each row is normalized such thatit sums to one.
The matrix Azh?en is defined sim-ilarly.
Denote the set of Chinese word embeddingsas Vzh, with each row a word embedding, and theset of English word embeddings as Ven.
With thetwo alignment matrices, we define the TranslationEquivalence Objective:JTEO-en?zh = ?Vzh ?Aen?zhVen?2 (3)JTEO-zh?en = ?Ven ?Azh?enVzh?2 (4)We optimize for a combined objective during train-ing.
For the Chinese embeddings we optimize for:JCO-zh + ?JTEO-en?zh (5)For the English embeddings we optimize for:JCO-en + ?JTEO-zh?en (6)During bilingual training, we chose the value of ?such that convergence is achieved for both JCO andJTEO.
A small validation set of word similaritiesfrom (Jin and Wu, 2012) is used to ensure the em-beddings have reasonable semantics.
2In the next sections, ?bilingual trained?
embed-dings refer to those initialized with MT alignmentsand trained with the objective defined by Equa-tion 5.
?Monolingual trained?
embeddings refer tothose intialized by alignment but trained withoutJTEO-en?zh.2In our experiments, ?
= 50.3.3 Curriculum trainingWe train 100k-vocabulary word embeddings usingcurriculum training (Turian et al 2010) with Equa-tion 5.
For each curriculum, we sort the vocabu-lary by frequency and segment the vocabulary by aband-size taken from {5k, 10k, 25k, 50k}.
Separatebands of the vocabulary are trained in parallel usingminibatch L-BFGS on the Chinese Gigaword cor-pus 3.
We train 100,000 iterations for each curricu-lum, and the entire 100k vocabulary is trained for500,000 iterations.
The process takes approximately19 days on a eight-core machine.
We show visual-ization of learned embeddings overlaid with Englishin Figure 1.
The two-dimensional vectors for this vi-sualization is obtained with t-SNE (van der Maatenand Hinton, 2008).
To make the figure comprehen-sible, subsets of Chinese words are provided withreference translations in boxes with green borders.Words across the two languages are positioned bythe semantic relationships implied by their embed-dings.Figure 1: Overlaid bilingual embeddings: English wordsare plotted in yellow boxes, and Chinese words in green;reference translations to English are provided in boxeswith green borders directly below the original word.4 Experiments4.1 Semantic SimilarityWe evaluate the Mandarin Chinese embeddings withthe semantic similarity test-set provided by the or-3Fifth Edition.
LDC catelog number LDC2011T13.
We onlyexclude cna cmn, the Traditional Chinese segment of the cor-pus.1395Table 1: Results on Chinese Semantic SimilarityMethod Sp.
Corr.
K. Tau(?100) (?100)Prior work (Jin and Wu, 2012) 5.0Tf-idfNaive tf-idf 41.5 28.7Pruned tf-idf 46.7 32.3Word EmbeddingsAlign-Init 52.9 37.6Mono-trained 59.3 42.1Biling-trained 60.8 43.3ganizers of SemEval-2012 Task 4.
This test-set con-tains 297 Chinese word pairs with similarity scoresestimated by humans.The results for semantic similarity are shown inTable 1.
We show two evaluation metrics: Spear-man Correlation and Kendall?s Tau.
For both, bilin-gual embeddings trained with the combined objec-tive defined by Equation 5 perform best.
For prunedtf-idf, we follow Reisinger et al(2010; Huang etal.
(2012) and count word co-occurrences in a 10-word window.
We use the best results from arange of pruning and feature thresholds to compareagainst our method.
The bilingual and monolingualtrained embeddings4 out-perform pruned tf-idf by14.1 and 12.6 Spearman Correlation (?100), respec-tively.
Further, they out-perform embeddings initial-ized from alignment by 7.9 and 6.4.
Both our tf-idfimplementation and the word embeddings have sig-nificantly higher Kendall?s Tau value compared toPrior work (Jin and Wu, 2012).
We verified Tau cal-culations with original submissions provided by theauthors.4.2 Named Entity RecognitionWe perform NER experiments on OntoNotes (v4.0)(Hovy et al 2006) to validate the quality of theChinese word embeddings.
Our experimental set-up is the same as Wang et al(2013).
With em-beddings, we build a naive feed-forward neural net-work (Collobert et al 2008) with 2000 hidden neu-rons and a sliding window of five words.
This naivesetting, without sequence modeling or sophisticated4Due to variations caused by online minibatch L-BFGS, wetake embeddings from five random points out of last 105 mini-batch iterations, and average their semantic similarity results.Table 2: Results on Named Entity RecognitionEmbeddings Prec.
Rec.
F1 ImproveAlign-Init 0.34 0.52 0.41Mono-trained 0.54 0.62 0.58 0.17Biling-trained 0.48 0.55 0.52 0.11Table 3: Vector Matching Alignment AER (lower is bet-ter)Embeddings Prec.
Rec.
AERMono-trained 0.27 0.32 0.71Biling-trained 0.37 0.45 0.59join optimization, is not competitive with state-of-the-art (Wang et al 2013).
Table 2 shows that thebilingual embeddings obtains 0.11 F1 improvement,lagging monolingual, but significantly better thanAlign-Init (as in Section3.2.1) on the NER task.4.3 Vector matching alignmentTranslation equivalence of the bilingual embeddingsis evaluated by naive word alignment to match wordembeddings by cosine distance.5 The Alignment Er-ror Rates (AER) reported in Table 3 suggest thatbilingual training using Equation 5 produces embed-dings with better translation equivalence comparedto those produced by monolingual training.4.4 Phrase-based machine translationOur experiments are performed using the Stan-ford Phrasal phrase-based machine translation sys-tem (Cer et al 2010).
In addition to NIST08 train-ing data, we perform phrase extraction, filteringand phrase table learning with additional data fromGALE MT evaluations in the past 5 years.
In turn,our baseline is established at 30.01 BLEU and rea-sonably competitive relative to NIST08 results.
Weuse Minimum Error Rate Training (MERT) (Och,2003) to tune the decoder.In the phrase-based MT system, we add one fea-ture to bilingual phrase-pairs.
For each phrase, theword embeddings are averaged to obtain a featurevector.
If a word is not found in the vocabulary, wedisregard and assume it is not in the phrase; if noword is found in a phrase, a zero vector is assigned5This is evaluated on 10,000 randomly selected sentencepairs from the MT training set.1396Table 4: NIST08 Chinese-English translation BLEUMethod BLEUOur baseline 30.01EmbeddingsRandom-Init Mono-trained 30.09Align-Init 30.31Mono-trained 30.40Biling-trained 30.49to it.
We then compute the cosine distance betweenthe feature vectors of a phrase pair to form a seman-tic similarity feature for the decoder.Results on NIST08 Chinese-English translationtask are reported in Table 46.
An increase of0.48 BLEU is obtained with semantic similaritywith bilingual embeddings.
The increase is modest,just surpassing a reference standard deviation 0.29BLEU Cer et al(2010)7 evaluated on a similar sys-tem.
We intend to publish further analysis on statis-tical significance of this result as an appendix.
Fromthese suggestive evidence in the MT results, randominitialized monolingual trained embeddings add lit-tle gains to the baseline.
Bilingual initialization andtraining seem to be offering relatively more consis-tent gains by introducing translational equivalence.5 ConclusionIn this paper, we introduce bilingual word embed-dings through initialization and optimization con-straint using MT alignments The embeddings arelearned through curriculum training on the ChineseGigaword corpus.
We show good performance onChinese semantic similarity with bilingual trainedembeddings.
When used to compute semantic simi-larity of phrase pairs, bilingual embeddings improveNIST08 end-to-end machine translation results byjust below half a BLEU point.
This implies that se-mantic embeddings are useful features for improv-ing MT systems.
Further, our results offer sugges-tive evidence that bilingual word embeddings act ashigh-quality semantic features and embody bilingualtranslation equivalence across languages.6We report case-insensitive BLEU7With 4-gram BLEU metric from Table 4AcknowledgmentsWe gratefully acknowledge the support of theDefense Advanced Research Projects Agency(DARPA) Broad Operational Language Transla-tion (BOLT) program through IBM.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the author(s) anddo not necessarily reflect the view of the DARPA,or the US government.
We thank John Bauer andThang Luong for helpful discussions.ReferencesA.
Klementiev, I. Titov and B. Bhattarai.
2012.
Induc-ing Crosslingual Distributed Representation of Words.COLING.Y.
Bengio, J. Louradour, R. Collobert and J. Weston.2009.
Curriculum Learning.
ICML.Y.
Bengio, R. Ducharme, P. Vincent and C. Jauvin.
2003.A Neural Probabilistic Language Model.
Journal ofMachine Learning Research.Y.
Bengio and Y. LeCunn.
2007.
Scaling learning algo-rithms towards AI.
Large-Scale Kernal Machines.J.
Boyd-Graber and P. Resnik.
2010.
Holistic sentimentanalysis across languages: multilingual supervised la-tent dirichlet alcation.
EMNLP.D.
Cer, M. Galley, D. Jurafsky and C. Manning.
2010.Phrasal: A Toolkit for Statistical Machine Translationwith Facilities for Extraction and Incorporation of Ar-bitrary Model Features.
In Proceedings of the NorthAmerican Association of Computational Linguistics -Demo Session (NAACL-10).D.
Cer, C. Manning and D. Jurafsky.
2010.
The BestLexical Metric for Phrase-Based Statistical MT Sys-tem Optimization.
NAACL.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
ICML.G.
Foster and R. Kuhn.
2009.
Stabilizing minimum errorrate training.
Proceedings of the Fourth Workshop onStatistical Machine Translation.M.
Galley, P. Chang, D. Cer, J. R. Finkel and C. D. Man-ning.
2008.
NIST Open Machine Translation 2008Evaluation: Stanford University?s System Description.Unpublished working notes of the 2008 NIST OpenMachine Translation Evaluation Workshop.S.
Green, S. Wang, D. Cer and C. Manning.
2013.
Fastand adaptive online training of feature-rich translationmodels.
ACL.G.
Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N.Jaitly, A.
Senior, V. Vanhoucke, P. Nguyen, T. Sainath1397and B. Kingsbury.
2012.
Deep Neural Networks forAcoustic Modeling in Speech Recognition.
IEEE Sig-nal Processing Magazine.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw and R.Weischedel.
2006.
OntoNotes: the 90% solution.NAACL-HLT.E.
H. Huang, R. Socher, C. D. Manning and A. Y. Ng.2012.
Improving Word Representations via GlobalContext and Multiple Word Prototypes.
ACL.P.
Jin and Y. Wu.
2012.
SemEval-2012 Task 4: Eval-uating Chinese Word Similarity.
Proceedings of theFirst Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main confer-ence and the shared task, and Volume 2: Proceedingsof the Sixth International Workshop on Semantic Eval-uation.
Association for Computational Linguistics.R.
Jones.
2006.
Generating query substitutions.
In Pro-ceedings of the 15th international conference on WorldWide Web.P.
Koehn, F. J. Och and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
HLT.H.
Le, A. Allauzen and F. Yvon 2012.
Continuous spacetranslation models with neural networks.
NAACL.P.
Liang, B. Taskar and D. Klein.
2006.
Alignment byagreement.
NAACL.M.
Luong, R. Socher and C. Manning.
2013.
Betterword representations with recursive neural networksfor morphology.
CONLL.L.
van der Maaten and G. Hinton.
2008.
Visualizing datausing t-SNE.
Journal of Machine Learning Research.A.
Maas and R. E. Daly and P. T. Pham and D. Huang andA.
Y. Ng and C. Potts.
2011.
Learning word vectorsfor sentiment analysis.
ACL.C.
Manning and P. Raghavan and H. Schtze.
2008.
Intro-duction to Information Retrieval.
Cambridge Univer-sity Press, New York, NY, USA.T.
Mikolov, M. Karafiat, L. Burget, J. Cernocky and S.Khudanpur.
2010.
Recurrent neural network basedlanguage model.
INTERSPEECH.T.
Mikolov, K. Chen, G. Corrado and J.
Dean.
2013.
Ef-ficient Estimation of Word Representations in VectorSpace.
arXiv:1301.3781v1.A.
Mnih and G. Hinton.
2008.
A scalable hierarchicaldistributed language model.
NIPS.F.
Morin and Y. Bengio.
2005.
Hierarchical probabilisticneural network language model.
AISTATS.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
ACL.M.
Pas?ca, D. Lin, J. Bigham, A. Lifchits and A. Jain.2006.
Names and similarities on the web: fact extrac-tion in the fast lane.
ACL.Y.
Peirsman and S. Pado?.
2010.
Cross-lingual inductionof selectional preferences with bilingual vector spaces.ACL.J.
Reisinger and R. J. Mooney.
2010.
Multi-prototypevector-space models of word meaning.
NAACL.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Comput.
Surv., 34:1-47, March.R.
Socher, J. Pennington, E. Huang, A. Y. Ng andC.
D. Manning.
2011.
Semi-Supervised RecursiveAutoencoders for Predicting Sentiment Distributions.EMNLP.R.
Socher, E. H. Huang, J. Pennington, A. Y. Ng, andC.
D. Manning.
2011.
Dynamic Pooling and Unfold-ing Recursive Autoencoders for Paraphrase Detection.NIPS.E.
Sumita.
2000.
Lexical transfer using a vector-spacemodel.
ACL.Y.
Tam, I.
Lane and T. Schultz.
2007.
Bilingual-LSAbased LM adaptation for spoken language translation.ACL.S.
Tellex and B. Katz and J. Lin and A. Fernandes andG.
Marton.
2003.
Quantitative evaluation of passageretrieval algorithms for question answering.
In Pro-ceedings of the 26th Annual International ACM SIGIRConference on Search and Development in Informa-tion Retrieval, pages 41-47.
ACM Press.J.
Turian and L. Ratinov and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
ACL.M.
Wang, W. Che and C. D. Manning.
2013.
Joint WordAlignment and Bilingual Named Entity RecognitionUsing Dual Decomposition.
ACL.K.
Yamada and K. Knight.
2001.
A Syntax-based Statis-tical Translation Model.
ACL.B.
Zhao and E. P. Xing 2006.
BiTAM: Bilingual topicAdMixture Models for word alignment.
ACL.1398
