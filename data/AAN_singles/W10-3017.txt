Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 120?125,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsHedgeHunter: A System for Hedge Detection and UncertaintyClassificationDavid ClausenDepartment of LinguisticsStanford UniversityStanford, CA 94305, USA.clausend@stanford.eduAbstractWith the dramatic growth of scientificpublishing, Information Extraction (IE)systems are becoming an increasingly im-portant tool for large scale data analy-sis.
Hedge detection and uncertainty clas-sification are important components of ahigh precision IE system.
This paperdescribes a two part supervised systemwhich classifies words as hedge or non-hedged and sentences as certain or uncer-tain in biomedical and Wikipedia data.
Inthe first stage, our system trains a logisticregression classifier to detect hedges basedon lexical and Part-of-Speech collocationfeatures.
In the second stage, we use theoutput of the hedge classifier to generatesentence level features based on the num-ber of hedge cues, the identity of hedgecues, and a Bag-of-Words feature vectorto train a logistic regression classifier forsentence level uncertainty.
With the result-ing classification, an IE system can thendiscard facts and relations extracted fromthese sentences or treat them as appropri-ately doubtful.
We present results for indomain training and testing and cross do-main training and testing based on a sim-ple union of training sets.1 IntroductionWith the rapid increase in domain specific (bio-medical) and domain general (WWW) text collec-tions information extraction is an increasingly im-portant tool for making use of these data sets.
Inorder to maximize the usefulness of extracted rela-tions an Information Extraction (IE) system needsthe ability to separate the factual and reliable re-lationships from the uncertain and unreliable rela-tionships.
Most work on this problem has focusedon the task of hedge detection where the goal isto classify a span of text as hedged or as non-hedged with the goal of facilitating sentence levelclassification of certain or uncertain.
Much of thework was conducted within the framework of theBioNLP 2009 shared task sub task on uncertaintydetection focusing on biomedical datasets (Kim etal., 2009) motivating further work in the biomedi-cal NLP field (Aramaki et al, 2009; Conway et al,2009).
Other work has focused on creating anno-tated datasets from both a linguistically sophisti-cated perspective (Saur??
and Pustejovsky, 2009) orfrom a language engineering perspective (Vinczeet al, 2008).Early work by Light et al (2004) framed thetask as determining the degree of speculation oruncertainty at the sentence level.
The presenceof a hedge cue, a phrase indicating that authorscannot back up their opinions or statements withfacts, is a high precision feature of sentence leveluncertainty.
Other early work focused on semi-supervised learning due to a lack of annotateddatasets (Medlock and Briscoe, 2007).
Linguis-tically motivated approaches achieved a robustbaseline on the sentence classification task (Kil-icoglu and Bergler, 2008) although their trainingmethods are hand tuned.
Morante and Daele-mans (2009) cast the problem as a sequence label-ing task and show that performance is highly do-main dependent and requires high precision hedgedetection in order to perform the complex taskof hedge scope labeling.
Szarvas (2008) demon-strates that semi-supervised learning is even moreeffective with more labeled training data and so-phisticated feature selection.HedgeHunter is built to perform the CoNLL-2010 sentence uncertainty classification task.
Thetask is a supervised learning task with trainingdata drawn from Wikipedia and biomolecular ar-ticles and abstracts.
Each training sentence is la-120beled as certain or uncertain and every hedge cueis also labeled.
HedgeHunter separates the taskinto two stages: hedge detection and uncertaintyclassification, with the goal of producing an in-dependent high precision hedge detection systemfor use in other tasks such as hedge scope detec-tion.
The system is designed to be expanded usingsemi-supervised learning although this is not im-plemented at this time.
This paper will describethe hedge detection stage in Section 2 and the sen-tence classification stage in Section 3.
Section 4describes the evaluation of the system and Section5 discusses the results.
Section 6 discusses the re-sults in a larger context and suggest future areasfor improvement.
Section 7 summarizes the con-clusions.2 Hedge DetectionHedge detection is largely based on the identi-fication of lexical items like suggest and mightwhich indicate sentence level uncertainty.
As aresult, reasonable hedge detection in English canbe accomplished by collecting a list of all lexicalitems that convey hedging.
These include epis-temic verbs (may, might, could, should, can, oughtto), psychological verbs of perception, knowing orconcluding (seems, guess, suppose, hope, assume,speculate, estimate), adverbs (possibly, unlikely,probably, approximately), adjectives (quite, rare,apparent) and many nouns.
While some of these,especially the epistemic verbs, are often appliedacross domains to indicate hedge cues, many areunique to a particular domain.
Further complicat-ing hedge detection in English is the fact that thesame word types occasionally have different, non-hedging uses.The form of a hedge cue often acts as a high pre-cision feature, whenever one is present in a sen-tence it is highly likely to be labeled as a hedgecue in the training set.
Lexical hedge cues oftenvary from domain to domain and contain multi-ple words so non-lexical features are required forrecognizing hedge cues robustly across domainsalthough they are unlikely to provide a large bene-fit due to the largely lexical nature of hedges.
As aresult HedgeHunter uses both lexical and POS fea-tures for classification.
Some hedges like ought tospan multiple words so we also use positional fea-tures in order to capture multi-word hedges.The hedge detection stage labels each word in asentence independently.
Labeling is done by lo-gistic regression using Quasi-Newton minimiza-tion to set feature weights.
This is a classifica-tion method that is both fast and robust for binaryclassification tasks like the one at hand.
Featuresare drawn from the target word to be labeled andits context, the three words to the left and right ofthe target word.
For the target word we extractfeatures based on the word form, the word lemmaand its POS as determined by a maximum entropyPOS tagger trained on the PennTreebank imple-mented in Stanford JavaNLP.
For the 6 words inthe context window we also extract features basedon the word, its lemma and its POS.3 Uncertainty ClassificationUncertainty classification involves partitioning theset of sentences in a dataset into certain and uncer-tain classes.
In most scientific writing sentencesare generally certain so uncertain sentences are theminority class.
This holds even more so for theWikipedia dataset due to the method by which an-notations were obtained and the encyclopedic na-ture of the dataset.
Wikipedia hedge cues wereidentified by the presence of the weasel word tagwhich editors are allowed to append to spans oftext in a Wikipedia article.
These are often appliedin a manner similar to hedge cues in the annotatedbiomedical datasets but they also focus on identi-fying non universal statements like those quanti-fied by some or few.
Due to the collaborative na-ture of Wikipedia, what qualifies as a weasel wordvaries greatly contributing to the increased varia-tion in hedge cues in this dataset.
Weasel wordsoften get edited quickly so there are not many ex-amples in the training set creating further difficul-ties.The presence of one or more hedge cues in asentence is a good indication that the sentenceshould be classified as uncertain, although as wewill see in the results section, non-hedge featuresare also useful for this task.
To capture this weextract features from each sentence including thenumber of hedge cues found by the hedge detec-tion stage and the string value of the first four lex-ical hedge cues found in each sentence.
To cap-ture any other non-hedge words which may con-tribute to sentence level uncertainty, we also in-clude BOW features based on vocabulary itemswith frequencies above the mean frequency in thecorpus.
This is achieved by creating binary fea-tures for the presence of every word in the vocab-121ulary.Classification is again performed by a logis-tic regression using Quasi-Newton minimization.It should be stressed that all hedge related fea-tures used by the uncertainty classification stageare taken from the results of the hedge detectionstage and not from the gold standard annotationdata.
This was done to allow the system to foldnew unannotated sentences into the training setto perform semi-supervised learning.
Time con-straints and implementation difficulties preventedfully implementing this system component.
Fu-ture work plans to extract high class conditionallikelihood features from unannotated sentences,annotate the sentences based on treating these fea-tures as hedges, and retrain the hedge detectionstage and uncertainty classification stage in an it-erative manner to improve coverage.4 EvaluationThe dataset provided for the CoNLL-2010 sharedtask consists of documents drawn from three sepa-rate domains.
Two domains, biomedical abstractsand full articles, are relatively similar while thethird, selected Wikipedia articles, differs consider-ably in both content and hedge cues for the reasonspreviously discussed.
Overall the dataset contains11,871 sentences from abstracts, 2,670 from fullarticles, and 11,111 from Wikipedia articles.Performance for the hedge detection systemwas calculated at the word level while perfor-mance for the uncertainty classification stage wascalculated at the sentence level using the classes ofhedged and uncertain as the positive class for pre-cision, recall and F1 statistics.
We compare ourhedge detection system to a state of the art sys-tem presented in Morante and Daelemans (2009)and trained on a dataset of 20,924 sentences drawnfrom clinical reports and biomedical abstracts andarticles.
The Morante system used 10 fold crossvalidation while our system randomly withholds10 percent of the dataset for testing so our resultsmay be viewed as less reliable.
We do providethe first evaluation of one system on both domainspecific and domain general datasets.
Table 1 pro-vides a breakdown of performance by system anddataset.We evaluated the performance of the Hedge-Hunter system on the withheld training dataincluding 5003 evaluation sentences from thebiomedical domain and 9634 sentences fromSystem Precision Recall F1MoranteAbstracts .9081 .7984 .8477Articles .7535 .6818 .7159Clinical .8810 .2751 .41.92HedgeHunterAbstracts .8758 .5800 .6979Articles .8704 .4052 .5529Wikipedia .5453 .2434 .3369All .6289 .3464 .4467Table 1: Hedge detection performanceWikipedia.
For uncertainty classification we com-pare our system to the results from the CoNLL-2010 shared task comparing to the state of the artsystems.
For more details see the task descriptionpaper (Farkas et al, 2010).
Table 2 summarizesthe results for the closed domain training subtask.Table 3 summarizes the best performing systemsin the Wikipedia and biomedical domain on thecross domain training subtask and compares to theHedgeHunter system.System Precision Recall F1TangBiomedical .8503 .8777 .8636GeorgesculWikipedia .7204 .5166 .6017HedgeHunterBiomedical .7933 .8063 .7997Wikipedia .7512 .4203 .5390Table 2: Uncertainty classification performanceclosedSystem Precision Recall F1LiBiomedical .9040 .8101 .8545JiWikipedia .6266 .5528 .5874HedgeHunterBiomedical .7323 .6405 .6833Wikipedia .7173 .4168 .5272Table 3: Uncertainty classification performancecross1225 ResultsThe Hedge Detection stage performed slightlyworse than the state of the art system.
Althoughprecision was comparable for biomedical articlesand abstracts our system suffered from very lowrecall compared to the Morante system.
TheMorante system included chunk tagging as an ap-proximation of syntactic constituency.
Since manymulti word hedge cues are constituents of highprecision words and very frequent words (oughtto) this constituency information likely boosts re-call.
Like the Morante system, HedgeHunter suf-fered a significant performance drop when testedacross domains, although our system sufferedmore due to the greater difference in domains be-tween biomedical and Wikipedia articles than be-tween biomedical and clinical reports and due tothe annotation standards for each dataset.
Hedge-Hunter achieved better results on biomedical ab-stracts than the full articles due to higher recallbased on the significantly larger dataset.
Oursystem produced the worst performance on theWikipedia data although this was mostly due toa drop in precision compared to the biomedicaldomain.
This is in line with the drop in perfor-mance experienced by other systems outside of thebiomedical domain and indicates that Wikipediadata is noisier than the peer reviewed articles thatappear in the biomedical literature confirming ourinformal observations.
Since the dataset has anoverwhelming number of certain sentences andunhedged words, there is already a large bias to-wards those classes as evidenced by high over-all classification accuracy (87% for certainty de-tection and 97% for hedge detection on all data)despite sometimes poor F1 scores for the minor-ity classes.
During development we experimentedwith SVMs for training but abandoned them dueto longer training times and it is possible that wecould improve the recall of our system by using adifferent classifier, a weaker prior or different pa-rameters that allowed for more recall by payingless attention to class priors.
We plan to expandour system using semi-supervised learning so it isnot necessarily a bad thing to have high precisionand low recall as this will allow us to expand ourdataset with high quality sentences and by lever-aging the vast amounts of unannotated data weshould be able to overcome our low recall.The uncertainty classification system performedrobustly despite the relatively poor performance ofthe hedge detection classifier.
The use of BOWfeatures supplemented the low recall of the hedgedetection stage while still relying on the hedge fea-tures when they were available as shown by fea-ture analysis.
We did not implement bi or tri-gram features although this would likely give afurther boost in recall.
Wikipedia data was stillthe worst performing domain although our crossdomain system performed near the state of the artsystem with higher precision.Overall our system produced a high precisionhedge detection system for biomedical domaindata which fed a high precision uncertainty classi-fier.
Recall for the hedge detection stage was lowoverall but the use of BOW features for the uncer-tainty classification stage overcame this to a smalldegree.
The amount of annotated training data hasa significant impact on performance of the Hedge-Hunter system with more data increasing recall forthe hedge detection task.
For the sentence uncer-tainty task the system still performed acceptablyon the Wikipedia data.6 DiscussionHedgeHunter confirmed many of the findings ofprevious research.
The most significant finding isthat domain adaptation in the task of hedge detec-tion is difficult.
Most new domains contain differ-ent vocabulary and hedges tend to be highly lex-icalized and subject to variation across domains.This is reinforced by feature analysis where thetop weighted features for our hedge detection clas-sifier were based on the word or its lemma and noton its POS.
Once our system learns that a partic-ular lexical item is a hedge it is easy enough toapply it precisely, the difficulty is getting the nec-essary training examples covering all the possiblelexical hedge cues the system may encounter.
Thelexicon of hedge cues used in biomedical articlestends to be smaller so it is easier to get higher re-call in this domain because the chance of seeing aparticular hedge cue in training is increased.
Withthe Wikipedia data, however, the set of hedge cuesis more varied due to the informal nature of thearticles.
This makes it less likely that the hedgedetection system will be exposed to a particularhedge in training.One possible avenue for future work shouldconsider using lexical resources like WordNet,measures of lexical similarity, or n-gram languagemodels to provide backoff feature weights for un-123seen lexical items.
This would increase the recallof the system despite the limited nature of anno-tated training sets by leveraging the lexical natureof hedges and their relatively closed class status.We also found that the size of the training setmatters significantly.
Each domain employs a cer-tain number of domain specific hedge cues alongwith domain general cues.
While it is easy enoughto learn the domain general cues, domain specificcues are difficult and can only be learned by see-ing the specific lexical items to be learned.
It isimportant that the training dataset include enoughexamples of all the lexical hedge cues for a spe-cific domain if the system is to have decent re-call.
Even with thousands of sentences to train on,HedgeHunter had low recall presumably becausethere were still unseen lexical hedge cues in thetest set.
Future work should concentrate on meth-ods of expanding the size of the training sets in or-der to cover a larger portion of the domain specifichedging vocabulary because it does not appear thatthere are good non-lexical features that are robustat detecting hedges across domains.
This may in-clude using lexical resources as described previ-ously or by leveraging the high precision natureof hedge cues and the tendency for multiple cuesto appear in the same sentence to perform semi-supervised learning.This work also confirmed that hedge cues pro-vide a very high precision feature for uncertaintyclassification.
The highest weighed features forthe classifier trained in the uncertainty classifi-cation stage were those that indicated the pres-ence and number of lexical hedge cues.
Contraryto some previous work which found that featurescounting the number of hedge cues did not im-prove performance, HedgeHunter found that thenumber of hedge cues was a strong feature withmore hedge cues indicating an increased likeli-hood of being uncertain (Szarvas, 2008).
It islargely a limitation of the task that we treat all un-certain sentences as equally uncertain.
From a lin-guistic perspective a speaker uses multiple hedgecues to reinforce their uncertainty and our systemseems to confirm that in terms of the likelihoodof class membership even if the datasets do notencode the degree of uncertainty directly.
Futurework should focus on creating more sophisticatedmodels of uncertainty that recognize the fact thatit is at least a scalar phenomena and not a binaryclassification.
Ideally a hedge detection and uncer-tainty quantification system would function to at-tach a probability to every fact or relation extractedfrom a sentence in an IE system determined in partby the hedging vocabulary used to express thatfact or relation.
This would yield a more nuancedview of how language conveys certainty and allowfor interesting inference possibilities for systemsleveraging the resulting IE system output.One surprising finding was that uncertain sen-tences often contained multiple hedge cues, some-times up to 4 or more.
This is useful because itallows us to hypothesize that a sentence that isunannotated and has a high chance of being un-certain due to containing a hedge cue that we haveseen in training, possibly contains other hedgecues that we have not seen.
We can then use thelarge amounts of unannotated sentences that areavailable to extract n-gram features that have highuncertainty class conditional probability and addthem to our training set with those features labeledas hedges as described in Medlock and Briscoe(2007).
Because hedges are high precision fea-tures for uncertainty this should not hurt precisiongreatly.
This allows us to increase the size of ourtraining set substantially in order to expose oursystem to a greater variety of hedge cues in a semi-supervised manner.
As with most semi-supervisedsystems we run the risk of drift resulting in a dropin precision.
Future work will have to determinethe correct balance between precision and recall,ideally by embedding this task within the largerIE framework to provide extrinsic evaluationThis work neglected to address the more diffi-cult task of hedge scope detection.
Determininghedge scope requires paring spans of sentencesthat fall within the hedge scope to a given hedgecue.
Along with a move towards a scalar notionof uncertainty we should move towards a scopebased instead of sentence based representation ofuncertainty.
Hedges take scope over subparts ofa sentence so just because a relation occurs in thesame sentence as a hedge cue does not mean thatthe given relation is hedged.
It seems unnecessar-ily strict to ignore all relations or facts in a sen-tence just because it contains a hedge.
Hedge de-tection is an important precursor to hedge scopedetection.
Without a high performing hedge de-tection system we cannot hope to link hedge cueswith their respective scopes.
This work hopes toproduce a method for training such a hedge de-tection system for use as a component of a hedge124scope finding system.This work also failed to integrate constituencyor dependency features into either stage of thesystem.
Dependencies encode important informa-tion and we plan to include features based on de-pendency relationships into future versions of thesystem.
At the hedge detection stage it shouldimprove recall by allowing the system to detectwhich multi word hedge cues are part of the samecue.
At the uncertainty classification stage itshould allow the extraction of multiword featuresnot just based on n-gram frequency.
For semi-supervised learning it should allow the systemto more accurately annotated multi word featuresthat have a high class conditional probability.
Thisshould be even more important when performingthe task of hedge scope detection where scope isoften delimitated at the phrase level and determin-ing the dependency relations between words cancapture this observation.7 ConclusionThis work described HedgeHunter, a two stagehedge detection and uncertainty classification sys-tem.
It confirmed the lexical nature of the hedgedetection task, the importance of hedge cues to un-certainty classification and sharpened the need forlarge amounts of training data in order to achievebroad coverage.
It highlights the issues involved indeveloping an open domain system by evaluatingacross very disparate datasets.
It provides a frame-work that can be extended to semi-supervisedlearning in order to leverage large amounts ofunannotated data to improve both in domain andcross domain performance.ReferencesEiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,Tomoko Ohkuma, Hiroshi Mashuichi, and KazuhikoOhe.
2009.
TEXT2TABLE: Medical Text Summa-rization System Based on Named Entity Recogni-tion and Modality Identification.
In Proceedings ofthe BioNLP 2009 Workshop, pages 185?192, Boul-der, Colorado, June.
Association for ComputationalLinguistics.Mike Conway, Son Doan, and Nigel Collier.
2009.
Us-ing Hedges to Enhance a Disease Outbreak ReportText Mining System.
In Proceedings of the BioNLP2009 Workshop, pages 142?143, Boulder, Colorado,June.
Association for Computational Linguistics.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing Speculative Language in Biomedical ResearchArticles: A Linguistically Motivated Perspective.In Proceedings of the Workshop on Current Trendsin Biomedical Natural Language Processing, pages46?53, Columbus, Ohio, June.
Association for Com-putational Linguistics.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.
InProceedings of the BioNLP 2009 Workshop Com-panion Volume for Shared Task, pages 1?9, Boulder,Colorado, June.
Association for Computational Lin-guistics.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The Language of Bioscience: Facts, Specu-lations, and Statements in Between.
In Proc.
of theHLT-NAACL 2004 Workshop: Biolink 2004, LinkingBiological Literature, Ontologies and Databases,pages 17?24.Ben Medlock and Ted Briscoe.
2007.
Weakly Super-vised Learning for Hedge Classification in ScientificLiterature.
In Proceedings of the ACL, pages 992?999, Prague, Czech Republic, June.Roser Morante andWalter Daelemans.
2009.
Learningthe Scope of Hedge Cues in Biomedical Texts.
InProceedings of the BioNLP 2009 Workshop, pages28?36, Boulder, Colorado, June.
Association forComputational Linguistics.Roser Saur??
and James Pustejovsky.
2009.
FactBank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Gyo?rgy Szarvas.
2008.
Hedge Classification inBiomedical Texts with a Weakly Supervised Selec-tion of Keywords.
In Proceedings of ACL-08: HLT,pages 281?289, Columbus, Ohio, June.
Associationfor Computational Linguistics.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope Corpus: Biomedical Texts Annotated for Un-certainty, Negation and their Scopes.
BMC Bioin-formatics, 9(Suppl 11):S9.125
