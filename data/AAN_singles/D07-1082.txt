Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
783?790, Prague, June 2007. c?2007 Association for Computational LinguisticsActive Learning for Word Sense Disambiguation with Methods forAddressing the Class Imbalance ProblemJingbo ZhuUniversity of Southern CaliforniaInformation Sciences InstituteNortheastern University, P.R.ChinaNatural Language Processing LabZhujingbo@mail.neu.edu.cnEduard HovyUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695hovy@isi.eduAbstractIn this paper, we analyze the effect ofresampling techniques, including under-sampling and over-sampling used in activelearning for word sense disambiguation(WSD).
Experimental results show that un-der-sampling causes negative effects on ac-tive learning, but over-sampling is a rela-tively good choice.
To alleviate the within-class imbalance problem of over-sampling,we propose a bootstrap-based over-sampling (BootOS) method that works bet-ter than ordinary over-sampling in activelearning for WSD.
Finally, we investigatewhen to stop active learning, and adopt twostrategies, max-confidence and min-error,as stopping conditions for active learning.According to experimental results, we sug-gest a prediction solution by consideringmax-confidence as the upper bound andmin-error as the lower bound for stoppingconditions.1 IntroductionWord sense ambiguity is a major obstacle to accu-rate information extraction, summarization, andmachine translation (Ide and Veronis, 1998).
Inrecent years, a variety of techniques for machinelearning algorithms have demonstrated remarkableperformance for automated word sense disam-biguation (WSD) (Chan and Ng, 2006; Dagan et.al., 2006; Xue et.
al., 2006; Kohomban and Lee.2005; Dang and Palmer, 2005), when enough la-beled training data is available.
However, creatinga large sense-tagged corpus is very expensive andtime-consuming, because these data have to be an-notated by human experts.Among the techniques to solve the knowledgebottleneck problem, active learning is a promisingway (Lewis and Gale, 1994; McCallum and Ni-gram, 1998).
The purpose of active learning is tominimize the amount of human labeling effort byhaving the system automatically select for humanannotation the most informative unannotated case.In real-world data, the distribution of the sensesof a word is often very skewed.
Some studies re-ported that simply selecting the predominant senseprovides superior performance, when a highlyskewed sense distribution and insufficient contextexist (Hoste et al, 2001; McCarthy et.
al., 2004).The data set is imbalanced when at least one of thesenses is heavily underrepresented compared to theother senses.
In general, a WSD classifier is de-signed to optimize overall accuracy without takinginto account the class imbalance distribution in areal-world data set.
The result is that the classifierinduced from imbalanced data tends to over-fit thepredominant class and to ignore small classes(Japkowicz and Stephen, 2002).
Recently, muchwork has been done in addressing the classimbalance problem, reporting that resamplingmethods such as over-sampling and under-sampling are useful in supervised learning withimbalanced data sets to induce more effectiveclassifiers (Estabrooks et al, 2004; Zhou and Liu,2006).In general framework of active learning, thelearner (i.e.
supervised classifier) is formed by us-ing supervised learning algorithms.
To date, how-ever, no-one has studied the effects of over-sampling and under-sampling on active learning783methods.
In this paper, we study active learningwith resampling methods addressing the class im-balance problem for WSD.
It is noteworthy thatneither of these techniques need modify thearchitecture or learning algorithm, making themvery easy to use and extend to other domains.Another problem in active learning is  knowingwhen to stop the process.
We address this problemin this paper, and discuss how to form the finalclassifier for use.
This is a problem of estimationof classifier effectiveness (Lewis and Gale, 1994).Because it is difficult to know when the classifierreaches maximum effectiveness, previous workused a simple stopping condition when the trainingset reaches desirable size.
However, in fact it isalmost impossible to predefine an appropriate sizeof desirable training data for inducing the mosteffective classifier.
To solve the problem, weconsider the problem of estimation of classifiereffectiveness as a second task of estimatingclassifier confidence.
This paper adopts twostrategies: max-confidence and min-error, and sug-gests a prediction solution by considering max-confidence as the upper bound and min-error as thelower bound for the stopping conditions.2 Related WorkThe ability of the active learner can be referred toas selective sampling, of which two major schemesexist: uncertainty sampling and committee-basedsampling.
The former method, for example pro-posed by Lewis and Gale (1994), is to use only oneclassifier to identify unlabeled examples on whichthe classifier is least confident.
The latter method(McCallum and Nigam, 1998) generates a commit-tee of classifiers (always more than two classifiers)and selects the next unlabeled example by the prin-ciple of maximal disagreement among these classi-fiers.
With selective sampling, the size of the train-ing data can be significantly reduced for textclassification (Lewis and Gale, 1994; McCallumand Nigam, 1998), and word sense disambiguation(Chen, et al 2006).A method similar to committee-based samplingis co-testing proposed by Muslea et al (2000),which trains two learners individually on twocompatible and uncorrelated views that should beable to reach the same classification accuracy.
Inpractice, however, these conditions of view selec-tion are difficult to meet in real-world word sensedisambiguation tasks.Recently, much work has been done on the classimbalance problem.
The well-known approach isresampling, in which some training material is du-plicated.
Two types of popular resampling methodsexist for addressing the class imbalance problem:over-sampling and under-sampling.
The basic ideaof resampling methods is to change the trainingdata distribution and make the data more balanced.It works ok in supervised learning, but has notbeen tested in active learning.
Previous work re-ports that cost-sensitive learning is a good solutionto the class imbalance problem (Weiss, 2004).
Inpractice, for WSD, the costs of various senses of adisambiguated word are unequal and unknown,and they are difficult to evaluate in the process oflearning.In recent years, there have been attempts to ap-ply active learning for word sense disambiguation(Chen et al, 2006).
However, to our best knowl-edge, there has been no such attempt to considerthe class imbalance problem in the process of ac-tive learning for WSD tasks.3 Resampling Methods3.1 Under-samplingUnder-sampling is a popular method in addressingthe class imbalance problem by changing the train-ing data distribution by removing some exemplarsof the majority class at random.
Some previouswork reported that under-sampling is effective inlearning on large imbalanced data sets (Japkowiczand Stephen, 2002).
However, as under-samplingremoves some potentially useful training samples,it could cause negative effects on the classifier per-formance.One-sided sampling is a method similar to un-der-sampling, in which redundant and borderlinetraining examples are identified and removed fromtraining data (Kubat and Matwin, 1997).
Kubanand Matwin reported that one-sided sampling iseffective in learning with two-class large imbal-anced data sets.
However, the relative computa-tional cost of one-sided sampling in active learningis very high, because sampling computations mustbe implemented for each learning iteration.
Ourprimitive experimental results show that, in themulti-class problem of WSD, one-sided samplingdegrades the performance of active learning.
And784due to the high computation complexity of one-sided sampling, we use random under-sampling inour comparison experiments instead.To control the degree of change of the trainingdata distribution, the ratio of examples from themajority and the minority class after removal fromthe majority class is called the removal rate (Joand Japkowicz, 2004).
If the removal rate is 1.0,then under-sampling methods build data sets withcomplete class balance.
However, it was reportedpreviously that perfect balance is not always theoptimal rate (Estabrooks et al, 2004).
In our com-parison experiments, we set the removal rate forunder-sampling to 0.8, since some cases have 0.8as the optimal rate reported in (Estabrooks et al,2004).3.2 Over-samplingOver-sampling is also a popular method in ad-dressing the class imbalance problem by resam-pling the small class until it contains as many ex-amples as the large one.
In contrast to under-sampling, over-sampling is the process of addingexamples to the minority class, and is accom-plished by random sampling and duplication.
Be-cause the process of over-sampling involvesmaking exact copies of examples, it usually in-creases the training cost and may lead to overfit-ting.
There is a recent variant of over-samplingnamed SMOTE (Chawla et al, 2002) which is asynthetic minority over-sampling technique.
Theauthors reported that a combination of SMOTEand under-sampling can achieve better classifierperformance in ROC space than only under-sampling the majority class.In our comparison experiments, we use over-sampling, measured by a resampling rate called theaddition rate (Jo and Japkowicz, 2004) that indi-cates the number of examples that should be addedinto the minority class.
The addition rate for over-sampling is also set to 0.8 in our experiments.3.3 Bootstrap-based Over-samplingWhile over-sampling decreases the between-classimbalance, it increases the within-class imbalance(Jo and Japkowicz, 2004) because of the increaseof exact copies of examples at random.
To allevi-ate this within-class imbalance problem, we pro-pose a bootstrap-based over-sampling method(BootOS) that uses a bootstrap resampling tech-nique in the process of over-sampling.
Bootstrap-ping, explained below, is a resampling techniquesimilar to jackknifing.There are two reasons for choosing a bootstrapmethod as resampling technique in the process ofover-sampling.
First, using a bootstrap set canavoid exactly copying samples in the minorityclass.
Second, the bootstrap method may give asmoothing of the distribution of the training sam-ples (Hamamoto et al, 1997), which can alleviatethe within-class imbalance problem cased by over-sampling.To generate the bootstrap set, we use a well-known bootstrap technique proposed by Hama-moto et al (1997) that does not select samples ran-domly, allowing all samples in the minorityclass(es) an equal chance to be selected.Algorithm BootOS(X, N, r, k)Input: Minority class sample set X={x1, x2, ?, xn} ofsize n; Difference in number of examples between themajority and the minority class = N; Addition rate = r(< 1.0); Number of nearest neighbors = k.Output: bootstrap sample set XB of size N*r=X (xB1, xB2, ?, xB(N*r)).
?1.
For i = 1 To N*r2.
If i == n then (*all samples in minority classsample set have been used*)3.             j = 1; //the first sample is selected again4.
Else5.
j = i; // the i-th sample is selected6.
Endif7.
Select j-th sample xj (also as xj,0) from X8.
Find the k nearest neighbor samples xj,1, xj,2,?, xj,k using similarity functions.9.
Compute a bootstrap sample xBi:,01kBi j ll1x xk == + ?10.
Endfor11.
returnFigure 1.
The BootOS algorithm4 Active Learning with ResamplingIn this work, we are interested in selective sam-pling for pool-based active learning, and focus onuncertainty sampling (Lewis and Gale, 1994).
Thekey point is how to measure the uncertainty of anunlabeled exemplar, and select a new exemplarwith maximum uncertainty to augment the trainingdata.
The maximum uncertainty implies that thecurrent classifier has the least confidence in itsclassification of this exemplar.
The well-knownentropy is a good uncertainty measurement widely785used in active learning (zhang and Chen, 2002;Chen et al, 2006):1( ) ( ) ( | ) log ( | )i j i jjU i H P p s w p s w== = ?
?in i    (1)where U is the uncertainty measurement functionH represents the entropy function.
In the WSDtask, p(sj|wi) is the predicted probability of sense sjoutputted by the current classifier, when given asample i containing a disambiguated word wi.Algorithm Active-Learning-with-Resampling(L,U,m)Input: Let L be initial small training data set; U thepool of unlabeled exemplarsOutput: labeled training data set L1.
Resample L to generate new training data set L*using resampling techniques such as under-sampling, over-sampling or BootOS, and then useL* to train the initial classifier2.
Loop while adding new instances into La.
use the current classifier to probabilistically la-bel all unlabeled exemplars in Ub.
Based on active learning rules, present m top-ranked exemplars to oracle for labelingc.
Augment L with the m new exemplars, and re-move them from Ud.
Resample L to generate new training data setL* using resampling techniques such as under-sampling, over-sampling, or BootOS, and useL* to retrain the current classifier3.
Until the predefined stopping condition is met.4.
returnFigure 2.
Active learning with resamplingIn step 1 and 2(d) in Fig.
2, if we do not gener-ate L*, and L is used directly to train the currentclassifier, we call it ordinary active learning.
In theprocess of active learning, we used the entropy-based uncertainty measurement for all active learn-ing frameworks in our comparison experiments.Actually our active learning with resampling is aheterogeneous approach in which the classifierused to select new instances is different from theresulting classifier (Lewis and Catlett, 1994).We utilize a maximum entropy (ME) model(Berger et al, 1996) to design the basic classifierused in active learning for WSD.
The advantage ofthe ME model is the ability to freely incorporatefeatures from diverse sources into a single, well-grounded statistical model.
A publicly availableME toolkit (Zhang et.
al., 2004) was used in ourexperiments.
In order to extract the linguistic fea-tures necessary for the ME model, all sentencescontaining the target word were automatically part-of-speech (POS) tagged using the Brill POS tagger(Brill, 1992).
Three knowledge sources were usedto capture contextual information: unordered singlewords in topical context, POS of neighboringwords with position information, and local colloca-tions.
These are same as three of the four knowl-edge sources used in (Lee and Ng, 2002).
Theirfourth knowledge source (named syntactic rela-tions) was not used in our work.5 Stopping ConditionsIn active learning algorithm, defining the stoppingcondition for active learning is a critical problem,because it is almost impossible for the human an-notator to label all unlabeled samples.
This is aproblem of estimation of classifier effectiveness(Lewis and Gale 1994).
In fact, it is difficult toknow when the classifier reaches maximumeffectiveness.
In previous work some researchersused a simple stopping condition when the trainingset reached a predefined desired size.
It is almostimpossible to predefine an appropriate size ofdesirable training data for inducing the mosteffective classifier.To solve the problem, we consider the problemof estimating  classifier effectiveness as theproblem of confidence estimation of classifier onthe remaining unlabeled samples.
Concretely, if wefind that the current classifier already hasacceptably strong confidence on its classificationresults for all remained unlabeled data, we assumethe current training data is sufficient to train theclassifier with maximum effectiveness.
In otherwords, if a classifier induced from the currenttraining data has strong classification confidenceon an unlabeled example, we could consider it as aredundant example.Based on above analyses, we adopt here twostopping conditions for active learning:?
Max-confidence: This strategy is based onuncertainty measurement, considering whetherthe entropy of each selected unlabeled exampleis less than a very small predefined thresholdclose to zero, such as 0.001.?
Min-error: This strategy is based on feedbackfrom the oracle when the active learner asksfor true labels for selected unlabeled examples,considering whether the current trainedclassifier could correctly predict the labels orthe accuracy performance of predictions on786selected unlabeled examples is already largerthan a predefined accuacy threshold.Once max-confidence and min-error conditionsare met, the current classifier is assumed to havestrong enough confidence on the classificationresults of all remained unlabeled data.6 Evaluation6.1 DataThe data used for our comparison experimentswere developed as part of the OntoNotes project(Hovy et al, 2006), which uses the WSJ part of thePenn Treebank (Marcus et al, 1993).
The sensesof noun words occurring in OntoNotes are linkedto the Omega ontology.
In OntoNotes, at least twohumans manually annotate the coarse-grainedsenses of selected nouns and verbs in their naturalsentence context.
To date, OntoNotes hasannotated several tens of thousands of examples,covering several hundred nouns and verbs, with aninter-annotator agreement rate of at least 90%.Those 38 random chosen ambiguous nouns usedin all following experiments are shown in Table 1.It is apparent that the sense distributions of mostnouns are very skewed (frequencies shown in thetable, separated by /).Words sense distribution  words sense distributionRate 1025/182 president 936/157/17People 815/67/7/5 part 456/102/75/16Point 471/88/37/19/9/6 director 517/23Revenue 517/23 bill 348/130/40Future 413/82/23 order 354/61/54/6/6Plant 376/51 board 369/15Today 238/149 policy 308/74Capital 325/21/8 term 147/137/52/13management 210/130 move 302/13/5Position 97/75/67/61/10/7 amount 236/57/16Home 267/17/16 power 154/134/15Leader 244/38 return 191/35/29/12/9administration 266/11 payment 201/69Account 233/18/13 control 90/66/64/21/12/5Lot 221/20 activity 218/23Drug 160/74 building 177/48/5Estate 214/11 house 112/71/25development 165/46/6 network 127/53/29Strategy 198/11 place 69/63/50/18/5Table 1.
Data set used in experiments6.2 ResultsIn the following active learning comparisonexperiments, we tested with five resamplingmethods including random sampling (Random),uncertainty sampling (Ordinary), under-sampling,over-sampling, and BootOS.
The 1-NN techniquewas used for bootstrap-based resampling ofBootOS in our experiments.
A 5 by 5-fold cross-validation was performed on each noun?s data.We used 20% randomly chosen data for held-outevaluation  and the other 80% as the pool ofunlabeled data for each round of the activelearning.
For all words, we started with arandomly chosen initial training set of 10examples, and we made 10 queries after eachlearning iteration.In the evaluation, average accuracy and recallare used as measures of performances for eachactive learning method.
Note that the macro-average way is adopted for recall evaluation ineach noun WSD task.
The accuracy measureindicates the percentage of testing instancescorrectly identified by the system.
The macro-average recall measure indicates how well thesystem performs on each sense.Experiment 1: Performance comparison ex-periments on active learning0.780.80.820.840.860.8810  30  50  70  90  110  130  150  170  190  210  230  250  270  290AverageAccuracyNumber of learned samplesActive learning for WSDRandomOrdinaryUnder-samplingOver-samplingBootOSFigure 3.
Average accuracy performance com-parison experiments0.360.380.40.420.440.460.480.50.5210  30  50  70  90  110  130  150  170  190  210  230  250  270  290AverageRecallNumber of learned samplesActive learning for WSDRandomOrdinaryUnder-samplingOver-samplingBootOSFigure 4.
Average recall performance comparisonexperiments787As shown in Fig.
3 and Fig.
4, when the number oflearned samples for each noun is smaller than 120,the BootOS has the best performance, followed byover-sampling and ordinary method.
As the num-ber of learned samples increases, ordinary, over-sampling and BootOS have similar performanceson accuracy and recall.
Our experiments also ex-hibit that random sampling method is the worst onboth accuracy and recall.Previous work (Estabrooks et al, 2004) reportedthat under-sampling of the majority class (pre-dominant sense) has been proposed as a goodmeans of increasing the sensitivity of a classifier tothe minority class (infrequent sense).
However, inour active learning experiments, under-sampling isapparently worse than ordinary, over-sampling andour BootOS.
The reason is that in highly imbal-anced data, too many useful training samples ofmajority class are discarded in under-sampling,causing the performance of active learning to de-grade.Experiment 2: Effectiveness of learning in-stances for infrequent sensesIt is important to enrich the corpora by learningmore instances for infrequent senses using activelearning with less human labeling.
This procedurenot only makes the corpora ?richer?, but alsoalleviates  the domain dependence problem facedby corpus-based supervised approaches to WSD.The objective of this experiment is to evaluatethe performance of active learning in learningsamples of infrequent senses from an unlabeledcorpus.
Due to highly skewed word sensedistributions in our data set, we consider all sensesother than the predominant sense as infrequentsenses in this experiment.0.20.250.30.350.40.450  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300percentageof learnedinstancesforinfrequentsensesNumber of learned samplesActive learning for WSDRandomOrdinaryUnder-samplingOver-samplingBootOSFigure 5.
Comparison experiments on learning in-stances for infrequent sensesFig.
5 shows that random sampling is the worstin active learning for infrequent senses.
The reasonis very obvious: the sense distribution of thelearned sample set by random sampling is almostidentical to that of the original data set.Under-sampling is apparently worse than ordi-nary active learning, over-sampling and BootOSmethods.
When the number of learned samples foreach noun is smaller than 80, BootOS achievesslight better performance than ordinary activelearning and over-sampling.When the number of learned samples is largerthan 80 and smaller than 160, these three methodsexhibit similar performance.
As the number of it-erations increases, ordinary active learning isslightly better than over-sampling and BootOS.
Infact, after the 16th iteration (10 samples chosen ineach iteration), results indicate that most instancesfor infrequent senses have been learned.Experiment 3: Effectiveness of Stopping Condi-tions for active learningTo evaluate the effectiveness of two strategiesmax-confidence and min-error as stopping condi-tions of active learning, we first construct an idealstopping condition when the classifier could reachthe highest accuracy performance at the first timein the procedure of active learning.
When the idealstopping condition is met, it means that the currentclassifier has reached maximum effectiveness.
Inpractice, it is impossible to exactly know when theideal stopping condition is met before all unlabeleddata are labeled by a human annotator.
We onlyuse this ideal method in our comparison experi-ments to analyze the effectiveness of our two pro-posed stopping conditions.For general purpose, we focus on the ordinaryactive learning to design the basic system, and toevaluate the effectiveness of three stop conditions.In the following experiments, the entropy thresholdused in max-confidence strategy is set to 0.001, andthe accuracy threshold used in min-error strategyis set to 0.9.In Table 2, the column ?Size?
stands for the sizeof unlabeled data set of corresponding noun wordused in active learning.
There are two columns foreach stopping condition: the left column ?num?presents number of learned instances and the rightcolumn ?%?
presents its percentage over all datawhen the corresponding stopping condition is met.788Ideal Max-confidence Min-error Words Sizenum % num % num %Rate 966 200 .23 410 .41 290 .29People 715 140 .20 290 .41 200 .28Point 504 90 .18 220 .44 120 .24Revenue 432 70 .16 110 .25 80 .19Future 414 120 .29 140 .34 60 .14Plant 342 210 .61 180 .53 110 .32Today 382 250 .65 240 .63 230 .60Capital 283 70 .25 180 .64 90 .32Management 272 200 .74 210 .77 210 .77Position 254 210 .83 230 .91 220 .87Home 240 60 .25 160 .67 60 .25Leader 226 60 .27 120 .53 70 .31administration 222 30 .14 90 .41 50 .23Account 211 50 .24 130 .62 70 .33Lot 185 30 .16 60 .32 40 .22Drug 187 130 .70 140 .75 120 .64Estate 180 20 .11 50 .28 30 .17Development 174 40 .23 150 .86 80 .46Strategy 167 10 .06 100 .60 10 .06President 888 120 .14 220 .25 120 .14Part 519 110 .21 240 .46 130 .25Director 432 110 .25 130 .30 90 .21Bill 414 120 .29 280 .68 150 .36Order 385 130 .34 220 .57 140 .36Board 307 40 .13 190 .62 40 .13Policy 306 90 .29 200 .65 150 .49Term 279 120 .43 190 .68 130 .47Move 256 50 .20 140 .55 50 .20Amount 247 210 .85 200 .81 140 .57Power 242 190 .78 190 .78 190 .78Return 221 90 .41 160 .72 100 .45Payment 216 120 .56 160 .74 150 .69Control 206 160 .78 200 .97 200 .97Activity 193 30 .16 130 .67 70 .36Building 184 90 .49 130 .71 110 .60House 166 100 .60 150 .90 110 .66Network 167 110 .66 130 .78 100 .60Place 164 120 .73 150 .91 120 .73Table 2 Effectiveness of three stopping conditionsAs shown in Table 2, the min-error strategybased on feedback of human annotator is veryclose to the ideal method.
Therefore, when com-paring to ideal stopping condition, min-error strat-egy is a good choice as stopping condition for ac-tive learning.
It is important to note that the min-error method does not need more additionalcomputational costs, it only depends upon thefeedback of human annotator when labeling thechosen unlabeled samples.From experimental results, we can see that max-confidence strategy is worse than min-errormethod.
However, we believe that the entropy ofeach unlabeled sample is a good signal to stop ac-tive learning.
So we suggest that there may be agood prediction solution in which the min-errorstrategy is used as the lower-bound of stoppingcondition, and max-confidence strategy as the up-per-bound of stopping condition for active learning.7 DiscussionAs discussed above, finding more instances forinfrequent senses at the earlier stages of activelearning is very significant in making the corpusricher, meaning less effort for human labeling.
Inpractice, another way to learn more instances forinfrequent senses is to first build a training data setby active learning or by human efforts, and thenbuild a supervised classifier to find more instancesfor infrequent sense.
However, it is interesting toknow how much initial training data is enough forthis task, and how much human labeling effortscould be saved.From experimental results, we found that amongthese chosen unlabeled instances by active learner,some instances are informative samples helpful forimproving classification performance, and otherinstances are borderline samples which are unreli-able because even a small amount of noise can leadthe sample to the wrong side of the decisionboundary.
The removal of these borderline samplesmight improve the performance of active learning.The proposed prediction solution based on max-confidence and min-error strategies is a coarseframework.
To predict when to stop active learningprocedure, it is logical to consider the changes ofaccuracy performance of the classifier as a signalto stop the learning iteration.
In other words, dur-ing the range predicted by the proposed solution, ifthe change of accuracy performance of the learner(classifier) is very small, we could assume that thecurrent classifier has reached maximum effective-ness.8 Conclusion and Future WorkIn this paper, we consider the class imbalanceproblem in WSD tasks, and analyze the effect ofresampling techniques including over-samplingand under-sampling in active learning.
Experimen-tal results show that over-sampling is a relativelygood choice in active learning for WSD in highlyimbalanced data.
Under-sampling causes negativeeffect on active learning.
A new over-samplingmethod named BootOS based on bootstrap tech-nique is proposed to alleviate the within-class im-balance problem of over-sampling, and works bet-ter than ordinary over-sampling in active learningfor WSD.
It is noteworthy that none of thesetechniques require to modify the architecture or789learning algorithm; therefore, they are very easy touse and extend to other applications.
To predictwhen to stop active learning, we adopt twostrategies including max-confidence and min-erroras stopping conditions.
According to ourexperimental results, we suggest a predictionsolution by considering max-confidence as theupper bound and min-error as the lower bound ofstopping conditions for active learning.In the future work, we will study how to exactlyidentify these borderline samples thus they are notfirstly selected in active learning procedure.
Theborderline samples have the higher entropy valuesmeaning least confident for the current classifier.The borderline instances can be detected using theconcept of Tomek links (Tomek 1976).
It is alsoworth studying cost-sensitive learning for activelearning with imbalanced data, and using suchtechniques for WSD.ReferencesA.
L. Berger, S. A. Della, and V. J  Della.
1996.
A maximumentropy approach to natural language processing.
Compu-tational Linguistics 22(1):39?71.E Brill.
1992.
A simple rule-based part of speech tagger.
Inthe Proceedings of the Third Conference on Applied Natu-ral Language Processing.Y.
S. Chan and H. T. Ng.
2006.
Estimating class priors indomain adaptation.
In Proc.
of ACL06.N.
Chawla, K. Bowyer, L. Hall, W. Kegelmeyer.
2002.SMOTE: synthetic minority over-sampling technique.
Jour-nal of Artificial Intelligence Research, 2002(16): 321-357J.
Chen, A. Schein, L. Ungar, M. Palmer.
2006.
An empiricalstudy of the behavior of active learning for word sense dis-ambiguation.
In Proc.
of HLT-NAACL06I.
Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, and C.Strapparava.
2006.
Direct Word Sense Matching for Lexi-cal Substitution.
In Proc.
of ACL'06H.
T. Dang and M. Palmer.
2005.
The Role of Semantic Rolesin Disambiguating Verb Senses.
In Proc.
of ACL'05.A.
Estabrooks, T. Jo and N. Japkowicz.
2004.
A multiple re-sampling method for learning from imbalanced data set.Computational Intelligence, 20(1):18-36Y.
Hamamoto, S. Uchimura and S. Tomita.
1997.
A bootstraptechnique for nearest neighbor classifier design.
IEEETransactions on Pattern Analysis and Machine Intelligence,19(1):73-79V.
Hoste, A. Kool, and W. Daelemans.
2001.
Classifier opti-mization and combination in the English all words task.
InProc.
of the SENSEVAL-2 workshopE.
Hovy, M. Marcus, M. Palmer, L. Ramshaw and R.Weischedel.
2006.
Ontonotes: The 90% Solution.
In Proc.of HLT-NAACL06.N.
Ide and J. Veronis.
1998.
Introduction to the special issueon word sense disambiguation: the state of the art.
Compu-tational Linguistics, 24(1):1-37N.
Japkowicz and S. Stephen.
2002.
The class imbalanceproblem: a systematic study.
Intelligent Data Analysis,6(5):429-450T.
Jo and N. Japkowicz.
2004.
Class imbalances versus smalldisjuncts.
SIGKSS Explorations, 6(1):40-49U.
S. Kohomban and W. S. Lee.
2005.
Learning SemanticClasses for Word Sense Disambiguation.
In Proc.
ofACL'05M.
Kubat and S. Matwin.
1997.
Addressing the curse of im-balanced training sets: one-sided selection.
In Proc.
ofICML97Y.K.
Lee and.
H.T.
Ng.
2002.
An empirical evaluation ofknowledge sources and learning algorithm for word sensedisambiguation.
In Proc.
of EMNLP-2002D.
D. Lewis and W. A. Gale.
1994.
A sequential algorithm fortraining text classifiers.
In Proc.
of SIGIR-94D.D.
Lewis and J. Catlett.
1994.
Heterogeneous uncertaintysampling for supervised learning.
In Proc.
of ICML94M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics,  19(2):313-330A.
McCallum and K. Nigram.
1998.
Employing EM in pool-based active learning for text classification.
In Proc.
15thICMLD.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding predominant senses in untagged text.
In Proc.
ofACL04I.
Muslea, S. Minton, and C. A. Knoblock.
2000.
Selectivesampling with redundant views.
In Proc.
of National Con-ference on Artificial IntelligenceI.
Tomek.
1976.
Two modifications of CNN.
IEEE Transac-tions on Systems, Man and Cybernetics, 6(6):769-772G.
M. Weiss.
2004.
Mining with rarity ?
problems and solu-tions: a unifying framework.
SIGKDD Explorations,6(1):7-19N.
Xue, J. Chen and M. Palmer.
2006.
Aligning Features withSense Distinction Dimensions.
In Proc.
of ACL'06Z.
Zhou, X. Liu.
2006.
Training cost-sensitive neural networkswith methods addressing the class imbalance problem.IEEE Transactions on Knowledge and Data Engineering,18(1):63-77L.
Zhang, J. Zhu, and T. Yao.
2004.
An evaluation of statisti-cal spam filtering techniques.
ACM Transactions on AsianLanguage Information Processing, 3(4):243?269.C.
Zhang and T. Chen.
2002.
An active learning frame-work for content-based information retrieval.
IEEETransactions on Multimedia, 4(2):260-268790
