Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 829?833,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Summary Prior Representation for Extractive SummarizationZiqiang Cao1,2?Furu Wei3Sujian Li1,2Wenjie Li4Ming Zhou3Houfeng Wang1,21Key Laboratory of Computational Linguistics, Peking University, MOE, China2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China3Microsoft Research, Beijing, China4Computing Department, Hong Kong Polytechnic University, Hong Kong{ziqiangyeah, lisujian, wanghf}@pku.edu.cn{furu, mingzhou}@microsoft.com cswjli@comp.polyu.edu.hkAbstractIn this paper, we propose the conceptof summary prior to define how mucha sentence is appropriate to be selectedinto summary without consideration ofits context.
Different from previouswork using manually compiled document-independent features, we develop a novelsummary system called PriorSum, whichapplies the enhanced convolutional neu-ral networks to capture the summaryprior features derived from length-variablephrases.
Under a regression framework,the learned prior features are concate-nated with document-dependent featuresfor sentence ranking.
Experiments on theDUC generic summarization benchmarksshow that PriorSum can discover differentaspects supporting the summary prior andoutperform state-of-the-art baselines.1 IntroductionSentence ranking, the vital part of extractivesummarization, has been extensively investigated.Regardless of ranking models (Osborne, 2002;Galley, 2006; Conroy et al., 2004; Li et al.,2007), feature engineering largely determines thefinal summarization performance.
Features oftenfall into two types: document-dependent features(e.g., term frequency or position) and document-independent features (e.g., stopword ratio or wordpolarity).
The latter type of features take effectsdue to the fact that, a sentence can often be judgedby itself whether it is appropriate to be includedin a summary no matter which document it lies in.Take the following two sentences as an example:1.
Hurricane Emily slammed into Dominica onSeptember 22, causing 3 deaths with its windgusts up to 110 mph.
?Contribution during internship at Microsoft Research2.
It was Emily, the hurricane which caused 3deaths and armed with wind guests up to 110mph, that slammed into Dominica on Tues-day.The first sentence describes the major informationof a hurricane.
With similar meaning, the secondsentence uses an emphatic structure and is some-what verbose.
Obviously the first one should bepreferred for a news summary.
In this paper, wecall such fact as summary prior nature1and learndocument-independent features to reflect it.In previous summarization systems, though notwell-studied, some widely-used sentence rankingfeatures such as the length and the ratio of stop-words, can be seen as attempts to measure thesummary prior nature to a certain extent.
Notably,Hong and Nenkova (2014) built a state-of-the-artsummarization system through making use of ad-vanced document-independent features.
However,these document-independent features are usuallyhand-crafted, difficult to exhaust each aspect ofthe summary prior nature.
Meanwhile, items rep-resenting the same feature may contribute differ-ently to a summary.
For example, ?September 22?and ?Tuesday?
are both indicators of time, but thelatter seldom occurs in a summary due to uncer-tainty.
In addition, to the best of our knowledge,document-independent features beyond word level(e.g., phrases) are seldom involved in current re-search.The CTSUM system developed by Wan andZhang (2014) is the most relevant to ours.
It at-tempted to explore a context-free measure namedcertainty which is critical to ranking sentences insummarization.
To calculate the certainty score,four dictionaries are manually built as features anda corpus is annotated to train the feature weightsusing Support Vector Regression (SVR).
How-1In this paper, ?summary prior features?
and ?document-independent features?
hold the same meaning.829ever, a low certainty score does not always rep-resent low quality of being a summary sentence.For example, the sentence below is from a topicabout ?Korea nuclear issue?
in DUC 2004: Clin-ton acknowledged that U.S. is not yet certain thatthe suspicious underground construction projectin North Korea is nuclear related.
The under-lined phrases greatly reduce the certainty of thissentence according to Wan and Zhang (2014)?smodel.
But, in fact, this sentence can summarizethe government?s attitude and is salient enough inthe related documents.
Thus, in our opinion, cer-tainty can just be viewed as a specific aspect of thesummary prior nature.To this end, we develop a novel summarizationsystem called PriorSum to automatically exploitall possible semantic aspects latent in the sum-mary prior nature.
Since the Convolutional NeuralNetworks (CNNs) have shown promising progressin latent feature representation (Yih et al., 2014;Shen et al., 2014; Zeng et al., 2014), PriorSumapplies CNNs with multiple filters to capture acomprehensive set of document-independent fea-tures derived from length-variable phrases.
Thenwe adopt a two-stage max-over-time pooling op-eration to associate these filters since phraseswith different lengths may express the same as-pect of summary prior.
PriorSum generates thedocument-independent features, and concatenatesthem with document-dependent ones to work forsentence regression (Section 2.1).We conduct extensive experiments on the DUC2001, 2002 and 2004 generic multi-documentsummarization datasets.
The experimental resultsdemonstrate that our model outperforms state-of-the-art extractive summarization approaches.Meanwhile, we analyze the different aspects sup-porting the summary prior in Section 3.3.2 MethodologyOur summarization system PriorSum follows thetraditional extractive framework (Carbonell andGoldstein, 1998; Li et al., 2007).
Specifically, thesentence ranking process scores and ranks the sen-tences from documents, and then the sentence se-lection process chooses the top ranked sentencesto generate the final summary in accordance withthe length constraint and redundancy among theselected sentences.Sentence ranking aims to measure the saliencyscore of a sentence with consideration of bothdocument-dependent and document-independentfeatures.
In this study, we apply an enhanced ver-sion of convolutional neural networks to automati-cally generate document-independent features ac-cording to the summary prior nature.
Meanwhile,some document-dependent features are extracted.These two types of features are combined in thesentence regression step.2.1 Sentence RankingPriorSum improves the standard convolutionalneural networks (CNNs) to learn the summaryprior since CNN is able to learn compressed rep-resentation of n-grams effectively and tackle sen-tences with variable lengths naturally.
We firstintroduce the standard CNNs, based on whichwe design our improved CNNs for obtainingdocument-independent features.The standard CNNs contain a convolution oper-ation over several word embeddings, followed bya pooling operation.
Let vi?
Rkdenote the k-dimensional word embedding of the ithword inthe sentence.
Assume vi:i+jto be the concatena-tion of word embeddings vi, ?
?
?
, vi+j.
A convo-lution operation involves a filter Wht?
Rl?hk,which operates on a window of h words to pro-duce a new feature with l dimensions:chi= f(Wht?
vi:i+h?1) (1)where f is a non-linear function and tanh is usedlike common practice.
Here, the bias term isignored for simplicity.
Then Whtis applied toeach possible window of h words in the sentenceof length N to produce a feature map: Ch=[ch1, ?
?
?
, chN?h+1].
Next, we adopt the widely-used max-over-time pooling operation (Collobertet al., 2011) to obtain the final features c?hfromCh.
That is, c?h= max{Ch}.
The idea behindthis pooling operation is to capture the most im-portant features in a feature map.In the standard CNNs, only the fixed-lengthwindows of words are considered to represent asentence.
As we know, the variable-length phrasescomposed of a sentence can better express the sen-tence and disclose its summary prior nature.
Tomake full use of the phrase information, we designan improved version of the standard CNNs, whichuse multiple filters for different window sizes aswell as two max-over-time pooling operations toget the final summary prior representation.
Specif-ically, let W1t, ?
?
?
,Wmtbe m filters for window830sizes from 1 to m, and correspondingly we canobtainm feature mapsC1, ?
?
?
,Cm.
For each fea-ture mapCi, We first adopt a max-over-time pool-ing operationmax{Ci}with the goal of capturingthe most salient features from each window size i.Next, a second max-over-time pooling operationis operated on all the windows to acquire the mostrepresentative features.
To formulate, the docu-ment independent features xpcan be generated by:xp= max{max{C1}, ?
?
?
,max{Cm}}.
(2)Kim (2014) also uses filters with varying win-dow sizes for sentence-level classification tasks.However, he reserves all the representations gen-erated by filters to a fully connected output layer.This practice greatly enlarges following parame-ters and ignores the relation among phrases withdifferent lengths.
Hence we use the two-stagemax-over-time pooling to associate all these fil-ters.Besides the features xpobtained throughthe CNNs, we also extract several document-dependent features notated as xe, shown in Table1.
In the end, xpis combined with xeto con-duct sentence ranking.
Here we follow the regres-sion framework of Li et al.
(2007).
The sentencesaliency y is scored by ROUGE-2 (Lin, 2004)(stopwords removed) and the model tries to esti-mate this saliency.?
= [xp, xe] (3)y?
= wTr?
?
(4)where wr?
Rl+|xe|is the regression weights.We use linear transformation since it is convenientto compare with regression baselines (see Section3.2).Feature DescriptionPOSITION The position of the sentence.AVG-TF The averaged term frequency values ofwords in the sentence.AVG-CF The averaged cluster frequency values ofwords in the sentence.Table 1: Extracted document-dependent features.2.2 Sentence SelectionA summary is obliged to offer both informativeand non-redundant content.
Here, we employ asimple greedy algorithm to select sentences, simi-lar to the MMR strategy (Carbonell and Goldstein,1998).
Firstly, we remove sentences less than 8words (as in Erkan and Radev (2004)) and sort therest in descending order according to the estimatedsaliency scores.
Then, we iteratively dequeue onesentence, and append it to the current summary ifit is non-redundant.
A sentence is considered non-redundant if it contains more new words comparedto the current summary content.
We empiricallyset the cut-off of new word ratio to 0.5.3 Experiments3.1 Experiment SetupIn our work, we focus on the generic multi-document summarization task and carry out ex-periments on DUC 2001 2004 datasets.
All thedocuments are from newswires and grouped intovarious thematic clusters.
The summary length islimited to 100 words (665 bytes for DUC 2004).We use DUC 2003 data as the development set andconduct a 3-fold cross-validation on DUC 2001,2002 and 2004 datasets with two years of data astraining set and one year of data as test set.We directly use the look-up table of 25-dimensional word embeddings trained by themodel of Collobert et al.
(2011).
These smallword embeddings largely reduces model param-eters.
The dimension l of the hidden document-independent features is experimented in the rangeof [1, 40], and the window sizes are experimentedbetween 1 and 5.
Through parameter experimentson development set, we set l = 20 and m = 3 forPriorSum.
To update the weights Whtand wr, weapply the diagonal variant of AdaGrad with mini-batches (Duchi et al., 2011).For evaluation, we adopt the widely-used auto-matic evaluation metric ROUGE (Lin, 2004), andtake ROUGE-1 and ROUGE-2 as the main mea-sures.3.2 Comparison with Baseline MethodsTo evaluate the summarization performance of Pri-orSum, we compare it with the best peer systems(PeerT, Peer26 and Peer65 in Table 2) participat-ing DUC evaluations.
We also choose as baselinesthose state-of-the-art summarization results onDUC (2001, 2002, and 2004) data.
To our knowl-edge, the best reported results on DUC 2001,2002 and 2004 are from R2N2 (Cao et al., 2015),ClusterCMRW (Wan and Yang, 2008) and REG-SUM2(Hong and Nenkova, 2014) respectively.R2N2 applies recursive neural networks to learn2REGSUM truncates a summary to 100 words.831feature combination.
ClusterCMRW incorporatesthe cluster-level information into the graph-basedranking algorithm.
REGSUM is a word regres-sion approach based on some advanced featuressuch as word polarities (Wiebe et al., 2005) andcategories (Tausczik and Pennebaker, 2010).
Forthese three systems, we directly cite their pub-lished results, marked with the sign ?*?
as in Ta-ble 2.
Meanwhile, LexRank (Erkan and Radev,2004), a commonly-used graph-based summariza-tion model, is introduced as an extra baseline.Comparing with this baseline can demonstrate theperformance level of regression approaches.
Thebaseline StandardCNN means that we adopt thestandard CNNS with fixed window size for sum-mary prior representation.To explore the effects of the learned summaryprior representations, we design a baseline sys-tem named Reg Manual which adopts manually-compiled document-independent features such asNUMBER (whether number exist), NENTITY(whether named entities exist) and STOPRATIO(the ratio of stopwords).
Then we combine thesefeatures with document-dependent features in Ta-ble 1 and tune the feature weights through LIB-LINEAR3support vector regression.From Table 2, we can see that PriorSum canachieve a comparable performance to the state-of-the-art summarization systems R2N2, Cluster-CMRW and REGSUM.
With respect to baselines,PriorSum significantly4outperforms Reg Manualwhich uses manually compiled features andthe graph-based summarization system LexRank.Meanwhile, PriorSum always enjoys a reasonableincrease over StandardCNN, which verifies the ef-fects of the enhanced CNNs.
It is noted that Stan-dardCNN can also achieve the state-of-the-art per-formance, indicating the summary prior represen-tation really works.3.3 AnalysisIn this section, we explore what PriorSum learnsaccording to the summary prior representations.Since the convolution layer follows a linear regres-sion output, we apply a simple strategy to measurehow much the learned document-independent fea-tures contribute to the saliency estimation.
Specif-ically, for each sentence, we ignore its document-dependent features through setting their values as3http://www.csie.ntu.edu.tw/?cjlin/liblinear/4T -test with p-value ?
0.05Year System ROUGE-1 ROUGE-22001 PeerT 33.03 7.86R2N2?35.88 7.64LexRank 33.43 6.09Reg Manual 34.55 7.18StandardCNN 35.19 7.63PriorSum 35.98 7.892002 Peer26 35.15 7.64ClusterCMRW?38.55 8.65LexRank 35.29 7.54Reg Manual 34.81 8.12StandardCNN 35.73 8.69PriorSum 36.63 8.972004 Peer65 37.88 9.18REGSUM?38.57 9.75LexRank 37.87 8.88Reg Manual 37.05 9.34StandardCNN 37.90 9.93PriorSum 38.91 10.07Table 2: Comparison results (%) on DUC datasets.Meanwhile, Yugoslavia?s P.M. told an emer-gency session Monday that the country is facedwith war.highscoredThe rebels ethnic Tutsis, disenchanted membersof President Laurent Kabila?s army took up arms,creating division among Congo?s 400 tribes.The blast killed two assailants, wounded 21 Is-raelis and prompted Israel to suspend implemen-tation of the peace accord with the Palestinians.The greatest need is that many, many of us havebeen psychologically traumatized, and very, veryfew are receiving help.lowscoredRuben Rivera: An impatient hitter who willchase pitches out of the strike zone.I think we should worry about tuberculosis andthe risk to the general population.Table 3: Example sentences selected by priorscores.zeros and then apply a linear transformation usingthe weight wrto get a summary prior score xp.The greater the score, the more possible a sentenceis to be included in a summary without contextconsideration.
We analyze what intuitive featuresare hidden in the summary prior representation.From Table 3, first we find that high-scoredsentences contains more named entities and num-bers, which conforms to human intuition.
Bycontrast, the features NENTITY and NUMBERin Reg Manual hold very small weights, only2%, 3% compared with the most significant fea-ture AVG-CF.
One possible reason is that namedentities or numbers are not independent features.For example, ?month + number?
is a commontimestamp for an event whereas ?number + a.m.?is over-detailed and seldom appears in a summary.We can also see that low-scored sentences are rel-atively informal and fail to provide facts, which832are difficult for human to generalize some spe-cific features.
For instance, informal sentencesseem to have more stopwords but the feature STO-PRATIO holds a relatively large positive weight inReg Manual.4 Conclusion and Future WorkThis paper proposes a novel summarization sys-tem called PriorSum to automatically learn sum-mary prior features for extractive summariza-tion.
Experiments on the DUC generic multi-document summarization task show that our pro-posed method outperforms state-of-the-art ap-proaches.
In addition, we demonstrate the dom-inant sentences discovered by PriorSum, and theresults verify that our model can learn different as-pects of summary prior.AcknowledgmentsWe thank all the anonymous reviewers for their in-sightful comments.
This work was partially sup-ported by National Key Basic Research Programof China (No.
2014CB340504), National NaturalScience Foundation of China (No.
61273278 and61272291), and National Social Science Founda-tion of China (No: 12&ZD227).
The correspon-dence author of this paper is Sujian Li.ReferencesZiqiang Cao, Furu Wei, Li Dong, Sujian Li, and MingZhou.
2015.
Ranking with recursive neural net-works and its application to multi-document sum-marization.
In AAAI-2015.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofSIGIR, pages 335?336.Ronan Collobert, Jason Weston, Lon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.John M Conroy, Judith D Schlesinger, Jade Goldstein,and Dianne P Oleary.
2004.
Left-brain/right-brainmulti-document summarization.
In Proceedings ofDUC.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.G?unes Erkan and Dragomir R Radev.
2004.Lexrank: Graph-based lexical centrality as saliencein text summarization.
J. Artif.
Intell.
Res.
(JAIR),22(1):457?479.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of EMNLP, pages 364?372.Kai Hong and Ani Nenkova.
2014.
Improvingthe estimation of word importance for news multi-document summarization.
In Proceedings of EACL.Yoon Kim.
2014.
Convolutional neural net-works for sentence classification.
arXiv preprintarXiv:1408.5882.Sujian Li, You Ouyang, Wei Wang, and Bin Sun.
2007.Multi-document summarization using support vec-tor regression.
In Proceedings of DUC.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proceedings of the ACLWorkshop, pages 74?81.Miles Osborne.
2002.
Using maximum entropy forsentence extraction.
In Proceedings of ACL Work-shop on Automatic Summarization, pages 1?8.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,and Gr?egoire Mesnil.
2014.
Learning semantic rep-resentations using convolutional neural networks forweb search.
In Companion publication of the 23rdinternational conference on World wide web com-panion, pages 373?374.Yla R Tausczik and James W Pennebaker.
2010.
Thepsychological meaning of words: Liwc and comput-erized text analysis methods.
Journal of languageand social psychology, 29(1):24?54.Xiaojun Wan and Jianwu Yang.
2008.
Multi-documentsummarization using cluster-based link analysis.
InProceedings of SIGIR, pages 299?306.Xiaojun Wan and Jianmin Zhang.
2014.
Ctsum: ex-tracting more certain summaries for news articles.In Proceedings of the 37th international ACM SIGIRconference on Research & development in informa-tion retrieval, pages 787?796.
ACM.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language resources and evalua-tion, 39(2-3):165?210.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation questionanswering.
In Proceedings of ACL.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation classification via con-volutional deep neural network.
In Proceedings ofCOLING, pages 2335?2344.833
