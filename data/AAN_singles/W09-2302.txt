Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 10?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsStatistical Phrase Alignment ModelUsing Dependency Relation ProbabilityToshiaki Nakazawa Sadao KurohashiGraduate School of Informatics, Kyoto UniversityYoshida-honmachi, Sakyo-kuKyoto, 606-8501, Japannakazawa@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jpAbstractWhen aligning very different language pairs,the most important needs are the use of struc-tural information and the capability of gen-erating one-to-many or many-to-many corre-spondences.
In this paper, we propose anovel phrase alignment method which modelsword or phrase dependency relations in depen-dency tree structures of source and target lan-guages.
The dependency relation model is akind of tree-based reordering model, and canhandle non-local reorderings which sequen-tial word-based models often cannot handleproperly.
The model is also capable of esti-mating phrase correspondences automaticallywithout any heuristic rules.
Experimental re-sults of alignment show that our model couldachieve F-measure 1.7 points higher than theconventional word alignment model with sym-metrization algorithms.1 IntroductionWe consider that there are two important needs inaligning parallel sentences written in very differ-ent languages such as Japanese and English.
Oneis to adopt structural or dependency analysis intothe alignment process to overcome the difference inword order.
The other is that the method needs tohave the capability of generating phrase correspon-dences, that is, one-to-many or many-to-many wordcorrespondences.
Most existing alignment methodssimply consider a sentence as a sequence of words(Brown et al, 1993), and generate phrase correspon-dences using heuristic rules (Koehn et al, 2003).Some studies incorporate structural information intothe alignment process after this simple word align-ment (Quirk et al, 2005; Cowan et al, 2006).
How-ever, this is not sufficient because the basic wordalignment itself is not good.On the other hand, a few models have been pro-posed which use structural information from the be-ginning of the alignment process.
Watanabe et al(2000) and Menezes and Richardson (2001) pro-posed a structural alignment methods.
These meth-ods use heuristic rules when resolving correspon-dence ambiguities.
Yamada and Knight (2001) andGildea (2003) proposed a tree-based probabilisticalignment methods.
These methods reorder, insertor delete sub-trees on one side to reproduce the otherside, but the constraints of using syntactic informa-tion is often too rigid.
Yamada and Knight flat-tened the trees by collapsing nodes.
Gildea clonedsub-trees to deal with the problem.
Cherry and Lin(2003) proposed a model which uses a source sidedependency tree structure and constructs a discrim-inative model.
However, there is the defect that itsalignment unit is a word, so it can only find one-to-one alignments.
Nakazawa and Kurohashi (2008)also proposed a model focusing on the dependencyrelations.
Their model has the constraint that contentwords can only correspond to content words on theother side, and the same applies for function words.This sometimes leads to an incorrect alignment.
Wehave removed this constraint to make more flexi-ble alignments possible.
Moreover, in their model,some function words are brought together, and thusthey cannot handle the situation where each func-tion word corresponds to a different part.
The small-est unit of our model is a single word, which shouldsolve this problem.10In this paper, we propose a novel phrase align-ment method which models word or phrase de-pendency relations in dependency tree structures ofsource and target languages.
For a pair of correspon-dences which has a parent-child relation on one side,the dependency relation on the other side is definedas the relation between the two correspondences.It is a kind of tree-based reordering model, andcan capture non-local reorderings which sequentialword-based models often cannot handle properly.The model is also capable of estimating phrase cor-respondences automatically without heuristic rules.The model is trained in two steps: Step 1 estimatesword translation probabilities, and Step 2 estimatesphrase translation probabilities and dependency re-lation probabilities.
Both Step 1 and Step 2 are per-formed iteratively by the EM algorithm.
During theStep 2 iterations, word correspondences are growninto phrase correspondences.2 Proposed ModelWe suppose that Japanese is the source language andEnglish is the target language in the description ofour model.
Note that the model is not specializedfor this language pair, and it can be applied to anylanguage pair.Because our model uses dependency tree struc-tures, both source and target sentences are parsedbeforehand.
Japanese sentences are converted intodependency structures using the morphological ana-lyzer JUMAN (Kurohashi et al, 1994), and the de-pendency analyzer KNP (Kawahara and Kurohashi,2006).
MSTparser (McDonald et al, 2005) is usedto convert English sentences.
Figure 1 shows an ex-ample of dependency structures.
The root of a tree isplaced at the extreme left and words are placed fromtop to bottom.2.1 OverviewThis section outlines our proposed model in compar-ison to the IBM models, which are the conventionalstatistical alignment models.In the IBM models (Brown et al, 1993), the bestalignment a?
between a given source sentence f andits target sentence e is acquired by the followingequation:a?
= argmaxap(f ,a|e)= argmaxap(f |e,a) ?
p(a|e) (1)?????????????????Aphotogateisusedforthephotodetector.
(accept)(light)(device)(photo)(gate)(used)(ni)(ha)(wo)Figure 1: An example of a dependency tree and its align-ment.where p(f |e,a) is called lexicon probability andp(a|e) is called alignment probability.Suppose f consists of nwords f1, f2, ..., fn, and econsists ofmwords e1, e2, ..., em and a NULL word(e0).
The alignment mapping a consists of associa-tions j ?
i = aj from source position j to targetposition i = aj .
The two probabilities above arebroken down as:p(f |e,a) =J?j=1p(fj |eaj ) (2)p(a|e) =I?i=1p(?j|ei) (3)where ?j is a relative position of words in thesource side which corresponds to ei.
Equation 2 isthe product of the word translation probabilities, andEquation 3 is the product of relative position proba-bilities.In the proposed model, we refine the IBM modelsin three ways.
First, as for Equation 2, we considerphrases instead of words.
Second, as for Equation 3,we consider dependencies of words instead of theirpositions in a sentence.Finally, the proposed model can find the bestalignment a?
by not using f -to-e alone, but simulta-neously with e-to-f .
That is, Equation 1 is modifiedas follows:a?
= argmaxap(f |e,a) ?
p(a|e) ?p(e|f ,a) ?
p(a|f) (4)Since our model regards a phrase as a basic unit,the above formula is calculated in a straightforwardway.
In contrast, the IBM models can considera many-to-one alignment by combining one-to-one11alignments, but they cannot consider a one-to-manyor many-to-many alignment.The models are estimated by EM-like algorithmwhich is very similar to (Liang et al, 2006).
Theimportant difference is that we are using tree struc-tures.We maximize the data likelihood:max?ef ,?fe?f ,e(log pef (f , e; ?ef ) + log pfe(f , e; ?fe))(5)In the E-step, we compute the posterior distributionof the alignments with the current parameter ?
:q(a; f , e) := pef (a|f , e; ?ef ) ?
pfe(a|f , e; ?fe) (6)In the M-step, we update the parameter ?:??
:= argmax?
?a,f ,eq(a; f , e) log pef (a, f , e; ?ef )+?a,f ,eq(a; f , e) log pfe(a, f , e; ?fe)= argmax?
?a,f ,eq(a; f , e) log p(e) ?
pef (a, f |e; ?ef )+?a,f ,eq(a; f , e) log p(f) ?
pfe(a, e|f ; ?fe)(7)Note that p(e) and p(f) have no effect on maxi-mization, and pef (a, f |e; ?ef ) and pfe(a, e|f ; ?fe)appeared in Equation 1 or Equation 4.In the following sections, we decompose the lexi-con probability and alignment probability.2.2 Phrase Translation ProbabilitySuppose f consists of N phrases F1, F2, ..., FN , ande consists of M phrases E1, E2, ..., EM .
The align-ment mapping a consists of associations j ?
i =Aj from source phrase j to target phrase i = Aj .We consider phrase translation probabilityp(Fj |Ei) instead of word translation probability.There is one restriction: that phrases composed ofmore than one word cannot be aligned to NULL.Only a single word can be aligned to NULL.We denote a phrase which the word fj belongs toas Fs(j), and a phrase which the word ei belongs toas Et(i).
With these notations, we refine Equation 2as follows:p(f |e,a) =J?j=1p(Fs(j)|EAs(j)) (8)Suppose phrase Fj and Ei are aligned where thenumber of words in Fj is denoted by |Fj | and thatnumber in Ei is |Ei|, the probability mass related tothis alignment in Equation 8 is as follows:p(Fj |Ei)|Fj | ?
p(Ei|Fj)|Ei| (9)We call this probability for the link between Fj andEi phrase alignment probability.
The upper part ofTable 1 shows phrase alignment probabilities for thealignment in Figure 1.2.3 Dependency Relation ProbabilityThe reordering model in the IBM Models is definedon the relative position between an alignment andits previous alignment, as shown in Equation 3.
Ourmodel, on the other hand, considers dependencies ofwords instead of positional relations.We start with a dependency relation where fc de-pends on fp in the source sentence.
In a possiblealignment, fc belongs to Fs(c), fp belongs to Fs(p),and Fs(c) depends on Fs(p).
In this situation, we con-sider the relation between EAs(p) and EAs(c) .
Evenif two languages have different word order, their de-pendency structures are similar in many cases, andEAs(c) tends to depend on EAs(p) .
Our model takesthis tendency into consideration.
In order to de-note the relationship between phrases, we introducerel(EAs(p) , EAs(c)).
This is defined as the path fromEAs(p) to EAs(c) .
It is represented by applying thenotations below:?
?c?
if going down to the child node?
?p?
if going down to the parent nodeFor example, in Figure 1, the path from ?for?
to?photodetector?
is ?c?, from ?the?
to ?for?
is ?p;p?because it travels across two nodes.
All the phrasesare considered as a single node, so the path from?photogate?
to ?the?
is ?p;c;c;c?
with the alignmentin Figure 1.We refine Equation 3 using rel as follows:p(a|e) = ?
(p,c)?Ds-pcpt(rel(EAs(p) , EAs(c))|pc)(10)where Ds-pc denotes a set of parent-childword pairs in the source sentence.
We callpt(rel(EAs(p) , EAs(c))|pc) target side dependencyrelation probability.
pt is a kind of tree-basedreordering model.12Table 1: A probability calculation example.Source Target Phrase alignment probability????
photodetector p(????
|photodetector)3 ?
p(photodetector|????)??
for p(??
|for)2 ?
p(for|??)??????
photogate p(??????
|a photogate)2 ?
p(a photogate|??????)2????
is used p(????
|is used)2 ?
p(is used|????
)2NULL the p(the|NULL)Source Target dependency Target Source dependencyc p relation probability c p relation probability?
?
pt(SAME|pc) A photogate ps(SAME|pc)?
??
pt(SAME|pc) photogate is ps(c|pc)??
?
pt(c|pc) used is ps(SAME|pc)?
?
pt(SAME|pc) for used ps(c|pc)?
???
pt(c|pc) the photodetector ps(NULL c|pc)???
???
pt(SAME|pc) photodetector for ps(c|pc)???
?
pt(c|pc)?
???
pt(SAME|pc)There are some special cases for rel.
When Fs(c)and Fs(p) are the same, that is, fc and fp belongto the same phrase, rel is represented as ?SAME?.When fp is aligned to NULL, fc is aligned to NULL,and both of them are aligned to NULL, rel is repre-sented as ?NULL p?, ?NULL c?, and ?NULL b?, re-spectively.
The lower part of Table 1 shows depen-dency relation probabilities corresponding to Figure1.Actually, we extend the dependency relationprobability to consider a wider relation, i.e, thegrandparent-child relation, as follows:p(a|e) = ?
(p,c)?Ds-pcpt(rel(EAs(p) , EAs(c))|pc) ??
(g,c)?Ds-gcpt(rel(EAs(g) , EAs(c))|gc)(11)where Ds-gc denotes a set of grandparent-child wordpairs in the source sentence.3 Model TrainingOur model is trained in two steps.
In Step 1, wordtranslation probability is estimated.
Then, in Step 2,possible phrases are acquired, and both phrase trans-lation probability and dependency relation probabil-ity are estimated.
In both steps, parameter estima-tion is done with the EM algorithm.3.1 Step 1In Step 1, word translation probability in each di-rection is estimated independently.
This is done inexactly the same way as in IBM Model 1.In this process, the alignment unit is a word.When we consider f -to-e alignment, each word onthe source side fj can correspond to a word on thetarget side ei or a NULL word, independently ofother source words.
The probability of one possiblealignment a is calculated as follows:p(a, f |e) =J?j=1p(fj |eaj ) (12)By considering all possible alignments, p(f |e) iscalculated as:p(f |e) =?ap(a, f |e) (13)As initial parameters of p(f |e), we use uniformprobabilities.
Then, after calculating Equation 12and 13, we give the fractional count p(a,f |e)p(f |e) to allword alignments in a, and we estimate p(f |e) byMLE.
We perform this estimation iteratively.The inverse model e-to-f can be calculated in thesame manner.3.2 Step 2Both phrase translation probability and dependencyrelation probability are estimated, and one undi-rected alignment is found using the e-to-f and f -to-eprobabilities simultaneously in this step.
In contrastto Step 1, it is impossible to enumerate all the possi-ble alignments.
To find the best alignment, we firstcreate an initial alignment based on phrase trans-lation probability only, and then gradually revise it13by considering the dependency relation probabilitywith a hill-climbing algorithm.The initial parameters of Step 2 are calculatedas follows.
The dependency relation probability iscalculated using the final alignment result of Step1, and we use the word translation probability esti-mated in Step 1 as the initial phrase translation prob-ability.3.2.1 Initial AlignmentWe first create an initial alignment based on thephrase translation probability without consideringthe dependency relation probabilities.For all the combinations of possible phrases(including NULL), phrase alignment probabilitiesare calculated (equation 9).
Correspondences areadopted one by one in descending order of geomet-ric mean of the phrase alignment probabilities.
Allthe words should be aligned only once, that is, thecorrespondences are adopted exclusively.
Genera-tion of possible phrases is explained in Section 3.2.3.3.2.2 Hill-climbingTo find better alignments, the initial alignment isgradually revised with a hill-climbing algorithm.
Weuse four kinds of revising operations:Swap: Focusing on any two correspondences, thepartners are swapped.
In the first step inFigure2, the correspondences ??
?
photo-gate?
and ????????
photodetector?
areswapped to ???
photodetector?
and ???????
?
photogate?.Extend: Focusing on one correspondence, thesource or target phrase is extended to includeits neighboring (parent or child) NULL-alignedword.Add: A new correspondence is added between asource word and a target word both of whichare aligned to NULL.Reject: A correspondence is rejected and the sourceand target phrase are aligned to NULL.Figure 2 shows an illustrative example of hillclimbing.
The alignment is revised only if the align-ment probability gets increased.
It is repeated un-til no operation can improve the alignment probabil-ity, and the final state is the best approximate align-ment.
As a by-product of hill-climbing, pseudo n-best alignment can be acquired.
It is used in collect-ing fractional counts.3.2.3 Phrase GenerationIf there is a word which is aligned to NULL in thebest approximate alignment, a new possible phraseis generated by merging the word into a neighbor-ing phrase which is not aligned to NULL.
In the lastalignment result in Figure 2, for example, ???
?is treated as being included in the correspondencebetween ??
??
and ?photodetector?
and the cor-respondence between ???
and ?for?.
As a result,we consider the correspondence between ??
?
???
and ?photodetector?
and the correspondence be-tween ?????
and ?for?
existing in parallel sen-tences.
The new possible phrase is taken into con-sideration from the next iteration.3.2.4 Model EstimationCollecting all the alignment results, we estimatephrase alignment probabilities and dependency rela-tion probabilities.One way of estimating parameters of phrasealignment probabilities is using the following equa-tions:p(Fj |Ei) = C(Fj , Ei)?k C(Fk, Ei)p(Ei|Fj) = C(Fj , Ei)?k C(Ek, Fj)(14)where C(Fj , Ei) is a frequency of Fj and Ei isaligned.However, if we use this in our model, the phrasetranslation probability of the new possible phrasecan become extremely high (often it becomes 1).To avoid this problem, we use the equations belowfor the estimation of phrase translation probabilityin place of Equation 14:p(Fj |Ei) = C(Fj , Ei)C(Ei) , p(Ei|Fj) =C(Fj , Ei)C(Fj) (15)C(Ei) is the frequency of the phrase Ei in the train-ing corpus which can be pre-counted.
This definitioncan resolve the problem where the phrase translationprobability of the new possible phrase becomes toohigh.As for the NULL, we use Equation 14 because wecannot pre-count the frequency of NULL.Using the estimated phrase alignment probabil-ities and dependency relation probabilities, we goback to the initial alignment described in Section3.2.1 iteratively.14?????????????????Aphotogateisusedforthephotodetector.?????????????????Aphotogateisusedforthephotodetector.?????????????????Aphotogateisusedforthephotodetector.?????????????????Aphotogateisusedforthephotodetector.????????????????
?Aphotogateisusedforthephotodetector.Ini al alignmentSwap RejectAdd Extend(accept)(light)(device)(ni)(ha)(photo)(gate)(used)(wo)Figure 2: An example of hill-climbing.4 Experimental ResultsWe conducted alignment experiments.
A JST1Japanese-English paper abstract corpus consistingof 1M parallel sentences was used for the modeltraining.
This corpus was constructed from a 2MJapanese-English paper abstract corpus by NICT2using the method of Uchiyama and Isahara (2007).As gold-standard data, we used 475 sentence pairswhich were annotated by hand.
The annotationswere only sure (S) alignments (there were no possi-ble (P ) alignments) (Och and Ney, 2003).
The unitof evaluation was word-base for both Japanese andEnglish.
We used precision, recall, and F-measureas evaluation criteria.We conducted two experiments to reveal 1) thecontribution of our proposed model compared to theexisting models, and 2) the effectiveness of usingdependency tree structure and phrases, which arelarger alignment units than words.
Trainings wererun on the original forms of words for both the pro-posed model and the models used for comparison.4.1 Comparison with Word Sequential ModelFor comparison, we used GIZA++ (Och and Ney,2003) which implements the prominent sequentialword-base statistical alignment model of IBM Mod-els.
We conducted word alignment bidirectionallywith its default parameters and merged them usingthree types of symmetrization heuristics (Koehn etal., 2003).
The results are shown in Table 2.1http://www.jst.go.jp/2http://www.nict.go.jp/The result of ?Step 1?
uses parameters estimatedafter 5 iterations of Step 1.
The alignment is ob-tained by the method of initial alignment shown inSection 3.2.1.
In ?Step 2-1?, the phrase translationprobabilities are the same as those in ?Step 1?.
In ad-dition, dependency relation probabilities estimatedfrom the ?Step 1?
alignment result are used.
By com-paring ?Step 1?
and ?Step 2-1?, we can see the ef-fectiveness of dependency relation probability.
Weperformed 5 iterations for Step 2 and calculated thealignment accuracy each time.
As a result, the pro-posed model could achieve a higher F-measure by1.7 points compared to the sequential model.
?In-tersection?
achieved best Precision, but its Recall isquite low.
?grow-diag-final-and?
achieved best Re-call, but its Precision is lower than our best resultwhere the Recall is almost same.
Thus, we can sayour result is better than sequential word alignmentmodels.4.2 Effectiveness of Dependency Trees andPhrasesTo confirm the effectiveness of dependency trees andphrases, we conducted alignment experiments on thefollowing four conditions:?
Using both dependency trees and phrases (re-ferred to as ?proposed?).?
Using dependency trees only.?
Using phrases only.?
Not using dependency trees or phrases (referredto as ?none?
)For the conditions which do not use dependencytrees, we used positional relations of a sentence as15Table 2: Results of alignment experiment.Precision Recall FStep 1 77.55 33.92 47.20Step 2-1 83.46 40.03 54.11Step 2-2 87.74 45.37 59.81Step 2-3 87.62 48.92 62.79Step 2-4 86.87 50.42 63.81Step 2-5 85.90 50.75 63.80Step 2-6 85.54 51.00 63.90Step 2-7 85.18 50.87 63.70Step 2-8 84.66 50.75 63.46intersection 90.34 34.28 49.71grow-final-and 81.32 48.85 61.04grow-diag-final-and 79.39 51.15 62.22Table 3: Effectiveness of dependency trees and phrases(results after 5 iterations in Step 2.
)Precision Recall Fproposed 85.54 51.00 63.90dependency tree only 89.77 39.47 54.83phrase only 84.41 47.33 60.65none 85.07 38.06 52.59a sequence of words instead of dependency tree re-lations.
The results are shown in Table 3.
All theresults are the alignment accuracy after 5 iterationsof Step 2.5 DiscussionTable 2 shows that our proposed model couldachieve reasonably high accuracy of alignment, andis better than sequential word-base models.
Asan example, alignment results of a word sequen-tial model are shown in Figure 3.
The gray col-ored cells are the gold-standard alignments, and theblack boxes are the outputs of the sequential model.The model failed to resolve the correspondence am-biguities between ??
(not) ??
(castrated) ???
(mice)?, and ???
????
; and ?non-castratedmice?, and ?castrated mice?
respectively.
This isbecause these words are placed close to each otherand are also close to the correspondence ?????
as?
which can be a clue to the word order.
Us-ing the tree structure in Figure 4, these words werecorrectly aligned.
This is because in the Englishtree, the phrase ?castrated mice?
does not dependon ?as?, and ?non-castrated mice?
does.
Similarlyin the Japanese tree, ????????
depends on ?????
and ???????
does not.As mentioned in Section 1, sequential statistical??
?exhibited ?
?astrong ?inhibitory ?effect ?on ?
?tumor ?growth ?in ?the ?castrated ?mice ?as ?
?in ?thenon-castrated ?mice ?????
??
????
?????
????
????
??
?
????
??
??
?
??
?Figure 3: An alignment example of the word sequentialmodel (grow-diag-final-and).??
?exhibited ?
??
??a?
?
?strong ??
?
?inhibitory ??
?effect ???on?
?
?
?tumor ??
?
?growth ??
?in ??
?
??the?
?
?
?castrated ??
?
?mice ??
?as ??
?in ??
??the?
?
?non-castrated ?
??
?mice ??????????????????????????????????????????????????????????????????????????????????????
?Figure 4: An alignment example of the proposed model.methods, which regard a sentence as a sequence ofwords, work well for language pairs that are not toodifferent in their language structure.
Japanese andEnglish have significantly different structures.
Oneof the issues is that Japanese sentences have a SOVword order, but in English, the word order is SVO, sothe dependency relations are often turned over.
Forlanguage pairs such as Japanese and English, deepersentence analysis using NLP resources is necessaryand useful.
Our method is therefore suitable for suchlanguage pairs.As another example of an alignment failure bythe sequential model, Figure 5 shows the phrase cor-respondence ??
?
??
?
photodetector?, whichwas correctly found as shown in Figure 6.
The pro-16Aphotogate ?
?is ?used ?for ?the ?photodetector ???
??
?
?
???????
??
?Figure 5: An unsuccessful example of phrase detection inthe sequential model (grow-diag-final-and).?
?A ???
?photogate ?
?is ?
??
?used ?
??
?for ?
??
??the?
?photodetector ?
?
??????????????????????????????????
?Figure 6: An example of phrase detection in the proposedmodel.posed method of generating possible phrases duringiterations works well and improves alignment.From the result of our second experiment, we cansee the following points:1.
Phrasal alignment improves the recall, but low-ers the precision.2.
By using dependency trees, precision can beimproved.3.
We can find a balance point by using bothphrasal alignment and dependency trees.The causes of alignment errors in our model canbe summarized into categories.
The biggest one isparsing errors.
Since our model is highly dependenton the parsing result, the alignments would easilyturn out wrong if the parsing result was incorrect.Sometimes the hill-climbing algorithm could notrevise the initial alignment.
Most of these caseswould happen when one word occurred severaltimes on one side, but some of those occurrenceswere omitted on the other side.
Let?s suppose thereare two identical words on the source side, but thetarget side has only one corresponding word.
Initialalignment is created without considering the depen-dencies at all, so it cannot judge which source wordshould be aligned to the corresponding target word.In this case, the best alignment searching sometimesgets the local solution.
This problem could be re-solved by considering local dependencies for am-biguous words.One difficulty is how to handle function words.Function words often do not have exactly corre-sponding words in the opposite language.
Japanesecase markers such as ??
(ha)?, ??
(ga)?
(subjec-tive case), ??
(wo)?
(objective case) and so on, andEnglish articles are typical examples of words, thatdo not have corresponding parts.
There is a differ-ence between alignment criteria for function wordsof gold-standard and our outputs, and it is somewhatdifficult to improve alignment accuracy.6 ConclusionIn this paper, we have proposed a linguistically-motivated probabilistic phrase alignment modelbased on dependency tree structures.
The model in-corporates the tree-based reordering model.
Experi-mental results show that the word sequential modeldoes not work well for linguistically different lan-guage pairs, and this can be resolved by using syn-tactic information.
We have conducted the experi-ments only on Japanese-English corpora.
To firmlysupport our claim that syntactic information is im-portant, it is necessary to do more investigation onother language pairs.Most frequent alignment errors are derived fromparsing errors.
Because our method depends heavilyon structural information, parsing errors easily makethe alignment accuracy worse.
Although the parsingaccuracy is high in general for both Japanese andEnglish, it sometimes outputs incorrect dependencystructures because technical or unknown words of-ten appears in scientific papers.
This problem couldbe resolved by introducing parsing probabilities intoour model using parsing tools which can output n-best parsing with their parsing probabilities.
Thiswill not only improve the alignment accuracy, it willallow revision of the parsing result.
Moreover, weneed to investigate the contribution of our alignmentresult to the translation quality.17ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Association for Computational Linguistics,19(2):263?312.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In Proceedings ofthe 41st Annual Meeting of the Association of Compu-tational Linguistics, pages 88?95.Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proceedings of the 2006 Conference onEMNLP, pages 232?241, Sydney, Australia, July.
As-sociation for Computational Linguistics.Daniel Gildea.
2003.
Loosely tree-based alignment formachine translation.
In Proceedings of the 41st An-nual Meeting on ACL, pages 80?87.Daisuke Kawahara and Sadao Kurohashi.
2006.
A fully-lexicalized probabilistic model for japanese syntacticand case structure analysis.
In Proceedings of the Hu-man Language Technology Conference of the NAACL,Main Conference, pages 176?183, New York City,USA, June.
Association for Computational Linguis-tics.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HLT-NAACL2003: Main Proceedings, pages 127?133.Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,and Makoto Nagao.
1994.
Improvements of Japanesemorphological analyzer JUMAN.
In Proceedings ofThe International Workshop on Sharable Natural Lan-guage, pages 22?28.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA,June.
Association for Computational Linguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 523?530, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.Arul Menezes and Stephen D. Richardson.
2001.
A best-first alignment algorithm for automatic extraction oftransfer mappings from bilingual corpora.
In Proceed-ings of the 39th Annual Meeting of the Association forComputational Linguistics (ACL) Workshop on Data-Driven Machine Translation, pages 39?46.Toshiaki Nakazawa and Sadao Kurohashi.
2008.Linguistically-motivated tree-based probabilisticphrase alignment.
In In Proceedings of the EighthConference of the Association for Machine Translationin the Americas (AMTA2008).Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Association for Computational Linguistics, 29(1):19?51.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 271?279.Masao Utiyama and Hitoshi Isahara.
2007.
A japanese-english patent parallel corpus.
In MT summit XI, pages475?482.Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki.2000.
Finding structural correspondences from bilin-gual parsed corpus for corpus-based translation.
InProceedings of the 18th International Conference onComputational Linguistics, pages 906?912.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of 39thAnnual Meeting of the ACL, pages 523?530.18
