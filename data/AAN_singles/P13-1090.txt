Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914?923,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSemi-Supervised Semantic Tagging of Conversational Understandingusing Markov Topic RegressionAsli CelikyilmazMicrosoftMountain View, CA, USAasli@ieee.orgDilek Hakkani-Tur, Gokhan TurMicrosoft ResearchMountain View, CA, USAdilek@ieee.orggokhan.tur@ieee.orgRuhi SarikayaMicrosoftRedmond, WA, USArusarika@microsoft.comAbstractFinding concepts in natural language ut-terances is a challenging task, especiallygiven the scarcity of labeled data for learn-ing semantic ambiguity.
Furthermore,data mismatch issues, which arise whenthe expected test (target) data does notexactly match the training data, aggra-vate this scarcity problem.
To deal withthese issues, we describe an efficient semi-supervised learning (SSL) approach whichhas two components: (i) Markov TopicRegression is a new probabilistic modelto cluster words into semantic tags (con-cepts).
It can efficiently handle seman-tic ambiguity by extending standard topicmodels with two new features.
First, it en-codes word n-gram features from labeledsource and unlabeled target data.
Sec-ond, by going beyond a bag-of-words ap-proach, it takes into account the inherentsequential nature of utterances to learn se-mantic classes based on context.
(ii) Ret-rospective Learner is a new learning tech-nique that adapts to the unlabeled targetdata.
Our new SSL approach improvessemantic tagging performance by 3% ab-solute over the baseline models, and alsocompares favorably on semi-supervisedsyntactic tagging.1 IntroductionSemantic tagging is used in natural language un-derstanding (NLU) to recognize words of seman-tic importance in an utterance, such as entities.Typically, a semantic tagging model require largeamount of domain specific data to achieve goodperformance (Tur and DeMori, 2011).
This re-quires a tedious and time intensive data collectionand labeling process.
In the absence of large la-beled training data, the tagging model can behavepoorly on test data (target domain).
This is usuallycaused by data mismatch issues and lack of cover-age that arise when the target data does not matchthe training data.To deal with these issues, we present a newsemi-supervised learning (SSL) approach, whichmainly has two components.
It initially starts withtraining supervised Conditional Random Fields(CRF) (Lafferty et al, 2001) on the source train-ing data which has been semantically tagged.
Us-ing the trained model, it decodes unlabeled datasetfrom the target domain.
With the data mismatchissues in mind, to correct errors that the supervisedmodel make on the target data, the SSL modelleverages the additional information by way of anew clustering method.
Our first contribution is anew probabilistic topic model, Markov Topic Re-gression (MTR), which uses rich features to cap-ture the degree of association between words andsemantic tags.
First, it encodes the n-gram contextfeatures from the labeled source data and the unla-beled target data as prior information to learn se-mantic classes based on context.
Thus, each latentsemantic class corresponds to one of the seman-tic tags found in labeled data.
MTR is not invari-ant to reshuffling of words due to its Markovianproperty; hence, word-topic assignments are alsoaffected by the topics of the surrounding words.Because of these properties, MTR is less sensitiveto the errors caused by the semantic ambiguities.Our SSL uses MTR to smooth the semantic tag pos-teriors on the unlabeled target data (decoded usingthe CRF model) and later obtains the best tag se-quences.
Using the labeled source and automati-914cally labeled target data, it re-trains a new CRF-model.Although our iterative SSL learning model candeal with the training and test data mismatch, itneglects the performance effects caused by adapt-ing the source domain to the target domain.
Infact, most SSL methods used for adaptation, e.g.,(Zhu, 2005), (Daume?-III, 2010), (Subramanya etal., 2010), etc., do not emphasize this issue.
Withthis in mind, we introduce a new iterative trainingalgorithm, Retrospective Learning, as our secondcontribution.
While retrospective learning itera-tively trains CRF models with the automaticallyannotated target data (explained above), it keepstrack of the errors of the previous iterations so asto carry the properties of both the source and targetdomains.In short, through a series of experiments weshow how MTR clustering provides additional in-formation to SSL on the target domain utter-ances, and greatly impacts semantic tagging per-formance.
Specifically, we analyze MTR?s perfor-mance on two different types of semantic tags:named-entities and descriptive tags as shown inTable 1.
Our experiments show that it is muchharder to detect descriptive tags compared tonamed-entities.Our SSL approach uses probabilistic clusteringmethod tailored for tagging natural language utter-ances.
To the best of our knowledge, our work isthe first to explore the unlabeled data to iterativelyadapt the semantic tagging models for target do-mains, preserving information from the previousiterations.
With the hope of spurring related workin domains such as entity detection, syntactic tag-ging, etc., we extend the earlier work on SSL part-of-speech (POS) tagging and show in the experi-ments that our approach is not only useful for se-mantic tagging but also syntactic tagging.The remainder of this paper is divided as fol-lows: ?2 gives background on SSL and semanticclustering methods, ?3 describes our new cluster-ing approach, ?4 presents the new iterative learn-ing, ?5 presents our experimental results and ?6concludes our paper.2 Related Work and Motivation(I) Semi-Supervised Tagging.
Supervised meth-ods for semantic tagging in NLU require a largenumber of in-domain human-labeled utterancesand gazetteers (movie, actor names, etc.
), increas-?
Are there any [comedies] with [Ryan Gosling]??
How about [oscar winning] movies by[James Cameron]??
Find [Woody Allen] movies similar to [Manhattan].
[Named Entities]director: James Cameron, Woody Allen,...actor: Ryan Gosling, Woody Allen,...title: Manhattan, Midnight in Paris,...[Descriptive Tags]restriction: similar, suitable, free,rate,...description: oscar winning, new release, gardening,...genre: spooky, comedies, feel good, romance,...Table 1: Samples of semantically tagged utter-ances from movie domain, named-entities and de-scriptive tags.ing the need for significant manual labor (Tur andDeMori, 2011).
Recent work on similar tasksovercome these challenges using SSL methods asfollows:?
(Wang et al, 2009; Li et al, 2009; Li,2010; Liu et al, 2011) investigate web querytagging using semi-supervised sequence models.They extract semantic lexicons from unlabeledweb queries, to use as features.
Our work dif-fers from these, in that, rather than just detectingnamed-entities, our utterances include descriptivetags (see Table 1).?
Typically the source domain has different dis-tribution than the target domain, due to topic shiftsin time, newly introduced features (e.g., until re-cently online articles did not include facebook?like?
feature.
), etc.
Adapting the source domainusing unlabeled data is the key to achieving goodperformance across domains.
Recent adaptationmethods for SSL use: expectation minimization(Daume?-III, 2010) graph-based learning (Chapelleet al, 2006; Zhu, 2005), etc.
In (Subramanya etal., 2010) an efficient iterative SSL method is de-scribed for syntactic tagging, using graph-basedlearning to smooth POS tag posteriors.
However,(Reisinger and Mooney, 2011) argues that vectorspace models, such as graph-learning, may fail tocapture the richness of word meaning, as simi-larity is not a globally consistent metric.
Ratherthan graph-learning, we present a new SSL usinga probabilistic model, MTR, to cluster words basedon co-occurrence statistics.
?Most iterative SSL methods, do not keep trackof the errors made, nor consider the divergencefrom the original model.
(Lavoie et al, 2011) ar-gues that iterative learning models should mitigatenew errors made by the model at each iteration by915keeping the history of the prior predictions.
Thisensures that a penalty is paid for diverging fromthe previous model?s predictions, which will betraded off against the benefit of reducing classi-fication loss.
We present a retrospective SSL forCRF, in that, the iterative learner keeps track of theerrors of the previous iterations so as to carry theproperties of both the source and target domains.
(II) Semantic Clustering.
A common prop-erty of several context-based word clustering tech-niques, e.g., Brown clustering (Brown et al,1992), Clustering by Committee (Pantel, 2003),etc., is that they mainly cluster based on local con-text such as nearby words.
Standard topic models,such as Latent Dirichlet Allocation (LDA) (Bleiet al, 2003), use a bag-of-words approach, whichdisregards word order and clusters words togetherthat appear in a similar global context.
Such mod-els have been effective in discovering lexicons inmany NLP tasks, e.g., named-entity recognition(Guo et al, 2009), word-sense disambiguation(Boyd-Graber et al, 2007; Li et al, 2010), syntac-tic/semantic parsing (Griffiths et al, 2005; Singhet al, 2010), speaker identification (Nyugen et al,2012), etc.
Recent topic models consider wordsequence information in documents (Griffiths etal., 2005; Moon et al, 2010).
The Hidden TopicMarkov Model (HTMM) by (Gruber et al, 2005),for instance, models sentences in documents asMarkov chains, assuming all words in a sentencehave the same topic.
While MTR has a similarMarkovian property, we encode features on wordsto allow each word in an utterance to sample fromany of the given semantic tags, as in ?what are[scary]genre movies by [Hitchcock]director?
?.In LDA, common words tend to dominate alltopics causing related words to end up in differ-ent topics.
In (Petterson et al, 2010), the vector-based features of words are used as prior informa-tion in LDA so that the words that are synonymsend up in same topic.
Thus, we build a seman-tically rich topic model, MTR, using word contextfeatures as side information.
Using a smoothingprior for each word-topic pair (instead of a con-stant ?
smoother), MTR assures that the words aredistributed over topics based on how similar theyare.
(e.g., ?scary?
and ?spooky?, which have sim-ilar context features, go into the same semantictag, ?genre?).
Thus, to best of our knowledge,MTR is the first topic model to incorporate wordfeatures while considering the sequence of words.3 Markov Topic Regression - MTR3.1 Model and AbstractionsLDA assumes that the latent topics of documentsare sampled independently from one of K topics.MTR breaks down this independence assumptionby allowing Markov relations between the hiddentags to capture the relations between consecutivewords (as sketched in Figure 1 and Algorithm 1).
(I) Semantic Tags (si): Each word wi of agiven utterance with Nj words, uj={wi}Nji=1?U ,j=1,..|U |, from a set of utterances U , is associatedwith a latent semantic tag (state) variable si?S,where S is the set of semantic tags.
We assume afixed K topics corresponding to semantic tags oflabeled data.
In a similar way to HTMM (Gruberet al, 2005) described for documents, MTR sam-ples each si from a Markov chain that is specificto its utterance uj .
Each state si generates a word,wi, based on the word-state co-occurrences.
MTRallows for sampling of consecutive words fromdifferent tag clusters.
The initial probabilities ofthe latent states are sampled from a Dirichlet dis-tribution over state variables, ?j , with ?
hyper-parameter for each uj .
(II) Tag Transition Indicator (?v): Given ut-terance uj , the decision to sample a wi from anew topic is determined by an indicator variable,cj,i, that is sampled from a Binomial(?v=wi) dis-tribution with a Beta conjugate prior.
(There are vbinomials for each vocabulary term.)
cj,i=1 sug-gests that a new state be sampled from K possibletags for the word wi in uj , and cj,i=0 suggests thatthe state si of wi should be the same as the previ-ous word?s latent state si?1.
The first position ofthe sequence is sampled from a new state, hencecj,i=1=1.
(III) Tag Transition Base Measure (?
): Priorprobability of a word given a tag should increasethe chances of sampling words from the correct se-mantic tag.
MTR constrains the generation of a tagsi given the previous tag si?1 and the current wibased on cj,i by using a vocabulary specific Betaprior, ?v?Beta(?v) 1, on each word in vocabularywv=1,..V .
We inject the prior information on se-mantic tags to define values of the base measure?v using external knowledge from two sources:(a) Entity Priors (?S): Prior probability onnamed-entities and descriptive tags denoted as1For each beta distribution we use symmetricBeta(?v)=Beta(?=?v ,?=?v).916latentsemantic tagdistribution oversemantic tagss1...w1...!j"c2c3#$kv%kvxv&ks2s3w2w3wn'VK|U|Vindicator forsamplingsemantic tagsvocabularyfeaturesas priorinformationsemantic tagdependentsmoothing coefficientsemantic tagindicatorparameterprior onper-wordstatetransitions$k!
Dir(%kv|x;&k)!k= exp(f(x;&k))semantic tagdistributionover tagssmoother fortag-wordpaircNjsNjFigure 1: The graph representation of the MarkovTopic Regression (MTR).
To demonstrate hiddenstate Markov Chain, the generation of each wordis explicitly shown (inside of the plate).?S=p(si|si?1,wi=v,wi?1).
We use web sources(wiki pages on movies and urls such as imdb.com)and labeled training data to extract entity lists thatcorrespond to the semantic tags of our domains.We keep the frequency of each n-gram to convertinto (empirical) prior probability distribution.
(b) Language Model Prior (?W ): Probabilitieson word transitions denoted as ?W=p(wi=v|wi?1).We built a language model using SRILM (Stol-cke, 2002) on the domain specific sources such astop wiki pages and blogs on online movie reviews,etc., to obtain the probabilities of domain-specificn-grams, up to 3-grams.
The observed priors, ?Sand ?W , are used for calculating the base measure?
for each vocabulary wv as:?si|si?1v ={?si|si?1,wi=vS , if ?si|si?1,wi=vS exists,?wi=v,wi?1W , otherwise (1)In Eq.
(1), we assume that the prior on the se-mantic tags, ?S , is more indicative of the deci-sion for sampling a wi from a new tag comparedto language model posteriors on word sequences,?W .
Here we represent the base-measure (hyper-parameter) of the semantic tag indicator variable,which is not to be confused with a probabilitymeasure 2We update the indicator parameter via mean cri-teria, ?v=wi=?Ki,j=1?si|sjv=wi /(K2).
If no prior on2The base-measure used in Eq.
(1) does not relate to aback-off model in LM sense.
Here, instead of using aconstant value for the hyper-parameters, we use probabilityscores that we obtain from LM.Algorithm 1 Markov Topic Regression1: for each semantic tag topic sk, k ?
1, ...,K do2: ?
draw a topic mixture ?k ?
Dir(?k|?k,x),3: ?
let ?k=exp(f(x;?k)); x={xv}Vlv=1, ?k?
RVl4: for each word wv in vocabulary v ?
1, ..., V do5: ?
draw a tag indicator mixture ?v ?
Beta(?
),6: for each utterance j ?
1, ..., |U | do7: ?draw transition distribution ?sj ?
Dir(?
)8: over states si and set cj1=1.9: ?for words wi in uj , i?
1, ..., Nj do10:  if i >1, toss a coin cj,i ?
Binomial(?wi).11:  If cj,i=1, draw si?Multi(?si,si?1j )?12: otherwise si=si?1.13:  Sample wi?Multi(?si ).?
Markov assumption over utterance words is used (See Eq.
(4)).a specific word exists, a default value is used forbase measure, ?v=0.01.
(IV) Topic-Word Distribution Priors (?k):Different from (Mimno et al, 2008), which usesasymmetric hyper-parameters on document-topicdistributions, in MTR, we learn the asymmetrichyper-parameters of the semantic tag-word distri-butions.
We use blocked Gibbs sampling, in whichthe topic assignments sk and hyper-parameters{?k}Kk=1 are alternately sampled at each Gibbssampling lag period g given all other variables.
Weimpose the prior knowledge on naturally relatedwords, such that if two words ?funny?
and ?hilar-ious?
indicate the same given ?genre?
class, thentheir latent tag distributions should also be simi-lar.
We enforce this on smoothing parameter ?k,v,e.g., ?k,?funny???k,?hilarious?
for a given tag k asfollows:At each g lag period of the Gibbs sampling, Klog-linear models with parameters, ?
(g)k ?RM , istrained to predict ?
(g)kv ?
?k, for each wv of a tagsk:?
(g)k = exp(f(xl;?
(g)k )) (2)where the log-linear function f is:n(g)kv = f(xlv;?
(g)k ) =?m?
(g)k,mxlv,m (3)Here x?RV?M is the input matrix x, whereinrows xv?RM represents M -dimensional scalarvector of explanatory features on vocabularywords.
We use the word-tag posterior probabili-ties obtained from a CRF sequence model trainedon labeled utterances as features.
The x={xl,xu}has labeled (l) and unlabeled (u) parts.
The labeledpart contains Vl size vocabulary of which we knowthe semantic tags, xl={(xl1,s1),...,(xlVl ,sVl)}.
Atthe start of the Gibbs sampling, we designate the917K latent topics to the K semantic tags of our la-beled data.
Therefore, we assign labeled words totheir designated topics.
This way we use observedscalar counts of each labeled word v associatedwith its semantic tag k, n(g)kv , as the output labelof its input vector, xlv; an indication of likelihoodof words getting sampled from the correspond-ing semantic label sk.
Since the impact of theasymmetric prior is equivalent to adding pseudo-counts to the sufficient statistics of the semantictag to which the word belongs, we predict thepseudo-counts ?
(g)kv using the scalar counts of thelabeled data, n(g)kv , based on the log-linear modelin Eq.
(2).
At g=0, we use ?
(0)kv =28, if xv?X l; oth-erwise ?
(0)kv =2?2, commonly used values for largeand small ?.
Note that larger ?-values indicatecorrelation between the word and the topic.3.2 Collapsed SamplerThe goal of MTR is to infer the degree of relation-ship between a word v and each semantic tag k,?kv.
To perform inference we need two compo-nents:?
a sampler which can draw from condi-tional PMTR(sji=k|sji?1, s\ji, ?, ?i, ?ji), whencj,i=1, where sji and sji?1 are the semantictags of the current wi=v of vocabulary v andprevious word wi?1 in utterance uj , and s\jiare the semantic tag topics of all words exceptfor wi; and,?
an estimation procedure for (?kv, ?k) (see?3.1).We integrate out the multinomial and binomial pa-rameters of the model: utterance-tag distributions?j , binomial state transition indicator distributionper each word ?v, and ?k for tag-word distribu-tions.
We use collapsed Gibbs sampling to re-duce random components and model the posteriordistribution by obtaining samples (sji, cj,i) drawnfrom this distribution.
Under the Markov assump-tion, for each word wi=v in a given utterance uj ,if cj,i=1, we sample a new tag si=k given theremaining tags and hyper-parameters ?k, ?, and?si|si?1wi=v .
Using the following parameters; n(si)ji ,which is the number of words assigned to a seman-tic class si=k excluding case i, and n(si?1)si is thenumber of transitions from class si?1 to si, whereindicator I(si?1, si)=1 if slot si=si?1, the updateequation is formulated as follows:p(sji = k|w, s?ji, ?, ?si|si?1wi ,?k) ?n(si)ji + ?kwin(k)(.)
+?v ?kv?
(n(si?1)si + ?)?
(n(si)si+1 + I(si?1, si) + I(si+1, si) + ?)n(si)(.)
+ I(si?1, k) +K?
(4)4 Semi-Supervised Semantic Labeling4.1 Semi Supervised Learning (SSL) withCRFIn (Subramanya et al, 2010), a new SSL methodis described for adapting syntactic POS tagging ofsentences in newswire articles along with searchqueries to a target domain of natural language(NL) questions.
They decode unlabeled queriesfrom target domain (t) using a CRF model trainedon the POS-labeled newswire data (source do-main (o)).
The unlabeled POS tag posteriors arethen smoothed using a graph-based learning algo-rithm.
On graph, the similarities are defined oversequences by constructing the graph over types,word 3-grams, where types capture the local con-text of words.
Since CRF tagger only uses lo-cal features of the input to score tag pairs, theytry to capture all the context with the graph withadditional context features on types.
Later, usingviterbi decoding, they select the 1-best POS tagsequence, s?j for each utterance uj .
Graph-basedSSL defines a new CRF objective function:?
(t)n+1 =argmin??RK{?
?j=1:llog p(sj |uj ; ?
(t)n ) + ???
(t)n ?2}?{?
?l+uj=l log pn(s?j |uj ; ?
(t)n )}(5)The first bracket in Eq.
(5) is the loss on the la-beled data and L2 regularization on parameters,?
(t)n , from nth iteration, same as standard CRF.The last term is the loss on unlabeled data fromtarget domain with a hyper-parameter ?
.
They usea small value for ?
to enable the new model to beas close as possible to the initial model trained onsource data.4.2 Retrospective Semi-Supervised CRFWe describe a Retrospective SSL (R-SSL) train-ing with CRF (Algorithm 2), using MTR as a918smoothing model, instead of a graph-based model,as follows:I. DECODING and SMOOTHING.
The poste-rior probability of a tag sji=k given a word wjiin unlabeled utterance uj from target domain (t)p?n(j, i)=p?n(sji=k|wji; ?
(t)n ), is decoded using then-th iteration CRF model.
MTR uses the decodedprobabilities as semantic tag prior features on vo-cabulary items.
We generate a word-tag matrix ofposteriors, x?
(0, 1)V?K , where K is the numberof semantic tags and V is the vocabulary size fromn-th iteration.
Each row is aK dimensional vectorof tag posterior probabilities xv={xv1,.
.
.
xvK} onthe vocabulary term, wv.
The labeled rows xl ofthe vocabulary matrix, x={xl,xu}, contain only{0,1} values, indicating the word?s observed se-mantic tags in the labeled data.
Since a labeledterm wv can have different tags (e.g., ?clint east-wood?
may be tagged as actor-name and director-name in the training data), ?Kk xvk?1 holds.
Thex is used as the input matrix of the kth log-linearmodel (corresponding to kth semantic tag (topic))to infer the ?
hyper-parameter of MTR in Eq.
(2).MTR generates smoothed conditional probabilities?kv for each vocabulary term v given semantic tagk.II.
INTERPOLATION.
For each word wji=vin unlabeled utterance uj , we interpolate tagmarginals from CRF and MTR for each semantictag sji = k:q?n(sji|wij ; ?
(t)n ) = piCRF posterior?
??
?p?n(sji|wij ; ?
(t)n )+(1?
pi)MTR????
?kv (6)III.
VITERBI.
Using viterbi decoding overthe tag marginals, q?n(sji|wij ; ?
(t)n ), and transitionprobabilities obtained from the CRF model of n-thiteration, we get p?n(s?j |uj ; ?
(t)n ), the 1-best decodes?j of each unlabeled utterance uj?Uun .IV.
RETROSPECTIVE SSL (R-SSL).
Afterwe decode the unlabeled data, we re-train a newCRF model at each iteration.
Each iteration makespredictions on the semantic tags of unlabeled datawith varying posterior probabilities.
Motivated by(Lavoie et al, 2011), we want the loss function tohave a dependency on the prior model predictions.Thus, R-SSL encodes the history of the prior pre-Algorithm 2 Retrospective Semi-Supervised CRFInput: Labeled U l, and unlabeled Uu data.Process: ?
(o)n =crf-train(Ul) at n=0, n=n+1 ?.While not convergedp?=posterior-decode(Uun ,?
(o)n )?=smooth-posteriors(p?)
using MTR,q?=interpolate-posteriors(p?,?),Uun=viterbi-decode(q?)?
(t)n+1=crf-retrospective(U l, Uun ,.
.
.
,Uu1 ,?
(t)n )?
(n):iteration, (t):target, (o):source domains.dictions, as follows:?
(t)n+1 =argmin??RK{?
?j=1:llog p(sj |uj ; ?
(t)n ) + ???
(t)n ?2}{?
?j=1:(l+u)max{0, p??
?n }}(7)where, p??
?n =1 ?
log hn(uj)p?n(s?j |uj ; ?
(t)n ).
Thefirst two terms are same as standard CRF.
Thelast term ensures that the predictions of the cur-rent model have the same sign as the predic-tions of the previous models (using labeled andunlabeled data), denoted by a maximum marginhinge weight, hn(uj)= 1n?1?n?11 p?n(s?j |uj ; ?
(t)n ).It should also be noted that with MTR, the R-SSLlearns the word-tag relations by using features thatdescribe the words in context, eliminating the needfor additional type representation of graph-basedmodel.
MTR provides a separate probability dis-tribution ?j over tags for each utterance j, implic-itly allowing for the same word v in separate utter-ances to differ in tag posteriors ?kv.5 Experiments5.1 Datasets and Tagsets5.1.1 Semantic Tagging DatasetsWe focus here on audiovisual media in the moviedomain.
The user is expected to interact by voicewith a system than can perform a variety of taskssuch as browsing, searching, querying informa-tion, etc.
To build initial NLU models for sucha dialog system, we used crowd-sourcing to col-lect and annotate utterances, which we considerour source domain.
Given movie domain-specifictasks, we asked the crowd about how they would919interact with the media system as if they were talk-ing to a person.Our data from target domain is internally col-lected from real-use scenarios of our spoken dia-log system.
The transcribed text forms of these ut-terances are obtained from speech recognition en-gine.
Although the crowd-sourced data is similarto target domain, in terms of pre-defined user in-tentions, the target domain contains more descrip-tive vocabulary, which is almost twice as large asthe source domain.
This causes data-mismatch is-sues and hence provides a perfect test-bed for adomain adaptation task.
In total, our corpus hasa 40K semantically tagged utterances from eachsource and target domains.
There are around 15named-entity and 10 descriptive tags.
We sep-arated 5K utterances to test the performance ofthe semantic tagging models.
The most frequententities are: movie-director (?James Cameron?
),movie-title (?Die Hard?
), etc.
; whereas top de-scriptive tags are: genre (?feel good?
), description(?black and white?, ?pg 13?
), review-rate (?epic?,?not for me?
), theater-location (?near me?,?citycenter?
), etc.Unlabeled utterances similar to the movie do-main are pulled from a month old web query logsand extracted over 2 million search queries fromwell-known sites, e.g., IMDB, Netflix, etc.
Wefiltered queries that are similar to our target setthat start with wh-phrases (?what?, ?who?, etc.)
aswell as imperatives ?show?, ?list?, etc.
In addition,we extracted web n-grams and entity lists (see ?3)from movie related web sites, and online blogs andreviews.
We collected around 300K movie reviewand blog entries on the entities observed in ourdata.
We extract prior distributions for entities andn-grams to calculate entity list ?
and word-tag ?priors (see ?3.1).5.1.2 Syntactic Tagging DatasetsWe use the Wall Street Journal (WSJ) section ofthe Penn Treebank as our labeled source data.
Fol-lowing previous research, we train on sections 00-18, comprised of 38,219 POS-tagged sentences.To evaluate the domain adaptation (DA) approachand to compare with results reported by (Subra-manya et al, 2010), we use the first and secondhalf of QuestionBank (Judge et al, 2006) as ourdevelopment and test sets (target).
The Question-Bank contains 4000 POS-tagged questions, how-ever it is difficult to tag with WSJ-trained tag-gers because the word order is different than WSJand contains a test-set vocabulary that is twiceas large as the one in the development set.
Asfor unlabeled data we crawled the web and col-lected around 100,000 questions that are similarin style and length to the ones in QuestionBank,e.g.
?wh?
questions.
There are 36 different tagsets in the Penn dataset which includes tag la-bels for verbs, nouns, adjectives, adverbs, modal,determiners, prepositions, etc.
More informationabout the Penn Tree-bank tag set can be found here(Marcus et al, 1993).5.2 ModelsWe evaluated several baseline models on twotasks:5.2.1 Semantic ClusteringSince MTR provides a mixture of propertiesadapted from earlier models, we present perfor-mance benchmarks on tag clustering using: (i)LDA; (ii) Hidden Markov Topic Model HMTM(Gruber et al, 2005); and, (iii) w-LDA (Pettersonet al, 2010) that uses word features as priors inLDA.
When a uniform ?
hyper-parameter is usedwith no external information on the state transi-tions in MTR, it reduces to a HMTM model.
Sim-ilarly, if no Markov properties are used (bag-of-words), MTR reduces to w-LDA.
Each topic modeluses Gibbs sampling for inference and parameterlearning.
We sample models for 1000 iterations,with a 500-iteration burn-in and a sampling lag of10.
For testing we iterated the Gibbs sampler us-ing the trained model for 10 iterations on the test-ing data.5.2.2 SSL for Semantic/Syntactic TaggingWe evaluated three different baselines against ourSSL models:?
CRF: a standard supervised sequence tagging.?
Self-CRF: a wrapper method for SSL usingself-training.
First a supervised learning algorithmis used to build a CRF model based on the labeleddata.
A CRF model is used to decode the unla-beled data to generate more labeled examples forre-training.?
SSL-Graph: A SSL model presented in (Sub-ramanya et al, 2010) that uses graph-based learn-ing as posterior tag smoother for CRF model usingEq.
(5).In addition to the three baseline, we evaluatedthree variations of our SSL method:?
SSL-MTR: Our first version of SSL uses MTR to920LDA w-LDA HMTM MTR0.60.70.80.982%77%84%82%79%78%74% ?
Descriptive Tags Named-Entities All TagsF-MeasureFigure 2: F-measure for semantic clustering per-formance.
Performance differences for three dif-ferent baseline models and our MTR approach bydifferent semantic tags.smooth the semantic tag posteriors of a unlabeleddata decoded by the CRF model using Eq.(5).?
R-SSL-Graph: Our second version usesgraph-learning to smooth the tag posteriors and re-train a new CRF model using retrospective SSL inEq.(7).?
R-SSL-MTR: Our full model uses MTR as aBayesian smoothing model, and retrospective SSLin Eq.
(7) for iterative CRF training.For all the CRF models, we use lexical fea-tures consisting of unigrams in a five-word win-dow around the current word.
To include contex-tual information, we add binary features for allpossible tags.
We inject dictionary constraints toall CRF models, such as features indicating labelprior information.
For each model we use sev-eral named entity features, e.g., movie-title, actor-name, etc., non-named entity (descriptive) fea-tures, e.g., movie-description, movie-genre, anddomain independent dictionaries, e.g, time, loca-tion, etc.
For graph-based learning, we imple-mented the algorithm presented in (Subramanyaet al, 2010) and used the same hyper-parametersand features.
For the rest of the hyper-parameters,we used: ?=0.01 for MTR, pi=0.5 for interpolationmixing.
These parameters were chosen based onthe performance of the development set.
All CRFobjective functions were optimized using Stochas-tic Gradient Descent.5.3 Results and Discussions5.3.1 Experiment 1: Clustering SemanticTags.Here, we want to demonstrate the performanceof MTR model for capturing relationships betweenwords and semantic tags against baseline topicmodels: LDA, HMTM, w-LDA.
We take the se-mantically labeled utterances from the movie tar-get domain and use the first half for training andthe rest for performance testing.
We use all thecollected unlabeled web queries from the moviedomain.
For fair comparison, each benchmarktopic model is provided with prior information onword-semantic tag distributions based on the la-beled training data, hence, each K latent topic isassigned to one of K semantic tags at the begin-ning of Gibbs sampling.We evaluate the performance separately on de-scriptive tags, named-entities, and all tags to-gether.
The performance of the four topic modelsare reported in Figure 2.
LDA shows the worst per-formance, even though some supervision is pro-vided by way of labeled semantic tags.
Althoughw-LDA improves semantic clustering performanceover LDA, the fact that it does not have Markovproperties makes it fall short behind MTR.
As forthe effect of word features in MTR, we see a 3%absolute performance gain over the second bestperforming HMTM baseline on named-entity tags,a 1% absolute gain on descriptive tags and a 2%absolute overall gain.
As expected, we see a dropin F-measure on all models on descriptive tags.5.3.2 Experiment 2: Domain AdaptationTask.We compare the performance of our SSL modelto that of state-of-the-art models on semantic andsyntactic tagging.
Each SSL model is built us-ing labeled training data from the source do-main and unlabeled training data from target do-main.
In Table 2 we show the results on Movieand QuestionBank target test datasets.
The re-sults of SSL-Graph on QuestionBank is takenfrom (Subramanya et al, 2010).
The self-training model, Self-CRF adds 3% improve-ment over supervised CRF models on movie do-main, but does not improve syntactic tagging.
Be-cause it is always inherently biased towards thesource domain, self-training tends to reinforcethe knowledge that the supervised model alreadyhas.
SSL-Graph works much better for bothsyntactic and semantic tagging compared to CRFand Self-CRF models.
Our Bayesian MTR ef-ficiently extracts information from the unlabeleddata for the target domain.
Combined with retro-spective training, R-SSL-MTR demonstrates no-ticeable improvements, ?2% on descriptive tags,and 1% absolute gains in overall semantic tag-921ging performance over SSL-Graph.
On syntac-tic tagging, the two retrospective learning modelsis comparable, close to 1% improvement over theSSL-Graph and SSL-MTR.Movie Domain QBankModel Desc.
NE All POSCRF 75.05 75.84 75.84 83.80Self-CRF 78.96 79.53 79.19 84.00SSL-Graph 80.27 81.35 81.23 86.80SSL-MTR 79.87 79.31 79.19 86.30R-SSL-Graph 80.58 81.95 81.52 87.12R-SSL-MTR 82.76 82.27 82.24 87.34Table 2: Domain Adaptation performancein F-measure on Semantic Tagging onMovie Target domain and POS tagging onQBank:QuestionBank.
Best performing modelsare bolded.5.3.3 Experiment 3: Analysis of SemanticDisambiguation.Here we focus on the accuracy of our models intagging semantically ambiguous words.
We inves-tigate words that have more than one observed se-mantic tag in training data, such as ?are there any[war]genre movies available.
?, ?remove all moviesabout [war]description.?).
Our corpus contained30,000 unique vocabulary, 55% of which are con-tained in one or more semantic categories.
Only6.5% of those are tagged as multiple categories(polysemous), which are the sources of semanticambiguity.
Table-3 shows the precision of two bestmodels for most confused words.We compare our two best SSL models with dif-ferent smoothing regularizes: R-SSL-MTR (MTR)and R-SSL-Graph (GRAPH).
We use preci-sion and recall criterion on semantically confusedwords.In Table 3 we show two most frequent descrip-tive tags; genre and description, and commonlymisclassified words by the two models.
Resultsindicate that the R-SSL-MTR, performs betterthan the R-SSL-Graph, in activating the correctmeaning of a word.
The results indicate that incor-porating context information with MTR is an effec-tive option for identifying semantic ambiguity.6 ConclusionsWe have presented a novel semi supervised learn-ing approach using a probabilistic clusteringgenre descriptionVocab.
GRAPH MTR GRAPH MTRwar 50% 100% 75% 88%popular 90% 89% 80% 100%kids 78% 86% ?
100%crime 49% 80% 86% 67%zombie 67% 89% 67% 86%Table 3: Classification performance in F-measurefor semantically ambiguous words on the most fre-quently confused descriptive tags in the movie do-main.method to semantically tag spoken language ut-terances.
Our results show that encoding priorson words and context information contributes sig-nificantly to the performance of semantic cluster-ing.
We have also described an efficient iterativelearning model that can handle data inconsisten-cies that leads to performance increases in seman-tic and syntactic tagging.As a future work, we will investigate using ses-sion data, namely the entire dialog between thehuman and the computer.
Rather than using sin-gle turn utterances, we hope to utilize the con-text information, e.g., information from previousturns for improving the performance of the seman-tic tagging of the current turns.ReferencesD.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research.J.
Boyd-Graber, D. Blei, and X. Zhu.
2007.
Atopic model for word sense disambiguation.
Proc.EMNLP.P.F.
Brown, V.J.D.
Pietra, P.V.
deSouza, and J.C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.O.
Chapelle, B. Schlkopf, and Alexander Zien.
2006.Semi-supervised learning.
MIT Press.H.
Daume?-III.
2010.
Frustratingly easy semi-supervised domain adaptation.
Proc.
Workshop onDomain Adaptation for Natural Language Process-ing at ACL.T.L Griffiths, M. Steyvers, D.M.
Blei, and J.M.
Tenen-baum.
2005.
Integrating topics and syntax.
Proc.
ofNIPS.A.
Gruber, M. Rosen-Zvi, and Y. Weiss.
2005.
Hiddentopic markov models.
Proc.
of ICML.H.
Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su.2009.
Domain adaptation with latent semantic asso-ciation for named entity recognition.
Proc.
NAACL.922J.
Judge, A. Cahill, and J.Van Genabith.
2006.Question-bank: Creating corpus of parse-annotatedquestions.
Proc.
Int.
Conf.
Computational Linguis-tics and ACL.A.
Lavoie, M.E.
Otey, N. Ratliff, and D. Sculley.
2011.History dependent domain adaptation.
Proc.
NIPSWorkshop on Domain Adaptation.X.
Li, Y.-Y.
Wang, and A. Acero.
2009.
Extractingstructured information from user queries with semi-supervised conditional random fields.
Proc.
of SI-GIR.L.
Li, B. Roth, and C. Sporleder.
2010.
Topic mod-els for word sense disambiguation and token-basedidiom detection.
Proc.
ACL.X.
Li.
2010.
Understanding semantic structure of nounphrase queries.
Proc.
ACL.J Liu, X. Li, A. Acero, and Ye-Yi Wang.
2011.
Lex-icon modeling for query understanding.
Proc.
ofICASSP.M.
P. Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguis-tics, 27:1?30.D.
Mimno, W. Li, and A. McCallum.
2008.Topic models conditioned on arbitrary features withdirichlet-multinomial regression.
Proc.
UAI.T.
Moon, K. Erk, and J. Baldridge.
2010.
Crouch-ing dirichlet, hidden markov model: Unsupervisedpos tagging with context local tag generation.
Proc.ACL.V.-A.
Nyugen, J. Boyd-Graber, and P. Resnik.
2012.Sits: A hierarchical nonparametric model usingspeaker identity for topic segmentation in multipartyconversations.
Proc.
ACL.P.
Pantel.
2003.
Clustering by committee.
Ph.D. The-sis, University of Alberta, Edmonton, Alta., Canada.J.
Petterson, A. Smola, T. Caetano, W. Buntine, andS.
Narayanamurthy.
2010.
Word features for latentdirichlet alocation.
In Proc.
NIPS.J.
Reisinger and R. Mooney.
2011.
Cross-cutting mod-els of lexical semantics.
In Proc.
of EMNLP.S.
Singh, D. Hillard, and C. Leggetter.
2010.Minimally-supervised extraction of entities fromtext advertisements.
Proc.
NAACL-HLT.A.
Stolcke.
2002.
An extensible language modelingtoolkit.
Proc.
Interspeech.A.
Subramanya, S. Petrov, and F. Pereira.
2010.
Effi-cient graph-based semi-supervised learning of struc-tured tagging models.
In Proc.
EMNLP.G.
Tur and R. DeMori.
2011.
Spoken language under-standing: Systems for extracting semantic informa-tion from speech.
Wiley Press.Y.-Y.
Wang, R. Hoffman, X. Li, and J. Syzmanski.2009.
Semi-supervised learning of semantic classesfor query understanding from the web and for theweb.
In The 18th ACM Conference on Informationand Knowledge Management.X.
Zhu.
2005.
Semi-supervised learning litera-ture survey.
Technical Report 1530, University ofWisconsin-Madison.923
