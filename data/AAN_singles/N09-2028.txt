Proceedings of NAACL HLT 2009: Short Papers, pages 109?112,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing Integer Linear Programming for Detecting Speech DisfluenciesKallirroi GeorgilaInstitute for Creative Technologies, University of Southern California13274 Fiji Way, Marina del Rey, CA 90292, USAkgeorgila@ict.usc.eduAbstractWe present a novel two-stage technique for de-tecting speech disfluencies based on IntegerLinear Programming (ILP).
In the first stagewe use state-of-the-art models for speech dis-fluency detection, in particular, hidden-eventlanguage models, maximum entropy modelsand conditional random fields.
During testingeach model proposes possible disfluency la-bels which are then assessed in the presence oflocal and global constraints using ILP.
Our ex-perimental results show that by using ILP wecan improve the performance of our modelswith negligible cost in processing time.
Theless training data is available the larger the im-provement due to ILP.1 IntroductionSpeech disfluencies (also known as speech repairs)occur frequently in spontaneous speech and can posedifficulties to natural language processing (NLP)since most NLP tools (e.g.
parsers, part-of-speechtaggers, information extraction modules) are tradi-tionally trained on written language.
Speech dis-fluencies can be divided into three intervals, thereparandum, the editing term and the correction(Heeman and Allen, 1999; Liu et al, 2006).
(it was) * (you know) it was setIn the above example, ?it was?
is the reparandum,?you know?
is the editing term and the remainingsentence is the correction.
The asterisk marks the in-terruption point at which the speaker halts the origi-nal utterance in order to start the repair.
The edit-ing term is optional and consists of one or morefilled pauses (e.g.
uh, uh-huh) or discourse mark-ers (e.g.
you know, so).
Some researchers includeediting terms in the definition of disfluencies.
Herewe focus only on detecting repetitions (the speakerrepeats some part of the utterance), revisions (thespeaker modifies the original utterance) or restarts(the speaker abandons an utterance and starts over).We also deal with complex disfluencies, i.e.
a seriesof disfluencies in succession (?I think I think uh Ibelieve that...?
).In previous work many different approaches todetecting speech disfluencies have been proposed.Different types of features have been used, e.g.
lexi-cal features only, acoustic and prosodic features onlyor a combination of both (Liu et al, 2006).
Fur-thermore, a number of studies have been conductedon human transcriptions while other efforts havefocused on detecting disfluencies from the speechrecognition output.In this paper we propose a novel framework forspeech disfluency detection based on Integer Lin-ear Programming (ILP).
With Linear Programming(LP) problems the goal is to optimize a linear ob-jective function subject to linear equality and linearinequality constraints.
When some or all the vari-ables of the objective function and the constraintsare non-negative integers, LP becomes ILP.
ILP hasrecently attracted much attention in NLP.
It has beenapplied to several problems including sentence com-pression (Clarke and Lapata, 2008) and relation ex-traction (Roth and Yih, 2004).
Some of these meth-ods (e.g.
(Roth and Yih, 2004)) follow the two-stageapproach of first hypothesizing a list of possible an-swers using a classifier and then selecting the bestanswer by applying ILP.
We have adopted this two-stage approach and applied it to speech disfluencydetection.In the first stage we use state-of-the-art tech-109niques for speech disfluency detection, in particular,Hidden-Event Language Models (HELMs) (Stolckeand Shriberg, 1996), Maximum Entropy (ME) mod-els (Ratnaparkhi, 1998) and Conditional RandomFields (CRFs) (Lafferty et al, 2001).
Nevertheless,any other classification method could be used in-stead.
During testing each classifier proposes pos-sible labels which are then assessed in the presenceof local and global constraints using ILP.
ILP makesthe final decision taking into account both the con-straints and the output of the classifier.In the following we use the Switchboard corpusand only lexical features for training our 3 classi-fiers.
Then we apply ILP to the output of each clas-sifier.
Our goal is not to investigate the best setof features or achieve the best possible results.
Inthat case we could also use prosodic features as theyhave been shown to improve performance.
Our tar-get is to show that by using ILP we can improve withnegligible cost in processing time the performanceof state-of-the-art techniques, especially when notmuch training data is available.The novelty of our work lies in the two follow-ing areas: First, we propose a novel approach fordetecting disfluencies with improvements over state-of-the-art models (HELMs, ME models and CRFs)that use similar lexical features.
Although the two-stage approach is not unique, as discussed above,the formulation of the ILP objective function andconstraints for disfluency detection is entirely novel.Second, we compare our models using the tasks ofboth detecting the interruption point and finding thebeginning of the reparandum.
In previous work (Liuet al, 2006) Hidden Markov Models (combinationof decision trees and HELMs) and ME models weretrained to detect the interruption points and thenheuristic rules were applied to find the correct on-set of the reparandum in contrast to CRFs that weretrained to detect both points at the same time.The structure of the paper is as follows: In sec-tion 2 we describe our data set.
In section 3 we de-scribe our approach in detail.
Then in section 4 wepresent our experiments and provide results.
Finallyin section 5 we present our conclusion and proposefuture work.2 Data SetWe use Switchboard (LDC catalog LDC99T42),which is traditionally used for speech disfluency ex-periments.
We transformed the Switchboard annota-tions into the following format:I BE was IE one IP I was rightBE (beginning of edit) is the point where thereparandum starts and IP is the interruption point(the point before the repair starts).
In the above ex-ample the beginning of the reparandum is the firstoccurrence of ?I?, the interruption point appears af-ter ?one?
and every word between BE and IP istagged as IE (inside edit).
Sometimes BE and IPoccur at the same point, e.g.
?I BE-IP I think?.The number of occurrences of BE and IP in ourtraining set are 34387 and 39031 respectively, in ourdevelopment set 3146 and 3499, and in our test set6394 and 7413.3 MethodologyIn the first stage we train our classifier.
Any clas-sifier can be used as long as it provides more thanone possible answer (i.e.
tag) for each word in theutterance.
Valid tags are BE, BE-IP, IP, IE or O. TheO tag indicates that the word is outside the disflu-ent part of the utterance.
ILP will be applied to theoutput of the classifier during testing.Let N be the number of words of each utter-ance and i the location of the word in the utterance(i=1,...,N ).
Also, let CBE(i) be a binary variable (1or 0) for the BE tag.
Its value will be determinedby ILP.
If it is 1 then the word will be tagged asBE.
In the same way, we use CBE?IP (i), CIP (i),CIE(i), CO(i) for tags BE-IP, IP, IE and O respec-tively.
Let PBE(i) be the probability given by theclassifier that the word is tagged as BE.
In the sameway, let PBE?IP (i), PIP (i), PIE(i), PO(i) be theprobabilities for tags BE-IP, IP, IE and O respec-tively.
Given the above definitions, the ILP problemformulation can be as follows:max[?Ni=1[PBE(i)CBE(i) + PBE?IP (i)CBE?IP (i)+PIP (i)CIP (i) + PIE(i)CIE(i) + PO(i)CO(i)]](1)subject to:CBE(i) + CBE?IP (i) + CIP (i) + CIE(i)+CO(i) = 1 ?i ?
(1, ..., N) (2)CBE(1) + CBE?IP (1) + CO(1) = 1 (3)CBE?IP (N) + CIP (N) + CO(N) = 1 (4)CBE(i)?
CBE?IP (i?
1)?
CIP (i?
1)?CO(i?
1) ?
0 ?i ?
(2, ..., N) (5)1?
CBE(i)?
CBE(i?
1) ?
0 ?i ?
(2, ..., N) (6)110Equation 1 is the linear objective function that wewant to maximize, i.e.
the overall probability of theutterance.
Equation 2 says that each word can haveone tag only.
Equation 3 denotes that the first word iseither BE, BE-IP or O.
Equation 4 says that the lastword is either BE-IP, IP or O.
For example the lastword cannot be BE because then we would expect tosee an IP.
Equation 5 defines the transitions that areallowed between tags as described in Table 1 (firstrow).
Equation 5 says that if we have a word taggedas BE it means that the previous word was tagged asBE-IP or IP or O.
It could not have been tagged asIE because IE must be followed by an IP before anew disfluency starts.
Also, it could not have beenBE because then we would expect to see an IP.
FromTable 1 we can easily define 4 more equations for therest of the tags.
Finally, equation 6 denotes that wecannot transition from BE to BE (we need an IP inbetween).We also formulate some additional rules thatdescribe common disfluency patterns.
First, let ushave an example of a long-context rule.
If we havethe sequence of words ?he was the one um you knowshe was the one?, we expect this to be tagged as ?heBE was IE the IE one IP um O you O know O she Owas O the O one O?, if we do not take into accountthe context in which this pattern occurs.
We incor-porate this rule into our ILP problem formulation asfollows: Let (w1,...,wN ) be a sequence of N wordswhere both w2 and wN?7 are personal pronouns,the word sequence w3,w4,w5 is the same as thesequence wN?6,wN?5,wN?4 and all the words inbetween (w6,...,wN?8) are filled pauses or discoursemarkers.
Then the probabilities given by the classi-fier are modified as follows: PBE(2)=PBE(2)+th1,PIE(3)=PIE(3)+th2, PIE(4)=PIE(4)+th3 andPIP (5)=PIP (5)+th4, where th1, th2, th3 and th4are empirically set thresholds (between 0.5 and 1,using the development set of the corpus).Now, here is an example of a short-context rule.If we have the same word appear 3 times in a row(?do do do?)
we expect this to be tagged as ?do BE-IP do IP do O?.
To incorporate this rule into our ILPproblem formulation we can modify the probabili-ties given by the classifier accordingly.In total we have used 7 rules that deal with short-context and 5 rules that deal with long-context de-pendencies.
From now on we will refer to the modelthat uses all rules (general ILP formulation and allpattern-based rules) as ILP and to the model thatFrom Tag To TagBE-IP or IP or O BEBE-IP or IP or O BE-IPBE or BE-IP or IP or IE IPBE or BE-IP or IP or IE IEBE-IP or IP or O OTable 1: Possible transitions between tags.uses only the general ILP constraints and the short-context pattern-based rules as ILP-.
In all rules, wecan skip editing terms (see example above).4 ExperimentsFor HELMs we use the SRI Statistical LanguageModeling Toolkit.
Each utterance is a sequence ofword and Part-of-Speech (POS) pairs fed into thetoolkit: i/prp BE was/vbd IE one/cd IPi/prp was/vbd right/jj.
We report resultswith 4-grams.
For ME we use the OpenNLP Max-Ent toolkit and for CRFs the toolkit CRF++ (bothavailable from sourceforge).
We experimentedwith different sets of features and we achieved thebest results with the following setup (i is the loca-tion of the word or POS in the sentence): Our wordfeatures are ?wi?, ?wi+1?, ?wi?1, wi?, ?wi, wi+1?,?wi?2, wi?1, wi?, ?wi, wi+1, wi+2?.
Our POS fea-tures have the same structure as the word features.For ILP we use the lp solve software also avail-able from sourceforge.For evaluating the performance of our models weuse standard metrics proposed in the literature, i.e.F-score and NIST Error Rate.
We report results forBE and IP.
F-score is the harmonic mean of preci-sion and recall (we equally weight precision and re-call).
Precision is computed as the ratio of the cor-rectly identified tags X to all the tags X detected bythe model (where X is BE or IP).
Recall is the ra-tio of the correctly identified tags X to all the tagsX that appear in the reference utterance.
The NISTError Rate measures the average number of incor-rectly identified tags per reference tag, i.e.
the sumof insertions, deletions and substitutions divided bythe total number of reference tags (Liu et al, 2006).To calculate the level of statistical significance wealways use the Wilcoxon signed-rank test.Table 2 presents comparative results between ourmodels.
The ILP and ILP- models lead to signif-icant improvements compared to the plain modelsfor HELMs and ME (p<10?8, plain models vs. ILPand ILP-).
With CRFs the improvement is smaller,111BE IPF-score Error F-score Error4gram 60.3 54.8 67.0 50.74gram ILP 76.0 38.1 79.0 38.04gram ILP- 73.9 39.5 77.9 38.3ME 63.8 52.6 72.8 44.3ME ILP 77.9 36.3 80.8 35.4ME ILP- 75.6 37.2 81.0 33.7CRF 78.6 34.3 82.0 31.7CRF ILP 80.1 34.5 82.5 33.3CRF ILP- 79.8 33.5 83.4 30.5Table 2: Comparative results between our models.25% 50% 75% 100%4gram 59.8 56.6 56.2 54.84gram ILP 40.2 38.9 38.2 38.04gram ILP- 42.1 40.7 39.8 39.5ME 61.6 56.9 54.7 52.6ME ILP 38.5 37.7 36.5 36.3ME ILP- 39.7 38.7 37.6 37.2CRF 40.3 37.1 35.5 34.3CRF ILP 37.1 36.2 35.2 34.5CRF ILP- 36.6 35.5 34.4 33.5Table 3: Error rate variation for BE depending on thetraining set size.p<0.03 (CRF vs. CRF with ILP), not significant(CRF vs. CRF with ILP-), p<0.0008 (CRF with ILPvs.
CRF with ILP-).
HELMs and ME models ben-efit more from the ILP model than the ILP- model(ME only for the BE tag) whereas ILP- appears toperform better than ILP for CRFs.Table 3 shows the effect of the training set size onthe error rates only for BE due to space restrictions.The trend is similar for IP.
The test set is always thesame.
Both ILP and ILP- perform better than theplain models.
This is true even when the ILP andILP- models are trained with less data (HELMs andME models only).
Note that HELM (or ME) withILP or ILP- trained on 25% of the data performs bet-ter than plain HELM (or ME) trained on 100% of thedata (p<10?8).
This is very important because col-lecting and annotating data is expensive and time-consuming.
Furthermore, for CRFs in particular thetraining process takes long especially for large datasets.
In our experiments CRFs took about 400 iter-ations to converge (approx.
136 min for the wholetraining set) whereas ME models took approx.
48min for the same number of iterations and trainingset size.
Also, ME models trained with 100 iter-ations (approx.
11 min) performed better than MEmodels trained with 400 iterations.
The cost of ap-plying ILP is negligible since the process is fast andapplied during testing.5 ConclusionWe presented a novel two-stage technique for de-tecting speech disfluencies based on ILP.
In the firststage we trained HELMs, ME models and CRFs.During testing each classifier proposed possible la-bels which were then assessed in the presence of lo-cal and global constraints using ILP.
We showed thatILP can improve the performance of state-of-the-artclassifiers with negligible cost in processing time,especially when not much training data is available.The improvement is significant for HELMs and MEmodels.
In future work we will experiment withacoustic and prosodic features and detect disfluen-cies from the speech recognition output.AcknowledgmentsThis work was sponsored by the U.S. Army Re-search, Development, and Engineering Command(RDECOM).
The content does not necessarily re-flect the position or the policy of the Government,and no official endorsement should be inferred.ReferencesJ.
Clarke and M. Lapata.
2008.
Global inference forsentence compression: An integer linear programmingapproach.
Journal of Artificial Intelligence Research,31:399?429.P.
Heeman and J. Allen.
1999.
Speech repairs, in-tonational phrases and discourse markers: Modelingspeakers?
utterances in spoken dialogue.
Computa-tional Linguistics, 25:527?571.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.Y.
Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten-dorf, and M. Harper.
2006.
Enriching speech recogni-tion with automatic detection of sentence boundariesand disfluencies.
IEEE Trans.
Audio, Speech and Lan-guage Processing, 14(5):1526?1540.A.
Ratnaparkhi.
1998.
Maximum Entropy Models fornatural language ambiguity resolution.
Ph.D. thesis,University of Pennsylvania.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InProc.
of CoNNL.A.
Stolcke and E. Shriberg.
1996.
Statistical languagemodeling for speech disfluencies.
In Proc.
of ICASSP.112
