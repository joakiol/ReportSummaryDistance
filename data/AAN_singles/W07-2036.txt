Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173?176,Prague, June 2007. c?2007 Association for Computational LinguisticsHIT: Web based Scoring Method for English Lexical SubstitutionShiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng LiInformation Retrieval Laboratory, School of Computer Science and Technology,Box 321, Harbin Institute of TechnologyHarbin, P.R.
China, 150001{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cnAbstractThis paper describes the HIT system and itsparticipation in SemEval-2007 EnglishLexical Substitution Task.
Two main stepsare included in our method: candidate sub-stitute extraction and candidate scoring.
Inthe first step, candidate substitutes for eachtarget word in a given sentence are ex-tracted from WordNet.
In the second step,the extracted candidates are scored andranked using a web-based scoring method.The substitute ranked first is selected as thebest substitute.
For the multiword subtask,a simple WordNet-based approach is em-ployed.1 IntroductionLexical substitution aims to find alternative wordsthat can occur in given contexts.
It is important inmany applications, such as query reformulation inquestion answering, sentence generation, andparaphrasing.
There are two key problems in thelexical substitution task, the first of which iscandidate substitute extraction.
Generally speaking,synonyms can be regarded as candidate substitutesof words.
However, some looser lexicalrelationships can also be considered, such asHypernyms and Hyponyms defined in WordNet(Fellbaum, 1998).
In addition, since lexicalsubstitution is context dependent, some wordswhich do not have similar meanings in generalmay also be substituted in some certain contexts(Zhao et al, 2007).
As a result, finding a lexicalknowledge base for substitute extraction is achallenging task.The other problem is candidate scoring andranking according to given contexts.
In the lexicalsubstitution task of SemEval-2007, context is con-strained as a sentence.
The system therefore has toscore the candidate substitutes of each target wordusing the given sentence.
The following questionsshould be considered here: (1) What words in thegiven sentence are ?useful?
context?
(2) How tocombine the context words and use them in rank-ing candidate substitutes?
For the first question, wecan use all words of the sentence, words in a win-dow, or words having syntactic relations with thetarget word.
For the second question, we can re-gard the context words as ?bag of words?, n-grams,or syntactic structures.In HIT, we extract candidate substitutes fromWordNet, in which both synonyms and hypernymsare investigated (Section 3.1).
After that, we scorethe candidates using a web-based scoring method(Section 3.2).
In this method, we first select frag-ments containing the target word from the givensentence.
Then we construct queries by replacingthe target word in the fragments with the candidatesubstitute.
Finally, we search Google using theconstructed queries and score each candidate basedon the counts of retrieved snippets.The rest of this paper is organized as follows:Section 2 reviews some related work on lexicalsubstitution.
Section 3 describes our system, espe-cially the web-based scoring method.
Section 4presents the results and analysis.2 Related WorkSynonyms defined in WordNet have been widelyused in lexical substitution and expansion (Smea-ton et al, 1994; Langkilde and Knight, 1998; Bol-173shakov and Gelbukh, 2004).
In addition, a lot ofmethods have been proposed to automatically con-struct thesauri of synonyms.
For example, Lin(1998) clustered words with similar meanings bycalculating the dependency similarity.
Barzilay andMcKeown (2001) extracted paraphrases using mul-tiple translations of literature works.
Wu and Zhou(2003) extracted synonyms with multiple resources,including a monolingual dictionary, a bilingualcorpus, and a monolingual corpus.
Besides thehandcrafted and automatic synonym resources, theweb has been exploited as a resource for lexicalsubstitute extraction (Zhao et al, 2007).As for substitute scoring, various methods havebeen investigated, among which the classificationmethod is the most widely used (Dagan et al, 2006;Kauchak and Barzilay, 2006).
In detail, a binaryclassifier is trained for each candidate substitute,using the contexts of the substitute as features.Then a new contextual sentence containing the tar-get word can be classified as 1 (the candidate is acorrect substitute in the given sentence) or 0 (oth-erwise).
The features used in the classification areusually similar with that in word sense disam-biguation (WSD), including bag of word lemmasin the sentence, n-grams and parts of speech (POS)in a window, etc.
There are other models presentedfor candidate substitute scoring.
Glickman et al(2006) proposed a Bayesian model and a NeuralNetwork model, which estimate the probability ofa word may occur in a given context.3 HIT System3.1 Candidate Substitute ExtractionIn HIT, candidate substitutes are extracted fromWordNet.
Both synonyms and hypernyms definedin WordNet are investigated.
Let w be a targetword, pos the specified POS of w. n the number ofw?s synsets defined in WordNet.
Then the systemextracts w?s candidate substitutes as follows:z Extracts all the synonyms in each synsetunder pos1 as candidate substitutes.z If w has no synonym for the i-th synset(1?i?n), then extracts the synonyms of itsnearest hypernym.z If pos is r (or a), and no candidate substi-tute can be extracted as described above,1 In this task, four kinds of POS are specified: n - noun, v -verb, a - adjective, r - adverb.then extracts candidate substitutes under thePOS a (or r).3.2 Candidate Substitute ScoringAs mentioned above, all words in the given sen-tence can be used as contextual information in thescoring of candidate substitutes.
However, it is ob-vious that not all context words are really usefulwhen determining a word?s substitutes.
An exam-ple can be seen from Figure 1.She turns eyes <head>bright</head> withexcitement towards Fiona , still tugging on thestring of the minitiature airship-cum-dancecard she has just received at the door .Figure 1.
An example of a context sentence.In the example above, words turns, eyes, with,and excitement are useful context words, while theothers are not.
The useless contexts may even benoise if they are used in the scoring.
As a result, itis important to select context words carefully.In HIT, we select context words based on thefollowing assumption: useful context words forlexical substitute are those near the target word inthe given sentence.
In other words, the words thatare far from the target word are not taken into con-sideration.
Obviously, this assumption is not al-ways true.
However, considering only theneighboring words can reduce the risk of bringingin noise.
Besides, Edmonds (1997) has also dem-onstrated in his paper that short-distance colloca-tions with neighboring words are more useful inlexical choice than long ones.Let w be the target word, t a candidate substitute,S the context sentence.
Our basic idea is that: Onecan substitute w in S with t, which generates a newsentence S?.
If S?
can be found on the web, then thesubstitute is admissible.
The more times S?
occurson the web, the more probable the substitute is.
Inpractice, however, it is difficult to find a wholesentence S?
on the web due to sparseness.
Instead,we use fragments of S?
which contains t and sev-eral neighboring context words (based on the as-sumption above).
Then the question is how to ob-tain one (or more) fragment of S?.A window with fixed size can be used here.
Su-ppose p is the position of t in S?, for instance, wecan construct a fragment using words from posi-tion p-r to p+r, where r is the radius of window.174However, a fixed r is difficult to set, since it maybe too large for some sentences, which makes thefragments too specific, while too small for someother sentences, which makes the fragments tooloose.
An example can be seen in Table 1.1(a) But when Daniel turned <head>blue</head>one time and he totally stopped breathing.1(b) Daniel turned t one time2(a) We recommend that you <head>check</head>with us beforehand.2(b) that you t with usTable 1.
Examples of fragments with fixed size.In Table1, 1(a) and 2(a) are two sentences fromthe test data of SemEval-2007Task10.
1(b) and 2(b)are fragments constructed according to 1(a) and2(a), where the window radius is 2 and t denotesany candidate substitute of the target word.
It isobvious that 1(b) is a rather strict fragment, whichmakes it difficult to find sentences containing it onthe web, while 2(b) is quite loose, which canhardly constrain the semantics of t.Having considered the problem above, we pro-pose a rule-based method that constructs fragmentswith varied lengths.
Let Ft be a fragment contain-ing t, the construction rules are as follows:Rule-1: Ft must contain at least two words be-sides t, at least one of which is non-stop word.Rule-2: Ft does not cross sub-sentence boundary(?,?
).Rule-3: Ft should be the shortest fragment thatsatisfies Rule-1 and Rule-2.According to the rules above, we construct atmost three fragments for each S?
: (1) t occurs at thebeginning of Ft, (2) t occurs in the middle of Ft,and (3) t occurs at the end of Ft.
Here we have an-other constraint: if one constructed fragment F1 isthe substring of F2, then F2 is removed.
Pleasenote that the morphology is not taken into accountwhen we construct queries.For the sentence 1(a) and 2(a) in Table 1, theconstructed fragments are as follows:For 1(a): Daniel turned t; t one time; turned toneFor 2(a): recommend that you t; t with us be-forehandTable 2.
Examples of the constructed fragmentsTo score a candidate substitute, we replace ?t?
inthe fragments with each candidate substitute anduse them as queries, which are then fed to Google.The score of t is computed according to the countsof retrieved snippets:?==nitWebMining iFSnippetcountntScore1))((1)(     (1)where n is the number of constructed fragments,Fti is the i-th fragment (query) corresponding to t,and count(Snippet(Fti)) is the count of snippetsretrieved by Fti.All candidate substitutes with scores larger than0 are ranked and the first 10 substitutes are re-tained for the oot subtask.
If the number of candi-dates whose scores are larger than 0 is less than 10,the system ranks the rest of the candidates by theirfrequencies using a word frequency list.
The sparecapacity is filled with those candidates with largestfrequencies.
For the best subtask, we simply outputthe substitute that ranks first in oot.3.3 Detection of MultiwordsThe method used to detect multiword in the HITsystem is quite similar to that employed in thebaseline system.
We also use WordNet to detect ifa multiword that includes the target word occurswithin a window of 2 words before and 2 wordsafter the target word.A difference from the baseline system lies inthat our system looks up WordNet using longermultiword candidates first.
If a longer one is foundin WordNet, then its substrings will be ignored.For example, if we find ?get alng with?
in Word-Net, we will output it as a multiword and will notcheck ?get alng?
any more.4 ResultsOur system is the only one that participates all thethree subtasks of Task10, i.e., best, oot, and mw.The evaluation results of our system can be foundin Table 3 to Table 5.
Our system ranks the fourthin the best subtask and seventh in the oot subtask.We have analyzed the results from two aspects,i.e., the ability of the system to extract candidatesubstitutes and the ability to rank the correct sub-stitutes in front.
There are a total of 6,873 manualsubstitutes for all the 1,710 items in the gold stan-dard, only 2,168 (31.54%) of which have been ex-tracted as candidate substitutes by our system.
Thisresult suggests that WordNet is not an appropriate175source for lexical substitute extraction.
In the fu-ture work, we will try some other lexical resources,such as the Oxford American Writer Thesaurusand Encarta.
In addition, we will also try themethod that automatically constructs lexical re-sources, such as the automatic clustering method.Further analysis shows that, 1,388 (64.02%) outof the 2,168 extracted correct candidates areranked in the first 10 in the oot output of our sys-tem.
This suggests that there is a big space for oursystem to improve the candidate scoring method.In the future work, we will consider more andricher features, such as the syntactic features, incandidate substitute scoring.
Furthermore, A dis-advantage of this method is that the web miningprocess is quite inefficient.
Therefore, we will tryto use the Web 1T 5-gram Version 1 from Google(LDC2006T13) in the future.P R ModeP ModeROVERALL 11.35 11.35 18.86 18.86Further AnalysisNMWT 11.97 11.97 19.81 19.81NMWS 12.55 12.38 19.93 19.65RAND 11.81 11.81 20.03 20.03MAN 10.81 10.81 17.53 17.53BaselinesWORDNET 9.95 9.95 15.58 15.58LIN 8.84 8.53 14.69 14.23Table 3. best results.P R ModeP ModeROVERALL 33.88 33.88 46.91 46.91Further AnalysisNMWT 35.60 35.60 48.48 48.48NMWS 36.63 36.63 49.33 49.33RAND 33.95 33.95 47.25 47.25MAN 33.81 33.81 46.53 46.53BaselinesWORDNET 29.70 29.35 40.57 40.57LIN 27.70 26.72 40.47 39.19Table 4. oot results.Our System WordNet BLP R P Rdetection 45.34 56.15 43.64 36.92identification 41.61 51.54 40.00 33.85Table 5. mw results.AcknowledgementsThis research was supported by National NaturalScience Foundation of China (60575042,60503072, 60675034).ReferencesBarzilay Regina and McKeown Kathleen R. 2001.
Ex-tracting paraphrases from a Parallel Corpus.
In Pro-ceedings of ACL/EACL.Bolshakov Igor A. and Gelbukh Alexander.
2004.
Syn-onymous Paraphrasing Using WordNet and Internet.In Proceedings of NLDB.Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-shtein Efrat, Strapparava Carlo.
2006.
Direct WordSense Matching for Lexical Substitution.
In Proceed-ings of ACL.Edmonds Philip.
1997.
Choosing the Word Most Typi-cal in Context Using a Lexical Co-occurrence Net-work.
In Proceedings of ACL.Fellbaum Christiane.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Glickman Oren, Dagan Ido, Keller Mikaela, BengioSamy.
2006.
Investigating Lexical Substitution Scor-ing for Subtitle Generation.
In Proceedings ofCoNLL.Kauchak David and Barzilay Regina.
2006.
Paraphras-ing for Automatic Evaluation.
In Proceedings ofHLT-NAACL.Langkilde I. and Knight K. 1998.
Generation that Ex-ploits Corpus-based Statistical Knowledge.
In Pro-ceedings of the COLING-ACL.Lin Dekang.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of COLING-ACL.Smeaton Alan F., Kelledy Fergus, and O?Donell Ruari.1994.
TREC-4 Experiments at Dublin City Univer-sity: Thresholding Posting Lists, Query Expansionwith WordNet and POS Tagging of Spanish.
In Pro-ceedings of TREC-4.Wu Hua and Zhou Ming.
2003.
Optimizing SynonymExtraction Using Monolingual and Bilingual Re-sources.
In Proceedings of IWP.Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, andZhang Yu.
2007.
Automatic Acquisition of Context-Specific Lexical Paraphrases.
In Proceedings ofIJCAI-07.176
