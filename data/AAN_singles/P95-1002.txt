Automatic Induction of Finite State Transducers for Simple Phonological RulesDaniel Gildea and Daniel JurafskyInternat ional  Computer  Sc ience Inst itute andUnivers i ty  o f  Cal i forn ia  at Berke ley{gi ldea, jurafsky} @ics i .berkeley.eduAbstractThis paper presents a method for learningphonological rules from sample pairs of un-derlying and surface forms, without negativeevidence.
The learned rules are represented asfinite state transducers that accept underlyingforms as input and generate surface forms asoutput.
The algorithm for learning them is anextension of the OSTIA algorithm for learn-ing general subsequential finite state transduc-ers.
Although OSTIA is capable of learningarbitrary s.-f.s.t's in the limit, large dictionariesof actual English pronunciations did not giveenough samples to correctly induce phonolog-ical rules.
We then augmented OSTIA withtwo kinds of knowledge specific to natural lan-guage phonology, biases from "universal gram-mar".
One bias is that underlying phones areoften realized as phonetically similar or iden-tical surface phones.
The other biases phono-logical rules to apply across natural phonolog-ical classes.
The additions helped in learningmore compact, accurate, and general transduc-ers than the unmodified OSTIA algorithm.
Animplementation f the algorithm successfullylearns a number of English postlexical rules.1 IntroductionJohnson (1972).
first observed that traditional phonolog-ical rewrite rules can be expressed as regular elationsif one accepts the constraint that no rule may reapplydirectly to its own output.
This means that finite statetransducers can be used to represent phonological rules,greatly simplifying the problem of parsing the output ofphonological rules in order to obtain the underlying, lex-ical forms (Karttunen 1993).
In this paper we explore an-other consequence of FST models of phonological rules:their weaker generative capacity also makes them easierto learn.
We describe our preliminary algorithm for learn-ing rules from sample pairs of input and output strings,and the results we obtained.In order to take advantage ofrecent work in transducerinduction, we have chosen to represent rules as subse-quential finite state transducers.
Subsequential finitestate transducers are a subtype of finite state transduc-ers with the following properties:1.
The transducer is deterministic, that is, there is onlyone arc leaving a given state for each input symbol.2.
Each time a transition is made, exactly one symbolof the input string is consumed.3.
A unique nd of string symbol is introduced.
At theend of each input string, the transducer makes anadditional transition on the end of string symbol.4.
All states are accepting.The length of the output strings associated with a subse-quential transducer's transitions i not constrained.The subsequential transducer for the English flappingrule in 1 is shown in Figure 1; an underlying t is realizedas a flap after a stressed vowel and any number of r's, andbefore an unstressed vowel.
(1) t ~ dx / (r r* V2 The  OST IA  AlgorithmOur phonological-rule induction algorithm is based onaugmenting the Onward Subsequential Transducer Infer-ence Algorithm (OSTIA) of Oncina et al (1993).
Thissection outlines the OSTIA algorithm to provide back-ground for the modifications that follow.OSTIA takes as input a training set of input-outputpairs.
The algorithm begins by constructing a tree trans-ducer which covers all the training samples.
The root ofthe tree is the transducer's initial state, and each leaf ofthe tree corresponds tothe end of an input sample.The output symbols are placed as near the root of thetree as possible while avoiding conflicts in the output ofa given arc.
An example of the result of this initial treeconstruction is shown in Figure 2.At this point, the transducer covers all and only thestrings of the training set.
OSTIA now attempts to gen-eralize the transducer, by merging some of its states to-gether.
For each pair of states (s, t) in the transducer, thealgorithm will attempt to merge s with t, building a new9#:t,b:bae  K - \] ae :0t :0n :n#:0d :0  ~ #:0  : - @Figure 2: Onward Tree Transducer for "bat", "batter", and "band" with Flapping Appliedcc: c T .Y" "V :dxV ~.~r : t r .
.# : tFigure 1: Subsequential Transducer for English Flap-ping: Labels on arcs are of the form (input sym-bol):(output symbol).
Labels with no colon indicate thesame input and output symbols.
'V' indicates any un-stressed vowel, "v" any stressed vowel, 'dx' a flap, and'C' any consonant other than 't', 'r' or 'dx'.
'#' is theend of string symbol.Figure 3: Result of Merging States 0and 1 of Figure 2far.
However, when trying to learn phonological rulesfrom linguistic data, the necessary training set may notbe available.
In particular, systematic phonological con-straints uch as syllable structure may rule out the neces-sary strings.
The algorithm does not have the languagebias which would allow it to avoid linguistically unnaturaltransducers.b :baeae :0n :ndt :0m m ler .
"dxer#: tstate with all of the incoming and outgoing transitions ofs and f. The result of the first merging operation on thetransducer of Figure 2 is shown in Figure 3, and the endresult of the OSTIA alogrithm in shown in Figure 4.3 Problems Using OSTIA to learnPhonological RulesThe OSTIA algorithm can be proven to learn any subse-quential relation in the limit.
That is, given an infinitesequence of valid input/output pairs, it will at some pointderive the target ransducer f om the samples een soFigure 4: Final Result of Merging Process on Transducerfrom Figure 2For example, OSTIA's tendency to produce overly"clumped" transducers is illustrated by the arcs with out"b ae" and "n d" in the transducer in Figure 4, or even Fig-ure 2.
OSTIA's default behavior is to emit the remainderof the output string for a transduction assoon as enoughinput symbols have been seen to uniquely identify theinput string in the training set.
This results in machineswhich may, seemingly at random, insert or delete se-quences of four or five phonemes, omething which is10linguistically implausible.
In addition, the incorrect dis-tribution of output symbols prevents the optimal mergingof states during the learning process, resulting in largeand inaccurate transducers.Another example of an unnatural generalization isshown in 4, the final transducer induced by OSTIA onthe three word training set of Figure 2.
For example, thetransducer of Figure 4 will insert an 'ae' after any 'b',and delete any 'ae' from the input.
Perhaps worse, it willfail completely upon seeing any symbol other than 'er' orend-of-string after a 't'.
While it might be unreasonableto expect any transducer t ained on three samples to beperfect, the transducer of Figure 4 illustrates on a smallscale how the OSTIA algorithm ight be improved.Similarly, if the OSTIA algorithm is training on casesof flapping in which the preceding environment is ev-ery stressed vowel but one, the algorithm has no wayof knowing that it can generalize the environment to allstressed vowels.
The algorithm needs knowledge aboutclasses of phonemes to fill in accidental gaps in trainingdata coverage.4 Using Alignment InformationOur first modification of OSTIA was to add the bias that,as a default, a phoneme is realized as itself, or as a sim-ilar phone.
Our algorithm guesses the most probablephoneme to phoneme alignment between the input andoutput strings, and uses this information to distribute theoutput symbols among the arcs of the initial tree trans-ducer.
This is demonstrated for the word "importance"in Figures 5 and 6.ih m p oal r t ah n sI I I I  /111ih m p oal dx ah n t sFigure 5: Alignment of "importance" with flapping, r-deletion and t-insertionThe modification proceeds in two stages.
First, adynamic programming method is used to compute acorrespondence b tween input and output phonemes.The alignment uses the algorithm of Wagner & Fischer(1974), which calculates the insertions, deletions, andsubstitutions which make up the minimum edit distancebetween the underlying and surface strings.
The costs ofedit operations are based on phonetic features; we used 26binary articulatory features.
The cost function for sub-stitutions was equal to the number of features changedbetween the two phonemes.
The cost of insertions anddeletions was 6 (roughly one quarter the maximum pos-sible substitution cost).
From the sequence of edit opera-tions, a mapping of output phonemes to input phonemesis generated according to the following rules:?
Any phoneme maps to an input phoneme for whichit substitutes?
Inserted phonemes map to the input phoneme im-mediately following the first substitution to the leftof the inserted phonemeSecond, when adding a new arc to the tree, all the un-used output phonemes up to and including those whichmap to the arc's input phoneme become the new ar-c's output, and are now marked as having been used.When walking down branches of the tree to add a newinput/output sample, the longest common prefix, n, of thesample's unused output and the output of each arc is cal-culated.
The next n symbols of the transduction's outputare now marked as having been used.
If the length, l, ofthe arc's output string is greater than n, it is necessary topush back the last l - n symbols onto arcs further downthe tree.
A tree transducer constructed by this processis shown in Figure 7, for comparison with the unalignedversion in Figure 2.Results of our alignment algorithm are summarized in?6.
The denser distribution of output symbols resultingfrom the alignment constrains the merging of states earlyin the merging loop of the algorithm.
Interestingly, pre-venting the wrong states from merging early on allowsmore merging later, and results in more compact trans-ducers.5 Generalizing Behavior With DecisionTreesIn order to allow OSTIA to make natural generalizationsin its rules, we added a decision tree to each state of themachine, describing the behavior of that state.
For exam-ple, the decision tree for state 2 of the machine in Figure1 is shown in Figure 8.
Note that if the underlying phoneis an unstressed vowel (\[-cons,-stress\]), the machine out-puts a flap, followed by the underlying vowel, otherwiseit outputs a 't' followed by the underlying phone.The decision trees describe the behavior of the machineat a given state in terms of the next input symbol bygeneralizing from the arcs leaving the state.
The decisiontrees classify the arcs leaving each state based on the arc'sinput symbol into groups with the same behavior.
Thesame 26 binary phonetic features used in calculating editdistance were used to classify phonemes in the decisiontrees.
Thus the branches of the decision tree are labeledwith phonetic feature values of the arc's input symbol,and the leaves of the tree correspond to the differentbehaviors.
By an arc's behavior, we mean its outputstring considered as a function of its input phoneme, andits destination state.
Two arcs are considered to have thesame behavior if they agree each of the following:?
the index i of the output symbol corresponding tothe input symbol (determined from the alignmentprocedure)?
the difference of the phonetic feature vectors of theinput symbol and symbol i of the output string?
the prefix of length i - 1 of the output string11~ t : d x  6ah:ah  7 n :n  8 s : t s  9Figure 6: Resulting initial transducer for "importance"b :b  ~ ae:aet :0d :d  #:0Figure 7: Initial Tree Transducer Constructed with Alignment Information: Note that output symbols have been pushedback across tate 3 during the constructionconsstress 21 2Outcomes:1: Output: dx \[ \], Destination State: 02: Output: t \[ \], Destination State: 03: On end of string: Output: t, Destination State: 0Figure 8: Example Decision Tree: This tree describes thebehavior of State 2 of the transducer in Figure 1.
\[ \] inthe output string indicates the arc's input symbol (withno features changed).?
the suffix of the output string beginning at positioni+1?
the destination stateAfter the process of merging states terminates, a deci-sion tree is induced at each state to classify the outgoingarcs.
Figure 9 shows a tree induced at the initial state ofthe transducer for flapping.Using phonetic features to build a decision tree guar-antees that each leaf of the tree represents a natural classof phonemes, that is, a set of phonemes that can be de-scribed by specifying values for some subset of the pho-netic features.
Thus if we think of the transducer as aset of rewrite rules, we can now express the context ofeach rule as a regular expression of natural classes ofpreceding phonemes.stressj "  .
.1 tenserounded 27---..<.y-offglide _,\--..<high 1w-off  glide / ' , , :prim-stress -/\+1 2Outcomes:1: Output: \[ \], Destination State: 02: Output: \[ \], Destination State: 1prim-stressix+1 2On end of string: Output: nil, Destination State: 0Figure 9: Decision Tree Before Pruning: The initial stateof the flapping transducerSome induced transducers may need to be generalizedeven further, since the input transducer to the decision12tree learning may have arcs which are incorrect merelybecause of accidental prior structure.
Consider again theEnglish flapping rule, which applies in the context of apreceding stressed vowel.
Our algorithm first learned atransducer whose decision tree is shown in Figure 9.
Inthis transducer all arcs leaving state 0 correctly lead to theflapping state on stressed vowels, except for those stressedvowels which happen ot to have occurred in the trainingset.
For these unseen vowels (which consisted of therounded iphthongs 'oy' and 'ow' with secondary stress),the transducers incorrectly returns to state 0.
In this case,we wish the algorithm to make the generalization that therule applies after all stressed vowels.stress1 250,000 training samples, and Figure 12 shows some per-formance results.
(2) t --* dx/(Zr * VV~,~t  .- r ( ,~r~NC V _ f ~VcVc :tcr : t r  V :dxV\ [  ~ ~"#: tFigure 11: Flapping Transducer Induced from 50,000SamplesFigure I0: The Same Decision Tree After PruningThis type of generalization can be accomplished bypruning the decision trees at each state of the machine.Pruning is done by stepping through each state of themachine and pruning as many decision odes as possibleat each state.
The entire training set of transductions istested after each branch is pruned.
If any errors are found,the outcome of the pruned node's other child is tested.
Iferrors are still found, the pruning operation is reversed.This process continues at the fringe of the decision treeuntil no more pruning is possible.
Figure 10 shows thecorrect decision tree for flapping, obtained by pruning thetree in Figure 9.The process of pruning the decision trees is compli-cated by the fact that the pruning operations allowed atone state depend on the status of the trees at each otherstate.
Thus it is necessary to make several passes throughthe states, attempting additional pruning at each pass, un-til no more improvement is possible.
In addition, testingeach pruning operation against he entire training set isexpensive, but in the case of synthetic data it gives thebest results.
For other applications it may be desirable tokeep a cross validation set for this purpose.6 Results and DiscussionWe tested our induction algorithm using a synthetic cor-pus of 99,279 input/output pairs.
Each pair consisted ofan underlying and a surface pronunciation of an individ-ual word of English.
The underlying string of each pairwas taken from the phoneme-based CMU pronunciationdictionary.
The surface string was generated from eachunderlying form by mechanically applying the one ormore rules we were attempting toinduce in each experi-ment.In our first experiment, we applied the flapping rulein (2) to training corpora of between 6250 and 50,000words.
Figure 11 shows the transducer induced fromSamples6250125002500050000OSTIA w/o Alignment OSTIA w/AlignmentStates % Error States % Error19 2.32257 16.40141 4.46192 3.143 0.343 0.143 0.063 0.01Figure 12: Results Using Alignment Information on En-glish FlappingAs can be seen from Figure 12, the use of alignmentinformation in creating the initial tree transducer dra-matically decreases the number of states in the learnedtransducer as well as the error performance on test data.The improved algorithm induced a flapping transducerwith the minimum number of states with as few as 6250samples.
The use of alignment information also reducedthe learning time; the additional cost of calculating align-ments is more than compensated for by quicker mergingof states.The algorithm also successfully induced transducerswith the minimum number of states for the t-insertionand t-deletion rules below, given only 6250 samples.In our second experiment, we applied our learningalgorithm to a more difficult problem: inducing multiplerules at once.
A data set was constructed by applyingthe t-insertion rule in (3), the t-deletion rule in (4) andthe flapping rule already seen in (2) one after another.As is seen in Figure 13, a transducer of minimum size(five states) was obtained with 12500 or more sampletransductions.
(3) 0 ---, t /n s(4) t---,O/n \ [+vocal ic \ ]  -stressThe effects of adding decision tress at each state of themachine for the composition of t-insertion, t-deletion andflapping are shown in Figure 14.13Samples6250125002500050000OSTIA w/Alignment to a rule such asStates % Error6 0.935 0.205 0.095 0.04Figure 13: Results on Three Rules ComposedMethodOSTIAAlignmentAdd D-treesPrune D-treesStates %Error329 22.095 0.205 0.045 0.01Figure 14: Results on Three Rules Composed 12,500Training, 49,280 TestFigure 15 shows the final transducer induced from thiscorpus of 12,500 words with pruned ecision trees.r t'r~._._IC s.... VC: t \ [r : t \ [ \ ]n,V:t\[\]Figure 15: Three Rule Transducer Induced from 12,500SamplesAn examination of the few errors (three samples) inthe induced flapping and three-rule transducers pointsout a flaw in our model.
While the learned transducercorrectly makes the generalization that flapping occursafter any stressed vowel, it does not flap after two stressedvowels in a row.
This is possible because no samplescontaining two stressed vowels in a row (or separated byan 'r') immediately followed by a flap were in the trainingdata.
This transducer will flap a 't' after any odd numberof stressed vowels, rather than simply after any stressedvowel.
Such a rule seems quite unnatural phonologically,and makes for an odd context-sensitive rewrite rule.
Anysort of simplest hypothesis criterion applied to a systemof rewrite rules would prefer a rule such as--+ V-+ vwhich is the equivalent of the transducer learned fromthe training data.
This suggests that, the traditional for-malism of context-sensitive rewrite rules contains im-plicit generalizations about how phonological rules usu-ally work that are not present in the transducer system.We hope that further experimentation will lead to a wayof expressing this language bias in our induction system.7 Re la ted  WorkJohnson (1984) gives one of the first computational l-gorithms for phonological rule induction.
His algorithmworks for rules of the form(5) a ---+ b/Cwhere C is the feature matrix of the segments arounda.
Johnson's algorithm sets up a system of constraintequations which C must satisfy, by considering both thepositive contexts, i.e., all the contexts Ci in which a boccurs on the surface, as well as all the negative contextsCj in which an a occurs on the surface.
The set of allpositive and negative contexts will not generally deter-mine a unique rule, but will determine a set of possiblerules.Touretzky et al (1990) extended Johnson's insight byusing the version spaces algorithm of Mitchell (1981) toinduce phonological rules in their Many Maps architec-ture.
Like Johnson's, their system looks at the underly-ing and surface realizations of single segments.
For eachsegment, he system uses the version space algorithm tosearch for the proper statement of the context.Riley (1991) and Withgott & Chen (1993) first pro-posed a decision-tree approach to segmental mapping.
Adecision tree is induced for each phoneme, classifyingpossible realizations of the phoneme in terms of contex-tual factors uch as stress and the surrounding phonemes.However, since the decision tree for each phoneme islearned separately, the the technique misses generaliza-tions about he behavior of similar phonemes.
In addi-tion, no generalizations are made about similar contextphonemes.
In a transducer based formalism, general-izations about similar context phonemes naturally followfrom generalizations about individual phonemes' behav-ior, as the context is represented by the current state ofthe machine, which in turn depends on the behavior ofthe machine on the previous phonemes.We hope that our hybrid model will be more successfulat learning long distance dependencies than the simpledecision tree approach.
To model ong distance rules suchas vowel harmony in a simple decision tree approach, onemust add more distant phonemes to the features used tolearn the decision tree.
In a transducer, this informationis represented in the current state of the transducer.148 ConclusionInferring finite state transducers seems to hold promise asa method for learning phonological rules.
Both of our ini-tial augmentations of OSTIA to bias it toward phonologi-cal naturalness improve performance.
Using informationon the alignment between input and output strings al-lows the algorithm to learn more compact, more accuratetransducers.
The addition of decision trees at each stateof the resulting transducer further improves accuracy andresults in phonologically more natural transducers.
Webelieve that further and more integrated uses of phonolog-ical naturalness, uch as generalizing across similar phe-nomena t different states of the transducer, interleavingthe merging of states and generalization of transitions,and adding memory to the model of transduction, couldhelp even more.Our current algorithm and most previous algorithmsare designed for obligatory rules.
These algorithms fallcompletely when faced with optional, probabilistic rules,such as flapping.
This is the advantage of probabilisticapproaches such as the Riley/Withgott approach.
Onearea we hope to investigate is the generalization of ouralgorithm to probabilistic rules with probabilistic finite-state transducers, perhaps by augmenting PFST inductiontechniques such as Stolcke & Omohundro (1994) withinsights from phonological naturalness.Besides aiding in the development of a practical toolfor learning phonological rules, our results point to theuse of constraints from universal grammar as a strongfactor in the machine and possibly human learning ofnatural language phonology.RILEY, MICHAEL D. 1991.
A statistical model for gener-ating pronunciation networks.
In IEEE ICASSP-91,737-740.STOLCKE, ANDREAS, 8?
STEPHEN OMOHUNDRO.
1994.Best-first model merging for hidden Markov modelinduction.
Technical Report TR-94-003, Interna-tional Computer Science Institute, Berkeley, CA.TOURETZKY, DAVID S., GILLETTE ELVGREN III, &DEIRDRE W. WHEELER.
1990.
Phonological ruleinduction: An architectural solution.
In Proceed-ings of the 12th Annual Conference of the CognitiveScience Society (COGSCI-90), 348-355.WAGNER, R. A., & M. J. FISCHER.
1974.
The string-to-string correction problem.
Journal of the Associa-tion for Computation Machinery 21.168-173.WITHGOTT, M. M., & E R. CHEN.
1993.
ComputationModels of American Speech.
Center for the Studyof Language and Information.AcknowledgmentsThanks to Jerry Feldman, Eric Fosler, Isabel Galiano-Ronda,Lauri Karttunen, Jose Oncina,Andreas Stolcke, and Gary Tajch-man.
This work was partially funded by ICSI.ReferencesJOHNSON, C. DOUGLAS.
1972.
FormalAspects of Phono-logical Description.
The Hague: Mouton.JOHNSON, MARK.
1984.
A discovery procedure forcertain phonological rules.
In Proceedings of theTenth International Conference on ComputationalLinguistics, 344-347, Stanford.KARTI'UNEN, LAURI.
1993.
Finite-state constraints.
InThe Last Phonological Rule, ed.
by John Goldsmith.University of Chicago Press.MITCHELL, TOM M. 1981.
Generalization as search.In Readings in Artificial Intelligence, ed.
by Bon-nie Lynn Webber & Nils J. Nilsson, 517-542.
LosAltos: Moi'gan Kaufmann.ONCINA, JO$1~, PEDRO GARC\[A, & ENRIQUE VIDAL.1993.
Learning subsequential transducers for pat-tern recognition tasks.
IEEE Transactions on PatternAnalysis and Machine Intelligence 15.448-458.15
