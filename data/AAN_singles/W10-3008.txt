Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 56?63,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCombining Manual Rules and Supervised Learning for Hedge Cue andScope DetectionMarek ReiComputer LaboratoryUniversity of CambridgeUnited KingdomMarek.Rei@cl.cam.ac.ukTed BriscoeComputer LaboratoryUniversity of CambridgeUnited KingdomTed.Briscoe@cl.cam.ac.ukAbstractHedge cues were detected using a super-vised Conditional Random Field (CRF)classifier exploiting features from theRASP parser.
The CRF?s predictions werefiltered using known cues and unseen in-stances were removed, increasing preci-sion while retaining recall.
Rules for scopedetection, based on the grammatical re-lations of the sentence and the part-of-speech tag of the cue, were manually-developed.
However, another supervisedCRF classifier was used to refine these pre-dictions.
As a final step, scopes were con-structed from the classifier output using asmall set of post-processing rules.
Devel-opment of the system revealed a number ofissues with the annotation scheme adoptedby the organisers.1 IntroductionSpeculative or, more generally, ?hedged?
languageis a way of weakening the strength of a statement.It is usually signalled by a word or phrase, called ahedge cue, which weakens some clauses or propo-sitions.
These weakened portions of a sentenceform the scope of the hedge cues.Hedging is an important tool in scientific lan-guage allowing scientists to guide research be-yond the evidence without overstating what fol-lows from their work.
Vincze et al (2008) showthat 19.44% of all sentences in the full papersof the BioScope corpus contain hedge cues.
De-tecting these cues is potentially valuable for taskssuch as scientific information extraction or liter-ature curation, as typically only definite informa-tion should be extracted and curated.
Most workso far has been done on classifying entire textsentences as hedged or not, but this risks los-ing valuable information in (semi-)automated sys-tems.
More recent approaches attempt to find thespecific parts of a text sentence that are hedged.Here we describe a system that is designed tofind hedge cues and their scopes in biomedical re-search papers.
It works in three stages:1.
Detecting the cues using a token-level super-vised classifier.2.
Finding the scopes with a combination ofmanual rules and a second supervised token-level classifier.3.
Applying postprocessing rules to convert thetoken-level annotation into predictions aboutscope.Parts of the system are similar to that of Moranteand Daelemans (2009) ?
both make use of ma-chine learning to tag tokens as being in a cue ora scope.
The most important differences are theuse of manually defined rules and the inclusion ofgrammatical relations from a parser as critical fea-tures.2 DataA revised version of the BioScope corpus (Vinczeet al, 2008), containing annotation of cues andscopes, was provided as training data for theCoNLL-2010 shared task (Farkas et al, 2010).This includes 9 full papers and 1273 abstractsfrom biomedical research fields.
A separate newset of full papers was released for evaluation aspart of the task.
Table 1 contains an overview ofthe training corpus statistics.
(1) provides an example sentence from the cor-pus illustrating the annotation provided for train-ing.
(2) shows the same sentence, representing cueswith angle brackets and scopes with round brack-ets.56Papers AbstractsDocuments 9 1273Sentences 2670 11871Cues 682 2694Scopes 668 2659Unique cues 100 112Cues with multiple words 10.70% 12.25%Scopes start with cue 72.00% 80.59%Scopes with multiple cues 2.10% 1.28%Table 1: Training data statistics.
(1) <sentence id=?S1.166?>We <xcopeid=?X1.166.2?><cue ref=?X1.166.2?type=?speculation?>expect</cue> that this cluster<xcope id=?X1.166.1?><cue ref=?X1.166.1?type=?speculation?>may</cue> represent a novelselenoproteinfamily</xcope></xcope>.</sentence>(2) We (<expect> that this cluster (<may> represent anovel selenoprotein family)).There are a number of conditions on the anno-tation that are imposed:?
Every cue has one scope.?
Every scope has one or more cues.?
The cue must be contained within its scope.?
Cues and scopes have to be continuous.For development of the system, before the eval-uation data were released, we used 60% of theavailable corpus for training and 40% for testing.The results we give below measure the system per-formance on the evaluation data while using allof the training data to build the supervised clas-sifiers.
The manually-developed rules are basedon the 60% of the development data we originallyreserved for training.All of the training and test data sentenceswere tokenised and parsed using the RASP sys-tem (Briscoe et al, 2006).
Multiple part-of-speech (POS) tag outputs were passed to the parser(to compensate for the high number of unseenwords in biomedical text), retaining just the high-est ranked directed graph of grammatical relations(GRs).
Each node in the graph represents a wordtoken annotated with POS, lemma, and positionalorder information.
In the case of parse failure theset of unconnected graphs returned by the highest-ranked spanning subanalyses for each sentencewere retained.3 Speculation cuesThe hedge cues are found using a ConditionalRandom Field (CRF) (Lafferty et al, 2001) classi-fier, implemented using CRF++ 1.
We chose theCRF model because we framed the task as oneof token-level sequential tagging and CRFs areknown to achieve state-of-the-art performance onrelated text classification tasks.
Each word tokenis assigned one of the following tags: F (first wordof a cue), I (inside a cue), L (last word of a cue),O (outside, not a cue), hereafter referred to as theFILO scheme.The feature types used for classification are de-fined in terms of the grammatical relations outputprovided by the RASP system.
We use binary fea-tures that indicate whether a word token is a heador a dependent in specific types of grammaticalrelation (GR).
This distinguishes between differ-ent functions of the same word (when used as asubject, object, modifier, etc.).
These features arecombined with POS and lemma of the word to dis-tinguish between uses of different cues and cuetypes.
We also utilise features for the lemma andPOS of the 3 words before and after the currentword.The list of feature types for training the classi-fier is:?
string?
lemma?
part-of-speech?
broad part-of-speech?
incoming GRs + POS?
outgoing GRs + POS?
incoming GRs + POS + lemma?
outgoing GRs + POS + lemma?
lemma + POS + POS of next word?
lemma + POS + POS of previous word?
3 previous lemma + POS combinations?
3 following lemma + POS combinations.Outgoing GRs are grammatical relations wherethe current word is the head, incoming GRs whereit is the dependent.The predictions from the classifier are com-pared to the list of known cues extracted from thetraining data; the longest possible match is markedas a cue.
For example, the classifier could outputthe following tag sequence:(3) This[O] indicates[F] that[O] these[O] two[O]lethal[O] mutations[O] .
.
.1http://crfpp.sourceforge.net57indicates is classified as a cue but that is not.The list of known cues contains ?indicates that?which matches this sentence, therefore the systemprediction is:(4) This <indicates that> these two lethal mutations .
.
.Experiments in section 5.1 show that our sys-tem is not good at finding previously unseen cues.Lemma is the most important feature type for cuedetection and when it is not available, there is notenough evidence to make good predictions.
There-fore, we compare all system predictions to the listof known cues and if there is no match, they areremoved.
The detection of unseen hedge cues is apotential topic for future research.4 Speculation scopesWe find a scope for each cue predicted in the pre-vious step.
Each word token in the sentence istagged with either F (first word of a scope), I (in-side a scope), L (last word of a scope) or O (out-side, not in a scope).
Using our example sentence(2) the correct tagging is:expect mayWe O Oexpect F Othat I Othis I Ocluster I Omay I Frepresent I Ia I Inovel I Iselenoprotein I Ifamily L L. O OTable 2: Example of scope tagging.If a cue contains multiple words, they are eachprocessed separately and the predictions are latercombined by postprocessing rules.As the first step, manually written rules are ap-plied that find the scope based on GRs and POStags.
We refine these predictions using a secondCRF classifier and further feature types extractedfrom the RASP system output.
Finally, postpro-cessing rules are applied to convert the tagging se-quence into scopes.
By default, the minimal scopereturned is the cue itself.4.1 Manual rulesManual rules were constructed based on the de-velopment data and annotation guidelines.
In thefollowing rules and examples:?
?below?
refers to nodes that are in the sub-graph of GRs rooted in the current node.?
?parent?
refers to the node that is the headof the current node in the directed, connectedGR graph.?
?before?
and ?after?
refer to word positionsin the text centered on the current node.?
?mark everything below?
means mark allnodes in the subgraph as being in the scope(i.e.
tag as F/I/L as appropriate).
However,the traversal of the graph is terminated whena text adjunct (TA) GR boundary or a wordPOS-tagged as a clause separator is found,since they often indicate the end of the scope.The rules for finding the scope of a cue are trig-gered based on the generalised POS tag of the cue:?
Auxiliary ?
VMMark everything that is below the parent andafter the cue.If the parent verb is passive, mark everythingbelow its subject (i.e.
the dependent of thesubj GR) before the cue.?
Verb ?
VVMark everything that is below the cue and af-ter the cue.If cue is appear or seem, mark everything be-low subject before the cue.If cue is passive, mark everything below sub-ject before the cue.?
Adjective ?
JJFind parent of cue.
If there is no parent, thecue is used instead.Mark everything that is below the parent andafter the cue.If parent is passive, mark everything belowsubject before the cue.If cue is (un)likely and the next word is to,mark everything below subject before thecue.?
Adverb ?
RRMark everything that is below the parent andafter the cue.58?
Noun ?
NNFind parent of cue.
If there is no parent, thecue is used instead.Mark everything that is below the parent andafter the cue.If parent is passive, mark everything belowsubject before the cue.?
Conjunction ?
CCMark everything below the conjunction.If the cue is or and there is another cue eitherbefore, combine them together.?
?Whether?
as a conjunction ?
CSWMark everything that is below the cue and af-ter the cue.?
Default ?
anything elseMark everything that is below the parent andafter the cue.If parent verb is passive, mark everything be-low subject before the cue.Either .
.
.
or .
.
.
is a frequent exception contain-ing two separate cues that form a single scope.
Anadditional rule combines these cues when they arefound in the same sentence.The partial GR graph for (5) is given in Figure1 (with positional numbering suppressed for read-ability).
(5) Lobanov et al thus developed a sensitive searchmethod to deal with this problem, but they alsoadmitted that it (<would> fail to identify highlyunusual tRNAs).Following the rules, would is identified as a cueword with the part-of-speech VM; this triggers thefirst rule in the list.
The parent of would is failsince they are connected with a GR where fail isthe head.
Everything that is below fail in the GRgraph and positioned after would is marked as be-ing in the scope.
Since fail is not passive, the sub-ject it is left out.
The final scope returned by therule is then would fail to identify highly unusualtRNAs.4.2 Machine learningThe tagging sequence from the manual rules isused as input to a second CRF classifier, alongwith other feature types from RASP.
The outputof the classifier is a modified sequence of FILOtags.The list of features for each token, used bothalone and as sequences of 5-grams before and afterthe token, is:Figure 1: Partial GR graph for sample sentence (5)?
tag from manual rules?
lemma?
POS?
is the token also the cue?
distance from the cue?
absolute distance from the cue?
relative position to the cue?
are there brackets between the word and thecue?
is there any other punctuation between theword and the cue?
are there any special (clause ending) wordsbetween the word and cue?
is the word in the GR subtree of the cue?
is the word in the GR subtree of the main verb?
is the word in the GR subject subtree of themain verbFeatures of the current word, used in combina-tion with the POS sequence of the cue:?
POS?
distance from the cue?
absolute distance from the cue?
relative position to the cue?
is the word in the GR subtree of the cue?
is the word in the GR subtree of the main verb?
is the word in the GR subject subtree of themain verb59Additional features:?
GR paths between the word and the cue: fullpath plus subpaths with up to 5 nodes?
GR paths in combination with the lemma se-quence of the cueThe scope of the hedge cue can often be foundby tracking the sequence of grammatical relationsin the GR graph of a sentence, as described bythe manual rules.
To allow the classifier to learnsuch regularities, we introduce the concept of aGR path.Given that the sentence has a full parse and con-nected graph, we can find the shortest connectedpath between any two words.
We take the con-nected path between the word and the cue and con-vert it into a string representation to use it as a fea-ture value in the classifier.
Path sections of differ-ent lengths allow the system to find both generaland more specific patterns.
POS tags are used asnode values to abstract away from word tokens.An example for the word unusual, using thegraph from Figure 1, is given below.
Five fea-tures representing paths with increasing lengthsplus one feature containing the full path are ex-tracted.
(6) 1: VM2: VM<?aux?VV03: VM<?aux?VV0?xcomp?>VV04: VM<?aux?VV0?xcomp?>VV0?dobj?>NP25: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2?ncmod?>JJ6: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2?ncmod?>JJLine 1 shows the POS of the cue would (VM).On line 2, this node is connected to fail (VV0) byan auxiliary GR type.
More links are added untilwe reach unusual (JJ).The presence of potential clause ending words,used by Morante and Daelemans (2009), is in-cluded as a feature type with values: whereas,but, although, nevertheless, notwithstanding, how-ever, consequently, hence, therefore, thus, instead,otherwise, alternatively, furthermore, moreover,since.4.3 Post-processingIf the cue contains multiple words, the tag se-quences have to be combined.
This is done byoverlapping the sequences and choosing the pre-ferred tag for each word, according to the hierar-chy F > L > I > O.Next, scopes are constructed from tag se-quences using the following rules:?
Scope start point is the first token tagged asF before the cue.
If none are found, the firstword of the cue is used as the start point.?
Scope end point is the last token tagged as Lafter the cue.
If none are found, look for tagsI and F. If none are found, the last word of thecue is used as end point.The scopes are further modified until none ofthe rules below return any updates:?
If the last token of the scope is punctuation,move the endpoint before the token.?
If the last token is a closing bracket, move thescope endpoint before the opening bracket.?
If the last token is a number and it is not pre-ceded by a capitalised word (e.g.
Table 16),move the scope endpoint before the token.This is a heuristic rule to handle trailing ci-tations which are frequent in the training dataand often misattached by the parser.Finally, scopes are checked for partial overlapand any instances are corrected.
For example, thesystem might return a faulty version (7) of the sen-tence (2) in which one scope is only partially con-tained within the other.
(7) We [<expect> that this cluster (<may> represent anovel] selenoprotein family).This prediction cannot be represented within theformat specified for the shared task and we wereunable to find cases where such annotation wouldbe needed.
These scopes are modified by movingthe end of the first scope to the end of the secondscope.
The example above would become:(8) We [<expect> that this cluster (<may> represent anovel selenoprotein family)].5 Results5.1 Hedge cuesIn evaluation a predicted cue is correct if it con-tains the correct substring of the sentence.
Token-level evaluation would not give accurate resultsbecause of varying tokenisation rules.
A sentenceis classified as hedged if it contains one or morecues.60The results below are obtained using the scorersimplemented by the organisers of the shared task.As our baseline system, we use simple stringmatching.
The list of known cues is collected fromthe training data and compared to the evaluationsentences.
The longest possible match is alwaysmarked as a cue.
ML1 to ML3 are variations ofthe system described in section 3.
All availabledata, from papers and abstracts, were used to trainthe CRF classifier.
ML1 uses the results of theclassifier directly.
The longest sequence of tokenstagged as being part of a cue is used to form the fi-nal prediction.
ML2 incorporates the list of knowncues, constructing a cue over the longest sequenceof matching tokens where at least one token hasbeen tagged as belonging to a cue.
ML3 uses thelist of known cues and also removes any predictedcues not seen in the training data.Baseline ML1 ML2 ML3Total cues 1047 1047 1047 1047Predicted cues 3062 995 1006 995Correctlypredicted cues1018 785 810 810Cue precision 0.332 0.789 0.805 0.814Cue recall 0.972 0.750 0.774 0.774Cue F-measure 0.495 0.769 0.789 0.793Sentenceprecision0.413 0.831 0.831 0.838Sentence recall 0.995 0.843 0.843 0.842SentenceF-measure0.584 0.837 0.837 0.840Table 3: Cue detection results.The baseline system returns nearly all cues butsince it matches every string, it also returns manyfalse positives, resulting in low precision.
ML1delivers more realistic predictions and increasesprecision to 0.79.
This illustrates how the use of aword as a hedge cue depends on its context and notonly on the word itself.
ML2 incorporates knowncues and increases both precision and recall.
ML3removes any unseen cue predictions further im-proving precision.
This shows the system is un-able to accurately predict cues that have not beenincluded in the training data.Table 4 lists the ten most common cues in thetest data and the number of cues found by the ML3system.In the cases of may and suggest, which are alsothe most common cues in the development data,the system finds all the correct instances.
Canand or are not detected as accurately because theyare both common words that in most cases areTP FP Goldmay 161 5 161suggest 124 0 124can 2 1 61or 9 12 52indicate that 49 2 50whether 42 6 42might 42 1 42could 30 17 41would 37 14 37appear 31 14 31Table 4: True and false positives of the ten mostcommon cues in the evaluation data, using ML3system.not functioning as hedges.
For example, there are1215 occurrences of or in the training data andonly 146 of them are hedge cues; can is a cue in 64out of 506 instances.
We have not found any ex-tractable features that reliably distinguish betweenthe different uses of these words.5.2 Hedge scopesA scope is counted as correct if it has the correctbeginning and end points in the sentence and isassociated with the correct cues.
Scope predictionsystems take cues as input, therefore we presenttwo separate evaluations ?
one with gold standardcues and the other with cues predicted by the ML3system from section 4.The baseline system looks at each cue andmarks a scope from the beginning of the cue to theend of the sentence, excluding the full stop.
Thesystem using manual rules applies a rule for eachcue to find its scope, as described in section 4.1.The POS tag of the cue is used to decide whichrule should be used and the GRs determine thescope.The final system uses the result from the manualrules to derive features, adds various further fea-tures from the parser and trains a CRF classifier torefine the predictions.We hypothesized that the speculative sentencesin abstracts may differ from the ones in full papersand a 10-fold cross-validation of the developmentdata supported this intuition.
Therefore, the orig-inal system (CRF1) only used data from the fullpapers to train the scope detection classifier.
Wepresent here also the system trained on all of theavailable data (CRF2).Post-processing rules are applied equally to allof these systems.The baseline system performs remarkably well.61Baseline ManualrulesManualrules +CRF1Manualrules +CRF2Total scopes 1033 1033 1033 1033Predicted 1047 1035 1035 1035Correctlypredicted596 661 686 683Precision 0.569 0.639 0.663 0.660Recall 0.577 0.640 0.664 0.661F-measure 0.573 0.639 0.663 0.661Table 5: Scope detection results using gold stan-dard cues.Baseline ManualrulesManualrules +CRF1Manualrules +CRF2Total scopes 1033 1033 1033 1033Predicted 995 994 994 994Correctlypredicted507 532 564 567Precision 0.510 0.535 0.567 0.570Recall 0.491 0.515 0.546 0.549F-measure 0.500 0.525 0.556 0.559Table 6: Scope detection results using predictedcues.It does not use any grammatical or lexical know-ledge apart from the cue and yet it delivers an F-score of 0.50 with predicted and 0.57 with goldstandard cues.Manual rules are essentially a more fine-grainedversion of the baseline.
Instead of a single rule,one of 8 possible rules is selected based on thePOS tag of the cue.
This improves the results,increasing the F-score to 0.53 with predicted and0.64 with gold standard cues.
The improvementsuggests that the POS tag of a cue is a good indi-cator of how it behaves in the sentence.Error analysis showed that 35% of faulty scopeswere due to incorrect or unconnected GR graphsoutput by the parser, and 65% due to exceptionsthat the rules do not cover.
An example of an ex-ception, the braces { } showing the scopes pre-dicted by the rules, is given in (9).
(9) Contamination is {(<probably> below 1%)}, whichis {(<likely> lower than the contamination rate of thepositive dataset) as discussed in 47}.as discussed in 47 is a modifier of the clausewhich is usually included in the scope but in thiscase should be left out.Finally, the last system combines features fromthe rule-based system with features from RASP totrain a second classifier and improves our resultsfurther, reaching 0.56 with predicted cues.Inclusion of the abstracts as training data gavea small improvement with predicted cues but notwith gold standard cues.
It is part of future workto determine if and how the use of hedges differsacross text sources.6 Annotation schemeDuring analysis of the data, several examples werefound that could not be correctly annotated due tothe restrictions of the markup.
This leads us tobelieve that the current rules for annotation mightnot be best suited to handle complex constructionscontaining hedged text.Most importantly, the requirement for the hedgescope to be continuous over the surface form oftext sentence does not work for some examplesdrawn from the development data.
In (10) belowit is uncertain whether fat body disintegration isindependent of the AdoR.
In contrast, it is statedwith certainty that fat body disintegration is pro-moted by action of the hemocytes, yet the latterassertion is included in the scope to keep it contin-uous.
(10) (The block of pupariation <appears> to involvesignaling through the adenosine receptor ( AdoR )) ,but (fat body disintegration , which is promoted byaction of the hemocytes , <seems> to be independentof the AdoR) .Similarly, according to the guidelines, the sub-ject of be likely should be included in its scope,as shown in example (11).
In sentence (12), how-ever, the subject this phenomenon is separated bytwo non-speculative clauses and is therefore leftout of the scope.
(11) Some predictors make use of the observation that(neighboring genes whose relative location isconserved across several prokaryotic organisms are<likely> to interact).
(12) This phenomenon, which is independent of tumournecrosis factor, is associated with HIV replication, and(is thus <likely> to explain at least in part theperpetuation of HIV infection in monocytes).In (13), arguably, there is no hedging as the sen-tence precisely describes a statistical technique forpredicting interaction given an assumption.
(13) More recently, other groups have come up withsophisticated statistical methods to estimate(<putatively> interacting domain pairs), based on the(<assumption> of domain reusability).Ultimately, dealing effectively with these andrelated examples would involve representing62hedge scope in terms of sets of semantic proposi-tions recovered from a logical semantic represen-tation of the text, in which anaphora, word sense,and entailments had been resolved.7 Related workMost of the previous work has been done on classi-fying sentences as hedged or not, rather than find-ing the scope of the hedge.The first linguistically and computationally mo-tivated study of hedging in biomedical texts isLight et al (2004).
They present an analysis of theproblem based on Medline abstracts and constructan initial experiment for automated classification.Medlock and Briscoe (2007) propose a weaklysupervised machine learning approach to thehedge classification problem.
They construct aclassifier with single words as features and usea small amount of seed data to bootstrap thesystem, achieving the precision/recall break-evenpoint (BEP) of 0.76.
Szarvas (2008) extends thiswork by introducing bigrams and trigrams as fea-ture types, improving feature selection and us-ing external data sources to construct lists of cuewords, achieving a BEP of 0.85.Kilicoglu and Bergler (2008) apply a combina-tion of lexical and syntactic methods, improvingon previous results and showing that quantifyingthe strength of a hedge can be beneficial for clas-sification of speculative sentences.Vincze et al (2008) created a publicly availableannotated corpus of biomedical papers, abstractsand clinical data called BioScope, parts of whichwere also used as training data for the CoNLL10shared task, building on the dataset and annota-tion scheme used for evaluation by Medlock andBriscoe (2007).Morante and Daelemans (2009) use the Bio-Scope corpus to approach the problem of identify-ing cues and scopes via supervised machine learn-ing.
They train a selection of classifiers to tag eachword and combine the results with a final classi-fier, finding 65.6% of the scopes in abstracts and35.9% of the scopes in papers.8 ConclusionsWe have shown that the GRs output by the RASPsystem can be effectively used as features for de-tecting cues in a supervised classifier and also asthe basis for manual rules and features for scopedetection.
We demonstrated that a small num-ber of manual rules can provide competitive re-sults, but that these can be further improved usingmachine learning techniques and post-processingrules.
The generally low ceiling for the scope de-tection results demonstrates the difficulty of bothannotating and detecting the hedge scopes in termsof surface sentential forms.Future work could usefully be directed at im-proving performance on unseen cue detection andon learning rules of the same form as those de-veloped manually from annotated training data.However, perhaps the most pressing issue is that ofestablishing the best possible annotation and con-sequent definition of the scope detection task.ReferencesTed Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL 2006 on InteractivePresentation Sessions.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: a linguistically motivated perspective.
BMCBioinformatics, 9 Suppl 11.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML 2001.Marc Light, Xin Y. Qiu, and Padmini Srinivasan.
2004.The language of bioscience: Facts, speculations, andstatements in between.
In Proceedings of BioLink2004.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of ACL 2007.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of the Workshop on BioNLP.Gyo?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selectionof keywords.
In Proceedings of ACL 2008.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9 Suppl 11.63
