Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 43?48,Columbus, June 2008. c?2008 Association for Computational LinguisticsImpact of Initiative on Collaborative Problem Solving ?Cynthia KerseyDepartment of Computer ScienceUniversity of Illinois at ChicagoChicago, Illinois 60613ckerse2@uic.eduAbstractEven though collaboration in peer learning hasbeen shown to have a positive impact for stu-dents, there has been little research into col-laborative peer learning dialogues.
We ana-lyze such dialogues in order to derive a modelof knowledge co-construction that incorpo-rates initiative and the balance of initiative.This model will be embedded in an artificialagent that will collaborate with students.1 IntroductionWhile collaboration in dialogue has long been re-searched in computational linguistics (Chu-Carrolland Carberry, 1998; Constantino-Gonza?lez andSuthers, 2000; Jordan and Di Eugenio, 1997;Lochbaum and Sidner, 1990; Soller, 2004; Vizca?
?no,2005), there has been little research on collabora-tion in peer learning.
However, this is an importantarea of study because collaboration has been shownto promote learning, potentially for all of the par-ticipants (Tin, 2003).
Additionally, while there hasbeen a focus on using natural language for intelli-gent tutoring systems (Evens et al, 1997; Graesseret al, 2004; VanLehn et al, 2002), peer to peer in-teractions are notably different from those of expert-novice pairings, especially with respect to the rich-ness of the problem-solving deliberations and ne-gotiations.
Using natural language in collaborativelearning could have a profound impact on the wayin which educational applications engage students inlearning.
?This work is funded by NSF grants 0536968 and 0536959.There are various theories as to why collaborationin peer learning is effective, but one of that is com-monly referenced is co-construction (Hausmann etal., 2004).
This theory is a derivative of construc-tivism which proposes that students construct an un-derstanding of a topic by interpreting new materialin the context of prior knowledge (Chi et al, 2001).Essentially, students who are active in the learn-ing process are more successful.
In a collaborativesituation this suggests that all collaborators shouldbe active participants in order to have a successfullearning experience.
Given the lack of research inmodeling peer learning dialogues, there has been lit-tle study of what features of dialogue characterizeco-construction.
I hypothesize that since instancesof co-construction closely resemble the concepts ofcontrol and initiative, these dialogue features can beused as identifiers of co-construction.While there is some dispute as to the definitionsof control and initiative (Jordan and Di Eugenio,1997; Chu-Carroll and Brown, 1998), it is generallyaccepted that one or more threads of control passbetween participants in a dialogue.
Intuitively, thissuggests that tracking the transfer of control can beuseful in determining when co-construction is occur-ring.
Frequent transfer of control between partici-pants would indicate that they are working togetherto solve the problem and perhaps also to constructknowledge.The ultimate goal of this research is to develop amodel of co-construction that incorporates initiativeand the balance of initiative.
This model will be em-bedded in KSC-PaL, a natural language based peeragent that will collaborate with students to solve43Figure 1: The data collection interfaceproblems in the domain of computer science datastructures.In section 2, I will describe how we collected thedialogues and the initial analysis of those dialogues.Section 3 details the on-going annotation of the cor-pus.
Section 4 describes the future development ofthe computational model and artificial agent.
This isfollowed by the conclusion in section 5.2 Data CollectionIn a current research project on peer learning, wehave collected computer-mediated dialogues be-tween pairs of students solving program comprehen-sion and error diagnosis problems in the domain ofdata structures.
The data structures that we are fo-cusing on are (1) linked lists, (2) stacks and (3) bi-nary search trees.
This domain was chosen becausedata structures and their related algorithms are oneof the core components of computer science educa-tion and a deep understanding of these topics is es-sential to a strong computer science foundation.2.1 InterfaceA computer mediated environment was chosen tomore closely mimic the situation a student will haveto face when interacting with KSC-PaL, the artificialpeer agent.
After observing face-to-face interactionsof students solving these problems, I developed aninterface consisting of four distinct areas (see Fig-ure 1):1.
Problem display: Displays the problem de-scription that is retrieved from a database.2.
Code display: Displays the code from the prob-lem statement.
The students are able to makechanges to the code, such as crossing-out linesand inserting lines, as well as undoing thesecorrections.3.
Chat Area: Allows for user input and an inter-leaved dialogue history of both students partic-ipating in the problem solving.
The history islogged for analysis.4.
Drawing area: Here users can diagram datastructures to aid in the explanation of parts ofthe problem being solved.
The drawing areahas objects representing nodes and links.
Theseobjects can then be placed in the drawing areato build lists, stacks or trees depending on thetype of problem being solved.The changes made in the shared workspace(drawing and code areas) are logged and propagatedto the partner?s window.
In order to prevent usersfrom making changes at the same time, I imple-mented a system that allows only one user to draw ormake changes to code at any point in time.
In orderto make a change in the shared workspace, a usermust request the ?pencil?
(Constantino-Gonza?lezand Suthers, 2000).
If the pencil is not currently al-located to her partner, the user receives the penciland can make changes in the workspace.
Otherwise,the partner is informed, through both text and an au-dible alert, that his peer is requesting the pencil.
Thechat area, however, allows users to type at the sametime, although they are notified by a red circle at thetop of the screen when their partner is typing.
While,this potentially results in interleaved conversations,it allows for more natural communication betweenthe peers.Using this interface, we collected dialogues fora total of 15 pairs where each pair was presentedwith five problems.
Prior to the collaborative prob-lem solving activities, the participants were individ-ually given pre-tests and at the conclusion of the ses-sion, they were each given another test, the post-test.
During problem solving the participants wereseated in front of computers in separate rooms andall problem solving activity was conducted using thecomputer-mediated interface.
The initial exercise letthe users become acquainted with the interface.
The44Prob.
3 Prob.
4 Prob.
5Predictor (Lists) (Stacks) (Trees)Pre-Test 0.530(p=0.005)0.657(p=0.000)0.663(p=0.000)Words 0.189(p=0.021)Wordsper Turn0.141(p=0.049)PencilTime0.154(p=0.039)TotalTurns0.108(p=0.088)CodeTurns0.136(p=0.076)Table 1: Post-test Score Predictors (R2)participants were allowed to ask questions regardingthe interface and were limited to 30 minutes to solvethe problem.
The remaining exercises had no timelimits, however the total session, including pre-testand post-test could not exceed three hours.
There-fore not all pairs completed all five problems.2.2 Initial AnalysisAfter the completion of data collection, I establishedthat the interface and task were conducive to learn-ing by conducting a paired t-test on the pre-test andpost-test scores.
This analysis showed that the post-test score was moderately higher than the pre-testscore (t(30)=2.83; p=0.007; effect size = 0.3).I then performed an initial analysis of the col-lected dialogues using linear regression analysis toidentify correlations between actions of the dyadsand their success at solving the problems presentedto them.
Besides the post-test, students solutionsto the problems were scored, as well; this is whatwe refer to as problem solving success.
The par-ticipant actions were also correlated with post-testscores and learning gains (the difference betweenpost-test score and pre-test score).
The data thatwas analyzed came from three of the five problemsfor all 15 dyads, although not all dyads attemptedall three problems.
Thus, I analyzed a total of 40subdialogues.
The problems that were analyzed areall error diagnosis problems, but each problem in-volves a different data structure - linked list, array-based stack and binary search tree.
Additionally,I analyzed the relationship between initiative andpost-test score, learning gain and successful problemsolving.
Before embarking on an exhaustive man-ual annotation of initiative, I chose to get a sense ofwhether initiative may indeed affect learning in thiscontext by automatically tagging for initiative usingan approximation of Walker and Whittaker?s utter-ance based allocation of control rules (Walker andWhittaker, 1990).
In this scheme, first each turn inthe dialogue must be tagged as either: (1) an asser-tion, (2) a command, (3) a question or (4) a prompt(turns not expressing propositional content).
Thiswas done automatically, by marking turns that endin a question mark as questions, those that start witha verb as commands, prompts from a list of com-monly used prompts (e.g.
ok, yeah) and the remain-ing turns as assertions.
Control is then allocated byusing the following rules based on the turn type:1.
Assertion: Control is allocated to the speakerunless it is a response to a question.2.
Command: Control is allocated to the speaker.3.
Question: Control is allocated to the speaker,unless it is a response to a question or a com-mand.4.
Prompt: Control is allocated to the hearer.Since the dialogues also have a graphics compo-nent, all drawing and code change moves had con-trol assigned to the peer drawing or making the codechange.The results of the regression analysis are summa-rized in tables 1 and 2, with blank cells representingnon-significant correlations.
Pre-test score, whichrepresents the student?s initial knowledge and/or ap-titude in the area, was selected as a feature becauseit is important to understand the strength of the cor-relation between previous knowledge and post testscore when identifying additional correlating fea-tures (Yap, 1979).
The same holds for the time re-lated features (pencil time and total time).
The re-maining correlations and trends to correlation sug-gest that participation is an important factor in suc-cessful collaboration.
Since a student is more likelyto take initiative when actively participating in prob-45Prob.
3 Prob.
4 Prob.
5Predictor (Lists) (Stacks) (Trees)Pre-Test 0.334(p=0.001)0.214(p=0.017)0.269(p=0.009)TotalTime0.186(p=0.022)0.125(p=0.076)0.129(p=0.085)TotalTurns0.129(p=0.061)0.134(p=0.065)DrawTurns0.116(p=0.076)0.122(p=0.080)CodeTurns0.130(p=0.071)Table 2: Problem Score Predictors (R2)lem solving, potentially there there is a relation be-tween these participation correlations and initiative.An analysis of initiative shows that there is a cor-relation of initiative and successful collaboration.
Inproblem 3, learning gain positively correlates withthe number of turns where a student has initiative(R2 = 0.156, p = 0.037).
And in problem 4, takinginitiative through drawing has a positive impact onpost-test score (R2 = 0.155, p = 0.047).3 AnnotationSince the preliminary analysis showed a correlationof initiative with learning gain, I chose to begin athorough data analysis by annotating the dialogueswith initiative shifts.
Walker and Whittaker claimthat initiative encompasses both dialogue controland task control (Walker and Whittaker, 1990), how-ever, several others disagree.
Jordan and Di Eugeniopropose that control and initiative are two separatefeatures in collaborative problem solving dialogues(Jordan and Di Eugenio, 1997).
While control andinitiative might be synonymous for the dialogues an-alyzed by Walker and Whittaker where a master-slave assumption holds, it is not the case in collab-orative dialogues where no such assumption exists.Jordan and Di Eugenio argue that the notion of con-trol should apply to the dialogue level, while ini-tiative should pertain to the problem-solving goals.In a similar vein, Chu-Carroll and Brown also ar-gue for a distinction between control and initiative,which they term task initiative and dialogue initia-tive (Chu-Carroll and Brown, 1998).
Since there isno universally agreed upon definition for initiative, Ihave decided to annotate for both dialogue initiativeand task initiative.
For dialogue initiative annota-tion, I am using Walker and Whittaker?s utterancebased allocation of control rules (Walker and Whit-taker, 1990), which are widely used to identify di-alogue initiative.
For task initiative, I have derivedan annotation scheme based on other research in thearea.
According to Jordan and Di Eugenio, in prob-lem solving (task) initiative the agent takes it uponhimself to address domain goals by either (1)propos-ing a solution or (2)reformulating goals.
In a simi-lar vein, Guinn (Guinn, 1998) defines task initiativeas belonging to the participant who dictates whichdecomposition of the goal will be used by both par-ticipants during problem-solving.
A third definitionis from Chu-Carroll and Brown.
They suggest thattask initiative tracks the lead in development of theagent?s plan.
Since the primary goal of the dialoguesstudied by Chu-Carroll and Brown is to develop aplan, this could be re-worded to state that task ini-tiative tracks the lead in development of the agent?sgoal.
Combining these definitions, task initiative canbe defined as any action by a participant to eitherachieve a goal directly, decompose a goal or refor-mulate a goal.
Since the goals of our problems areunderstanding and potentially correcting a program,actions in our domain that show task initiative in-clude actions such as explaining what a section ofcode does or identifying a section of code that is in-correct.Two coders, the author and an outside annotator,have coded 24 dialogues (1449 utterances) for bothdialogue and task initiative.
This is approximately45% of the corpus.
The resulting intercoder reli-ability, measured with the Kappa statistic, is 0.77for dialogue initiative annotation and 0.68 for taskinitiative, both of which are high enough to supporttentative conclusions.
Using multiple linear regres-sion analysis on these annotated dialogues, I foundthat, in a subset of the problems, there was a sig-nicant correlation between post-test score (after re-moving the effects of pre-test scores) and the num-ber of switches in dialogue initiative (R2 =0.157,p=0.014).
Also, in the same subset, there was acorrelation between post-test score and the numberof turns that a student had initiative (R2 =0.077,p=0.065).
This suggests that both taking the ini-46tiative and taking turns in leading problem solvingresults in learning.Given my hypothesis that initiative can be usedto identify co-construction, the next step is to an-notate the dialogues using a subset of the DAMSLscheme (Core and Allen, 1997) to identify episodesof co-construction.
Once annotated, I will use ma-chine learning techniques to identify co-constructionusing initiative as a feature.
Since this is a classi-fication problem, algorithms such as ClassificationBased on Associations (Liu, 2007) will be used.
Ad-ditionally, I will explore those algorithms that takeinto account the sequence of actions, such as hiddenMarkov models or neural networks.4 Computational ModelThe model will be implemented as an artificialagent, KSC-PaL, that interacts with a peer in collab-orative problem solving using an interface similar tothe one that was used in data collection (see Fig-ure 1).
This agent will be an extension of the TuTalksystem, which is designed to support natural lan-guage dialogues for educational applications (Jordanet al, 2006).
TuTalk contains a core set of dialoguesystem modules that can be replaced or enhanced asrequired by the application.
The core modules areunderstanding and generation, a dialogue managerwhich is loosely characterized as a finite state ma-chine with a stack and a student model.
To imple-ment the peer agent, I will replace TuTalk?s studentmodel and add a planner module.Managing the information state of the dialogue(Larsson and Traum, 2000), which includes the be-liefs and intentions of the participants, is importantin the implementation of any dialogue agent.
KSC-PaL will use a student model to assist in manage-ment of the information state.
This student modeltracks the current state of problem solving as wellas estimates the student?s knowledge of conceptsinvolved in solving the problem by incorporatingproblem solution graphs (Conati et al, 2002).
So-lution graphs are Bayesian networks where eachnode represents either an action required to solvethe problem or a concept required as part of prob-lem solving.
After analyzing our dialogues, I real-ized that the solutions to the problems in our do-main are different from standard problem-solvingtasks.
Given that our tasks are program compre-hension tasks and that the dialogues are peer led,there can be no assumption as to the order in whicha student will analyze code statements.
Thereforea graph comprised of connected subgraphs that eachrepresent a section of the code more closely matcheswhat I observed in our dialogues.
So, we are using amodified version of solution graphs that has clustersof nodes representing facts that are relevant to theproblem.
Each cluster contains facts that are depen-dent on one another.
For example, one cluster repre-sents facts related to the push method for a stack.
Asthe code is written, it would be impossible to com-prehend the method without understanding the pre-fix notation for incrementing.
A user?s utterancesand actions can then be matched to the nodes withinthe clusters.
This provides the agent with informa-tion related to the student?s knowledge as well as thecurrent topic under discussion.A planner module will be added to TuTalk to pro-vide KSC-PaL with a more sophisticated method ofselecting scripts.
Unlike TuTalk?s dialogue managerwhich uses a simple matching of utterances to con-cepts in order to determine the script to be followed,KSC-PaL?s planner will incorporate the results of thedata analysis above and will also include the statusof the student?s knowledge, as reflected in the stu-dent model, in making script selections.
This plan-ner will potentially be a probabilistic planner suchas the one in (Lu, 2007).5 ConclusionIn conclusion, we are developing a computationalmodel of knowledge construction which incorpo-rates initiative and the balance of initiative.
Thismodel will be embedded in an artificial agent thatcollaborates with students to solve data structureproblems.
As knowledge construction has beenshown to promote learning, this research could havea profound impact on educational applications bychanging the way in which they engage students inlearning.AcknowledgmentsThe graphical interface is based on a graphical inter-face developed by Davide Fossati for an intelligenttutoring system in the same domain.47ReferencesMichelene T. H. Chi, Stephanie A. Siler, Jeong Heisawn,Takashi Yamauchi, and Robert G. Hausmann.
2001.Learning from human tutoring.
Cognitive Science,25(4):471?533.Jennifer Chu-Carroll and Michael K. Brown.
1998.
Anevidential model for tracking initiative in collabora-tive dialogue interactions.
User Modeling and User-Adapted Interaction, 8(3?4):215?253, September.Jennifer Chu-Carroll and Sandra Carberry.
1998.
Col-laborative response generation in planning dialogues.Computational Linguistics, 24(3):355?400.Cristina Conati, Abigail Gertner, and Kurt Vanlehn.2002.
Using bayesian networks to manage uncer-tainty in student modeling.
User Modeling and User-Adapted Interaction, 12(4):371?417.Mar?
?a de los Angeles Constantino-Gonza?lez andDaniel D. Suthers.
2000.
A coached collaborativelearning environment for entity-relationship modeling.Intelligent Tutoring Systems, pages 324?333.Mark G. Core and James F. Allen.
1997.
Coding dia-logues with the DAMSL annotation scheme.
In DavidTraum, editor, Working Notes: AAAI Fall Symposiumon Communicative Action in Humans and Machines,pages 28?35, Menlo Park, California.
American Asso-ciation for Artificial Intelligence.Martha W. Evens, Ru-Charn Chang, Yoon Hee Lee,Leem Seop Shim, Chong Woo Woo, Yuemei Zhang,Joel A. Michael, and Allen A. Rovick.
1997.
Circsim-tutor: an intelligent tutoring system using natural lan-guage dialogue.
In Proceedings of the fifth conferenceon Applied natural language processing, pages 13?14,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Arthur C. Graesser, Shulan Lu, George Tanner Jackson,Heather Hite Mitchell, Mathew Ventura, Andrew Ol-ney, and Max M. Louwerse.
2004.
Autotutor: A tutorwith dialogue in natural language.
Behavior ResearchMethods, Instruments, & Computers, 36:180?192(13),May.Curry I. Guinn.
1998.
An analysis of initiative selectionin collaborative task-oriented discourse.
User Model-ing and User-Adapted Interaction, 8(3-4):255?314.Robert G.M.
Hausmann, Michelee T.H.
Chi, and Mar-guerite Roy.
2004.
Learning from collaborative prob-lem solving: An analysis of three hypothesized mech-anisms.
In K.D Forbus, D. Gentner, and T. Regier, edi-tors, 26th Annual Converence of the Cognitive ScienceSociety, pages 547?552, Mahwah, NJ.Pamela W. Jordan and Barbara Di Eugenio.
1997.
Con-trol and initiative in collaborative problem solving di-alogues.
In Working Notes of the AAAI Spring Sympo-sium on Computational Models for Mixed Initiative,pages 81?84, Menlo Park, CA.Pamela W. Jordan, Michael Ringenberg, and Brian Hall.2006.
Rapidly developing dialogue systems that sup-port learning studies.
In Proceedings of ITS06 Work-shop on Teaching with Robots, Agents, and NLP, pages1?8.Staffan Larsson and David R. Traum.
2000.
Informationstate and dialogue management in the trindi dialoguemove engine toolkit.
Nat.
Lang.
Eng., 6(3-4):323?340.Bing Liu.
2007.
Web data mining: exploring hyperlinks,contents, and usage data.
Springer.Karen E. Lochbaum and Candice L. Sidner.
1990.
Mod-els of plans to support communication: An initial re-port.
In Thomas Dietterich and William Swartout, ed-itors, Proceedings of the Eighth National Conferenceon Artificial Intelligence, pages 485?490, Menlo Park,California.
AAAI Press.Xin Lu.
2007.
Expert Tutoring and Natural LanguageFeedback in Intelligent Tutoring Systems.
Ph.D. thesis,University of Illinois at Chicago.Amy Soller.
2004.
Computational modeling and analysisof knowledge sharing in collaborative distance learn-ing.
User Modeling and User-Adapted Interaction,Volume 14(4):351?381, January.Tan Bee Tin.
2003.
Does talking with peers help learn-ing?
the role of expertise and talk in convergent groupdiscussion tasks.
Journal of English for AcademicPurposes, 2(1):53?66.Kurt VanLehn, Pamela W. Jordan, Carolyn PensteinRose?, Dumisizwe Bhembe, Michael Bo?ttner, AndyGaydos, Maxim Makatchev, Umarani Pappuswamy,Michael A. Ringenberg, Antonio Roque, StephanieSiler, and Ramesh Srivastava.
2002.
The architec-ture of why2-atlas: A coach for qualitative physics es-say writing.
In ITS ?02: Proceedings of the 6th Inter-national Conference on Intelligent Tutoring Systems,pages 158?167, London, UK.
Springer-Verlag.Aurora Vizca??no.
2005.
A simulated student can im-prove collaborative learning.
International Journal ofArtificial Intelligence in Education, 15(1):3?40.Marilyn Walker and Steve Whittaker.
1990.
Mixed ini-tiative in dialogue: an investigation into discourse seg-mentation.
In Proceedings of the 28th annual meetingon Association for Computational Linguistics, pages70?78, Morristown, NJ, USA.
Association for Com-putational Linguistics.Kim Onn Yap.
1979.
Pretest-posttest correlation andregression models.
Presented at the Annual Meet-ing of the American Educational Research Association(63rd, San Francisco, California), April 8?12.48
