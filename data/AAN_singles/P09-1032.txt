Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 280?287,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPLearning with Annotation NoiseEyal BeigmanOlin Business SchoolWashington University in St. Louisbeigman@wustl.eduBeata Beigman KlebanovKellogg School of ManagementNorthwestern Universitybeata@northwestern.eduAbstractIt is usually assumed that the kind of noiseexisting in annotated data is random clas-sification noise.
Yet there is evidencethat differences between annotators are notalways random attention slips but couldresult from different biases towards theclassification categories, at least for theharder-to-decide cases.
Under an annota-tion generation model that takes this intoaccount, there is a hazard that some of thetraining instances are actually hard caseswith unreliable annotations.
We showthat these are relatively unproblematic foran algorithm operating under the 0-1 lossmodel, whereas for the commonly usedvoted perceptron algorithm, hard trainingcases could result in incorrect predictionon the uncontroversial cases at test time.1 IntroductionIt is assumed, often tacitly, that the kind ofnoise existing in human-annotated datasets used incomputational linguistics is random classificationnoise (Kearns, 1993; Angluin and Laird, 1988),resulting from annotator attention slips randomlydistributed across instances.
For example, Os-borne (2002) evaluates noise tolerance of shallowparsers, with random classification noise taken tobe ?crudely approximating annotation errors.?
Ithas been shown, both theoretically and empiri-cally, that this type of noise is tolerated well bythe commonly used machine learning algorithms(Cohen, 1997; Blum et al, 1996; Osborne, 2002;Reidsma and Carletta, 2008).Yet this might be overly optimistic.
Reidsmaand op den Akker (2008) show that apparent dif-ferences between annotators are not random slipsof attention but rather result from different biasesannotators might have towards the classificationcategories.
When training data comes from oneannotator and test data from another, the first an-notator?s biases are sometimes systematic enoughfor a machine learner to pick them up, with detri-mental results for the algorithm?s performance onthe test data.
A small subset of doubly anno-tated data (for inter-annotator agreement check)and large chunks of singly annotated data (fortraining algorithms) is not uncommon in compu-tational linguistics datasets; such a setup is proneto problems if annotators are differently biased.1Annotator bias is consistent with a number ofnoise models.
For example, it could be that anannotator?s bias is exercised on each and every in-stance, making his preferred category likelier forany instance than in another person?s annotations.Another possibility, recently explored by BeigmanKlebanov and Beigman (2009), is that some itemsare really quite clear-cut for an annotator with anybias, belonging squarely within one particular ca-tegory.
However, some instances ?
termed hardcases therein ?
are harder to decide upon, and thisis where various preferences and biases come intoplay.
In a metaphor annotation study reported byBeigman Klebanov et al (2008), certain markupsreceived overwhelming annotator support whenpeople were asked to validate annotations after acertain time delay.
Other instances saw opinionssplit; moreover, Beigman Klebanov et al (2008)observed cases where people retracted their ownearlier annotations.To start accounting for such annotator behavior,Beigman Klebanov and Beigman (2009) proposeda model where instances are either easy, and thenall annotators agree on them, or hard, and theneach annotator flips his or her own coin to de-1The different biases might not amount to much in thesmall doubly annotated subset, resulting in acceptable inter-annotator agreement; yet when enacted throughout a largenumber of instances they can be detrimental from a machinelearner?s perspective.280cide on a label (each annotator can have a different?coin?
reflecting his or her biases).
For annota-tions generated under such a model, there is a dan-ger of hard instances posing as easy ?
an observedagreement between annotators being a result of allcoins coming up heads by chance.
They thereforedefine the expected proportion of hard instances inagreed items as annotation noise.
They providean example from the literature where an annota-tion noise rate of about 15% is likely.The question addressed in this article is: Howproblematic is learning from training data with an-notation noise?
Specifically, we are interested inestimating the degree to which performance oneasy instances at test time can be hurt by the pre-sence of hard instances in training data.Definition 1 The hard case bias, ?
, is the portionof easy instances in the test data that are misclas-sified as a result of hard instances in the trainingdata.This article proceeds as follows.
First, we showthat a machine learner operating under a 0-1 lossminimization principle could sustain a hard casebias of ?
( 1?N) in the worst case.
Thus, while an-notation noise is hazardous for small datasets, it isbetter tolerated in larger ones.
However, 0-1 lossminimization is computationally intractable forlarge datasets (Feldman et al, 2006; Guruswamiand Raghavendra, 2006); substitute loss functionsare often used in practice.
While their tolerance torandom classification noise is as good as for 0-1loss, their tolerance to annotation noise is worse.For example, the perceptron family of algorithmshandle random classification noise well (Cohen,1997).
We show in section 3.4 that the widelyused Freund and Schapire (1999) voted percep-tron algorithm could face a constant hard case biaswhen confronted with annotation noise in trainingdata, irrespective of the size of the dataset.
Finally,we discuss the implications of our findings for thepractice of annotation studies and for data utiliza-tion in machine learning.2 0-1 LossLet a sample be a sequence x1, .
.
.
, xN drawn uni-formly from the d-dimensional discrete cube Id ={?1, 1}d with corresponding labels y1, .
.
.
, yN ?
{?1, 1}.
Suppose further that the learning al-gorithm operates by finding a hyperplane (w,?
),w ?
Rd, ?
?
R, that minimizes the empirical er-rorL(w,?)
=?j=1...N [yj?sgn(?i=1...d xijwi??)]2.
Let there be H hard cases, such that the an-notation noise is ?
= HN .2Theorem 1 In the worst case configuration of in-stances a hard case bias of ?
= ?
( 1?N) cannot beruled out with constant confidence.Idea of the proof : We prove by explicit con-struction of an adversarial case.
Suppose there isa plane that perfectly separates the easy instances.The ?
(N) hard instances will be concentrated ina band parallel to the separating plane, that isnear enough to the plane so as to trap only about?
(?N) easy instances between the plane and theband (see figure 1 for an illustration).
For a ran-dom labeling of the hard instances, the centrallimit theorem shows there is positive probabilitythat there would be an imbalance between +1 and?1 labels in favor of ?1s on the scale of?N ,which, with appropriate constants, would lead tothe movement of the empirically minimal separa-tion plane to the right of the hard case band, mis-classifying the trapped easy cases.Proof : Let v = v(x) =?i=1...d xi denote thesum of the coordinates of an instance in Id andtake ?e =?d ?
F?1(??
?
2?d2 + 12) and ?h =?d ?
F?1(?
+??
?
2?d2 + 12), where F (t) is thecumulative distribution function of the normal dis-tribution.
Suppose further that instances xj suchthat ?e < vj < ?h are all and only hard instances;their labels are coinflips.
All other instances areeasy, and labeled y = y(x) = sgn(v).
In this case,the hyperplane 1?d(1 .
.
.
1) is the true separationplane for the easy instances, with ?
= 0.
Figure 1shows this configuration.According to the central limit theorem, for d,Nlarge, the distribution of v is well approximated byN (0,?d).
If N = c1 ?
2d, for some 0 < c1 < 4,the second application of the central limit the-orem ensures that, with high probability, about?N = c1?2d items would fall between ?e and ?h(all hard), and??
?
2?d2N = c1?
?2d would fallbetween 0 and ?e (all easy, all labeled +1).Let Z be the sum of labels of the hard cases,Z =?i=1...H yi.
Applying the central limit the-orem a third time, for large N , Z will, with ahigh probability, be distributed approximately as2In Beigman Klebanov and Beigman (2009), annotationnoise is defined as percentage of hard instances in the agreedannotations; this implies noise measurement on multiply an-notated material.
When there is just one annotator, no dis-tinction between easy vs hard instances can be made; in thissense, all hard instances are posing as easy.2810 ?e ?hFigure 1: The adversarial case for 0-1 loss.Squares correspond to easy instances, circles ?
tohard ones.
Filled squares and circles are labeled?1, empty ones are labeled +1.N (0,??N).
This implies that a value as low as?2?
cannot be ruled out with high (say 95%) con-fidence.
Thus, an imbalance of up to 2?
?N , or of2?c1?2d, in favor of ?1s is possible.There are between 0 and ?h about 2?c1?
?2dmore?1 hard instances than +1 hard instances, asopposed to c1?
?2d easy instances that are all +1.As long as c1 < 2?c1, i.e.
c1 < 4, the empiricallyminimal threshold would move to ?h, resulting ina hard case bias of ?
=???c12d(1??
)?c12d= ?
( 1?N).To see that this is the worst case scenario, wenote that 0-1 loss sustained on ?
(N) hard casesis the order of magnitude of the possible imba-lance between ?1 and +1 random labels, whichis ?(?N).
For hard case loss to outweigh the losson the misclassified easy instances, there cannotbe more than ?
(?N) of the latter 2Note that the proof requires that N = ?
(2d)namely, that asymptotically the sample includesa fixed portion of the instances.
If the sample isasymptotically smaller, then ?e will have to be ad-justed such that ?e =?d ?
F?1(?
( 1?N) + 12).According to theorem 1, for a 10K dataset with15% hard case rate, a hard case bias of about 1%cannot be ruled out with 95% confidence.Theorem 1 suggests that annotation noise asdefined here is qualitatively different from moremalicious types of noise analyzed in the agnosticlearning framework (Kearns and Li, 1988; Haus-sler, 1992; Kearns et al, 1994), where an adver-sary can not only choose the placement of the hardcases, but also their labels.
In worst case, the 0-1loss model would sustain a constant rate of errordue to malicious noise, whereas annotation noiseis tolerated quite well in large datasets.3 Voted PerceptronFreund and Schapire (1999) describe the votedperceptron.
This algorithm and its many vari-ants are widely used in the computational lin-guistics community (Collins, 2002a; Collins andDuffy, 2002; Collins, 2002b; Collins and Roark,2004; Henderson and Titov, 2005; Viola andNarasimhan, 2005; Cohen et al, 2004; Carreraset al, 2005; Shen and Joshi, 2005; Ciaramita andJohnson, 2003).
In this section, we show that thevoted perceptron can be vulnerable to annotationnoise.
The algorithm is shown below.Algorithm 1 Voted PerceptronTrainingInput: a labeled training set (x1, y1), .
.
.
, (xN , yN )Output: a list of perceptrons w1, .
.
.
, wNInitialize: t?
0; w1 ?
0; ?1 ?
0for t = 1 .
.
.
N doy?t ?
sign(?wt, xt?+ ?t)wt+1 ?
wt + yt?y?t2 ?
xt?t+1 ?
?t + yt?y?t2 ?
?wt, xt?end forForecastingInput: a list of perceptrons w1, .
.
.
, wNan unlabeled instance xOutput: A forecasted label yy?
?PNt=1 sign(?wt, xt?+ ?t)y ?
sign(y?
)The voted perceptron algorithm is a refinementof the perceptron algorithm (Rosenblatt, 1962;Minsky and Papert, 1969).
Perceptron is a dy-namic algorithm; starting with an initial hyper-plane w0, it passes repeatedly through the labeledsample.
Whenever an instance is misclassifiedby wt, the hyperplane is modified to adapt to theinstance.
The algorithm terminates once it haspassed through the sample without making anyclassification mistakes.
The algorithm terminatesiff the sample can be separated by a hyperplane,and in this case the algorithm finds a separatinghyperplane.
Novikoff (1962) gives a bound on thenumber of iterations the algorithm goes throughbefore termination, when the sample is separableby a margin.282The perceptron algorithm is vulnerable to noise,as even a little noise could make the sample in-separable.
In this case the algorithm would cycleindefinitely never meeting termination conditions,wt would obtain values within a certain dynamicrange but would not converge.
In such setting,imposing a stopping time would be equivalent todrawing a random vector from the dynamic range.Freund and Schapire (1999) extend the percep-tron to inseparable samples with their voted per-ceptron algorithm and give theoretical generaliza-tion bounds for its performance.
The basic ideaunderlying the algorithm is that if the dynamicrange of the perceptron is not too large then wtwould classify most instances correctly most ofthe time (for most values of t).
Thus, for a samplex1, .
.
.
, xN the new algorithm would keep trackof w0, .
.
.
, wN , and for an unlabeled instance x itwould forecast the classification most prominentamongst these hyperplanes.The bounds given by Freund and Schapire(1999) depend on the hinge loss of the dataset.
Insection 3.2 we construct a difficult setting for thisalgorithm.
To prove that voted perceptron wouldsuffer from a constant hard case bias in this set-ting using the exact dynamics of the perceptron isbeyond the scope of this article.
Instead, in sec-tion 3.3 we provide a lower bound on the hingeloss for a simplified model of the perceptron algo-rithm dynamics, which we argue would be a goodapproximation to the true dynamics in the settingwe constructed.
For this simplified model, weshow that the hinge loss is large, and the boundsin Freund and Schapire (1999) cannot rule out aconstant level of error regardless of the size of thedataset.
In section 3.4 we study the dynamics ofthe model and prove that ?
= ?
(1) for the adver-sarial setting.3.1 Hinge LossDefinition 2 The hinge loss of a labeled instance(x, y) with respect to hyperplane (w,?)
and mar-gin ?
> 0 is given by ?
= ?
(?, ?)
= max(0, ?
?y ?
(?w, x?
?
?)).?
measures the distance of an instance frombeing classified correctly with a ?margin.
Figure 2shows examples of hinge loss for various datapoints.Theorem 2 (Freund and Schapire (1999))After one pass on the sample, the probabilitythat the voted perceptron algorithm does not?
??
???
?Figure 2: Hinge loss ?
for various data points in-curred by the separator with margin ?.predict correctly the label of a test instancexN+1 is bounded by 2N+1EN+1[d+D?
]2whereD = D(w,?, ?)
=?
?Ni=1 ?2i .This result is used to explain the convergence ofweighted or voted perceptron algorithms (Collins,2002a).
It is useful as long as the expected value ofD is not too large.
We show that in an adversarialsetting of the annotation noise D is large, hencethese bounds are trivial.3.2 Adversarial Annotation NoiseLet a sample be a sequence x1, .
.
.
, xN drawn uni-formly from Id with y1, .
.
.
, yN ?
{?1, 1}.
Easycases are labeled y = y(x) = sgn(v) as before,with v = v(x) =?i=1...d xi.
The true separationplane for the easy instances is w?
= 1?d(1 .
.
.
1),??
= 0.
Suppose hard cases are those wherev(x) > c1?d, where c1 is chosen so that thehard instances account for ?N of all instances.3Figure 3 shows this setting.3.3 Lower Bound on Hinge LossIn the simplified case, we assume that the algo-rithm starts training with the hyperplane w0 =w?
= 1?d(1 .
.
.
1), and keeps it throughout thetraining, only updating ?.
In reality, each hard in-stance can be decomposed into a component that isparallel to w?, and a component that is orthogonalto it.
The expected contribution of the orthogonal3See the proof of 0-1 case for a similar construction usingthe central limit theorem.2830 c1?dFigure 3: An adversarial case of annotation noisefor the voted perceptron algorithm.component to the algorithm?s update will be posi-tive due to the systematic positioning of the hardcases, while the contributions of the parallel com-ponents are expected to cancel out due to the sym-metry of the hard cases around the main diagonalthat is orthogonal to w?.
Thus, while wt will notnecessarily parallel w?, it will be close to parallelfor most t > 0.
The simplified case is thus a goodapproximation of the real case, and the bound weobtain is expected to hold for the real case as well.For any initial value ?0 < 0 all misclassified in-stances are labeled ?1 and classified as +1, hencethe update will increase ?0, and reach 0 soonenough.
We can therefore assume that ?t ?
0for any t > t0 where t0  N .Lemma 3 For any t > t0, there exist ?
=?
(?, T ) > 0 such that E(?2) ?
?
?
?.Proof : For ?
?
0 there are two main sourcesof hinge loss: easy +1 instances that are clas-sified as ?1, and hard -1 instances classified as+1.
These correspond to the two components ofthe following sum (the inequality is due to disre-garding the loss incurred by a correct classificationwith too wide a margin):E(?2) ?[?]?l=012d(dl)(?
?d?l?d+ ?)2+12d?l=c1?d12d(dl)(l?d??
?d+ ?
)2Let 0 < T < c1 be a parameter.
For ?
> T?d,misclassified easy instances dominate the loss:E(?2) ?[?]?l=012d(dl)(?
?d?l?d+ ?)2?
[T?d]?l=012d(dl)(T?d?d?l?d+ ?
)2?T?d?l=012d(dl)(T ?l?d+ ?)2?1?2pi?
T0(T + ?
?
t)2e?t2/2dt = HT (?
)The last inequality follows from a normal ap-proximation of the binomial distribution (see, forexample, Feller (1968)).For 0 ?
?
?
T?d, misclassified hard casesdominate:E(?2) ?12d?l=c1?d12d(dl)(l?d??
?d+ ?
)2?12d?l=c1?d12d(dl)(l?d?T?d?d+ ?)2?12?1?2pi?
???1(?)(t?
T + ?
)2e?t2/2dt= H?(?
)where ??1(?)
is the inverse of the normal distri-bution density.Thus E(?2) ?
min{HT (?),H?(?
)}, andthere exists ?
= ?
(?, T ) > 0 such thatmin{HT (?),H?(?)}
?
?
?
?
2Corollary 4 The bound in theorem 2 does notconverge to zero for large N .We recall that Freund and Schapire (1999) boundis proportional to D2 =?Ni=1 ?2i .
It follows fromlemma 3 that D2 = ?
(N), hence the bound is in-effective.3.4 Lower Bound on ?
for Voted PerceptronUnder Simplified DynamicsCorollary 4 does not give an estimate on the hardcase bias.
Indeed, it could be that wt = w?
foralmost every t. There would still be significanthinge in this case, but the hard case bias for thevoted forecast would be zero.
To assess the hardcase bias we need a model of perceptron dyna-mics that would account for the history of hyper-planesw0, .
.
.
, wN the perceptron goes through on284a sample x1, .
.
.
, xN .
The key simplification inour model is assuming that wt parallels w?
for allt, hence the next hyperplane depends only on theoffset ?t.
This is a one dimensional Markov ran-dom walk governed by the distributionP(?t+1?
?t = r|?t) = P(x|yt ?
y?t2?
?w?, x?
= r)In general ?d ?
?t ?
d but as mentioned beforelemma 3, we may assume ?t > 0.Lemma 5 There exists c > 0 such that with a highprobability ?t > c ?
?d for most 0 ?
t ?
N .Proof : Let c0 = F?1(?2 +12); c1 = F?1(1??
).We designate the intervals I0 = [0, c0 ?
?d]; I1 =[c0 ?
?d, c1 ?
?d] and I2 = [c1 ?
?d, d] and defineAi = {x : v(x) ?
Ii} for i = 0, 1, 2.
Note that theconstants c0 and c1 are chosen so that P(A0) =?2and P(A2) = ?.
It follows from the constructionin section 3.2 that A0 and A1 are easy instancesand A2 are hard.
Given a sample x1, .
.
.
, xN , amisclassification of xt ?
A0 by ?t could only hap-pen when an easy +1 instance is classified as ?1.Thus the algorithm would shift ?t to the left byno more than |vt ?
?t| since vt = ?w?, xt?.
Thisshows that ?t ?
I0 implies ?t+1 ?
I0.
In thesame manner, it is easy to verify that if ?t ?
Ijand xt ?
Ak then ?t+1 ?
Ik, unless j = 0 andk = 1, in which case ?t+1 ?
I0 because xt ?
A1would be classified correctly by ?t ?
I0.We construct a Markov chain with three statesa0 = 0, a1 = c0 ?
?d and a2 = c1 ?
?d governedby the following transition distribution:????1?
?2 0?2?2 1?
?
?2?212 ?3?212 + ????
?Let Xt be the state at time t. The principal eigen-vector of the transition matrix (13 ,13 ,13) gives thestationary probability distribution of Xt.
ThusXt ?
{a1, a2} with probability 23 .
Since the tran-sition distribution of Xt mirrors that of ?t, andsince aj are at the leftmost borders of Ij , respec-tively, it follows that Xt ?
?t for all t, thusXt ?
{a1, a2} implies ?t ?
I1?I2.
It follows that?t > c0 ?
?d with probability 23 , and the lemmafollows from the law of large numbers 2Corollary 6 With high probability ?
= ?
(1).Proof : Lemma 5 shows that for a samplex1, .
.
.
, xN with high probability ?t is most ofthe time to the right of c ??d.
Consequentlyfor any x in the band 0 ?
v ?
c ?
?d we getsign(?w?, x?+?t) = ?1 for most t hence by defi-nition, the voted perceptron would classify suchan instance as ?1, although it is in fact a +1 easyinstance.
Since there are ?
(N) misclassified easyinstances, ?
= ?
(1) 24 DiscussionIn this article we show that training with annota-tion noise can be detrimental for test-time resultson easy, uncontroversial instances; we termed thisphenomenon hard case bias.
Although underthe 0-1 loss model annotation noise can be tole-rated for larger datasets (theorem 1), minimizingsuch loss becomes intractable for larger datasets.Freund and Schapire (1999) voted perceptron al-gorithm and its variants are widely used in compu-tational linguistics practice; our results show thatit could suffer a constant rate of hard case bias ir-respective of the size of the dataset (section 3.4).How can hard case bias be reduced?
One pos-sibility is removing as many hard cases as onecan not only from the test data, as suggested inBeigman Klebanov and Beigman (2009), but fromthe training data as well.
Adding the second an-notator is expected to detect about half the hardcases, as they would surface as disagreements be-tween the annotators.
Subsequently, a machinelearner can be told to ignore those cases duringtraining, reducing the risk of hard case bias.
Whilethis is certainly a daunting task, it is possible thatfor annotation studies that do not require expertannotators and extensive annotator training, thenewly available access to a large pool of inexpen-sive annotators, such as the Amazon MechanicalTurk scheme (Snow et al, 2008),4 or embeddingthe task in an online game played by volunteers(Poesio et al, 2008; von Ahn, 2006) could providesome solutions.Reidsma and op den Akker (2008) suggest adifferent option.
When non-overlapping parts ofthe dataset are annotated by different annotators,each classifier can be trained to reflect the opinion(albeit biased) of a specific annotator, using dif-ferent parts of the datasets.
Such ?subjective ma-chines?
can be applied to a new set of data; anitem that causes disagreement between classifiersis then extrapolated to be a case of potential dis-agreement between the humans they replicate, i.e.4http://aws.amazon.com/mturk/285a hard case.
Our results suggest that, regardlessof the success of such an extrapolation scheme indetecting hard cases, it could erroneously invali-date easy cases: Each classifier would presumablysuffer from a certain hard case bias, i.e.
classifyincorrectly things that are in fact uncontroversialfor any human annotator.
If each such classifierhas a different hard case bias, some inter-classifierdisagreements would occur on easy cases.
De-pending on the distribution of those easy cases inthe feature space, this could invalidate valuablecases.
If the situation depicted in figure 1 corre-sponds to the pattern learned by one of the clas-sifiers, it would lead to marking the easy casesclosest to the real separation boundary (those be-tween 0 and ?e) as hard, and hence unsuitable forlearning, eliminating the most informative mate-rial from the training data.Reidsma and Carletta (2008) recently showedby simulation that different types of annotatorbehavior have different impact on the outcomes ofmachine learning from the annotated data.
Our re-sults provide a theoretical analysis that points inthe same direction: While random classificationnoise is tolerable, other types of noise ?
such asannotation noise handled here ?
are more proble-matic.
It is therefore important to develop modelsof annotator behavior and of the resulting imper-fections of the annotated datasets, in order to di-agnose the potential learning problem and suggestmitigation strategies.ReferencesDana Angluin and Philip Laird.
1988.
Learning fromNoisy Examples.
Machine Learning, 2(4):343?370.Beata Beigman Klebanov and Eyal Beigman.
2009.From Annotator Agreement to Noise Models.
Com-putational Linguistics, accepted for publication.Beata Beigman Klebanov, Eyal Beigman, and DanielDiermeier.
2008.
Analyzing Disagreements.
InCOLING 2008 Workshop on Human Judgments inComputational Linguistics, pages 2?7, Manchester,UK.Avrim Blum, Alan Frieze, Ravi Kannan, and SantoshVempala.
1996.
A Polynomial-Time Algorithm forLearning Noisy Linear Threshold Functions.
In Pro-ceedings of the 37th Annual IEEE Symposium onFoundations of Computer Science, pages 330?338,Burlington, Vermont, USA.Xavier Carreras, Llu?is Ma`rquez, and Jorge Castro.2005.
Filtering-Ranking Perceptron Learning forPartial Parsing.
Machine Learning, 60(1):41?71.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense Tagging of Unknown Nouns in WordNet.In Proceedings of the Empirical Methods in NaturalLanguage Processing Conference, pages 168?175,Sapporo, Japan.William Cohen, Vitor Carvalho, and Tom Mitchell.2004.
Learning to Classify Email into ?SpeechActs?.
In Proceedings of the Empirical Methodsin Natural Language Processing Conference, pages309?316, Barcelona, Spain.Edith Cohen.
1997.
Learning Noisy Perceptrons bya Perceptron in Polynomial Time.
In Proceedingsof the 38th Annual Symposium on Foundations ofComputer Science, pages 514?523, Miami Beach,Florida, USA.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
InProceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 263?370,Philadelphia, USA.Michael Collins and Brian Roark.
2004.
Incremen-tal Parsing with the Perceptron Algorithm.
In Pro-ceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics, pages 111?118,Barcelona, Spain.Michael Collins.
2002a.
Discriminative TrainingMethods for Hidden Markov Hodels: Theory andExperiments with Perceptron Algorithms.
In Pro-ceedings of the Empirical Methods in Natural Lan-guage Processing Conference, pages 1?8, Philadel-phia, USA.Michael Collins.
2002b.
Ranking Algorithms forNamed Entity Extraction: Boosting and the VotedPerceptron.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 489?496, Philadelphia, USA.Vitaly Feldman, Parikshit Gopalan, Subhash Khot, andAshok Ponnuswami.
2006.
New Results for Learn-ing Noisy Parities and Halfspaces.
In Proceedingsof the 47th Annual IEEE Symposium on Foundationsof Computer Science, pages 563?574, Los Alamitos,CA, USA.William Feller.
1968.
An Introduction to ProbabilityTheory and Its Application, volume 1.
Wiley, NewYork, 3rd edition.Yoav Freund and Robert Schapire.
1999.
Large Mar-gin Classification Using the Perceptron Algorithm.Machine Learning, 37(3):277?296.Venkatesan Guruswami and Prasad Raghavendra.2006.
Hardness of Learning Halfspaces with Noise.In Proceedings of the 47th Annual IEEE Symposiumon Foundations of Computer Science, pages 543?552, Los Alamitos, CA, USA.286David Haussler.
1992.
Decision Theoretic General-izations of the PAC Model for Neural Net and otherLearning Applications.
Information and Computa-tion, 100(1):78?150.James Henderson and Ivan Titov.
2005.
Data-DefinedKernels for Parse Reranking Derived from Proba-bilistic Models.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 181?188, Ann Arbor, Michigan, USA.Michael Kearns and Ming Li.
1988.
Learning in thePresence of Malicious Errors.
In Proceedings of the20th Annual ACM symposium on Theory of Comput-ing, pages 267?280, Chicago, USA.Michael Kearns, Robert Schapire, and Linda Sellie.1994.
Toward Efficient Agnostic Learning.
Ma-chine Learning, 17(2):115?141.Michael Kearns.
1993.
Efficient Noise-TolerantLearning from Statistical Queries.
In Proceedingsof the 25th Annual ACM Symposium on Theory ofComputing, pages 392?401, San Diego, CA, USA.Marvin Minsky and Seymour Papert.
1969.
Percep-trons: An Introduction to Computational Geometry.MIT Press, Cambridge, Mass.A.
B. Novikoff.
1962.
On convergence proofs on per-ceptrons.
Symposium on the Mathematical Theoryof Automata, 12:615?622.Miles Osborne.
2002.
Shallow Parsing Using Noisyand Non-Stationary Training Material.
Journal ofMachine Learning Research, 2:695?719.Massimo Poesio, Udo Kruschwitz, and ChamberlainJon.
2008.
ANAWIKI: Creating Anaphorically An-notated Resources through Web Cooperation.
InProceedings of the 6th International Language Re-sources and Evaluation Conference, Marrakech,Morocco.Dennis Reidsma and Jean Carletta.
2008.
Reliabilitymeasurement without limit.
Computational Linguis-tics, 34(3):319?326.Dennis Reidsma and Rieks op den Akker.
2008.
Ex-ploiting Subjective Annotations.
In COLING 2008Workshop on Human Judgments in ComputationalLinguistics, pages 8?16, Manchester, UK.Frank Rosenblatt.
1962.
Principles of Neurodynamics:Perceptrons and the Theory of Brain Mechanisms.Spartan Books, Washington, D.C.Libin Shen and Aravind Joshi.
2005.
Incremen-tal LTAG Parsing.
In Proceedings of the HumanLanguage Technology Conference and EmpiricalMethods in Natural Language Processing Confer-ence, pages 811?818, Vancouver, British Columbia,Canada.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and Fast ?
But is itGood?
Evaluating Non-Expert Annotations for Nat-ural Language Tasks.
In Proceedings of the Empir-ical Methods in Natural Language Processing Con-ference, pages 254?263, Honolulu, Hawaii.Paul Viola and Mukund Narasimhan.
2005.
Learningto Extract Information from Semi-Structured TextUsing a Discriminative Context Free Grammar.
InProceedings of the 28th Annual International ACMSIGIR Conference on Research and Developmentin Information Retrieval, pages 330?337, Salvador,Brazil.Luis von Ahn.
2006.
Games with a purpose.
Com-puter, 39(6):92?94.287
