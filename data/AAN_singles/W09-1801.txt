Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 1?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSummarization with a Joint Modelfor Sentence Extraction and CompressionAndre?
F. T.
Martins??
and Noah A.
Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal{afm,nasmith}@cs.cmu.eduAbstractText summarization is one of the oldest prob-lems in natural language processing.
Popu-lar approaches rely on extracting relevant sen-tences from the original documents.
As a sideeffect, sentences that are too long but partlyrelevant are doomed to either not appear in thefinal summary, or prevent inclusion of otherrelevant sentences.
Sentence compression is arecent framework that aims to select the short-est subsequence of words that yields an infor-mative and grammatical sentence.
This workproposes a one-step approach for documentsummarization that jointly performs sentenceextraction and compression by solving an in-teger linear program.
We report favorable ex-perimental results on newswire data.1 IntroductionAutomatic text summarization dates back to the1950s and 1960s (Luhn, 1958; Baxendale, 1958; Ed-mundson, 1969).
Today, the proliferation of digitalinformation makes research on summarization tech-nologies more important than ever before.
In the lasttwo decades, machine learning techniques have beenemployed in extractive summarization of singledocuments (Kupiec et al, 1995; Aone et al, 1999;Osborne, 2002) and multiple documents (Radev andMcKeown, 1998; Carbonell and Goldstein, 1998;Radev et al, 2000).
Most of this work aims onlyto extract relevant sentences from the original doc-uments and present them as the summary; this sim-plification of the problem yields scalable solutions.Some attention has been devoted by the NLPcommunity to the related problem of sentence com-pression (Knight and Marcu, 2000): given a longsentence, how to maximally compress it into a gram-matical sentence that still preserves all the rele-vant information?
While sentence compression isa promising framework with applications, for exam-ple, in headline generation (Dorr et al, 2003; Jin,2003), little work has been done to include it as amodule in document summarization systems.
Mostexisting approaches (with some exceptions, like thevine-growth model of Daume?, 2006) use a two-stagearchitecture, either by first extracting a certain num-ber of salient sentences and then feeding them intoa sentence compressor, or by first compressing allsentences and extracting later.
However, regardlessof which operation is performed first?compressionor extraction?two-step ?pipeline?
approaches mayfail to find overall-optimal solutions; often the sum-maries are not better that the ones produced by ex-tractive summarization.
On the other hand, a pilotstudy carried out by Lin (2003) suggests that sum-marization systems that perform sentence compres-sion have the potential to beat pure extractive sys-tems if they model cross-sentence effects.In this work, we address this issue by merging thetasks of sentence extraction and sentence compres-sion into a global optimization problem.
A carefuldesign of the objective function encourages ?sparsesolutions,?
i.e., solutions that involve only a smallnumber of sentences whose compressions are to beincluded in the summary.
Our contributions are:?
We cast joint sentence extraction and compressionas an integer linear program (ILP);?
We provide a new formulation of sentence com-pression using dependency parsing informationthat only requires a linear number of variables,and combine it with a bigram model;?
We show how the full model can be trained in amax-margin framework.
Since a dataset of sum-maries comprised of extracted, compressed sen-tences is unavailable, we present a procedure thattrains the compression and extraction models sep-arately and tunes a parameter to interpolate the1two models.The compression model and the full system arecompared with state-of-the-art baselines in standardnewswire datasets.
This paper is organized as fol-lows: ?2?3 provide an overview of our two buildingblocks, sentence extraction and sentence compres-sion.
?4 describes our method to perform one-stepsentence compression and extraction.
?5 shows ex-periments in newswire data.
Finally, ?6 concludesthe paper and suggests future work.2 Extractive summarizationExtractive summarization builds a summary by ex-tracting a few informative sentences from the docu-ments.
Let D , {t1, .
.
.
, tM} be a set of sentences,contained in a single or in multiple related docu-ments.1 The goal is to extract the best sequence ofsentences ?ti1 , .
.
.
, tiK ?
that summarizes D whosetotal length does not exceed a fixed budget of Jwords.
We describe some well-known approachesthat wil serve as our experimental baselines.Extract the leading sentences (Lead).
Forsingle-document summarization, the simplestmethod consists of greedily extracting the leadingsentences while they fit into the summary.
A sen-tence is skipped if its inclusion exceeds the budget,and the next is examined.
This performs extremelywell in newswire articles, due to the journalisticconvention of summarizing the article first.Rank by relevance (Rel).
This method ranks sen-tences by a relevance score, and then extracts the topones that can fit into the summary.
The score is typ-ically a linear function of feature values:scorerel(ti) , ?>f(ti) = ?Dd=1 ?dfd(ti), (1)Here, each fd(ti) is a feature extracted from sen-tence ti, and ?d is the corresponding weight.
In ourexperiments, relevance features include (i) the recip-rocal position in the document, (ii) a binary featureindicating whether the sentence is the first one, and(iii) the 1-gram and 2-gram cosine similarity withthe headline and with the full document.1For simplicity, we describe a unified framework for singleand multi-document summarization, although they may requirespecialized strategies.
Here we experiment only with single-document summarization and assume t1, ..., tM are ordered.Maximal Marginal Relevance (MMR).
For longdocuments or large collections, it becomes impor-tant to penalize the redundancy among the extractedsentences.
Carbonell and Goldstein (1998) proposedgreedily adding sentences to the summary S to max-imize, at each step, a score of the form?
?
scorerel(ti) ?
(1 ?
?)
?
scorered(ti, S), (2)where scorerel(ti) is as in Eq.
1 and scorered(ti, S)accounts for the redundancy between ti and the cur-rent summary S. In our experiments, redundancy isthe 1-gram cosine similarity between the sentenceti and the current summary S. The trade-off be-tween relevance and redundancy is controlled by?
?
[0, 1], which is tuned on development data.McDonald (2007) proposed a non-greedy variantof MMR that takes into account the redundancy be-tween each pair of candidate sentences.
This is castas a global optimization problem:S?
= argmaxS?
?
?ti?S scorerel(ti) ?
(1 ?
?)
?
?ti,tj?S scorered(ti, tj), (3)where scorerel(ti) , ?>relfrel(ti), scorered(ti, tj) ,?>redfred(ti, tj), and frel(ti) and fred(ti, tj) are featurevectors with corresponding learned weight vectors?rel and ?red.
He has shown how the relevance-basedmethod and the MMR framework (in the non-greedyform of Eq.
3) can be cast as an ILP.
By introducingindicator variables ?
?i?i=1,...,M and ?
?ij?i,j=1,...,Mwith the meanings?i ={ 1 if ti is to be extracted0 otherwise?ij ={ 1 if ti and tj are both to be extracted0 otherwise(4)one can reformulate Eq.
3 as an ILP with O(M2)variables and constraints:max??i?,??ij??
?
?Mi=1 ?iscorerel(ti) ?
(5)(1 ?
?)
?
?Mi=1?Mj=1 ?ijscorered(ti, tj),subject to binary constraints ?i, ?ij ?
{0, 1}, thelength constraint?Mi=1 ?iNi ?
J (where Ni is thenumber of words of the ith sentence), and the fol-lowing ?agreement constraints?
for i, j = 1, .
.
.
,M2(that impose the logical relation ?ij = ?i ?
?j):?ij ?
?i, ?ij ?
?j , ?ij ?
?i + ?j ?
1 (6)Let us provide a compact representation of the pro-gram in Eq.
5 that will be used later.
Define our vec-tor of parameters as ?
, [??rel,?(1??)?red].
Pack-ing all the feature vectors (one for each sentence, andone for each pair of sentences) into a matrix F,F ,[ Frel 00 Fred], (7)with Frel , [frel(ti)]1?i?M and Fred ,[fred(ti, tj)]1?i<j?M , and packing all the variables?i and ?ij into a vector ?, the program in Eq.
5 canbe compactly written asmax?
?>F?, (8)subject to binary and linear constraints on ?.
Thisformulation requires O(M2) variables and con-straints.
If we do not penalize sentence redundancy,the redundancy term may be dropped; in this simplercase, F = Frel, the vector ?
only contains the vari-ables ?
?i?, and the program in Eq.
8 only requiresO(M) variables and constraints.
Our method (to bepresented in ?4) will build on this latter formulation.3 Sentence CompressionDespite its simplicity, extractive summarization hasa few shortcomings: for example, if the original sen-tences are too long or embed several clauses, thereis no way of preventing lengthy sentences from ap-pearing in the final summary.
The sentence com-pression framework (Knight and Marcu, 2000) aimsto select the best subsequence of words that stillyields a short, informative and grammatical sen-tence.
Such a sentence compressor is given a sen-tence t , ?w1, .
.
.
, wN ?
as input and outputs a sub-sequence of length L, c , ?wj1 , .
.
.
, wjL?, with1 ?
j1 < .
.
.
< jL ?
N .
We may representthis output as a binary vector s of length N , wheresj = 1 iff word wj is included in the compression.Note that there are O(2N ) possible subsequences.3.1 Related WorkPast approaches to sentence compression includea noisy channel formulation (Knight and Marcu,2000; Daume?
and Marcu, 2002), heuristic methodsthat parse the sentence and then trim constituents ac-cording to linguistic criteria (Dorr et al, 2003; Zajicet al, 2006), a pure discriminative model (McDon-ald, 2006), and an ILP formulation (Clarke and La-pata, 2008).
We next give an overview of the twolatter approaches.McDonald (2006) uses the outputs of two parsers(a phrase-based and a dependency parser) as fea-tures in a discriminative model that decomposesover pairs of consecutive words.
Formally, given asentence t = ?w1, .
.
.
, wN ?, the score of a compres-sion c = ?wj1 , .
.
.
, wjL?
decomposes as:score(c; t) = ?Ll=2 ?>f(t, jl?1, jl) (9)where f(t, jl?1, jl) are feature vectors that dependon the original sentence t and consecutive positionsjl?1 and jl, and ?
is a learned weight vector.
Thefactorization in Eq.
9 allows exact decoding with dy-namic programming.Clarke and Lapata (2008) cast the problem as anILP.
In their formulation, Eq.
9 may be expressed as:score(c; t) =N?i=1?i?>f(t, 0, i) +N?i=1?i?>f(t, i, n + 1) +N?1?i=1N?j=i+1?ij?>f(t, i, j), (10)where ?i, ?i, and ?ij are additional binary variableswith the following meanings:?
?i = 1 iff word wi starts the compression;?
?i = 1 iff word wi ends the compression;?
?ij = 1 iff words wi and wj appear consecutivelyin the compression;and subject to the following agreement constraints:?Ni=1 ?i = 1?Ni=1 ?i = 1sj = ?j +?j?1i=1 ?ijsi = ?i +?Nj=i+1 ?ij .
(11)3This framework also allows the inclusion of con-straints to enforce grammaticality.To compress a sentence, one needs to maximizethe score in Eq.
10 subject to the constraints inEq.
11.
Representing the variables through?
, ?
?1, .
.
.
, ?N , ?1, .
.
.
, ?N , ?11, .
.
.
, ?NN ?
(12)and packing the feature vectors into a matrix F, weobtain the ILPmaxs,??>F?
(13)subject to linear and integer constraints on the vari-ables s and ?.
This particular formulation requiresO(N2) variables and constraints.3.2 Proposed MethodWe propose an alternative model for sentence com-pression that may be formulated as an ILP, as inEq.
13, but with only O(N) variables and con-straints.
This formulation is based on the output of adependency parser.Directed arcs in a dependency tree link pairs ofwords, namely a head to its modifier.
A dependencyparse tree is characterized by a set of labeled arcsof the form (head, modifier, label); see Fig.1 for anexample.
Given a sentence t = ?w1, .
.
.
, wN ?, wewrite i = pi(j) to denote that the ith word is thehead (the ?parent?)
of the jth word; if j is the root,we write pi(j) = 0.
Let s be the binary vector de-Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees.
Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al,2005a).
Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al, 2005b).
The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and neighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and Pereira, 2006).
How-ever, in the data-driven parsing setting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al, 2005a).The goal of this work is to further our currentunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting.
We start byinvestigating and extending the edge-factored modelof McDonald et al (2005b).
In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence.
To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm ?
none of which havepreviously been known to have exact non-projectiveimplementations.We then switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions.
For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case.
For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods.
A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related WorkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No?va?k, 2005; McDon-ald et al, 2005b).
These approaches can often beclassified into two broad categories.
In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorithms (Yamada and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005).
In the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, asis the case for edge-factored models (Paskin, 2001;McDonald et al, 2005a; McDonald et al, 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004).
Theoretical studies ofnote include the work of Neuhaus and Bo?ker (1997)showing that the recognition problem for a mini-$Figure 1: A dependency parse for an English sentence;example from McDonald and Satta (2007).scribing a possible compression c for the sentencet.
For each word j, we consider four possible cases,accounting for the inclusion or not of j and pi(j) inthe compression.
We introduce (mutually exclusive)binary variables ?j11, ?j10, ?j01, and ?j00 to indicateeac of t ese cases, i.e., for a, b ?
{0, 1},?jab , sj = a ?
spi(j) = b.
(14)Consider feature vectors f11(t, j), f10(t, j), f01(t, j),and f00(t, j), that look at the surface sentence and atthe status of the word j and its head pi(j); these fea-tures have corresponding weight vectors ?11, ?10,?01, and ?00.
The score of c is written as:score(c; t) = ?Nj=1?a,b?
{0,1} ?jab?>abfab(t, j)= ?a,b?
{0,1} ?>abFab?ab= ?>F?, (15)where Fab , [fab(t, 1), .
.
.
, fab(t,N)], ?ab ,(?jab)j=1,...,N , ?
, (?11,?10,?01,?00), and F ,Diag(F11,F10,F01,F00) (a block-diagonal matrix).We have reached in Eq.
15 an ILP isomorphic tothe one in Eq.
13, but only with O(N) variables.There are some agreement constraints between thevariables ?
and s that reflect the logical relations inEq.
14; these may be written as linear inequalities(cf.
Eq.
6), yielding O(N) constraints.Given this proposal and ?3.1, it is also straight-forward to extend this model to include bigram fea-tures as in Eq.
10; the combination of dependencyrelation features and bigram features yields a modelthat is more powerful than both models in Eq.
15 andEq.
10.
Such a model is expressible as an ILP withO(N2) variables and constraints, making use of thevariables s, ?, ?, ?
and ?.
In ?5, we compare theperformance of this model (called ?Bigram?)
and themodel in Eq.
15 (called ?NoBigram?
).24 Joint Compression and ExtractionWe ext describe our joint mod l for sentenc com-pression and extracti n. Let D , {t1, .
.
.
, tM} bea set of sentences as in ?2, each expressed as a se-quence of words, ti , ?wi1, .
.
.
, wiNi?.
Following?3, we represent a compression of ti as a binary vec-tor si = ?si1, .
.
.
, siNi?, where sij = 1 iff word wij2It should be noted that more efficient decoders are possiblethat do not require solving an ILP.
In particular, inference in theNoBigram variant can performed in polynomial time with dy-namic programming algorithms that propagate messages alongthe dependency parse tree; for the Bigram variant, dynamic pro-gramming can till be employed with some additional storage.Our ILP formulation, however, is more suited to the final goalof performing document summarization (of which our sentencecompression model will be a component); furthermore, it alsoallows the straightforward inclusion of global linguistic con-straints, which, as shown by Clarke and Lapata (2008), cangreatly improve th grammaticality of the compressions.4is included in the compression.
Now, define a sum-mary of D as a set of sentences obtained by extract-ing and compressing sentences from D. More pre-cisely, let ?1, .
.
.
, ?M be binary variables, one foreach sentence ti in D; define ?i = 1 iff a compres-sion of sentence ti is used in the summary.
A sum-mary of D is then represented by the binary vari-ables ?
?1, .
.
.
, ?M , s1, .
.
.
, sM ?.
Notice that thesevariables are redundant:?i = 0 ?
?j ?
{1, .
.
.
, Ni} sij = 0, (16)i.e., an empty compression means that the sentenceis not to be extracted.
In the sequel, it will becomeclear why this redundancy is convenient.Most approaches up to now are concerned with ei-ther extraction or compression, not both at the sametime.
We will combine the extraction scores in Eq.
8and the compression scores in Eq.
15 to obtain a sin-gle, global optimization problem;3 we rename theextraction features and parameters to Fe and ?e andthe compression features and parameters to Fc and?c:max?,?,s?Te Fe?+?Mi=1 ?Tc Fci?i, (17)subject to agreement constraints on the variables ?iand si (see Eqs.
11 and 14), and new agreement con-straints on the variables ?
and s1, .
.
.
, sM to enforcethe relation in Eq.
16:sij ?
?i, ?i = 1, .
.
.
,M,?j = 1, .
.
.
, Ni?i ?
?Nij=1 sij , ?i = 1, .
.
.
,M(18)The constraint that the length of the summary cannotexceed J words is encoded as:?Mi=1?Nij=1 sij ?
J.
(19)All variables are further restricted to be binary.
Wealso want to avoid picking just a few words frommany sentences, which typically leads to ungram-matical summaries.
Hence it is desirable to obtain?sparse?
solutions with only a few sentences ex-tracted and compressed (and most components of ?are zero) To do so, we add the constraint?Nij=1 sij ?
?i?Ni, i = 1, .
.
.
,M, (20)3In what follows, we use the formulation in Eq.
8 with-out the redundancy terms; however these can be included ina straightforward way, naturally increasing the number of vari-ables/constraints.which states, for each sentence ti, that ti should beignored or have at least ?Ni words extracted.
We fix?
= 0.8, enforcing compression rates below 80%.4To learn the model parameters ?
= ?
?e,?c?, wecan use a max-margin discriminative learning al-gorithm like MIRA (Crammer and Singer, 2003),which is quite effective and scalable.
However, thereis not (to our knowledge) a single dataset of ex-tracted and compressed sentences.
Instead, as willbe described in Sec.
5.1, there are separate datasetsof extracted sentences, and datasets of compressedsentences.
Therefore, instead of globally learningthe model parameters, ?
= ?
?e,?c?, we propose thefollowing strategy to learn them separately:?
Learn ?
?e using a corpus of extracted sentences,?
Learn ?
?c using a corpus of compressed sentences,?
Tune ?
so that ?
= ??
?e, ???c?
has good perfor-mance on development data.
(This is necessarysince each set of weights is learned up to scaling.
)5 Experiments5.1 Datasets, Evaluation and EnvironmentFor our experiments, two datasets were used:The DUC 2002 dataset.
This is a collection ofnewswire articles, comprised of 59 document clus-ters.
Each document within the collections (out ofa total of 567 documents) has one or two manuallycreated abstracts with approximately 100 words.5Clarke?s dataset for sentence compression.
Thisis the dataset used by Clarke and Lapata (2008).
Itcontains manually created compressions of 82 news-paper articles (1,433 sentences) from the British Na-tional Corpus and the American News Text corpus.6To evaluate the sentence compressor alone, wemeasured the compression rate and the precision,recall, and F1-measure (both macro and micro-averaged) with respect to the ?gold?
compressed4There are alternative ways to achieve ?sparseness,?
eitherin a soft way, by adding a term ?
?Pi ?i to the objective, orusing a different hard constraint, likePi ?i ?
K, to limit thenumber of sentences from which to pick words.5http://duc.nist.gov6http://homepages.inf.ed.ac.uk/s0460084/data5Compression Micro-Av.
Macro-Av.Ratio P R F1 P R F1HedgeTrimmer 57.64% 0.7099 0.5925 0.6459 0.7195 0.6547 0.6367McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696NoBigram 71.20% 0.7399 0.7626 0.7510 0.7645 0.7730 0.7604Bigram 71.35% 0.7472 0.7720 0.7594 0.7737 0.7848 0.7710Table 1: Results for sentence compression in the Clarke?s test dataset (441 sentences) for our implementation of thebaseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model,NoBigram and Bigram.
The compression ratio associated with the reference compressed sentences in this dataset is69.06%.
In the rightmost column, the statistically indistinguishable best results are emboldened, based on a pairedt-test applied to the sequence of F1 measures (p < 0.01).sentences, calculated on unigrams.7To evaluate the full system, we used Rouge-N(Lin and Hovy, 2002), a popular n-gram recall-based automatic evaluation measure.
This scorecompares the summary produced by a system withone or more valid reference summaries.All our experiments were conducted on a PC witha Intel dual-core processor with 2.66 GHz and 2 GbRAM memory.
We used ILOG CPLEX, a commer-cial integer programming solver.
The interface withCPLEX was coded in Java.5.2 Sentence CompressionWe split Clarke?s dataset into two partitions, oneused for training (1,188 sentences) and the other fortesting (441 sentences).
This dataset includes onemanual compression for each sentence, that we useas reference for evaluation purposes.
Compressionratio, i.e., the fraction of words included in the com-pressed sentences, is 69.32% (micro-averaged overthe training partition).For comparison, two baselines were imple-mented: a simple compressor based on Hedge Trim-mer, the headline generation system of Dorr et al(2003) and Zajic et al (2006),8 and the discrimina-7Notice that this evaluation score is not able to properly cap-ture the grammaticality of the compression; this is a known is-sue that typically is addressed by requiring human judgments.8Hedge Trimmer applies a deterministic compression proce-dure whose first step is to identify the lowest leftmost S node inthe parse tree that contains a NP and a VP; this node is taken asthe root of the compressed sentence (i.e., all words that are notspanned by this node are discarded).
Further steps describedby Dorr et al (2003) include removal of low content units, andan ?iterative shortening?
loop that keeps removing constituentsuntil a desired compression ratio is achieved.
The best resultswere obtained without iterative shortening, which is explainedby the fact that the selection of the lowest leftmost S node (firsttive model described by McDonald (2006), whichcaptures ?soft syntactic evidence?
(we reproducedthe same set of features).
Both systems requirea phrase-structure parser; we used Collins?
parser(Collins, 1999);9 the latter system also derives fea-tures from a dependency parser; we used the MST-Parser (McDonald et al, 2005).10We implemented the two variants of our compres-sor described in ?3.2.NoBigram.
This variant factors the compressionscore as a sum over individual scores, each depend-ing on the inclusion or not of each word and its headin the compression (see Eq.
15).
An upper bound of70% was placed on the compression ratio.
As statedin ?3.2, inference amounts to solving an ILP withO(N) variables and constraints, N being the sen-tence length.
We also used MSTParser to obtain thedependency parse trees.Bigram.
This variant includes an extra term stand-ing for a bigram score, which factors as a sum overpairs of consecutive words.
As in McDonald (2006),we include features that depend on the ?in-between?words in the original sentence that are to be omittedin the compression.11 As stated in ?3.2, inferencethrough this model can be done by solving an ILPwith O(N2) variables and constraints.step of the algorithm) already provides significant compression,as illustrated in Table 1.9http://people.csail.mit.edu/mcollins/code.html10http://sourceforge.net/projects/mstparser11The major difference between this variant and model ofMcDonald (2006) is that the latter employs ?soft syntactic ev-idence?
as input features, while we make the dependency rela-tions part of the output features.
All the non-syntactic featuresare the same.
Apart from this, notice that our variant does notemploy a phrase-structure parser.6For both variants, we used MSTParser to obtainthe dependency parse trees.
The model parametersare learned in a pure discriminative way through amax-margin approach.
We used the 1-best MIRAalgorithm (Crammer and Singer, 2003; McDonaldet al, 2005) for training; this is a fast online algo-rithm that requires solving the inference problem ateach step.
Although inference amounts to solvingan ILP, which in the worst case scales exponentiallywith the size of the sentence, training the model isin practice very fast for the NoBigram model (a fewminutes in the environment described in ?5.1) andfast enough for the Bigram model (a couple of hoursusing the same equipment).
This is explained by thefact that sentences don?t usually exceed a few tensof words, and because of the structure of the ILPs,whose constraint matrices are very sparse.Table 1 depicts the micro- and macro-averagedprecision, recall and F1-measure.
We can see thatboth variants outperform the Hedge Trimmer base-line by a great margin, and are in line with the sys-tem of McDonald (2006); however, none of our vari-ants employ a phrase-structure parser.
We also ob-serve that our simpler NoBigram variant, which usesa linear-sized ILP, achieves results similar to thesetwo systems.5.3 Joint Compression and ExtractionFor the summarization task, we split the DUC 2002dataset into a training partition (427 documents) anda testing partition (140 documents).
The trainingpartition was further split into a training and a de-velopment set.
We evaluated the performance ofLead, Rel, and MMR as baselines (all are describedin ?2).
Weights for Rel were learned via the SVM-Rank algorithm;12 to create a gold-standard ranking,we sorted the sentences by Rouge-2 score13 (with re-spect to the human created summaries).
We includea Pipeline baseline as well, which ranks all sentencesby relevance, then includes their compressions (us-ing the Bigram variant) while they fit into the sum-mary.We tested two variants of our joint model, com-bining the Rel extraction model with (i) the NoBi-12SVMRank is implemented in the SVMlight toolkit(Joachims, 1999), http://svmlight.joachims.org.13A similar system was implemented that optimizes theRouge-1 score instead, but it led to inferior performance.Rouge-1 Rouge-2Lead 0.384 ?
0.080 0.177 ?
0.083Rel 0.389 ?
0.074 0.178 ?
0.080MMR ?
= 0.25 0.392 ?
0.071 0.178 ?
0.077Pipeline 0.380 ?
0.073 0.173 ?
0.073Rel + NoBigr ?
= 1.5 0.403 ?
0.080 0.180 ?
0.082Rel + Bigr ?
= 4.0 0.403 ?
0.076 0.180 ?
0.076Table 2: Results for sentence extraction in the DUC2002dataset (140 documents).
Bold indicates the best resultswith statistical significance, according to a paired t-test(p < 0.01); Rouge-2 scores of all systems except Pipelineare indistinguishable according to the same test, with p >0.05.gram compression model (?3.2) and (ii) the Bigramvariant.
Each variant was trained with the proce-dure described in ?4.
To keep tractability, the in-ference ILP problem was relaxed (the binary con-straints were relaxed to unit interval constraints) andnon-integer solution values were rounded to producea valid summary, both for training and testing.14Whenever this procedure yielded a summary longerthan 100 words, we truncated it to fit the word limit.Table 2 depicts the results of each of the abovesystems in terms of Rouge-1 and Rouge-2 scores.We can see that both variants of our system are ableto achieve the best results in terms of Rouge-1 andRouge-2 scores.
The suboptimality of extracting andcompressing in separate stages is clear from the ta-ble, as Pipeline performs worse than the pure ex-tractive systems.
We also note that the configurationRel + Bigram is not able to outperform Rel + No-Bigram, despite being computationally more expen-sive (about 25 minutes to process the whole test set,against the 7 minutes taken by the Rel + NoBigramvariant).
Fig.
2 exemplifies the summaries producedby our system.
We see that both variants were ableto include new pieces of information in the summarywithout sacrificing grammaticality.These results suggest that our system, being capa-ble of performing joint sentence extraction and com-pression to summarize a document, offers a power-ful alternative to pure extractive systems.
Finally, wenote that no labeled datasets currently exist on whichour full model could have been trained with super-vision; therefore, although inference is performed14See Martins et al (2009) for a study concerning the impactof LP relaxations in the learning problem.7MMR baseline:Australian novelist Peter Carey was awarded the coveted BookerPrize for fiction Tuesday night for his love story, ?Oscar and Lu-cinda?.A panel of five judges unanimously announced the award of the$26,250 prize after an 80-minute deliberation during a banquet atLondon?s ancient Guildhall.Carey, who lives in Sydney with his wife and son, said in a briefspeech that like the other five finalists he had been asked to attendwith a short speech in his pocket in case he won.Rel + NoBigram:Australian novelist Peter Carey was awarded the coveted BookerPrize for fiction Tuesday night for his love story, ?Oscar and Lu-cinda?.A panel of five judges unanimously announced the award of the$26,250 prize after an 80-minute deliberation during a banquet atLondon?s ancient Guildhall.The judges made their selection from 102 books published in Britainin the past 12 months and which they read in their homes.Carey, who lives in Sydney with his wife and son, said in a briefspeech that like the other five finalists he had been asked to attendwith a short speech in his pocket in case he won.Rel + Bigram:Australian novelist Peter Carey was awarded the coveted BookerPrize for fiction Tuesday night for his love story, ?Oscar and Lu-cinda?.A panel of five judges unanimously announced the award of the$26,250 prize after an 80-minute deliberation during a banquet atLondon?s ancient Guildhall.He was unsuccessful in the prize competition in 1985 when hisnovel, ?Illywhacker,?
was among the final six.Carey called the award a ?great honor?
and he thanked the prizesponsors for ?provoking so much passionate discussion about liter-ature perhaps there will be more tomorrow?.Carey was the only non-Briton in the final six.Figure 2: Summaries produced by the strongest base-line (MMR) and the two variants of our system.
Deletedwords are marked as such.jointly, our training procedure had to learn sepa-rately the extraction and the compression models,and to tune a scalar parameter to trade off the twomodels.
We conjecture that a better model couldhave been learned if a labeled dataset with extractedcompressed sentences existed.6 Conclusion and Future WorkWe have presented a summarization system that per-forms sentence extraction and compression in a sin-gle step, by casting the problem as an ILP.
The sum-mary optimizes an objective function that includesboth extraction and compression scores.
Our modelencourages ?sparse?
summaries that involve only afew sentences.
Experiments in newswire data sug-gest that our system is a valid alternative to exist-ing extraction-based systems.
However, it is worthnoting that further evaluation (e.g., human judg-ments) needs to be carried out to assert the qualityof our summaries, e.g., their grammaticality, some-thing that the Rouge scores cannot fully capture.Future work will address the possibility of in-cluding linguistic features and constraints to furtherimprove the grammaticality of the produced sum-maries.Another straightforward extension is the inclusionof a redundancy term and a query relevance termin the objective function.
For redundancy, a simi-lar idea of that of McDonald (2007) can be applied,yielding a ILP with O(M2 + N) variables and con-straints (M being the number of sentences and N thetotal number of words).
However, such model willtake into account the redundancy among the origi-nal sentences and not their compressions; to modelthe redundancy accross compressions, a possibil-ity is to consider a linear redundancy score (similarto cosine similarity, but without the normalization),which would result in an ILP with O(N +?i P 2i )variables and constraints, where Pi ?
M is the num-ber of sentences in which word wi occurs; this is noworse than O(M2N).We also intend to model discourse, which, asshown by Daume?
and Marcu (2002), plays an im-portant role in document summarization.
Anotherfuture direction is to extend our ILP formulationsto more sophisticated models that go beyond worddeletion, like the ones proposed by Cohn and Lapata(2008).AcknowledgmentsThe authors thank the anonymous reviewers for helpfulcomments, Yiming Yang for interesting discussions, andDipanjan Das and Sourish Chaudhuri for providing theircode.
This research was supported by a grant from FCTthrough the CMU-Portugal Program and the Informa-tion and Communications Technologies Institute (ICTI)at CMU, and also by Priberam Informa?tica.8ReferencesC.
Aone, M. E. Okurowski, J. Gorlinsky, and B. Larsen.1999.
A trainable summarizer with knowledge ac-quired from robust nlp techniques.
In Advances in Au-tomatic Text Summarization.
MIT Press.P.
B. Baxendale.
1958.
Machine-made index for tech-nical literature?an experiment.
IBM Journal of Re-search Development, 2(4):354?361.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proc.
of SIGIR.J.
Clarke and M. Lapata.
2008.
Global inference forsentence compression an integer linear programmingapproach.
JAIR, 31:399?429.T.
Cohn and M. Lapata.
2008.
Sentence compressionbeyond word deletion.
In Proc.
COLING.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.K.
Crammer and Yoram Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
J. Mach.Learn.
Res., 3:951?991.H.
Daume?
and D. Marcu.
2002.
A noisy-channel modelfor document compression.
In Proc.
of ACL.H.
Daume?.
2006.
Practical Structured Learning Tech-niques for Natural Language Processing.
Ph.D. thesis,University of Southern California.B.
Dorr, D. Zajic, and R. Schwartz.
2003.
Hedge trim-mer: A parse-and-trim approach to headline gener-ation.
In Proc.
of HLT-NAACL Text SummarizationWorkshop and DUC.H.
P. Edmundson.
1969.
New methods in automatic ex-tracting.
Journal of the ACM, 16(2):264?285.R.
Jin.
2003.
Statistical Approaches Toward Title Gener-ation.
Ph.D. thesis, Carnegie Mellon University.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In Advances in Kernel Methods - SupportVector Learning.
MIT Press.K.
Knight and D. Marcu.
2000.
Statistics-basedsummarization?step one: Sentence compression.
InProc.
of AAAI/IAAI.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A trainabledocument summarizer.
In Proc.
of SIGIR.C.-Y.
Lin and E. Hovy.
2002.
Manual and automaticevaluation of summaries.
In Proc.
of the ACL Work-shop on Automatic Summarization.C.-Y.
Lin.
2003.
Improving summarization performanceby sentence compression-a pilot study.
In Proc.
of theInt.
Workshop on Inf.
Ret.
with Asian Languages.H.
P. Luhn.
1958.
The automatic creation of litera-ture abstracts.
IBM Journal of Research Development,2(2):159?165.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009.Polyhedral outer approximations with application tonatural language parsing.
In Proc.
of ICML.R.
T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.2005.
Non-projective dependency parsing using span-ning tree algorithms.
In Proc.
of HLT-EMNLP.R.
McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proc.
of EACL.R.
McDonald.
2007.
A study of global inference algo-rithms in multi-document summarization.
In Proc.
ofECIR.M.
Osborne.
2002.
Using maximum entropy for sen-tence extraction.
In Proc.
of the ACL Workshop onAutomatic Summarization.D.
R. Radev and K. McKeown.
1998.
Generating naturallanguage summaries from multiple on-line sources.Computational Linguistics, 24(3):469?500.D.
R. Radev, H. Jing, and M. Budzikowska.
2000.Centroid-based summarization of multiple documents:sentence extraction, utility-based evaluation, and userstudies.
In Proc.
of the NAACL-ANLP Workshop onAutomatic Summarization.D.
Zajic, B. Dorr, J. Lin, and R. Schwartz.
2006.Sentence compression as a component of a multi-document summarization system.
In Proc.
of the ACLDUC Workshop.9
