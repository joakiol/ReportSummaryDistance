First Joint Conference on Lexical and Computational Semantics (*SEM), pages 301?309,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUGroningen: Negation detection with Discourse Representation StructuresValerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nlCenter for Language and Cognition Groningen (CLCG)University of Groningen, The NetherlandsAbstractWe use the NLP toolchain that is used to con-struct the Groningen Meaning Bank to addressthe task of detecting negation cue and scope,as defined in the shared task ?Resolving theScope and Focus of Negation?.
This toolchainapplies the C&C tools for parsing, using theformalism of Combinatory Categorial Gram-mar, and applies Boxer to produce seman-tic representations in the form of DiscourseRepresentation Structures (DRSs).
For nega-tion cue detection, the DRSs are convertedto flat, non-recursive structures, called Dis-course Representation Graphs (DRGs).
DRGssimplify cue detection by means of edge la-bels representing relations.
Scope detectionis done by gathering the tokens that occurwithin the scope of a negated DRS.
The re-sult is a system that is fairly reliable for cuedetection and scope detection.
Furthermore, itprovides a fairly robust algorithm for detect-ing the negated event or property within thescope.1 IntroductionNothing is more home to semantics than the phe-nomenon of negation.
In classical theories of mean-ing all states of affairs are divided in two truth val-ues, and negation plays a central role to determinewhich truth value is at stake for a given sentence.Negation lies at the heart of deductive inference, ofwhich consistency checking (searching for contra-dictions in texts) is a prime example in natural lan-guage understanding.It shouldn?t therefore come as a surprise thatdetecting negation and adequately representing itsscope is of utmost importance in computational se-mantics.
In this paper we present and evaluate a sys-tem that transforms texts into logical formulas ?
us-ing the C&C tools and Boxer (Bos, 2008) ?
in thecontext of the shared task on recognising negationin English texts (Morante and Blanco, 2012).We will first sketch the background and the basicsof the formalism that we employ in our analysis ofnegation (Section 2).
In Section 3 we explain howwe detect negation cues and scope.
Finally, in Sec-tion 4 we present the results obtained in the sharedtask, and we discuss them in Section 5.2 BackgroundThe semantic representations that are used in thisshared task on detecting negation in texts are con-structed by means of a pipeline of natural languageprocessing components, of which the backbone isprovided by the C&C tools and Boxer (Curran etal., 2007).
This tool chain is currently in use semi-automatically for constructing a large semanticallyannotated corpus, the Groningen Meaning Bank(Basile et al, 2012).The C&C tools are applied for tagging the datawith part-of-speech and super tags and for syntacticparsing, using the formalism of Combinatory Cate-gorial Grammar, CCG (Steedman, 2001).
The out-put of the parser, CCG derivations, form the in-put of Boxer, producing formal semantic representa-tions in the form of Discourse Representation Struc-tures (DRSs), the basic meaning-carrying structuresin the framework of Discourse Representation The-ory (Kamp and Reyle, 1993).
DRT is a widely ac-cepted formal theory of natural language meaningthat has been used to study a wide range of linguistic301<<IPRPNP?v0.
( x1person(x1)?
(v0 @ x1) )>sawVBD(S[dcl]\NP)/NP?v0.
?v1.
?v2.
(v1 @ ?v3.
(v0 @ ?v4.
( e5see(e5)agent(e5, v3)patient(e5, v4); (v2 @ e5) ) ) )<nothingDTNP?v0.
?
( x1thing(x1); (v0 @ x1) )*suspiciousJJS[adj]\NP?v0.
?v1.
(v0 @ ?v2.
( suspicious(v2)  ; (v1 @ v2) ) )suspiciousNP\NP?v0.
?v1.
(v0 @ ?v2.
( suspicious(v2)  ; (v1 @ v2)) )nothing suspiciousNP?v0.
?
( x1thing(x1)suspicious(x1); (v0 @ x1) )saw nothing suspiciousS[dcl]\NP?v0.
?v1.
(v0 @ ?v2.
?
( x3 e4thing(x3)suspicious(x3)see(e4)agent(e4, v2)patient(e4, x3); (v1 @ e4) ) )I saw nothing suspiciousS[dcl]?v0.
( x1person(x1)?
?
( x2 e3thing(x2)suspicious(x2)see(e3)agent(e3, x1)patient(e3, x2); (v0 @ e3) ) )..S[dcl]\S[dcl]?v0.v0I saw nothing suspicious .S[dcl]?v0.
( x1person(x1)?
?
( x2 e3thing(x2)suspicious(x2)see(e3)agent(e3, x1)patient(e3, x2); (v0 @ e3) ) )Figure 1: CCG derivation and unresolved semantics for the sentence ?I saw nothing suspicious?phenomena, such as anaphoric pronouns, temporalrelations (Kamp and Reyle, 1993), presuppositions(Van der Sandt, 1992), abstract anaphora and rhetor-ical relations (Asher, 1993; Asher and Lascarides,2003).A DRS contains two parts: a set of of discoursereferents, and a set of conditions.
Negation is repre-sented in a condition by a unary operator in DRT.
Asan example, Figure 1 shows the derivation for onesentence as produced by the pipeline, illustratinghow lexical semantic entries are used to constructa DRS for a whole sentence, guided by the syntac-tic parse tree.
Here, machinery of the ?-calculus isemployed to deal with variable renaming when re-quired.DRSs are recursive structures by nature.
They canbe produced in several formats (in Prolog or XML)and translated into first-order formulas.
The rep-resentations can also be generated as a set of tu-ples, forming together a directed graph equivalentto the original DRS, where discourse referents andsymbols are nodes and predicates and relations areviewed as labelled edges.
These ?flat?
DiscourseRepresentation Graphs, DRGs, are often more suit-able for certain processing tasks.
The tuples alsohold additional information, mapping DRS condi-tions to surface tokens.
This mapping is importantin tasks where surface realisation plays a role.
Wealso use it in this shared task to get back from com-plex structures to a flat, token-based annotation ofscope.3 MethodThe shared task aims at detecting negation in text ?systems are supposed to label tokens that are in thescope of negation, and also identify the token thattriggered the negation.
The basic idea of our methodwas to run the existing Boxer system for semanticanalysis, then traverse the produced DRSs, and, onencountering an embbeded negated DRS, output the302tokens associated with this negation, as well as thetoken triggering it.As this isn?t what Boxer is usually asked to do,it required some bookkeeping adjustments.
Boxer?sanaphora resolution feature was turned off becauseit is not necessary for the task and would leadto unwanted inclusion of antecedents into negationscopes.
Also, its features for representing tense in-formation and rhetorical relations were not used.The rest of this section pays a closer look at hownegation cues are detected and how scope is as-signed to tokens.
We address the issues of trans-lating a formal representation such as DRS into theformat required by the shared task ?
a represen-tation more oriented at the surface form.
We sub-mitted two runs of our system, which both used theC&C tools and Boxer.
For the second run, we addedsome postprocessing steps that tune the result to-wards a higher performance, especially on scope de-tection.
While these postprocessing steps improveperformance, many of them may be specific to thegenre and style of the texts used in the shared task.3.1 Cue detectionSince Boxer has been developed as a system togenerate full semantic representations, its lexiconimplicitly contains a list of negation cues: thosewords giving rise to semantic representations of theform ?B, where B is the DRS representing themeaning of the scope of the negation.
Key exampleshere are determiners and noun phrases (no, none, no-one), and verb phrase negation (not).However, negation detection is not the primaryfunction of Boxer, as it is part of the larger aim ofproviding interpretable semantic representation forEnglish texts, and doing so robustly.
So for the cur-rent task, after investigating the development datamade available by the organisers, Boxer?s lexiconwas revised at a few points to account for particu-lar negation cues that Boxer originally did not de-tect.
This included the detection of never as negationcue, as well as words with a negative prefix or suffix(e.g.
inadequate, motionless).
These affix negationswere detected using an automatically generated listof negatively affixed nouns, adjectives and adverbsfrom WordNet (Fellbaum, 1998).
The list was cre-ated by means of an algorithm that returns all nouns,adjectives and adverbs in WordNet that start withone of a, an, dis, in, il, im, ir, non, non-, un, or endwith one of less, lessness, lessly, and have a directantonym such that the lemma form equals the stemof the affixed negation (i.e., without the affix).On the other hand, not everything that introducesa negated DRS in Boxer is a typical negation cue.A case in point is the quantifier all, which up un-til the shared task received a semantics similar to?P?Q.?
?x(P (x)?
?Q(x)) in Boxer?s lexicon.
Asa consequence, Boxer predicted all to be a nega-tion cue trigger, in contrast to the shared task goldstandard data.
Such instances were replaced by log-ically equivalent representations (in the case of all:?P?Q.
?x(P (x) ?
Q(x))).In order to obtain the tokens that triggered thenegated DRS, Boxer?s DRG output was used.
Oc-currences of predicates, relations and connectives inthe DRG output carry explicit associations with thetokens in the input whose lexical entries they comefrom.
For basic cue detection, the system annotatesas a negation cue those tokens (or affixes) associatedwith the connective?
(represented in the DRG as therelation subordinates:neg).
Example (1) shows apart of the DRG?s tuple format that represents thenegation cue ?no?.
Argument structure tuples (la-beled concept and instance) are also shown, cor-responding to a noun in the negation scope, as in?no problem?.
The first and third columns representnodes of the DRG graph (both discourse units in thisexample), the second column represents the label ofthe edge between the nodes, and the fourth columnshows the token associated with the relation (if any).(1)...
... ... ...k1 subordinates:neg k6 nok6 concept c1:problemc1:problem instance k6:x1 problem... ... ...
...In this case, the token ?no?
is detected as negationcue because it is associated with the relation subor-dinates:neg.In the case of affix negation, ideally only the af-fix should be associated with the negation tuple, andthe stem with a corresponding instance tuple.
How-ever, since the last column contains tokens, this doesnot easily fit into the format.
We therefore associatethe whole affix-negated token with the negation tu-ple and use separate tuples for affix and stem in orderto preserve the information which part of the word303is the cue and which part is in the scope of the nega-tion.
The resulting three tuples from a sentence con-taining the word ?injustice?
are shown in the follow-ing example:(2)... ... ... ...k3 subordinates:neg k4 injusticek4 concept c2:in:71k4 concept c3:justice:1... ... ...
...The target nodes of the two argument structure tu-ples (labeled concept because ?injustice?
is a noun)are labeled with the relevant part of the affix-negatedword, and a special ID to indicate the presence ofa prefix or suffix.
This information is used by thescript producing the token-based result format.
Al-though multi-word cues, such as neither...nor and onthe contrary, were not correctly predicted as such byBoxer, no effort was made to include them.
Due tothe token-based detection approach, the cue detec-tion algorithm would have to be severly complicatedto include these cases as one negation cue.
Becauseof the relatively small frequency of multi-word cues,we decided not to include special processing steps toaccount for them.The second run includes some postprocessingsteps implemented on top of the basic output.
SinceBoxer is not designed to deal with dialogue, inter-jections were originally ignored as negation cues.Therefore, the postprocessing script added the word?no?
as a negation cue (with empty scope) when itoccurred as an interjection (tagged ?UH?).
It also ex-cluded negations with the cue ?no?
when occurringas part of the expression ?no doubt?
and not imme-diately preceded by a verb with the lemma ?have?or ?be?
as in ?I have no doubt that...?, which is to beannotated as a negation.
High precision and recallfor cue detection on the training data suggested thatno further processing steps were worth the effort.3.2 Scope detectionThe tokens in the scope of a negation are deter-mined on the basis of the detected negation cue.
Itis associated with the negation connective of somenegated DRS ?B, so the system annotates as scopeall the tokens (and stems in the case of affix nega-tion) directly or indirectly associated with predicatesand relations inside B.
This includes tokens di-rectly associated with predicates that appear withinthe negated DRS, as well as those predicates outsideof the negated DRS whose discourse referent occurswithin the negation scope as the second argument ofa thematic role relation.<PRN?v0.
<P(x <1 NpvNN Np(ers.o <1(vnvRs)s0nv <1(?oN.e Np@><P(VrNBN Np@><1(Figure 2: DRS for the sentence ?I saw nothing suspi-cious?An example is given in Figure 2, where e.g.
thetokens see and suspicious are associated, respec-tively, with see(e4) and suspicious(x3).
Althoughthe predicate person(x2) associated with the pro-noun I occurs outside of the negated DRS, its refer-ent occurs as an argument within the negated DRSin Agent(e4, x2) and therefore it is taken to be partof the scope of the negation.
The desired scope isthus detected, containing the tokens I, saw and sus-picious.Again, in the second run some postprocessingsteps were implemented to improve performance.We observed that the scopes in the manually anno-tated data were usually continuous, except for nega-tion cues within them.
However, the scopes pro-duced by the DRS algorithm contained many ?gaps?between the tokens of the detected scope, due to anintrinsic feature of the DRS representation.
Conven-tionally, DRSs only explicitly contain content words(i.e.
nouns, verbs, adjectives, adverbs), while func-tion words, such as determiners, modals and auxil-iary verbs, are represented e.g.
as structural proper-ties or temporal features, or not at all, as in the caseof the infinitival to.
Thus, when retrieving the sur-face representation of the negated scopes from theDRSs, not all structural properties can be directly as-sociated with a surface token and thus not all tokensrequired for the scope are retrieved.
Because in thegold standard annotation these function words wereconsidered part of the negation scope, we designedan ad hoc mechanism to include them, namely fillingall the gaps that occur in the negation scope (leaving304out the negation cue).
For the same reason, deter-miners immediately preceding the detected scopeswere added in postprocessing.
Finally, conjunc-tions were removed from the beginning of negationscopes, because they were sometimes wrongly rec-ognized by our pipeline as adverbs.3.3 Negated event/property detectionAlthough not among our main goals, we also ad-dressed the issue of detecting the ?negated event orproperty?
in negation scopes within factual state-ments.
This is done using a heuristic algorithm thatuses the detected scope, as well as the syntax treeprovided as part of the data.Since the scope is provided as a set of tokens, thefirst step is to identify what we call the scope con-stituent, i.e.
a constituent in the syntax tree that cor-responds to the scope.
This is done by going throughthe tokens in the scope from left to right and de-termining for each token the largest constituent thatstarts with this token.
The first constituent found inthis way the category of whose root is one of SBAR,S and VP is taken to be the scope constituent.In the second step, the scope VP is determinedas the first VP encountered when doing a pre-order,left-to-right traversal of the scope constituent.
Thefirst verb directly dominated by this VP node deter-mines how the process continues: (i) For non-factualmodals (e.g.
may, must, should), no event/propertyis annotated.
(ii) For futurity modals (e.g.
would,will, shall), the negated event/property is determinedrecursively by taking the first embedded VP as thenew scope VP.
(iii) For forms of the verb be, thealgorithm first looks for the head of an embeddedADJP or NP.
If one is found, this is annotated as anegated property.
Otherwise, the verb is assumed tobe a passive auxiliary and the negated event/propertyis again determined recursively on the basis of thefirst embedded VP.
(iv) In all other cases, the verbitself is annotated as the negated event.To limit the undesired detection of negatedevents/properties outside of factual statements, thealgorithm is not applied to any sentence that con-tains a question mark.4 ResultsHere we discuss our results on the Shared Task ascompared to the gold standard annotations providedby (Morante and Daelemans, 2012).
The output ofour two runs will be discussed with respect to Task 1.The first run includes the results of our system with-out postprocessing steps and in the second run thesystem is augmented with the postprocessing steps,as discussed in Section 3.During the process of evaluating the results of thetraining data, an issue with the method of evaluationwas discovered.
In the first version of the evaluationscript precision was calculated using the standardformula: tptp+fp .
However, partial matches are ex-cluded from this calculation (they are only countedas false negatives), which means that in the caseof scopes(cue match), precision is calculated as thenumber of exact scope matches (true positives) di-vided by the number of exact scope matches plusthe number of completely wrong instances with nooverlap (false positives).
As precision is a measurefor calculating correctly detected instances amongall detected instances, it seems that partial matchesshould also be taken into account as detected in-stance.
Therefore, we proposed a new evaluationmethod (B): tpsystem , where system includes all de-tected negations of the current system (includingpartial matches).
However, this measure may be toostrict as it penalizes a system harder for outputting apartially correct scope than for outputting no scopeat all.1 This choice between two evils seems to in-dicate that precision is too simple a measure for tar-gets where partial matches are possible.
Therefore,in our evaluation of scope detection, we will focuson the scope tokens measure where there are no par-tial matches.
For cue and negated event/property de-tection, we use the stricter, but more meaningful Bversion.
The difference here is almost negligible be-cause these targets typically have just one token.4.1 Run 1 (without postprocessing)Table 1 shows the results of the basic system with-out postprocessing, with the most important resultsfor our system highlighted.
As we can see, thebasic system performs well on cue detection (F1=1This was pointed out by an anonymous reviewer.305Table 1: Results of the first run (without postprocessing)Task gold system tp fp fn precision (%) recall (%) F1 (%)Cues: 264 261 219 33 45 86.90 82.95 84.88Scopes(cue match): 249 261 32 37 217 46.38 12.85 20.12Scopes(no cue match): 249 261 32 37 217 46.38 12.85 20.12Scope tokens(no cue match): 1805 1821 1269 552 536 69.69 70.30 69.99Negated(no cue match): 173 169 89 76 82 53.94 52.05 52.98Full negation: 264 261 20 33 244 37.74 7.58 12.62Cues B: 264 261 219 33 45 83.91 82.95 83.43Scopes B (cue match): 249 261 32 37 217 12.26 12.85 12.55Scopes B (no cue match): 249 261 32 37 217 12.26 12.85 12.55Negated B (no cue match): 173 169 89 76 82 52.66 52.05 52.35Full negation B: 264 261 20 33 244 7.66 7.58 7.62Table 2: Results of the second run (with postprocessing)Task gold system tp fp fn precision (%) recall (%) F1 (%)Cues: 264 261 224 28 40 88.89 84.85 86.82Scopes(cue match): 249 256 102 32 147 76.12 40.96 53.26Scopes(no cue match): 249 256 102 32 147 76.12 40.96 53.26Scope tokens(no cue match): 1805 2146 1485 661 320 69.20 82.27 75.17Negated(no cue match): 173 201 111 85 59 56.63 65.29 60.65Full negation: 264 261 72 28 192 72.00 27.27 39.56Cues B: 264 261 224 28 40 85.82 84.85 85.33Scopes B (cue match): 249 256 102 32 147 39.84 40.96 40.39Scopes B (no cue match): 249 256 102 32 147 39.84 40.96 40.39Negated B (no cue match): 173 201 111 85 59 55.22 65.29 59.83Full negation B: 264 261 72 28 192 27.59 27.27 27.4383.43%), and reasonably well on the detection ofscope tokens (F1= 69.99%).Note that the results for Scopes(cue match) andScopes(no cue match) are the same for our system.Since we make use of token-based cue detection, theonly cases of partial cue detection are instances ofmulti-word cues, which, as discussed above, werenot accounted for in our system.
In these cases, thepart of the cue that is not detected has a large chanceof becoming part of the scope of the cue that is de-tected due to collocation.
So, we hypothesize thatScopes(cue match) and Scopes(no cue match) are thesame because in all cases of partial cue detection, thescope incorrectly contains part of the gold-standardcue, which affects both measures negatively.There is a large discrepancy between the detec-tion of scope tokens and the detection of com-plete scopes, as the latter is low on both precision(12.26%) and recall (12.85%).
The relatively highprecision and recall for scope tokens (69.69% and70.30%, respectively) suggests that there are manycases of partial scope detection, i.e.
cases where thescope is either under- or overdetected with respectto the gold standard scope.
Since the postprocessingsteps for scope detection were developed to reduceexactly this under- and overdetection, we expect thatthe results for the second run are significantly better.The same holds for negated/event property detection(F1= 52.98%) since it uses the results from scopedetection.4.2 Run 2 (with postprocessing)Table 2 reports the results of the extended system,which extends the basic system with postprocessingsteps for cue detection and especially for scope de-tection.
The postprocessing steps indeed result inhigher precision and recall for all tasks, except forScope tokens, which shows a negligible decrease inprecision (from 69.69% to 69.20%).
This suggeststhat there are more cases of overdetected scopesthan underdetected scopes, because the number ofwrongly detected tokens (false positives) increasedwhile the number of undetected scope tokens (falsenegatives) decreased.
This is probably due to thegap-filling mechanism that was implemented as apostprocessing step for scope detection, generaliz-306ing that all scopes should be continuous.
We willelaborate more on this point in the discussion in Sec-tion 5.As expected, the detection of complete scopesshows the highest increase in F1 score (from 12.55%to 40.39%).
This indicates that the postprocessingsteps effectively targeted the weak points of the ba-sic system.While there are no postprocessing steps fornegated event or property detection, the F1 score forthis task also increases (from 52.35% to 59.83%),as expected, due to the improvement in scope detec-tion.5 DiscussionOverall, we can say that both of our systems performwell on cue detection, with a small increase when in-cluding the postprocessing steps.
This was expectedsince the postprocessing for cue detection targetedonly two specific types of cues, namely, interjectionsand occurrences of ?no doubt?.
The scope detectionbenefits consideraby from adding the postprocessingsteps, as was their main goal.
In the final results ofthe shared task, run 2 of our system ended secondout of five in the open track, while run 1 was rankedlast.
We will here discuss some points that deservespecial attention.5.1 Affix NegationAs discussed above, affix negations received a spe-cial treatment because they were not originally de-tected as negation cues in Boxer.
In the DRS, thetoken containing the affixed negation cue is associ-ated with two predicates, representing the negativeaffix and the negated stem.
The algorithm securesthat only the affix is annotated as the negation cueand that the negated stem is annotated as part of thescope.
An example of a sentence containing affixnegation is shown in (3) (cardboard 31).2(3) a.
[You do yourself an] in[justice].
goldb.
You do yourself an in[justice].
run1c.
You do yourself [an] in[justice].
run22In the following, boldfaced tokens represent the negationcues, brackets embed the scope and underlining signifies thenegated event or property (subscripts added in case of multiplenegation cues).Table 3: Results of negated event/property detection ongold standard cue and scope annotationTask prec.
(%) rec.
(%) F1(%)Negated (no cue match): 64.06 76.88 69.89Negated B (no cue match): 59.71 76.88 67.22Note that in neither of the runs the complete scopefrom the gold standard is detected, although post-processing increases the recall of scope tokens byadding the determiner ?an?
to the scope of the nega-tion.
However, examples like this are not unambigu-ous with respect to their negation scope.
For ex-ample, the sentence in (3) can be interpreted in twoways: ?It is not the case that you do yourself (some-thing that is) justice?
and ?It is the case that you doyourself (something that is) not justice?.
While thegold standard annotation assumes the former, wide-scope reading, our system predicts the narrow scopereading for the negation.
The narrow scope read-ing can be motivated by means of Grice?s Maxim ofManner (Grice, 1975); the choice of an affix nega-tion instead of a verbal negation signals a narrowscope, because in case a wide scope negation is in-tended, a verbal negation would be more perspicu-ous.
Thus, the difference in the output of our sys-tem and the gold standard annotation is in this casecaused by a different choice in disambiguating nega-tion scope, rather than by a shortcoming of the de-tection algorithm.5.2 Negated event/property detectionAlthough correct detection of the negated event orproperty was not our prime concern, the resultsobtained with our algorithm were quite promising.Among the systems participating in the closed trackof task 1, our extended system is ranked third outof seven for negated event/property detection eventhough the performance on scope detection is lowerthan all of the other systems in this track.
Sincenegated event/property detection depends on the de-tected scope, it seems that our heuristic algorithmfor detecting the negated event/property is very ro-bust against noisy input.
The performance of the de-tection algorithm on the gold-standard annotation ofscopes is shown in Table 3.
Although we cannotcompare these results to the performance of othersystems on the gold standard data, it should be noted307that the results shown here are unmatched by thetest results of any other system.
It would thereforebe worthwile for future work to refine the negatedevent/property detection algorithm outlined here.5.3 PostprocessingThe results for the two versions of our systemshowed that the postprocessing steps implementedin the extended system improved the results consid-erably, especially for scope detection.
Example (4)(cardboard 62) shows the effect of postprocessing onthe detection of scopes for negative interjections.
(4) a.
?No1, [I saw]2 nothing2.?
goldb.
?
[No], [I saw] nothing.?
run1c.
?
[No1, I saw]2 nothing2.?
run2In Run 1, the system correctly detects the cue ?noth-ing?
and the event ?saw?, although the detectedscope is too wide due to an error in the output ofthe parser we used.
In Run 2, postprocessing alsocorrectly recognizes the interjection ?no?
as a nega-tion cue.
Gap filling in this case makes the scopeoverdetection worse by also adding the comma tothe scope.
A similar case of the overdetection ofscope is shown in (5) (cardboard 85).
(5) a.
[The box] is a half-pound box of honey-dew tobacco and [does] not [help us inany way].
goldb.
[The box] is a half-pound box of hon-eydew tobacco and does not [help us inany way].
run1c.
[The box is a half-pound box of honey-dew tobacco and does] not [help us inany way].
run2Note that in Run 1 the first part of the coordinatedstructure is correctly excluded from the scope ofthe negation, but the auxiliary ?does?
is incorrectlynot counted as scope.
The gap-filling mechanismthen adds the intermediary part to the scope of thenegation, resulting in an increase in recall for scopetokens detection (since ?does?
is now part of thescope) but a lower precision because of the overgen-eration of the coordinated part.Nevertheless, the increased precision and recallfor scope detection can mainly be ascribed to thegap-filling mechanism implemented in the postpro-cessing steps for scope detection.
As discussedabove, the presence of gaps in the original outputis due to the non-sequential nature of the DRT rep-resentation and the fact that function words are notdirectly associated with any element in the represen-tations.
This suggests that future work on surface re-alisation from DRSs should focus on translating thestructural properties of DRSs into function words.5.4 Differences between textsWe noted that there was a difference between theperformance on text 1 (The Adventure of the RedCircle) and text 2 (The Adventure of the CardboardBox).
The results for text 2 were overall higher thanthe results for text 1 (except for a 1% decline in re-call for Full negation).
There was a higher scopeprecision for text 2 and after the postprocessing stepsan even larger difference was found for scope de-tection (15% versus 44% increase in F1 score forScopes).
We hypothesize that this difference may bedue to a higher number of multiword expressions intext 1 (7 vs. 2) and to the fact that text 1 seems tohave more scopes containing gaps.
This latter ob-servation is supported by the fact that gap filling re-sults in more overgeneration (more false positives),which is reflected in the ratios of false positives intext 1 (38%) and text 2 (27%).
Thus, while the post-processing steps improve performance, they seem tobe genre and style dependent.
This motivates furtherdevelopment of the ?clean?, theoretically motivatedversion of our system in order to secure domain-independent broad coverage of texts, which is thegoal of the Groningen Meaning Bank project.6 ConclusionParticipating in this shared task on negation detec-tion gave us a couple of interesting insights into ournatural language processing pipeline that we are de-veloping in the context of the Groningen MeaningBank.
It also showed that it is not easy to trans-fer the information about negation from a formal,logical representation of scope to a theory-neutralsurface-oriented approach.
The results were in linewith what we expected beforehand, with the highestloss appearing in the awkward translation from oneformalism to another.308ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofconversation.
Studies in natural language processing.Cambridge University Press.Nicholas Asher.
1993.
Reference to Abstract Objects inDiscourse.
Kluwer Academic Publishers.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semantically an-notated corpus.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation (LREC?12).
To appear.Johan Bos.
2008.
Wide-Coverage Semantic Analysiswith Boxer.
In J. Bos and R. Delmonte, editors, Se-mantics in Text Processing.
STEP 2008 ConferenceProceedings, volume 1 of Research in ComputationalSemantics, pages 277?286.
College Publications.James Curran, Stephen Clark, and Johan Bos.
2007.
Lin-guistically Motivated Large-Scale NLP with C&C andBoxer.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 33?36, Prague, Czech Republic.Christiane Fellbaum, editor.
1998.
WordNet.
An Elec-tronic Lexical Database.
The MIT Press.H.
P. Grice.
1975.
Logic and conversation.
In P. Coleand J. L. Morgan, editors, Syntax and Semantics: Vol.3: Speech Acts, pages 41?58.
Academic Press, SanDiego, CA.Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic; An Introduction to Modeltheoretic Semantics ofNatural Language, Formal Logic and DRT.
Kluwer,Dordrecht.Roser Morante and Eduardo Blanco.
2012.
*SEM 2012Shared Task: Resolving Scope and Focus of Negation.In Proceedings of the First Joint Conference on Lexi-cal and Computational Semantics (*SEM 2012), Mon-treal, Canada.
To appear.Roser Morante and Walter Daelemans.
2012.ConanDoyle-neg: Annotation of negation in ConanDoyle stories.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation (LREC?12).
To appear.Mark Steedman.
2001.
The Syntactic Process.
The MITPress.Rob Van der Sandt.
1992.
Presupposition Projection asAnaphora Resolution.
Journal of Semantics, 9:333?377.309
