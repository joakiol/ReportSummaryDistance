Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1106?1115,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Hierarchical Neural Autoencoder for Paragraphs and DocumentsJiwei Li, Minh-Thang Luong and Dan JurafskyComputer Science Department, Stanford University, Stanford, CA 94305, USAjiweil, lmthang, jurafsky@stanford.eduAbstractNatural language generation of coherentlong texts like paragraphs or longer doc-uments is a challenging problem for re-current networks models.
In this paper,we explore an important step toward thisgeneration task: training an LSTM (Long-short term memory) auto-encoder to pre-serve and reconstruct multi-sentence para-graphs.
We introduce an LSTM model thathierarchically builds an embedding for aparagraph from embeddings for sentencesand words, then decodes this embeddingto reconstruct the original paragraph.
Weevaluate the reconstructed paragraph us-ing standard metrics like ROUGE and En-tity Grid, showing that neural models areable to encode texts in a way that preservesyntactic, semantic, and discourse coher-ence.
While only a first step toward gener-ating coherent text units from neural mod-els, our work has the potential to signifi-cantly impact natural language generationand summarization1.1 IntroductionGenerating coherent text is a central task in naturallanguage processing.
A wide variety of theoriesexist for representing relationships between textunits, such as Rhetorical Structure Theory (Mannand Thompson, 1988) or Discourse Representa-tion Theory (Lascarides and Asher, 1991), for ex-tracting these relations from text units (Marcu,2000; LeThanh et al, 2004; Hernault et al, 2010;Feng and Hirst, 2012, inter alia), and for extract-ing other coherence properties characterizing therole each text unit plays with others in a discourse(Barzilay and Lapata, 2008; Barzilay and Lee,1Code for models described in this paper are available atwww.stanford.edu/?jiweil/.2004; Elsner and Charniak, 2008; Li and Hovy,2014, inter alia).
However, applying these to textgeneration remains difficult.
To understand howdiscourse units are connected, one has to under-stand the communicative function of each unit,and the role it plays within the context that en-capsulates it, recursively all the way up for theentire text.
Identifying increasingly sophisticatedhuman-developed features may be insufficient forcapturing these patterns.
But developing neural-based alternatives has also been difficult.
Al-though neural representations for sentences cancapture aspects of coherent sentence structure (Jiand Eisenstein, 2014; Li et al, 2014; Li and Hovy,2014), it?s not clear how they could help in gener-ating more broadly coherent text.Recent LSTM models (Hochreiter and Schmid-huber, 1997) have shown powerful results on gen-erating meaningful and grammatical sentences insequence generation tasks like machine translation(Sutskever et al, 2014; Bahdanau et al, 2014; Lu-ong et al, 2015) or parsing (Vinyals et al, 2014).This performance is at least partially attributableto the ability of these systems to capture localcompositionally: the way neighboring words arecombined semantically and syntactically to formmeanings that they wish to express.Could these models be extended to deal withgeneration of larger structures like paragraphs oreven entire documents?
In standard sequence-to-sequence generation tasks, an input sequenceis mapped to a vector embedding that representsthe sequence, and then to an output string ofwords.
Multi-text generation tasks like summa-rization could work in a similar way: the sys-tem reads a collection of input sentences, andis then asked to generate meaningful texts withcertain properties (such as?for summarization?being succinct and conclusive).
Just as the localsemantic and syntactic compositionally of wordscan be captured by LSTM models, can the com-1106positionally of discourse releations of higher-leveltext units (e.g., clauses, sentences, paragraphs, anddocuments) be captured in a similar way, withclues about how text units connect with each an-other stored in the neural compositional matrices?In this paper we explore a first step toward thistask of neural natural language generation.
We fo-cus on the component task of training a paragraph(document)-to-paragraph (document) autoencoderto reconstruct the input text sequence from a com-pressed vector representation from a deep learn-ing model.
We develop hierarchical LSTM mod-els that arranges tokens, sentences and paragraphsin a hierarchical structure, with different levels ofLSTMs capturing compositionality at the token-token and sentence-to-sentence levels.We offer in the following section to a brief de-scription of sequence-to-sequence LSTM models.The proposed hierarchical LSTM models are thendescribed in Section 3, followed by experimentalresults in Section 4, and then a brief conclusion.2 Long-Short Term Memory (LSTM)In this section we give a quick overview of LSTMmodels.
LSTM models (Hochreiter and Schmid-huber, 1997) are defined as follows: given asequence of inputs X = {x1, x2, ..., xnX}, anLSTM associates each timestep with an input,memory and output gate, respectively denoted asit, ftand ot.
For notations, we disambiguate e andh where etdenote the vector for individual textunite (e.g., word or sentence) at time step t whilehtdenotes the vector computed by LSTM modelat time t by combining etand ht?1.
?
denotes thesigmoid function.
The vector representation htforeach time-step t is given by:[itftotlt]=[??
?tanh]W ?
[ht?1et](1)ct= ft?
ct?1+ it?
lt(2)hst= ot?
ct(3)where W ?
R4K?2KIn sequence-to-sequencegeneration tasks, each input X is paired witha sequence of outputs to predict: Y ={y1, y2, ..., ynY}.
An LSTM defines a distributionover outputs and sequentially predicts tokens us-ing a softmax function:P (Y |X)=?t?
[1,ny]p(yt|x1, x2, ..., xt, y1, y2, ..., yt?1)=?t?
[1,ny]exp(f(ht?1, eyt))?y?exp(f(ht?1, ey?
))(4)f(ht?1, eyt) denotes the activation function be-tween eh?1and eyt, where ht?1is the representa-tion outputted from the LSTM at time t?
1.
Notethat each sentence ends up with a special end-of-sentence symbol <end>.
Commonly, the inputand output use two different LSTMs with differ-ent sets of convolutional parameters for capturingdifferent compositional patterns.In the decoding procedure, the algorithm termi-nates when an <end> token is predicted.
At eachtimestep, either a greedy approach or beam searchcan be adopted for word prediction.
Greedy searchselects the token with the largest conditional prob-ability, the embedding of which is then combinedwith preceding output for next step token predic-tion.
For beam search, (Sutskever et al, 2014) dis-covered that a beam size of 2 suffices to providemost of benefits of beam search.3 Paragraph AutoencoderIn this section, we introduce our proposed hierar-chical LSTM model for the autoencoder.3.1 NotationLet D denote a paragraph or a document, whichis comprised of a sequence of NDsentences,D = {s1, s2, ..., sND, endD}.
An additional?endD?
token is appended to each document.Each sentence s is comprised of a sequence oftokens s = {w1, w2, ..., wNs} where Nsdenotesthe length of the sentence, each sentence end-ing with an ?ends?
token.
The word w is as-sociated with a K-dimensional embedding ew,ew= {e1w, e2w, ..., eKw}.
Let V denote vocabu-lary size.
Each sentence s is associated with a K-dimensional representation es.An autoencoder is a neural model where outputunits are directly connected with or identical to in-put units.
Typically, inputs are compressed intoa representation using neural models (encoding),which is then used to reconstruct it back (decod-ing).
For a paragraph autoencoder, both the inputX and output Y are the same document D. The1107autoencoder first compresses D into a vector rep-resentation eDand then reconstructs D based oneD.For simplicity, we define LSTM(ht?1, et) tobe the LSTM operation on vectors ht?1and ettoachieve htas in Equ.1 and 2.
For clarification,we first describe the following notations used inencoder and decoder:?
hwtand hstdenote hidden vectors from LSTMmodels, the subscripts of which indicatetimestep t, the superscripts of which indi-cate operations at word level (w) or sequencelevel (s).
hst(enc) specifies encoding stageand hst(dec) specifies decoding stage.?
ewtand estdenotes word-level and sentence-level embedding for word and sentence at po-sition t in terms of its residing sentence ordocument.3.2 Model 1: Standard LSTMThe whole input and output are treated as onesequence of tokens.
Following Sutskever et al(2014) and Bahdanau et al (2014), we trainedan autoencoder that first maps input documentsinto vector representations from a LSTMencodeand then reconstructs inputs by predicting to-kens within the document sequentially from aLSTMdecode.
Two separate LSTMs are imple-mented for encoding and decoding with no sen-tence structures considered.
Illustration is shownin Figure 1.3.3 Model 2: Hierarchical LSTMThe hierarchical model draws on the intuition thatjust as the juxtaposition of words creates a jointmeaning of a sentence, the juxtaposition of sen-tences also creates a joint meaning of a paragraphor a document.Encoder We first obtain representation vectorsat the sentence level by putting one layer of LSTM(denoted as LSTMwordencode) on top of its containingwords:hwt(enc) = LSTMwordencode(ewt, hwt?1(enc))(5)The vector output at the ending time-step is usedto represent the entire sentence ases= hwendsTo build representation eDfor the current doc-ument/paragraph D, another layer of LSTM (de-noted as LSTMsentenceencode) is placed on top of all sen-tences, computing representations sequentially foreach timestep:hst(enc) = LSTMsentenceencode(est, hst?1(enc)) (6)Representation esendDcomputed at the final timestep is used to represent the entire document:eD= hsendD.Thus one LSTM operates at the token level,leading to the acquisition of sentence-level rep-resentations that are then used as inputs into thesecond LSTM that acquires document-level repre-sentations, in a hierarchical structure.Decoder As with encoding, the decoding algo-rithm operates on a hierarchical structure with twolayers of LSTMs.
LSTM outputs at sentence levelfor time step t are obtained by:hst(dec) = LSTMsentencedecode(est, hst?1(dec)) (7)The initial time step hs0(d) = eD, the end-to-endoutput from the encoding procedure.
hst(d) is usedas the original input into LSTMworddecodefor subse-quently predicting tokens within sentence t + 1.LSTMworddecodepredicts tokens at each position se-quentially, the embedding of which is then com-bined with earlier hidden vectors for the next time-step prediction until the endstoken is predicted.The procedure can be summarized as follows:hwt(dec) = LSTMsentencedecode(ewt, hwt?1(dec)) (8)p(w|?)
= softmax(ew, hwt?1(dec)) (9)During decoding, LSTMworddecodegenerates eachword token w sequentially and combines it withearlier LSTM-outputted hidden vectors.
TheLSTM hidden vector computed at the final timestep is used to represent the current sentence.This is passed to LSTMsentencedecode, combinedwith hstfor the acquisition of ht+1, and outputtedto the next time step in sentence decoding.For each timestep t, LSTMsentencedecodehas to firstdecide whether decoding should proceed or cometo a full stop: we add an additional token endDtothe vocabulary.
Decoding terminates when tokenendDis predicted.
Details are shown in Figure 2.1108Figure 1: Standard Sequence to Sequence Model.Figure 2: Hierarchical Sequence to Sequence Model.Figure 3: Hierarchical Sequence to Sequence Model with Attention.3.4 Model 3: Hierarchical LSTM withAttentionAttention models adopt a look-back strategy bylinking the current decoding stage with input sen-tences in an attempt to consider which part of theinput is most responsible for the current decodingstate.
This attention version of hierarchical modelis inspired by similar work in image caption gen-eration and machine translation (Xu et al, 2015;Bahdanau et al, 2014).Let H = {hs1(e), hs2(e), ..., hsN(e)} be the1109collection of sentence-level hidden vectors foreach sentence from the inputs, outputted fromLSTMSentenceencode.
Each element in H contains in-formation about input sequences with a strong fo-cus on the parts surrounding each specific sentence(time-step).
During decoding, suppose that estde-notes the sentence-level embedding at current stepand that hst?1(dec) denotes the hidden vector out-putted from LSTMsentencedecodeat previous time stept?1.
Attention models would first link the current-step decoding information, i.e., hst?1(dec) whichis outputted from LSTMsentencedecwith each of theinput sentences i ?
[1, N ], characterized by astrength indicator vi:vi= UTf(W1?
hst?1(dec) +W2?
hsi(enc)) (10)W1,W2?
RK?K, U ?
RK?1.
viis then normal-ized:ai=exp(vi)?i?exp(v?i)(11)The attention vector is then created by averagingweights over all input sentences:mt=?i?
[1,ND]aihsi(enc) (12)LSTM hidden vectors for current step is thenachieved by combining ct, estand hst?1(dec):[itftotlt]=[??
?tanh]W ?
[hst?1(dec)estmt](13)ct= ft?
ct?1+ it?
lt(14)hst= ot?
ct(15)where W ?
R4K?3K.
htis then used for wordpredicting as in the vanilla version of the hierar-chical model.3.5 Training and TestingParameters are estimated by maximizing likeli-hood of outputs given inputs, similar to standardsequence-to-sequence models.
A softmax func-tion is adopted for predicting each token withinoutput documents, the error of which is first back-propagated through LSTMworddecodeto sentences,then through LSTMsentencedecodeto document repre-sentation eD, and last through LSTMsentenceencodeandLSTMwordencodeto inputs.
Stochastic gradient de-scent with minibatches is adopted.dataset S per D W per D W per SHotel-Review 8.8 124.8 14.1Wikipedia 8.4 132.9 14.8Table 1: Statistics for the Datasets.
W, S and D re-spectively represent number of words, number ofsentences, and number of documents/paragraphs.For example, ?S per D?
denotes average numberof sentences per document.For testing, we adopt a greedy strategy withno beam search.
For a given document D, eDis first obtained given already learned LSTMencodeparameters and word embeddings.
Then in decod-ing, LSTMsentencedecodecomputes embeddings at eachsentence-level time-step, which is first fed into thebinary classifier to decide whether sentence de-coding terminates and then into LSTMworddecodeforword decoding.4 Experiments4.1 DatasetWe implement the proposed autoencoder on twodatasets, a highly domain specific dataset consist-ing of hotel reviews and a general dataset extractedfrom Wkipedia.Hotel Reviews We use a subset of hotel reviewscrawled from TripAdvisor.
We consider only re-views consisting sentences ranging from 50 to 250words; the model has problems dealing with ex-tremely long sentences, as we will discuss later.We keep a vocabulary set consisting of the 25,000most frequent words.
A special ?<unk>?
tokenis used to denote all the remaining less frequenttokens.
Reviews that consist of more than 2 per-cent of unknown words are discarded.
Our train-ing dataset is comprised of roughly 340,000 re-views; the testing set is comprised of 40,000 re-views.
Dataset details are shown in Table 1.Wikipedia We extracted paragraphs fromWikipedia corpus that meet the aforementionedlength requirements.
We keep a top frequentvocabulary list of 120,000 words.
Paragraphswith larger than 4 percent of unknown words arediscarded.
The training dataset is comprised ofroughly 500,000 paragraphs and testing containsroughly 50,000.11104.2 Training Details and ImplementationPrevious research has shown that deep LSTMswork better than shallow ones for sequence-to-sequence tasks (Vinyals et al, 2014; Sutskever etal., 2014).
We adopt a LSTM structure with fourlayer for encoding and four layer for decoding,each of which is comprised of a different set of pa-rameters.
Each LSTM layer consists of 1,000 hid-den neurons and the dimensionality of word em-beddings is set to 1,000.
Other training details aregiven below, some of which follow Sutskever et al(2014).?
Input documents are reversed.?
LSTM parameters and word embeddings areinitialized from a uniform distribution be-tween [-0.08, 0.08].?
Stochastic gradient decent is implementedwithout momentum using a fixed learningrate of 0.1.
We stated halving the learningrate every half epoch after 5 epochs.
Wetrained our models for a total of 7 epochs.?
Batch size is set to 32 (32 documents).?
Decoding algorithm allows generating atmost 1.5 times the number of words in inputs.?
0.2 dropout rate.?
Gradient clipping is adopted by scaling gra-dients when the norm exceeded a thresholdof 5.Our implementation on a single GPU2processes aspeed of approximately 600-1,200 tokens per sec-ond.
We trained our models for a total of 7 itera-tions.4.3 EvaluationsWe need to measure the closeness of the output(candidate) to the input (reference).
We first adopttwo standard evaluation metrics, ROUGE (Lin,2004; Lin and Hovy, 2003) and BLEU (Papineniet al, 2002).ROUGE is a recall-oriented measure widelyused in the summarization literature.
It measuresthe n-gram recall between the candidate text andthe reference text(s).
In this work, we only haveone reference document (the input document) andROUGE score is therefore given by:ROUGEn=?gramn?inputcountmatch(gramn)?gramn?inputcount(gramn)(16)2Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores.where countmatchdenotes the number of n-gramsco-occurring in the input and output.
We reportROUGE-1, 2 and W (based on weighted longestcommon subsequence).BLEU Purely measuring recall will inappropri-ately reward long outputs.
BLEU is designed toaddress such an issue by emphasizing precision.n-gram precision scores for our situation are givenby:precisionn=?gramn?outputcountmatch(gramn)?gramn?outputcount(gramn)(17)BLEU then combines the average logarithm ofprecision scores with exceeded length penaliza-tion.
For details, see Papineni et al (2002).Coherence Evaluation Neither BLEU norROUGE attempts to evaluate true coherence.There is no generally accepted and readily avail-able coherence evaluation metric.3Because ofthe difficulty of developing a universal coherenceevaluation metric, we proposed here only atailored metric specific to our case.
Based on theassumption that human-generated texts (i.e., inputdocuments in our tasks) are coherent (Barzilayand Lapata, 2008), we compare generated outputswith input documents in terms of how muchoriginal text order is preserved.We develop a grid evaluation metric similar tothe entity transition algorithms in (Barzilay andLee, 2004; Lapata and Barzilay, 2005).
The keyidea of Barzilay and Lapata?s models is to firstidentify grammatical roles (i.e., object and sub-ject) that entities play and then model the transi-tion probability over entities and roles across sen-tences.
We represent each sentence as a feature-vector consisting of verbs and nouns in the sen-tence.
Next we align sentences from output doc-uments to input sentences based on sentence-to-sentence F1 scores (precision and recall are com-puted similarly to ROUGE and BLEU but at sen-tence level) using feature vectors.
Note that multi-ple output sentences can be matched to one input3Wolf and Gibson (2005) and Lin et al (2011) proposedmetrics based on discourse relations, but these are hard to ap-ply widely since identifying discourse relations is a difficultproblem.
Indeed sophisticated coherence evaluation metricsare seldom adopted in real-world applications, and summa-rization researchers tend to use simple approximations likenumber of overlapped tokens or topic distribution similarity(e.g., (Yan et al, 2011b; Yan et al, 2011a; Celikyilmaz andHakkani-T?ur, 2011)).1111Input-Wiki washington was unanimously elected President by the electors in both the 1788 ?
1789 and1792 elections .
he oversaw the creation of a strong, well-financed national government thatmaintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , andwon acceptance among Americans of all types .
washington established many forms in govern-ment still used today , such as the cabinet system and inaugural address .
his retirement aftertwo terms and the peaceful transition from his presidency to that of john adams established atradition that continued up until franklin d .
roosevelt was elected to a third term .
washingtonhas been widely hailed as the ?
father of his country ?
even during his lifetime.Output-Wiki washington was elected as president in 1792 and voters <unk> of these two elections until1789 .
he continued suppression <unk> whiskey rebellion of the french revolution war gov-ernment , strong , national well are involved in the establishment of the fin advanced operations, won acceptance .
as in the government , such as the establishment of various forms of inau-guration speech washington , and are still in use .
<unk> continued after the two terms of hisquiet transition to retirement of <unk> <unk> of tradition to have been elected to the thirdparagraph .
but , ?
the united nations of the father ?
and in washington in his life , has beenwidely praised .Input-Wiki apple inc .
is an american multinational corporation headquartered in cupertino , california ,that designs , develops , and sells consumer electronics , computer software , online services ,and personal com - puters .
its bestknown hardware products are the mac line of computers , theipod media player , the iphone smartphone , and the ipad tablet computer .
its online servicesinclude icloud , the itunes store , and the app store .
apple?s consumer software includes the osx and ios operating systems , the itunes media browser , the safari web browser , and the ilifeand iwork creativity and productivity suites .Output-Wiki apple is a us company in california , <unk> , to develop electronics , softwares , and pc , sells.
hardware include the mac series of computers , ipod , iphone .
its online services , includingicloud , itunes store and in app store .
softwares , including os x and ios operating system ,itunes , web browser , < unk> , including a productivity suite .Input-Wiki paris is the capital and most populous city of france .
situated on the seine river , in the north ofthe country , it is in the centre of the le-de-france region .
the city of paris has a population of2273305 inhabitants .
this makes it the fifth largest city in the european union measured by thepopulation within the city limits .Output-Wiki paris is the capital and most populated city in france .
located in the <unk> , in the north of thecountry , it is the center of <unk> .
paris , the city has a population of <num> inhabitants .this makes the eu ?
s population within the city limits of the fifth largest city in the measurement.Input-Review on every visit to nyc , the hotel beacon is the place we love to stay .
so conveniently locatedto central park , lincoln center and great local restaurants .
the rooms are lovely .
beds socomfortable , a great little kitchen and new wizz bang coffee maker .
the staff are so accommo-dating and just love walking across the street to the fairway supermarket with every imaginablegoodies to eat .Output-Review every time in new york , lighthouse hotel is our favorite place to stay .
very convenient , centralpark , lincoln center , and great restaurants .
the room is wonderful , very comfortable bed , akitchenette and a large explosion of coffee maker .
the staff is so inclusive , just across the streetto walk to the supermarket channel love with all kinds of what to eat .Table 2: A few examples produced by the hierarchical LSTM alongside the inputs.sentence.
Assume that sentence sioutputis alignedwith sentence si?input, where i and i?denote positionindex for a output sentence and its aligned input.The penalization score L is then given by:L =2Noutput?
(Noutput?
1)??i?[1,Noutput?1]?j?
[i+1,Noutput]|(j ?
i)?
(j??
i?)|(18)Equ.
18 can be interpreted as follows: (j ?
i)denotes the distance in terms of position index be-tween two outputted sentences indexed by j and i,and (j??
i?)
denotes the distance between theirmirrors in inputs.
As we wish to penalize thedegree of permutation in terms of text order, wepenalize the absolute difference between the twocomputed distances.
This metric is also relevantto the overall performance of prediction and re-call: an irrelevant output will be aligned to a ran-dom input, thus being heavily penalized.
The de-ficiency of the proposed metric is that it concernsitself only with a semantic perspective on coher-ence, barely considering syntactical issues.4.4 ResultsA summary of our experimental results is givenin Table 3.
We observe better performances forthe hotel-review dataset than the open domainWikipedia dataset, for the intuitive reason that1112Model Dataset BLEU ROUGE-1 ROUGE-2 Coherence(L)Standard Hotel Review 0.241 0.571 0.302 1.92Hierarchical Hotel Review 0.267 0.590 0.330 1.71Hierarchical+Attention Hotel Review 0.285 0.624 0.355 1.57Standard Wikipedia 0.178 0.502 0.228 2.75Hierarchical Wikipedia 0.202 0.529 0.250 2.30Hierarchical+Attention Wikipedia 0.220 0.544 0.291 2.04Table 3: Results for three models on two datasets.
As with coherence score L, smaller values signifiesbetter performances.documents and sentences are written in a morefixed format and easy to predict for hotel reviews.The hierarchical model that considers sentence-level structure outperforms standard sequence-to-sequence models.
Attention models at thesentence level introduce performance boost overvanilla hierarchical models.With respect to the coherence evaluation, theoriginal sentence order is mostly preserved: the hi-erarchical model with attention achieves L = 1.57on the hotel-review dataset, equivalent to the factthat the relative position of two input sentencesare permuted by an average degree of 1.57.
Evenfor the Wikipedia dataset where more poor-qualitysentences are observed, the original text order canstill be adequately maintained with L = 2.04.5 Discussion and Future WorkIn this paper, we extended recent sequence-to-sequence LSTM models to the task of multi-sentence generation.
We trained an autoencoderto see how well LSTM models can reconstruct in-put documents of many sentences.
We find thatthe proposed hierarchical LSTM models can par-tially preserve the semantic and syntactic integrityof multi-text units and generate meaningful andgrammatical sentences in coherent order.
Ourmodel performs better than standard sequence-to-sequence models which do not consider the intrin-sic hierarchical discourse structure of texts.While our work on auto-encoding for largertexts is only a preliminary effort toward allowingneural models to deal with discourse, it nonethe-less suggests that neural models are capable of en-coding complex clues about how coherent texts areconnected .The performance on this autoencoder task couldcertainly also benefit from more sophisticated neu-ral models.
For example one extension might alignthe sentence currently being generated with theoriginal input sentence (similar to sequence-to-sequence translation in (Bahdanau et al, 2014)),and later transform the original task to sentence-to-sentence generation.
However our long-termgoal here is not on perfecting this basic multi-textgeneration scenario of reconstructing input docu-ments, but rather on extending it to more importantapplications.That is, the autoencoder described in this work,where input sequenceX is identical to output Y , isonly the most basic instance of the family of doc-ument (paragraph)-to-document (paragraph) gen-eration tasks.
We hope the ideas proposed inthis paper can play some role in enabling suchmore sophisticated generation tasks like summa-rization, where the inputs are original documentsand outputs are summaries or question answering,where inputs are questions and outputs are the ac-tual wording of answers.
Sophisticated genera-tion tasks like summarization or dialogue systemscould extend this paradigm, and could themselvesbenefit from task-specific adaptations.
In sum-marization, sentences to generate at each timestepmight be pre-pointed to or pre-aligned to specificaspects, topics, or pieces of texts to be summa-rized.
Dialogue systems could incorporate infor-mation about the user or the time course of thedialogue.
In any case, we look forward to moresophi4d applications of neural models to the im-portant task of natural language generation.6 AcknowledgementThe authors want to thank Gabor Angeli, SamBowman, Percy Liang and other members of theStanford NLP group for insightful comments andsuggestion.
We also thank the three anonymousACL reviewers for helpful comments.
This workis supported by Enlight Foundation Graduate Fel-lowship, and a gift from Bloomberg L.P, which wegratefully acknowledge.1113ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
arXiv preprintcs/0405039.Asli Celikyilmaz and Dilek Hakkani-T?ur.
2011.
Dis-covery of topically coherent sentences for extractivesummarization.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 491?499.
Association for Computational Lin-guistics.Micha Elsner and Eugene Charniak.
2008.Coreference-inspired coherence modeling.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics on Hu-man Language Technologies: Short Papers, pages41?44.
Association for Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-level discourse parsing with rich linguistic fea-tures.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers-Volume 1, pages 60?68.
Associationfor Computational Linguistics.Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,et al 2010.
Hilda: a discourse parser using sup-port vector machine classification.
Dialogue & Dis-course, 1(3).Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Yangfeng Ji and Jacob Eisenstein.
2014.
Represen-tation learning for text-level discourse parsing.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics, volume 1,pages 13?24.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and represen-tations.
In IJCAI, volume 5, pages 1085?1090.Alex Lascarides and Nicholas Asher.
1991.
Discourserelations and defeasible knowledge.
In Proceedingsof the 29th annual meeting on Association for Com-putational Linguistics, pages 55?62.
Association forComputational Linguistics.Huong LeThanh, Geetha Abeysinghe, and ChristianHuyck.
2004.
Generating discourse structures forwritten texts.
In Proceedings of the 20th inter-national conference on Computational Linguistics,page 329.
Association for Computational Linguis-tics.Jiwei Li and Eduard Hovy.
2014.
A model of coher-ence based on distributed sentence representation.Jiwei Li, Rumeng Li, and Eduard Hovy.
2014.
Recur-sive deep models for discourse parsing.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages2061?2069.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 71?78.Association for Computational Linguistics.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.Automatically evaluating text coherence using dis-course relations.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 997?1006.
Association for ComputationalLinguistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.Thang Luong, Ilya Sutskever, Quoc V Le, OriolVinyals, and Wojciech Zaremba.
2015.
Addressingthe rare word problem in neural machine translation.ACL.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Daniel Marcu.
2000.
The rhetorical parsing of unre-stricted texts: A surface-based approach.
Computa-tional linguistics, 26(3):395?448.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems, pages 3104?3112.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2014.Grammar as a foreign language.
arXiv preprintarXiv:1412.7449.1114Florian Wolf and Edward Gibson.
2005.
Representingdiscourse coherence: A corpus-based study.
Com-putational Linguistics, 31(2):249?287.Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,Ruslan Salakhutdinov, Richard Zemel, and YoshuaBengio.
2015.
Show, attend and tell: Neural im-age caption generation with visual attention.
arXivpreprint arXiv:1502.03044.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Xiaoming Li, and Yan Zhang.
2011a.
Timeline gen-eration through evolutionary trans-temporal summa-rization.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 433?443.
Association for Computational Lin-guistics.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011b.
Evolution-ary timeline summarization: a balanced optimiza-tion framework via iterative substitution.
In Pro-ceedings of the 34th international ACM SIGIR con-ference on Research and development in InformationRetrieval, pages 745?754.
ACM.1115
