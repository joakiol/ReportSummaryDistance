Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723?730,Sydney, July 2006. c?2006 Association for Computational LinguisticsContinuous Space Language Models for Statistical Machine TranslationHolger Schwenk and Daniel Dchelotte and Jean-Luc GauvainLIMSI-CNRS, BP 13391403 Orsay cedex, FRANCE{schwenk,dechelot,gauvain}@limsi.frAbstractStatistical machine translation systems arebased on one or more translation mod-els and a language model of the targetlanguage.
While many different trans-lation models and phrase extraction al-gorithms have been proposed, a standardword n-gram back-off language model isused in most systems.In this work, we propose to use a new sta-tistical language model that is based on acontinuous representation of the words inthe vocabulary.
A neural network is usedto perform the projection and the proba-bility estimation.
We consider the trans-lation of European Parliament Speeches.This task is part of an international evalua-tion organized by the TC-STAR project in2006.
The proposed method achieves con-sistent improvements in the BLEU scoreon the development and test data.We also present algorithms to improve theestimation of the language model proba-bilities when splitting long sentences intoshorter chunks.1 IntroductionThe goal of statistical machine translation (SMT)is to produce a target sentence e from a source sen-tence f .
Among all possible target sentences theone with maximal probability is chosen.
The clas-sical Bayes relation is used to introduce a targetlanguage model (Brown et al, 1993):e?
= argmaxe Pr(e|f) = argmaxe Pr(f |e) Pr(e)where Pr(f |e) is the translation model and Pr(e)is the target language model.
This approach isusually referred to as the noisy source-channel ap-proach in statistical machine translation.Since the introduction of this basic model, manyimprovements have been made, but it seems thatresearch is mainly focused on better translationand alignment models or phrase extraction algo-rithms as demonstrated by numerous publicationson these topics.
On the other hand, we are awareof only a small amount of papers investigatingnew approaches to language modeling for statis-tical machine translation.
Traditionally, statisticalmachine translation systems use a simple 3-gramback-off language model (LM) during decoding togenerate n-best lists.
These n-best lists are thenrescored using a log-linear combination of featurefunctions (Och and Ney, 2002):e?
?
argmaxe Pr(e)?1 Pr(f |e)?2 (1)where the coefficients ?i are optimized on a devel-opment set, usually maximizing the BLEU score.In addition to the standard feature functions, manyothers have been proposed, in particular severalones that aim at improving the modeling of the tar-get language.
In most SMT systems the use of a4-gram back-off language model usually achievesimprovements in the BLEU score in comparisonto the 3-gram LM used during decoding.
It seemshowever difficult to improve upon the 4-gram LM.Many different feature functions were explored in(Och et al, 2004).
In that work, the incorporationof part-of-speech (POS) information gave only asmall improvement compared to a 3-gram back-off LM.
In another study, a factored LM usingPOS information achieved the same results as the4-gram LM (Kirchhoff and Yang, 2005).
Syntax-based LMs were investigated in (Charniak et al,7232003), and reranking of translation hypothesis us-ing structural properties in (Hasan et al, 2006).An interesting experiment was reported at theNIST 2005 MT evaluation workshop (Och, 2005):starting with a 5-gram LM trained on 75 millionwords of Broadcast News data, a gain of about0.5 point BLEU was observed each time when theamount of LM training data was doubled, using atthe end 237 billion words of texts.
Most of thisadditional data was collected by Google on the In-ternet.
We believe that this kind of approach is dif-ficult to apply to other tasks than Broadcast Newsand other target languages than English.
There aremany areas where automatic machine translationcould be deployed and for which considerably lessappropriate in-domain training data is available.We could for instance mention automatic trans-lation of medical records, translation systems fortourism related tasks or even any task for whichBroadcast news and Web texts is of limited help.In this work, we consider the translation of Eu-ropean Parliament Speeches from Spanish to En-glish, in the framework of an international evalua-tion organized by the European TC-STAR projectin February 2006.
The training data consists ofabout 35M words of aligned texts that are alsoused to train the target LM.
In our experiments,adding more than 580M words of Broadcast Newsdata had no impact on the BLEU score, despitea notable decrease of the perplexity of the targetLM.
Therefore, we suggest to use more complexstatistical LMs that are expected to take better ad-vantage of the limited amount of appropriate train-ing data.
Promising candidates are random forestLMs (Xu and Jelinek, 2004), random cluster LMs(Emami and Jelinek, 2005) and the neural networkLM (Bengio et al, 2003).
In this paper, we inves-tigate whether the latter approach can be used in astatistical machine translation system.The basic idea of the neural network LM, alsocalled continuous space LM, is to project the wordindices onto a continuous space and to use a prob-ability estimator operating on this space.
Since theresulting probability functions are smooth func-tions of the word representation, better generaliza-tion to unknown n-grams can be expected.
A neu-ral network can be used to simultaneously learnthe projection of the words onto the continuousspace and to estimate the n-gram probabilities.This is still a n-gram approach, but the LM pos-terior probabilities are ?interpolated?
for any pos-sible context of length n-1 instead of backing-offto shorter contexts.
This approach was success-fully used in large vocabulary speech recognition(Schwenk and Gauvain, 2005), and we are inter-ested here if similar ideas can be applied to statis-tical machine translation.This paper is organized as follows.
In the nextsection we first describe the baseline statisticalmachine translation system.
Section 3 presentsthe architecture of the continuous space LM andsection 4 summarizes the experimental evaluation.The paper concludes with a discussion of futureresearch directions.2 Statistical Translation EngineA word-based translation engine is used based onthe so-called IBM-4 model (Brown et al, 1993).A brief description of this model is given belowalong with the decoding algorithm.The search algorithm aims at finding what tar-get sentence e is most likely to have produced theobserved source sentence f .
The translation modelPr(f |e) is decomposed into four components:1. a fertility model;2. a lexical model of the form t(f |e), whichgives the probability that the target word etranslates into the source word f ;3. a distortion model, that characterizes howwords are reordered when translated;4. and probabilities to model the insertion ofsource words that are not aligned to any tar-get words.An A* search was implemented to find the besttranslation as predicted by the model, when givenenough time and memory, i.e., provided pruningdid not eliminate it.
The decoder manages par-tial hypotheses, each of which translates a subsetof source words into a sequence of target words.Expanding a partial hypothesis consists of cover-ing one extra source position (in random order)and, by doing so, appending one, several or possi-bly zero target words to its target word sequence.For details about the implemented algorithm, thereader is referred to (De?chelotte et al, 2006).Decoding uses a 3-gram back-off target lan-guage model.
Equivalent hypotheses are merged,and only the best scoring one is further expanded.The decoder generates a lattice representing the724weIweshouldshouldmustrememberremindrememberthat,thatthat,thatyou,,,becausebecausebecauseitItheythatcancancan besaybe, becausecanittheywethatcancancanbebehavebehavebehaveitithasforgottenhas forgottenhashasforgottenforgottenbeenforgottenbeenforgottenforgotten..forgotten......Figure 1: Example of a translation lattice.
Sourcesentence: ?conviene recordarlo , porque puedeque se haya olvidado .
?, Reference 1: ?it is ap-propriate to remember this , because it may havebeen forgotten .?
Reference 2: ?it is good to re-member this , because maybe we forgot it .
?explored search space.
Figure 1 shows an exampleof such a search space, here heavily pruned for thesake of clarity.2.1 Sentence SplittingThe execution complexity of our SMT decoder in-creases non-linearly with the length of the sen-tence to be translated.
Therefore, the source textis split into smaller chunks, each one being trans-lated separately.
The chunks are then concatenatedtogether.
Several algorithms have been proposedin the literature that try to find the best splits, seefor instance (Berger et al, 1996).
In this work, wefirst split long sentences at punctuation marks, theremaining segments that still exceed the allowedlength being split linearly.
In a second pass, ad-joining very short chunks are merged together.During decoding, target LM probabilities of thetype Pr(w1|<s>) and Pr(</s>|wn?1wn) will berequested at the beginning and at the end of thehypothesized target sentence respectively.1 This iscorrect when a whole sentence is translated, butleads to wrong LM probabilities when processingsmaller chunks.
Therefore, we define a sentencebreak symbol, <b>, that is used at the beginningand at the end of a chunk.
During decoding a 3-gram back-off LM is used that was trained on textwhere sentence break symbols have been added.Each chunk is translated and a lattice is gen-1The symbols <s> and </s> denote the begin and end ofsentence marker respectively.erated.
The individual lattices are then joined,omitting the sentence break symbols.
Finally, theresulting lattice is rescored with a LM that wastrained on text without sentence breaks.
In thatway we find the best junction of the chunks.
Sec-tion 4.1 provides comparative results of the differ-ent algorithms to split and join sentences.2.2 Parameter TuningIt is nowadays common practice to optimize thecoefficients of the log-linear combination of fea-ture functions by maximizing the BLEU score onthe development data (Och and Ney, 2002).
Thisis usually done by first creating n-best lists thatare then reranked using an iterative optimizationalgorithm.In this work, a slightly different procedure wasused that operates directly on the translation lat-tices.
We believe that this is more efficient thanreranking n-best lists since it guarantees that al-ways all possible hypotheses are considered.
Thedecoder first generates large lattices using the cur-rent set of parameters.
These lattices are thenprocessed by a separate tool that extracts the bestpath, given the coefficients of six feature functions(translations, distortion, fertility, spontaneous in-sertion, target language model probability, and asentence length penalty).
Then, the BLEU scoreof the extracted solution is calculated.
This tool iscalled in a loop by the public numerical optimiza-tion tool Condor (Berghen and Bersini, 2005).
Thesolution vector was usually found after about 100iterations.
In our experiments, only two cyclesof lattice generation and parameter optimizationwere necessary (with a very small difference in theBLEU score).In all our experiments, the 4-gram back-off andthe neural network LM are used to calculate lan-guage model probabilities that replace those of thedefault 3-gram LM.
An alternative would be to de-fine each LM as a feature function and to combinethem under the log-linear model framework, us-ing maximum BLEU training.
We believe that thiswould not make a notable difference in our experi-ments since we do interpolate the individual LMs,the coefficients being optimized to minimize per-plexity on the development data.
However, thisraises the interesting question whether the two cri-teria lead to equivalent performance.
The resultsection provides some experimental evidence onthis topic.7253 Continuous Space Language ModelsThe architecture of the neural network LM isshown in Figure 2.
A standard fully-connectedmulti-layer perceptron is used.
The inputs tothe neural network are the indices of the n?1previous words in the vocabulary hj=wj?n+1,.
.
.
, wj?2, wj?1 and the outputs are the posteriorprobabilities of all words of the vocabulary:P (wj = i|hj) ?i ?
[1, N ] (2)where N is the size of the vocabulary.
The inputuses the so-called 1-of-n coding, i.e., the ith wordof the vocabulary is coded by setting the ith ele-ment of the vector to 1 and all the other elementsto 0.
The ith line of the N ?
P dimensional pro-jection matrix corresponds to the continuous rep-resentation of the ith word.
Let us denote cl theseprojections, dj the hidden layer activities, oi theoutputs, pi their softmax normalization, and mjl,bj , vij and ki the hidden and output layer weightsand the corresponding biases.
Using these nota-tions, the neural network performs the followingoperations:dj = tanh(?lmjl cl + bj)(3)oi =?jvij dj + ki (4)pi = eoi /N?r=1eor (5)The value of the output neuron pi corresponds di-rectly to the probability P (wj = i|hj).
Training isperformed with the standard back-propagation al-gorithm minimizing the following error function:E =N?i=1ti log pi + ???
?jlm2jl +?ijv2ij??
(6)where ti denotes the desired output, i.e., the prob-ability should be 1.0 for the next word in the train-ing sentence and 0.0 for all the other ones.
Thefirst part of this equation is the cross-entropy be-tween the output and the target probability dis-tributions, and the second part is a regulariza-tion term that aims to prevent the neural networkfrom overfitting the training data (weight decay).The parameter ?
has to be determined experimen-tally.
Training is done using a resampling algo-rithm (Schwenk and Gauvain, 2005).projectionlayer hiddenlayeroutputlayerinputprojectionssharedLM probabilitiesfor all wordsprobability estimationNeural Networkdiscreterepresentation:indices in wordlistcontinuousrepresentation:P dimensional vectorsNwj?1 PHNP (wj=1|hj)wj?n+1wj?n+2P (wj=i|hj)P (wj=N|hj)cloiMVdjp1 =pN =pi =Figure 2: Architecture of the continuous spaceLM.
hj denotes the context wj?n+1, .
.
.
, wj?1.
Pis the size of one projection and H ,N is the sizeof the hidden and output layer respectively.
Whenshort-lists are used the size of the output layer ismuch smaller then the size of the vocabulary.It can be shown that the outputs of a neural net-work trained in this manner converge to the poste-rior probabilities.
Therefore, the neural networkdirectly minimizes the perplexity on the train-ing data.
Note also that the gradient is back-propagated through the projection-layer, whichmeans that the neural network learns the projec-tion of the words onto the continuous space that isbest for the probability estimation task.The complexity to calculate one probabilitywith this basic version of the neural network LM isquite high due to the large output layer.
To speedup the processing several improvements were used(Schwenk, 2004):1.
Lattice rescoring: the statistical machinetranslation decoder generates a lattice usinga 3-gram back-off LM.
The neural networkLM is then used to rescore the lattice.2.
Shortlists: the neural network is only used topredict the LM probabilities of a subset of thewhole vocabulary.3.
Efficient implementation: collection of allLM probability requests with the same con-text ht in one lattice, propagation of severalexamples at once through the neural networkand utilization of libraries with CPU opti-mized matrix-operations.The idea behind short-lists is to use the neural726network only to predict the s most frequent words,s being much smaller than the size of the vocab-ulary.
All words in the vocabulary are still con-sidered at the input of the neural network.
TheLM probabilities of words in the short-list (P?N )are calculated by the neural network and the LMprobabilities of the remaining words (P?B) are ob-tained from a standard 4-gram back-off LM:P?
(wt|ht) ={P?N (wt|ht)PS(ht) if wt ?
short-listP?B(wt|ht) else(7)PS(ht) =?w?short?list(ht)P?B(w|ht) (8)It can be considered that the neural network redis-tributes the probability mass of all the words in theshort-list.
This probability mass is precalculatedand stored in the data structures of the back-offLM.
A back-off technique is used if the probabilitymass for a input context is not directly available.It was not envisaged to use the neural networkLM directly during decoding.
First, this wouldprobably lead to slow translation times due to thehigher complexity of the proposed LM.
Second, itis quite difficult to incorporate n-gram languagemodels into decoding, for n>3.
Finally, we be-lieve that the lattice framework can give the sameperformances than direct decoding, under the con-dition that the alternative hypotheses in the latticesare rich enough.
Estimates of the lattice oracleBLEU score are given in the result section.4 Experimental EvaluationThe experimental results provided here were ob-tained in the framework of an international evalua-tion organized by the European TC-STAR project2in February 2006.
This project is envisaged as along-term effort to advance research in all coretechnologies for speech-to-speech translation.The main goal of this evaluation is to trans-late public European Parliament Plenary Sessions(EPPS).
The training material consists of the min-utes edited by the European Parliament in sev-eral languages, also known as the Final Text Edi-tions (Gollan et al, 2005).
These texts werealigned at the sentence level and they are usedto train the statistical translation models (see Ta-ble 1 for some statistics).
In addition, about 100hof Parliament plenary sessions were recorded andtranscribed.
This data is mainly used to train2http://www.tc-star.org/Spanish EnglishSentence Pairs 1.2MTotal # Words 37.7M 33.8MVocabulary size 129k 74kTable 1: Statistics of the parallel texts used to trainthe statistical machine translation system.the speech recognizers, but the transcriptions werealso used for the target LM of the translation sys-tem (about 740k words).Three different conditions are considered inthe TC-STAR evaluation: translation of the Fi-nal Text Edition (text), translation of the tran-scriptions of the acoustic development data (ver-batim) and translation of speech recognizer output(ASR).
Here we only consider the verbatim condi-tion, translating from Spanish to English.
For thistask, the development data consists of 792 sen-tences (25k words) and the evaluation data of 1597sentences (61k words).
Parts of the test data ori-gins from the Spanish parliament which results ina (small) mismatch between the development andtest data.
Two reference translations are provided.The scoring is case sensitive and includes punctu-ation symbols.The translation model was trained on 1.2M sen-tences of parallel text using the Giza++ tool.
Allback-off LMs were built using modified Kneser-Ney smoothing and the SRI LM-toolkit (Stolcke,2002).
Separate LMs were first trained on theEnglish EPPS texts (33.8M words) and the tran-scriptions of the acoustic training material (740kwords) respectively.
These two LMs were then in-terpolated together.
Interpolation usually results inlower perplexities than training directly one LMon the pooled data, in particular if the corporacome from different sources.
An EM procedurewas used to find the interpolation coefficients thatminimize the perplexity on the development data.The optimal coefficients are 0.78 for the Final Textedition and 0.22 for the transcriptions.4.1 Performance of the sentence splittingalgorithmIn this section, we first analyze the performance ofthe sentence split algorithm.
Table 2 compares theresults for different ways to translate the individ-ual chunks (using a standard 3-gram LM versusan LM trained on texts with sentence breaks in-serted), and to extracted the global solution (con-727LM used Concatenate Latticeduring decoding 1-best joinWithoutsentence breaks 40.20 41.63Withsentence breaks 41.45 42.35Table 2: BLEU scores for different ways to trans-late sentence chunks and to extract the global so-lution (see text for details).catenating the 1-best solutions versus joining thelattices followed by LM rescoring).
It can beclearly seen that joining the lattices and recalculat-ing the LM probabilities gives better results thanjust concatenating the 1-best solutions of the in-dividual chunks (first line: BLEU score of 41.63compared to 40.20).
Using a LM trained on textswith sentence breaks during decoding gives an ad-ditional improvement of about 0.7 points BLEU(42.35 compared to 41.63).In our current implementation, the selection ofthe sentence splits is based on punctuation marksin the source text, but our procedure is compatiblewith other methods.
We just need to apply the sen-tence splitting algorithm on the training data usedto build the LM during decoding.4.2 Using the continuous space languagemodelThe continuous space language model was trainedon exactly the same data than the back-off refer-ence language model, using the resampling algo-rithm described in (Schwenk and Gauvain, 2005).In this work, we use only 4-gram LMs, but thecomplexity of the neural network LM increasesonly slightly with the order of the LM.
For eachexperiment, the parameters of the log-linear com-bination were optimized on the development data.Perplexity on the development data set is a pop-ular and easy to calculate measure to evaluate thequality of a language model.
However, it is notclear if perplexity is a good criterion to predictthe improvements when the language model willbe used in a SMT system.
For information, andcomparison with the back-off LM, Figure 3 showsthe perplexities for different configurations of thecontinuous space LM.
The perplexity clearly de-creases with increasing size of the short-list and avalue of 8192 was used.
In this case, 99% of therequested LM probabilities are calculated by theneural network when rescoring a lattice.72737475767778798081820  5  10  15  20  25  30  35PerplexityNumber of training epochs4-gram back-off LMshort-list of 2kshort-list of 4kshort-list of 8kFigure 3: Perplexity of different configurations ofthe continuous space LM.Although the neural network LM could be usedalone, better results are obtained when interpolat-ing it with the 4-gram back-off LM.
It has eventurned out that it was advantageous to train severalneural network LMs with different context sizes3and to interpolate them altogether.
In that way,a perplexity decrease from 79.6 to 65.0 was ob-tained.
For the sake of simplicity we will still callthis interpolation the neural network LM.Back-off LM Neural LM3-gram 4-gram 4-gramPerplexity 85.5 79.6 65.0Dev data:BLEU 42.35 43.36 44.42WER 45.9% 45.1% 44.4%PER 31.8% 31.3% 30.8%Eval data:BLEU 39.77 40.62 41.45WER 48.2% 47.4% 46.7%PER 33.6% 33.1% 32.8%Table 3: Result comparison for the different LMs.BLEU uses 2 reference translations.
WER=worderror rate, PER=position independent WER.Table 3 summarizes the results on the devel-opment and evaluation data.
The coefficients ofthe feature functions are always those optimizedon the development data.
The joined translationlattices were rescored with a 4-gram back-off andthe neural network LM.
Using a 4-gram back-off LM gives an improvement of 1 point BLEU3The values are in the range 150. .
.400.
The other param-eters are: H=500, ?=0.00003 and the initial learning rate was0.005 with an exponential decay.
The networks were trainedfor 20 epochs through the training data.728Spanish: es el nico premio Sajarov que no ha podido recibir su premio despus de ms de tresmil quinientos das de cautiverio .Backoff LM: it is only the Sakharov Prize has not been able to receive the prize after three thousand, five days of detention .CSLM : it is the only Sakharov Prize has not been able to receive the prize after three thousandfive days of detention .Reference 1: she is the only Sakharov laureate who has not been able to receive her prize aftermore than three thousand five hundred days in captivity .Reference 2: she is the only Sacharov prizewinner who couldn?t yet pick up her prize after morethan three thousand five hundred days of imprisonment .Figure 4: Example translation using the back-off and the continuous space language model (CSLM).on the Dev data (+0.8 on Test set) compared tothe 3-gram back-off LM.
The neural network LMachieves an additional improvement of 1 pointBLEU (+0.8 on Test data), on top of the 4-gramback-off LM.
Small improvements of the word er-ror rate (WER) and the position independent worderror rate (PER) were also observed.As usually observed in SMT, the improvementson the test data are smaller than those on the de-velopment data which was used to tune the param-eters.
As a rule of thumb, the gain on the test datais often half as large as on the Dev-data.
The 4-gram back-off and neural network LM show botha good generalization behavior.42.84343.243.443.643.84444.20  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  16466687072747678BLEUscorePerplexityInterpolation coefficient4-gram back-off LMBLEU scorePerplexityFigure 5: BLEU score and perplexity in functionof the interpolation coefficient of the back-off 4-gram LM.Figure 5 shows the perplexity and the BLEUscore for different interpolation coefficients of the4-gram back-off LM.
For a value of 1.0 the back-off LM is used alone, while only the neural net-work LMs are used for a value of 0.0.
Using anEM procedure to minimize perplexity of the inter-polated model gives a value of 0.189.
This valuealso seems to correspond to the best BLEU score.This is a surprising result, and has the advan-tage that we do not need to tune the interpola-tion coefficient in the framework of the log-linearfeature function combination.
The weights of theother feature functions were optimized separatelyfor each experiment.
We noticed a tendency toa slightly higher weight for the continuous spaceLM and a lower sentence length penalty.In a contrastive experiment, the LM trainingdata was substantially increased by adding 352Mwords of commercial Broadcast News data and232M words of CNN news collected on the Inter-net.
Although the perplexity of the 4-gram back-off LM decreased by 5 points to 74.1, we observedno change in the BLEU score.
In order to estimatethe oracle BLEU score of the lattices we build a 4-gram back-off LM on the development data.
Lat-tice rescoring achieved a BLEU score of 59.10.There are many discussions about the BLEUscore being or not a meaningful measure to as-sess the quality of an automatic translation sys-tem.
It would be interesting to verify if the contin-uous space LM has an impact when human judg-ments of the translation quality are used, in partic-ular with respect to fluency.
Unfortunately, this isnot planed in the TC-STAR evaluation campaign,and we give instead an example translation (seeFigure 4).
In this case, two errors were corrected(insertion of the word ?the?
and deletion of thecomma).5 Conclusion and Future WorkSome SMT decoders have an execution complex-ity that increases rapidly with the length of thesentences to be translated, which are usually split729into smaller chunks and translated separately.
Thiscan lead to translation errors and bad modelingof the LM probabilities of the words at both endsof the chunks.
We have presented a lattice join-ing and rescoring approach that obtained signifi-cant improvements in the BLEU score comparedto simply concatenating the 1-best solutions ofthe individual chunks.
The task considered is thetranslation of European Parliament Speeches inthe framework of the TC-STAR project.We have also presented a neural network LMthat performs probability estimation in a contin-uous space.
Since the resulting probability func-tions are smooth functions of the word represen-tation, better generalization to unknown n-gramscan be expected.
This is particularly interestingfor tasks where only limited amounts of appropri-ate LM training material are available, but the pro-posed LM can be also trained on several hundredmillions words.
The continuous space LM is usedto rescore the translation lattices.
We obtainedan improvement of 0.8 points BLEU on the testdata compared to a 4-gram back-off LM, which it-self had already achieved the same improvementin comparison to a 3-gram LM.The results reported in this paper have been ob-tained with a word based SMT system, but thecontinuous space LM can also be used with aphrase-based system.
One could expect that thetarget language model plays a different role ina phrase-based system since the phrases inducesome local coherency on the target sentence.
Thiswill be studied in the future.
Another promis-ing direction that we have not yet explored, is tobuild long-span LM, i.e.
with n much greater than4.
The complexity of our approach increases onlyslightly with n. Long-span LM could possibly im-prove the word-ordering of the generated sentenceif the translation lattices include the correct paths.ReferencesYoshua Bengio, Rejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3(2):1137?1155.A.
Berger, S. Della Pietra, and Vincent J. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22:39?71.Frank Vanden Berghen and Hugues Bersini.
2005.CONDOR, a new parallel, constrained extension ofpowell?s UOBYQA algorithm: Experimental resultsand comparison with the DFO algorithm.
Journal ofComputational and Applied Mathematics, 181:157?175.P.
Brown, S. Della Pietra, Vincent J. Della Pietra, andR.
Mercer.
1993.
The mathematics of statisti-cal machine translation.
Computational Linguistics,19(2):263?311.E.
Charniak, K. Knight, and K. Yamada.
2003.Syntax-based language models for machine transla-tion.
In Machine Translation Summit.Daniel De?chelotte, Holger Schwenk, and Jean-LucGauvain.
2006.
The 2006 LIMSI statistical ma-chine translation system for TC-STAR.
In TC-STARSpeech to Speech Translation Workshop, Barcelona.Ahmad Emami and Frederick Jelinek.
2005.
Ran-dom clusterings for language modeling.
In ICASSP,pages I:581?584.C.
Gollan, M. Bisani, S. Kanthak, R. Schlueter, andH.
Ney.
2005.
Cross domain automatic transcrip-tion on the TC-STAR EPPS corpus.
In ICASSP.Sasa Hasan, Olivier Bender, and Hermann Ney.
2006.Reranking translation hypothesis using structuralproperties.
In LREC.Katrin Kirchhoff and Mei Yang.
2005.
Improved lan-guage modeling for statistical machine translation.In ACL?05 workshop on Building and Using Paral-lel Text, pages 125?128.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In ACL, pages 295?302,University of Pennsylvania.F.-J.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith,K.
Eng, V. Jain, Z. Jin, and D. Radev.
2004.
A smor-gasbord of features for statistical machine transla-tion.
In NAACL, pages 161?168.Franz-Joseph Och.
2005.
The Google statistical ma-chine translation system for the 2005 Nist MT eval-uation, Oral presentation at the 2005 Nist MT Eval-uation workshop, June 20.Holger Schwenk and Jean-Luc Gauvain.
2005.
Train-ing neural network language models on very largecorpora.
In EMNLP, pages 201?208.Holger Schwenk.
2004.
Efficient training of largeneural networks for language modeling.
In IJCNN,pages 3059?3062.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In ICSLP, pages II: 901?904.Peng Xu and Frederick Jelinek.
2004.
Random forestin language modeling.
In EMNLP, pages 325?332.730
