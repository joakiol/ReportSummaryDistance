A Question Answering System Supported by Information Extraction*Rohini SrihariCymfony Inc.5500 Main StreetWilliamsville, NY 14221rohini@cymfony.comWei LiCymfony Inc.5500 Main StreetWilliamsville, NY14221wei@cymfony.comAbstractThis paper discusses an informationextraction (IE) system, Textract, in naturallanguage (NL) question answering (QA) andexamines the role of IE in QA application.
Itshows: (i) Named Entity tagging is animportant component for QA, (ii) an NLshallow parser provides a structural basis forquestions, and (iii) high-level domainindependent IE can result in a QAbreakthrough.IntroductionWith the explosion of information in Internet,Natural language QA is recognized as acapability with great potential.
Traditionally,QA has attracted many AI researchers, but mostQA systems developed are toy systems or gamesconfined to lab and a very restricted omain.More recently, Text Retrieval Conference(TREC-8) designed a QA track to stimulate theresearch for real world application.Due to little linguistic support from textanalysis, conventional IR systems or searchengines do not really perform the task ofinformation retrieval; they in fact aim at onlydocument retrieval.
The following quote from theQA Track Specifications (www.research.att.com/-singhal/qa-track-spec.txt) in the TRECcommunity illustrates this point.Current information retrieval systems allowus to locate documents hat might contain thepertinent information, but most of them leaveit to the user to extract he useful informationfrom a ranked list.
This leaves the (oftenunwilling) user with a relatively largeamount of text o consume.
There is an urgentneed for tools that would reduce the amountof text one might have to read in order toobtain the desired information.
This trackaims at doing exactly that for a special (andpopular) class of information seekingbehavior: QUESTION ANSWERING.
Peoplehave questions and they need answers, notdocuments.
Automatic question answeringwill definitely be a significant advance in thestate-of-art information retrieval technology.Kupiec (1993) presented a QA systemMURAX using an on-line encyclopedia.
Thissystem used the technology of robust shallowparsing but suffered from the lack of basicinformation extraction support.
In fact, the mostsiginifcant IE advance, namely the NE (NamedEntity) technology, occured after Kupiec (1993),thanks to the MUC program (MUC-7 1998).High-level IE technology beyond NE has notbeen in the stage of possible application untilrecently.AskJeeves launched a QA portal(www.askjeeves.com).
It is equipped with afairly sophisticated natural language questionparser, but it does not provide direct answers tothe asked questions.
Instead, it directs the user tothe relevant web pages, just as the traditionalsearch engine does.
In this sense, AskJeeves hasonly done half of the job for QA.We believe that QA is an ideal test bed fordemonstrating the power of IE.
There is a naturalco-operation between IE and IR; we regard QAas one major intelligence which IE can offer IR.
* This work was supported in part by the SBIR grants F30602-98-C-0043 and F30602-99-C-0102 from Air ForceResearch Laboratory (AFRL)/IFED.166An important question then is, what type ofIE can support IR in QA and how well does itsupport it?
This forms the major topic of thispaper.
We structure the remaining part of thepaper as follows.
In Section 1, we first give anoverview of the underlying IE technology whichour organization has been developing.
Section 2discusses the QA system.
Section 3 describes thelimitation of the current system.
Finally, inSection 4, we propose a more sophisticated QAsystem supported by three levels of IE.1 Overview of Textract IEThe last decade has seen great advance andinterest in the area of IE.
In the US, the DARPAsponsored Tipster Text Program \[Grishman1997\] and the Message UnderstandingConferences (MUC) \[MUC-7 1998\] have beenthe driving force for developing this technology.In fact, the MUC specifications for various IEtasks have become de facto standards in the IEresearch community.
It is therefore necessary topresent our IE effort in the context of the MUCprogram.MUC divides IE into distinct tasks,namely, NE (Named Entity), TE (TemplateElement), TR (Template Relation), CO(Co-reference), and ST (Scenario Templates)\[Chinchor & Marsh 1998\].
Our proposal forthree levels of IE is modelled after the MUCstandards using MUC-style representation.However, we have modified the MUC IE taskdefinitions in order to make them more usefuland more practical.
More precisely, we propose ahierarchical, 3-level architecture for developing akernel IE system which is domain-independentthroughout.The core of this system is a state-of-the-artNE tagger \[Srihari 1998\], named Textract 1.0.The Textract NE tagger has achieved speed andaccuracy comparable tothat of the few deployedNE systems, such as NetOwl \[Krupka &Hausman 1998\] and Nymble \[Bikel et al1997\].It is to be noted that in our definition of NE,we significantly expanded the type ofinformation to be extracted.
In addition to all theMUC defined NE types (person, organization,location, time, date, money and percent), thefollowing types/sub-types of information are alsoidentified by the TextractNE module:?
duration, frequency, age?
number, fraction, decimal, ordinal, mathequation?
weight, length, temperature, angle, area,capacity, speed, rate?
product, software?
address, email, phone, fax, telex, www?
name (default proper name)Sub-type information like company,government agency, school (belonging to thetype organization) and military person, religiousperson (belonging to person) are also identified.These new sub-types provide a better foundationfor defining multiple relationships between theidentified entities and for supporting questionanswering functionality.
For example, the key toa question processor is to identify the askingpoint (who, what, when, where, etc.).
In manycases, the asking point corresponds to an NEbeyond the MUC definition, e.g.
thehow+adjective questions: how long (duration orlength), how far (length), how often (frequency),how old (age), etc.Level-2 IE, or CE (Correlated Entity), isconcerned with extracting pre-defined multiplerelationships between the entities.
Consider theperson entity as an example; the TextractCEprototype is capable of extracting the keyrelationships uch as age, gender, affiliation,position, birthtime, birth__place, spouse,parents, children, where.from, address, phone,fax, email, descriptors.
As seen, the informationin the CE represents a mini-CV or profile of theentity.
In general, the CE template integrates andgreatly enriches the information contained inMUC TE and TR.The final goal of our IE effort is to furtherextract open-ended general events (GE, or level 3IE) for information like who did what (to whom)when (or how often) and where.
By generalevents, we refer to argument structures centeringaround verb notions plus the associatedinformation of time/frequency and location.
Weshow an example of our defined GE extractedfrom the text below:Julian Hill, a research chemist whoseaccidental discovery of a tough, taffylikecompound revolutionized everyday life afterit proved its worth in warfare and courtship,167died on Sunday in Hockessin, Del.\[1\] <GE_TEMPLATE> :=PREDICATE: dieARGUMENTI: Julian HillTIME: SundayLOCATION: Hockessin, DelFigure 1 is the overall system architecture forthe IE system Textract hat our organization hasbeen developing.Kernet IE Modutes  L|ngui_sti_cLM_odu!esI .
.
.
.
.
.
.
.
.
.
.
.
.
I I .
.
.
.
.
.
.
.
.
.
II I I I ,,l l I , !
II I I I'l J ' I I!
II II I I I ,i I i ', I ,I !
I I , l ?
i  , ,I I I II IF L - -  - - - -~  .
.
.
.
.
L - -  - -  - -  .
- -  - -  - -  | .
.
.
.Apptication ModutesNE: NIiml~ EnlilyTitl~klll QA: Que~tlon AnsweringCE: Come,led Entity ExtrmClkm BR: In~lllgenl ~ws lngGE: Gcn~mI Evenl Ex~ct~on AS; Auio~ SUl lenco :  ce-  mfcmnc ~1 momial l  s ~ p~Figure 1: Textract IE System ArchitectureThe core of the system consists of threekernel IE modules and six linguistic modules.The multi-level linguistic modules erve as anunderlying support system for different levels ofIE.
The IE results are stored in a database whichis the basis for IE-related applications like QA,BR (Browsing, threading and visualization) andAS (Automatic Summarization).
The approachto IE taken here, consists of a unique blend ofmachine learning and FST (finite statetransducer) rule-based system \[Roche & Schabes1997\].
By combining machine learning with anFST rule-based system, we are able to exploit hebest of both paradigms while overcoming theirrespective weaknesses \[Srihari 1998, Li & Srihari2000\].2 NE-Supported QAThis section presents the QA system based onNamed Entity tagging.
Out of the 200 questionsthat comprised the TREC-8 QA trackcompetition, over 80% asked for an NE, e.g.
who(PERSON), when (T IME\ [  DATE), where(LOCATION), how far (LENGTH).
Therefore,the NE tagger has been proven to be very helpful.Of course, the NE of the targeted type is onlynecessary but not complete in answering suchquestions because NE by nature only extractsisolated individual entities from the text.Nevertheless, using even crude methods like "thenearest NE to the queried key words" or "the NEand its related key words within the same line (orsame paragraph, etc.
)", in most cases, the QAsystem was able to extract ext portions whichcontained answers in the top five list.Figure 2 illustrates the system design ofTextractQA Prototype.
There are twocomponents for the QA prototype: QuestionProcessor and Text Processor.
The Text Matchermodule links the two processing results and triesto find answers to the processed question.Matching is based on keywords, plus the NEtype and their common location within a samesentence.Quest ion  Prc~:essori: :eXt P r~_~ .
.
.
.
.
.
.
.
.
.
.
.
?
~i i ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i .
.
.
.
iFigure 2: Textract/QA 1.0 Prototype ArchitectureThe general algorithm for questionanswering is as follows:168Process QuestionShallow parse questionDetermine Asking PointQuestion expansion (using word lists)Process DocumentsTokenization, POS tagging, NE IndexingShallow Parsing (not yet utilized)Text MatcherIntersect search engine results with NErank answers2.1 Question ProcessingThe Question Processing results are a list ofkeywords plus the information for asking point.For example, the question:\[2\] Who won the 1998 Nobel Peace Prize?contains the following keywords: won, 1998,Nobel, Peace, Prize.
The asking point Who refersto the NE type person.
The output beforequestion expansion is a simple 2-feature templateas shown below:\[3\] asking_point: PERSONkey_word: { won, 1998, Nobel,Peace, Prize }The following is an example where theasking point does not correspond to any type ofNE in our definition.\[3\] Why did David Koresh ask the FBI for aword processor ?The system then maps it to the followingquestion template :\[4\] asking_point:key_word:REASON{ ask, David, Koresh,FBI, word, processor }The question processor scans the question tosearch for question words (wh-words) and mapsthem into corresponding NE types/sub-types orpre-defined notions like REASON.We adopt wo sets of pattern matching rulesfor this purpose: (i) structure based patternmatching rules; (ii) simple key word basedpattern matching rules (regarded as default rules).It is fairly easy to exhaust the second set of rulesas interrogative question words/phrases form aclosed set.
In comparison, the development ofthe first set of rules are continuously beingfine-tuned and expanded.
This strategy of usingtwo set of rules leads to the robustness of thequestion processor.The first set of rules are based on shallowparsing results of the questions, using CymfonyFST based Shallow Parser.
This parser identifiesbasic syntactic onstructions like BaseNP (BasicNoun Phrase), BasePP (Basic PrepositionalPhrase) and VG (Verb Group).The following is a sample of the first set ofrules:\[6\] Name NP (city I country I company) -->CITYICOUNTRYICOMPANY\[7\] Name NP(person_w) --> PERSON\[8\] Name NP(org_w) --> ORGANIZATION\[9\] Name NP(NOT person_w, NOT org_w)--> NAMERule \[6\] checks the head word of the NP.
Itcovers cases like VG\[Name\] NP\[a country\] thatVG\[is developing\] NP\[a magnetic levitationrailway system\].
Rule \[7\] works for cases likeVG\[Name\] NP\[the first private citizen\] VG\[tofly\] PP\[in space\] as citizen belongs to the wordclass person_w.
Rule \[9\] is a catch-all rule: if theNP is not of class person (person_w) ororganization (org_w), then the asking point is aproper name (default NE), often realized inEnglish in capitalized string of words.
Examplesinclude Name a film that has won the GoldenBear in the Berlin Film Festival.The word lists org_w and person_w arecurrently manually maintained based oninspection of large volumes of text.
An effort isunderway to automate the learning of such wordlists by utilizing machine learning techniques.We used the following patterntransformations to expand our ruleset:(Please) name NP\[X\]--> what/which Aux(be) (the name of) NP\[X\]--> NP(what/which...X)In other words, the four rules are expanded to12 rules.
For example, Rule \[10\] belowcorresponds to Rule \[6\]; Rule \[11\] is derived169from Rule \[7\].\[10\] what/which Aux(be) NP (city \[ country \[company) -->CITY I COUNTRY \[ COMPANY\[11\] NP(what/which ... person_w) -->PERSONRule \[10\] extracts the asking point fromcases like NP\[What\] Aux\[is\] NP\[the largestcountry\] PP\[in the world\].
Rule \[11\] covers thefollowing questions: NP\[What costumedesigner\] VG\[decided\] that NP\[MichaelJacksonl VG\[should only wear\] NP\[one glove\],NP\[Which former Ku Klux Klan member\]VG\[won\] NP\[an elected office\] PP\[in the U.S.\],NP\[What Nobel laureate\] VG\[was expelled\]PP\[from the Philippines\] PP\[before theconference\] PP\[on East Timor\], NP\[Whatfamous communist leader\] VG\[died\] PP\[inMexico City\], etc.As seen, shallow parsing helps us to capture avariety of natural anguage question expressions.However, there are cases where some simple keyword based pattern matching would be enough tocapture the asking point.
That is our second setof rules.
These rules are used when the first set ofrules has failed to produce results.
The followingis a sample of such rules:\[ 12\] who/whom --> PERSON\[13\] when --> TIME/DATE\[14\] where/what place --> LOCATION\[15\] what time (of day) --> TIME\[16\] what day (of the week) --> DAY\[17\] what/which month --> MONTH\[18\] what age/how old --> AGE\[19\] what brand --> PRODUCT\[20\] what --> NAME\[21\] how far/tall/high --> LENGTH\[22\] how large/hig/small --> AREA\[23\] how heavy --> WEIGHT\[24\] how rich --> MONEY\[25\] how often --> FREQUENCY\[26\] how many --> NUMBER\[27\] how long --> LENGTH/DURATION\[28\] why/for what --> REASONIn the stage of question expansion, thetemplate in \[4\] would be expanded to thetemplate shown in \[29\]:\[29\] asking_point:key_word:{because{because of\]due to{thanks to{since Iin order{to VB}{ asklaskslasked\[asking,David,Koresh,FBI,word, processor}The last item in the asking._point list attemptsto find an infinitive by checking the word tofollowed by a verb (with the part-of-speech tagVB).
As we know, infinitive verb phrases areoften used in English to explain a reason for someaction.2.2 Text ProcessingOn the text processing side, we first send thequestion directly to a search engine in order tonarrow down the document pool to the first n, say200, documents for IE processing.
Currently,this includes tokenization, POS tagging and NEtagging.
Future plans include several evels ofparsing as well; these are required to support CEand GE extraction.
It should be noted that allthese operations are extremely robust and fast,features necessary for large volume textindexing.
Parsing is accomplished throughcascaded finite state transducer grammars.2.3 Text MatchingThe Text Matcher attempts to match the questiontemplate with the processed ocuments for boththe asking point and the key words.
There is apreliminary ranking standard built-in the matcherin order to find the most probable answers.
Theprimary rank is a count of how many uniquekeywords are contained within a sentence.
Thesecondary ranking is based on the order that thekeywords appear in the sentence compared totheir order in the question.
The third ranking isbased on whether there is an exact match or avariant match for the key verb.In the TREC-8 QA track competition,Cymfony QA accuracy was 66.0%.
Consideringwe have only used NE technology to support QAin this run, 66.0% is a very encouraging result.3 LimitationThe first limitation comes from the types ofquestions.
Currently only wh-questions arehandled although it is planned that yes-noquestions will be handled once we introduce CE170and GE templates to support QA.
Among thewh-questions, the why-question andhow-question t are more challenging because theasking point cannot be simply mapped to the NEtypes/sub-types.The second limitation is from the nature ofthe questions.
Questions like Where can l findthe homepage for Oscar winners or Where can Ifind info on Shakespeare's works might beanswerable asily by a system based on awell-maintained data base of home pages.
Sinceour system is based on the processing of theunderlying documents, no correct answer can beprovided if there is no such an answer (explicitlyexpressed in English) in the processeddocuments.
In TREC-8 QA, this is not a problemsince every question is guaranteed to have at leastone answer in the given document pool.However, in the real world scenario such as a QAportal, it is conceived that the IE results based onthe processing of the documents hould becomplemented by other knowledge sources uchas e-copy of yellow pages or other manuallymaintained and updated ata bases.The third limitation is the lack of linguisticprocessing such as sentence-level parsing andcross-sentential co-reference (CO).
This problemwill be gradually solved when high-level IEtechnology is introduced into the system.4 Future Work: Multi-level IE Supported QAA new QA architecture is under development; iwill exploit all levels of the IE system, includingCE and GE.The first issue is how much CE cancontribute to a better support of QA.
It is foundthat there are some frequently seen questionswhich can be better answered once the CEinformation is provided.
These questions are oftwo types: (i) what/who questions about an NE;(ii) relationship questions.Questions of the following format require CEtemplates as best answers: who/what is NE?
Forexample, Who is Julian Hill?
Who is BillClinton?
What is Du Pont?
What is Cymfony?To answer these questions, the system can simply1 For example, How did one make a chocolate cake?How+Adjective questions (e.g.
how long, how big,how old, etc.)
are handled fairly well.retrieve the corresponding CE template toprovide an "assembled" answer, as shown below.Q: Who is Julian Hill?A: name: Julian Werner Hilltype: PERSONage: 91gender: MALEposition: research chemistaffiliation: Du Pont Co.education: Washington University;MITQ: What is Du Pont?A: name: Du Pont Co,type: COMPANYstaff: Julian Hill; Wallace Carothers.Questions specifically about a CErelationship include: For which company didJulian Hill work?
(affiliation relationship) Whoare employees of Du Pont Co.?
(staffrelationship) What does Julian Hill do?
(position/profession relationship) Whichuniversity did Julian Hill graduate from?
(education relationship), etc.
2The next issue is the relationships betweenGE and QA.
It is our belief that the GEtechnology will result in a breakthrough for QA.In order to extract GE templates, the textgoes through a series of linguistic processing asshown in Figure 1.
It should be noted that thequestion processing is designed to go throughparallel processes and share the same NLPresources until the point of matching and ranking.The merging of question templates and GEtemplates in Template Matcher are fairlystraightforward.
As they both undergo the sameNLP processing, the resulting semantic templatesare of the same form.
Both question templatesand GE templates correspond to fairlystandard/predictable patterns (the PREDICATEvalue is open-ended, but the structure remainsstable).
More precisely, a user can ask questionson general events themselves (did what) and/oron the participants of the event (who, whom,what) and/or the time, frequency and place ofevents (when, how often, where).
This addresses2 An alpha version of TextractQA supported by bothNE and CE has been implemented and is being tested.171by far the most types of general questions of apotential user.For example, if a user is interested incompany acquisition events, he can ask questionslike: Which companies ware acquired byMicrosoft in 1999?
Which companies didMicrosoft acquire in 1999?
Our system will thenparse these questions into the templates as shownbelow:\[31\] <Q_TEMPLATE> :=PREDICATE: acquireARGUMENT1: MicrosoftARGUMENT2: WHAT(COMPANY)TIME: 1999If the user wants to know when someacquisition happened, he can ask: When wasNetscape acquired?
Our system will thentranslate it into the pattern below:\[32\] <QTEMPLATE> :=PREDICATE: acquireARGUMENT1: WHOARGUMENT2: NetscapeTIME: WHENNote that WHO, WHAT, WHEN above arevariable to be instantiated.
Such questiontemplates serve as search constraints o filter theevents in our extracted GE template database.Because the question templates and the extractedGE template share the same structure, a simplemerging operation would suffice.
Nevertheless,there are two important questions to be answered:(i) what if a different verb with the same meaningis used in the question from the one used in theprocessed text?
(ii) what if the question asksabout something beyond the GE (or CE)information?
These are issues that we arecurrently researching.ReferencesBikel D.M.
et al (1997) Nymble: aHigh-PerformanceLearning Name-finder.
"Proceedings of the FifthConference on Applied Natural LanguageProcessing", Morgan Kaufmann Publishers, pp.194-201Chinchor N. and Marsh E. (1998) MUC- 7 InformationExtraction Task Definition (version 5.1),"Proceedings ofMUC-7".Grishman R. (1997) TIPSTER Architecture DesignDocument Version 2.3.
Technical report, DARPAKrupka G.R.
and Hausman K. (1998) IsoQuest Inc.:Description of the NetOwl (TM) Extractor Systemas Used for MUC-7, "Proceedings ofMUC-7".Kupiec J.
(1993) MURAX: A Robust LinguisticApproach For Question Answering Using AnOn-Line Encyclopaedia, "Proceedings ofSIGIR-93 93" Pittsburgh, Penna.Li, W & Srihari, R. 2000.
Flexible InformationExtraction Learning Algorithm, Final TechnicalReport, Air Force Research Laboratory, RomeResearch Site, New YorkMUC-7 (1998) Proceedings of the Seventh MessageUnderstanding Conference (MUC-7), published onthe website _http://www.muc.saic.com/Roche E. and Schabes Y.
(1997) Finite-StateLanguage Processing, MIT Press, Cambridge, MASrihari R. (1998) A Domain Independent EventExtraction Toolkit, AFRL-IF-RS-TR-1998-152Final Technical Report, Air Force ResearchLaboratory, Rome Research Site, New York172
