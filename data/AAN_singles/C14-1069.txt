Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 731?740, Dublin, Ireland, August 23-29 2014.Low-Dimensional Manifold Distributional Semantic ModelsGeorgia AthanasopoulouSchool of Electronic &Computer EngineeringT.U.C.
Chania, Greecegathanasopoulou@isc.tuc.grElias IosifAthena Research andInnovation Center,15125 Maroussi, Greeceiosif.elias@gmail.comAlexandros PotamianosSchool of Electrical &Computer EngineeringN.T.U.A, Athens, Greeceapotam@gmail.comAbstractMotivated by evidence in psycholinguistics and cognition, we propose a hierarchical distributedsemantic model (DSM) that consists of low-dimensional manifolds built on semantic neighbor-hoods.
Each semantic neighborhood is sparsely encoded and mapped into a low-dimensionalspace.
Global operations are decomposed into local operations in multiple sub-spaces; resultsfrom these local operations are fused to come up with semantic relatedness estimates.
ManifoldDSM are constructed starting from a pairwise word-level semantic similarity matrix.
The pro-posed model is evaluated on semantic similarity estimation task significantly improving on thestate-of-the-art.1 IntroductionThe estimation of semantic similarity between words, sentences and documents is a fundamental problemfor many research disciplines including computational linguistics (Malandrakis et al., 2011), semanticweb (Corby et al., 2006), cognitive science and artificial intelligence (Resnik, 2011; Budanitsky andHirst, 2001).
In this paper, we study the geometrical structure of the lexical space in order to extract se-mantic relations among words.
In (Karlgren et al., 2008), the high-dimensional lexical space is assumedto consist of manifolds of very low dimensionality that are embedded in this high dimensional space.The manifold hypothesis is compatible with evidence from psycholinguistics and cognitive science.
In(Tenenbaum et al., 2011), the question ?How does the mind work??
is answered as follows: cognitiveorganization is based on domains with similar items connected to each other and lexical informationis represented hierarchically, i.e., a domain that consists of similar lexical entries may be representedby a more abstract concept.
An example of such a domain is {blue, red, yellow, pink, ...} that corre-sponds by the concept of color.
An inspiring analysis about the geometry of thought, as well as cognitiveevidence for the low-dimensional manifold assumption can be found in (Gardenfors, 2000), e.g., thedomain of color is argued to be cognitively represented as an one-dimensional manifold.
Following thelow-dimensional manifold hypothesis we propose to extend distributional semantic models (DSMs) intoa hierarchical model of domains (or concepts) that contain semantically similar words.
Global operationson the lexical space are decomposed into local operations on the low-dimensional domain sub-manifolds.Our goal is to exploit this hierarchical low-rank model to estimate relations between words, such as se-mantic similarity.There has been much research interest on devising data-driven approaches for estimating semanticsimilarity between words.
DSMs (Baroni and Lenci, 2010) are based on the distributional hypothesisof meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlapof their linguistic contexts.
DSMs are typically constructed from co-occurrence statistics of word tuplesthat are extracted on existing corpora or on corpora specifically harvested from the web.
In (Iosif andPotamianos, 2013), general-purpose, language-agnostic algorithms were proposed for estimating seman-tic similarity using no linguistic resources other than a corpus created via web queries.
The key idea ofthis work was the construction of semantic networks and semantic neighborhoods that capture smoothThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footer areadded by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/731co-occurrence and context similarity statistics.
The majority of DSMs adopt high-dimensional represen-tations, while the underlying space geometry is not explicitly taken into consideration during the designof algorithms aimed for performing several semantic tasks.We propose the construction of a low-dimensional manifold DSM consting of four steps: 1) identifythe domains that correspond to the low-dimensional manifolds, 2) run the dimensionality reduction al-gorithm for each domain, 3) construct a DSM for each domain, and 4) combine the manifold DSMs tocome up with global measures of lexical relations.
A variety of algorithms can be found in the literaturefor projecting a set of tokens into low dimensional sub-spaces, given a token similarity or dissimilaritymatrix.
Depending on the nature of the dataset, these projection algorithms may or may not preservethe local geometries of the original dataset.
Most dimensionality reduction algorithms make the implicitassumption that the underlying space is metric, e.g., Multidimensional Scaling (MDS) (Torgerson, 1952)or Principal Component Analysis (PCA) (Jolliffe, 2005) or the ones using non-negative matrix factor-ization (Tsuge et al., 2001) and typically fail to capture the geometry of manifolds embedded in highdimensional spaces.
A variety of dimensionality reduction algorithms have been developed that respectthe local geometry.
Some examples are the Isomap algorithm (Tenenbaum et al., 2000) that performsthe projection based on a weighted neighborhood graph, Local Linear Embedings (LLE) (Roweis andSaul, 2000) that assigns neighbors to each data point, Random Projections (Baraniuk and Wakin, 2009),(Li et al., 2006) that preserves the manifold geometry by executing random linear projections and oth-ers (Hessian Eigenmaps (HLLE) (Donoho and Grimes, 2003); Maximum Variance Unfolding (MVU)(Wang, 2011)).
The manifold hypothesis has also been studied by the representation learning commu-nity where the local geometry is disentangled from the global geometry mainly by using neighborhoodgraphs (Weston et al., 2012) or coding schemes (Yu et al., 2009).
For a review see (Bengio et al., 2013).A fundamental problem with all aforementioned methods when applied to lexical semantic spaces isthat they do not account for ambiguous tokens, i.e., word senses.
The main assumption of dimensionalityreduction and manifold unfolding algorithms is that each token (word) belongs to a single sub-manifold.This in fact is not true for polysemous words, for example the word ?green?
could belong both to thedomain colors, as well as to the domain plants.
In essence, lexical semantic spaces are manifolds thathave singularities: the manifold collapses in the neighborhood of polysemous words that can be thoughtof semantic black holes that can instantaneously transfer you from one domain to another.
Our proposedsolution to this problem is to allow words to live in multiple sub-manifolds.The algorithms proposed in this paper build on recent research work on distributional semantic modelsand manifold representational learning.
Manifold DSMs can be trained directly from a corpus and donot require a-priori knowledge or any human-annotated resources (just like DSMs).
We show that theproposed low-dimensional, sparse and hierarchical manifold representation significantly improves on thestate-of-the-art for the problem of semantic similarity estimation.2 Metrics of Semantic SimilaritySemantic similarity metrics can be broadly divided into the following types: (i) metrics that rely onknowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledgesource.
Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni andLenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954).
DSMs can be distin-guished into (i) unstructured: use bag-of-words model (Iosif and Potamianos, 2010) and (ii) structured:exploitation of syntactic relationships between words (Grefenstette, 1994; Baroni and Lenci, 2010).
Thevector space model (VSM) constitutes the main implementation for both unstructured and structuredDSMs.
Cosine similarity constitutes a measurement of word similarity that is widely used on top ofthe VSM.
The similarity between two words is estimated as the cosine of their respective vectors whoseelements correspond to corpus-based co-occurrence statistics.
In essence, the similarity between wordsis computed via second-order co-occurrences.Direct (i.e., first-order) co-occurrences can be also used for the estimation of semantic similarity (Bol-legala et al., 2007; Gracia et al., 2006).
The exploitation of first-order co-occurrence statistics constitutesthe simplest form of unstructured DSMs.
A key parameter for such models is the definition of the contextin which the words of interest co-occur: from entire documents (Bollegala et al., 2007) to paragraphs732(V?eronis, 2004) and sentences (Iosif and Potamianos, 2013).
The effect of co-occurrence context forthe task of similarity computation between nouns is discussed in (Iosif and Potamianos, 2013).
Theunderlying assumption is that two words that co-occur in a specified context are semantically related.3 Collapsed Manifold Hypothesis, Low-Dimensionality and SparsityThe intuition behind this work is that although the lexical semantic space proper is high-dimensional, itis organized in such a way that interesting semantic relations can be exported from manifolds of muchlower dimensionality embedded in this high dimensional space (Karlgren et al., 2008).
We assume that(at least some of) these sub-manifolds contain semantically similar words (or word senses).
For example,a potential sub-manifold in the lexical space could be the one that contains the colors (e.g., red, blue,green).
But in fact many words, such as book, green, fruit, are expected to belong simultaneously insemantically different manifolds because they have multiple meanings.A simple way to bootstrap the manifold recreation process is to build a domain around each word,i.e., the semantic neighborhood of each word defines a domain.
For example, in Figure 1 we showthe semantic neighborhood of fruit.
The connections between words indicate high semantic similarity,i.e., this is a pruned semantic similarity graph of all words in the semantic neighborhood of the word?fruit?.
It is clear from this example that in a typical neighborhood there exist word pairs that should benativegenusbshurbbplantfloweringbtreebspeciesbbgardenbsoilbanimalbwaterbseedflowerbbfruitbvegetablebapplejuicedaiquiriborangebdrinkbzestblemonsugarsaltbflourbcreambbutterbbbmilkbcornbhoneybtomatobb bbFigure 1: Visualization of the semantic neighborhood of the word ?fruit?.?connected?
to each other because they have close semantic relation, like {flower, plant} and others thatshould not be ?connected?
because they are semantically apart, like {garden, salt}.
A sparse encoding ofthe semantic similarity relations in a neighborhood is needed in order to achieve (via multi-dimensionalscaling) a parsimonious representation with good geometric properties1.The graph connectivity or sparseness matrix identifies the word pairs that should be encoded in aneighborhood is defined as?S ?
{0, 1}n?n, where value?S(i, j) = 1 indicates that the ith, jthwordpair is encoded, while?S(i, j) = 0 indicates that the pair is ignored (n is the number of words andi, j = 1, .., n in the neighborhood).
We define the degree of sparseness of matrix?S as the percentage of0?s in the matrix.4 Dimensionality ReductionIn this section, the Sparse Projection (SP) algorithm is described (see also Algorithm 1).
SP is the corealgorithm for constructing manifold DSMs presented in Section 5.
SP is a dimensionality reductionalgorithm that projects a set of n words into a vector space of d dimensions.
The input to the algorithmis a dissimilarity or semantic distance matrix P ?
Rn?n, where element P(i, j) encodes the degreeof dissimilarity between words wiand wj.
The output of SP are the d-dimensional coordinate vectorsof the n projected words that form a matrix X ?
Rn?d.
Each row xi?
R1?dof matrix X ?
Rn?dcorresponds to the coordinates of the ithword wi.
Once X is estimated the dissimilarity matrix isrecomputed and updated to new values, as discussed next.
Each paragraph that follows corresponds to amodule in Algorithm 1.1Compare for example with Isomap (Tenenbaum et al., 2000) were a short- and long-distance metric is used.
When usingsparse encoding the long-distance metric is set to a very large fixed number (similarity set to 0).
In both cases, the underlyingmanifold is unfolded and low-dimensional representation with (close to) metric properties are discovered.733Semantic Distance Re-estimation: Given the matrix X ?
Rn?dcontaining the vector projections ofwords in the d-dimensional space, the dissimilarity matrix is re-estimated using the Euclidean distance2.Let?P ?
Rn?nbe the matrix with the new dissimilarity scores then the new dissimilarity score betweenwords wiand wjis simply:?P(i, j) = ?xi?
xj?2, where xi, xjare the vectors corresponding to wordswi, wjrespectively, i, j = 1, .., n and ?.
?2is the Euclidean norm.Connectivity Graph and Sparsity: As discussed in Section 3, given a set of words only a smallsubset of lexical relations should be explicitly encoded between pairs of these words.
Therefore,the SP algorithm should only take into account strongly related word pairs and ignore the rest.
Thisis the main difference between our approach compared to the generic MDS algorithm proposed in(Torgerson, 1952).
In order to apply the sparseness constraint, we first construct the connectivitymatrix?S ?
{0, 1}n?n.
Word pairs (wi, wj) with small similarity values (or equivalently large semanticdistance) are penalized: zero values are assigned to their corresponding position (i, j) in?S matrix.
Inessence, the matrix?S is obtained by hard {0, 1} thresholding on the dissimilarity matrix P: all valuesthat are under a threshold are set to 0, while all values equal or greater to the threshold are set to 1.Let n be the number of words under investigation, then the number of word pairs is p =n?(n?1)2.
Thedegree of sparseness is defined as the number of unordered word pairs (wi, wj), i 6= j where?S(i, j) = 0normalized over the total number of pairs p3.Error Criterion: The algorithm employs a local and a global error criterion defined as follows:1.
The local error corresponds to the projection error for each individual word wie ?
Rn?1, wherei = 1...n and is defined as the sum of the dissimilarity matrix errors before and after projectioncomputed only for the words that are ?connected?
to wi, as follows:ei=n?j=1?S(i, j) ?
(?P(i, j)?P(i, j))2(1)2.
The global error of the projection is simply the sum over local errors for all words: etot=?ni=1eiAlgorithm 1 Sparse projection (SP)Require: v // Vocabulary: vector of n wordsRequire: P // n?n dissimilarity matrix1:?S?
ComputeConnectivityMatrix(S)2: for each word wi?
v do3: Xi?
RandomInitialization(Xi)4: end for5: k = 0 // Iteration counter: initialization6: ektot= inf // Global error: initialization7: repeat8: k = k + 19: for each word wi?
v do10: for each direction z do11: X?MoveWordToDirection(wi, z)12: ezi?
ComputeLocalError(?S,P,X,i)13: end for14: z?i?
FindDirectionOfMinLocalError(ezi)15: X = MoveWordToDirection(wi, z?i)16: end for17: ektot?
UpdateGlobalError(?S,P,X)18: until ek?1tot< ektot// Stopping condition19:?P?
SemanticDistanceReestimation(X)20:?P?
SparseDistanceNormalizedRanges(?P,?S)21: return X // n?d matrix with coordinates;22: return?S // n?n matrix with connections;23: return?P // n?n updated dissimilarity matrix;24: return?P // n?n sparse-normalized distances;Random Walk SP: In function MoveWordToDirection(?)
of Algorithm 1, the pseudo-variable directionz refers to a standard set of perturbations of each word in the d-dimensional space.
For example, if thedimension of the projection is d = 2 then the coordinates of each word are modeled as (k1, k2), wherek1, k2?
R. A potential set of perturbations are the following: (k1, k2+ s), (k1, k2?
s), (k1+ s, k2)and (k1?
s, k2), where s is the perturbation step parameter of the algorithm.
For coordinates systemsnormalized in [0, 1]dwe chose a value of s equal to 0.1.
Good convergence properties to global maximahave been experimentally shown for this algorithm for multiple runs on (noisy) randomly generated data.2Other metrics, e.g., cosine similarity, have also been tested out but results are not shown here due to lack of space.
Euclideandistance performed somewhat better that cosine similarity for the semantic similarity estimation task.3The SP algorithm with 0% degree of sparseness is equivalent to the MDS algorithm.734Sparse Semantic Distance Normalized Ranges: This function normalizes all the distance scores of?Pin a range of values, [0 r1], where r1?
R+is an arbitrary positive constant and also it imposes thesparsity constraint as follows: if?S(i, j) = 0 then?P(i, j) = r1.
If?S(i, j) = 1 then?P(i, j) = r2?
?P(i,j)r3,where r3is the maximum distance over all ?connected?
pairs, i.e.
r3, max{?P?S}, with  denotingthe Hadamard product, and r2?
R+can be either equal to r1or slightly smaller than r1.
The assignmentof r2< r1aims to differentiate the ?unconnected?
pairs from the ?connected?
but dissimilar ones4.5 Low-Dimensional Manifold DSMsThe end-to-end low-dimensional manifold DSM (LDMS) system is depicted in Figure 2.
Note thatv1, v2, ..., v|V|?
V are the domains or sub-manifolds of the LDMS, for each domain via separate DSMis built.
V is the set of domains (concept vocabulary) and |V| denotes to the cardinality of V. The inputFigure 2: LDMS system.to LDMS is a (global) similarity matrix S ?
Rn?n, where n is the total number of tokens (words) inthe LDMS model.
Note that S can be estimated using any of the baseline semantic similarity metrics5presented in Section 2.
Since the SP algorithm uses as input a dissimilarity or semantic distance matrix,the pairwise word similarity matrix S ?
Rn?nis transformed to a semantic distance (or dissimilarity)matrix P ?
Rn?nas: P(i, j) = c1?
e?c2?S(i,j)where c1, c2?
R are constants and the i, j indexes runfrom 1 to n. In this work, we used c1=c2=20.
The transformation defined by (5) was selected in orderto non-linearly scale and increase the relative distance of dissimilar words compared to similar ones6.The steps followed by the LDMS system are the following:1.
Domain Selection: The domains v1, v2, ..., v|V|are created as follows: for each word wiin ourmodel we create a corresponding domain vithat consists of all the words that are semanticallysimilar to wi, i.e., the ith domain is the semantic neighborhood of word wi.
Thus in our modelthe vocabulary size is equal to the domain set cardinality, i.e., n = |V|.
Domain viis created byselecting the top N most semantically similar words to wibased on the (global) similarity matrixS ?
Rn?n.
We have experimented with various domain sizes N ranging between 20 and 200neighbors; note that each word in the LDMS may belong to multiple domains.2.
Sparse Projections on Domains: Following the selection of domain vi?
V the (local) dissimilaritymatrix for each domain Pvi?
RN?Nis defined as a submatrix of P ?
Rn?n.
Then, the SPalgorithm is applied to each domain separately, resulting in i = 1, .., |V| re-estimated boundedsemantic distance matrices?Pvi.3.
Fusion: To reach a decision on the strength of the semantic relation between words wiand wjthesemantic distance matrices from each domain?Pvimust be combined.
Only domains were bothwords wiand wjappear are relevant in this fusion process.
This procedure is described next.4We experimented with various values for r1and r2achieving comparable performance; we selected r2?
0.9r1that hadslightly better performance.
The value of r1can be chosen arbitrary, the results reported here were obtained for r1= 20 andr2= 18.5Here, the Google-based Semantic Relatedness was employed using a corpus of web-harvested document snippets.6Similar nonlinear scaling function from similarity to distance can be found in the literature, e.g., (Borg, 2005)7355.1 FusionMotivation: Given a set of words L = {w1, w2, ...wn} we assume that their corresponding set of wordsenses7is M = {s11, s12, .., s1n1, .., .., sn1, sn2, .., snnn}.
The set of senses is defined as M = ?ni=1Mi,where Mi= {si1, si2, ..., sini} is the set of senses for word wi.
Let S(.)
be a metric of semantic similar-ity, e.g., the metric defined in Section 2, which is symmetric, i.e., S(x, y) ?
S(y, x).
The notations Sw(.
)and Ss(.)
are used in order to distinguish the similarity at word and sense level, respectively.
According tothe maximum sense similarity assumption (Resnik, 1995), the similarity between wiand wj, Sw(wi, wj),is defined as the pairwise maximum similarity between their corresponding senses Ss(sik, sjl):Sw(wi, wj) ?
Ss(sik, sjl), where (k, l) = argmax(p?Mi,r?Mj)Ss(sip, sjr).Note that the maximum pairwise similarity metric (or equivalently the minimum pairwise distancemetric) is also known as the ?common sense?
set similarity (or distance) employed by human cognitionwhen evaluating the similarity (or distance) between two sets.Fusion of local dissimilarity scores: Next we describe a domain fusion model that follows the min-imum pairwise distance (dissimilarity) principle motivated by human cognition.
The steps for the re-computation of the (global) dissimilarity between words wiand wjare:1.
Search for all the domains where wiand wjco-exist.2.
Let U ?
V be the subset of domains from the previous step.
The distances between words wiandwjare retrieved from domain dissimilarity matrices?Pufor all u ?
U .
The distances are stored intovector d ?
R|U |?1.3.
Motivated by the maximum sense similarity assumption (see above) the dissimilarity between wiand wjis defined as8:?P(i, j) = mink=1..|U |{dk} (2)4.
If words wiand wjdo not co-exist in any domain then r1is assigned as their dissimilarity score,where r1is the upper bound of?Pumatrices as defined in the previous section.For example, let one pair of words (w1, w2) co-exists in |U | = 3 different domains with correspondinglocal distances d = [9 20 11] then the global distance of (w1, w2) is 9.6 EvaluationIn this section, we evaluate the performance of the proposed approach with respect to the task of simi-larity judgment between nouns.
Results are reported with respect to several domain/neighborhood sizes,sparse percentages and domain dimensions.The performance of similarity metrics were evaluated against human ratings from three standarddatasets of noun pairs, namely WS353 (Finkelstein et al., 2001), RG (Rubenstein and Goodenough,1965) MC (Miller and Charles, 1991).
The first and the second datasets consist of the subset of 272 and57 pairs, respectively, that are also included in SemCor39corpus, while the third dataset consists of 28noun pairs.
The Pearson?s correlation coefficient was selected as evaluation metric to compare estimatedsimilarities against the ground truth.The similarity matrix computed using the Google-based Semantic Relatedness (Gracia et al., 2006)was used as baseline, as well as to bootstrap the LDMS global similarity matrix S, for a list of 8752 nounsextracted from the SemCor3 corpus10.
The performance of the proposed LDMS approach is presentedin Table 1.
In addition, the performance of other unsupervised similarity estimation algorithms arereported for comparison purposes: 1) SEMNET is an alternative implementation of unstructured DSMsbased on the idea of semantic neighborhoods and networks (Iosif and Potamianos, 2013) 2) WikiRelate!includes various taxonomy-based metrics that are typically applied to the WordNet hierarchy; the basic7This is a simplification.
In reality, some of the word senses will be the same, so strictly speaking this is not a set definition.8Other fusion methods have also been evaluated, e.g., (weighted) average.
Results are omitted here due to lack of space.Minimum pairwise distance fusion outperformed other fusion schemes.9http://www.cse.unt.edu/?rada/downloads.html10The baseline similarity matrix and the 8752 nouns are public available in:http://www.telecom.tuc.gr/?iosife/downloads.html736idea behind WikiRelate!
is to adapt these metrics to a hierarchy extracted from the links between thepages of the English Wikipedia (Strube and Ponzetto, 2006) .
3) TypeDM is a structured DSM (Baroniand Lenci, 2010), 4) AAHKPS1 constitutes an unstructured paradigm of DSM development using fourbillion web documents that were acquired via crawling (Agirre et al., 2009), 5) Moreover, two well-established dimensionality reduction algorithms (Isomap and LLE) that support the manifold hypothesis,were applied to the task of semantic similarity computation11.
LDMS, Isomap and LLE were given asinput the matrix P ?
Rn?n, where n = 8752 is the number of words in our models.
Isomap and LLEused dimensionality reduction down to d = 5 and neighborhood size equal to N = 120.
SEMNET wasrun for neighborhood size equal to N = 100.
While LDMS run for dimensionality down to d = 5,domain/neighborhood size equal to N = 140 and degree of sparseness 90%.
The proposed LDMSsystem surpassed the performance of the baseline system for all three datasets, as well as the performanceof the other corpus-based approaches for the WS353 and MC datasets.
The dimensionality reductionalgorithms (Isomap - LLE) are shown to perform poorly for this particular task.Datasets AlgorithmBaseline SEMNET WikiRelate!
TypeDM AAHKPS1 Isomap LLE LDMSWS353 0.61 0.64 0.48 - - 0.14 0.04 0.69RG 0.81 0.87 0.53 0.82 - 0.04 0 0.86MC 0.85 0.91 0.45 - 0.89 -0.04 -0.04 0.94Table 1: Performance of various algorithms for the task of similarity judgment.The performance (Pearson correlation) of the LDMS approach is shown in Figures 3a, 3b and 4a asa function of neighborhood size and degree of sparseness.
Results are presented for all three datasets:WS353, MC, and RG.
The baseline performance is also plotted (dotted line).
For all three datasets,we see a clear relationship between neighborhood size, degree of sparseness and performance.
Sparserepresentations achieve peak performance for larger neighborhood sizes.
High degree of sparsenessbetween 80 and 90% achieves the best results for domain/neighborhood sizes between 100 and 140.
Thefigures show that there is potential for even better performance by fine-tuning the LDMS parameters.The performance of LDMS is shown in Figure 4b as a function of the projection dimension d. The de-gree of sparseness is fixed at 80% and the domain/neighborhood size is equal to 100 for all experiments.It is observed that the performance for all three datasets remains relatively constant when at least d = 3is used.
In fact results are slightly better for d = 3 than for higher dimensions but the differences inperformance are not significant.
The results suggest that even a 3D sub-space is adequate for accuratelyrepresenting the semantics of each underlying domain.20 40 60 80 100 120 140 160 180 2000.30.40.50.60.70.80.910.705Neighborhood sizeCorrelationBaseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse20 40 60 80 100 120 140 160 180 20000.10.20.30.40.50.60.70.80.91 0.947Neighborhood sizeCorrelationBaseline95% Sparse90% Sparse80% Sparse40% Sparse0% SparseFigure 3: Performance as a function of domain size N and sparseness percentage for the (a) WS353dataset and (b) MC dataset.11LDMS is not directly comparable with Isomap-LLE algorithms because it represents only the domains in low-dimensionalspaces and not the whole dataset.73720 40 60 80 100 120 140 160 180 20000.10.20.30.40.50.60.70.80.910.865Neighborhood sizeCorrelationBaseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse2 3 4 5 6 7 80.60.650.70.750.80.850.90.951DimensionsCorrelationMCRGWS353Figure 4: Performance for the (a) RG dataset as a function of domain size N and sparseness percentageand (b) WS353, MC, RG datasets as a function of projection dimension d.7 ConclusionsIn this work, we proposed a novel, hierarchical DSM that was applied to semantic relation estimationtask obtaining very good results.
The proposed representation consists of low-dimensional manifoldsthat are derived from sparse projections of semantic neighborhoods.
The core idea of low dimensionalsubspaces was motivated by cognitive models of conceptual spaces.
The validity of this motivation wasexperimentally verified via the estimation of semantic similarity between nouns.
The proposed approachwas found to be (at least) competitive with other state-of-the-art DSM approaches that adopt flat featurerepresentations and do not explicitly include the sparsity and dimensionality as a key design parameter.The poor performance of Isomap and LLE can be attributed to the nature of the specific application,i.e., word semantics.
A key characteristic of this application is the ambiguity of word senses.
Thesealgorithms assume only one sense for each word (i.e., a word is represented as a single point in a high-dimensional space).
Although the disambiguation task is not explicitly addressed, LDMS approachhandles the ambiguity of words by isolating each word?s senses in different domains.Our initial intuition regarding the semantic fragmentation of lexical neighborhoods due to singularitiesintroduced by word senses was supported by the high performance when large (i.e., 80% - 90%) degree ofsparseness was imposed.
The hypothesis of low-dimensional representation was validated by the findingthat as little as three dimensions are adequate for representing domain/neighborhood semantics.
It wasalso observed that the parameters of the LDMS model, i.e., number of dimensions, neighborhoodsizeand degree of sparseness, are interrelated: very sparse projections achieve best results with very lowdimensionality when large neighborhood sizes are used.This is only a first step toward using ensembles of low-dimensional DSMs for semantic relation esti-mation.
As future work we plan to further investigate the creation of domains based on more complexgeometric properties of the underlying space (Kreyszig, 2007).
A more formal investigation of the re-lation between sparseness, dimensionality and performance is also needed.
Finally, creating multi-levelhierarchical representations that are consistent with cognitive organization is an important challenge thatcan further improve manifold DSM performance.AcknowledgmentsThis work has been partially funded by two projects supported by the EU Seventh Framework Pro-gramme (FP7): 1) PortDial, grant number 296170 and 2) SpeDial, grant number 611396.738ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa.
2009.
A study on similarity and relatednessusing distributional and wordnet-based approaches.
In Proceedings of Human Language Technologies, pages19?27.
Association for Computational Linguistics.R.
G Baraniuk and M. B Wakin.
2009.
Random projections of smooth manifolds.
Foundations of computationalmathematics, 9(1):51?77.M.
Baroni and A. Lenci.
2010.
Distributional memory: A general framework for corpus-based semantics.
Com-putational Linguistics, 36(4):673?721.Y.
Bengio, A. Courville, and P. Vincent.
2013.
Representation learning: A review and new perspectives.D.
Bollegala, Y. Matsuo, and M. Ishizuka.
2007.
Measuring semantic similarity between words using web searchengines.
In Proc.
of International Conference on World Wide Web, pages 757?766, Banff, Alberta, Canada.Ingwer Borg.
2005.
Modern multidimensional scaling: Theory and applications.
Springer.A.
Budanitsky and G. Hirst.
2001.
Semantic distance in wordnet: An experimental, application-oriented evalua-tion of five measures.
In Workshop on WordNet and Other Lexical Resources.O.
Corby, R. Dieng-Kuntz, F. Gandon, and C. Faron-Zucker.
2006.
Searching the semantic web: Approximatequery processing based on ontologies.
Intelligent Systems, IEEE, 21(1):20?27.D.
L Donoho and C. Grimes.
2003.
Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data.
Proceedings of the National Academy of Sciences, 100(10):5591?5596.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.
2001.
Placing search incontext: The concept revisited.
In Proceedings of the 10th international conference on World Wide Web, pages406?414.
ACM.P.
Gardenfors.
2000.
Conceptual spaces: The geometry of thought.
Cambridge, Massachusetts: USA.
ISBN,262071991.J.
Gracia, R. Trillo, M. Espinoza, and E. Mena.
2006.
Querying the web: A multiontology disambiguation method.In Proc.
of International Conference on Web Engineering, pages 241?248, Palo Alto, California, USA.G.
Grefenstette.
1994.
Explorations in Automatic Thesaurus Discovery.
Kluwer Academic Publishers, Norwell,MA, USA.Z.
Harris.
1954.
Distributional structure.
Word, 10(23):146?162.E.
Iosif and A. Potamianos.
2010.
Unsupervised semantic similarity computation between terms using webdocuments.
Knowledge and Data Engineering, IEEE Transactions on, 22(11):1637?1647.E.
Iosif and A. Potamianos.
2013.
Similarity computation using semantic networks created from web-harvesteddata.
Natural Language Engineering (DOI: 10.1017/S1351324913000144).I.
Jolliffe.
2005.
Principal component analysis.
Wiley Online Library.J.
Karlgren, A. Holst, and M. Sahlgren.
2008.
Filaments of meaning in word space.
In Advances in InformationRetrieval, pages 531?538.
Springer.E.
Kreyszig.
2007.
Introductory functional analysis with applications.
Wiley.
com.P.
Li, T. J Hastie, and K. W Church.
2006.
Very sparse random projections.
In Proceedings of the 12th ACMSIGKDD international conference on Knowledge discovery and data mining, pages 287?296.
ACM.N.
Malandrakis, A. Potamianos, E. Iosif, and S. S Narayanan.
2011.
Kernel models for affective lexicon creation.In INTERSPEECH, pages 2977?2980.G.
A Miller and W. G Charles.
1991.
Contextual correlates of semantic similarity.
Language and cognitiveprocesses, 6(1):1?28.P.
Resnik.
1995.
Using information content to evaluate semantic similarity in a taxanomy.
In Proc.
of InternationalJoint Conference for Artificial Intelligence, pages 448?453.739P.
Resnik.
2011.
Semantic similarity in a taxonomy: An information-based measure and its application to prob-lems of ambiguity in natural language.
arXiv preprint arXiv:1105.5444.S.
T Roweis and L. K Saul.
2000.
Nonlinear dimensionality reduction by locally linear embedding.
Science,290(5500):2323?2326.H.
Rubenstein and J.
B Goodenough.
1965.
Contextual correlates of synonymy.
Communications of the ACM,8(10):627?633.Michael Strube and Simone Paolo Ponzetto.
2006.
Wikirelate!
computing semantic relatedness using wikipedia.In AAAI, pages 1419?1424.J.
B Tenenbaum, V. De Silva, and J.
C Langford.
2000.
A global geometric framework for nonlinear dimensional-ity reduction.
Science, 290(5500):2319?2323.J.
B Tenenbaum, C. Kemp, T. L Griffiths, and N. D Goodman.
2011.
How to grow a mind: Statistics, structure,and abstraction.
science, 331(6022):1279?1285.Warren S Torgerson.
1952.
Multidimensional scaling: I. theory and method.
Psychometrika, 17(4):401?419.S.
Tsuge, M. Shishibori, S. Kuroiwa, and K. Kita.
2001.
Dimensionality reduction using non-negative matrixfactorization for information retrieval.
In Systems, Man, and Cybernetics, 2001 IEEE International Conferenceon, volume 2, pages 960?965 vol.2.J.
V?eronis.
2004.
Hyperlex: Lexical cartography for information retrieval.
Computer Speech and Language,18(3):223?252.Jianzhong Wang.
2011.
Maximum variance unfolding.
In Geometric Structure of High-Dimensional Data andDimensionality Reduction, pages 181?202.
Springer.J.
Weston, F. Ratle, H. Mobahi, and R. Collobert.
2012.
Deep learning via semi-supervised embedding.
In NeuralNetworks: Tricks of the Trade, pages 639?655.
Springer.K.
Yu, T. Zhang, and Y. Gong.
2009.
Nonlinear learning using local coordinate coding.
In Advances in NeuralInformation Processing Systems, pages 2223?2231.740
