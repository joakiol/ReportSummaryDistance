Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 379?390, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsNatural Language Questions for the Web of DataMohamed Yahya1, Klaus Berberich1, Shady Elbassuoni2Maya Ramanath3, Volker Tresp4, Gerhard Weikum11 Max Planck Institute for Informatics, Germany2 Qatar Computing Research Institute3 Dept.
of CSE, IIT-Delhi, India 4 Siemens AG, Corporate Technology, Munich, Germany{myahya,kberberi,weikum}@mpi-inf.mpg.deselbassuoni@qf.org.qaramanath@cse.iitd.ac.in volker.tresp@siemens.comAbstractThe Linked Data initiative comprises struc-tured databases in the Semantic-Web datamodel RDF.
Exploring this heterogeneousdata by structured query languages is tediousand error-prone even for skilled users.
To easethe task, this paper presents a methodologyfor translating natural language questions intostructured SPARQL queries over linked-datasources.Our method is based on an integer linear pro-gram to solve several disambiguation tasksjointly: the segmentation of questions intophrases; the mapping of phrases to semanticentities, classes, and relations; and the con-struction of SPARQL triple patterns.
Our so-lution harnesses the rich type system providedby knowledge bases in the web of linked data,to constrain our semantic-coherence objectivefunction.
We present experiments on both thequestion translation and the resulting queryanswering.1 Introduction1.1 MotivationRecently, very large, structured, and semanticallyrich knowledge bases have become available.
Ex-amples are Yago (Suchanek et al2007), DBpe-dia (Auer et al2007), and Freebase (Bollacker etal., 2008).
DBpedia forms the nucleus of the Web ofLinked Data (Heath and Bizer, 2011), which inter-connects hundreds of RDF data sources with a totalof 30 billion subject-property-object (SPO) triples.The diversity of linked-data sources and their highheterogeneity make it difficult for humans to searchand discover relevant information.
As linked datais in RDF format, the standard approach would beto run structured queries in triple-pattern-based lan-guages like SPARQL, but only expert programmersare able to precisely specify their information needsand cope with the high heterogeneity of the data(and absence or very high complexity of schema in-formation).
For less initiated users the only optionto query this rich data is by keyword search (e.g.,via services like sig.ma (Tummarello et al2010)).None of these approaches is satisfactory.
Instead, theby far most convenient approach would be to searchin knowledge bases and the Web of linked data bymeans of natural-language questions.As an example, consider a quiz question like?Which female actor played in Casablanca and ismarried to a writer who was born in Rome?
?.The answer could be found by querying sev-eral linked data sources together, like the IMDB-style LinkedMDB movie database and the DB-pedia knowledge base, exploiting that there areentity-level sameAs links between these collections.One can think of different formulations of theexample question, such as ?Which actress fromCasablanca is married to a writer from Rome??.
Apossible SPARQL formulation, assuming a user fa-miliar with the schema of the underlying knowl-edge base(s), could consist of the following sixtriple patterns (joined by shared-variable bind-ings): ?x hasGender female, ?x isa actor, ?xactedIn Casablanca (film), ?x marriedTo ?w,?w isa writer, ?w bornIn Rome.
This complexquery, which involves multiple joins, would yieldgood results, but it is difficult for the user to come379up with the precise choices for relations, classes, andentities.
This would require familiarity with the con-tents of the knowledge base, which no average useris expected to have.
Our goal is to automatically cre-ate such structured queries by mapping the user?squestion into this representation.
Keyword search isusually not a viable alternative when the informationneed involves joining multiple triples to constructthe final result, notwithstanding good attempts likethat of Pound et al2010).
In the example, the obvi-ous keyword query ?female actress Casablanca mar-ried writer born Rome?
lacks a clear specification ofthe relations among the different entities.1.2 ProblemGiven a natural language question qNL and a knowl-edge base KB, our goal is to translate qNL into aformal query qFL that captures the information needexpressed by qNL.We focus on input questions that put the em-phasis on entities, classes, and relations betweenthem.
We do not consider aggregations (counting,max/min, etc.)
and negations.
As a result, we gener-ate structured queries of the form known as conjunc-tive queries or select-project-join queries in databaseterminology.
Our target language is SPARQL 1.0,where the above focus leads to queries that consist ofmultiple triple patterns, that is, conjunctions of SPOsearch conditions.
We do not use any pre-existingquery templates, but generate queries from scratchas they involve a variable number of joins with a-priori unknown join structure.A major challenge is in the ambiguity ofthe phrases occurring in a natural-language ques-tion.
Phrases can denote entities (e.g., the cityof Casablanca or the movie Casablanca), classes(e.g., actresses, movies, married people), or rela-tions/properties (e.g., marriedTo between people,played between people and movies).
A priori, we donot know if a phrase should be mapped to an entity,a class, or a relation.
In fact, some phrases may de-note any of these three kinds of targets.
For example,a phrase like ?wrote score for?
in a question aboutfilm music composers, could map to the composer-film relation wroteSoundtrackForFilm, to the classof movieSoundtracks (a subclass of music pieces),or to an entity like the movie ?The Score?.
Depend-ing on the choice, we may arrive at a structurallygood query (with triple patterns that can actuallybe joined) or at a meaningless and non-executablequery (with disconnected triple patterns).
This gen-eralized disambiguation problem is much more chal-lenging than the more focused task of named entitydisambiguation (NED).
It is also different from gen-eral word sense disambiguation (WSD), which fo-cuses on the meaning of individual words (e.g., map-ping them to WordNet synsets).1.3 ContributionIn our approach, we introduce new elements towardsmaking translation of questions into SPARQL triplepatterns more expressive and robust.
Most impor-tantly, we solve the disambiguation and mappingtasks jointly, by encoding them into a comprehen-sive integer linear program (ILP): the segmentationof questions into meaningful phrases, the mappingof phrases to semantic entities, classes, and rela-tions, and the construction of SPARQL triple pat-terns.
The ILP harnesses the richness of large knowl-edge bases like Yago2 (Hoffart et al2011b), whichhas information not only about entities and relations,but also about surface names and textual patternsby which web sources refer to them.
For example,Yago2 knows that ?Casablanca?
can refer to the cityor the film, and ?played in?
is a pattern that can de-note the actedIn relation.
In addition, we can lever-age the rich type system of semantic classes.
For ex-ample, knowing that Casablanca is a film, for trans-lating ?played in?
we can focus on relations with atype signature whose range includes films, as op-posed to sports teams, for example.
Such informa-tion is encoded in judiciously designed constraintsfor the ILP.
Although we intensively harness Yago2,our approach does not depend on a specific choice ofknowledge base or language resource for type infor-mation and phrase/name dictionaries.
Other knowl-edge bases such as DBpedia can be easily pluggedin.Based on these ideas, we have developed a frame-work and system, called DEANNA (DEep Answersfor maNy Naturally Asked questions), that com-prises a full suite of components for question de-composition, mapping constituents into the seman-tic concept space, generating alternative candidatemappings, and computing a coherent mapping of allconstituents into a set of SPARQL triple patterns that380can be directly executed on one or more linked datasources.2 BackgroundWe use the Yago2 knowledge base, with its richtype system, as a semantic backbone.
Yago2 is com-posed of instances of binary relations derived fromWikipedia and WordNet.
The instances, called facts,provide both ontological information and instancedata.
Figure 1 shows sample facts from Yago2.
Eachfact is composed of semantic items that can be di-vided into relations, entities, and classes.
Entitiesand classes together are referred to as concepts.Subject Predicate Objectfilm subclassOfproductionCasablanca (film)type film?Casablanca?
means Casablanca (film)?Casablanca?
means Casablanca, MoroccoIngrid Bergman actedIn Casablanca (film)Figure 1: Sample knowledge baseExamples of relations are type, subclassOf, andactedIn.
Each relation has a type signature: classesfor the relation?s domain and range.
Classes, such asperson and film group entities.
Entities are repre-sented in canonical form such as Ingrid Bergmanand Casablanca (film).
A special type of entitiesare literals, such as strings, numbers, and dates.3 FrameworkGiven a natural language question, Figure 2 showsthe tasks DEANNA performs to translate a ques-tion into a structured query.
The first three stepsprepare the input for constructing a disambiguationgraph for mapping the phrases in a question ontoentities, classes, and relations, in a coherent man-ner.
The fourth step formulates this generalized dis-ambiguation problem as an ILP with complex con-straints and computes the best solution using anILP solver.
Finally, the fifth and sixth step togetheruse the disambiguated mapping to construct an exe-cutable SPARQL query.A question sentence is a sequence of tokens,qNL = (t0, t1, ..., tn).
A phrase is a contiguous sub-sequence of tokens (ti, ti+1, ..., ti+l) ?
qNL, 0 ?i, 0 ?
l ?
n. The input question is fed into the fol-lowing pipeline of six steps:1.
Phrase detection.
Phrases are detected thatpotentially correspond to semantic items such as?Who?, ?played in?, ?movie?
and ?Casablanca?.2.
Phrase mapping to semantic items.
This in-cludes finding that the phrase ?played in?
can ei-ther refer to the semantic relation actedIn or toplayedForTeam and that the phrase ?Casablanca?can potentially refer to Casablanca (film) orCasablanca, Morocco.
This step merely constructsa candidate space for the mapping.
The actual dis-ambiguation is addressed by step 4, discussed below.3.
Q-unit generation.
Intuitively, a q-unit is a triplecomposed of phrases.
Their generation and role willbe discussed in detail in the next section.4.
Joint disambiguation, where the ambiguities inthe phrase-to-semantic-item mapping are resolved.This entails resolving the ambiguity in phrase bor-ders, and above all, choosing the best fitting can-didates from the semantic space of entities, classes,and relations.
Here, we determine for our runningexample that ?played in?
refers to the semantic re-lation actedIn and not to playedForTeam and thephrase ?Casablanca?
refers to Casablanca (film)and not Casablanca, Morocco.5.
Semantic items grouping to form semantictriples.
For example, we determine that the relationmarriedTo connects person referred to by ?Who?and writer to form the semantic triple personmarriedTo writer.
This is done via q-units.6.
Query generation.
For SPARQL queries, seman-tic triples such as person marriedTo writer haveto be mapped to suitable triple patterns with appro-priate join conditions expressed through commonvariables: ?x type person, ?x marriedTo ?w, and?w type writer for the example.3.1 Phrase DetectionA detected phrase p is a pair < Toks, l > whereToks is a phrase and l is a label, l ?
{concept, relation}, indicating whether a phrase isa relation phrase or a concept phrase.
Pr is the set ofall detected relation phrases and Pc is the set of alldetected concept phrases.One special type of detected relation phrase isthe null phrase, where no relation is explicitly men-tioned, but can be induced.
The most prominent ex-ample of this is the case of adjectives, such as ?Aus-tralian movie?, where we know there is a relationbeing expressed between ?Australia?
and ?movie?.We use multiple detectors for detecting phrases of381questionphrasesstructuredqueryConcept&RelationPhraseDetectionJointDisambi-guationmappingscandidategraphentities & namesclasses & subclassesrelations & pattternsincl.
dictionaries & statisticsKnowledge BaseConcept&RelationPhraseMapping1 2 3 45SemanticItemsGroupingQueryGene-ration6selecteds-nodestriplepatternsQ-unitGenera-tionFigure 2: Architecture of DEANNA.different types.
For concept detection, we use a de-tector that works against a phrase-concept dictionarywhich looks as follows:{?Rome?,?eternal city?}
?
Rome{?Casablanca?}
?
Casablanca (film)We experimented with using third-party named en-tity recognizers but the results were not satisfactory.This dictionary was mostly constructed as part ofthe knowledge base, independently of the question-to-query translation task in the form of instances ofthe means relation in Yago2, an example of which isshown in Figure 1For relation detection, we experimented with var-ious approaches.
We mainly rely on a relation detec-tor based on ReVerb (Fader et al2011) with addi-tional POS tag patterns, in addition to our own whichlooks for patterns in dependency parses.3.2 Phrase MappingAfter phrases are detected, each phrase is mappedto a set of semantic items.
The mapping of conceptphrases also relies on the phrase-concept dictionary.To map relation phrases, we rely on a corpus oftextual patterns to relation mappings of the form:{?play?,?star in?,?act?,?leading role?}
?
actedIn{?married?, ?spouse?,?wife?}
?
marriedToDistinct phrase occurrences will map to differentsemantic item instances.
We discuss why this is im-portant when we discuss the construction of the dis-ambiguation graph and variable assignment in thestructured query.3.3 Dependency Parsing & Q-Unit GenerationDependency parsing identifies triples of to-kens, or triploids, ?trel, targ1, targ2?, wheretrel, targ1, targ2 ?
qNL are seeds for phrases, withthe triploid acting as a seed for a potential SPARQLtriple pattern.
Here, trel is the seed for the relationphrase, while targ1 and targ2 are seeds for the twoarguments.
At this point, there is no attempt toassign subject/object roles to the arguments.Triploids are collected by looking for specific de-pendency patterns in dependency graphs (de Marn-effe et al2006).
The most prominent pattern welook for is a verb and its arguments.
Other patternsinclude adjectives and their arguments, preposition-ally modified tokens and objects of prepositions.By combining triploids with detected phrases, weobtain q-units.
A q-unit is a triple of sets of phrases,?
{prel ?
Pr}, {parg1 ?
Pc}, {parg2 ?
Pc}?, wheretrel ?
prel and similarly for arg1 and arg2.
Concep-tually, one can view a q-unit as a placeholder nodewith three sets of edges, each connecting the sameq-node to a phrase that corresponds to a relation orconcept phrase in the same q-unit.
This notion ofnodes and edges will be made more concrete whenwe present our disambiguation graph construction.3.4 Disambiguation of Phrase MappingsThe core contribution of this paper is a frameworkfor disambiguating phrases into semantic items ?covering relations, classes, and entities in a unifiedmanner.
This can be seen as a joint task combining382named entity disambiguation for entities, word sensedisambiguation for classes (common nouns), and re-lation extraction.
The next section presents the dis-ambiguation framework in detail.3.5 Query GenerationOnce phrases are mapped to unique semantic items,we proceed to generate queries in two steps.
First,semantic items are grouped into triples.
This is doneusing the triploids generated earlier.
The power ofusing a knowledge base is that we have a rich typesystem that allows us to tell if two semantic itemsare compatible or not.
Each relation has a type sig-nature and we check whether the candidate items arecompatible with the signature.We did not assign subject/object roles in triploidsand q-units because a natural language relationphrase might express the inverse of a semantic rela-tion, e.g., the natural language expression ?directedby?
and the relation isDirectorOf with respect tothe movies domain are inverses of each other.
There-fore, we check which assignment of arg1 and arg2is compatible with the semantic relation.
If both ar-rangements are compatible, then we give preferenceto the assignment given by the dependency parsers.Once semantic items are grouped into triples, itis an easy task to expand them to SPARQL triplepatterns.
This is done by replacing each seman-tic class with a distinct type-constrained variable.Note that this is the reason why each distinct phrasemaps to a distinct instance of a semantic class, toensure correct variable assignment.
This becomesclear when we consider the question ?Which singeris married to a singer?
?, which requires two distinctvariables each constrained to bind to an entity oftype singer.4 Joint DisambiguationThe goal of the disambiguation step is to computea partial mapping of phrases onto semantic items,such that each phrase is assigned to at most onesemantic item.
This step also resolves the phrase-boundary ambiguity, by enforcing that only non-overlapping phrases are mapped.
As the result ofdisambiguating one phrase can influence the map-ping of other phrases, we consider all phrases jointlyin one big disambiguation task.In the following, we construct a disambiguationgraph that encodes all possible mappings.
We im-pose a variety of complex constraints (mutual ex-clusion among overlapping phrases, type constraintsamong the selected semantic items, etc.
), and definean objective function that aims to maximize the jointquality of the mapping.
The graph construction it-self may resemble similar models used in NED (e.g.,(Milne and Witten, 2008; Kulkarni et al2009; Hof-fart et al2011a)).
Recall, however, that our task ismore complex because we jointly consider entities,classes, and relations in the candidate space of pos-sible mappings.
Because of this complication andto capture our complex constraints, we do not em-ploy graph algorithms, but model the general disam-biguation problem as an ILP.4.1 Disambiguation GraphJoint disambiguation takes place over a disambigua-tion graph DG = (V,E), where V = Vs ?
Vp ?
Vqand E = Esim ?
Ecoh ?
Eq, where:?
Vs is the set of semantic items, vs ?
Vs is ans-node.?
Vp is the set of phrases, vp ?
Vp is called a p-node.
We denote the set of p-nodes correspond-ing to relation phrases by Vrp and the set of p-nodes corresponding to concept phrases by Vrc .?
Vq is a set of placeholder nodes for q?units,called q-nodes.
They represent phrase triples.?
Esim ?
Vp ?
Vs is a set of weighted similarityedges that capture the strength of the mappingof a phrase to a semantic item.?
Ecoh ?
Vs ?
Vs is a set of weighted coherenceedges that capture the semantic coherence be-tween two semantic items.
Semantic coherenceis discussed in more detail later in this section.?
Eq ?
Vq?Vp?d, where d ?
{rel, arg1, arg2}is a q-edge.
Each such edge connects a place-holder q-node to a p-node with a specific roleas a relation, or one of the two arguments.
Aq-unit, as presented earlier, can be seen as a q-node along with its outgoing q-edges.Figure 3 shows the disambiguation graph for ourrunning example (excluding coherence edges be-tween s-nodes).4.2 Edge WeightsWe next describe how the weights on similarityedges and semantic coherence edges are defined.383q1q2q3a writerCasablancaplayedplayed inWhomarriedmarried tois married towas bornbornRomec:writerr:bornInr:bornOnDatee:Max_Borne:Born_(film)e:Sydne_Romer:Romee:White_Housee:Casablancae:Casablanca_(film)e:Played_(film)r:actedInr:hasMusicalRolec:persone:Married_(series)c: married_personr:marriedToq-nodes p-nodess-nodesarg1 arg2relFigure 3: Disambiguation graph for the running example.4.2.1 Semantic CoherenceSemantic coherence, Cohsem, captures to whatextent two semantic items occur in the same context.This is different from semantic similarity (Simsem),which is usually evaluated using the distance be-tween nodes in a taxonomy (Resnik, 1995).
Whilewe expect Simsem(George Bush, Woody Allen) tobe higher than Simsem(Woody Allen, Terminator)we would like Cohsem(Woody Allen, Terminator),both of which are from the entertainment domain, tobe higher than Cohsem(George Bush, Woody Allen).For Yago2, we characterize an entity e by its in-links InLinks(e): the set of Yago2 entities whosecorresponding Wikipedia pages link to the entity.To be able to compare semantic items of differentsemantic types (entities, relations, and classes), weneed to extend this to classes and relations.
For classc with entities e, its inlinks are defined as follows:InLinks(c) =?e?c Inlinks(e)For relations, we only consider those that map en-tities to entities (e.g.
actedIn, produced), for whichwe define the set of inlinks as follows:InLinks(r) =?
(e1,e2)?r(InLinks(e1) ?
InLinks(e2))The intuition behind this is that when the two argu-ments of an instance of the relation co-occur, thenthe relation is being expressed.We define the semantic coherence (Cohsem) be-tween two semantic items s1 and s2 as the Jaccardcoefficient of their sets of inlinks.4.2.2 Similarity WeightsSimilarity weights are computed differently forentities, classes, and relations.
For entities, we use anormalized prior score based on how often a phraserefers to a certain entity in Wikipedia.
For classes,we use a normalized prior that reflects the numberof members in a class.
Finally, for relations, similar-ity reflects the maximum n-gram similarity betweenthe phrase and any of the relation?s surface forms.We use Lucene for indexing and searching the rela-tion surface forms.4.3 Disambiguation Graph ProcessingThe result of disambiguation is a subgraph of thedisambiguation graph, yielding the most coherentmappings.
We employ an ILP to this end.
Beforedescribing our ILP, we state some necessary defini-tions:?
Triple dimensions: d ?
{rel, arg1, arg2}?
Tokens: T = {t0, t1, ..., tn}.?
Phrases: P = {p0, p1, ..., pk}.?
Semantic items: S = {s0, s1, ..., sl}.?
Token occurrences: P(t) = {p ?
P | t ?
p}.?
Xi ?
{0, 1} indicates if p-node i is selected.?
Yij ?
{0, 1} indicates if p-node i maps to s-node j.?
Zkl ?
{0, 1} indicates if s-nodes k, l are bothselected so that their coherence edge matters.?
Qmnd ?
{0, 1} indicates if the q-edge betweenq-node m and p-node n for d is selected.?
Cj , Ej and Rj are {0, 1} constants indicatingif s-node j is a class, entity, or relation, resp.?
wij is the weight for a p?s similarity edge.?
vkl is the weight for an s?s semantic coherenceedge.?
trc ?
{0, 1} indicates if the relation s-node r istype-compatible with the concept s-node c.Given the above definitions, our objective func-tion ismaximize ?
?i,j wijYij + ?
?k,l vklZkl+?
?m,n,dQmnd384subject to the following constraints:1.
A p-node can be assigned to one s-node at most:?j Yij ?
1, ?i2.
If a p-s similarity edge is chosen, then the respec-tive p-node must be chosen:Yij ?
Xi, ?j3.
If s-nodes k and l are chosen (Zkl = 1), then thereare p-nodes mapping to each of the s-nodes k and l( Yik = 1 for some i and Yjl = 1 for some j):Zkl ?
?i Yik and Zkl ?
?j Yjl4.
No token can appear as part of two phrases:?i?P(t)Xi ?
1, ?t ?
T5.
At most one q-edge is selected for a dimension:?nQmnd ?
1, ?m, d6.
If the q-edge mnd is chosen (Qmnd = 1) thenp-node n must be selected:Qmnd ?
Xn, ?m, d7.
Each semantic triple should include a relation:Er ?
Qmn?d +Xn?
+ Yn?r ?
2 ?m,n?, r, d = rel8.
Each triple should have at least one class:Cc1 + Cc2 ?
Qmn?
?d1 +Xn??
+ Yn???c1+Qmn??
?d2 +Xn???
+ Yn??
?c2 ?
5,?m,n?
?, n??
?, r, c1, c2, d1 = arg1, d2 = arg2This is not invoked for existential questions thatreturn Boolean answers and are translated to ASKqueries in SPARQL.
An example is the question?Did Tom Cruise act in Top Gun?
?, which can betranslated to ASK {Tom Cruise actedIn Top Gun}.9.
Type constraints are respected (through q-edges):trc1 + trc2 ?
Qmn?d1 +Xn?
+ Yn?r+Qmn?
?d2 +Xn??
+ Yn???c1+Qmn??
?d3 +Xn???
+ Yn??
?c2 ?
7?m,n?, n?
?, n??
?, r, c1, c2,d1 = rel, d2 = arg1, d3 = arg2The above is a sophisticated ILP, and most likelyNP-hard.
However, even with ten thousands of vari-ables it is within the regime of modern ILP solvers.In our experiments, we used Gurobi (Gur, 2011), andachieved run-times ?
typically of a few seconds.q1q2q3a writerCasablancaplayed inWhois married towas bornRomec:writerr:bornInr:Romee:Casablancar:actedInc:personr:marriedToq-nodes p-nodess-nodesFigure 4: Computed subgraph for the running example.Figure 4 shows the resulting subgraph for the dis-ambiguation graph of Figure 3.
Note how commonp-nodes between q-units capture joins.5 Evaluation5.1 DatasetsOur experiments are based on two collections ofquestions: the QALD-1 task for question answer-ing over linked data (QAL, 2011) and a collectionof questions used in (Elbassuoni et al2011; El-bassuoni et al2009) in the context of the NAGAproject, for informative ranking of SPARQL queryanswers (Elbassuoni et al2009) evaluated theSPARQL queries, but the underlying questions areformulated in natural language.)
The NAGA collec-tion is based on linking data from IMDB with theYago2 knowledge base.
This is an interesting linked-data case: IMDB provides data about movies, actors,directors, and movie plots (in the form of descrip-tive keywords and phrases); Yago2 adds semantictypes and relational facts for the participating enti-ties.
Yago2 provides nearly 3 million concepts and100 relations, of which 41 lie within the scope ofour framework.Typical example questions for these two col-lections are: ?Which software has been publishedby Mean Hamster Software??
for QALD-1, and?Which director has won the Academy Award forBest director and is married to an actress that haswon the Academy Award for Best Actress??
forNAGA.
For both collections, some questions areout-of-scope for our setting, because they mentionentities or relations that are not available in the un-derlying datasets, contain date or time comparisons,or involve aggregation such as counting.
After re-385moving these questions, our test set consists of 27QALD-1 training questions out of a total of 50 and44 NAGA questions, out of a total of 87.
We usedthe 19 questions from the QALD-1 test set that arewithin the scope of our method for tuning the hyper-parameters (?, ?, ?)
in the ILP objective function.5.2 Evaluation MetricsWe evaluated the output of DEANNA at threestages in the processing pipeline: a) after the dis-ambiguation of phrases, b) after the generation ofthe SPARQL query, and c) after obtaining answersfrom the underlying linked-data sources.
This way,we could obtain insights into our building blocks,in addition to assessing the end-to-end performance.In particular, we could assess the goodness of thequestion-to-query translation independently of theactual answer quality which may depend on partic-ularities of the underlying datasets (e.g., slight mis-matches between query terminology and the namesin the data.
)At each of the three stages, the output was shownto two human assessors who judged whether an out-put item was good or not.
If the two were in dis-agreement, then a third person resolved the judg-ment.For the disambiguation stage, the judges lookedat each q-node/s-node pair, in the context of thequestion and the underlying data schemas, and de-termined whether the mapping was correct or notand whether any expected mappings were missing.For the query-generation stage, the judges lookedat each triple pattern and determined whether thepattern was meaningful for the question or not andwhether any expected triple pattern was missing.Note that, because our approach does not use anyquery templates, the same question may generate se-mantically equivalent queries that differ widely interms of their structure.
Hence, we rely on our eval-uation metrics that are based on triple patterns, asthere is no gold-standard query for a given ques-tion.
For the query-answering stage, the judges wereasked to identify if the result sets for the generatedqueries are satisfactory.With these assessments, we computed over-all quality measures by both micro-averaging andmacro-averaging.
Micro-averaging aggregates overall assessed items (e.g., q-node/s-node pairs or triplepatterns) regardless of the questions to which theybelong.
Macro-averaging first aggregates the itemsfor the same question, and then averages the qualitymeasure over all questions.For a question q and item set s in one of the stagesof evaluation, let correct(q, s) be the number of cor-rect items in s, ideal(q) be the size of the ideal itemset and retrieved(q, s) be the number of retrieveditems, we define coverage and precision as follows:cov(q, s) = correct(q, s)/ideal(q)prec(q, s) = correct(q, s)/retrieved(q, s).5.3 Results & Discussion5.3.1 DisambiguationTable 1 shows the results for disambiguation interms of macro and micro coverage and precision.For both datasets, coverage is high as few mappingsare missing.
We obtain perfect precision for QALD-1 as no mapping that we generate is incorrect, whilefor NAGA we generate few incorrect mappings.5.3.2 Query GenerationTable 2 shows the same metrics for the generatedtriple patterns.
The results are similar to those fordisambiguation.
Missing or incorrect triple patternscan be attributed to (i) incorrect mappings in the dis-ambiguation stage or (ii) incorrect detection of de-pendencies between phrases despite having the cor-rect mappings.5.3.3 Question AnsweringTable 3 shows the results for query answering.Here, we attempt to generate answers to questionsby executing the generated queries over the datasets.The table shows the number of questions for whichthe system successfully generated SPARQL queries(#queries), and among those, how many resultedin satisfactory answers as judged by our evalua-tors (#satisfactory).
Answers were considered un-satisfactory when: 1) the generated SPARQL querywas wrong, 2) the result set was empty due to theincompleteness of the underlying knowledge base,or 3) a small fraction of the result set was relevantto the question.
For both sets of questions, most ofthe queries that were perceived unsatisfactory wereones that returned no answers.
Table 4 shows aset of example QALD questions, the correspondingSPARQL queries and sample answers.386Benchmark QALD-1 NAGAcovmacro 0.973 0.934precmacro 1.000 0.934covmicro 0.963 0.945precmicro 1.000 0.941Table 1: DisambiguationBenchmark QALD-1 NAGAcovmacro 0.975 0.894precmacro 1.000 0.941covmicro 0.956 0.847precmicro 1.000 0.906Table 2: Query generationBenchmark QALD-1 NAGA#questions 27 44#queries 20 41#satisfactory 10 15#relaxed +3 +3Table 3: Query answeringQuestion Generated Query Sample Answers1.
Who was the wife of PresidentLincoln?
?x marriedTo Abraham Lincoln .
?x type personMary Todd Lincoln2.
In which films did Julia Robertsas well as Richard Gere play?
?x type movie .
Richard Gere actedIn ?x .Julia Roberts actedIn ?xRunaway BridePretty Woman3.
Which actors were born inGermany?
?x type actor .
?x bornIn Germany NONETable 4: Example questions, the generated SPARQL queries and their answersQueries that produced no answers, such as thethird query in Table 4 were further relaxed using anincarnation of the techniques described in (Elbas-suoni et al2009), by retaining the triple patternsexpressing type constraints and relaxing all othertriple patterns.
Relaxing a triple pattern was doneby replacing all entities with variables and castingentity mentions into keywords that are attached tothe relaxed triple pattern.
For example, the QALDquestion ?Which actors were born in Germany?
?was translated into the following SPARQL query:?x type actor .
?x bornIn Germany which pro-duced no answers when run over the Yago2 knowl-edge base since the relation bornIn relates peo-ple to cities and not countries in Yago2.
Thequery was then relaxed into: ?x type actor .
?xbornIn ?z[Germany].
This relaxed (and keyword-augmented) triple-pattern query was then processedthe same way as triple-pattern queries without anykeywords.
The results of such query were thenranked based on how well they match the keywordconditions specified in the relaxed query using theranking model in (Elbassuoni et al2009).
Usingthis technique, the top ranked results for the relaxedquery were all actors born in German cities as shownin Table 5.After relaxation, the judges again assessed the re-sults of the relaxed queries and determined whetherthey were satisfactory or not.
The number of addi-tional queries that obtained satisfactory answers af-ter relaxation are shown under #relaxed in Table 3.The evaluation data, in addition to a demonstra-tion of our system (Yahya et al2012), can be foundat http://mpi-inf.mpg.de/yago-naga/deanna/.6 Related WorkQuestion answering has a long history in NLP andIR research.
The Web and Wikipedia have proved tobe a valuable resource for answering fact-orientedquestions.
State-of-the-art methods (Hirschman andGaizauskas, 2001; Kwok et al2001; Zheng, 2002;Katz et al2007; Dang et al2007; Voorhees, 2003)cast the user?s question into a keyword query to aWeb search engine (perhaps with phrases for loca-tion and person names or other proper nouns).
Keyto finding good results is to retrieve and rank sen-tences or short passages that contain all or most key-words and are likely to yield good answers.
Togetherwith trained classifiers for the question type (andthus the desired answer type), this methodology per-forms fairly well for both factoid and list questions.IBM?s Watson project (Ferrucci et al2010)demonstrated a new kind of deep QA.
A key ele-ment in Watson?s approach is to decompose com-plex questions into several cues and sub-cues,with the aim of generating answers from matchesfor the various cues (tapping into the Web andWikipedia).
Knowledge bases like DBpedia (Aueret al2007), Freebase (Bollacker et al2008), andYago (Suchanek et al2007)) are used for both an-swering parts of questions that can be translated tostructured form (Chu-Carroll et al2012) and type-checking possible answer candidates and thus filter-ing out spurious results (Kalyanpur et al2011).The recent QALD-1 initiative (QAL, 2011) pro-posed a benchmark task to translate questions intoSPARQL queries over linked-data sources like DB-pedia and MusicBrainz.
FREyA (Damljanovic etal., 2011), the best performing system, relies on387Q: ?x type actor .
?x wasBornIn ?z[Germany]Martin Lawrence type actor .
Martin Lawrence wasBornIn Frankfurt am MainRobert Schwentke type actor .
Robert Schwentke wasBornIn StuttgartWilly Millowitsch type actor .
Willy Millowitsch wasBornIn CologneJerry Zaks type actor .
Jerry Zaks wasBornIn StuttgartTable 5: Top-4 results for the QALD question ?Which actors were born in Germany??
after relaxationinteraction with the user to interpret the question.Earlier work on mapping questions into structuredqueries includes the work by Frank et al2007) andUnger and Cimiano (2011).
Frank et al2007) usedlexical-conceptual templates for query generation.However, this work did not address the crucial issueof disambiguating the constituents of the question.In Pythia, Unger and Cimiano (2011) relied on anontology-driven grammar for the question languageso that questions could be directly mapped onto thevocabulary of the underlying ontology.
Such gram-mars are obviously hard to craft for very large, com-plex, and evolving knowledge bases.
Nalix is an at-tempt to bring question answering to XML data (Li,Yang, and Jagadish, 2007) by mapping questions toXQuery expressions, relying on human interactionto resolve possible ambiguity.Very recently, Unger et al2012) developed atemplate-based approach based on Pythia, wherequestions are automatically mapped to structuredqueries in a two step process.
First, a set of querytemplates are generated for a question, independentof the knowledge base, determining the structure ofthe query.
After that, each template is instantiatedwith semantic items from the knowledge base.
Thisperforms reasonably well for the QALD-1 bench-mark: out of 50 test questions, 34 could be mapped,and 19 were correctly answered.Efforts on user-friendly exploration of struc-tured data include keyword search over relationaldatabases (Bhalotia et al2002) and structured key-word search (Pound et al2010).
The latter is a com-promise between full natural language and struc-tured queries, where the user provides the structureand the system takes care of the disambiguation ofkeyword phrases.Our joint disambiguation method was inspiredby recent work on NED (Milne and Witten, 2008;Kulkarni et al2009; Hoffart et al2011a) andWSD (Navigli, 2009).
In contrast to this prior workon related problems, our graph construction andconstraints are more complex, as we address thejoint mapping of arbitrary phrases onto entities,classes, or relations.
Moreover, instead of graph al-gorithms or factor-graph learning, we use an ILP forsolving the ambiguity problem.
This way, we can ac-commodate expressive constraints, while being ableto disambiguate all phrases in a few seconds.DEANNA uses dictionaries of names and phrasesfor entities, classes, and relations.
Spitkovsky andChang (2012) recently released a huge dictionary ofpairs of phrases and Wikipedia links, derived fromGoogle?s Web index.
For relations, Nakashole et al(2012) released PATTY, a large taxonomy of pat-terns with semantic types.7 Conclusions and Future WorkWe presented a method for translating natural-language questions into structured queries.
The nov-elty of this method lies in modeling several map-ping stages as a joint ILP problem.
We harness typesignatures and other information from large-scaleknowledge bases.
Although our model, in princi-ple, leads to high combinatorial complexity, we ob-served that the Gurobi solver could handle our ju-diciously designed ILP very efficiently.
Our experi-mental studies showed very high precision and goodcoverage of the query translation, and good resultsin the actual question answers.Future work includes relaxing some of the limita-tions that our current approach still has.
First, ques-tions with aggregations cannot be handled at thispoint.
Second, queries sometimes return empty an-swers although they perfectly capture the originalquestion, but the underlying data sources are incom-plete or represent the relevant information in an un-expected manner.
We plan to extend our approach ofcombining structured data with textual descriptions,and generate queries that combine structured searchpredicates with keyword or phrase matching.388ReferencesAuer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyga-niak, R.; and Ives, Z. G. 2007.
DBpedia: A Nucleusfor a Web of Open Data.
In ISWC/ASWC.Bhalotia, G.; Hulgeri, A.; Nakhe, C.; Chakrabarti, S.; andSudarshan, S. 2002.
Keyword Searching and Brows-ing in Databases using BANKS.
In ICDE.Bollacker, K. D.; Evans, C.; Paritosh, P.; Sturge, T.; andTaylor, J.
2008.
Freebase: a Collaboratively CreatedGraph Database for Structuring Human Knowledge.In SIGMOD.Chu-Carroll, J.; Fan, J.; Boguraev, B. K.; Carmel, D.; andSheinwald, D.; Welty, C. 2012.
Finding needles in thehaystack: Search and candidate generation.
In IBM J.Res.
& Dev., vol 56, no.3/4.Damljanovic, D.; Agatonovic, M.; and Cunningham, H.2011.
FREyA: an Interactive Way of Querying LinkedData using Natural Language.Dang, H. T.; Kelly, D.; and Lin, J. J.
2007.
Overview ofthe trec 2007 question answering track.
In TREC.de Marneffe, M. C.; Maccartney, B.; and Manning, C. D.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC.Elbassuoni, S.; Ramanath, M.; Schenkel, R.; Sydow, M.;and Weikum, G. 2009.
Language-model-based rank-ing for queries on rdf-graphs.
In CIKM.Elbassuoni, S.; Ramanath, M.; and Weikum, G. 2011.Query relaxation for entity-relationship search.
InESWC.Fader, A.; Soderland, S.; and Etzioni, O.
2011.
Iden-tifying relations for open information extraction.
InEMNLP.Ferrucci, D. A.; Brown, E. W.; Chu-Carroll, J.; Fan, J.;Gondek, D.; Kalyanpur, A.; Lally, A.; Murdock, J. W.;Nyberg, E.; Prager, J. M.; Schlaefer, N.; and Welty,C.
A.
2010.
Building Watson: An Overview of theDeepQA Project.
AI Magazine 31(3).Frank, A.; Krieger, H.-U.
; Xu, F.; Uszkoreit, H.; Crys-mann, B.; Jo?rg, B.; and Scha?fer, U.
2007.
QuestionAnswering from Structured Knowledge Sources.
J.Applied Logic 5(1).Gurobi Optimization, Inc. 2012.
Gurobi Optimizer Ref-erence Manual.
http://www.gurobi.com/.Heath, T., and Bizer, C. 2011.
Linked Data: Evolvingthe Web into a Global Data Space.
San Rafael, CA:Morgan & Claypool, 1 edition.Hirschman, L., and Gaizauskas, R. 2001.
Natural Lan-guage Question Answering: The View from Here.
Nat.Lang.
Eng.
7.Hoffart, J.; Mohamed, A. Y.; Bordino, I.; Fu?rstenau, H.;Pinkal, M.; Spaniol, M.; Taneva, B.; Thaterm S.; andWeikum, G. 2011.
Robust Disambiguation of NamedEntities in Text.
In EMNLP.Hoffart, J.; Suchanek, F. M.; Berberich, K.; Lewis-Kelham, E.; de Melo, G.; and Weikum, G. 2011.Yago2: exploring and querying world knowledge intime, space, context, and many languages.
In WWW(Companion Volume).Kalyanpur, A.; Murdock, J. W.; Fan, J.; and Welty, C. A.2011.
Leveraging community-built knowledge fortype coercion in question answering.
In InternationalSemantic Web Conference.Katz, B.; Felshin, S.; Marton, G.; Mora, F.; Shen, Y. K.;Zaccak, G.; Ammar, A.; Eisner, E.; Turgut, A.; andWestrick, L. B.
2007.
CSAIL at TREC 2007 Ques-tion Answering.
In TREC.Kulkarni, S.; Singh, A.; Ramakrishnan, G.; andChakrabarti, S. 2009.
Collective annotation ofwikipedia entities in web text.
In KDD.Kwok, C. C. T.; Etzioni, O.; and Weld, D. S. 2001.
Scal-ing Question Answering to the Web.
In WWW.Li, Y.; Yang, H.; and Jagadish, H. V. 2007.
NaLIX:A Generic Natural Language Search Environment forXML Data.
ACM Trans.
Database Syst.
32(4).Milne, D. N., and Witten, I. H. 2008.
Learning to linkwith wikipedia.
In CIKM.Ndapandula Nakashole, Gerhard Weikum and FabianSuchanek 2012.
PATTY: A Taxonomy of RelationalPatterns with Semantic Types.
In EMNLP.Navigli, R. 2009.
Word sense disambiguation: A survey.ACM Comput.
Surv.
41(2).Pound, J.; Ilyas, I. F.; and Weddell, G. E. 2010.
Ex-pressive and Flexible Access to Web-extracted Data: AKeyword-based Structured Query Language.
In SIG-MOD.2011.
1st Workshop on Question Answering overLinked Data (QALD-1).
http://www.sc.cit-ec.uni-bielefeld.de/qald-1.Resnik, P. 1995.
Using Information Content to EvaluateSemantic Similarity in a Taxonomy.
In IJCAI.Spitkovsky, V. I. Spitkovsky; Chang, A. X. ; 2012.
ACross-Lingual Dictionary for English Wikipedia Con-cepts.
In LREC.Suchanek, F. M.; Kasneci, G.; and Weikum, G. 2007.Yago: a core of semantic knowledge.
In WWW.Tummarello, G.; Cyganiak, R.; Catasta, M.; Danielczyk,S.
; Delbru, R.; and Decker, S. 2010.
Sig.ma: Liveviews on the web of data.
J.
Web Sem.
8(4).Unger, C.; and Cimiano, P. 2011.
Pythia: CompositionalMeaning Construction for Ontology-Based QuestionAnswering on the Semantic Web.
In NLDB.Unger, C.; Bu?hmann, L.; Lehmann, J.; Ngonga Ngomo,A.-C.; Gerber, D.; and Cimiano, P. 2012.
Template-based question answering over RDF data.
In WWW.Voorhees, E. M. 2003.
Overview of the trec 2003 ques-tion answering track.
In TREC.389Yahya, M.; Berberich, K.; Elbassuoni, S.; Ramanath, M.;Tresp, V.; and Weikum, G. 2012.
Deep answersfor naturally asked questions on the web of data.
InWWW.Zheng, Z.
2002.
AnswerBus Question Answering Sys-tem.
In HLT.390
