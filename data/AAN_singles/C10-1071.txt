Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 626?634,Beijing, August 2010Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine TranslationYoung-Suk LeeIBM Researchysuklee@us.ibm.comBing ZhaoIBM Researchzhaob@us.ibm.comXiaoqiang LuoIBM Researchxiaoluo@us.ibm.comAbstractWe present a constituent parsing-basedreordering technique that improves theperformance of the state-of-the-art Eng-lish-to-Japanese phrase translation sys-tem that includes distortion models by4.76 BLEU points.
The phrase transla-tion model with reordering applied at thepre-processing stage outperforms a syn-tax-based translation system that incor-porates a phrase translation model, a hi-erarchical phrase-based translationmodel and a tree-to-string grammar.
Wealso show that combining constituent re-ordering and  the syntax model improvesthe translation quality by additional  0.84BLEU points.1 IntroductionSince the seminal work by (Wu, 1997) and (Ya-mada and Knight, 2001), there have been greatadvances in syntax-based statistical machinetranslation to accurately model the word orderdistortion between the source and the target lan-guages.Compared with the IBM source-channel mod-els (Brown et al, 1994) and the phrase transla-tion models (Koehn et al, 2003), (Och and Ney,2004) which are good at capturing local reorder-ing within empirical phrases, syntax-based mod-els have been effective in  capturing the long-range reordering between language pairs withvery different word orders like Japanese-English(Yamada and Knight, 2001), Chinese-English(Chiang, 2005) and Urdu-English (Zollmann etal.
2008), (Callison-Burch et al 2010).However, (Xu et al, 2009) show that apply-ing dependency parsing-based reordering as pre-processing (pre-ordering hereafter) to phrasetranslation models produces translation qualitiessignificantly better than a hierarchical phrase-based  translation model (Hiero hereafter) im-plemented in (Zollman and Venugopal, 2006)for English-to-Japanese translation.
They alsoreport that the two models result in comparabletranslation qualities for English-to-Korean/Hindi/Turkish/Urdu, underpinning thelimitations of syntax-based models for handlinglong-range reordering exhibited by the strictlyhead-final Subject-Object-Verb (SOV) orderlanguages like Japanese and the largely head-initial Subject-Verb-Object (SVO) order lan-guages like English.In this paper,  we present a novel constituentparsing-based reordering technique that usesmanually written context free (CFG hereafter)and context sensitive grammar (CSG hereafter)rules.
The technique improves the performanceof the state-of-the-art English-to-Japanesephrase translation system that includes distortionmodels by 4.76 BLEU points.
The phrase trans-lation model with constituent pre-ordering con-sistently outperforms a syntax-based translationsystem that integrates features from a phrasetranslation model, Hiero and a tree-to-stringgrammar.
We also achieve an additional 0.84BLEU point improvement by  applying an ex-tended set of  reordering rules that incorporatenew rules learned from the syntax model fordecoding.The rest of the paper is organized as follows.In Section 2, we summarize  previous work re-lated to this paper.
In Section 3, we give anoverview of the syntax model with which wecompare the performance of a phrase translation626model with pre-ordering.
We also discuss achart-based decoder used in all of our experi-ments.
In Section 4, we describe the constituentparsing-based reordering rules.
We show theimpact of pre-ordering on a phrase translationmodel and compare its performance with thesyntax model.
In Section 5, we discuss experi-mental results from the combination of syntaxmodel and pre-ordering.
Finally in Section 6,we discuss future work.2 Related WorkAlong the traditions of unsupervised learning by(Wu, 1997), (Chiang, 2005) presents a modelthat uses hierarchical phrases, Hiero.
Themodel is a synchronous context free grammarlearned from a parallel corpus without any lin-guistic annotations and is applied to Chinese-to-English translation.
(Galley and Manning, 2008)propose a hierarchical phrase reordering modelthat uses shift-reduce parsing.In line with the syntax-based model of (Ya-mada and Knight, 2001) that transforms a sourcelanguage parse tree into a target language stringfor Japanese-English translation, linguisticallymotivated syntactic features have been directlyincorporated into both modeling and decoding.
(Liu, et.
al.
2006), (Zhao and Al-Onaizan, 2008)propose a  source tree to target string grammar(tree-to-string grammar hereafter) in order toutilize the source language parsing informationfor translation.
(Liu, et.
al.
2007) proposepacked forest to allow ambiguities in the sourcestructure for the tree-to-string grammar.
(Dingand Palmer, 2005) and (Zhang et.
al., 2006) pro-pose a tree-to-tree grammar, which generates thetarget tree structure from the high-precisionsource syntax.
(Shen, et.
al., 2008) propose astring to dependency tree grammar to use thetarget syntax when the target is English forwhich parsing is more accurate than other lan-guages.
(Marcu et al, 2006) introduce a syntaxmodel that uses syntactified target languagephrases.
(Chang and Toutanova, 2007) propose aglobal discriminative statistical word ordermodel that combines syntactic and surfacemovement information, which improves  thetranslation quality by 2.4 BLEU points in Eng-lish-to-Japanese translation.
(Zollmann, et.
al.,2008) compare various translation models andreport that the syntax augmented model worksbetter for Chinese-to-English and Urdu-to-English, but not for Arabic-to-English transla-tion.
(Carreras and Collins, 2009) propose ahighly flexible reordering operations during treeadjoining grammar parsing for German-Englishtranslation.
(Callison-Burch et al, 2010) report adramatic impact of syntactic translation modelson Urdu-to-English translation.Besides the approaches which integrate  thesyntactic features into translation models, thereare approaches showing improvements via pre-ordering for model training and decoding.
(Xiaand McCord, 2004), (Collins et al, 2005) and(Wang, et.
al.
2007) apply pre-ordering to thetraining data according to language-pair specificreordering rules to improve the translation quali-ties of French-English, German-English andChinese-English, respectively.
(Habash, 2007)uses syntactic preprocessing for Arabic-to-English translation.
(Xu et al, 2009) use a de-pendency parsing-based pre-ordering to improvetranslation qualities of English to five SOV lan-guages including Japanese.The current work is related to (Xu et al,2009) in terms of the language pair and transla-tion models explored.
However, we use con-stituent parsing with hierarchical rules, while(Xu et al, 2009) use dependency parsing withprecedence rules.
The two approaches have dif-ferent rule coverage and result in different wordorders especially for phrases headed by verbsand prepositions.
We also present techniques forcombining the syntax model with tree-to-stringgrammar and pre-ordering for additional per-formance improvement.
The total  improvementby the current techniques over the state-of-the-art phrase translation model is  5.6 BLEU points,which is an improvement gap not attested else-where with reordering approaches.3 Syntax Model and Chart-Based De-coderIn this section, we give an overview of  the syn-tax model incorporating a tree-to-string gram-mar.
We will compare  the syntax model per-formance with  a phrase translation model thatuses the pre-ordering technique we propose inSection 4.
We also describe the chart-based de-coder that we use in all of the experiments re-ported in this paper.6273.1 Tree-to-String GrammarTree-to-string grammar utilizes the source lan-guage parse to model reordering probabilitiesfrom a source tree to the target string (Liu et.
al.,2006), (Liu et.
al., 2007), (Zhao and Al-Onaizan, 2008) so that long distance word reor-dering becomes local in the parse tree.Reordering patterns of the source languagesyntax and their probabilities are automaticallylearned from the word-aligned source-parsedparallel data and incorporated as a tree-to-stringgrammar for decoding.
Source side parsing andthe resulting reordering patterns bound thesearch space.
Parsing also assigns linguistic la-bels to the chunk, e.g.
NP-SBJ, and allows sta-tistics to be clustered reasonably.
Each syn-chronous context free grammar (SCFG) rewrit-ing rule rewrites a source treelet into a targetstring, with both sides containing hiero-stylevariables.
For instance, the rule [X, VP] [X,VB] [X,NP] ?
[X, NP] [X, VB] rewrites a VPwith two constituents VB and NP  into an NPVB order in the target, shown below.The tree-to-string grammar introduces possiblesearch space to generate an accurate word order,which is refined on the basis of supports fromother models.
However, if the correct word or-der cannot be generated by the tree-to-stringgrammar, the system can resort to rules fromHiero or a phrase translation model for extendedrule coverage.3.2 Chart-based DecoderWe use a  chart-based decoder ?
a template de-coder that generalizes over various decodingschemes in terms of the dot-product in Earley-style parsing (Earley, 1970) ?
to support variousdecoding schemes such as phrase, Hiero(Chiang, 2005), Tree-to-String, and the mixtureof all of the above.This framework allows one to strictly com-pare different decoding schemes using the samefeature and parameter setups.
For the experi-mental results in Sections 4 & 5, we applied (1)phrase decoding for the baseline phrase transla-tion system that includes distortion models, (2)Hiero decoding for the Hiero system that incor-porates a phrase translation model, and (3)Tree-to-string decoding for the syntax-basedsystems that incorporate features  from phrasetranslation, Hiero and tree-to-string grammarmodels.The decoder seeks the best hypothesis *e  ac-cording to the Bayesian decision rule (1):)1()()(minarg*},{deeDde??
??
?d is one derivation path, rewriting the sourcetree into the target string via the probabilisticsynchronous context free tree-to-string grammar(PSCFG).
)(e?
is the cost functions computedfrom general n-gram language models.
In thiswork, we use two sets of interpolated 5-gramlanguage models.
)(d?
is a vector of cost func-tions defined on the derivation sequence.
Wehave integrated  18 cost functions ranging  fromthe basic relative frequencies and IBM model-1scores to counters for different types of rulesincluding blocks, glue, Hiero, and tree-to-stringgrammar rules.
Additional cost functions arealso integrated into the decoder, including meas-uring the function/content-word mismatch be-tween source and target, similar to (Chiang et.al., 2009) and length distribution for non-terminals in (Shen et.
al., 2009).4 Parsing and Reordering RulesWe apply a set of manually acquired reorderingrules to the parsing output from a constituentparser to pre-order the data for model trainingand decoding.4.1 Parsing with Functional TagsWe use a maximum entropy English parser (Rat-naparkhi, 1999) trained on OntoNotes (Hovy,2006) data.
OntoNotes data include most of theWall Street Journal data in Penn Treebank(Marcus et al, 1993) and additional data frombroadcast conversation, broadcast news and weblog.SNP-SBJX1X2VPVBX3NPX1 X3 X2Src treeletTgt string628Figure 1.
Parse Tree and Word Alignment before ReorderingFigure 2.
Parse Tree and Word Alignment after ReorderingThe parser is trained with all of the functionaland part-of-speech (POS)  tags in the originaldistribution: total 59 POS tags and 364 phraselabels.We use functional tags since reordering de-cisions for machine translation are highly in-fluenced by the function of a phrase, as will beshown later in this section.
An example parsetree with functional tags is given at the top halfof  Figure 1.
NP-SBJ indicates a subject nounphrase, SBAR-ADV, an adverbial clause.4.2 Structural Divergence between Eng-lish and JapaneseJapanese is a strictly head-final language, i.e.the head is located at the end of  a phrase.This leads to  a high degree of distortions withEnglish, which is largely head initial.SBAR-ADVSVPVBNINNP-SBJPRPVPVPNP VBNP VPDT NNS VBNPPNPDT NNINMDNNNP-SBJPRPVPMD VPVB NPNP VPDT NNS VBN PPSIN NPDT NNSBAR-ADVIN SVPVBNyou           must       undo   the        changes     made      by        that     installation         if        needed???
???
, ??
??????
?
??
??
?
??
??
???
??
?
?needed if you sbj  the changes that  installation by  made undo     mustS???
???
, ??
??????
?
??
??
?
??
??
???
??
?
?629The word order contrast between the twolanguages is illustrated by the human wordalignment at the bottom half of Figure 1.
Allinstances of word alignments are non-monotonic except for the sequence that installa-tion, which is monotonically aligned to theJapanese morpheme sequence ????????.
Note that there are no wordboundaries in Japanese written text, and we ap-ply Japanese morpheme segmentation to obtainmorpheme sequences in the figure.
All of theJapanese examples in this paper are presentedwith morpheme segmentation.The manual reordering rules are written by aperson who is proficient with English and Japa-nese/Korean grammars, mostly on the basis ofperusing parsed English texts.4.3 CFG Reordering RulesOur reordering rules are mostly CFG rules anddivided into head and modifier  rules.Head reordering rules in Table 1 move verbsand prepositions from the phrase initial to thephrase final positions (Rules 1-11).
Reorderingof the head phrase in an adverbial clause alsobelongs to this group (Rules 12-14).
The labelsequences in Before RO and After RO are theimmediate children of the Parent Node beforeand after reordering.
VBX stands for VB, VBZ,VBP, VBD, VBN and VBG.
XP+ stands for oneor more POS and/or phrase labels such as MD,VBX, NP, PP, VP, etc.
In 2 & 4, RB is  the tagfor negation not.
In 5, RP is the tag for a verbparticle.Modifier reordering rules in Table 2 movemodified phrases from the phrase initial to thephrase final positions within an NP (Rules 1-3).They also include placement of NP, PP, ADVPwithin a VP (Rules 4 & 5).
The subscripts in arule, e.g.
PP1 and PP2 in Rule 3, indicate thedistinctness of each phrase sharing the samelabel.4.4 CSG Reordering RulesSome reordering rules cannot be captured byCFG rules, and we resort to CSG rules.11 These CSG rules apply to trees of depth two or more, andthe applications are dependent on surrounding contexts.Therefore,  they are different from CFG rules which applyonly to trees of depth one, and TSG (tree substitutiongrammar) rules for which variables are independentlysubstituted by substitution.
The readers are referred toParent Node Before RO After RO1 VP MD VP VP MD2 VP MD RB VP VP MD RB3 VP VBX XP+ XP+ VBX4 VP VBX RB XP+ XP+ VBX RB5 VP VBX RP XP+ XP+ VBX RP6 ADJP-PRD JJ XP+ XP+ JJ7 PP IN NP NP IN8 PP IN S S IN9 SBAR-TMP IN S S IN10 SBAR-ADV IN S S IN11 SBAR-PRP IN S S IN12 SBAR-TMP WHADVP S S WHADVP13 SBAR-ADV WHADVP S S WHADVP14 SBAR-PRP WHADVP S S WHADVPTable 1.
Head Reordering RulesParentNodeBefore RO After RO1 NP NP SBAR SBAR NP2 NP NP PP PP NP3 NP NP PP1 PP2 PP1 PP2 NP4 VP VBX NP PP PP NP VBX5 VP VBX NP ADVP-TMP PPPP NP ADVP-TMP VBXTable 2.
Modifier Reordering RulesFor instance, in the parse tree and wordalignment in Figure 1,  the last two Englishwords if needed under SBAR-ADV is aligned tothe first  two Japanese words ???
??
?.In order to change the English order to the cor-responding Japanese order, SBAR-ADV domi-nated by the VP should move across the VP tosentence initial position, as shown in the tophalf of Figure 2,  requiring a CSG rule.The adverbial clause reordering in Figure 2 isdenoted as Rule 1 in Table 3, which lists twoother CSG rules, Rule 2 & 3.2  The subscripts inTable 3 are interpreted in the same way as thosein Table 2.
(Joshi and Schabes, 1997) for formal definitions of variousgrammar formalisms.2Rule 3 is applied after all CFG rules, see Section 4.6.Therefore, VBX?s are located at the end of each corre-sponding VP.630Before  RO ?
After RO1 (S XP1+ (VP XP2+ SBAR-ADV ))?
(S SBAR-ADV XP1 + (VP XP2+ ))2 (S XP1+ (VP (XP2+ SBAR-ADV )))?
(S XP1+ SBAR-ADV (VP (XP2 + )))3 (VP1 ADVP-MNR (VP2 XP+ VBX2 ) VBX1)?
(VP1 (VP2 XP+ ADVP-MNR VBX2) VBX1)Table 3.
CSG Reordering RulesADVP-MNR stands for a manner adverbialphrase such as explicitly in the following: Thesoftware version has been explicitly verified asworking.
Rule 3 in Table 3 indicates that aADVP-MNR has to immediately precede a verbin Japanese, resulting in the substring ?...asworking explicitly verified...?
after reordering.Note that functional tags allow us to write re-ordering rules specific to  semantic phrases.
Forinstance, in Rule 1, SBAR-ADV under VPmoves to the sentence initial position under S,but an SBAR without any functional tags donot.
It typically stays within a VP as the com-plement of the verb.4.5 Subject Marker InsertionJapanese extensively uses case particles thatdenote the role of the preceding noun phrase,for example,  as subject, object, etc.
We insertsbj, denoting the subject marker, at the end of asubject noun phrase NP-SBJ.
The inserted sub-ject marker sbj mostly gets translated into thesubject particle?
or?
in Japanese.34.6 Reordering Rule ApplicationThe rules are applied categorically, sequentiallyand recursively.
CSG Rules 1 and 2 in Table 3are applied before all of the CFG rules.
AmongCFG rules, the modifier rules in Table 2 areapplied before the head rules in Table 1.
CSGRule 3 in Table 3 is applied last,  followed bythe subject marker insertion operation.CFG head and modifier rules are applied re-cursively.
The top half of Figure 2 is the parsetree obtained by applying reordering rules to theparse tree in Figure 1.
After reordering, theword alignment becomes mostly monotonic, asshown at the bottom half of Figure 2.3 The subject marker insertion is analogous to the insertionoperation  in (Yamada and Knight, 2001), which covers awide range of insertion of case particles and verb inflec-tions in general.4.7 Experimental ResultsAll systems are trained on a parallel corpus,primarily from the Information Technology (IT)domain and evaluated on the data from the samedomain.
The training data statistics is in Table 4and the evaluation data statistics is in Table 5.Japanese tokens are morphemes and Englishtokens are punctuation tokenized words.Corpus Stats English Japanesesentence count 3,358,635 3,358,635token count 57,231,649 68,725,865vocabulary size 242,712 348,221Table 4.
Training Corpus StatisticsData Sets Sentence Count Token CountTuning 600 11,761DevTest 437 8,158Eval 600 11,463Table 5.
Evaluation Data StatisticsWe measure the translation quality with IBMBLEU (Papineni et al, 2002) up to 4 grams,using 2 reference translations, BLEUr2n4.
ForBLEU score computation, we character-segment Kanji and Kana sequences in the refer-ence and the machine translation output.
Vari-ous system performances are shown in Table 6.Models Tuning DevTest EvalPhrase (BL) 0.5102 0.5330 0.5486Hiero 0.5385 0.5574 0.5724Syntax 0.5561 0.5777 0.5863Phrase+RO1 0.5681 0.5793 0.5962Table 6.
Model Performances (BLEUr2n4)Phrase (BL) is the baseline phrase translationsystem that  incorporates lexical distortionmodels (Al-Onaizan and Papineni, 2006).Hiero is the hierarchical phrase-based system(Chiang, 2006) that incorporates the phrasetranslation model.
Syntax is the syntax modeldescribed in Section 3, which incorporates thephrase translation, Hiero and tree-to-stringgrammar models.
Phrase+RO1 is the phrasetranslation model with pre-ordering  for systemtraining and decoding,  using the rules describedin this section.
Phrase+RO1 improves the trans-lation quality of the baseline model by 4.76BLEU points and outperforms the syntax modelby over 0.9 BLEU points.6315 Constituent Reordering and SyntaxModel CombinedTranslation qualities of systems that combinethe syntax model and pre-ordering are shown inTable 7.
Syntax+RO1 indicates the  syntaxmodel with pre-ordering discussed in Section 4.Syntax+RO2 indicates the syntax model with amore extensive pre-ordering for decoding dis-cussed below .Models Tuning DevTest EvalPhrase+RO1 0.5681 0.5793 0.5962Syntax+RO1 0.5742 0.5802 0.6003Syntax+RO2 0.5769 0.5880 0.6046Table 7.
Syntax Model with Pre-orderingAnalyses of the syntax model in Table 6 re-vealed that automatically learned rules by thetree-to-string grammar include new rules notcovered by the manually written rules,  some ofwhich are shown in Table 8.Parent  Node Before  RO After ROADJP-PRD RB JJ PP PP RB JJADVP-TMP RB PP PP RBADVP ADVP PP PP ADVPNP NP VP VP NPTable 8.
New CFG rules automatically learnedby Tree-to-String grammarWe augment the manual rules with the newautomatically learned  rules.
We call this ex-tended set of reordering rules RO2.
We use themanual reordering rules RO1 for model train-ing, but use the extended rules RO2 for decod-ing.
And we obtain the translation output Syn-tax+RO2 in Table 7.
Syntax+RO2 outperformsPhrase+RO1 by 0.84 BLEU points, and Syn-tax+RO1 by 0.43 BLEU points.In Table 9, we show the ratio of each ruletype preserved in the derivation of one-besttranslation output of the following two models:Syntax  and Syntax+RO2.
In the table,?Blocks?
indicate phrases from the phrase trans-lation model and ?Glue Rules?
denote the de-fault grammar rule for monotone decoding.The syntax model without pre-ordering (Syn-tax) heavily utilizes the Hiero and tree-to-stringgrammar rules, whereas the syntax model withpre-ordering (Syntax+RO2) mostly depends onmonotone decoding with blocks and glue rules.Rule Type Syntax Syntax+RO2Blocks 46.3% 51.2%Glue Rules  6.0% 37.3%Hiero Rules 18.3%   1.3%Tree-to-String 29.4% 10.2%Table 9.
Ratio of each rule type preserved in thetranslation derivation of Syntax and Syn-tax+RO26 Summary and Future ResearchWe have proposed a constituent pre-orderingtechnique for English-to-Japanese translation.The technique improves the performance of thestate-of-the-art phrase translation models by4.76 BLEU points and outperforms a syntax-based translation system that incorporates aphrase translation model, Hiero and a tree-to-string grammar.
We have also shown that com-bining constituent pre-ordering and  the syntaxmodel improves the translation quality by addi-tional  0.84 BLEU points.While achieving solid performance im-provement over the existing translation modelsfor English-to-Japanese translation, our workhas revealed some limitations of syntax modelsboth in terms of grammar representations andmodeling.
Whereas many syntax models arebased on CFG rules for probability acquisition,the current research shows that there are varioustypes of reordering that require the generativecapacity beyond CFG.
While most of the reor-dering rules for changing the English order tothe Japanese order (and vice versa) should ap-ply categorically,4 often the probabilities oftree-to-string grammar rules are not highenough to survive in the translation derivations.As for the reordering rules that require thegenerative capacity beyond CFG, we maymodel mildly context sensitive grammars suchas tree adjoining grammars (Joshi and Schabes,1997), as in (Carreras and Collins, 2009).
The4 Assuming that the parses are correct, the head reorderingrules in Table 1 have to apply categorically to change theEnglish order into the Japanese order because English ishead initial and Japanese is head final without any excep-tions.
Similarly, most of the modifier reordering rules inTable 2 have to apply categorically because most modifi-ers follow the modified head phrase in English, e.g.
a rela-tive clause modifier follows the head noun phrase, aprepositional phrase modifier follows the head nounphrase, etc., whereas modifier phrases precede the modi-fied head phrases in Japanese.632extended domain of locality of  tree adjoininggrammars should suffice to capture non-CFGreordering rules for many language pairs.
Alter-natively, we can adopt enriched feature repre-sentations so that  a tree of depth one can actu-ally convey information on a tree of severaldepths, such as parent annotation of (Klein andManning, 2003).Regarding the issue of modeling, we can in-troduce a rich set of features, as in (Ittycheriahand Roukos, 2007), the weights of which aretrained to ensure that the tree-to-string grammarrules generating the accurate target orders areassigned probabilities high enough not to getpruned out  in the translation derivation.AcknowledgementWe would like to acknowledge IBM RTTS(Realtime Translation Systems) team for tech-nical discussions on the topic and the provisionof linguistic resources.
We also would like tothank IBM SMT (Statistical Machine Transla-tion) team for various software tools and theanonymous reviewers for their helpful com-ments .ReferencesY.
Al-Onaizan and K. Papineni.
2006.
Distortionmodels for statistical machine translation.
Pro-ceedings of ACL-COLING.
Pages 529-536.C.
Baker, S. Bethard, M. Bloodgood, R. Brown, C.Callison-Burch, G. Coppersmith, B. Dorr, W. Fi-lardo, K. Giles, A. Irvine, M. Kayser, L. Levin, J.Martineau, J. Mayfield, S. Miller, A. Phillips, A.Philpot, C. Piatko, L. Schwartz, D. Zajic.
2010.Semantically Informed Machine Translation(SIMT).
Final Report of the 2009 Summer Campfor Applied Language Exploration.P.
Brown, V. Della Pietra, S. Della Pietra, and R.Mercer.
1993.
The mathematics of statistical ma-chine translation: parameter estimation, Computa-tional Linguistics, 19(2):263?311.X.
Carreras and M. Collins.
2009.
Non-projectiveparsing for statistical machine translation.
Pro-ceedings of the 2009 EMNLP.
Pages 200-209.P.
Chang and C. Toutanova.
2007.
A DiscriminativeSyntactic Word Order Model for Machine Trans-lation.
Proceedings of ACL.
Pages 9-16.D.
Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
Pro-ceedings of ACL.
Pages 263-270.D.
Chiang, W. Wang and  Kevin Knight.
2009.11,001 new features for statistical machine trans-lation.
Proceedings of HLT-NAACL.
Pages 218-226.M.
Collins, P. Koehn, I. Kucerova.
2005.
ClauseRestructuring for Statistical Machine Translation.Proceedings of  ACL.
Pages 531-540.Y.
Ding and M. Palmer.
2005.
Machine translationusing probabilistic synchronous dependency in-sertion grammars.
Proceedings of ACL.
Pages541-548.J.
Earley.
1970.
An efficient context-free parsingalgorithm.
Communications of the ACM.
Vol.
13.Pages 94?102.M.
Galley and C. Manning.
2008.
A Simple and Ef-fective Hierarchical Phrase Reordering Model.Proceedings of EMNLP.N.
Habash.
2007.
Syntactic Preprocessing for Statis-tical Machine Translation.
Proceedings of theMachine Translation Summit.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw andR.
Weischedel.
2006.
OntoNotes: The 90% Solu-tion.
Proceedings of HLT.
Pages 57-60.A.
Ittycheriah and S. Roukos.
2007.
Direct Transla-tion Model 2.
Proceedings of HLT-NAACL.A.
Joshi and Y. Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and K. Salomaa, edi-tors, Handbook of Formal Languages, volume 3.Springer.D.
Klein and C. Manning.
2003.
Accurate Unlexi-calized Parsing.
Proceedings of 41st ACL.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation, Proceedings ofHLT?NAACL.
Pages 48?54.Y.
Liu, Q. Liu and S. Lin.
2006.
Tree-to-stringalignment template for statistical machine transla-tion.
Proceedings of ACL-COLING.Y.
Liu, Y. Huang, Q. Liu, and S. Lin.
2007.
Forest-to-string statistical translation rules.
Proceedingsof the 45th ACL.D.
Marcu, W. Wang, A. Echihabi and K. Knight.2006.
SPMT: Statistical Machine Translation withSyntactified Target Language Phrases.
Proceed-ings of EMNLP.
Pages 44-52.M.
Marcus, B. Santorini and M.  Marcinkiewicz.1993.
Building a Large Annotated Corpus of Eng-633lish: the Penn Treebank.
Computational Linguis-tics,  19(2):  313-330.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Com-putational Linguistics: Vol.
30.
Pages 417?
449.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of ma-chine translation.
Proceddings  of ACL.
Pages311?318.A.
Ratnaparkhi.
1999.
Learning to Parse NaturalLanguage with Maximum Entropy Models.
Ma-chine Learning: Vol.
34.
Pages 151-178.L.
Shen, J. Xu and R. Weischedel.
2008.
A newstring-to-dependency machine translation algo-rithm with a target dependency language model.Proceedings of ACL.L.
Shen, J. Xu, B. Zhang, S. Matsoukas and RalphWeischedel.
2009.
Effective Use of Linguisticand Contextual Information for Statistical Ma-chine Translation.
Proceedings of EMNLP.C.
Wang, M. Collins, P. Koehn.
2007.
Chinese Syn-tactic Reordering for Statistical Machine Transla-tion.
Proceedings of EMNLP-CoNLL.D.
Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3): 377-404.F.
Xia and M. McCord.
2004.
Improving a StatisticalMT System with Automatically Learned RewritePatterns.
Proceedings of COLING.P.
Xu, J. Kang, M. Ringgaard, F. Och.
2009.
Using adependency parser to improve SMT for subject-verb-object languages.
Proceedings of HLT-NAACL.K.
Yamada and K. Knight.
2001.
A Syntax-basedStatistical Translation Model.
Proceedings of the39th ACL.
Pages 523-530.H.
Zhang, L. Huang, D. Gildea and K. Knight.2006.
Synchronous binarization for machinetranslation.
Proceedings of the HLT-NAACL.Pages 256-263.B.
Zhao and Y. Al-onaizan.
2008.
Generalizing Lo-cal and Non-Local Word-Reordering Patterns forSyntax-Based Machine Translation.
Proceedingsof EMNLP.
Pages 572-581.A.
Zollmann and A. Venugopal.
2006.
Syntax aug-mented machine translation via chart parsing.Proceedings of NAACL 2006 -Workshop on sta-tistical machine  translation.A.
Zollmann, A. Venugopal, F. Och and J. Ponte.2008.
A Systematic Comparison of Phrase-Based,Hierarchical and Syntax-Augmented MT.
Pro-ceedings of COLING.634
