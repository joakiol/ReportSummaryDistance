Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344?352,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPositive Results for Parsing with a Bounded Stack using a Model-BasedRight-Corner TransformWilliam SchulerDept.
of Computer Science and EngineeringMinneapolis, MNschuler@cs.umn.eduAbstractStatistical parsing models have recently beenproposed that employ a bounded stack in time-series (left-to-right) recognition, using a right-corner transform defined over training trees tominimize stack use (Schuler et al, 2008).
Cor-pus results have shown that a vast majorityof naturally-occurring sentences can be parsedin this way using a very small stack boundof three to four elements.
This suggests thatthe standard cubic-time CKY chart-parsingalgorithm, which implicitly assumes an un-bounded stack, may be wasting probabilitymass on trees whose complexity is beyond hu-man recognition or generation capacity.
Thispaper first describes a version of the right-corner transform that is defined over entireprobabilistic grammars (cast as infinite setsof generable trees), in order to ensure a faircomparison between bounded-stack and un-bounded PCFG parsing using a common un-derlying model; then it presents experimentalresults that show a bounded-stack right-cornerparser using a transformed version of a gram-mar significantly outperforms an unbounded-stack CKY parser using the original grammar.1 IntroductionStatistical parsing models have recently been pro-posed that employ a bounded stack in time-series(left-to-right) recognition, in order to directly andtractably incorporate incremental phenomena suchas (co-)reference or disfluency into parsing deci-sions (Schuler et al, 2008; Miller and Schuler,2008).
These models make use of a right-cornertree transform, based on the left-corner transformdescribed by Johnson (1998), and are supported bycorpus results suggesting that most sentences (in En-glish, at least) can be parsed using a very smallstack bound of three to four elements (Schuler etal., 2008).
This raises an interesting question: ifmost sentences can be recognized with only threeor four elements of stack memory, is the standardcubic-time CKY chart-parsing algorithm, which im-plicitly assumes an unbounded stack, wasting prob-ability mass on trees whose complexity is beyondhuman recognition or generation capacity?This paper presents parsing accuracy results us-ing transformed and untransformed versions of acorpus-trained probabilistic context-free grammarsuggesting that this is indeed the case.
Experimentalresults show a bounded-memory time-series parserusing a transformed version of a grammar signifi-cantly outperforms an unbounded-stack CKY parserusing the original grammar.Unlike the tree-based transforms described previ-ously, the model-based transform described in thispaper does not introduce additional context fromcorpus data beyond that contained in the origi-nal probabilistic grammar, making it possible topresent a fair comparison between bounded- andunbounded-stack versions of the same model.
Sincethis transform takes a probabilistic grammar as in-put, it can also easily accommodate horizontal andvertical Markovisation (annotating grammar sym-bols with parent and sibling categories) as describedby Collins (1997) and subsequently.The remainder of this paper is organized as fol-lows: Section 2 describes related approaches to pars-ing with stack bounds; Section 3 describes an exist-ing bounded-stack parsing framework using a right-corner transform defined over individual trees; Sec-tion 4 describes a redefinition of this transform to ap-344ply to entire probabilistic grammars, cast as infinitesets of generable trees; and Section 5 describes anevaluation of this transform on the Wall Street Jour-nal corpus of the Penn Treebank showing improvedresults for a transformed bounded-stack version of aprobabilistic grammar over the original unboundedgrammar.2 Related WorkThe model examined here is formally similar toCombinatorial Categorial Grammar (CCG) (Steed-man, 2000).
But the CCG account is a competencemodel as well as a performance model, in that itseeks to unify category representations used in pro-cessing with learned generalizations about argumentstructure; whereas the model described in this paperis exclusively a performance model, allowing gen-eralizations about lexical argument structures to belearned in some other representation, then combinedwith probabilistic information about parsing strate-gies to yield a set of derived incomplete constituents.As a result, the model described in this paper has afreer hand to satisfy strict working memory bounds,which may not permit some of the alternative com-position operations proposed in the CCG account,thought to be associated with available prosody andquantifier scope analyses.1Other models (Abney and Johnson, 1991; Gibson,1991) seek to explain human processing difficultiesas a result of memory capacity limits in parsing or-dinary phrase structure trees.
The Abney-Johnsonand Gibson models adopt a left-corner parsing strat-egy, of which the right-corner transform described inthis paper is a variant, in order to minimize memoryusage.
But the transform-based model described inthis paper exploits a conception of chunking (Miller,1956) ?
in this case, grouping recognized wordsinto stacked-up incomplete constituents ?
to oper-ate within much stricter estimates of human short-term memory bounds (Cowan, 2001) than assumedby Abney and Johnson.1The lack of support for some of these available scope anal-yses may not necessarily be problematic for the present model.The complexity of interpreting nested raised quantifiers mayplace them beyond the capability of human interactive incre-mental interpretation, but not beyond the capability of post-hocinterpretation (understood after the listener has had time to thinkabout it).Several existing incremental systems are orga-nized around a left-corner parsing strategy (Roark,2001; Henderson, 2004).
But these systems gen-erally keep large numbers of constituents open formodifier attachment in each hypothesis.
This al-lows modifiers to be attached as right children of anysuch open constituent.
But if any number of openconstituents are allowed, then either the assumptionthat stored elements have fixed syntactic (and se-mantic) structure will be violated, or the assump-tion that syntax operates within a bounded mem-ory store will be violated, both of which are psy-cholinguistically attractive as simplifying assump-tions.
The HHMM model examined in this pa-per upholds both the fixed-element and bounded-memory assumptions by hypothesizing fixed reduc-tions of right child constituents into incomplete par-ents in the same memory element, to make room fornew constituents that may be introduced at a latertime.
These in-element reductions are defined natu-rally on phrase structure trees as the result of align-ing right-corner transformed constituent structuresto sequences of random variables in a factored time-series model.3 BackgroundThe recognition model examined in this paper is afactored time-series model, based on a HierarchicHidden Markov Model (Murphy and Paskin, 2001),which probabilistically estimates the contents of amemory store of three to four partially-completedconstituents over time.
Probabilities for expansions,transitions and reductions in this model can be de-fined over trees in a training corpus, transformedand mapped to the random variables in an HHMM(Schuler et al, 2008).
In Section 4 these probabil-ities will be computed directly from a probabilisticcontext-free grammar, in order to evaluate the con-tribution of stack bounds without introducing addi-tional corpus context into the model.3.1 A Bounded-Stack ModelHHMMs are factored HMMs which mimic abounded-memory pushdown automaton (PDA), sup-porting simple push and pop operations on abounded stack-like memory store.HMMs characterize speech or text as a sequence345of hidden states qt (in this case, stacked-up syntac-tic categories) and observed states ot (in this case,words) at corresponding time steps t. A most likelysequence of hidden states q?1..T can then be hypothe-sized given any sequence of observed states o1..T :q?1..T = argmaxq1..TP(q1..T | o1..T ) (1)= argmaxq1..TP(q1..T )?P(o1..T | q1..T ) (2)def= argmaxq1..TT?t=1P?A(qt | qt-1)?P?B(ot | qt) (3)using Bayes?
Law (Equation 2) and Markov in-dependence assumptions (Equation 3) to definea full P(q1..T | o1..T ) probability as the productof a Transition Model (?A) prior probabilityP(q1..T ) def= ?t P?A(qt | qt-1) and an ObservationModel (?B) likelihood probability P(o1..T | q1..T ) def=?t P?B(ot | qt).Transition probabilities P?A(qt | qt-1) over com-plex hidden states qt can be modeled using synchro-nized levels of stacked-up component HMMs in anHHMM.
HHMM transition probabilities are calcu-lated in two phases: a reduce phase (resulting in anintermediate, marginalized state ft), in which com-ponent HMMs may terminate; and a shift phase (re-sulting in a modeled state qt), in which unterminatedHMMs transition, and terminated HMMs are re-initialized from their parent HMMs.
Variables overintermediate ft and modeled qt states are factoredinto sequences of depth-specific variables ?
one foreach of D levels in the HHMM hierarchy:ft = ?f1t .
.
.
fDt ?
(4)qt = ?q1t .
.
.
qDt ?
(5)Transition probabilities are then calculated as aproduct of transition probabilities at each level, us-ing level-specific reduce ?R,d and shift ?S,d models:P?A(qt|qt-1) =?ftP(ft|qt-1)?P(qt|ft qt-1) (6)def=?f1..DtD?d=1P?R,d(fdt |fd+1t qdt-1qd-1t-1 )?P?S,d(qdt |fd+1t fdt qdt-1qd-1t ) (7)with fD+1t and q0t defined as constants.
In Viterbidecoding, the sums are replaced with argmax opera-tors.
This decoding process preserves ambiguity by.
.
.. .
.. .
.. .
.f3t?1f2t?1f1t?1q1t?1q2t?1q3t?1ot?1f3tf2tf1tq1tq2tq3totFigure 1: Graphical representation of a Hierarchic Hid-den Markov Model.
Circles denote random variables, andedges denote conditional dependencies.
Shaded circlesare observations.maintaining competing analyses of the entire mem-ory store.
A graphical representation of an HHMMwith three levels is shown in Figure 1.Shift and reduce probabilities can then be definedin terms of finitely recursive Finite State Automata(FSAs) with probability distributions over transition,recursive expansion, and final-state status of states ateach hierarchy level.
In the version of HHMMs usedin this paper, each intermediate variable is a reduc-tion or non-reduction state fdt ?
G?
{1,0} (indi-cating, respectively, a complete reduced constituentof some grammatical category from domain G, ora failure to reduce due to an ?active?
transition be-ing performed, or a failure to reduce due to an?awaited?
transition being performed, as defined inSection 4.3); and each modeled variable is a syn-tactic state qdt ?
G?G (describing an incompleteconstituent consisting of an active grammatical cat-egory from domain G and an awaited grammaticalcategory from domain G).
An intermediate vari-able fdt at depth d may indicate reduction or non-reduction according to ?F-Rd,d if there is a reductionat the depth level immediately below d, but must in-dicate non-reduction (0) with probability 1 if therewas no reduction below:2P?R,d(fdt | fd+1t qdt-1qd-1t-1 ) def={if fd+1t 6?G : [fdt =0]if fd+1t ?G : P?F-Rd,d(fdt | qdt-1, qd-1t-1 ) (8)2Here [?]
is an indicator function: [?]
= 1 if ?
is true, 0otherwise.346where fD+1t ?G and q0t = ROOT.Shift probabilities over the modeled variable qdtat each level are defined using level-specific transi-tion ?Q-Tr,d and expansion ?Q-Ex,d models:P?S,d(qdt | fd+1t fdt qdt-1qd-1t ) def=??
?if fd+1t 6?G, fdt 6?G : [qdt = qdt-1]if fd+1t ?G, fdt 6?G : P?Q-Tr,d(qdt | fd+1t fdt qdt-1qd-1t )if fd+1t ?G, fdt ?G : P?Q-Ex,d(qdt | qd-1t )(9)where fD+1t ?G and q0t = ROOT.
This model isconditioned on reduce variables at and immediatelybelow the current FSA level.
If there is no reduc-tion immediately below the current level (the firstcase above), it deterministically copies the currentFSA state forward to the next time step.
If thereis a reduction immediately below the current levelbut no reduction at the current level (the second caseabove), it transitions the FSA state at the currentlevel, according to the distribution ?Q-Tr,d.
And ifthere is a reduction at the current level (the third caseabove), it re-initializes this state given the state at thelevel above, according to the distribution ?Q-Ex,d.The overall effect is that higher-level FSAs are al-lowed to transition only when lower-level FSAs ter-minate.
An HHMM therefore behaves like a prob-abilistic implementation of a pushdown automaton(or shift?reduce parser) with a finite stack, where themaximum stack depth is equal to the number of lev-els in the HHMM hierarchy.3.2 Tree-Based TransformsThe right-corner transform used in this paper is sim-ply the left-right dual of a left-corner transform(Johnson, 1998).
It transforms all right branchingsequences in a phrase structure tree into left branch-ing sequences of symbols of the form A?/A??
?, de-noting an incomplete instance of an ?active?
categoryA?
lacking an instance of an ?awaited?
category A??
?yet to come.3 These incomplete constituent cate-gories have the same form and much of the samemeaning as non-constituent categories in a Combi-natorial Categorial Grammar (Steedman, 2000).3Here ?
and ?
are node addresses in a binary-branching tree,defined as paths of left (0) or right (1) branches from the root.Rewrite rules for the right-corner transform areshown below:4?
Beginning case: the top of a right-expandingsequence in an ordinary phrase structure tree ismapped to the bottom of a left-expanding se-quence in a right-corner transformed tree:A?A??0?A??1??A?A?/A??1A??0??
(10)This case of the right-corner transform may beconsidered a constrained version of CCG typeraising.?
Middle case: each subsequent branch in aright-expanding sequence of an ordinary phrasestructure tree is mapped to a branch in a left-expanding sequence of the transformed tree:A??
A???A????0?A????1??A?A?/A????1A?/A????A????0??
(11)This case of the right-corner transform may beconsidered a constrained version of CCG for-ward function composition.?
Ending case: the bottom of a right-expandingsequence in an ordinary phrase structure tree ismapped to the top of a left-expanding sequencein a right-corner transformed tree:A??
A???a????A?A?/A????A???a???
(12)This case of the right-corner transform may beconsidered a constrained version of CCG for-ward function application.4These rules can be applied recursively from bottom upon a source tree, synchronously associating subtree structuresmatched to variables ?, ?, and ?
on the left side of each rulewith transformed representations of these subtree structures onthe right.347a) binary-branching phrase structure tree:SNPNPJJstrongNNdemandPPINforNPNPposNNPNNPnewNNPNNPyorkNNPcityPOS?sNNSJJgeneralNNSNNobligationNNSbondsVPVBNVBNproppedPRTupNPDTtheNNJJmunicipalNNmarketb) result of right-corner transform:SS/NNS/NNS/NPS/VPNPNP/NNSNP/NNSNP/NNSNP/NPNP/PPNPNP/NNJJstrongNNdemandINforNPposNPpos/POSNNPNNP/NNPNNP/NNPNNPnewNNPyorkNNPcityPOS?sJJgeneralNNobligationNNSbondsVBNVBN/PRTVBNproppedPRTupDTtheJJmunicipalNNmarketFigure 2: Trees resulting from a) a sample phrase structure tree for the sentence Strong demand for New York City?sgeneral obligations bonds propped up the municipal market, and b) a right-corner transform of this tree.
Sequences ofleft children are recognized from the bottom up through in-element transitions in a Hierarchic Hidden Markov Model.Right children are recognized by expanding to additional stack elements.The completeness of the above transform rules canbe demonstrated by the fact that they cover all pos-sible subtree configurations (with the exception ofbare terminals, which are simply copied).
Thesoundness of the above transform rules can bedemonstrated by the fact that each rule transformsa right-branching subtree into a left-branching sub-tree labeled with an incomplete constituent.An example of a right-corner transformed tree isshown in Figure 2(b).
An important property of thistransform is that it is reversible.
Rewrite rules for re-versing a right-corner transform are simply the con-verse of those shown above.Sequences of left children in the resulting mostly-left-branching trees are recognized from the bot-tom up, through transitions at the same stack ele-ment.
Right children, which are much less frequentin the resulting trees, are recognized through cross-element expansions in a bounded-stack recognizer.4 Model-Based TransformsIn order to compare bounded- and unbounded-stackversions of the same model, the formulation ofthe right-corner and bounded-stack transforms in-troduced in this paper does not map trees to trees,but rather maps probability models to probability348models.
This eliminates complications in comparingmodels with different numbers of dependent vari-ables ?
and thus different numbers of free parame-ters ?
because the model which ordinarily has morefree parameters (the HHMM, in this case) is derivedfrom the model that has fewer (the PCFG).
Sincethey are derived from a simpler underlying model,the additional parameters of the HHMM are not free.Mapping probability models from one format toanother can be thought of as mapping the infinitesets of trees that are defined by these models fromone format to another.
Probabilities in the trans-formed model are therefore defined by calculatingprobabilities for the relevant substructures in thesource model, then marginalizing out the values ofnodes in these structures that do not appear in thedesired expression in the target model.A bounded-stack HHMM ?Q,F can therefore bederived from an unbounded PCFG ?G by:1. organizing the rules in the source PCFGmodel ?G into direction-specific versions (dis-tinguishing rules for expanding left and rightchildren, which occur respectively as active andawaited constituent categories in incompleteconstituent labels);2. enforcing depth limits on these direction-specific rules; and3.
mapping these probabilities to HHMM randomvariable positions at the appropriate depth.4.1 Direction-specific rulesAn inspection of the tree-based right-corner trans-form rewrites defined in Section 3.2 will show twothings: first, that constituents occurring as left chil-dren in an original tree (with addresses ending in?0?)
always become active constituents (occurringbefore the slash, or without a slash) in incompleteconstituent categories, and constituents occurring asright children in an original tree (with addresses end-ing in ?1?)
always become awaited constituents (oc-curring after the slash); and second, that left chil-dren expand locally downward in the transformedtree (so each A??0/...
locally dominates A??0?0/...
),whereas right children expand locally upward (soeach .../A?
?1 is locally dominated by .../A?
?1?1).This means that rules from the original grammar ?if distinguished into rules applying only to left andright children (active and awaited constituents) ?can still be locally modeled following a right-cornertransform.
A transformed tree can be generatedin this way by expanding downward along the ac-tive constituents in a transformed tree, then turningaround and expanding upward to fill in the awaitedconstituents, then turning around again to generatethe active constituents at the next depth level, and soon.4.2 Depth boundsThe locality of the original grammar rules in a right-corner transformed tree allows memory limits on in-complete constituents to be applied directly as depthbounds in the zig-zag generation traversal definedabove.
These depth limits correspond directly to thedepth levels in an HHMM.In the experiments described in Section 5,direction-specific and depth-specific versions of theoriginal grammar rules are implemented in an ordi-nary CKY-style dynamic-programming parser, andcan therefore simply be cut off at a particular depthlevel with no renormalization.But in an HHMM, this will result in label-bias ef-fects, in which expanded constituents may have novalid reduction, forcing the system to define distri-butions for composing constituents that are not com-patible.
For example, if a constituent is expanded atdepth D, and that constituent has no expansions thatcan be completely processed within depth D, it willnot be able to reduce, and will remain incompatiblewith the incomplete constituent above it.
Probabili-ties for depth-bounded rules must therefore be renor-malized to the domain of allowable trees that can begenerated within D depth levels, in order to guaran-tee consistent probabilities for HHMM recognition.This is done by determining the (depth- anddirection-specific) probability P?B-L,d(1 |A?
?0)or P?B-R,d(1 |A?
?1) that a tree generated at eachdepth d and rooted by a left or right child will fitwithin depth D. These probabilities are then esti-mated using an approximate inference algorithm,similar to that used in value iteration (Bellman,1957), which estimates probabilities of infinite treesby exploiting the fact that increasingly longer treescontribute exponentially decreasing probabilitymass (since each non-terminal expansion must349avoid generating a terminal with some probabilityat each step from the top down), so a sum overprobabilities of trees with increasing length k isguaranteed to converge.
The algorithm calculatesprobabilities of trees with increasing length k untilconvergence, or to some arbitrary limit K:P?B-L,d,k(1 |A?
?0) def=?A??1?0,A??1?1P?G(A?
?0  A?
?0?0 A??0?1)?
P?B-L,d,k?1(1 |A??0?0)?
P?B-R,d,k?1(1 |A?
?0?1) (13)P?B-R,d,k(1 |A?
?1) def=?A??1?0,A??1?1P?G(A?
?1  A?
?1?0 A??1?1)?
P?B-L,d+1,k?1(1 |A??1?0)?
P?B-R,d,k?1(1 |A?
?1?1) (14)Normalized probability distributions for depth-bounded expansions ?G-L,d and ?G-R,d can now becalculated using converged ?B-L,d and ?B-R,d esti-mates:P?G-L,d(A?
?0  A?
?0?0 A?
?0?1) def=P?G(A?
?0  A?
?0?0 A??0?1)?
P?B-L,d(1 |A?
?0?0) ?
P?B-R,d(1 |A?
?0?1) (15)P?G-R,d(A?
?1  A?
?1?0 A?
?1?1) def=P?G(A?
?1  A?
?1?0 A??1?1)?
P?B-L,d+1(1 |A?
?1?0) ?
P?B-R,d(1 |A?
?1?1) (16)4.3 HHMM probabilitiesConverting PCFGs to HHMMs requires the calcu-lation of expected frequencies F?G-L*,d(A?
? A???
)of generating symbols A???
in the left-progeny of anonterminal symbol A?
(in other words, of A???
be-ing a left child of A?, or a left child of a left childof A?, etc.).
This is done by summing over sub-trees of increasing length k using the same approx-imate inference technique described in Section 4.2,which guarantees convergence since each subtree ofincreasing length contributes exponentially decreas-ing probability mass to the sum:F?G-L*,d(A?
? A???)
=??k=0F?G-L*,d(A?
k A???)(17)where:F?G-L*,d(A?
k A?
?0k) =?A?
?0k?1 ,A??0k?1?1P?G-L*,d(A?
k?1 A??0k?1)?
P?G-L,d(A?
?0k?1  A?
?0k A?
?0k?1?1) (18)and P?G-L*,d(A?
0 A??)
= [A?
=A??
].A complete HHMM can now be defined us-ing depth-bounded right-corner PCFG probabilities.HHMM probabilities will be defined over syntac-tic states consisting of incomplete constituent cat-egories A?/A??
?.Expansions depend on only the incomplete con-stituent category ../A?
(for any active category ?..?
)at qd?1t :P?Q-Ex,d(a??0??
| ../A?)
=?A??0,A??1P?G-R,d?1(A?
 A?
?0 A??1)?F?G-L*,d(A?
?0 ? a??0??)?A??0,A??1,a??0??P?G-R,d?1(A?
 A?
?0 A??1)?F?G-L*,d(A?
?0 ? a??0??
)(19)Transitions depend on whether an ?active?
or?awaited?
transition was performed at the currentlevel.
If an active transition was performed (wherefdt = 1), the transition depends on only the in-complete constituent category A??0???0/..
(for anyawaited category ?..?)
at qdt?1, and the incompleteconstituent category ../A?
(for any active category?..?)
at qd?1t?1 :P?Q-Tr,d(A??0??/A??0??
?1 |1, A??0??
?0/.., ../A?)
=?A??0,A??1P?G-R,d?1(A?
 A?
?0 A?
?1)?F?G-L*,d (A??0?A??0??
)F?G-L*,d (A?0?A?0?0)?F?G-L*,d (A?00A?0?0)?P?G-L,d(A??0??
 A??0??
?0 A??0???1)?A??0,A??1,A??0??,A??0???1P?G-R,d?1(A?
 A?
?0 A?
?1)?F?G-L*,d (A??0?A??0??
)F?G-L*,d (A?0?A?0?0)?F?G-L*,d (A?00A?0?0)?P?G-L,d(A??0??
 A??0??
?0 A??0??
?1)(20)If an awaited transition was performed (where fdt =0), the transition depends on only the complete con-stituent category A???
?0 at fd+1t , and the incomplete350constituent category A?/A???
at qdt?1:P?Q-Tr,d(A?/A???
?1 |0, A???
?0, A?/A???)
=P?G-R,d(A???
 A???
?0 A????1)?A???
?1 P?G-R,d(A???
 A???
?0 A???
?1)(21)Reduce probabilities depend on the complete con-stituent category at fd+1t , and the incomplete con-stituent category A??0???0/..
(for any awaited cate-gory ?..?)
at qdt?1, and the incomplete constituent cat-egory ../A?
(for any active category ?..?)
at qd?1t?1 .
Ifthe complete constituent category at fd+1t does notmatch the awaited category of qdt?1, the probabilityis [fdt = f0].
If the complete constituent categoryat fd+1t does match the awaited category of qdt?1:P?F-Rd,d(1 |A??0?
?/.., ../A?)
=?A??0,A?
?1 P?G-R,d?1(A?
 A?
?0 A??1)?(F?G-L*,d(A?
?0 ? A??0??)?F?G-L*,d(A?
?0 0 A??0??))?A??0,A?
?1 P?G-R,d?1(A?
 A?
?0 A??1)?F?G-L*,d(A?
?0 ? A??0??)(22)and:P?F-Rd,d(A??0??
|A??0?
?/.., ../A?)
=?A??0,A?
?1 P?G-R,d?1(A?
 A?
?0 A??1)?F?G-L*,d(A?
?0 0 A??0??)?A??0,A?
?1 P?G-R,d?1(A?
 A?
?0 A??1)?F?G-L*,d(A?
?0 ? A??0??
)(23)The correctness of the above distributions can bedemonstrated by the fact that all terms other than?G-L,d and ?G-R,d probabilities will cancel out inany sequence of transitions between an expansionand a reduction, leaving only those terms that wouldappear as factors in an ordinary PCFG parse.55 ResultsA PCFG model was extracted from sections 2?21of the Wall Street Journal Treebank.
In order tokeep the transform process manageable, punctua-tion was removed from the corpus, and rules oc-curring less frequently than 10 times in the corpus5It is important to note, however, that these probabilities arenot necessarily incrementally balanced, so this correctness onlyapplies to parsing with an infinite beam.model (sect 22?24, len>40) Funbounded PCFG 66.03bounded PCFG (D=4) 66.08Table 1: Results of CKY parsing using bounded and un-bounded PCFG.were deleted from the PCFG.
The right-corner andbounded-stack transforms described in the previoussection were then applied to the PCFG.
The origi-nal and bounded PCFG models were evaluated in aCKY recognizer on sections 22?24 of the Treebank,with results shown in Table 1.6 Results were signif-icant only for sentences longer than 40 words.
Onthese sentences, the bounded PCFG model achievesabout a .15% reduction of error over the originalPCFG (p < .1 using one-tailed pairwise t-test).
Thissuggests that on long sentences the probability masswasted due to parsing with an unbounded stack issubstantial enough to impact parsing accuracy.6 ConclusionPrevious work has explored bounded-stack parsingusing a right-corner transform defined on trees tominimize stack usage.
HHMM parsers trained onapplications of this tree-based transform of train-ing corpora have shown improvements over ordinaryPCFG models, but this may have been attributable tothe richer dependencies of the HHMM.This paper has presented an approximate in-ference algorithm for transforming entire PCFGs,rather than individual trees, into equivalent right-corner bounded-stack HHMMs.
Moreover, a com-parison with an untransformed PCFG model sug-gests that the probability mass wasted due to pars-ing with an unbounded stack is substantial enoughto impact parsing accuracy.AcknowledgmentsThis research was supported by NSF CAREERaward 0447685 and by NASA under awardNNX08AC36A.
The views expressed are not nec-essarily endorsed by the sponsors.6A CKY recognizer was used in both cases in order to avoidintroducing errors due to model approximation or beam limitsnecessary for incremental processing with large grammars.351ReferencesSteven P. Abney and Mark Johnson.
1991.
Memory re-quirements and local ambiguities of parsing strategies.J.
Psycholinguistic Research, 20(3):233?250.Richard Bellman.
1957.
Dynamic Programming.Princeton University Press, Princeton, NJ.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Computa-tional Linguistics (ACL ?97).Nelson Cowan.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storage ca-pacity.
Behavioral and Brain Sciences, 24:87?185.Edward Gibson.
1991.
A computational theory of hu-man linguistic processing: Memory limitations andprocessing breakdown.
Ph.D. thesis, Carnegie Mellon.James Henderson.
2004.
Lookahead in deterministicleft-corner parsing.
In Proc.
Workshop on Incremen-tal Parsing: Bringing Engineering and Cognition To-gether, Barcelona, Spain.Mark Johnson.
1998.
Finite state approximation ofconstraint-based grammars using left-corner grammartransforms.
In Proceedings of COLING/ACL, pages619?623.Tim Miller and William Schuler.
2008.
A syntactic time-series model for parsing fluent and disfluent speech.
InProceedings of the 22nd International Conference onComputational Linguistics (COLING?08).George A. Miller.
1956.
The magical number seven, plusor minus two: Some limits on our capacity for process-ing information.
Psychological Review, 63:81?97.Kevin P. Murphy and Mark A. Paskin.
2001.
Linear timeinference in hierarchical HMMs.
In Proc.
NIPS, pages833?840.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.William Schuler, Samir AbdelRahman, Tim Miller, andLane Schwartz.
2008.
Toward a psycholinguistically-motivated model of language.
In Proceedings of COL-ING, Manchester, UK, August.Mark Steedman.
2000.
The syntactic process.
MITPress/Bradford Books, Cambridge, MA.352
