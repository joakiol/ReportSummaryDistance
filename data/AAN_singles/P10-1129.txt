Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1268?1277,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsReading Between the Lines:Learning to Map High-level Instructions to CommandsS.R.K.
Branavan, Luke S. Zettlemoyer, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{branavan, lsz, regina}@csail.mit.eduAbstractIn this paper, we address the task ofmapping high-level instructions to se-quences of commands in an external en-vironment.
Processing these instructionsis challenging?they posit goals to beachieved without specifying the steps re-quired to complete them.
We describea method that fills in missing informa-tion using an automatically derived envi-ronment model that encodes states, tran-sitions, and commands that cause thesetransitions to happen.
We present an ef-ficient approximate approach for learningthis environment model as part of a policy-gradient reinforcement learning algorithmfor text interpretation.
This design enableslearning for mapping high-level instruc-tions, which previous statistical methodscannot handle.11 IntroductionIn this paper, we introduce a novel method formapping high-level instructions to commands inan external environment.
These instructions spec-ify goals to be achieved without explicitly stat-ing all the required steps.
For example, considerthe first instruction in Figure 1 ?
?open controlpanel.?
The three GUI commands required for itssuccessful execution are not explicitly describedin the text, and need to be inferred by the user.This dependence on domain knowledge makes theautomatic interpretation of high-level instructionsparticularly challenging.The standard approach to this task is to startwith both a manually-developed model of the en-vironment, and rules for interpreting high-level in-structions in the context of this model (Agre and1Code, data, and annotations used in this work are avail-able at http://groups.csail.mit.edu/rbg/code/rl-hli/Chapman, 1988; Di Eugenio and White, 1992;Di Eugenio, 1992; Webber et al, 1995).
Givenboth the model and the rules, logic-based infer-ence is used to automatically fill in the intermedi-ate steps missing from the original instructions.Our approach, in contrast, operates directly onthe textual instructions in the context of the in-teractive environment, while requiring no addi-tional information.
By interacting with the en-vironment and observing the resulting feedback,our method automatically learns both the mappingbetween the text and the commands, and the un-derlying model of the environment.
One partic-ularly noteworthy aspect of our solution is the in-terplay between the evolving mapping and the pro-gressively acquired environment model as the sys-tem learns how to interpret the text.
Recording thestate transitions observed during interpretation al-lows the algorithm to construct a relevant modelof the environment.
At the same time, the envi-ronment model enables the algorithm to considerthe consequences of commands before they are ex-ecuted, thereby improving the accuracy of inter-pretation.
Our method efficiently achieves both ofthese goals as part of a policy-gradient reinforce-ment learning algorithm.We apply our method to the task of mappingsoftware troubleshooting guides to GUI actions inthe Windows environment (Branavan et al, 2009;Kushman et al, 2009).
The key findings of ourexperiments are threefold.
First, the algorithmcan accurately interpret 61.5% of high-level in-structions, which cannot be handled by previousstatistical systems.
Second, we demonstrate thatexplicitly modeling the environment also greatlyimproves the accuracy of processing low-level in-structions, yielding a 14% absolute increase inperformance over a competitive baseline (Brana-van et al, 2009).
Finally, we show the importanceof constructing an environment model relevant tothe language interpretation task ?
using textual1268"open control panel, double click system, then go to the advanced tab"Document (input)D"open control panel"left-click Advanceddouble-click Systemleft-click Control Panelleft-click Settingsleft-click StartInstructionsDDocDumenemto (ip)Iiosmsrumenemto (ip)Iios (Command Sequence (output)D: :::::"double click system""go to the advanced tab"::Figure 1: An example mapping of a document containing high-level instructions into a candidate se-quence of five commands.
The mapping process involves segmenting the document into individual in-struction word spans Wa, and translating each instruction into the sequence ~c of one or more commandsit describes.
During learning, the correct output command sequence is not provided to the algorithm.instructions enables us to bias exploration towardtransitions relevant for language learning.
This ap-proach yields superior performance compared to apolicy that relies on an environment model con-structed via random exploration.2 Related WorkInterpreting Instructions Our approach is mostclosely related to the reinforcement learning algo-rithm for mapping text instructions to commandsdeveloped by Branavan et al (2009) (see Section 4for more detail).
Their method is predicated on theassumption that each command to be executed isexplicitly specified in the instruction text.
This as-sumption of a direct correspondence between thetext and the environment is not unique to that pa-per, being inherent in other work on grounded lan-guage learning (Siskind, 2001; Oates, 2001; Yuand Ballard, 2004; Fleischman and Roy, 2005;Mooney, 2008; Liang et al, 2009; Matuszek etal., 2010).
A notable exception is the approachof Eisenstein et al (2009), which learns how anenvironment operates by reading text, rather thanlearning an explicit mapping from the text to theenvironment.
For example, their method can learnthe rules of a card game given instructions for howto play.Many instances of work on instruction inter-pretation are replete with examples where in-structions are formulated as high-level goals, tar-geted at users with relevant knowledge (Winograd,1972; Di Eugenio, 1992; Webber et al, 1995;MacMahon et al, 2006).
Not surprisingly, auto-matic approaches for processing such instructionshave relied on hand-engineered world knowledgeto reason about the preconditions and effects ofenvironment commands.
The assumption of afully specified environment model is also com-mon in work on semantics in the linguistics lit-erature (Lascarides and Asher, 2004).
While ourapproach learns to analyze instructions in a goal-directed manner, it does not require manual speci-fication of relevant environment knowledge.Reinforcement Learning Our work combinesideas of two traditionally disparate approaches toreinforcement learning (Sutton and Barto, 1998).The first approach, model-based learning, con-structs a model of the environment in which thelearner operates (e.g., modeling location, velocity,and acceleration in robot navigation).
It then com-putes a policy directly from the rich informationrepresented in the induced environment model.In the NLP literature, model-based reinforcementlearning techniques are commonly used for dia-log management (Singh et al, 2002; Lemon andKonstas, 2009; Schatzmann and Young, 2009).However, if the environment cannot be accuratelyapproximated by a compact representation, thesemethods perform poorly (Boyan and Moore, 1995;Jong and Stone, 2007).
Our instruction interpreta-tion task falls into this latter category,2 renderingstandard model-based learning ineffective.The second approach ?
model-free methodssuch as policy learning ?
aims to select the opti-2For example, in the Windows GUI domain, clicking onthe File menu will result in a different submenu depending onthe application.
Thus it is impossible to predict the effects ofa previously unseen GUI command.1269left-cikdo-uteo-ft-btoiuueioiiiiiiLEFT_CLICK"ooooooopstart-eoiiiiuuioioteoouf-uidoikui-ft-btoiuuoiuieoieuci-e-okuuiuuioioteoouf-uidoikui-ft-btoiuuoiuieoieuci-e-okuu -uteoFigure 2: A single step in the instruction mapping process formalized as an MDP.
State s is comprised ofthe state of the external environment E , and the state of the document (d,W ), where W is the list of allword spans mapped by previous actions.
An action a selects a span Wa of unused words from (d,W ),and maps them to an environment command c. As a consequence of a, the environment state changes toE ?
?
p(E ?|E , c), and the list of mapped words is updated to W ?
= W ?Wa.mal action at every step, without explicitly con-structing a model of the environment.
While pol-icy learners can effectively operate in complex en-vironments, they are not designed to benefit froma learned environment model.
We address thislimitation by expanding a policy learning algo-rithm to take advantage of a partial environmentmodel estimated during learning.
The approach ofconditioning the policy function on future reach-able states is similar in concept to the use of post-decision state information in the approximate dy-namic programming framework (Powell, 2007).3 Problem FormulationOur goal is to map instructions expressed in a nat-ural language document d into the correspondingsequence of commands ~c = ?c1, .
.
.
, cm?
exe-cutable in an environment.
As input, we are givena set of raw instruction documents, an environ-ment, and a reward function as described below.The environment is formalized as its states andtransition function.
An environment state E spec-ifies the objects accessible in the environment ata given time step, along with the objects?
prop-erties.
The environment state transition functionp(E ?|E , c) encodes how the state changes from Eto E ?
in response to a command c.3 During learn-ing, this function is not known, but samples from itcan be collected by executing commands and ob-3While in the general case the environment state transi-tions maybe stochastic, they are deterministic in the softwareGUI used in this work.serving the resulting environment state.
A real-valued reward function measures how well a com-mand sequence~c achieves the task described in thedocument.We posit that a document d is composed of asequence of instructions, each of which can takeone of two forms:?
Low-level instructions: these explicitly de-scribe single commands.4 E.g., ?double clicksystem?
in Figure 1.?
High-level instructions: these correspond toa sequence of one or more environment com-mands, none of which are explicitly de-scribed by the instruction.
E.g., ?open controlpanel?
in Figure 1.4 BackgroundOur innovation takes place within a previouslyestablished general framework for the task ofmapping instructions to commands (Branavanet al, 2009).
This framework formalizes themapping process as a Markov Decision Process(MDP) (Sutton and Barto, 1998), with actionsencoding individual instruction-to-command map-pings, and states representing partial interpreta-tions of the document.
In this section, we reviewthe details of this framework.4Previous work (Branavan et al, 2009) is only able to han-dle low-level instructions.1270startr	tstatartst		r	tstat	sa	r	aa	at	rasa stat		r	astart	aass	rrrsratstsFigure 3: Using information derived from future states to interpret the high-level instruction ?open con-trol panel.?
Ed is the starting state, and c1 through c4 are candidate commands.
Environment states areshown as circles, with previously visited environment states colored green.
Dotted arrows show knownstate transitions.
All else being equal, the information that the control panel icon was observed in stateE5 during previous exploration steps can help to correctly select command c3.States and Actions A document is interpretedby incrementally constructing a sequence of ac-tions.
Each action selects a word span from thedocument, and maps it to one environment com-mand.
To predict actions sequentially, we track thestates of the environment and the document overtime as shown in Figure 2.
This mapping state s isa tuple (E , d,W ) where E is the current environ-ment state, d is the document being interpreted,and W is the list of word spans selected by previ-ous actions.
The mapping state s is observed priorto selecting each action.The mapping action a is a tuple (c,Wa) thatrepresents the joint selection of a span of wordsWa and an environment command c. Some of thecandidate actions would correspond to the correctinstruction mappings, e.g., (c = double-click sys-tem, Wa = ?double click system?).
Others suchas (c = left-click system, Wa = ?double click sys-tem?)
would be erroneous.
The algorithm learnsto interpret instructions by learning to constructsequences of actions that assign the correct com-mands to the words.The interpretation of a document d begins at aninitial mapping state s0 = (Ed, d, ?
), Ed being thestarting state of the environment for the document.Given a state s = (E , d,W ), the space of possi-ble actions a = (c,Wa) is defined by enumerat-ing sub-spans of unused words in d and candidatecommands in E .5 The action to execute, a, is se-lected based on a policy function p(a|s) by find-ing argmaxa p(a|s).
Performing action a in state5Here, command reordering is possible.
At each step, thespan of selected words Wa is not required to be adjacent tothe previous selections.
This reordering is used to interpretsentences such as ?Select exit after opening the File menu.
?s = (E , d,W ) results in a new state s?
accordingto the distribution p(s?|s, a), where:a = (c,Wa),E ?
?
p(E ?|E , c),W ?
= W ?Wa,s?
= (E ?, d,W ?
).The process of selecting and executing actionsis repeated until all the words in d have beenmapped.6A Log-Linear Parameterization The policyfunction used for action selection is defined as alog-linear distribution over actions:p(a|s; ?)
=e???(s,a)?a?e???(s,a?
), (1)where ?
?
Rn is a weight vector, and ?
(s, a) ?
Rnis an n-dimensional feature function.
This repre-sentation has the flexibility to incorporate a varietyof features computed on the states and actions.Reinforcement Learning Parameters of thepolicy function p(a|s; ?)
are estimated to max-imize the expected future reward for analyzingeach document d ?
D:?
= argmax?Ep(h|?)
[r(h)] , (2)where h = (s0, a0, .
.
.
, sm?1, am?1, sm) is ahistory that records the analysis of document d,p(h|?)
is the probability of selecting this analysisgiven policy parameters ?, and the reward r(h) isa real valued indication of the quality of h.6To account for document words that are not part of aninstruction, c may be a null command.12715 AlgorithmWe expand the scope of learning approaches forautomatic document interpretation by enabling theanalysis of high-level instructions.
The main chal-lenge in processing these instructions is that, incontrast to their low-level counterparts, they cor-respond to sequences of one or more commands.A simple way to enable this one-to-many mappingis to allow actions that do not consume words (i.e.,|Wa| = 0).
The sequence of actions can then beconstructed incrementally using the algorithm de-scribed above.
However, this change significantlycomplicates the interpretation problem ?
we needto be able to predict commands that are not di-rectly described by any words, and allowing ac-tion sequences significantly increases the space ofpossibilities for each instruction.
Since we can-not enumerate all possible sequences at decisiontime, we limit the space of possibilities by learn-ing which sequences are likely to be relevant forthe current instruction.To motivate the approach, consider the deci-sion problem in Figure 3, where we need to find acommand sequence for the high-level instruction?open control panel.?
The algorithm focuses oncommand sequences leading to environment stateswhere the control panel icon was previously ob-served.
The information about such states is ac-quired during exploration and is stored in a partialenvironment model q(E ?|E , c).Our goal is to map high-level instructions tocommand sequences by leveraging knowledgeabout the long-term effects of commands.
We dothis by integrating the partial environment modelinto the policy function.
Specifically, we modifythe log-linear policy p(a|s; q, ?)
by adding look-ahead features ?
(s, a, q) which complement thelocal features used in the previous model.
Theselook-ahead features incorporate various measure-ments that characterize the potential of futurestates reachable via the selected action.
Althoughprimarily designed to analyze high-level instruc-tions, this approach is also useful for mappinglow-level instructions.Below, we first describe how we estimate thepartial environment transition model and how thismodel is used to compute the look-ahead features.This is followed by the details of parameter esti-mation for our algorithm.5.1 Partial Environment Transition ModelTo compute the look-ahead features, we first needto collect statistics about the environment transi-tion function p(E ?|E , c).
An example of an envi-ronment transition is the change caused by click-ing on the ?start?
button.
We collect this informa-tion through observation, and build a partial envi-ronment transition model q(E ?|E , c).One possible strategy for constructing q is to ob-serve the effects of executing random commandsin the environment.
In a complex environment,however, such a strategy is unlikely to producestate samples relevant to our text analysis task.Instead, we use the training documents to guidethe sampling process.
During training, we executethe command sequences predicted by the policyfunction in the environment, caching the resultingstate transitions.
Initially, these commands mayhave little connection to the actual instructions.
Aslearning progresses and the quality of the interpre-tation improves, more promising parts of the en-vironment will be observed.
This process yieldssamples that are biased toward the content of thedocuments.5.2 Look-Ahead FeaturesWe wish to select actions that allow for the bestfollow-up actions, thereby finding the analysiswith the highest total reward for a given docu-ment.
In practice, however, we do not have in-formation about the effects of all possible futureactions.
Instead, we capitalize on the state tran-sitions observed during the sampling process de-scribed above, allowing us to incrementally buildan environment model of actions and their effects.Based on this transition information, we can es-timate the usefulness of actions by considering theproperties of states they can reach.
For instance,some states might have very low immediate re-ward, indicating that they are unlikely to be partof the best analysis for the document.
While theusefulness of most states is hard to determine, itcorrelates with various properties of the state.
Weencode the following properties as look-ahead fea-tures in our policy:?
The highest reward achievable by an actionsequence passing through this state.
Thisproperty is computed using the learned envi-ronment model, and is therefore an approxi-mation.1272?
The length of the above action sequence.?
The average reward received at the envi-ronment state while interpreting any docu-ment.
This property introduces a bias towardscommonly visited states that frequently re-cur throughout multiple documents?
correctinterpretations.Because we can never encounter all states andall actions, our environment model is always in-complete and these properties can only be com-puted based on partial information.
Moreover, thepredictive strength of the properties is not knownin advance.
Therefore we incorporate them as sep-arate features in the model, and allow the learningprocess to estimate their weights.
In particular, weselect actions a based on the current state s andthe partial environment model q, resulting in thefollowing policy definition:p(a|s; q, ?)
=e???(s,a,q)?a?e???
(s,a?,q), (3)where the feature representation ?
(s, a, q) hasbeen extended to be a function of q.5.3 Parameter EstimationThe learning algorithm is provided with a set ofdocuments d ?
D, an environment in which to ex-ecute command sequences ~c, and a reward func-tion r(h).
The goal is to estimate two sets ofparameters: 1) the parameters ?
of the policyfunction, and 2) the partial environment transitionmodel q(E ?|E , c), which is the observed portion ofthe true model p(E ?|E , c).
These parameters aremutually dependent: ?
is defined over a featurespace dependent on q, and q is sampled accordingto the policy function parameterized by ?.Algorithm 1 shows the procedure for jointlearning of these parameters.
As in standard policygradient learning (Sutton et al, 2000), the algo-rithm iterates over all documents d ?
D (steps 1,2), selecting and executing actions in the environ-ment (steps 3 to 6).
The resulting reward is usedto update the parameters ?
(steps 8, 9).
In the newjoint learning setting, this process also yields sam-ples of state transitions which are used to estimateq(E ?|E , c) (step 7).
This updated q is then usedto compute the feature functions ?
(s, a, q) duringthe next iteration of learning (step 4).
This pro-cess is repeated until the total reward on trainingdocuments converges.Input: A document set D,Feature function ?,Reward function r(h),Number of iterations TInitialization: Set ?
to small random values.Set q to the empty set.for i = 1 ?
?
?T do1foreach d ?
D do2Sample history h ?
p(h|?)
whereh = (s0, a0, ?
?
?
, an?1, sn) as follows:Initialize environment to document specificstarting state Edfor t = 0 ?
?
?n?
1 do3Compute ?
(a, st, q) based on latest q4Sample action at ?
p(a|st; q, ?
)5Execute at on state st: st+1 ?
p(s|st, at)6Set q = q ?
{(E ?, E , c)} where E ?, E , c are the7environment states and commands from st+1,st, and atend??8?t[?
(st, at, q)??a??
(st, a?, q) p(a?|st; q, ?)]?
?
?
+ r(h)?9endendOutput: Estimate of parameters ?Algorithm 1: A policy gradient algorithm thatalso learns a model of the environment.This algorithm capitalizes on the synergy be-tween ?
and q.
As learning proceeds, the methoddiscovers a more complete state transition functionq, which improves the accuracy of the look-aheadfeatures, and ultimately, the quality of the result-ing policy.
An improved policy function in turnproduces state samples that are more relevant tothe document interpretation task.6 Applying the ModelWe apply our algorithm to the task of interpret-ing help documents to perform software relatedtasks (Branavan et al, 2009; Kushman et al,2009).
Specifically, we consider documents fromMicrosoft?s Help and Support website.7 As inprior work, we use a virtual machine set-up to al-low our method to interact with a Windows 2000environment.Environment States and Actions In this appli-cation of our model, the environment state is theset of visible user interface (UI) objects, along7http://support.microsoft.com/1273with their properties (e.g., the object?s label, par-ent window, etc).
The environment commandsconsist of the UI commands left-click , right-click ,double-click , and type-into.
Each of these commandsrequires a UI object as a parameter, while type-intoneeds an additional parameter containing the textto be typed.
On average, at each step of the in-terpretation process, the branching factor is 27.14commands.Reward Function An ideal reward functionwould be to verify whether the task specified bythe help document was correctly completed.
Sincesuch verification is a challenging task, we rely ona noisy approximation: we assume that each sen-tence specifies at least one command, and that thetext describing the command has words matchingthe label of the environment object.
If a historyh has at least one such command for each sen-tence, the environment reward function r(h) re-turns a positive value, otherwise it returns a neg-ative value.
This environment reward function isa simplification of the one described in Branavanet al (2009), and it performs comparably in ourexperiments.Features In addition to the look-ahead featuresdescribed in Section 5.2, the policy also includesthe set of features used by Branavan et al (2009).These features are functions of both the text andenvironment state, modeling local properties thatare useful for action selection.7 Experimental SetupDatasets Our model is trained on the samedataset used by Branavan et al (2009).
For test-ing we use two datasets: the first one was usedin prior work and contains only low-level instruc-tions, while the second dataset is comprised ofdocuments with high-level instructions.
This newdataset was collected from the Microsoft Helpand Support website, and has on average 1.03high-level instructions per document.
The seconddataset contains 60 test documents, while the firstis split into 70, 18 and 40 document for training,development and testing respectively.
The com-bined statistics for these datasets is shown below:Total # of documents 188Total # of words 7448Vocabulary size 739Avg.
actions per document 10Reinforcement Learning Parameters Follow-ing common practice, we encourage explorationduring learning with an -greedy strategy (Suttonand Barto, 1998), with  set to 0.1.
We also iden-tify dead-end states, i.e.
states with the lowest pos-sible immediate reward, and use the induced en-vironment model to encourage additional explo-ration by lowering the likelihood of actions thatlead to such dead-end states.During the early stages of learning, experiencegathered in the environment model is extremelysparse, causing the look-ahead features to providepoor estimates.
To speed convergence, we ignorethese estimates by disabling the look-ahead fea-tures for a fixed number of initial training itera-tions.Finally, to guarantee convergence, stochas-tic gradient ascent algorithms require a learningrate schedule.
We use a modified search-then-converge algorithm (Darken and Moody, 1990),and tie the learning rate to the ratio of trainingdocuments that received a positive reward in thecurrent iteration.Baselines As a baseline, we compare ourmethod against the results reported by Branavanet al (2009), denoted here as BCZB09.As an upper bound for model performance, wealso evaluate our method using a reward signalthat simulates a fully-supervised training regime.We define a reward function that returns posi-tive one for histories that match the annotations,and zero otherwise.
Performing policy-gradientwith this function is equivalent to training a fully-supervised, stochastic gradient algorithm that op-timizes conditional likelihood (Branavan et al,2009).Evaluation Metrics We evaluate the accuracyof the generated mapping by comparing it againstmanual annotations of the correct action se-quences.
We measure the percentage of correctactions and the percentage of documents whereevery action is correct.
In general, the sequentialnature of the interpretation task makes it difficultto achieve high action accuracy.
For example, ex-ecuting an incorrect action early on, often leadsto an environment state from which the remaininginstructions cannot be completed.
When this hap-pens, it is not possible to recover the remainingactions, causing cascading errors that significantlyreduce performance.1274Low-level instruction dataset High-level instruction datasetaction document action high-level action documentBCZB09 0.647 0.375 0.021 0.022 0.000BCZB09 + annotation ?
0.756 0.525 0.035 0.022 0.000Our model 0.793 0.517 ?
0.419 ?
0.615 ?
0.283Our model + annotation 0.793 0.650 ?
0.357 0.492 0.333Table 1: Accuracy of the mapping produced by our model, its variants, and the baseline.
Values markedwith ?
are statistically significant at p < 0.01 compared to the value immediately above it.8 ResultsAs shown in Table 1, our model outperformsthe baseline on the two datasets, according toall evaluation metrics.
In contrast to the base-line, our model can handle high-level instructions,accurately interpreting 62% of them in the sec-ond dataset.
Every document in this set con-tains at least one high-level action, which on av-erage, maps to 3.11 environment commands each.The overall action performance on this dataset,however, seems unexpectedly low at 42%.
Thisdiscrepancy is explained by the fact that in thisdataset, high-level instructions are often locatedtowards the beginning of the document.
If theseinitial challenging instructions are not processedcorrectly, the rest of the actions for the documentcannot be interpreted.As the performance on the first dataset indi-cates, the new algorithm is also beneficial for pro-cessing low-level instructions.
The model outper-forms the baseline by at least 14%, both in termsof the actions and the documents it can process.Not surprisingly, the best performance is achievedwhen the new algorithm has access to manuallyannotated data during training.We also performed experiments to validate theintuition that the partial environment model mustcontain information relevant for the language in-terpretation task.
To test this hypothesis, we re-placed the learned environment model with one ofthe same size gathered by executing random com-mands.
The model with randomly sampled envi-ronment transitions performs poorly: it can onlyprocess 4.6% of documents and 15% of actionson the dataset with high-level instructions, com-pared to 28.3% and 41.9% respectively for our al-gorithm.
This result also explains why trainingwith full supervision hurts performance on high-level instructions (see Table 1).
Learning directlyfrom annotations results in a low-quality environ-ment model due to the relative lack of exploration,High-level instructionstartdevice managertExtracted low-level instruction paraphrasestattmy computerstattcontrol panelstattadministrative toolsstattcomputer managementstattdevice managerHigh-level instructionstarttnetwork toolttcontrol paneltExtracted low-level instruction paraphrasesttstartstratatsettingssttcontrol panelstattnetwork and dial-up connectionsFigure 4: Examples of automatically generatedparaphrases for high-level instructions.
The modelmaps the high-level instruction into a sequence ofcommands, and then translates them into the cor-responding low-level instructions.hurting the model?s ability to leverage the look-ahead features.Finally, to demonstrate the quality of thelearned word?command alignments, we evaluateour method?s ability to paraphrase from high-levelinstructions to low-level instructions.
Here, thegoal is to take each high-level instruction and con-struct a text description of the steps required toachieve it.
We did this by finding high-level in-structions where each of the commands they areassociated with is also described by a low-levelinstruction in some other document.
For exam-ple, if the text ?open control panel?
was mappedto the three commands in Figure 1, and each ofthose commands was described by a low-level in-struction elsewhere, this procedure would createa paraphrase such as ?click start, left click set-ting, and select control panel.?
Of the 60 high-level instructions tagged in the test set, this ap-proach found paraphrases for 33 of them.
29 of1275these paraphrases were correct, in the sense thatthey describe all the necessary commands.
Fig-ure 4 shows some examples of the automaticallyextracted paraphrases.9 Conclusions and Future WorkIn this paper, we demonstrate that knowledgeabout the environment can be learned and used ef-fectively for the task of mapping instructions to ac-tions.
A key feature of this approach is the synergybetween language analysis and the construction ofthe environment model: instruction text drives thesampling of the environment transitions, while theacquired environment model facilitates languageinterpretation.
This design enables us to learn tomap high-level instructions while also improvingaccuracy on low-level instructions.To apply the above method to process a broadrange of natural language documents, we need tohandle several important semantic and pragmaticphenomena, such as reference, quantification, andconditional statements.
These linguistic construc-tions are known to be challenging to learn ?
exist-ing approaches commonly rely on large amountsof hand annotated data for training.
An interest-ing avenue of future work is to explore an alter-native approach which learns these phenomena bycombining linguistic information with knowledgegleaned from an automatically induced environ-ment model.AcknowledgmentsThe authors acknowledge the support of theNSF (CAREER grant IIS-0448168, grant IIS-0835445, and grant IIS-0835652) and the Mi-crosoft Research New Faculty Fellowship.
Thanksto Aria Haghighi, Leslie Pack Kaelbling, TomKwiatkowski, Martin Rinard, David Silver, MarkSteedman, Csaba Szepesvari, the MIT NLP group,and the ACL reviewers for their suggestions andcomments.
Any opinions, findings, conclusions,or recommendations expressed in this paper arethose of the authors, and do not necessarily reflectthe views of the funding organizations.ReferencesPhilip E. Agre and David Chapman.
1988.
What areplans for?
Technical report, Cambridge, MA, USA.J.
A. Boyan and A. W. Moore.
1995.
Generalizationin reinforcement learning: Safely approximating thevalue function.
In Advances in NIPS, pages 369?376.S.R.K Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofACL, pages 82?90.Christian Darken and John Moody.
1990.
Note onlearning rate schedules for stochastic optimization.In Advances in NIPS, pages 832?838.Barbara Di Eugenio and Michael White.
1992.
On theinterpretation of natural language instructions.
InProceedings of COLING, pages 1147?1151.Barbara Di Eugenio.
1992.
Understanding natural lan-guage instructions: the case of purpose clauses.
InProceedings of ACL, pages 120?127.Jacob Eisenstein, James Clarke, Dan Goldwasser, andDan Roth.
2009.
Reading to learn: Constructingfeatures from semantic abstracts.
In Proceedings ofEMNLP, pages 958?967.Michael Fleischman and Deb Roy.
2005.
Intentionalcontext in situated natural language learning.
InProceedings of CoNLL, pages 104?111.Nicholas K. Jong and Peter Stone.
2007.
Model-basedfunction approximation in reinforcement learning.In Proceedings of AAMAS, pages 670?677.Nate Kushman, Micah Brodsky, S.R.K.
Branavan,Dina Katabi, Regina Barzilay, and Martin Rinard.2009.
Wikido.
In Proceedings of HotNets-VIII.Alex Lascarides and Nicholas Asher.
2004.
Impera-tives in dialogue.
In P. Kuehnlein, H. Rieser, andH.
Zeevat, editors, The Semantics and Pragmaticsof Dialogue for the New Millenium.
Benjamins.Oliver Lemon and Ioannis Konstas.
2009.
User sim-ulations for context-sensitive speech recognition inspoken dialogue systems.
In Proceedings of EACL,pages 505?513.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less super-vision.
In Proceedings of ACL, pages 91?99.Matt MacMahon, Brian Stankiewicz, and BenjaminKuipers.
2006.
Walk the talk: connecting language,knowledge, and action in route instructions.
In Pro-ceedings of AAAI, pages 1475?1482.C.
Matuszek, D. Fox, and K. Koscher.
2010.
Follow-ing directions using statistical machine translation.In Proceedings of Human-Robot Interaction, pages251?258.Raymond J. Mooney.
2008.
Learning to connectlanguage and perception.
In Proceedings of AAAI,pages 1598?1601.1276James Timothy Oates.
2001.
Grounding knowledgein sensors: Unsupervised learning for language andplanning.
Ph.D. thesis, University of MassachusettsAmherst.Warren B Powell.
2007.
Approximate Dynamic Pro-gramming.
Wiley-Interscience.Jost Schatzmann and Steve Young.
2009.
The hiddenagenda user simulation model.
IEEE Trans.
Audio,Speech and Language Processing, 17(4):733?747.Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker.
2002.
Optimizing dialogue man-agement with reinforcement learning: Experimentswith the njfun system.
Journal of Artificial Intelli-gence Research, 16:105?133.Jeffrey Mark Siskind.
2001.
Grounding the lexicalsemantics of verbs in visual perception using forcedynamics and event logic.
Journal of Artificial In-telligence Research, 15:31?90.Richard S. Sutton and Andrew G. Barto.
1998.
Re-inforcement Learning: An Introduction.
The MITPress.Richard S. Sutton, David McAllester, Satinder Singh,and Yishay Mansour.
2000.
Policy gradient meth-ods for reinforcement learning with function approx-imation.
In Advances in NIPS, pages 1057?1063.Bonnie Webber, Norman Badler, Barbara Di Euge-nio, Libby Levison Chris Geib, and Michael Moore.1995.
Instructions, intentions and expectations.
Ar-tificial Intelligence, 73(1-2).Terry Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press.Chen Yu and Dana H. Ballard.
2004.
On the integra-tion of grounding language and learning objects.
InProceedings of AAAI, pages 488?493.1277
