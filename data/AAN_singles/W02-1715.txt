SALT: An XML Application for Web-based Multimodal DialogManagementKuansan WangSpeech Technology Group, Microsoft ResearchOne Microsoft Way, Microsoft CorporationRedmond, WA, 98006, USAhttp://research.microsoft.com/stgAbstractThis paper describes the SpeechApplication Language Tags, or SALT, anXML based spoken dialog standard formultimodal or speech-only applications.
Akey premise in SALT design is thatspeech-enabled user interface shares a lotof the design principles and computationalrequirements with the graphical userinterface (GUI).
As a result, it is logical tointroduce into speech the object-oriented,event-driven model that is known to beflexible and powerful enough in meetingthe requirements for realizingsophisticated GUIs.
By reusing this richinfrastructure, dialog designers arerelieved from having to develop theunderlying computing infrastructure andcan focus more on the core user interfacedesign issues than on the computer andsoftware engineering details.
The paperfocuses the discussion on the Web-baseddistributed computing environment andelaborates how SALT can be used toimplement multimodal dialog systems.How advanced dialog effects (e.g.,cross-modality reference resolution,implicit confirmation, multimediasynchronization) can be realized in SALTis also discussed.IntroductionMultimodal interface allows a human user tointeraction with the computer using more thanone input methods.
GUI, for example, ismultimodal because a user can interact with thecomputer using keyboard, stylus, or pointingdevices.
GUI is an immensely successful concept,notably demonstrated by the World Wide Web.Although the relevant technologies for theInternet had long existed, it was not until theadoption of GUI for the Web did we witness asurge on its usage and rapid improvements inWeb applications.GUI applications have to address the issuescommonly encountered in a goal-oriented dialogsystem.
In other words, GUI applications can beviewed as conducting a dialog with its user in aniconic language.
For example, it is very commonfor an application and its human user to undergomany exchanges before a task is completed.
Theapplication therefore must manage the interactionhistory in order to properly infer user?s intention.The interaction style is mostly system initiativebecause the user often has to follow theprescribed interaction flow where allowablebranches are visualized in graphical icons.
Manyapplications have introduced mixed initiativefeatures such as type-in help or search box.However, user-initiated digressions are oftenrecoverable only if they are anticipated by theapplication designers.
The plan-based dialogtheory (Sadek et al1997, Allen 1995, Cohen et al1989) suggests that, in order for the mixedinitiative dialog to function properly, thecomputer and the user should be collaboratingpartners that actively assist each other in planningthe dialog flow.
An application will be perceivedas hard to use if the flow logic is obscure orunnatural to the user and, similarly, the user willfeel frustrated if the methods to express intentsare too limited.
It is widely believed that spokenlanguage can improve the user interface as itprovides the user a natural and less restrictiveway to express intents and receive feedbacks.The Speech Application Language Tags (SALT2002) is a proposed standard for implementingspoken language interfaces.
The core of SALT isa collection of objects that enable a softwareprogram to listen, speak, and communicate withother components residing on the underlyingplatform (e.g., discourse manager, other inputmodalities, telephone interface, etc.).
Like theirpredecessors in the Microsoft SpeechApplication Interface (SAPI), SALT objects areprogramming language independent.
As a result,SALT objects can be embedded into a HTML orany XML document as the spoken languageinterface (Wang 2000).
Introducing speechcapabilities to the Web is not new (Aron 1991, Lyet al1993, Lau et al1997).
However, it is theutmost design goal of SALT that advanced dialogmanagement techniques (Sneff et al1998,Rudnicky et al1999, Lin et al1999, Wang 1998)can be realized in a straightforward manner inSALT.The rest of the paper is organized as follows.
InSec.
1, we first review the dialog architecture onwhich the SALT design is based.
It is argued thatadvanced spoken dialog models can be realizedusing the Web infrastructure.
Specifically,various stages of dialog goals can be modeled asWeb pages that the user will navigate through.Considerations in flexible dialog designs havedirect implications on the XML documentstructures.
How SALT implements thesedocument structures are outlined.
In Sec.
2, theXML objects providing spoken languageunderstanding and speech synthesis are described.These objects are designed using the event drivenarchitecture so that they can included in the GUIenvironment for multimodal interactions.
Finallyin Sec.
3, we describe how SALT, which is basedon XML, utilizes the extensibility of XML toallow new extensions without losing documentportability.1 Dialog Architecture OverviewWith the advent of XML Web services, the Webhas quickly evolved into a gigantic distributedcomputer where Web services, communicating inXML, play the role of reusable softwarecomponents.
Using the universal description,discovery, and integration (UDDI) standard, Webservices can be discovered and linked updynamically to collaborate on a task.
In otherwords, Web services can be regarded as thesoftware agents envisioned in the open agentarchitecture (Bradshaw 1996).
Conceptually, theWeb infrastructure provides a straightforwardmeans to realize the agent-based approachsuitable for modeling highly sophisticated dialog(Sadek et al1997).
This distributed model sharesthe same basis as the SALT dialog managementarchitecture.1.1 Page-based Dialog ManagementAn examination on human to humanconversation on trip planning shows thatexperienced agents often guide the customers individing the trip planning into a series of moremanageable and relatively untangled subtasks(Rudnicky et al1999).
Not only the observationcontributes to the formation of the plan-baseddialog theory, but the same principle is alsowidely adopted in designing GUI-basedtransactions where the subtasks are usuallyencapsulated in visual pages.
Take a travelplanning Web site for example.
The first pageusually gathers some basic information of the trip,such as the traveling dates and the originating anddestination cities, etc.
All the possible travelplans are typically shown in another page, inwhich the user can negotiate on items such as theprice, departure and arrival times, etc.
To someextent, the user can alter the flow of interaction.
Ifthe user is more flexible for the flight than thehotel reservation, a well designed site will allowthe user to digress and settle the hotel reservationbefore booking the flight.
Necessaryconfirmations are usually conducted in separatepages before the transaction is executed.The designers of SALT believe that spokendialog can be modeled by the page-basedinteraction as well, with each page designed toachieve a sub-goal of the task.
There seems noreason why the planning of the dialog cannotutilize the same mechanism that dynamicallysynthesizes the Web pages today.1.2 Separation of Data and PresentationSALT preserves the tremendous amount offlexibility of a page-based dialog system indynamically adapting the style and presentationof a dialog (Wang 2000).
A SALT page iscomposed of three portions: (1) a data sectioncorresponding to the information the systemneeds to acquire from the user in order to achievethe sub-goal of the page; (2) a presentationsection that, in addition to GUI objects, containsthe templates to generate speech prompts and therules to recognize and parse user?s utterances; (3)a script section that includes inference logic forderiving the dialog flow in achieving the goal ofthe page.
The script section also implements thenecessary procedures to manipulate thepresentation sections.This document structure is motivated by thefollowing considerations.
First, the separation ofthe presentation from the rest localizes the naturallanguage dependencies.
An application can beported to another language by changing only thepresentation section without affecting othersections.
Also, a good dialog must dynamicallystrike a balance between system initiative anduser initiative styles.
However, the needs toswitch the interaction style do not necessitatechanges in the dialog planning.
The SALTdocument structure maintains this type ofindependence by separating the data section fromthe rest of the document, so that when there areneeds to change the interaction style, the scriptand the presentation sections can be modifiedwithout affecting the data section.
The samemechanism also enables the app to switch amongvarious UI modes, such as in the mobileenvironments where the interactions must be ableto seamlessly switching between a GUI andspeech-only modes for hand-eye busy situations.The presentation section may vary significantlyamong the UI modes, but the rest of the documentcan remain largely intact.1.3 Semantic Driven MultimodalIntegrationSALT follows the common GUI practice andemploys an object-oriented, event-driven modelto integrate multiple input methods.
Thetechnique tracks user?s actions and reports themas events.
An object is instantiated for each eventto describe the causes.
For example, when a userclicks on a graphical icon, a mouse click event isfired.
The mouse-click event object containsinformation such as coordinates where the clicktakes place.
SALT extends the mechanism forspeech input, in which the notion of semanticobjects (Wang 2000, Wang 1998) is introducedto capture the meaning of spoken language.When the user says something, speech events,furnished with the corresponding semanticobjects, are reported.
The semantic objects arestructured and categorized.
For example, anutterance ?Send mail to John?
is composed oftwo nested semantic objects: ?John?
representingthe semantic type ?Person?
and the wholeutterance the semantic type ?Email command.
?SALT therefore enables a multimodal integrationalgorithm based on semantic type compatibility(Wang 2001).
The same command can bemanifest in a multimodal expression, as in ?Sendemail to him [click]?
where the email recipient isgiven by a point-and-click gesture.
Here thesemantic type provides a straightforward way toresolve the cross modality reference: the handlerfor the GUI mouse click event can beprogrammed into producing a semantic object ofthe type ?Person?
which can subsequently beidentified as a constituent of the ?emailcommand?
semantic object.
Because the notionof semantic objects is quite generic, dialogdesigners should find little difficulty employingother multimodal integration algorithms, such asthe unification based approach described in(Johnston et al1997), in SALT.2 Basic Speech Elements in SALTSALT speech objects encapsulate speechfunctionality.
They resemble to the GUI objectsin many ways.
Because they share the same highlevel abstraction, SALT speech objectsinteroperate with GUI objects in a seamless andconsistent manner.
Multimodal dialog designerscan elect to ignore the modality ofcommunication, much the same way as they areinsulated from having to distinguish whether atext string is entered to a field through a keyboardor cut and pasted with a pointing device.2.1 The Listen ObjectThe ?listen?
object in SALT is the speech inputobject.
The object must be initialized with aspeech grammar that defines the language modeland the lexicon relevant to the recognition task.The object has a start method that, uponinvocation, collects the acoustic samples andperforms speech recognition.
If the languagemodel is a probabilistic context free grammar(PCFG), the object can return the parse tree of therecognized outcome.
Optionally, dialogdesigners can embed XSLT templates or scriptsin the grammar to shape the parse tree into anydesired format.
The most common usage is totransform the parse tree into a semantic treecomposed of semantic objects.A SALT object is instantiated in an XMLdocument whenever a tag bearing the objectname is encountered.
For example, a listen objectcan be instantiated as follows:<listen id=?foo?
onreco=?f()?onnoreco=?g()?
mode=?automatic?><grammar src=?../meeting.xml?/></listen>The object, named ?foo,?
is given a speechgrammar whose universal resource indicator(URI) is specified via a <grammar> constituent.As in the case of HTML, methods of an object areinvoked via the object name.
For example, thecommand to start the recognition is foo.start() inthe ECMAScript syntax.
Upon a successfulrecognition and parsing, the listen object raisesthe event ?onreco.?
The event handler, f(), isassociated in the HTML syntax as shown above.If the recognition result is rejected, the listenobject raises the ?onnoreco?
event, which, in theabove example, invokes function g().
Asmentioned in Sec.
0, these event handlers residein the script section of a SALT page that managesthe within-page dialog flow.
Note that SALT isdesigned to be agnostic to the syntax of theeventing mechanism.
Although the examplesthrough out this article use HTML syntax, SALTcan operate with other eventing standards, suchas World Wide Web Consortium (W3C) XMLDocument Object Model (DOM) Level 2, ECMACommon Language Infrastructure (CLI), or theupcoming W3C proposal called XML Events.The SALT listen object can operate in one of thethree modes designed to meet different UIrequirements.
The automatic mode, shown above,automatically detects the end of utterance and cutoff the audio stream.
The mode is most suitablefor push-to-talk UI or telephony based systems.Reciprocal to the start method, the listen objectalso has a stop method for forcing the recognizerto stop listening.
The designer can explicitlyinvoke the stop method and not rely on therecognizer?s default behavior.
Invoking the stopmethod becomes necessary when the listen objectoperates under the single mode, where therecognizer is mandated to continue listening untilthe stop method is called.
Under the single mode,the recognizer is required to evaluate and returnhypotheses based on the full length of the audio,even though some search paths may have reacheda legitimate end of sentence token in the middleof the audio stream.
In contrast, the third multiplemode allows the listen object to reporthypotheses as soon as it sees fit.
The single modeis designed for push-hold-and-talk type of UI,while the multiple mode is for real-time ordictation type of applications.The listen object also has methods to modify thePCFG it contains.
Rules can be dynamicallyactivated and deactivated to control theperplexity of the language model.
The semanticparsing templates in the grammar can bemanipulated to perform simple referenceresolution.
For example, the grammar below (inSAPI format) demonstrates how a deicticreference can be resolved inside the SALT listenobject:<rule propname=?drink?
?><option> the </option><list><phrase propvalue=?coffee?> left </phrase><phrase propvalue=?juice?> right </phrase></list><option> one </option></rule>In this example, the propname and propvalueattributes are used to generate the semanticobjects.
If the user says ?the left one,?
the abovegrammar directs the listen object to return thesemantic object as <drink text=?the leftone?>coffee</drink>.
This mechanism forcomposing semantic objects is particularly usefulfor processing expressions closely tied to howdata are presented.
The grammar above may beused when the computer asks the user for choiceof the drink by displaying the pictures of thechoices side by side.
However, if the display istiny, the choices may be rendered as a list, towhich a user may say ?the first one?
or ?thebottom one.?
SALT allows dialog designers toapproach this problem by dynamically adjustingthe speech grammar.2.2 The prompt objectThe SALT ?prompt?
object is the speech outputobject.
Like the listen object, the prompt objecthas a start method to begin the audio playback.The prompt object can perform text to speechsynthesis (TTS) or play pre-recorded audio.
ForTTS, the prosody and other dialog effects can becontrolled by marking up the text with synthesisdirectives.Barge-in and bookmark are two events of theprompt object particularly useful for dialogdesigns.
The prompt object raises a barge-inevent when the computer detects user utteranceduring a prompt playback.
SALT provides a richprogram interface for the dialog designers tospecify the appropriate behaviors when thebarge-in occurs.
Designers can choose whether todelegate SALT to cut off the outgoing audiostream as soon as speech is detected.
Delegatedcut-off minimizes the barge-in response time, andis close to the expected behavior for users whowish to expedite the progress of the dialogwithout waiting for the prompt to end.
Similarly,non-delegated barge-in let the user changeplayback parameters without interrupting theoutput.
For example, the user can adjust the speedand volume using speech commands while theaudio playback is in progress.
SALT willautomatically turn on echo cancellation for thiscase so that the playback has minimal impacts onthe recognition.The timing of certain user action or the lackthereof often bears semantic implications.Implicit confirmation is a good example, wherethe absence of an explicit correction from theuser is considered as a confirmation.
The promptobject introduces an event for reporting thelandmarks of the playback.
The typical way ofcatching the playback landmarks in SALT is assuch:<prompt id=?bar?
onbookmark=?f()?
?>Traveling to New York?<bookmark name=?imp_confirm?/>There are <emph> 3 </emph> flights available?</prompt>When the synthesizer reaches the TTS markup<bookmark>, the onbookmark event is raised andthe event hander f() is invoked.
When a barge-inis detected, the dialog designer can determine ifthe barge-in occurs before or after the bookmarkby inspecting whether the function f() has beencalled or not.Multimedia synchronization is another mainusage for TTS bookmarks.
When the speechoutput is accompanied with, for example,graphical animations, TTS bookmarks are aneffective mechanism to synchronize theseparallel outputs.To include dynamic content in the prompt, SALTadopts a simple template-based approach forprompt generation.
In other words, the carrierphrases can be either pre-recorded or hard-coded,while the key phrases can be inserted andsynthesized dynamically.
The prompt object thatconfirms a travel plan may appear as thefollowing in HTML:<input name=?origin?
type=?text?
/><input name=?destination?
type=?text?
/><input name=?date?
type=?text?
/>?<prompt ?> Do you want to fly from<value targetElement=?origin?/> to<value targetElement=?destination?/> on<value targetElement=?date?/>?</prompt>As shown above, SALT uses a <value> tag insidea prompt object to refer to the data contained inother parts of the SALT page.
In this example, theprompt object will insert the values in the HTMLinput objects in synthesizing the prompt.2.3 Declarative Rule-based ProgrammingAlthough the examples use proceduralprogramming in managing the dialog flowcontrol, SALT designers can practice inferenceprogramming in a declarative rule-based fashionin which rules are attached to the SALT objectscapturing user?s actions, e.g., the listen object.Instead of authoring procedural event handlers,designers can declare inside the listen object rulesthat will be evaluated and invoked when thesemantic objects are returned.
This is achievedthrough a SALT <bind> element as demonstratedbelow:<listen ?>  <grammar ?/><bind test=?/@confidence $lt$ 50?targetElement=?prompt_confirm?targetMethod=?start?targetElement=?listen_confirm?targetMethod=?start?
/><bind test=?/@confidence $ge$ 50?targetElement=?origin?value=?/city/origin?targetElement=?destination?value=?/city/destination?targetElement=?date?value=?/date?
/>     ?</listen>The predicate of each rule is applied in turnsagainst the result of the listen object.
They areexpressed in the standard XML Pattern languagein the ?test?
clause of the <bind> element.
In thisexample, the first rule checks if the confidencelevel is above the threshold.
If not, the ruleactivates a prompt object (prompt_confirm) forexplicit confirmation, followed by a listen objectlisten_confirm to capture the user?s response.
Thespeech objects are activated via the start methodof the respective object.
Object activations arespecified in the targetElement and thetargetMethod clauses of the <bind> element.Similarly, the second rule applies when theconfidence score exceeds the prescribed level.The rule extracts the relevant semantic objectsfrom the parsed outcome and assigns them to therespective elements in the SALT page.
As shownabove, SALT reuses the W3C XPATH languagefor extracting partial semantic objects from theparsed outcome.3 SALT ExtensibilitiesNaturally spoken language is a modality that canbe used in widely diverse environments whereuser interface constraints and capabilities varysignificantly.
As a result, it is only practical todefine into SALT the speech functions that areuniversally applicable and implementable.
Forexample, the basic speech input function inSALT only deals with speaker independentrecognition and understanding, even thoughspeaker dependent recognition or speakerverification are in many cases very useful.
As aresult, extensibility is crucial to a naturallanguage interface like SALT.SALT follows the XML standards that allowextensions being introduced on demand withoutsacrificing document portability.
Functions thatare not already defined in SALT can beintroduced at the component level, or as a newfeature of the markup language.
In addition,SALT requires the standard of XML be followedso that extensions can be identified and themethods to process the extensions can bediscovered and integrated.3.1 Component extensibilitySALT components can be extended with newfunctions individually through the componentconfiguration mechanism of the <param>element.
For example, the <listen> element hasan event to signal when speech is detected in theincoming audio stream.
However, the standarddoes not specify an algorithm for detectingspeech.
A SALT document author, however, candeclare reference cues so that the document canbe rendered in the similar way among differentprocessors.
The <param> element can be used toset the reference algorithm and threshold fordetecting speech in the <listen> object:<listen onspeechdetected = ?handler( )?
?><param xmlns:xyz=?urn://xyz.edu/algo-1?><xyz:method>energy</xyz:method><xyz:threshold>0.7</xyz:threshold></param>   ?</listen>Here the parameters are set using an algorithmwhose uniform resource name (URN),xyz.edu/algo-1, is declared as an attribute of theXML namespace of <param>.
The parameters forconfiguring this specific speech detection methodare further specified in the child elements.
Adocument processor can perform a schematranslation on the URN namespace into anyschema the processor understands.
For example,if the processor implements the speech detectionalgorithm where the detection threshold has adifferent range, the adjustment can be easilymade when the document is parsed.The same mechanism is used to extend thefunctionality.
For instance, the <listen> objectcan be used for speaker verification because thealgorithm used for verification and theprogrammatic interfaces share a lot in commonwith the recognition.
In SALT, a <listen> objectcan be extended for speaker verification throughconfiguration parameters:<listen  onreco=?success( )?
onerror=?na( )?onnoreco=?failed( )?><param xmlns:v=?urn:abc.com/spkrveri?><v:cohort>../../data</v:cohort><v:threshold>0.85</v:threshold> ?</param> ?</listen>In this example, the <listen> object is extendedfor speaker verification that compares a user?svoice against a cohort set.
The events ?onreco?and ?onnoreco?
are invoked when the voicepasses or fails the test, respectively.
As in theprevious case, the extension must be decoratedwith URN that specifies the intended behavior ofthe document author.
Being an extension, thedocument processor might not be able to discernthe semantics implied by the URN natively.However, XML based protocols allow theprocessor to query and employ Web services thatcan either (1) transform the extended documentsegment into an XML schema the processorunderstands, or (2) perform the functiondescribed by the URN.
By closely followingXML standards, SALT documents fully enjoy thebenefits of extensibility and portability of XMLwith SALT.3.2 Language extensibilityIn addition to component extensibility, the wholelanguage of SALT can be enhanced with newfunctionality using XML.
Communicating withother modalities, input devices, and advanceddiscourse and context management are just a fewpotential use for language-wise extension.SALT message extension, or the <smex> element,is the standard conduit between a SALTdocument and the outside world.
The messageelement takes <param> as its child element toforge a connection to an external component.Once a link is established, SALT document cancommunicate with the external component byexchanging text messages.
The <smex> elementhas a ?sent?
attribute to which a text message canbe assigned to.
When its value is changed, thenew value is regarded as a message intended forthe external component and immediatelydispatched.
When an incoming message isreceived, the object places the message on aproperty called ?received?
and raises the?onreceive?
event.3.2.1 Telephony InterfaceTelephones are one of the most important accessdevices for spoken language enabled Webapplications.
Call control functions play a centralrole in a telephony SALT application.
The<smex> element in SALT is a perfect match withthe telephony call standard known as ECMA 323.ECMA 323 defines the standard XML schemasfor messages of telephony functions, rangingfrom simple call answering, disconnecting,transferring to switching functionality suitablefor large call centers.
ECMA 323 employs thesame message exchange model as the design of<smex>.
This allows SALT application to tapinto a rich telephony call controls withoutneeding a complicated SALT processor.
Asshown in (SALT 2002), sophisticated telephonyapplications can be authored in SALT in a verystraightforward manner.3.2.2 Advanced Context ManagementIn addition to devices such as telephones, SALT<smex> object may also be employed to connectto Web services or other software components tofacilitate advanced discourse semantic analysisand context managements.
Such capabilities, asdescribed in (Wang 2001), empower the user torealize the full potential of interacting withcomputers in natural language.
For example, theuser can simply say ?Show driving directions tomy next meeting?
without having to explicitlyand tediously instruct the computer to first lookup the next meeting, obtain its location, copy thelocation to another program that maps out adriving route.The customized semantic analysis can beachieved in SALT as follows:<listen id=?first_pass?
?><grammar src=???
/> <!-- basic grammar --><bind targetElement=?contextManager?targetAttribute=?sent?
value=?/?
/>?</listen><smex id=?contextManager?
?><param xmlns:ws=?WebServices?><ws:url>http://personal.com</ws:url><ws:user>?</ws:user> ?</param><bind targetElement=?realTarget?
?
/></smex>Here the <listen> element includes a rudimentarygrammar to analyze the basic sentence structureof user utterance.
Instead of resolving everyreference (e.g.
?my next meeting?
), the result iscascaded to the <smex> element linked to a Webservice specializing in resolving personalreferences.SummaryIn this paper, we describe the design philosophyof SALT in using the existing Web architecturefor distributed multimodal dialog.
SALT allowsflexible and powerful dialog management byfully taking advantage of the well publicizedbenefits of XML, such as separation of data frompresentation.
In addition, XML extensibilityallows new functions to be introduced as neededwithout sacrificing document portability.
SALTuses this mechanism to accommodate diverseWeb access devices and advanced dialogmanagement.ReferencesSadek M.D., Bretier P., Panaget F. (1997)  ARTIMIS:Natural dialog meets rational agency.
In Proc.IJCAI-97, Japan.Allen J.F.
(1995)  Natural language understanding, 2ndEdition, Benjamin-Cummings, Redwood City, CA.Cohen P.R, Morgan J., Pollack M.E (1989)  Intentionsin communications, MIT Press, Cambridge MA.SALT Forum (2002) Speech Application LanguageTags Specification,  http://www.saltforum.org.Wang K. (2000) Implementation of a multimodaldialog system using extensible markup language, InProc.
ICSLP-2000, Beijing China.Aron B.
(1991) Hyperspeech: navigation inspeech-only hypermedia, in Proc.
Hypertext-91, SanAntonio TX.Ly E., Schmandt C., Aron B.
(1993) Speechrecognition architectures for multimediaenvironments, in Proc.
AVIOS-93, San Jose, CA.Lau R., Flammia G., Pao C., Zue V. (1994)Webgalaxy: Integrating spoken language andhyptertext navigation, in Proc.
EuroSpeech-97,Rhodes, Greece.Sneff S., Hurley E., Lau R., Pao C., Schmid P., Zue V.(1998) Galaxy-II: A reference architecture forconversational system development, in Proc.ICSLP-98, Sydney Australia.Rudnicky A., Xu W. (1999) An agenda-based dialogmanagement architecture for spoken languagesystems, in Proc.
ASRU-99, Keystone CO.Lin B.-S, Wang H.-M, Lee L.-S (1999) A distributedarchitecture for cooperative spoken dialog agentswith coherent dialog state and history, in Proc.ASRU-99, Keystone CO.Bradshaw J.M.
(1996) Software Agents, AAAI/MITPress, Cambridge, MA.Wang K (1998) An event driven dialog system, inProc.
ICSLP-98, Sydney Australia.Wang K (2001) Semantic modeling for dialog systemsin a pattern recognition framework, in Proc.
ASRU2001, Trento Italy.Johnston M., Cohen P.R., McGee D.,  Oviatt S.L.,Pittman J.A., Smith I (1997) Unification basedmultimodal integration, in Proc.
35th ACL, MadridSpain.Wang K (2001) Natural language enabled Webapplications, in Proc.
1st NLP and XML Workshop,Tokyo, Japan.
