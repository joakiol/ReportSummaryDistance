Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsUM-Checker: A Hybrid System for English Grammatical Error Cor-rectionJunwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong ZengNatural Language Processing & Portuguese-Chinese Machine Translation Laboratory,Department of Computer and Information Science,University of Macau, Macau S.A.R., Chinanlp2ct.
{vincent, anson}@gmail.com,{derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.comAbstractThis paper describes the NLP2CT Grammati-cal Error Detection and Correction system forthe CoNLL 2013 shared task, with a focus onthe errors of article or determiner (ArtOrDet),noun number (Nn), preposition (Prep), verbform (Vform) and subject-verb agreement(SVA).
A hybrid model is adopted for this spe-cial task.
The process starts with spell-checking as a preprocessing step to correct anypossible erroneous word.
We used a Maxi-mum Entropy classifier together with manual-ly rule-based filters to detect the grammaticalerrors in English.
A language model based onthe Google N-gram corpus was employed toselect the best correction candidate from aconfusion matrix.
We also explored a graph-based label propagation approach to overcomethe sparsity problem in training the model.
Fi-nally, a number of deterministic rules wereused to increase the precision and recall.
Theproposed model was evaluated on the test setconsisting of 50 essays and with about 500words in each essay.
Our system achieves the5thand 3rdF1 scores on official test set amongall 17 participating teams based on gold-standard edits before and after revision, re-spectively.1 IntroductionWith the increasing number of people all overthe world who study English as their second lan-guage1, grammatical errors in writing often oc-curs due to cultural diversity, language habits,education background, etc.
Thus, there is a sub-stantial and increasing need of using computer1 A well-known fact is that the most popular languagechosen as a first foreign language is English.techniques to improve the writing ability for sec-ond language learners.
Grammatical error correc-tion is the task of automatically detecting andcorrection erroneous word usage and ill-formedgrammatical constructions in text (Dahlmeier etal., 2012).In recent decades, this special task has gainedmore attention by some organizations such as theHelping Our Own (HOO) challenge (Dale andKilgarriff, 2010; Dale et al 2012).
Although theperformance of grammatical error correction sys-tems has been improved, it is still mostly limitedto dealing with the determiner and prepositionerror types with a very low recall and precision.This year, the CoNLL-2013 shared task extendsto include a more comprehensive list of errortypes, as shown in Table 1.To take on this challenge, this paper proposespipe-line architecture in combination with sever-al error detection and correction models based ona hybrid approach.
As a preprocessing step wefirstly employ a spelling correction to correct themisspelled words.
To correct the grammaticalerrors, a hybrid system is designed that integrat-ed with Maximum Entropy (ME) classifier, de-terministic filter and N-gram language modelscorer, each of which is constructed as an indi-vidual model.
According to the phenomena ofthe problems, we use different combinations ofthe models trained on specific data to tackle thecorresponding types of errors.
For instance, Prepand Nn have a strong inter-relation with thewords (surface) that are preceding and followingthe active word.
This can be detected and recov-ered by using a language model.
On the otherhand, SVA is more complicated and it is moreeffective to determine the mistakes by using thelinguistic and grammatical rules.
The correction34Error Type Description ExampleVformReplacement The solution can be obtain (obtained) by using technology.InsertionHowever, the world has always beyond our imagination and ?
(has)never let us down.Deletion It also indicates that the economy has been (?)
dramatically grown.SVASubject-verb-AgreementMy brothers is (are) nutritionists.ArtOrDetReplacementThe leakage of these (this) confidential information can be a sensitiveissue to personal, violation of freedom and breakdown of safety.Insertion The survey was done by ?
(the) United Nations.DeletionThe air cargo of the (?)
Valujet plane was on fire after the plane hadtaken off.Nn Noun number He receives two letter (letters).PrepReplacement They work under (in) a conductive environment.InsertionDefinitely, there are point of view that agree ?
(with) the technologybut also the voices of objection.DeletionToday, the surveillance technology has become almost manifest to (?
)wherever we go.Table 1: The error types with descriptions and examples.components are combined into a pipeline of cor-rection steps to form an end-to-end correctionsystem.
Different types of corrections may inter-act with each other.
Therefore, only for each fo-cus word in a sentence will pass the filter andpredict by the system.Take the sentence for example, ?The patentapplications do not need to be censored.
?, if theword ?applications?
is changed to ?application?
(Nn error) by a correction module, then the fol-lowing auxiliary verb ?do?
should be revised to?does?
(SVA error) accordingly.
That is, if a mis-take is introduced by a component in the priorstep, subsequent analyses are most likely affect-ed negatively.
To avoid the errors propagatedinto further components, we proposed to deploythe analytical (pipelined) components in the or-der of Nn, ArtOrDet, Vform, SVA and Prep.For non-native language learners, over 90%usage of prepositions and articles are correctlyused, which makes the errors very sparse (Ro-zovskaya and Roth, 2010c) in a text, and about10% error is not ?sparse?
by the way.
This factorseverely restricts the improvement of data-drivensystems.
Different from the previous methods toovercome error sparsity, we explored a graph-based label propagation method that makes useof the prediction on large amount of unlabeleddata.
The predicted data are then used toresample our training data.
This semi-supervisedmethod may fix a skewed label distribution in thetraining set and is helpful to enhance the models.The paper is organized as follows.
We firstlyreview and discuss the related work.
The dataused to construct the models is described in Sec-tion 3.
Section 4 discusses the proposed modelbased on semi-supervised learning, and the over-all hybrid system is given in Section 5.
Themethods of grammatical error detection and cor-rection are detailed in Section 6, followed by anevaluation, discussion and a conclusion to endthe paper.2 Related WorkThe issues of grammatical error correction havebeen discussed from different perspectives forseveral decades.
In this section, we briefly re-view some related methods.The use of machine learning methods to tacklethis problem has shown a promising perfor-mance.
These methods are normally createdbased on a large corpus of well-formed nativeEnglish texts (Tetreault and Chodorow 2008;Tetreault et al 2010) or annotated non-nativedata (Gamon, 2010; Han et al 2010).
Althoughthe manually error-tagged text is much more ex-pensive, it has shown improvements over themodels trained solely on well-formed native text(Kochmar et al 2012).
Additionally, both gener-ative and discriminative classifiers were widelyused.
Among them, Maximum Entropy was gen-erally used (Rozovskaya and Roth, 2011;Sakaguchi et al 2012; Quan et al 2012) andobtained a good result for preposition and articlecorrection using a large feature set.
Naive Bayes35were also applied to recognize or correct the er-rors in speech or texts (Lynch et al 2012).
How-ever, only using classifiers always cannot give asatisfied performance.
Thus, grammar rules andprobabilistic language model can be used as asimple but effective assistant for correction ofspelling (Kantrowitz et al2003) and grammati-cal errors (Dahlmeier et al 2012; Lynch et al2012; Quan et al 2012; Rozovskaya et al2012).3 Data SetThe training data is the NUS Corpus of LearnerEnglish (NUCLE) that provided by the NationalUniversity of Singapore (Dahlmeier et al 2013).The NUCLE contains more than one millionwords (1,400 essays) and has been annotatedwith error-tags and correction-labels.
There are27 categories of errors, with 45,106 errors in to-tal.
In this CoNLL-2013 shared task, five typesof errors (around 32% of the total errors) areconcerned.
Figure 1 shows the statistics infor-mation of error types.Figure 1.
The distribution of different error types inthe training set.As the distribution of different errors respectsthe real environment, there is a serious problemhidden in it.
Roughly estimated, the ratio be-tween the correct and error classes in NUCLE isaround 100:1, or even more.
The imbalanceproblem may be heavily harmful to machinelearning methods.
Therefore, researchers (Ro-zovskaya et al 2012; Dahlmeier et al 2012)provided several approaches such as reducingcorrect instances to deal with error sparsity.
In-stead of downsampling the data, we try to up-sample error instances.
Different from UI system(Rozovskaya et al 2012) which simulates learn-ers to make mistakes artificially, we propose asemi-supervised learning method that makes useof a large amount of unlabeled data which is easyto collect.
In practice, semi-supervised learningrequires less human effort and gives higher accu-racy in creating a model.4 Error Examples Expansion UsingGraph-Based Label PropagationAs mentioned before, the corpus contains a lowamount of error examples, which results in ahigh sparsity in the label distribution.
In reality,the balance between the error and correct data iscrucial for training a robust grammar detectionmodels.
Our experiment results demonstrate thattoo many correct data lead to unfavorable errordetection rate.
In order to resolve this obstacle,this paper introduces to using external datasources, i.e., a large amount of easily accessibleraw texts, to automatically achieve more labeledexample for training a stronger model.
This pa-per employs transductive graph-based semi-supervised learning approach.4.1 Graph-Based Label PropagationGraph-based label propagation is one of the criti-cal subclasses of SSL.
Graph-based label propa-gation methods have recently shown they canoutperform the state-of-the-art in several naturallanguage processing (NLP) tasks, e.g., POS tag-ging (Subramanya et al 2010), knowledge ac-quisition (Talukdar et al 2008), shallow seman-tic parsing for unknown predicate (Das andSmith, 2011).
This study uses graph SSL to en-rich training data, mainly the examples with in-correct tag, from raw texts.This approach constructs a k nearest-neighbor(k-nn) similarity graph over the labeled and un-labeled data in the first step.
The vertices in theconstructed graph consist of all instances (featurevector) that occur in labeled and unlabeled text,and edge weights between vertices are computedusing their Euclidean distance.
Pairs of verticesare connected by weighted edges which encodethe degree to which they are expected to have thesame label (Zhu, 2003).
In the second step, labelpropagation operates on the constructed graph.The primary objective is to propagate labels froma few labeled vertices to the unlabeled ones byoptimizing a loss function based on the con-straints or properties derived from the graph, e.g.smoothness (Zhu et al 2003; Subramanya andBilmes, 2008; Talukdar et al 2009), or sparsity(Das and Smith, 2012).
This paper uses propaga-tion method (MAD) in (Talukdar et al 2009).Vform9%SVA10%ArtOrDet42%Nn24%P ep15%36Figure 2.
Workflow of our proposed system.4.2 ImplementationIn this paper, the labeled data is taken from NU-CLE corpus.
They are regarded as the ?seed?data, including 93,000 correct and 1,200 incor-rect instances.
The unlabeled data is collectedfrom the English side of news magazine corpus(LDC2005T10).
Based on that, a 5-NN similaritygraph is constructed.
With the graph and theproperties of the labeled data derived from theNUCLE, the MAD algorithm is used to propa-gate the error-tag (label) from labeled vertices tothe unlabeled vertices.
Afterwards, the unlabeledexamples with incorrect tag are added into theoriginal training data for training.5 System DescriptionThis section describes the details of our system,including preprocessing of training set, confusionset generating, classifier training and languagemodels building.
The grammatical error correc-tion procedure is shown in Figure 2.5.1 PreprocessingAs mentioned in Section 3, there is a largeamount (68%) of other error types which mayresult in new errors or confuse the system withwrong information in correction.
In order tomake the best use of the corpus, it needs to filterall errors not covered by the CoNLL 2013 sharedtask, and then generate a separate corpus for eacherror type.
Therefore, we recovered other irrele-vant errors accordingly.
For each error type, wealso recover other 4 types of errors, and then wegot a pure training data set which only includesone error type.For the misspelled problem, we used an opensource toolkit (JMySpell2) which allows us touse the dictionaries form OpenOffice.
JMySpell2 Available at https://kenai.com/projects/jmyspell.gives a list of suggestion candidate words, andwe select the first one to replace the originalword.5.2 Confusion Set GeneratingConfusion sets include the correction candidateswhich are used to modify the wrong places of asentence.
We generated a confusion set for eachtype of error correction component.The confusion set for Nn, Vform and SVA wasbuilt on Penn Treebank3.
The format can be de-scribed as that each prototype word follows allpossible combinations with Part-Of-Speech (POS)and variants.
For instance, the format of the word?look?
in confusion set should looks like ?looklook#VB look#VBP looking#VBG looks#VBZlooked#VBN look#NN looks#NNS?.
The proto-type ?look?
and POS are the constraints forchoosing the correct candidate.
In order to quick-ly find the candidates according to each detectederror place, we indexed the confusion set in Lu-cene4 which is another open source toolkit with ahigh-performance, full-featured text search en-gine library.For ArtOrDet and Prep, the confusion sets aremanually created because the possible modifica-tions are not so many which are discussed inSection 6.1 and 6.2.5.3 Maximum Entropy ClassifierThe machine learning algorithm we used to trainthe detection models is Maximum Entropy (ME),which can classify the data by giving a probabil-ity distribution.
It is similar to multiclass logisticregression models, but much more profitablewith sparse explanatory feature vectors.
For MEclassifier, the feature of text data is suitable fortraining the model, so we choose it as our detec-tion classifier.3 Available at http://www.cis.upenn.edu/~treebank/.4 Available at http://lucene.apache.org/.SourceTextRule-basedFilterArtOrDetLM ScorerNnMEclassifierLM ScorerVformSVAMEclassifierRule-basedFilterRule-basedFilterRule-basedFilterPrepLM ScorerHybrid SystemCorrectText37We employed Stanford Classifier5 which is aJava implementation of maximum entropy(Manning & Klein, 2003).5.4 N-gram Language ModelThe probabilistic language model is constructedon Google Web 1T 5-gram corpus (Brants andFranz, 2006) by using the SRILM toolkit(Stolcke, 2002).
All generated modification can-didates are scored by it and only candidates thatstrictly increase than a threshold can be kept.The normalized language model score is de-fined as1 log Pr( )lmscore ss?
(1)in which s is the corrected sentence and |s| is thesentence length in tokens (Dahlmeier et al2012).6 Grammatical Error Correction6.1 Article and DeterminerThe component for ArtOrDet task integrates withthe language model and rule-based techniques.Language models are constructed to select thebest candidate from a confusion set of possiblearticle choices {a, the, an, ?
}, given the pre-corrected sentence.
Each Noun Phrase (NP) inthe test sentence will be pre-corrected as correc-tion candidates.
However, only using a languagemodel to determine the best correction will oftenresult in a low precision, because a certainamount of correct usages of ArtOrDet are mis-judged.In order to avoid this problem, we proposed avoting method based on multiple language mod-els.
We integrated two separate language models:one was converted from the large Google corpus(general LM) and the other one was constructedfrom a small in-domain corpus (in-domain LM).Additionally, the in-domain corpus involves twoparts.
One is the training data which has beentotally corrected according to the gold answer.The other one includes the sentences which aresimilar to the test set.
We extracted them fromsome well-formed native English corpora such asEnglish News Magazine of LDC2005T106 usingterm frequency-inverse document frequency (TF-IDF) as the similarity score.
Each document Di is5 Available athttp://nlp.stanford.edu/software/classifier.shtml.6Available at http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2005T10.represented as a vector (wi1, wi2,?, win), and n isthe size of the vocabulary.
So wij is calculated asfollows:)log( jijij idftfw ??
(2)where tfij is term frequency (TF) of the j-th wordin the vocabulary in the document Di, and idfj isthe is the inverse document frequency (IDF) ofthe j-th word calculated.
The similarity betweentwo sentences is then defined as the cosine of theangle between two vectors.Each candidate sentence will be scored bythese two LMs and compared with a threshold.Only if both of the LMs agree, the modificationwill be kept.
We believe this method could filtera lot of wrong modification and improve the pre-cision.6.2 PrepositionFor Prep error type, we used the same method asArtOrDet.
The only difference is confusion ma-trix.
Our system corrects the unnecessary, miss-ing and unwanted errors for the five most fre-quently prepositions which are in, for, to, of andon.
While developing our system, we found thatadding more prepositions did not increase per-formance in our experiments.
Thus the confusionset is {in, for, to, of, on, ?
}.6.3 Noun NumberA single noun in the sentence that is hard to dis-tinguish whether it is singular or plural, so wetreat a noun phrase as a observe subject.
Ourstrategy of correcting noun number error is to usea filter contains rule-based and machine learningmethod.
It can filter a part of nouns that absolute-ly right, and the rest of nouns will be detected bythe language model generated by SRILM7.The rule-based filter of our system containsseveral criteria.
It can detect the noun phrase byarticle, i.e.
it can simply find out that the noun issingular which with an article of ?a?
or ?an?.The determiner and cardinal number also will betaken into consider by the rule-based model suchas ?I have three apple.
?, then system can find outthe ?apple?
should be ?apples?.
The correct nounwill keep the original one, and the incorrect nounwill be replaced with a new candidate.After the first level filtering by the rules, therest of noun phrases are indeterminacy by system.Therefore, we use a ME classifier for further fil-tering.
We use lexical, POS and dependency7 http://www.speech.sri.com/projects/srilm/.38parse information as features.
The features arelisted in Table 2.In previous steps, most of the error can be de-tected, but also it may give a lot of wrong sug-gests, in order to reduce this situation, we use N-gram language model scorer to evaluate on thecandidates and choose the highest probabilityone.Feature ExampleObserver wordWord (w0) resourcePOS (p0) NNFirst word in NPWord (wNP-1st) aPOS (pNP-1st) DTDependency Relation detPrevious word before observed wordWord (w-1) goodPOS (p-1) JJWord after observed wordWord (w1) andPOS (p1) CCHead word of observed wordWord (whead) waterPOS (phead) NNDependency relation rcomdWord Combinationw0 + wNP-1st resource + aw0 + w-1 resource + goodw0 + w1 resource + andw0 + whead resource + waterwNP-1st + whead a + waterPOS Combinationp0 + pNP-1st NN + DTp0 + p-1 NN + JJp0 + p1 NN + CCp0 + phead NN + NNpNP-1st + phead DT + NNTable 2: Features for Nn and the example: ?An exam-ple is water which is a good resource and is plentiful.
?6.4 Verb FormDetermining the correct form of a verb in Eng-lish is complex, involving a relatively wide rangeof choices.
A verb can have many forms, such asbase, gerund, preterite, past participle and so on.To detect the tense of verb error is much morerelated to the semantics level than syntax level.Therefore, it is hard to extract a common featurefor training model.
We chose to separate it intoseveral problems and use rule-based model to dothe Vform correction.For auxiliary verbs, there are three categories,one is modal verbs (do, can, may, will, might,should, must, need and dare), the other is theform of ?be?
and ?have?.
In a verb phrase, nor-mally modals precede ?have?
and ?be?, and?have?
proceed ?be?, then we can get the order-ing like this: Modal, Have, Be.
Auxiliary verbscan incorporate with other verbs, and have dif-ferent combination.
Based on the previous studyof the core language engine (Alshawi, 1992), wedefine the rules that contain the type of verb,which tense of verbs can be used with, and theirentries in the lexicon.
For example:(can (aux (modal) (vform pres)  (COMPFORM bare))This means ?can?
is a modal verb, it can beused with a verb that in the present tense, when?can?
used alone with the main verb should ascomplement the base (bare) form.
In here, theCOMPFORM attribute is the entry condition inthe grammar.6.5 Subject-Verb AgreementThe basic principle of Subject-Verb Agreementis singular subjects need singular verbs; pluralsubjects need plural verbs, such as following sen-tences:My brother is a nutritionist.My sisters are dancers.Therefore, the subject of the sentence is thekey point.
To decide whether the verb is singularor plural should look into the context and findout the POS of the subject.
We utilize the exist-ing information given by NUCLE to extract thesubject of the verb.
For example, the sentence?Statistics show that the number are continuingto grow with the existing population explosion.
?Figure 3 shows the parse tree of this sentence.Figure 3.
Parse tree of the example sentence.RootS1NP1VP1VBP1NNPSSBARIN1S2NP2 VP2?DT2 NN2 VBP2arenumberthethatshowStatistics..?39Through Figure 3, the observed words are?show?
and ?are?, the subjects are ?statistics?and ?number?
respectively that we can conclude?statistics?
should use plural verb and ?number?should use singular verb ?is?
instead of ?are?.The other features extracted for training arelisted in Table 3.Feature ExampleObserver wordWord (w0) arePOS (p0) VBPSubject NPFirst word (wNP-1st) thePOS of first word (pNP-1st) DTHead word (wNP-head) numberPOS of head word (pNP-head) NNPrevious word before observed wordWord (w-1) numberPOS (p-1) NNNP after observed wordFirst word (wNPa-1st) thePOS of first word (pNPa-1st) DTHead word (wNPa-head) explosionPOS of head word (pNPa-head) NNWord combinationw0 + wNP-1st are + thew0 + wNP-head are + numberw0 + w-1 are + numberw0 + wNPa-1st are + thew0 + wNPa-head are + explosionPOS combinationp0 + pNP-1st VBP + DTp0 + pNP-head VBP + NNp0 + p-1 VBP + NNp0 + pNPa-1st VBP + DTp0 + pNPa-head VBP + NNTable 3: Features for SVA and the example: ?Statis-tics show that the number are continuing to grow withthe existing population explosion.
?The purpose of extracting the noun phrase af-ter the observed word is in the situation of thesubject is after the verb, such as ?Where are myscissors?
?, ?scissors?
is the subject of this sen-tence.7 Evaluation and DiscussionThe evaluation is provided by the organizer andgenerated by M2 scorer (Dahlmeier & Ng, 2012).The result consists of precision, recall and F-score.
Our grammatical error correction systemhas proposed 1,011 edits.
The evaluation resultof our system output for the CoNLL-2013 testdata is shown in Table 4.Results Precision Recall F-scoreBeforeRevision0.2849 0.1753 0.2170AfterRevision0.3712 0.2366 0.2890Table 4: Evaluation result of Precision, Recall and F-score.Error Type Error # Correct # %ArtOrDet 690 145 21.01Nn 396 92 23.23Vform 122 8 6.55SVA 124 37 29.83Prep 311 6 1.93Table 5: Detail information of evaluation result (Be-fore Revision).Error Type Error # Correct # %ArtOrDet 725 177 24.42Nn 484 132 27.27Vform 151 16 10.60SVA 138 47 34.06Prep 325 9 2.77Table 6: Detail information of evaluation result (AfterRevision).The data in table 5 and 6 are the detailed in-formation for each error type which was calcu-lated by us, the table 5 is the data before revision,and the table 6 is that after revision.
Second col-umn is the amount of the gold edits, and the thirdcolumn is the amount of our correct edits, andthe last column is the percentage of correct edits.We analyzed the results in detail, and found sev-eral critical reasons of causing low recall.
Firstly,the five error types are associated relatively, ifone is modified, it may cause a chain reaction,such as the article will affect the noun number,and the noun number will cause the SVA errors.Some Nn errors still cannot be detected or givena wrong correction by our system, which de-creases the precision and recall of SVA.
Anotherreason is our system does not perform well inVform and Prep error correction.
In our output,just a few errors have been revised.
This meansthe quantity of correction rules is not enough thatcannot cover all the linguistic phenomena.
For40instance, the situation of missing verb or unnec-essary verb cannot be detected.
On the otherhand, the hybrid method of our system has fil-tered some wrong suggestion candidates that im-prove the precision.8 ConclusionWe have presented the hybrid system for Englishgrammatical error correction.
It achieves a 28.9%F1-score on the official test set.
We believe that ifwe find more appropriate features, our systemcan still be improved and achieve a better per-formance.AcknowledgmentsThe authors are grateful to the Science andTechnology Development Fund of Macau andthe Research Committee of the University ofMacau for the funding support for our research,under the reference No.
017/2009/A andMYRG076(Y1-L2)-FST13-WF.
The authors alsowish to thank the anonymous reviewers for manyhelpful comments as well as Liangye He, YuchuLin and Jiaji Zhou who give us a lot of help.ReferencesHiyan Alshawi.
1992.
The core language engine.
TheMIT Press.Jon Louis Bentley.
1980.
Multidimensional divide-and-conquer.
Communications of the ACM,23:214?229.Alina Beygelzimer, Sham Kakade, and John Lang-ford.
2006.
Cover trees for nearest neighbor.
In:Proceedings of the 23rd International Confer-ence on Machine Learning, pp.
97?104.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.
Linguistic Data Consortium,Philadelohia, PA.Olivier Chapelle, Bernhard Sch?lkopf, AlexanderZien, and others.
2006.
Semi-supervised learning.MIT press Cambridge.Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun FengNg.
2012.
NUS at the HOO 2012 Shared Task.
In:Proceedings of the Seventh Workshop onBuilding Educational Applications UsingNLP, pp.
216?224.Daniel Dahlmeier & Hwee Tou Ng, and Siew MeiWu (2013).
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner Eng-lish.
To appear in Proceedings of the 8th Work-shop on Innovative Use of NLP for BuildingEducational Applications (BEA 2013).
Atlanta,Georgia, USA.Daniel Dahlmeier, and Hwee Tou Ng (2012).
BetterEvaluation for Grammatical Error Correction.Proceedings of the 2012 Conference of theNorth American Chapter of the Associationfor Computational Linguistics (NAACL 2012),pp.
568 ?
572.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In: Pro-ceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pp.
54?62.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In:Proceedings of the 13th European Workshopon Natural Language Generation, pp.
242?249.Dipanjan Das and Noah A. Smith 2012.
Graph-basedlexicon expansion with sparsity-inducing penalties.In: Proceedings of the 2012 Conference of theNorth American Chapter of the Associationfor Computational Linguistics: Human Lan-guage Technologies, pp.
677?687.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing: a meta-classifierapproach.
In: Human Language Technologies:The 2010 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pp.
163?171.Andrew B. Goldberg and Xiaojin Zhu.
2006.
Seeingstars when there aren?t many stars: graph-basedsemi-supervised learning for sentiment categoriza-tion.
In: Proceedings of the First Workshop onGraph Based Methods for Natural LanguageProcessing, pp.
45?52.Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using an error-annotated learnercorpus to develop an ESL/EFL error correctionsystem.
In: Proceedings of LREC, pp.
763?770.Mark Kantrowitz.
2003.
Method and apparatus foranalyzing affect and emotion in text.
U.S. PatentNo.
6,622,140.Ekaterina Kochmar.
2011.
Identification of a writer?snative language by error analysis.
Master?s thesis,University of Cambridge.Gerard Lynch, Erwan Moreau, and Carl Vogel.
2012.A Naive Bayes classifier for automatic correctionof preposition and determiner errors in ESL text.In: Proceedings of the Seventh Workshop onBuilding Educational Applications UsingNLP, pp.
257?262.41Christopher Manning and Dan Klein.
2003.
Optimiza-tion, Maxent Models, and Conditional Estimationwithout Magic.
Tutorial at HLT-NAACL 2003and ACL 2003.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault (2013).
TheCoNLL-2013 Shared Task on Grammatical ErrorCorrection.
To appear in Proceedings of the Sev-enteenth Conference on Computational Natu-ral Language Learning.Li Quan, Oleksandr Kolomiyets, and Marie-FrancineMoens.
2012.
KU Leuven at HOO-2012: a hybridapproach to detection and correction of determinerand preposition errors in non-native English text.In: Proceedings of the Seventh Workshop onBuilding Educational Applications UsingNLP, pp.
263?271.Juan Ramos.
2003.
Using tf-idf to determine wordrelevance in document queries.
In: Proceedings ofthe First Instructional Conference on MachineLearning.Alla Rozovskaya and Dan Roth.
2010.
Training para-digms for correcting errors in grammar and usage.In: Human Language Technologies: The 2010Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics, pp.
154?162.Alla Rozovskaya, Mark Sammons, and Dan Roth.2012.
The UI system in the HOO 2012 shared taskon error correction.
In: Proceedings of the Sev-enth Workshop on Building Educational Ap-plications Using NLP, pp.
272?280.Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo,Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-machi, and Yuji Matsumoto.
2012.
NAIST at theHOO 2012 Shared Task.
In: Proceedings of theSeventh Workshop on Building EducationalApplications Using NLP, pp.
281?288.Andreas Stolcke and others.
2002.
SRILM-an exten-sible language modeling toolkit.
In: Proceedingsof the International Conference on SpokenLanguage Processing, pp.
901?904.Partha Pratim Talukdar and Koby Crammer.
2009.New regularized algorithms for transductive learn-ing.
In: Machine Learning and KnowledgeDiscovery in Databases.
Springer, pp.
442?457.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In: Proceedings of the Acl2010 Conference Short Papers, pp.
353?358.Joel R. Tetreault and Martin Chodorow.
2008.
Theups and downs of preposition error detection inESL writing.
In: Proceedings of the 22nd Inter-national Conference on Computational Lin-guistics Volume 1, pp.
865?872.42
