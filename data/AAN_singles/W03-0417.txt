Training a Naive Bayes Classifier via the EM Algorithm with a ClassDistribution ConstraintYoshimasa Tsuruoka??
and Jun?ichi Tsujii??
?Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN?CREST, JST (Japan Science and Technology Corporation)Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN{tsuruoka,tsujii}@is.s.u-tokyo.ac.jpAbstractCombining a naive Bayes classifier with theEM algorithm is one of the promising ap-proaches for making use of unlabeled data fordisambiguation tasks when using local con-text features including word sense disambigua-tion and spelling correction.
However, the useof unlabeled data via the basic EM algorithmoften causes disastrous performance degrada-tion instead of improving classification perfor-mance, resulting in poor classification perfor-mance on average.
In this study, we introducea class distribution constraint into the iterationprocess of the EM algorithm.
This constraintkeeps the class distribution of unlabeled dataconsistent with the class distribution estimatedfrom labeled data, preventing the EM algorithmfrom converging into an undesirable state.
Ex-perimental results from using 26 confusion setsand a large amount of unlabeled data showthat our proposed method for using unlabeleddata considerably improves classification per-formance when the amount of labeled data issmall.1 IntroductionMany of the tasks in natural language processing canbe addressed as classification problems.
State-of-the-art machine learning techniques including Support Vec-tor Machines (Vapnik, 1995), AdaBoost (Schapire andSinger, 2000) and Maximum Entropy Models (Ratna-parkhi, 1998; Berger et al, 1996) provide high perfor-mance classifiers if one has abundant correctly labeledexamples.However, annotating a large set of examples generallyrequires a huge amount of human labor and time.
Thisannotation cost is one of the major obstacles to applyingmachine learning techniques to real-world NLP applica-tions.Recently, learning algorithms called minimally super-vised learning or unsupervised learning that can make useof unlabeled data have received much attention.
Sincecollecting unlabeled data is generally much easier thanannotating data, such techniques have potential for solv-ing the problem of annotation cost.
Those approaches in-clude a naive Bayes classifier combined with the EM al-gorithm (Dempster et al, 1977; Nigam et al, 2000; Ped-ersen and Bruce, 1998), Co-training (Blum and Mitchell,1998; Collins and Singer, 1999; Nigam and Ghani, 2000),and Transductive Support Vector Machines (Joachims,1999).
These algorithms have been applied to sometasks including text classification and word sense disam-biguation and their effectiveness has been demonstratedto some extent.Combining a naive Bayes classifier with the EM algo-rithm is one of the promising minimally supervised ap-proaches because its computational cost is low (linear tothe size of unlabeled data), and it does not require thefeatures to be split into two independent sets unlike co-training.However, the use of unlabeled data via the basic EMalgorithm does not always improve classification perfor-mance.
In fact, this often causes disastrous performancedegradation resulting in poor classification performanceon average.
To alleviate this problem, we introduce aclass distribution constraint into the iteration process ofthe EM algorithm.
This constraint keeps the class dis-tribution of unlabeled data consistent with the class dis-tribution estimated from labeled data, preventing the EMalgorithm from converging into an undesirable state.In order to assess the effectiveness of the proposedmethod, we applied it to the problem of semantic disam-biguation using local context features.
Experiments wereconducted with 26 confusion sets and a large number ofunlabeled examples collected from a corpus of one hun-dred million words.This paper is organized as follows.
Section 2 brieflyreviews the naive Bayes classifier and the EM algorithmas means of using unlabeled data.
Section 3 presents theidea of using a class distribution constraint and how toimpose this constraint on the learning process.
Section4 describes the problem of confusion set disambiguationand the features used in the experiments.
Experimentalresults are presented in Section 5.
Related work is dis-cussed in Section 6.
Section 7 offers some concludingremarks.2 Naive Bayes ClassifierThe naive Bayes classifier is a simple but effective classi-fier which has been used in numerous applications of in-formation processing such as image recognition, naturallanguage processing, information retrieval, etc.
(Escud-ero et al, 2000; Lewis, 1998; Nigam and Ghani, 2000;Pedersen, 2000).In this section, we briefly review the naive Bayes clas-sifier and the EM algorithm that is used for making useof unlabeled data.2.1 Naive Bayes ModelLet x be a vector we want to classify, and ck be a possibleclass.
What we want to know is the probability that thevector x belongs to the class ck.
We first transform theprobability P (ck|x) using Bayes?
rule,P (ck|x) = P (ck)?P (x|ck)P (x).
(1)Class probability P (ck) can be estimated from trainingdata.
However, direct estimation of P (ck|x) is impossi-ble in most cases because of the sparseness of trainingdata.By assuming the conditional independence of the ele-ments of a vector, P (x|ck) is decomposed as follows,P (x|ck) =d?j=1P (xj |ck), (2)where xj is the jth element of vector x.
Then Equation 1becomesP (ck|x) = P (ck)?
?dj=1 P (xj |ck)P (x).
(3)With this equation, we can calculate P (ck|x) and classifyx into the class with the highest P (ck|x).Note that the naive Bayes classifier assumes the con-ditional independence of features.
This assumption how-ever does not hold in most cases.
For example, word oc-currence is a commonly used feature for text classifica-tion.
However, obvious strong dependencies exist amongword occurrences.
Despite this apparent violation of theassumption, the naive Bayes classifier exhibits good per-formance for various natural language processing tasks.There are some implementation variants of the naiveBayes classifier depending on their event models (Mc-Callum and Nigam, 1998).
In this paper, we adopt themulti-variate Bernoulli event model.
Smoothing wasdone by replacing zero-probability with a very small con-stant (1.0?
10?4).2.2 EM AlgorithmThe Expectation Maximization (EM) algorithm (Demp-ster et al, 1977) is a general framework for estimatingthe parameters of a probability model when the data hasmissing values.
This algorithm can be applied to min-imally supervised learning, in which the missing valuescorrespond to missing labels of the examples.The EM algorithm consists of the E-step in which theexpected values of the missing sufficient statistics giventhe observed data and the current parameter estimates arecomputed, and the M-step in which the expected valuesof the sufficient statistics computed in the E-step are usedto compute complete data maximum likelihood estimatesof the parameters (Dempster et al, 1977).In our implementation of the EM algorithm with thenaive Bayes classifier, the learning process using unla-beled data proceeds as follows:1.
Train the classifier using only labeled data.2.
Classify unlabeled examples, assigning probabilisticlabels to them.3.
Update the parameters of the model.
Each proba-bilistically labeled example is counted as its proba-bility instead of one.4.
Go back to (2) until convergence.3 Class Distribution Constraint3.1 MotivationAs described in the previous section, the naive Bayesclassifier can be easily extended to exploit unlabeled databy using the EM algorithm.
However, the use of unla-beled data for actual tasks exhibits mixed results.
Theperformance is improved for some cases, but not in allcases.
In our preliminary experiments, using unlabeleddata by means of the EM algorithm often caused signifi-cant deterioration of classification performance.To investigate the cause of this, we observed thechange of class distribution of unlabeled data occuring inthe process of the EM algorithm.
What we found is thatsometimes the class distribution of unlabeled data greatlydiverges from that of the labeled data.
For example, whenthe proportion of class A examples in labeled data wasabout 0.9, the EM algorithm would sometimes convergeinto states where the proportion of class A is about 0.7.This divergence of class distribution clearly indicated theEM algorithm converged into an undesirable state.One of the possible remedies for this phenomenon isthat of forcing class distribution of unlabeled data not todiverge from the class distribution estimated from labeleddata.
In this work, we introduce a class distribution con-straint (CDC) into the training process of the EM algo-rithm.
This constraint keeps the class distribution of un-labeled data consistent with that of labeled data.3.2 Calibrating Probabilistic LabelsWe implement class distribution constraints by calibrat-ing probabilistic labels assigned to unlabeled data in theprocess of the EM algorithm.
In this work, we consideronly binary classification: classes A and B.Let pi be the probabilistic label of the ith examplerepresenting the probability that this example belongs toclass A.Let ?
be the proportion of class A examples in the la-beled data L. If the proportion of the class A examples(the proportion of the examples whose p i is greater than0.5) in unlabeled data U is different from ?, we considerthat the values of the probabilistic labels should be cali-brated.The basic idea of the calibration is to shift all the prob-ability values of unlabeled data to the extent that the classdistribution of unlabeled data becomes identical to that oflabeled data.
In order for the shifting of the probabilityvalues not to cause the values to go outside of the rangefrom 0 to 1, we transform the probability values by aninverse sigmoid function in advance.
After the shifting,the values are returned to probability values by a sigmoidfunction.The whole calibration process is given below:1.
Transform the probabilistic labels p1, ...pn by the in-verse function of the sigmoid function,f(x) =11 + e?x.
(4)into real value ranging from ??
to ?.
Let thetransformed values be q1, ...qn.2.
Sort q1, ...qn in descending order.
Then, pick up thevalue qborder that is located at the position of pro-portion ?
in these n values.3.
Since qborder is located at the border between theexamples of label A and those of label B, the valueshould be close to zero (= probability is 0.5).
Thuswe calibrate all qi by subtracting qborder.4.
Transform q1, ...qn by a sigmoid function back intoprobability values.This calibration process is conducted between the E-step and the M-step in the EM algorithm.4 Confusion Set DisambiguationWe applied the naive Bayes classifier with the EM algo-rithm to confusion set disambiguation.
Confusion set dis-ambiguation is defined as the problem of choosing thecorrect word from a set of words that are commonlyconfused.
For example, quite may easily be mistypedas quiet.
An automatic proofreading system wouldneed to judge which is the correct use given the con-text surrounding the target.
Example confusion sets in-clude: {principle, principal}, {then, than}, and {weather,whether}.Until now, many methods have been proposed for thisproblem including winnow-based algorithms (Goldingand Roth, 1999), differential grammars (Powers, 1998),transformation based learning (Mangu and Brill, 1997),decision lists (Yarowsky, 1994).Confusion set disambiguation has very similar char-acteristics to a word sense disambiguation problem inwhich the system has to identify the meaning of a pol-ysemous word given the surrounding context.
The meritof using confusion set disambiguation as a test-bed for alearning algorithm is that since one does not need to an-notate the examples to make labeled data, one can con-duct experiments using an arbitrary amount of labeleddata.4.1 FeaturesAs the input of the classifier, the context of the target mustbe represented in the form of a vector.
We use a binaryfeature vector which contains only the values of 0 or 1 foreach element.In this work, we use the local context surrounding thetarget as the feature of an example.
The features of atarget are the two preceding words and the two followingwords.
For example, if the disambiguation target is quietand the system is given the following sentence?...between busy and quiet periods and it...?the contexts of this example are represented as follows:busy?2, and?1, periods+1, and+2In the input vector, only the elements corresponding tothese features are set to 1, while all the other elements areset to 0.Table 1: Confusion Sets used in the ExperimentsConfusion Set Baseline #UnlabeledI, me 86.4 474726accept, except 53.2 14876affect, effect 79.1 20653among, between 80.1 101621amount, number 76.1 50310begin, being 93.0 82448cite, sight 95.1 3498country, county 80.8 17810fewer, less 91.6 35413its, it?s 83.7 177488lead, led 53.5 25195maybe, may be 92.4 36519passed, past 66.8 24450peace, piece 57.0 11219principal, principle 61.7 8670quiet, quite 88.8 29618raise, rise 60.8 13392sight, site 61.1 9618site, cite 96.0 5594than, then 63.8 216286their, there 63.8 372471there, they?re 96.4 146462they?re, their 96.9 237443weather, whether 87.5 29730your, you?re 88.6 108185AVERAGE 78.2 901475 ExperimentTo conduct large scale experiments, we used the BritishNational Corpus 1 that is currently one of the largest cor-pora available.
The corpus contains roughly one hundredmillion words collected from various sources.The confusion sets used in our experiments are thesame as in Golding?s experiment (1999).
Since our al-gorithm requires the classification to be binary, we de-composed three-class confusion sets into pairwise binaryclassifications.
Table 1 shows the resulting confusion setsused in the following experiments.
The baseline perfor-mances, achieved by simply selecting the majority class,are shown in the second column.
The number of unla-beled data are shown in the rightmost column.The 1,000 test sets were randomly selected from thecorpus for each confusion set.
They do not overlap thelabeled data or the unlabeled data used in the learningprocess.1Data cited herein has been extracted from the British Na-tional Corpus Online service, managed by Oxford UniversityComputing Services on behalf of the BNC Consortium.
Allrights in the texts cited are reserved.Table 2: Results of Confusion Sets Disambiguation with32 Labeled DataNB + EMConfusion Set NB NB+EM +CDCI, me 87.4 96.3 96.0accept, except 77.2 89.0 81.1affect, effect 86.4 91.6 93.6among, between 80.1 64.4 79.5amount, number 69.6 61.6 68.8begin, being 95.1 86.6 95.1cite, sight 95.1 95.1 95.1country, county 77.5 70.4 76.0fewer, less 89.0 77.4 85.4its, it?s 85.3 92.3 94.2lead, led 65.3 64.2 63.7maybe, may be 91.1 77.6 92.9passed, past 77.9 70.2 82.0peace, piece 78.4 81.5 82.1principal, principle 72.8 88.7 79.4quiet, quite 85.3 75.9 83.5raise, rise 83.7 86.1 81.0sight, site 67.7 68.7 67.9site, cite 96.2 93.3 92.8than, then 74.7 84.0 85.3their, there 88.4 91.4 90.2there, they?re 96.4 96.4 89.1they?re, their 96.9 96.9 96.9weather, whether 90.6 92.3 93.7your, you?re 87.8 81.8 90.3AVERAGE 83.8 82.9 85.4The results are shown in Table 2 through Table 5.These four tables correspond to the cases in which thenumber of labeled examples is 32, 64, 128 and 256 asindicated by the table captions.
The first column showsthe confusion sets.
The second column shows the clas-sification performance of the naive Bayes classifier withwhich only labeled data was used for training.
The thirdcolumn shows the performance of the naive Bayes classi-fier with which unlabeled data was used via the basic EMalgorithm.
The rightmost column shows the performanceof the EM algorithm that was extended with our proposedcalibration process.Notice that the effect of unlabeled data were very dif-ferent for each confusion set.
As shown in Table 2, theprecision was significantly improved for some confusionsets including {I, me}, {accept, except} and {affect, ef-fect} .
However, disastrous performance deteriorationcan be observed, especially that of the basic EM algo-rithm, in some confusion sets including {among, be-tween}, {country, county}, and {site, cite}.On average, precision was degraded by the use of un-Table 3: Results of Confusion Sets Disambiguation with64 Labeled DataNB + EMConfusion Set NB NB+EM +CDCI, me 89.4 96.8 95.7accept, except 82.9 89.3 87.5affect, effect 89.4 92.4 93.6among, between 79.9 76.3 80.5amount, number 71.5 68.7 69.1begin, being 95.8 92.1 95.7cite, sight 95.1 95.8 96.4country, county 78.7 73.4 74.5fewer, less 87.6 74.3 87.3its, it?s 85.8 94.0 92.5lead, led 76.2 66.8 72.8maybe, may be 92.6 84.0 96.2passed, past 79.7 72.5 88.4peace, piece 81.1 81.2 82.4principal, principle 75.2 90.2 89.8quiet, quite 86.5 84.0 89.2raise, rise 85.7 85.6 86.9sight, site 71.9 69.0 69.0site, cite 96.3 95.8 95.5than, then 79.7 83.8 83.2their, there 90.5 91.9 92.1there, they?re 96.2 85.2 91.4they?re, their 96.9 96.9 95.8weather, whether 90.6 91.4 93.3your, you?re 88.0 83.3 94.2AVERAGE 85.7 84.6 87.7labeled data via the basic EM algorithm (from 83.3% to82.9%).
On the other hand, the EM algorithm with theclass distribution constraint improved average classifica-tion performance (from 83.3% to 85.4%).
This improvedprecision nearly reached the performance achieved bytwice the size of labeled data without unlabeled data (seethe average precision of NB in Table 3).
This perfor-mance gain indicates that the use of unlabeled data ef-fectively doubles the labeled training data.In Table 3, the tendency of performance improvement(or degradation) in the use of unlabeled data is almost thesame as in Table 2.
The basic EM algorithm degraded theperformance on average, while our method improved av-erage performance (from 85.7% to 87.7%).
This perfor-mance gain effectively doubled the size of labeled data.The results with 128 labeled examples are shown in Ta-ble 4.
Although the use of unlabeled examples by meansof our proposed method still improved average perfor-mance (from 87.6% to 88.6%), the gain is smaller thanthat for a smaller amount of labeled data.With 256 labeled examples (Table 5), the average per-Table 4: Results of Confusion Sets Disambiguation with128 Labeled DataNB + EMConfusion Set NB NB+EM +CDCI, me 90.7 96.9 96.4accept, except 85.7 90.7 89.4affect, effect 91.9 93.1 93.3among, between 80.0 76.3 80.1amount, number 78.2 68.9 69.3begin, being 94.4 88.1 95.0cite, sight 96.9 96.9 98.1country, county 81.3 75.1 75.7fewer, less 89.9 74.9 89.4its, it?s 88.6 93.2 95.2lead, led 80.5 82.5 82.2maybe, may be 94.5 80.9 94.4passed, past 81.8 74.1 85.5peace, piece 84.1 81.3 82.5principal, principle 79.8 89.8 89.5quiet, quite 86.5 82.7 90.1raise, rise 85.2 86.4 87.7sight, site 75.6 70.3 70.5site, cite 96.1 95.8 97.0than, then 81.7 84.2 84.5their, there 91.8 91.5 91.2there, they?re 95.9 83.4 91.3they?re, their 96.9 96.9 96.7weather, whether 92.0 92.6 95.1your, you?re 88.9 84.1 94.5AVERAGE 87.6 85.2 88.6formance gain was negligible (from 89.2% to 89.3%).Figure 1 summarizes the average precisions for differ-ent number of labeled examples.
Average peformancewas improved by the use of unlabeled data with our pro-posed method when the amount of labeled data was small(from 32 to 256) as shown in Table 2 through Table5.
However, when the number of labeled examples waslarge (more than 512), the use of unlabeled data degradedaverage performance.5.1 Effect of the amount of unlabeled dataWhen the use of unlabeled data improves classificationperformance, the question of how much unlabeled dataare needed becomes very important.
Although unlabeleddata are generally much more obtainable than labeleddata, acquiring more than several-thousand unlabeled ex-amples is not always an easy task.
As for confusion setdisambiguation, Table 1 indicates that it is sometimes im-possible to collect tens of thousands examples even in avery large corpus.In order to investigate the effect of the amount of un-Table 5: Results of Confusion Sets Disambiguation with256 Labeled DataNB + EMConfusion Set NB NB+EM +CDCI, me 93.4 96.6 96.4accept, except 89.7 90.3 91.2affect, effect 93.4 93.5 93.9among, between 79.6 75.1 80.4amount, number 81.4 68.9 69.2begin, being 94.6 89.9 96.6cite, sight 97.6 97.9 98.4country, county 84.2 76.5 77.5fewer, less 90.8 83.0 89.2its, it?s 90.2 93.3 94.5lead, led 82.9 79.8 82.6maybe, may be 96.0 87.1 94.7passed, past 83.5 74.6 86.3peace, piece 84.6 81.4 85.7principal, principle 83.4 90.5 90.5quiet, quite 88.6 86.8 91.2raise, rise 88.0 87.1 88.4sight, site 79.2 71.7 73.2site, cite 97.3 97.6 97.4than, then 82.3 85.5 85.9their, there 93.6 92.1 92.0there, they?re 96.5 83.0 91.1they?re, their 96.8 90.8 97.3weather, whether 93.8 91.9 94.7your, you?re 89.7 83.8 94.6AVERAGE 89.2 85.9 89.3labeled data, we conducted experiments by varying theamount of unlabeled data for some confusion sets that ex-hibited significant performance gain by using unlabeleddata.Figure 2 shows the relationship between the classifica-tion performance and the amount of unlabeled data forthree confusion sets: {I, me}, {principal, principle}, and{passed, past}.
The number of labeled examples in allcases was 64.Note that performance continued to improve evenwhen the number of unlabeled data reached more thanten thousands.
This suggests that we can further improvethe performance for some confusion sets by using a verylarge corpus containing more than one hundred millionwords.Figure 2 also indicates that the use of unlabeled datawas not effective when the amount of unlabeled data wassmaller than one thousand.
It is often the case with mi-nor words that the number of occurrences does not reachone thousand even in a one-hundred-million word corpus.Thus, constructing a very very large corpus (containing7580859095100100 1000Precision(%)Number of Labeled ExamplesNBNB+EMNB+EM+CDCFigure 1: Relationship between Average Precision andthe Amount of Labeled Data60657075808590951001000 10000 100000Precision(%)Number of Unlabeled ExamplesI, meprincipal, principlepassed, pastFigure 2: Relationship between Precision and theAmount of Unlabeled Datamore than billions of words) appears to be beneficial forinfrequent words.6 Related WorkNigam et al(2000) reported that the accuracy of text clas-sification can be improved by a large pool of unlabeleddocuments using a naive Bayes classifier and the EM al-gorithm.
They presented two extensions to the basic EMalgorithm.
One is a weighting factor to modulate the con-tribution of the unlabeled data.
The other is the use ofmultiple mixture components per class.
With these exten-sions, they reported that the use of unlabeled data reducesclassification error by up to 30%.Pedersen et al(1998) employed the EM algorithm andGibbs Sampling for word sense disambiguation by usinga naive Bayes classifier.
Although Gibbs Sampling re-sults in a small improvement over the EM algorithm, theresults for verbs and adjectives did not reach baseline per-formance on average.
The amount of unlabeled data usedin their experiments was relatively small (from severalhundreds to a few thousands).Yarowsky (1995) presented an approach that signif-icantly reduces the amount of labeled data needed forword sense disambiguation.
Yarowsky achieved accura-cies of more than 90% for two-sense polysemous words.This success was likely due to the use of ?one sense perdiscourse?
characteristic of polysemous words.Yarowsky?s approach can be viewed in the context ofco-training (Blum and Mitchell, 1998) in which the fea-tures can be split into two independent sets.
For wordsense disambiguation, the sets correspond to the localcontexts of the target word and the ?one sense per dis-course?
characteristic.
Confusion sets however do nothave the latter characteristic.The effect of a huge amount of unlabeled data forconfusion set disambiguation is discussed in (Banko andBrill, 2001).
Bank and Brill conducted experiments ofcommittee-based unsupervised learning for two confu-sion sets.
Their results showed that they gained a slightimprovement by using a certain amount of unlabeleddata.
However, test set accuracy began to decline as ad-ditional data were harvested.As for the performance of confusion set disambigua-tion, Golding (1999) achieved over 96% by a winnow-based approach.
Although our results are not directlycomparable with their results since the data sets aredifferent, our results does not reach the state-of-the-art performance.
Because the performance of a naiveBayes classifier is significantly affected by the smoothingmethod used for paramter estimation, there is a chance toimprove our performance by using a more sophisticatedsmoothing technique.7 ConclusionThe naive Bayes classifier can be combined with the well-established EM algorithm to exploit the unlabeled data.
However, the use of unlabeled data sometimes causesdisastrous degradation of classification performance.In this paper, we introduce a class distribution con-straint into the iteration process of the EM algorithm.This constraint keeps the class distribution of unlabeleddata consistent with the true class distribution estimatedfrom labeled data, preventing the EM algorithm fromconverging into an undesirable state.Experimental results using 26 confusion sets and alarge amount of unlabeled data showed that combiningthe EM algorithm with our proposed constraint consis-tently reduced the average classification error rates whenthe amount of labeled data is small.
The results alsoshowed that use of unlabeled data is especially advan-tageous when the amount of labeled data is small (up toabout one hundred).7.1 Future WorkIn this paper, we empirically demonstrated that a classdistribution constraint reduced the chance of undesirableconvergence of the EM algorithm.
However, the theoret-ical justification of this constraint should be clarified infuture work.ReferencesMichele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InProceedings of the Association for Computational Lin-guistics.Adam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71.Avrim Blum and Tom Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.
InCOLT: Proceedings of the Workshop on Computa-tional Learning Theory, Morgan Kaufmann Publish-ers.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Royal Statstical Society B 39, pages 1?38.G.
Escudero, L. arquez, and G. Rigau.
2000.
Naive bayesand exemplar-based approaches to word sense disam-biguation revisited.
In Proceedings of the 14th Euro-pean Conference on Artificial Intelligence.Andrew R. Golding and Dan Roth.
1999.
A winnow-based approach to context-sensitive spelling correc-tion.
Machine Learning, 34(1-3):107?130.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InProc.
16th International Conf.
on Machine Learning,pages 200?209.
Morgan Kaufmann, San Francisco,CA.David D. Lewis.
1998.
Naive Bayes at forty: The in-dependence assumption in information retrieval.
InClaire Ne?dellec and Ce?line Rouveirol, editors, Pro-ceedings of ECML-98, 10th European Conference onMachine Learning, number 1398, pages 4?15, Chem-nitz, DE.
Springer Verlag, Heidelberg, DE.Lidia Mangu and Eric Brill.
1997.
Automatic rule acqui-sition for spelling correction.
In Proc.
14th Interna-tional Conference on Machine Learning, pages 187?194.
Morgan Kaufmann.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for naive bayes text classifica-tion.
In AAAI-98 Workshop on Learning for Text Cat-egorization.Kamal Nigam and Rayid Ghani.
2000.
Analyzing the ef-fectiveness and applicability of co-training.
In CIKM,pages 86?93.Kamal Nigam, Andrew Kachites Mccallum, SebastianThrun, and Tom Mitchell.
2000.
Text classificationfrom labeled and unlabeled documents using EM.
Ma-chine Learning, 39(2/3):103?134.Ted Pedersen and Rebecca Bruce.
1998.
Knowledgelean word-sense disambiguation.
In AAAI/IAAI, pages800?805.Ted Pedersen.
2000.
A simple approach to building en-sembles of naive bayesian classifiers for word sensedisambiguation.
In Proceedings of the First AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 63?69,Seattle, WA, May.David M. W. Powers.
1998.
Learning and applicationof differential grammars.
In T. Mark Ellison, editor,CoNLL97: Computational Natural Language Learn-ing, pages 88?96.
Association for Computational Lin-guistics, Somerset, New Jersey.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, the University of Pennsylvania.Robert E. Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based system for text categorization.Machine Learning, 39(2/3):135?168.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
New York.David Yarowsky.
1994.
Decision lists for lexical ambi-guity resolution: Application to accent restoration inspanish and french.
In Meeting of the Association forComputational Linguistics, pages 88?95.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
Proc.
of the33rd Annual Meeting of the Association for Computa-tional Linguistics, pages 189?196.
