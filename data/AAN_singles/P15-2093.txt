Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 567?572,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Cross-lingual Word Embeddings via Matrix Co-factorizationTianze Shi Zhiyuan Liu Yang Liu Maosong SunState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and TechnologyTsinghua University, Beijing 100084, Chinastz11@mails.tsinghua.edu.cn{liuzy, liuyang2011, sms}@tsinghua.edu.cnAbstractA joint-space model for cross-lingualdistributed representations generalizeslanguage-invariant semantic features.In this paper, we present a matrix co-factorization framework for learningcross-lingual word embeddings.
Weexplicitly define monolingual trainingobjectives in the form of matrix de-composition, and induce cross-lingualconstraints for simultaneously factorizingmonolingual matrices.
The cross-lingualconstraints can be derived from parallelcorpora, with or without word alignments.Empirical results on a task of cross-lingualdocument classification show that ourmethod is effective to encode cross-lingualknowledge as constraints for cross-lingualword embeddings.1 IntroductionWord embeddings allow one to represent words ina continuous vector space, which characterizes thelexico-semanic relations among words.
In manyNLP tasks, they prove to be high-quality features,successful applications of which include languagemodelling (Bengio et al., 2003), sentiment analy-sis (Socher et al., 2011) and word sense discrimi-nation (Huang et al., 2012).Like words having synonyms in the same lan-guage, there are also word pairs across lan-guages which share resembling semantic proper-ties.
Mikolov et al.
(2013a) observed a strongsimilarity of the geometric arrangements of cor-responding concepts between the vector spaces ofdifferent languages, and suggested that a cross-lingual mapping between the two vector spaces istechnically plausible.
In the meantime, the joint-space models for cross-lingual word embeddingsare very desirable, as language-invariant seman-tic features can be generalized to make it easy totransfer models across languages.
This is espe-cially important for those low-resource languages,where it allows one to develop accurate word rep-resentations of one language by exploiting theabundant textual resources in another language,e.g., English, which has a high resource density.The joint-space models are not only technicallyplausible, but also useful for cross-lingual modeltransfer.
Further, studies have shown that usingcross-lingual correlation can improve the qualityof word representations trained solely with mono-lingual corpora (Faruqui and Dyer, 2014).Defining a cross-lingual learning objective iscrucial at the core of the joint-space model.
Her-mann and Blunsom (2014) and Chandar A P etal.
(2014) tried to calculate parallel sentence (ordocument) representations and to minimize thedifferences between the semantically equivalen-t pairs.
These methods are useful in capturingsemantic information carried by high-level units(such as phrases and beyond) and usually do notrely on word alignments.
However, they sufferfrom reduced accuracy for representing rare to-kens, whose semantic information may not be wellgeneralized.
In these cases, finer-grained informa-tion at lexical level, such as aligned word pairs,dictionaries, and word translation probabilities, isconsidered to be helpful.Ko?cisk`y et al.
(2014) integrated word aligningprocess and word embedding in machine transla-tion models.
This method makes full use of paral-lel corpora and produces high-quality word align-ments.
However, it is unable to exploit the richermonolingual corpora.
On the other hand, Zou et al.
(2013) and Faruqui and Dyer (2014) learnt wordembeddings of different languages in separate s-paces with monolingual corpora and projected theembeddings into a joint space, but they can onlycapture linear transformation.In this paper, we address the above challengeswith a framework of matrix co-factorization.
We567simultaneously learn word embeddings in multi-ple languages via matrix factorization, with in-duced constraints to assure cross-lingual seman-tic relations.
It provides the flexibility of con-structing learning objectives from separate mono-lingual and cross-lingual corpora.
Intricate rela-tions across languages, rather than simple linearprojections, are automatically captured.
Addition-ally, our method is efficient as it learns from globalstatistics.
The cross-lingual constraints can be de-rived both with or without word alignments, giventhat there is a valid measure of cross-lingual co-occurrences or similarities.We test the performance in a task of cross-lingual document classification.
Empirical result-s and a visualization of the joint semantic spacedemonstrate the validity of our model.2 FrameworkWithout loss of generality, here we only considerbilingual embedding learning of the two languagesl1and l2.
Given monolingual corpora Dliandsentence-aligned parallel data Dbi, our task is tofind word embedding matrices of the size |Vli|?dwhere each line corresponds to the embedding ofa single word.
We also define vocabularies of con-textsUliand we learn context embedding matricesCliof the size |Uli| ?
d at the same time.1These matrices are obtained by simultaneousmatrix factorization of the monolingual word-context PMI (point-wise mutual information) ma-trices Mli.
During monolingual factorization, weput a cross-lingual constraint (cost) on it, ensuringcross-lingual semantic relations.
We formalize theglobal loss function asLtotal=?i?{1,2}?i?
Lmono(Wli, Cli)+?c?
Lcross(Wl1, Cl1,Wl2, Cl2),(1)where Lmonoand Lcrossare the monolingual andcross-lingual objectives respectively.
?iand ?cweigh the contribution of the different parts to thetotal objective.
An overview of our algorithm isillustrated in Figure 1.3 Monolingual ObjectiveOur monolingual objective follows the GloVemodel (Pennington et al., 2014), which learnsfrom global word co-occurrence statistics.
For aword-context pair (j, k) in language li, we try to1In this paper, we let Uli= Vli.MonolingualcorporaL1L2Bilingual corpus???
???????
?bilingual relationsand constraints??????????
?PMImatricesL1-L2Figure 1: The framework of cross-lingual word embeddingvia matrix co-factorization.minimize the difference between the dot produc-t of the embeddings wlij?
clikand their PMI valueMlijk.
Mlijk=Xlijk??j,kXlijk?jXlijk?
?kXlijk, where Xliis thematrix of word-context co-occurrence counts.
AsPennington et al.
(2014), we add separate termsbliwj, blickfor each word and context to absorb theeffect of any possible word-specific biases.
We al-so add an additional matrix bias blifor the easeof sharing embeddings among matrices.
The lossfunction is written as the sum of the weightedsquare error,Llimono=?j,kf(Xlijk)(wlij?
clik+ bliwj+ blick+ bli?Mlijk)2,(2)where we choose the same weighting function asthe GloVe model to place less confidence on thoseword-context pairs with rare occurrences,f(x) ={(x/xmax)?if x < xmax1 otherwise.
(3)Notice that we only have to optimize those Xlijk6=0, which can be solved efficiently since the matrixof co-occurrence counts is usually sparse.4 Cross-lingual ObjectivesAs the most important part in our model, the cross-lingual objective describes the cross-lingual wordrelations and sets constraints when we factorizemonolingual co-occurrence matrices.
It can be de-rived from either cross-lingual co-occurrences orsimilarities between cross-lingual word pairs.4.1 Cross-lingual ContextsThe monolingual objective stems from the distri-butional hypothesis (Harris, 1954) and optimizes568words in similar contexts into similar embeddings.It is natural to further extend this idea to definecross-lingual contexts, for which we have multi-ple choices.For the definition of cross-lingual contexts, wehave multiple choices.
A straightforward optionis to count all the word co-occurrences in alignedsentence pairs, which is equivalent to a uniformword alignment model adopted by Gouws et al.(2015).
For the sentence-aligned bilingual corpusDbi= {(Sl1, Sl2)}, where each Sliis a monolin-gual sentence, we count the co-occurrences asXbijk=?
(Sl1,Sl2)?Dbi#(j, Sl1)?#(k, Sl2), (4)where Xbiis the matrix of cross-lingual co-occurrence counts, and #(j, S) is a functioncounting the number of j?s in the sequence S. Wethen use a similar loss function as Equation 2, withthe exception that we optimize for the dot product-s of wl1j?
wl2k.
This method works without wordalignments and we denote it as CLC-WA (Cross-lingual context without word alignments).We can also leverage word alignments and de-fine CLC+WA (Cross-lingual context with wordalignments).
The idea is to count those word-s co-occurring with k as the context of j, wherek ?
Vl2is the translationally equivalent wordof j ?
Vl1.
An example is shown in Figure 2.CLC+WA is expected to contain more precise in-formation than CLC-WA, and we will compare thetwo definitions in the following experiments.Once we have counted the co-occurrences, ana?
?ve solution is to concatenate the bilingual vo-cabularies and perform matrix factorization as awhole.
To induce additional flexibility, such asseparate weighting, we divide the matrix into threeparts.
It is also more reasonable to calculate PMIvalues without mixing the monolingual and bilin-gual corpora.4.2 Cross-lingual SimilaritiesAn alternative way to set cross-lingual constraintsis to minimize the distances between similar wordpairs.
Here the semantic similarities can be mea-sured by equivalence in translation, sim(j, k),which is produced by a machine translation sys-tem.
In this paper, we use the translation proba-bilities produced by a machine translation system.Minimizing the distances of related words in thetwo languages weighted by their similarities givesus the cross-lingual objective?
we    must    do    all    we    can,    not    just    to   ??
wir    alles    daran    setzen    m?ssen, nicht nur ?Figure 2: An example of CLC+WA, where we show thecross-lingual context of the German word ?m?ussen?
in thedashed box.Table 1: Accuracy for cross-lingual classification.Model en?de de?enMachine translation 68.1 67.4Majority class 46.8 46.8Klementiev et al.
77.6 71.1BiCVM 83.7 71.4BAE 91.8 74.2BilBOWA 86.5 75.0CLC-WA 91.3 77.2CLC+WA 90.0 75.0CLSim 92.7 80.2Lcross=?j?Vl1,k?Vl2sim(j, k) ?
distance(wl1j, wl2k), (5)where wl1jand wl2kare the embeddings of j and kin l1and l2respectively.
In this paper, we choosethe distance function to be the Euclidean distance,distance(wl1j, wl2k) = ||wl1j?
wl2k||2.
Notice thatsimilar to the monolingual objective, we may op-timize for only those sim(j, k) 6= 0, which is ef-ficient as the matrix of translation probabilities ordictionary is sparse.
We call this method CLSim.5 ExperimentsTo evaluate the quality of the relatedness betweenwords in different languages, we induce the taskof cross-lingual document classification for theEnglish-German language pair, where a classifieris trained in one language and later used to classi-fy documents in another.
We exactly replicated theexperiment settings of Klementiev et al.
(2012).5.1 Data and TrainingFor optimizing the monolingual objectives, Weused exactly the same subset of RCV1/RCV2 cor-pora (Lewis et al., 2004) as by Klementiev et al.
(2012), which were sampled to balance the num-ber of tokens between languages.
Our preprocess-ing strategy followed Chandar A P et al.
(2014),where we lowercased all words, removed punctu-ations and used the same vocabularies (|Ven| =43, 614 and |Vde| = 50, 110).
When counting5690.50.550.60.650.70.750.80.850.90.951 10 100AccurayWeight of cross-lingual objectiveen?dede?en(a)0.30.40.50.60.70.80.91 10 100AccurayPercentage of RCV used for training (%)en?dede?en(b)0.50.550.60.650.70.750.80.850.90.951 10 100AccurayPercentage of Europarl used for training (%)en?dede?en(c)Figure 3: Cross-lingual document classification accuracy, with (a) varying weighting of cross-lingual objective (b) varying sizeof training monolingual corpora, and (c) varying size of training bilingual corpus.word co-occurrences, we use a decreasing weight-ing function as Pennington et al.
(2014), where d-word-apart word pairs contribute 1/d to the totalcount.
We used a symmetric window size of 10words for all our experiments.The cross-lingual constraints were derived us-ing the English and German sections of the Eu-roparl v7 parallel corpus (Koehn, 2005), whichwere similarly preprocessed.
For CLC+WA andCLSim, we obtained word alignments and trans-lation probabilities with SyMGIZA++ (Junczys-Dowmunt and Sza?, 2012).
We did not use Eu-roparl for monolingual training.The documents for classification were ran-domly selected by Klementiev et al.
(2012)from those in RCV1/RCV2 that are assignedto only one single topic among the four:CCAT (Corporate/Industrial), ECAT (Economics),GCAT (Government/Social), and MCAT (Market-s).
1,000/5,000 documents in each language wereused as a train/test set and we kept another 1,000documents as a development set for hyperparame-ter tuning.
Each document was represented as anidf-weighted average embedding of all its tokens,and a multi-class document classifier was trainedfor 10 epochs with an averaged perceptron algo-rithm as by Klementiev et al.
(2012).
A classifiertrained with English documents is used to classifyGerman documents and vice versa.We trained our models using stochastic gradientdescent.
We run 50 iterations for all of our exper-iments and the dimensionality of the embeddingsis 40.
We set xmaxto be 100 for cross-lingual co-occurrences and 30 for monolingual ones, while?
is fixed to 3/4.
Other parameters are chosenaccording to the performance on the developmentset.5.2 ResultsWe present the empirical results on the task ofcross-lingual document classification in Table 1,where the performance of our models is comparedwith some baselines and previous work.
The effec-t of weighting between parts of the total objectiveand the amount of training data on the quality ofthe embeddings is demonstrated in Figure 3.The baseline systems are Majority class wheretest documents are simply classified as the classwith the most training samples, and Machinetranslation where a phrased-based machine trans-lation system is used to translate test documentsinto the same language as the training documents.We also summarize the classification accuracyreported in some previous work, including Multi-task learning (Klementiev et al., 2012), Bilingualcompositional vector model (BiCVM) (Herman-n and Blunsom, 2014), Bilingual autoencoder forbags-of-words (BAE) (Chandar A P et al., 2014),and BilBOWA (Gouws et al., 2015).
A more re-cent work of Soyer et al.
(2015) developed a com-positional approach and reported an accuracy of90.8% (en?de) and 80.1% (de?en) when usingfull RCV and Europarl corpora.Our method outperforms the previous work andwe observe improvements when we exploit wordtranslation probabilities (CLSim) over the mod-el without word-level information (CLC-WA).The best result is achieved with CLSim.
Itis interesting to notice that CLC+WA, whichmakes use of word alignments in defining cross-lingual contexts, does not provide better perfor-mance than CLC-WA.
We guess that sentence-level co-occurrence is more suitable for captur-ing sentence-level semantic relations in the task ofdocument classification.570development companybusinessfinancecathedraltowerswisdomlearningphysicianconsultantindustriebranchewirtschaftbanknationalparkweisheittheoriemethodenarzt doktorEnglishGermanFigure 4: A visualization of the joint vector space.5.3 VisualizationFigure 4 gives a visualization of some selectedwords using t-SNE (Van der Maaten and Hin-ton, 2008) where we observe the topical nature ofword embeddings.
Regardless of their source lan-guages, words sharing a common topic, e.g.
econ-omy, are closely aligned with each other, revealingthe semantic validity of the joint vector space.6 Related WorkMatrix factorization has been successfully appliedto learn word representations, which use severallow-rank matrices to approximate the original ma-trix with extracted statistical information, usuallyword co-occurrence counts or PMI.
Singular valuedecomposition (SVD) (Eckart and Young, 1936),SVD-based latent semantic analysis (LSA) (Lan-dauer et al., 1998), latent semantic indexing (LSI)(Deerwester et al., 1990), and the more recently-proposed global vectors for word representation(GloVe) (Pennington et al., 2014) find their wideapplications in the area of NLP and informationretrieval (Berry et al., 1995).
Additionally, there isevidence that some neural-network-based models,such as Skip-gram (Mikolov et al., 2013b) whichexhibits state-of-the-art performance, are also im-plicitly factorizing a PMI-based matrix (Levy andGoldberg, 2014).
The strategy for matrix factor-ization in this paper, as Pennington et al.
(2014),is in a stochastic fashion, which better handles un-observed data and allows one to weigh samples ac-cording to their importance and confidence.Joint matrix factorization allows one to decom-pose matrices with some correlational constraints.Collective matrix factorization has been develope-d to handle pairwise relations (Singh and Gordon,2008).
Chang et al.
(2013) generalized LSA toMulti-Relational LSA, which constructs a 3-waytensor to combine the multiple relations betweenwords.
While matrix factorization is widely usedin recommender systems, matrix co-factorizationhelps to handle multiple aspects of the data andimproves in predicting individual decisions (Honget al., 2013).
Multiple sources of information,such as content and linkage, can also be connectedwith matrix co-factorization to derive high-qualitywebpage representations (Zhu et al., 2007).
Theadvantage of this approach is that it automatical-ly finds optimal parameters to optimize both sin-gle matrix factorization and relational alignments,which avoids manually defining a projection ma-trix or transfer function.
To the best of our knowl-edge, we are the first to introduce this technique tolearn cross-lingual word embeddings.7 ConclusionsIn this paper, we introduced a framework of matrixco-factorization to learn cross-lingual word em-beddings.
It is capable of capturing the lexico-semantic similarities of different languages in aunified vector space, where the embeddings arejointly learnt instead of projected from separatevector spaces.
The overall objective is divided intomonolingual parts and a cross-lingual one, whichenables one to use different weighting and learn-ing strategies, and to develop models either withor without word alignments.
Exploiting globalcontext and similarity information instead of localones, our proposed models are computationally ef-ficient and effective.With matrix co-factorization, it allows one tointegrate external information, such as syntacticcontexts and morphology, which is not discussedin this paper.
Its application in statistical ma-chine translation and cross-lingual model transferremains to be explored.
Learning multiple em-beddings per word and compositional embeddingswith matrix factorization are also interesting fu-ture directions.AcknowledgmentsThis research is supported by the 973 Program(No.
2014CB340501) and the National Natu-ral Science Foundation of China (NSFC No.61133012, 61170196 & 61202140).
We thank theanonymous reviewers for the valuable comments.We also thank Ivan Titov and Alexandre Klemen-tiev for kindly offering their evaluation package,which allowed us to replicate their experiment set-tings exactly.571ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155.Michael W Berry, Susan T Dumais, and Gavin WO?Brien.
1995.
Using linear algebra for intelligentinformation retrieval.
SIAM review, 37(4):573?595.Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In Proceedings of NIPS, pages 1853?1861.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of EMNLP, pages 1602?1612.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JAsIs,41(6):391?407.Carl Eckart and Gale Young.
1936.
The approximationof one matrix by another of lower rank.
Psychome-trika, 1(3):211?218.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proceedings of EACL, pages 462?471.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
Bilbowa: Fast bilingual distributed represen-tations without word alignments.
In ICML, pages748?756.Zellig S Harris.
1954.
Distributional structure.
Word,10(23):146?162.Karl Moritz Hermann and Phil Blunsom.
2014.
Multi-lingual models for compositional distributed seman-tics.
In Proceedings of ACL, pages 58?68.
ACL.Liangjie Hong, Aziz S Doumith, and Brian D Davison.2013.
Co-factorization machines: modeling user in-terests and predicting individual decisions in twitter.In Proceedings of WSDM, pages 557?566.
ACM.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of ACL, pages 873?882.ACL.Marcin Junczys-Dowmunt and Arkadiusz Sza?.
2012.Symgiza++: symmetrized word alignment model-s for statistical machine translation.
In Securityand Intelligent Information Systems, pages 379?390.Springer.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proceedings of COLING.ICCL.Tom?a?s Ko?cisk`y, Karl Moritz Hermann, and Phil Blun-som.
2014.
Learning bilingual word representationsby marginalizing alignments.
In Proceedings of A-CL, pages 224?229.
ACL.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5, pages 79?86.Thomas K Landauer, Peter W Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic anal-ysis.
Discourse processes, 25(2-3):259?284.Omer Levy and Yoav Goldberg.
2014.
Neural wordembedding as implicit matrix factorization.
In Pro-ceedings of NIPS, pages 2177?2185.David D Lewis, Yiming Yang, Tony G Rose, and FanLi.
2004.
Rcv1: A new benchmark collection fortext categorization research.
JMLR, 5:361?397.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.2013a.
Exploiting similarities among languages formachine translation.
arXiv:1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
pages 1532?1543.Ajit P Singh and Geoffrey J Gordon.
2008.
Relationallearning via collective matrix factorization.
In Pro-ceedings of SIGKDD, pages 650?658.
ACM.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofEMNLP, pages 151?161.
ACL.Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa.2015.
Leveraging monolingual data for crosslingualcompositional word representations.
In Proceedingsof ICLR.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-sne.
JMLR, 9:2579?2605.Shenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong.2007.
Combining content and link for classificationusing matrix factorization.
In Proceedings of SIGIR,pages 487?494.
ACM.Will Y Zou, Richard Socher, Daniel M Cer, andChristopher D Manning.
2013.
Bilingual word em-beddings for phrase-based machine translation.
InProceedings of EMNLP, pages 1393?1398.572
