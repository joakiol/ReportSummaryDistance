Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 29?32,New York, June 2006. c?2006 Association for Computational LinguisticsTemporal Classification of Text and Automatic Document DatingAngelo DalliUniversity of Sheffield211, Portobello StreetSheffield, S1 4DP, UKangelo@dcs.shef.ac.ukAbstractTemporal information is presently under-utilised for document and text processingpurposes.
This work presents an unsuper-vised method of extracting periodicity in-formation from text, enabling time seriescreation and filtering to be used in thecreation of sophisticated language modelsthat can discern between repetitive trendsand non-repetitive writing pat-terns.
Thealgorithm performs in O(n log n) time forinput of length n. The temporal languagemodel is used to create rules based ontemporal-word associations inferred fromthe time series.
The rules are used toautomatically guess at likely documentcreation dates, based on the assumptionthat natural languages have unique signa-tures of changing word distributions overtime.
Experimental results on news itemsspanning a nine year period show that theproposed method and algorithms are ac-curate in discovering periodicity patternsand in dating documents automaticallysolely from their content.1 IntroductionVarious features have been used to classify andpredict the characteristics of text and related textdocuments, ranging from simple word count mod-els to sophisticated clustering and Bayesian modelsthat can handle both linear and non-linear classes.The general goal of most classification research isto assign objects from a pre-defined domain (suchas words or entire documents) to two or moreclasses/categories.
Current and past research haslargely focused on solving problems like tagging,sense disambiguation, sentiment classification,author and language identification and topic classi-fication.
We introduce an unsupervised methodthat classifies text and documents according totheir predicted time of writing/creation.
Themethod uses a sophisticated temporal languagemodel to predict likely creation dates for a docu-ment, hence dating it automatically.
This short pa-per presents some background information aboutexisting techniques and the implemented system,followed by a brief explanation of the classifica-tion and dating method, and finally concludingwith results and evaluation performed on the LDCGigaWord English Corpus (LDC, 2003).2 BackgroundTemporal information is presently under-utilisedfor document and text processing purposes.
Pastand ongoing research work has largely focused onthe identification and tagging of temporal expres-sions, with the creation of tagging methodologiessuch as TimeML/TIMEX (Gaizauskas and Setzer,2002; Pustejovsky et al, 2003; Ferro et al, 2004),TDRL (Aramburu and Berlanga, 1998) and associ-ated evaluations such as the ACE TERN competi-tion (Sundheim et al 2004).Temporal analysis has also been applied inQuestion-Answering systems (Pustejovsky et al,2004; Schilder and Habel, 2003; Prager et al,2003), email classification (Kiritchenko et al290100200300400500600237 46 24 12 17 10 19 307 22 3 16 18 13 35 33 31 14 17 5 60100200300400500600237 46 24 12 17 10 19 307 22 3 16 18 13 35 33 31 14 17 5 60100020003000400050006000700080009000100001 164 327 490 653 816 979 1142 1305 1468 1631 1794 1957050010001500200025003000350040001 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081Figure 1 Effects of applying the temporal periodical algorithm on time series for "January" (top) and "the" (bottom)with original series on the left and the remaining time series component after filtering on the right.
Y-axis showsfrequency count and X-axis shows the day number (time).2004), aiding the precision of Information Re-trieval results (Berlanga et al, 2001), documentsummarisation (Mani and Wilson, 2000), timestamping of event clauses (Filatova and Hovy,2001), temporal ordering of events (Mani et al,2003) and temporal reasoning from text (Boguraevand Ando, 2005; Moldovan et al, 2005).
There isalso a large body of work on time series analysisand temporal logic in Physics, Economics andMathematics, providing important techniques andgeneral background information.
In particular, thiswork uses techniques adapted from Seasonal Auto-Regressive Integrated Moving Average models(SARIMA).
SARIMA models are a class of sea-sonal, non-stationary temporal models based on theARIMA process (defined as a non-stationary ex-tension of the stationary ARMA model).
Non-stationary ARIMA processes are defined by:( ) ( ) ( ) ttd ZBXBB ??
=?1            (1)where d is non-negative integer, and ( )X?
( )X?
polynomials of degrees p and q respec-tively.
The exact parameters for each process (oneprocess per word) are determined automatically bythe system.
A discussion of the general SARIMAmodel is beyond the scope of this paper (detailscan be found in Mathematics & Physics publica-tions).
The NLP application of temporal classifica-tion and prediction to guess at likely document andtext creation dates is a novel application that hasnot been considered much before, if at all.3 Temporal Periodicity AnalysisWe have created a high-performance system thatdecomposes time series into two parts: a periodiccomponent that repeats itself in a predictable man-ner, and a non-periodic component that is left afterthe periodic component has been filtered out fromthe original time series.
Figure 1 shows an exampleof the filtering results on time-series of the words?January?
and ?the?.
The time series are based ontraining documents selected at random from theGigaWord English corpus.
10% of all the docu-ments in the corpus were used as training docu-ments, with the rest being available for evaluationand testing.
A total of 395,944 time series spanning9 years were calculated from the GigaWord cor-pus.
Figure 2 presents pseudo-code for the timeseries decomposition algorithm:301.
Find min/max/mean and standard devia-tion of time series2.
Start with a pre-defined maximum win-dow size (presently set to 366 days)3.
While window size bigger than 1 repeatsteps a. to d. below:a.
Look at current value in timeseries (starting first value)b.
Do values at positions current,current + window size, current +2 x window size, etc.
vary byless than ?
standard deviation?c.
If yes, mark currentvalue/window size pair as beingpossible decomposition matchd.
Look at next value in time se-ries until the end is reachede.
Decrease window size by one4.
Select the minimum number of decompo-sition matches that cover the entiretime series using a greedy algorithmFigure 2 Time Series Decomposition AlgorithmThe time series decomposition algorithm wasapplied to the 395,944 time series, taking an aver-age of 419ms per series.
The algorithm runs in O(nlog n) time for a time series of length n.The periodic component of the time series isthen analysed to extract temporal association rulesbetween words and different ?seasons?, includingDay of Week, Week Number, Month Number,Quarter, and Year.
The procedure of determining ifa word, for example, is predominantly peaking ona weekly basis, is to apply a sliding window of size7 (in the case of weekly periods) and determiningif the periodic time series always spikes within thiswindow.
Figure 3 shows the frequency distributionof the periodic time series component of the daysof week names (?Monday?, ?Tuesday?, etc.)
Notethat the frequency counts peak exactly on that par-ticular day of the week.
For example, the word?Monday?
is automatically associated with Day 1,and ?April?
associated with Month 4.
The creationof temporal association rules generalises inferencesobtained from the periodic data.
Each associationrule has the following information:?
Word ID?
Period Type (Week, Month, etc.)?
Period Number and Score MatrixThe period number and score matrix represent aprobability density function that shows the likeli-hood of a word appearing on a particular periodnumber.
For example, the score matrix for ?Janu-ary?
will have a high score for period 1 (and periodtype set to Monthly).
Figure 4 shows some exam-ples of extracted association rules.
The PDF scoresare shown in Figure 4 as they are stored internally(as multiples of the standard deviation of that timeseries) and are automatically normalised during theclassification process at runtime.
Rule generalisa-tion is not possible in such a straightforward man-ner for the non-periodic data.
The use of non-periodic data to optimise the results of the temporalclassification and automatic dating system is notcovered in this paper.4 Temporal Classification and DatingThe periodic temporal association rules are utilisedto automatically guess at the creation date ofdocuments automatically.
Documents are inputinto the system and the probability density func-tions for each word are weighted and added up.Each PDF is weighted according to the inversedocument frequency (IDF) of each associatedword.
Periods that obtain high score are thenranked for each type of period and two guesses perperiod type are obtained for each document.
Tenguesses in total are thus obtained for Day of Week,Week Number, Month Number, Quarter, and Year(5 period types x 2 guesses each).Su M T W Th F S0 22660 10540 7557 772 2130 3264 116721 12461 37522 10335 6599 1649 3222 34142 3394 18289 38320 9352 7300 2543 22613 2668 4119 18120 36933 10427 5762 21474 2052 2602 3910 17492 36094 9098 56675 5742 1889 2481 2568 17002 32597 78496 7994 7072 1924 1428 3050 14087 21468Av 8138 11719 11806 10734 11093 10081 7782St 7357 12711 12974 12933 12308 10746 6930Figure 3 Days of Week Temporal Frequency Distribu-tion for extracted Periodic Componentdisplayed in a Weekly Period Type formatJanuaryWeek 1 2 3 4 5Score 1.48 2.20 3.60 3.43 3.52Month 1 Score 2.95Quarter 1 Score 1.50ChristmasWeek 2 5 36 42 44Score 1.32 0.73 1.60 0.83 1.3231Week 47 49 50 51 52Score 1.32 2.20 2.52 2.13 1.16Month 1 9 10 11 12Score 1.10 0.75 1.63 1.73 1.98Quarter 4 Score 1.07Figure 4 Temporal Classification Rules for PeriodicComponents of "January" and "Christmas"5 Evaluation, Results and ConclusionThe system was trained using 67,000 news itemsselected randomly from the GigaWord corpus.
Theevaluation took place on 678,924 news items ex-tracted from items marked as being of type ?story?or ?multi?.
Table 1 presents a summary of results.Processing took around 2.33ms per item.Type Correct Incorrect Avg.ErrorDOW 218,899(32.24%)460,025(67.75%)1.89daysWeek 24,660(3.53%)654,264(96.36%)14.37wksMonth 122,777(18.08%)556,147(81.91%)2.57mthsQuarter 337,384(49.69%)341,540(50.30%)1.48qtsYear 596,009(87.78%)82,915(12.21%)1.74yrsCombined 422,358(62.21%)256,566(37.79%)210daysTable 1 Evaluation Results SummaryThe actual date was extracted from each news itemin the GigaWord corpus and the day of week(DOW), week number and quarter calculated fromthe actual date.
Average errors for each type ofclassifier were calculated automatically.
For resultsto be considered correct, the system had to havethe predicted value ranked in the first positionequal to the actual value (of the type of period).The system results show that reasonable accuratedates can be guessed at the quarterly and yearlylevels.
The weekly classifier had the worst per-formance of all classifiers.
The combined classifieruses a simple weighted formula to guess the finaldocument date using input from all classifiers.
Theweights for the combined classifier have been seton the basis of this evaluation.
The temporal classi-fication and analysis system presented in this papercan handle any Indo-European language in its pre-sent form.
Further work is being carried out to ex-tend the system to Chinese and Arabic.
Currentresearch is aiming at improving the accuracy of theclassifier by using the non-periodic componentsand improving the combined classification method.ReferencesAramburu, M. Berlanga, R. 1998.
A Retrieval Languagefor Historical Documents.
LNCS, 1460, pp.
216-225.Berlanga, R. Perez, J. Aramburu, M. Llido, D. 2001.Techniques and Tools for the Temporal Analysis ofRetrieved Information.
LNCS, 2113, pp.
72-81.Boguraev, B. Ando, R.K. 2005.
TimeML-CompliantText Analysis for Temporal Reasoning.
IJCAI-2005.Ferro, L. Gerber, L. Mani, I. Sundheim, B. Wilson, G.2004.
TIDES Standard for the Annotation of Tempo-ral Expressions.
The MITRE Corporation.Filatova, E. Hovy, E. 2001.
Assigning time-stamps toevent-clauses.
Proc.
EACL 2001, Toulouse, France.Gaizauskas, R. Setzer, A.
2002.
Annotation Standardsfor Temporal Information in NL.
Proc.
LREC 2002.Kiritchenko, S. Matwin, S. Abu-Hakima, S. 2004.
EmailClassification with Temporal Features.
Proc.
IIPWM2004, Zakopane, Poland.
pp.
523-534.Linguistic Data Consortium (LDC).
2003.
English Gi-gaword Corpus.
David Graff, ed.
LDC2003T05.Mani, I. Wilson, G. 2000.
Robust temporal processingof news.
Proc.
ACL 2000, Hong Kong.Mani, I. Schiffman, B. Zhang, J.
2003.
Inferring tempo-ral ordering of events in news.
HLT-NAACL 2003.Moldovan, D. Clark, C. Harabagiu, S. 2005.
TemporalContext Representation and Reasoning.
IJCAI-2005.Prager, J. Chu-Carroll, J.
Brown, E. Czuba, C. 2003.Question Answering using predictive annotation.Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gai-zauskas, R. Setzer, A. Katz, G. 2003.
TimeML: Ro-bust Specification of event and temporal expressionsin text.
IWCS-5.Pustejovsky, J. Sauri, R. Castano, J. Radev, D. Gai-zauskas, R. Setzer, A. Sundheim, B. Katz, G.
2004.?Representing Temporal and Event Knowledge forQA Systems?.
New Directions in QA, MIT Press.Schilder, F. Habel, C. 2003.
Temporal Information Ex-traction for Temporal QA.
AAAI NDQA, pp.
35-44.Sundheim, B. Gerber, L. Ferro, L. Mani, I. Wilson, G.2004.
Time Expression Recognition and Normaliza-tion (TERN).
http://timex2.mitre.org.32
