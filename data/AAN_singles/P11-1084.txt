Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835?845,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsBinarized Forest to String TranslationHao ZhangGoogle Researchhaozhang@google.comLicheng FangComputer Science DepartmentUniversity of Rochesterlfang@cs.rochester.eduPeng XuGoogle Researchxp@google.comXiaoyun WuGoogle Researchxiaoyunwu@google.comAbstractTree-to-string translation is syntax-aware andefficient but sensitive to parsing errors.
Forest-to-string translation approaches mitigate therisk of propagating parser errors into transla-tion errors by considering a forest of alterna-tive trees, as generated by a source languageparser.
We propose an alternative approach togenerating forests that is based on combiningsub-trees within the first best parse throughbinarization.
Provably, our binarization for-est can cover any non-consitituent phrases ina sentence but maintains the desirable prop-erty that for each span there is at most onenonterminal so that the grammar constant fordecoding is relatively small.
For the purposeof reducing search errors, we apply the syn-chronous binarization technique to forest-to-string decoding.
Combining the two tech-niques, we show that using a fast shift-reduceparser we can achieve significant quality gainsin NIST 2008 English-to-Chinese track (1.3BLEU points over a phrase-based system, 0.8BLEU points over a hierarchical phrase-basedsystem).
Consistent and significant gains arealso shown in WMT 2010 in the English toGerman, French, Spanish and Czech tracks.1 IntroductionIn recent years, researchers have explored a widespectrum of approaches to incorporate syntax andstructure into machine translation models.
The uni-fying framework for these models is synchronousgrammars (Chiang, 2005) or tree transducers(Graehl and Knight, 2004).
Depending on whetheror not monolingual parsing is carried out on thesource side or the target side for inference, there arefour general categories within the framework:?
string-to-string (Chiang, 2005; Zollmann andVenugopal, 2006)?
string-to-tree (Galley et al, 2006; Shen et al,2008)?
tree-to-string (Lin, 2004; Quirk et al, 2005;Liu et al, 2006; Huang et al, 2006; Mi et al,2008)?
tree-to-tree (Eisner, 2003; Zhang et al, 2008)In terms of search, the string-to-x models explore allpossible source parses and map them to the targetside, while the tree-to-x models search over the sub-space of structures of the source side constrainedby an input tree or trees.
Hence, tree-to-x mod-els are more constrained but more efficient.
Mod-els such as Huang et al (2006) can match multi-level tree fragments on the source side which meanslarger contexts are taken into account for transla-tion (Poutsma, 2000), which is a modeling advan-tage.
To balance efficiency and accuracy, forest-to-string models (Mi et al, 2008; Mi and Huang, 2008)use a compact representation of exponentially manytrees to improve tree-to-string models.
Tradition-ally, such forests are obtained through hyper-edgepruning in the k-best search space of a monolin-gual parser (Huang, 2008).
The pruning parametersthat control the size of forests are normally hand-tuned.
Such forests encode both syntactic variantsand structural variants.
By syntactic variants, we re-fer to the fact that a parser can parse a substring intoeither a noun phrase or verb phrase in certain cases.835We believe that structural variants which allow moresource spans to be explored during translation aremore important (DeNeefe et al, 2007), while syn-tactic variants might improve word sense disam-biguation but also introduce more spurious ambi-guities (Chiang, 2005) during decoding.
To focuson structural variants, we propose a family of bina-rization algorithms to expand one single constituenttree into a packed forest of binary trees containingcombinations of adjacent tree nodes.
We control thefreedom of tree node binary combination by restrict-ing the distance to the lowest common ancestor oftwo tree nodes.
We show that the best results areachieved when the distance is two, i.e., when com-bining tree nodes sharing a common grand-parent.In contrast to conventional parser-produced-forest-to-string models, in our model:?
Forests are not generated by a parser but bycombining sub-structures using a tree binarizer.?
Instead of using arbitary pruning parameters,we control forest size by an integer number thatdefines the degree of tree structure violation.?
There is at most one nonterminal per span sothat the grammar constant is small.Since GHKM rules (Galley et al, 2004) can covermulti-level tree fragments, a synchronous grammarextracted using the GHKM algorithm can have syn-chronous translation rules with more than two non-terminals regardless of the branching factor of thesource trees.
For the first time, we show that simi-lar to string-to-tree decoding, synchronous binariza-tion significantly reduces search errors and improvestranslation quality for forest-to-string decoding.To summarize, the whole pipeline is as follows.First, a parser produces the highest-scored tree foran input sentence.
Second, the parse tree is re-structured using our binarization algorithm, result-ing in a binary packed forest.
Third, we apply theforest-based variant of the GHKM algorithm (Miand Huang, 2008) on the new forest for rule extrac-tion.
Fourth, on the translation forest generated byall applicable translation rules, which is not neces-sarily binary, we apply the synchronous binarizationalgorithm (Zhang et al, 2006) to generate a binarytranslation forest.
Finally, we use a bottom-up de-coding algorithm with intergrated LM intersectionusing the cube pruning technique (Chiang, 2005).The rest of the paper is organized as follows.
InSection 2, we give an overview of the forest-to-string models.
In Section 2.1, we introduce a moreefficient and flexible algorithm for extracting com-posed GHKM rules based on the same principle ascube pruning (Chiang, 2007).
In Section 3, we in-troduce our source tree binarization algorithm forproducing binarized forests.
In Section 4, we ex-plain how to do synchronous rule factorization in aforest-to-string decoder.
Experimental results are inSection 5.2 Forest-to-string TranslationForest-to-string models can be described ase = Y( arg maxd?D(T ), T?F (f)P (d|T ) ) (1)where f stands for a source string, e stands for a tar-get string, F stands for a forest, D stands for a setof synchronous derivations on a given tree T , andY stands for the target side yield of a derivation.The search problem is finding the derivation withthe highest probability in the space of all deriva-tions for all parse trees for an input sentence.
Thelog probability of a derivation is normally a lin-ear combination of local features which enables dy-namic programming to find the optimal combinationefficiently.
In this paper, we focus on the modelsbased on the Synchronous Tree Substitution Gram-mars (STSG) defined by Galley et al (2004).
In con-trast to a tree-to-string model, the introduction of Faugments the search space systematically.
When thefirst-best parse is wrong or no good translation rulesare applicable to the first-best parse, the model canrecover good translations from alternative parses.In STSG, local features are defined on tree-to-string rules, which are synchronous grammar rulesdefining how a sequence of terminals and nontermi-nals on the source side translates to a sequence oftarget terminals and nonterminals.
One-to-one map-ping of nonterminals is assumed.
But terminals donot necessarily need to be aligned.
Figure 1 shows atypical English-Chinese tree-to-string rule with a re-ordering pattern consisting of two nonterminals anddifferent numbers of terminals on the two sides.836VPVBDwasVP-C.x1:VBN PPPby.x2:NP-C?
bei?
x2 x1Figure 1: An example tree-to-string rule.Forest-to-string translation has two stages.
Thefirst stage is rule extraction on word-aligned paralleltexts with source forests.
The second stage is ruleenumeration and DP decoding on forests of inputstrings.
In both stages, at each tree node, the task onthe source side is to generate a list of tree fragmentsby composing the tree fragments of its children.
Wepropose a cube-pruning style algorithm that is suit-able for both rule extraction during training and ruleenumeration during decoding.At the highest level, our algorithm involves threesteps.
In the first step, we label each node in the in-put forest by a boolean variable indicating whether itis a site of interest for tree fragment generation.
If itis marked true, it is an admissible node.
In the caseof rule extraction, a node is admissible if and only ifit corresponds to a phrase pair according to the un-derlying word alignment.
In the case of decoding,every node is admissible for the sake of complete-ness of search.
An initial one-node tree fragment isplaced at each admissible node for seeding the treefragment generation process.
In the second step,we do cube-pruning style bottom-up combinationsto enumerate a pruned list of tree fragments at eachtree node.
In the third step, we extract or enumerate-and-match tree-to-string rules for the tree fragmentsat the admissible nodes.2.1 A Cube-pruning-inspired Algorithm forTree Fragment CompositionGalley et al (2004) defined minimal tree-to-stringrules.
Galley et al (2006) showed that tree-to-stringrules made by composing smaller ones are impor-tant to translation.
It can be understood by the anal-ogy of going from word-based models to phrase-based models.
We relate composed rule extractionto cube-pruning (Chiang, 2007).
In cube-pruning,the process is to keep track of the k-best sorted lan-guage model states at each node and combine thembottom-up with the help of a priority queue.
Wecan imagine substituting k-best LM states with kcomposed rules at each node and composing thembottom-up.
We can also borrow the cube pruningtrick to compose multiple lists of rules using a pri-ority queue to lazily explore the space of combina-tions starting from the top-most element in the cubeformed by the lists.We need to define a ranking function for com-posed rules.
To simulate the breadth-first expansionheuristics of Galley et al (2006), we define the fig-ure of merit of a tree-to-string rule as a tuple m =(h, s, t), where h is the height of a tree fragment,s is the number of frontier nodes, i.e., bottom-levelnodes including both terminals and non-terminals,and t is the number of terminals in the set of frontiernodes.
We define an additive operator +:m1 + m2= ( max{h1, h2} + 1, s1 + s2, t1 + t2 )and a min operator based on the order <:m1 < m2 ????
?h1 < h2 ?h1 = h2 ?
s1 < s2 ?h1 = h2 ?
s1 = s2 ?
t1 < t2The + operator corresponds to rule compositions.The < operator corresponds to ranking rules by theirsizes.
A concrete example is shown in Figure 2,in which case the monotonicity property of (+, <)holds: if ma < mb, ma +mc < mb +mc.
However,this is not true in general for the operators in our def-inition, which implies that our algorithm is indeedlike cube-pruning: an approximate k-shortest-pathalgorithm.3 Source Tree BinarizationThe motivation of tree binarization is to factorizelarge and rare structures into smaller but frequentones to improve generalization.
For example, PennTreebank annotations are often flat at the phraselevel.
Translation rules involving flat phrases are un-likely to generalize.
If long sequences are binarized,837????????????????????
?VBD (1, 1, 0)VBDwas(2, 1, 1)??????????????????????????????????????????????????????????????????????????????????????
?VP-C (1, 1, 0)VP-CVPB PP(2, 2, 0)VP-CVPB PPP NP-C(3, 3, 1)????????????????????????????????????????????????????????????????
?=(1, 1, 0) (2, 2, 0) (3, 3, 1)(1, 1, 0) VPVBD VP-C(2, 2, 0) VPVBD VP-CVPB PP(3, 3, 0) VPVBD VP-CVPB PPP NP-C(4, 4, 1)(2, 1, 1) VPVBDwasVP-C(3, 2, 1) VPVBDwasVP-CVPB PP(3, 3, 1) VPVBDwasVP-CVPB PPP NP-C(4, 4, 2)Figure 2: Tree-to-string rule composition as cube-pruning.
The left shows two lists of composed rules sorted by theirgeometric measures (height, # frontiers,# frontier terminals), under the gluing rule of VP ?
VBD VP?C.The right part shows a cube view of the combination space.
We explore the space from the top-left corner to theneighbors.the commonality of subsequences can be discov-ered.
For example, the simplest binarization meth-ods left-to-right, right-to-left, and head-out exploresharing of prefixes or suffixes.
Among exponentiallymany binarization choices, these algorithms pick asingle bracketing structure for a sequence of siblingnodes.
To explore all possible binarizations, we usea CYK algorithm to produce a packed forest of bi-nary trees for a given sibling sequence.With CYK binarization, we can explore any spanthat is nested within the original tree structure, butstill miss all cross-bracket spans.
For example,translating from English to Chinese, The phrase?There is?
should often be translated into one verbin Chinese.
In a correct English parse tree, however,the subject-verb boundary is between ?There?
and?is?.
As a result, tree-to-string translation based onconstituent phrases misses the good translation rule.The CYK-n binarization algorithm shown in Al-gorithm 1 is a parameterization of the basic CYKbinarization algorithm we just outlined.
The idea isthat binarization can go beyond the scope of parentnodes to more distant ancestors.
The CYK-n algo-rithm first annotates each node with its n nearestancestors in the source tree, then generates a bina-rization forest that allows combining any two nodeswith common ancestors.
The ancestor chain labeledat each node licenses the node to only combine withnodes having common ancestors in the past n gener-ations.The algorithm creates new tree nodes on the fly.New tree nodes need to have their own states in-dicated by a node label representing what is cov-ered internally by the node and an ancestor chainrepresenting which nodes the node attaches to ex-ternally.
Line 22 and Line 23 of Algorithm 1 up-date the label and ancestor annotations of new treenodes.
Using the parsing semiring notations (Good-man, 1999), the ancestor computation can be sum-marized by the (?,?)
pair.
?
produces the ances-tor chain of a hyper-edge.
?
produces the ancestorchain of a hyper-node.
The node label computationcan be summarized by the (concatenate, min) pair.concatenate produces a concatenation of node la-bels.
min yields the label with the shortest length.A tree-sequence (Liu et al, 2007) is a sequence ofsub-trees covering adjacent spans.
It can be provedthat the final label of each new node in the forestcorresponds to the tree sequence which has the min-imum length among all sequences covered by thenode span.
The ancestor chain of a new node is thecommon ancestors of the nodes in its minimum treesequence.For clarity, we do full CYK loops over all O(|w|2)spans and O(|w|3) potential hyper-edges, where |w|is the length of a source string.
In reality, only de-scendants under a shared ancestor can combine.
Ifwe assume trees have a bounded branching factorb, the number of descendants after n generations isstill bounded by a constant c = bn.
The algorithm isO(c3 ?
|w|), which is still linear to the size of inputsentence when the parameter n is a constant.838VPVBD+VBNVBDwasVBNPPPbyNP-CVPVBDwasVP-CVBN+PVBN PbyNP-C(a) (b)VPVBD+VBN+PVBD+VBNVBDwasVBNPbyNP-CVPVBD+VBN+PVBDwasVBN+PVBN PbyNP-C(c) (d)1 2 3 40 VBD VBD+VBN VBD+VBN+P VP1 VBN VBN+P VP-C2 P PP3 NP-CFigure 3: Alternative binary parses created for the origi-nal tree fragment in Figure 1 through CYK-2 binarization(a and b) and CYK-3 binarization (c and d).
In the chartrepresentation at the bottom, cells with labels containingthe concatenation symbol + hold nodes created throughbinarization.Figure 3 shows some examples of alternative treesgenerated by the CYK-n algorithm.
In this example,standard CYK binarization will not create any newtrees since the input is already binary.
The CYK-2and CYK-3 algorithms discover new trees with anincreasing degree of freedom.4 Synchronous Binarization forForest-to-string DecodingIn this section, we deal with binarization of transla-tion forests, also known as translation hypergraphs(Mi et al, 2008).
A translation forest is a packedforest representation of all synchronous derivationscomposed of tree-to-string rules that match thesource forest.
Tree-to-string decoding algorithmswork on a translation forest, rather than a source for-est.
A binary source forest does not necessarily al-ways result in a binary translation forest.
In the tree-to-string rule in Figure 4, the source tree is alreadyADJPRB+JJx0:RB JJresponsiblePPINforNP-CNPBDTthex1:NNx2:PP?
x0fuze??
x2de?
x1ADJPRB+JJx0:RB JJresponsiblex1:PP?
x0fuze??
x1PPINforNP-CNPBDTthex0:NNx1:PP?
x1de?
x0Figure 4: Synchronous binarization for a tree-to-stringrule.
The top rule can be binarized into two smaller rules.binary with the help of source tree binarization, butthe translation rule involves three variables in the setof frontier nodes.
If we apply synchronous binariza-tion (Zhang et al, 2006), we can factorize it intotwo smaller translation rules each having two vari-ables.
Obviously, the second rule, which is a com-mon pattern, is likely to be shared by many transla-tion rules in the derivation forest.
When beams arefixed, search goes deeper in a factorized translationforest.The challenge of synchronous binarization for aforest-to-string system is that we need to first matchlarge tree fragments in the input forest as the firststep of decoding.
Our solution is to do the matchingusing the original rules and then run synchronousbinarization to break matching rules down to factorrules which can be shared in the derivation forest.This is different from the offline binarization schemedescribed in (Zhang et al, 2006), although the corealgorithm stays the same.5 ExperimentsWe ran experiments on public data sets for Englishto Chinese, Czech, French, German, and Spanish839Algorithm 1 The CYK-n Binarization Algorithm1: function CYKBINARIZER(T,n)2: for each tree node ?
T in bottom-up topological order do3: Make a copy of node in the forest output F4: Ancestors[node] = the nearest n ancestors of node5: Label [node] = the label of node in T6: L?
the length of the yield of T7: for k = 2...L do8: for i = 0, ..., L?
k do9: for j = i + 1, ..., i + k ?
1 do10: lnode ?
Node[i, j]; rnode ?
Node[j, i + k]11: if Ancestors[lnode] ?
Ancestors[rnode] 6= ?
then12: pnode ?
GETNODE(i, i + k)13: ADDEDGE(pnode, lnode, rnode)return F14: function GETNODE(begin, end)15: if Node[begin, end] /?
F then16: Create a new node for the span (begin, end)17: Ancestors[node] = ?18: Label [node] = the sequence of terminals in the span (begin, end) in T19:return Node[begin, end]20: function ADDEDGE(pnode, lnode, rnode)21: Add a hyper-edge from lnode and rnode to pnode22: Ancestors[pnode] = Ancestors[pnode] ?
(Ancestors[lnode] ?Ancestors[rnode])23: Label [pnode] = min{Label[pnode], CONCATENATE(Label[lnode], Label[rnode])}translation to evaluate our methods.5.1 SetupFor English-to-Chinese translation, we used all theallowed training sets in the NIST 2008 constrainedtrack.
For English to the European languages, weused the training data sets for WMT 2010 (Callison-Burch et al, 2010).
For NIST, we filtered out sen-tences exceeding 80 words in the parallel texts.
ForWMT, the filtering limit is 60.
There is no filteringon the test data set.
Table 1 shows the corpus statis-tics of our bilingual training data sets.Source Words Target WordsEnglish-Chinese 287M 254MEnglish-Czech 66M 57MEnglish-French 857M 996MEnglish-German 45M 43MEnglish-Spanish 216M 238MTable 1: The Sizes of Parallel Texts.At the word alignment step, we did 6 iterationsof IBM Model-1 and 6 iterations of HMM.
ForEnglish-Chinese, we ran 2 iterations of IBM Model-4 in addition to Model-1 and HMM.
The word align-ments are symmetrized using the ?union?
heuris-tics.
Then, the standard phrase extraction heuristics(Koehn et al, 2003) were applied to extract phrasepairs with a length limit of 6.
We ran the hierar-chical phrase extraction algorithm with the standardheuristics of Chiang (2005).
The phrase-length limitis interpreted as the maximum number of symbolson either the source side or the target side of a givenrule.
On the same aligned data sets, we also ran thetree-to-string rule extraction algorithm described inSection 2.1 with a limit of 16 rules per tree node.The default parser in the experiments is a shift-reduce dependency parser (Nivre and Scholz, 2004).It achieves 87.8% labelled attachment score and88.8% unlabeled attachment score on the standardPenn Treebank test set.
We convert dependencyparses to constituent trees by propagating the part-of-speech tags of the head words to the correspond-ing phrase structures.We compare three systems: a phrase-based sys-tem (Och and Ney, 2004), a hierarchical phrase-based system (Chiang, 2005), and our forest-to-string systemwith different binarization schemes.
Inthe phrase-based decoder, jump width is set to 8.
Inthe hierarchical decoder, only the glue rule is applied840to spans longer than 10.
For the forest-to-string sys-tem, we do not have such length-based reorderingconstraints.We trained two 5-gram language models withKneser-Ney smoothing for each of the target lan-guages.
One is trained on the target side of the par-allel text, the other is on a corpus provided by theevaluation: the Gigaword corpus for Chinese andnews corpora for the others.
Besides standard fea-tures (Och and Ney, 2004), the phrase-based decoderalso uses a Maximum Entropy phrasal reorderingmodel (Zens and Ney, 2006).
Both the hierarchi-cal decoder and the forest-to-string decoder only usethe standard features.
For feature weight tuning, wedo Minimum Error Rate Training (Och, 2003).
Toexplore a larger n-best list more efficiently in train-ing, we adopt the hypergraph-based MERT (Kumaret al, 2009).To evaluate the translation results, we use BLEU(Papineni et al, 2002).5.2 Translation ResultsTable 2 shows the scores of our system with thebest binarization scheme compared to the phrase-based system and the hierarchical phrase-based sys-tem.
Our system is consistently better than the othertwo systems in all data sets.
On the English-Chinesedata set, the improvement over the phrase-based sys-tem is 1.3 BLEU points, and 0.8 over the hierarchi-cal phrase-based system.
In the tasks of translat-ing to European languages, the improvements overthe phrase-based baseline are in the range of 0.5 to1.0 BLEU points, and 0.3 to 0.5 over the hierar-chical phrase-based system.
All improvements ex-cept the bf2s and hier difference in English-Czechare significant with confidence level above 99% us-ing the bootstrap method (Koehn, 2004).
To demon-strate the strength of our systems including the twobaseline systems, we also show the reported best re-sults on these data sets from the 2010 WMT work-shop.
Our forest-to-string system (bf2s) outperformsor ties with the best ones in three out of four lan-guage pairs.5.3 Different Binarization MethodsThe translation results for the bf2s system in Ta-ble 2 are based on the cyk binarization algorithmwith bracket violation degree 2.
In this section, weBLEUdev testEnglish-Chinese pb 29.7 39.4hier 31.7 38.9bf2s 31.9 40.7?
?English-Czech wmt best - 15.4pb 14.3 15.5hier 14.7 16.0bf2s 14.8 16.3?English-French wmt best - 27.6pb 24.1 26.1hier 23.9 26.1bf2s 24.5 26.6?
?English-German wmt best - 16.3pb 14.5 15.5hier 14.9 15.9bf2s 15.2 16.3?
?English-Spanish wmt best - 28.4pb 24.1 27.9hier 24.2 28.4bf2s 24.9 28.9?
?Table 2: Translation results comparing bf2s, thebinarized-forest-to-string system, pb, the phrase-basedsystem, and hier, the hierarchical phrase-based system.For comparison, the best scores from WMT 2010 are alsoshown.
??
indicates the result is significantly better thanboth pb and hier.
?
indicates the result is significantlybetter than pb only.vary the degree to generate forests that are incremen-tally augmented from a single tree.
Table 3 showsthe scores of different tree binarization methods forthe English-Chinese task.It is clear from reading the table that cyk-2 is theoptimal binarization parameter.
We have verifiedthis is true for other language pairs on non-standarddata sets.
We can explain it from two angles.
Atdegree 2, we allow phrases crossing at most onebracket in the original tree.
If the parser is reason-ably good, crossing just one bracket is likely to covermost interesting phrases that can be translation units.From another point of view, enlarging the forestsentails more parameters in the resulting translationmodel, making over-fitting likely to happen.5.4 Binarizer or Parser?A natural question is how the binarizer-generatedforests compare with parser-generated forests intranslation.
To answer this question, we need a841BLEUrules dev testno binarization 378M 28.0 36.3head-out 408M 30.0 38.2cyk-1 527M 31.6 40.5cyk-2 803M 31.9 40.7cyk-3 1053M 32.0 40.6cyk-?
1441M 32.0 40.3Table 3: Comparing different source tree binarizationschemes for English-Chinese translation, showing bothBLEU scores and model sizes.
The rule counts includenormal phrases which are used at the leaf level duringdecoding.parser that can generate a packed forest.
Our fastdeterministic dependency parser does not generatea packed forest.
Instead, we use a CRF constituentparser (Finkel et al, 2008) with state-of-the-art ac-curacy.
On the standard Penn Treebank test set, itachieves an F-score of 89.5%.
It uses a CYK algo-rithm to do full dynamic programming inference, sois much slower.
We modified the parser to do hyper-edge pruning based on posterior probabilities.
Theparser preprocesses the Penn Treebank training datathrough binarization.
So the packed forest it pro-duces is also a binarized forest.
We compare twosystems: one is using the cyk-2 binarizer to generateforests; the other is using the CRF parser with prun-ing threshold e?p, where p = 2 to generate forests.1Although the parser outputs binary trees, we foundcross-bracket cyk-2 binarization is still helpful.BLEUdev testcyk-2 14.9 16.0parser 14.7 15.7Table 4: Binarized forests versus parser-generated forestsfor forest-to-string English-German translation.Table 4 shows the comparison of binarization for-est and parser forest on English-German translation.The results show that cyk-2 forest performs slightly1All hyper-edges with negative log posterior probabilitylarger than p are pruned.
In Mi and Huang (2008), the thresh-old is p = 10.
The difference is that they do the forest pruningon a forest generated by a k-best algorithm, while we do theforest-pruning on the full CYK chart.
As a result, we need moreaggressive pruning to control forest size.better than the parser forest.
We have not done fullexploration of forest pruning parameters to fine-tunethe parser-forest.
The speed of the constituent parseris the efficiency bottleneck.
This actually demon-strates the advantage of the binarizer plus forest-to-string scheme.
It is flexible, and works with anyparser that generates projective parses.
It does notrequire hand-tuning of forest pruning parameters fortraining.5.5 Synchronous BinarizationIn this section, we demonstrate the effect of syn-chronous binarization for both tree-to-string andforest-to-string translation.
The experiments are onthe English-Chinese data set.
The baseline systemsuse k-way cube pruning, where k is the branchingfactor, i.e., the maximum number of nonterminals onthe right-hand side of any synchronous translationrule in an input grammar.
The competing systemdoes online synchronous binarization as described inSection 4 to transform the grammar intersected withthe input sentence to the minimum branching factork?
(k?
< k), and then applies k?-way cube pruning.Typically, k?
is 2.BLEUdev testhead-out cube pruning 29.2 37.0+ synch.
binarization 30.0 38.2cyk-2 cube pruning 31.7 40.5+ synch.
binarization 31.9 40.7Table 5: The effect of synchronous binarization for tree-to-string and forest-to-string systems, on the English-Chinese task.Table 5 shows that synchronous binarization doeshelp reduce search errors and find better translationsconsistently in all settings.6 Related WorkThe idea of concatenating adjacent syntactic cate-gories has been explored in various syntax-basedmodels.
Zollmann and Venugopal (2006) aug-mented hierarchial phrase based systems with jointsyntactic categories.
Liu et al (2007) proposed tree-sequence-to-string translation rules but did not pro-vide a good solution to place joint subtrees into con-nection with the rest of the tree structure.
Zhang et842al.
(2009) is the closest to our work.
But their goalwas to augment a k-best forest.
They did not bina-rize the tree sequences.
They also did not put con-straint on the tree-sequence nodes according to howmany brackets are crossed.Wang et al (2007) used target tree binarization toimprove rule extraction for their string-to-tree sys-tem.
Their binarization forest is equivalent to ourcyk-1 forest.
In contrast to theirs, our binarizationscheme affects decoding directly because we matchtree-to-string rules on a binarized forest.Different methods of translation rule binarizationhave been discussed in Huang (2007).
Their argu-ment is that for tree-to-string decoding target sidebinarization is simpler than synchronous binariza-tion and works well because creating discontinoussource spans does not explode the state space.
Theforest-to-string senario is more similar to string-to-tree decoding in which state-sharing is important.Our experiments show that synchronous binariza-tion helps significantly in the forest-to-string case.7 ConclusionWe have presented a new approach to tree-to-stringtranslation.
It involves a source tree binarizationstep and a standard forest-to-string translation step.The method renders it unnecessary to have a k-bestparser to generate a packed forest.
We have demon-strated state-of-the-art results using a fast parser anda simple tree binarizer that allows crossing at mostone bracket in each binarized node.
We have alsoshown that reducing search errors is important forforest-to-string translation.
We adapted the syn-chronous binarization technqiue to improve searchand have shown significant gains.
In addition, wealso presented a new cube-pruning-style algorithmfor rule extraction.
In the new algorithm, it is easy toadjust the figure-of-merit of rules for extraction.
Inthe future, we plan to improve the learning of trans-lation rules with binarized forests.AcknowledgmentsWe would like to thank the members of the MT teamat Google, especially Ashish Venugopal, Zhifei Li,John DeNero, and Franz Och, for their help and dis-cussions.
We would also like to thank Daniel Gildeafor his suggestions on improving the paper.ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on statisti-cal machine translation and metrics for machine trans-lation.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and Metrics(MATR),pages 17?53, Uppsala, Sweden, July.
Association forComputational Linguistics.
Revised August 2010.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Conference of the Association forComputational Linguistics (ACL-05), pages 263?270,Ann Arbor, MI.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 755?763,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, companion volume, pages 205?208, Sap-poro, Japan.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL-08:HLT, pages 959?967, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of the 2004 Meeting of the North Americanchapter of the Association for Computational Linguis-tics (NAACL-04), pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the International Conference on ComputationalLinguistics/Association for Computational Linguistics(COLING/ACL-06), pages 961?968, July.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proceedings of the 2004 Meeting of theNorth American chapter of the Association for Compu-tational Linguistics (NAACL-04).843Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th BiennialConference of the Association for Machine Translationin the Americas (AMTA), Boston, MA.Liang Huang.
2007.
Binarization, synchronous bina-rization, and target-side binarization.
In Proceedingsof the NAACL/AMTA Workshop on Syntax and Struc-ture in Statistical Translation (SSST), pages 33?40,Rochester, NY.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings of the46th Annual Conference of the Association for Compu-tational Linguistics: Human Language Technologies(ACL-08:HLT), Columbus, OH.
ACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Meeting of the North American chap-ter of the Association for Computational Linguistics(NAACL-03), Edmonton, Alberta.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In 2004 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 388?395, Barcelona, Spain, July.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 163?171, Sun-tec, Singapore, August.
Association for ComputationalLinguistics.Dekang Lin.
2004.
A path-based transfer model formachine translation.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics(COLING-04), pages 625?630, Geneva, Switzerland.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the International Conferenceon Computational Linguistics/Association for Compu-tational Linguistics (COLING/ACL-06), Sydney, Aus-tralia, July.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007.Forest-to-string statistical translation rules.
In Pro-ceedings of the 45th Annual Conference of the Associ-ation for Computational Linguistics (ACL-07), Prague.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 206?214, Honolulu, Hawaii, Octo-ber.
Association for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of the 46th An-nual Conference of the Association for ComputationalLinguistics: Human Language Technologies (ACL-08:HLT), pages 192?199.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedings ofColing 2004, pages 64?70, Geneva, Switzerland, Aug23?Aug 27.
COLING.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41th Annual Conference of the Association for Com-putational Linguistics (ACL-03).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Conference of the Association for Com-putational Linguistics (ACL-02).Arjen Poutsma.
2000.
Data-oriented translation.
InProceedings of the 18th International Conference onComputational Linguistics (COLING-00).Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd Annual Con-ference of the Association for Computational Linguis-tics (ACL-05), pages 271?279, Ann Arbor, Michigan.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Conference of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (ACL-08:HLT), Columbus, OH.ACL.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-based ma-chine translation accuracy.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 746?754, Prague, Czech Republic, June.
Association forComputational Linguistics.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings on the Workshop on Statistical Ma-chine Translation, pages 55?63, New York City, June.Association for Computational Linguistics.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the 2006 Meeting of the844North American chapter of the Association for Compu-tational Linguistics (NAACL-06), pages 256?263, NewYork, NY.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProceedings of ACL-08: HLT, pages 559?567, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, andChew Lim Tan.
2009.
Forest-based tree sequence tostring translation model.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 172?180,Suntec, Singapore, August.
Association for Computa-tional Linguistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation, pages 138?141, New York City, June.
As-sociation for Computational Linguistics.845
