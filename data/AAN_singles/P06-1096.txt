Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761?768,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn End-to-End Discriminative Approach to Machine TranslationPercy Liang Alexandre Bouchard-Co?te?
Dan Klein Ben TaskarComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{pliang, bouchard, klein, taskar}@cs.berkeley.eduAbstractWe present a perceptron-style discriminative ap-proach to machine translation in which large featuresets can be exploited.
Unlike discriminative rerank-ing approaches, our system can take advantage oflearned features in all stages of decoding.
We firstdiscuss several challenges to error-driven discrim-inative approaches.
In particular, we explore dif-ferent ways of updating parameters given a trainingexample.
We find that making frequent but smallerupdates is preferable to making fewer but larger up-dates.
Then, we discuss an array of features andshow both how they quantitatively increase BLEUscore and how they qualitatively interact on spe-cific examples.
One particular feature we investi-gate is a novel way to introduce learning into theinitial phrase extraction process, which has previ-ously been entirely heuristic.1 IntroductionThe generative, noisy-channel paradigm has his-torically served as the foundation for most of thework in statistical machine translation (Brown etal., 1994).
At the same time, discriminative meth-ods have provided substantial improvements overgenerative models on a wide range of NLP tasks.They allow one to easily encode domain knowl-edge in the form of features.
Moreover, param-eters are tuned to directly minimize error ratherthan to maximize joint likelihood, which may notcorrespond well to the task objective.In this paper, we present an end-to-end dis-criminative approach to machine translation.
Theproposed system is phrase-based, as in Koehn etal.
(2003), but uses an online perceptron trainingscheme to learn model parameters.
Unlike mini-mum error rate training (Och, 2003), our system isable to exploit large numbers of specific featuresin the same manner as static reranking systems(Shen et al, 2004; Och et al, 2004).
However,unlike static rerankers, our system does not relyon a baseline translation system.
Instead, it up-dates based on its own n-best lists.
As parameterestimates improve, the system produces better n-best lists, which can in turn enable better updatesin future training iterations.
In this paper, we fo-cus on two aspects of the problem of discrimina-tive translation: the inherent difficulty of learningfrom reference translations, and the challenge ofengineering effective features for this task.Discriminative learning from reference transla-tions is inherently problematic because standarddiscriminative methods need to know which out-puts are correct and which are not.
However, aproposed translation that differs from a referencetranslation need not be incorrect.
It may differin word choice, literalness, or style, yet be fullyacceptable.
Pushing our system to avoid such al-ternate translations is undesirable.
On the otherhand, even if a system produces a reference trans-lation, it may do so by abusing the hidden struc-ture (sentence segmentation and alignment).
Wecan therefore never be entirely sure whether or nota proposed output is safe to update towards.
Wediscuss this issue in detail in Section 5, where weshow that conservative updates (which push thesystem towards a local variant of the current pre-diction) are more effective than more aggressiveupdates (which try to directly update towards thereference).The second major contribution of this work isan investigation of an array of features for ourmodel.
We show how our features quantitativelyincrease BLEU score, as well as how they qual-itatively interact on specific examples.
We firstconsider learning weights for individual phrasesand part-of-speech patterns, showing gains fromeach.
We then present a novel way to parameter-ize and introduce learning into the initial phraseextraction process.
In particular, we introducealignment constellation features, which allow usto weight phrases based on the word alignmentpattern that led to their extraction.
This kind of761feature provides a potential way to initially extractphrases more aggressively and then later down-weight undesirable patterns, essentially learning aweighted extraction heuristic.
Finally, we use POSfeatures to parameterize a distortion model in alimited distortion decoder (Zens and Ney, 2004;Tillmann and Zhang, 2005).
We show that over-all, BLEU score increases from 28.4 to 29.6 onFrench-English.2 Approach2.1 Translation as structured classificationMachine translation can be seen as a structuredclassification task, in which the goal is to learna mapping from an input (French) sentence x toan output (English) sentence y.
Given this setup,discriminative methods allow us to define a broadclass of features ?
that operate on (x,y).
For ex-ample, some features would measure the fluencyof y and others would measure the faithfulness ofy as a translation of x.However, the translation task in this frameworkdiffers from traditional applications of discrimina-tive structured classification such as POS taggingand parsing in a fundamental way.
Whereas inPOS tagging, there is a one-to-one correspondencebetween the words x and the tags y, the correspon-dence between x and y in machine translation isnot only much more complex, but is in fact un-known.
Therefore, we introduce a hidden corre-spondence structure h and work with the featurevector ?
(x,y,h).The phrase-based model of Koehn et al (2003)is an instance of this framework.
In their model,the correspondence h consists of (1) the segmen-tation of the input sentence into phrases, (2) thesegmentation of the output sentence into the samenumber of phrases, and (3) a bijection betweenthe input and output phrases.
The feature vec-tor ?
(x,y,h) contains four components: the logprobability of the output sentence y under a lan-guage model, the score of translating x into ybased on a phrase table, a distortion score, and alength penalty.1 In Section 6, we vastly increasethe number of features to take advantage of the fullpower of discriminative training.Another example of this framework is the hier-archical model of Chiang (2005).
In this modelthe correspondence h is a synchronous parse tree1More components can be added to the feature vector ifadditional language models or phrase tables are available.over input and output sentences, and features in-clude the scores of various productions used in thetree.Given features ?
and a corresponding set of pa-rameters w, a standard classification rule f is toreturn the highest scoring output sentence y, max-imizing over correspondences h:f(x;w) = argmaxy,hw ?
?(x,y,h).
(1)In the phrase-based model, computing theargmax exactly is intractable, so we approximatef with beam decoding.2.2 Perceptron-based trainingTo tune the parameters w of the model, we use theaveraged perceptron algorithm (Collins, 2002) be-cause of its efficiency and past success on variousNLP tasks (Collins and Roark, 2004; Roark et al,2004).
In principle, w could have been tuned bymaximizing conditional probability or maximiz-ing margin.
However, these two options requireeither marginalization or numerical optimization,neither of which is tractable over the space of out-put sentences y and correspondences h. In con-trast, the perceptron algorithm requires only a de-coder that computes f(x;w).Recall the traditional perceptron update rule onan example (xi,yi) isw?
w + ?(xi,yt)?
?
(xi,yp), (2)where yt = yi is the target output and yp =f(xi;w) = argmaxyw ?
?
(xi,y) is the predic-tion using the current parameters w.We adapt this update rule to work with hiddenvariables as follows:w?
w + ?(xi,yt,ht)??
(xi,yp,hp), (3)where (yp,hp) is the argmax computation inEquation 1, and (yt,ht) is the target that we up-date towards.
If (yt,ht) is the same argmax com-putation with the additional constraint that yt =yi, then Equation 3 can be interpreted as a Viterbiapproximation to the stochastic gradientEP (h|xi,yi;w)?
(xi,yi,h)?EP (y,h|xi;w)?
(xi,y,h)for the following conditional likelihood objective:P (yi | xi) ?
?hexp(w ?
?
(xi,yi,h)).762                                                             ff          fi    ff                                     Figure 1: Given the current prediction (a), thereare two possible updates, local (b) and bold (c).Although the bold update (c) reaches the referencetranslation, a bad correspondence is used.
The lo-cal update (b) does not reach the reference, but ismore reasonable than (c).Discriminative training with hidden variableshas been handled in this probabilistic framework(Quattoni et al, 2004; Koo and Collins, 2005), butwe choose Equation 3 for efficiency.It turns out that using the Viterbi approximation(which we call bold updating) is not always thebest strategy.
To appreciate the difficulty, considerthe example in Figure 1.
Suppose we make theprediction (a) with the current set of parameters.There are often several acceptable output transla-tions y, for example, (b) and (c).
Since (c)?s outputmatches the reference translation, should we up-date towards (c)?
In this case, the answer is nega-tive.
The problem with (c) is that the correspon-dence h contains an incorrect alignment (?, a).However, since h is unobserved, the training pro-cedure has no way of knowing this.
While the out-put in (b) is farther from the reference, its corre-spondence h is much more reasonable.
In short,it does not suffice for yt to be good; both yt andht need to be good.
A major challenge in usingthe perceptron algorithm for machine translationis determining the target (yt,ht) in Equation 3.Section 5 discusses possible targets to update to-wards.3 DatasetOur experiments were done on the French-Englishportion of the Europarl corpus (Koehn, 2002),Dataset TRAIN DEV TESTYears ?99?
?01 ?02 ?03# sentences 67K first 1K first 1K# words (unk.)
715K 10.4K (35) 10.8K (48)Table 1: The Europarl dataset split we used andvarious statistics on length 5?15 sentences.
Thenumber of French word tokens is given, alongwith the number that were not seen among the414K total sentences in TRAIN (which includes alllengths).which consists of European parliamentary pro-ceedings from 1996 to 2003.We split the data into three sets according toTable 1.
TRAIN served two purposes: it wasused to construct the features, and the 5?15 lengthsentences were used for tuning the parameters ofthose features.
DEV, which consisted of the first1K length 5?15 sentences in 2002, was used toevaluate the performance of the system as we de-veloped it.
Note that the DEV set was not used totune any parameters; tuning was done exclusivelyon TRAIN.
At the end we ran our models once onTEST to get final numbers.24 ModelsOur experiments used phrase-based models(Koehn et al, 2003), which require a translationtable and language model for decoding andfeature computation.
To facilitate comparisonwith previous work, we created the translationtables using the same techniques as Koehn et al(2003).3 The language model was a Kneser-Neyinterpolated trigram model generated using theSRILM toolkit (Stolcke, 2002).
We built ourown phrase-based beam decoder that can handlearbitrary features.4 The contributions of featuresare incrementally added into the score as decoding2We also experimented with several combinations of jack-knifing to prevent overfitting, in which we selected featureson TRAIN-OLD (1996?1998 Europarl corpus) and tuned theparameters on TRAIN, or vice-versa.
However, it turned outthat using TRAIN-OLD was suboptimal since that data is lessrelevant to DEV.
Another alternative is to combine TRAIN-OLD and TRAIN into one dual-purpose dataset.
The differ-ences between this and our current approach were inconclu-sive.3In other words, we used GIZA++ to construct a wordalignment in each direction and a growth heuristic to com-bine them.
We extracted all the substrings that are closed un-der this high-quality word alignment and computed surfacestatistics from cooccurrences counts.4In our experiments, we used a beam size of 10, which wefound to be only slightly worse than using a beam of 100.763proceeds.We experimented with two levels of distortion:monotonic, where the phrasal alignment is mono-tonic (but word reordering is still possible withina phrase) and limited distortion, where only ad-jacent phrases are allowed to exchange positions(Zens and Ney, 2004).
In the future, we plan to ex-plore our discriminative framework on a full dis-tortion model (Koehn et al, 2003) or even a hier-archical model (Chiang, 2005).Throughout the following experiments, wetrained the perceptron algorithm for 10 iterations.The weights were initialized to 1 on the trans-lation table, 1 on the language model (the blan-ket features in Section 6), and 0 elsewhere.
Thenext two sections give experiments on the two keycomponents of a discriminative machine transla-tion system: choosing the proper update strategy(Section 5) and including powerful features (Sec-tion 6).5 Update strategiesThis section describes the importance of choosinga good update strategy?the difference in BLEUscore can be as large as 1.2 between differentstrategies.
An update strategy specifies the target(yt,ht) that we update towards (Equation 3) giventhe current set of parameters and a provided ref-erence translation (xi,yi).
As mentioned in Sec-tion 2.2, faithful output (i.e.
yt = yi) does notimply that updating towards (yt,ht) is desirable.In fact, such a constrained target might not evenbe reachable by the decoder, for example, if thereference is very non-literal.We explored the following three ways to choosethe target (yt,ht):?
Bold updating: Update towards the highestscoring option (y,h), where y is constrainedto be the reference yi but h is unconstrained.Examples not reachable by the decoder areskipped.?
Local updating: Generate an n-best list usingthe current parameters.
Update towards theoption with the highest BLEU score.55Since BLEU score (k-BLEU with k = 4) involves com-puting a geometric mean over i-grams, i = 1, .
.
.
, k, it is zeroif the translation does not have at least one k-gram in commonwith the reference translation.
Since a BLEU score of zerois both unhelpful for choosing from the n-best and commonwhen computed on just a single example, we instead used asmoothed version for choosing the target:P4i=1i-BLEU(x,y)24?i+1 .We still report NIST?s usual 4-gram BLEU.                               ff  fi fl                           ff  fi fl  ffi     	   ffi     	      fl    !
 fi  "# $ % &      '     ffi     	         # $ % &      '     ffi     	    (       Figure 2: The three update strategies under twoscenarios.?
Hybrid updating: Do a bold update if the ref-erence is reachable.
Otherwise, do a local up-date.Figure 2 shows the space of translationsschematically.
On each training example, our de-coder produces an n-best list.
The reference trans-lation may or may not be reachable.Bold updating most resembles the traditionalperceptron update rule (Equation 2).
We are en-sured that the target output y will be correct, al-though the correspondence h might be bad.
An-other weakness of bold updating is that we mightnot make full use of the training data.Local updating uses every example, but its stepsare more cautious.
It can be viewed as ?dy-namic reranking,?
where parameters are updatedusing the best option on the n-best list, similarto standard static reranking.
The key differenceis that, unlike static reranking, the parameter up-dates propagate back to the baseline classifier, sothat the n-best list improves over time.
In this re-gard, dynamic reranking remedies one of the mainweaknesses of static reranking, which is that theperformance of the system is directly limited bythe quality of the baseline classifier.Hybrid updating combines the two strategies:it makes full use of the training data as in localupdating, but still tries to make swift progress to-wards the reference translation as in bold updat-ing.We conducted experiments to see which of theupdating strategies worked best.
We trained on764Decoder Bold Local HybridMonotonic 34.3 34.6 34.5Limited distortion 33.5 34.7 33.6Table 2: Comparison of BLEU scores between dif-ferent updating strategies for the monotonic andlimited distortion decoders on DEV.5000 of the 67K available examples, using theBLANKET+LEX+POS feature set (Section 6).
Ta-ble 2 shows that local updating is the most effec-tive, especially when using the limited distortiondecoder.In bold updating, only a small fraction of the5000 examples (1296 for the monotonic decoderand 1601 for the limited distortion decoder) hadreachable reference translations, and, therefore,contributed to parameter updates.
One mighttherefore hypothesize that local updating performsbetter simply because it is able to leverage moredata.
This is not the full story, however, since thehybrid approach (which makes the same numberof updates) performs significantly worse than lo-cal updating when using the limited distortion de-coder.To see the problem with bold updating, recallthe example in Figure 1.
Bold updating tries toreach the reference at all costs, even if it meansabusing the hidden correspondence in the process.In the example, the alignment (?, a) is unreason-able, but the algorithm has no way to recognizethis.
Local updating is much more stable since itonly updates towards sentences in the n-best list.When using the limited distortion decoder, boldupdating is even more problematic because theadded flexibility of phrase swaps allows more pre-posterous alignments to be produced.
Limiteddistortion decoding actually performs worse thanmonotonic decoding with bold updating, but bet-ter with local updating.Another difference between bold updating andlocal updating is that the BLEU score on the train-ing data is dramatically higher for bold updatingthan for local (or hybrid) updating: 80 for the for-mer versus 40 for the latter.
This is not surprisinggiven that bold updating aggressively tries to ob-tain the references.
However, what is surprising isthat although bold updating appears to be overfit-ting severely, its BLEU score on the DEV does notsuffer much in the monotonic case.Model DEV BLEU TEST BLEUMonotonicBLANKET (untuned) 33.0 28.3BLANKET 33.4 28.4BLANKET+LEX 35.0 29.2BLANKET+LEX+POS 35.3 29.6Pharaoh (MERT) 34.5 28.8Full-distortionPharaoh (MERT) 34.9 29.5Table 3: Main results on our system with differ-ent feature sets compared to minimum error-ratetrained Pharaoh.6 FeaturesThis section shows that by adding an array ofexpressive features and discriminatively learn-ing their weights, we can obtain a 2.3 increasein BLEU score on DEV.
We add these fea-tures incrementally, first tuning blanket features(Section 6.1), then adding lexical features (Sec-tion 6.2), and finally adding part-of-speech (POS)features (Section 6.3).
Table 3 summarizes theperformance gains.For the experiments in this section, we used thelocal updating strategy and the monotonic decoderfor efficiency.
We train on all 67K of the length 5?15 sentences in TRAIN.66.1 Blanket featuresThe blanket features (BLANKET) consist of thetranslation log-probability and the language modellog-probability, which are two of the componentsof the Pharaoh model (Section 2.1).
After discrim-inative training, the relative weight of these twofeatures is roughly 2:1, resulting in a BLEU scoreincrease from 33.0 (setting both weights to 1) to33.4.The following simple example gives a flavorof the discriminative approach.
The untunedsystem translated the French phrase trente-cinqlangues into five languages in a DEV example.Although the probability P (five | trente-cinq) =0.065 is rightly much smaller than P (thirty-five |trente-cinq) = 0.279, the language model favorsfive languages over thirty-five languages.
Thetrained system downweights the language modeland recovers the correct translation.6We used sentences of length 5?15 to facilitate compar-isons with Koehn et al (2003) and to enable rapid experimen-tation with various feature sets.
Experiments on sentences oflength 5?50 showed similar gains in performance.7656.2 Lexical featuresThe blanket features provide a rough guide fortranslation, but they are far too coarse to fix spe-cific mistakes.
We therefore add lexical fea-tures (LEX) to allow for more fine-grained con-trol.
These features come in two varieties.
Lexicalphrase features indicate the presence of a specifictranslation phrase, such as (y a-t-il, are there), andlexical language model features indicate the pres-ence of a specific output n-gram, such as of the.Lexical language model features have been ex-ploited successfully in discriminative languagemodeling to improve speech recognition perfor-mance (Roark et al, 2004).
We confirm the util-ity of the two kinds of lexical features: BLAN-KET+LEX achieves a BLEU score of 35.0, an im-provement of 1.6 over BLANKET.To understand the effect of adding lexical fea-tures, consider the ten with highest and lowestweights after training:64 any comments ?
-55 (des, of)63 (y a-t-il, are there) -52 (y a-t-il, are there any)62 there any comments -42 there any of57 any comments -39 of comments46 (des, any) -38 of comments ?These features can in fact be traced back to thefollowing example:Input y a-t-il des observations ?B are there any of comments ?B+L are there any comments ?The second and third rows are the outputs ofBLANKET (wrong) and BLANKET+LEX (correct),respectively.
The correction can be accredited totwo changes in feature weights.
First, the lexicalfeature (y a-t-il, are there any) has been assigneda negative weight and (y a-t-il, are there) a pos-itive weight to counter the fact that the formerphrase incorrectly had a higher score in the origi-nal translation table.
Second, (des, of) is preferredover (des, any), even though the former is a bettertranslation in isolation.
This apparent degradationcauses no problems, because when des should ac-tually be translated to of, these words are usuallyembedded in larger phrases, in which case the iso-lated translation probability plays no role.Another example of a related phenomenon isthe following:Input ... pour cela que j ?
ai vote?
favorablement .B ... for that i have voted in favour .B+L ... for this reason i voted in favour .Counterintuitively, the phrase pair(j ?
ai, I have) ends up with a very negativeweight.
The reason behind this is that in French,j ?
ai is often used in a paraphrastic constructionwhich should be translated into the simple pastin English.
For that to happen, j ?
ai needs tobe aligned with I.
Since (j ?
ai, I) has a smallscore compare to (j ?
ai, I have) in the originaltranslation table, downweighting the latter pairallows this sentence to be translated correctly.A general trend is that literal phrase translationsare downweighted.
Lessening the pressure to liter-ally translate certain phrases allows the languagemodel to fill in the gaps appropriately with suit-able non-literal translations.
This point highlightsthe strength of discriminative training: weights arejointly tuned to account for the intricate interac-tions between overlapping phrases, which is some-thing not achievable by estimating the weights di-rectly from surface statistics.6.3 Part-of-speech featuresWhile lexical features are useful for eliminatingspecific errors, they have limited ability to gener-alize to related phrases.
This suggests the use ofsimilar features which are abstracted to the POSlevel.7 In our experiments, we used the TreeTag-ger POS tagger (Schmid, 1994), which ships pre-trained on several languages, to map each wordto its majority POS tag.
We could also relativelyeasily base our features on context-dependent POStags: the entire input sentence is available beforedecoding begins, and the output sentence is de-coded left-to-right and could be tagged incremen-tally.Where we had lexical phrase features, suchas (la re?alisation du droit, the right), we nowalso have their POS abstractions, for instance(DT NN IN NN, DT NN).
This phrase pair isundesirable, not because of particular lexical factsabout la re?alisation, but because dropping a nom-inal head is generally to be avoided.
The lexicallanguage model features have similar POS coun-terparts.
With these two kinds of POS features,we obtained an 0.3 increase in BLEU score fromBLANKET+LEX to BLANKET+LEX+POS.Finally, when we use the limited distortion de-coder, it is important to learn when to swap adja-cent phrases.
Unlike Pharaoh, which simply has auniform penalty for swaps, we would like to usecontext?in particular, POS information.
For ex-ample, we would like to know that if a (JJ, JJ)7We also tried using word clusters (Brown et al, 1992)instead of POS but found that POS was more helpful.766securerefugeabrisu?rzerogrowthratecroissanceze?ro,thatsame,ceme?me(a) (b) (c)Figure 3: Three constellation features with exam-ple phrase pairs.
Constellations (a) and (b) havelarge positive weights and (c) has a large negativeweight.phrase is constructed after a (NN, NN) phrase,they are reasonable candidates for swapping be-cause of regular word-order differences betweenFrench and English.
While the bulk of our resultsare presented for the monotonic case, the limiteddistortion results of Table 2 use these lexical swapfeatures; without parameterized swap features, ac-curacy was below the untuned monotonic baseline.An interesting statistic is the number of nonzerofeature weights that were learned using eachfeature set.
BLANKET has only 4 features,while BLANKET+LEX has 1.55 million features.8Remarkably, BLANKET+LEX+POS has fewerfeatures?only 1.24 million.
This is an effectof generalization ability?POS information some-what reduces the need for specific lexical features.6.4 Alignment constellation featuresKoehn et al (2003) demonstrated that choosingthe appropriate heuristic for extracting phrases isvery important.
They showed that the differencein BLEU score between various heuristics was aslarge as 2.0.The process of phrase extraction is difficult tooptimize in a non-discriminative setting: manyheuristics have been proposed (Koehn et al,2003), but it is not obvious which one should bechosen for a given language pair.
We propose anatural way to handle this part of the translationpipeline.
The idea is to push the learning processall the way down to the phrase extraction by pa-rameterizing the phrase extraction heuristic itself.The heuristics in Koehn et al (2003) decidewhether to extract a given phrase pair based on theunderlying word alignments (see Figure 3 for threeexamples), which we call constellations.
Since wedo not know which constellations correspond to8Both the language model and translation table compo-nents have two features, one for known words and one forunknown words.Features -CONST +CONSTBLANKET 31.8 32.2BLANKET+LEX 32.2 32.5BLANKET+LEX+POS 32.3 32.5Table 4: DEV BLEU score increase resulting fromadding constellation features.good phrase pairs, we introduce an alignment con-stellation feature to indicate the presence of a par-ticular alignment constellation.9Table 4 details the effect of adding constella-tion features on top of our previous feature sets.10We get a minor increase in BLEU score from eachfeature set, although there is no gain by addingPOS features in addition to constellation features,probably because POS and constellation featuresprovide redundant information for French-Englishtranslations.It is interesting to look at the constellations withhighest and lowest weights, which are perhaps sur-prising at first glance.
At the top of the list areword inversions (Figure 3 (a) and (b)), while longmonotonic constellations fall at the bottom of thelist (c).
Although monotonic translations are muchmore frequent than word inversions in our dataset,when translations are monotonic, shorter segmen-tations are preferred.
This phenomenon is anothermanifestation of the complex interaction of phrasesegmentations.7 Final resultsThe last column of Table 3 shows the performanceof our methods on the final TEST set.
Our best testBLEU score is 29.6 using BLANKET+LEX+POS,an increase of 1.3 BLEU over our untuned featureset BLANKET.
The discrepancy between DEV per-formance and TEST performance is due to tem-poral distance from TRAIN and high variance inBLEU score.11We also compared our model with Pharaoh(Koehn et al, 2003).
We tuned Pharaoh?s four pa-rameters using minimum error rate training (Och,2003) on DEV.12 We obtained an increase of 0.89As in the POS features, we map each phrase pair to itsmajority constellation.10Due to time constraints, we ran these experiments on5000 training examples using bold updating.11For example, the DEV BLEU score for BLANKET+LEXranges from 28.6 to 33.2, depending on which block of 1000sentences we chose.12We used the training scripts from the 2006 MT SharedTask.
We still tuned our model parameters on TRAIN and767BLEU over the Pharaoh, run with the monotoneflag.13 Even though we are using a monotonic de-coder, our best results are still slightly better thanthe version of Pharaoh that permits arbitrary dis-tortion.8 Related workIn machine translation, most discriminative ap-proaches currently fall into two general categories.The first approach is to reuse the components of agenerative model, but tune their relative weights ina discriminative fashion (Och and Ney, 2002; Och,2003; Chiang, 2005).
This approach only works inpractice with a small handful of parameters.The second approach is to use reranking, inwhich a baseline classifier generates an n-best listof candidate translations, and a separate discrim-inative classifier chooses amongst them (Shen etal., 2004; Och et al, 2004).
The major limita-tion of a reranking system is its dependence onthe underlying baseline system, which bounds thepotential improvement from discriminative train-ing.
In machine translation, this limitation is areal concern; it is common for all translations onmoderately-sized n-best lists to be of poor qual-ity.
For instance, Och et al (2004) reported thata 1000-best list was required to achieve perfor-mance gains from reranking.
In contrast, the de-coder in our system can use the feature weightslearned in the previous iteration.Tillmann and Zhang (2005) present a discrim-inative approach based on local models.
Theirformulation explicitly decomposed the score ofa translation into a sequence of local decisions,while our formulation allows global estimation.9 ConclusionWe have presented a novel end-to-end discrimi-native system for machine translation.
We stud-ied update strategies, an important issue in on-line discriminative training for MT, and concludethat making many smaller (conservative) updatesis better than making few large (aggressive) up-dates.
We also investigated the effect of addingmany expressive features, which yielded a 0.8 in-crease in BLEU score over monotonic Pharaoh.Acknowledgments We would like to thank ourreviewers for their comments.
This work was sup-only used DEV to optimize the number of training iterations.13This result is significant with p-value 0.0585 based onapproximate randomization (Riezler and Maxwell, 2005).ported by a FQRNT fellowship to second authorand a Microsoft Research New Faculty Fellowshipto the third author.ReferencesPeter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-Basedn-gram Models of Natural Language.
Computational Lin-guistics, 18(4):467?479.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The Mathematicsof Statistical Machine Translation: Parameter Estimation.Computational Linguistics, 19:263?311.David Chiang.
2005.
A Hierarchical Phrase-Based Model forStatistical Machine Translation.
In ACL 2005.Michael Collins and Brian Roark.
2004.
Incremental Parsingwith the Perceptron Algorithm.
In ACL 2004.Michael Collins.
2002.
Discriminative Training Methods forHidden Markov Models: Theory and Experiments withPerceptron Algorithms.
In EMNLP 2002.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In HLT-NAACL2003.Philipp Koehn.
2002.
Europarl: A Multilingual Corpus forEvaluation of Machine Translation.Terry Koo and Michael Collins.
2005.
Hidden-Variable Mod-els for Discriminative Reranking.
In EMNLP 2005.Franz Josef Och and Hermann Ney.
2002.
DiscriminativeTraining and Maximum Entropy Models for StatisticalMachine Translation.
In ACL 2002.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, andAnoop Sarkar.
2004.
A Smorgasbord of Features for Sta-tistical Machine Translation.
In HLT-NAACL 2004.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In ACL 2003.Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2004.Conditional Random Fields for Object Recognition.
InNIPS 2004.Stefan Riezler and John T. Maxwell.
2005.
On Some Pit-falls in Automatic Evaluation and Significance Testing forMT.
In Workshop on Intrinsic and Extrinsic EvaluationMethods for MT and Summarization (MTSE).Brian Roark, Murat Saraclar, Michael Collins, and MarkJohnson.
2004.
Discriminative Language Modeling withConditional Random Fields and the Perceptron Algorithm.In ACL 2004.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Conferenceon New Methods in Language Processing.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.
Dis-criminative Reranking for Machine Translation.
In HLT-NAACL 2004.Andreas Stolcke.
2002.
SRILM An Extensible LanguageModeling Toolkit.
In ICSLP 2002.Christoph Tillmann and Tong Zhang.
2005.
A LocalizedPrediction Model for Statistical Machine Translation.
InACL 2005.Richard Zens and Hermann Ney.
2004.
Improvements in Sta-tistical Phrase-Based Translation.
In HLT-NAACL 2004.768
