Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsUnsupervised Part-of-Speech Tagging in Noisy and Esoteric Domainswith a Syntactic-Semantic Bayesian HMMWilliam M. DarlingSchool of Computer ScienceUniversity of Guelphwdarling@uoguelph.caMichael J. PaulDept.
of Computer ScienceJohns Hopkins Universitympaul@cs.jhu.eduFei SongSchool of Computer ScienceUniversity of Guelphfsong@uoguelph.caAbstractUnsupervised part-of-speech (POS) tag-ging has recently been shown to greatlybenefit from Bayesian approaches whereHMM parameters are integrated out, lead-ing to significant increases in tagging ac-curacy.
These improvements in unsuper-vised methods are important especially inspecialized social media domains such asTwitter where little training data is avail-able.
Here, we take the Bayesian approachone step further by integrating semantic in-formation from an LDA-like topic modelwith an HMM.
Specifically, we presentPart-of-Speech LDA (POSLDA), a syntac-tically and semantically consistent genera-tive probabilistic model.
This model dis-covers POS specific topics from an unla-belled corpus.
We show that this modelconsistently achieves improvements in un-supervised POS tagging and language mod-eling over the Bayesian HMM approachwith varying amounts of side informationin the noisy and esoteric domain of Twitter.1 IntroductionThe explosion of social media in recent years hasled to the need for NLP tools like part-of-speech(POS) taggers that are robust enough to handledata that is becoming increasingly ?noisy.?
Unfor-tunately, many NLP systems fail at out-of-domaindata and struggle with the informal style of socialtext.
With spelling errors, abbreviations, uncom-mon acronyms, and excessive use of slang, sys-tems that are designed for traditional corpora suchas news articles may perform poorly when givendifficult input such as a Twitter feed (Ritter et al,2010).Recognizing the limitations of existing sys-tems, Gimpel et al (2011) develop a POS taggerspecifically for Twitter, by creating a training cor-pus as well as devising a tag set that includes partsof speech that are uniquely found in online lan-guage, such as emoticons (smilies).
This is an im-portant step forward, but a POS tagger tailored toTwitter cannot tackle the social Web as a whole.Other online communities have their own styles,slang, memes, and other idiosyncrasies, so a sys-tem trained for one community may not apply toothers.For example, the 140-character limit of Twit-ter encourages abbreviations and word-droppingthat may not be found in less restrictive venues.The first-person subject is often assumed in ?sta-tus messages?
that one finds in Twitter and Face-book, so the pronominal subject can be dropped,even in English (Weir, 2012), leading to messageslike ?Went out?
instead of ?I went out.?
Notonly does Twitter follow these unusual grammat-ical patterns, but many messages contain ?hash-tags?
which could be considered their own syn-tactic class not found in other data sources.
Forthese reasons, POS parameters learned from Twit-ter data will not necessarily fit other social data.In general, concerns about the limitations ofdomain-dependent models have motivated the useof sophisticated unsupervised methods.
Inter-est in unsupervised POS induction has been re-vived in recent years after Bayesian HMMs areshown to increase accuracy by up to 14 percent-age points over basic maximum-likelihood esti-mation (Goldwater and Griffiths, 2007).
Despitefalling well short of the accuracy obtained withsupervised taggers, unsupervised approaches arepreferred in situations where there is no access to1large quantities of training data in a specific do-main, which is increasingly common with Webdata.
We therefore hope to continue improvingaccuracy with unsupervised approaches by intro-ducing semantics as an additional source of infor-mation for this task.The ambiguities of language are amplifiedthrough social media, where new words orspellings of words are routinely invented.
For ex-ample, ?ow?
on Twitter can be a shorthand for?how,?
in addition to its more traditional use asan expression of pain (ouch).
While POS assign-ment is inherently a problem of syntactic disam-biguation, we hypothesize that the underlying se-mantic content can aid the disambiguation task.If we know that the overall content of a messageis about police, then the word ?cop?
is likely tobe a noun, whereas if the context is about shop-ping, this could be slang for acquiring or stealing(verb).
The HMM approach will often be able totag these occurrences appropriately given the con-text, but in many cases the syntactic context maybe limited or misleading due to the noisy natureof the data.
Thus, we believe that semantic con-text will offer additional evidence toward makingan accurate prediction.Following this intuition, this paper presents asemantically and syntactically coherent Bayesianmodel that uncovers POS-specific sub-topicswithin general semantic topics, as in latent Dirich-let alocation (LDA) (Blei et al, 2003), which wecall part-of-speech LDA, or POSLDA.
The re-sulting posterior distributions will reflect special-ized topics such as ?verbs about dining?
or ?nounsabout politics?.
To the best of our knowledge, wealso present the first experiments with unsuper-vised tagging for a social media corpus.
In thiswork, we focus on Twitter because the labeledcorpus by Gimpel et al (2011) allows us to quan-titatively evaluate our approach.
We demonstratethe model?s utility as a predictive language modelby its low perplexity on held-out test data as com-pared to several related topic models, and mostimportantly, we show that this model achievesstatistically significant and consistent improve-ments in unsupervised POS tagging accuracy overa Bayesian HMM.
These results support our hy-pothesis that semantic information can directlyimprove the quality of POS induction, and our ex-periments present an in-depth exploration of thistask on informal social text.The next section discusses related work, whichis followed by a description of our model,POSLDA.
We then present POS tagging resultson the Twitter POS dataset (Gimpel et al, 2011).Section 5 describes further experiments on thePOSLDA model and section 6 includes a discus-sion on the results and why POSLDA can do bet-ter on POS tagging than a vanilla Bayesian HMM.Finally, section 7 concludes with a discussion onfuture work.2 Related WorkModern unsupervised POS tagging originateswith Merialdo (1993) who trained a trigramHMM using maximum likelihood estimation(MLE).
Goldwater and Griffiths (2007) improvedupon this approach by treating the HMM in aBayesian sense; the rows of the transition matrixare random variables with proper Bayesian priorsand the state emission probabilities are also ran-dom variables with their own priors.
The posteriordistribution of tags is learned using Gibbs sam-pling and this model improves in accuracy overthe MLE approach by up to 14 percentage points.In the ?Topics and Syntax?
model (orHMMLDA), the generative process of a corpusis cast as a composite model where syntax ismodeled with an HMM and semantics are mod-eled with LDA (Griffiths et al, 2005).
Here, onestate of an HMM is replaced with a topic modelsuch that the words with long-range dependen-cies (?content?
words) will be drawn from a setof topics.
The remaining states are reserved for?syntax?
words that exhibit only short-range de-pendencies.
Griffiths et al (2005) briefly touchon POS tagging with their model, but its supe-riority to a plain Bayesian HMM is not shownand the authors note that this is partially becauseall semantic-like words get assigned to the sin-gle semantic class in their model.
This misses thedistinction between at least nouns and verbs, butmany other semantic-dependent words as well.
Ifmore variation could be provided in the seman-tic portion of the model, the POS tagging resultswould likely improve.3 Part-of-Speech LDA (POSLDA)In their canonical form, topic models do not cap-ture local dependencies between words (i.e.
syn-tactic relations), but they do capture long-range2K*SCON+ SFUNSM??w1z1c1???
?w2z2c2w3z3c3.........Figure 1: Graphical model depiction of POSLDA.context such as the overall topical content or gistof a document.
Conversely, under an HMM,words are assumed completely independent oftheir broader context by the Markov assumption.We seek to bridge these restrictions with our uni-fied model, Part-of-Speech LDA (POSLDA).Under this model, each word token is now asso-ciated with two latent variables: a semantic topicz and a syntactic class c. We posit that the top-ics are generated through the LDA process, whilethe classes are generated through an HMM.
Theobserved word tokens are then dependent on boththe topic and the class: rather than a single multi-nomial for a particular topic z or a particular classc, there are distributions for each topic-class pair(z, c) from which we assume words are sampled.We denote the set of classes C = CCON ?
CFUN,which includes the set of content or ?semantic?classes CCON for word types such as nouns andverbs that depend on the current topic, and func-tional or ?syntactic-only?
classes CFUN.
If a wordis generated from a functional class, it does notdepend on the topic.
This allows our model toaccommodate functional words like determinerswhich appear independently of the topical contentof a document.We use the same notation as LDA, where ?
is adocument-topic distribution and ?
is a topic-worddistribution.
Additionally, we denote the HMMtransition rows as pi, which we assume is drawnfrom a Dirichlet with hyperparameter ?.
DenoteS = |C| and K = |Z|, the numbers of classesand topics, respectively.
There are SFUN worddistributions ?
(FUN) for function word classes andK ?
SCON word distributions ?
(CON) for contentword classes.
A graphical model depiction ofPOSLDA is shown in Figure 1.Thus, the generative process of a corpus can bedescribed as:1.
Draw pi ?
Dirichlet(?)2.
Draw ?
?
Dirichlet(?)3.
For each document d ?
D:(a) Draw ?d ?
Dirichlet(?
)(b) For each word token wi ?
d:i.
Draw ci ?
pici?1ii.
If ci /?
CCON:A.
Draw wi ?
?(FUN)ciiii.
Else:A.
Draw zi ?
?dB.
Draw wi ?
?
(CON)ci,ziIn topic models, it is generally true that com-mon function words may overwhelm the worddistributions, leading to suboptimal results thatare difficult to interpret.
This is usually accom-modated by data pre-processing (e.g.
stop wordremoval), by backing off to ?background?
wordmodels (Chemudugunta et al, 2006), or by per-forming term re-weighting (Wilson and Chew,2010).
In the case of POSLDA, these commonwords are naturally captured by the functionalclasses.3.1 Relations to Other ModelsThe idea of having multinomials for the crossproducts of topics and classes is related to multi-faceted topic models where word tokens are as-sociated with multiple latent variables (Paul andGirju, 2010; Ahmed and Xing, 2010).
Under suchmodels, words can be explained by a latent topicas well as a second underlying variable such asthe perspective or dialect of the author, and wordsmay depend on both factors.
In our case, the sec-ond variable is the part-of-speech ?
or functionalpurpose ?
of the token.We note that POSLDA is a generalization ofmany existing models.
POSLDA becomes aBayesian HMM when the number of topics K =1; the original LDA model when the number of3classes S = 1; and the HMMLDA model of Grif-fiths et al (2005) when the number of contentword classes SCON = 1.
The beauty of these gen-eralizations is that one can easily experiment withany of these models by simply altering the modelparameters under a single POSLDA implementa-tion.3.2 InferenceAs with many complex probabilistic models, ex-act posterior inference is intractable for POSLDA.Nevertheless, a number of approximate inferencetechniques are at our disposal.
In this work, weuse collapsed Gibbs sampling to sample the latentclass assignments and topic assignments (c andz), and from these we can compute estimates ofthe multinomial parameters for the topics (?
), thedocument-topic portions (?
), and the HMM tran-sition matrix (pi).
Under a trigram version of themodel ?
which we employ for all our experimentsin this work ?
the sampling equation for word to-ken i is as follows:p(ci, zi|c?i, z?i,w) ??????
?ci ?n(d)zi +?zin(d).
+?.n(ci,zi)w +?n(ci,zi).
+W?ci ?
SCON?ci ?n(ci)w +?n(ci).
+W?ci ?
SFUNwhere?ci =n(ci?2,ci?1,ci)+?cin(ci?2,ci?1)+?.?n(ci?1,ci,ci+1)+?cin(ci?1,ci)+?.
?n(ci,ci+1,ci+2)+?cin(ci,ci+1)+?.Although we sample the pair (ci, zi) jointly as ablock, which requires computing a sampling dis-tribution over SFUN +K ?SCON, it is also valid tosample ci and zi separately, which requires onlyS + K computations.
In this case, the samplingprocedure would be somewhat different.
Despitethe lower number of computations per iteration,however, the sampler is likely to converge fasterwith our blocked approach because the two vari-ables are tightly coupled.
The intuition is that anon-block-based sampler could have difficulty es-caping local optima because we are interested inthe most probable pair; a highly probable classc sampled on its own, for example, could pre-vent the sampler from choosing a more likely pair(c?, z).4 POS Tagging ExperimentsTo demonstrate the veracity of our approach, weperformed a number of POS tagging experimentsusing the POSLDA model.
Our data is the re-cent Twitter POS dataset released at ACL 2011 byGimpel et al (2011) consisting of approximately26,000 words across 1,827 tweets.
This datasetprovides a unique opportunity to test our unsuper-vised approach in a domain where it would likelybe of most use ?
one that is novel and thereforelacking large amounts of training data.
We feelthat this sort of specialized domain will becomethe norm ?
particularly in social media analysis?
as user generated content continues to grow insize and accessibility.
The Twitter dataset uses adomain-dependent tag set of 25 tags that are de-scribed in (Gimpel et al, 2011).For our experiments, we follow the establishedform of Merialdo (1993) and Goldwater and Grif-fiths (2007) for unsupervised POS tagging bymaking use of a tag dictionary to constrain thepossible tag choices for each word and there-fore render the problem closer to disambiguation.Like Goldwater and Griffiths (2007), we employa number of dictionaries with varying degrees ofknowledge.We use the full corpus of tweets1 and constructa tag dictionary which contains the tag informa-tion for a word only when it appears more than dtimes in the corpus.
We ran experiments for d =1, 2, 3, 5, 10, and ?
where the problem becomesPOS clustering.
We report both tagging accu-racy and the variation of information (VI), whichcomputes the information lost in moving fromone clustering C to another C ?
: V I(C,C ?)
=H(C) +H(C ?)?
2I(C,C ?)
(Meila?, 2007).
Thiscan be interpreted as a measure of similarity be-tween the clusterings, where a smaller value indi-cates higher similarity.We run our Gibbs sampler for 20,000 iterationsand obtain a maximum a posteriori (MAP) esti-mate for each word?s tag by employing simulatedannealing.
Each posterior probability p(c, z|?)
inthe sampling distribution is raised to the power of1?
where ?
is a temperature that approaches 0 asthe sampler converges.
This approach is akin to1The Twitter POS dataset consists of three subsets oftweets: development, training, and testing.
Because we areperforming fully unsupervised tagging, however, we com-bine these three subsets into one.4Accuracy 1 2 3 5 10 ?random 62.8 49.6 45.2 40.2 35.0BHMM 78.4 65.4 59.0 51.8 44.0POSLDA 80.9 67.5 62.0 55.9 47.6VIrandom 2.34 3.31 3.56 3.81 4.05 5.86BHMM 1.41 2.47 2.84 3.22 3.61 5.07POSLDA 1.30 2.34 2.66 2.98 3.35 4.96Corpus stats% ambig.
54.2 67.9 72.2 76.4 80.4 100tags / token 2.62 5.91 7.19 8.59 10.3 25Table 1: POS tagging results on Twitter dataset.bringing a system from an arbitrary state to onewith the lowest energy, thus viewing the Gibbssampling procedure as a random search whosegoal is to identify the MAP tag sequence ?
a tech-nique that is also employed by Goldwater andGriffiths (2007).
Finally, we run each experiment5 times from random initializations and report theaverage accuracy and variation of information.4.1 Results for Twitter DatasetIn our experiments, we use 8 content classesthat correspond to the following parts-of-speech:noun, proper noun, proper noun + possessive,proper noun + verbal, verb, adjective, adverb, andother abbreviations / foreign words.
We chosethese classes because intuitively they are the typesof words whose generative probability will de-pend on the given latent topic.
As the Twitter POSdata consists of 25 distinct tags, this leaves 17 re-maining classes for function words.
In this sec-tion, we report results for K = 10 topics.
Wewill discuss the effect of varyingK in section 4.2.We set symmetric priors with ?
= 1.0/K = 0.1,?
= 0.5, and ?
= 0.01.As is demonstrated in Table 1, our POSLDAmodel shows marked improvements over a ran-dom tag assignment and, more importantly, theBayesian HMM approach described by Goldwa-ter and Griffiths (2007).
It does so for every set-ting of d on both accuracy and variation of infor-mation.
For d = 1 our method outperforms theBHMM by 2.5 percentage points.
With highervalues of d, however, POSLDA increases its im-provement over the BHMM to up to 4.1 percent-age points.
The increase in tagging accuracy asd increases suggests that our method may be par-ticularly suitable for domains with little trainingK Accuracy ?1 (HMM) 78.6 0.235 80.0 0.0610 80.9 0.1715 80.1 0.1020 80.2 0.2125 80.1 0.2530 80.2 0.1535 80.1 0.1240 79.9 0.2045 80.1 0.12Table 2: POS tagging results as K varies on Twitterdataset.data.2 For d = ?, where we are performingPOS clustering, our model improves the variationof information by 0.11.
Each of these improve-ments over the Bayesian HMM is statistically sig-nificant with p  0.01.
Despite the clear im-provements in POS tagging accuracy and cluster-ing that we demonstrate in this section, we trainedour POSLDA model with a ?blind?
topic settingof K = 10.
In the following section, we willinvestigate how this parameter affects the achiev-able results with our technique.4.2 Topic VarianceIn the previous section we set the number of topicsa priori to K = 10.
However, it is well known intopic modeling research that different datasets ex-hibit different numbers of ?inherent?
topics (Bleiet al, 2003).
Therefore, a POSLDA model fit withthe ?correct?
number of topics will likely achievehigher accuracy in POS tagging.
A standard ap-proach to tuning the number of topics to fit a topicmodel is to try a number of different topics andchoose the one that results in the lowest perplexityon a held-out test set (Claeskens and Hjort, 2008).Here, we can choose the optimal K more directlyby trying a number of different values and choos-ing the one that maximizes the POS tagging accu-racy.For this experiment, we again make use of theTwitter POS dataset (Gimpel et al, 2011).
We usethe same setup as that described above with sim-ulated annealing, 20,000 iterations, and a tag dic-2The differences in tagging accuracy in terms of per-centage points between POSLDA and the BHMM ford = {1, 2, 3, 5, 10} are ?a = {2.5, 2.1, 3.0, 4.1, 3.6},respectively.
For clustering, the increases in VI areeven more clear as d increases.
They are ?V I ={0.11, 0.13, 0.18, 0.24, 0.26}.5tionary with d = 1.
As before, we set ?
= 1.0/K,?
= 0.5, and ?
= 0.01.
We perform experimentswith K = {1, 5, 10, .
.
.
, 40, 45}, where K = 1corresponds to the Bayesian HMM.
The resultsaveraged over 3 runs are tabulated in Table 2 withthe associated standard deviations (?
), and showngraphically in Figure 2.topicsaccuracy78.579.079.580.080.581.0llllllllll10 20 30 40Figure 2: Number of topics K vs. POS tagging ac-curacy on the Twitter dataset.
The average accuracies,along with their standard errors, are shown in black,while a smoothed curve of the same data is shown inblue.As we expect, the tagging accuracy depends onthe number of topics specified by the model.
Infact, the accuracy improves by nearly a full per-centage point from both the previous and nexttopic settings when we hit a critical point atK = 10.
When K = 1 the model reduces tothe Bayesian HMM and our accuracy suffers.
Itsteadily increases until we hit the critical pointand then drops off again but plateaus at a levelthat is approximately 1.5 percentage points higherthan the BHMM.
This shows that determining anappropriate setting for the number of topics is es-sential for the best possible tagging accuracy us-ing POSLDA.
Nevertheless, even with a ?blind?setting within a large range of topic values (herefrom K = 5 to at least K = 45), we see markedimprovements over the baseline system that doesnot include any semantic topic information.5 Model EvaluationIn this section we present further experimentson the raw output of POSLDA to demonstrateits capabilities beyond simply POS tagging.
Weshow the model?s ability both qualitatively andquantitatively to capture the semantic (or ?con-tent?)
and syntactic (or ?functional?)
axes of in-formation prevalent in a corpus made up of socialmedia data.
We begin qualitatively with topic in-terpretability when the model is learned given acollection of unannotated Twitter messages, andthen present quantitative results on the ability ofPOSLDA as a predictive language model in theTwitter domain.5.1 Topic InterpretabilityJudging the interpretability of a set of topics ishighly subjective, and there are understandablyvarious differing approaches of evaluating topiccohesiveness.
For example, Chang et al (2009)look at ?word intrusion?
where a user determinesan intruding word from a set of words that doesnot thematically fit with the other words, and?topic intrusion?
where a user determines whetherthe learned document-topic portion ?d appropri-ately describes the semantic theme of the doc-ument.
In this section, we are most interestedin subjectively demonstrating the low incidenceof ?word intrusion?
both in terms of semantics(theme) and syntax (part-of-speech).
We do notconduct formal experiments to demonstrate this,but we subjectively show that our model learnssemantic and syntactic word distributions that arelikely robust towards problems of word intrusionand that are therefore ?interpretable?
for humansexamining the learned posterior word distribu-tions.Table 3 shows three topics ?
manually la-belled as ?party?, ?status update?, and ?politics??
learned from the relatively small Twitter POSdataset.
We set the number of topics K = 20,the number of classes S = 25, and the num-ber of content word classes SCON = 8, followingour earlier POS tagging experiments.
We showthe top five words from three POS-specific top-ics labelled manually as noun, verb, and adjec-tive.
Given the relatively small size of the dataset,the short length of the documents, and the eso-teric language and grammar use, the interpretabil-ity of the topics is reasonable.
All three topicsassign high probability to words that one would6PARTY STATUS UPDATE POLITICSnoun verb adj noun verb adj noun verb adjparty gets awesome day is nice anything say lateman is old pm looking nasty truth has realshit knew original school so last face wait highmen were fake today have hard city cant republicanperson wasnt drunk body got tired candidate going importantTable 3: Example topics learned from the Twitter POS dataset with POSLDA.CONJ DET PREP RPand the to tobut a of itor my in upn your for awayin this on inyet that with onplus is at aroundnd some NUMBER outan an if overto his from offTable 4: Example topic-independent function classdistributions (CFUN) learned from the Twitter POSdataset with POSLDA.expect to have high importance with one or twooutliers.
More importantly, however, the POS-specific topics also generally reflect their syntac-tic roles.
Each of the verbs is assuredly (evenwithout the proper context) a verb (with the sin-gle outlier being the word ?so?
), and the samething for the nouns.
The adjectives seem to fitas well; though many of the words could be con-sidered nouns depending on the context, it is clearhow given the topic each of the words could verywell act as an adjective.
A final point worthmentioning is that, unlike LDA, we do not per-form stopword removal.
Instead, the POSLDAmodel has pushed stopwords to their own func-tion classes (rather than content) freeing us fromhaving to perform pre- or post-processing stepsto ensure interpretable topics.
The top words infour of these topic-independent function classes,learned from the Twitter POS dataset, are shownin Table 4.3 These function word distributions areeven more cohesive than the content word distri-butions, showing that the standard stopwords havebeen accounted for as we expect in their respec-tive function classes.3Note that we make use of the tag dictionary when learn-ing these word distributions.5.2 Predictive Language ModelingWhile we have demonstrated that our model canachieve improved accuracy in POS tagging forTwitter data, it can also be useful for other kindsof language analysis in the social media do-main.
In the following experiments, we test thePOSLDA model quantitatively by determining itsability as a predictive language model.
Follow-ing a standard practice in topic modeling research(Blei et al, 2003; Griffiths et al, 2005), we fit amodel to a training set and then compute the per-plexity of a held-out test set.
For this experiment,we use the Twitter POS training dataset describedearlier (16,348 words across 999 tweets).
We thenperform testing on the Twitter POS testing dataset(8,027 words across 500 tweets).
We comparethe perplexity ?
a monotonically decreasing func-tion of the log likelihood ?
to LDA, a BayesianHMM, and HMMLDA.
Finally, we use Minka?sfixed-point method (Wallach, 2008) to optimizethe hyperparameters ?
and ?.topicsperplexity640660680700720l l l l l l5 10 15 20 25 30modell BHMMHMMLDALDAPOSLDAFigure 3: Perplexity of POSLDA and other probabilis-tic models.7Figure 3 shows the perplexity on the held-outTwitter test set for models trained with K ={5, 10, 15, 20, 25, 30}.
The Bayesian HMM is notaffected by the number of topics and is able tobeat the HMMLDA model at K = 5.
It alsoachieves lower perplexity than the LDA model atK = 5, 25, and 30.
Our POSLDA model, how-ever, achieves the lowest perplexity of all testedmodels at all topic settings that we tested.
Thisdemonstrates that POSLDA is a good candidatefor both language modeling and for further la-tent probabilistic model-based analysis of Twitterdata.6 DiscussionIn the previous section we demonstrated bothqualitatively and quantitatively that our modelcaptures two sources of information from unstruc-tured texts: thematic (or semantics) and func-tional (or syntactic).
An important question toconsider is why ?
as we demonstrated in sec-tion 4 ?
learning this sort of information im-proves our ability to perform unsupervised POStagging.
One reason is discussed in the introduc-tion: semantic information can help disambiguatethe POS for a word that typically serves a differ-ent function depending on the topic that it is nor-mally associated with.
This phenomenon likelyplays an important role in the accuracy improve-ments that we observe.
However, another featureof the model is the distinction between ?content?POS classes and ?function?
POS classes.
The for-mer will depend on the current topic while thelatter are universal across thematic space.
Thiswill also represent an improvement over the bareHMM because words that depend on the cur-rent topic ?
typically nouns, verbs, adjectives,and adverbs ?
will be forced to these classes dueto their long-range thematic dependencies whilewords with only short-range dependencies will bepushed to the function POS classes.
This lattertype of words ?
conjunctions, determiners, etc.?
naturally do not depend on themes so as theyare pushed to the function-only POS classes, andso one step of disambiguation has already beenperformed.
This is the same behaviour as in theHMMLDA model by Griffiths et al (2005), buthere we are able to perform proper POS taggingbecause there is more than just a single contentword class and we are therefore able to discernbetween the topic-dependent parts-of-speech.7 Conclusions and Future WorkIn this paper, we have shown that incorporatingsemantic topic information into a Bayesian HMMcan result in impressive increases in accuracy forunsupervised POS tagging.
Specifically, we pre-sented POSLDA ?
a topic model consistent acrossthe axes of both semantic and syntactic meanings.Using this model to perform unsupervised POStagging results in consistent and statistically sig-nificant increases in POS tagging accuracy anddecreases in variation of information when per-forming POS clustering.
These improvements aredemonstrated on a novel release of data from themicroblogging social network site Twitter.
Thistype of dataset is of particular interest because un-supervised POS tagging will likely be most im-portant in specialized idiosyncratic domains withatypical features and small amounts of labelledtraining data.
Crucially, we showed that evenwith the inconsistent and at times strange use ofgrammar, slang, and acronyms, the syntactic por-tion of the model demonstrably improves not onlythe predictive ability of the model in terms ofperplexity, but also the accuracy in unsupervisedPOS tagging.
This is important because in gen-eral tweets are far from being representative of?proper?
grammar.
Nevertheless, there clearly ex-ists some adherence to syntactic structure as theuse of the HMM within our model improves wordprediction and POS tagging.This work represents the first ?
to our knowl-edge ?
application of latent thematic informationto the unsupervised POS tagging task.4 How-ever, due to the encouraging results, there are anumber of future research directions that presentthemselves from this work.
One immediate task isto extend POSLDA to a nonparametric Bayesianmodel.
Section 4.2 shows how varying the num-ber of topics K in the model can affect the tag-ging accuracy by up to a full percentage point.
Anonparametric version of the model would free usfrom having to perform the initial model selectionstep to get the best accuracy.
Another avenue forfuture work is to infuse more structure into themodel such as word morphology.4There has been some work done to include semantic in-formation collected separately in a supervised POS taggingapproach (Toutanova and Johnson, 2008).8AcknowledgmentsWilliam Darling is supported by an NSERC Doc-toral Postgraduate Scholarship, and Michael Paulis supported by an NSF Graduate Research Fel-lowship.
The authors would like to thank theanonymous reviewers for their helpful commentsand suggestions.ReferencesAmr Ahmed and Eric P. Xing.
2010.
Stayinginformed: supervised and semi-supervised multi-view topical analysis of ideological perspective.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 1140?1150, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InNeural Information Processing Systems.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2006.
Modeling general and specific as-pects of documents with a probabilistic topic model.In NIPS, pages 241?248.G.
Claeskens and N.L.
Hjort.
2008.
Model Selectionand Model Averaging.
Cambridge University Press.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flani-gan, and Noah A. Smith.
2011.
Part-of-speech tag-ging for twitter: Annotation, features, and experi-ments.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 744?751.
Association for Computational Lin-guistics.Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2005.
Integrating topicsand syntax.
In In Advances in Neural InformationProcessing Systems 17, pages 537?544.
MIT Press.Donna Harman.
1992.
Overview of the first text re-trieval conference (trec-1).
In TREC, pages 1?20.M.
Meila?.
2007.
Comparing clusteringsan informa-tion based distance.
Journal of Multivariate Analy-sis, 98(5):873?895, May.Bernard Merialdo.
1993.
Tagging english text witha probabilistic model.
Computational Linguistics,20:155?171.Michael J. Paul and Roxana Girju.
2010.
Atwo-dimensional topic-aspect model for discover-ing multi-faceted topics.
In AAAI.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT?10, pages 172?180, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, ACL?05, pages 354?362, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Kristina Toutanova and Mark Johnson.
2008.
Abayesian lda-based model for semi-supervised part-of-speech tagging.
In J.C. Platt, D. Koller,Y.
Singer, and S. Roweis, editors, Advances inNeural Information Processing Systems 20, pages1521?1528.
MIT Press, Cambridge, MA.Hanna Wallach, David Mimno, and Andrew McCal-lum.
2009.
Rethinking lda: Why priors matter.
InNIPS.Hanna M. Wallach.
2008.
Structured Topic Models forLanguage.
Ph.D. thesis, University of Cambridge.Andrew Weir.
2012.
Left-edge deletion in english andsubject omission in diaries.
English Language andLinguistics.Andrew T. Wilson and Peter A. Chew.
2010.
Termweighting schemes for latent dirichlet alocation.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 465?473, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.9
