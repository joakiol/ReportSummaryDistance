Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56?61,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsBLAST: A Tool for Error Analysis of Machine Translation OutputSara StymneDepartment of Computer and Information ScienceLinko?ping University, Linko?ping, Swedensara.stymne@liu.seAbstractWe present BLAST, an open source tool for er-ror analysis of machine translation (MT) out-put.
We believe that error analysis, i.e., toidentify and classify MT errors, should be anintegral part of MT development, since it givesa qualitative view, which is not obtained bystandard evaluation methods.
BLAST can aidMT researchers and users in this process, byproviding an easy-to-use graphical user inter-face.
It is designed to be flexible, and can beused with any MT system, language pair, anderror typology.
The annotation task can beaided by highlighting similarities with a ref-erence translation.1 IntroductionMachine translation evaluation is a difficult task,since there is not only one correct translation of asentence, but many equally good translation options.Often, machine translation (MT) systems are onlyevaluated quantitatively, e.g.
by the use of automaticmetrics, which is fast and cheap, but does not giveany indication of the specific problems of a MT sys-tem.
Thus, we advocate human error analysis of MToutput, where humans identify and classify the prob-lems in machine translated sentences.In this paper we present BLAST,1 a graphical toolfor performing human error analysis, from any MTsystem and for any language pair.
BLAST has agraphical user interface, and is designed to be easy1The BiLingual Annotation/Annotator/Analysis SupportTool, available for download at http://www.ida.liu.se/?sarst/blast/and intuitive to work with.
It can aid the user byhighlighting similarities with a reference sentence.BLAST is flexible in that it can be used with out-put from any MT system, and with any hierarchicalerror typology.
It has a modular design, allowingeasy extension with new modules.
To the best of ourknowledge, there is no other publicly available toolfor MT error annotation.
Since we believe that erroranalysis is a vital complement to MT evaluation, wethink that BLAST can be useful for many other MTresearchers and developers.2 MT Evaluation and Error AnalysisHovy et al (2002) discussed the complexity of MTevaluation, and stressed the importance of adjustingevaluation to the purpose and context of the trans-lation.
However, MT is very often only evaluatedquantitatively using a single metric, especially in re-search papers.
Quantitative evaluations can be au-tomatic, using metrics such as Bleu (Papineni etal., 2002) or Meteor (Denkowski and Lavie, 2010),where the MT output is compared to one or more hu-man reference translations.
Metrics, however, onlygive a single quantitative score, and do not give anyinformation about the strengths and weaknesses ofthe system.
Comparing scores from different met-rics can give a very rough indication of some majorproblems, especially in combination with a part-of-speech analysis (Popovic?
et al, 2006).Human evaluation is also often quantitative, forinstance in the form of estimates of values such asadequacy and fluency, or by ranking sentences fromdifferent systems (e.g.
Callison-Burch et al (2007)).A combination of human and automatic metrics is56human-targeted metrics such as HTER, where a hu-man corrects the output of a system to the clos-est correct translation, on which standard metricssuch as TER is then computed (Snover et al, 2006).While these types of evaluation are certainly useful,they are expensive and time-consuming, and still donot tell us anything about the particular errors of asystem.2Thus, we think that qualitative evaluation is animportant complement, and that error analysis, theidentification and classification of MT errors, is animportant task.
There have been several suggestionsfor general MT error typologies (Flanagan, 1994;Vilar et al, 2006; Farru?s et al, 2010), targeted atdifferent user groups and purposes, focused on eitherevaluation of single systems, or comparison betweensystems.
It is also possible to focus error analysis ata specific problem, such as verb form errors (Murataet al, 2005).We have not been able to find any other freelyavailable tool for error analysis of MT.
Vilar et al(2006) mentioned in a footnote that ?a tool for high-lighting the differences [between the MT system anda correct translation] also proved to be quite useful?for error analysis.
They do not describe this tool anyfurther, and do not discuss if it was also used to markand store the error annotations themselves.Some tools for post-editing of MT output, a re-lated activity to error analysis, have been describedin the literature.
Font Llitjo?s and Carbonell (2004)presented an online tool for eliciting informationfrom the user when post-editing sentences, in or-der to improve a rule-based translation system.
Thepost-edit operations were labeled with error cate-gories, making it a type of error analysis.
This toolwas highly connected to their translation system,and it required users to post-edit sentences by mod-ifying word alignments, something that many usersfound difficult.
Glenn et al (2008) described a post-editing tool used for HTER calculation, which hasbeen used in large evaluation campaigns.
The toolis a pure post-editing tool and the edits are not clas-sified.
Graphical tools have also successfully beenused to aid humans in other MT-related tasks, suchas human MT evaluation of adequacy, fluency and2Though it does, at least in principle, seem possible to mineHTER annotations for more informationsystem comparison (Callison-Burch et al, 2007),and word alignment (Ahrenberg et al, 2003).3 System OverviewBLAST is a tool for human annotations of bilingualmaterial.
Its main purpose is error analysis for ma-chine translation.
BLAST is designed for use in anyMT evaluation project.
It is not tied to the informa-tion provided by specific MT systems, or to specificlanguages, and it can be used with any hierarchi-cal error typology.
It has a preprocessing modulefor automatically aiding the annotator by highlight-ing similarities between the MT output and a refer-ence.
Its modular design allows easy integration ofnew modules for preprocessing.
BLAST has threeworking modes for handling error annotations: foradding new annotations, for editing existing annota-tions, and for searching among annotations.BLAST can handle two types of annotations: er-ror annotations and support annotations.
Error an-notations are based on a hierarchical error typology,and are used to annotate errors in MT output.
Errorannotations are added by the users of BLAST.
Sup-port annotations are used as a support to the user,currently to mark similarities in the system and ref-erence sentences.
The support annotations are nor-mally created automatically by BLAST, but they canalso be modified by the user.
Both annotation typesare stored with the indices of the words they applyto.Figure 1 shows a screenshot of BLAST.
The MToutput is shown to the annotator one segment at atime, in the upper part of the screen.
A segment nor-mally consists of a sentence and the MT output canbe accompanied by a source sentence, a referencesentence, or both.
Error annotations are marked inthe segments by bold, underlined, colored text, andsupport annotations are marked by light backgroundcolors.
The bottom part of the tool, contains the er-ror typology, and controls for updating annotationsand navigation.
The error typology is shown usinga menu structure, where submenus are activated bythe user clicking on higher levels.3.1 Design goalsWe created BLAST with the goal that it should beflexible, and allow maximum freedom for the user,57Figure 1: Screenshot of BLASTbased on the following goals:?
Independent of the MT system being analyzed,particularly not dependent on specific informa-tion given by a particular MT system, such asalignment information?
Compatible with any error typology?
Language pair independent?
Possible to mark where in a sentence an erroroccurs?
Possible to view either source or reference sen-tences, or both?
Possible to automatically highlight similaritiesbetween the system and the reference sentences?
Containing a search function for errors?
Simple to understand and useThe current implementation of BLAST fulfils allthese goals, with the possible small limitation thatthe error typology has to be hierarchical.
We believethis limitation is minor, however, since it is possibleto have a relatively flat structure if desired, and tore-use the same submenu in many places, allowingcross-classification within a hierarchical typology.The flexibility of the tool gives users a lot of free-dom in how to use it in their evaluation projects.However, we believe that it is important within ev-ery error annotation project to use a set error typol-ogy and guidelines for annotation, but the annotationtool should not limit users in making these choices.3.2 Error TypologiesAs described above, BLAST is easily configurablewith new typologies for annotation, with the onlyrestriction that the typology is hierarchical.
BLASTcurrently comes with the following implemented ty-pologies, some of which are general, and some ofwhich are targeted at specific language (pairs):?
Vilar et al (2006)?
General?
Chinese?
Spanish?
Farru?s et al (2010)?
Catalan?Spanish?
Flanagan (1994) (slightly modified into a hier-archical structure)?
French58?
German?
Our own tentative fine-grained typology?
General?
SwedishThe error typologies can be very big, and it is hardto fit an arbitrarily large typology into a graphicaltool.
BLAST thus uses a menu structure which al-ways shows the categories in the first level of the ty-pology.
Lower subtypologies are only shown whenthey are activated by the user clicking on a higherlevel.
In Figure 1, the subtypologies to Word orderwere activated by the user first clicking on Word or-der, then on Phrase level.It is important that typologies are easy to extendand modify, especially in order to cover new targetlanguages, since the translation problems to someextent will be dependent on the target language, forinstance with regard to the different agreement phe-nomena in languages.
The typologies that come withBLAST can serve as a starting point for adjusting ty-pologies, especially to new target languages.3.3 ImplementationBLAST is implemented as a Java application usingSwing for the graphical user interface.
Using Javamakes it platform independent, and it is currentlytested on Unix, Linux, Mac, and Windows.
BLASThas an object-oriented design, with a particular fo-cus on modular design, to allow it to be easily ex-tendible with new modules for preprocessing, read-ing and writing to different file formats, and present-ing statistics.
Unicode is used in order to allow ahigh number of languages, and sentences can be dis-played both right to left, and left to right.
BLASTis open source and is released under the LGPL li-cense.33.4 File formatsThe main file types used in BLAST is the annotationfile, containing the translation segments and annota-tions, and the typology file.
These files are storedin a simple text file format.
There is also a configu-ration file, which can be used for program settings,besides using command line options, for instance toconfigure color schemes, and to change preprocess-ing settings.
The statistics of an annotation project3http://www.gnu.org/copyleft/lesser.htmlare printed in a text file in a human-readable format(see Section 4.5).The annotation file contains the translation seg-ments for the MT system, and possibly for thesource and reference sentences, and all error andsupport annotations.
The annotations are stored withthe indices of the word(s) in the segments that weremarked, and a label identifying the error type.
Theannotation file is initially created automatically byBLAST based on sentence aligned files.
It is thenupdated by BLAST with the annotations added bythe user.The typology file has a header with main informa-tion, and then an item for each menu containing:?
The name of the menu?
A list of menu items, containing:?
Display name?
Internal name (used in annotation file, andinternally in BLAST)?
The name of its submenu (if any)The typology files have to be specified by the user,but BLAST comes with several typology files, as de-scribed in Section 3.2.4 Working with BLASTBLAST has three different working modes: annota-tion, edit and search.
The main mode is annotation,which allows the user to add new error annotations.The edit mode allows the user to edit and remove er-ror annotations.
The search mode allows the user tosearch for errors of different types.
BLAST can alsocreate support annotations, that can later be updatedby the user, and calculate and print statistics of anannotation project.4.1 AnnotationThe annotation mode is the main working mode inBLAST, and it is active in Figure 1.
In annotationmode a segment is shown with all its current er-ror annotations.
The annotations are marked withbold and colored text, where the color depends onthe main type of the error.
For each new annotationthe user selects the word or words that are wrong,and selects an error type.
In figure 1, the words notelevision, and the error type Word order?Phraselevel?Long are selected in order to add a new error59annotation.
BLAST ignores identical annotations,and warns the user if they try to add an annotationfor the exact same words as another annotation.4.2 EditIn edit mode the user can change existing error an-notations.
In this mode only one annotation at a timeis shown, and the user can switch between them.
Foreach annotation affected words are highlighted, andthe error typology area shows the type of the error.The currently shown error can be changed to a dif-ferent error type, or it can be removed.
The editmode is useful for revising annotations, and for cor-recting annotation errors.4.3 SearchIn search mode, it is possible to search for errors ofa certain type.
To search, users choose the error typethey want to search for in the error typology, andthen search backwards or forwards for error annota-tions of that type.
It is possible both to search forspecific errors deep in the typology, and to searchfor all errors of a type higher in the typology, forinstance, to search for all word order errors, regard-less of subclassification.
Search is active between allsegments, not only for the currently shown segment.Search is useful for controlling the consistency ofannotations, and for finding instances of specific er-rors.4.4 Support annotationsError annotation is a hard task for humans, and thuswe try to aid it by including automatic preprocess-ing, where similarities between the system and refer-ence sentences are marked at different levels of sim-ilarity.
Even if the goal of the error analysis often isnot to compare the MT output to a single reference,but to the closest correct equivalent, it can still beuseful to be able to see the similarities to one ref-erence sentence, to be able to identify problematicparts easier.For this module we have adapted the codefor alignment used in the Meteor-NEXT metric(Denkowski and Lavie, 2010) to BLAST.
In Meteor-NEXT the system and reference sentences arealigned at the levels of exact matching, stemmedmatching, synonyms, and paraphrases.
All thesemodules work on lower-cased data, so we added amodule for exact matching with the original casingkept.
The exact and lower-cased matching worksfor most languages, and stemming for 15 languages.The synonym module uses WordNet, and is onlyavailable for English.
The paraphrase module isbased on an automatic paraphrase induction method(Bannard and Callison-Burch, 2005), it is currentlytrained for five languages, but the Meteor-NEXTcode for training it for additional languages is in-cluded.Support annotations are normally only created au-tomatically, but BLAST allows the user to edit them.The mechanism for adding, removing or changingsupport annotations is separate from error annota-tions, and can be used regardless of mode.4.5 Create StatisticsThe statistics module prints statistics about the cur-rently loaded annotation project.
The statistics areprinted to a file, in a human-readable format.
It con-tains information about the number of sentences anderrors in the project, average number of errors persentence, and how many sentences there are withcertain numbers of errors.
The main part of thestatistics is the number and percentage of errors foreach node in the error typology.
It is also possible toget the number of errors for cross-classifications, byspecifying regular expressions for the categories tocross-classify in the configuration file.5 Future ExtensionsBLAST is under active development, and we plan toadd new features.
Most importantly we want to addthe possibility to annotate two MT systems in paral-lel, which can be useful if the purpose of the annota-tion is to compare MT systems.
We are also workingon refining and developing the existing proposals forerror typologies, which is an important complementto the tool itself.
We intend to define a new fine-grained general error typology, with extensions to anumber of target languages.The modularity of BLAST also makes it possibleto add new modules, for instance for preprocess-ing and to support other file formats.
One examplewould be to support error annotation of only specificphenomena, such as verb errors, by adding a prepro-cessing module for highlighting verbs with support60annotations, and a suitable verb-focused error typol-ogy.
We are also working on a preprocessing modulebased on grammar checker techniques (Stymne andAhrenberg, 2010), that highlights parts of the MToutput that it suspects are non-grammatical.Even though the main purpose of BLAST is forerror annotation of machine translation output, thefreedom in the use of error typologies and supportannotations also makes it suitable for other taskswhere bilingual material is used, such as for anno-tations of named entities in bilingual texts, or foranalyzing human translations, e.g.
giving feedbackto second language learners, with only the additionof a suitable typology, and possibly a preprocessingmodule.6 ConclusionWe presented BLAST; a flexible tool for annotationof bilingual segments, specifically intended for erroranalysis of MT.
BLAST facilitates the error analysistask, which we believe is vital for MT researchers,and could also be useful for other users of MT.
Itsflexibility makes it possible to annotate translationsfrom any MT system and between any languagepairs, using any hierarchical error typology.ReferencesLars Ahrenberg, Magnus Merkel, and Michael Petterst-edt.
2003.
Interactive word alignment for languageengineering.
In Proceedings of EACL, pages 49?52,Budapest, Hungary.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL, pages 597?604, Ann Arbor, Michigan,USA.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)evaluation of machine translation.
In Proceedings ofWMT, pages 136?158, Prague, Czech Republic, June.Michael Denkowski and Alon Lavie.
2010.
METEOR-NEXT and the METEOR paraphrase tables: Improvedevaluation support for five target languages.
In Pro-ceedings of WMT and MetricsMATR, pages 339?342,Uppsala, Sweden.Mireia Farru?s, Marta R. Costa-jussa`, Jose?
B. Marin?o, andJose?
A. R. Fonollosa.
2010.
Linguistic-based evalu-ation criteria to identify statistical machine translationerrors.
In Proceedings of EAMT, pages 52?57, SaintRaphae?l, France.Mary Flanagan.
1994.
Error classification for MTevaluation.
In Proceedings of AMTA, pages 65?72,Columbia, Maryland, USA.Ariadna Font Llitjo?s and Jaime Carbonell.
2004.
Thetranslation correction tool: English-Spanish user stud-ies.
In Proceedings of LREC, pages 347?350, Lisbon,Portugal.Meghan Lammie Glenn, Stephanie Strassel, LaurenFriedman, and Haejoong Lee.
2008.
Managementof large annotation projects involving multiple humanjudges: a case study of GALE machine translationpost-editing.
In Proceedings of LREC, pages 2957?2960, Marrakech, Morocco.Eduard Hovy, Margaret King, and Andrei Popescu-Belis.2002.
Principles of context-based machine translationevaluation.
Machine Translation, 17(1):43?75.Masaki Murata, Kiyotaka Uchimoto, QingMa, ToshiyukiKanamaru, and Hitoshi Isahara.
2005.
Analysis ofmachine translation systems?
errors in tense, aspect,and modality.
In Proceedings of PACLIC 19, pages155?166, Taipei, Taiwan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of ACL,pages 311?318, Philadelphia, Pennsylvania, USA.Maja Popovic?, Adria` de Gisper, Deepa Gupta, PatrikLambert, Hermann Ney, Jose?
Marin?o, and RafaelBanchs.
2006.
Morpho-syntactic information for au-tomatic error analysis of statistical machine translationoutput.
In Proceedings of WMT, pages 1?6, New YorkCity, New York, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human notation.In Proceedings of AMTA, pages 223?231, Cambridge,Massachusetts, USA.Sara Stymne and Lars Ahrenberg.
2010.
Using a gram-mar checker for evaluation and postprocessing of sta-tistical machine translation.
In Proceedings of LREC,pages 2175?2181, Valetta, Malta.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error analysis of machine transla-tion output.
In Proceedings of LREC, pages 697?702,Genoa, Italy.61
