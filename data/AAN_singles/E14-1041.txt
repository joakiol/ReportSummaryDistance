Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 386?394,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsAcquisition of Noncontiguous Class Attributes from Web Search QueriesMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractPrevious methods for extracting attributes(e.g., capital, population) of classes (Em-pires) from Web documents or searchqueries assume that relevant attributes oc-cur verbatim in the source text.
The ex-tracted attributes are short phrases thatcorrespond to quantifiable properties ofvarious instances (ottoman empire, ro-man empire, mughal empire) of the class.This paper explores the extraction of non-contiguous class attributes (manner (it)claimed legitimacy of rule), from fact-seeking and explanation-seeking queries.The attributes cover properties that arenot always likely to be extracted as shortphrases from inherently-noisy queries.1 IntroductionMotivation: Resources such as Wikipedia (Remy,2002) and Freebase (Bollacker et al., 2008) aimat organizing knowledge around classes (Food in-gredients, Astronomical objects, Religions) andtheir instances (wheat flower, uranus, hinduism).Due to inherent limitations associated with main-taining and expanding human-curated resources,their content may be incomplete.
For example,attributes representing the energy (or energy per100g) or solubility in water are available in bothWikipedia and Freebase for many instances ofFood ingredients (e.g., for olive oil, honey, fennel).But the attributes are missing for some instances(e.g., cornmeal).
Moreover, structured informa-tion about how long (it) lasts unopened or manner(it) helps in weight loss is generally missing forFood ingredients, from both resources.
Such in-formation is also often absent from among the at-tributes acquired from either documents or queriesby previous extraction methods (Pas?ca et al., 2007;Van Durme et al., 2008).
Previously extracted at-tributes tend to be short, often nominal, phraseslike nutritional value and taste.
Even when ex-tracted attributes are not nominal (Pas?ca, 2012),they remain relatively short phrases such as goodfor skin.
As such, previous attributes have limitedability to capture the finer-grained properties be-ing asked about in queries such as ?how long doesolive oil last unopened?
and ?how does honey helpin weight loss?.
The presence of such queriessuggests that such information is relevant to Webusers.
Identifying noncontiguous properties, orattributes of interest to Web users, helps fillingsome of the gaps in existing knowledge resources,which otherwise could not be filled by attributesextracted with previous methods.Contributions: The contributions of this paperare twofold.
First, it introduces a method for theacquisition of noncontiguous class attributes, fromfact or explanation-seeking Web search querieslike ?how long does olive oil last unopened?
or?how does honey help in weight loss?.
The re-sulting attributes are more diverse than, and there-fore subsume, the scope of attributes extractedby previous methods.
Indeed, previous meth-ods are unlikely to extract attributes as specificas length/duration (it) lasts unopened and man-ner (it) helps in weight loss, for the instances oliveoil and honey of the class Food ingredients.
Con-versely, previously extracted attributes like nutri-tional value and solubility in water are roughlyequivalent to the finer-grained nutritional value(it) has and reason (it) dissolves in water, ex-tracted from the queries ?what nutritional valuedoes honey have?
and ?why does glucose dissolvein water?
respectively.
Second, the noncontiguousattributes can be simultaneously interpreted as bi-nary relations pertaining to instances and classes.The relations (helps in weight loss) connect an in-stance (honey) or, more generally, a class (Foodingredients), on one hand; and a loosely-typed un-known argument (manner) whose value is of in-terest to Web users, on the other hand.
Because386Web users already inquire about the value of oneof their arguments, the extracted relations are morelikely to be relevant for the respective instancesand classes, than relations extracted from arbitrarydocument sentences (Fader et al., 2011).2 Noncontiguous AttributesIntuitions: Users tend to formulate their Websearch queries based on knowledge that they al-ready possess at the time of the search (Pas?ca,2007).
Therefore, search queries play two rolessimultaneously: in addition to requesting new in-formation, they indirectly convey knowledge inthe process.
In particular, attributes correspondto quantifiable properties of instances and theirclasses.
The extraction of attributes from queriesstarts from the intuition that, if an attribute A is rel-evant for a class C, then users are likely to ask forthe value of the attribute A, for various instancesI of the class C. If nutritional value and diameterare relevant attributes of the classes Food ingre-dients and Astronomical objects respectively, it islikely that users submit queries to inquire aboutthe values of the attributes for instances of thetwo classes.
Such queries could take the form?what is the (nutritional value)Aof (olive oil)I?and ?what is the (diameter)Aof (jupiter)I?
; orthe more compact ?
(nutritional value)Aof (oliveoil)I?
and ?
(diameter)Aof (jupiter)I?.
In thiscase, the attributes are relatively short phrases(nutritional value, diameter), and are expected toappear as contiguous phrases within queries.
Pre-vious methods on attribute extraction from queriesspecifically target this type of attributes.
In fact,some methods apply dedicated extraction patterns(e.g., A of I) over either queries (Pas?ca et al.,2007) or documents (Tokunaga et al., 2005).
Othermethods expand manually-provided seed sets ofattributes, with other phrases that co-occur withinstances within queries, in similar contexts as theseed attributes do (Pas?ca, 2007).While simpler properties are often mentioned inqueries as short, contiguous phrases, finer-grainedproperties often are not.
Queries seeking the rea-son for solidification for some Food ingredientscould, but rarely do, contain the attribute ver-batim (?what is the reason for the solidificationof honey?).
Instead, queries are more likely toinquire about the expected value, while specify-ing the instance and the properties encoded bythe attribute (?
(why)Adoes (honey)I(solidify)A?
).Readable descriptions (names) of the attributescan be recovered from the queries, by assemblingthe type of the expected value and the proper-ties together (reason (it) solidifies).
Thus, factand explanation-seeking queries are an intriguingsource of noncontiguous attributes that are not re-stricted to short phrases, and are not required tooccur as contiguous phrases in queries.Acquisition from Queries: The extractionmethod proposed in this paper takes as input a setof target classes, each of which is available as aset of instances that belong to the class; and a setof anonymized queries independent from one an-other.
As illustrated in Figure 1, the method se-lects queries that contain an instance of a classtogether with what is deemed to be likely a non-contiguous attribute, and outputs ranked lists ofattributes for each class.
The extraction consistsin several stages:?
selection of a subset of queries that containan instance in a form that suggests the queries askfor the value of a noncontiguous attribute of theinstance;?
extraction of noncontiguous attributes, fromquery fragments that describe the property of in-terest and the type of its expected value;?
aggregation and ranking of attributes of in-dividual instances of a class, into attributes of aclass.Extraction Patterns: In order to determinewhether a query contains an attribute of a class,the query is matched against the extraction pat-terns from Table 1.
The use of patterns in attributeextraction has been previously suggested in (Pas?caet al., 2007; Tokunaga et al., 2005), where the pat-tern what is the A of I extracts noun-phrase Aattributes of instances I from queries and docu-ments.
In our case, the patterns are constructedsuch that they match fact-seeking and explanation-seeking questions that likely inquire about thevalue of a relevant property of an instance I of theclass C. For example, the first pattern from Ta-ble 1 matches queries such as ?when did everquestbecome free to play?
and ?when was radon dis-covered as an element?, which inquire about thedate or time when certain events affected certainproperties of the instances everquest and radon re-spectively.
Instances I of the class C may be avail-able as non-disambiguated items, that is, as strings(java) whose meaning is otherwise unknown; oras disambiguated items, that is, as strings associ-387Query logswho discovered the element iron what family does zinc belong to in the periodic tablewhen was radon discovered as an element   how does oxygen return to the atmospherewhy does chlorine react with water   what elements does argon combine withhow does javascript run   who created haskell   how does java executewho invented the programming language cobol   how long does python take to learnhow does java compile   when was c# first released   where does python install tohow does c# differ from c++   how does javascript store dateswhen did minecraft come out for xbox 360   when did everquest become free to playwho does the voice in black ops 2   when did league of legends become free to playwhen was fable 2 released   how much does world of warcraft cost to play onlineExtracted class attributesChemical elements: {who can you unlock in band hero   how many copies did halo reach sell the first daydate/time (it) was discovered as an element, manner (it) returns to the atmosphere,who discovered the element, manner (it) enters the soil, reason (it) reacts with water,elements (it) combines with, manner (it) reacts with other elements,family (it) belongs to in the periodic table, number of electrons (it) gains, ...}Target classesProgramming languages: {manner (it) executes, length/duration (it) takes to learn, file extension (it) uses, ...}manner (it) differs from c++, manner (it) compiles, manner (it) stores dates,who is using (it), location (it) installs to, date/time (it) was first released,manner (it) runs, who created (it), who invented the programming language,date/time (it) was released, number of copies (it) sold first day,Video games: {date/time (it) came out for xbox 360, date/time it came out for ps2,price/quantity/degree (it) costs to play online, date/time (it) became free to play, ...}date/time (it) came out for pc, who does the voice in (it), who can you unlock in (it),Chemical elements: {radon, chlorine, argon, nitrogen, oxygen, carbon,hydrogen, iron, zinc, ...}cobol, lisp, actionscript, ...}Video games: {minecraft, black ops II, league of legends, halo reach, everquest,fable 2, world of warcraft, band hero, ...}Programming languages: {c#, javascript, haskell, json, perl, java, python, prolog,how many electrons does chlorine gain   who is using lisphow does oxygen interact with other elements   how does nitrogen enter the soilFigure 1: Overview of extraction of noncontiguous attributes from Web search queriesated with pointers to knowledge base entries with adisambiguated meaning (Java (programming lan-guage)).
In the first case, the matching of a queryfragment, on one hand, to the portion of an ex-traction pattern corresponding to an instance I , onthe other hand, consists in simple string match-ing.
In the second case, the matching requiresthat the disambiguation of the query fragment, inthe context of the query, matches the desired dis-ambiguated meaning of I from the pattern.
Thesubset of queries matching any of the extractionpatterns, for any instances I of a class C, are thequeries that contribute to extracting noncontigu-ous attributes of the class C.Collecting Attributes of Individual Instances:A small set of rules optionally converts wh-prefixes into coarse-grained types of the expectedvalues (e.g., how long into length/duration; orwhen into date/time).
In the case of what-prefixedqueries, the adjacent noun phrase, if any, is con-sidered to be the expected type (?what nutritionalvalue ..?
into nutritional value).
Similar ruleshave been employed for shallow analysis of open-domain questions (Dumais et al., 2002).
The pred-icate verbs in the remainder of the query are up-dated, to match the tense specified by the auxil-iary verb (e.g., ?when did ..?
), if any, followingthe wh-prefix.
Thus, the verb come is convertedto the past tense came, in the case of the query?when did minecraft come out for xbox 360?.
An388Extraction Pattern?
Examples of Matched Querieswhen [does|did|do|was|were] [a|an|the|<nothing>] I A?
when did everquest become free to playwhy [does|did|do|was|were] [a|an|the|<nothing>] I A?
why does chlorine interact with waterwhere [does|did|do|was|were] [a|an|the|<nothing>] I A?
where does radon occur naturallyhow [does|did|do|was|were] [a|an|the|<nothing>] I A?
how does nitrogen enter the soilwho [does|did|do|was|were] [a|an|the|<nothing>] I A?
who did claude monet study underhow A [does|did|do|was|were] [a|an|the|<nothing>] I A?
how fast does oxygen dissolve in waterwho A I?
who invented the programming language cobol(Note: A does not start with [is|are|was|were])what A [does|did|do|was|were] [a|an|the|<nothing>] I A?
what elements does argon combine withwhichA [does|did|do|was|were] [a|an|the|<nothing>] I A?
which ports does minecraft useTable 1: The extraction patterns match queries thatare likely to inquire about the value of a noncon-tiguous attribute of an instance (I=a required in-stance; A=a required non-empty sequence of arbi-trary tokens)attribute is constructed from the concatenation ofthe wh-prefix or expected type (date/time); theslot pronoun it, in lieu of the instance (date/time(it)); and the query remainder after tense conver-sion (date/time (it) came out for xbox 360).
If thelinking verb following the wh-prefix is a form ofbe (e.g., was), then the linking verb is also re-tained after the slot pronoun, to form a more co-herent attribute (date/time (it) was first released).Since constructed attributes are noun phrases, theyare more consistent with, and can be more eas-ily inserted among, existing attributes in struc-tured data repositories (infobox entries of articlesin Wikipedia, or property names or topics in Free-base).Aggregation into Class Attributes: Attributes ofa class C are aggregated from attributes of indi-vidual instances I of the class.
An attribute Ais deemed more relevant for C if the attribute isextracted for more of the instances I of the classC, and for fewer instances I that do not belong tothe class C. Concretely, the score of an attributefor a class is the lower bound of the Wilson scoreinterval (Brown et al., 2001) where the numberof positive observations is the number of queriesfor which the attribute A is extracted for some in-stance I in the class C, |{Query(I, A)}I?C|; andthe number of negative observations is the num-ber of queries for which the attribute A is ex-tracted for some instances I outside of the classC, |{Query(I, A)}I /?C|.
The scores are internallycomputed at 95% confidence.
Attributes of eachclass are ranked in decreasing order of their scores.Reduction of Near-Duplicate Attributes: Due tolexical variations across queries from which at-tributes are extracted, some of the attributes areequivalent or nearly equivalent to one another.
Forexample, gained independence, won its indepen-dence and gained its freedom of the class Coun-tries are roughly equivalent, although they employdistinct tokens.
The diversity and potential useful-ness of a ranked list of attributes can be increased,if groups of near-duplicate attributes are identifiedin the list, and merged together.A lower-ranked attribute is marked as a near-duplicate of a higher-ranked (i.e., earlier) attributefrom the list, if all tokens from the lower-rankedattribute match either tokens from the higher-ranked attribute (gained independence vs. wonits independence), or tokens from synonyms ofphrases from the earlier attribute (gained indepen-dence vs. won its independence; or takes to showsymptoms vs. takes to come out).
Stop words,which include linking verbs, pronouns, determin-ers, conjunctions, wh-prefixes and prepositions,are not required to match.
Synonyms may be ei-ther derived from existing lexical resources (e.g.,WordNet (Fellbaum, 1998)), or mined from largedocument collections (Madnani and Dorr, 2010).Lower-ranked near-duplicate attributes are mergedwith the higher-ranked ones from the ranked list,thus improving the diversity of the list.3 Experimental SettingTextual Data Sources: The experiments relyon a random sample of around 1 billion fully-anonymized queries in English, submitted to ageneral-purpose Web search engine.
Each queryis available independently from other queries, andis accompanied by its frequency of occurrence inthe query logs.Target Classes: Table 2 shows the set of 40 tar-get classes for evaluating the attributes extractedfrom queries.
In an effort to reuse experimentalsetup proposed in previous work, each of the 40manually-compiled classes introduced in (Pas?ca,2007) is mapped into the Wikipedia category thatbest matches it.
For example, the evaluationclasses Aircraft Model, Movie, Religion and Ter-389Class (Examples of Instances)Actors (keanu reeves, milla jovovich, ben affleck), Air-craft (boeing 737, bombardier crj200, embraer 170), An-imated characters (bugs bunny, pink panther (character),yosemite sam), Association football clubs (a.s. roma, flu-minense football club, real madrid), Astronomical objects(alpha centauri, jupiter, delta corvi), Automobiles (nis-san gt-r, tesla model s, toyota prius), Awards (grammyaward, justin winsor prize (library), palme d?or), Battlesand operations of world war ii (battle of midway, opera-tion postmaster, battle of milne bay), Chemical elements(plutonium, radon, hydrogen), Cities (rio de janeiro, os-aka, chiang mai), Companies (best buy, aveeno, pep-sico), Countries (costa rica, rwanda, south korea), Cur-rencies by country (japanese yen, swiss franc, koreanwon), Digital cameras (canon eos 400d, nikon d3000,pentax k10d), Diseases and disorders (anorexia nervosa,hyperlysinemia, repetitive strain injury), Drugs (flutica-sone propionate, phentermine, tramadol), Empires (ot-toman empire, roman empire, mughal empire), Films (thefifth element, mockingbird don?t sing, ten thousand yearsolder), Flowers (trachelospermum jasminoides, lavandulastoechas, evergreen rose), Food ingredients (carrot, oliveoil, fennel), Holidays (good friday, easter, halloween),Hurricanes in North America (hurricane katrina, hurri-cane wilma, hurricane dennis), Internet search engines(google, baidu, lycos), Mobile phones (nokia n900, htcdesire, samsung s5560), Mountains (mount rainier, cerrosan luis obispo, steel peak), National Basketball Associa-tion teams (los angeles lakers, cleveland cavaliers, indianapacers), National parks (yosemite national park, orang na-tional park, tortuguero national park), Newspapers (theeconomist, corriere del trentino, seattle medium), Organi-zations designated as terrorist (taliban, shining path, eta),Painters (claude monet, domingo antonio velasco, tarci-sio merati), Programming languages (javascript, prolog,obliq), Religious faiths traditions and movements (con-fucianism, fudoki, omnism), Rivers (danube, pingo river,viehmoorgraben), Skyscrapers (taipei 101, 15 penn plaza,eqt plaza), Sports events (tour de france, 1984 scottish cupfinal, rotlewi versus rubinstein), Stadiums (fenway park,chengdu longquanyi, stade geoffroy-guichard), Treaties(treaty of versailles, franco-indian alliance, treaty of cor-doba), Universities and colleges (cornell university, nu-gaal university, gale college), Video games (minecraft,league of legends, everquest), Wine (madeira wine, yel-low tail (wine), port wine)Table 2: Set of 40 Wikipedia categories used astarget classes in the evaluation of attributesroristGroup from (Pas?ca, 2007) are mapped intothe Wikipedia categories Aircraft, Films, Religiousfaiths traditions and movements and Organiza-tions designated as terrorist respectively.
Thename of the Wikipedia category only serves as aconvenience label for its target class, and is nototherwise exploited in any way during the evalua-tion.
Instead, a target class consists in a set of titlesof Wikipedia articles, of which sample titles (e.g.,the Wikipedia article titled nissan gt-r) are shownin lowercase for each class (e.g., Automobiles) inTable 2.
The set of instances of a class is selectedfrom all articles listed under the respective cate-Label Examples of Attributesvital Astronomical objects: manner (it) generates itsenergyFood ingredients: temperature (it) solidifiesReligion: date/time (it) became a religionokay Astronomical objects: manner (it) became aconstellationFood ingredients: reason (it) sparks in the mi-crowaveReligion: manner (it) feels about abortionwrong Astronomical objects: reason (it) has armsFood ingredients: manner (it) cleans penniesReligion: who owns (it)Table 3: Correctness labels manually assigned toattributes extracted for various classesgory in Wikipedia, or listed under sub-categoriesof the respective category.The target classes contain between 41 (for Na-tional Basketball Association teams) and 66,934(for Films) instances, with an average of 10,730instances per class.Synonym Repository: A synonym repository ex-tracted separately from Web documents containsmappings from each of around 60,000 phrases inEnglish, to lists of their synonym phrases.
For ex-ample, the top synonyms available for the phrasesturn off and contagious are [switch off, extinguish,turn out, ..] and [infectious, catching, communica-ble, ..] respectively.Parameter Settings: Queries that match any ofthe extraction patterns from Table 1 are syntac-tically parsed (Petrov et al., 2010).
As a pre-requisite, the portion I of the patterns from thetable must match a disambiguated instance froma query.A variation of the tagger introducedin (Cucerzan, 2007) maps query fragmentsto their disambiguated, corresponding Wikipediainstances (i.e., to Wikipedia articles).
The taggeris simplified to select the longest instance men-tions, and does not use gazetteers or queries fortraining.
Depending on the sources of textualdata available for training, any taggers (Cucerzan,2007; Ratinov et al., 2011; Pantel et al., 2012) thatdisambiguate text fragments relative to Wikipediaentries can be employed.4 Evaluation ResultsAttribute Accuracy: The top 50 attributes, fromthe ranked lists extracted for each target class, aremanually assigned correctness labels.
As shown inTable 3, an attribute is marked as vital, if it mustbe present among representative attributes of the390Class Precision of Extracted Attributes%vital %okay %wrong ScoreAwards 29 14 7 0.72Chemical elements 46 2 2 0.94Companies 42 1 7 0.85Food ingredients 31 9 10 0.71Programming languages 31 7 12 0.69Stadiums 42 5 3 0.89Video games 33 14 3 0.80...Avg-All-Classes 33 10 7 0.76Table 4: Accuracy of top 50 class attributes ex-tracted from fact-seeking and explanation-seekingqueries, over the evaluation set of 40 target classesclass; okay, if it provides useful but non-essentialinformation; and wrong, if it is incorrect (Pas?ca,2007).
For example, the attributes manner (it) gen-erates its energy, manner (it) became a constella-tion and reason (it) has arms are annotated as vital,okay and wrong respectively for the class Astro-nomical objects.
To compute the precision scoreover a set of attributes, the correctness labels areconverted to numeric values: vital to 1.0, okay to0.5, and wrong to 0.0.
Precision is the sum of thecorrectness values of the attributes, divided by thenumber of attributes.Table 4 summarizes the precision scores overthe evaluation set of target classes.
The scoresvary from one class to another, for example 0.71for Food ingredients but 0.94 for Chemical el-ements.
The average score is 0.76, indicatingthat attributes extracted from fact and explanation-seeking queries have encouraging levels of accu-racy.
The results already take into account thedetection of near-duplicate attributes.
More pre-cisely, the highest-ranked attribute in each groupof near-duplicate attributes, examples of which areshown in Table 5, is retained and evaluated; thelower-ranked attributes from each group are notconsidered in the evaluation.
Attributes like num-ber of passengers (it) can hold, number of pas-sengers it fits and number of passengers it seatsare nearly equivalent, but are still not marked asnear-duplicates for the class Aircraft, when theyshould.
Conversely, the attribute location (it)lives is marked as a near-duplicate of location (it)lives in new york, when it should not.
Never-theless, a significant number of near-duplicates,which would otherwise crowd the ranked lists ofattributes with redundant information, are identi-fied and discarded.Target Class: Group of Near-Duplicate AttributesActors: movies (it) plays in, played in, acts in, acted in,played, played onAutomobiles: date (it) was first manufactured, first pro-duced, first madeBattles and operations of World War II: reason (it) hap-pened, took place, occurredChemical elements: manner (it) returns to the atmo-sphere, gets back into the atmosphere, got into the atmo-sphere, gets into the atmosphere, enters the environment,enters the atmosphereCompanies: location (it) makes its products, manufac-tures its products, produces its products, gets its products,makes its products, manufactures their productsCompanies: date/time (it) began outsourcing, started out-sourcing, outsourcedCountries: date (it) got its independence, gained indepen-dence, gained its independence, got independence, gottheir independence, won its independence, achieved inde-pendence, received its independence, gained its freedomDiseases and disorders: length/duration (it) takes to showsymptoms, takes to show up, takes to show, takes to ap-pear, takes to manifest, takes to come outTable 5: Groups of near-duplicate attributes iden-tified for various classes.
Attributes within a groupare ranked according to their individual scores.Removing all but the first attribute of each group,from the ranked list of attributes of the respectiveclass, improves the diversity of the listDiscussion: The set of patterns shown in Table 1is extensible.
Moreover, the patterns are subjectto errors.
They may cause false matches, resultingin erroneous extractions.
The extent to which thisoccurs is indirectly measured in the overall preci-sion results.
The modification of some of the pat-terns, or the addition of new ones, would likely af-fect the expected coverage and precision of the ex-tracted attributes.
If a pattern is particularly noisy,it is likely to cause systematic errors, and thereforeproduce attributes of lower quality.Since attributes in Wikipedia and Freebase areinitially entered manually by human editors, theircorrectness is virtually guaranteed.
As for at-tributes extracted automatically, previous compar-isons indicate that attributes tend to have higherquality when extracted from queries instead ofdocuments (Pas?ca, 2007).
Indeed, a set ofextraction patterns applied to text produces at-tributes whose average precision at rank 50 is 0.44when extracted from documents, vs. 0.63 fromqueries (Pas?ca et al., 2007).
More importantly,previously available or extracted attributes are vir-tually always simple, short noun phrases like nu-tritional value, taste or solubility in water.
Even ifnot confined to noun phrases, they are still short,391Run: [Ranked Attributes for a Sample of Classes]Class: Automobiles:D: [(it) goes on sale, (it) will go on sale, (it) is an en-gineering playground, (it) will be available in japan, (it)shows up in japan, (it) is a technical tour de force, (it) un-veiled at tas 2008, (it) runs a 7:38, (it) is a unique car, (it)uses a premium midship package, (it) features an all-new3.8-litre, (it) is one of the fastest cars, (it) made a quickdrive-by, ..]Q: [price/quantity/degree (it) weights, year (it) wasbanned from bathurst, manner (it) launch controlworks, engine (it) has, kind of engine (it) has,price/quantity/degree (it) costs in japan, number of horse-power (it) has, price/quantity/degree horsepower (it) has,number of seats (it) has, speed (it) goes, who designed(it), ..]Class: Mobile phones:D: [(it) was announced on september 17 2008, (it) ceasedwith version, (it) was scheduled to be released in late2010, (it) also supports qt (toolkit), (it) supports hardwarecapable, (it) can synchronize with microsoft outlook, (it)also supports python (programming language), ..]Q: [date/time (it) came out in australia, who carries (it),reason (it) keeps rebooting, colours (it) comes in, videoformat (it) supports, date/time (it) was released, date/time(it) came out in the uk, length/duration (it)?s battery lasts,who sells (it), how much (it) costs, ..]Class: Mountains:D: [(it) is an active volcano, (it) is in the distance, (it)is the highest peak in cascade range, (it) is 14,410 feet,(it) was established in 1899, (it) comes into view, (it) wasestablished as a national park, ..]Q: [date/time (it) last erupted, manner (it) erupted in 1882,manner (it) formed, date/time (it) first became active,manner (it) got its name, number of eruptions (it) had,type of magma (it) has, reason (it) became a national park,kind of animals (it) has, ..]Table 6: Top relations extracted for a sample oftarget classes via open-domain relations from doc-uments (D) or via attributes from queries (Q)like vegan, healthy or gluten free (Van Durme etal., 2008; Pas?ca, 2012).
In comparison, attributesextracted in this paper accommodate propertiesthat are sometimes awkward or even impossibleto express through short phrases.Noncontiguous Attributes as Relations: Non-contiguous attributes extracted from fact-seekingqueries are embodiments of relations linking theinstances mentioned in the queries, on one hand,and the values being requested by the queries, onthe other hand.
Therefore, the method proposed inthis paper can also be regarded as a method for theacquisition of relevant relations of various classes.The extracted relations specify the left argument(i.e., the instance) and the linking relation name(i.e., the attribute).
They only specify the typeof the, but not the actual, right argument (i.e., thevalue being requested).An additional experiment compares the accu-racy of relations extracted as noncontiguous at-tributes from queries, vs. relations extracted by aprevious open-domain method (Fader et al., 2011)from 500 million Web documents.
The previousmethod, including its extraction patterns and itsranking scheme, is designed with instances ratherthan classes in mind.
For fairness to the methodin (Fader et al., 2011), the evaluation procedureis slightly adjusted.
The set of instances associ-ated with each target class, over which the twomethods are evaluated, is reduced to a single repre-sentative instance selected a-priori.
The instancesare shown as the first instances in parentheses foreach class in the earlier Table 2.
Thus, the classattributes are extracted using only the instanceskeanu reeves, boeing 737 and bugs bunny in thecase of the classes Actors, Aircraft and Animatedcharacters respectively.Table 6 suggests that noncontiguous attributesextracted from queries tend to capture higher-quality relations than arbitrary relations extractedfrom documents.
Because fact-seeking queries in-quire about the value of some relations (attributes)of an instance, the relations themselves tends tobe more relevant than relations extracted from ar-bitrary document sentences.
Nevertheless, rela-tions derived from queries likely serve as a usefulcomplement, rather than replacement, of relationsfrom documents.
The former only discover whatrelations may be relevant; the latter also identifytheir occurrences within text.5 Related WorkSources of text from which relations (Zhu etal., 2009; Carlson et al., 2010; Lao et al.,2011) and, more specifically, attributes can beextracted include Web documents and data inhuman-compiled encyclopedia.
In Web docu-ments, attributes are available within unstruc-tured (Tokunaga et al., 2005; Pas?ca et al., 2007),structured (Raju et al., 2008) and semi-structuredtext (Yoshinaga and Torisawa, 2007), layout for-matting tags (Wong et al., 2008), itemized lists ortables (Cafarella et al., 2008).
In human-compiledencyclopedia (Wu and Weld, 2010), data relevantto attribute extraction includes infoboxes and cat-egory labels (Nastase and Strube, 2008; Hoffartet al., 2013) associated with Wikipedia articles.In order to acquire class attributes, a commonstrategy is to first acquire attributes of instances,then aggregate or propagate (Talukdar and Pereira,3922010) attributes, from instances to the classes towhich the instances belong.
The role of Websearch queries, as an alternative textual data sourceto Web documents in open-domain informationextraction, has been investigated in the tasks of at-tribute extraction (Pas?ca, 2007; Pas?ca, 2012), aswell as in collecting sets of related instances (Jainand Pennacchiotti, 2010).To increase diversity within a ranked list of at-tributes, the extraction method in this paper em-ploys a synonym vocabulary to approximatelyidentify groups of near-duplicate attributes.
Asreported for previous methods, the resulting listsmay still contain lexically different but semanti-cally equivalent attributes.
Scenarios where de-tecting all equivalent attributes is important maybenefit from other techniques for paraphrase ac-quisition (Madnani and Dorr, 2010).Sophisticated techniques are sometimes em-ployed to identify the type of the expected an-swers of open-domain questions (Pinchak et al.,2009).
In comparison, the loose typing of thevalues of our noncontiguous attributes is mostlycoarse-grained.
It relies on wh-prefixes (when,how long, where, how) and possibly subsequentwords (what nutritional value) from the queries,to determine whether the values are expected tobe a date/time, length/duration, location, manner,nutritional value etc.Relations extracted from document sentences(e.g., ?Claude Monet was born in Paris?)
are tu-ples of an instance (claude monet), a text fragmentacting as the lexicalized relation (was born in), andanother instance (paris) (cf.
(Fader et al., 2011;Mausam et al., 2012)).
For convenience, the re-lation and second instance may be concatenated,as in was born in paris for claude monet.
Butdocument sentences mentioning an instance do notnecessarily refer to properties of the instance thatpeople other than the author of the document arelikely to inquire about.
Consequently, even top-ranked extracted relations occasionally includeless informative ones, such as comes into view formount rainier, is on the table for madeira wine,or allows for features for javascript (Fader et al.,2011).
Comparatively, relations extracted via non-contiguous attributes from queries tend to refer toproperties that have values that Web users inquireabout in their search queries.
Therefore, the rela-tions extracted from queries are more likely to re-fer to salient properties, such as date/time (it) hadits last eruption for mount rainier; length/duration(it) lasts for madeira wine; and manner (it) storesdate information for javascript.6 ConclusionBy requesting values for attributes of individualinstances, fact-seeking and explanation-seekingqueries implicitly assert the relevance of the prop-erties encoded by the attributes, for the respec-tive instances and their classes.
The extracted at-tributes are not required to take the form of con-tiguous short phrases in the source queries, thusallowing for the acquisition of a broader range ofattributes than those extracted by previous meth-ods.
Furthermore, since Web users are interestedin their values, the relations to which the ex-tracted attributes refer tend to be more relevantthan relations extracted from arbitrary documentsusing previous methods.
Current work exploresthe role of distributional similarities in expandingextracted attributes for narrow classes; and the ex-traction of noncontiguous attributes and relationsfrom natural-language queries without a wh-prefix(e.g., cars driven by james bond).AcknowledgmentsThe author would like to thank Efrat Farkash andMichael Kleyman for assistance with the synonymrepository.ReferencesK.
Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.2008.
Freebase: A collaboratively created graph databasefor structuring human knowledge.
In Proceedings of the2008 International Conference on Management of Data(SIGMOD-08), pages 1247?1250, Vancouver, Canada.L.
Brown, T. Cai, and A. DasGupta.
2001.
Interval es-timation for a binomial proportion.
Statistical Science,16(2):101?117.M.
Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.2008.
WebTables: Exploring the power of tables on theWeb.
In Proceedings of the 34th Conference on VeryLarge Data Bases (VLDB-08), pages 538?549, Auckland,New Zealand.A.
Carlson, J. Betteridge, R. Wang, E. Hruschka, andT.
Mitchell.
2010.
Coupled semi-supervised learning forinformation extraction.
In Proceedings of the 3rd ACMConference on Web Search and Data Mining (WSDM-10),pages 101?110, New York.S.
Cucerzan.
2007.
Large-scale named entity disambigua-tion based on Wikipedia data.
In Proceedings of the 2007Conference on Empirical Methods in Natural LanguageProcessing (EMNLP-07), pages 708?716, Prague, CzechRepublic.393S.
Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002.Web question answering: Is more always better?
In Pro-ceedings of the 24th ACM Conference on Research andDevelopment in Information Retrieval (SIGIR-02), pages207?214, Tampere, Finland.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identifyingrelations for open information extraction.
In Proceedingsof the 2011 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-11), pages 1535?1545,Edinburgh, Scotland.C.
Fellbaum, editor.
1998.
WordNet: An Electronic LexicalDatabase and Some of its Applications.
MIT Press.J.
Hoffart, F. Suchanek, K. Berberich, and G. Weikum.
2013.YAGO2: a spatially and temporally enhanced knowledgebase from Wikipedia.
Artificial Intelligence Journal.
Spe-cial Issue on Artificial Intelligence, Wikipedia and Semi-Structured Resources, 194:28?61.A.
Jain and M. Pennacchiotti.
2010.
Open entity extrac-tion from Web search query logs.
In Proceedings of the23rd International Conference on Computational Linguis-tics (COLING-10), pages 510?518, Beijing, China.N.
Lao, T. Mitchell, and W. Cohen.
2011.
Random walk in-ference and learning in a large scale knowledge base.
InProceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-11), pages529?539, Edinburgh, Scotland.N.
Madnani and B. Dorr.
2010.
Generating phrasal andsentential paraphrases: a survey of data-driven methods.Computational Linguistics, 36(3):341?387.Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni.2012.
Open language learning for information extraction.In Proceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL-12),pages 523?534, Jeju Island, Korea.V.
Nastase and M. Strube.
2008.
Decoding Wikipedia cat-egories for knowledge acquisition.
In Proceedings ofthe 23rd National Conference on Artificial Intelligence(AAAI-08), pages 1219?1224, Chicago, Illinois.M.
Pas?ca, B.
Van Durme, and N. Garera.
2007.
The role ofdocuments vs. queries in extracting class attributes fromtext.
In Proceedings of the 16th International Conferenceon Information and Knowledge Management (CIKM-07),pages 485?494, Lisbon, Portugal.M.
Pas?ca.
2007.
Organizing and searching the World WideWeb of facts - step two: Harnessing the wisdom of thecrowds.
In Proceedings of the 16th World Wide Web Con-ference (WWW-07), pages 101?110, Banff, Canada.M.
Pas?ca.
2012.
Attribute extraction from conjecturalqueries.
In Proceedings of the 24th International Confer-ence on Computational Linguistics (COLING-12), Mum-bai, India.P.
Pantel, T. Lin, and M. Gamon.
2012.
Mining entity typesfrom query logs via user intent modeling.
In Proceedingsof the 50th Annual Meeting of the Association for Compu-tational Linguistics (ACL-12), pages 563?571, Jeju Island,Korea.S.
Petrov, P. Chang, M. Ringgaard, and H. Alshawi.
2010.Uptraining for accurate deterministic question parsing.
InProceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-10), pages705?713, Cambridge, Massachusetts.C.
Pinchak, D. Lin, and D. Rafiei.
2009.
Flexible answertyping with discriminative preference ranking.
In Pro-ceedings of the 12th Conference of the European Chapterof the Association for Computational Linguistics (EACL-09), pages 666?674, Athens, Greece.S.
Raju, P. Pingali, and V. Varma.
2008.
An unsupervised ap-proach to product attribute extraction.
In Proceedings ofthe 31st International Conference on Research and Devel-opment in Information Retrieval (SIGIR-08), pages 35?42,Singapore.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguation toWikipedia.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics (ACL-11),pages 1375?1384, Portland, Oregon.M.
Remy.
2002.
Wikipedia: The free encyclopedia.
OnlineInformation Review, 26(6):434.P.
Talukdar and F. Pereira.
2010.
Experiments in graph-basedsemi-supervised learning methods for class-instance ac-quisition.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL-10),pages 1473?1481, Uppsala, Sweden.K.
Tokunaga, J. Kazama, and K. Torisawa.
2005.
Automaticdiscovery of attribute words from Web documents.
InProceedings of the 2nd International Joint Conference onNatural Language Processing (IJCNLP-05), pages 106?118, Jeju Island, Korea.B.
Van Durme, T. Qian, and L. Schubert.
2008.
Class-driven attribute extraction.
In Proceedings of the 22ndInternational Conference on Computational Linguistics(COLING-08), pages 921?928, Manchester, United King-dom.T.
Wong, W. Lam, and T. Wong.
2008.
An unsuper-vised framework for extracting and normalizing productattributes from multiple Web sites.
In Proceedings of the31st International Conference on Research and Develop-ment in Information Retrieval (SIGIR-08), pages 35?42,Singapore.F.
Wu and D. Weld.
2010.
Open information extraction usingWikipedia.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL-10),pages 118?127, Uppsala, Sweden.N.
Yoshinaga and K. Torisawa.
2007.
Open-domainattribute-value acquisition from semi-structured texts.
InProceedings of the 6th International Semantic Web Con-ference (ISWC-07), Workshop on Text to Knowledge: TheLexicon/Ontology Interface (OntoLex-2007), pages 55?66, Busan, South Korea.J.
Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen.
2009.
Stat-Snowball: a statistical approach to extracting entity rela-tionships.
In Proceedings of the 18th World Wide WebConference (WWW-09), pages 101?110, Madrid, Spain.394
