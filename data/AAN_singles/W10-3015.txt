Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 106?113,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsExploiting Multi-Features to Detect Hedges and Their Scope inBiomedical TextsHuiwei Zhou1, Xiaoyan Li2, Degen Huang3, Zezhong Li4, Yuansheng Yang5Dalian University of TechnologyDalian, Liaoning, China{1zhouhuiwei, 3huangdg, 5yangys}@dlut.edu.cn2lixiaoyan@mail.dlut.edu.cn4lizezhonglaile@163.comAbstractIn this paper, we present a machine learningapproach that detects hedge cues and theirscope in biomedical texts.
Identifying hedgedinformation in texts is a kind of semanticfiltering of texts and it is important since itcould extract speculative information fromfactual information.
In order to deal with thesemantic analysis problem, various evidentialfeatures are proposed and integrated through aConditional Random Fields (CRFs) model.Hedge cues that appear in the training datasetare regarded as keywords and employed as animportant feature in hedge cue identificationsystem.
For the scope finding, we construct aCRF-based system and a syntacticpattern-based system, and compare theirperformances.
Experiments using test datafrom CoNLL-2010 shared task show that ourproposed method is robust.
F-score of thebiological hedge detection task and scopefinding task achieves 86.32% and 54.18% inin-domain evaluations respectively.1.
IntroductionIdentifying sentences in natural language textswhich contain unreliable or uncertain informationis an increasingly important task of informationextraction since the extracted information thatfalls in the scope of hedge cues cannot bepresented as factual information.
Szarvas et al(2008) report that 17.69% of the sentences in theabstracts section of the BioScope corpus and22.29% of the sentences in the full papers sectioncontain hedge cues.
Light et al (2004) estimatethat 11% of sentences in MEDLINE abstractscontain speculative fragments.
Szarvas (2008)reports that 32.41% of gene names mentioned inthe hedge classification dataset described inMedlock and Briscoe (2007) appear in aspeculative sentence.
Many Wikipedia articlescontain a specific weasel tag which marksentences as non-factual (Ganter and Strube,2009).There are some Natural Language Processing(NLP) researches that demonstrate the benefit ofhedge detection experimentally in severalsubjects, such as the ICD-9-CM coding ofradiology reports and gene named EntityExtraction (Szarvas, 2008), question answeringsystems (Riloff et al, 2003), informationextraction from biomedical texts (Medlock andBriscoe, 2007).The CoNLL-2010 Shared Task (Farkas et al,2010) ?Learning to detect hedges and their scopein natural language text?
proposed two tasksrelated to speculation research.
Task 1 aimed toidentify sentences containing uncertainty andTask 2 aimed to resolve the in-sentence scope ofhedge cues.
We participated in both tasks.In this paper, a machine learning system isconstructed to detect sentences in texts whichcontain uncertain or unreliable information and tofind the scope of hedge cues.
The system worksin two phases: in the first phase uncertainsentences are detected, and in the second phasein-sentence scopes of hedge cues are found.
In theuncertain information detecting phase, hedgecues play an important role.
The sentences thatcontain at least one hedge cue are considered asuncertain, while sentences without cues areconsidered as factual.
Therefore, the task ofuncertain information detection can be convertedinto the task of hedge cue identification.
Hedgecues that appear in the training dataset arecollected and used as keywords to find hedges.Furthermore, the detected keywords areemployed as an important feature in hedge cueidentification system.
In addition to keywords,various evidential features are proposed andintegrated through a machine learning model.Finding the scope of a hedge cue is to determineat sentence level which words are affected by the106hedge cue.
In the scope finding phase, weconstruct a machine learning-based system and asyntactic pattern-based system, and compare theirperformances.For the learning algorithm, Conditional randomfields (CRFs) is adopted relying on its flexiblefeature designs and good performance insequence labeling problems as described inLafferty et al (2001).
The main idea of CRFs isto estimate a conditional probability distributionover label sequences, rather than over localdirected label sequences as with Hidden MarkovModels (Baum and Petrie, 1966) and MaximumEntropy Markov Models (McCallum et al,2000).Evaluation is carried out on the CoNLL-2010shared task (Farkas et al, 2010) dataset in whichsentences containing uncertain information areannotated.
For the task of detecting uncertaininformation, uncertain cues are annotated.
Andfor the task of finding scopes of hedge cues,hedge cues and their scope are annotated asshown in sentence (a): hedge cue indicate that,and its scope indicate that dhtt is widelyexpressed at low levels during all stages ofDrosophila development are annotated.
(a)Together, these data <xcopeid="X8.74.1"><cue ref="X8.74.1"type="speculation">indicate that</cue> dhttis widely expressed at low levels during allstages of Drosophila development</xcope>.2.
Related WorkIn the past few years, a number of studies onhedge detection from NLP perspective have beenproposed.
Elkin et al (2005) exploitedhandcrafted rule-based negation/uncertaintydetection modules to detect the negation oruncertainty information.
However, their detectionmodules were hard to develop due to the lack ofstandard corpora that used for evaluating theautomatic detection and scope resolution.
Szarvaset al (2008) constructed a corpus annotated fornegations, speculations and their linguistic scopes.It provides a common resource for the training,testing and comparison of biomedical NLPsystems.Medlock and Briscoe (2007) proposed anautomatic classification of hedging in biomedicaltexts using weakly supervised machine learning.They started with a very limited amount ofannotator-labeled seed data.
Then they iteratedand acquired more training seeds without muchmanual intervention.
The best classifier usingtheir model achieved 0.76 precision/recallbreak-even-point (BEP).
Further, Medlock(2008) illuminated the hedge identification taskincluding annotation guidelines, theoreticalanalysis and discussion.
He argued for separationof the acquisition and classification phases insemi-supervised machine learning method andpresented a probabilistic acquisition model.
Inprobabilistic model he assumed bigrams andsingle terms as features based on the intuition thatmany hedge cues are bigrams and single termsand achieves a peak performance of around 0.82BEP.Morante and Daelemans (2009) presented ameta-learning system that finds the scope ofhedge cues in biomedical texts.
The systemworked in two phases: in the first phase hedgecues are identified, and in the second phase thefull scopes of these hedge cues are found.
Theperformance of the system is tested on threesubcorpora of the BioScope corpus.
In the hedgefinding phase, the system achieves an F-score of84.77% in the abstracts subcorpus.
In the scopefinding phase, the system with predicted hedgecues achieves an F-score of 78.54% in theabstracts subcorpus.The research on detecting uncertaininformation is not restricted to analyzebiomedical documents.
Ganter and Strube (2009)investigated Wikipedia as a source of trainingdata for the automatic hedge detection using wordfrequency measures and syntactic patterns.
Theyshowed that the syntactic patterns worked betterwhen using the manually annotated test data,word frequency and distance to the weasel tagwas sufficient when using Wikipedia weasel tagsthemselves.3.
Identifying Hedge CuesPrevious studies (Light et al, 2004) showed thatthe detection of hedging could be solvedeffectively by looking for specific keywordswhich were useful for deciding whether asentence was speculative.
Szarvas (2008) reducesthe number of keyword candidates withoutexcluding helpful keywords for hedgeclassification.
Here we also use a simplekeyword-based hedge cue detection method.3.1 Keyword-based Hedge Cue DetectionIn order to recall as many hedge cues as possible,107all hedge cues that appear in the training datasetare used as keywords.
Hedge cues are representedby one or more tokens.
The list of all hedge cuesin the training dataset is comprised of 143 cues.90 hedge cues are unigrams, 24 hedge cues arebigrams, and the others are trigrams, four-gramsand five-grams.
Besides, hedge cues that appearin the training dataset and their synonyms inWordNet 1  are also selected as keywords forhedge cue detection.
The complete list of themcontains 438 keywords, 359 of which areunigrams.
Many tokens appear in different gramscues, such as possibility appears in five-gramscue cannot rule out the possibility, four-gram cuecannot exclude the possibility, trigrams cue raisethe possibility and unigram cue possibility.
Tofind the complete cues, keywords are matchedthrough a maximum matching method (MM) (Liuet al, 1994).
For example, though indicate andindicate that are both in keywords list, indicatethat is extracted as a keyword in sentence (a)through MM.3.2 CRF-based Hedge Cue DetectionCandidate cues are extracted based on keywordslist in keyword-based hedge cue detection stage.But the hedge cue is extremely ambiguous, soCRFs are applied to correct the falseidentification results that occurred in thekeyword-based hedge cue detection stage.
Theextracted hedge cues are used as one feature forCRFs-based hedge cue detection.A CRF identifying model is generated byapplying a CRF tool to hedge cue labeledsequences.
Firstly, hedge cue labeled sentencesare transformed into a set of tokenized wordsequences with IOB2 labels:B-cue Current token is the beginning of ahedge cueI-cue Current token is inside of  a hedge cueO Current token is outside of any hedgecueFor sentence (a) the system assigns the B-cuetag to indicate, the I-cue tag to that and the O tagto the rest of tokens as shown in Figure1.The hedge cues that are found bykeyword-based method is also given IOB2 labelsfeature as shown in Figure1.1Available at http://wordnet.princeton.edu/Text?thesedataindicatethatdhttis...Keyword Labels Feature...OOBIOO...Cue Labels...OOB-cueI-cueOO...Figure 1: Example of Cues labels and Keywordslabels FeatureDiverse features including keyword feature areemployed to our CRF-based hedge cue detectionsystem.
(1) Word Features?
Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)Where Word (0) is the current word, Word (-1)is the first word to the left, Word (1) is the firstword to the right, etc.
(2) Stem FeaturesThe motivation for stemming in hedgeidentification is that distinct morphological formsof hedge cues are used to convey the samesemantics (Medlock, 2008).
In our method,GENIA Tagger2 (Tsuruoka et al, 2005) is appliedto get stem features.?
Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)where Stem (0) is the stem for the current word,Stem(-1) is the first stem to the left, Stem (1) is thefirst stem to the right, etc.
(3) Part-Of-Speech FeaturesSince most of hedge cues in the training datasetare verbs, auxiliaries, adjectives and adverbs.Therefore, Part-of-Speech (POS) may provideuseful evidence about the hedge cues and theirboundaries.
GENIA Tagger is also used togenerate this feature.?
POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)where POS (0) is the current POS, POS (-1) isthe first POS to the left, POS (1) is the first POSto the right, etc.
(4) Chunk FeaturesSome hedge cues are chunks consisting of morethan one token.
Chunk features may contribute tothe hedge cue boundaries.
We use GENIATagger to get chunk features for each token.
The2Available athttp://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/108chunk features include unigram, bigram, andtrigram types, listed as follows:?
Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)?
Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2)?
Chunk (i?2) + Chunk (i?1)+Chunk (i) (i=0,+1,+2)where Chunk (0) is the chunk label for thecurrent word, Chunk (?1) is the chunk label forthe first word to the left , Chunk (1) is the chunklabel for the first word to the right, etc.
(5) Keyword FeaturesKeyword labels feature is an important feature.?
Keyword (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?,+n)where Keyword (0) is the current keyword label,Keywords (-1) is the keyword label for the firstkeyword to the left, Keywords (1) is the keywordlabel for the first keyword to the right, etc.Feature sets can be easily redefined bychanging the window size n. The relationship ofthe window size and the F-score observed in ourexperiments will be reported in Section 5.4.
Hedge Scope FindingIn this task, a CRFs classifier is applied to predictfor all the tokens in the sentence whether a tokenis the first token of the scope sequence (F-scope),the last token of the scope sequence (L-scope), orneither (None).
For sentence (a) in Section 1, theclassifier assigns F-scope to indicate, L-scope tobenchmarks, and None to the rest of the tokens.Only sentences that assigned cues in the firstphase are selected for hedge scope finding.Besides, a syntactic pattern-based system isconstructed, and compared with the CRF-basedsystem.4.1 CRF-based SystemThe features that used in CRF-based hedge cuedetection systems are also used for scope findingexcept for the keyword features.
The features are:(1) Word Features?
Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)(2) Stem Features?
Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)(3) Part-Of-Speech Features?
POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)(4) Chunk FeaturesThe chunk features include unigram, bigram,and trigram types, listed as follows:?
Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n)?
Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2)?
Chunk (i?2) + Chunk (i?1)+Chunk (i) (i=0,+1,+2)(5) Hedge cues FeaturesHedge cues labels that are doped out in Task 1are selected as an important feature.?
Hedge cues (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?,+n)where Hedge cues (0) is the cue label for thecurrent word, Hedge cues (?1) is the cue label forthe first word to the left , Hedge cues (1) is thecue label for the first word to the right, etc.The scope of the sequence must be consistentwith the hedge cues.
That means that the numberof the F-scope and L-scope must be the same withthe hedge cues.
However, sometimes theirnumber predicted by classifier is not same.Therefore, we need to process the output of theclassifier to get the complete sequence of thescope.
The following post processing rules areadapted.?
If the number of F-scope, L-scope and hedgecue is the same, the sequence will start at thetoken predicted as F-scope, and end at thetoken predicted as L-scope.?
If one token has been predicted as F-scopeand none has been predicted as L-scope, thesequence will start at the token predicted asF-scope and end at the end of the sentence.Since when marking the scopes of keywords,linguists always extend the scope to the biggestsyntactic unit possible.?
If one token has been predicted as L-scopeand none has been predicted as F-scope, thesequence will start at the hedge cue and end atthe token predicted as L-scope.
Since scopesmust contain their cues.?
If one token has been predicted as F-scopeand more than one has been predicted asL-scope, the sequence will end at the first tokenpredicted as L-scope.
Statistics from predictionon CoNLL-2010 Shared Task evaluation datashow that 20 sentences are in this case.
And thescope of 6 sentences extends to the firstL-scope, and the scope of 3 sentences end atthe last L-scope, the others are predictedmistakenly.
Our system prediction andgold-standard annotation are shown in sentence(b1) and (b2) respectively.109(b1) our system annotation:dRas85DV12 <xcope id="X3.64.1"><cueref="X3.64.1" type="speculation">may</cue>be more potent than dEGFR?</xcope> becausedRas85DV12 can activate endogenous PI3Ksignaling</xcope> [16].
(b2) gold-standard annotation:dRas85DV12 <xcope id="X3.64.1"><cueref="X3.64.1"type="speculation">may</cue> be morepotent than dEGFR?</xcope> becausedRas85DV12 can activate endogenous PI3Ksignaling [16].?
If one token has been predicted as L-scopeand more than one has been predicted asF-scope, the sequence will start at the firsttoken predicted as F-scope.?
If an L-scope is predicted before an F-scope,the sequence will start at the token predicted asF-scope, and finished at the end of the sentence.4.2 Syntactic Pattern-based SystemHedge scopes usually can be determined on thebasis of syntactic patterns dependent on the cue.Therefore, a syntactic pattern-based system isalso implemented for hedge scope finding.
Whenthe sentence is predicted as uncertain, the toolkitof Stanford Parser3 (Klein and Manning, 2003) isutilized to parse the sentence into a syntactic tree,which can release a lot of information about thegrammatical structure of sentences that isbeneficial for the finding of hedge scope.
Forsentence (c) the Stanford Parser gives thesyntactic tree as showed in Figure 2.
(c) This <xcope id="X*.*.
*"><cue ref="X*.*.
*"type="speculation"> may </cue> represent aviral illness</xcope>.It is obvious to see from the syntactic tree, allthe words of the parsed sentence concentrate atthe places of leaves.
We use the following rules tofind the scope.?
If the tag of the word is ?B-cue?, it is predictedas F-scope.?
If the POS of the hedge cue is verbs andauxiliaries, the L-scope is signed at the end of theclause.?
If the POS of the hedge cue is attributive3Available athttp://nlp.stanford.edu/software/lex-parser.shtmladjectives, the L-scope is signed at the followingnoun phrase.?
If the POS of the hedge cue is prepositions, theL-scope is signed at the following noun phrase.?
If none of the above rules apply, the scope of ahedge cue starts with the hedge cue and ends atthe following clause.Figure 2: Syntactic tree parsed by StanfordParser5.
Experiments and DiscussionWe evaluate our method using CoNLL-2010shared task dataset.
The evaluation of uncertaininformation detection task is carried out using thesentence-level F-score of the uncertainty class.As mentioned in Section 1, Task 1 is convertedinto the task of hedge cues identification.Sentences can be classified as certain or uncertainaccording to the presence or absence of a fewhedge cues within the sentences.
In task offinding in-sentence scopes of hedge cues, a scopeis correct if all the tokens in the sentence havebeen assigned the correct scope class for aspecific hedge signal.5.1 Detecting Uncertain InformationIn the CoNLL-2010 Shared Task 1, ourin-domain system obtained the F-score of 85.77%.Sentence-level results of in-domain systemsunder the condition n=3 (window size) aresummarized in Table 1.System Prec.
Recall F-scoreKeyword-based 41.15 99.24 58.18CRF-based system(without keywordfeatures)88.66 80.13 84.18CRF-based system+ keyword features86.21 84.68 85.44CRF-based system 86.49 85.06 85.77110+ keyword features+ MMTable 1: Official in-domain results for Task 1(n=3)The keyword-based system extracts hedge cuesthrough maximum matching method (MM).
Ascan be seen in Table 1, the system achieves a highrecall (99.24%).
This can be explained thatalmost all of the hedge cues in the test dataset arein the keywords list.
However, it also bringsabout the low precision since not all potentialspeculative keywords convey real speculation.
Sothe keyword-based method can be combined withour CRF-based method to get better performance.All the CRF-based systems in Table 1significantly outperform the keyword-basedsystem, since the multi-features achieve a highprecision.
And the result with keyword features isbetter than the result without it.
The keywordfeatures improve the performance by recalling 39true positives.
In addition, further improvement isachieved by using Maximum Matching method(MM).In the test dataset, there should be a few hedgecues not in the training dataset.
And theadditional resources besides the manually labeleddata are allowed for in-domain predictions.Therefore, the synonyms of the keywords can beused for in-domain systems.
The synonyms of thekeywords are added to the keywords list, and areexpected to improve detecting performance.
Thesynonyms are obtained from WordNet.Table 2 shows the relationship between thewindow size and the sentence-level results.
Thistable shows the results with and withoutsynonyms.
Generally, the results with synonymsare better than the results without them.
Withrespect to window size, the wider the windowsize, the better precision can be achieved.However, large window size leads to low recallwhich is probably because of data sparse.
Thebest F-score 86.32 is obtained when the windowsize is +/-4.WindowsizeSynonymsPrec.
Recall F-scorewithoutsynonyms85.27 86.46 85.86 1withsynonyms85.66 86.20 85.93withoutsynonyms86.35 85.70 86.02 2with 86.14 84.94 85.53withoutsynonyms86.49 85.06 85.77 3withsynonyms86.69 84.94 85.81withoutsynonyms86.34 84.81 85.57 4withsynonyms87.21 85.44 86.32Table 2: Sentence-level results relative tosynonyms and window size for speculationdetection5.2 Finding Hedge ScopeIn the CoNLL-2010 Shared Task 2, ourin-domain system obtained the F-score of 44.42%.Table 3 shows the scope finding results.
Forin-domain scope finding system, we use thehedge cues extracted by the submitted CRF-basedin-domain system (the best result 85.77 in Table1).
The result of the syntactic pattern-basedsystem is not ideal probably due to the syntacticparsing errors and limited annotation rules.System Prec.
Recall F-scoresyntactic pattern-based 44.31 42.59 43.45CRF-based 45.32 43.56 44.42Table 3: Official in-domain results for Task 2Through analyzing the false of our scopefinding system, we found that many of our falsescope were caused by such scope as sentence (d1)shows.
Our CRF-based system signed theL-scope to the end of sentence mistakenly.
Theincorrectly annotation of our system andgold-standard annotation are shown in sentence(d1) and (d2) respectively.
So an additional rule isadded to our CRF-based system to correct theL-scope.
The rule is:?
If one token has been predicted as L-scope,and if the previous token is ?
)?, or ?
]?, theL-scope will be modified just before thepaired token ?(?
or ?[?.
(d1) The incorrectly predicted version:These factors were <cue ref="X1.178.1"type="speculation">presumed</cue> to bepathogenic</xcope> (85).
(d2) Gold-standard annotation:These factors were <cue ref="X1.178.1"type="speculation">presumed</cue> to bepathogenic (85) </xcope>.111F-score is reached to 51.83 by combining thisadditional rule with the submitted CRF-basedin-domain system as shown in Table 4.TP FP FN Prec.
Recall F-score525 468 508 52.87 50.82 51.83Table 4: Official in-domain results for Task 2Several best results of Task 1 are exploited toinvestigate the relationship between the windowsize and the scope finding results.
From theresults of Table 5, we can see that the case of n=4gives the best precision, recall and F-score.
Andthe case of n=2 and the case of n=3 based on thesame task 1 system have a very similar score.With respect to the different systems of Task 1, inprinciple, the higher the F-score of Task 1, thebetter the performance of Task 2 can be expected.However, the result is somewhat different fromthe expectation.
The best F-score of Task 2 isobtained under the case F-score (task 1) =86.02.This indicates that it is not certain that Task 2system based on the best Task 1 result gives thebest scope finding performance.F-score(Task 1)WindowsizePrec.
Recall F-score86.32 43254.3252.5952.9051.6950.0550.3452.9851.2951.5986.02 43254.8553.1353.1352.5750.9250.9253.6852.0052.0085.86 43254.1952.5052.5052.5750.9250.9253.3751.7051.70Table 5: Scope finding results relative to theresults of task 1 and window sizeIn the case that scopes longer than n (windowsize) words, the relevant cue will thus not fall intothe +/-n word window of the L-scope and allhedge cue features will be O tag.
The hedge cuefeatures will be useless for detecting L-scopes.Taking into account the importance of hedge cuefeatures, the following additional features arealso incorporated to capture hedge cue features.?
Distance to the closest preceding hedge cue?
Distance to the closest following hedge cue?
Stem of the closest preceding hedge cue?
Stem of the closest following hedge cue?
POS of the closest preceding hedge cue?
POS of the closest following hedge cueTable 6 shows the results when the additionalhedge cue features are used.
The results withadditional hedge cue feature set are constantlybetter than the results without them.
In most ofcases, the improvement is significant.
The bestF-score 54.18% is achieved under the caseF-score (task 1) =86.02 and n=4.F-score(Task 1)WindowsizePrec.
Recall F-score86.32 43254.7354.2253.4152.0851.6050.8253.3752.8852.0886.02 43255.3554.7553.9453.0552.4751.6954.1853.5852.7985.86 43254.4953.7953.0952.8652.1851.5053.6652.9752.29Table 6: Scope finding results relative to theresults of Task 1 and window size with additionalcue featuresThe upper-bound results of CRF-based systemassuming gold-standard annotation of hedge cuesare show in Table 7.TP FP FN Prec.
Recall F-score618 427 415 59.14 59.83 59.48Table 7: Scope finding result with gold-standardhedge signalsA comparative character analysis of syntacticpattern-based method and CRF-based methodwill be interesting, which can provide insightsleading to better methods in the future.6.
ConclusionIn this paper, we have exploited various usefulfeatures evident to detect hedge cues and theirscope in biomedical texts.
For hedge detectiontask, keyword-based system is integrated withCRF-based system by introducing keywordfeatures to CRF-based system.
Our experimentalresults show that the proposed method improvesthe performance of CRF-based system by theadditional keyword features.
Our system hasachieved a state of the art F-score 86.32% on thesentence-level evaluation.
For scope finding task,112two different systems are established: CRF-basedand syntactic pattern-based system.
CRF-basedsystem outperforms syntactic pattern-basedsystem due to its evidential features.In the near future, we will improve the hedgecue detection performance by investigating moreimplicit information of potential keywords.
Onthe other hand, we will study on how to improvescope finding performance by integratingCRF-based and syntactic pattern-based scopefinding systems.ReferencesLeonard E. Baum, and Ted Petrie.
1966.
Statisticalinference for probabilistic functions of finite stateMarkov chains.
Annals of MathematicalStatistics, 37(6):1554?1563.Peter L. Elkin, Steven H. Brown, Brent A. Bauer,Casey S. Husser, William Carruth, Larry R.Bergstrom, and Dietlind L. Wahner-Roedler.
2005.A controlled trial of automated classification ofnegation from clinical notes.
BMC MedicalInformatics and Decision Making, 5(13).Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nosCsirik, and Gy?rgy Szarvas.
2010.
TheCoNLL-2010 Shared Task: Learning to DetectHedges and their Scope in Natural Language Text.In Proceedings of CoNLL-2010: Shared Task,2010, pages 1?12.Viola Ganter, and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingwikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 173?176.Dan Klein, and Christopher D. Manning.
2003.Accurate unlexicalized parsing.
In Proceedings ofthe 41st Meeting of the Association forComputational Linguistics, pages 423?430.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the EighteenthInternational Conference on MachineLearning, pages 282?289.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: facts,speculations, and statements in between.
InHLT-NAACL 2004 Workshop: BioLINK 2004,Linking Biological Literature, Ontologies andDatabases, pages 17?24.Yuan Liu, Qiang Tan, and Kunxu Shen.
1994.
Theword segmentation rules and automatic wordsegmentation methods for Chinese informationprocessing.
QingHua University Press andGuangXi Science and Technology Press.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov modelsfor information extraction and segmentation.
InProceedings of ICML 2000, pages 591?598.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of BiomedicalInformatics, 41(4):636?654.Ben Medlock, and Ted Briscoe.
2007.
Weaklysupervised learning for hedge classification inscientific literature.
In Proceedings of ACL-07,pages 992?999.Roser Morante, and Walter Daelemans.
2009.Learning the scope of hedge cues in biomedicaltexts.
In Proceedings of the Workshop onBioNLP, ACL 2009, pages 28?36.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of the 7thConference on Computational NaturalLanguage Learning, pages 25?32.Gy?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selectionof keywords.
In Proceedings of ACL: HLT, pages281?289.Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas,and J?nos Csirik.
2008.
The BioScope corpus:biomedical texts annotated for uncertainty, negationand their scopes.
In Proceedings of BioNLP 2008:Current Trends in Biomedical NaturalLanguage, pages 38?45.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,Jun?ichi Tsujii.
2005.
Developing a robustpart-of-speech tagger for biomedical text.
InAdvances in Informatics 2005, pages 382?392.113
