Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 28?35,Sydney, July 2006. c?2006 Association for Computational LinguisticsInteractive Question Answering and Constraint Relaxationin Spoken Dialogue SystemsSebastian VargesCSLIStanford UniversityStanford, CA 94305, USAvarges@stanford.eduFuliang Weng, Heather Pon-BarryResearch and Technology CenterRobert Bosch Corporation4009 Miranda Ave, Palo Alto, CA, USAfuliang.weng, heather.pon-barry@rtc.bosch.comAbstractWe explore the relationship between ques-tion answering and constraint relaxation inspoken dialog systems.
We develop dia-logue strategies for selecting and present-ing information succinctly.
In particular,we describe methods for dealing with theresults of database queries in information-seeking dialogs.
Our goal is to structurethe dialogue in such a way that the user isneither overwhelmed with information norleft uncertain as to how to refine the queryfurther.
We present evaluation results ob-tained from a user study involving 20 sub-jects in a restaurant selection task.1 IntroductionInformation presentation is an important issuewhen designing a dialogue system.
This is espe-cially true when the dialogue system is used in ahigh-stress environment, such as driving a vehi-cle, where the user is already occupied with thedriving task.
In this paper, we explore efficientdialogue strategies to address these issues, andpresent implemented knowledge management, di-alogue and generation components that allow cog-nitively overloaded users ?
see (Weng et al, 2004),for example ?
to obtain information from the di-alogue system in a natural way.
We describe aknowledge manager that provides factual and on-tological information, a content optimizer that reg-ulates the amount of information, and a genera-tor that realizes the selected content.
The domaindata is divided between domain-specific ontolo-gies and a database back-end.
We use the systemfor both restaurant selection and MP3 player tasks,and conducted experiments with 20 subjects.There has been substantial previous work oninformation presentation in spoken dialogue sys-tems.
(Qu and Green, 2002) also present aconstraint-based approach to cooperative informa-tion dialogue.
Their experiments focus on over-constrained queries, whereas we also deal with un-derconstrained ones.
Moreover, we guide the userthrough the dialogue by making suggestions aboutquery refinements, which serve a similar ro?le tothe conditional responses of (Kruijff-Korbayova etal., 2002).
(Hardy et al, 2004) describe a dialoguesystem that uses an error-correcting database man-ager for matching caller-provided information todatabase entries.
This allows the system to se-lect the most likely database entry, but, in contrastto our approach, does not modify constraints ata more abstract level.
In contrast to all the ap-proaches mentioned above, our language gener-ator uses overgeneration and ranking techniques(Langkilde, 2000; Varges and Mellish, 2001).This facilitates variation and alignment with theuser utterance.A long-standing strand of research in NLP isin natural language access to databases (Androut-sopoulos et al, 1995).
It mainly focused on map-ping natural language input to database queries.Our work can be seen as an extension of this workby embedding it into a dialogue system and al-lowing the user to refine and relax queries, andto engage in clarification dialogs.
More recently,work on question answering (QA) is moving to-ward interactive question answering that gives theuser a greater role in the QA process (HLT, forth-coming).
QA systems mostly operate on free textwhereas we use a relational database.
(Thus, oneneeds to ?normalize?
the information contained infree text to use our implemented system withoutfurther adaption.
)28In the following section, we give an overviewof the dialogue system.
We then describe theknowledge management, dialogue and generationcomponents in separate sections.
In section 6 wepresent evaluation results obtained from a userstudy.
This is followed by a discussion section andconclusions.2 System architectureOur dialogue system employs the following archi-tecture: the output of a speech recognizer (Nu-ance, using a statistical language model) is ana-lyzed by both a general-purpose statistical depen-dency parser and a (domain-specific) topic classi-fier.
Parse trees and topic labels are matched bythe ?dialogue move scripts?
of the dialogue man-ager (Mirkovic and Cavedon, 2005; Weng et al,2005).
The scripts serve to license the instantia-tion of dialogue moves and their integration intothe ?dialogue move tree.?
The use of dialoguemove scripts is motivated by the need to quicklytailor the system to new domains: only the scriptsneed to be adapted, not the underlying machineryimplemented in Java.
The scripts define short se-quences of dialog moves, for example a commandmove (?play song X?)
may be followed either bya disambiguation question or a confirmation thatthe command will be executed.
A dialogue pro-ceeds by integrating such scripted sequences intothe dialogue move tree, yielding a relatively ?flat?dialogue structure.Query constraints are built by dialogue movescripts if the parse tree matches input patternsspecified in the scripts.
These query constraintsare the starting point for the processing strategiesdescribed in this paper.
The dialogue system isfully implemented and has been used in restau-rant selection and MP3 player tasks.
There are 41task-independent, generic dialogue move scripts,52 restaurant selection scripts and 89 MP3 playerscripts.
The examples in this paper are mostlytaken from the restaurant selection task.3 Knowledge and Content managementThe Knowledge Manager (KM) controls access todomain knowledge that is structured according todomain-dependent ontologies.
The KM makes useof OWL, a W3C standard, to represent the onto-logical relationships between domain entities.
Theknowledge base can be dynamically updated withnew instances at any point.
In a typical interac-tion, the Dialog Manager converts a user?s queryinto a semantic frame (i.e., a set of semantic con-straints) and sends this to the KM via the contentoptimizer.
For example, in the Restaurant domain,a request such as ?I want to find an inexpensiveJapanese restaurant that takes reservations?
resultsin the semantic frame below, where Category is asystem property, and the other constraints are in-herited properties of the Restaurant class:(1) system:Category = restaurant:Restaurantrestaurant:PriceLevel = 0-10restaurant:Cuisine = restaurant:japaneserestaurant:Reservations = yesIn addition to the KM module, we employ aContent Optimization (CO) module that acts asan intermediary between dialogue and knowledgemanagement during the query process.
It receivessemantic frames from the Dialogue Manager, re-vises the semantic frames if necessary (see below),and queries the Knowledge Manager.The content optimizer also resolves remainingambiguities in the interpretation of constraints.For example, if the user requests an unknown cui-sine type, the otherwise often accurate classifierwill not be able to provide a label since it oper-ates under a closed-world assumption.
In contrast,the general purpose parser may be able to pro-vide an accurate syntactic analysis.
However, theparse still needs to be interpreted by the contentoptimizer which has the domain-specific knowl-edge to determine that ?Montenegrin restaurant?is a cuisine constraint rather than a service levelconstraint, for example.
(See also section 7).Depending on the items in the query result set,configurable properties, and (potentially) a usermodel, the CO module selects and performs an ap-propriate optimization strategy.
To increase porta-bility, the module contains a library of domain-independent strategies and makes use of externalconfiguration files to tailor it to specific domains.The CO module can modify constraints de-pending on the number of items in the resultset, the system ontology, and information froma user model.
Constraints can be relaxed, tight-ened, added or removed.
The manner in whicha constraint is modified depends on what kind ofvalues it takes.
For example, for the Cuisineconstraint, values are related hierarchically (e.g.,Chinese, Vietnamese, and Japanese are all sub-types of Asian), whereas PriceLevel values arelinear (e.g., cheap, moderate, expensive), andacceptsCreditCards values are binary (e.g., ac-29cepted or not accepted).If the original query returns no results, the con-tent optimizer selects a constraint to modify andthen attempts to relax the constraint value.
If re-laxation is impossible, it removes the constraintinstead.
Constraint relaxation makes use of theontological relationships in the knowledge base.For example, relaxing a Cuisine constraint entailsreplacing it with its parent-concept in the domainontology.
Relaxing a linear constraint entails re-placing the current value with an adjacent value.Relaxing a binary constraint entails replacing thecurrent value with its opposite value.Based on the ontological structures, the contentoptimizer also calculates statistics for every set ofitems returned by the knowledge manager in re-sponse to a user?s query.
If the result set is large,these figures can be used by the dialogue managerto give meaningful responses (e.g., in the MP3 do-main, ?There are 85 songs.
Do you want to listthem by a genre such as Rock, Pop, or Soul??
).The content optimizer also produces constraintsthat represent meta-knowledge about the ontology,for example, in response to a user input ?Whatcuisines are there??
:(2) rdfs:subClassOf = restaurant:CuisineThe processing modules described in the nextsections can use meta-level constraints in similarways to object-level constraints (see (1)).4 Dialogue strategies for dealing withquery resultsIn the following two sections, we describe howour dialogue and generation strategies tie in withthe choices made by the content optimizer.
Con-sider the following discourse-initial interaction forwhich the semantic frame (1) is constructed:(3)U: i want to find an inexpensive Japaneserestaurant that takes reservationsS: I found 9 inexpensive Japaneserestaurants that take reservations .Here are the first few :S: GINZA JAPANESE RESTAURANTS: OKI SUSHI CAFES: YONA SUSHIS: Should I continue?The example query has a relatively small resultset which can be listed directly.
This is not alwaysthe case, and thus we need dialogue strategies thatdeal with different result set sizes.
For example, itdoes not seem sensible to produce ?I found 2500restaurants.
Here are the first few: ...?.
At whatpoint does it become unhelpful to list items?
Wedo not have a final answer to this question ?
how-ever, it is instructive that the (human) wizard inour data collection experiments did not start list-ing when the result set was larger than about 10items.
In the implemented system, we define di-alogue strategies that are activated at adjustablethresholds.Even if the result set is large and the systemdoes not list any result items, the user may stillwant to see some example items returned for thequery.
This observation is based on commentsby subjects in experimental dry-runs that in somecases it was difficult to obtain any query result atall.
For example, speech recognition errors maymake it difficult to build up a sufficiently complexquery.
In response to this, we always give someexample items even if the result set is large.
(Analternative would be to start listing items after acertain number of dialogue turns.)
Furthermore,the system should encourage the user to refine thequery by suggesting constraints that have not beenused yet.
This is done by maintaining a list of con-straints in the generator that is used up as the di-alogue progresses.
This list is roughly ordered byhow likely the constraint will be useful.
For exam-ple, using cuisine type is suggested before propos-ing to ask for information about reservations orcredit cards.In our architecture, information flows from theCO module to the generator (see section 5) via thedialogue move scripts of the dialogue manager.These are conditioned on the size of the final re-sult set and whether or not any modifications wereperformed.
Table 1 summarizes the main dialoguestrategies.
These dialogue strategies represent im-plicit confirmations and are used if NLU has a highconfidence in its analysis of the user utterance (see(Varges and Purver, 2006) for more details on ourhandling of robustness issues).
Small result setsup to a threshold t1 are listed in a single sentence.For medium-sized result sets up to a threshold t2,the system starts listing immediately.
For large re-sult sets, the generator shows example items andmakes suggestions as to what constraint the usermay use next.
If the CO module performs any con-straint modification, the first, constraint realizingsentence of the system turns reflects the modifica-tion.
(?NP-original?
and ?NP-optimized?
in table 1are used for brevity and are explained in the nextsection.
)30|resultfinal| mod example realization fexps1a 0 no I?m sorry but I found no restaurants on Mayfield Road that serve Mediterranean food.
0s1b 0 yes I?m sorry but I found no [NP-original].
I did not even find any [NP-optimized].
0s2a small: no There are 2 cheap Thai restaurants in Lincoln in my database: Thai Mee Choke and 61> 0, < t1 Noodle House.s2b small yes I found no cheap Greek restaurants that have a formal dress code but there are 04 inexpensive restaurants that serve other Mediterranean food and have a formaldress code in my database: ... .s3a medium: no I found 9 restaurants with a two star rating and a formal dress code that are open 212>= t1, < t2 for dinner and serve French food.
Here are the first ones: ... .s3b medium yes I found no [NP-original].
However, there are N [NP-optimized].
Here are the first few: ... .
5s4a large: no I found 258 restaurants on Page Mill Road, for example Maya Restaurant , 300>= t2 Green Frog and Pho Hoa Restaurant.
Would you like to try searching by cuisine?s4b large yes I found no [NP-original].
However, there are N [NP-optimized].
Would you like to try 16searching by [Constraint]?Table 1: Dialogue strategies for dealing with query results (last column explained in sec.
6)5 GenerationThe generator produces turns that verbalize theconstraints used in the database query.
This isimportant since the system may miss or misinter-pret constraints, leading to uncertainty for the userabout what constraints were used.
For this rea-son, a generic system response such as ?I found 9items.?
is not sufficient.The input to the generator consists of the nameof the dialogue move and the relevant instantiatednodes of the dialogue move tree.
From the in-stantiated move nodes, the generator obtains thedatabase query result including information aboutquery modifications.
The core of the generator isa set of productions1 written in the Java ExpertSystem Shell (Friedman-Hill, 2003).
We followthe bottom-up generation approach for productionsystems described in (Varges, 2005) and performmild overgeneration of candidate moves, followedby ranking.
The highest-ranked candidate is se-lected for output.Productions map individual database con-straints to phrases such as ?open for lunch?,?within 3 miles?
and ?a formal dress code?, andrecursively combine them into NPs.
This includesthe use of coordination to produce ?restaurantswith a 5-star rating and a formal dress code?,for example.
The NPs are integrated into sen-tence templates, several of which can be combinedto form an output candidate turn.
For example,a constraint realizing template ?I found no [NP-1Productions are ?if-then?
rules that operate over a sharedknowledge base of facts.original] but there are [NUM] [NP-optimized] inmy database?
(see below for further explanation)can be combined with a follow-up sentence tem-plate such as ?You could try to look for [NP-constraint-suggestion]?.The selection of which sentence template to useis determined by the dialogue move scripts.
Typ-ically, a move-realizing production produces sev-eral alternative sentences.
On the other hand, theNP generation rules realize constraints regardlessof the specific dialogue move at hand.
This al-lows us to also use them for clarification ques-tions based on constraints constructed from classi-fier information if the parser and associated parse-matching patterns fail; all that is required is a newsentence template, for example ?Are you lookingfor [NP]??.
We currently use 102 productionsoverall in the restaurant and MP3 domains, 38 ofthem to generate NPs that realize 19 possible inputconstraints (for both domains).The decision of the CO module to relax or re-move constraints also affects the generator: thereare two sets of constraints, an ?original?
one di-rectly constructed from the user utterance, and an?optimized?
one used by the KM module to obtainthe query result (see section 3).
In case of con-straint modifications, these two sets are not identi-cal but often overlapping.
To avoid generating sep-arate sets of NPs independently for the two con-straint sets, we assign unique indices to the con-straints and hand the generator two index sets astargets of NP generation.
We overgenerate NPsand check their index sets before integrating theminto sentence templates.31turn speaker utterance1.1 User i need a restaurant with a five star food rating excellent service1.2 System I found 425 restaurants with a five star rating and excellent service , for exampleEL CAPITAN FISH MARKET , K-NINE COUNTRY CLUB , and KAISERHOF GERMANAMERICAN .
You could try to look for cuisine type .1.3 User which restaurants take reservations1.4 System I found 353 restaurants that have a five star rating , have excellent service andtake reservations , for example GRAND CENTRAL CAFE , CHASIN CHICKEN , andCHON WOO RESTAURANT .
Would you like to try searching by price ?Figure 1: Example dialogue 1 from system evaluation using restaurant selection taskRanking of candidate output moves is done byusing a combination of factors.
First, the rankercomputes an alignment score for each candidate,based on its ngram-based overlap with the userutterance.
For example, this allows us to prefer?restaurants that serve Chinese food?
over ?Chi-nese restaurants?
if the user used a wording moresimilar to the first.
We note that the GriceanMaxim of Brevity, applied to NLG in (Dale andReiter, 1995), suggests a preference for the sec-ond, shorter realization.
However, if the userthought it necessary to use ?serves?, maybe toavoid confusion of constraints or even to correct anearlier mislabeling, then the system should makeit clear that it understood the user correctly byusing those same words, thus preferring the firstrealization.
Mild overgeneration combined withalignment also allows us to map the constraintPriceLevel=0-10 in example (1) above to both?cheap?
and ?inexpensive?, and use alignment to?play back?
the original word choice to the user.As these examples show, using alignment for rank-ing in NLG allows one to employ overgenerationtechniques even in situations where no corpus datais available.2Second, ranking uses a variation score to ?cycle?over sentence-level paraphrases.
In the extremecase of repeated identical user inputs, the systemsimply chooses one paraphrase after the other, andstarts over when all paraphrases have been used.Third, we use an ngram filter based on badexamples ngrams, removing, for example, ?Chi-nese cheap restaurants?
but keeping ?cheap Chi-nese restaurant.?
For generalization, we replaceconstraint realizations with semantic tags derivedfrom the constraint names (except for the headnoun), for example the trigram ?CUISINE PRICErestaurants?.
An alternative is to use a more com-2However, we do have wizard-of-oz data to inform thesystem design (see section 7).plex grammar formalism to prevent ungrammati-cal candidate moves.6 EvaluationWe conducted experimental studies involving 20subjects in a MP3 player task and 20 subjects in arestaurant selection task.
In the following, we con-centrate on the restaurant selection task because itis more challenging for constraint handling and in-formation presentation.Each subject in the restaurant selection taskwas given 9 scenario descriptions involving 3 con-straints.
Subjects were instructed to use their ownwords to find a fitting restaurant.
We use a back-end database of 2500 restaurants containing thefollowing information for each restaurant: restau-rant name, cuisine type, city and street names,service level, rating, whether they accept creditcards or reservations, price level, open hours, dresscode, additional information (e.g.
vegetarian food)and distance from current location.
Some of theseconstraints have a fixed number of values (e.g.service level and dress code), whereas those tak-ing named entities as values are obviously open-ended.Figures 1 and 2 show two of the dialogues fromthe experiments.
To exemplify the challengesthe system is faced with, user turns are shownin the form of the output of the speech recog-nizer (we currently use the best ASR hypothesisas input to the parser).
The task of the first dia-logue (figure 1) was to find a restaurant that hasthe following constraints: [Reservations=yes, Ser-viceLevel:excellent, Rating=5star].
Despite someminor speech recognition errors, the system cor-rectly builds a query for the two constraints pro-vided by the user in (1.1.)
and verbalizes these in(1.2) to show its understanding of the user input.It also gives some example items and makes a sug-gestion.
This suggestion is not taken up by the user32turn speaker utterance2.1 User breakfast2.2 System Are you looking for a restaurant that is open for breakfast ?2.3 User yes located on bower street2.4 System I found 71 restaurants on bower street that serve breakfast , for example ,SOUPLANTATION-TO-GO , RAMAYANI WESTWOOD , and , PHILLY WEST .You could try to look for cuisine type .2.5 User which2.6 System I?m sorry, I do not understand what you mean by: which2.7 System Please try rephrasing that.2.8 User casual dress2.9 System There are 51 restaurants on bower street that serve breakfast and have a casualdress code in my database , for example , BERTY?S , LABOTETEGA , and ,JAVAN RESTAURANT .
Would you like to try searching by price ?2.10 User british2.11 System I found no restaurants on bower street that serve british food , have a casualdress code and are open for breakfast but there are 8 restaurants on bower street that are open forbreakfast , serve European food and have a casual dress code in my database .
Here are the first few :2.12 System HOTEL BEL-AIRFigure 2: Example dialogue 2 from system evaluation using restaurant selection task?
in fact, we find that suggestions are generally nottaken up by the user.
We believe this is due to thenature of the tasks, which specified exactly whichcriteria to match.
On the other hand, in more openapplication scenarios, where users may not knowwhat questions can be asked, suggestions may beuseful.
In (1.3) the user issues a sub-query thatfurther constrains the result set.
By again summa-rizing the constraints used, the system confirms in(1.4) that it has interpreted the new constraint as arevision of the previous query.
The alternative isto start a new query, which would be wrong in thiscontext.The task of the second dialogue, figure 2, was tofind a restaurant that meets the constraints [Busi-nessHours:breakfast, StreetName=?bower street?,DressCode=casual].
This user tends to giveshorter, keyword-style input to the system (2.1,2.8).
In (2.3), the user reacts to a clarificationquestion and adds another constraint which thesystem summarizes in (2.4).
(2.5) is an ASR er-ror which the system cannot handle (2.6, 2.7).
Theuser constraint of (2.8) is correctly used to revisethe query (2.9), but ?british?
(2.10) is another ASRerror that leads to a cuisine constraint not intendedin the scenario/by the user.
This additional con-straint yields an empty result set, from which thesystem recovers automatically by relaxing the hi-erarchically organized cuisine constraint to ?Eu-ropean food?.
In (2.11) the system uses dialoguestrategy s3b for medium-sized result sets with con-straint modifications (section 4).
The result ofboth dialogues is that all task constraints are met.We conducted 20 experiments in the restaurantdomain, 2 of which were restarted in the middle.Overall, 180 tasks were performed involving 1144user turns and 1818 system turns.
Two factors con-tributing to the higher number of system turns area) some system turns are counted as two turns,such as 2.6, 2.7 in figure 2, and b) restaurants inlonger enumerations of result items are counted asindividual turns.
On average, user utterances aresignificantly shorter than system utterances (4.9words, standard deviation ?
= 3.82 vs 15,4 words,?
= 13.53).
This is a result of the ?constraint sum-maries?
produced by the generator.
The high stan-dard deviation of the system utterances can be ex-plained by the above-mentioned listing of individ-ual result items (e.g.
utterance (2.12) in figure 2).We collected usage frequencies for the dia-logue strategies presented in section 4: there wasno occurrence of empty final result sets (strat-egy s1a/b) because the system successfully re-laxed constraints if it initially obtained no results.Strategy s2a (small result sets without modifica-tions) was used for 61 inputs, i.e.
constraint setsconstructed from user utterances.
Strategy s3a/b(medium-sized result sets) was used for 217 timesand required constraint relaxations in 5 cases.Strategy s4a/b (large result sets) was used for33316 inputs and required constraint relaxations in16 cases.
Thus, the system performed constraintmodifications in 21 cases overall.
All of theseyielded non-empty final result sets.
For 573 in-puts, no modification was required.
There were noempty final result set despite modifications.On average, the generator produced 16 outputcandidates for inputs of two constraints, 160 can-didates for typical inputs of 3 constraints and 320candidates for 4 constraints.
Such numbers caneasily be handled by simply enumerating candi-dates and selecting the ?best?
one.Task completion in the experiments was high:the subjects met al target constraints in 170 out of180 tasks, i.e.
completion rate was 94.44%.
Anerror analysis revealed that the reasons for onlypartially meeting the task constraints were varied.For example, in one case a rating constraint (?fivestars?)
was interpreted as a service constraint bythe system, which led to an empty result set.
Thesystem recovered from this error by means of con-straint relaxation but the user seems to have beenleft with the impression that there are no restau-rants of the desired kind with a five star rating.7 DiscussionBased on wizard-of-oz data, the system alter-nates specific and unspecific refinement sugges-tions (?You could search by cuisines type?
vs ?Canyou refine your query??).
Furthermore, many ofthe phrases used by the generator are taken fromwizard-of-oz data too.
In other words, the sys-tem, including the generator, is informed by em-pirical data but does not use this data directly (Re-iter and Dale, 2000).
This is in contrast to genera-tion systems such as the ones described in (Langk-ilde, 2000) and (Varges and Mellish, 2001).Considering the fact that the domain ontologyand database schema are known in advance, it istempting to make a closed world assumption inthe generator (which could also help system de-velopment and testing).
However, this seems toorestrictive: assume, for example, that the user hasasked for Montenegrin food, which is an unknowncuisine type, and that the statistical parser com-bined with the parse-matching patterns in the di-alogue manager has labeled this correctly.
Thecontent optimization module will remove this con-straint since there is no Montenegrin restaurant inthe database.
If we now want to generate ?I did notfind any restaurants that serve Montenegrin food...?, we do need to be able to use generation inputthat uses unseen attribute-value pairs.
The priceone has to pay for this increased robustness andflexibility is, of course, potentially bad output ifNLU mislabels input words.
More precisely, wefind that if any one of the interpretation modulesmakes an open-world assumption, the generatorhas to do as well, at least as long as we want toverbalize the output of that module.7.1 Future workOur next application domain will be in-car naviga-tion dialogues.
This will involve dialogues that de-fine target destinations and additional route plan-ning constraints.
It will allow us to explore theeffects of cognitive constraints due to changingdriving situations on dialogue behavior.
The nav-igation domain may also affect the point of inter-action between dialogue system and external de-vices: we may query a database to disambiguateproper names such as street names as soon as theseare mentioned by the user, but start route planningonly when all planning constraints are collected.An option for addressing the current lack of auser model is to extend the work in (Cheng et al,2004).
They select the level of detail to be com-municated to the user by representing the driver?sroute knowledge to avoid repeating known infor-mation.Another avenue of future research is to automat-ically learn constraint relaxation strategies from(appropriately annotated) evaluation data.
Usermodeling could be used to influence the order inwhich refinement suggestions are given and deter-mine the thresholds for the information presenta-tion moves described in section 4.One could handle much larger numbers of gen-eration candidates either by using packing (Langk-ilde, 2000) or by interleaving rule-based genera-tion with corpus-based pruning (Varges and Mel-lish, 2001) if complexity should become an issuewhen doing overgeneration.8 ConclusionsWe described strategies for selecting and present-ing succinct information in spoken dialogue sys-tems.
Verbalizing the constraints used in a query iscrucial for robustness and usability ?
in fact, it canbe regarded as a special case of providing feed-back to the user about what the system has heardand understood (see (Traum, 1994), for example).34The specific strategies we use include ?backing-off?
to more general constraints (by the system)or suggesting query refinements (to be requestedexplicitly by the user).
Our architecture is config-urable and open: it can be parametrized by em-pirically derived values and extended by new con-straint handling techniques and dialogue strate-gies.
Constraint relaxation techniques have widelybeen used before, of course, for example in syn-tactic and semantic processing.
The presented pa-per details how these techniques, when used at thecontent determination level, tie in with dialogueand generation strategies.
Although we focussedon the restaurant selection task, our approach isgeneric and can be applied across domains, pro-vided that the dialogue centers around accessingand selecting potentially large amounts of factualinformation.Acknowledgments This work is supported bythe US government?s NIST Advanced TechnologyProgram.
Collaborating partners are CSLI, RobertBosch Corporation, VW America, and SRI Inter-national.
We thank the many people involved insystem design, development and evaluation, andthe reviewers of this paper.ReferencesIon Androutsopoulos, G.D. Ritchie, and P. Thanisch.1995.
Natural Language Interfaces to Databases ?An Introduction.
Natural Language Engineering,1(1):29?81.Hua Cheng, Lawrence Cavedon, and Robert Dale.2004.
Generating Navigation Information Based onthe Driver?s Route Knowledge.
In Proceedings ofthe Coling 2004 Workshop on Robust and Adap-tive Information Processing for Mobile Speech In-terfaces, pages 31?38, Geneva, Switzerland.Robert Dale and Ehud Reiter.
1995.
ComputationalInterpretations of the Gricean Maxims in the Gener-ation of Referring Expressions.
Cognitive Science,19:233?263.Ernest Friedman-Hill.
2003.
Jess in Action: JavaRule-Based Systems.
Manning Publications.Hilda Hardy, Tomek Strzalkowski, Min Wu, CristianUrsu, Nick Webb, Alan Biermann, R. Bryce Inouye,and Ashley McKenzie.
2004.
Data-driven strategiesfor an automated dialogue system.
In Proceedingsof the 42nd Meeting of the Association for Compu-tational Linguistics (ACL?04), Main Volume, pages71?78, Barcelona, Spain, July.forthcoming.
Proceedings of the workshop on Interac-tive Question Answering at HLT-NAACL 2006.Ivana Kruijff-Korbayova, Elena Karagjosova, and Stef-fan Larsson.
2002.
Enhancing collaboration withconditional responses in information-seeking dia-logues.
In Proc.
of 6th workshop on the semanticsand pragmatics of dialogue (EDILOG-02).Irene Langkilde.
2000.
Forest-based Statistical Sen-tence Generation.
In Proc NAACL-00, pages 170?177.Danilo Mirkovic and Lawrence Cavedon.
2005.
Prac-tical Plug-and-Play Dialogue Management.
In Pro-ceedings of the 6th Meeting of the Pacific Associa-tion for Computational Linguistics (PACLING).Yan Qu and Nancy Green.
2002.
A Constraint-basedApproach for Cooperative Information-Seeking Di-alogue.
In Proceedings of the International Work-shop on Natural Language Generation (INLG-02).Ehud Reiter and Robert Dale.
2000.
Building AppliedNatural Language Generation Systems.
CambridgeUniversity Press, Cambridge, UK.David Traum.
1994.
A Computational Theoryof Grounding in Natural Language Conversation.Ph.D.
thesis, Computer Science Dept., U. Rochester.Sebastian Varges and Chris Mellish.
2001.
Instance-based Natural Language Generation.
In Proc.NAACL-01.Sebastian Varges and Matthew Purver.
2006.
Ro-bust language analysis and generation for spoken di-alogue systems (short paper).
In Proceedings of theECAI 06 Workshop on the Development and Evalu-ation of Robust Spoken Dialogue Systems.Sebastian Varges.
2005.
Chart generation using pro-duction systems (short paper).
In Proc.
of 10th Eu-ropean Workshop On Natural Language Generation.Fuliang Weng, L. Cavedon, B. Raghunathan,D.
Mirkovic, H. Cheng, H. Schmidt, H. Bratt,R.
Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg,and C. Bergmann.
2004.
Developing a conversa-tional dialogue system for cognitively overloadedusers.
In Proceedings of the International Congresson Intelligent Transportation Systems (ICSLP).Fuliang Weng, Lawrence Cavedon, Badri Raghu-nathan, Danilo Mirkovic, Ben Bei, Heather Pon-Barry, Harry Bratt, Hua Cheng, Hauke Schmidt, Ro-hit Mishra, Brian Lathrop, Qi Zhang, Tobias Schei-deck, Kui Xu, Tess Hand-Bender, Stanley Peters,Liz Shriberg, and Carsten Bergmann.
2005.
A Flex-ible Conversational Dialog System for MP3 Player.In demo session of HLT-EMNLP 2005.35
