In: Proceedings of CoNLL-2000 and LLL-2000, pages 37-42, Lisbon, Portugal, 2000.Incorporating Position Information into a MaximumEntropy/Min imum Divergence Translation ModelGeorge  FosterRALI, Universit6 de Montr6alfos ter@i ro ,  umontrea l ,  caAbstractI describe two methods for incorporating infor-mation about the relative positions of bilingualword pairs into a Maximum Entropy/MinimumDivergence translation model.
The better of thetwo achieves over 40% lower test corpus perplex-ity than an equivalent combination of a trigramlanguage model and the classical IBM transla-tion model 2.1 Introduct ionStatistical Machine Translation (SMT) systemsuse a model ofp(tls), the probability that a texts in the source language will translate into a textt in the target language, to determine the besttranslation for a given source text.
A straight-forward way of modeling this distribution is toapply a chain-rule xpansion of the form:ItLp(t lS  ) = Hp(tiltl...ti-l,S),i=1(i)where ti denotes the ith token in t. 1 The objectsto be modeled in this case belong to the familyof conditional distributions p(wlhi, s), the prob-ability of the ith word in t, given the tokenswhich precede it and the source text.The main motivation for modeling p(tls ) interms of p(wlhi ,s) is that it simplifies the "de-coding" problem of finding the most likely tar-get text.
In particular, if hi is known, findingthe best word at the current position requiresonly a straightforward search through the target1This ignores the issue of normalization over targettexts of all possible lengths, which can be easily enforcedwhen desired by using a stop token or a prior distributionover lengths.vocabulary, and efficient dynamic-programmingbased heuristics can be used to extend this tosequences of words.
This is very important forapplications uch as TransType (Foster et al,1997; Langlais et al, 2000), where the task isto make real-time predictions of the text a hu-man translator will type next, based on thesource text under translation and some prefixof the target text that has already been typed.The standard "noisy channel" approach used inSMT, where p(tls ) c< p(t)p(slt), is generally tooexpensive for such applications because it doesnot permit direct calculation of the probabil-ity of a word or sequence of words beginning atthe current position.
Complex and expensivesearch strategies are required to find the besttarget text in this approach (Garcfa-Varea etal., 1998; Niessen et al, 1998; Ochet al, 1999;Wang and Waibel, 1998).The challenge in modeling p(wlhi,s ) is tocombine two disparate sources of conditioninginformation in an effective way.
One obviousstrategy is to use a linear combination of sep-arate language and translation components, ofthe form:p(w\[hi, s) -- Ap(w\[hi) + (1 - A)p(w\[i, s).
(2)where p(w\[hi) is a language model, p(wli , s) isa translation model, and A E \[0, 1\] is a com-bining weight.
However, this appears to bea weak technique (Langlais and Foster, 2000),even when A is allowed to depend on variousfeatures of the context (hi, s).In previous work (Foster, 2000), I de-scribed a Maximum Entropy/Minimum Diver-gence (MEMD) model (Berger et al, 1996)for p(w\[hi, s) which incorporates a trigram lan-guage model and a translation component whichis an analog of the well-known IBM transla-tion model 1 (Brown et al, 1993).
This model37significantly outperforms an equivalent linearcombination of a trigram and model 1 in test-corpus perplexity, despite using several orders ofmagnitude fewer translation parameters.
Likemodel 1, its translation component is based onlyon the occurrences in s of words which are po-tential translations for w, and does not takeinto account the positions of these words rel-ative to w. An obvious enhancement is to in-corporate such positional information into theMEMD model, thereby making its translationcomponent analogous to the IBM model 2.
Thisis the problem I address in this paper.2 Mode ls2.1 L inear  Mode lAs a baseline for comparison I used a linear com-bination as in (2) of a standard interpolated tri-gram language model and the IBM translationmodel 2 (IBM2), with the combining weight Aoptimized using the EM algorithm.
IBM2 is de-rived as follows: 2lp(wli, s ) = ~p(w, j l i ,  s )j=Ol~p(w\]s j )p( j l i ,  l)j=Owhere I = \[s\[, and the hidden variable j givesthe position in s of the (single) source token sjassumed to give rise to w, or 0 if there is none.The model consists of a set of word-pair param-eters p(t\[s) and position parameters p(j\[i,/); inmodel 1 (IBM1) the latter are fixed at 1/(1 + 1),as each position, including the empty position0, is considered equally likely to contain a trans-lation for w. Maximum likelihood estimates forthese parameters can be obtained with the EMalgorithm over a bilingual training corpus, asdescribed in (Brown et al, 1993).2.2 MEMD Mode l  1A MEMD model for p(w\[hi, s) has the generalform:p(wlhi, s) = q(w\[hi, s) exp(~ ?
f(w, hi, s))Z(hi ,s)2Model 2 was originally formulated for p(tls), butsince target words are predicted independently it canalso be used for p(wlhi  , s).
The only necessary modifica-tion in this case is that  the position parameters can nolonger be conditioned on It\[.where q(w\[hi,s) is a reference distribution,f(w, hi, s) maps (w, hi, s) into an n-dimensionalfeature vector, (~ is a corresponding vector offeature weights (the parameters of the model),and Z(hi, s) = ~w q(w\[hi, s) exp((~-f(w, hi)) isa normalizing factor.
For a given choice of qand f, the IIS algorithm (Berger et al, 1996)can be used to find maximum likelihood valuesfor the parameters ~.
It can be shown (DellaPietra et al, 1995) that these are the also thevalues which minimize the Kullback-Liebler di-vergence D(p\[\[q) between the model and thereference distribution under the constraint thatthe expectations of the features (ie, the compo-nents of f) with respect o the model must equaltheir expectations with respect o the empiricaldistribution derived from the training corpus.Thus the reference distribution serves as a kindof prior, and should reflect some initial knowl-edge about the true distribution; and the useof any feature is justified to the extent that itsempirical expectation is accurate.In the present context, the natural choice forthe reference distribution q is a trigram lan-guage model.
To create a MEMD analog toIBM model 1 (MEMD1), I used boolean fea-tures corresponding to bilingual word pairs:1, sEsandt - - - -wfst(W,S) = 0, elsewhere (s, t) is a (source,target) word pair.
Usingthe notational convention that ast is 0 wheneverthe corresponding feature fst does not exist inthe model, MEMD1 can be written compactlyas:p(wlhi,s) = q(wlhi) exp(~ asw)/Z(hi,s).sEsDue to the theoretical properties of MEMDoutlined above, it is necessary to select a sub-set of all possible features fst to avoid overfittingthe training corpus.
Using a reduced feature setis also computationally advantageous, since thetime taken to calculate the normalization con-stant Z(hi, s) grows linearly with the expectednumber of features which are active per sourceword s E s. This is in contrast o IBM1, whereuse of all available word-pair parameters p(tls )is standard, and engenders only a very slightoverfitting effect.
In (Foster, 2000) I describe an38effective technique for selecting MEMD word-pair features.2.3 MEMD Mode l  2IBM2 incorporates position information by in-troducing a hidden position variable and mak-ing independence hypotheses.
This approach isnot applicable to MEMD models, whose fea-tures must capture events which are directlyobservable in the training corpus.
3 It would bepossible to use pure position features of the formfi#, which capture the presence of any wordpair at position (i, j, l) and are superficially sim-ilar to IBM2's position parameters, but thesewould add almost no information to MEMD1.On the other hand, features like fstijl, indicat-ing the presence of a specific pair (s, t) at posi-tion (i, j , /) ,  would cause severe data sparsenessproblems.Encoding Posit ions as Feature ValuesA simple solution to this di lemma is to let thevalue of a word-pair feature reflect the currentposition of the pair rather just its presence orabsence.
A reasonable choice for this is thevalue of the corresponding IBM2 position pa-rameter p(jli, /):fst(W, i, s) = { P(Jsli'o, l), elseS E s and t = wwhere js is the position of s in s, or the mostlikely position according to IBM2 if it occursmore than once: 5s = argmaxj:sj=s P(jli, l).
Us-ing the same convention as in the previous sec-tion, the resulting model (MEMD2R) can bewritten:q(wlhi) exP(E~es aswP(5~ li, l)) p(wlhi, s ) =Z(hi,  s)MEMD2R is simple and compact but poses atechnical difficulty due to its use of real-valuedfeatures, in that the IIS training algorithm re-quires integer or boolean features for efficientimplemention.
Since likelihood is a concavefunction of ~, any hillclimbing method such asgradient ascent 4 is guaranteed to find maximum3Although it is possible to extend the basic frameworkto allow for embedded Hidden Markov Models (Lalferty,1995).4I found that the "stochastic" variant of this algo-rithm, in which model parameters are updated after eachtraining example, gave the best performance.likelihood parameter values, but convergence isslower than IIS and requires tuning a gradientstep parameter.
Unfortunately, apart from thisproblem, MEMD2R also turns out to performslightly worse than MEMD1, as described be-low.Using Class-based Posi t ion FeaturesSince the basic problem with incorporating po-sition information is one of insufficient data, anatural solution is to try to group word pair andposition combinations with similar behaviourinto classes such that the frequency of eachclass in the training corpus is high enough forreliable estimation.
To do this, I made twopreliminary assumptions: 1) word pairs withsimilar MEMD1 weights should be grouped to-gether; and 2) position configurations with sim-ilar IBM2 probabilities hould be grouped to-gether.
This converts the problem from oneof finding classes in the five-dimensional space(s, t, i, j, l) to one of identifying rectangular ar-eas on a 2-dimensional grid where one axis con-tains position configurations (i, j, l), ordered byp(jli,/); and the other contains word pairs (s, t),ordered by ast.
To simplify further, I parti-tioned both axes so as to approximately bal-ance the total corpus frequency of all word pairsor position configurations within each parti-tion.
Thus the only parameters required to com-pletely specify a classification are the number ofposition and word-pair partitions.
Each combi-nation of a position partit ion and a word pairpartit ion corresponds to a class, and all classescan be expected to have roughly the same em-pirical counts.The model (MEMD2B) based on this schemehas one feature for each class; if A designates theset of triples (i, j, l) in a position partit ion andB designates the set of pairs (s, t) in a word-pairpartition, then for all A, B there is a feature:fA,B(w,i,s) l = ~j=l  5\[(i,j,l) EA A(sj,w) B Aj = )sj\],where 5\[X\] is 1 when X is true and 0 other-wise.
For robustness, I used these position fea-tures along with pure MEMDl-style word-pairfeatures fst.
The weights O~A, s on the positionfeatures can thus be interpreted as correctionterms for the pure word-pair weights as,t which39segment file pairs sentence pairs English tokens French tokenstrain 922 1,639,250 29,547,936 31,826,112held-out 1 30 54,758 978,394 1,082,350held-out 2 30 59,435 1,111,454 1,241,581test 30 53,676 984,809 1,103,320Table 1: Corpus segmentation.
The train segment was the main training corpus; the held-out 1segment was used for combining weights for the trigram and the overall linear model; and theheld-out 2 segment was used for the MEMD2B partition search.reflect the proximity of the words in the pair.
p(TIS) -1~IT\], where p is the model being eval-The model is: uated, and (S, T) is the test corpus, considered.to be a set of statistically independent sentencep(w\[hi,s) = q(wlhi)exp(~ses a w + aA(i,j~,O,B(s,t))pair s (s,t).
Perplexity is a good indicator ofZ(hi,s)where A(i,Ss,l) gives the partition for the cur-rent position, B(s, t) gives the partition for thecurrent word pair, and following the usual con-vention, aA(i,j~,0,S(s,t) is zero if these are unde-fined.To find the optimal number of position par-titions m and word-pair partitions n, I per-formed a greedy search, beginning at a small ini-tial point (m, n) and at each iteration trainingtwo MEMD2B models characterized by (km, n)and (m, kn), where k > 1 is a scaling factor(note that both these models contain kmn po-sition features).
The model which gives thebest performance on a validation corpus is usedas the starting point for the next iteration.Since training MEMD models is very expen-sive, to speed up the search I relaxed the con-vergence criterion from a training corpus per-plexity 5 drop of < .1% (requiring 20-30 IIS it-erations) to < .6% (requiring approximately 10IIS iterations).
I stopped the search when thebest model's performance on the validation cor-pus did not decrease significantly from that ofthe model at the previous tep, indicating thatovertraining was beginning to occur.3 Resu l tsI tested the models on the Canadian Hansardcorpus, with English as the source languageand French as the target language.
After sen-tence alignment using the method describedin (Simard et al, 1992), the corpus was splitinto disjoint segments as shown in table 1.To evaluate performance, I used perplexity:5Defined in the next sectionl~erformance for the TransType application de-scribed in the introduction, and it has also beenused in the evaluation of full-fledged SMT sys-tems (A1-Onaizan et al, 1999).
To ensure a faircomparison, all models used the same target vo-cabulary.
For all MEMD models, I used 20,000word-pair features selected using the methoddescribed in (Foster, 2000); this is suboptimalbut gives reasonably good performance and fa-cilitates experimentation.Figures 1 and 2 show, respectively, the pathtaken by the MEMD2B partition search, andthe validation corpus perplexities of each modeltested during the search.
As shown in figure 1,the search consisted of 6 iterations.
Since on allprevious iterations no increase in position parti-tions beyond the initial value of 10 was selected,on the 5th iteration I tried decreasing the num-ber of position partitions to 5.
This model wasnot selected either, so on the final step only thenumber of word-pair partitions was augmented,yielding an optimal combination of 10 positionpartitions and 4000 word-pair partitions.Table 2 gives the final results for all mod-els.
The IBM models tested here incorporatea reduced set of 1M word-pair parameters, e-lected using the method described in (Foster,2000), which gives slightly better test-corpusperformance than the unrestricted set of all 35Mword pairs which cooccur within aligned sen-tence pairs in the training corpus.The basic MEMD1 model (without positionparameters) attains about 30% lower perplex-ity than the model 2 baseline, and MEMD2Bwith an optimal-sized set of position param-eters achieves in a further drop of over 10%.Interestingly, the difference between IBM1 and40model word-pair position perplexity improvementparameters parameters over baselinetrigramtrigram + IBM1trigram + IBM2MEMD1MEMD2RMEMD2BMEMD2B01,000,0001,000,00020,00020,00020,00020,00000115,5680010 x 1010 x 400061.043.235.224.528.422.120.2O%30.4%19.3%37.2%42.6%Table 2: Model performances.
Linear interpolation is designated with a + sign; and the MEMD2Bposition parameters are given as rex, where m and n are the numbers of position partitions andword-pair partitions respectively.400020001000,500250\i t I i5 10 20 50number of posi~on parti~ons20.620.420.2~.
20o~ u19.919.619.410 position classes - -~20 position classes -~50 position dasses -0--5 position classes .x ....II I I r50 250 500 1000 2000word-pair classes19.24000Figure 1: MEMD2B partition search path, be-ginning at the point (10, 10).
Arrows out of eachpoint show the configurations tested at each it-eration.IBM2's performance (18.5% lower perplexity forIBM2) is about the same as the difference be-tween MEMD1 and MEMD2B (17.6% lower forMEMD2B).4 ConclusionThis paper deals with the problem of incorpo-rating information about the positions of bilin-gual word pairs into a MEMD model which isanalogous to the classical IBM model 1, therebycreating a MEMD analog to the IBM model 2.
Iproposed and evaluated two methods for accom-plishing this: using IBM2 position parameterprobabilities as MEMD feature values, whichwas unsuccessful; and adding features whichFigure 2: Validation corpus perplexities for var-ious MEMD2B models.
Each connected line inthis graph corresponds to a vertical column ofsearch points in figure 1.capture the occurrence of a word-pair with aMEMD1 weight that falls into a specific rangeof values at a position to which IBM2 assignsa probability in a certain range.
The secondmodel achieved over 40% lower test perplex-ity than a linear combination of a trigram andIBM2, despite using several orders of magnitudefewer parameters.This work represents a novel approach totranslation modeling which is most appropriatefor applications like TransType which need tomake rapid predictions of upcoming text.
How-ever, it is not inconceivable that it could alsobe used for full-fledged MT.
One partial impedi-ment to this is that the MEMD framework lacks41a mechanism equivalant o the EM algorithmfor estimating probabilities associated with hid-den variables.
The solution I have proposedhere can be seen as a first step to investigat-ing ways of getting around this problem.AcknowledgementsThis work was carried out as part of theTransType project at RALI, funded by the Nat-ural Sciences and Engineering Research Councilof Canada.ReferencesYaser A1-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation:Final report, JHU workshop 1999.
Technicalreport, The Center for Language and SpeechProcessing, The Johns Hopkins University,www.clsp.jhu.edu/ws99/projects/mt/final_report.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Maximum Entropyapproach to Natural Language Processing.
Com-putational Linguistics, 22(1):39-71.Peter F. Brown, Stephen A. Della Pietra, VincentDella J. Pietra, and Robert L. Mercer.
1993.The mathematics of Machine Translation: Pa-rameter estimation.
Computational Linguistics,19(2):263-312, June.S.
Della Pietra, V. Della Pietra, and J. Lafferty.1995.
Inducing features of random fields.
Tech-nical Report CMU-CS-95-144, CMU.George Foster, Pierre Isabelle, and Pierre Plamon-don.
1997.
Target-text Mediated Interactive Ma-chine Translation.
Machine Translation, 12:175-194.George Foster.
2000.
A Maximum Entropy / Min-imum Divergence translation model.
In Proceed-ings of the 38th Annual Meeting of the Associationfor Computational Linguistics (ACL-38), HongKong, October.Ismael Garcfa-Varea, Francisco Casacuberta, andHermann Ney.
1998.
An iterative, DP-basedsearch algorithm for statistical machine trans-lation.
In Proceedings of the 5th InternationalConference on Spoken Language Processing (IC-SLP) 1998, Sydney, Australia, December.
pages1135-1138.John D. Lafferty.
1995.
Gibbs-markov models.
InComputing Science and Statistics: Proceedings ofthe 27th Symposium on the Interface.
InterfaceFoundation.Ph.
Langlais and G. Foster.
2000.
Using context-dependent interpolation to combine statisticallanguage and translation models for interactiveMT.
In Content-Based Multimedia InformationAccess (RIAO), Paris, France, April.Ph.
Langlais, G. Foster, and G. Lapalme.
2000.
Unitcompletion for a computer-aided translation typ-ing system.
In Proceedings of the 5th Conferenceon Applied Natural Language Processing (ANLP-5), Seattle, Washington, May.S.
Niessen, S. Vogel, H. Ney, and C. Tillmann.1998.
A DP based search algorithm for statisticalmachine translation.
In Proceedings of the 36thAnnual Meeting of the Association \]or Computa-tional Linguistics (ACL) and 17th InternationalConference on Computational Linguistics (COL-ING) 1998, pages 960-967, MontrEal, Canada,August.Franz Jose\] Och, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for statis-tical machine translation.
In Proceedings of the~nd Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), College Park,Maryland.Michel Simard, George F. Foster, and Pierre Is-abelle.
1992.
Using cognates to align sentences inbilingual corpora.
In Proceedings of the 4th Con-ference on Theoretical and Methodological Is-sues in Machine Translation (TMI), Montr@al,Qu@bec.Ye-yi Wang and Alex Waibel.
1998.
Fast decod-ing for statistical machine translation.
In Proceed-ings of the 5th International Conference on Spo-ken Language Processing (ICSLP) 1998, Sydney,Australia, December, pages 2775-2778.42
