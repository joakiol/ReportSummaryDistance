Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117?125,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLearning Probabilistic Synchronous CFGs for Phrase-based TranslationMarkos MylonakisILLCUniversity of Amsterdamm.mylonakis@uva.nlKhalil Sima?anILLCUniversity of Amsterdamk.simaan@uva.nlAbstractProbabilistic phrase-based synchronousgrammars are now considered promis-ing devices for statistical machine transla-tion because they can express reorderingphenomena between pairs of languages.Learning these hierarchical, probabilisticdevices from parallel corpora constitutes amajor challenge, because of multiple la-tent model variables as well as the riskof data overfitting.
This paper presentsan effective method for learning a familyof particular interest to MT, binary Syn-chronous Context-Free Grammars with in-verted/monotone orientation (a.k.a.
Bi-nary ITG).
A second contribution con-cerns devising a lexicalized phrase re-ordering mechanism that has complimen-tary strengths to Chiang?s model.
Thelatter conditions reordering decisions onthe surrounding lexical context of phrases,whereas our mechanism works with thelexical content of phrase pairs (akin tostandard phrase-based systems).
Surpris-ingly, our experiments on French-Englishdata show that our learning method ap-plied to far simpler models exhibits per-formance indistinguishable from the Hierosystem.1 IntroductionA fundamental problem in phrase-based machinetranslation concerns the learning of a probabilisticsynchronous context-free grammar (SCFG) overphrase pairs from an input parallel corpus.
Chi-ang?s Hiero system (Chiang, 2007) exemplifiesthe gains to be had by combining phrase-basedtranslation (Och and Ney, 2004) with the hierar-chical reordering capabilities of SCFGs, particu-larly originating from Binary Inversion Transduc-tion Grammars (BITG) (Wu, 1997).
Yet, exist-ing empirical work is largely based on successfulheuristic techniques, and the learning of Hiero-likeBITG/SCFG remains an unsolved problem,The difficulty of this problem stems from theneed for simultaneously learning of two kinds ofpreferences (see Fig.1) (1) lexical translation prob-abilities (P (?e, f?
| X)) of source (f ) and target(e) phrase pairs, and (2) phrase reordering prefer-ences of a target string relative to a source string,expressed in synchronous productions probabil-ities (for monotone or switching productions).Theoretically speaking, both kinds of preferencesmay involve latent structure relative to the paral-lel corpus.
The mapping between source-targetsentence pairs can be expressed in terms of la-tent phrase segmentations and latent word/phrase-alignments, and the hierarchical phrase reorder-ing can be expressed in terms of latent binarysynchronous hierarchical structures (cf.
Fig.
1).But each of these three kinds of latent structuresmay be made explicit using external resources:word-alignment could be considered solved us-ing Giza++ (Och and Ney, 2003)), phrase pairscan be obtained from these word-alignments (Ochand Ney, 2004), and the hierarchical synchronousstructure can be grown over source/target linguis-tic syntactic trees output by an existing parser.The Joint Phrase Translation Model (Marcu andWong, 2002) constitutes a specific case, albeitwithout the hierarchical, synchronous reorderingStart S ?
X 1 / X 1 (1)Monotone X ?
X 1 X 2 /X 1 X 2 (2)Switching X ?
X 1 X 2 /X 2 X 1 (3)Emission X ?
e / f (4)Figure 1: A phrase-pair SCFG (BITG)117component.
Other existing work, e.g.
(Chiang,2007), assumes the word-alignments are given inthe parallel corpus, but the problem of learningphrase translation probabilities is usually avoidedby using surface counts of phrase pairs (Koehn etal., 2003).
The problem of learning the hierar-chical, synchronous grammar reordering rules isoftentimes addressed as a learning problem in itsown right assuming all the rest is given (Blunsomet al, 2008b).A small number of efforts has been dedicatedto the simultaneous learning of the probabilitiesof phrase translation pairs as well as hierarchi-cal reordering, e.g., (DeNero et al, 2008; Zhanget al, 2008; Blunsom et al, 2009).
Of these,some concentrate on evaluating word-alignment,directly such as (Zhang et al, 2008) or indirectlyby evaluating a heuristically trained hierarchicaltranslation system from sampled phrasal align-ments (Blunsom et al, 2009).
However, veryfew evaluate on actual translation performance ofinduced synchronous grammars (DeNero et al,2008).
In the majority of cases, the Hiero system,which constitutes the yardstick by which hierar-chical systems are measured, remains superior intranslation performance, see e.g.
(DeNero et al,2008).This paper tackles the problem of learning gen-erative BITG models as translation models assum-ing latent segmentation and latent reordering: thisis the most similar setting to the training of Hiero.Unlike all other work that heuristically selects asubset of phrase pairs, we start out from an SCFGthat works with all phrase pairs in the training setand concentrate on the aspects of learning.
Thislearning problem is fraught with the risks of over-fitting and can easily result in inadequate reorder-ing preferences (see e.g.
(DeNero et al, 2006)).Almost instantly, we find that the translationperformance of all-phrase probabilistic SCFGslearned in this setting crucially depends on the in-terplay between two aspects of learning:?
Defining a more constrained parameterspace, where the reordering productionsare phrase-lexicalised and made sensitive toneighbouring reorderings, and?
Defining an objective function that effec-tively smoothes the maximum-likelihood cri-terion.One contribution of this paper is in devis-ing an effective, data-driven smoothed Maximum-Likelihood that can cope with a model workingwith all phrase pair SCFGs.
This builds uponour previous work on estimating parameters of a?bag-of-phrases?
model for Machine Translation(Mylonakis and Sima?an, 2008).
However, learn-ing SCFGs poses significant novel challenges, thecore of which lies on the hierarchical nature of astochastic SCFG translation model and the rele-vant additional layer of latent structure.
We ad-dress these issues in this work.
Another importantcontribution is in defining a lexicalised reorder-ing component within BITG that captures orderdivergences orthogonal to Chiang?s model (Chi-ang, 2007) but somewhat akin to Phrase-BasedStatistical Machine Translation reordering models(Koehn et al, 2003).Our analysis shows that the learning difficul-ties can be attributed to a rather weak generativemodel.
Yet, our best system exhibits Hiero-levelperformance on French-English Europarl data us-ing an SCFG-based decoder (Li et al, 2009).
Ourfindings should be insightful for others attemptingto make the leap from shallow phrase-based sys-tems to hierarchical SCFG-based translation mod-els using learning methods, as opposed to heuris-tics.The rest of the paper is structured as follows.Section 2 briefly introduces the SCFG formalismand discusses its adoption in the context of Statis-tical Machine Translation (SMT).
In section 3, weconsider some of the pitfalls of stochastic SCFGgrammar learning and address them by introduc-ing a novel learning objective and algorithm.
Inthe section that follows we browse through latenttranslation structure choices, while in section 5 wepresent our empirical experiments on evaluatingthe induced stochastic SCFGs on a translation taskand compare their performance with a hierarchicaltranslation baseline.
We close with a comparisonof related work and a final discussion including fu-ture research directions.2 Synchronous Grammars for MachineTranslationSynchronous Context Free Grammars (SCFGs)provide an appealing formalism to describe thetranslation process, which explains the generationof parallel strings recursively and allows capturinglong-range reordering phenomena.
Formally, anSCFG G is defined as the tuple (N,E, F,R, S),118where N is the finite set of non-terminals withS ?
N the start symbol, F and E are finite setsof words for the source and target language and Ris a finite set of rewrite rules.
Every rule expandsa left-hand side non-terminal to a right-hand sidepair of strings, a source language string over thevocabulary F?N and a target language string overE ?
N .
The number of non-terminals in the twostrings is equal and the rule is complemented witha mapping between them.String pairs in the language of the SCFG arethose with a valid derivation, consisting of a se-quence of rule applications, starting from S andrecursively expanding the linked non-terminals atthe right-hand side of rules.
Stochastic SCFGsaugment every rule in R with a probability, underthe constraint that probabilities of rules with thesame left-hand side sum up to one.
The probabil-ity of each derived string pair is then the productof the probabilities of rules used in the derivation.Unless otherwise stated, for the rest of the paperwhen we refer to SCFGs we will be pointing totheir stochastic extension.The rank of an SCFG is defined as the maxi-mum number of non-terminals in a grammar?s ruleright-hand side.
Contrary to monolingual ContextFree Grammars, there does not always exist a con-version of an SCFG of a higher rank to one of alower rank with the same language of string pairs.For this, most machine translation applications fo-cus on SCFGs of rank two (binary SCFGs), orbinarisable ones witch can be converted to a bi-nary SCFG, given that these seem to cover mostof the translation phenomena encountered in lan-guage pairs (Wu, 1997) and the related processingalgorithms are less demanding computationally.Although SCFGS were initially introduced formachine translation as a stochastic word-basedtranslation process in the form of the Inversion-Transduction Grammar (Wu, 1997), they were ac-tually able to offer state-of-the-art performance intheir latter phrase-based implementation by Chi-ang (Chiang, 2005).
Chiang?s Hiero hierarchi-cal translation system is based on a synchronousgrammar with a single non-terminal X coveringall learned phrase-pairs.
Beginning from the startsymbol S, an initial phrase-span structure is con-structed monotonically using a simple ?glue gram-mar?
:S ?S 1 X 2 / S 1 X 2S ?X 1 / X 1The true power of the system lies in expandingthese initial phrase-spans with a set of hierarchi-cal translation rules, which allow conditioning re-ordering decisions based on lexical context.
Forthe French to English language pair, some exam-ples would be:S ?
X 1 e?conomiques / financial X 1S ?
cette X 1 de X 2 / this X 1 X 2S ?
politique X 1 commune de X 2 /X 2?
s common X 1 policyFurther work builds on the Hiero grammar to ex-pand it with constituency syntax motivated non-terminals (Zollmann and Venugopal, 2006).3 Synchronous Grammar LearningThe learning of phrase-based stochastic SCFGswith a Maximum Likelihood objective is exposedto overfitting as other all-fragment models such asPhrase-Based SMT (PBSMT) (Marcu and Wong,2002; DeNero et al, 2006) and Data-OrientedParsing (DOP) (Bod et al, 2003; Zollmann andSima?an, 2006).
Maximum Likelihood Estima-tion (MLE) returns degenerate grammar estimatesthat memorise well the parallel training corpus butgeneralise poorly to unseen data.The bias-variance decomposition of the gener-alisation error Err sheds light on this learningproblem.
For an estimator p?
with training data D,Err can be expressed as the expected Kullback-Leibler (KL) divergence between the target distri-bution q and that the estimate p?.
This error decom-poses into bias and variance terms (Heskes, 1998):Err =bias?
??
?KL(q, p?)+variance?
??
?EDKL(p?, p?)
(5)Bias is the KL-divergence between q and the meanestimate over all training data p?
= EDp?(D).
Vari-ance is the expected divergence between the av-erage estimate and the estimator?s actual choice.MLE estimators for all-fragment models are zero-biased with zero divergence between the averageestimate and the true data distribution.
In contrast,their variance is unboundedly large, leading to un-bounded generalisation error on unseen cases.1193.1 Cross Validated MLEA well-known method for estimating generalisa-tion error is k-fold Cross-Validation (CV) (Hastieet al, 2001).
By partitioning the training data Dinto k parts Hk1 , we estimate Err as the expectederror over all 1 ?
i ?
k, when testing on Hi witha model trained by MLE on the rest of the dataD?i = ?j 6=iHj .Here we use CV to leverage the bias-variancetrade-off for learning stochastic all-phrase SCFGs.Given an input all-phrase SCFG grammar withphrase-pairs extracted from the training data, wemaximise training data likelihood (MLE) subjectto CV smoothing: for each data part Hi (1 ?
i ?k), we consider only derivations which employgrammar rules extracted from the rest of the dataD?i.
Other work (Mylonakis and Sima?an, 2008)has also explored MLE under CV for a ?bag-of-phrases model?
that does not deal with reorderingpreferences, does not employ latent hierarchicalstructure and works with a non-hierarchical de-coder, and partially considers the sparsity issuesthat arise within CV training.
The present paperdeals with these issues.Because of the latent segmentation and hi-erarchical variables, CV-smoothed MLE cannotbe solved analytically and we devise a CV in-stance of the Expectation-Maximization (EM) al-gorithm, with an implementation based on a syn-chronous version of the Inside-Outside algorithm(see Fig.
2).
For each word-aligned sentence pairin a partitionHi, the set of eligible derivations (de-noted D?i) are those that can be built using onlyphrase-pairs and productions found inD?i.
An es-sential part of the learning process involves defin-ing the grammar extractor G(D), a function fromdata to an all-phrase SCFG.
We will discuss vari-ous extractors in section 4.Our CV-EM algorithm is an EM instance, guar-anteeing convergence and a non-decreasing CV-smoothed data likelihood after each iteration.
Therunning time remains O(n6), where n is inputlength, but by considering only derivation spanswhich do not cross word-alignment points, thisruns in reasonable times for relatively large cor-pora.3.2 Bayesian Aspects of CV-MLEBeside being an estimator, the CV-MLE learningalgorithm has the added value of being a grammarlearner focusing on reducing generalisation error,INPUT: Word-aligned parallel training data DGrammar extractor GThe number of parts k to partition DOUTPUT: SCFG G with rule probabilities p?Partition training data D into parts H1, .
.
.
,Hk.For 1 ?
i ?
k doExtract grammar rules set Gi = G(Hi)Initialise G = ?iGi, p?0 uniformLet j = 0RepeatLet j = j + 1E-step:For 1 ?
i ?
k doCalculate expected counts given G, p?j?1,for derivations D?i of Hiusing rules from ?k 6=iG(k)M-step: set p?j to ML estimate givenexpected countsUntil convergenceFigure 2: The CV Expectation Maximization al-gorithmin the sense that probabilities of grammar produc-tions should reflect the frequency with which theseproductions are expected to be used for translatingfuture data.
Additionally, since the CV criterionprohibits for every data point derivations that userules only extracted from the same data part, suchrules are assigned zero probabilities in the final es-timate and are effectively excluded from the gram-mar.
In this way, the algorithm ?shapes?
the inputgrammar, concentrating probability mass on pro-ductions that are likely to be used with future data.One view point of CV-MLE is that each par-tition D?i and Hi induces a prior probabilityPrior(pi; D?i) on every parameter assignment pi,obtained from D?i.
This prior assigns zero prob-ability to all pi parameter sets with non-zero prob-abilities for rules not in G(D?i), and uniformlydistributes probability to the rest of the parametersets.
In light of this, the CV-MLE objective can bewritten as follows:argmaxpi?iPrior(pi; D?i)?
P (Hi | pi) (6)This data-driven prior aims to directly favour pa-rameter sets which are expected to better gener-alise according to the CV criterion, without rely-ing on arbitrary constraints such as limiting the120length of phrase pairs in the right-hand side ofgrammar rules.
Furthermore, other frequently em-ployed priors such as the Dirichlet distribution andthe Dirichlet Process promote better generalisingrule probability distributions based on externallyset hyperparameter values, whose selection is fre-quently sensitive in terms of language pairs, oreven the training corpus itself.
In contrast, the CV-MLE prior aims for a data-driven Bayesian model,focusing on getting information from the data, in-stead of imposing external human knowledge onthem (see also (Mackay and Petoy, 1995)).3.3 Smoothing the ModelOne remaining wrinkle in the CV-EM scheme isthe treatment of boundary cases.
There will oftenbe sentence-pairs in Hi, that cannot be fully de-rived by the grammar extracted from the rest of thedata D?i either because of (1) ?unknown?
words(i.e.
not appearing in other parts of the CV parti-tion) or (2) complicated combinations of adjacentword-alignments.
We employ external smoothingof the grammar, prior to learning.Our solution is to extend the SCFG extractedfromD?i with new emission productions derivingthe ?unknown?
phrase-pairs (i.e., found in Hi butnot in D?i).
Crucially, the probabilities of theseproductions are drawn from a fixed smoothing dis-tribution, i.e., they remain constant throughout es-timation.
Our smoothing distribution of phrase-pairs for all pre-terminals considers source-targetphrase lengths drawn from a Poisson distributionwith unit mean, drawing subsequently the wordsof each of the phrases uniformly from the vocab-ulary of each language, similar to (Blunsom et al,2009).psmooth(f/e) =ppoisson(|f |; 1) ppoisson(|e|; 1)V |f |f V|e|eSince the smoothing distribution puts strongerpreference on shorter phrase-pairs and avoidscompeting with the ?known?
phrase-pairs, it leadsthe learner to prefer using as little as possible suchsmoothing rules, covering only the phrase-pairsrequired to complete full derivations.4 Parameter Spaces and GrammarExtractorsA Grammar Extractor (GE) plays a major role inour probabilistic SCFG learning pipeline.
A GE isa function from a word-aligned parallel corpus to aprobabilistic SCFG model.
Together with the con-straints that render a proper probabilistic SCFG1,this defines the parameter space.The extractors used in this paper create SCFGsproductions of two different kinds: (a) hierarchi-cal synchronous productions that define the spaceof possible derivations up to the level of the SCFGpre-terminals, and (2) the phrase-pair emissionrules that expand the pre-terminals to phrase-pairsof varying lengths.
Given the word-alignments,the set of phrase-pairs extracted is the set of alltranslational equivalents (without length upper-bound) under the word-alignment as defined in(Och and Ney, 2004; Koehn et al, 2003).Below we focus on the two grammar extrac-tors employed in our experiments.
We start outfrom the most generic, BITG-like formulation,and aim at incremental refinement of the hierar-chical productions in order to capture relevant,content-based phrase-pair reordering preferencesin the training data.Single non-terminal SCFG This is a phrase-based binary SCFG grammar employing a singlenon-terminal X covering each extracted phrase-pair.
The other productions consist of monotoneand switching expansions of phrase-pair spanscovered by X .
Finally, the whole sentence-pair isconsidered to be covered by X .
We will call this?plain SCFG?
extractor.
See Fig.
1.Lexicalised Reordering SCFG One weaknessof the plain SCFG is that the reordering deci-sions in the derivations are made without referenceto lexical content of the phrases; this is becauseall phrase-pairs are covered by the same non-terminal.
As a refinement, we propose a gram-mar extractor that aims at modelling the reorderingbehaviour of phrase-pairs by taking their contentinto account.
This time, the X non-terminal is re-served for phrase-pairs and spans which will takepart in monotonic productions only.
Two freshnon-terminals, XSL and XSR, are used for cov-ering phrase-pairs that participate in order switch-ing with other, adjacent phrase-pairs.
The non-terminal XSL covers phrase-pairs which appearfirst in the source language order, and the latterthose which follow them.
The grammar rules pro-duced by this GE, dubbed ?switch grammar?, arelisted in Fig.
3.1The sum of productions that have the same left-hand la-bel must be one.121Start S ?
X 1 /X 1Monotone ExpansionX ?
X 1 X 2 /X 1 X 2XSL ?
X 1 X 2 / X 1 X 2XSR ?
X 1 X 2 /X 1 X 2Switching ExpansionX ?
XSL 1 XSR 2 /XSR 2 XSL 1XSL ?
XSL 1 XSR 2 /XSR 2 XSL 1XSR ?
XSL 1 XSR 2 /XSR 2 XSL 1Phrase-Pair EmissionX ?
e/fXSL ?
e / fXSR ?
e / fFigure 3: Lexicalised-Reordering SCFGThe reordering information captured by theswitch grammar is in a sense orthogonal to thatof Hiero-like systems utilising rules such as thoselisted in section 2.
Hiero rules encode hier-archical reordering patterns based on surround-ing context.
In contrast, the switch grammarmodels the reordering preferences of the phrase-pairs themselves, similarly to the monotone-swap-discontinuous reordering models of Phrase-basedSMT models (Koehn et al, 2003).
Furthermore, itstrives to match pairs of such preferences, combin-ing together phrase-pairs with compatible reorder-ing preferences.5 ExperimentsIn this section we proceed to integrate our esti-mates within an SCFG-based decoder.
We subse-quently evaluate our performance in relation to astate-of-the-art Hiero baseline on a French to En-glish translation task.5.1 DecodingThe joint model of bilingual string derivations pro-vided by the learned SCFG grammar can be usedfor translation given a input source sentence, sinceargmaxe p(e|f) = argmaxe p(e, f).
We use ourlearned stochastic SCFG grammar with the decod-ing component of the Joshua SCFG toolkit (Liet al, 2009).
The full translation model inter-polates log-linearly the probability of a grammarderivation together with the language model prob-ability of the target string.
The model is furthersmoothed, similarly to phrase-based models andthe Hiero system, with smoothing features ?i suchas the lexical translation scores of the phrase-pairsinvolved and rule usage penalties.
As usual withstatistical translation, we aim for retrieving the tar-get sentence e corresponding to the most probablederivation D??
(f, e) with rules r, with:p(D) ?
p(e)?lmpscfg(e, f)?scfg?i?r?D?i(r)?iThe interpolation weights are tuned using Mini-mum Error Rate Training (Och, 2003).5.2 ResultsWe test empirically the learner?s output gram-mars for translating from French to English, us-ing k = 5 for the Cross Validation data partition-ing.
The training material is a GIZA++ word-aligned corpus of 200K sentence-pairs from theEuroparl corpus (Koehn, 2005), with our devel-opment and test parallel corpora of 2K sentence-pairs stemming from the same source.
Train-ing the grammar parameters until convergence de-mands around 6 hours on an 8-core 2.26 GHz IntelXeon system.
Decoding employs a 4-gram lan-guage model, trained on English Europarl data of19.5M words, smoothed using modified Kneser-Ney discounting (Chen and Goodman, 1998), andlexical translation smoothing features based on theGIZA++ alignments.In a sense, the real baseline to which we mightcompare against should be a system employing theMLE estimate for the grammar extracted from thewhole training corpus.
However, as we have al-ready discussed, this assigns zero probability to allsentence-pairs outside of the training data and issubsequently bound to perform extremely poorly,as decoding would then completely rely on thesmoothing features.
Instead, we opt to compareagainst a hierarchical translation baseline providedby the Joshua toolkit, trained and tuned on thesame data as our learning algorithm.
The grammarused by the baseline is much richer than the oneslearned by our algorithm, also employing ruleswhich translate with context, as shown in section2.
Nevertheless, since it is not clear how the re-ordering rules probabilities of a grammar similarto the ones we use could be trained heuristically,we choose to relate the performance of our learnedstochastic SCFG grammars to the particular, state-of-the-art in SCFG-based translation, system.Table 1 presents the translation performance re-sults of our systems and the baseline.
On first122SystemLexicalBLEUSmoothingjoshua-baseline No 27.79plain scfg No 28.04switch scfg No 28.48joshua-baseline Yes 29.96plain scfg Yes 29.75switch scfg Yes 29.88Table 1: Empirical results, with and without addi-tional lexical translation smoothing features dur-ing decodingobservation, it is evident that our learning algo-rithm outputs stochastic SCFGs which manage togeneralise, avoiding the degenerate behaviour ofplain MLE training for these models.
Given thenotoriety of the estimation process, this is note-worthy on its own.
Having a learning algorithmat hand which realises in a reasonable extent thepotential of each stochastic grammar design (asimplemented in the relevant grammar extractors),we can now compare between the two grammarextractors used in our experiments.
The resultstable highlights the importance of conditioningthe reordering process on lexical grounds.
Theplain grammar with the single phrase-pair non-terminal cannot accomplish this and achieves alower BLEU score.
On the other hand, the switchSCFG allows such conditioning.
The learner takesadvantage of this feature to output a grammarwhich performs better in taking reordering deci-sions, something that is reflected in both the actualtranslations as well as the BLEU score achieved.Furthermore, our results highlight the impor-tance of the smoothing decoding features.
Theunsmoothed baseline system itself scores consid-erably less when employing solely the heuristictranslation score.
Our unsmoothed switch gram-mar decoding setup improves on the baseline bya considerable difference of 0.7 BLEU.
Subse-quently, when adding the smoothing lexical trans-lation features, both systems record a significantincrease in performance, reaching comparable lev-els of performance.The degenerate behaviour of MLE for SCFGscan be greatly limited by constraining ourselvesto grammars employing minimal phrase-pairs; phrase-pairs which cannot be further brokendown into smaller ones according to the word-alignment.
One could argue that it is enough toperform plain MLE with such minimal phrase-pairSCFGs, instead of using our more elaborate learn-ing algorithm with phrase-pairs of all lengths.
Toinvestigate this, for our final experiment we useda plain MLE estimate of the switch grammar totranslate, limiting the grammar?s phrase-pair emis-sion rules to only those which involve minimalphrase-pairs.
The very low score of 17.82 BLEU(without lexical smoothing) not only highlightsthe performance gains of using longer phrase-pairsin hierarchical translation models, but most impor-tantly provides a strong incentive to address theoverfitting behaviour of MLE estimators for suchmodels, instead of avoiding it.6 Related workMost learning of phrase-based models, e.g.,(Marcu and Wong, 2002; DeNero et al, 2006;Mylonakis and Sima?an, 2008), works without hi-erarchical components (i.e., not based on the ex-plicit learning of an SCFG/BITG).
These learningproblems pose other kinds of learning challengesthan the ones posed by explicit learning of SCFGs.Chiang?s original work (Chiang, 2007) is also re-lated.
Yet, the learning problem is not expressed interms of an explicit objective function because sur-face heuristic counts are used.
It has been very dif-ficult to match the performance of Chiang?s modelwithout use of these heuristic counts.A somewhat related work, (Blunsom et al,2008b), attempts learning new non-terminal labelsfor synchronous productions in order to improvetranslation.
This work differs substantially fromour work because it employs a heuristic estimatefor the phrase pair probabilities, thereby concen-trating on a different learning problem: that of re-fining the grammar symbols.
Our approach mightalso benefit from such a refinement but we do notattempt this problem here.
In contrast, (Blunsomet al, 2008a) works with the expanded phrase pairset of (Chiang, 2005), formulating an exponentialmodel and concentrating on marginalising out thelatent segmentation variables.
Again, the learningproblem is rather different from ours.
Similarly,the work in (Zhang et al, 2008) reports on a multi-stage model, without a latent segmentation vari-able, but with a strong prior preferring sparse esti-mates embedded in a Variational Bayes (VB) esti-mator.
This work concentrates the efforts on prun-ing both the space of phrase pairs and the space of(ITG) analyses.123To the best of our knowledge, this work is thefirst to attempt learning probabilistic phrase-basedBITGs as translation models in a setting whereboth a phrase segmentation component and a hi-erarchical reordering component are assumed la-tent variables.
Like this work, (Mylonakis andSima?an, 2008; DeNero et al, 2008) also employan all-phrases model.
Our paper shows that it ispossible to train such huge grammars under itera-tive schemes like CV-EM, without need for sam-pling or pruning.
At the surface of it, our CV-EM estimator is also a kind of Bayesian learner,but in reality it is a more specific form of regu-larisation, similar to smoothing techniques used inlanguage modelling (Chen and Goodman, 1998;Mackay and Petoy, 1995).7 Discussion and Future ResearchPhrase-based stochastic SCFGs provide a rich for-malism to express translation phenomena, whichhas been shown to offer competitive performancein practice.
Since learning SCFGs for machinetranslation has proven notoriously difficult, mostsuccessful SCFGmodels for SMT rely on rules ex-tracted from word-alignment patterns and heuris-tically computed rule scores, with the impact andthe limits imposed by these choices yet unknown.Some of the reasons behind the challenges ofSCFG learning can be traced back to the introduc-tion of latent variables at different, competing lev-els: word and phrase-alignment as well as hier-archical reordering structure, with larger phrase-pairs reducing the need for extensive reorderingstructure and vice versa.
While imposing priorssuch as the often used Dirichlet distribution or theDirichlet Process provides a method to overcomethese pitfalls, we believe that the data-driven reg-ularisation employed in this work provides an ef-fective alternative to them, focusing more on thedata instead of importing generic external humanknowledge.We believe that this work makes a significantstep towards learning synchronous grammars forSMT.
This is an objective not only worthy be-cause of promises of increased performance, but,most importantly, also by increasing the depth ofour understanding on SCFGs as vehicles of latenttranslation structures.
Our usage of the inducedgrammars directly for translation, instead of an in-termediate task such as phrase-alignment, aims ex-actly at this.While the latent structures that we exploredin this paper were relatively simple in compar-ison with Hiero-like SCFGs, they take a differ-ent, content-driven approach on learning reorder-ing preferences than the context-driven approachof Hiero.
We believe that these approaches are notmerely orthogonal, but could also prove comple-mentary.
Taking advantage of the possible syner-gies between content and context-driven reorder-ing learning is an appealing direction of future re-search.
This is particularly promising for otherlanguage pairs, such as Chinese to English, whereHiero-like grammars have been shown to performparticularly well.Acknowledgments: Both authors are supportedby a VIDI grant (nr.
639.022.604) from TheNetherlands Organization for Scientific Research(NWO).ReferencesP.
Blunsom, T. Cohn, and M. Osborne.
2008a.
A dis-criminative latent variable model for statistical ma-chine translation.
In Proceedings of ACL-08: HLT,pages 200?208.
Association for Computational Lin-guistics.Phil Blunsom, Trevor Cohn, and Miles Osborne.2008b.
Bayesian synchronous grammar induction.In Advances in Neural Information Processing Sys-tems 21, Vancouver, Canada, December.Phil Blunsom, Trevor Cohn, Chris Dyer, and MilesOsborne.
2009.
A gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of the47th Annual Meeting of the Association of Compu-tational Linguistics, Singapore, August.
Associationfor Computational Linguistics.R.
Bod, R. Scha, and K. Sima?an, editors.
2003.
DataOriented Parsing.
CSLI Publications, Stanford Uni-versity, Stanford, California, USA.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Tech-nical Report TR-10-98, Harvard University, August.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL 2005, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33.J.
DeNero, D. Gillick, J. Zhang, and D. Klein.
2006.Why generative phrase models underperform sur-face heuristics.
In Proceedings on the Workshop onStatistical Machine Translation, pages 31?38, NewYork City.
Association for Computational Linguis-tics.124John DeNero, Alexandre Bouchard-Co?te?, and DanKlein.
2008.
Sampling alignment structure un-der a Bayesian translation model.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing, pages 314?323, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.T.
Hastie, R. Tibshirani, and J. H. Friedman.
2001.
TheElements of Statistical Learning.
Springer.Tom Heskes.
1998.
Bias/variance decompositions forlikelihood-based estimators.
Neural Computation,10:1425?1433.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In HLT-NAACL 2003.P.
Koehn.
2005.
Europarl: A Parallel Corpus for Sta-tistical Machine Translation.
In MT Summit 2005.Zhifei Li, Chris Callison-Burch, Chris Dyer, San-jeev Khudanpur, Lane Schwartz, Wren Thornton,Jonathan Weese, and Omar Zaidan.
2009.
Joshua:An open source toolkit for parsing-based machinetranslation.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, pages 135?139,Athens, Greece, March.
Association for Computa-tional Linguistics.David J. C. Mackay and Linda C. Bauman Petoy.
1995.A hierarchical dirichlet language model.
NaturalLanguage Engineering, 1:1?19.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proceedings of Empirical methods in natural lan-guage processing, pages 133?139.
Association forComputational Linguistics.Markos Mylonakis and Khalil Sima?an.
2008.
Phrasetranslation probabilities with itg priors and smooth-ing as learning objective.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 630?639, Honolulu, USA,October.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403.H.
Zhang, Ch.
Quirk, R. C. Moore, and D. Gildea.2008.
Bayesian learning of non-compositionalphrases with synchronous parsing.
In Proceedingsof ACL-08: HLT, pages 97?105, Columbus, Ohio,June.
Association for Computational Linguistics.A.
Zollmann and K. Sima?an.
2006.
An efficientand consistent estimator for data-oriented parsing.Journal of Automata, Languages and Combinatorics(JALC), 10 (2005) Number 2/3:367?388.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings on the Workshop on Statistical Ma-chine Translation, pages 138?141, New York City,June.
Association for Computational Linguistics.125
