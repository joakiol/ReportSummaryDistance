Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 1?6,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproving Syntactic Coordination Resolution Using Language ModelingPhilip V. OgrenCenter for Computational PharmacologyUniversity of Colorado Denver12801 E. 17th AveAurora, CO 80045, USAphilip@ogren.infoAbstractDetermining the correct structure of coordi-nating conjunctions and the syntactic con-stituents that they coordinate is a difficult task.This subtask of syntactic parsing is exploredhere for biomedical scientific literature.
Inparticular, the intuition that sentences contain-ing coordinating conjunctions can often berephrased as two or more smaller sentencesderived from the coordination structure is ex-ploited.
Generating candidate sentences cor-responding to different possible coordinationstructures and comparing them with a lan-guage model is employed to help determinewhich coordination structure is best.
Thisstrategy is used to augment a simple baselinesystem for coordination resolution which out-performs both the baseline system and a con-stituent parser on the same task.1 IntroductionFor this work, coordination resolution (CR) refers tothe task of automatically identifying the correct co-ordination structure of coordinating conjunctions.
Inthis study the conjunctions and and or and the con-juncts they coordinate are examined.
CR is an im-portant subtask of syntactic parsing in the biomed-ical domain because many information extractiontasks require correct syntactic structures to performwell, in particular coordination structures.
For ex-ample, (Cohen et al, 2009) showed that using a con-stituent parser trained on biomedical data to providecoordination structures to a high-precision protein-protein interaction recognition system resulted ina significant performance boost from an overall F-measure of 24.7 to 27.6.
Coordination structures arethe source of a disproportionate number of parsingerrors for both constituent parsers (Clegg and Shep-herd, 2007) and dependency parsers (Nivre and Mc-Donald, 2008).CR is difficult for a variety of reasons related tothe linguistic complexity of the phenomenon.
Thereare a number of measurable characteristics of coor-dination structures that support this claim includingthe following: constituent types of conjuncts, num-ber of words per conjunct, number of conjuncts perconjunction, and the number of conjunctions that arenested inside the conjunct of another conjunction,among others.
Each of these metrics reveal widevariability of coordination structures.
For example,roughly half of all conjuncts consist of one or twowords while the other half consist of three or morewords including 15% of all conjuncts that have tenor more words.
There is also an increased preva-lence of coordinating conjunctions in biomedical lit-erature when compared with newswire text.
Table 1lists three corpora in the biomedical domain that areannotated with deep syntactic structures; CRAFT(described below), GENIA (Tateisi et al, 2005), andPenn BIOIE (Bies et al, 2005).
The number of co-ordinating conjunctions they contain as a percentageof the number of total tokens in each corpus are com-pared with the Penn Treebank corpus (Marcus et al,1994).
The salient result from this table is that thereare 50% more conjunctions in biomedical scientifictext than in newswire text.
It is also interesting tonote that 15.4% of conjunctions in the biomedicalcorpora are nested inside a conjunct of another con-1junction as compared with 10.9% for newswire.Table 1: Biomedical corpora that provide coordinationstructures compared with the Penn Treebank corpus.Corpus Tokens ConjunctionsCRAFT 246,008 7,115 2.89%GENIA 490,970 14,854 3.03%BIOIE 188,341 5,036 2.67%subtotal 925,319 27,005 2.92%PTB 1,173,766 22,888 1.95%The Colorado Richly Annotated Full-Text(CRAFT) Corpus being developed at the Univer-sity of Colorado Denver was used for this work.Currently, the corpus consists of 97 full-text open-access scientific articles that have been annotated bythe Mouse Genome Institute1 with concepts fromthe Gene Ontology2 and Mammalian PhenotypeOntology3.
Thirty-six of the articles have beenannotated with deep syntactic structures similarto that of the Penn Treebank corpus described in(Marcus et al, 1994).
As this is a work in progress,eight of the articles have been set aside for a finalholdout evaluation and results for these articlesare not reported here.
In addition to the standardtreebank annotation, the NML tag discussed in(Bies et al, 2005) and (Vadas and Curran, 2007)which marks nominal subconstituents which donot observe the right-branching structure commonto many (but not all) noun phrases is annotated.This is of particular importance for coordinatednoun phrases because it provides an unambiguousrepresentation of the correct coordination structure.The coordination instances in the CRAFT datawere converted to simplified coordination structuresconsisting of conjunctions and their conjuncts usinga script that cleanly translates the vast majority ofcoordination structures.2 Related WorkThere are two main approaches to CR.
The first ap-proach considers CR as a task in its own right where1http://www.informatics.jax.org/2http://geneontology.org/3http://www.informatics.jax.org/searches/MP_form.shtmlthe solutions are built specifically to perform CR.Often the task is narrowly defined, e.g.
only coor-dinations of the pattern noun-1 conjunction noun-2noun-3 are considered, and relies on small trainingand testing data sets.
Generally, such research ef-forts do not attempt to compare their results withprevious results other than in the broadest and most-qualified way.
Studies by (Chantree et al, 2005),(Nakov and Hearst, 2005), and (Resnik, 1999) arerepresentative examples of such work.
A study by(Shimbo and Hara, 2007) performed CR on sen-tences from the GENIA corpus containing one in-stance of the word ?and?
coordinating noun phrases.They used a sequence alignment algorithm modifiedfor CR drawing on the intuition that conjuncts havesimilar syntactic constructs.
In each of these studies,promising results were achieved by careful applica-tion of their respective approaches.
However, eachstudy is limited in important respects because theynarrowly constrain the problem, use limited train-ing data, and make certain unrealistic assumptionsin their experimental setup that make general appli-cation of their solutions problematic.
For example,in the study by (Shimbo and Hara, 2007) they choseonly sentences that have one instance of ?and?
be-cause their algorithm does not handle nested con-junctions.
Additionally, they assume an oracle thatprovides the system with only sentences that containcoordinated noun phrases.The work most similar to this study was done by(Hara et al, 2009) in that they define the CR taskessentially the same as is done here.
Their approachinvolves a grammar tailored for coordination struc-tures that is coupled with a sequence alignment al-gorithm that uses perceptrons for learning featureweights of an edit graph.
The evaluation metric theyuse is slightly less strict than the metric used forthis study in that they require identification of theleft boundary of the left-most conjunct and the rightboundary of the right-most conjunct to be countedcorrect.
Two other important differences are thatthe evaluation data comes from the GENIA corpusand they use gold-standard part-of-speech tags forthe input data.
Regardless of these relatively minordifferences, their performance of 61.5 F-measure faroutperforms what is reported below and experimentsthat are directly comparable to their work will beperformed.2The second main approach considers CR withinthe broader task of syntactic parsing.
Any syntac-tic parser that generates constituents or dependen-cies must necessarily perform CR to perform well.Typically, a syntactic parser will have a single, cen-tral algorithm that is used to determine all con-stituents or dependencies.
However, this does notpreclude parsers from giving special attention to CRby adding CR-specific rules and features.
For exam-ple, (Nilsson et al, 2006) show that for dependencyparsing it is useful to transform dependency struc-tures that make conjunctions the head of their con-juncts into structures in which coordination depen-dencies are chained.
(Charniak and Johnson, 2005)discusses a constituent-based parser that adds twofeatures to the learning model that directly addresscoordination.
The first measures parallelism in thelabels of the conjunct constituents and their childrenand the second measures the lengths of the conjunctconstituents.
The work done by (Hogan, 2007) fo-cuses directly on coordination of noun phrases inthe context of the Collins parser (Collins, 2003) bybuilding a right conjunct using features from the al-ready built left conjunct.3 Using a Language ModelConsider the following sentence:Tyr mutation results in increased IOP andaltered diurnal changes.By exploiting the coordination structure we canrephrase this sentence as two separate sentences:?
Tyr mutation results in increased IOP.?
Tyr mutation results in altered diurnal changes.Using this simple rewrite strategy a candidate sen-tence for each possible conjunct can be composed.For this sentence there are six possible left conjunctscorresponding to each word to the left of the con-junction.
For example, the candidate conjunct cor-responding to the third word is results in increasedIOP and the corresponding sentence rewrite is Tyrmutation altered diurnal changes.
The resultingcandidate sentences can be compared by calculat-ing a sentence probability using a language model.Ideally, the candidate sentence corresponding to thecorrect conjunct boundary will have a higher proba-bility than the other candidate sentences.
One prob-lem with this approach is that the candidate sen-tences are different lengths.
This has a large andundesirable (for this task) impact on the probabilitycalculation.
A simple and effective way to normal-ize for sentence length is by adding4 the probabilityof the candidate conjunct (also computed by usingthe language model) to the probability of the candi-date sentence.
The probability of each candidate iscalculated using this simple metric and then rank or-dered.
Because the number of candidate conjunctsvaries from one sentence to the next (as determinedby the token index of the conjunction) it is useful totranslate the rank into a percentile.
The rank per-centile of the candidate conjuncts will be applied tothe task of CR as described below.
However, it isinformative to directly evaluate how good the rankpercentile scores of the correct conjuncts are.To build a language model a corpus of more than80,000 full-text open-access scientific articles wereobtained from PubMed Central5.
The articles areprovided in a simple XML format which was parsedto produce plain text documents using only sectionsof the articles containing contentful prose (i.e.
byexcluding sections such as e.g.
acknowledgmentsand references.)
The plain text documents wereautomatically sentence segmented, tokenized, andpart-of-speech tagged resulting in nearly 13 millionsentences and over 250 million tagged words.
A lan-guage model was then built using this data with theSRILM toolkit described in (Stolcke, 2002).
De-fault options were used for creating the languagemodel except that the order of the model was set tofour and the ?-tagged?
option was used.
Thus, a 4-gram model with Good-Turing discounting and Katzbackoff for smoothing was built.For each token to the left of a conjunction a candi-date conjunct/sentence pair is derived, its probabil-ity calculated, and a rank percentile score is assignedto it relative to the other candidates.
Because mul-tiple conjuncts can appear on the left-hand-side ofthe conjunction, the left border of the leftmost con-junct is considered here.
The same is done for tokens4logprobs are used here5http://www.ncbi.nlm.nih.gov/pmc/about/ftp.html.
The corpus was downloaded in September of2008.3Figure 1: The first column can be read as ?The correctconjunct candidate had the highest rank percentile 32.1%of the time.?
The second column can be read as ?Thecorrect conjunct candidate had a rank percentile of 90%or greater 17.6% of the time.?
The columns add to one.on the right-hand-side of the conjunction.
Figure 1shows a histogram of the rank percentile scores forthe correct left conjunct.
The height of the bars cor-respond to the percentage of the total number of con-junctions in which the correct candidate was rankedwithin the percentile range.
Thus, the columns addto one and generalizations can be made by addingthe columns together.
For example, 66.7% of theconjunctions (by adding the first three columns) fallabove the eightieth percentile.
The overall averagerank percentage for all of the left-hand-side con-juncts was 81.1%.
The median number of candi-dates on the left-hand-side is 17 (i.e.
the median to-ken index of the conjunction is 17).
Similar resultswere obtained for the right-hand-side data but werewithheld for space considerations.
The overall av-erage rank percentage for right-hand-side conjunctswas 82.2%.
This slightly better result is likely dueto the smaller median number of candidates on theright-hand-side of 12 (i.e.
the median token indexof the conjunction is 12 from the end of the sen-tence.)
These data suggest that the rank percentile ofthe candidate conjuncts calculated in this way couldbe an effective feature to use for CR.4 Coordination ResolutionTable 2 reports the performance of two CR systemsthat are described below.
Results are reported as F-Measure at both the conjunct and conjunction lev-els where a true positive requires all boundaries tobe exact.
That is, for conjunct level evaluation aconjunct generated by the system must have exactlythe same extent (i.e.
character offsets) as the con-junct in the gold-standard data in addition to be-ing attached to the same conjunction.
Similarly, atthe conjunction level a true positive requires that acoordination structure generated by the system hasthe same number of conjuncts each with extents ex-actly the same as the corresponding conjunct in thegold-standard coordination structure.
Where 10-foldcross-validation is performed, training is performedon roughly 90% of the data and testing on the re-maining 10% with the results micro-averaged.
Here,the folds are split at the document level to avoid theunfair advantage of training and testing on differentsections of the same document.Table 2: Coordination resolution results at the conjunctand conjunction levels as F-Measure.Conjunct ConjunctionOpenNLP + PTB 55.46 36.56OpenNLP + CRAFT 58.87 39.50baseline 59.75 40.99baseline + LM 64.64 46.40The first system performs CR within the broadertask of syntactic parsing.
Here the constituent parserfrom the OpenNLP project6 is applied.
This parserwas chosen because of its availability and ease ofuse for both training and execution.
It has alsobeen shown by (Buyko et al, 2006) to perform wellon biomedical data.
The output of the parser isprocessed by the same conversion script describedabove.
The parser was trained and evaluated onboth the Penn Treebank and CRAFT corpora.
Forthe latter, 10-fold cross-validation was performed.Preliminary experiments that attempted to add ad-ditional training data from the GENIA and PennBIOIE corpora proved to be slightly detrimental toperformance in both cases.
Table 2 shows that CRimproves at the conjunction level by nearly threepoints (from 36.56 to 39.50) by simply training onbiomedical data rather than using a model trainedon newswire.The second system that performs CR as a separate6http://opennlp.sf.net4task by using token-level classification to determineconjunct boundaries is introduced and evaluated.
Inbrief, each token to the left of a conjunction is clas-sified as being either a left-hand border of a conjunctfor that conjunction or not.
Similarly, tokens to theright of a conjunction are classified as either a right-hand border of a conjunct or not.
From these token-level classifications and some simple assumptionsabout the right-hand and left-hand borders of leftand right conjuncts, respectively,7 a complete coor-dination structure can be constructed.
The classifierused was SVMlight described in (Joachims, 1999)using a linear kernel.
The baseline system uses anumber of shallow lexical features (many commonto named entity recognition systems) including part-of-speech tags, word and character n-grams, the dis-tance between the focus token and the conjunction,and word-level features such as whether the tokenis a number or contains a hyphen.
A more detaileddescription of the baseline system is avoided here asthis remains a major focus of current and future re-search efforts and the final system will likely changeconsiderably.
Table 2 shows the results of 10-foldcross-validation for the baseline system.
This sim-ple baseline system performs at 40.99 F-measure atthe conjunction level which is modestly better thanthe syntactic parser trained on CRAFT.The baseline system as described above was aug-mented using the language modeling approach de-scribed in Section 3 by adding a simple feature toeach token being classified whose value is the rankpercentile of the probability of the correspondingconjunct candidate.
Again, 10-fold cross-validationwas performed.
Table 2 shows that this augmentedbaseline system performs at 46.40 F-measure at theconjunction level which out-performs the baselinesystem and the CRAFT-trained parser by 5.4 and 6.9points, respectively.
This increase in performancedemonstrates that a language model can be effec-tively purposed for CR.While the use of a language model to improve CRresults is promising, the results in Table 2 also speakto how difficult this task is for machines to perform.In contrast, the task is comparatively easy for hu-mans to perform consistently.
To calculate inter-7For example, the left-hand border of the conjunct to theright of a conjunction will always be the first word followingthe conjunction.annotator agreement on the CR task, 500 sentencescontaining either the word ?and?
or ?or?
were ran-domly chosen from the 13 million sentence corpusdescribed in Section 3 and annotated with coordi-nation structures by two individuals, the author andanother computer scientist with background in biol-ogy.
Our positive specific agreement8 was 91.93 and83.88 at the conjunct and conjunction level, respec-tively, for 732 conjunctions.
This represents a dra-matic gulf between system and human performanceon this task but also suggests that large improve-ments for automated CR should be expected.5 Future WorkThere is much that can be done to move this workforward.
Creating comparable results to the studydiscussed in Section 2 by (Hara et al, 2009) is a toppriority.
As alluded to earlier, there is much that canbe done to improve the baseline system.
For exam-ple, constraining coordination structures to not over-lap except where one is completely nested withinthe conjunct of another should be enforced as par-tially overlapping coordination structures never oc-cur in the training data.
Similarly, a conjunction thatappears inside parentheses should have a coordina-tion structure that is completely contained inside theparentheses.
Thorough error analysis should alsobe performed.
For example, it would be interestingto characterize the conjuncts that have a low rankpercentile for their calculated probability.
Also, itwould be useful to measure performance across anumber of metrics such as phrase type of the con-juncts, length of conjuncts, whether a coordinationstructure is nested inside another, etc.
Demonstrat-ing that CR can improve syntactic parsing perfor-mance and improve the performance of an informa-tion extraction system would give this work greatersignificance.ConclusionThis work has demonstrated that a language modelcan be used to improve performance of a simple CRsystem.
This is due to the high rank percentile of theprobability of the correct conjunct compared withother possible conjuncts.8This measure is directly comparable with F-measure.5ReferencesAnn Bies, Seth Kulick, and Mark Mandel.
2005.
Parallelentity and treebank annotation.
In CorpusAnno ?05:Proceedings of the Workshop on Frontiers in CorpusAnnotations II, pages 21?28, Morristown, NJ, USA.Association for Computational Linguistics.Ekaterina Buyko, Joachim Wermter, Michael Poprat, andUdo Hahn.
2006.
Automatically adapting an NLPcore engine to the biology domain.
In Proceedingsof the Joint BioLINK-Bio-Ontologies Meeting.
A JointMeeting of the ISMB Special Interest Group on Bio-Ontologies and the BioLINK Special Interest Group onText Data M ining in Association with ISMB, pages65?68.
Citeseer.Francis Chantree, Adam Kilgarriff, Anne De Roeck, andAlistair Willis.
2005.
Disambiguating coordinationsusing word distribution information.
Proceedings ofRANLP2005.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 173?180, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Andrew Clegg and Adrian Shepherd.
2007.
Bench-marking natural-language parsers for biological appli-cations using dependency graphs.
BMC Bioinformat-ics, 8(1):24.Kevin B. Cohen, Karin Verspoor, Helen L. Johnson,Chris Roeder, Philip V. Ogren, William A. Baumgart-ner Jr, Elizabeth White, Hannah Tipney, and LawrenceHunter.
2009.
High-precision biological event extrac-tion with a concept recognizer.
In Proceedings of theWorkshop on BioNLP: Shared Task, pages 50?58.
As-sociation for Computational Linguistics.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational linguis-tics, 29(4):589?637.Kazuo Hara, Masashi Shimbo, Hideharu Okuma, andYuji Matsumoto.
2009.
Coordinate structure analysiswith global structural constraints and alignment-basedlocal features.
In ACL-IJCNLP ?09: Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume2, pages 967?975, Morristown, NJ, USA.
Associationfor Computational Linguistics.Deirdre Hogan.
2007.
Coordinate noun phrase disam-biguation in a generative parsing model.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 680?687, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Thorsten Joachims.
1999.
Making large scale SVMlearning practical.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of English: The Penn Treebank.
Computationallinguistics, 19(2):313?330.Preslav Nakov and Marti Hearst.
2005.
Using the web asan implicit training set: Application to structural ambi-guity resolution.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 835?842, Vancouver, British Columbia, Canada, October.Association for Computational Linguistics.Jens Nilsson, Joakim Nivre, and Johan Hall.
2006.Graph transformations in data-driven dependencyparsing.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 257?264, Sydney, Australia, July.
Associa-tion for Computational Linguistics.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-08: HLT, pages 950?958,Columbus, Ohio, June.
Association for ComputationalLinguistics.Philip Resnik.
1999.
Semantic similarity in a Taxonomy:An Information-Based Measure and its Application toProblems of Ambiguity in Natural Language.
Journalof Artificial Intelligence, 11(11):95?130.Masashi Shimbo and Kazuo Hara.
2007.
A discrim-inative learning model for coordinate conjunctions.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 610?619.Andreas Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Seventh International Conferenceon Spoken Language Processing, volume 3.
Citeseer.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Ju-nichi Tsujii.
2005.
Syntax Annotation for the GE-NIA corpus.
In Second International Joint Conferenceon Natural Language Processing (IJCNLP05), pages222?227.David Vadas and James Curran.
2007.
Adding nounphrase structure to the penn treebank.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 240?247, Prague,Czech Republic, June.
Association for ComputationalLinguistics.6
