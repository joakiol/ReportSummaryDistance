Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 72?81,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsLinguistic categorization and complexityKatya PertsovaUNC-Chapel HillLinguistics Dept, CB 3155Chapel Hill, NC 27599, USApertsova@unc.eduAbstractThis paper presents a memoryless categoriza-tion learner that predicts differences in cate-gory complexity found in several psycholin-guistic and psychological experiments.
In par-ticular, this learner predicts the order of diffi-culty of learning simple Boolean categories,including the advantage of conjunctive cate-gories over the disjunctive ones (an advantagethat is not typically modeled by the statisticalapproaches).
It also models the effect of la-beling (positive and negative labels vs. posi-tive labels of two different kinds) on categorycomplexity.
This effect has implications forthe differences between learning a single cat-egory (e.g., a phonological class of segments)vs. a set of non-overlapping categories (e.g.,affixes in a morphological paradigm).1 IntroductionLearning a linguistic structure typically involves cat-egorization.
By ?categorization?
I mean the taskof dividing the data into subsets, as in learningwhat sounds are ?legal?
and what are ?illegal,?
whatmorpheme should be used in a particular morpho-syntactic context, what part of speech a given wordsis, and so on.
While there is an extensive literatureon categorization models within the fields of psy-chology and formal learning, relatively few connec-tions have been made between this work and learn-ing of linguistic patterns.One classical finding from the psychological liter-ature is that the subjective complexity of categoriescorresponding to Boolean connectives follows theorder shown in figure 1 (Bruner et al, 1956; Neisserand Weene, 1962; Gottwald, 1971).
In psycholog-ical experiments subjective complexity is measuredin terms of the rate and accuracy of learning an arti-ficial category defined by some (usually visual) fea-tures such as color, size, shape, and so on.
Thisfinding appears to be consistent with the complex-ity of isomorphic phonological and morphologicallinguistic patterns as suggested by typological stud-ies not discussed here for reasons of space (Mielke,2004; Cysouw, 2003; Clements, 2003; Moreton andPertsova, 2012).
Morphological patterns isomorphicto those in figure 1 appear in figure 2.The first goal of this paper is to derive the abovecomplexity ranking from a learning bias.
While thedifficulty of the XOR category is notorious and itis predicted by many models, the relative differencebetween AND and OR is not.
This is because thesetwo categories are complements of each other (solong as all features are binary), and in this sensehave the same structure.
A memorizing learner canpredict the order AND > OR simply because ANDhas fewer positive examples, but it will also incor-rectly predict XOR > OR and AND > AFF.
Manypopular statistical classification models do not pre-dict the order AND > OR (such as models basedon linear classifiers, decision tree classifiers, naiveBayes classifiers, and so on).
This is because thesame classifier would be found for both of these cat-egories given that AND and OR differ only with re-spect to what subset of the stimuli is assigned a pos-itive label.
Models proposed by psychologists, suchas SUSTAIN (Love et al, 2004), RULEX (Nosof-sky et al, 1994b), and Configural Cue (Gluck and72AFF (affirmation) AND OR XOR/??
N?
M?
N?
M?
N?
M?
N?
Mcircle circle AND black triangle OR white (black AND triangle)OR (white AND circle)Figure 1: Boolean categories over two features, shape and color: AFF > AND > OR > XORaffirmation AND OR XOR/?sg pl?part m. - -im+part m. - -imsg pl?part -s -+part - -sg pl?poss - -s+poss -s -ssg placc.
- -snom.
-s -Hebrew, verb English, verb English nouns Old French,agreement in pres.
agreement in pres.
o-stem nounsFigure 2: Patterns of syncretism isomorphic to the structure of Boolean connectivesBower, 1988) also do not predict the order AND >OR fore similar reasons.
Feldman (2000) speculatesthat this order is due to a general advantage of theUP-versions of a category over the DOWN-versions(for a category that divides the set of instances intotwo uneven sets, the UP-version is the version inwhich the smaller subset is positively labeled, andthe DOWN-version is the version in which the largersubset is positively labeled).
However, he offers noexplanation for this observation.
On the other hand,it is known that the choice of representations can af-fect learnability.
For instance, k-DNF formulas arenot PAC-learnable while k-CNF formulas describingthe same class of patterns are PAC-learnable (Kearnsand Vazirani, 1994).
Interestingly, this result alsoshows that conjunctive representations have an ad-vantage over the disjunctive ones because a verysimple strategy for learning conjunctions (Valiant,1984) can be extended to the problem of learningk-CNFs.
The learner proposed here includes in itscore a similar intersective strategy which is respon-sible for deriving the order AND > OR.The second goal of the paper is to provide a uni-fied account of learning one vs. several categoriesthat partition the feature space (the second problemis the problem of learning paradigms).
The moststraight-forward way of doing this ?
treating cate-gory labels as another feature with n values for nlabels ?
is not satisfactory for several reasons dis-cussed in section 2.
In fact, there is empirical ev-idence that the same pattern is learned differentlydepending on whether it is presented as learning adistinction between positive and negative instancesof a category or whether it is presented as learningtwo different (non-overlapping) categories.
This ev-idence will be discussed in section 3.I should stress that the learner proposed here is notdesigned to be a model of ?performance.?
It makesa number of simplifying assumptions and does notinclude parameters that are fitted to match the be-havioral data.
The main goal of the model is to pre-dict the differences in subjective complexity of cate-gories as a function of their logical structure and thepresence/absence of negative examples.2 Learning one versus many categoriesCompare the task of learning a phonological in-ventory with the task of learning an inventory ofmorph-meaning pairs (as in learning an inflectionalparadigm).
The first task can be viewed as divid-ing the set of sounds into attested and non-attested(?accidental gaps?).
At first glance, the second taskcan be analogously viewed as dividing the set ofstimuli defined by morpho-syntactic features plusan n-ry feature (for n distinct morphs) into pos-sible vs. impossible combinations of morphs andmeanings.
However, treating morphs as feature val-ues leads to the possibility of paradigms in which73Neutral (AND/ORn)f1 f1f2 A Bf2 B BBiasedANDb ORbf1 f1f2 A ?Af2 ?A ?Af1 f1f2 ?A Af2 A ATable 1: Three AND/OR conditions in Gottwald?s studydifferent morphs are used with exactly the sameset of features as well as paradigms with ?acciden-tal gaps,?
combinations of morpho-syntactic featurevalues that are impossible in a language.
In fact,however, morphs tend to partition the space of pos-sible instances so that no instance is associated withmore than one morph.
That is, true free variationis really rare (Kroch, 1994).
Secondly, system-widerather than lexical ?accidental gaps?
are also rare inmorphology (Sims, 1996).
Therefore, I construe theclassification problem in both cases as learning a setof non-overlapping Boolean formulas correspond-ing to categories.
This set can consist of just oneformula, corresponding to learning a single categoryboundary, or it can consist of multiple formulas thatpartition the feature space, corresponding to learn-ing non-overlapping categories each associated witha different label.3 Effects of labeling on categorycomplexityA study by Gottwald (1971) found interesting dif-ferences in the subjective complexity of learningpatterns in figure 1 depending on whether the datawas presented to subjects as learning a single cat-egory (stimuli were labeled A vs. ?A) or whetherit was presented as learning two distinct categories(the same stimuli were labeled A vs. B).
Follow-ing this study, I refer to learning a single category as?biased labeling?
(abbreviated b) and learning sev-eral categories as ?neutral labeling?
(abbreviated n).Observe that since the AND/OR category divides thestimuli into unequal sets, it has two different biasedversions: one biased towards AND and one biasedtowards OR (as demonstrated in table 1).
The orderof category complexity found by Gottwald wasAFFn, AFFb> ANDb> AND/ORn> ORb, XORb> XORnThese results show that for the XOR category theneutral labeling was harder than biased labeling.
Onthe other hand, for the AND/OR category the neutrallabeling was of intermediate difficulty, and, interest-ingly, easier than ORb.
This is interesting becauseit goes against an expectation that learning two cat-egories should be harder than learning one category.Pertsova (2012) partially replicated the above find-ing with morphological stimuli (where null vs. overtmarking was the analog of biased vs. neutral label-ing).
Certain results from this study will be high-lighted later.4 The learning algorithmThis proposal is intended to explain the complex-ity differences found in learning categories in thelab and in the real world (as evinced by typologi-cal facts).
I focus on two factors that affect categorycomplexity, the logical structure of a category andthe learning mode.
The learning mode refers to bi-ased vs. neutral labeling, or, to put it differently,to the difference between learning a single categoryand learning a partition of a feature space into sev-eral categories.
The effect of the learning mode oncategory complexity is derived from the followingtwo assumptions: (i) the algorithm only responds tonegative instances when they contradict the currentgrammar, and (ii) a collection of instances can onlybe referred to if it is associated with a positive label.The first assumption is motivated by observations ofBruner et.
al (1956) that subjects seemed to rely lesson negative evidence than on positive evidence evenin cases when such evidence was very informative.The second assumption corresponds to a commonsentiment that having a linguistic label for a cate-gory aids in learning (Xu, 2002).4.1 Some definitionsFor a finite nonempty set of features F , we definethe set of instances over these features, I(F ), as fol-lows.
LetRf be a set of feature values for a feature f(e.g., Rheight = {high,mid, low}).
Each instance iis a conjunction of feature values given by the func-tions f ?
Rf for all features f ?
F .
A categoryis a set of instances that can be described by some74non-contradictory Boolean formula ?.1 Namely, ?describes a set of instances X if and only if it is log-ically equivalent to the disjunction of all instancesin X .
For instance, in the world with three binaryfeatures p, q, w, the formula p ?
q describes the setof instances {{pqw}, {pqw?}}
(where each instanceis represented as a set).
We will say that a formula?
subsumes a formula ?
if and only if the set of in-stances that ?
describes is a superset of the set ofinstances that ?
describes.
An empty conjunction ?describes the set of all instances.The goal of the learner is to learn a set of Booleanformulas describing the distribution of positive la-bels (in the neutral mode all labels are positive, inthe biased mode there is one positive label and onenegative label).
A formula describing the distribu-tion of a label l is encoded as a set of entries of theform eli (an i-th entry for label l).
The distributionof l is given by el1 ?
.
.
.
?
eln , the disjunction of nformulas corresponding to entries for l. Each entryeli consists of two components: a maximal conjunc-tion ?max and an (optional) list of other formulasEX (for exceptions).
A particular entry e with twocomponents, e[?max] and e[EX] = {?1 .
.
.
?n}, de-fines the formula e[?max] ?
?
(?1 ?
?2 ?
.
.
.
?
?n).e[?max] can intuitively be thought of as a rule ofthumb for a particular label and EX as a list of ex-ceptions to that rule.
In the neutral mode exceptionsare pointers to other entries or, more precisely, for-mulas encoded by those entries.
In the biased modethey are formulas corresponding to instances (i.e.,conjunctions of feature values for all features).
Thealgorithm knows which mode it is in because the bi-ased mode contains negative labels while the neutralmode does not.
Finally, an instance i is consistentwith an entry e if and only if the conjunction en-coded by i logically implies the formula encoded bye.
For example, an instance {pqw} is consistent withan entry encoding the formula {p}.Note that while this grammar can describe arbi-trarily complex patterns/partitions, each entry in theneutral learning mode can only describe what lin-guistics often refer to as ?elsewhere?
patterns (moreprecisely Type II patterns in the sense of Pertsova(2011)).
And the e[?max] component of each entry1The set of Boolean formulas is obtained by closing the setof feature values under the operations of conjunction, negation,and disjunction.by definition can only describe conjunctions.
Thereare additional restrictions on the above grammar: (i)the exceptions cannot have a wider distribution than?the rule of thumb?
(i.e., an entry el cannot corre-spond to a formula that does not pick out any in-stances), (ii) no loops in the statement of exceptionsis possible: that is, if an entry A is listed as an ex-ception to the entry B, then B cannot also be an ex-ception for A (a more complicated example of a loopinvolves a longer chain of entries).When learning a single category, there is onlyone entry in the grammar.
In this case arbitrarilycomplex categories are encoded as a complement ofsome conjunction with respect to a number of otherconjunctions (corresponding to instances).4.2 General descriptionThe general organization of the algorithm is as fol-lows.
Initially, each positive label is assumed to cor-respond to a single grammatical entry, and the ?maxcomponent of this entry is computed incrementallythrough an intersective generalization strategy thatextracts features invariant across all instances usedwith the same label.
When the grammar overgener-alizes by predicting two different labels for at leastone instance, exceptions are introduced.
The pro-cess of exception listing can also lead to overgener-alizations if exceptions are pointers to other entriesin the grammar.
When these overgeneralizations aredetected the algorithm creates another entry for thesame label.
This latter process can be viewed aspositing homophonous entries when learning form-meaning mappings, or as creating multiple ?clus-ters?
for a single category as in the prototype modelSUSTAIN (Love et al, 2004), and it corresponds toexplicitly positing a disjunctive rule.
Note that ifexceptions are not formulas for other labels, but in-dividual instances, then exception listing does notlead to overgeneralization and no sub-entries are in-troduced.
Thus, when learning a single category thelearner generalizes by using an intersective strategy,and then lists exceptions one-by-one as they are dis-covered in form of negative evidence.The problem of learning Boolean formulas isknown to be hard (Dalmau, 1999).
However, it isplausible that human learners employ an algorithmthat is not generally efficient, but can easily han-dle certain restricted types of formulas under certain75simple distributions of data.
(Subclasses of Booleanformulas are efficiently learnable in various learningframeworks (Kearns et al, 1994).)
If the learning al-gorithm can easily learn certain patterns (providingan explanation for what patterns and distributionscount as simple), we do not need to require that itbe in general efficient.4.3 Detailed descriptionFirst I describe how the grammar is updated in re-sponse to the data.
The update routine uses a strat-egy that in word-learning literature is called cross-situational inference.
This strategy incrementally fil-ters out features that change from one instance tothe next and keeps only those features that remaininvariant across the instances that have the same la-bel.
Obviously, this strategy leads to overgeneral-izations, but not if the category being learned is anaffirmation or conjunction.
This is because affirma-tions and conjunctions are defined by a single set offeature values which are shared by all instances of acategory (for proof see Pertsova (2007) p. 122).
Af-ter the entry for a given label has been updated, thealgorithm checks whether this entry subsumes or issubsumed by any other entry.
If so, this means thatthere is at least one instance for which several labelsare predicted to occur (there is competition amongthe entries).
The algorithm tries to resolve competi-tion by listing more specific entries as exceptions tothe more general ones.2 However there are cases inwhich this strategy will either not resolve the com-petition, or not resolve it correctly.
In particular,the intermediate entries that are in competition maybe such that neither subsumes the other.
Or afterupdating the entries using the intersective strategyone entry may be subsumed by another based on theinstances that have been seen so far, but not if wetake the whole set of instances into account.
Thesecases are detected when the predictions of the cur-rent grammar go against an observed stimulus (step11 in the function ?Update?
below).
Finally, excep-tion listing fails if it would lead to a ?loop?
(see sec-2This idea is familiar in linguistics from at least the times ofPa?nini.
In Distributed Morphology, it is referred to as the SubsetPrinciple for vocabulary insertion (Halle and Marantz, 1993).Similar principles are assumed in rule-ordering systems and inOT (i.e., more specific rules/constraints are typically orderedbefore the more general ones).tion 4.1).
The XOR pattern is an example of a simplepattern that will lead to a loop at some point duringlearning.
In general this happens whenever the dis-tribution of the two labels are intertwined in such away that neither can be stated as a complement ofthe invariant features of the other.The following function is used to add an excep-tion:AddException(expEntry, ruleEntry):1. if adding expEntry to ruleEntry[EX] leadsto a loop then FAIL2.
else add expEntry to ruleEntry[EX]The routine below is called within the main func-tion (presented later); it is used to update the gram-mar in response to an observed instance x with thelabel li (the index of the label is decided in the mainfunction).UpdateInput: G (current grammar); x (an observed in-stance), li (a label for this instance)Output: newG1: newG?
G2: if ?eli ?
newG then3: eli [?max]?
eli [?max] ?
x4: else5: add the entry eli to newG with valueseli [?max] = x; eli [EX] = {}.6: for all el?j ?
newG (el?j 6= eli) do7: if el?j subsumes eli then8: AddException(eli , el?j )9: else if eli subsumes el?j then10: AddException(el?j , eli)11: if ?el?j ?
newG (l?
6= l) such that x is consistentwith el?j then12: AddException(eli , el?j )Before turning to the main function of the algo-rithm, it is important to note that because a grammarmay contain several different entries for a single la-bel, this creates ambiguity for the learner.
Namely,in case a grammar contains more than one entry forsome label, say two A labels, the learner has to de-cide after observing a datum (x,A), which entry toupdate, eA1 or eA2 .
I assume that in such cases thelearner selects the entry that is most similar to the76current instance, where similarity is calculated as thenumber of features shared between x and eAi [?max](although other metrics of similarity could be ex-plored).Finally, I would like to note that the value of anentry el(x) can change even if the algorithm has notupdated this entry.
This is because the value of someother entry that is listed as an exception in el(x)may change.
This is one of the factors contributingto the difference between the neutral and the biasedlearning modes: if exceptions themselves are entriesfor other labels, the process of exception listing be-comes generalizing.MainInput: an instance-label pair (x, l), previous hy-pothesis G (initially set to an empty set)Output: newG (new hypothesis)1: set E to the list of existing entries for the label lin G2: k ?
|E|3: if E 6= {} then4: set elcurr to eli ?
E that is most similar to x5: E ?
E ?
elcurr6: else7: curr ?
k + 18: if l is positive and (?
?elcurr ?
G or x is notconsistent with elcurr ) then9: if update(G, x, lcurr) fails then10: goto step 311: else12: newG?
update(G, x, lcurr)13: else if l is negative and there is an entry e in Gconsistent with x (positive label was expected)then14: add x to e[EX] and minimize e[EX] to getnewGNotice that the loop triggered when update failsis guaranteed to terminate because when the list ofall entries for a label l is exhausted, a new entry isintroduced and this entry is guaranteed not to causeupdate to fail.This learner will succeed (in the limit) on mostpresentations of the data, but it may fail to convergeon certain patterns if the crucial piece of evidenceneeded to resolve competition is seen very early onand then never again (it is likely that a human learnerwould also not converge in such a case).This algorithm can be additionally augmented bya procedure similar to the selective attention mech-anism incorporated into several psychological mod-els of categorization to capture the fact that certainhard problems become easy if a subject can ignoreirrelevant features from the outset (Nosofsky et al,1994a).
One (not very efficient, but easy) way toincorporate selective attention into the above algo-rithm is as follows.
Initially set the number of rel-evant features k to 1.
Generate all subsets of F oflength k, select one such subset Fk and apply theabove learning algorithm assuming that the featurespace is Fk.
When processing a particular instance,ignore all of its features except those that are in Fk.If we discover two instances that have the same as-signment of features in Fk but that appear with twodifferent labels, this means that the selected set offeatures is not sufficient (recall that free variation isruled out).
Therefore, when this happens we canstart over with a new Fk.
If all sets of length khave been exhausted, increase k to k + 1 and re-peat.
As a result of this change, patterns definableby smaller number of features would generally beeasier to learn than those definable by larger numberof features.5 Predictions of the model for learningBoolean connectivesWe can evaluate predictions of this algorithm withrespect to category complexity in terms of the pro-portion of errors it predicts during learning, and interms of the computational load, roughly measuredas the number of required runs through the mainloop of the algorithm.
Recall that a single data-pointmay require several such runs if the update routinefails and a new sub-category has to be created.Below, I discuss how the predictions of this al-gorithm compare to the subjective complexity rank-ing found in Gottwald?s experiment.
First, considerthe relative complexity order in the neutral learningmode: AFF > AND/OR > XOR.In terms of errors, the AFF pattern is predictedto be learned without errors by the above algorithm(since the intersective strategy does not overgener-alize when learning conjunctive patterns).
Whenlearning an AND/OR pattern certain orders of datapresentation will lead to an intermediate overgener-77alization of the label associated with the disjunctivecategory to the rest of the instances.
This will hap-pen if the OR part of the pattern is processed beforethe AND part.
When learning an XOR pattern, thelearner is guaranteed to overgeneralize one of thelabels on any presentation of the data.
Let?s walkthrough the learning of the XOR pattern, repeatedbelow for convenience.f1 f1f2 A Bf2 B ASuppose for simplicity that the space of featuresincludes only f1 and f2, and that the first two ex-amples that the learner observes are (A, {f1, f2})and (A, {f1, f2}).
After intersecting {f1, f2}and {f1, f2} the learner will overgeneralize Ato the whole paradigm.
If the next example is(B, {f1, f2}), the learner will partially correct thisovergeneralization by assuming thatA occurs every-where except where B does (i.e., except {f1, f2}).But it will continue to incorrectly predict A in theremaining fourth cell that has not been seen yet.When B is observed in that cell, the learner will at-tempt to update the entry for B through the inter-section but this attempt will fail (because the en-try for B will subsume the entry for A, but wecan?t list A as an exception for B since B is al-ready listed as an exception for A).
Therefore, anew sub-entry for B, {f1, f2}, will be introducedand listed as another exception for A.
Thus, the fi-nal grammar will contain entries corresponding tothese formulas: B : (f1 ?
f2) ?
(f1 ?
f2) andA : ?
((f1 ?
f2) ?
(f1 ?
f2)).Overall the error pattern predicted by the learneris consistent with the order AFF > AND/OR >XOR.I now turn to a different measure of complexitybased on the number of computational steps neededto learn a pattern (where a single step is equated to asingle run of the main function).
Note that the speedof learning a particular pattern depends not only onthe learning algorithm but also on the distribution ofthe data.
Here I will consider two possible proba-bility distributions which are often used in catego-rization experiments.
In both distributions the stim-uli is organized in blocks.
In the first one (whichI call ?instance balanced?)
each block contains allpossible instances repeated once; in the second dis-tribution (?label balanced?)
each block contains allpossible instances with the minimum number of rep-etitions to insure equal numbers of each label.
Thedistributions differ only for those patterns that havean unequal number of positive/negative labels (e.g.,AND/OR).
Let us now look at the minimum andmaximum number of runs through the main loop ofthe algorithm required for convergence for each typeof pattern.
The minimum is computed by finding theshortest sequence of data that leads to convergenceand counting the number of runs on this data.
Themaximum is computed analogously by finding thelongest sequence of data.
The table below summa-rizes min.
and max.
number of runs for the featurespace with 3 binary features (8 possible instances)and for two distributions.Min Max Max(instance) (label)AFF 4 7 7AND/OR 4 8 11XOR 7 9 9Table 2: Complexity in the neutral modeThe difference between AFF and AND/OR inthe number of runs to convergence is more obvi-ous for the label balanced distribution.
On the otherhand, the difference between AND/OR and XOR isclearer for the instance balanced distribution.
Thisdifference is not expected to be large for the labelbalanced distribution, which is not consistent withGottwald?s experiment in which the stimuli werelabel balanced, and neutral XOR was significantlymore difficult to learn than any other condition.We now turn to the biased learning mode.
Here,the observed order of difficulty was: AFFb>ANDb> ORb, XORb.
In terms of errors, both AFFb andANDb are predicted to be learned with no errorssince both are conjunctive categories.
ORb is pre-dicted to involve a temporary overgeneralization ofthe positive label to the negative contexts.
The sameis true for XORb except that the proportion of errorswill be higher than for ORb (since the latter categoryhas fewer negative instances).The minimum and maximum number of runs re-quired to converge on the biased categories for twotypes of distributions (instance balanced and label78balanced) is given below.
Notice that the minimumnumbers are lower than in the previous table becausein the biased mode some categories can be learnedfrom positive examples alone.Min Max Max(instance) (label)AFFb 2 7 7ANDb 2 8 8ORb 4 16 22XORb 6 16 16Table 3: Complexity in the biased modeThe difference between affirmation and conjunc-tion is not very large which is not surprising (bothare conjunctive categories).
Again we see that thetwo types of distributions give us slightly differentpredictions.
While ANDb seems to be learned fasterthan ORb in both distributions, it is not clear whetherand to what extent ORb and XORb are on averagedifferent from each other in the label balanced dis-tribution.
Recall that Gottwald found no significantdifference between ORb and XORb (in fact numer-ically ORb was harder than XORb).
Interestingly,in a morphological analogue of Gottwald?s study inwhich the number of instances rather than labels wasbalanced, I found the opposite difference: ORb waseasier to learn than XORb (the number of people toreach learning criterion was 8 vs. 4 correspondingly)although the difference in error rates on the testingtrials was not significant (Pertsova, 2012).
Moretesting is needed to confirm whether the relative dif-ficulty of these two categories is reliably affected bythe type of distribution as predicted by the learner.3Finally, we look at the effect of labeling withineach condition.
In the AFF condition, Gottwaldfound no significant difference between neutral la-beling and biased labeling.
This could be due tothe fact that subjects were already almost at ceiling3Another possible reason for the fact that Gottwald did notfind a difference between ORb and XORb is this: if selective at-tention is used during learning, it will take longer for the learnerto realize that ORb requires the use of two features compared toXORb especially when the number of positive and negative ex-amples are balanced.
In particular, a one feature analysis ofORb can explain 5/6 of the data with label balanced stimuli,while a one feature analysis of XORb can only explain 1/2 ofthe data, so it will be quickly abandoned.in learning this pattern (median number of trials toconvergence for both conditions was ?
5).
In theAND/OR condition, Gottwald observed the interest-ing order ANDb > AND/OR > ORb.
This orderis also predicted by the current algorithm.
Namely,the neutral category AND/OR is predicted to beharder than ANDb because (1) ANDb requires lesscomputational resources (2) on some distributionsof data overgeneralization will occur when learn-ing an AND/OR pattern but not an ANDb category.The AND/OR > ORb order is also predicted and isparticularly pronounced for label balanced distribu-tion.
Since two labels are available when learningthe AND/OR pattern, the AND portion of the pat-tern can be learned quickly and subsequently listedas an exception for the OR portion (which becomesthe?elsewhere?
case).
On the other hand, whenlearning the ORb category, the conjunctive part ofthe pattern is initially ignored because it is not as-sociated with a label.
The learner only starts payingattention to negative instances when it overgeneral-izes.
For a similar reason, the biased XOR categoryis predicted to be harder to learn than the neutralXOR category.
This latter prediction is not consis-tent with Gottwald?s finding, who found XORn notjust harder than other categories but virtually impos-sible to learn: 6 out of 8 subjects in this conditionfailed to learn it after more than 256 trials.
In con-trast to this result (and in line with the predictionsof the present learner), Pertsova (2012) found thatthe neutral XOR condition was learned by 8 out of12 subjects on less than 64 trials compared to only 4out of 12 subjects in the biased XOR condition.To conclude this section, almost all complexityrankings discussed in this paper are predicted by theproposed algorithm.
This includes the difficult tomodel AND > OR ranking which obtains in the bi-ased learning mode.
The only exception is the neu-tral XOR pattern, which was really difficult to learnin Gottwald?s non-linguistic experiment (but not inPertsova?s morphological experiment), and which isnot predicted to be more difficult than biased XOR.Further empirical testing is needed to clarify the ef-fect of labeling within the XOR condition.79Type I Type II Type III Type IV Type V Type VIFigure 3: Shepard et.
al.
hierarchy6 Other predictionsAnother well-studied hierarchy of category com-plexity is the hierarchy of symmetric patterns (4 pos-itive and 4 negative instances) in the space of threebinary features originally established by Shepard et.al (1961).
These patterns are shown in figure 3 us-ing cubes to represent the three dimensional featurespace.Most studies find the following order of complex-ity for the Shepad patterns: I > II > III, IV, V > VI(Shepard et al, 1961; Nosofsky et al, 1994a; Love,2002; Smith et al, 2004).
However, a few studiesfind different rankings for some of these patterns.
Inparticular, Love (2002) finds IV > II with a switchto unsupervised training procedure.
Nosofsky andPalmeri (1996) find the numerical order I> IV> III> V > II > VI with intergral stimulus dimensions(feature values that are difficult to pay selective at-tention to independent of other features, e.g., hue,brightness, saturation).
More recently Moreton andPersova (2012) also found the order IV > III > V,VI (as well as I > II, III, > VI) in an unsupervisedphonotactics learning experiment.So, one might wonder what predictions does thepresent learner make with respect to these patterns.We already know that it predicts Type I (affirmation)to be easier than all other types.
For the rest of thepatterns the predictions in terms of speed of acquisi-tion are II > III > IV, V > VI in the neutral learningmode (similar to the typical findings).
In the biasedlearning mode, patterns II through VI are predictedto be learned roughly at the same speed (since all re-quire listing four exceptions).
If selective attentionis used, Type II will be the second easiest to learnafter Type I because it can be stated using only twofeatures.
However, based on the error rates, the orderof difficulty is predicted to be I > IV > III > V >II > VI (similar to the order found by Nosofsky andPalmeri (1996)).
No errors are ever made with TypeI.
The proportion of errors in other patterns dependson how closely the positive examples cluster to eachother.
For instance, when learning a Type VI pattern(in the biased mode) the learner?s grammar will becorrect on 6 out of 8 instances after seeing any twopositive examples (the same is not true for any otherpattern, although it is almost true for III).
After see-ing the next instance (depending on what it is andon the previous input) the accuracy of the grammarwill either stay the same, go up to 7/8, or go down to1/2.
But the latter event has the lowest probability.Note that this learner predicts non-monotonic behav-ior: it is possible that a later grammar is less accuratethan the previous grammar.
So, for a non-monotoniclearner the predictions based on the speed of acqui-sition and accuracy do not necessarily coincide.There are many differences across the categoriza-tion experiments that may be responsible for the dif-ferent rankings.
More work is needed to control forsuch differences and to pin down the sources for dif-ferent complexity results found with the patterns infigure 3.7 SummaryThe current proposal presents a unified account forlearning a single category and a set of categories par-titioning the stimuli space.
It is consistent with manypredictions about subjective complexity rankings ofsimple categories, including the ranking AND >OR, not predicted by most categorization models,and the difference between the biased and the neu-tral learning modes not previously modeled to myknowledge.ReferencesJerome S. Bruner, Jacqueline J. Goodnow, and George A.Austin.
1956.
A study of thinking.
John Wiley andSons, New York.80George N. Clements.
2003.
Feature economy in soundsystems.
Phonology, 20(3):287?333.Michael Cysouw.
2003.
The paradigmatic structure ofperson marking.
Oxford studies in typology and lin-guistic theory.
Oxford University Press, Oxford.V?
?ctor Dalmau.
1999.
Boolean formulas are hard tolearn for most gate bases.
In Osamu Watanabe andTakashi Yokomori, editors, Algorithmic Learning The-ory, volume 1720 of Lecture Notes in Computer Sci-ence, pages 301?312.
Springer Berlin / Heidelberg.Jacob Feldman.
2000.
Minimization of Boolean com-plexity in human concept learning.
Nature, 407:630?633.Mark A. Gluck and Gordon H. Bower.
1988. evaluatingan adaptive network model of human learning.
Jour-nal of memory and language, 27:166?195.Richard L. Gottwald.
1971.
Effects of response labels inconcept attainment.
Journal of Experimental Psychol-ogy, 91(1):30?33.Morris Halle and Alec Marantz.
1993.
Distributed mor-phology and the pieces of inflection.
In K. Hale andS.
J. Keyser, editors, The View from Building 20, pages111?176.
MIT Press, Cambridge, Mass.Michael Kearns and Umesh Vazirani.
1994.
An intro-duction to computational learning theory.
MIT Press,Cambridge, MA.Michael Kearns, Ming Li, and Leslie Valiant.
1994.Learning boolean formulas.
J. ACM, 41(6):1298?1328, November.Anthony Kroch.
1994.
Morphosyntactic variation.
InKatharine Beals et al, editor, Papers from the 30thregional meeting of the Chicago Linguistics Soci-ety: Parasession on variation and linguistic theory.Chicago Linguistics Society, Chicago.Bradley C. Love, Douglas L. Medin, and Todd M.Gureckis.
2004.
SUSTAIN: a network model of cat-egory learning.
Psychological Review, 111(2):309?332.Bradley C. Love.
2002.
Comparing supervised and unsu-pervised category learning.
Psychonomic Bulletin andReview, 9(4):829?835.Jeff Mielke.
2004.
The emergence of distinctive features.Ph.D.
thesis, Ohio State University.Elliott Moreton and Katya Pertsova.
2012.
Is phonolog-ical learning special?
Handout from a talk at the 48thMeeting of the Chicago Society of Linguistics, April.Ulrich Neisser and Paul Weene.
1962.
Hierarchies inconcept attainment.
Journal of Experimental Psychol-ogy, 64(6):640?645.Robert M. Nosofsky and Thomas J. Palmeri.
1996.Learning to classify integral-dimension stimuli.
Psy-chonomic Bulletin and Review, 3(2):222?226.Robert M. Nosofsky, Mark A. Gluck, Thomas J. Palmeri,Stephen C. McKinley, and Paul Gauthier.
1994a.Comparing models of rule-based classification learn-ing: a replication and extension of Shepard, Hov-land, and Jenkins (1961).
Memory and Cognition,22(3):352?369.Robert M. Nosofsky, Thomas J. Palmeri, and Stephen C.McKinley.
1994b.
Rule-plus-exception model of clas-sification learning.
Psychological Review, 101(1):53?79.Katya Pertsova.
2007.
Learning Form-Meaning Map-pings in the Presence of Homonymy.
Ph.D. thesis,UCLA.Katya Pertsova.
2011.
Grounding systematic syncretismin learning.
Linguistic Inquiry, 42(2):225?266.Katya Pertsova.
2012.
Logical complexity in morpho-logical learning.
In Proceedings of the 38th AnnualMeeting of the Berkeley Linguistics Society.Roger N. Shepard, C. L. Hovland, and H. M. Jenkins.1961.
Learning and memorization of classifications.Psychological Monographs, 75(13, Whole No.
517).Andrea Sims.
1996.
Minding the Gaps: inflectionaldefectiveness in a paradigmatic theory.
Ph.D. thesis,The Ohio State University.J.
David Smith, John Paul Minda, and David A. Wash-burn.
2004.
Category learning in rhesus monkeys:a study of the Shepard, Hovland, and Jenkins (1961)tasks.
Journal of Experimental Psychology: General,133(3):398?404.Leslie G. Valiant.
1984.
A theory of the learnable.
InProceedings of the sixteenth annual ACM symposiumon Theory of computing, STOC ?84, pages 436?445,New York, NY, USA.
ACM.Fei Xu.
2002.
The role of language in acquiring objectkind concepts in infancy.
Cognition, 85(3):223 ?
250.81
