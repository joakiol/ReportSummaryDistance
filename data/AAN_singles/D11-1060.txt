Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 648?658,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLarge-Scale Noun Compound InterpretationUsing Bootstrapping and the Web as a CorpusSu Nam KimComputer Science & Software EngineeringUniversity of MelbourneMelbourne, VIC 3010Australiasnkim@csse.unimelb.edu.auPreslav NakovDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nakov@comp.nus.edu.sgAbstractResponding to the need for semantic lexicalresources in natural language processing ap-plications, we examine methods to acquirenoun compounds (NCs), e.g., orange juice, to-gether with suitable fine-grained semantic in-terpretations, e.g., squeezed from, which aredirectly usable as paraphrases.
We employbootstrapping and web statistics, and utilizethe relationship between NCs and paraphras-ing patterns to jointly extract NCs and suchpatterns in multiple alternating iterations.
Inevaluation, we found that having one com-pound noun fixed yields both a higher numberof semantically interpreted NCs and improvedaccuracy due to stronger semantic restrictions.1 IntroductionNoun compounds (NCs) such as malaria mosquitoand colon cancer tumor suppressor protein are chal-lenging for text processing since the relationshipbetween the nouns they are composed of is im-plicit.
NCs are abundant in English and understand-ing their semantics is important in many natural lan-guage processing (NLP) applications.
For example,a question answering system might need to knowwhether protein acting as a tumor suppressor is agood paraphrase for tumor suppressor protein.
Sim-ilarly, a machine translation system facing the un-known noun compound Geneva headquarters mighttranslate it better if it could first paraphrase it asGeneva headquarters of the WTO.
Given a queryfor ?migraine treatment?, an information retrievalsystem could use paraphrasing verbs like relieve andprevent for query expansion and result ranking.Most work on noun compound interpretation hasfocused on two-word NCs.
There have been twogeneral lines of research: the first one derives the NCsemantics from the semantics of the nouns it is madeof (Rosario and Hearst, 2002; Moldovan et al, 2004;Kim and Baldwin, 2005; Girju, 2007; Se?aghdha,2009; Tratz and Hovy, 2010), while the second onemodels the relationship between the nouns directly(Vanderwende, 1994; Lapata, 2002; Kim and Bald-win, 2006; Nakov and Hearst, 2006; Nakov andHearst, 2008; Butnariu and Veale, 2008).In either case, the semantics of an NC is typi-cally expressed by an abstract relation like CAUSE(e.g., malaria mosquito), SOURCE (e.g., olive oil),or PURPOSE (e.g., migraine drug), coming from asmall fixed inventory.
Some researchers however,have argued for a more fine-grained, even infinite,inventory (Finin, 1980).
Verbs are particularly use-ful in this respect and can capture elements of thesemantics that the abstract relations cannot.
For ex-ample, while most NCs expressing MAKE, can beparaphrased by common patterns like be made ofand be composed of, some NCs allow more specificpatterns, e.g., be squeezed from for orange juice, andbe topped with for bacon pizza.Recently, the idea of using fine-grained para-phrasing verbs for NC semantics has been gain-ing popularity (Butnariu and Veale, 2008; Nakov,2008b); there has also been a related shared task atSemEval-2010 (Butnariu et al, 2010).
This interestis partly driven by practicality: verbs are directly us-able as paraphrases.
Still, abstract relations remaindominant since they offer a more natural generaliza-tion, which is useful for many NLP applications.648One good contribution to this debate would be adirect study of the relationship between fine-grainedand coarse-grained relations for NC interpretation.Unfortunately, the existing datasets do not allowthis since they are tied to one particular granular-ity; moreover, they only contain a few hundred NCs.Thus, our objective is to build a large-scale datasetof hundreds of thousands of NCs, each interpreted(1) by an abstract semantic relation and (2) by a setof paraphrasing verbs.
Having such a large datasetwould also help the overall advancement of the field.Since there is no universally accepted abstract re-lation inventory in NLP, and since we are interestedin NC semantics from both a theoretical and a prac-tical viewpoint, we chose the set of abstract relationsproposed in the theory of Levi (1978), which is dom-inant in theoretical linguistics and has been also usedin NLP (Nakov and Hearst, 2008).We use a two-step algorithm to jointly harvestNCs and patterns (verbs and prepositions) that in-terpret them for a given abstract relation.
First,we extract NCs using a small number of seed pat-terns from a given abstract relation.
Then, usingthe extracted NCs, we harvest more patterns.
Thisis repeated until no new NCs and patterns can beextracted or for a pre-specified number of itera-tions.
Our approach combines pattern-based extrac-tion and bootstrapping, which is novel for NC in-terpretation; however, such combinations have beenused in other areas, e.g., named entity recognition(Riloff and Jones, 1999; Thelen and Riloff, 2002;Curran et al, 2007; McIntosh and Curran, 2009).The remainder of the paper is organized as fol-lows: Section 2 gives an overview of related work,Section 3 motivates our semantic representation,Sections 4, 5, and 6 explain our method, dataset andexperiments, respectively, Section 7 discusses theresults, Section 8 provides error analysis, and Sec-tion 9 concludes with suggestions for future work.2 Related WorkAs we mentioned above, the implicit relation be-tween the two nouns forming a noun compound canoften be expressed overtly using verbal and prepo-sitional paraphrases.
For example, student loan is?loan given to a student?, while morning tea can beparaphrased as ?tea in the morning?.Thus, many NLP approaches to NC semanticshave used verbs and prepositions as a fine-grainedsemantic representation or as features when pre-dicting coarse-grained abstract relations.
For ex-ample, Vanderwende (1994) associated verbs ex-tracted from definitions in an online dictionary withabstract relations.
Lauer (1995) expressed NC se-mantics using eight prepositions.
Kim and Baldwin(2006) predicted abstract relations using verbs asfeatures.
Nakov and Hearst (2008) proposed a fine-grained NC interpretation using a distribution overWeb-derived verbs, prepositions and coordinatingconjunctions; they also used this distribution to pre-dict coarse-grained abstract relations.
Butnariu andVeale (2008) adopted a similar fine-grained verb-centered approach to NC semantics.
Using a dis-tribution over verbs as a semantic interpretation wasalso carried out in a recent challenge: SemEval-2010Task 9 (Butnariu et al, 2009; Butnariu et al, 2010).In noun compound interpretation, verbs andprepositions can be seen as patterns connecting thetwo nouns in a paraphrase.
Similar pattern-based ap-proaches have been popular in information extrac-tion and ontology learning.
For example, Hearst(1992) extracted hyponyms using patterns such asX, Y, and/or other Zs, where Z is a hypernym ofX and Y. Berland and Charniak (1999) used sim-ilar patterns to extract meronymy (part-whole) re-lations, e.g., parts/NNS of/IN wholes/NNS matchesbasements of buildings.
Unfortunately, matches arerare, which makes it difficult to build large semanticinventories.
In order to overcome data sparseness,pattern-based approaches are often combined withbootstrapping.
For example, Riloff and Jones (1999)used a multi-level bootstrapping algorithm to learnboth a semantic lexicon and extraction patterns, e.g.,owned by X extracts COMPANY and facilities in Xextracts LOCATION.
That is, they learned seman-tic lexicons using extraction patterns, and then, al-ternatively, they extracted new patterns using theselexicons.
They also introduced a second level ofbootstrapping to retain the most reliable examplesonly.
While the method enables the extraction oflarge lexicons, its quality degrades rapidly, whichmakes it impossible to run for too many iterations.Recently, Curran et al (2007) and McIntosh andCurran (2009) proposed ways to control degradationusing simultaneous learning and weighting.649Bootstrapping has been applied to noun com-pound extraction as well.
For example, Kim andBaldwin (2007) used it to produce a large numberof semantically interpreted noun compounds froma small number of seeds.
In each iteration, themethod replaced one component of an NC with itssynonyms, hypernyms and hyponyms to generate anew NC.
These new NCs were further filtered basedon their semantic similarity with the original NC.While the method acquired a large number of nouncompounds without significant semantic drifting, itsaccuracy degraded rapidly after each iteration.
Moreimportantly, the variation of the sense pairs was lim-ited since new NCs had to be semantically similar tothe original NCs.Recently, Kozareva and Hovy (2010) combinedpatterns and bootstrapping to learn the selectionalrestrictions for various semantic relations.
Theyused patterns involving the coordinating conjunctionand, e.g., ?
* and John fly to *?, and learned argu-ments such as Mary/Tom and France/New York.
Un-like in NC interpretation, it is not necessary for theirarguments to form an NC, e.g., Mary France andFrance Mary are not NCs.
Rather, they were in-terested in building a semantic ontology with a pre-defined set of semantic relations, similar to YAGO(Suchanek et al, 2007), where the pattern work forwould have arguments like a company/UNICEF.3 Semantic RepresentationInspired by (Finin, 1980), Nakov and Hearst (2006)and (Nakov, 2008b) proposed that NC semantics isbest expressible using paraphrases involving verbsand/or prepositions.
For example, bronze statue isa statue that is made of, is composed of, consists of,contains, is of, is, is handcrafted from, is dipped in,looks like bronze.
They further proposed that se-lecting one such paraphrase is not enough and thatmultiple paraphrases are needed for a fine-grainedrepresentation.
Finally, they observed that not allparaphrases are equally good (e.g., is made of isarguably better than looks like or is dipped in forMAKE), and thus proposed that the semantics of anoun compound should be expressed as a distribu-tion over multiple possible paraphrases.
This line ofresearch was later adopted by SemEval-2010 Task 9(Butnariu et al, 2010).It easily follows that the semantics of abstract re-lations such as MAKE that can hold between thenouns in an NC can be represented in the same way:as a distribution over paraphrasing verbs and prepo-sitions.
Note, however, that some NCs are para-phrasable by more specific verbs that do not nec-essarily support the target abstract relation.
For ex-ample, malaria mosquito, which expresses CAUSE,can be paraphrased using verbs like carry, which donot imply direct causation.
Thus, while we will befocusing on extracting NCs for a particular abstractrelation, we are interested in building semantic rep-resentations that are specific for these NCs and donot necessarily apply to all instances of that relation.Traditionally, the semantics of a noun compoundhave been represented as an abstract relation drawnfrom a small closed set.
Unfortunately, no such set isuniversally accepted, and mapping between sets hasproven challenging (Girju et al, 2005).
Moreover,being both abstract and limited, such sets captureonly part of the semantics; often multiple meaningsare possible, and sometimes none of the pre-definedones suits a given example.
Finally, it is unclearhow useful these sets are since researchers have of-ten fallen short of demonstrating practical uses.Arguably, verbs have more expressive power andare more suitable for semantic representation: thereis an infinite number of them (Downing, 1977), andthey can capture fine-grained aspects of the mean-ing.
For example, while both wrinkle treatment andmigraine treatment express the same abstract rela-tion TREATMENT-FOR-DISEASE, fine-grained dif-ferences can be revealed using verbs, e.g., smoothcan paraphrase the former, but not the latter.In many theories, verbs play an important role inNC derivation (Levi, 1978).
Moreover, speakers of-ten use verbs to make the hidden relation betweenthe noun in a noun compound overt.
This allows forsimple extraction and for straightforward use in NLPtasks like textual entailment (Tatu and Moldovan,2005) and machine translation (Nakov, 2008a).Finally, a single verb is often not enough, andthe meaning is better approximated by a collectionof verbs.
For example, while malaria mosquito ex-presses CAUSE (and is paraphrasable using cause),further aspects of the meaning can be captured withmore verbs, e.g., carry, spread, be responsible for,be infected with, transmit, pass on, etc.6504 MethodWe harvest noun compounds expressing some targetabstract semantic relation (in the experiments below,this is Levi?s MAKE2), starting from a small numberof initial seed patterns: paraphrasing verbs and/orprepositions.
Optionally, we might also be givena small number of noun compounds that instanti-ate the target abstract relation.
We then learn morenoun compounds and patterns for the relation by al-ternating between the following two bootstrappingsteps, using the Web as a corpus.
First, we extractmore noun compounds that are paraphrasable withthe available patterns (see Section 4.1).
We thenlook for new patterns that can paraphrase the newly-extracted noun compounds (see Section 4.2).
Thesetwo steps are repeated until no new noun compoundscan be extracted or until a pre-determined number ofiterations has been reached.
A schematic descriptionof the algorithm is shown in Figure 1.
(+ H/M of NCs)PatternsQuery GenerationNC ExtractionPatternFilteringRulesFilteringNCRulesrepeatcollected NCs^Query Generationw/ NCs^collected Patternsstopif newNCs = 0orIteration limit exceededSnippet by Yahoo!Pattern ExtractionSnippet by Yahoo!Figure 1: Our bootstrapping algorithm.4.1 Bootstrapping Step 1: Noun CompoundExtractionGiven a list of patterns (verbs and/or prepositions),we mine the Web to extract noun compounds thatmatch these patterns.
We experiment with the fol-lowing three bootstrapping strategies for this step:?
Loose bootstrapping uses the available pat-terns and imposes no further restrictions.?
Strict bootstrapping requires that, in additionto the patterns themselves, some noun com-pounds matching each pattern be made avail-able as well.
A pattern is only instantiated inthe context of either the head or the modifier ofa noun compound that is known to match it.?
NC-only strict bootstrapping is a stricter ver-sion of strict bootstrapping, where the list ofpatterns is limited to the initial seeds.Below we describe each of the sub-steps of the NCextraction process: query generation, snippet har-vesting, and noun compound acquisition & filtering.4.1.1 Query GenerationWe generate generalized exact-phrase queries tobe used in a Web search engine (we use Yahoo!
):"* that PATTERN *" (loose)"HEAD that PATTERN *" (strict)"* that PATTERN MOD" (strict)where PATTERN is an inflected form of a verb, MODand HEAD are inflected forms the modifier and thehead of a noun compound that is paraphrasable bythe pattern, that is the word that, and * is thesearch engine?s star operator.We use the first pattern for loose bootstrappingand the other two for both strict bootstrapping andNC-only strict bootstrapping.Note that the above queries are generalizations ofthe actual queries we use against the search engine.In order to instantiate these generalizations, we fur-ther generate the possible inflections for the verbsand the nouns involved.
For nouns, we produce sin-gular and plural forms, while for verbs, we vary notonly the number (singular and plural), but also thetense (we allow present, past, and present perfect).When inflecting verbs, we distinguish between ac-tive verb forms like consist of and passive ones likebe made from and we treat them accordingly.
Over-all, in the case of loose bootstrapping, we generateabout 14 and 20 queries per pattern for active andpassive patterns, respectively, while for strict boot-strapping and NC-only strict bootstrapping, the in-stantiations yield about 28 and 40 queries for activeand passive patterns, respectively.651For example, given the seed be made of, we couldgenerate "* that were made of *".
If weare further given the NC orange juice, we could alsoproduce "juice that was made of *" and"* that is made of oranges".4.1.2 Snippet ExtractionWe execute the above-described instantiations ofthe generalized queries against a search engine asexact phrase queries, and, for each one, we collectthe snippets for the top 1,000 returned results.4.1.3 NC Extraction and FilteringNext, we process the snippets returned by thesearch engine and we acquire potential noun com-pounds from them.
Then, in each snippet, we lookfor an instantiation of the pattern used in the queryand we try to extract suitable noun(s) that occupy theposition(s) of the *.For loose bootstrapping, we extract two nouns,one from each end of the matched pattern, whilefor strict bootstrapping and for NC-only strict boot-strapping, we only extract one noun, either preced-ing or following the pattern, since the other nounis already fixed.
We then lemmatize the extractednoun(s) and we form NC candidates from the twoarguments of the instantiated pattern, taking into ac-count whether the pattern is active or passive.Due to the vast number of snippets we have toprocess, we decided not to use a syntactic parser or apart-of-speech (POS) tagger1; thus, we use heuristicrules instead.
We extract ?phrases?
using simple in-dicators such as punctuation (e.g., comma, period),coordinating conjunctions2 (e.g., and, or), preposi-tions (e.g., at, of, from), subordinating conjunctions(e.g., because, since, although), and relative pro-nouns (e.g., that, which, who).
We then extract thenouns from these phrases, we lemmatize them usingWordNet, and we form a list of NC candidates.While the above heuristics work reasonably wellin practice, we perform some further filtering, re-moving all NC candidates for which one or more ofthe following conditions are met:1In fact, POS taggers and parsers are unreliable for Web-derived snippets, which often represent parts of sentences andcontain errors in spelling, capitalization and punctuation.2Note that filtering the arguments using such indicators indi-rectly subsumes the pattern "X PATTERN Y and" proposedin (Kozareva and Hovy, 2010).1. the candidate NC is one of the seed examplesor has been extracted on a previous iteration;2. the head and the modifier are the same;3. the head or the modifier are not both listed asnouns in WordNet (Fellbaum, 1998);4. the candidate NC occurs less than 100 times inthe Google Web 1T 5-gram corpus3;5. the NC is extracted less than N times (we tried5 and 10) in the context of the pattern for allinstantiations of the pattern.4.2 Bootstrapping Step 2: Pattern ExtractionThis is the second step of our bootstrapping algo-rithm as shown on Figure 1.
Given a list of nouncompounds, we mine the Web to extract patterns:verbs and/or prepositions that can paraphrase eachNC.
The idea is to turn the NC?s pre-modifier intoa post-modifying relative clause and to collect theverbs and prepositions that are used in such clauses.Below we describe each of the sub-steps of the NCextraction process: query generation, snippet har-vesting, and NC extraction & filtering.4.2.1 Query GenerationThe process of extraction starts with exact-phrasequeries issued against a Web search engine (againYahoo!)
using the following generalized pattern:"HEAD THAT?
* MOD"where MOD and HEAD are inflected forms of NC?smodifier and head, respectively, THAT?
stands forthat, which, who or the empty string, and * standsfor 1-6 instances of search engine?s star operator.For example, given orange juice, we could gen-erate queries like "juice that * oranges","juices which * * * * * * oranges",and "juices * * * orange".4.2.2 Snippet ExtractionThe same as in Section 4.1.2 above.3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T136524.2.3 Pattern Extraction and FilteringWe split the extracted snippets into sentences, andfilter out all incomplete ones and those that do notcontain (a possibly inflected version of) the targetnouns.
We further make sure that the word sequencefollowing the second mentioned target noun is non-empty and contains at least one non-noun, thus en-suring the snippet includes the entire noun phrase.We then perform shallow parsing, and we extract allverb forms, and the following preposition, betweenthe target nouns.
We allow for adjectives and partici-ples to fall between the verb and the preposition butnot nouns; we further ignore modal verbs and aux-iliaries, but we retain the passive be, and we makesure there is exactly one verb phrase between the tar-get nouns.
Finally, we lemmatize the verbs to formthe patterns candidates, and we apply the followingpattern selection rules:1. we filter out all patterns that were provided asinitial seeds or were extracted previously;2. we select the top 20 most frequent patterns;3. we filter out all patterns that were extracted lessthan N times (we tried 5 and 10) and with lessthanM NCs per pattern (we tried 20 and 50).5 Target Relation and Seed ExamplesAs we mentioned above, we use the inventory ofabstract relations proposed in the popular theoreti-cal linguistics theory of Levi (1978).
In this theory,noun compounds are derived from underlying rel-ative clauses or noun phrase complement construc-tions by means of two general processes: predicatedeletion and predicate nominalization.
Given a two-argument predicate, predicate deletion removes thatpredicate, but retains its arguments to form an NC,e.g., pie made of apples ?
apple pie.
In contrast,predicate nominalization creates an NC whose headis a nominalization of the underlying predicate andwhose modifier is either the subject or the object ofthat predicate, e.g., The President refused GeneralMacArthur?s request.
?
presidential refusal.According to Levi, predicate deletion can be ap-plied to abstract predicates, whose semantics can beroughly approximated using five paraphrasing verbs(CAUSE, HAVE, MAKE, USE, and BE) and fourprepositions (IN, FOR, FROM, and ABOUT).Typically, in predicate deletion, the modifier isderived from the object of the underlying relativeclause; however, the first three verbs also allow forit to be derived from the subject.
Levi expresses thedistinction using indexes.
For example, music box isMAKE1 (object-derived), i.e., the box makes music,while chocolate bar is MAKE2 (subject-derived),i.e., the bar is made of chocolate (note the passive).Due to time constraints, we focused on one re-lation of Levi?s, MAKE2, which is among the mostfrequent relations an NC can express and is presentin some form in many relation inventories (Warren,1978; Barker and Szpakowicz, 1998; Rosario andHearst, 2001; Nastase and Szpakowicz, 2003; Girjuet al, 2005; Girju et al, 2007; Girju et al, 2009;Hendrickx et al, 2010; Tratz and Hovy, 2010).In Levi?s theory, MAKE2 means that the head ofthe noun compound is made up of or is a product ofits modifier.
There are three subtypes of this relation(we do not attempt to distinguish between them):(a) the modifier is a unit and the head is a configu-ration, e.g., root system;(b) the modifier represents a material and the headis a mass or an artefact, e.g., chocolate bar;(c) the head represents human collectives andthe modifier specifies their membership, e.g.,worker teams.There are 20 instances of MAKE2 in the appendixof (Levi, 1978), and we use them all as seed NCs.As seed patterns, we use a subset of the human-proposed paraphrasing verbs and prepositions cor-responding to these 20 NCs in the dataset in (Nakov,2008b), where each NC is paraphrased by 25-30 an-notators.
For example, for chocolate bar, we findthe following list of verbs (the number of annotatorswho proposed each verb is shown in parentheses):be made of (16), contain (16), be made from(10), be composed of (7), taste like (7), con-sist of (5), be (3), have (2), melt into (2), bemanufactured from (2), be formed from (2),smell of (2), be flavored with (1), sell (1), tasteof (1), be constituted by (1), incorporate (1),serve (1), contain (1), store (1), be made with(1), be solidified from (1), be created from (1),be flavoured with (1), be comprised of (1).653Seed NCs: bronze statue, cable network, candy cigarette, chocolate bar, concrete desert, copper coin, daisy chain, glass eye,immigrant minority, mountain range, paper money, plastic toy, sand dune, steel helmet, stone tool, student committee,sugar cube, warrior castle, water drop, worker teamSeed patterns: be composed of, be comprised of, be inhabited by, be lived in by, be made from, be made of, be made up of,be manufactured from, be printed on, consist of, contain, have, house, include, involve, look like, resemble, taste likeTable 1: Our seed examples: 20 noun compounds and 18 verb patterns.As we can see, the most frequent patterns are ofhighest quality, e.g., be made of (16), while the lessfrequent ones can be wrong, e.g., serve (1).
There-fore, we filtered out all verbs that were proposed lessthan five times with the 20 seed NCs.
We further re-moved the verb be, which is too general, thus endingup with 18 seed patterns.
Note that some patternscan paraphrase multiple NCs: the total number ofseed NC-pattern pairs is 84.The seed NCs and patterns are shown in Table 1.While some patterns, e.g., taste like do not expressthe target relation MAKE2, we kept them anywaysince they were proposed by several human anno-tators and since they do express the fine-grained se-mantics of some particular instances of that relation;thus, we thought they might be useful, even for thegeneral relation.
For example, taste like has beenproposed 8 times for candy cigarette, 7 times forchocolate bar, and 2 times for sugar cube, and thusit clearly correlates well with some seed examples,even if it does not express MAKE2 in general.6 Experiments and EvaluationUsing the NCs and patterns in Table 1 as initialseeds, we ran our algorithm for three iterations ofloose bootstrapping and strict bootstrapping, andfor two iterations of NC-only strict bootstrapping.We only performed up to three iterations becauseof the huge number of noun compounds extractedfor NC-only strict bootstrapping (which we only ranfor two iterations) and because of the low number ofnew NCs extracted by loose bootstrapping on itera-tion 3.
While we could have run strict bootstrappingfor more iterations, we opted for a comparable num-ber of iterations for all three methods.Examples of noun compounds that we have ex-tracted are bronze bell (be made of, be made from)and child team (be composed of, include).
Exam-ple patterns are be filled with (cotton bag, water cup)and use (water sculpture, wood statue).Limits Extracted & Retained(see 4.2.3) NCs Patterns Patt.+NCLoose BootstrappingN=5,M=50 1,662 / 61.67 12 / 65.83 1,337N=10,M=20 590 / 61.52 9 / 65.56 316Strict BootstrappingN=5,M=50 25,375 / 67.42 16 / 71.43 9,760N=10,M=20 16,090 / 68.27 16 / 78.98 5,026NC-only Strict BootstrappingN=5 205,459 / 69.59 ?
?N=10 100,550 / 70.43 ?
?Table 2: Total number and accuracy in % for NCs, pat-terns and NC-pattern pairs extracted and retained for eachof the three methods over all iterations.Tables 2 and 3 show the overall results.
As wementioned in section 4.2.3, at each iteration, we fil-tered out all patterns that were extracted less thanNtimes or with less than M NCs.
Note that we onlyused the 10 most frequent NCs per pattern as NCseeds for NC extraction in the next iteration of strictbootstrapping and NC-only strict bootstrapping.
Ta-ble 3 shows the results for two value combinationsof (N ;M ): (5;50) and (10;20).
Note also that ifsome NC was extracted by several different patterns,it was only counted once.
Patterns are subject toparticular NCs, and thus we show (1) the numberof patterns extracted with all NCs, i.e., unique NC-pattern pairs, (2) the accuracy of these pairs,4 and(3) the number of unique patterns retained after fil-tering, which will be used to extract new noun com-pounds on the second step of the current iteration.4One of the reviewers suggested that evaluating the accuracyof NC-pattern pairs could potentially conceal some of the driftof our algorithm.
For example, while water cup / be filled withis a correct NC-pattern pair, water cup is incorrect for MAKE2;it is probably an instance of Levi?s FOR.
Thus, the same boot-strapping technique evaluated against a fixed set of semantic re-lations (which is the more traditional approach) could arguablyshow bootstrapping going ?off the rails?
more quickly than whatwe observe here.
However, our goal, as stated in Section 3, is tofind NC-specific paraphrases, and our evaluation methodologyis more adequate with respect to this goal.654Limits Seeds Iteration 1 Iteration 2 Iteration 3(see 4.2.3) Patt.
NCs Patt.
NCs Patterns NCs Patterns NCsLoose BootstrappingN=5,M=50 ?
18 ?
1,144 / 63.11 1,136 / 64.44 / 9 390 / 58.72 201 / 70.00 / 3 128 / 57.03N=10,M=20 ?
18 ?
502 / 61.55 294 / 62.50 / 8 78 / 60.26 22 / 90.00 / 1 10 / 70.00Strict BootstrappingN=5,M=50 20 18 ?
7,011 / 70.65 5,312 / 74.00 / 10 11,214 / 67.15 4,448 / 60.00 / 6 7,150 / 64.69N=10,M=20 20 18 ?
4,826 / 71.26 2,838 / 79.38 / 10 7,371 / 67.26 2,188 / 78.33 / 6 3,893 / 66.48NC-only Strict BootstrappingN=5 20 18 ?
7,011 / 70.65 ?
198,448 / 69.55 ?
?N=10 20 18 ?
4,826 / 71.26 ?
95,524 / 70.59 ?
?Table 3: Evaluation results for up to three iterations.
For NCs, we show the number of unique NCs extracted andtheir accuracy in %.
For patterns, we show the number of unique NC-pattern pairs extracted, their accuracy in %, andthe number of unique patterns retained and used to extract NCs on the second step of the current iteration.
The firstcolumn shows the pattern filtering thresholds used (see Section 4.2.3 for details).The above accuracies were calculated based onhuman judgments by an experienced, well-trainedannotator.
We also hired a second annotator for asmall subset of the examples.For NCs, the first annotator judged whether eachNC is an instance of MAKE2.
All NCs were judged,except for iteration 2 of NC-only strict bootstrap-ping, where their number was prohibitively high andonly the most frequent noun compounds extractedfor each modifier and for each head were checked:9,004 NCs for N=5 and 4,262 NCs for N=10.For patterns, our first annotator judged the cor-rectness of the unique NC-pattern pairs, i.e., whetherthe NC is paraphrasable with the target pattern.Given the large number of NC-pattern pairs, the an-notator only judged patterns with their top 10 mostfrequent NCs.
For example, if there were 5 patternsextracted, then the NC-pattern pairs to be judgedwould be no more than 5 ?
10 = 50.Our second annotator judged 340 random exam-ples: 100 NCs and 20 patterns with their top 10 NCsfor each iteration.
The Cohen?s kappa (Cohen, 1960)between the two annotators is .66 (85% initial agree-ment), which corresponds to substantial agreement(Landis and Koch, 1977).7 DiscussionTables 2 and 3 show that fixing one of the two nounsin the pattern, as in strict bootstrapping and NC-onlystrict bootstrapping, yields significantly higher ac-curacy (?2 test) for both NC and NC-pattern pairextraction compared to loose bootstrapping.The accuracy for NC-only strict bootstrapping isa bit higher than for strict bootstrapping, but the ac-tual differences are probably smaller since the eval-uation of the former on iteration 2 was done for themost frequent NCs, which are more accurate.Note that the number of extracted NCs is muchhigher with the strict methods because of the highernumber of possible instantiations of the generalizedquery patterns.
For NC-only strict bootstrapping,the number of extracted NCs grows exponentiallysince the number of patterns does not diminish asin the other two methods.
The number of extractedpatterns is similar for the different methods since weselect no more than 20 of them per iteration.Overall, the accuracy for all methods decreasesfrom one iteration to the next since errors accumu-late; still, the degradation is slow.
Note also the ex-ception of loose bootstrapping on iteration 3.Comparing the results for N=5 and N=10, wecan see that, for all three methods, using the latteryields a sizable drop in the number of extracted NCsand NC-pattern pairs; it also tends to yield a slightlyimproved accuracy.
Note, however, the exceptionof loose bootstrapping for the first two iterations,where the less restrictive N=5 is more accurate.As a comparison, we implemented the methodof Kim and Baldwin (2007), which generates newsemantically interpreted NCs by replacing eitherthe head or the modifier of a seed NC with suit-able synonyms, hypernyms and sister words fromWordNet, followed by similarity filtering usingWordNet::Similarity (Pedersen et al, 2004).655Rep.
Iter.
1 Iter.
2 Iter.
3 AllSyn.
11/81.81 3/66.67 0 14/78.57Hyp.
27/85.19 35/77.14 33/66.67 95/75.79Sis.
381/82.05 1,736/69.33 17/52.94 2,134/75.12All 419/82.58 1,774/71.68 50/62.00 2,243/75.47Table 4: Number of extracted noun compounds and ac-curacy in % for the method of Kim and Baldwin (2007).The abbreviations Syn., Hyp., and Sis.
indicate using syn-onyms, hypernyms, and sister words, respectively.The results for three bootstrapping iterations us-ing the same list of 20 initial seed NCs as in our pre-vious experiments, are shown in Table 4.
We can seethat the overall accuracy of their method is slightlybetter than ours.
Note, however, that our method ac-quired a much larger number of NCs, while allow-ing more variety in the NC semantics.
Moreover, foreach extracted noun compound, we also generated alist of fine-grained paraphrasing verbs.8 Error AnalysisBelow we analyze the errors of our method.Many problems were due to wrong POS assign-ment.
For example, on Step 2, because of the omis-sion of that in ?the statue has such high quality gold(that) demand is ...?, demand was tagged as a nounand thus extracted as an NCmodifier instead of gold.The problem also arose on Step 1, where we usedWordNet to check whether the NC candidates werecomposed of two nouns.
Since words like clear,friendly, and single are listed in WordNet as nouns(which is possible in some contexts), we extractedwrong NCs such as clear cube, friendly team, andsingle chain.
There were similar issues with verb-particle constructions since some particles can beused as nouns as well, e.g., give back, break down.Some errors were due to semantic transparencyissues, where the syntactic and the semantic head ofa target NP were mismatched (Fillmore et al, 2002;Fontenelle, 1999).
For example, from the sentence?This wine is made from a range of white grapes.
?,we would extract range rather than grapes as the po-tential modifier of wine.In some cases, the NC-pattern pair was correct,but the NC did not express the target relation, e.g.,while contain is a good paraphrase for toy box, thenoun compound itself is not an instance of MAKE2.There were also cases where the pair of extractednouns did not make a good NC, e.g., worker work oryear toy.
Note that this is despite our checking thatthe candidate NC occurred at least 100 times in theGoogle Web 1T 5-gram corpus (see Section 4.1.3).We hypothesized that such bad NCs would tend tohave a low collocation strength.
We tested this hy-pothesis using the Dice coefficient, calculated usingthe Google Web 1T 5-gram corpus.
Figure 2 shows aplot of the NC accuracy vs. collocation strength forstrict bootstrapping with N=5, M=50 for all threeiterations (the results for the other experiments showa similar trend).
We can see that the accuracy im-proves slightly as the collocation strength increases:compare the left and the right ends of the graph (theresults are mixed in the middle though).?Acc.i1??Acc.i2?
?Acc.i3?4050607080901000.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Figure 2: NC accuracy vs. collocation strength.9 Conclusion and Future WorkWe have presented a framework for building a verylarge dataset of noun compounds expressing a giventarget abstract semantic relation.
For each extractednoun compound, we generated a corresponding fine-grained semantic interpretation: a frequency distri-bution over suitable paraphrasing verbs.In future work, we plan to apply our frame-work to the remaining relations in the inventory ofLevi (1978), and to release the resulting dataset tothe research community.
We believe that having alarge-scale dataset of noun compounds interpretedwith both fine- and coarse-grained semantic rela-tions would be an important contribution to the de-bate about which representation is preferable for dif-ferent tasks.
It should also help the overall advance-ment of the field of noun compound interpretation.656AcknowledgmentsThis research is partially supported (for the sec-ond author) by the SmartBook project, funded bythe Bulgarian National Science Fund under GrantD002-111/15.12.2008.We would like to thank the anonymous reviewersfor their detailed and constructive comments, whichhave helped us improve the paper.ReferencesKen Barker and Stan Szpakowicz.
1998.
Semi-automaticrecognition of noun modifier relationships.
In Pro-ceedings of the 17th International Conference onComputational Linguistics, pages 96?102.Matthew Berland and Eugene Charniak.
1999.
Findingparts in very large corpora.
In Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?99, pages 57?64.Cristina Butnariu and Tony Veale.
2008.
A concept-centered approach to noun-compound interpretation.In Proceedings of the 22nd International Conferenceon Computational Linguistics, COLING ?08, pages81?88.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2009.
Semeval-2010 task 9: The interpretation ofnoun compounds using paraphrasing verbs and prepo-sitions.
In Proceedings of the Workshop on SemanticEvaluations: Recent Achievements and Future Direc-tions, SEW ?09, pages 100?105.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2010.
SemEval-2010 task 9: The interpretation ofnoun compounds using paraphrasing verbs and prepo-sitions.
In Proceedings of the 5th International Work-shop on Semantic Evaluation, SemEval-2, pages 39?44.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 1(20):37?46.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual exclu-sion bootstrapping.
In Proceedings of the Conferenceof the Pacific Association for Computational Linguis-tics, PACLING ?07, pages 172?180.Pamela Downing.
1977.
On the creation and use of En-glish compound nouns.
Language, 53:810?842.Christiane Fellbaum, editor.
1998.
WordNet, An Elec-tronic Lexical Database.
MIT Press, Cambridge, Mas-sachusetts, USA.Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.2002.
Seeing arguments through transparent struc-tures.
In Proceedings of the Third International Con-ference on Language Resources and Evaluation, vol-ume III of LREC ?02, pages 787?791.Timothy Wilking Finin.
1980.
The semantic interpre-tation of compound nominals.
Ph.D. thesis, Univer-sity of Illinois at Urbana-Champaign, Champaign, IL,USA.
AAI8026491.Thierry Fontenelle.
1999.
Semantic resources for wordsense disambiguation: a sine qua non.
Linguistica eFilologia, 9:25?41.Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of noun compounds.Computer Speech and Language, 19(44):479?496.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semantic rela-tions between nominals.
In Proceedings of the 4th Se-mantic Evaluation Workshop, SemEval-1, pages 13?18.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2009.Classification of semantic relations between nominals.Language Resources and Evaluation, 43(2):105?121.Roxana Girju.
2007.
Improving the interpretation ofnoun phrases with cross-linguistic information.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, ACL ?07, pages568?575.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe Fourteenth International Conference on Computa-tional Linguistics, COLING ?92, pages 539?545.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid O?
Se?aghdha, Sebastian Pado?, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakow-icz.
2010.
SemEval-2010 task 8: Multi-way classifi-cation of semantic relations between pairs of nominals.In Proceedings of the 5th International Workshop onSemantic Evaluation, SemEval-2, pages 33?38.Su Nam Kim and Timothy Baldwin.
2005.
Automaticinterpretation of compound nouns using WordNet sim-ilarity.
In Proceedings of 2nd International Joint Con-ference on Natural Language Processing, IJCNLP ?05,pages 945?956.Su Nam Kim and Timothy Baldwin.
2006.
Interpret-ing semantic relations in noun compounds via verb se-mantics.
In Proceedings of the 44th Annual Meetingof the Association for Computational Linguistics and21st International Conference on Computational Lin-guistics, ACL-COLING ?06, pages 491?498.657Su Nam Kim and Timothy Baldwin.
2007.
Interpretingnoun compounds using bootstrapping and sense col-location.
In Proceedings of Conference of the PacificAssociation for Computational Linguistics, PACLING?07, pages 129?136.Zornitsa Kozareva and Eduard Hovy.
2010.
Learningarguments and supertypes of semantic relations usingrecursive patterns.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, ACL ?10, pages 1482?1491.Richard J. Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Maria Lapata.
2002.
The disambiguation of nominaliza-tions.
Computational Linguistics, 28(3):357?388.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Noun Compounds.
Ph.D.thesis, Dept.
of Computing, Macquarie University,Australia.Judith Levi.
1978.
The Syntax and Semantics of ComplexNominals.
Academic Press, New York, USA.Tara McIntosh and James Curran.
2009.
Reducing se-mantic drift with bagging and distributional similar-ity.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, ACL-IJCNLP ?09, pages 396?404.Dan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the se-mantic classification of noun phrases.
In Proceedingsof the HLT-NAACL?04 Workshop on ComputationalLexical Semantics, pages 60?67.Preslav Nakov and Marti A. Hearst.
2006.
Using verbsto characterize noun-noun relations.
In Proceedingsof the 12th International Conference on Artificial In-telligence: Methodology, Systems, and Applications,AIMSA ?06, pages 233?244.Preslav Nakov and Marti Hearst.
2008.
Solving rela-tional similarity problems using the web as a corpus.In Proceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics, ACL ?08, pages452?460.Preslav Nakov.
2008a.
Improved Statistical MachineTranslation Using Monolingual Paraphrases.
In Pro-ceedings of the 18th European Conference on ArtificialIntelligence, ECAI ?08, pages 338?342.Preslav Nakov.
2008b.
Noun compound interpretationusing paraphrasing verbs: Feasibility study.
In Pro-ceedings of the 13th international conference on Arti-ficial Intelligence: Methodology, Systems, and Appli-cations, AIMSA ?08, pages 103?117.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Proceedings ofthe 5th International Workshop on Computational Se-mantics, pages 285?301.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity - measuring the relat-edness of concepts.
In Proceedings of the NineteenthNational Conference on Artificial Intelligence, AAAI?04, pages 1024?1025.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence, AAAI ?99, pages474?479.Barbara Rosario and Marti Hearst.
2001.
Classify-ing the semantic relations in noun compounds via adomain-specific lexical hierarchy.
In Proceedings ofthe 6th Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?01, pages 82?90.Barbara Rosario and Marti Hearst.
2002.
The descentof hierarchy, and selection in relational semantics.
InProceedings of Annual Meeting of the Association forComputational Linguistics, ACL ?02, pages 247?254.Diarmuid O?
Se?aghdha.
2009.
Semantic classificationwith WordNet kernels.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, Companion Volume: ShortPapers, NAACL ?09, pages 237?240.Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.2007.
YAGO: A core of semantic knowledge - unify-ing WordNet and Wikipedia.
In Proceedings of 16thInternational World Wide Web Conference, WWW?07, pages 697?706.Marta Tatu and Dan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
In Proceed-ings of Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, HLT-EMNLP ?05, pages 371?378.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?02, pages 214?221.Stephen Tratz and Eduard Hovy.
2010.
A taxonomy,dataset, and classifier for automatic noun compoundinterpretation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, ACL ?10, pages 678?687.Lucy Vanderwende.
1994.
Algorithm for automatic in-terpretation of noun sequences.
In Proceedings of the15th Conference on Computational linguistics, pages782?788.Beatrice Warren.
1978.
Semantic patterns of noun-nouncompounds.
In Gothenburg Studies in English 41,Goteburg, Acta Universtatis Gothoburgensis.658
