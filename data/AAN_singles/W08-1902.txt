Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9?17Manchester, August 2008Lexical Access Based on Underspecified InputMichael ZOCKLIF-CNRS?Equipe TALEP163, Avenue de LuminyF-13288 Marseille Cedex 9michael.zock@lif.univ-mrs.frDidier SCHWABGroupe GETALPLaboratoire d?Informatique de Grenoble385 avenue de la Bibliothque - BP 53F-38041 Grenoble Cedex 9didier.schwab@imag.frAbstractWords play a major role in language pro-duction, hence finding them is of vital im-portance, be it for speaking or writing.Words are stored in a dictionary, and thegeneral belief holds, the bigger the bet-ter.
Yet, to be truly useful the resourceshould contain not only many entries and alot of information concerning each one ofthem, but also adequate means to reveal thestored information.
Information access de-pends crucially on the organization of thedata (words) and on the navigational tools.It also depends on the grouping, rankingand indexing of the data, a factor too oftenoverlooked.We will present here some preliminary re-sults, showing how an existing electronicdictionary could be enhanced to supportlanguage producers to find the word theyare looking for.
To this end we have startedto build a corpus-based association ma-trix, composed of target words and ac-cess keys (meaning elements, related con-cepts/words), the two being connected attheir intersection in terms of weight andtype of link, information used subsequentlyfor grouping, ranking and navigation.1 Context and problemWhen speaking or writing we encounter basi-cally either of the following two situations: onewhere everything works automatically, somehowlike magic, words popping up one after anotherc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.like spring water, and another where we look de-liberately and often painstakingly for a specific,possibly known word.
We will be concerned herewith this latter situation: a speaker/ writer usingan electronic dictionary to look for such a word.Unfortunately, alphabetically organized dictionar-ies are not well suited for this kind of reverselookup where the inputs are meanings (elements ofthe word?s definition) or conceptually related ele-ments (collocations, associations), and the outputsthe target words.Without any doubt, lexicographers have madeconsiderable efforts to assist language users, build-ing huge resources, composed of many words andlots of information associated with each one ofthem.
Still, it is not unfair to say most dictionar-ies have been conceived from the reader?s point ofview.
The lexicographers have hardly taken intoaccount the language producer?s perspective,1con-sidering conceptual input, incomplete as it may be,as starting point.
While readers start with words,looking generally for their corresponding mean-ings, speakers or writers usually start with the op-posite, meanings or concepts,2which should be theentry points of a dictionary, which ideally is neu-tral in terms of access direction.3The problem is that we still don?t know verywell what concepts are, whether they are compo-sitional and if so, how many primitives there are(Wilks, 1977; Wierzbicka, 1996; Goddard, 1998).1Roget?s thesaurus (Roget, 1852), Miller and Fellbaum?sWordNet (Fellbaum, 1998) and Longman?s Language Activa-tor (Summers, 1993), being notable exceptions (For more de-tails, see next section).2Of course, this does not preclude, that we may have touse words to refer to them in a concept-based query.3While we agree with Polgu`ere theoretically when hepleads for dictionary neutrality with regard to lexical access(Polgu`ere, 2006), from a practical point of view the situationis obviously quite different for the speaker and listener, evenif both of them draw on the same resource.9Neither do we know how to represent them.
Yet,there are ways around this problem as we willshow.
Whether concepts and words are organizedand accessed differently is a question we cannotanswer here.
We can agree though on the factthat getting information concerning words is fairlyunproblematic when reading, at least in the caseof most western languages.
Words can gener-ally be found easily in a dictionary, provided theuser knows the spelling, the alphabet and how tobuild lemma starting from an inflected form.
Un-like words, which are organized alphabetically (inwestern languages) or by form (stroke counts inChinese), concepts are organized topically: theyare clustered into functional groups according totheir role in real world, or our perception of it.Psychologist have studied the difficulties peo-ple have when trying to produce or access words(Aitchinson, 2003).
In particular, they have stud-ied the tip-of-the-tongue phenomenon (Brown andMcNeill, 1996) and the effects an input can haveon the quality of an output (error analysis (Cutler,1982)) and on the ease of its production: positiveor negative priming effect (activation/inhibition).Obviously, these findings allow certain conclu-sions, and they might guide us when developingtools to help people find the needed word.
In par-ticular, they reveal two facts highly relevant for ourgoal:1. even if people fail to access a given word, theymight know a lot about it: origin, meaning(word definition, role played in a given sit-uation), part of speech, number of syllables,similar sounding words, etc.
Yet, despite allthis knowledge, they seem to lack some cru-cial information to be able to produce the pho-netic form.
The word gets blocked at the verylast moment, even though it has reached thetip-of-the-tongue.
This kind of nuisance is allthe more likely as the target word is rare andprimed by a similar sounding word.2.
unlike words in printed or electronic dictio-naries, words in our mind may be inexis-tent as tokens.
What we seem to have inour minds are decomposed, abstract entitieswhich need to be synthesized over time.4Ac-4This may be very surprising, yet, this need not be the caseif we consider the fact that speech errors are nearly alwaysdue to competing elements from the same level or an adja-cent one, unless they are the result of a surrounding conceptwhich has been activated, or which is about to be translatedcording to Levelt (Levelt, 1996) the genera-tion of words (synthesis) involves the follow-ing stages: conceptual preparation, lexical se-lection, phonological- and phonetic encoding,articulation.
Bear in mind that having per-formed ?lexical selection?
does not imply ac-cess to the phonetic form (see the experimentson the tip-of-the-tongue phenomenon).What can be concluded from these observa-tions?
It seems that underspecified input is suffi-ciently frequent to be considered as normal.
Hencewe should accept it, and make the best out of it byusing whatever information is available (accessi-ble), no matter how incomplete, since it may stillcontribute to find the wanted information, be it byreducing the search space.
Obviously, the more in-formation we have the better, as this reduces thenumber of words among which to choose.2 Related work and goalWhile more dictionaries have been built for thereader than for the writer, there have been someonomasiological attempts as early as in the mid-dle of the 19th century.
For example, Roget?sThesaurus (Roget, 1852), T?ong?s Chinese andEnglish instructor (T?ong, 1862), or Boissiere?sanalogical dictionary (Boissi`ere, 1862).5Newerwork includes Mel?
?cuk?s ECD (Mel?
?cuk et al,1999), Miller and Fellbaum?s WordNet (Fellbaum,1998), Richardson and Dolan?s MindNet (Richard-son et al, 1998), Dong?s HowNet (Dong andDong, 2006) and Longman?s Language Activa-tor (Summers, 1993).
There is also the work ofinto words.
Put differently, we do not store words at all inour mind, at least not in the layman?s or lexicographer?s sensewho consider word-forms and their meanings as one.
If weare right, than rather continue to consider the human mind asa word store we could consider it as a word factory.
Indeed,by looking at some of the work done by psychologists who tryto emulate the mental lexicon (for a good survey see (Harley,2004), pages 359-374) one gets the impression that words aresynthesized rather than located and read out.
Taking a look atall this work, generally connectionist models, one may con-clude that, rather than having words in our mind we have aset of more or less abstract features (concepts, syntactic infor-mation, phonemes), distributed across various layers, whichneed to be synthesized over time.
To do so we proceed fromabstract meanings to concrete sounds, which at some pointwere also just abstract features.
By propagating energy ratherthan data (as there is no message passing, transformation orcumulation of information, there is only activation spreading,that is, changes of energy levels, call it weights, electronicimpulses, or whatever), that we propagate signals, activatingultimately certain peripheral organs (larynx, tongue, mouth,lips, hands) in such a way as to produce movements or sounds,that, not knowing better, we call words.5For a more recent proposal see (Robert et al, 1993).10(Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008),various collocation dictionaries (BBI, OECD) andBernstein?s Reverse Dictionary.6Finally, there isM.
Rundell?s MEDAL, a thesaurus produced withthe help of Kilgariff?s Sketch Engine (Kilgarriff etal., 2004).As one can see, a lot of progress has been ac-complished over the last few years, yet more can bedone, especially with regard to unifying linguisticand encyclopedic knowledge.
Let?s take an exam-ple to illustrate our point.Suppose, you were looking for a word express-ing the following ideas: ?superior dark coffee madefrom beans from Arabia?, and that you knew thatthe target word was neither espresso nor cappuc-cino.
While none of this would lead you directlyto the intended word, mocha, the information athand, i.e.
the word?s definition or some of its ele-ments, could certainly be used.
In addition, peopledraw on knowledge concerning the role a concept(or word) plays in language and in real world, i.e.the associations it evokes.
For example, they mayknow that they are looking for a noun standing fora beverage that people take under certain circum-stances, that the liquid has certain properties, etc.In sum, people have in their mind an encyclope-dia: all words, concepts or ideas being highly con-nected.
Hence, any one of them has the potential toevoke the others.
The likelihood for this to happendepends, of course, on factors such as frequency(associative strength), distance (direct vs. indirectaccess), prominence (saliency), etc.How is this supposed to work for a dictionaryuser?
Suppose you were looking for the wordmocha (target word: tw), yet the only token com-ing to your mind were computer (source word:sw).
Taking this latter as starting point, the systemwould show all the connected words, for example,Java, Perl, Prolog (programing languages), mouse,printer (hardware), Mac, PC (type of machines),etc.
querying the user to decide on the direction ofsearch by choosing one of these words.
After all,s/he knows best which of them comes closest to thetw.
Having started from the sw?computer?, andknowing that the twis neither some kind of soft-ware nor a type of computer, s/he would probablychoose Java, which is not only a programming lan-guage but also an island.
Taking this latter as the6There is also at least one electronic incarnationof a dictionary with reverse access, combining a dic-tionary (WordNet) and an encyclopedia (Wikipedia)(http://www.onelook.com/reverse-dictionary.shtml).new starting point s/he might choose coffee (sinces/he is looking for some kind of beverage, possiblymade from an ingredient produced in Java, coffee),and finally mocha, a type of beverage made fromthese beans.
Of course, the word Java might justas well trigger Kawa which not only rhymes withthe sw, but also evokes Kawa Igen, a javanese vol-cano, or familiar word of coffee in French.As one can see, this approach allows word ac-cess via multiple routes: there are many ways lead-ing to Rome.
Also, while the distance coveredin our example is quite unusual, it is possible toreach the goal quickly.
It took us actually veryfew moves, four, to find an indirect link, betweentwo, fairly remotely related terms: computer andmocha.
Of course, cyber-coffee fans might be evenquicker in reaching their goal.3 The lexical matrix revisitedThe main question that we are interested in hereis how, or in what terms, to index the dictionaryin order to allow for quick and intuitive access towords.
Access should be possible on the basisof meaning (or meaning elements), various kindsof associations (most prominently ?syntagmatic?ones) and, more generally speaking, underspeci-fied input.
To this end we have started to build anassociation matrix (henceforth AM), akin to, yetdifferent from G. Miller?s initial proposal of WN(Miller et al, 1990).
He suggested to build a lex-ical matrix by putting on one axis all the forms,i.e.
words of the language, and on the other, theircorresponding meanings.
The latter being definedin terms of synsets.
The corresponding meaning-form relations are signaled via a boolean (pres-ence/absence).
Hence, looking at the intersectionof meanings and forms, one can see which mean-ings are expressed by, or converge toward whatforms, or conversely, what form expresses whichmeanings.
Whether this is the way WN is actuallyimplemented is not clear to us, though we believethat it is not.
Anyhow, our approach is different,and we hope the reader will understand in a mo-ment the reasons why.We will also put on one axis all the form ele-ments, i.e.
the lemmata or expressions of a givenlanguage (we refer to them as target words, hence-forth tw).
On the other axis we will place the trig-gers or access-words (henceforth aw), that is, thewords or concepts capable and likely to evoke thetw.
These are typically the kind of words psy-11chologists have gathered in their association ex-periments (Jung and Riklin, 1906; Deese, 1965;Schvaneveldt, 1989).
Note, that instead of puttinga boolean value at the intersection of the twand theaw, we will put weights and the type of link hold-ing between the co-occurring terms.
This gives usquadruplets.
For example, an utterance like ?thisis the key of the door?
might yield the aw(key),the tw(door), the link type lt(part of), and a weight(let?s say 15).The fact that we have these two kinds of in-formation is very important later on, as it allowsthe search engine to cluster by type the possibleanswers to be given in response to a user query(word(s) provided as input) and to rank them.Since the number of hits, i.e.
words from whichthe user must choose, may be substantial (depend-ing on the degree of specification of the input), it isimportant to group and rank them to ease naviga-tion, allowing the user to find directly and quicklythe desired word, or at least the word with whichto continue search.Obviously, different word senses (homographs),require different entries (bank-money vs bank-river), but so will synonyms, as every word-form,synonym or not, is likely to be evoked by a differ-ent key- or access-word (similarity of sound).7Also, we will need a new line for every differentrelation between a awand a tw.
Whether more thanone line is needed in the case of identical links be-ing expressed by different linguistic resources (thelock of the door vs. the door?s lock vs. the doorhas a lock) remains an open empirical question.Let us see quickly how our AM is supposedto work.
Imagine you wanted to find the wordfor the following concept: hat of a bishop.
Insuch a case, any of the following concepts orwords might come to your mind: church, Vati-can, abbot, monk, monastery, ceremony, ribbon,and of course rhyming words like: brighter, fighter,lighter, righter, tighter, writer,8as, indeed, any ofthem could remind us of the tw: mitre.
Hence, allof them are possible aw.Once this resource is built, access is quitestraightforward.
The user gives as input all thewords coming to his mind when thinking of a given7Take, for example, the nouns rubbish and garbage whichcan be considered as synonyms.
Yet, while the former mayremind you of a rabbit or (horse)-radish, the latter may evokethe word cabbage.8The question, whether rhyming words should be com-puted is not crucial at this stage.idea or concept,9and the system will display allconnected words.
If the user can find the item heis looking for in this list, search stops, otherwiseit will continue, the user giving other words of thelist, or words evoked by them.Of course, remains the question of how to buildthis resource, in particular, how to populate theaxis devoted to the trigger words, i.e.
access-keys.
At present we consider three approaches:one, where we use the words occurring in worddefinitions (see also, (Dutoit and Nugues, 2002;Bilac et al, 2004)), the other is to mine a well-balanced corpus, to find co-occurrences within agiven window (Ferret and Zock, 2006), the sizedepending a bit on the text type (encyclopedia) ortype of corpus.
Still another solution would beto draw on the association lists produced by psy-chologists, see for example http://www.usf.edu/, orhttp://www.eat.rl.ac.uk.Of course, the idea of using matrices in linguis-tics is not new.
There are at least two authors whohave proposed its use: M. Gross (Gross, 1984)used it for coding the syntactic behavior of lex-ical items, hence the term lexicon-grammar, andG.
Miller, the father of WN (Miller et al, 1990)suggested it to support lexical access.
While theformer work is not relevant for us here, Miller?sproposal is.
What are the differences between hisproposal and ours?
There are basically four maindifferences:1. we use, collocations or access-words, i.e awsrather than synsets; Hence, any of the follow-ing aws(cat, grey, computer device, cheese,Speedy Gonzales) could point toward the tw?mouse?, none of them are part of the mean-ing, leave alone synonyms.2.
we mark explicitly the weight and the type oflink between the twand the aw(isa, part of,etc.
),10whereas WN uses only a binary value.Both the weight and link are necessary infor-mation for ranking and grouping, i.e.
naviga-tion.3.
our AM is corpus-sensitive (see below),hence, we can, at least in principle, accommo-9The quantifier all shouldn?t be taken too literally.
Whatwe have in mind are ?salient?
words available in the speaker?smind at a given moment10Hence, if several links are possible between the twandthe aw, several cells will be used.
Think of the many possiblerelations between a city and a country, example: Paris andFrance (part of, biggest city of, located in, etc.
)12date the fact that a speaker is changing topics,adapting the weight of a given word or find amore adequate awin this new context.
Thinkof ?piano?
in the contexts of a concert or mov-ing your household.
Only the latter wouldevoke the notion of weight.4.
relying on a corpus, we can take advantage ofsyntagmatic associations (often encyclopedicknowledge), something which is difficult toobtain for WN.4 Keep the set of lexical candidates smallHere and in the next section we describe how theidea of the AM has been computationally dealtwith.
The goal is to reduce the number of hits,i.e.
possible tws(output), as a function of the in-put, i.e, the number of relevant awsgiven by thespeaker/writer.
To achieve this goal we apply lex-ical functions to the aws, considering the intersec-tion of the obtained sets to be the relevant tws.4.1 Lexical FunctionsThe usefulness of lexical functions for linguisticsin general and for language production in particu-lar has been shown by Mel?
?cuk (Mel?
?cuk, 1996).We will use them here, as they seem to fit also ourneeds of information extraction or lexical access.Mel?
?cuk has coined the term lexical functions torefer to the fact that two terms are systematicallyrelated.
For example, the lexical function Generrefers to the fact that some term (let?s say ?cat?
)can be replaced by a more general term (let?s say?animal?
).Lexical functions encode the combinability ofwords.
While ?big?
and ?strong?
express the sameidea (intensity, magnitude), they cannot be com-bined freely with any noun: strong can be as-sociated with fever, whereas big cannot.
Ofcourse, this kind of combinability between lexicalterms is language specific, because unlike in En-glish, in French one can say grosse fi`evre or fortefi`evre, both being correct (Schwab and Lafourcade,2007).
Our AM handles, of course these kind offunctions.
Here is a list of some of them:- paradigmatic associations: hypernymy(?cat?
- ?animal?
), hyponymy, synonymy, orantonymy,.
.
.
;- syntagmatic associations: collocations (?fear?being associated with ?strong?
or ?little?
);- morphological relations ie.
terms being de-rived from another part of speech: applyingthe change-part-of-speech lexical functionfcposto ?garden?
will yield: fcpos(?garden?)
={?to garden?, ?gardener?, .
.
.
}- sound-related items: homophones, rhymes.4.2 Assumptions concerning searchThe purpose of using lexical functions is to reducethe number of possible outcomes from which theuser must choose.
The list contains either the twor another promising awthe user may want use tocontinue search.
Hence, lexical functions are use-ful for search provided that:1. the speaker/writer is able to specify the kindof relations s/he wants to use.
The problemhere lies in the nature and number of the func-tions, some of them being very well specified,while others are not.2.
the larger the number of trigger words thesmaller the list of words from which tochoose: the speaker/writer can add or deletewords to broaden or narrow the scope ofhis/her query.These hypotheses are being modeled by usingset properties of lexical functions.
The idea is toapply all functions, or a selection of them, to theawsand to give the speaker/writer the intersectionas result (see section 5.3.5 for an example)5 ExperimentWe have started with a simple, preliminary exper-iment.
Only one lexical function was used: neigh-borhood (henceforth fneig).
Let fneigbe the func-tion producing the set of co-occurring terms withina given window (sentence or a paragraph).11Theresult produced by the system and returned to theuser is the intersection of the application of fneigto the aws.
In the next section we explain how thisfunction is applied to two corpora (Wordnet andWikipedia), to show their respective qualities andshortcomings for this specific task.5.1 WordNet5.1.1 DescriptionWordNet (henceforth WN) is a lexical databasefor English developed under the guidance of G.11The scope or window size will vary with the text type(normal text vs. encyclopedia).
The optimal size is at thispoint still an empirical question.13Miller (Miller et al, 1990).
One of his goals wasto support lexical access akin to the human mind,association-based.
Knowledge is stored in a net-work composed of nodes and links (nodes beingwords or concepts and the links are the means ofconnecting them) and access to knowledge, i.e.search, takes place by entering the network at somepoint and follow the links until one has reached thegoal (unless one has given up before).
This kindof navigation in a huge conceptual/lexical networkcan be considered equivalent to spreading activa-tion taking place in our brain.Of course, such a network has to be built, andnavigational support must be provided to find thelocation where knowledge or words are stored.This is what Miller and his coworkers did by build-ing WN.
The resource has been built manually, andit contains at present about 150.000 entries.The structure of the dictionary is different fromconventional, alphabetical resources.
Words areorganized in WN in two ways.
Semantically sim-ilar words, i.e.
synonyms, are grouped as clus-ters.
These sets of synonyms, called synsets, arethen linked in various ways, depending on thekind of relationship they entertain with the ad-jacent synset.
For example, their neighbors canbe more general or specific (hyperonymy vs. hy-ponymy), they can be part of some reference ob-ject (meronymy: car-motor), they can be the op-posite (antonymy: hot-cold), etc.
While WN is aresource it can also be seen as a corpus.5.1.2 Using WN as a corpusThere are many good reasons to use WN forlearning fn.
For one, there are many extensions,and second, the one we are using, eXtended WN(Mihalcea and Moldovan, 2001) spares us the trou-ble of having to address issues like: (a) seg-mentation: we do not need to identify sentenceboundaries ; (b) semantic ambiguity: words beingtagged, we get good precision; (c) lemmatization:since only verbs, nouns, adjectives and adverbs aretagged, we need neither a stoplist nor a lemmatizer.Despite all these qualities, two important prob-lems remain nevertheless for this kind of corpus:(a) size: though, all words are tagged, the cor-pus remains small as it contains only 63.941 dif-ferent words; (b) in consequence, the corpus lacksmany syntagmatic associations encoding encyclo-pedic knowledge.5.2 Using Wikipedia as corpusWikipedia is a free, multilingual encyclopedia, ac-cessible on the Web.12For our experiment we havechosen the English version which of this day (12thof may 2008) contains 2,369,180 entries.Wikipedia has exactly the opposite properties ofWN.
While it covers well encyclopedic relations, itis only raw text.
Hence problems like text segmen-tation, lemmatisation and stoplist definition needto be addressed.Our experiments with Wikipedia were very rudi-mentary, given that we considered only 1000 doc-uments.
These latter were obtained in response tothe term ?wine?, by following the links obtained forabout 72.000 words.5.3 Prototype5.3.1 Building the resource and using it.Building the resource requires processing a cor-pus and building the database.
Given a corpuswe apply our neighborhood function to a prede-termined window (a paragraph in the case of ency-clopedias).13The result, i.e.
the co-occurrences,will be stored in the database, together with theirweight, i.e.
number of times two terms appear to-gether, and the type of link.
As mentionned above,both kinds of information are needed later on forranking and navigation.14At present, cooccurences are stored as triplets(tw, aw, times), where times represents the numberof times the two terms cooccur in the corpus, thescope of coccurence being here the paragraph.5.3.2 Processing of the Wikipedia pageFor each Wikipedia page, a preprocessorconverts HTML pages into plain text.
Next,a part-of-speech tagger (http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/) is usedto annotate all the words of the paragraph underconsideration.
This allows the filtering of allirrelevant words, to keep but a bag of words,that is, the nouns, adjectives, verbs and adverbsoccuring in the paragraph.
These words will beused to fill the triplets of our database.12http://www.wikipedia.org13The optimal window-size depends probably on the texttype (encyclopedia vs. unformatted text).
Yet, in the absenceof clear criteria, we consider the optimal window-size as anopen, empirical question.14This latter aspect is not implemented yet, but will beadded in the future, as it is a necessary component for easynavigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007).145.3.3 Corpus BuildingWe start arbitrarily from some page (for our ex-periment, we have chosen ?wine?
as input), applythe algorithm outlined here above and pick thenrandomly a noun within this page to fetch with thisinput a new page on Wikipedia.
This process is re-peated until a given sample size is obtained (in ourcase 1000 pages).
Of course, instead of pickingrandomly a noun, we could have decided to pro-cess all the nouns of a given page, and to add thenincrementally the nouns of the next pages.
Yet,doing this would have led us to privilege a specifictopic (in our case ?wine?)
instead of a more generalone.5.3.4 UsageWe have developed a website in Java as aservlet.
Interactions with humans are simple: peo-ple can add or delete a word from the current list(see Input in the figure on top of the next page).The example presented shows that with very fewwords, hence very quickly, we can obtain the de-sired word.Given some input, the system provides the userwith a list of words cooccuring with the aws.
Theoutput is an ordered list of words, the order de-pending on the overall score, i.e.
number of cooc-currences between the awand the tw.
For exam-ple, if the aws?wine?
and ?harvest?
co-occur withthe tw?bunch?
respectively 5 and 8 times, thenthe overall score of cooccurence of ?bunch?
is 13:((wine, harvest), bunch, 13).
Hence, all words witha higher score will precede it, while those with alower score will follow it.5.3.5 Examples and Comparison of theresults of the two corporaHere below are the examples extracted from theWN corpus (see figure-1).
Our goal was to findthe word ?vintage?.
Trigger words are ?wine?
and?harvest?, yielding respectively 488 and 30 hits, i.e.words.
As one can see ?harvest?
is a better ac-cess term than ?wine?.
Combining the two will re-duce the list to 6 items.
Please note that the tw?vintage?
is not among them, eventhough it existsin WordNet, which illustrates nicely the fact thatstorage does not guarantee accessibility (Sinopal-nikova and Smrz, 2006).Looking at figure-1 you will see that the resultshave improved considerably with Wikipedia.
Thesame input, ?wine?
evokes many more words (1845as opposed to 488).
For ?harvest?
we get 983 hits in-Input WordNet Wikipedia488 words 1845wordsgrape sweet aloholic countryserve france god characteristicswine small fruit regulation grapedry bottle appellation systemproduce red bottled likebread hold christian track.
.
.
.
.
.
.
.
.
.
.
.30 words 983 wordsmonth fish produce graingrape revolutionary autumn farmscalendar festival energy cutharvest butterfish dollar combine groundperson make balance rainwine first amount rich.
.
.
.
.
.
.
.
.
.
.
.6 words 45 wordsmake grape grape vintagewine fish someone bottle produce+harvest commemorate person fermentation juice.
.
.
.
.
.
Beaujolais tasteviticulture FranceBordeaux vineyard.
.
.
.
.
.Figure 1: Comparing two corpora (eXtendedWordNet and Wikipedia) with various inputsstead of 30 (the intersection containing 62 words).Combining the two reduces the set to 45 itemsamong which we will find, of course, the targetword.We hope that this example is clear enough toconvince the reader that it makes sense to use realtext as corpus to extract from it the kind of in-formation (associations) people are likely to givewhen looking for a word.6 Conclusion and perspectivesWe have addressed in this paper the problem ofword finding for speakers or writers.
Conclud-ing that most dictionaries are not well suited to al-low for this kind of reverse access based on mean-ings (or meaning related elements, associations),we looked at work done by psychologists to getsome inspiration.
Next we tried to clarify whichof these findings could help us build the dictionaryof tomorrow, that is, a tool integrating linguisticand encyclopedic knowledge, allowing navigationby taking either or as starting point.
While lin-guistic knowledge is more prominent for analysis(reading), encyclopedic facts are more relevant forproduction.
We?ve presented then our ideas of howto build a resource, allowing lexical access based15on underspecified, i.e.
imperfect input.
To achievethis goal we?ve started building an AM composedof form elements (the words and expressions ofa given language) and aws.
The role of the lat-ter being to lead to or to evoke the tw.
In the lastpart we?ve described briefly the results obtained bycomparing two resources (WN and Wikipedia) andvarious inputs.
Given the fact that the project isstill quite young, only preliminary results can beshown at this point.Our next steps will be to take a closer look at thefollowing work: clustering of similar words (Lin,1998), topic signatures (Lin and Hovy, 2000) andKilgariff?s sketch engine (Kilgarriff et al, 2004).We plan also to add other lexical functions to en-rich our database with aws.
We plan to experimentwith corpora, trying to find out which ones are bestfor our purpose15and we will certainly experimentwith the window size16to see which size is bestfor which text type.
Finally, we plan to insert inour AM the relations holding between the awandthe tw.
As these links are contained in our corpus,we should be able to identify and type them.
Thequestion is, to what extent this can be done auto-matically.Obviously, the success of our resource will de-pend on the quality of the corpus, the quality ofthe aws, weights and links, and the representativ-ity of all this for a given population.
While we dobelieve in the justification of our intuitions, morework is needed to reveal the true potential of theapproach.
The ultimate judge being, of course, thefuture user.15For example, we could consider a resource like Con-ceptNet of the Open Mind Common-Sense project (Liuh andSingh, 2004).16For example, it would have been interesting to considercoocurrences beyond the scope of the paragraph, by consider-ing the logical structure of the Wikipedia document.
Anyhow,our experiment needs to be redone with more data than just1000 pages, the size chosen here for lack of time.
Indeed onecould consider using the entire corpus of Wikipedia or mixedcorporaReferencesAitchinson, Jean.
2003.
Words in the Mind: an Intro-duction to the Mental Lexicon (3d edition).
Black-well, Oxford.Bilac, S., W. Watanabe, T. Hashimoto, T. Tokunaga,and H. Tanaka.
2004.
Dictionary search basedon the target word description.
In Proc.
of theTenth Annual Meeting of The Association for NLP(NLP2004), pages 556?559, Tokyo, Japan.Boissi`ere, P. 1862.
Dictionnaire analogique de lalangue franc?aise : r?epertoire complet des mots parles id?ees et des id?ees par les mots.
Larousse et A.Boyer, Paris.Brown, R. and D. McNeill.
1996.
The tip of the toungephenomenon.
Journal of Verbal Learning and Ver-bal Behaviour, 5:325?337.Cutler, A, editor, 1982.
Slips of the Tongue and Lan-guage Production.
Mouton, Amsterdam.Deese, James.
1965.
The structure of associations inlanguage and thought.
Johns Hopkins Press.Dong, Zhendong and Qiang Dong.
2006.
HOWNETand the computation of meaning.
World Scientific,London.Dutoit, Dominique and P. Nugues.
2002.
A lexicalnetwork and an algorithm to find words from defini-tions.
In van Harmelen, F., editor, ECAI2002, Proc.of the 15th European Conference on Artificial Intel-ligence, pages 450?454, Lyon.
IOS Press, Amster-dam.Fellbaum, Christiane, editor, 1998.
WordNet: An Elec-tronic Lexical Database and some of its Applica-tions.
MIT Press.Ferret, Olivier and Michael Zock.
2006.
Enhancingelectronic dictionaries with an index based on associ-ations.
In ACL ?06: Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the ACL, pages 281?288.Fontenelle, Thierry.
1997.
Using a bilingual dictionaryto create semantic networks.
International Journalof Lexicography, 10(4):275?303.Goddard, Cliff.
1998.
Bad arguments against seman-tic primitives.
Theoretical Linguistics, 24(2-3):129?156.16Gross, Maurice.
1984.
Lexicon-grammar and the anal-ysis of french.
In Proc.
of the 11th COLING, pages275?282, Stanford, CA.Harley, Trevor.
2004.
The psychology of language.Psychology Press, Taylor and Francis, Hove andNew York.Jung, Carl and F. Riklin.
1906.
ExperimentelleUntersuchungen ?uber Assoziationen Gesunder.
InJung, C. G., editor, Diagnostische Assoziationsstu-dien, pages 7?145.
Barth, Leipzig, Germany.Kilgarriff, Adam, Pavel Rychl?y, Pavel Smr?z, and DavidTugwell.
2004.
The Sketch Engine.
In Proceedingsof the Eleventh EURALEX International Congress,pages 105?116, Lorient, France.Levelt, Willem.
1996.
A theory of lexical accessin speech production.
In Proc.
of the 16th Con-ference on Computational Linguistics, Copenhagen,Denmark.Lin, Chin-Yew and Eduard H. Hovy.
2000.
The auto-mated acquisition of topic signatures for text summa-rization.
In COLING, pages 495?501.
Morgan Kauf-mann.Lin, Dekang.
1998.
Automatic retrieval and clusteringof similar words.
In COLING-ACL, pages 768?774,Montreal.Liuh, H. and P. Singh.
2004.
ConceptNet: a practi-cal commonsense reasoning toolkit.
BT TechnologyJournal.Mel?
?cuk, I., N. Arbatchewsky-Jumarie, L. Iordanskaja,S.
Mantha, and A. Polgu`ere.
1999.
Dictionnaireexplicatif et combinatoire du franc?ais contemporainRecherches lexico-s?emantiques IV.
Les Presses del?Universit?e de Montr?eal, Montr?eal.Mel?
?cuk, Igor.
1996.
Lexical functions: A tool forthe description of lexical relations in the lexicon.
InWanner, L., editor, Lexical Functions in Lexicogra-phy and Natural Language Processing, pages 37?102.
Benjamins, Amsterdam/Philadelphia.Mihalcea, Rada and Dan Moldovan.
2001.
ExtendedWordNet: progress report.
In NAACL 2001 - Work-shop on WordNet and Other Lexical Resources, Pitts-burgh, USA.Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, andK.
Miller.
1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexi-cography, 3(4), pages 235?244.Moerdijk, Fons.
2008.
Frames and semagrams; Mean-ing description in the general dutch dictionary.
InProceedings of the Thirteenth Euralex InternationalCongress, EURALEX, Barcelona.Polgu`ere, Alain.
2006.
Structural properties of lexi-cal systems: Monolingual and multilingual perspec-tives.
Sidney.
Coling workshop ?Multilingual Lan-guage Resources and Interoperability?.Richardson, S., W. Dolan, and L. Vanderwende.
1998.Mindnet: Acquiring and structuring semantic infor-mation from text.
In ACL-COLING?98, pages 1098?1102.Robert, Paul, Alain Rey, and J. Rey-Debove.
1993.Dictionnaire alphabetique et analogique de laLangue Franc?aise.
Le Robert, Paris.Roget, P. 1852.
Thesaurus of English Words andPhrases.
Longman, London.Schvaneveldt, R., editor, 1989.
Pathfinder Associa-tive Networks: studies in knowledge organization.Ablex, Norwood, New Jersey, US.Schwab, Didier and Mathieu Lafourcade.
2007.
Mod-elling, detection and exploitation of lexical functionsfor analysis.
ECTI Transactions Journal on Com-puter and Information Technology, 2(2):97?108.Sierra, Gerardo.
2000.
The onomasiological dictio-nary: a gap in lexicography.
In Proceedings of theNinth Euralex International Congress, pages 223?235, IMS, Universit?at Stuttgart.Sinopalnikova, Anna and Pavel Smrz.
2006.
Knowinga word vs. accessing a word: Wordnet and word as-sociation norms as interfaces to electronic dictionar-ies.
In Proceedings of the Third International Word-Net Conference, pages 265?272, Korea.Summers, Della.
1993.
Language Activator: theworld?s first production dictionary.
Longman, Lon-don.T?ong, Ting-K?u.
1862.
Ying ?u tsap ts?
?un (The Chineseand English Instructor).
Canton.Wierzbicka, Anna.
1996.
Semantics: Primes and Uni-versals.
Oxford University Press, Oxford.Wilks, Yorick.
1977.
Good and bad arguments aboutsemantic primitives.
Communication and Cognition,10(3?4):181?221.Zock, Michael and Slaven Bilac.
2004.
Word lookupon the basis of associations : from an idea to aroadmap.
In Workshop on ?Enhancing and usingelectronic dictionaries?, pages 29?35, Geneva.
COL-ING.Zock, Michael.
2006.
Navigational aids, a criticalfactor for the success of electronic dictionaries.
InRapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp,editors, Perspectives on Cognition: A Festschrift forManfred Wettler, pages 397?414.
Pabst Science Pub-lishers, Lengerich.Zock, Michael.
2007.
If you care to find what youare looking for, make an index: the case of lexicalaccess.
ECTI, Transaction on Computer and Infor-mation Technology, 2(2):71?80.17
