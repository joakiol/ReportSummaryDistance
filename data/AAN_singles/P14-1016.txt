Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 165?174,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsWeakly Supervised User Profile Extraction from TwitterJiwei Li1, Alan Ritter2, Eduard Hovy11Language Technology Institute,2Machine Learning DepartmentCarnegie Mellon University, Pittsburgh, PA 15213, USAbdlijiwei@gmail.com, rittera@cs.cmu.edu, ehovy@andrew.cmu.eduAbstractWhile user attribute extraction on socialmedia has received considerable attention,existing approaches, mostly supervised,encounter great difficulty in obtaining goldstandard data and are therefore limitedto predicting unary predicates (e.g., gen-der).
In this paper, we present a weakly-supervised approach to user profile extrac-tion from Twitter.
Users?
profiles from so-cial media websites such as Facebook orGoogle Plus are used as a distant sourceof supervision for extraction of their at-tributes from user-generated text.
In addi-tion to traditional linguistic features usedin distant supervision for information ex-traction, our approach also takes into ac-count network information, a unique op-portunity offered by social media.
We testour algorithm on three attribute domains:spouse, education and job; experimentalresults demonstrate our approach is ableto make accurate predictions for users?
at-tributes based on their tweets.11 IntroductionThe overwhelming popularity of online social me-dia creates an opportunity to display given as-pects of oneself.
Users?
profile information insocial networking websites such as Facebook2orGoogle Plus3provides a rich repository personalinformation in a structured data format, making itamenable to automatic processing.
This includes,for example, users?
jobs and education, and pro-vides a useful source of information for applica-tions such as search4, friend recommendation, on-1Both code and data are available at http://aclweb.org/aclwiki/index.php?title=Profile_data2https://www.facebook.com/3https://plus.google.com/4https://www.facebook.com/about/graphsearch@[shanenicholson] has taken all the kids today soI can go shopping-CHILD FREE!
#iloveyoushano#iloveyoucreditcardTamworth promo day with my handsome classy husband@[shanenicholson]Spouse: shanenicholsonI got accepted to be part of the UofM engineering safetypilot program in [FSU]Here in class.
(@ [Florida State University] - WilliamsBuilding)Don?t worry , guys !
Our beloved [FSU] will always con-tinue to rise ?
to the top !Education: Florida State University (FSU)first day of work at [HuffPo], a sports bar woo come visitme yo..start to think we should just add a couple desks to the[HuffPo] newsroom for Business Insider writersjust back from [HuffPo], what a hell !Job: HuffPoTable 1: Examples of Twitter message clues foruser profile inference.line advertising, computational social science andmore.Although profiles exist in an easy-to-use, struc-tured data format, they are often sparsely popu-lated; users rarely fully complete their online pro-files.
Additionally, some social networking ser-vices such as Twitter don?t support this type ofstructured profile data.
It is therefore difficult toobtain a reasonably comprehensive profile of auser, or a reasonably complete facet of information(say, education level) for a class of users.
Whilemany users do not explicitly list all their personalinformation in their online profile, their user gen-erated content often contains strong evidence tosuggest many types of user attributes, for exampleeducation, spouse, and employment (See Table 1).Can one use such information to infer more de-tails?
In particular, can one exploit indirect cluesfrom an unstructured data source like Twitter toobtain rich, structured user profiles?In this paper we demonstrate that it is feasi-ble to automatically extract Facebook-style pro-165files directly from users?
tweets, thus makinguser profile data available in a structured formatfor upstream applications.
We view user profileinference as a structured prediction task whereboth text and network information are incorpo-rated.
Concretely, we cast user profile predic-tion as binary relation extraction (Brin, 1999),e.g., SPOUSE(Useri, Userj), EDUCATION(Useri,Entityj) and EMPLOYER(Useri, Entityj).
Inspiredby the concept of distant supervision, we collecttraining tweets by matching attribute ground truthfrom an outside ?knowledge base?
such as Face-book or Google Plus.One contribution of the work presented here isthe creation of the first large-scale dataset on threegeneral Twitter user profile domains (i.e., EDUCA-TION, JOB, SPOUSE).
Experiments demonstratethat by simultaneously harnessing both text fea-tures and network information, our approach isable to make accurate user profile predictions.
Weare optimistic that our approach can easily be ap-plied to further user attributes such as HOBBIESand INTERESTS (MOVIES, BOOKS, SPORTS orSTARS), RELIGION, HOMETOWN, LIVING LOCA-TION, FAMILY MEMBERS and so on, where train-ing data can be obtained by matching ground truthretrieved from multiple types of online social me-dia such as Facebook, Google Plus, or LinkedIn.Our contributions are as follows:?
We cast user profile prediction as an informa-tion extraction task.?
We present a large-scale dataset for this taskgathered from various structured and unstruc-tured social media sources.?
We demonstrate the benefit of jointly rea-soning about users?
social network structurewhen extracting their profiles from text.?
We experimentally demonstrate the effective-ness of our approach on 3 relations: SPOUSE,JOB and EDUCATION.The remainder of this paper is organized as fol-lows: We summarize related work in Section 2.The creation of our dataset is described in Section3.
The details of our model are presented in Sec-tion 4.
We present experimental results in Section5 and conclude in Section 6.2 Related WorkWhile user profile inference from social media hasreceived considerable attention (Al Zamal et al,2012; Rao and Yarowsky, 2010; Rao et al, 2010;Rao et al, 2011), most previous work has treatedthis as a classification task where the goal is to pre-dict unary predicates describing attributes of theuser.
Examples include gender (Ciot et al, 2013;Liu and Ruths, 2013; Liu et al, 2012), age (Rao etal., 2010), or political polarity (Pennacchiotti andPopescu, 2011; Conover et al, 2011).A significant challenge that has limited previousefforts in this area is the lack of available trainingdata.
For example, researchers obtain training databy employing workers from Amazon MechanicalTurk to manually identify users?
gender from pro-file pictures (Ciot et al, 2013).
This approach isappropriate for attributes such as gender with asmall numbers of possible values (e.g., male or fe-male), for which the values can be directly iden-tified.
However for attributes such as spouse oreducation there are many possible values, makingit impossible to manually search for gold standardanswers within a large number of tweets whichmay or may not contain sufficient evidence.Also related is the Twitter user timeline extrac-tion algorithm of Li and Cardie (2013).
This workis not focused on user attribute extraction, how-ever.Distant Supervision Distant supervision, alsoknown as weak supervision, is a method for learn-ing to extract relations from text using groundtruth from an existing database as a source ofsupervision.
Rather than relying on mention-level annotations, which are expensive and timeconsuming to generate, distant supervision lever-ages readily available structured data sources asa weak source of supervision for relation ex-traction from related text corpora (Craven etal., 1999).
For example, suppose r(e1, e2) =IsIn(Paris, France) is a ground tuple in thedatabase and s =?Paris is the capital of France?contains synonyms for both ?Paris?
and ?France?,then we assume that s may express the factr(e1, e2) in some way and can be used as pos-itive training examples.
In addition to the wideuse in text entity relation extraction (Mintz et al,2009; Ritter et al, 2013; Hoffmann et al, 2011;Surdeanu et al, 2012; Takamatsu et al, 2012),distant supervision has been applied to multiple166Figure 1: Illustration of Goolge Plus ?knowledgebase?.fields such as protein relation extraction (Cravenet al, 1999; Ravikumar et al, 2012), event extrac-tion from Twitter (Benson et al, 2011), sentimentanalysis (Go et al, 2009) and Wikipedia infoboxgeneration (Wu and Weld, 2007).Homophily Online social media offers a richsource of network information.
McPherson etal.
(2001) discovered that people sharing moreattributes such as background or hobby havea higher chance of becoming friends in socialmedia.
This property, known as HOMOPHILY(summarized by the proverb ?birds of a featherflock together?)
(Al Zamal et al, 2012) has beenwidely applied to community detection (Yang andLeskovec, 2013) and friend recommendation (Guyet al, 2010) on social media.
In the user attributeextraction literature, researchers have consideredneighborhood context to boost inference accuracy(Pennacchiotti and Popescu, 2011; Al Zamal et al,2012), where information about the degree of theirconnectivity to their pre-labeled users is includedin the feature vectors.
A related algorithm by Mis-love et al (2010) crawled Facebook profiles of4,000 Rice University students and alumni and in-ferred attributes such as major and year of ma-triculation purely based on network information.Mislove?s work does not consider the users?
textstream, however.
As we demonstrate below, rely-ing solely on network information is not enough toenable inference about attributes.3 Dataset CreationWe now describe the generation of our distantlysupervised training dataset in detail.
We makeuse of Google Plus and Freebase to obtain groundfacts and extract positive/negative bags of post-ings from users?
twitter streams according to theground facts.Figure 2: Example of fetching tweets containingentity USC mention from Miranda Cosgrove (anAmerican actress and singer-songwriter)?s twitterstream.Education/Job We first used the Google PlusAPI5(shown in Figure 1) to obtain a seed setof users whose profiles contain both their educa-tion/job status and a link to their twitter account.6Then, we fetched tweets containing the mention ofthe education/job entity from each correspondentuser?s twitter stream using Twitter?s search API7(shown in Figure 2) and used them to constructpositive bags of tweets expressing the associatedattribute, namely EDUCATION(Useri, Entityj), orEMPLOYER(Useri, Entityj).
The Freebase API8is employed for alias recognition, to match termssuch as ?Harvard University?, ?Harvard?, ?Har-vard U?
to a single The remainder of each corre-sponding user?s entire Twitter feed is used as neg-ative training data.9We expanded our dataset from the seed usersaccording to network information provided byGoogle Plus and Twitter.
Concretely, we crawledcircle information of users in the seed set fromboth their Twitter and Google Plus accounts andperformed a matching to pick out shared usersbetween one?s Twitter follower list and GooglePlus Circle.
This process assures friend identityand avoids the problem of name ambiguity whenmatching accounts across websites.
Among candi-date users, those who explicitly display Job or Ed-ucation information on Google Plus are preserved.We then gathered positive and negative data as de-scribed above.Dataset statistics are presented in Table 2.
Our5https://developers.google.com/+/api/6An unambiguous twitter account link is needed here be-cause of the common phenomenon of name duplication.7https://twitter.com/search8http://wiki.freebase.com/wiki/Freebase_API9Due to Twitter user timeline limit, we crawled at most3200 tweets for each user.167education dataset contains 7,208 users, 6,295 ofwhich are connected to others in the network.
Thepositive training set for the EDUCATION is com-prised of 134,060 tweets.Spouse Facebook is the only type of social me-dia where spouse information is commonly dis-played.
However, only a tiny amount of individ-ual information is publicly accessible from Face-book Graph API10.
To obtain ground truth for thespouse relation at large scale, we turned to Free-base11, a large, open-domain database, and gath-ered instances of the /PEOPLE/PERSON/SPOUSErelation.
Positive/negative training tweets are ob-tained in the same way as was previously de-scribed for EDUCATION and JOB.
It is worthnoting that our Spouse dataset is not perfect, asindividuals retrieved from Freebase are mostlycelebrities, and thus it?s not clear whether thisgroup of people are representative of the generalpopulation.SPOUSE is an exception to the ?ho-mophily?
effect.
But it exhibits anotherunique property, known as, REFLEXIVITY: factIsSpouseOf(e1, e2) and IsSpouseOf(e2, e1)will hold or not hold at the same time.
Given train-ing data expressing the tuple IsSpouseOf(e1, e2)from user e1?s twitter stream, we also gather usere2?s tweet collection, and fetch tweets with themention of e1.
We augment negative trainingdata from e2as in the case of Education and Job.Our Spouse dataset contains 1,636 users, wherethere are 554 couples (1108 users).
Note thatthe number of positive entities (3,121) is greaterthan the number of users as (1) one user can havemultiple spouses at different periods of time (2)multiple entities may point to the same individual,e.g., BarackObama, Barack Obama and Barack.4 ModelWe now describe our approach to predicting userprofile attributes.4.1 NotationMessage X: Each user i ?
[1, I] is associ-ated with his Twitter ID and his tweet corpusXi.
Xiis comprised of a collection of tweetsXi= {xi,j}j=Nij=1, where Nidenotes the numberof tweets user i published.10https://developers.facebook.com/docs/graph-api/11http://www.freebase.com/Education Job Spouse#Users 7,208 1,806 1,636#Users Con-nected6,295 1,407 1,108#Edges 11,167 3,565 554#Pos Entities 451 380 3121#Pos Tweets 124,801 65,031 135,466#Aver PosTweets per User17.3 36.6 82.8#Neg Entity 6,987,186 4,405,530 8,840,722#Neg Tweets 16,150,600 10,687,403 12,872,695Table 2: Statistics for our DatasetTweet Collection Lei: Leidenotes the collectionof postings containing the mention of entity e fromuser i. Lei?
Xi.Entity attribute indicator zki,eand zki,x: Foreach entity e ?
Xi, there is a boolean variable zki,e,indicating whether entity e expresses attribute k ofuser i.
Each posting x ?
Leiis associated with at-tribute indicator zki,xindicating whether posting xexpresses attribute k of user i. zki,eand zki,xareobserved during training and latent during testing.Neighbor set Fki: Fkidenotes the neighbor setof user i.
For Education (k = 0) and Job (k = 1),Fkidenotes the group of users within the networkthat are in friend relation with user i.
For Spouseattribute, Fkidenote current user?s spouse.4.2 ModelThe distant supervision assumes that if entity ecorresponds to an attribute for user i, at least oneposting from user i?s Twitter stream containing amention of emight express that attribute.
For user-level attribute prediction, we adopt the followingtwo strategies:(1) GLOBAL directly makes aggregate (entity)level prediction for zki,e, where features for alltweets from Leiare aggregated to one vector fortraining and testing, following Mintz et al (2009).
(2) LOCAL makes local tweet-level predictionsfor each tweet zei,x, x ?
Lkiin the first place, mak-ing the stronger assumption that all mentions of anentity in the users?
profile are expressing the asso-ciated attribute.
An aggregate-level decision zki,eisthen made from the deterministic OR operators.zei,x={1 ?x ?
Lei, s.t.zki,x= 10 Otherwise(1)The rest of this paper describes GLOBAL in de-tail.
The model and parameters with LOCAL areidentical to those in GLOBAL except that LOCAL168encode a tweet-level feature vector rather than anaggregate one.
They are therefore excluded forbrevity.
For each attribute k, we use a model thatfactorizes the joint distribution as product of twodistributions that separately characterize text fea-tures and network information as follows:?
(zki,e, Xi, Fki: ?)
?
?text(zki,e, Xi)?Neigh(zki,e, Fki)(2)Text Factor We use ?text(zke, Xi) to capture thetext related features which offer attribute clues:?text(zke, , Xi) = exp[(?ktext)T?
?text(zki,e, Xi)](3)The feature vector ?text(zki,e, Xi) encodes the fol-lowing standard general features:?
Entity-level: whether begins with capital let-ter, length of entity.?
Token-level: for each token t ?
e, word iden-tity, word shape, part of speech tags, nameentity tags.?
Conjunctive features for a window of k(k=1,2) words and part of speech tags.?
Tweet-level: All tokens in the correspondenttweet.In addition to general features, we employattribute-specific features, such as whether the en-tity matches a bag of words observed in the listof universities, colleges and high schools for Edu-cation attribute, whether it matches terms in a listof companies for Job attribute12.
Lists of universi-ties and companies are taken from knowledge baseNELL13.Neighbor Factor For Job and Education, webias friends to have a larger possibility to sharethe same attribute.
?Neigh(zki,e, Fki) captures suchinfluence from friends within the network:?Neigh(zki,e, Fki) =?j?Fki?Neigh(zke, Xj)?Neigh(zki,e, Xj)= exp[(?kNeigh)T?
?Neigh(zki,e, Xj)](4)Features we explore include the whether entity eis also the correspondent attribute with neighboruser j, i.e., I(zej,k= 0) and I(zej,k= 1).12Freebase is employed for alias recognition.13http://rtw.ml.cmu.edu/rtw/kbbrowser/Input: Tweet Collection {Xi}, Neighbor set{Fki}Initialization:?
for each user i:for each candidate entity e ?
Xizki,e= argmaxz??
(z?, Xi) from textfeaturesEnd Initializationwhile not convergence:?
for each user i:update attribute values for j ?
Fkifor each candidate entity e ?
Xizki,e= argmaxz??
(z?, Xi, Fki)end while:Figure 3: Inference for NEIGH-LATENT setting.For Spouse, we set Fspousei= {e} and theneighbor factor can be rewritten as:?Neigh(zki,e, Xj) = ?Neigh(Ci, Xe) (5)It characterizes whether current user Cito be thespouse of user e (if e corresponds to a Twitteruser).
We expect clues about whether Cibeing en-tity e?s spouse from e?s Twitter corpus will in turnfacilitate the spouse inference procedure of user i.?Neigh(Ci, Xe) encodes I(Ci?
Se), I(Ci6?
Se).Features we explore also include whether Ci?stwitter ID appears in e?s corpus.4.3 TrainingWe separately trained three classifiers regardingthe three attributes.
All variables are observedduring training; we therefore take a feature-basedapproach to learning structure prediction modelsinspired by structure compilation (Liang et al,2008).
In our setting, a subset of the features(those based on network information) are com-puted based on predictions that will need to bemade at test time, but are observed during train-ing.
This simplified approach to learning avoidsexpensive inference; at test time, however, we stillneed to jointly predict the best attribute values forfriends as is described in section 4.4.4.4 InferenceJob and Education Our inference algorithmfor Job/Education is performed on two settings,depending on whether neighbor information is169observed (NEIGH-OBSERVED) or latent (NEIGH-LATENT).
Real world applications, where networkinformation can be partly retrieved from all typesof social networks, can always falls in between.Inference in the NEIGH-OBSERVED setting istrivial; for each entity e ?
Gi, we simply predictit?s candidate attribute values using Equ.6.zki,e= argmaxz??
(z?, Xi, Fki) (6)For NEIGH-LATENT setting, attributes for eachnode along the network are treated latent and userattribute prediction depends on attributes of hisneighbors.
The objective function for joint infer-ence would be difficult to optimize exactly, andalgorithms for doing so would be unlikely to scaleto network of the size we consider.
Instead, we usea sieve-based greedy search approach to inference(shown in Figure 3) inspired by recent work oncoreference resolution (Raghunathan et al, 2010).Attributes are initialized using only text features,maximizing ?text(e,Xi), and ignoring networkinformation.
Then for each user we iteratively re-estimate their profile given both their text featuresand network features (computed based on the cur-rent predictions made for their friends) which pro-vide additional evidence.In this way, highly confident predictions will bemade strictly from text in the first round, then thenetwork can either support or contradict low con-fidence predictions as more decisions are made.This process continues until no changes are madeat which point the algorithm terminates.
We em-pirically found it to work well in practice.
We ex-pect that NEIGH-OBSERVED performs better thanNEIGH-LATENT since the former benefits fromgold network information.Spouse For Spouse inference, if candidate entitye has no correspondent twitter account, we directlydetermine zki,e= argmaxz??
(z?, Xi) from textfeatures.
Otherwise, the inference of zki,edependson the zke,Ci.
Similarly, we initialize zki,eand zke,Ciby maximizing text factor, as we did for Educa-tion and Job.
Then we iteratively update zkgivenby the rest variables until convergence.5 ExperimentsIn this Section, we present our experimental re-sults in detail.Education JobAFFINITY 74.3 14.5Table 3: Affinity values for Education and Job.5.1 Preprocessing and Experiment SetupEach tweet posting is tokenized using Twitter NLPtool introduced by Noah?s Ark14with # and @separated following tokens.
We assume that at-tribute values should be either name entities orterms following @ and #.
Name entities are ex-tracted using Ritter et al?s NER system (2011).Consecutive tokens with the same named entitytag are chunked (Mintz et al, 2009).
Part-of-speech tags are assigned based on Owoputi et alstweet POS system (Owoputi et al, 2013).Data is divided in halves.
The first is used astraining data and the other as testing data.5.2 Friends with Same AttributeOur network intuition is that users are much morelikely to be friends with other users who share at-tributes, when compared to users who have no at-tributes in common.
In order to statistically showthis, we report the value of AFFINITY defined byMislove et al(2010), which is used to quantita-tively evaluate the degree of HOMOPHILY in thenetwork.
AFFINITY is the ratio of the fraction oflinks between attribute (k)-sharing users (Sk), rel-ative to what is expected if attributes are randomlyassigned in the network (Ek).Sk=?i?j?FkiI(Pki= Pkj)?i?j?FkiIEk=?mTkm(Tkm?
1)Uk(Uk?
1)(7)where Tkmdenotes the number of users with mvalue for attribute k and Uk=?mTkm.
Table 3shows the affinity value of the Education and Job.As we can see, the property of HOMOPHILY in-deed exists among users in the social network withrespect to Education and Job attribute, as signifi-cant affinity is observed.
In particular, the affinityvalue for Education is 74.3, implying that usersconnected by a link in the network are 74.3 timesmore likely affiliated in the same school than asexpected if education attributes are randomly as-signed.
It is interesting to note that Education ex-hibits a much stronger HOMOPHILY property than14https://code.google.com/p/ark-tweet-nlp/downloads/list170Job.
Such affinity demonstrates that our approachthat tries to take advantage of network informationfor attribute prediction of holds promise.5.3 Evaluation and DiscussionWe evaluate settings described in Section 4.2 i.e.,GLOBAL setting, where user-level attribute is pre-dicted directly from jointly feature space and LO-CAL setting where user-level prediction is madebased on tweet-level prediction along with differ-ent inference approaches described in Section 4.4,i.e.
NEIGH-OBSERVED and NEIGH-LATENT, re-garding whether neighbor information is observedor latent.Baselines We implement the following base-lines for comparison and use identical processingtechniques for each approach for fairness.?
Only-Text: A simplified version of our algo-rithm where network/neighbor influence is ig-nored.
Classifier is trained and tested only basedon text features.?
NELL: For Job and Education, candidate is se-lected as attribute value once it matches bag ofwords in the list of universities or companiesborrowed from NELL.
For Education, the list isextended by alias identification based on Free-base.
For Job, we also fetch the name abbrevia-tions15.
NELL is only implemented for Educa-tion and Job attribute.For each setting from each approach, we reportthe (P)recision, (R)ecall and (F)1-score.
For LO-CAL setting, we report the performance for bothentity-level prediction (Entity) and posting-levelprediction (Tweet).
Results for Education, Job andSpouse from different approaches appear in Table4, 5 and 6 respectively.Local or Global For horizontal comparison, weobserve that GLOBAL obtains a higher Precisionscore but a lower Recall than LOCAL(ENTITY).This can be explained by the fact that LOCAL(U)sets zki,e= 1 once one posting x ?
Leiis identifiedas attribute related, while GLOBAL tend to be moremeticulous by considering the conjunctive featurespace from all postings.Homophile effect In agreement with our ex-pectation, NEIGH-OBSERVED performs better thanNEIGH-LATENT since erroneous predictions in15http://www.abbreviations.com/NEIGH-LATENT setting will have negative in-fluence on further prediction during the greedysearch process.
Both NEIGH-OBSERVED andNEIGH-LATENT where network information isharnessed, perform better than Only-Text, whichthe prediction is made independently on user?s textfeatures.
The improvement of NEIGH-OBSERVEDover Only-Text is 22.7% and 6.4% regarding F-1 score for Education and Job respectively, whichfurther illustrate the usefulness of making use ofHomophile effect for attribute inference on onlinesocial media.
It is also interesting to note the im-provement much more significant in Education in-ference than Job inference.
This is in accord withwhat we find in Section 5.2, where education net-work exhibits stronger HOMOPHILE property thanJob network, enabling a significant benefit for ed-ucation inference, but limited for job inference.Spouse prediction also benefits from neighbor-ing effect and the improvement is about 12% forLOCAL(ENTITY) setting.
Unlike Education andJob prediction, for which in NEIGH-OBSERVEDsetting all neighboring variables are observed, net-work variables are hidden during spouse predic-tion.
By considering network information, themodel benefits from evident clues offered by tweetcorpus of user e?s spouse when making predictionfor e, but also suffers when erroneous decision aremade and then used for downstream predictions.NELL Baseline Notably, NELL achieves high-est Recall score for Education inference.
It isalso worth noting that most of education men-tions that NELL fails to retrieve are those in-volve irregular spellings, such as HarvardUniv andCornell U, which means Recall score for NELLbaseline would be even higher if these irregularspellings are recognized in a more sophisticatedsystem.
The reason for such high recall is that asour ground truths are obtained from Google plus,the users from which are mostly affiliated with de-cent schools found in NELL dictionary.
However,the high recall from NELL is sacrificed at preci-sion, as users can mention school entities in manyof situations, such as paying a visit or reportingsome relevant news.
NELL will erroneously clas-sify these cases as attribute mentions.NELL does not work out for Job, with a fairlypoor 0.0156 F1 score for LOCAL(ENTITY) and0.163 for LOCAL(TWEET).
Poor precision is ex-pected for as users can mention firm entity in agreat many of situations.
The recall score for171GLOBAL LOCAL(ENTITY) LOCAL(TWEET)P R F P R F P R FOur approachNEIGH-OBSERVED 0.804 0.515 0.628 0.524 0.780 0.627 0.889 0.729 0.801NEIGH-LATENT 0.755 0.440 0.556 0.420 0.741 0.536 0.854 0.724 0.783Only-Text ?- 0.735 0.393 0.512 0.345 0.725 0.467 0.809 0.724 0.764NELL ?- ?- ?- ?- 0.170 0.798 0.280 0.616 0.848 0.713Table 4: Results for Education PredictionGLOBAL LOCAL(ENTITY) LOCAL(TWEET)P R F P R F P R FOur approachNEIGH-OBSERVED 0.643 0.330 0.430 0.374 0.620 0.467 0.891 0.698 0.783NEIGH-LATENT 0.617 0.320 0.421 0.226 0.544 0.319 0.804 0.572 0.668Only-Text ?- 0.602 0.304 0.404 0.155 0.501 0.237 0.764 0.471 0.583NELL ?- ?- ?- ?- 0.0079 0.509 0.0156 0.094 0.604 0.163Table 5: Results for Job PredictionGLOBAL LOCAL(ENTITY) LOCAL(TWEET)P R F P R F P R FOur approach ?- 0.870 0.560 0.681 0.593 0.857 0.701 0.904 0.782 0.839Only-Text ?- 0.852 0.448 0.587 0.521 0.781 0.625 0.890 0.729 0.801Table 6: Results for Spouse PredictionNELL in job inference is also quite low as jobrelated entities exhibit a greater diversity of men-tions, many of which are not covered by the NELLdictionary.Vertical Comparison: Education, Job andSpouse Job prediction turned out to be muchmore difficult than Education, as shown in Ta-bles 4 and 5.
Explanations are as follows: (1)Job contains a much greater diversity of mentionsthan Education.
Education inference can benefit alot from the dictionary relevant feature which Jobmay not.
(2) Education mentions are usually asso-ciated with clear evidence such as homework, ex-ams, studies, cafeteria or books, while situationsare much more complicated for job as vocabular-ies are usually specific for different types of jobs.
(3) The boundary between a user working in anda fun for a specific operation is usually ambigu-ous.
For example, a Google engineer may con-stantly update information about outcome prod-ucts of Google, so does a big fun.
If the aforemen-tioned engineer barely tweets about working con-ditions or colleagues (which might still be ambigu-ous), his tweet collection, which contains many ofmentions about outcomes of Google product, willbe significantly similar to tweets published by aGoogle fun.
Such nuisance can be partly solvedby the consideration of network information, butnot totally.The relatively high F1 score for spouse predic-tion is largely caused by the great many of non-individual related entities in the dataset, the iden-tification of which would be relatively simpler.
Adeeper look at the result shows that the classifierfrequently makes wrong decisions for entities suchas userID and name entities.
Significant as somespouse relevant features are, such as love, hus-band, child, in most circumstances, spouse men-tions are extremely hard to recognize.
For exam-ple, in tweets ?Check this out, @alancross, it?sawesome bit.ly/1bnjYHh.?
or ?Happy Birth-day @alancross !?.
alancross can reasonably beany option among current user?s friend, colleague,parents, child or spouse.
Repeated mentions addno confidence.
Although we can identify alan-cross as spouse attribute once it jointly appearwith other strong spouse indicators, they are stillmany cases where they never co-appear.
How tointegrate more useful side information for spouserecognition constitutes our future work.6 Conclusion and Future WorkIn this paper, we propose a framework for user at-tribute inference on Twitter.
We construct the pub-licly available dataset based on distant supervisionand experiment our model on three useful userprofile attributes, i.e., Education, Job and Spouse.Our model takes advantage of network informa-tion on social network.
We will keep updating thedataset as more data is collected.One direction of our future work involves ex-ploring more general categories of user profile at-172tributes, such as interested books, movies, home-town, religion and so on.
Facebook would anideal ground truth knowledge base.
Another direc-tion involves incorporating richer feature space forbetter inference performance, such as multi-mediasources (i.e.
pictures and video).7 AcknowledgmentsA special thanks is owned to Dr. Julian McAuleyand Prof. Jure Leskovec from Stanford Universityfor the Google+ circle/network crawler, withoutwhich the network analysis would not have beenconducted.
This work was supported in part byDARPA under award FA8750-13-2-0005.ReferencesFaiyaz Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of twitter users from neighbors.
InICWSM.Edward Benson, Aria Haghighi, and Regina Barzilay.2011.
Event discovery in social media feeds.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 389?398.
As-sociation for Computational Linguistics.Sergey Brin.
1999.
Extracting patterns and relationsfrom the world wide web.
In The World Wide Weband Databases.Morgane Ciot, Morgan Sonderegger, and Derek Ruths.2013.
Gender inference of twitter users in non-english contexts.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle, Wash, pages 18?21.Michael Conover, Jacob Ratkiewicz, Matthew Fran-cisco, Bruno Gonc?alves, Filippo Menczer, andAlessandro Flammini.
2011.
Political polarizationon twitter.
In ICWSM.Mark Craven and Johan Kumlien 1999.
Construct-ing biological knowledge bases by extracting infor-mation from text sources.
In ISMB, volume 1999,pages 77?86.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Ido Guy, Naama Zwerdling, Inbal Ronen, DavidCarmel, and Erel Uziel.
2010.
Social media recom-mendation based on people and tags.
In Proceedingsof the 33rd international ACM SIGIR conference onResearch and development in information retrieval,pages 194?201.
ACM.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke SZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In ACL, pages 541?550.Jiwei Li and Claire Cardie.
2013.
Timeline generation:Tracking individuals on twitter.
Proceedings of the23rd international conference on World wide web.Percy Liang, Hal Daum?e III, and Dan Klein.
2008.Structure compilation: trading structure for features.In Proceedings of the 25th international conferenceon Machine learning.Wendy Liu and Derek Ruths.
2013.
Whats in a name?using first names as features for gender inference intwitter.
In 2013 AAAI Spring Symposium Series.Wendy Liu, Faiyaz Zamal, and Derek Ruths.
2012.Using social media to infer gender composition ofcommuter populations.
In Proceedings of the Whenthe City Meets the Citizen Workshop, the Interna-tional Conference on Weblogs and Social Media.Miller McPherson, Lynn Smith-Lovin, and James MCook.
2001.
Birds of a feather: Homophily in socialnetworks.
Annual review of sociology, pages 415?444.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Alan Mislove, Bimal Viswanath, Krishna Gummadi,and Peter Druschel.
2010.
You are who you know:inferring user profiles in online social networks.
InProceedings of the third ACM international confer-ence on Web search and data mining, pages 251?260.
ACM.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT, pages 380?390.Marco Pennacchiotti and Ana Popescu.
2011.
A ma-chine learning approach to twitter user classification.In ICWSM.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing.Delip Rao and David Yarowsky.
2010.
Detecting latentuser properties in social media.
In Proc.
of the NIPSMLSN Workshop.173Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of the 2nd in-ternational workshop on Search and mining user-generated contents, pages 37?44.
ACM.Delip Rao, Michael Paul, Clayton Fink, DavidYarowsky, Timothy Oates, and Glen Coppersmith.2011.
Hierarchical bayesian models for latent at-tribute detection in social media.
In ICWSM.Haibin Liu, Michael Wall, Karin Verspoor, et al 2012.Literature mining of protein-residue associationswith graph rules learned through distant supervision.Journal of biomedical semantics, 3(Suppl 3):S2.Alan Ritter, Sam Clark, Mausam, Oren Etzioni, et al2011.
Named entity recognition in tweets: an ex-perimental study.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1524?1534.
Association for Compu-tational Linguistics.Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-zioni.
2013.
Modeling missing data in distant su-pervision for information extraction.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 455?465.
Association for Computational Linguistics.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervi-sion for relation extraction.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages721?729.
Association for Computational Linguis-tics.Fei Wu and Daniel S Weld.
2007.
Autonomously se-mantifying wikipedia.
In Proceedings of the six-teenth ACM conference on Conference on infor-mation and knowledge management, pages 41?50.ACM.Jaewon Yang and Jure Leskovec.
2013.
Overlappingcommunity detection at scale: A nonnegative matrixfactorization approach.
In Proceedings of the sixthACM international conference on Web search anddata mining, pages 587?596.
ACM.174
