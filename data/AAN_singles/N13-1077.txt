Proceedings of NAACL-HLT 2013, pages 668?672,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsZipfian corruptions for robust POS taggingAnders S?gaardCenter for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkAbstractInspired by robust generalization and adver-sarial learning we describe a novel approachto learning structured perceptrons for part-of-speech (POS) tagging that is less sensitive todomain shifts.
The objective of our method isto minimize average loss under random distri-bution shifts.
We restrict the possible targetdistributions to mixtures of the source distri-bution and random Zipfian distributions.
Ouralgorithm is used for POS tagging and eval-uated on the English Web Treebank and theDanish Dependency Treebank with an average4.4% error reduction in tagging accuracy.1 IntroductionSupervised learning approaches have advanced thestate of the art on a variety of tasks in natural lan-guage processing, often resulting in systems ap-proaching the level of inter-annotator agreement onin-domain data, e.g.
in POS tagging, where Shenet al(2007) report a tagging accuracy of 97.3%.However, performance of state-of-the-art supervisedsystems is known to drop considerably on out-of-domain data.
State-of-the-art POS taggers trainedon the Penn Treebank (Marcus et al 1993) mappedto Google?s universal tag set (Petrov et al 2011)achieve tagging accuracies in the range of 89?91%on Web 2.0 data (Petrov and McDonald, 2012) .To bridge this gap we may consider using semi-supervised or transfer learning methods to adjust tonew target domains (Blitzer et al 2006; Daume III,2007), pooling unlabeled data from those domains.However, in many applications this is not possible.If we want to provide an online service or design apiece of software with many potential users coveringa wide range of use cases, we do not know the targetdomain in advance.
This is the usual problem of ro-bust learning, but in this paper we describe a novellearning algorithm that goes beyond robust learningby making various assumptions about the differencebetween the source domain and the (unknown) targetdomain.
Under these assumptions we can minimizeaverage loss under (all possible or a representativesample of) domain shifts.
We evaluate our approachon two recently introduced cross-domain POS tag-ging datasets.Our approach is inspired by work in robust gen-eralization (Ben-Tal and Nemirovski, 1998; Trafalisand Gilbert, 2007) and adversarial learning (Glober-son and Roweis, 2006; Dekel and Shamir, 2008;S?gaard and Johannsen, 2012).
Our approach alsobears similarities to feature bagging (Sutton et al2006).
Sutton et al(2006) noted that in learning oflinear models useful features are often swamped bycorrelating, but more indicative features.
If the moreindicative features are absent in the target domaindue to out-of-vocabulary (OOV) effects, we are leftwith the swamped features which were not updatedproperly.
This is, indirectly, the problem solved inadversarial learning with corrupted data points.
Ad-versarial learning can also be seen as a way of av-eraging exponentially many models (Hinton et al2012).Adversarial learning techniques have been devel-oped for security-related learning tasks, e.g.
wheresystems need to be robust to failing sensors.
We alsoshow how we can do better than straight-forward ap-668plication of adversarial learning techniques by mak-ing a second assumption about our data, namely thatdomains are mixtures of Zipfian distributions overour features.
Similar assumptions have been madebefore in computational linguistics, e.g.
by Goldbergand Elhadad (2008).2 Approach overviewIn this paper we consider the structured perceptron(Collins, 2002) ?
with POS tagging as our practicalapplication.
The structured perceptron is prone tofeature swamping (Sutton et al 2006), and we wantto prevent that using a technique inspired by adver-sarial learning (Globerson and Roweis, 2006; Dekeland Shamir, 2008).
The modification presented hereto the structured perceptron only affects a single lineof code in a publicly available implementation (seebelow), but the consequences are significant.Online adversarial learning (S?gaard and Jo-hannsen, 2012), briefly, works by sampling randomcorruptions of our data, or random feature deletions,in the learning phase.
A discriminative learner see-ing corrupted data points with missing features willnot update part of the model and will thus try tofind a decision boundary classifying the training datacorrectly relying on the remaining features.
This de-cision boundary may be very different from the deci-sion boundary found otherwise by the discriminativelearner.
If we sample enough corruptions, the modellearned from the corrupted data will converge on themodel minimizing average loss over all corruptions(Dekel and Shamir, 2008).Example Consider the plot in Figure 1.
The solidline with no stars (2d-fit) is the SVM fit in twodimensions, while the dashed line is what that fitamounts to if the feature x is missing in the tar-get.
The solid line with stars (1d-fit) is our fit if wecould predict the missing feature, training an SVMonly with the y feature.
The 1d-fit decision bound-ary only misclassifies a single data point comparedto the original fit which misclassifies more than 15negatives with the x feature missing.The plot thus shows that the best fit in m dimen-sions is often not the best in < m dimensions.
Con-sequently, if we think there is a risk that features willbe missing in the target, finding the best fit in m di-mensions is not necessarily the best we can do.
Of0 2 4 6 8 10 12?14?12?10?8?6?4?20242d-fit2d-fit with missing feature1d-fitFigure 1: The best fit in m dimensions is often not thebest in < m dimensions.course we do not know what features will be miss-ing in advance.
The intuition in adversarial learningis that we may obtain more robust decision bound-aries by minimizing loss over a set of possible fea-ture deletions.
We extend this idea below, modelingnot only OOV effects, but a broader class of distri-butional shifts.3 Structured perceptronThe structured perceptron (Collins, 2002) modelssequences as Markov chains of unobserved variables(POS), each emitting an observed variable (a wordform).
The structured perceptron is similar to the av-eraged perceptron (Freund and Schapire, 1999), ex-cept data points are sequences of vectors rather thanjust vectors.
Consequently, the structured percep-tron does not predict a class label but a sequence oflabels (using Viterbi decoding).
In learning we up-date the features at the positions where the predictedlabels are different from the true labels.
We do thisby adding weight to features present in the correctsolution and subtracting weight from features onlypresent in the predicted solution.
The generic aver-aged perceptron learning algorithm is presented inFigure 2.
A publicly available and easy-to-modifyPython reimplementation of the structured percep-tron can be found in the LXMLS toolkit.1 We usethe LXMLS toolkit as our baseline with the defaultfeature model, but use the PTB tagset rather than theGoogle tagset (Petrov et al 2011) used by defaultin the LXMLS toolkit.1https://github.com/gracaninja/lxmls-toolkit6691: X = {?yi,xi?
}Ni=12: w0 = 0,v = 0, i = 03: for k ?
K do4: for n ?
N do5: if sign(w ?
x) 6= yn then6: wi+1 ?
update(wi)7: i?
i+ 18: end if9: v?
v + wi10: end for11: end for12: return w = v/(N ?K)Figure 2: Generic averaged perceptron4 Minimizing loss under OOV effectsWe will think of domain shifts as data point corrup-tions.
S?gaard and Johannsen (2012) model domainshifts using binary vectors of length m where m isthe size of of our feature representation.
Each vectorthen represents an expected OOV effect by encodingwhat features are (predicted to be) missing in the tar-get data, i.e.
the ith feature will be missing if the ithelement of the binary vector is 0.
However, sincewe are minimizing average loss under OOV effectsit makes sense to restrict the class of vectors to en-code OOV effects that we are likely to observe.
Thiscould, for example, involve fixing an expected rateof missing features or bounding it by some interval,or it could involve distinguishing between featuresthat are likely to be missing in the target and fea-tures that are not.
Here is what we do in this paper:Rather than thinking of domain shifts as some-thing that deletes features, we propose to see do-main shifts as something making certain featuresless likely to occur in our data.
We will in otherwords simulate soft OOV effects, rather than hardOOV effects.
One way to think of this is as an im-portance weighting of our features.
This section pro-vides some intuition for using inverse Zipfian distri-butions as weight functions.Say we are interested in making a model ?D1learned from a known distribution D1 robust againstthe distributional differences betweenD1 and an un-known distribution D2.
These two distributions aresomehow related to a distributionD0 (the underlyinglanguage distribution from which the domain distri-butions are sampled).It is common to assume that linguistic distribu-1: X = {?yi,xi?
}Ni=12: w0 = 0,v = 0, i = 03: for k ?
K do4: for n ?
N do5: ?
?
random.zipf(3,M)6: if sign(w ?
x ?
?)
6= yn then7: wi+1 ?
update(wi)8: i?
i+ 19: end if10: v?
v + wi11: end for12: end for13: return w = v/(N ?K)Figure 3: Z3SPtions follow power laws (Zipf, 1935; Goldberg andElhadad, 2008).
We will assume thatD1 = D0?Z1whereZ1 is some Zipfian distribution.
SayD0 ?
Z0is the master Zipfian distribution of language L0.
Ifwe assume that (otherwise independent) domainsL1and L2 follow products of Zipfians Z0 ?
Z1 andZ0 ?Z2, we derive the following:Say w = ?Z0?Z1 is the model learned from thesource data.
The ideal model is w?
= ?Z0?Z2 , butboth Zipfians Z1 and Z2 are unknown.
Since Z2is unknown (and in many applications, we want tomodel several Zi), the overall best model we canhope for is w?
= ?Z0 .
Z0 is also unknown, but wecan observe a finite sample Z0 ?Z1.
Since the den-sity of Z1 is directly related to the weights in w, acrude estimate of ?Z0 would be w?
?
w 1Z1 .
Sincewe cannot observe Z1, we instead try to minimizeaverage loss under all hypotheses about Z1.In practice, we implement the idea of reweight-ing by random inverse Zipfian distributitons (insteadof binary vectors) in the following way: Passingthrough the data in averaged perceptron learning(Figure 2), we consider one data point at a time.
Inorder to minize loss in all possible domains, we needto consider all possible inverse Zipfian reweightings.This would be possible if we provided a convexformulation of the minimization problem along thelines of Dekel and Shamir (2008), but instead werandomly sample from a Zipfian and factor its in-verse into our dataset.
The parameter of the Zipfiansis set (to 3) on development data (the EWT-email de-velopment data).
The modified learning algorithm,Z3SP, is presented in Figure 3.6705 POS taggingPOS tagging is the problem of assigning syntacticcategories or POS to tokenized word forms in run-ning text.
Most approaches to POS tagging use su-pervised learning to learn sequence labeling modelsfrom annotated ressources.
The major ressource forEnglish is the Wall Street Journal (WSJ) sections ofthe English Treebank (Marcus et al 1993).
POStaggers are usually trained on Sect.
0?18 and eval-uated on Sect.
22?24.
In this paper we are not in-terested in in-domain performance on WSJ data, butrather in developing a robust POS tagger that is lesssensitive to domain shifts than current state-of-the-art POS taggers and use the splits from a recent pars-ing shared task rather than the standard POS taggingones.6 ExperimentsWe train our tagger on Sections 2?21 of the WSJsections of the English Treebank, in the Ontotes4.0 release.
This was also the training data usedin the experiments in the Parsing the Web (PTW)shared task at NAACL 2012.2 In the shared taskthey used the coarse-grained Google tagset (Petrovet al 2011).
We believe this tagset is too coarse-grained for most purposes (Manning, 2011) and doexperiments with the original PTB tagset instead.Our evaluation data comes from the English WebTreebank (EWT),3 which was also used in the PTWshared task.
The EWT contains development andevaluation data for five domains: answers (from Ya-hoo!
), emails (from the Enron corpus), BBC news-groups, Amazon reviews, and weblogs.
In order notto optimize on in-domain data, we tune on the Emaildevelopment data and evaluate on the remaining do-mains (the test sections).The Web 2.0 data used for evaluation contains alot of non-canonical language use.
An example isthe sentence you r retarded.
from the Email section.The POS tagger finds no support for r as a verb in thetraining data, but needs to infer this from the context.We also include experiments on the Danish De-pendency Treebank (DDT) (Buch-Kromann, 2003),which comes with meta-data enabling us to singleout four domains: newspaper, law, literature and2https://sites.google.com/site/sancl2012/home/shared-task3LDC Catalog No.
: LDC2012T13.SP BSP Z3SPEWT-answers 85.22 85.45 85.59EWT-newsgroups 86.82 86.94 87.42EWT-reviews 84.92 85.14 85.67EWT-weblogs 87.00 87.06 87.39DDT-law 92.38 92.80 93.35DDT-lit 93.61 93.80 93.85DDT-mag 94.71 94.44 94.68Table 1: Results.
BSP samples binary vectors with prob-abilities {0 : 0.1, 1 : 0.9}magazines.
We train our tagger on the newspaperdata and evaluate on the remaining three sections.6.1 ResultsThe results are presented in Table 1.
We first notethat improvements over the structured perceptronare statistically significant with p < 0.01 across alldomains, except DDT-mag.
We also note that us-ing inverse Zipfian reweightings is better than usingbinary vectors in almost all cases.
We believe thatthese are strong results given that we are assumingno knowledge of the target domain, and our mod-ification of the learning algorithm does not affectcomputational efficiency at training or test time.
Theaverage error reduction of Z3SP over the structuredperceptron (SP) is 8%.
Since using inverse Zipfianreweightings seems more motivated for node poten-tials than for edge potentials, we also tried usingBSP for edge potentials and Z3SP for node poten-tials.
This mixed model acchieved 93.70, 93.91 and94.35 on the DDT data, which on average is slightlybetter than Z3SP.7 ConclusionsInspired by robust generalization and adversariallearning we introduced a novel approach to learningstructured perceptrons for sequential labeling, whichis less sensitive to OOV effects.
We evaluated ourapproach on POS tagging data from the EWT andthe DDT with an average 4.4% error reduction overthe structured perceptron.AcknowledgementsAnders S?gaard is funded by the ERC Starting GrantLOWLANDS No.
313695.671ReferencesAharon Ben-Tal and Arkadi Nemirovski.
1998.
Robustconvex optimization.
Mathematics of Operations Re-search, 23(4).John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.Matthias Buch-Kromann.
2003.
The Danish Depen-dency Treebank and the DTAG Treebank Tool.
InTLT.Michael Collins.
2002.
Discriminative training methodsfor Hidden Markov Models.
In EMNLP.Hal Daume III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.Ofer Dekel and Ohad Shamir.
2008.
Learning to classifywith missing and corrupted features.
In ICML.Yoav Freund and Robert Schapire.
1999.
Large marginclassification using the perceptron algorithm.
MachineLearning, 37:277?296.Amir Globerson and Sam Roweis.
2006.
Nightmareat test time: robust learning by feature deletion.
InICML.Yoav Goldberg and Michael Elhadad.
2008. splitSVM:fast, space-efficient, non-heuristic, polynomial kernelcomputation for NLP applications.
In ACL.Geoffrey Hinton, N. Srivastava, A. Krizhevsky,I.
Sutskever, and R. Salakhutdinov.
2012.
Im-proving neural networks by preventing co-adaptationof feature detectors.
http://arxiv.org/abs/1207.0580.Chris Manning.
2011.
Part-of-speech tagging from97%?to 100%: Is it time for some linguistics?
In CI-CLing.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Lin-guistics, 19(2):313?330.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 Shared Task on Parsing the Web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Slav Petrov, Dipanjan Das, and Ryan McDonald.2011.
A universal part-of-speech tagset.
CoRRabs/1104.2086.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifica-tion.
In ACL.Anders S?gaard and Anders Johannsen.
2012.
Robustlearning in random subspaces: equipping NLP againstOOV effects.
In COLING.Charles Sutton, Michael Sindelar, and Andrew McCal-lum.
2006.
Reducing weight undertraining in struc-tured discriminative learning.
In NAACL.T Trafalis and R Gilbert.
2007.
Robust support vectormachines for classification and computational issues.Optimization Methods and Software, 22:187?198.George Zipf.
1935.
The psycho-biology of language.Houghton Mifflin.672
