Proceedings of the SIGDIAL 2013 Conference, pages 442?451,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsStructured Discriminative Model For Dialog State TrackingSungjin LeeLanguage Technologies Institute,Carnegie Mellon University,Pittsburgh, Pennsylvania, USAsungjin.lee@cs.cmu.eduAbstractMany dialog state tracking algorithms havebeen limited to generative modeling due to theinfluence of the Partially Observable MarkovDecision Process framework.
Recent analyses,however, raised fundamental questions on theeffectiveness of the generative formulation.
Inthis paper, we present a structureddiscriminative model for dialog state trackingas an alternative.
Unlike generative models,the proposed method affords the incorporationof features without having to considerdependencies between observations.
It alsoprovides a flexible mechanism for imposingrelational constraints.
To verify theeffectiveness of the proposed method, weapplied it to the Let?s Go domain (Raux et al2005).
The results show that the proposedmodel is superior to the baseline andgenerative model-based systems in accuracy,discrimination, and robustness to mismatchesbetween training and test datasets.1 IntroductionWith the recent remarkable growth of speech-enabled applications, dialog state tracking hasbecome a critical component not only for typicaltelephone-based spoken dialog systems but alsofor multi-modal dialog systems on mobiledevices and in automobiles.
With presentAutomatic Speech Recognition (ASR) andSpoken Language Understanding errors, it isimpossible to directly observe the true user goaland action.
It is crucial, therefore, to accuratelyestimate the true dialog state from erroneousobservations as a dialog unfolds.Since the Partially Observable MarkovDecision Process (POMDP) framework hasoffered a well-founded theory for both statetracking and decision making, most earlierstudies adopted generative temporal models, thetypical way to formulate belief state updates forPOMDP-based systems (Williams and Young,2007).
Several approximate methods have alsoemerged to tackle the vast complexity ofrepresenting and maintaining belief states, e.g.,partition-based approaches (Gasic and Young,2011; Lee and Eskenazi, 2012a; Williams, 2010;Young et al 2010) and Bayesian network (BN)-based methods (Raux and Ma, 2011; Thomsonand Young, 2010).To verify the effectiveness of these techniques,some were deployed in a real user system for theSpoken Dialog Challenge (Black et al 2010).The results demonstrated that the use ofstatistical approaches helps estimate the truedialog state and achieves increased robustness toASR errors (Thomson et al 2010b; Lee andEskenazi 2012b; Williams, 2011; Williams,2012).
However, further analysis also raisedseveral fundamental questions about theformulation of the belief update as a generativetemporal model: limitation in modelingcorrelations between observations in differenttime slices; and the insensitive discriminationbetween true and false dialog states (Williams,2012).
There are more potential downsides ofgenerative models, which will be discussed indetail in Section 2.On the other hand, natural languageprocessing, computer vision and other machinelearning research areas have increasinglyprofited from discriminative approaches.Discriminative approaches directly model theclass posteriors, allowing them to incorporate arich set of features without worrying about theirdependencies on one another.
This could result ina deficient probability distribution withgenerative models (Sutton and McCallum, 2006).442The aim of this paper is to describe a firstattempt to adopt a structured discriminativemodel for dialog state tracking.
To handlenonlinearity of confidence score and variablecardinality of the possible values of outputvariables, the traditional approaches applied toother tasks have been modified.To verify the effectiveness of the proposedmethod, we applied it to the Let?s Go1 domain(Raux et al 2005).
The proposed model wascompared with its unstructured version withoutrelational constraints, the baseline system whichalways takes the top ASR hypothesis in the entiredialog, and finally the AT&T Statistical DialogToolkit2 (ASDT) which is one of the state-of-the-art generative model-based systems.This paper is structured as follows.
Section 2describes previous research and the novelty ofour approach.
Section 3 elaborates on ourproposed structured discriminative approach.Section 4 explains the experimental setup.Section 5 presents and discusses the results.Finally, Section 6 concludes with a briefsummary and suggestions for future research.2 Background and Related WorkA statistical dialog system needs to update itsdialog state when taking the action    andobserving  .
Since the POMDP frameworkassumes the Markovian property between states,updating a belief state involves only the previousbelief state, the system action, and the currentobservation:(  )     (  |  )?
(  |    ) ( )(1)where  ( )  denotes the probability distributionover states  ,  ( | )  the likelihood of   giventhe state  ,  (  |    )  the state transitionprobability, and   is a normalizing constant.In practice, however, belief state updates(Equation 1) in many domains are oftencomputationally intractable due to thetremendously large size of the belief state space.In order to reduce the complexity of the beliefstates, the following belief state factorization hasbeen commonly applied to the belief updateprocedure (Williams et al 2005):()    (2)1 In this task, users call the spoken dialog system to2 http://www2.research.att.com/sw/tools/asdt/(  |)?(|)??
(  |)??
(|)??
(      )where  ,  ,   , represents the user goal, thedialog history, and the user action, respectively.Partition-based approaches (Gasic and Young,2011; Lee and Eskenazi, 2012; Williams, 2010;Young et al 2010) attempt to group user goalsinto a small number of partitions and split apartition only when this distinction is required byobservations.
This property endows it with thehigh scalability that is suitable for fairly complexdomains.
In partition-based approaches, the goalmodel in Equation 2 is further approximated asfollows:?
(  |)  ?
(  | )(3)where   is a partition from the current turn.
Oneof the flaws of the partition-based approaches isthat when one defines a partition to be aCartesian product of subsets of possible values ofmultiple concepts, it will be difficult to adoptsophisticated prior distributions over partitions.That may lead to either employing very simplepriors such as uniform distribution ormaintaining partition structures separately foreach concept.
This is one of the main reasonsthat the previous partition-based approachescould not incorporate probabilistic or softrelational constraints into the models.To allow for relational constraints andalleviate the complexity problem at the sametime, Dynamic Bayesian Networks (DBN) withmore detailed structures for the user goal havealso been developed (Thomson and Young,2010).
Nevertheless, there is still a limitation onthe types of constraints they can afford.
SinceDBN is a directed network, it is not quite suitablefor specifying undirected constraints.
Forexample, in the Let?s Go domain, users can saythe same name for the arrival place as thedeparture place if they are distracted, missing theprompt for the arrival place and so repeatingthemselves with the departure place.
It is alsopossible for some place names with similarpronunciations to be recognized as the same (e.g.Forbes and Forward).
The system can, in this443case, use the constraint that the departure andarrival places may not be identical.Another drawback of both approaches is that itis hard to incorporate a rich set of observationfeatures, which are often partly dependent oneach other.
One can create a feature whichreflects ASR error correlations betweenobservations in different time slices.
For example,a hypothesis that repeats with low confidencescores is likely to be a manifestation of ASRerror correlations.
Thus, the highest confidencescore that a hypothesis has attained so far couldbe a useful feature in preventing repeatedincorrect hypotheses from defeating the correcthypothesis (which had a higher score but wasonly seen once).
Another useful feature could bethe distribution of confidence scores that ahypothesis has attained thus far, since it may nothave the same effect as having a singleobservation with the total score due to thepotential nonlinearity of confidence scores.There are many other potentially useful features.The entire list of features is found in Section 3.2.Dynamic Probabilistic Ontology Trees (Rauxand Ma, 2011) is another method based uponDBN which does not impose explicit temporalstructures.
Since it does not impose temporalstructures, it is more flexible in consideringmultiple observations together.
However, it isstill difficult to capture co-dependent features,which are exemplified above, withoutintroducing probabilistic deficiency due to itsgenerative foundation (Appendix E).
Moreover,the quality of the confidence score will be criticalto all generative models up to that point sincethey do not usually try to handle potentialnonlinearity in confidence scores.As far as discriminative models are concerned,the Maximum Entropy (MaxEnt) model has beenapplied (Bohus and Rudnicky, 2006).
But themodel is limited to a set of separate models foreach concept, not incorporating relationaldependencies.
Also, it is restricted to maintainonly top K-best hypotheses where K is apredefined parameter, resulting in potentialdegradation of performance and difficulties inextending it to structured models.
In Section 3,our structured discriminative model is described.It is designed to take into consideration theaforementioned limitations of generative modelsand the previous discriminative approach.3 Structured Discriminative ModelUnlike generative models, discriminative modelsdirectly model the class posterior given theobservations.
Maximum Entropy is one of mostpowerful undirected graphical models (AppendixA).
But for some tasks that predict structuredoutputs, e.g.
a dialog state, MaxEnt becomesimpractical as the number of possible outputsastronomically grows.
For example, in the LetsGo domain, the size of possible joint outputconfigurations is around 1017.
To address thisproblem, Conditional Random Field (CRF) wasintroduced which allows dependencies betweenoutput variables to be incorporated into thestatistical model (Appendix B).3.1 Model Structure for Dialog StateTrackingWe now describe our model structure for dialogstate tracking in detail using the Let?s Go domainas a running example.
The graphicalrepresentation of the model is shown in Fig.
1.The global output nodes for each concept (clearnodes in Fig.
1) are unlike other temporalmodels, where a set of output nodes are newlyintroduced for each time slice.
Instead, as adialog proceeds, a set of new observations(shaded nodes in Fig.
1) are continuouslyattached to the model structure and the featureFigure 1: Factor graph representing the structured discriminative model in the Let?s Go domain.
The shadednodes show observed random variables.
The smaller solid node is the deterministic parameters and explicitlyrepresents parameter sharing between two associated factors.444functions are responsible for producing fixedlength feature vectors.
The sequence ofobservations includes not only ASR N-best listsbut also system actions from the beginning of thedialog to the current time slice  .
Any outputnode can be freely connected to any other toimpose desirable constraints between themwhether or not the connections form a loop (solidlines in Fig.
1).In practice, models rely extensively onparameter tying, e.g., transition parameters in aHidden Markov Model.
One specific example ofrelational constraints and parameter tyingnaturally arises in the Let?s Go domain: thefeature function which indicates whether a placeis valid on a given route could use the sameweights for both departure and arrival places (thesolid node and the associated factor nodes in Fig.1).
Parameter tying is also implicitly takingplace.
This is crucial for robust estimation of themodel parameters in spite of data sparseness.Some concepts such as from and to can haveabout 104 values but most of them are not seen inthe training corpus.
Thus we aggregate severalfeature functions which differ only by outputlabels into one common feature function so thatthey can gather their statistics together.
Forexample, we can aggregate the observationfeature functions (dotted lines in Fig.
1)associated with each output label except forNone (Section 3.2).
Here, None is a special valueto indicate that the true hypothesis has not yetappeared in the ASR N-best lists.
Since there aregenerally a large number of values for eachconcept, the probability of the true hypothesiswill be very small unless the true hypothesisappears on the N-best lists.
Thus we can makeinferences on the model very quickly by focusingonly on the observed hypotheses at the cost oflittle performance degradation.
Additionally, thefeature function aggregation allows for the entireobserved hypotheses to be incorporated withoutbeing limited to only the pre-defined number ofhypotheses.3.2 Model FeaturesIn this section, we describe the model featureswhich are central to the performance ofdiscriminative models.
Features can be broadlysplit into observation features and relationalfeatures.
To facilitate readers?
understanding anexample of feature extraction is illustrated in Fig.2.One of the most fundamental features fordialog state tracking should exploit theconfidence scores assigned to an informedhypothesis.
The simplest form could be directuse of confidence scores.
But often pre-trainedconfidence measures fail to match the empiricaldistribution of a given dialog domain (Lee andEskenazi, 2012; Thomson et al2010a).
Also thedistribution of confidence scores that ahypothesis has attained so far may not have thesame effect as the total score of the confidencescores (e.g., in Fig.
2, two observations for 61Cwith confidence score 0.3 vs. 0.6 which is thesum of the scores).
Thus we create a featurefunction that divides the range of confidencescores into bins and returns the frequency ofobservations that fall into the corresponding bin:(){(       ())(4)where      ( )  returns the set of confidencescores whose action informs   in the sequence ofobservations.
(   )  computes thefrequency of observations that fall into thebin.There are two types of grounding actionswhich are popular in spoken dialog systems, i.e.,implicit and explicit confirmation.
To leverageaffirmative or negative responses to such systemacts, the following feature functions areintroduced in a similar fashion as thefeature function:Figure 2: A simplified example of feature extraction for the route concept.
It shows the values that each featurewill have when three consecutive user inputs are given.445(){(       ())(5)(){(       ())(6)where      ( )  /      ( )  returns the set ofconfidence scores whose associated actionaffirms / negates   in the sequence ofobservations.
(){()(7)where          ( ) indicates whether or not theuser has negated the system?s implicitconfirmation in the sequence of observations.Another interesting feature function is the so-called baseline feature which exploits the outputof a baseline system.
The following featurefunction emulates the output of the baselinesystem which always selects the top ASRhypothesis for the entire dialog:(){(           ())(8)where          ( )  returns the maximumconfidence score whose action informs   in thesequence of observations.
(   )  indicateswhether or not the maximum score falls into thebin.Yet another feature function of this kind is theaccumulated score which adds up all confidencescores associated with inform and affirm andsubtracts the ones with negation:(){?
()?
()?
()(9)Note that such feature functions as( )  and          ( )  are notindependent of the others defined previously,which may cause generative models to producedeficient probability distributions (Appendix E).It is known that prior information can boostthe performance (Williams, 2012) if the prior iswell-estimated.
One of advantages of generativemodels is that they provide a natural mechanismto incorporate a prior.
Discriminative modelsalso can exploit a prior by introducing additionalfeature functions:(){(            ( ))(10)where           ( ) returns the fraction ofoccurrences of   in the set of true labels.If the system cannot process a certain userrequest, it is highly likely that the user changehis/her goal.
The following feature function isdesigned to take care of such cases:()  {( )(11)where     ( ) indicates whether or not   is out-of-coverage.As with other log-linear models, we also havefeature functions for bias:()()   {(12)Note that we have an additional bias term forNone to estimate an appropriate weight for it.Regarding relational constraints, we havecreated two feature functions.
To reflect thepresumption that it is likely for the truehypothesis for the place concepts (i.e.
from andto) to be valid on the true hypothesis for theroute concept, we have:(   ){(   )(13)where      (   )  indicates whether or not theplace   is valid on the route  .
Another featurefunction considers the situation where the sameplace name for both departure and arrival placesis given:(     ){(14)3.3 Inference & Parameter EstimationOne of the common grounding actions of spokendialog systems is to ask a confirmation questionabout hypotheses which do not have sufficientmarginal beliefs.
This makes marginal inference446to be one of the fundamental reasoning tools fordialog state tracking.
In treelike graphs, exactmarginal probabilities are efficiently computableby using the Junction Tree algorithm (Lauritzenand Spiegelhalter, 1988) but in general it isintractable on structured models with loops.Since it is highly likely to have loopystructures in various domains (e.g.
Fig.
1), weneed to adopt approximate inference algorithmsinstead.
Note that CRF (Equation 16) is aninstance of the exponential family.
For theexponential family, it is known that the exactinference can be formulated as an optimizationproblem (Wainwright and Jordan, 2008).
Thevariational formulation opens the door to variousapproximate inference methods.
Among manypossible approximations, we adopt the TreeReweighted Belief Propagation (TRBP) methodwhich convexifies the optimization problem thatit guarantees finding the global solution(Appendix C).On the other hand, joint inference alsobecomes important for either selecting ahypothesis to confirm or determining the finaljoint configuration when there exist strongrelational dependencies between concepts.Moreover, we would like to find not just the bestconfiguration but rather the top  configurations.Since the number of concept nodes is generallymoderate, we approximate the inference bysearching for the top   configurations onlywithin the Cartesian product of the tophypotheses of each concept.
For domains with alarge number of concepts, one can use moreadvanced methods, e.g., Best Max-MarginalFirst (Yanover and Weiss, 2004) and SpanningTree Inequalities and Partitioning forEnumerating Solutions (Fromer and Globerson,2009).The goal of parameter estimation is tominimize the empirical risk.
In this paper, weadopt the negative of the conditional loglikelihood (Appendix D).
Given the partialderivative (Equation 26), we employ theOrthant-wise Limited-memory Quasi Newtonoptimizer (Andrew and Gao, 2007) for L1regularization to avoid model overfitting.4 Experimental SetupIn order to evaluate the proposed method, twovariants of the proposed method (discriminativemodel (DM) and structured discriminative model(SDM)) were compared with the baseline system,which always takes the top ASR hypothesis forthe entire dialog and outputs the jointconfiguration using the highest average score,and the ASDT system as being the state-of-the-art partition-based model (PBM).
To train andevaluate the models, two datasets from theSpoken Dialog Challenge 2010 are used: a)AT&T system (Williams, 2011), b) Cambridgesystem (Thomson et.
al, 2010b).For discriminative models, we used 10 binsfor the feature functions that need to discretizetheir inputs (Section 3.2).
Parameter tying forrelational constraints was applied to dataset Abut not to dataset B.
To make sure that TRBPproduces an upper bound on the original entropy,the constants    were set to be     for SDM and1 for DM (Appendix C).
Also the weights for L1regularization were set to be 10 and 2.5 for theprior features and the other features, respectively.These values were chosen through cross-validation over several values rather than doing athorough search.
For the ASDT system, wemodified it to process implicit confirmation andincorporate the prior distribution which wasestimated on the training corpus.
The priordistribution was smoothed by approximateGood-Turing estimation on the fly when thesystem encounters an unseen value at run time.Two aspects of tracker performance weremeasured at the end of each dialog, i.e.
Accuracyand Receiver Operating Characteristic (ROC).Accuracy measures the percent of dialogs wherethe tracker?s top hypothesis is correct.
ROCassesses the discrimination of the tophypothesis?s score.
Note that we consideredNone as being correct if there is no ASRhypothesis corresponding to the transcription.
Ifall turns are evaluated regardless of context,concepts which appear earlier in the dialog willbe measured more times than concepts later inthe dialog.
In order to make comparisons acrossconcepts fair, concepts are only measured whenRoute From To Date Time JointTraining 378 334 309 33 30 378Test 379 331 305 54 50 379(a) Dataset ARoute From To Date Time JointTraining 94 403 353 18 217 227Test 99 425 376 18 214 229(b) Dataset BTable 1: Counts for each concept represent thenumber of dialogs which have non-empty utterancesfor that concept.
From and To concepts add up thecounts for their sub-concepts.
Joint denotes the jointconfiguration of all concepts.447they are in focus.
It does not, however, allow fora tracker to receive score for new estimationsabout concepts that are not in focus.
In addition,dialogs with more turns will have a greater effectthan dialogs with fewer turns.
Therefore we onlymeasure concepts which appear in the dialog atthe last turn of the dialog before restart.
Thestatistics of the training and test datasets aresummarized in Table 1.5 Results and DiscussionThe results indicate that discriminative methodsoutperform the baseline and generative methodby a large performance gap for both dataset Aand B (Table 2).
Also, SDM exceeds DM,demonstrating the effectiveness of usingrelational constraints.
Furthermore, theperformance of SDM surpasses that of the bestsystem in the Dialog State Tracking Challenge3(Lee and Eskenazi, 2013).
Even though thegenerative model underperforms discriminativemodels, it is also shown that dialog state trackingmethods in general are effective in improvingrobustness to ASR errors.
Another noteworthyresult is that the gains for Joint by usingdiscriminative models are much larger than thosefor All.
Estimating joint configurations correctlyis crucial to eventually satisfy the user?s request.This result implies that the proposed modelperforms evenly well for all concepts and is morerobust to the traits of each concept.
For example,PBM works relatively poorly for To on dataset A.What makes To different is that the quality of the3 http://research.microsoft.com/en-us/events/dstc/ASR hypotheses of the training data is muchbetter than that of test data: the baseline accuracyon the training data is 84.79% while 77.05% onthe test data.
Even though PBM suffers thismismatch, the discriminative models are doingwell without significant differences, implyingthat the discriminative models achieverobustness by considering not just the confidencescore but also several features together.Since there has been no clear evidence that theuse of N-best ASR hypotheses is helpful fordialog state tracking (Williams, 2012), we alsoreport accuracies while varying the number of N-best hypotheses.
The results show that the use ofN-bests helps boost accuracy across all modelson dataset A.
However, interestingly it hampersthe performance in the case of dataset B. Itdemonstrates that the utility of N-bests dependson various factors, e.g., the quality of N-bestsand dialog policies.
The system which yieldeddataset A employs implicit and explicitconfirmation much more frequently than thesystem which produced dataset B does.
Theproposed model trained on dataset A withoutconfirmation features incorporated actuallyshowed a slight degradation in accuracy whenusing more than 3-bests.
This result indicates thatwe need to take into consideration the type ofdialog strategy to determine how manyhypotheses to use.
Thus, it can be conceivable todynamically change the range of N-bestsaccording to how a dialog proceeds.
That allowsthe system to reduce processing time when adialog goes well.All (%)  JointN-best Baseline PBM DM SDM  Baseline PBM DM SDM1-best 74.80 77.93 83.65 83.74  53.56 54.62 60.16 60.693-best 74.80 84.00 88.83 89.10  53.56 64.38 70.18 70.985-best 74.80 84.54 89.54 89.81  53.56 65.70 72.30 73.09All 74.80 84.81 89.81 90.26  53.56 65.96 73.09 74.67(a) Dataset AAll  JointN-best Baseline PBM DM SDM  Baseline PBM DM SDM1-best 65.46 68.73 78.00 80.12  11.35 12.23 26.20 30.133-best 65.46 68.02 78.00 79.51  11.35 11.35 27.51 28.825-best 65.46 67.40 77.92 79.15  11.35 11.79 24.89 25.76All 65.46 66.61 78.00 79.24  11.35 11.79 24.89 25.76(b) Dataset BTable 2: Accuracy of the comparative models.
The best performaces across the models are marked in bold.
Allmeans a weighted average accuracy across all concepts.448The ROC curves assess the discrimination ofthe top hypothesis?
score (Fig.
3).
Note that thediscriminative models are far better than PBM onboth dataset A and B.
In fact, PBM turns out tobe even worse than the baseline.
The betterdiscrimination can give rise to additional valuesof a tracker.
For example, it can reduceunnecessary confirmations for values withsufficiently high belief.
Also, it enables a modelto adapt to test data in an unsupervised mannerby allowing us to set a proper threshold toproduce predictive labels.6 ConclusionIn this paper, we presented the first attempt, toour knowledge, to create a structureddiscriminative model for dialog state tracking.Unlike generative models, the proposed methodallows for the incorporation of various featureswithout worrying about dependencies betweenobservations.
It also provides a flexiblemechanism to impose relational constraints.
Theresults show that the discriminative models aresuperior to the generative model in accuracy,discrimination, and robustness to mismatchesbetween training and test datasets.
Since we usedrelatively simple features for this work, there ismuch room to boost performance through featureengineering.
Also, more thorough search forregularization weights can give additionalperformance gain.
Moreover, one can applydifferent loss functions, e.g., hinge loss to obtainstructured support vector machine.
In order tofurther confirm if the performance improvementby the proposed method can be translated to theenhancement of the overall spoken dialogsystem, we need to deploy and assess it with realusers.AcknowledgmentsThis work was funded by NSF grant IIS0914927.The opinions expressed in this paper do notnecessarily reflect those of NSF.
The authorwould like to thank Maxine Eskenazi for helpfulcomments and discussion.ReferencesG.
Andrew and J. Gao, 2007.
Scalable training of L1-regularized log-linear models.
In Proceedings ofICML.A.
Black et al 2011.
Spoken dialog challenge 2010:Comparison of live and control test results.
InProceedings of SIGDIAL.D.
Bohus and A. Rudnicky, 2006.
A K hypotheses +other belief updating model.
In Proceedings ofAAAI Workshop on Statistical and EmpiricalApproaches for Spoken Dialogue Systems.M.
Fromer and A. Globerson, 2009.
An LP View ofthe M-best MAP problem.
Advances in NeuralInformation Processing Systems, 22:567-575.M.
Gasic and S. Young, 2011.
Effective handling ofdialogue state in the hidden information statePOMDP-based dialogue manager.
ACMTransactions on Speech and Language Processing,7(3).S.
Lauritzen and D. J. Spiegelhalter, 1988.
LocalComputation and Probabilities on GraphicalStructures and their Applications to ExpertSystems.
Journal of Royal Statistical Society,50(2):157?224.S.
Lee and M. Eskenazi, 2012a.
Exploiting Machine-Transcribed Dialog Corpus to Improve MultipleDialog  States Tracking Methods.
In Proceedingsof SIGDIAL, 2012.Figure 3: Weighted average ROC curves across all concepts449S.
Lee and M. Eskenazi, 2012b.
POMDP-based Let?sGo System for Spoken Dialog Challenge.
InProceedings of SLT.S.
Lee and M. Eskenazi, 2013.
Recipe For BuildingRobust Spoken Dialog State Trackers: Dialog StateTracking Challenge System Description.
Submittedto SIGDIAL, 2013.A.
Raux, B. Langner, D. Bohus, A. W Black, and M.Eskenazi, 2005.
Let?s Go Public!
Taking a SpokenDialog System to the Real World.
In Proceedingsof Interspeech.A.
Raux and Y. Ma, 2011.
Efficient ProbabilisticTracking of User Goal and Dialog History forSpoken Dialog Systems.
In Proceedings ofInterspeech.C.
Sutton and A. McCallum, 2006.
An Introduction toConditional Random Fields for RelationalLearning.
Introduction to Statistical RelationalLearning.
Cambridge: MIT Press.B.
Thomson and S. Young, 2010.
Bayesian update ofdialogue state: A POMDP framework for spokendialogue systems.
Computer Speech & Language,24(4):562-588.B.
Thomson, F. Jurccek, M. Gasic, S. Keizer, F.Mairesse, K. Yu, S. Young, 2010a.
Parameterlearning for POMDP spoken dialogue models.
InProceedings of SLT.B.
Thomson, K. Yu, S. Keizer, M. Gasic, F. Jurcicek,F.
Mairesse, S. Young, 2010b.
Bayesian dialoguesystem for the Let's Go spoken dialogue challenge.In Proceedings of SLT.M.
Wainwright and M. Jordan, 2008.
GraphicalModels, Exponential Families, and VariationalInference.
Foundations and Trends in MachineLearning, 1(1-2):1?305.J.
Williams and S. Young, 2007.
Partially observableMarkov decision processes for spoken dialogsystems.
Computer Speech & Language,21(2):393-422.J.
Williams, 2010.
Incremental partitionrecombination for efficient tracking of multipledialog states.
In Proceedings of ICASSP.J.
Williams, 2011.
An Empirical Evaluation of aStatistical Dialog System in Public Use, InProceedings of SIGDIAL.J.
Williams, 2012.
A Critical Analysis of TwoStatistical Spoken Dialog Systems in Public Use.In Proceedings of SLT.C.
Yanover and Y. Weiss, 2004.
Finding the M MostProbable Configurations Using Loopy BeliefPropagation.
In Advances in Neural InformationProcessing Systems 16.
MIT Press.S.
Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson and K. Yu, 2010.
The HiddenInformation State Model: a practical framework forPOMDP-based spoken dialogue management.Computer Speech and Language, 24(2):150?174.Appendix A.
Maximum EntropyMaximum Entropy directly models the classposterior given the observations:( | )( )(   (   ))  (15)where  ( ) is a normalization function,   the modelparameters, and  (   ) the vector of feature functionswhich are key to performance.Appendix B.
Conditional Random FieldLet   be a factor graph over outputs  .
Then, ifthe distribution  ( | ) factorizes according toand   *  +  is the set of factors in  , theconditional distribution can be written as:( | )( )?
((     ))(16)In practice, models rely extensively on parametertying.
To formalize this, let the factors of   bepartitioned to   *          +, where eachis a clique template whose parameters are tied.Each clique template is a set of factors which hasan associated vector of feature functions(     )  and parameters   .
From these itfollows (Sutton and McCallum, 2006):( | )( )?
?
((     ))(17)where the normalizing function is:( )  ??
?
((     ))(18)Appendix C. Tree-reweighted Belief PropagationUnlike treelike graphs, computing exact marginalprobabilities is in general intractable onstructured models with loops.
Therefore, we needto adopt approximate inference algorithmsinstead.
Note that CRF (Equation 16) is aninstance of exponential family:(   )      (   ( )   ( )) (19)where   is a function of the observations   andthe parameters   above,  ( ) a vector ofsufficient statistics consisting of indicatorfunctions for each configuration of each cliqueand each variable, and  ( ) is the log-partition450function    ?
(   ( ) ) .
For exponentialfamily, it is known that the exact inference canbe formulated as an optimization problem(Wainwright and Jordan, 2008):( )( ) (20)where  *  |       ( )+ is the marginalpolytope,  ( ) is the mapping from parametersto marginals, and  ( ) is the entropy.
ApplyingDanskin?s theorem to Equation 20 yields:( )( ) (21)Thus both the partition function (Equation 20)and marginals (Equation 21) can be computed atonce.
The variational formulation opens the doorto various approximate inference methods: toderive a tractable algorithm, one approximatesthe log-partition function  ?
( )  by using asimpler feasible region of   and a tractable  ( ).Then the approximate marginals are taken as theexact gradient of  ?
.
Among many possibleapproximations, we adopt the Tree ReweightedBelief Propagation (TRBP) method whichconvexifies the optimization problem that itguarantees finding the global solution.
TRBPtakes the local polytope as a relaxation of themarginal polytope:* |?
(  )(   ) ?
(  )+ (22)where  and   index each clique and outputvariable, respectively.
TRBP approximates theentropy as follows:( )  ?
(  )  ?
(  )(23)where  ( )  denotes the mutual information andthe constants    need to be selected so that theygenerate an upper bound on the original entropy.Appendix D. Parameter Estimation ForConditional Random FieldThe goal of parameter estimation is to minimizethe empirical risk:( )  ?
(       )(24)where there is summation over all trainingexamples.
The loss function  (       )quantifies the difference between the true andestimated outputs.
In this paper, we adopt thenegative of the conditional log likelihood:( )  ?
?
(     )( ) (25)The partial derivative of the log likelihood withrespect to a vector of parameters    associatedwith a clique template    is:?
(     )?
?
()(|  )(26)Appendix E. Probabilistic DeficiencyTo include interdependent features in agenerative model, we have two choices: enhancethe model to represent dependencies among theinputs, or make independence assumptions.
Thefirst approach is often difficult to do whileretaining tractability.
For example, it is hard tomodel the dependence between        ,,        ,            , and.
On the other hand, the secondapproach can hurt performance by resulting inpoor probability estimates.
Let?s consider thejoint probability  (         )  which thegenerative approach is based on.
Because of theindependence assumption, the joint probabilitycan be written as  ( ) (  | )  (  | ) .
Forexample, let?s assume that we observe twohypotheses 61D and 61B with confidence score0.6 and 0.2, respectively.
Then the conditionalprobabilities can be written as:(                       |   )(         |   )(             |   )(                       |   )(         |   )(             |   )Since           and                havea strong correlation, their probability estimatesshould also be positively correlated.
To simplifythe discussion, now suppose 61B and 61D areequiprobable,  (   )    (   )  and havesimilar conditional probabilities:(         |   )   (         |   )(             |   )(             |   )Then, multiplying those conditional probabilities,(         | )   (             | ) ,will increase or decrease the confidence of theclassifier too much, even though no newevidence has been added.451
