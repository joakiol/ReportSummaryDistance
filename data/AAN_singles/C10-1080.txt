Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 707?715,Beijing, August 2010Joint Parsing and TranslationYang Liu and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences{yliu,liuqun} @ict.ac.cnAbstractTree-based translation models, which ex-ploit the linguistic syntax of source lan-guage, usually separate decoding into twosteps: parsing and translation.
Althoughthis separation makes tree-based decodingsimple and efficient, its translation perfor-mance is usually limited by the numberof parse trees offered by parser.
Alter-natively, we propose to parse and trans-late jointly by casting tree-based transla-tion as parsing.
Given a source-languagesentence, our joint decoder produces aparse tree on the source side and a transla-tion on the target side simultaneously.
Bycombining translation and parsing mod-els in a discriminative framework, our ap-proach significantly outperforms a forest-based tree-to-string system by 1.1 ab-solute BLEU points on the NIST 2005Chinese-English test set.
As a parser,our joint decoder achieves an F1 score of80.6% on the Penn Chinese Treebank.1 IntroductionRecent several years have witnessed the rapiddevelopment of syntax-based translation models(Chiang, 2007; Galley et al, 2006; Shen et al,2008; Quirk et al, 2005; Liu et al, 2006; Huanget al, 2006; Eisner, 2003; Zhang et al, 2008; Chi-ang, 2010), which incorporate formal or linguis-tic syntax into translation process.
Depending onwhether modeling the linguistic syntax of sourcelanguage or not, we divide them into two cate-gories: string-based and tree-based models.
11Mi et al (2008) also distinguish between string-basedand tree-based models but depending on the type of input.sourcetargetparse+translatestring treestringsourcetargetstringparsetreetranslatestring(a)(b)Figure 1: Tree-based decoding: (a) separate pars-ing and translation versus (b) joint parsing andtranslation.String-based models include string-to-string(Chiang, 2007) and string-to-tree (Galley et al,2006; Shen et al, 2008).
Regardless of the syn-tactic information on the source side, they treatdecoding as a parsing problem: the decoder parsesa source-language sentence using the source pro-jection of a synchronous grammar while buildingthe target sub-translations in parallel.Tree-based models include tree-to-string (Liuet al, 2006; Huang et al, 2006) and tree-to-tree(Quirk et al, 2005; Eisner, 2003; Zhang et al,2008; Chiang, 2010).
These models explicitlyuse source parse trees and divide decoding intotwo separate steps: parsing and translation.
Aparser first parses a source-language sentence intoa parse tree, and then a decoder converts the treeto a translation on the target side (see Figure 1(a)).Figure 2 gives a training example for tree-to-string translation, which consists of a Chinesetree, an English sentence, and the word align-ment between them.
Romanized Chinese wordsare given to facilitate identification.
Table 1 shows707??
?
?9 ?1?
!NR P NR VV AS NNNPB NPB NPBPP VPBVPIPbushi yu shalong juxing le huitanBush held a meeting with SharonFigure 2: A training example that consists of aChinese parse, an English sentence, and the wordalignment between them.a set of tree-to-string rules obtained from Figure2.
The source side of a rule is a tree fragmentand the target side is a string.
We use x to denotenon-terminals and the associated subscripts indi-cate the correspondence between non-terminalson both sides.Conventionally, decoding for tree-to-stringtranslation is cast as a tree parsing problem (Eis-ner, 2003).
The tree parsing algorithm visits eachnode in the input source tree in a top-down orderand tries to match each translation rule against thelocal sub-tree rooted at the node.
For example, thefirst rule in Table 1 matches a sub-tree rooted atIP0,6 in Figure 2.
The descendent nodes of IP0,6(i.e., NPB0,1, PP1,3, and VPB3,6) can be furthermatched by other rules in Table 1.
The matchingprocedure runs recursively until the entire tree iscovered.
Finally, the output on the target side canbe taken as a translation.Compared with its string-based counterparts,tree-based decoding is simpler and faster: thereis no need for synchronous binarization (Huanget al, 2009b; Zhang et al, 2006) and tree parsinggenerally runs in linear time (Huang et al, 2006).While separating parsing and translation makestree-based decoding simple and efficient, itssearch space is limited by the number of parsetrees offered by parser.
Studies reveal that tree-based systems are prone to produce degeneratetranslations due to the propagation of parsing mis-takes (Quirk and Corston-Oliver, 2006).
Thisproblem can be alleviated by offering more alter-(1) IP(x1:NPB VP(x2:PP x3:VPB))?x1 x3 x2(2) NPB(NR(bushi))?Bush(3) PP(P(yu) x1:NPB)?with x1(4) NPB(NR(shalong))?Sharon(5) VPB(VV(juxing) AS(le) x1:NPB)?held a x1(6) NPB(NN(huitan))?meetingTable 1: Tree-to-string rules extracted from Figure2.natives to the pipeline.
An elegant solution is toreplace 1-best trees with packed forests that en-code exponentially many trees (Mi et al, 2008;Liu et al, 2009).
Mi et al (2008) present anefficient algorithm to match tree-to-string rulesagainst packed forests that encode millions oftrees.
They prove that offering more alternativesto tree parsing improves translation performancesubstantially.In this paper, we take a further step towards thedirection of offering multiple parses to translationby proposing joint parsing and translation.
Asshown in Figure 1(b), our approach parses andtranslates jointly as it finds a parse tree and atranslation of a source-language sentence simul-taneously.
We integrate the tree-to-string model(Liu et al, 2006; Huang et al, 2006), n-gram lan-guage model, probabilistic context-free grammar(PCFG), and Collins?
Model 1 (Collins, 2003) in adiscriminative framework (Och, 2003).
Allowingparsing and translation to interact with each other,our approach obtains an absolute improvement of1.1 BLEU points over a forest-based tree-to-stringtranslation system (Mi et al, 2008) on the 2005NIST Chinese-English test set.
As a parser, ourjoint decoder achieves an F1 score of 80.6% onthe Penn Chinese Treebank.2 Joint Parsing and Translation2.1 Decoding as ParsingWe propose to integrate parsing and translationinto a single step.
To achieve joint parsing andtranslation, we cast tree-to-string decoding as amonolingual parsing problem (Melamed, 2004;Chiang, 2007; Galley et al, 2006): the de-coder takes a source-language string as input andparses it using the source-projection of SCFGwhile building the corresponding sub-translationssimultaneously.708For example, given the Chinese sentence bushiyu sha long juxing le huitan in Figure 2, thederivation in Table 1 explains how a Chinese tree,an English string, and the word alignment be-tween them are generated synchronously.
Unlikethe string-based systems as described in (Chiang,2007; Galley et al, 2006; Shen et al, 2008), weexploit the linguistic syntax on the source sideexplicitly.
Therefore, the source parse trees pro-duced by our decoder are meaningful from a lin-guistic point of view.As tree-to-string rules usually have multiplenon-terminals that make decoding complexitygenerally exponential, synchronous binarization(Huang et al, 2009b; Zhang et al, 2006) is akey technique for applying the CKY algorithmto parsing with tree-to-string rules.
2 Huang etal.
(2009b) factor each tree-to-string rule into twoSCFG rules: one from the root nonterminal tothe subtree, and the other from the subtree to theleaves.
In this way, one can uniquely reconstructthe original tree using a two-step SCFG deriva-tion.For example, consider the first rule in Table 1:IP(x1:NPB VP(x2:PP x3:VPB))?x1 x3 x2We use a specific non-terminal, say, T, touniquely identify the left-hand side subtree andproduce two SCFG rules: 3IP ?
?T 1 ,T 1 ?
(1)T ?
?NPB 1 PP 2 VPB 3 ,NPB 1 VPB 3 PP 2 ?
(2)where the boxed numbers indicate the correspon-dence between nonterminals.Then, the rule (2) can be further binarized intotwo rules that have at most two non-terminals:T ?
?NPB 1 PP-VPB 2 ,NPB 1 PP-VPB 2 ?
(3)PP-VPB ?
?PP 1 VPB 2 ,VPB 2 PP 1 ?
(4)where PP-VPB is an intermediate virtual non-terminal.2But CKY is not the only choice.
The Earley algorithmcan also be used to parse with tree-to-string rules (Zhao andAl-Onaizan, 2008).
As the Earley algorithm binarizes multi-nonterminal rules implicitly, there is no need for synchronousbinarization.3It might look strange that the node VP disappears.
Thisnode is actually stored in the monolithic node T. Please referto page 573 of (Huang et al, 2009b) for more details abouthow to convert tree-to-string rules to SCFG rules.We call rules the tree roots of which are vir-tual non-terminals virtual rules and others naturalrules.
For example, the rule (1) is a natural ruleand the rules (3) and (4) are virtual rules.
We fol-low Huang et al (2009b) to keep the probabilitiesof a natural rule unchanged and set those of a vir-tual rule to 1.
4After binarizing tree-to-string rules into SCFGrules that have at most two non-terminals, we canuse the CKY algorithm to parse a source sentenceand produce its translation simultaneously as de-scribed in (Chiang, 2007; Galley et al, 2006).2.2 Adding Parsing ModelsAs our decoder produces ?genuine?
parse treesduring decoding, we can integrate parsing mod-els as features together with translation featuressuch as the tree-to-string model, n-gram languagemodel, and word penalty into a discriminativeframework (Och, 2003).
We expect that pars-ing and translation could interact with each other:parsing offers linguistically motivated reorderingto translation and translation helps parsing resolveambiguity.2.2.1 PCFGWe use the probabilistic context-free grammar(PCFG) as the first parsing feature in our decoder.Given a PCFG, the probability for a tree is theproduct of probabilities for the rules that it con-tains.
That is, if a tree pi is a context-free deriva-tion that involves K rules of the form ?k ?
?k ,its probability is given byP(pi) =?k=1...KPpcfg(?k ?
?k) (5)For example, the probability for the tree in Fig-ure 2 isP(pi) = Ppcfg(IP ?
NPB VP)?Ppcfg(NPB ?
NR)?Ppcfg(NR ?
bushi)?.
.
.
(6)4This makes the scores of hypotheses in the same chartcell hardly comparable because some hypotheses are cov-ered by a natural non-terminal and others covered by a virtualnon-terminal.
To alleviate this problem, we follow Huang etal.
(2009b) to separate natural and virtual hypotheses in dif-ferent beams.709IPTNPB PP-VPPP VPBIPNPB VPPP VPBFigure 3: Reconstructing original tree from virtualrules.
We first construct the tree on the left bysubstituting the trees of the rules (1), (3), and (4)and then restore the original tree on the right viathe monolithic node T.There are 13 PCFG rules involved.
We omit theremaining 10 rules.We formalize the decoding process as a deduc-tive system to illustrate how to include a PCFG.Given a natural ruleVP ?
?PP 1 VPB 2 ,VPB 2 PP 1 ?
(7)the following deductive step grows an item in thechart by the rule(PP1,3) : (w1, e1) (VPB3,6) : (w2, e2)(VP1,6) : (w, e2e1)(8)where PP1,3 denotes the recognition of the non-terminal PP spanning from the substring from po-sition 1 through 3 (i.e., yu shalong in Figure 2), w1and e1 are the score and translation of the first an-tecedent item, respectively, and the resulting itemscore is calculated as: 5w = w1 + w2 + logPpcfg(VP ?
PP VPB) (9)As the PCFG probabilities of natural rules arefixed during decoding, they can be pre-computedand stored in the rule table.
Therefore, includingPCFG for natural rules hardly increases decodingcomplexity.However, calculating the PCFG probabilitiesfor virtual rules is quite different due to the pres-ence of virtual non-terminals.
For instance, usingthe rule (4) in Section 2.1 to generate an item leadsto the following deductive step:(PP1,3) : (w1, e1) (VPB3,6) : (w2, e2)(PP-VPB1,6) : (w, e2e1)(10)5The logarithmic form of probability is used to avoid ma-nipulating very small numbers for practical reasons.
w1 andw2 take the PCFG probabilities of the two antecedent itemsinto consideration.As PP-VPB is a virtual non-terminal, the sub-tree it dominates is a virtual tree, for which wecannot figure out its PCFG probability.
There-fore, we have to postpone the calculation of PCFGprobabilities until reaching a natural non-terminalsuch as IP.
In other words, only when using therule (1) to produce an item, the decoding algo-rithm can update PCFG probabilities because theoriginal tree can be restored from the special nodeT now.
Figure 3 shows how to reconstruct theoriginal tree from virtual rules.
We first constructthe tree on the left by substituting the trees of therules (1), (3), and (4) and then restore the origi-nal tree on the right via T. Now, we can calculatethe PCFG probability of the original tree.
6 Inpractice, we pre-compute this PCFG probabilityand store it in the rule (1) to reduce computationaloverhead.2.2.2 Lexicalized PCFGAlthough widely used in natural language pro-cessing, PCFGs are often criticized for the lack oflexicalization, which is very important to capturethe lexical dependencies between words.
There-fore, we use Collins?
Model 1 (Collins, 2003), asimple and effective lexicalized parsing model, asthe second parsing feature in our decoder.Following Collins (2003), we first lexicalize atree by associating a headword h with each non-terminal.
Figure 4 gives the lexicalized tree corre-sponding to Figure 2.
The left-hand side of a rulein a lexicalized PCFG is P (h) and the right-handside has the form:Ln(ln) .
.
.
L1(l1)H(h)R1(?1) .
.
.
Rm(?m) (11)where H is the head-child that inherits theheadword h from its parent P , L1 .
.
.
Ln andR1 .
.
.
Rm are left and right modifiers of H , andl1 .
.
.
ln and ?1 .
.
.
?m are the corresponding head-words.
Either n or m may be zero, and n =m = 0 for unary rules.
Collins (2003) extends theleft and right sequences to include a terminatingSTOP symbol.
Thus, Ln+1 = Rm+1 = STOP.6Postponing the calculation of PCFG probabilities alsoleads to the ?hard-to-compare?
problem mentioned in foot-note 4 due to the presence of virtual non-terminals.
We stillmaintain multiple beams for natural and virtual hypotheses(i.e., items) to alleviate this prblem.710??
?
?9 ?1?
!NR P NR VV AS NNNPB NPB NPBPP VPBVPIPbushi yu shalong juxing le huitanbushibushiyu shalongshalongyujuxing le huitanhuitanjuxingjuxingjuxingFigure 4: The lexicalized tree corresponding toFigure 2.Collins (2003) breaks down the generation ofthe right-hand side of a rule into a sequence ofsmaller steps.
The probability of a rule is decom-posed as:Ph(H|P (h)) ?
?i=1...n+1Pl(Li(li)|P (h),H, t,?)
?
?j=1...m+1Pr(Rj(?j)|P (h),H, t,?)
(12)where t is the POS tag of of the headword h and ?is the distance between words that captures head-modifier relationship.For example, the probability of the lexicalizedrule IP(juxing) ?
NPB(bushi) VP(juxing) canbe computed as 7Ph(VP|IP, juxing)?Pl(NPB(bushi)|IP,VP, juxing)?Pl(STOP|IP,VP, juxing)?Pr(STOP|IP,VP, juxing) (13)We still use the deductive system to explainhow to integrate the lexicalized PCFG into the de-coding process.
Now, Eq.
(8) can be rewritten as:(PPyu1,3) : (w1, e1) (VPBjuxing3,6 ) : (w2, e2)(VPjuxing1,6 ) : (w, e2e1)(14)where yu and juxing are the headwords attachedto PP1,3, VPB3,6, and VP1,6.
The resulting item7For simplicity, we omit POS tag and distance in the pre-sentation.
In practice, we implemented the Collins?
Model 1exactly as described in (Collins, 2003).score is given byw = w1 + w2 + logPh(VPB|VP, juxing) +logPl(PP(yu)|VP,VPB, juxing) +logPl(STOP|VP,VPB, juxing) +logPr(STOP|VP,VPB, juxing) (15)Unfortunately, the lexicalized PCFG probabili-ties of most natural rules cannot be pre-computedbecause the headword of a non-terminal must bedetermined on the fly during decoding.
Considerthe third rule in Table 1PP(P(yu) x1:NPB) ?
with x1It is impossible to know what the headword ofNPB is in advance, which depends on the ac-tual sentence being translated.
However, we couldsafely say that the headword attached to PP is al-ways yu because PP should have the same head-word with its child P.Similar to the PCFG scenario, calculating lex-icalized PCFG for virtual rules is different fromnatural rules.
Consider the rule (4) in Section 2.1,the corresponding deductive step is(PPyu1,3) : (w1, e1) (VPBjuxing3,6 ) : (w2, e2)(PP-VPB?1,6) : (w, e2e1)(16)where ???
denotes that the headword ofPP-VPB1,6 is undefined.We still need to postpone the calculation of lex-icalized PCFG probabilities until reaching a nat-ural non-terminal such as IP.
In other words,only when using the rule (1) to produce an item,the decoding algorithm can update the lexicalizedPCFG probabilities.
After restoring the originaltree from T, we need to visit backwards to fron-tier nodes of the tree to find headwords and calcu-late lexicalized PCFG probabilities.
More specifi-cally, updating lexicalized PCFG probabilities forthe rule the rule (1) involves the following steps:1.
Reconstruct the original tree from the rules(1), (3), and (4) as shown in Figure 3;2.
Attach headwords to all nodes;3.
Calculate the lexicalized PCFG probabilitiesaccording to Eq.
(12).711Back-off Pl(Li(li)| .
.
.
)level Ph(H| .
.
. )
Pr(Rj(?j)| .
.
.
)1 P , h, t P , H , h, t, ?2 P , t P , H , t, ?3 P P , H , ?Table 2: The conditioning variables for each levelof back-off.As suggested by Collins (2003), we use back-off smoothing for sub-model probabilities duringdecoding.
Table 2 shows the various levels ofback-off for each type of parameter in the lexi-calized parsing model we use.
For example, Phestimation p interpolates maximum-likelihood es-timates p1 = Ph(H|P, h, t), p2 = Ph(H|P, t),and p3 = Ph(H|P ) as follows:p1 = ?1p1 + (1?
?1)(?2p2 + (1?
?2)p3) (17)where ?1, ?2, and ?3 are smoothing parameters.3 ExperimentsIn this section, we try to answer two questions:1.
Does tree-based translation by parsing out-perform the conventional tree parsing algo-rithm?
(Section 3.1)2.
How about the parsing performance of thejoint decoder?
(Section 3.2)3.1 Translation EvaluationWe used a bilingual corpus consisting of 251Ksentences with 7.3M Chinese words and 9.2M En-glish words to extract tree-to-string rules.
TheChinese sentences in the bilingual corpus wereparsed by an in-house parser (Xiong et al, 2005),which obtains an F1 score of 84.4% on the PennChinese Treebank.
After running GIZA++ (Ochand Ney, 2003) to obtain word alignments, weused the GHKM algorithm (Galley et al, 2004)and extracted 11.4M tree-to-string rules from thesource-side parsed, word-aligned bilingual cor-pus.
Note that the bilingual corpus does not con-tain the bilingual version of Penn Chinese Tree-bank.
In other words, all tree-to-string rules werelearned from noisy parse trees and alignments.
Weused the SRILM toolkit (Stolcke, 2002) to train a4-gram language model on the Xinhua portion ofthe GIGAWORD corpus, which contains 238MEnglish words.
We trained PCFG and Collins?Model 1 on the Penn Chinese Treebank.We used the 2002 NIST MT Chinese-Englishtest set as the development set and the 2005 NISTtest set as the test set.
Following Huang (2008),we modified our in-house parser to produce andprune packed forests on the development and testsets.
There are about 105M parse trees encodedin a forest of a sentence on average.
We also ex-tracted 1-best trees from the forests.As the development and test sets have manylong sentences (?
100 words) that make our de-coder prohibitively slow, we divided long sen-tences into short sub-sentences simply based onpunctuation marks such as comma and period.The source trees and target translations of sub-sentences were concatenated to form the tree andtranslation of the original sentence.We compared our parsing-based decoder withthe tree-to-string translation systems based on thetree parsing algorithm, which match rules againsteither 1-best trees (Liu et al, 2006; Huang et al,2006) or packed forests (Mi et al, 2008).
All thethree systems used the same rule set containing11.4M tree-to-string rules.
Given the 1-best treesof the test set, there are 1.2M tree-to-string rulesthat match fragments of the 1-best trees.
For theforest-based system (Mi et al, 2008), the num-ber of filtered rules increases to 1.9M after replac-ing 1-best trees with packed forests, which con-tain 105M trees on average.
As our decoder takesa string as input, 7.7M tree-to-string rules can beused to parse and translate the test set.
We bi-narized 99.6% of tree-to-string rules into 16.2MSCFG rules and discarded non-binarizable rules.As a result, the search space of our decoder ismuch larger than those of the tree parsing coun-terparts.Table 3 shows the results.
All the three sys-tems used the conventional translation featuressuch as relative frequencies, lexical weights, rulecount, n-gram language model, and word count.Without any parsing models, the tree-based sys-tem achieves a BLEU score of 29.8.
The forest-based system outperforms the tree-based systemby +1.8 BLEU points.
Note that each hyperedge712Algorithm Input Parsing model # of rules BLEU (%) Time (s)tree - 1.2M 29.8 0.56tree parsing forest PCFG 1.9M 31.6 9.49- 32.0 51.41PCFG 32.4 55.52parsing stringLex7.7M 32.6 89.35PCFG + Lex 32.7 91.72Table 3: Comparison of tree parsing and parsing for tree-to-string translation in terms of case-insensitiveBLEU score and average decoding time (second per sentence).
The column ?parsing model?
indicateswhich parsing models were used in decoding.
We use ?-?
to denote using only translation features.?Lex?
represents the Collins?
Model 1.
We excluded the extra parsing time for producing 1-best treesand packed forests.Forest size Exact match (%) Precision (%)1 0.55 41.5390 0.74 47.75.8M 0.92 54.166M 1.48 62.0105M 2.22 65.9Table 4: Comparison of 1-best trees produced byour decoder and the parse forests produced by themonolingual Chinese parser.
Forest size repre-sents the average number of trees stored in a for-est.in a parse forest is assigned a PCFG probabil-ity.
Therefore, the forest-based system actually in-cludes PCFG as a feature (Mi et al, 2008).
With-out incorporating any parsing models as features,our joint decoder achieves a BLEU score of 32.0.Adding PCFG and Collins?
Model 1 (i.e., ?Lex?
inTable 2) increases translation performance.
Whenboth PCFG and Collins?
Model 1 are used, ourjoint decoder outperforms the tree parsing systemsbased on 1-best trees (+2.9) and packed forests(+1.1) significantly (p < 0.01).
This result is alsobetter than that of using only translation featuressignificantly (from 32.0 to 32.7, p < 0.05).Not surprisingly, our decoder is much slowerthan pattern matching on 1-best trees and packedforests (with the same beam size).
In particu-lar, including Collins?
Model 1 increases decodingtime significantly because its sub-model probabil-ities requires back-off smoothing on the fly.How many 1-best trees produced by our de-coder are included in the parse forest produced bya standard parser?
We used the Chinese parserto generate five pruned packed forests with dif-ferent sizes (average number of trees stored in aforest).
As shown in Table 4, only 2.22% of thetrees produced by our decoder were included inthe biggest forest.
One possible reason is thatwe used sub-sentence division to reduce decodingcomplexity.
To further investigate the matchingrate, we also calculated labeled precision, whichindicates how many brackets in the parse matchthose in the packed forest.
The labeled precisionon the biggest forest is 65.9%, suggesting that the1-best trees produced by our decoder are signifi-cantly different from those in the packed forestsproduced by a standard parser.
83.2 Parsing EvaluationWe followed Petrov and Klein (2007) to divide thePenn Chinese Treebank (CTB) version 5 as fol-lows: Articles 1-270 and 400-1151 as the trainingset, Articles 301-325 as the development set, andArticles 271-300 as the test set.
We used max-F1training (Och, 2003) to train the feature weights.We did not use sub-sentence division as the sen-tences in the test set have no more than 40 words.8The packed forest produced by our decoder (?rule?forest) might be different from the forest produced by amonolingual parser (?parser?
forest).
While tree-based andforest-based decoders search in the intersection of the twoforests (i.e., matched forest), our decoder directly exploresthe ?rule?
forest, which represents the true search space oftree-to-string translation.
This might be the key difference ofour approach from forest-based translation (Mi et al, 2008).As sub-sentence division makes direct comparison of the twoforests quite difficult, we leave this to future work.713Parsing model F1 (%) Time (s)- 62.7 23.9PCFG 65.4 24.7Lex 79.8 48.8PCFG + Lex 80.6 50.4Table 5: Effect of parsing models on parsing per-formance (?
40 words) and average decodingtime (second per sentence).
We use ?-?
to denoteonly using translation features.Table 5 shows the results.
Translation featureswere used for all configurations.
Without pars-ing models, the F1 score is 62.7.
Adding Collins?Model 1 results in much larger gains than addingPCFG.
With all parsing models integrated, ourjoint decoder achieves an F1 score of 80.6 on thetest set.
Although lower than the F1 score of thein-house parser that produces the noisy trainingdata, this result is still very promising becausethe tree-to-string rules that construct trees in thedecoding process are learned from noisy trainingdata.4 Related WorkCharniak et al (2003) firstly associate lexical-ized parsing model with syntax-based translation.They first run a string-to-tree decoder (Yamadaand Knight, 2001) to produce an English parseforest and then use a lexicalized parsing model toselect the best translation from the forest.
As theparsing model operates on the target side, it actu-ally serves as a syntax-based language model formachine translation.
Recently, Shen et al (2008)have shown that dependency language model isbeneficial for capturing long-distance relationsbetween target words.
As our approach adds pars-ing models to the source side where the sourcesentence is fixed during decoding, our decoderdoes parse the source sentence like a monolingualparser instead of a syntax-based language model.More importantly, we integrate translation modelsand parsing models in a discriminative frameworkwhere they can interact with each other directly.Our work also has connections to joint parsing(Smith and Smith, 2004; Burkett and Klein, 2008)and bilingually-constrained monolingual parsing(Huang et al, 2009a) because we use anotherlanguage to resolve ambiguity for one language.However, while both joint parsing and bilingually-constrained monolingual parsing rely on the targetsentence, our approach only takes a source sen-tence as input.Blunsom and Osborne (2008) incorporate thesource-side parse trees into their probabilisticSCFG framework and treat every source-parsePCFG rule as an individual feature.
The differ-ence is that they parse the test set before decodingso as to exploit the source syntactic information toguide translation.More recently, Chiang (2010) has shownthat (?exact?)
tree-to-tree translation as pars-ing achieves comparable performance with Hiero(Chiang, 2007) using much fewer rules.
Xiao etal.
(2010) integrate tokenization and translationinto a single step and improve the performance oftokenization and translation significantly.5 ConclusionWe have presented a framework for joint parsingand translation by casting tree-to-string transla-tion as a parsing problem.
While tree-to-stringrules construct parse trees on the source sideand translations on the target side simultaneously,parsing models can be integrated to improve bothtranslation and parsing quality.This work can be considered as a final step to-wards the continuum of tree-to-string translation:from single tree to forest and finally to the inte-gration of parsing and translation.
In the future,we plan to develop more efficient decoding al-gorithms, analyze forest matching systematically,and use more sophisticated parsing models.AcknowledgementThe authors were supported by National Nat-ural Science Foundation of China, Contracts60736014 and 60903138, and 863 State KeyProject No.
2006AA010108.
We are grateful tothe anonymous reviewers for their insightful com-ments.
We thank Liang Huang, Hao Zhang, andTong Xiao for discussions on synchronous bina-rization, Haitao Mi and Hao Xiong for provid-ing and running the baseline systems, and WenbinJiang for helping us train parsing models.714ReferencesBlunsom, Phil and Miles Osborne.
2008.
Probabilis-tic inference for machine translation.
In Proc.
ofEMNLP 2008.Burkett, David and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Proc.of EMNLP 2008.Charniak, Eugene, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for statisticalmachine translation.
In Proc.
of MT Summit IX.Chiang, David.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):201?228.Chiang, David.
2010.
Learning to translate withsource and target syntax.
In Proc.
of ACL 2010.Collins, Michael.
2003.
Head-driven statistical mod-els for natural language parsing.
ComputationalLinguistics, 29(4):589?637.Eisner, Jason.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
of ACL2003.Galley, Michel, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of NAACL 2004.Galley, Michel, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of ACL 2006.Huang, Liang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA 2006.Huang, Liang, Wenbin Jiang, and Qun Liu.
2009a.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proc.
of EMNLP 2009.Huang, Liang, Hao Zhang, Daniel Gildea, and KevinKnight.
2009b.
Binarization of synchronouscontext-free grammars.
Computational Linguistics,35(4):559?595.Huang, Liang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL2008.Liu, Yang, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of ACL 2006.Liu, Yang, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.of ACL 2009.Melamed, I. Dan.
2004.
Statistical machine transla-tion by parsing.
In Proc.
of ACL 2004.Mi, Haitao, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL 2008.Och, Franz J. and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Och, Franz.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL 2003.Petrov, Slav and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proc.
of NAACL 2007.Quirk, Chris and Simon Corston-Oliver.
2006.
Theimpact of parsing quality on syntactically-informedstatistical machine translation.
In Proc.
of EMNLP2006.Quirk, Chris, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proc.
of ACL 2005.Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProc.
of ACL 2008.Smith, David and Noah Smith.
2004.
Bilingual pars-ing with factored estimation: using english to parsekorean.
In Proc.
of EMNLP 2004.Stolcke, Andreas.
2002.
Srilm - an extension languagemodel modeling toolkit.
In Proc.
of ICSLP 2002.Xiao, Xinyan, Yang Liu, Young-Sook Hwang, QunLiu, and Shouxun Lin.
2010.
Joint tokenizationand translation.
In Proc.
of COLING 2010.Xiong, Deyi, Shuanglong Li, Qun Liu, and ShouxunLin.
2005.
Parsing the penn chinese treebank withsemantic knowledge.
In Proc.
of IJCNLP 2005.Yamada, Kenji and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
of ACL2001.Zhang, Hao, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translatio.
In Proc.
of NAACL 2007.Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A treesequence alignment-based tree-to-tree translationmodel.
In Proc.
of ACL 2008.Zhao, Bing and Yaser Al-Onaizan.
2008.
General-izing local and non-local word-reordering patternsfor syntax-based machine translation.
In Proc.
ofEMNLP 2008.715
