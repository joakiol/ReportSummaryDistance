Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87?92,Dublin, Ireland, August 23-24 2014.Semantic Role Labelling with minimal resources:Experiments with FrenchRasoul Kaljahi?
?, Jennifer Foster?, Johann Roturier?
?NCLT, School of Computing, Dublin City University, Ireland{rkaljahi, jfoster}@computing.dcu.ie?Symantec Research Labs, Dublin, Irelandjohann roturier@symantec.comAbstractThis paper describes a series of French se-mantic role labelling experiments whichshow that a small set of manually anno-tated training data is superior to a muchlarger set containing semantic role labelswhich have been projected from a sourcelanguage via word alignment.
Using uni-versal part-of-speech tags and dependen-cies makes little difference over the orig-inal fine-grained tagset and dependencyscheme.
Moreover, there seems to be noimprovement gained from projecting se-mantic roles between direct translationsthan between indirect translations.1 IntroductionSemantic role labelling (SRL) (Gildea and Juraf-sky, 2002) is the task of identifying the predicatesin a sentence, their semantic arguments and theroles these arguments take.
The last decade hasseen considerable attention paid to statistical SRL,thanks to the existence of two major hand-craftedresources for English, namely, FrameNet (Bakeret al., 1998) and PropBank (Palmer et al., 2005).Apart from English, only a few languages haveSRL resources and these resources tend to be oflimited size compared to the English datasets.French is one of those languages which sufferfrom a scarcity of hand-crafted SRL resources.The only available gold-standard resource is asmall set of 1000 sentences taken from Europarl(Koehn, 2005) and manually annotated with Prop-bank verb predicates (van der Plas et al., 2010b).This dataset is then used by van der Plas et al.
(2011) to evaluate their approach to projecting theSRLs of English sentences to their translationsThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/in French.
They additionally build a large, ?ar-tificial?
or automatically labelled dataset of ap-proximately 1M Europarl sentences by projectingthe SRLs from English sentences to their Frenchtranslations and use it for training an SRL system.We build on the work of van der Plas et al.
(2010b) by answering the following questions: 1)How much artificial data is needed to train anSRL system?
2) Is it better to use direct trans-lations than indirect translations, i.e.
is it betterto use for projection a source-target pair wherethe source represents the original sentence and thetarget represents its direct translation as opposedto a source-target pair where the source and tar-get are both translations of an original sentencein a third language?
3) Is it better to use coarse-grained syntactic information (in the form of uni-versal part-of-speech tags and universal syntacticdependencies) than to use fine-grained syntacticinformation?
We find that SRL performance lev-els off after only 5K training sentences obtainedvia projection and that direct translations are nomore useful than indirect translations.
We alsofind that it makes very little difference to FrenchSRL performance whether we use universal part-of-speech tags and syntactic dependencies or morefine-grained tags and dependencies.The surprising result that SRL performance lev-els off after just 5K training sentences leads usto directly compare the small hand-crafted set of1K sentences to the larger artificial training set.We use 5-fold cross-validation on the small datasetand find that the SRL performance is substantiallyhigher (>10 F1in identification and classification)when the hand-crafted annotations are used.2 Related WorkThere has been relatively few works in FrenchSRL.
Lorenzo and Cerisara (2012) propose a clus-tering approach for verb predicate and argumentlabelling (but not identification).
They choose87VerbNet style roles (Schuler, 2006) and manu-ally annotate sentences with them for evaluation,achieving an F1of 78.5.Gardent and Cerisara (2010) propose a methodfor semi-automatically annotating the French de-pendency treebank (Candito et al., 2010) withPropbank core roles (no adjuncts).
They firstmanually augment TreeLex (Kup?s?c and Abeill?e,2008), a syntactic lexicon of French, with seman-tic roles of syntactic arguments of verbs (i.e.
verbsubcategorization).
They then project this anno-tation to verb instances in the dependency trees.They evaluate their approach by performing erroranalysis on a small sample and suggest directionsfor improvement.
The annotation work is howeverat its preliminary stage and no data is published.As mentioned earlier, van der Plas et al.
(2011)use word alignments to project the SRLs of theEnglish side of EuroParl to its French side result-ing in a large artificial dataset.
This idea is basedon the Direct Semantic Transfer hypothesis whichassumes that a semantic relationship between twowords in a sentence can be transferred to anytwo words in the translation which are aligned tothese source-side words.
Evaluation on their 1Kmanually-annotated dataset shows that a syntactic-semantic dependency parser trained on this artifi-cial data set performs significantly better than di-rectly projecting the labelling from its English side?
a promising result because, in a real-world sce-nario, the English translations of the French datato be annotated do not necessarily exist.Pad?o and Lapata (2009) also make use of wordalignments to project SRLs from English to Ger-man.
The word alignments are used to computethe semantic similarity between syntactic con-stituents.
In order to determine the extent of se-mantic correspondence between English and Ger-man, they manually annotate a set of parallel sen-tences and find that about 72% of the frames and92% of the argument roles exist in both sides, ig-noring their lexical correspondence.3 Datasets, SRL System and EvaluationWe use the two datasets described in (van der Plaset al., 2011) and the delivery report of the Clas-sic project (van der Plas et al., 2010a).
Theseare the gold standard set of 1K sentences whichwas annotated by manually identifying each verbpredicate, finding its equivalent English framesetin PropBank and identifying and labelling its ar-guments based on the description of the frame-set (henceforth known as Classic1K), and the syn-thetic dataset consisting of more than 980K sen-tences (henceforth known as Classic980K), whichwas created by word aligning an English-Frenchparallel corpus (Europarl) using GIZA++ (Ochand Ney, 2003) and projecting the French SRLsfrom the English SRLs via the word alignments.The joint syntactic-semantic parser described in(Titov et al., 2009) was used to produce the En-glish SRLs and the dependency parses of theFrench side were produced using the ISBN parserdescribed in (Titov and Henderson, 2007).We use LTH (Bj?orkelund et al., 2009), adependency-based SRL system, in all of our ex-periments.
This system was among the best-performing systems in the CoNLL 2009 sharedtask (Haji?c et al., 2009) and is straightforward touse.
It comes with a set of features tuned for eachshared task language (English, German, Japanese,Spanish, Catalan, Czech, Chinese).
We comparedthe performance of the English and Spanish fea-ture sets on French and chose the former due to itshigher performance (by 1 F1point).To evaluate SRL performance, we use theCoNLL 2009 shared task scoring script1, whichassumes a semantic dependency between the argu-ment and predicate and the predicate and a dummyroot node and then calculates the precision (P), re-call (R) and F1of identification of these dependen-cies and classification (labelling) of them.4 Experiments4.1 Learning CurveThe ultimate goal of SRL projection is to build atraining set which partially compensates for thelack of hand-crafted resources.
van der Plas etal.
(2011) report encouraging results showing thattraining on their projected data is beneficial overdirectly obtaining the annotation via projectionwhich is not always possible.
Although the qualityof such automatically-generated training data maynot be comparable to the manual one, the possi-bility of building much bigger data sets may pro-vide some advantages.
Our first experiment inves-tigates the extent to which the size of the synthetictraining set can improve performance.We randomly select 100K sentences from Clas-sic980K, shuffle them and split them into 20 sub-1https://ufal.mff.cuni.cz/conll2009-st/eval09.pl88010000200003000040000500006000070000800009000010000020304050607080PrecisionRecallF1Figure 1: Learning curve with 100K training dataof projected annotations010000200003000040000500006000070000800009000010000020304050607080PrecisionRecallF1Figure 2: Learning curve with 100K training dataof projected annotations on only direct translationssets of 5K sentences.
We then split the first 5K into10 sets of 500 sentences.
We train SRL modelson the resulting 29 subsets using LTH.
The per-formance of the models evaluated on Classic1Kis presented in Fig.
1.
Surprisingly, the best F1(58.7) is achieved by only 4K sentences, and af-ter that the recall (and consequently F1) tends todrop though precision shows a positive trend, sug-gesting that the additional sentences bring little in-formation.
The large gap between precision andrecall is also interesting, showing that the projec-tions do not have wide semantic role coverage.24.2 Direct TranslationsEach sentence in Europarl was written in one ofthe official languages of the European Parliamentand translated to all of the other languages.
There-fore both sides of a parallel sentence pair can be in-direct translations of each other.
van der Plas et al.
(2011) suggest that translation divergence may af-2Note that our results are not directly comparable with(van der Plas et al., 2011) because they split Classic1K intodevelopment and test sets, while we use the whole set fortesting.
We do not have access to their split.fect automatic projection of semantic roles.
Theytherefore select for their experiments only those276K sentences from the 980K which are directtranslations between English and French.
Moti-vated by this idea, we replicate the learning curvein Fig.
1 with another set of 100K sentences ran-domly selected from only the direct translations.The curve is shown in Fig.
2.
There is no no-ticeable difference between this and the graph inFig.
1, suggesting that the projections obtained viadirect translations are not of higher quality.4.3 Impact of Syntactic AnnotationBeing a dependency-based semantic role labeller,LTH employs a large set of features based on syn-tactic dependency structure.
This inspires us tocompare the impact of different types of syntacticannotations on the performance of this system.Based on the observations from the previoussections, we choose two different sizes of trainingsets.
The first set contains the first 5K sentencesfrom the original 100K, as we saw that more thanthis amount tends to diminish performance.
Thesecond set contains the first 50K from the original100K, the purpose of which is to check if changingthe parses affects the usefulness of adding moredata.
We will call these data sets Classic5K andClassic50K respectively.Petrov et al.
(2012) create a set of 12 univer-sal part-of-speech (POS) tags which should in the-ory be applicable to any natural language.
It isinteresting to know whether these POS tags aremore useful for SRL than the original set of the 29more fine-grained POS tags used in French Tree-bank which we have used so far.
To this end, weconvert the original POS tags of the data to uni-versal POS tags and retrain and evaluate the SRLmodels.
The results are given in the second row ofTable 1 (OrgDep+UniPOS).
The first row of thetable (Original) shows the performance usingthe original annotation.
Even though the scoresincrease in most cases ?
due mostly to a rise inrecall ?
the changes are small.
It is worth notingthat identification seems to benefit more from theuniversal POS tags.Similar to universal POS tags, McDonald et al.
(2013) introduce a set of 40 universal dependencytypes which generalize over the dependency struc-ture specific to several languages.
For French, theyprovide a new treebank, called uni-dep-tb,manually annotating 16,422 sentences from vari-895K 50KIdentification Classification Identification ClassificationP R F1P R F1P R F1P R F1Original 85.95 59.64 70.42 71.34 49.50 58.45 86.67 58.07 69.54 72.44 48.54 58.13OrgDep+UniPOS 86.71 60.46 71.24 71.11 49.58 58.43 86.82 58.71 70.05 72.30 48.90 58.34StdUniDep+UniPOS 86.14 59.76 70.57 70.60 48.98 57.84 86.38 58.90 70.04 71.61 48.83 58.07CHUniDep+UniPOS 85.98 59.21 70.13 70.66 48.66 57.63 86.47 58.26 69.61 71.74 48.34 57.76Table 1: SRL performance using different syntactic parses with Classic 5K and 50K training setsous domains.
We now explore the utility of thisnew dependency scheme in SRL.The French universal dependency treebankcomes in two versions, the first using the stan-dard dependency structure based on basic Stanforddependencies (de Marneffe and Manning, 2008)where content words are the heads except in cop-ula and adposition constructions, and the secondwhich treats content words as the heads for allconstructions without exemption.
We use bothschemes in order to verify their effect on SRL.In order to obtain universal dependencies forour data, we train parsing models with Malt-Parser (Nivre et al., 2006) using the entireuni-dep-tb.3We then parse our data us-ing these MaltParser models.
The input POStags to the parser are the universal POS tagsused in OrgDep+UniPOS.
We train and evalu-ate new SRL models on these data.
The resultsare shown in the third and fourth rows of Table1.
StdUniDept+UniPOS is the setting usingstandard dependencies and CHUDep+UPOS usingcontent-head dependencies.According to the third and fourth rows in Table1, content-head dependencies are slightly less use-ful than standard dependencies.
The general ef-fect of universal dependencies can be compared tothose of original ones by comparing these resultsto OrgDep+UniPOS - the use of universal de-pendencies appears to have only a modest (nega-tive) effect.
However, we must be careful of draw-ing too many conclusions because in addition tothe difference in dependency schemes, the trainingdata used to train the parsers as well as the parsersthemselves are different.Overall, we observe that the universal annota-tions can be reliably used when the fine-grainedannotation is not available.
This can be especially3Based on our preliminary experiments on the pars-ing performance, we use LIBSVM as learning algorithm,nivreeager as parsing algorithm for the standard depen-dency models and stackproj for the content-head ones.Identification ClassificationP R F1P R F11K 83.76 83.00 83.37 68.40 67.78 68.095K 85.94 59.62 70.39 71.30 49.47 58.401K+5K 85.74 66.53 74.92 71.48 55.46 62.46SelfT 83.82 83.66 83.73 67.91 67.79 67.85Table 2: Average scores of 5-fold cross-validationwith Classic 1K (1K), 5K (5K), 1K plus 5K(1K+5K) and self-training with 1K seed and 5Kunlabeled data (SelfT)useful for languages which lack such resourcesand require techniques such as cross-lingual trans-fer to replace them.4.4 Quality vs. QuantityIn Section 4.1, we saw that adding more data an-notated through projection did not elevate SRLperformance.
In other words, the same perfor-mance was achieved using only a small amountof data.
This is contrary to the motivation for cre-ating synthetic training data, especially when thehand-annotated data already exist, albeit in a smallsize.
In this section, we compare the performanceof SRL models trained using manually-annotateddata with SRL models trained using 5K of artifi-cial or synthetic training data.
We use the originalsyntactic annotations for both datasets.To this end, we carry out a 5-fold cross-validation on Classic1K.
We then evaluate theClassic5K model, on each of the 5 test sets gen-erated in the cross-validation.
The average scoresof the two evaluation setups are compared.
Theresults are shown in Table 2.While the 5K model achieves higher precision,its recall is far lower resulting in dramaticallylower F1.
This high precision and low recall is dueto the low confidence of the model trained on pro-jected data suggesting that a considerable amountof information is not transferred during the projec-tion.
This issue can be attributed to the fact that the90Classic projection uses intersection of alignmentsin the two translation directions, which is the mostrestrictive setting and leaves many source predi-cates and arguments unaligned.We next add the Classic5K projected data tothe manually annotated training data in each foldof another cross-validation setting and evaluatethe resulting models on the same test sets.
Theresults are reported in the third row of the Ta-ble 2 (1K+5K).
As can be seen, the low qual-ity of the projected data significantly degrades theperformance compared to when only manually-annotated data are used for training.Finally, based on the observation that the qual-ity of labelling using manually annotated data ishigher than using the automatically projected data,we replicate 1K+5K with the 5K data labelled us-ing the model trained on the training subset of 1Kat each cross-validation fold.
In other words, weperform a one-round self-training with this model.The performance of the resulting model evaluatedin the same cross-validation setting is given in thelast row of Table 2 (SelfT).As expected, the labelling obtained by mod-els trained on manual annotation are more usefulthan the projected ones when used for training newmodels.
It is worth noting that, unlike with the1K+5K setting, the balance between precision andrecall follows that of the 1K model.
In addition,some of the scores are the highest among all re-sults, although the differences are not significant.4.5 How little is too little?In the previous section we saw that using a manu-ally annotated dataset with as few as 800 sentencesresulted in significantly better SRL performancethan using projected annotation with as many as5K sentences.
This unfortunately indicates theneed for human labour in creating such resources.It is interesting however to know the lower boundof this requirement.
To this end, we reverse ourcross-validation setting and train on 200 and teston 800 sentences.
We then compare to the 5Kmodels evaluated on the same 800 sentence setsat each fold.
The results are presented in Table 3.Even with only 200 manually annotated sentences,the performance is considerably higher than with5K sentences of projected annotations.
However,as one might expect, compared to when 800 sen-tences are used for training, this small model per-forms significantly worse.Identification ClassificationP R F1P R F11K 82.34 79.61 80.95 64.14 62.01 63.065K 85.95 59.64 70.42 71.34 49.50 58.45Table 3: Average scores of 5-fold cross-validationwith Classic 1K (1K) and 5K (5K) using 200 sen-tences for training and 800 for testing at each fold5 ConclusionWe have explored the projection-based approachto SRL by carrying out experiments with a largeset of French semantic role labels which have beenautomatically transferred from English.
We havefound that increasing the number of these artificialprojections that are used in training an SRL sys-tem does not improve performance as might havebeen expected when creating such a resource.
In-stead it is better to train directly on what little goldstandard data is available, even if this dataset con-tains only 200 sentences.
We suspect that the dis-appointing performance of the projected datasetoriginates in the restrictive way the word align-ments have been extracted.
Only those alignmentsthat are in the intersection of the English-Frenchand French-English word alignment sets are re-tained resulting in low SRL recall.
Recent prelim-inary experiments show that less restrictive align-ment extraction strategies including extracting theunion of the two sets or source-to-target align-ments lead to a better recall and consequently F1both when used for direct projection to the testdata or for creating the training data and then ap-plying the resulting model to the test data.We have compared the use of universal POStags and dependency labels to the original, morefine-grained sets and shown that there is only alittle difference.
However, it remains to be seenwhether this finding holds for other languages orwhether it will still hold for French when SRL per-formance can be improved.
It might also be in-teresting to explore the combination of universaldependencies with fine-grained POS tags.AcknowledgmentsThis research has been supported by the IrishResearch Council Enterprise Partnership Scheme(EPSPG/2011/102) and the computing infrastruc-ture of the CNGL at DCU.
We thank Lonneke vander Plas for providing us the Classic data.
We alsothank the reviewers for their helpful comments.91ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th ACL, pages 86?90.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 43?48.Marie Candito, Benot Crabb?e, and Pascal Denis.
2010.Statistical french dependency parsing: treebankconversion and first results.
In Proceedings ofLREC?2010.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependenciesrepresentation.
In Proceedings of the COLINGWorkshop on Cross-Framework and Cross-DomainParser Evaluation.Claire Gardent and Christophe Cerisara.
2010.
Semi-Automatic Propbanking for French.
In TLT9 -The Ninth International Workshop on Treebanks andLinguistic Theories.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit, pages 79?86.Anna Kup?s?c and Anne Abeill?e.
2008.
Growingtreelex.
In Proceedings of the 9th International Con-ference on Computational Linguistics and Intelli-gent Text Processing, CICLing?08, pages 28?39.Alejandra Lorenzo and Christophe Cerisara.
2012.Unsupervised frame based semantic role induction:application to french and english.
In Proceedings ofthe ACL 2012 Joint Workshop on Statistical Parsingand Semantic Processing of Morphologically RichLanguages, pages 30?35.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuz-man Ganchev, Keith Hall, Slav Petrov, HaoZhang, Oscar T?ackstr?om, Claudia Bedini, N?uriaBertomeu Castell?o, and Jungmee Lee.
2013.
Uni-versal dependency annotation for multilingual pars-ing.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 92?97.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In In Proceedings of LREC.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Sebastian Pad?o and Mirella Lapata.
2009.
Cross-lingual annotation projection of semantic roles.
J.Artif.
Int.
Res., 36(1):307?340.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC, May.Karin Kipper Schuler.
2006.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. the-sis, University of Pennsylvania.Ivan Titov and James Henderson.
2007.
A latent vari-able model for generative dependency parsing.
InProceedings of the 10th International Conference onParsing Technologies, pages 144?155.Ivan Titov, James Henderson, Paola Merlo, andGabriele Musillo.
2009.
Online projectivisation forsynchronous parsing of semantic and syntactic de-pendencies.
In In Proceedings of the InternationJoint Conference on Artificial Intelligence (IJCAI),pages 1562?1567.Lonneke van der Plas, James Henderson, and PaolaMerlo.
2010a.
D6.
2: Semantic role annotation of afrench-english corpus.Lonneke van der Plas, Tanja Samard?zi?c, and PaolaMerlo.
2010b.
Cross-lingual validity of propbankin the manual annotation of french.
In Proceedingsof the Fourth Linguistic Annotation Workshop, LAWIV ?10, pages 113?117.Lonneke van der Plas, Paola Merlo, and James Hen-derson.
2011.
Scaling up automatic cross-lingualsemantic role annotation.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 299?304.92
