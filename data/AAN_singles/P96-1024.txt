Parsing Algorithms and MetricsJ oshua  GoodmanHarvard University33 Oxford St.Cambridge,  MA 02138goodman@das.harvard.eduAbst rac tMany different metrics exist for evaluatingparsing results, including Viterbi, Cross-ing Brackets Rate, Zero Crossing BracketsRate, and several others.
However, mostparsing algorithms, including the Viterbialgorithm, attempt o optimize the samemetric, namely the probability of gettingthe correct labelled tree.
By choosinga parsing algorithm appropriate for theevaluation metric, better performance canbe achieved.
We present wo new algo-rithms: the "Labelled Recall Algorithm,"which maximizes the expected LabelledRecall Rate, and the "Bracketed RecallAlgorithm," which maximizes the Brack-eted Recall Rate.
Experimental resultsare given, showing that the two new al-gorithms have improved performance overthe Viterbi algorithm on many criteria, es-pecially the ones that they optimize.1 In t roduct ionIn corpus-based approaches to parsing, one is givena treebank (a collection of text annotated with the"correct" parse tree) and attempts to find algo-rithms that, given unlabelled text from the treebank,produce as similar a parse as possible to the one inthe treebank.Various methods can be used for finding theseparses.
Some of the most common involve induc-ing Probabilistic Context-Free Grammars (PCFGs),and then parsing with an algorithm such as the La-belled Tree (Viterbi) Algorithm, which maximizesthe probability that the output of the parser (the"guessed" tree) is the one that the PCFG produced.This implicitly assumes that the induced PCFG doesa good job modeling the corpus.There are many different ways to evaluate theseparses.
The most common include the LabelledTree Rate (also called the Viterbi Criterion or Ex-act Match Rate), Consistent Brackets Recall Rate(also called the Crossing Brackets Rate), Consis-tent Brackets Tree Rate (also called the Zero Cross-ing Brackets Rate), and Precision and Recall.
De-spite the variety of evaluation metrics, nearly all re-searchers use algorithms that maximize performanceon the Labelled Tree Rate, even in domains wherethey are evaluating using other criteria.We propose that by creating algorithms that op-timize the evaluation criterion, rather than somerelated criterion, improved performance can beachieved.In Section 2, we define most of the evaluationmetrics used in this paper and discuss previous ap-proaches.
Then, in Section 3, we discuss the La-belled Recall Algorithm, a new algorithm that max-imizes performance on the Labelled Recall Rate.
InSection 4, we discuss another new algorithm, theBracketed Recall Algorithm, that maximizes perfor-mance on the Bracketed Recall Rate (closely relatedto the Consistent Brackets Recall Rate).
Finally, wegive experimental results in Section 5 using thesetwo algorithms in appropriate domains, and com-pare them to the Labelled Tree (Viterbi) Algorithm,showing that each algorithm generally works bestwhen evaluated on the criterion that it optimizes.2 Eva luat ion  Met r i csIn this section, we first define basic terms and sym-bols.
Next, we define the different metrics used inevaluation.
Finally, we discuss the relationship ofthese metrics to parsing algorithms.2.1 Basic DefinitionsLet Wa denote word a of the sentence under consid-eration.
Let w b denote WaW~+l...Wb-lWb; in partic-ular let w~ denote the entire sequence of terminals(words) in the sentence under consideration.In this paper we assume all guessed parse trees arebinary branching.
Let a parse tree T be defined as aset of triples (s, t, X)--where s denotes the positionof the first symbol in a constituent,  denotes theposition of the last symbol, and X represents a ter-minal or nonterminal symbol--meeting the followingthree requirements:177?
The sentence was generated by the start sym-bol, S. Formally, (1, n, S) E T.?
Every word in the sentence is in the parse tree.Formally, for every s between 1 and n the triple(s,s, ws) E T.?
The tree is binary branching and consistent.Formally, for every (s,t, X) in T, s ?
t, there isexactly one r, Y, and Z such that s < r < t and(s,r,Y) E T and ( r+ 1,t,Z) e T.Let Tc denote the "correct" parse (the one in thetreebank) and let Ta denote the "guessed" parse(the one output by the parsing algorithm).
LetNa denote \[Tal, the number of nonterminals in theguessed parse tree, and let Nc denote \[Tel, the num-ber of nonterminals in the correct parse tree.2.2 Evaluation MetricsThere are various levels of strictness for determin-ing whether a constituent (element of Ta) is "cor-rect."
The strictest of these is Labelled Match.
Aconstituent (s,t, X) E Te is correct according to La-belled Match if and only if (s, t, X) E To.
In otherwords, a constituent in the guessed parse tree is cor-rect if and only if it occurs in the correct parse tree.The next level of strictness is Bracketed Match.Bracketed match is like labelled match, except hatthe nonterminal label is ignored.
Formally, a con-stituent (s, t, X) ETa  is correct according to Brack-eted Match if and only if there exists a Y such that(s,t ,Y) E To.The least strict level is Consistent Brackets (alsocalled Crossing Brackets).
Consistent Brackets islike Bracketed Match in that the label is ignored.It is even less strict in that the observed (s , t ,X)need not be in Tc-- i t  must simply not be ruled outby any (q, r, Y) e To.
A particular triple (q, r, Y)rules out (s,t, X) if there is no way that (s , t ,X)and (q, r, Y) could both be in the same parse tree.In particular, if the interval (s, t) crosses the interval(q, r), then (s, t, X)  is ruled out and counted as anerror.
Formally, we say that (s, t) crosses (q, r) ifand only i f s<q<t  <rorq<s<r<t .If Tc is binary branching, then Consistent Brack-ets and Bracketed Match are identical.
The follow-ing symbols denote the number of constituents hatmatch according to each of these criteria.L = ITc n Tal : the number of constituentsin Ta that are correct according to LabelledMatch.B = I{(s,t ,X) : (s,t ,X) ETa  and for someY (s,t ,Y)  E Tc}\]: the number of constituentsin Ta that are correct according to BracketedMatch.C = I{(s, t, X) ETa : there is no (v, w, Y) E Tccrossing (s,t)}\[ : the number of constituents inTG correct according to Consistent Brackets.Following are the definitions of the six metricsused in this paper for evaluating binary branchingtrees:Thein the following table:(1) Labelled Recall Rate = L/Nc.
(2) Labelled Tree Rate = 1 if L = ATe.
It is alsocalled the Viterbi Criterion.
(3) Bracketed Recall Rate = B/Nc.
(4) Bracketed Tree Rate = 1 if B = Nc.
(5) Consistent Brackets Recall Rate = C/NG.
It isoften called the Crossing Brackets Rate.
In thecase where the parses are binary branching, thiscriterion is the same as the Bracketed RecallRate.
(6) Consistent Brackets Tree Rate = 1 if C = No.This metric is closely related to the BracketedTree Rate.
In the case where the parses arebinary branching, the two metrics are the same.This criterion is also called the Zero CrossingBrackets Rate.preceding six metrics each correspond to cellsII Recall I TreeConsistent Brackets C/NG 1 if C = NcBrackets B/Nc  1 if B = NcLabels L/Nc 1 if L = Arc2.3 Maximizing MetricsDespite this long list of possible metrics, there isonly one metric most parsing algorithms attempt omaximize, namely the Labelled Tree Rate.
That is,most parsing algorithms assume that the test corpuswas generated by the model, and then attempt oevaluate the following expression, where E denotesthe expected value operator:Ta = argmTaXE ( 1 i f L  = gc)  (1)This is true of the Labelled Tree Algorithm andstochastic versions of Earley's Algorithm (Stolcke,1993), and variations uch as those used in Pickyparsing (Magerman and Weir, 1992).
Even in prob-abilistic models not closely related to PCFGs, suchas Spatter parsing (Magerman, 1994), expression (1)is still computed.
One notable exception is Brill'sTransformation-Based Error Driven system (Brill,1993), which induces a set of transformations de-signed to maximize the Consistent Brackets RecallRate.
However, Brill's system is not probabilistic.Intuitively, if one were to match the parsing algo-rithm to the evaluation criterion, better performanceshould be achieved.Ideally, one might try to directly maximizethe most commonly used evaluation criteria, suchas Consistent Brackets Recall (Crossing Brackets)178Rate.
Unfortunately, this criterion is relatively diffi-cult to maximize, since it is time-consuming to com-pute the probability that a particular constituentcrosses some constituent in the correct parse.
Onthe other hand, the Bracketed Recall and BracketedTree Rates are easier to handle, since computing theprobability that a bracket matches one in the correctparse is inexpensive.
It is plausible that algorithmswhich optimize these closely related criteria will dowell on the analogous Consistent Brackets criteria.2.4 Which  Met r i cs  to  UseWhen building an actual system, one should use themetric most appropriate for the problem.
For in-stance, if one were creating a database query sys-tem, such as an ATIS system, then the Labelled Tree(Viterbi) metric would be most appropriate.
A sin-gle error in the syntactic representation f a querywill likely result in an error in the semantic represen-tation, and therefore in an incorrect database query,leading to an incorrect result.
For instance, if theuser request "Find me all flights on Tuesday" is mis-parsed with the prepositional phrase attached to theverb, then the system might wait until Tuesday be-fore responding: a single error leads to completelyincorrect behavior.
Thus, the Labelled Tree crite-rion is appropriate.On the other hand, consider a machine assistedtranslation system, in which the system providestranslations, and then a fluent human manually ed-its them.
Imagine that the system is given theforeign language quivalent of "His credentials arenothing which should be laughed at," and makesthe single mistake of attaching the relative clauseat the sentential level, translating the sentence as"His credentials are nothing, which should make youlaugh."
While the human translator must makesome changes, he certainly needs to do less editingthan he would if the sentence were completely mis-parsed.
The more errors there are, the more editingthe human translator needs to do.
Thus, a criterionsuch as the Labelled Recall criterion is appropriatefor this task, where the number of incorrect con-stituents correlates to application performance.3 Labe l led  Reca l l  Pars ingConsider writing a parser for a domain such as ma-chine assisted translation.
One could use the La-belled Tree Algorithm, which would maximize theexpected number of exactly correct parses.
How-ever, since the number of correct constituents i abetter measure of application performance for thisdomain than the number of correct trees, perhapsone should use an algorithm which maximizes theLabelled Recall criterion, rather than the LabelledTree criterion.The Labelled Recall Algorithm finds that tree TGwhich has the highest expected value for the La-belled Recall Rate, L/Nc (where L is the number ofcorrect labelled constituents, and Nc is the numberof nodes in the correct parse).
This can be writtenas follows:Ta = arg n~xE(L/Nc) (2)It is not immediately obvious that the maximiza-tion of expression (2) is in fact different from themaximization of expression (1), but a simple exam-ple illustrates the difference.
The following grammargenerates four trees with equal probability:S ~ A C 0.25S ~ A D 0.25S --* EB  0.25S --~ FB  0.25A, B, C, D, E, F ~ xx  1.0The four trees areS SX XX X X XX X(3)S SE B F BX XX X X XX XFor the first tree, the probabilities of being correctare S: 100%; A:50%; and C: 25%.
Similar countingholds for the other three.
Thus, the expected valueof L for any of these trees is 1.75.On the other hand, the optimal Labelled Recallparse isSX XX XThis tree has 0 probability according to the gram-mar, and thus is non-optimal according to the La-belled Tree Rate criterion.
However, for this treethe probabilities of each node being correct are S:100%; A: 50%; and B: 50%.
The expected value ofL is 2.0, the highest of any tree.
This tree thereforeoptimizes the Labelled Recall Rate.3.1 A lgor i thmWe now derive an algorithm for finding the parsethat maximizes the expected Labelled Recall Rate.We do this by expanding expression (2) out into aprobabilistic form, converting this into a recursiveequation, and finally creating an equivalent dynamicprogramming algorithm.We begin by rewriting expression (2), expandingout the expected value operator, and removing the179which is the same for all TG, and so plays no NC 'role in the maximization.Ta = argmTaX~,P(Tc l w~) ITnTclTcThis can be further expanded to(4)Ta = arg mTax E P(Tc I w~)E1 if (s,t ,X) 6 TcTc (,,t,X)eT(5)Now, given a PCFG with start symbol S, the fol-lowing equality holds:P(s .
1,4)=E P(Tc I ~7)( 1 if (s, t, X) 6 Tc) (6)TcBy rearranging the summation i  expression (5)and then substituting this equality, we getTa =argm~x E P(S =~ s - t .
.
.
(,,t,X)eT(7)At this point, it is useful to introduce the Insideand Outside probabilities, due to Baker (1979), andexplained by Lari and Young (1990).
The Insideprobability is defined as e(s,t,X) = P(X =~ w~)and the Outside probability is f(s, t, X) = P(S =~8- I  n w 1 Xwt+l).
Note that while Baker and othershave used these probabilites for inducing rammars,here they are used only for parsing.Let us define a new function, g(s, t, X).g(s,t,X) P(S =~ , -1. .
n = w 1 Awt+ 1 \[w'~)P(S :~ ,-t n wl Xw,+I)P(X =~ w's)P(S wE)= f(s, t, X) x e(s, t, X)/e(1, n, S)Now, the definition of a Labelled Recall Parse canbe rewritten asT =arg%ax g(s,t,X) (8)(s,t,X)eTGiven the matrix g(s, t, X) it is a simple matter ofdynamic programming to determine the parse thatmaximizes the Labelled Recall criterion.
DefineMAXC(s, t) = n~xg(s, t, X)+max (MAXC(s, r) + MAXC(r + 1,t))r l s _<r<tfor length := 2 to nfor s := 1 to n - length+lt := s + length - I;loop over nontermina ls  Xlet max_g:=max imum of g(s , t ,X)loop over r such that s <= r < tlet best_sp l i t :=max of maxc\[s,r \ ]  + maxc\[ r+l , t \ ]maxc\[s,  t\] := max_g + best split;Figure h Labelled Recall AlgorithmIt is clear that MAXC(1, n) contains the score ofthe best parse according to the Labelled Recall cri-terion.
This equation can be converted into the dy-namic programming algorithm shown in Figure 1.For a grammar with r rules and k nonterminals,the run time of this algorithm is O(n 3 + kn 2) sincethere are two layers of outer loops, each with runtime at most n, and an inner loop, over nonterminalsand n. However, this is dominated by the computa-tion of the Inside and Outside probabilities, whichtakes time O(rna).By modifying the algorithm slightly to record theactual split used at each node, we can recover thebest parse.
The entry maxc\[1, n\] contains the ex-pected number of correct constituents, given themodel.4 Bracketed Recal l  Pars ingThe Labelled Recall Algorithm maximizes the ex-pected number of correct labelled constituents.However, many commonly used evaluation met-rics, such as the Consistent Brackets RecallRate, ignore labels.
Similarly, some gram-mar induction algorithms, such as those used byPereira and Schabes (1992) do not produce mean-ingful labels.
In particular, the Pereira and Schabesmethod induces a grammar f om the brackets in thetreebank, ignoring the labels.
While the inducedgrammar has labels, they are not related to thosein the treebank.
Thus, although the Labelled RecallAlgorithm could be used in these domains, perhapsmaximizing a criterion that is more closely tied tothe domain will produce better results.
Ideally, wewould maximize the Consistent Brackets Recall Ratedirectly.
However, since it is time-consuming to dealwith Consistent Brackets, we instead use the closelyrelated Bracketed Recall Rate.For the Bracketed Recall Algorithm, we find theparse that maximizes the expected Bracketed RecallRate, B/Nc.
(Remember that B is the number ofbrackets that are correct, and Nc is the number ofconstituents in the correct parse.
)180TG = arg rn~x E(B/Nc)  (9)Following a derivation similar to that used for theLabelled Recall Algorithm, we can rewrite equation(9) asTa=argm~x ~ ~_P(S :~ ,-1.~ ,~ wl(s,t)ET X(I0)The algorithm for Bracketed Recall parsing is ex-tremely similar to that for Labelled Recall parsing.The only required change is that we sum over thesymbols X to calculate max_g, rather than maximizeover them.5 Exper imenta l  Resu l tsWe describe two experiments for testing these algo-rithms.
The first uses a grammar without meaning-ful nonterminal symbols, and compares the Brack-eted Recall Algorithm to the traditional LabelledTree (Viterbi) Algorithm.
The second uses a gram-mar with meaningful nonterminal symbols and per-forms a three-way comparison between the LabelledRecall, Bracketed Recall, and Labelled Tree Algo-rithms.
These experiments show that use of an algo-rithm matched appropriately to the evaluation cri-terion can lead to as much as a 10% reduction inerror rate.In both experiments he grammars could not parsesome sentences, 0.5% and 9%, respectively.
The un-parsable data were assigned a right branching struc-ture with their rightmost element attached high.Since all three algorithms fail on the same sentences,all algorithms were affected equally.5.1 Exper iment  w i th  Grammar  Induced byPere i ra  and  Schabes MethodThe experiment of Pereira and Schabes (1992) wasduplicated.
In that experiment, a grammar wastrained from a bracketed form of the TI section of theATIS corpus 1 using a modified form of the Inside-Outside Algorithm.
Pereira and Schabes then usedthe Labelled Tree Algorithm to select the best parsefor sentences in held out test data.
The experi-ment was repeated here, except that both the La-belled Tree and Labelled Recall Algorithm were runfor each sentence.
In contrast o previous research,we repeated the experiment ten times, with differ-ent training set, test set, and initial conditions eachtime.Table 1 shows the results of running this ex-periment, giving the minimum, maximum, mean,and standard deviation for three criteria, Consis-tent Brackets Recall, Consistent Brackets Tree, and1For our experiments the corpus was slightlycleaned up.
A diff file for "ed" between the orig-inal ATIS data and the cleaned-up version is avail-able from ftp://ftp.das.harvard.edu/pub/goodman/atis-ed/ ti_tb.par-ed and ti_tb.pos-ed.
The number ofchanges made was small, less than 0.2%Criteria I\[ Min I Max I Mean I SDev ILabelled Tree AlgorithmCons Brack Rec 86.06 93.27 90.13 2.57Cons Brack Tree 51.14 77.27 63.98 7.96Brack Rec 71.38 81.88 75.87 3.18Bracketed Recall AlgorithmCons Brack Rec 88.02 94.34 91.14 2.22Cons Brack Tree 53.41 76.14 63.64 7.82Brack Rec 72.15 80.69 76.03 3.14DifferencesCons Brack Rec -1.55 2.45 1.01 1.07\] Cons Brack Tree -3.41 3.41 -0.34 2.34Brack Rec -1.34 2.02 0.17 1.20Table 1: Percentages Correct for Labelled Tree ver-sus Bracketed Recall for Pereira and SchabesBracketed Recall.
We also display these statisticsfor the paired differences between the algorithms.The only statistically significant difference is thatfor Consistent Brackets Recall Rate, which was sig-nificant to the 2% significance level (paired t-test).Thus, use of the Bracketed Recall Algorithm leadsto a 10% reduction in error rate.In addition, the performance of the Bracketed Re-call Algorithm was also qualitatively more appeal-ing.
Figure 2 shows typical results.
Notice that theBracketed Recall Algorithm's Consistent BracketsRate (versus iteration) is smoother and more nearlymonotonic than the Labelled Tree Algorithm's.
TheBracketed Recall Algorithm also gets off to a muchfaster start, and is generally (although not always)above the Labelled Tree level.
For the Labelled TreeRate, the two are usually very comparable.5.2 Experiment with Grammar I nduced byCountingThe replication of the Pereira and Schabes experi-ment was useful for testing the Bracketed Recall Al-gorithm.
However, since that experiment induces agrammar with nonterminals not comparable to thosein the training, a different experiment is needed toevaluate the Labelled Recall Algorithm, one in whichthe nonterminals in the induced grammar are thesame as the nonterminals in the test set.5.2.1 Grammar Induction by CountingFor this experiment, a very simple grammar wasinduced by counting, using a portion of the PennTree Bank, version 0.5.
In particular, the trees werefirst made binary branching by removing epsilon pro-ductions, collapsing singleton productions, and con-verting n-ary productions (n > 2) as in figure 3.
Theresulting trees were treated as the "Correct" trees inthe evaluation.
Only trees with forty or fewer sym-bols were used in this experiment.181Oo?;o?-p{DD.100908070605040302010!
I I I I I. .
_ - -  " .
.
.
.
.
.
.
.
i -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: - - -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J..... ...?4  .----...
..----" k .-.'"
" ' "  , '?, ,-4 ,% , - - - - "  ~?
- "  " - -' ~ / .
\ .~  (;:':"~" ''J':";'-'~:"":':'-/'~'-'~ .
.
.
.
.
.
.
.
.
.
.
.
.
_ ... .
.
.
.
.
.
.
.::::::-...:.:.
:::7- :':'::::'...:......-... : /.
J  ,'/ :/ ;/ "  ,..... .// ;.
/  / ;A/ .. ?
?...."
oo,- ,%, / ,t : " /  I0 lOLabelled Tree Algorithm: Consistent Brackets RecallBracketed Recall Algorithm: Consistent Brackets Recall .
.
.
.
.Labelled Tree Algorithm: Labelled Tree ......Bracketed Recall Algorithm: Labelled Tree ...........I | I I I20 30 40 50 60Iteration Number70Figure 2: Labelled Tree versus Bracketed Recall in Pereira and Schabes GrammarXbecomesXA X_ContB X_ContC DBracketsLabelsII Recall I Tree ILabelled Recall Labelled TreeTable 3: Metrics and Corresponding AlgorithmsFigure 3: Conversion of Productions to BinaryBranching 6 Conc lus ions  and Future  WorkA grammar was then induced in a straightforwardway from these trees, simply by giving one count foreach observed production.
No smoothing was done.There were 1805 sentences and 38610 nonterminalsin the test data.5.2.2 ResultsTable 2 shows the results of running all three algo-rithms, evaluating against five criteria.
Notice thatfor each algorithm, for the criterion that it optimizesit is the best algorithm.
That is, the Labelled TreeAlgorithm is the best for the Labelled Tree Rate,the Labelled Recall Algorithm is the best for theLabelled Recall Rate, and the Bracketed Recall Al-gorithm is the best for the Bracketed Recall Rate.Matching parsing algorithms to evaluation crite-ria is a powerful technique that can be used to im-prove performance.
In particular, the Labelled Re-call Algorithm can improve performance versus theLabelled Tree Algorithm on the Consistent Brack-ets, Labelled Recall, and Bracketed Recall criteria.Similarly, the Bracketed Recall Algorithm improvesperformance (versus Labelled Tree) on ConsistentBrackets and Bracketed Recall criteria.
Thus, thesealgorithms improve performance not only on themeasures that they were designed for, but also onrelated criteria.Furthermore, in some cases these techniques canmake parsing fast when it was previously imprac-tical.
We have used the technique outlined in thispaper in other work (Goodman, 1996) to efficientlyparse the DOP model; in that model, the only pre-viously known algorithm which summed over all the182CriterionLabel I Label Brack Cons Brack Cons BrackAlgorithm Tree \] Recall Recall Recall TreeLabel Tree 4.54~ 48.60% 60.98% 66.35% 12.07%Label Recall 3.71% 49.66~ 61.34% 68.39% 11.63%Bracket Recall 0.11% 4.51% 61.63~ 68.17% 11.19%Table 2: Grammar Induced by Counting: Three Algorithms Evaluated on Five Criteriapossible derivations was a slow Monte Carlo algo-rithm (Bod, 1993).
However, by maximizing theLabelled Recall criterion, rather than the LabelledTree criterion, it was possible to use a much sim-pler algorithm, a variation on the Labelled RecallAlgorithm.
Using this technique, along with otheroptimizations, we achieved a 500 times speedup.In future work we will show the surprising re-sult that the last element of Table 3, maximizingthe Bracketed Tree criterion, equivalent to maximiz-ing performance on Consistent Brackets Tree (ZeroCrossing Brackets) Rate in the binary branchingcase, is NP-complete.
Furthermore, we will showthat the two algorithms presented, the Labelled Re-call Algorithm and the Bracketed Recall Algorithm,are both special cases of a more general algorithm,the General Recall Algorithm.
Finally, we hope toextend this work to the n-ary branching case.7 AcknowledgementsI would like to acknowledge support from NationalScience Foundation Grant IRI-9350192, NationalScience Foundation infrastructure grant CDA 94-01024, and a National Science Foundation Gradu-ate Student Fellowship.
I would also like to thankStanley Chen, Andrew Kehler, Lillian Lee, and Stu-art Shieber for helpful discussions, and comments onearlier drafts, and the anonymous reviewers for theircomments.Conference on Empirical Methods in Natural Lan-guage Processing.
To appear.Lari, K. and S.J.
Young.
1990.
The estimation ofstochastic ontext-free grammars using the inside-outside algorithm.
Computer Speech and Lan-guage, 4:35-56.Magerman, David.
1994.
Natural Language Parsingas Statistical Pattern Recognition.
Ph.D. thesis,Stanford University University, February.Magerman, D.M.
and C. Weir.
1992.
Efficiency, ro-bustness, and accuracy in picky chart parsing.
InProceedings of the Association for ComputationalLinguistics.Pereira, Fernando and Yves Schabes.
1992.
Inside-Outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th Annual Meetingof the ACL, pages 128-135, Newark, Delaware.Stolcke, Andreas.
1993.
An efficient probabilisticcontext-free parsing algorithm that computes pre-fix probabilities.
Technical Report TR-93-065, In-ternational Computer Science Institute, Berkeley,CA.Re ferencesBaker, J.K. 1979.
Trainable grammars for speechrecognition.
In Proceedings of the Spring Confer-ence of the Acoustical Society of America, pages547-550, Boston, MA, June.Bod, Rens.
1993.
Using an annotated corpus as astochastic grammar.
In Proceedings of the SixthConference of the European Chapter of the ACL,pages 37-44.Brill, Eric.
1993.
A Corpus-Based Approach to Lan-guage Learning.
Ph.D. thesis, University of Penn-sylvania.Goodman, Joshua.
1996.
Efficient algorithms forparsing the DOP model.
In Proceedings of the183
