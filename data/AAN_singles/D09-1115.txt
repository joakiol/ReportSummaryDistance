Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105?1113,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPLattice-based System Combination for Statistical Machine TranslationYang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu?Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cnAbstractCurrent system combination methods usu-ally use confusion networks to find consensustranslations among different systems.
Requir-ing one-to-one mappings between the wordsin candidate translations, confusion networkshave difficulty in handling more general situa-tions in which several words are connected toanother several words.
Instead, we propose alattice-based system combination model thatallows for such phrase alignments and useslattices to encode all candidate translations.Experiments show that our approach achievessignificant improvements over the state-of-the-art baseline system on Chinese-to-Englishtranslation test sets.1 IntroductionSystem combination aims to find consensus transla-tions among different machine translation systems.It has been proven that such consensus translationsare usually better than the output of individual sys-tems (Frederking and Nirenburg, 1994).In recent several years, the system combinationmethods based on confusion networks developedrapidly (Bangalore et al, 2001; Matusov et al, 2006;Sim et al, 2007; Rosti et al, 2007a; Rosti et al,2007b; Rosti et al, 2008; He et al, 2008), whichshow state-of-the-art performance in benchmarks.
Aconfusion network consists of a sequence of sets ofcandidate words.
Each candidate word is associatedwith a score.
The optimal consensus translation canbe obtained by selecting one word from each set tomaximizing the overall score.To construct a confusion network, one first needto choose one of the hypotheses (i.e., candidatetranslations) as the backbone (also called ?skeleton?in the literature) and then decide the word align-ments of other hypotheses to the backbone.
Hy-pothesis alignment plays a crucial role in confusion-network-based system combination because it has adirect effect on selecting consensus translations.However, a confusion network is restricted insuch a way that only 1-to-1 mappings are allowedin hypothesis alignment.
This is not the fact evenfor word alignments between the same languages.
Itis more common that several words are connectedto another several words.
For example, ?be capa-ble of?
and ?be able to?
have the same meaning.Although confusion-network-based approaches re-sort to inserting null words to alleviate this problem,they face the risk of producing degenerate transla-tions such as ?be capable to?
and ?be able of?.In this paper, we propose a new system combina-tion method based on lattices.
As a more generalform of confusion network, a lattice is capable ofdescribing arbitrary mappings in hypothesis align-ment.
In a lattice, each edge is associated with asequence of words rather than a single word.
There-fore, we select phrases instead of words in eachcandidate set and minimize the chance to produceunexpected translations such as ?be capable to?.We compared our approach with the state-of-the-artconfusion-network-based system (He et al, 2008)and achieved a significant absolute improvement of1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST2008 Chinese-to-English test set.1105He feels like applesHe prefer applesHe feels like applesHe is fond of apples(a) unidirectional alignmentsHe feels like applesHe prefer applesHe feels like applesHe is fond of apples(b) bidirectional alignmentsHe feels like ?
apples?
prefer ofis fond(c) confusion networkhe feels like apples?
preferis fond of(d) latticeFigure 1: Comparison of a confusion network and a lat-tice.2 Background2.1 Confusion Network and LatticeWe use an example shown in Figure 1 to illustrateour idea.
Suppose that there are three hypotheses:He feels like applesHe prefer applesHe is fond of applesWe choose the first sentence as the backbone.Then, we perform hypothesis alignment to build aconfusion network, as shown in Figure 1(a).
Notethat although ?feels like?
has the same meaning with?is fond of?, a confusion network only allows forone-to-one mappings.
In the confusion networkshown in Figure 1(c), several null words ?
are in-serted to ensure that each hypothesis has the samelength.
As each edge in the confusion network onlyhas a single word, it is possible to produce inappro-priate translations such as ?He is like of apples?.In contrast, we allow many-to-many mappingsin the hypothesis alignment shown in Figure 2(b).For example, ?like?
is aligned to three words: ?is?,?fond?, and ?of?.
Then, we use a lattice shown inFigure 1(d) to represent all possible candidate trans-lations.
Note that the phrase ?is fond of?
is attachedto an edge.
Now, it is unlikely to obtain a translationlike ?He is like of apples?.A lattice G = ?V,E?
is a directed acyclic graph,formally a weighted finite state automation (FSA),where V is the set of nodes and E is the set of edges.The nodes in a lattice are usually labeled accordingto an appropriate numbering to reflect how to pro-duce a translation.
Each edge in a lattice is attachedwith a sequence of words as well as the associatedprobability.As lattice is a more general form of confusionnetwork (Dyer et al, 2008), we expect that replac-ing confusion networks with lattices will further im-prove system combination.2.2 IHMM-based Alignment MethodSince the candidate hypotheses are aligned us-ing Indirect-HMM-based (IHMM-based) alignmentmethod (He et al, 2008) in both direction, we brieflyreview the IHMM-based alignment method first.Take the direction that the hypothesis is aligned tothe backbone as an example.
The conditional prob-ability that the hypothesis is generated by the back-bone is given byp(e?1J|eI1) =?aJ1J?j=1[p(aj|aj?1, I)p(e?j|eaj)]l (1)Where eI1= (e1, ..., eI) is the backbone, e?J1=(e?1, ..., e?J) is a hypothesis aligned to eI1, and aJ1=(a1, .., aJ) is the alignment that specifies the posi-tion of backbone word that each hypothesis word isaligned to.The translation probability p(e?j|ei) is a linear in-terpolation of semantic similarity psem(e?j|ei) andsurface similarity psur(e?j|ei) and ?
is the interpo-lation factor:p(e?j|ei) = ??psem(e?j|ei)+(1??
)?psur(e?j|ei) (2)The semantic similarity model is derived by usingthe source word sequence as a hidden layer, so thebilingual dictionary is necessary.
The semantic sim-1106ilarity model is given bypsem(e?j|ei) =K?k=0p(fk|ei)p(e?j|fk, ei)?K?k=0p(fk|ei)p(e?j|fk)(3)The surface similarity model is estimated by calcu-lating the literal matching rate:psur(e?j|ei) = exp{?
?
[s(e?j, ei)?
1]} (4)where s(e?j, ei) is given bys(e?j, ei) =M(e?j, ei)max(|e?j|, |ei|)(5)where M(e?j, ei) is the length of the longest matchedprefix (LMP) and ?
is a smoothing factor that speci-fies the mapping.The distortion probability p(aj= i|aj?1= i?, I)is estimated by only considering the jump distance:p(i|i?, I) =c(i?
i?
)?Ii=1c(l ?
i?
)(6)The distortion parameters c(d) are grouped into 11buckets, c(?
?4), c(?3), ..., c(0), ..., c(5), c(?
6).Since the alignments are in the same language, thedistortion model favor monotonic alignments andpenalize non-monotonic alignments.
It is given ina intuitive wayc(d) = (1 + |d?
1|)?K, d = ?4, ..., 6 (7)where K is tuned on held-out data.Also the probability p0of jumping to a null wordstate is tuned on held-out data.
So the overall distor-tion model becomesp(i|i?, I) ={p0if i = null state(1?
p0) ?
p(i|i?, I) otherwise3 Lattice-based System CombinationModelLattice-based system combination involves the fol-lowing steps:(1) Collect the hypotheses from the candidate sys-tems.
(2) Choose the backbone from the hypotheses.This is performed using a sentence-level MinimumBayes Risk (MBR) method.
The hypothesis with theminimum cost of edits against all hypotheses is se-lected.
The backbone is significant for it influencesnot only the word order, but also the following align-ments.
The backbone is selected as follows:EB= argminE?
?E?E?ETER(E?, E) (8)(3) Get the alignments of the backbone and hy-pothesis pairs.
First, each pair is aligned in both di-rections using the IHMM-based alignment method.In the IHMM alignment model, bilingual dictionar-ies in both directions are indispensable.
Then, weapply a grow-diag-final algorithm which is widelyused in bilingual phrase extraction (Koehn et al,2003) to monolingual alignments.
The bidirec-tional alignments are combined to one resorting tothe grow-diag-final algorithm, allowing n-to-n map-pings.
(4)Normalize the alignment pairs.
The word or-der of the backbone determines the word order ofconsensus outputs, so the word order of hypothesesmust be consistent with that of the backbone.
Allwords of a hypotheses are reordered according tothe alignment to the backbone.
For a word alignedto null, an actual null word may be inserted to theproper position.
The alignment units are extractedfirst and then the hypothesis words in each unit areshifted as a whole.
(5) Construct the lattice in the light of phrasepairs extracted on the normalized alignment pairs.The expression ability of the lattice depends on thephrase pairs.
(6) Decode the lattice using a model similar to thelog-linear model.The confusion-network-based system combina-tion model goes in a similar way.
The first two stepsare the same as the lattice-based model.
The differ-ence is that the hypothesis pairs are aligned just inone direction due to the expression limit of the con-fusion network.
As a result, the normalized align-ments only contain 1-to-1 mappings (Actual nullwords are also needed in the case of null alignment).In the following, we will give more details about thesteps which are different in the two models.11074 Lattice ConstructionUnlike a confusion network that operates wordsonly, a lattice allows for phrase pairs.
So phrasepairs must be extracted before constructing a lat-tice.
A major difficulty in extracting phrase pairsis that the word order of hypotheses is not consistentwith that of the backbone.
As a result, hypothesiswords belonging to a phrase pair may be discon-tinuous.
Before phrase pairs are extracted, the hy-pothesis words should be normalized to make surethe words in a phrase pair is continuous.
We call aphrase pair before normalization a alignment unit.The problem mentioned above is shown in Fig-ure 2.
In Figure 2 (a), although (e?1e?3, e2) should bea phrase pair, but /e?10 and /e?30 are discontin-uous, so the phrase pair can not be extracted.
Onlyafter the words of the hypothesis are reordered ac-cording to the corresponding words in the backboneas shown in Figure 2 (b), /e?10 and /e?30 be-come continuous and the phrase pair (e?1e?3, e2) canbe extracted.
The procedure of reordering is calledalignment normalizationEh: e?1e?2e?3EB:e1e2e3(a)Eh: e?2e?1e?3EB:e1e2e3(b)Figure 2: An example of alignment units4.1 Alignment NormalizationAfter the final alignments are generated in the grow-diag-final algorithm, minimum alignment units areextracted.
The hypothesis words of an alignmentunit are packed as a whole in shift operations.See the example in Figure 2 (a) first.
All mini-mum alignment units are as follows: (e?2, e1), (e?1e?3,e2) and (?, e3).
(e?1e?2e?3, e1e2) is an alignment unit,but not a minimum alignment unit.Let a?i= (e?
?i, e?i) denote a minimum alignmentunit, and assume that the word string e?
?icovers wordse?i1,..., e?imon the hypothesis side, and the wordstring e?icovers the consecutive words ei1,..., einonthe backbone side.
In an alignment unit, the wordstring on the hypothesis side can be discontinuous.The minimum unit a?i= (e?
?i, e?i) must observe thefollowing rules:EB: e1e2e3Eh:e?1e?2 (a)e1e2e3e?2?e?1EB: e1e2Eh: e?1e?2e?3e1e2e?1e?3e?1e?2e?3(b)EB: e1e2Eh:e?1e?2e?3e1?e2e?1e?2e?3(c)Figure 3: Different cases of null insertion?
?
e?ik?
e?
?i, ea?ik?
e?i?
?
eik?
e?i, e?aik= null or e?aik?
e??i?
?
a?j= (e?
?j, e?j), e?j= ei1, ..., eikor e?j=eik, ..., ein, k ?
[1, n]Where a?ikdenotes the position of the word in thebackbone that e?ikis aligned to, and aikdenotes theposition of the word in the hypothesis that eikisaligned to.An actual null word may be inserted to a properposition if a word, either from the hypothesis or fromthe backbone, is aligned to null.
In this way, theminimum alignment set is extended to an alignmentunit set, which includes not only minimum align-ment units but also alignment units which are gener-ated by adding null words to minimum alignmentunits.
In general, the following three conditionsshould be taken into consideration:?
A backbone word is aligned to null.
A nullword is inserted to the hypothesis as shown inFigure 3 (a).?
A hypothesis word is aligned to null and it isbetween the span of a minimum alignment unit.A new alignment unit is generated by insert-ing the hypothesis word aligned to null to theminimum alignment unit.
The new hypothesisstring must remain the original word order ofthe hypothesis.
It is illustrated in Figure 3 (b).?
A hypothesis word is aligned to null and it isnot between the hypothesis span of any mini-mum alignment unit.
In this case, a null word1108e1e2?e3e??4e??5e??6(a)e1?e2e3e??1e??2e??3(b)e1?e2e3e??1e??2e??3e??4(c)e1?e2?e3e??1e??2e??3e??4e??5(d)e1?e2?e3e??1e??2e??3e??4e??5e?
?6(e)Figure 4: A toy instance of lattice constructionare inserted to the backbone.
This is shown inFigure 3 (c).4.2 Lattice Construction AlgorithmThe lattice is constructed by adding the normalizedalignment pairs incrementally.
One backbone arc ina lattice can only span one backbone word.
In con-trast, all hypothesis words in an alignment unit mustbe packed into one hypothesis arc.
First the lattice isinitialized with a normalized alignment pair.
Thengiven all other alignment pairs one by one, the lat-tice is modified dynamically by adding the hypothe-sis words of an alignment pair in a left-to-right fash-ion.A toy instance is given in Figure 4 to illustrate theprocedure of lattice construction.
Assume the cur-rent inputs are: an alignment pair as in Figure 4 (a),and a lattice as in Figure 4 (b).
The backbone wordsof the alignment pair are compared to the backbonewords of the lattice one by one.
The procedure is asfollows:?
e1is compared with e1.
Since they are thesame, the hypothesis arc e?
?4, which comes fromthe same node with e1in the alignment pair,is compared with the hypothesis arc e?
?1, whichcomes from the same node with e1in the lat-tice.
The two hypothesis arcs are not the same,so e?
?4is added to the lattice as shown in Figure4(c).
Both go to the next backbone words.?
e2is compared with ?.
The lattice remains thesame.
The lattice goes to the next backboneword e2.?
e2is compared with e2.
There is no hypothesisarc coming from the same node with the bonearc e2in the alignment pair, so the lattice re-mains the same.
Both go to the next backbonewords.?
?
is compared with e3.
A null backbone arc isinserted into the lattice between e2and e3.
Thehypothesis arc e?
?5is inserted to the lattice, too.The modified lattice is shown in Figure 4(d).The alignment pair goes to the next backboneword e3.?
e3is compared with e3.
For they are the sameand there is no hypothesis arc e?
?6in the lattice,e?
?6is inserted to the lattice as in Figure 4(e).?
Both arrive at the end and it is the turn of thenext alignment pair.When comparing a backbone word of the givenalignment pair with a backbone word of the lattice,the following three cases should be handled:?
The current backbone word of the given align-ment pair is a null word while the current back-bone word of the lattice is not.
A null back-bone word is inserted to the lattice.?
The current backbone word of the lattice is anull word while the current word of the givenalignment pair is not.
The current null back-bone word of the lattice is skipped with nothingto do.
The next backbone word of the lattice iscompared with the current backbone word ofthe given alignment pair.1109Algorithm 1 Lattice construction algorithm.1: Input: alignment pairs {pn}Nn=12: L?
p13: Unique(L)4: for n?
2 .. N do5: pnode = pn?
first6: lnode = L ?
first7: while pnode ?
barcnext 6= NULL do8: if lnode ?
barcnext = NULL or pnode ?bword = null and lnode ?
bword 6= null then9: INSERTBARC(lnode, null)10: pnode = pnode ?
barcnext11: else12: if pnode ?
bword 6= null and lnode ?bword = null then13: lnode = lnode ?
barcnext14: else15: for each harc of pnode do16: if NotExist(lnode, pnode ?
harc)then17: INSERTHARC(lnode, pnode ?harc)18: pnode = pnode ?
barcnext19: lnode = lnode ?
barcnext20: Output: lattice L?
The current backbone words of the given align-ment pair and the lattice are the same.
Let{harcl} denotes the set of hypothesis arcs,which come from the same node with the cur-rent backbone arc in the lattice, and harchde-notes one of the corresponding hypothesis arcsin the given alignment pair.
In the {harcl},if there is no arc which is the same with theharch, a hypothesis arc projecting to harchisadded to the lattice.The algorithm of constructing a lattice is illus-trated in Algorithm 1.
The backbone words of thealignment pair and the lattice are processed one byone in a left-to-right manner.
Line 2 initializes thelattice with the first alignment pair, and Line 3 re-moves the hypothesis arc which contains the samewords with the backbone arc.
barc denotes the back-bone arc, storing one backbone word only, and harcdenotes the hypothesis arc, storing the hypothesiswords.
For there may be many alignment units spanthe same backbone word range, there may be morethan one harc coming from one node.
Line 8 ?
10consider the condition 1 and function InsertBarc inLine 9 inserts a null bone arc to the position rightbefore the current node.
Line 12?13 deal with con-dition 2 and jump to the next backbone word of thelattice.
Line 15?19 handle condition 3 and functionInsertHarc inserts to the lattice a harc with the samehypothesis words and the same backbone word spanwith the current hypothesis arc.5 DecodingIn confusion network decoding, a translation is gen-erated by traveling all the nodes from left to right.So a translation path contains all the nodes.
Whilein lattice decoding, a translation path may skip somenodes as some hypothesis arcs may cross more thanone backbone arc.Similar to the features in Rosti et al (2007a), thefeatures adopted by lattice-based model are arc pos-terior probability, language model probability, thenumber of null arcs, the number of hypothesis arcspossessing more than one non-null word and thenumber of all non-null words.
The features are com-bined in a log-linear model with the arc posteriorprobabilities being processed specially as follows:log p(e/f) =Narc?i=1log (Ns?s=1?sps(arc))+ ?L(e) + ?Nnullarc(e)+ ?Nlongarc(e) + ?Nword(e)(9)where f denotes the source sentence, e denotes atranslation generated by the lattice-based system,Narcis the number of arcs the path of e covers,Nsis the number of candidate systems and ?sis theweight of system s. ?
is the language model weightand L(e) is the LM log-probability.
Nnullarcs(e) isthe number of the arcs which only contain a nullword, and Nlongarc(e) is the number of the arcswhich store more than one non-null word.
Theabove two numbers are gotten by counting bothbackbone arcs and hypothesis arcs.
?
and ?
are thecorresponding weights of the numbers, respectively.Nword(e) is the non-null word number and ?
is itsweight.Each arc has different confidences concerned withdifferent systems, and the confidence of system sis denoted by ps(arc).
ps(arc) is increased by11101/(k+1) if the hypothesis ranking k in the system scontains the arc (Rosti et al, 2007a; He et al, 2008).Cube pruning algorithm with beam search is em-ployed to search for the consensus output (Huangand Chiang, 2005).
The nodes in the lattice aresearched in a topological order and each node re-tains a list of N best candidate partial translations.6 ExperimentsThe candidate systems participating in the systemcombination are as listed in Table 1: System A is aBTG-based system using a MaxEnt-based reorder-ing model; System B is a hierarchical phrase-basedsystem; System C is the Moses decoder (Koehn etal., 2007); System D is a syntax-based system.
10-best hypotheses from each candidate system on thedev and test sets were collected as the input of thesystem combination.In our experiments, the weights were all tuned onthe NIST MT02 Chinese-to-English test set, includ-ing 878 sentences, and the test data was the NISTMT05 Chinese-to-English test set, including 1082sentences, except the experiments in Table 2.
A 5-gram language model was used which was trainedon the XinHua portion of Gigaword corpus.
The re-sults were all reported in case sensitive BLEU scoreand the weights were tuned in Powell?s method tomaximum BLEU score.
The IHMM-based align-ment module was implemented according to He etal.
(2008), He (2007) and Vogel et al (1996).
In allexperiments, the parameters for IHMM-based align-ment module were set to: the smoothing factor forthe surface similarity model, ?
= 3; the controllingfactor for the distortion model, K = 2.6.1 Comparison withConfusion-network-based modelIn order to compare the lattice-based system withthe confusion-network-based system fairly, we usedIHMM-based system combination model on behalfof the confusion-network-based model described inHe et al (2008).
In both lattice-based and IHMM-based systems, the bilingual dictionaries were ex-tracted on the FBIS data set which included 289Ksentence pairs.
The interpolation factor of the simi-larity model was set to ?
= 0.1.The results are shown in Table 1.
IHMM standsfor the IHMM-based model and Lattice stands forthe lattice-based model.
On the dev set, the lattice-based system was 3.92 BLEU points higher than thebest single system and 0.36 BLEU point higher thanthe IHMM-based system.
On the test set, the lattice-based system got an absolute improvement by 3.73BLEU points over the best single system and 1.23BLEU points over the IHMM-based system.System MT02 MT05BLEU% BLEU%SystemA 31.93 30.68SystemB 32.16 32.07SystemC 32.09 31.64SystemD 33.37 31.26IHMM 36.93 34.57Lattice 37.29 35.80Table 1: Results on the MT02 and MT05 test setsThe results on another test sets are reported in Ta-ble 2.
The parameters were tuned on the newswirepart of NIST MT06 Chinese-to-English test set, in-cluding 616 sentences, and the test set was NISTMT08 Chinese-to-English test set, including 1357sentences.
The BLEU score of the lattice-based sys-tem is 0.93 BLEU point higher than the IHMM-based system and 3.0 BLEU points higher than thebest single system.System MT06 MT08BLEU% BLEU%SystemA 32.51 25.63SystemB 31.43 26.32SystemC 31.50 23.43SystemD 32.41 26.28IHMM 36.05 28.39Lattice 36.53 29.32Table 2: Results on the MT06 and MT08 test setsWe take a real example from the output of thetwo systems (in Table 3) to show that higher BLEUscores correspond to better alignments and bettertranslations.
The translation of System C is selectedas the backbone.
From Table 3, we can see thatbecause of 1-to-1 mappings, ?Russia?
is aligned to?Russian?
and ??s?
to ?null?
in the IHMM-basedmodel, which leads to the error translation ?Russian1111Source: ?dIE?h?i??dIEd?i?1?
?SystemA: Russia merger of state-owned oil company and the state-run gas company in RussiaSystemB: Russia ?s state-owned oil company is working with Russia ?s state-run gas company mergersSystemC: Russian state-run oil company is combined with the Russian state-run gas companySystemD: Russia ?s state-owned oil companies are combined with Russia ?s state-run gas companyIHMM: Russian ?s state-owned oil company working with Russia ?s state-run gas companyLattice: Russia ?s state-owned oil company is combined with the Russian state-run gas companyTable 3: A real translation example?s?.
Instead, ?Russia ?s?
is together aligned to ?Rus-sian?
in the lattice-based model.
Also due to 1-to-1 mappings, null word aligned to ?is?
is inserted.As a result, ?is?
is missed in the output of IHMM-based model.
In contrast, in the lattice-based sys-tem, ?is working with?
are aligned to ?is combinedwith?, forming a phrase pair.6.2 Effect of Dictionary ScaleThe dictionary is important to the semantic similar-ity model in IHMM-based alignment method.
Weevaluated the effect of the dictionary scale by usingdictionaries extracted on different data sets.
The dic-tionaries were respectively extracted on similar datasets: 30K sentence pairs, 60K sentence pairs, 289Ksentence pairs (FBIS corpus) and 2500K sentencepairs.
The results are illustrated in Table 4.
In or-der to demonstrate the effect of the dictionary sizeclearly, the interpolation factor of similarity modelwas all set to ?
= 0.1.From Table 4, we can see that when the cor-pus size rise from 30k to 60k, the improvementswere not obvious both on the dev set and on thetest set.
As the corpus was expanded to 289K, al-though on the dev set, the result was only 0.2 BLEUpoint higher, on the test set, it was 0.63 BLEU pointhigher.
As the corpus size was up to 2500K, theBLEU scores both on the dev and test sets declined.The reason is that, on one hand, there are more noiseon the 2500K sentence pairs; on the other hand, the289K sentence pairs cover most of the words appear-ing on the test set.
So we can conclude that in or-der to get better results, the dictionary scale must beup to some certain scale.
If the dictionary is muchsmaller, the result will be impacted dramatically.MT02 MT05BLEU% BLEU%30k 36.94 35.1460k 37.09 35.17289k 37.29 35.802500k 37.14 35.62Table 4: Effect of dictionary scale6.3 Effect of Semantic AlignmentsFor the IHMM-based alignment method, the transla-tion probability of an English word pair is computedusing a linear interpolation of the semantic similar-ity and the surface similarity.
So the two similaritymodels decide the translation probability togetherand the proportion is controlled by the interpolationfactor.
We evaluated the effect of the two similaritymodels by varying the interpolation factor ?.We used the dictionaries extracted on the FBISdata set.
The result is shown in Table 5.
We got thebest result with ?
= 0.1.
When we excluded thesemantic similarity model (?
= 0.0) or excluded thesurface similarity model (?
= 1.0), the performancebecame worse.7 ConclusionThe alignment model plays an important role insystem combination.
Because of the expressionlimitation of confusion networks, only 1-to-1 map-pings are employed in the confusion-network-basedmodel.
This paper proposes a lattice-based systemcombination model.
As a general form of confusionnetworks, lattices can express n-to-n mappings.
Soa lattice-based model processes phrase pairs while1112MT02 MT05BLEU% BLEU%?
= 1.0 36.41 34.92?
= 0.7 37.21 35.65?
= 0.5 36.43 35.02?
= 0.4 37.14 35.55?
= 0.3 36.75 35.66?
= 0.2 36.81 35.55?
= 0.1 37.29 35.80?
= 0.0 36.45 35.14Table 5: Effect of semantic alignmentsa confusion-network-based model processes wordsonly.
As a result, phrase pairs must be extracted be-fore constructing a lattice.On NIST MT05 test set, the lattice-based sys-tem gave better results with an absolute improve-ment of 1.23 BLEU points over the confusion-network-based system (He et al, 2008) and 3.73BLEU points over the best single system.
OnNIST MT08 test set, the lattice-based system out-performed the confusion-network-based system by0.93 BLEU point and outperformed the best singlesystem by 3.0 BLEU points.8 AcknowledgementThe authors were supported by National Natural Sci-ence Foundation of China Contract 60736014, Na-tional Natural Science Foundation of China Con-tract 60873167 and High Technology R&D ProgramProject No.
2006AA010108.
Thank Wenbin Jiang,Tian Xia and Shu Cai for their help.
We are alsograteful to the anonymous reviewers for their valu-able comments.ReferencesSrinivas Bangalore, German Bordel, and Giuseppe Ric-cardi.
2001.
Computing consensus translation frommultiple machine translation systems.
In Proc.
ofIEEE ASRU, pages 351?354.Christopher Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of ACL/HLT 2008, pages 1012?1020, Colum-bus, Ohio, June.Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proc.
of ANLP, pages95?100.Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-hmm-based hy-pothesis alignment for computing outputs from ma-chine translation systems.
In Proc.
of EMNLP, pages98?107.Xiaodong He.
2007.
Using word-dependent translationmodels in hmm based word alignment for statisticalmachine translation.
In Proc.
of COLING-ACL, pages961?968.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies (IWPT), pages 53?64.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of the 45th ACL, DemonstrationSession.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In Proc.
of IEEE EACL, pages 33?40.Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz.
2007a.
Improved word-level system com-bination for machine translation.
In Proc.
of ACL,pages 312?319.Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,Richard Schwartz, Necip Fazil Ayan, and Bonnie J.Dorr.
2007b.
Combining outputs from multiple ma-chine translation systems.
In Proc.
of NAACL-HLT,pages 228?235.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothesisalignment for building confusion networks with appli-cation to machine translaiton system combination.
InProc.
of the Third ACL WorkShop on Statistical Ma-chine Translation, pages 183?186.Khe Chai Sim, William J. Byrne, Mark J.F.
Gales,Hichem Sahbi, and Phil C. Woodland.
2007.
Con-sensus network decoding for statistical machine trans-lation system combination.
In Proc.
of ICASSP, pages105?108.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statistical trans-lation.
In Proc.
of COLING, pages 836?841.1113
