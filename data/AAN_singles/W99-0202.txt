Is Hillary Rodham Clinton the President?
Disambiguating Namesacross DocumentsYael RAVINT.
J. Watson Research Center, IBMP.O.
Box 704,Yorktown Heights, NY 10598ravin@us.ibm.comAbstractA number of research and software developmentgroups have developed name identificationtechnology, but few have addressed the issue ofcross-document coreference, or identifying thesame named entities across documents.
In acollection of documents, where there aremultiple discourse contexts, there exists a many-to-many correspondence b tween names andentities, making it a challenge to automaticallymap them correctly.
Recently, Bagga andBaldwin proposed a method for determiningwhether two names refer to the same entity bymeasuring the similarity between the documentcontexts in which they appear.
Inspired by theirapproach, we have revisited our current cross-document coreference heuristics that makerelatively simple decisions based on matchingstrings and entity types.
We have devised animproved and promising algorithm, which wediscuss in this paper.IntroductionThe need to identify and extract importantconcepts in online text documents i  by nowcommonly acknowledged by researchers andpractitioners in the fields of informationretrieval, knowledge management and digitallibraries.
It is a necessary first step towardsachieving a reduction in the ever-increasingvolumes of online text.
In this paper we focus onthe identification of one kind of concept - namesand the entities they refer to.There are several challenging aspects to theidentification of names: identifying the textstrings (words or phrases) that express names;relating names to the entities discussed in thedocument; and relating named entities acrossdocuments.
In relating names to entities, theZunaid KAZIT.
J. Watson Research Center, IBMP.O.
Box 704,Yorktown Heights, NY 10598Zhkazi@us.ibm.commain difficulty is the many-to-many mappingbetween them.
A single entity can be referred toby several name variants: Ford Motor Company,Ford Motor Co., or simply Ford.
A singlevariant often names everal entities: Ford refersto the car company, but also to a place (Ford,Michigan) as well as to several people: PresidentGerald Ford, Senator Wendell Ford, and others.Context is crucial in identifying the intendedmapping.
A document usually defines a singlecontext, in which it is quite unlikely to findseveral entities corresponding to the samevariant.
For example, if the document alksabout the car company, it is unlikely to alsodiscuss Gerald Ford.
Thus, within documents,the problem is usually reduced to a many-to-onemapping between several variants and a singleentity.
In the few cases where multiple ntities inthe document may potentially share a namevariant, the problem is addressed by carefuleditors, who refrain from using ambiguousvariants.
If Henry Ford, for example, ismentioned in the context of the car company, hewill most likely be referred to by theunambiguous Mr. Ford.Much recent work has been devoted to theidentification of names within documents and tolinking names to entities within the document.Several research groups \[DAR95, DAR98\], aswell as a few commercial software packages\[NetOw197\], have developed name identificationtechnologyk In contrast, few have investigatednamed entities across documents.
In a collectionof documents, there are multiple contexts;variants may or may not refer to the same entity;i among them our own research group, whosetechnology is now embedded in IBM's IntelligentMiner for Text \[IBM99\].9and ambiguity is a much greater problem.Cross-document coreference was brieflyconsidered as a task for the Sixth MessageUnderstanding Conference but then discarded asbeing too difficult \[DAR95\].Recently, Bagga and Baldwin \[BB98\] proposeda method for determining whether two names(mostly of people) or events refer to the sameentity by measuring the similarity between thedocument contexts in which they appear.Inspired by their approach, we have revisited ourcurrent cross-document coreference heuristicsand have devised an improved algorithm thatseems promising.
In contrast o the approach in\[BB98\], our algorithm capitalizes on the carefulintra-document name recognition we havedeveloped.
To minimize the processing costinvolved in comparing contexts we definecompatible names -- groups of names that aregood candidates for coreference -- and comparetheir internal structures first, to decide whetherthey corefer.
Only then, if needed, we apply ourown version of context comparisons, reusing atool -- the Context Thesaurus -- which we havedeveloped independently, as part of anapplication to assist users in querying acollection of documents.Cross-document coreference depends heavily onthe results of intra-document coreference, aprocess which we describe in Section 1.
InSection 2 we discuss our current cross-documentcoreference.
One of our challenges is torecognize that some "names" we identify are notvalid, in that they do not have a single referent.Rather, they form combinations of componentnames.
In Section 3 we describe our algorithmfor splitting these combinations.
Another cross-document challenge is to merge different names.Our intra-document analysis stipulates morenames than there are entities mentioned in thecollection.
In Sections 4-5 we discuss how wemerge these distinct but eoreferent names acrossdocuments.
Section 4 defines compatible namesand how their internal structure determinescoreference.
Section 5 describes the ContextThesaurus and its use to compare contexts inwhich names occur.
Section 6 describespreliminary results and future work.1 lntra-Document Name IdentificationOur group has developed a set of tools, calledTalent, to analyze and process information intext.
One of the Talent tools is Nominator, thename identification module \[RW96\].
Weillustrate the process of intra-document nameidentification -- more precisely, name discovery-- with an excerpt from \[NIST93\]....The profess ional  conduct of lawyers inother jur isd ict ions is gu ided by Amer icanBar Assoc iat ion  rules ...
The ABA hassteadfast ly  reserved ...
But Robert  Jordan,a partner  at Steptoe & Johnson who took thelead in ... "The pract ice  of law inWashington is very  d i f ferent  from what it isin Dubuque," he said .
.
.
.
Mr. Jordan ofSteptoe & Johnson ...Before the text is processed by Nominator, it isanalyzed into tokens - words, tags, andpunctuation elements.
Nominator forms acandidate name list by scanning the tokenizeddocument and collecting sequences ofcapitalized tokens as well as some special ower-case ones.
The list of candidate names extractedfrom the sample document contains:Amer ican Bar Assoc iat ionRobert JordanSteptoe & JohnsonABAWashingtonDubuqueMr.
Jordan of Steptoe & JohnsonEach candidate name is examined for thepresence of conjunctions, prepositions orpossessives ('s).
These may indicate points atwhich the candidate name should be split intocomponent names.
A set of heuristics is appliedto each candidate name, to split it into as manysmaller independent names as are found in it.Mr.
Jordan of Steptoe & Johnson is split intoMr.
Jordan and Steptoe & Johnson.
Withoutrecourse to semantics or world knowledge, wed6 not always have sufficient evidence.
In suchcases we prefer to err on the conservative sideand not split, so as to not lose any information.This explains the presence of "names" such asAmerican Television & Communications andHouston Industries lnc.
or Dallas's MCorp andFirst RepublicBank and Houston's First CityBancorp.
of Texas in our intra-document results.We discuss later the splitting of these conjoined"names" at the collection level.10As the last step in name identification within thedocument, Nominator links all variants referringto the same entity.
For example ABA is linked toAmerican Bar Association as a possibleabbreviation.
Each linked group is categorizedby an entity type and assigned a canonical stringas identifier.
The result for the sample text isshown below.
Each canonical string is followedby its entity type (PL for PLACE; PR forPERSON) and the variant names linked to it.Amer ican  Bar  Assoc ia t ion  \[ORG\] : ABASteptoe  & Johnson  \[ORG\]Wash ington  \[PL\]Dubuque \[PL\]Rober t  Jo rdan  \[PR\] : Mr. Jo rdanIn a typical document, a single entity may bereferred to by many name variants, which differin their degree of potential ambiguity.
Todisambiguate highly ambiguous variants, welink them to unambiguous ones occurring withinthe document.
Nominator cycles through the listof names, identifying 'anchors', or variant namesthat unambiguously refer to certain entity types.When an anchor is identified, the list of namecandidates is scanned for ambiguous variantsthat could refer to the same entity.
They aregrouped together with the anchor in anequivalence group.A few simple indicators determine the entitytype of a name, such as Mr. for a person or Inc.for an organization.
More commonly, however,several pieces of positive and negative videnceare accumulated in order to make this judgment.We have defined a set of obligatory and optionalcomponents for each entity type.
For a humanname, these components include a professionaltitle (e.g., Attorney General), a personal title(e.g., Dr.), a first name, and others.
The variouscomponents are inspected.
Some combinationsmay result in a high negative score -- highlyconfident that this cannot be a person name.
Forexample, if the name lacks a personal title and afirst name, and its last name is marked as anorganization word  (e.g., Department), it willreceive a high negative score.
This is the casewith Justice Department or Frank SinatraBuilding.
The same combination but with a lastname that is not a listed organization wordresults in a low positive score, as for JusticeJohnson or Frank Sinatra.Names with low or zero scores are first tested aspossible variants of names with high positivescores.
However, if they are incompatible withany, they are assigned a weak entity type.
Thusin the absence of any other evidence in thedocument, Beverly Hills or Susan Hills will beclassified as PR?
(PR?
is preferred to PL?
as ittends to be the correct choice most of the time.
)2 Current System for Cross-DocumentCoreferenceThe choice of a canonical string as the identifierfor equivalence groups within each document isvery important for later merging acrossdocuments.
The document-based canonicalstring should be explicit enough to distinguishbetween different named entities, yet normalizedenough to aggregate all mentions of the sameentity across documents.
Canonical strings ofhuman names are comprised of the followingparts, if found: first name, middle name, lastname, and suffix (e.g., Jr.).
Professional orpersonal titles and nicknames are not included asthese are less permanent features of people'snames and may vary across documents.
Identicalcanonical strings with the same entity type (e.g.,PR) are merged across documents.
For example,in the \[NIST93\] collection, Alan Greenspan hasthe following variants across documents --Federal Reserve Chairman Alan Greenspan, Mr.Greenspan, Greenspan, Federal Reserve BoardChairman Alan Greenspan, Fed Chairman AlanGreenspan -- but a single canonical string --Alan Greenspan.The current aggregation also merges near-identical canonical strings: it normalizes overhyphens, slashes and spaces to merge canonicalnames such as Allied-Signal and Allied Signal,PC-TV and PC/TV.
It normalizes over "empty"words (People's Liberation Army and PeopleLiberation Army; Leadership Conference onCivil Rights and Leadership Conference of CivilRights).
Finally, it merges identical stemmedwords of sufficient length (CommunicationsDecency Act and Communication Decency Ac O.Normalization is not allowed for people'snames, to avoid combining names such asSmithberg and Smithburg.11Merging of identical names with different entitytypes is controlled by a table of aggregateabletypes.
For example, PR?
can merge with PL, asin Beverly Hills \[PR?\] and Beverly Hills \[PL\].But ORG and PL cannot merge, so Boston\[ORG\] does not merge with Boston \[PL\].
As afurther precaution, no aggregation occurs if themerge is ambiguous, that is, if a canonical namecould potentially merge with more than oneother canonical name.
For example, PresidentClinton could be merged with Bill Clinton,Chelsea Clinton, or Hillary Rodham Clinton.To prevent erroneous aggregation of differententities, we currently do not aggregate overdifferent canonical strings.
We keep thecanonical place New York (city or state) distinctfrom the canonical New York City and New YorkState.
Similarly, with human names: Jerry O.Williams in one document is separate from JerryWilliams in another; or, more significantly, JerryLewis from one document is distinct from JerryLee Lewis from another.
We are conservativewith company names too, preferring to keep thecanonical name Allegheny International and itsvariants separate from the canonical nameAllegheny Ludlum and its variant, AlleghenyLudlum Corp.
Even with such conservativecriteria, aggregation over documents is quitedrastic.
The name dictionary for 20MB of WSJtext contains 120,257 names before aggregationand 42,033 names after.But conservative aggregation is not always right.We have identified several problems with ourcurrent algorithm that our new algorithmpromises to handle.1) Failure to merge -- often, particularly famouspeople or places, may be referred to by differentcanonical strings in different documents.Consider, for example, some of the canonicalstrings identified for President Clinton in ourNew York Times \[NYT98\] collection of 2330documents:Bil l  Cl inton \[PR\]Mr. Cl inton \[PR\]President Cl inton \[PR\]Wi l l iam Jefferson Cl inton \[PR\]Cl inton \[uncategorized\]Because of our decision not to merge underambiguity (as mentioned above), our final list ofnames includes many names that should havebeen further aggregated.2) Failure to split -- there is insufficient intra-document evidence for splitting "names" that arecombinations of two or more component names,such as ABC, Paramount and Disney, or B.Brown of Dallas County Judicial District Court.Note that splitting is complex: sometimes evenhumans are undecided, for combinations such asBoston Consulting Group in San Francisco.3) False merge -- due to an implementationdecision, tl~e current aggregation does notinvolve a second pass over the intra-documentvocabulary.
This means that canonical namesare aggregated depending on the order in whichdocuments are analyzed, with the result thatcanonical names with different entity types aremerged when they are encountered if the mergeseems unambiguous at the time, even thoughsubsequent ames encountered may invalidate it.3 Splitting NamesWe address the "splitting" problem first.
Theheuristics for splitting names within thedocument \[WRC97\] fail to address two kinds ofcombined names.
First, there is a residue ofnames containing and, such as Hoechst andSchering A.G., in which the and may or may notbe part of the organization ame.
The cross-document algorithm to handle these is similar tothe intra-document one: Iterate over the namestring; break it into component strings atcommas and and; verify that each componentcorresponds to an independently existingcanonical string.
If all do, split the name.
Thedifference is that at the collection level, there aremore canonical strings available for thisverification.
If the name is split, we repair thecross document statistics by folding theoccurrence statistics of the combined form withthose of each of the parts.
On the collectionlevel, we split strings like AT&T Wireless andPrimeeo Personal Communications andMicrosoft Network and AT&T Worldnet, forwhich there was not enough evidence within thedocument.12More complex is the case of organization amesof the form X of  Y or X in Y, where Yis a place,such as Fox News Channel in New York City orPrudential Securities in Shanghai.
The intra-document heuristic that splits names if theircomponents occur on their own within thedocument is not appropriate here: the short formmay be licensed in the document only becausethe full form serves as its antecedent.
We needevidence that the short form occurs by itself inother contexts.
First, we sort these names andverify that there are no ambiguities.
Forexample, it may appear that Union Bank ofSwitzerland in San Francisco is a candidate forsplitting, since Union Bank of Switzerlandoccurs as a canonical name, but the existence ofUnion Bank of Switzerland in New York signalsan ambiguity -- there are several distinct entitieswhose name starts with Union Bank ofSwitzerland and so no splitting applies.
Similarambiguity is found with Federal District Courtin New York, Federal District Court inPhiladelphia, etc.
24 Merging NamesAs discussed in \[BB98\], a promising approachto determining whether names corefer is thecomparison of their contexts.
However, sincethe cost of context comparison for all similarcanonical strings would be prohibitivelyexpensive, we have devised means of definingcompatible names that are good candidates forcoreference, based on knowledge obtainedduring intra-document processing.
Ouralgorithm sorts names with common substringsfrom least to most ambiguous.
For example, PRnames are sorted by identical ast names.
Theleast ambiguous ones also contain a first nameand middle name, followed by ones containing afirst name and middle initial, followed by onescontaining only a first name, a first initial andfinally the ones with just a last name.
PR namesmay also carry gender information, determinedeither on the basis of the first name (e.g.
Bill butnot Jamie) or a gender prefix (e.g.
Mr., but not2 Note that this definition of ambiguity is dependenton names found in the collection.
For example, in the\[NYT98\] collection, the only Prudential Securitiesin/of.., found was Prudential Securities in Shanghai.President) of the canonical form or one of itsvariants.
PL names are sorted by common initialstrings.
The least ambiguous have the pattern of<small place, big place>.
By comparing theinternal structure of these sorted groups, we areable to divide them into mutually exclusive sets(ES), whose incompatible features prevent anymerging; and a residue of mergeable names(MN), which are compatible with some or all ofthe exclusive ones.
For some of the mergeablenames, we are able to stipulate coreference withthe exclusive names without any further tests.For others, we need to compare contexts beforereaching a conclusion.To illustrate with an example, we collected thefollowing sorted group for last name Clinton3:Wil l iam Jefferson Clinton\[PR\] (ES i)Hi l lary Rodham Clinton\[PR\] (ES 2)Larry Cl inton \[PR?\] (ES 3)George Cl inton \[PR?\] (ES 4)Chelsea Cl inton \[PR\] (ES 5)The following MNs can be merged with these,based on compatibility, as indicated:Bill Cl inton \[PR\] MN w/ I, n icknameBill Cl inton \[PR?\] MN w/ i, n icknameHi l lary Cl inton \[PR\] MN w/ 2, f irst namePresident Cl inton \[PR\] (m)MN w/ 1,3,4, genderPresident Cl inton \[PR\] (f)MN w/ 2,5, genderPresident Cl inton \[PR\] MN w/ all ESsMrs.
Cl inton \[PR\] MN w/ 2,5, genderMr.
Cl inton \[PR\] MN w/ 1,3,4, genderCl intons \[uncategorized\] MN w/ all ESsThere is too much ambiguity (or uncertainty) tostipulate coreference among the members of thissorted group.
There is, however, one stipulatedmerge we apply to Bill Clinton \[PR\] and BillClinton \[PR?\].
We have found that when thecanonical string is identical, a weak entity typecan safely combine with a strong one.
There aremany cases of PR?
to PR merging, some of PL?to ORG, (e.g., Digital City), and a fair numberof PL?
to PR, as in Carla Hills, U.S. and Mrs.3 Intra-document analysis identified PresidentClinton once as referring to a male, since PresidentClinton and Mr. Clinton were merged within thedocument(s); another time as referring to a female,since only President Clinton and Mrs. Clintonappeared in the document(s) in question and weremerged; and a third President Clinton, based ondocuments where there was insufficient evidence forgender.13Carla Hills.
We discuss merging involvingcontext comparison i  the following section.5 Comparing ContextsThe tool used for comparing contexts, theContext Thesaurus (CT), is a Talent tool thattakes arbitrary text as input and returns a rankedlist of terms that are related to the input text withrespect to a given collection of documents.More specifically, the CT is used in anapplication we call Prompted Query Refinement\[CB97\], where it provides a ranked list ofcanonical strings found in the collection that arerelated to users' queries, out of which users mayselect additional terms to add to their queries.~' Pseudoli Document \ [~ ' / \ i  , Documents__I Collection J XXXf I~ '~"  '" I, -~  .
.
.XXX.~L xxx k / /  w','~":~ / I...v....iz777Figure 1 (Context Thesaurus)The CT works with a collection concordance,listing the collection contexts in which aparticular canonical string occurs.
The size ofthe context is parameterized, but the default isusually three sentences -- the sentence where thestring occurs, the preceding and followingsentence within the same paragraph.
We alsocollect occurrence statistics for each canonicalstring.
The use of the concordance to generaterelations among terms was inspired by thephrase finder procedure described in \[JC94\].The CT is an ordinary information retrievaldocument index -- we use IBM's Net-Questionquery system \[IBM99\] -- which indexes pecialdocuments, referred to as "pseudo documents"(Figure 1).
A pseudo document containscollection contexts in which a particularcanonical string occurs.
The title of the pseudodocument is the canonical string itself.
When aquery is issued against the index, the querycontent is matched against the content of thepseudo documents.
The result is a ranked list ofpseudo documents most similar to the query.Recall that the titles of the pseudo documentsare terms, or canonical strings.
What is in factreturned to the user or the application looks likea ranked list of related terms.
If the query itselfis a single term, or a canonical string, the resultis roughly a list of canonical strings in whosecontext he query canonical string occurs mostoften.As an example, the query American foreignpolicy in Europe issued against a CT for the\[NYT98\] collection retums the following hit list:American foreign policy, business interest,American people, long decision-makingprocess, Clinton foreign policy, Americanpolicy, Women in Foreign Service, GeorgetownUniversity School of Foreign Service,alliance official, senior American, foreigncorporation, conventional weapon, CentralAmerican subsidiary, Armed ServicesCommittee, American court, foreign policyadviser, foreign policy table, Serbs inBosniaWe can use a CT to simulate the effect ofcontext comparisons, as suggested by \[BB98\].To determine whether President Clinton in onedocument is the same person as Bill Clinton inanother, we query the CT with each item.
TheNet-Question index returns a ranked hit list ofdocuments (in our case canonical strings) inwhich each item occurs.
The rank of a canonicalstring in the resulting hit list is an interpretationof the strength of association between thequeried item and the hit-list canonical string.The underlying assumption for merging the twocanonical forms is the fact that if they corefer,the contexts in which they each occur shouldcontain similar canonical strings.
Hence, if thetwo hit lists have a sufficient number ofcanonical strings in common (determinedempirically to exceed 50%), we assert that theoriginal items corefer.We have identified four cases for merging thatcan benefit from context comparisons after allsimpler methods have been exhausted.141) One-to-one merging occurs when there areonly two names -- one mergeable and oneexclusive.
This is the case, for example, withtwo ORG canonical strings, where one is asubstring of the other, as in Amazon.com andAmazon.com Books.
Note that we cannot simplystipulate identity here because differentorganizations may share a common prefix, asAlVA Enterprises and ANA Hotel Singapore, orAmerican Health Products and American HealthAssociation.
We invoke queries to the CT usingthese canonical forms and aggregate if there ismore than 50% overlap in the hit lists.The query for Amazon.com returns:Amazon.com, Amazon.com Books, Manesh Shah,growing competition, amazon.com, smallnumber, Jeff Bezos, Kathleen Smith, onlinecommerce, associates program, Yahoo 's Web,Robert Natale, classified advertising, JasonGreen, audiotapes, Internet company, largerinventory, Day One, Society of Mind,Renaissance CapitalThe query for Amazon.corn Books returns:small number, Amazon.com Books, audiotapes,Amazon.com, banned book, Manesh Shah,growing, competition, Jeff Bezos, bookstore,superstores, Kathleen Smith, onlinecommerce, Yahoo 's Web, Robert Natale,classified advertising, Jason Green, largerinventory ,  Internet company, RenaissanceCapital, Day OneSince there is an 80% match, there is more thanample evidence for merging the two names.2) One-to-many merging occurs when there isone mergeable name but several distinctexclusive ones that are compatible with it.
Forexample, Cohen \[PR\] can match either MarcCohen \[PR\] or William Cohen \[PR\].3) A many-to-one merging occurs quitefrequently in the corpora we have experimentedwith.
Several names of type PR, PR?
or evenuncategorized names share the same last nameand have compatible fn-st or middle namesacross documents.
For example:I: Madeleine Korbel Albright \[PR\] -2: Madeleine K. Albright \[PR\] - MN3: Madeleine Albright \[PR\] MNESQuerying the CT results in a 60% matchbetween 1and 2, a 90% match between 2 and 3,and an 80% match between 3 and 1.
Again, thereis sufficient evidence for merging the threenames.4) The most complex case involves a many-to-many match, as illustrated by the Clintonexample mentioned before.Here are the results of the CT context matches4:ESI: Will iam Jefferson Clinton \[PR\]ES2: Hillary Rodham Clinton \[PR\], HillaryClinton \[PR\], Mrs. Clinton\[PR\]ES3: Larry Clinton \[PR?\]ES4: George Clinton\[PR?\]ES5: Chelsea Clinton \[PR\]ES6: Bill Clinton \[PR?\], Bill Clinton \[PR\],President Clinton \[PR\], Mr. Clinton \[PR\],Clintons \[uncategorized\]Notice that Bill Clinton failed to merge withWilliam Jefferson Clinton.
This examplesuggests that failing to merge compatible namesusing the CT, we can use other information.
Forexample, we can check if the mergeablecanonical string is a variant name of the other,or if there is an overlap in the variant names ofthe two canonical strings.
Our variant namescontain titles and professional descriptions, uchas then-Vice President or Professor of Physics,and checking for overlap in these descriptionswill increase our accuracy, as reported in similarwork by Radev and Mckeown \[RM97\].6 Results and Future WorkWe report here on preliminary results only,while we work on the implementation f variousaspects of our new algorithm to be able toconduct a larger scale evaluation.
We plan toevaluate our results with \[BB98\]'s proposedmeasure.
So far, we have experimented withvarious examples from two collections - a smallset of current New York Times articles \[NYT98\]and a larger collection of Wall Street Journalarticles from 1990 \[NIST93\].
Here are somestatistics comparing the two:Collection sizeUnique CFsUnmerged PRUPR?
withNYT WSJ16MB 70MB35,974 89,02410.413 41,2414 Note: for simplification, we combined the threePresident Clinton described above into a singlecanonical name, unmarked for gender.15identical stringsMerged PR/PR?
with 10,186 38,120identical stringsThe distribution of distinct entities (or exclusivenames) and mergeable names variessignificantly from one sorted group to another.On one hand, there are the "famous" entities,such as President Bush (see below).
These tendto have at least one exclusive name with a highnumber of occurrences.
There are quite a fewmergeable names -- a famous entity is assumedto be part of the reader's general knowledge andis therefore not always fully and formallyintroduced -- and a careful context comparison isusually required.
On the other end of the scale,there are the non-famous entities.
There may bea great number of exclusive names, especiallyfor common last names but the frequency ofoccurrences is relatively low.
There are 68members in the sorted group for "Anderson" and7 is the highest number of occurrences.Expensive processing may not be justified forlow-frequency exclusive names.
It seems that wecan establish a tradeoff between processing costversus overall accuracy gain and decide ahead oftime how much disambiguation processing isrequired for a given application.Canonical names with Bush as last name:Neil Bush \[PR\] (m) freq.
: 309 (ES I)Senior Bush \[PR\] freq.
: 14 (ES 2)Prescott Bush \[PR\] (m) freq.
: 13 (ES 3)Lips Bush \[PR\] (m) freq.
: 10 (ES 4)Top Bush \[PR\] (m) freq.
: 9 (ES 5)Frederick Bush \[PR\] (m) freq.
: 7 (ES 6)Jeb Bush \[PR\] (m) freq.
: 5 (ES 7)James Bush \[PR\] (m) freq.
: 4 (ES 8)Keith Bush \[PR?\] (m) freq.
: 2 (ES 9)George W. Bush \[PR?\] (m) freq.
: 2 (ES 10)Charles Bush \[PR?\] (m) freq.
: 1 (ES Ii)Marvin Bush \[PR?\] (m) freq.
: 1 (ES 12)Nicholas Bush \[PR?\] (m) freq.
: 1 (ES 13)Marry Bush \[PR?\] freq.
: 1 (ES 14)George Bush \[PR\] (m) freq.
: 861(MN w/ i0)President Bush \[PR\] (m) freq: 1608(MN w/l-14)then-Vice President Bush \[PR\] (m) freq.
: 12(biN w/ 1-14)Mr. Bush \[PR\] freq.
: 5 (MN w/l-14)Vice President Bush \[PR\] (m) freq.
: 2(MN w/ 1-14)Barbara Bush \[PR?\] (f) freq.
: 29 (ES 15)Mary K. Bush \[PR\] (f) freq.
: 18 (ES 16)Nancy Bush \[PR?\] (f) freq.
: 1 (ES 17)Sharon Bush \[PR?\] (f) freq.
: 1 (ES 18)Mrs. Bush \[PR\] freq.
: 2 (MN w/ 14, 15-18)Bush \[uncategorized\] freq.
: 700 (MN w/ 1-18)Bush \[PR\] freq.
: 5 (MN w/ 1-18)Congress and President Bush \[PR\] freq.
: 5(MN w/ 1-18)U.S. President Bush \[PR\] freq.
:2 (MN w/l-18)Dear President Bush \[PR\] freq.
:l (MN w/l-18)AcknowledgementsVarious members of our group contributed to theTalent tools, in particular Roy Byrd, whodeveloped the current cross-documentaggregation.
Our thanks to Eric Brown forsuggesting to use CT for context comparisons.ReferencesBB98.
A. Bagga and B. Baldwin.
Entity-based cross-document coreferencing using the vector spacemodel.
In Proceedings of COLING-ACL 1998,pages 79-85.CB97.
J. Cooper and R. J. Byrd.
Lexical navigation:Visually prompted query expansion andrefinement.
In DIGLIB 97, Philadelphia, PA, 1997.DAR95.
Tipster Text Program.
Sixth MessageUnderstanding Conference (MUC-6).DAR98.
Tipster Text Program.
Seventh MessageUnderstanding Conference (MUC- 7).IBM99.
http ://www.software.ibm.com/data/iminer/.JC94 Y. Jing and W. B. Croft.
An associationthesaurus for information retrieval.
In RIAO 94,pages 146-160, 1994.NetOw197.
NetOwl Extractor Technical Overview(White Paper).
http://www.isoquest.com/, March1997.NIST93.
Tipster Information-Retrieval Text ResearchCollection', CD-ROM, NIST, Gaithersburg,Maryland.NYT98.
Articles from http://www.nvtimes.com/.RM97.
D. R. Radev and K. R. McKeown.
Building aGeneration Knowledge Source using Internet-Accessible Newswire.
In 5th Conference onApplied Natural Language Processing, pages 221-228, 1997.RW96.
Y. Ravin and N. Wacholder.
ExtractingNames from Natural-Language Text.
IBMResearch Report 20338.
1996.WRC97.
N. Wacholder, Y. Ravin and M. Choi.Disambiguation of proper names in text.
In 5thConference on Applied Natural LanguageProcessing, pages 202-208, 1997.16
