Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 599?604,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproved Correction Detection in Revised ESL SentencesHuichao Xue and Rebecca HwaDepartment of Computer Science,University of Pittsburgh,210 S Bouquet St, Pittsburgh, PA 15260, USA{hux10,hwa}@cs.pitt.eduAbstractThis work explores methods of automat-ically detecting corrections of individualmistakes in sentence revisions for ESLstudents.
We have trained a classifierthat specializes in determining whetherconsecutive basic-edits (word insertions,deletions, substitutions) address the samemistake.
Experimental result shows thatthe proposed system achieves an F1-scoreof 81% on correction detection and 66%for the overall system, out-performing thebaseline by a large margin.1 IntroductionQuality feedback from language tutors canhelp English-as-a-Second-Language (ESL) stu-dents improve their writing skills.
One of the tu-tors?
tasks is to isolate writing mistakes withinsentences, and point out (1) why each case isconsidered a mistake, and (2) how each mistakeshould be corrected.
Because this is time consum-ing, tutors often just rewrite the sentences with-out giving any explanations (Fregeau, 1999).
Dueto the effort involved in comparing revisions withthe original texts, students often fail to learn fromthese revisions (Williams, 2003).Computer aided language learning tools offera solution for providing more detailed feedback.Programs can be developed to compare the stu-dent?s original sentences with the tutor-revisedsentences.
Swanson and Yamangil (2012) haveproposed a promising framework for this purpose.Their approach has two components: one to de-tect individual corrections within a revision, whichthey termed correction detection; another to deter-mine what the correction fixes, which they termederror type selection.
Although they reported ahigh accuracy for the error type selection classifieralone, the bottleneck of their system is the othercomponent ?
correction detection.
An analysis oftheir system shows that approximately 70% of thesystem?s mistakes are caused by mis-detectionsin the first place.
Their correction detection al-gorithm relies on a set of heuristics developedfrom one single data collection (the FCE corpus(Yannakoudakis et al, 2011)).
When determiningwhether a set of basic-edits (word insertions, dele-tions, substitutions) contributes to the same cor-rection, these heuristics lack the flexibility to adaptto a specific context.
Furthermore, it is not clear ifthe heuristics will work as well for tutors trainedto mark up revisions under different guidelines.We propose to improve upon the correction de-tection component by training a classifier that de-termines which edits in a revised sentence addressthe same error in the original sentence.
The classi-fier can make more accurate decisions adjusted tocontexts.
Because the classifier were trained on re-visions where corrections are explicitly marked byEnglish experts, it is also possible to build systemsadjusted to different annotation standards.The contributions of this paper are: (1) We showempirically that a major challenge in correctiondetection is to determine the number of edits thataddress the same error.
(2) We have developed amerging model that reduces mis-detection by 1/3,leading to significant improvement in the accu-racies of combined correction detection and er-ror type selection.
(3) We have conducted experi-ments across multiple corpora, indicating that theproposed merging model is generalizable.2 Correction DetectionComparing a student-written sentence with its re-vision, we observe that each correction can be de-composed into a set of more basic edits such asword insertions, word deletions and word substi-tutions.
In the example shown in Figure 1, thecorrection ?to change ?
changing?
is composedof a deletion of to and a substitution from change599Figure 1: Detecting corrections from revisions.
Our system detects individual corrections by comparing the original sentencewith its revision, so that each correction addresses one error.
Each polygon corresponds to one correction; the labels are codesof the error types.
The codes follow the annotation standard in FCE corpus (Nicholls, 2003).
In this example, W is incorrectWord order; UT is Unnecessary preposiTion; FV is wrong Verb Form; RN is Nnoun needs to be Replaced; ID is IDiom error.Figure 2: A portion of the example from Figure 1 undergoing the two-step correction detection process.
The basic edits areindicated by black polygons.
The corrections are shown in red polygons.
(a) (b)Figure 3: Basic edits extracted by the edit-distance algo-rithm (Levenshtein, 1966) do not necessarily match our lin-guistic intuition.
The ideal basic-edits are shown in Figure3a, but since the algorithm only cares about minimizing thenumber of edits, it may end up extracting basic-edits shownin Figure 3b.to changing; the correction ?moment ?
minute?is itself a single word substitution.
Thus, we canbuild systems to detect corrections which operatesin two steps: (1) detecting the basic edits that tookplace during the revision, and (2) merging thosebasic edits that address the same error.
Figure 2 il-lustrates the process for a fragment of the examplesentence from Figure 1.In practice, however, this two-step approachmay result in mis-detections due to ambiguities.Mis-detections may be introduced from eithersteps.
While detecting basic edits, Figures 3 givesan example of problems that might arise.
Becausethe Levenshtein algorithm only tries to minimizethe number of edits, it does not care whether theedits make any linguistic sense.
For merging basicedits, Swanson and Yamangil applied a distanceheuristic ?
basic-edits that are close to each other(e.g.
basic edits with at most one word lying inbetween) are merged.
Figure 4 shows cases forwhich the heuristic results in the wrong scope.These errors caused their system to mis-detect30% of the corrections.
Since mis-detected cor-rections cannot be analyzed down the pipeline,(a) The basic edits are addressing the same problem.
Butthese basic edits are non-adjacent, and therefore not merged byS&Y?s algorithm.
(b) The basic edits in the above two cases address differentproblems though they are adjacent.
S&Y?s merging algorithmincorrectly merges them.Figure 4: Merging mistakes by the algorithm proposed inSwanson and Yamangil (2012) (S&Y), which merges adja-cent basic edits.the correction detection component became thebottle-neck of their overall system.
Out of the42% corrections that are incorrectly analyzed1,30%/42%?70% are caused by mis-detections inthe first place.
An improvement in correction de-tection may increase the system accuracy overall.We conducted an error analysis to attribute er-rors to either step when the system detects a wrongset of corrections for a sentence.
We examinethe first step?s output.
If the resulting basic ed-its do not match with those that compose the ac-tual corrections, we attribute the error to the firststep.
Otherwise, we attribute the error to the sec-ond step.
Our analysis confirms that the mergingstep is the bottleneck in the current correction de-tection system ?
it accounts for 75% of the mis-detections.
Therefore, to effectively reduce thealgorithm?s mis-detection errors, we propose to1Swanson and Yamangil reported an overall system with58% F-score.600build a classifier to merge with better accuracies.Other previous tasks also involve comparingtwo sentences.
Unlike evaluating grammar er-ror correction systems (Dahlmeier and Ng, 2012),correction detection cannot refer to a gold stan-dard.
Our error analysis above also highlights ourtask?s difference with previous work that identifycorresponding phrases between two sentences, in-cluding phrase extraction (Koehn et al, 2003) andparaphrase extraction (Cohn et al, 2008).
Theyare fundamentally different in that the granularityof the extracted phrase pairs is a major concernin our work ?
we need to guarantee each detectedphrase pair to address exactly one writing prob-lem.
In comparison, phrase extraction systemsaim to improve the end-to-end MT or paraphrasingsystems.
A bigger concern is to guarantee the ex-tracted phrase pairs are indeed translations or para-phrases.
Recent work therefore focuses on identi-fying the alignment/edits between two sentences(Snover et al, 2009; Heilman and Smith, 2010).3 A Classifier for Merging Basic-EditsFigures 4 highlights the problems with indiscrimi-nantly merging basic-edits that are adjacent.
Intu-itively, it seems that the decision should be morecontext dependent.
Certain patterns may indicatethat two adjacent basic-edits are a part of the samecorrection while others may indicate that they eachaddress a different problem.
For example, in Fig-ure 5a, when the insertion of one word is followedby the deletion of the same word, the insertionand deletion are likely addressing one single error.This is because these two edits would combine to-gether as a word-order change.
On the other hand,in Figure 5b, if one edit includes a substitution be-tween words with the same POS?s, then it is likelyfixing a word choice error by itself.
In this case, itshould not be merged with other edits.To predict whether two basic-edits address thesame writing problem more discriminatively, wetrain a Maximum Entropy binary classifier basedon features extracted from relevant contexts forthe basic edits.
We use features in Table 1 in theproposed classifier.
We design the features to in-dicate: (A) whether merging the two basic-editsmatches the pattern for a common correction.
(B)whether one basic-edit addresses one single error.We train the classifier using samples extractedfrom revisions where individual corrections areexplicitly annotated.
We first extract the basic-(a) The pattern indicates thatthe two edits address thesame problem(b) The pattern indicates thatthe two edits do not addressthe same problemFigure 5: Patterns indicating whether two edits address thesame writing mistake.Figure 6: Extracting training instances for the merger.
Ourgoal is to train classifiers to tell if two basic edits shouldbe merged (True or False).
We break each correction (outerpolygons, also colored in red) in the training corpus into a setof basic edits (black polygons).
We construct an instance foreach consecutive pair of basic edits.
If two basic edits wereextracted from the same correction, we will mark the outcomeas True, otherwise we will mark the outcome as False.edits that compose each correction.
We then createa training instance for each pair of two consecutivebasic edits: if two consecutive basic edits need tobe merged, we will mark the outcome as True, oth-erwise it is False.
We illustrate this in Figure 6.4 Experimental SetupWe combine Levenshtein algorithm with differentmerging algorithms for correction detection.4.1 DatasetAn ideal data resource would be a real-world col-lection of student essays and their revisions (Tajiriet al, 2012).
However, existing revision corporado not have the fine-grained annotations necessaryfor our experimental gold standard.
We insteaduse error annotated data, in which the correctionswere provided by human experts.
We simulate therevisions by applying corrections onto the originalsentence.
The teachers?
annotations are treated asgold standard for the detailed corrections.We considered four corpora with different ESLpopulations and annotation standards, includingFCE corpus (Yannakoudakis et al, 2011), NU-CLE corpus (Dahlmeier et al, 2013), UIUC cor-pus2(Rozovskaya and Roth, 2010) and HOO2011corpus (Dale and Kilgarriff, 2011).
These corporaall provide experts?
corrections along with error2UIUC corpus contains annotations of essays collectedfrom ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003).601Type name descriptionAgap-between-edits Gap between the two edits.
In particular, we use the number of words between the two edits?original words, as well as the revised words.
Note that Swanson and Yamangil?s approach is aspecial case that only considers if the basic-edits have zero gap in both sentences.tense-change We detect patterns such as: if the original-revision pair matches the pattern ?V-ing?to V?.word-order-error Whether the basic-edits?
original word set and the revised word set are the same (one or zero).same-word-set If the original sentence and the revised sentence have the same word set, then it?s likely that allthe edits are fixing the word order error.revised-to The phrase comprised of the two revised words.Beditdistance=1 If one basic-edit is a substitution, and the original/revised word only has 1 edit distance, itindicates that the basic-edit is fixing a misspelling error.not-in-dict If the original word does not have a valid dictionary entry, then it indicates a misspelling error.word-choice If the original and the revised words have the same POS, then it is likely fixing a word choiceerror.preposition-error Whether the original and the revised words are both prepositions.Table 1: Features used in our proposed classifier.corpus sentencessentences with?
2 correctionsrevised sentencesFCE 33,900 53.45%NUCLE 61,625 48.74%UIUC 883 61.32%HOO2011 966 42.05%Table 2: Basic statistics of the corpora that we consider.type mark-ups.
The basic statistics of the corporaare shown in Table 2.
In these corpora, around halfof revised sentences contains multiple corrections.We have split each corpus into 11 equal parts.
Onepart is used as the development dataset; the rest areused for 10-fold cross validation.4.2 Evaluation MetricsIn addition to evaluating the merging algorithmson the stand-alone task of correction detection, wehave also plugged in the merging algorithms intoan end-to-end system in which every automati-cally detected correction is further classified intoan error type.
We replicated the error type selectordescribed in Swanson and Yamangil (2012).
Theerror type selector?s accuracies are shown in Table33.
We compare two merging algorithms, com-bined with Levenshtein algorithm:S&Y The merging heuristic proposed by Swan-son and Yamangil, which merges the adjacent ba-sic edits into single corrections.MaxEntMerger We use the Maximum Entropyclassifier to predict whether we should merge thetwo edits, as described in Section 34.We evaluate extrinsically the merging compo-nents?
effect on overall system performance by3Our replication has a slightly lower error type selectionaccuracy on FCE (80.02%) than the figure reported by Swan-son and Yamangil (82.5%).
This small difference on errortype selection does not affect our conclusions about correc-Corpus Error Types AccuracyFCE 73 80.02%NUCLE 27 67.36%UIUC 8 80.23%HOO2011 38 64.88%Table 3: Error type selection accuracies on different cor-pora.
We use a Maximum Entropy classifier along with fea-tures suggested by Swanson and Yamangil for this task.
Thereported figures come from 10-fold cross validations on dif-ferent corpora.comparing the boundaries of system?s detectedcorrections with the gold standard.
We evaluateboth (1) the F-score in detecting corrections (2)the F-score in correctly detecting both the correc-tions?
and the error types they address.5 ExperimentsWe design experiments to answer two questions:1.
Do the additional contextual informationabout correction patterns help guide the mergingdecisions?
How much does a classifier trained forthis task improve the system?s overall accuracy?2.
How well does our method generalize over re-visions from different sources?Our major experimental results are presented inTable 4 and Table 6.
Table 4 compares the over-all educational system?s accuracies with differentmerging algorithms.
Table 6 shows the system?sF1score when trained and tested on different cor-pora.
We make the following observations:First, Table 4 shows that by incorporating cor-rection patterns into the merging algorithm, thetion detection.4We use the implementation at http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.602errors in correction detection step were reduced.This led to a significant improvement on the over-all system?s F1-score on all corpora.
The improve-ment is most noticeable on FCE corpus, wherethe error in correction detection step was reducedby 9%.
That is, one third of the correction mis-detections were eliminated.
Table 5 shows that thenumber of merging errors are significantly reducedby the new merging algorithm.
In particular, thenumber of false positives (system proposes mergeswhen it should not) is significantly reduced.Second, our proposed model is able to gener-alize over different corpora.
As shown in Table6.
The models built on corpora can generally im-prove the correction detection accuracy5.
Mod-els built on the same corpus generally performthe best.
Also, as suggested by the experimentalresult, among the four corpora, FCE corpus is acomparably good resource for training correctiondetection models with our current feature set.
Onereason is that FCE corpus has many more traininginstances, which benefits model training.
We triedvarying the training dataset size, and test it on dif-ferent corpora.
Figure 7 suggests that the model?saccuracies increase with the training corpus size.6 ConclusionsA revision often contains multiple corrections thataddress different writing mistakes.
We explorebuilding computer programs to accurately detectindividual corrections in one single revision.
Onemajor challenge lies in determining whether con-secutive basic-edits address the same mistake.
Wepropose a classifier specialized in this task.
Ourexperiments suggest that: (1) the proposed classi-fier reduces correction mis-detections in previoussystems by 1/3, leading to significant overall sys-tem performance.
(2) our method is generalizableover different data collections.AcknowledgementsThis work is supported by U.S. National Sci-ence Foundation Grant IIS-0745914.
We thankthe anonymous reviewers for their suggestions;we also thank Homa Hashemi, Wencan Luo, FanZhang, Lingjia Deng, Wenting Xiong and YafeiWei for helpful discussions.5We currently do not evaluate the end-to-end system overdifferent corpora.
This is because different corpora employdifferent error type categorization standards.Method Corpus CorrectionDetection F1OverallF1-scoreS&Y FCE 70.40% 57.10%MaxEntMerger FCE 80.96% 66.36%S&Y NUCLE 61.18% 39.32%MaxEntMerger NUCLE 63.88% 41.00%S&Y UIUC 76.57% 65.08%MaxEntMerger UIUC 82.81% 70.55%S&Y HOO2011 68.73% 50.95%MaxEntMerger HOO2011 75.71% 56.14%Table 4: Extrinsic evaluation, where we plugged the twomerging models into an end-to-end feedback detection sys-tem by Swanson and Yamangil.Merging algorithm TP FP FN TNS&Y 33.73% 13.46% 5.71% 47.10%MaxEntMerger 36.04% 3.26% 3.41% 57.30%Table 5: Intrinsic evaluation, where we evaluate the pro-posed merging model?s prediction accuracy on FCE corpus.This table shows a breakdown of true-positives (TP), false-positives (FP), false-negatives (FN) and true-negatives (TN)for the system built on FCE corpus.trainingtestingFCE NUCLE UIUC HOO2011S&Y 70.44 61.18% 76.57% 68.73%FCE 80.96% 61.26% 83.07% 75.43%NUCLE 74.53% 63.88% 78.57% 74.73%UIUC 77.25% 58.21% 82.81% 70.83%HOO2011 71.94% 54.99% 71.19% 75.71%Table 6: Correction detection experiments by building themodel on one corpus, and applying it onto another.
Weevaluate the correction detection performance with F1score.When training and testing on the same corpus, we run a 10-fold cross validation.101 102 103 104 105Number of sentences in the training corpus0.400.450.500.550.600.650.700.750.80F 1 scoreHOO2011UIUCFCENUCLEFigure 7: We illustrate the performance of correction detec-tion systems trained on subsets of FCE corpus.
Each curve inthis figure represents the F1-scores for correction detectionof the model trained on a subset of FCE and tested on differ-ent corpora.
When testing on FCE, we used111of the FCEcorpus, which we kept as development data.603ReferencesTrevor Cohn, Chris Callison-Burch, and Mirella Lap-ata.
2008.
Constructing corpora for the develop-ment and evaluation of paraphrase systems.
Com-putational Linguistics, 34(4):597?614.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages568?572, Montr?eal, Canada, June.
Association forComputational Linguistics.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The NUS corpus of learner english.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th European Workshop on Natural Lan-guage Generation, pages 242?249.
Association forComputational Linguistics.Laureen A Fregeau.
1999.
Preparing ESL studentsfor college writing: Two case studies.
The InternetTESL Journal, 5(10).Sylviane Granger.
2003.
The International Corpus ofLearner English: a new resource for foreign lan-guage learning and teaching and second languageacquisition research.
Tesol Quarterly, 37(3):538?546.Shicun Gui and Huizhong Yang.
2003.
Zhong-guo xuexizhe yingyu yuliaohu.
(chinese learner en-glish corpus).
Shanghai: Shanghai Waiyu JiaoyuChubanshe.Michael Heilman and Noah A Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 1011?1019.Association for Computational Linguistics.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 48?54.Association for Computational Linguistics.V.
I. Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707710.D.
Nicholls.
2003.
The Cambridge Learner Corpus:Error coding and analysis for lexicography and ELT.In Proceedings of the Corpus Linguistics 2003 con-ference, pages 572?581.Alla Rozovskaya and Dan Roth.
2010.
AnnotatingESL errors: Challenges and rewards.
In Proceed-ings of the NAACL HLT 2010 fifth workshop on inno-vative use of NLP for building educational applica-tions, pages 28?36.
Association for ComputationalLinguistics.Matthew G Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
TER-Plus: paraphrase, se-mantic, and alignment enhancements to translationedit rate.
Machine Translation, 23(2-3):117?127.Ben Swanson and Elif Yamangil.
2012.
Correctiondetection and error type selection as an ESL educa-tional aid.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 357?361, Montr?eal, Canada, June.Association for Computational Linguistics.Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-sumoto.
2012.
Tense and aspect error correction forESL learners using global context.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Short Papers-Volume 2,pages 198?202.
Association for Computational Lin-guistics.Jason Gordon Williams.
2003.
Providing feedbackon ESL students written assignments.
The InternetTESL Journal, 4(10).Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 180?189.
Association for ComputationalLinguistics.604
