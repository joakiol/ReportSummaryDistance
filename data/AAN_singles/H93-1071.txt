QUERY PROCESSING FOR RETRIEVAL  FROMLARGE TEXT BASESJohn Broglio and W. Bruce CroftComputer Science DepartmentUniversity of MassachusettsAmherst, MA 01003ABSTRACTNatural language xperiments in information retrieval haveoften been inconclusive due to the lack of large text baseswith associated queries and relevance judgments.
This pa-per describes experiments in incremental query processingand indexing with the INQUERY information retrieval sys-tem on the TIPSTER queries and document collection.
Theresults measure the value of processing tailored for differentquery styles, use of syntactic tags to produce search phrases,recognition and application of generic concepts, and auto-matic concept extraction based on interword associations ina large text  base.1.
INTRODUCTION:  T IPSTER ANDINQUERYPrevious research has suggested that retrieval effective-ness might be enhanced by the use of multiple repre-sentations and by automated language processing tech-niques.
Techniques include automatic or interactive in-troduction of synonyms \[Har88\], forms-based interfaces\[CD90\], automatic recognition of phrases \[CTLgl\], andrelevance feedback \[SB90\].
The recent development ofthe T IPSTER corpus with associated queries and rel-evance judgments has provided new opportunities forjudging the effectiveness of these techniques on large het-erogenous document collections.1.1.
T IPSTER Text Base and QueryTopicsThe T IPSTER documents comprise two volumes of text,of approximately one gigabyte ach, from sources uch asnewspaper and magazine articles and government pub-lications (Federal Register).
Accompanying the collec-tions are two sets of fifty topics.
Each topic is a full textdescription, in a specific format, of an information eed.
(Figure 1).Each T IPSTER topic offers several representations ofthe same information need.
The Topic and Descrip-tion fields are similar to what might be entered as aquery in a traditional information retrieval system.
TheNarrative field expands on the information eed, givingan overview of the classes of documents which would or<top><dom> Domain: International Economics<Tit le> Topic: Satellite Launch Contracts<desc> Description:Document will cite the signing of a contract or prelimi-nary agreement, or the making of a tentative reservation,to launch a commercial satellite.<narr> Narrative:A relevant document will mention the signing of a con-tract or preliminary agreement, or the making of a ten-tative reservation, to launch a commerciM satellite.<con> Concept(s):1. contract, agreement2.
launch vehicle, rocket, payload, satellite3.
launch services, commercial space industry, commer-cial launch industry4.
Arianespace, Martin Marietta, General Dynamics,McDonnell Douglas5.
Titan, Delta II, Atlas, Ariane, Proton</ top>Figure 1: A T IPSTER topic.would not be considered satisfactory, and describes factsthat must be present in relevant documents, for exam-ple, the location of the company.
The Concepts field listswords and phrases which are pertinent o the query.
TheFactors field lists constraints on the geographic and/ortime frame of the query.
All of these fields offer opportu-nities for different kinds of natural anguage processing.1.2.
The INQUERY Information Re-trieval SystemINQUERY is a probabilistic information retrieval sys-tem based upon a Bayesian inference network model\[TC91, Tur91\].
The object network consists of objectnodes (documents) (o/s)  and concept representationnodes (r,~'s).
In a typical network information retrievalsystem, the text representation nodes will correspond to353words extracted from the text \[SM83\], although repre-sentations based on more sophisticated language anal-ysis are possible.
The estimation of the probabilitiesP(rm\[oj) is based on the occurrence frequencies of con-cepts in both individual objects and large collections ofobjects.
In the INQUERY system, representation nodesare the word stems and numbers that occur in the text,after stopwords are discarded.2.
QUERY PROCESSINGEXPERIMENTSOur current set of natural anguage techniques for queryenhancement are:?
deletion of potentially misleading text;?
grouping of proper names and interrelated nounphrase concepts;?
automatic oncept expansion;?
simple rule-based interactive query modification.Future experiments will use more extensive automaticnoun phrase processing and paragraph level retrieval.In addition to the traditional recall/precision table, weshow tables of the precision for the top n documentsretrieved, for 5 values of n. The recall/precision tablemeasures the ability of the system to retrieve all of thedocuments known to be relevant.
The precision for thetop n documents gives a better measure of what a personwould experience in using the system.2 .1 .
De le t ion  processes .Table 1 illustrates an incremental query treatment.
The(Words)  column shows results from the unprocessedwords of the query alone.
(Formatting information, suchas field markers, has been removed.)
The first active pro-cessing (Del l )  removes words and phrases which refer tothe information retrieval processes rather than the infor-mation need, for example, A relevant document will de-scribe .
.
.
.
We further remove words and phrases whichare discursive, like point of view, sort of, discuss, men-lion as well as expressions which would require deep in-ference to process, such as effects of or purpose of (Fig-ure 2).
Some of these expressions would be useful inother retrieval contexts and different lists would be ap-propriate in different domains.
An interactive user isgiven feedback regarding deletions and could have thecapability of selectively preventing deletion.In the experiment in the fourth column (-NARIq.)
theNarrative field has been deleted from each query.
Sincethe Narrative field is usually a very abstract discussion ofthe criteria for document relevance, it is not well-suitedto a system like INQUERY, which relies on matchingwords from the query to words in the document.
Newterms introduced by the Narrative field are rarely usefulas retrieval terms (but note the small loss in precision atthe very lowest level of recall).2 .2 .
Group ing  Noun Phrases  and  Recog-n i z ing  ConceptsThe simplest phrasing or grouping techniques are recog-nition of proper noun groups (Caps in Table 1) andrecognition of multiple spellings for common conceptssuch as United States.Prox imi ty  and phrase  operators  for  nounphrases .
Simple noun phrase processing is done in twoways.
Sequences of proper nouns are recognized asnames and grouped as arguments to a proximity opera-tor.
The proximity operator equires that its argumentsappear in strict order in a document, but allows an in-terword distance of three or less.
Thus a query such asGeorge Bush matches George Herbert Walker Bush in adocument.Secondly, the query is passed through a syntactic part ofspeech tagger \[Chu88\], and rules are used rules to iden-tify noun phrases (Figure 2).
Experiments howed thatvery simple noun phrase rules work better than longer,more complex, noun phrases.
We believe this is becausethe semantic relationships expressed in associated groupsof noun phrases in a query may be expressed in a doc-ument as a compound noun group, a noun phrase withprepositional phrase arguments, a complex sentence, or asequence of sentences linked by anaphora.
This hypoth-esis is supported by the success of the unordered textwindow operator used in the interactive query modifica-tion experiments (Table 4).On the other hand, there are verbal "red herrings" insome query noun phrases due to overpreclse xpression.For example, the phrase U.S. House of Representativeswould be more effective for retrieval without the U.S.component (Congress might be even nicer).2 .3 .
Concept  Recogn i t ionCont ro l led  vocabu lary .
The INQUERY system hasbeen designed so that it is easy to add optional ob-ject types to implement a controlled indexing vocabu-lary \[CCH92\].
For example, when a document refers toa company by name, the document is indexed both bythe the company name (words in the text) and the objecttype (~company).
The standard INQUERY documentparsers recognize the names of companies \[Rau91\], coun-354Table 1: Precision and recMI tables for experiments starting with words-only queries (Words) through phrase (Del l )and word (Del2) deletion to proper noun (Caps) and noun phrase (NP) grouping.
The queries were evaluated onVolume 1 of the TIPSTER document collection, using relevance judgements from the 1992 Text Retrieval andEvaluation Conference (TREC).Recall Words0 71.6 73.5 (+ 2.7)10 49.2 52.7 (+ 7.0)20 41.2 44.2 (+ 7.5)30 35.3 38.9 (+10.4)40 30.7 34.6 (+12.6)50 26.2 30.3 (+15.6)60 22.1 25.5 (+15.5)70 18.7 21.1 (+12.9)80 15.0 17.0 (+13.4)90 9.2 10.5 (+13.7)100 2.4 2.8 (+19.9)avg 29.2 31.9 (+ 9.2)Precision (% change) - 50 queriesDell Del2 -Narr NP76.2 (+ 6.4)54.7 (+11.0)46.1 (+12.1)40.5 (+14.8)35.9 (+17.1)31.7 (+21.1)26.9 (+21.8)22.0 (+17.0)17.8 (+18.4)11.1 (+20.0)3.2 (+33.8)33.3 (+13.9)83.2 (+16.2)59.6 (+2J 1)50.6 (+22.9)45.2 (+28.2)39.9 (+30.0)35.9 (+37.1.
)31.0 (+40.4)26.1 (+40.0)20.5 (+36.6)12.7 (+37.3)2.6 (+10.2)37.0 (+26.7)Caps81.9 (+14.4)60.0 (+21.9)51.3 (+24.6)45.9 (+30.1)40.5 (+32.1)35.6 (+36.0)30.9 (+40.3)25.8 (+38.2)19.9 (+32.8)12.3 (+33.4)2.5 (+ 5.2)37.0 (+26.5)83.5 (+16.6)62.9 (+27.8)54.5 (+32.4)48.8 (+38.5)43.6 (+42.1)37.8 (+44.1)32.6 (+47.9)27.2 (+46.1)21.4 (+42.6)12.9 (+39.8)2.9 (+23.2)38.9 (+33.2)Recall Words5 54.4 57.2 (+ 5.1)15 46,4 49.7 (+ 7.1)30 44.2 47.2 (+ 6.8)100 33.9 37.0 (+ 9.1)200 27.5 30.1 (+ 9.5)Precision (% change) - 50 queriesDell Del2 -Narr NP58.4 (+ 7.4)50.9 (+ 9.7)49.3 (+11.5)38.7 (+14.2)315 (+14.5)66.4 (+22.1)57.1 (+23.1)53.6 (+21.3)43.0 (+26.8)35.4 (+28.7)Caps65.0 (+20.6)57.5 (+23.9)53.3 (+20.6)43.2 (+27.4)35.2 (+28.0)66.8 (+22.8)62.8 (+35.3)56.3 (+27.4)45.0 (+32.7)37.2 (+35.3)tries, and cities in the United States.With wide-ranging queries like the TIPSTER topics, wehave had some success with adding//city (and #foreign-country) concepts to queries that request informationon the location of an event (Table 2).
But the terms//company and #usa have not yet proved consistentlyuseful.
The #corapany concept may be used to good ef-fect to restrict other operators.
For example, looking forthe terms machine, translation, and #company in an n-word text window would give good results with respect ocompanies working on or marketing machine translationproducts.
But, the current implementation f the #com-pany concept recognizer has some shortcomings whichare exposed by this set of queries.
Our next version ofthe recognizer will be more precise and complete x, andwe expect significant improvement from these it.The #usa term tends to have unexpected effects, be-cause a large part of the collection consists of arti-cles from U.S. publications.
In these documents U.S.nationality is often taken for granted (term frequency1 Ra lph Weischedel 's group at BBN have been generous in shar-ing their  company database  for this purpose.of #usa=294408, #foreigneountry=472021), and it islikely that it may be mentioned explicitly only whenthat presupposition is violated, or when both U.S. andnon-U.S, issues are being discussed together in the samedocument.
Therefore, because focussing on the #usaconcept will bring in otherwise irrelevant documents, itis more effective to put negative weight on the #foreign-country concept where the query interest is restrictedto U.S. matters.
For the same reason, in a query fo-cussed only on non-U.S, interests, we would expect theopposite: using #foreigncountry should give better per-formance than #NOT(#usa}.Research continues on the 'right' mix of concept recog-nizers for a document collection.
In situations wheretext and queries are more predictable, such as commer-cial customer support environments, an expanded set ofspecial terms and recognizers i  appropriate.
Names ofproducts and typical operations and objects can be rec-ognized and treated specially both at indexing and atquery time.
Our work in this area reveals a significantimprovement due to domain-specific concept recognizers,however, standardized queries and relevance judgmentsare still being developed.355Original:Document will cite the signing of a contract or prelimi-nary agreement, or the making of a tentative reservation,to launch a commercial satellite.Discourse  phrase  and  word delet ion:the signing of a contract or preliminary agreement, orthe making of a tentative reservation, to launch a com-mercial satellite.Proper  noun group  recogn i t ion  (Concept  field):#3(Martin Marietta) #5(General Dynamics)#3(mcDonnell Douglas) #3(Delta II)Noun phrase  group ing  (and  s topword  delet ion) :#PHRASE (signing contract)#PHRASE (preliminary agreement)#PHRASE (making tentative reservation)#PHRASE (commercial satellite)Figure 2: Progressive changes in the Description field ofthe Topic.Automat ic  concept  expans ion .
We have promisingpreliminary results for experiments in automatic onceptexpansion.
The Expand results in Table 3 were pro-duced by adding five additional concepts to each query.The concepts were selected based on their preponder-ant association with the query terms in text of the 1987Wall Street Journal articles from Volume 1 of the TIP-STER corpus.
The improvement is modest, and we an-ticipate better results from refinements in the selectiontechniques and a larger and more heterogenous sampleof the corpus.2 .4 .
Semi -Automat ic  query  process ing .In the following experiments in interactive query pro-cessing, human intervention was used to modify the out-put of the best automatic query processing.
The personmaking the modifications was permitted to1.
Add words from the Narrative field;2.
Delete words or phrases from the query;3.
Specify a text window size for the occurrence ofwords or phrases in the query.The third restriction simulates a paragraph-based re-trieval.Table 4 summarizes the results of the interactive querymodification techniques compared with the best auto-matic query processing Q-1 (similar to NP  in the otherTable 2: The effect of replacing the query word locationwith the concepts #us-city and #foreigncountrv.
(Wedo not yet have a #foreigncity recognizer).Recall255075avgPrecision (8 queries)NoCity - -  City - -  - City+FC -45.8 46.7 (+2.0) 46.8 (+2.3)30.3 30.4 (+0.2) 30.7 (+1.2)15.0 14.9 (-1.2) 15.2 (+1.4)30.4 30.6 (+0.9) 30.9 (+1.8)tables).
The Q-M query-set was created with rules (1)and (2) only.
The Q-O query-set used all three rules.The improvement over the results from automaticallygenerated queries demonstrates the effectiveness of sim-ple user modifications after automatic query processinghas been performed.
The most dramatic improvementcomes at the top end of the recall scale, which is a highlydesirable behavior in an interactive system.
The resultsalso suggest hat, based on the text window simulation,paragraph-based retrieval can significantly improve ef-fectiveness.3.
CONCLUSIONThe availability of the large T IPSTER text base andquery sets has enabled us to undertake a series of exper-iments in natural anguage processing of documents andqueries for information retrieval.
We have seen steadyimprovements due to lexical and phrase-level processingof natural language queries.
Our experiments with in-teractive modification of the resulting queries indicatehow much potential gain there is in this area, providedwe can refine our phrasing and selection criteria, andprovided actual paragraph retrieval is at least as goodas our text window simulation of it.
Refinement of ourrecognition and use of controlled indexing vocabulary isalready showing benefits in more predictable domains,and we expect to see improvement in the results in theT IPSTER queries as well.The experiments in automatic oncept expansion basedon cooccurrence behavior in large corpora are extremelyinteresting.
Although the effects shown here are verypreliminary, it is reassuring that they are positive evenat this early stage.It is clear that incremental pplication of local (word andphrase-level) natural anguage processing is beneficial ininformation retrieval.
At this stage, the only expectedlimits to this approach are represented by the improve-ment achieved with the experiments in interactive querymodification.356Table 3: Automatic concept expansion (Expand)  com-pared with the automatic query baseline (NP) .Recall0102030405060708090100avgPrecision (50 queries)NP77.155.248.341.536.732.027.922.117.512.52.433.9- Expand -75.2 (-2.4)56.1 (+1.7)49.0 (+1.4)43.0 (+3.4)37.7 (+2.8)32.9 (+3.0)27.9 (+0.3)22.9 (+3.5)18.o (+2.8)12.8 (+2.7)2.7 (+12.1)34.4 (+1.4)Precision (50 queries)Recall(#Docs) NP5 58.415 51.530 48.7100 34.6200 26.3- Expand -58.o (-0.7)53.5 (+3.9)50.1 (+2.9)35.5 (+2.0)26.9 (+2.3)References\[CCH92\] James P. Callas, W.  Bruce Croft, and Stephen M.Harding.
The INQUERY retrieval system.
In Proceed-ings of the Third International Conference on Databaseand Expert Systems Applications, pages 78-83.
Springer-Verlag, 1992.\[Chu88\] Kenneth Church.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Proceedingsof the ?nd Conference on Applied Natural Language Pro-cessing, pages 136-143, 1988.\[CDg0\] W. B. Croft and R. Das.
Experiments with queryacquisition and use in document retrieval systems.
InProceedings of the A CM SIGIR Conference on Researchand Development in Information Retrieval, pages 349-368, 1990.\[CTLgl\] W. B. Croft, H.H.
Turtle, and D.D.
Lewis.
Theuse of phrases and structured queries in information re-trieval.
In Proceedings of the ACM SIGIR Conferenceon Research and Development i  Information Retrieval,pages 32-45, 1991.\[Har88\] D. Harman.
Towards interactive query expansion.In Y. Chiaramella, editor, Proceedings of the 11 th Inter-national Conference on Research and Development inInformation Retrieval, pages 321-332.
ACM, June 1988.\[Rau91\] Lisa F. Rau.
Extracting company names from text.In Proceedings of the Sixth IEEE Conference on Artifi-cial Intelligence Applications, 1991.\[SM83\] Gerard Salton and Michael J. MeGilI.
Introductionto Modern Information Retrieval.
McGraw-Hill, 1983.Table 4: A comparison of two semi-automatic methods ofconstructing adhoc queries.
The methods were evaluatedon Volume 1 of the T IPSTER document collection, usingrelevance judgements from the 1992 Text Retrieval andEvaluation Conference (TREC).Recur0I02030405060708090100avgQ-1 I - -Q -M- -83.9 83.8 ( -0 .2)60.5 64.1 (+6.0)52.7 55.4 (+5.1)46.8 48.6 (+4.3)40.5 42.1 (+3.9)35.0 30.4 (+4.1)30.5 30.9 (+1.5)25.4 25.0 ( -1.4)19.9 18.3 ( -7.8)12.1 11.8 ( -3.0)2.5 2.3 (-6.5)37.2 38.1 (+2.3)Precision (50 queries)-M  - - q -O93.071.663.454.246.840.434.128.421.713.42.442.7(+10.8)(+18.3)(+20.3)(+16.3)(+15.5)(+15.6)(+11.8)(+11.6)(+ 9.1)(+10.3)(- 2.5)(+14.6)Recall(#Does)51530100200Precision (50 queries)q-1 I - -Q 'M- -64.8 67.2 (+3.7)59.2 63.9 (+7.9)54.1 57.5 (+6.3)42.4 45.5 (+7.3)35.6 36.7 (+3.1)I - -  q -o76.4 (+17.9)72.4 (+11.7)64.9 (+20.0)49.4 (+16.5)39.2 (+10.1)\[SBY0\] Gerard Salton and Chris Buekley.
Improving re-trieval performance by relevance feedback.
JASIS,41:288-297, 1990.\[TC91\] Howard Turtle and W. Bruce Croft.
Evaluation of aninference network-based retrieval model.
A CM Transac-tions on Information Systems, 9(3), July 1991.\[Tur9\]\] Howard Robert Turtle.
Inference networks for doc-ument retrieval.
PhD thesis, Department of Computerand Information Science, University of Massachusetts,Amherst, 1991.357
