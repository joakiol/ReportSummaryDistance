Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 53?61,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Reinforcement Learning to Create Communication ChannelManagement Strategies for Diverse UsersRebecca LunsfordCenter for Spoken Lang.
UnderstandingOregon Health & Science UniversityBeaverton, OR, USAlunsforr@ohsu.eduPeter HeemanCenter for Spoken Lang.
UnderstandingOregon Health & Science UniversityBeaverton, OR, USAheemanp@ohsu.eduAbstractSpoken dialogue systems typically do notmanage the communication channel, insteadusing fixed values for such features as theamplitude and speaking rate.
Yet, the qual-ity of a dialogue can be compromised if theuser has difficulty understanding the system.In this proof-of-concept research, we exploreusing reinforcement learning (RL) to createpolicies that manage the communication chan-nel to meet the needs of diverse users.
To-wards this end, we first formalize a prelimi-nary communication channel model, in whichusers provide explicit feedback regarding is-sues with the communication channel, and thesystem implicitly alters its amplitude to ac-commodate the user?s optimal volume.
Sec-ond, we explore whether RL is an appropri-ate tool for creating communication channelmanagement strategies, comparing two differ-ent hand-crafted policies to policies trainedusing both a dialogue-length and a novel an-noyance cost.
The learned policies performedbetter than hand-crafted policies, with thosetrained using the annoyance cost learning anequitable tradeoff between users with differ-ing needs and also learning to balance findinga user?s optimal amplitude against dialogue-length.
These results suggest that RL can beused to create effective communication chan-nel management policies for diverse users.Index Terms: communication channel, spoken di-alogue systems, reinforcement learning, amplitude,diverse users1 IntroductionBoth Spoken Dialog Systems (SDS) and AssistiveTechnology (AT) tend to have a narrow focus, sup-porting only a subset of the population.
SDS typ-ically aim to support the ?average man?, ignoringwide variations in potential users?
ability to hear andunderstand the system.
AT aims to support peo-ple with a recognized disability, but doesn?t sup-port those whose impairment is not severe enoughto warrant the available devices or services, or thosewho are unaware or have not acknowledged that theyneed assistance.
However, SDS should be able tomeet the needs of users whose abilities fall within,and between, the extremes of severly impaired andperfectly abled.When aiming to support users with widely differ-ing abilities, the cause of a user?s difficulty is lessimportant than adapting the communication channelin a manner that aids understanding.
For example,speech that is presented more loudly and slowly canhelp a hearing-impaired elderly person understandthe system, and can also help a person with no hear-ing loss who is driving in a noisy car.
Although oneuser?s difficulty is due to impairment and the otherdue to an adverse environment, a similar adaptationmay be appropriate to both.During human-human communication, speakersmanage the communication channel; implicitly al-tering their manner of speech to increase the likeli-hood of being understood while concurrently econo-mizing effort (Lindblom, 1990).
In addition to theseimplicit actions, speakers also make statements re-ferring to breakdowns in the communication chan-53nel, explicitly pointing out potential problems orcorrections, (e.g.
?Could you please speak up??
)(Jurafsky et al, 1997).As for human-computer dialogue, SDS are proneto misrecognition of users?
spoken utterances.
Muchresearch has focused on developing techniques forovercoming or avoiding system misunderstandings.Yet, as the quality of automatic speech recognitionimproves and SDS are deployed to diverse popula-tions and in varied environments, systems will needto better attend to possible human misunderstand-ings.
Future SDS will need to manage the commu-nication channel, in addition to managing the task,to aid in avoiding these misunderstandings.Researchers have explored the use of reinforce-ment learning (RL) to create dialogue policies thatbalance and optimize measures of task success (e.g.,see (Scheffler and Young, 2002; Levin et al, 2000;Henderson et al, 2008; Walker, 2000)).
Along theselines, RL is potentially well suited to creating poli-cies for the subtask of managing the communica-tion channel, as it can learn to adapt to the userwhile continuing the dialogue.
In doing so, RL maychoose actions that appear costly at the time, but leadto better overall dialogues.Our long term goal is to learn how to manage thecommunication channel along with the task, movingaway from just ?what?
to say and also focusing on?how?
to say it.
For this proof-of-concept, our goalsare twofold: 1) to formalize a communication chan-nel model that encompasses diverse users, initiallyfocusing just on explicit user actions and implicitsystem actions, and 2) to determine whether RL isan appropriate tool for learning an effective commu-nication channel management strategy for diverseusers.
To explore the above issues, we use a simplecommunication channel model in which the systemneeds to determine and maintain an amplitude levelthat is pleasant and effective for users with differ-ing amplitude preferences and needs.
As our goalincludes decreasing the amount of potentially an-noying utterances (i.e., those in which the system?samplitude setting is in discord with the user?s op-timal amplitude), we introduce a user-centric costmetric, which we have termed annoyance cost.
Wethen compare hand-crafted policies against policiestrained using both annoyance and more traditionaldialogue-length cost components.2 Related Work2.1 How People Manage the ChannelWhen conversing, speakers implicitly adjust fea-tures of their speech (e.g., speaking rate, loudness)to maintain the communication channel.
For ex-ample, speakers produce Lombard speech when innoisy conditions, produce clear speech to better ac-commodate a hard of hearing listener, and alter theirspeech to more closely resemble the interlocutor?s(Junqua, 1993; Lindblom, 1990).
These changes in-crease the intelligibility of the speech, thus helpingto maintain the communication channel (Payton etal., 1994).
Research has also shown that speakersadjust their speaking style when addressing a com-puter; exhibiting the same speech adaptations seenduring human-human communication (Bell et al,2003; Lunsford et al, 2006).In addition to altering their speech implicitly,speakers also explicitly point out communicationchannel problems (Jurafsky et al, 1997).
Exam-ples include; requesting a change in speaking rate oramplitude (?Could you please speak up??
), explain-ing sources of communication channel interference(?Oh, that noise is the coffee grinder.?
), or askingtheir interlocutor to repeat an utterance (?What wasthat??).
These explicit utterances identify some is-sue with the communication channel that must beremedied before continuing the dialogue.
In re-sponse, interlocutors will rewind to a previous pointin the dialogue and alter their speech to ensure theyare understood.
This approach, of adapting onesspeech in response to a communication problem, oc-curs even when conversing with a computer (Stent etal., 2008).Both implicit speech alterations and explicit ut-terances regarding the communication channel of-ten address issues of amplitude.
This is to beexpected, as speaking at an appropriate amplitudeis critical to maintaining an effective communica-tion channel, with sub-optimal amplitude affectinglisteners?
understanding and performance (Baldwinand Struckman-Johnson, 2002).
In addition, Bald-win (2001) found that audible, but lowered, ampli-tude can negatively affect both younger and oldersubjects?
reaction time and ability to respond cor-rectly while multitasking, and that elderly listenersare likely to need higher amplitudes than younger54listeners to maintain similar performance.
Just aslow amplitude can be difficult to understand, highamplitude can be annoying, and, in the extreme,cause pain.2.2 How Systems Manage the ChannelTowards improving listener understanding in a po-tentially noisy environment, Martinson and Brock(2007) take advantage of the mobility and sensorycapabilities of a robot.
To determine the best courseof action, the robot maintains a noise map of the en-vironment, measuring the environmental noise priorto each TTS utterance.
The robot then rotates to-ward the listener, changes location, alters its am-plitude, or pauses until the noise abates.
A similartechnique, useful for remote listeners who may bein a noisy environment or using a noisy communica-tion medium, could analyze the signal-to-noise ratioto ascertain the noise level in the listener?s environ-ment.
Although these techniques may be useful foradjusting amplitude to compensate for noise in thelistener?s environment, they do not address speechalterations needed to accommodate users with dif-ferent hearing abilities or preferences.Given the need to adapt to individual users, itseems reasonable that users themselves would sim-ply adjust volume on their local device.
However,there are issues with this approach.
First, man-ual adjustment of the volume would prove problem-atic when the user?s hands and eyes are busy, suchas when driving a car.
Second, during an ongo-ing dialogue speakers tend to minimize pauses, re-sponding quickly when given the turn (Sacks et al,1974).
Stopping to alter the amplitude could re-sult in longer than natural pauses, which systemsoften respond to with increasingly lengthy ?time-out?
responses (Kotelly, 2003), or repeating the sameprompt endlessly (Villing et al, 2008).
Third, al-though we focus on amplitude adaptations in thispaper, amplitude is only one aspect of the commu-nication channel.
A fully functional communicationchannel management solution would also incorpo-rate adaptations of features such as speaking rate,pausing, pitch range, emphasis, etc.
This extendedset of features, because of their number and interac-tion between them, do not readily lend themselvesto listener manipulation.3 Reinforcement LearningRL has been used to create dialogue strategies thatspecify what action to perform in each possiblesystem state so that a minimum dialogue cost isachieved (Walker, 2000; Levin et al, 2000).
To ac-complish this, RL starts with a policy, namely whataction to perform in each state.
It then uses this pol-icy, with some exploration, to estimate the cost ofgetting from each state with each possible action tothe final state.
As more simulations are run, RL re-fines its estimates and its current policy.
RL willconverge to an optimal solution as long as assump-tions about costs and state transitions are met.
RL isparticularly well suited for learning dialogue strate-gies as it will balance opposing goals (e.g., minimiz-ing excessive confirmations vs. ensuring accurateinformation).RL has been applied to a number of dialoguescenarios.
For form-filling dialogues, in which theuser provides parameters for a database query, re-searchers have used RL to decide what order to usewhen prompting for the parameters and to decreaseresource costs such as database access (Levin et al,2000; Scheffler and Young, 2002).
System misun-derstanding caused by speech recognition errors hasalso been modeled to determine whether, and how,the system should confirm information (Schefflerand Young, 2002).
However, there is no known workon using RL to manage the communication channelso as to avoid user misunderstanding.User Simulation: To train a dialogue strategy us-ing RL, some method must be chosen to emulaterealistic user responses to system actions.
Trainingwith actual users is generally considered untenablesince RL can require millions of runs.
As such, re-searchers create simulated users that mimic the re-sponses of real users.
The approach employed tocreate these users varies between researchers; rang-ing from simulations that employ only real user data(Henderson et al, 2008), to those that model userswith probabilistic simulations based on known re-alistic user behaviors (Levin et al, 2000).
Ai etal.
suggest that less realistic user simulations that al-low RL to explore more of the dialogue state spacemay perform as well or better than simulations thatstatistically recreate realistic user behavior (Ai et al,2007).
For this proof-of-concept work, we employ a55hand-crafted user simulation that allows full explo-ration of the state space.Costs: Although it is agreed that RL is a viableapproach to creating optimal dialogue policies, thereremains much debate as to what cost functions resultin the most useful policies.
Typically, these costs in-clude a measure of efficiency (e.g., number of turns)and a measure of solution quality (e.g., the user suc-cessfully completed the transaction) (Scheffler andYoung, 2002; Levin et al, 2000).
For manag-ing the communication channel, it is unclear howthe cost function should be structured.
In this workwe compare two cost components, a more traditionaldialogue-length cost versus a novel annoyance cost,to determine which best supports the creation of use-ful policies.4 Communication Channel ModelBased on the literature reviewed in Section 2.1, wedevised a preliminary model that captures essentialelements of how users manage the communicationchannel.
For now, we only include explicit user ac-tions, in which users directly address issues withthe communication channel, as noted by Jurafskyet al (1997).
In addition, the users modeled areboth consistent and amenable; they provide feed-back every time the system?s utterances are too loudor too soft, and abandon the interaction only whenthe system persists in presenting utterances outsidethe user?s tolerance (either ten utterances that are tooloud or ten that are too soft).For this work, we wish to create policies that treatall users equitably.
That is, we do not want to trainpolices that give preferential treatment to a subset ofusers simply because they are more common.
To ac-complish this, we use a flat rather than normal distri-bution of users within the simulation, with both theoptimal amplitude and the tolerance range randomlygenerated for each user.
To represent users with dif-fering amplitude needs, simulated users are modeledto have an optimal amplitude between 2 and 8, anda tolerance range of 1, 3 or 5.
For example, a usermay have a optimal amplitude of 4, but be able totolerate an amplitude between 2 and 6.When interacting with the computer, the user re-sponds with: (a) the answer to the system?s query ifthe amplitude is within their tolerance range; (b) toosoft (TS) if below their range; or (c) too loud (TL)if the amplitude is above their tolerance range.
Asa simplifying assumption, TS and TL represent anyuser responses that address communication channelissues related to amplitude.
For example, the userresponse ?Pardon me??
would be represented by TSand ?There?s no need to shout!?
by TL.
With thisuser model, the user only responds to the domaintask when the system employs an amplitude settingwithin the user?s tolerance range.For the system, we need to ensure that the sys-tem?s amplitude range can accommodate any user-tolerable amplitude.
For this reason, the system?samplitude can vary between 0 and 10, and is ini-tially set to 5 prior to each dialogue.
In addition toperforming domain actions, the system specifies theamount the amplitude should change: -2, -1, +0, +1,+2.
Each system communication to the user consistsof both a domain action and the system?s amplitudechange.
Thus, the system manages the communica-tion channel using only implicit actions.
If the userresponds with TS or TL, the system will then restatewhat it just said, perhaps altering the amplitude priorto re-addressing the user.5 Hand-crafted PoliciesTo help in determining whether RL is an appropriatetool for learning communication channel manage-ment strategies, we designed two hand-crafted poli-cies for comparison.
The first handcrafted policy,termed no-complaints, finds a tolerable amplitudeas quickly as possible, then holds that amplitude forthe remainder of the dialogue.
As such, this policyonly changes the amplitude in response to explicitcomplaints from the user.
Specifically, the policy in-creases the amplitude by 2 after a TS response, anddrops it by 2 after a TL.
If altering the amplitude by2 would cause the system to return to a setting al-ready identified as too soft or too loud, the systemuses an amplitude change of 1.The second policy, termed find-optimal, searchesfor the user?s optimal amplitude, then maintains thatamplitude for the remainder of the dialogue.
Forthis policy, the system first increases the amplitudeby 1 until the user responds with TL (potentially inresponse to the system?s first utterance), then de-creases the amplitude by 1 until the user either re-56sponds with TS or the optimal amplitude is clearlyidentified based on the previous feedback.
An am-plitude change of 2 is used only when both the op-timal amplitude is obvious and a change of 2 willbring the amplitude setting to the optimal ampli-tude.6 RL and System EncodingTo learn communication channel management poli-cies we use RL with system and user actions spec-ified using Information State Update rules (Hender-son et al, 2008).
Following Heeman (2007), we en-code commonsense preconditions rather than tryingto learn them, and only use a subset of the informa-tion state for RL.Domain Task: We use a domain task that requiresthe user to supply 9 pieces of information, excludinguser feedback relating to the communication chan-nel.
The system has a deterministic way of selectingits actions, thus no learning is needed for the domaintask.State Variables: For RL, each state is representedby two variables; AmpHistory and Progress.
Am-pHistory models the user by tracking all previ-ous user feedback.
In addition, it tracks the cur-rent amplitude setting.
The string contains oneslot for each potential amplitude setting (0 through10), with the current setting contained within ?
[]?.Thus, at the beginning of each interaction, the stringis ?-----[-]-----?, where ?-?
represents noknown data.
Each time the user responds, the stringis updated to reflect which amplitude settings are toosoft (?<?
), too loud (?>?
), or within the user?s toler-ance (?O?).
When the user responds with TL/TS,the system also updates all settings above/below thecurrent setting.
The Progress variable is requiredto satisfy the Markov property needed for RL.
Thisvariable counts the number of successful informa-tion exchanges (i.e., the user did not respond withTS or TL).
As the domain task requires 9 pieces ofinformation, the Progress variable ranged from 1 to9.Costs: Our user model only allows up to 10 utter-ances that are too soft or too loud.
If the cutoff isreached, the domain task has not been completed, soa solution quality cost of 100 is incurred.
Cuttingoff dialogues in this way has the additional benefitof preventing a policy from looping forever duringtesting.
During training, to allow the system to bet-ter model the cost of choosing the same action re-peatedly, we use a longer cutoff of 1000 utterancesrather than 10.In addition to solution quality, two different costcomponents are utilized.
The first, a dialogue-lengthcost (DC), assigns a cost of 1 for each user utterance.The second, an annoyance cost (AC), assigns a costcalculated as the difference between the system?samplitude setting and the user?s optimal amplitude.This difference is multiplied by 3 when the sys-tem?s amplitude setting is below the user?s optimal.This multiplier was chosen based on research thatdemonstrated increased response times and errorsduring cognitively challenging tasks when speechwas presented below, rather than above, typical con-versational levels (Baldwin and Struckman-Johnson,2002).
Thus, only utterances at the optimal ampli-tude have no cost.7 ResultsWith the above system and user models, we trainedpolicies using the two cost functions discussedabove, eight with the DC component and eight us-ing the AC component.
All used Q-Learning andthe ?-greedy method to explore the state space with?
set at 20% (Sutton and Barto, 1998).
Dialogue runswere grouped into epochs of 100; after each epoch,the current dialogue policy was updated.
We trainedeach policy for 60,000 epochs.
After certain epochs,we tested the policy on 5000 user tasks.For our simple domain, the solution quality costremained 0 after about the 100th epoch, as all poli-cies learned to avoid user abandonment.
Because ofthis, only the dialogue-length cost(DC) and annoy-ance cost(AC) components are reflected in the fol-lowing analyses.7.1 DC-Trained PoliciesBy 40,000 epochs, all eight DC policies convergedto one common optimal policy.
Dialogues resultingfrom the DC policies average 9.76 user utteranceslong.
DC policies start each dialogue using the de-fault amplitude setting of 5.
After receiving the ini-tial user response, they aggressively explore the am-plitude range.
If the initial user response is TL (or57DC ACAmpHistory System Amp User AmpHistory System Amp User-----[-]----- Query1 +0 5 TS -----[-]----- Query1 +1 6 TS<<<<<[<]----- Query1 +2 7 Answer <<<<<<[<]---- Query1 +1 7 Answer<<<<<<-[0]--- Query2 +0 7 Answer <<<<<<<[0]--- Query2 +1 8 Answer<<<<<<-[0]--- Query3 +0 7 Answer <<<<<<<0[0]-- Query3 +1 9 Answer<<<<<<-[0]--- Query4 +0 7 Answer <<<<<<00[0]- Query4 +1 10 TL<<<<<<-[0]--- Query5 +0 7 Answer <<<<<<<000[>] Query4 -2 8 Answer<<<<<<-[0]--- Query6 +0 7 Answer <<<<<<<0[0]0> Query5 +0 8 Answer.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.dialogue length cost = 10 annoyance cost = 12Table 1: Comparison of DC (left) and AC (right) interactions with a user who has an optimal amplitude of 8 and atolerance range of 3.
The policies continue as shown, without changing the amplitude level, until all 9 queries areanswered.TS), they continue by decreasing (or increasing) theamplitude by -2 (or +2) until they find a tolerablevolume, in which case they stop.
Table 1 illustratesthe above noted aspects of the policy.
Additionally,if the policy receives user feedback that is contraryto the last feedback (i.e., TS after TL, or TL afterTS), the policy backtracks one amplitude setting.
Inaddition, if the current amplitude is near the bound-ary (3 or 7), the policy will change the volume by-1 or +1 as changing it by -2 or +2 would cause itto move outside users?
amplitude range of 2-8.
Inessence, the DC policies are quite straightforward;aggressively changing the amplitude if the user com-plains, and assuming the amplitude is correct if theuser does not complain.7.2 AC-Trained PoliciesBy 55,000 epochs, AC policies converged to one oftwo optimal solutions, with an average annoyancecost of 7.49.
As illustrated in Table 1, the behav-ior of the AC policies is substantially more complexthan the DC policies.
First, the AC policies startby increasing the amplitude, delivering the first ut-terance at a setting of 6 or 7.
Second, the policiesdo not stop exploring after they find a tolerable set-ting, instead attempting to bracket the user?s toler-ance range, thus identifying the user?s optimal am-plitude.
Third, AC policies sometimes avoid lower-ing the amplitude, even when doing so would con-cretely identify the user?s optimal amplitude.
By do-ing so, the policies potentially incur a cost of 1 forall following turns, but avoid incurring a one timecost of 3 or 6.
In essence, the AC policies attempt tofind the user?s optimal amplitude but may stop shortas they approach the end of the dialogue, favoring aslightly too high amplitude over one that might betoo low.7.3 Comparing AC- and DC- Trained PoliciesThe costs for the AC and DC trained policy sets can-not be directly compared as each set used a differentcost function.
However, we can compare them usingeach others?
cost function.First, we compare the two sets of policies in termsof average dialogue-length.
For example, in Table 1,following a DC policy results in a dialogue-lengthof 10.
However, for the same user, following the ACpolicy results in a dialogue-length of 11, one utter-ance longer due to the TL response to Query4.The average dialogue-length of the DC and ACpolicies, averaged across users, is shown in the right-most two columns of Figure 1.
As expected, the DCpolicies perform better in terms of dialogue-length,averaging 9.76 utterances long.
However, the ACpolicies average 10.32 utterances long, only 0.52 ut-terances longer.
This similarity in length is to be ex-pected, as system communication outside the user?stolerance range impedes progress and is costly usingeither cost component.We also compared the AC and DC policies?
aver-age dialogue-length for users with the same optimalamplitude (i.e., each column shows the average costacross users with tolerance ranges of 1, 3 and 5), asshown in Figure 1.
From this figure it is clear thatthere is little difference in dialogue-length betweenAC and DC policies for users with the same optimal58amplitude.
In addition, for both policies, the lengthsare similar between users with differing optimal am-plitudes.024681012142 3 4 5 6 7 8 AveAverageDialogueLengthUser?s Optimal AmplitudeAC PoliciesDC PoliciesFigure 1: Comparison of the dialogue-length between ACand DC policies for users with differing optimal ampli-tudes.Second, we compare the two sets of polices interms of annoyance costs.
For example, in Table 1,following the AC policy results in an annoyance costof 12.
For the same user, following the DC policy re-sults in an annoyance cost of 36; 9 for Query1 as it isthree below the user?s optimal amplitude, and 3 foreach of the following nine utterances as they are allone below optimal.As shown in the rightmost columns of Figure 2,DC policies average annoyance cost was 13.35, asubstantial 78% increase over the average cost of7.49 for AC policies.
Figure 2 also illustrates thatthe AC and DC policies perform quite differently forusers with differing optimal amplitudes.
For exam-ple, users of the DC policies whose optimal is at (5),or slightly below (4), the system?s default setting (5)average lower annoyance costs than those using theAC policies.
However, these lowered costs for usersin the mid-range is gained at the expense of userswhose optimal amplitude is farther afield, especiallythose users requiring higher amplitude settings.
Thissubstantial difference between users with differentoptimal amplitudes is because, for DC policies, theinteraction is often conducted at the very edge of theusers?
tolerance.
In contrast, the AC policies riskmore intolerable utterances, but use this informationto decrease overall costs by better meeting users?amplitude needs.
As such, users of the AC policiescan expect the majority of the task to be conductedat, or only one setting above, their optimal ampli-tude.051015202530352 3 4 5 6 7 8 AveAverageAnnoyanceCostUser?s Optimal AmplitudeAC PoliciesDC PoliciesFigure 2: Comparison of the annoyance cost between ACand DC policies for users with differing optimal ampli-tudes.7.4 Comparing Hand-crafted and LearnedPoliciesEach of the two hand-crafted policies were run witheach user simulation (i.e., optimal amplitude from2-8 and tolerance ranges of 1, 3, or 5).
In addition,we varied the domain task size, requiring between 4and 10 pieces of information.
DC and AC policieswere also trained for these domain task sizes.As shown in Figure 3, The no-complain policy?sannoyance costs ranged from 7.81 for dialogues re-quiring four pieces of information to 14.67 for thoserequiring ten pieces.
The cost increases linearly withthe amount of information required, because the no-complain policy maintains the first amplitude settingfound that does not result in a user response of TSor TL.
This ensures the amplitude setting is toler-able to the user, but may not be the user?s optimalamplitude.In contrast, the find-optimal policy?s annoyancecosts initially increase from 9.67 for four pieces ofinformation to 12.24 for seven through ten pieces.The cost does not continue to increase when theamount of information required is greater than sevenbecause, for dialogues long enough to allow the sys-tem to concretely identify the user?s optimal ampli-tude, the cost is zero for all subsequent utterances.Figure 3 also includes the mean annoyance costfor the DC and AC policies.
Although one mightexpect the DC trained policies to resemble theno-complain policy, the learned policy performsslightly better.
This difference is because the DCpolicies learn the range of users?
optimal amplitudesettings (2-8), and do not move the amplitude below2 or above 8.
In contrast, the no-complain policies59Figure 3: Average user annoyance costs for hand-crafted,DC and AC policies across dialogues requiring differingamounts of information.behave consistently regardless of the current setting,and thus will incur costs for exploring settings out-side the range of users?
optimal amplitudes.
Simi-larly, AC policies could be anticipated to closely re-semble the find-optimal policy.
However, the ACpolicies average cost is lower than the costs for ei-ther hand-crafted policy, regardless of the amountof information required.
This difference is, in part,due to differences in behavior at the ends of theusers?
optimal amplitude range, like the DC poli-cies.
However, additional factors include the ACpolicies?
more varied use of amplitude changes andtheir balancing of the remaining duration of the di-alogue against the cost to perform additional explo-ration, as discussed in subsection 7.2.8 Discussion and Future WorkThe first objective of this work was to create a modelof the communication channel that takes into ac-count the abilities and preferences of diverse users.In this model, each user has an optimal amplitude,but will answer a system query delivered within arange around that amplitude, although they find non-preferred, especially too soft, amplitudes annoying.When outside the user?s tolerance, the user pro-vides explicit feedback regarding the communica-tion channel breakdown.
For the system, the modelspecifies a composite system action, pairing a do-main action with a possible communication chan-nel management action to change the amplitude.
Bymodeling explicit user actions, and implicit systemactions, this model captures some essential elementsof how people manage the communication channel.The second objective was to determine whetherRL is appropriate for learning communication chan-nel management.
As expected, the learned policiesfound and maintained a tolerable amplitude settingand eliminated user abandonment.
We also com-pared the learned policies with handcrafted solu-tions, and found that the learned policies performedbetter.
This is primarily due to RL?s ability to auto-matically balance the opposing goals of finding theuser?s optimal amplitude and minimizing dialogue-length.An added benefit of RL is that it optimizes the sys-tem?s behavior for the users on which it is trained.In this work, we purposely used a flat distribution ofusers, which caused RL to find a policy (especiallywhen using annoyance costs) that does not penal-ize the outliers, which are usually those with specialneeds.
In fact, we could modify the user distribution,or the simulated users?
behavior, and RL would op-timize the system?s behavior automatically.In this work, we contrasted dialogue length (DC)against annoyance cost (AC) components.
We foundthat the AC and DC policies share the objective offinding an amplitude setting within the user?s tol-erance range because both incur stepwise costs forintolerable utterances.
But, AC policies further re-fine this objective by incurring costs for tolerable,but non-optimal, amplitudes as well.
AC policiesare using information that is not explicitly commu-nicated to the system, but which none-the-less RLcan use while learning a policy.As this was exploratory work, the user model doesnot yet fully reflect expected user behavior.
For ex-ample, as the system?s amplitude decreases, usersmay misunderstand the system?s query or fail to re-spond at all.
In future work we will use an enhanceduser model that includes more natural user behavior.In addition, because we wanted the system to focuson learning a communication channel managementstrategy, the domain task was fixed.
In future work,we will use RL to learn policies that both accom-plish a more complex domain task, and model con-nections between domain tasks and communicationchannel management.
Ultimately, we need to con-duct user-testing to measure the efficacy of the com-munication channel management policies.
We feelconfident that learned policies trained using a com-munication channel model which reflects the rangeof users?
abilities and preferences will prove effec-tive for supporting all users.60ReferencesHua Ai, Joel R. Tetreault, and Diane J. Litman.
2007.Comparing user simulation models for dialog strategylearning.
In NAACL-HLT, April.Carryl L. Baldwin and David Struckman-Johnson.
2002.Impact of speech presentation level on cognitive taskperformance: implications for auditory display design.Ergonomics, 45(1):62?74.Carryl L. Baldwin.
2001.
Impact of age-related hearingimpairment on cognitive task performance: evidencefor improving existing methodologies.
In Human Fac-tors and Ergonomics Society Annual Meeting; Aging,pages 245?249.Linda Bell, Joakim Gustafson, and Mattias Heldner.2003.
Prosodic adaptation in humancomputer inter-action.
In Proceedings of ICPhS 03, volume 1, pages833?836.Peter Heeman.
2007.
Combining reinforcement learningwith information-state update rules.
In Proceedingsof the Conference of the North American Chapter ofthe Association for Computational Linguistics, pages268?275, Rochester, NY, April.James Henderson, Oliver Lemon, and Kallirroi Georgila.2008.
Hybrid reinforcement/supervised learning ofdialogue policies from fixed data sets.
Comput.
Lin-guist., 34(4):487?511.J.
C. Junqua.
1993.
The lombard reflex and its roleon human listeners and automatic speech recogniz-ers.
The Journal of the Acoustical Society of America,93(1):510?524, January.Dan Jurafsky, Liz Shriberg, and Debra Biasca.
1997.Switchboard: SWBD-DAMSL Coders Manual.Blade Kotelly.
2003.
The Art and Business of SpeechRecognition.
Addison-Wesley, January.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A stochas-tic model of human-machine interaction for learningdialog strategies.
IEEE Transactions on Speech andAudio Processing, 8(1):11?23.Bjorn Lindblom, 1990.
Explaining phonetic variation: Asketch of the H & H theory, pages 403?439.
KluwerAcademic Publishers.Rebecca Lunsford, Sharon Oviatt, and Alexander M.Arthur.
2006.
Toward open-microphone engagementfor multiparty interactions.
In Proceedings of the 8thInternational Conference on Multimodal Interfaces,pages 273?280, New York, NY, USA.
ACM.Eric Martinson and Derek Brock.
2007.
Improv-ing human-robot interaction through adaptation to theauditory scene.
In HRI ?07: Proceedings of theACM/IEEE international conference on Human-robotinteraction, pages 113?120, New York, NY, USA.ACM.K.
L. Payton, R. M. Uchanski, and L. D. Braida.
1994.Intelligibility of conversational and clear speech innoise and reverberation for listeners with normal andimpaired hearing.
The Journal of the Acoustical Soci-ety of America, 95(3):1581?1592, March.Harvey Sacks, Emanuel A. Schlegoff, and Gail Jefferson.1974.
A simplest sytsematic for the organization ofturn-taking for conversation.
Language, 50(4):696?735, December.K.
Scheffler and S. J.
Young.
2002.
Automatic learningof dialogue strategy using dialogue simulation and re-inforcement learning.
In Proceedings of Human Lan-guage Technology, pages 12?18, San Diego CA.A.
Stent, M. Huffman, and S. Brennan.
2008.
Adapt-ing speaking after evidence of misrecognition: Localand global hyperarticulation.
Speech Communication,50(3):163?178, March.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning: An Introduction.Jessica Villing, Cecilia Holtelius, Staffan Larsson, An-ders Lindstro?m, Alexander Seward, and Nina Aaberg.2008.
Interruption, resumption and domain switchingin in-vehicle dialogue.
In GoTAL ?08: Proceedings ofthe 6th international conference on Advances in Natu-ral Language Processing, pages 488?499, Berlin, Hei-delberg.
Springer-Verlag.Marilyn A. Walker.
2000.
An application of reinforce-ment learning to dialogue strategy selection in a spo-ken dialogue system for email.
Journal of AritificialIntelligence Research, 12:387?416.61
