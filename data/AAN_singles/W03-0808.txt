InfoXtract: A Customizable Intermediate Level InformationExtraction Engine?Rohini K. SrihariCymfony, Inc.State University of New York at Buffalorohini@Cymfony.comWei Li, Cheng Niu and Thomas CornellCymfony Inc.600 Essjay Road, Williamsville, NY 14221, USA{wei, cniu, cornell}@Cymfony.comKeywords: Information Extraction, Named Entity Tagging, Machine Learning, Domain Porting?
This work was supported in part by SBIR grants F30602-01-C-0035, F30602-03-C-0156, andF30602-02-C-0057 from the Air Force Research Laboratory (AFRL)/IFEA.AbstractInformation extraction (IE) systems assistanalysts to assimilate information fromelectronic documents.
This paper focuses onIE tasks designed to support informationdiscovery applications.
Since informationdiscovery implies examining large volumesof documents drawn from various sources forsituations that cannot be anticipated a priori,they require IE systems to have breadth aswell as depth.
This implies the need for adomain-independent IE system that caneasily be customized for specific domains:end users must be given tools to customizethe system on their own.
It also implies theneed for defining new intermediate level IEtasks that are richer than thesubject-verb-object (SVO) triples producedby shallow systems, yet not as complex as thedomain-specific scenarios defined by theMessage Understanding Conference (MUC).This paper describes a robust, scalable IEengine designed for such purposes.
Itdescribes new IE tasks such as entity profiles,and concept-based general events whichrepresent realistic goals in terms of what canbe accomplished in the near-term as well asproviding useful, actionable information.These new tasks also facilitate the correlationof output from an IE engine with existingstructured data.
Benchmarking results for thecore engine and applications utilizing theengine are presented.1 IntroductionThis paper focuses on new intermediate levelinformation extraction tasks that are defined andimplemented in an IE engine, named InfoXtract.InfoXtract is a domain independent, but portableinformation extraction engine that has been designedfor information discovery applications.The last decade has seen great advances in the areaof IE.
In the US, MUC [Chinchor & Marsh 1998] hasbeen the driving force for developing this technology.The most successful IE task thus far has beenNamed Entity (NE) tagging.
The state-of-the-artexemplified by systems such as NetOwl [Krupka &Hausman 1998], IdentiFinder [Miller et al1998] andInfoXtract [Srihari et al2000] has reached near humanperformance, with 90% or above F-measure.
On theother hand, the deep level MUC IE task ScenarioTemplate (ST) is designed to extract detailedinformation for predefined event scenarios of interest.It involves filling the slots of complicated templates.
Itis generally felt that this task is too ambitious forcommercial application at present.Information Discovery (ID) is a term which hastraditionally been used to describe efforts in datamining [Han 1999].
The goal is to extract novelpatterns of transactions which may reveal interestingtrends.
The key assumption is that the data is alreadyin a structured form.
ID in this paper is defined withinthe context of unstructured text documents; it is theability to extract, normalize/disambiguate, merge andlink entities, relationships, and events which providessignificant support for ID applications.
Furthermore,there is a need to accumulate information acrossdocuments about entities and events.
Due to rapidlychanging events in the real world, what is of nointerest one day, may be especially interesting thefollowing day.
Thus, information discoveryapplications demand breadth and depth in IEtechnology.A variety of IE engines, reflecting various goals interms of extraction as well as architectures are nowavailable.
Among these, the most widely used are theGATE system from the University of Sheffield[Cunningham et al2003], the IE components fromClearforest (www.clearforest.com), SIFT from BBN[Miller et al1998], REES from SRA [Aone &Ramon-Santacruz 1998] and various tools providedby Inxight (www.inxight.com).
Of these, the GATEsystem most closely resembles InfoXtract in terms ofits goals as well as the architecture and customizationtools.
Cymfony differentiates itself by using a hybridmodel that efficiently combines statistical andgrammar-based approaches, as well as by using aninternal data structure known as a token-list that canrepresent hierarchical linguistic structures and IEresults for multiple modules to work on.The research presented here focuses on a newintermediate level of information extraction whichsupports information discovery.
Specifically, itdefines new IE tasks such as Entity Profile (EP)extraction, which is designed to accumulateinteresting information about an entity acrossdocuments as well as within a discourse.
Furthermore,Concept-based General Event (CGE) is defined as adomain-independent, representation of eventinformation but more feasible than MUC ST.InfoXtract represents a hybrid model for extractingboth shallow and intermediate level IE: it exploitsboth statistical and grammar-based paradigms.
A keyfeature is the ability to rapidly customize the IE enginefor a specific domain and application.
Informationdiscovery applications are required to process anenormous volume of documents, and hence any IEengine must be able to scale up in terms of processingspeed and robustness; the design and architecture ofInfoXtract reflect this need.In the remaining text, Section 2 defines the newintermediate level IE tasks.
Section 3 presentsextensions to InfoXtract to support cross-documentIE.
Section 4 presents the hybrid technology.
Section5 delves into the engineering architecture andimplementation of InfoXtract.
Section 6 discussesdomain porting.
Section 7 presents two applicationswhich have exploited InfoXtract, and finally, Section8 summarizes the research contributions.2 InfoXtract: Defining New IE TasksInfoXtract [Li & Srihari 2003, Srihari et al2000] is adomain-independent and domain-portable, inter-mediate level IE engine.
Figure 1 illustrates theoverall architecture of the engine.A description of the increasingly sophisticated IEoutputs from the InfoXtract engine is given below:?
NE:  Named Entity objects represent key itemssuch as proper names of person, organization,product, location, target, contact informationsuch as address, email, phone number, URL, timeand numerical expressions such as date, year andvarious measurements weight, money,percentage, etc.?
CE:  Correlated Entity objects capture relation-ship mentions between entities such as theaffiliation relationship between a person and hisemployer.
The results will be consolidated intothe information object Entity Profile (EP) basedon co-reference and alias support.?
EP:  Entity Profiles are complex rich informationobjects that collect entity-centric information, inparticular, all the CE relationships that a givenentity is involved in and all the events this entityis involved in.
This is achieved throughdocument-internal fusion and cross-documentfusion of related information based on supportfrom co-reference, including alias association.Work is in progress to enhance the fusion bycorrelating the extracted information withinformation in a user-provided existing database.?
GE:  General Events are verb-centric informationobjects representing ?who did what to whomwhen and where?
at the logical level.Concept-based GE (CGE) further requires thatparticipants of events be filled by EPs instead ofNEs and that other values of the GE slots (theaction, time and location) be disambiguated andnormalized.?
PE:  Predefined Events are domain specific oruser-defined events of a specific event type, suchas Product Launch and Company Acquisition inthe business domain.
They represent a simplifiedversion of MUC ST. InfoXtract provides a toolkitthat allows users to define and write their ownPEs based on automatically generated PE ruletemplates.The InfoXtract engine has been deployed bothinternally to support Cymfony?s Brand Dashboard?product and externally to a third-party integrator forbuilding IE applications in the intelligence domain.Document ProcessorKnowledge ResourcesLexiconResourcesGrammarsProcessManagerTokenlistLegendOutputManagerSourceDocumentNLP/IE Processor(s)TokenizerTokenlistLexicon LookupPOS TaggingNamed EntityDetectionShallowParsingDeep ParsingRelationshipDetectionDocumentpoolNECEEPSVOTimeNormalizationAlias andCoreferenceProfile/EventLinking/MergingAbbreviationsPOS = Part of SpeechNE = Named EntityCE = Correlated EntityEP = Entity ProfileSVO = Subject-Verb-ObjectGE = General EventPE = Predefined EventGrammar ModuleProcedure orStatistical ModelHybridModuleGEStatisticalModelsLocationNormalizationli tiPEInfoXtractRepositoryEventExtractionCase RestorationFigure 1:  InfoXtract Engine Architecture3 Hybrid TechnologyInfoXtract represents a hybrid model for IE since itcombines both grammar formalisms as well asmachine learning.
Achieving the right balance of thesetwo paradigms is a major design objective ofInfoXtract.
The core of the parsing and informationextraction process in InfoXtract is organized verysimply as a pipeline of processing modules.
Allmodules operate on a single in-memory data structure,called a token list.
A token list is essentially asequence of tree structures, overlaid with a graphwhose edges define relations that may be eithergrammatical or informational in nature.
The nodes ofthese trees are called tokens.
InfoXtract?s typicalmode of processing is to skim along the roots of thetrees in the token list, building up structure?strip-wise?.
So even non-terminal nodes behave, inthe typical case, as complex tokens.
Representing amarked up text using trees explicitly, rather thanimplicitly as an interpretation of paired bracketsymbols, has several advantages.
For example, itallows a somewhat richer organization of theinformation contained ?between the brackets,?allowing us to construct direct links from a root nodeto its semantic head, for example.The processing modules that act on token lists canrange from lexical lookup to the application of handwritten grammars to statistical analysis based onmachine learning all the way to arbitrary procedureswritten in C++.
The configuration of the InfoXtractprocessing pipeline is controlled by a configurationfile, which handles pre-loading required resources aswell as ordering the application of modules.
Despitethe variety of implementation strategies available,InfoXtract Natural Language Processing (NLP)modules are restricted in what they can do to the tokenlist to actions of the following three types :1.
Assertion and erasure of token properties(features, normal forms, etc.)2.
Grouping token sequences into higher levelconstituent tokens.3.
Linking token pairs with a relational link.Grammatical analysis of the input text makes use of acombination of phrase structure and relationalapproaches to grammar.
Basically, early modulesbuild up structure to a certain level (includingrelatively simple noun phrases, verb groups andprepositional phrases), after which furthergrammatical structure is represented by assertingrelational links between tokens.
This mix of phrasestructural and relational approaches is very similar tothe approach of Lexical Functional Grammar (LFG)[Kaplan & Bresnan 1982], much scaled down.Our grammars are written in a formalismdeveloped for our own use, and also in a modifiedformalism developed for outside users, based on ourin-house experiences.
In both cases, the formalismmixes regular expressions with boolean expressions.Actions affecting the token list are implemented asside effects of pattern matching.
So although ourprocessing modules are in the technical sense tokenlist transducers, they do not resemble Finite StateTransducers (FSTs) so much as the regular expressionbased pattern-action rules used in Awk or Lex.Grammars can contain (non-recursive) macros, withparameters.This means that some long-distance dependencies,which are very awkward to represent directly in finitestate automata can be represented very compactly inmacro form.
While this has the advantage ofdecreasing grammar sizes, it does increase the size ofthe resulting automata.
Grammars are compiled to aspecial type of finite state automata.
These token listautomata can be thought of as an extension of treewalking automata [M?nnich et al2001, Aho &Ullman 1971, Engelfriet et al1999].
These are linearautomata (as opposed to standard finite state treeautomata [G?cseg &  Steinby 1997], which are morenaturally thought of as parallel) which run over trees.The problem with linear automata on trees is that therecan be a number of ?next?
nodes to move the readhead to: right sister, left sister, parent, first child, etc.So the vocabulary of the automaton is increased toinclude not only symbols that might appear in the text(test instructions) but also symbols that indicate whereto move the read head (directive instructions).
Wehave extended the basic tree walking formalism inseveral directions.
First we extend the power of testinstructions to allow them to check features of thecurrent node and to perform string matching againstthe semantic head of the current node (so that asyntactically complex constituent can be matchedagainst a single word).
Second, we include symbolsfor action instructions, to implement side effects.Finally, we allow movement not only along the rootsequence (string-automaton style) and branches of atree (tree-walking style) but also along the theterminal frontier of the tree and along relational links.These extensions to standard tree walkingautomata extend the power of that formalismtremendously, and could pose problems.
However, thegrammar formalisms that compile into these token listwalking automata are restrictive, in the sense thatthere exist many token list transductions that areimplementable as automata that are notimplementable as grammars.
Also the nature of theshallow parsing task itself is such that we only need todip into the reserves of power that this representationaffords us on relatively rare occasions.
As a result, theautomata that we actually plug into the InfoXtractNLP pipeline generally run very fast.Recently, we have developed an extended finitestate formalism named Expert Lexicon, following thegeneral trend of lexicalist approaches to NLP.
Anexpert lexicon rule consists of both grammaticalcomponents as well as proximity-based keywordmatching.
All Expert Lexicon entries are indexed,similar to the case for the finite state tool in INTEX[Silberztein 2000].
The pattern matching time istherefore reduced dramatically compared to asequential finite state device.Some unique features of this formalism include: (i)the flexibility of inserting any number of ExpertLexicons at any level of the process; (ii) the capabilityof proximity checking within a window size as ruleconstraints in addition to pattern matching using anFST call, so that the rule writer can exploit thecombined advantages of both; and (iii) support for thepropagation of semantic tagging results, toaccommodate principles like one sense per discourse.Expert lexicons are used in customization of lexicons,named entity glossaries, and alias lists, as well asconcept tagging.Both supervised machine learning and unsuper-vised learning are used in InfoXtract.
Supervisedlearning is used in hybrid modules such as NE [Srihariet al2000], NE Normalization [Li et al2002] andCo-reference.
It is also used in the preprocessingmodule for orthographic case restoration of caseinsensitive input [Niu et al2003].
Unsupervisedlearning involves acquisition of lexical knowledgeand rules from a raw corpus.
The former includesword clustering, automatic name glossary acquisitionand thesaurus construction.
The latter involvesbootstrapped learning of NE and CE rules, similar tothe techniques used in [Riloff 1996].
The results ofunsupervised learning can be post-edited and added asadditional resources for InfoXtract processing.Table 1: SVO/CE BenchmarkingSVO CECORRECT 196 48INCORRECT 13 0SPURIOUS 10 2MISSING 31 10PRECISION 89.50% 96.0%RECALL 81.67% 82.8%F-MEASURE 85.41% 88.9%AccuracyInfoXtract has been benchmarked using the MUC-7data sets which are recognized as standards by theresearch community.
Precision and recall figures forthe person and location entity types were above 90%.For organization entity types, precision and recallwere in the high 80?s reflecting the fact thatorganization names tend to be very domain specific.InfoXtract provides the ability to create customizednamed entity glossaries, which will boost theperformance of organization tagging for a givendomain.
No such customization was done in thetesting just described.
The accuracy of shallowparsing is well over 90% reflecting very highperformance part-of-speech tagging and named entitytagging.
Table 1 shows the benchmarks for CErelationships which are the basis for EPs and for theSVO parsing which supports event extraction.4 Engineering ArchitectureThe InfoXtract engine has been developed as amodular, distributed application and is capable ofprocessing up to 20 MB per hour on a singleprocessor.
The system has been tested on very large (>1 million) document collections.
The architecturefacilitates the incorporation of the engine into externalapplications requiring an IE subsystem.
Requests toprocess documents can be submitted through a webinterface, or via FTP.
The results of processing adocument can be returned in XML.
Since varioustools are available to automatically populate databasesbased on XML data models, the results are easilyusable in web-enabled database applications.Configuration files enable the system to be used withdifferent lexical/statistical/grammar resources, as wellas with subsets of the available IE modules.InfoXtract supports two modes of operation, activeand passive.
It can act as an active retriever ofdocuments to process or act as a passive receiver ofdocuments to process.
When in active mode,InfoXtract is capable of retrieving documents viaHTTP, FTP, or local file system.
When in passivemode, InfoXtract is capable of accepting documentsvia HTTP.
Figure 2 illustrates a multiple processorconfiguration of InfoXtract focusing on the typicaldeployment of InfoXtract within an application.Server BServer CServer AProcessor 4Processor 6Processor 2DocumentRetrieverInfoXtractControllerDocumentManagerProcessor 1Processor 3Processor 5Extracted infodatabaseDocumentsExternal ContentProviderJava InfoXtract(JIX)ExternalApplicationFigure 2:  High Level ArchitectureThe architecture facilitates scalability bysupporting multiple, independent Processors.
TheProcessors can be running on a single server (ifmultiple CPUs are available) and on multiple servers.The Document Manager distributes requests toprocess documents to all available Processors.
Eachcomponent is an independent application.
All directinter-module communication is accomplished usingthe Common Object Request Broker Architecture(CORBA).
CORBA provides a robust, programminglanguage independent, and platform neutralmechanism for developing and deploying distributedapplications.
Processors can be added and removedwithout stopping the InfoXTract engine.
All modulesare self-registering and will announce their presenceto other modules once they have completedinitialization.The Document Retriever module is only used inthe active retriever mode.
It is responsible forretrieving documents from a content provider andstoring the documents for use by the InfoXtractController.
The Document Retriever handles allinterfacing with the content provider?s retrievalprocess, including interface protocol (authentication,retrieve requests, etc.
), throughput management, anddocument packaging.
It is tested to be able to retrievedocuments from content providers such as NorthernLight, Factiva, and LexisNexis.
Since the DocumentRetriever and the InfoXtract Controller do notcommunicate directly, it is possible to run theDocument Retriever standalone and process allretrieved documents in a batch mode at a later time.The InfoXtract Controller module is used only inthe active retriever mode.
It is responsible forretrieving documents to be processed, submittingdocuments for processing, storing extractedinformation, and system logging.
The InfoXtractController is a multi-threaded application that iscapable of submitting multiple simultaneous requeststo the Document Manager.
As processing results arereturned, they are stored to a repository or database, anXML file, or both.The Document Manager module is responsible formanaging document submission to availableProcessors.
As Processors are initialized, they registerwith the Document Manager.
The Document Manageruses a round robin scheduling algorithm for sendingdocuments to available Processors.
A document queueis maintained with a size of four documents perProcessor.
The Processor module forms the core of theIE engine.
InfoXtract utilizes a multi-level approachto NLP.
Each level utilizes the results of the previouslevels in order to achieve more sophisticated parsing.The JIX module is a web application that isresponsible for accepting requests for documents to beprocessed.
This module is only used in the passivemode.
The document requests are received via theHTTP Post request.
Processing results are returned inXML format via the HTTP Post response.In Table 2 we present an example of theperformance that can be expected based on theapplication of all modules within the engine.
It shouldbe noted that considerably faster processing perprocessor can be achieved if output is restricted to acertain IE level, such as named entity tagging only.The output in this benchmark includes all major taskssuch as NE, EP, parsing and event extraction as wellas XML generation.This configuration provides throughput ofapproximately 12,000 documents (avg.
10KB) perday.
A smaller average document size will increasethe document throughput.
Increased throughput canbe achieved by dedicating a CPU for each runningProcessor.
Each Processor instance requiresapproximately 500 MB of RAM to run efficiently.Processing speed increases linearly with additionalProcessors/CPUs, and CPU speed.
In the current state,with no speed optimization, using a bank of eightprocessors, it is able to process approximately100,000 documents per day.
Thus, InfoXtract issuitable for high volume deployments.
The use ofCORBA provides seamless inter-process andover-the-wire communication between modules.Computing resources can be dynamically assigned tohandle increases in document volume.Table 2:  Benchmark for EfficiencyServerConfiguration2 CPU @ 1 GHz, 2 GBRAMOperating System Redhat Linux 7.2DocumentCollection Size500 Documents, 5 MBtotal sizeEngineConfigurationInfoXtract Controller,Document Manager,and 2 Processorsrunning on a singleserverProcessing Time 30 MinutesA standard document input model is used todevelop effective preprocessing capabilities.Preprocessing adapts the engine to the source bypresenting metadata, zoning information in astandardized format and performing restoration tasks(e.g.
case restoration).
Efforts are underway toconfigure the engine such that zone-specificprocessing controls are enabled.
For example, zonesidentified as titles or subtitles must be tagged usingdifferent criteria than running text.
The engine hasbeen deployed on a variety of input formats includingHUMINT documents (all uppercase), the ForeignBroadcast Information Services feed (FBIS), livefeeds from content providers such as Factiva (DowJones/Reuters), LexisNexis, as well as web pages.
Auser-trainable, high-performance case restorationmodule [Niu et al2003] has been developed thattransforms case insensitive input such as speechtranscripts into mixed-case before being processed bythe engine.
The case restoration module eliminates theneed for separate IE engines for case-insensitive andcase-sensitive documents; this is easier and more costeffective to maintain.5 Corpus-level IEEfforts have extended IE from the document level tothe corpus level.
Although most IE systems performcorpus-level information consolidation at anapplication level, it is felt that much can be gained bydoing this as an extended step in the IE engine.
Arepository has been developed for InfoXtract that isable to hold the results of processing an entire corpus.A proprietary indexing scheme for indexing token-listdata has been developed that enables querying overboth the linguistic structures as well as statisticalsimilarity queries (e.g., the similarity between twodocuments or two entity profiles).
The repository isused by a fusion module in order to generatecross-document entity profiles as well as for textmining operations.
The results of the repositorymodule can be subsequently fed into a relationaldatabase to support applications.
This has theadvantage of filtering much of the noise from theengine level and doing sophisticated informationconsolidation before populating a relational database.The architecture of these subsequent stages is shownin Figure 3.DatabasesFusionModuleCorpus-level IEInfoXtractTextMiningFBIS, NewswireDocumentsInfoXtractRepository 1InfoXtractRepository 2IDPFigure 3:  Extensions to InfoXtractInformation Extraction has two anchor points: (i)entity-centric information which leads to an EP, and(ii) action-centric information which leads to an eventscenario.
Compared with the consolidation ofextracted events into cross-document event scenario,cross-document EP merging and consolidation is amore tangible task, based mainly on resolving aliases.Even with modest recall, the corpus-level EPdemonstrates tremendous value in collectinginformation about an entity.
This is as shown in Table3 for only part of the profile of ?Mohamed Atta?
fromone experiment based on a collection of news articles.The extracted EP centralizes a significant amount ofvaluable information about this terrorist.6 Domain PortingConsiderable efforts have been made to keep the coreengine as domain independent as possible; domainspecialization or tuning happens with minimumchange to the core engine, assisted by automatic orsemi-automatic domain porting tools we havedeveloped.Cymfony has taken several distinct approaches inachieving domain portability: (i) the use of a standarddocument input model, pre-processors andconfiguration scripts in order to tailor input and outputformats for a given application, (ii) the use of tools inorder to customize lexicons and grammars, and (iii)unsupervised machine learning techniques forlearning new named entities (e.g.
weapons) andrelationships based on sample seeds provided by auser.Table 3:  Sample Entity ProfileName Mohamed AttaAliases Atta; MohamedPosition apparent mastermind;ring leader; engineer; leaderAge 33; 29; 33-year-old;34-year-oldWhere-from United Arab Emirates;Spain; Hamburg; Egyptian;?
?Modifiers on the first plane; evasive;ready; in Spain; in seat 8D?Descriptors hijacker; al-Amir; purportedringleader; a square-jawed33-year-old pilot; ?
?Association bin Laden; AbdulazizAlomari; Hani Hanjour;Madrid; American MediaInc.
; ?
?Involved-events move-events (2);accuse-events (9),convict-events (10),confess-events (2),arrest-events (3),rent-events (3),  .....It has been one of Cymfony?s primary objectivesto facilitate domain portability [Srihari 1998] [Li &Srihari 2000a,b, 2003].
This has resulted in adevelopment/customization environment known asthe Lexicon Grammar Development Environment(LGDE).
The LGDE permits users to modify namedentity glossaries, alias lexicons and general-purposelexicons.
It also supports example-based grammarwriting; users can find events of interest in sampledocuments, process these through InfoXtract andmodify the constraints in the automatically generatedrule templates for event detection.
With some basictraining, users can easily use the LGDE to customizeInfoXtract for their applications.
This facilitatescustomization of the system in user applicationswhere access to the input data to InfoXtract isrestricted.7 ApplicationsThe InfoXtract engine has been used in twoapplications, the Information Discovery Portal (IDP)and Brand Dashboard (www.branddashboard.com).
The IDP supports both the traditional top-downmethods of browsing through large volumes ofinformation as well as novel, data-driven browsing.
Asample user interface is shown in Figure 4.Users may select ?watch lists?
of entities (people,organizations, targets, etc.)
that they are interested inmonitoring.
Users may also customize the sources ofinformation they are interested in processing.Top-down methods include topic-centric browsingwhereby documents are classified by topics ofinterest.
IE-based browsing techniques includeentity-centric and event-centric browsing.Entity-centric browsing permits users to track keyentities (people, organizations, targets) of interest andmonitor information pertaining to them.
Event-centricbrowsing focuses on significant actions includingmoney movement and people movement events.Visualization of extracted information is a keycomponent of the IDP.
The Information Mesh enablesa user to visualize an entity, its attributes and itsrelation to other entities and events.
Starting from anentity (or event), relationship chains can be traversedto explore related items.
Timelines facilitatevisualization of information in the temporal axis.Information Discovery PortalAssociationsWho/what is beingassociated with al-Qaeda ?OrganizationsReligiousPoliticalTerrorist- al-Jihad (34)- HAMAS (16)- Hizballah (5)- ?morePeopleIncidents- Attacks (125)- Bombing (64)- Threats (45)- ?moreLocationsWeaponsGovernmentsOverallCoverageEvents Info.
Sources DocumentsTrack...
Organizations People Targetsal-QaedaOverall Coverage of  al-Qaeda Over Time0 1020 3040 505/7/2001 5/14/2001 5/21/2001 5/28/2001 6/4/2001 6/11/2001 6/18/2001 6/25/2001 7/2/2001 7/9/2001 7/16/2001 7/23/2001 7/30/2001#ReportsAlerts for Week of August 6, 2001(3)  new reports of al-Qaeda terrorist activity(1)  new report of  bin Laden sighting(4)  new quotes by bin Laden(1)  new target identifiedFigure 4:  Information Discovery PortalRecent efforts have included a tight integration ofInfoXtract with visualization tools such as theWeb-based Timeline Analysis System (WebTAS)(http://www.webtas.com).
The IDP reflects the abilityfor users to select events of interest and automaticallyexport them to WebTAS for visualization.
Efforts areunderway to integrate higher-level event scenarioanalysis tools such as the Terrorist Modus OperandiDetection System (TMODS) (www.21technologies.com) into the IDP.Brand Dashboard is a commercial application formarketing and public relations organizations tomeasure and assess media perception of consumerbrands.
The InfoXtract engine is used to analyzeseveral thousand electronic sources of informationprovided by various content aggregators (Factiva,LexisNexis, etc.).
The engine is focused on taggingand generating brand profiles that also capture salientinformation such as the descriptive phrases used indescribing brands (e.g.
cost-saving, non-habitforming) as well as user-configurable specificmessages that companies are trying to promote andtrack (safe and reliable, industry leader, etc.).
Theoutput from the engine is fed into a database-drivenweb application which then produces report cards forbrands containing quantitative metrics pertaining tobrand perception, as well as qualitative informationdescribing characteristics.
A sample screenshot fromBrand Dashboard is presented in Figure 5.
It depicts areport card for a particular brand, highlighting brandstrength as well as highlighting metrics that havechanged the most in the last time period.
The ?buzzbox?
on the right hand side illustratescompanies/brands, people, analysts, and messagesmost frequently associated with the brand in question.Figure 5:  Report Card from Brand Dashboard8 Summary and Future WorkThis paper has described the motivation behindInfoXtract, a domain independent, portable,intermediate-level IE engine.
It has also discussed thearchitecture of the engine, both from an algorithmicperspective and software engineering perspective.Current efforts to improve InfoXtract include thefollowing: support for more diverse input formats,more use of metadata in the extraction tasks, supportfor structured data, and capabilities for processingforeign languages.
Finally, support for more intuitivedomain customization tools, especially thesemi-automatic learning tools is a major focus.AcknowledgmentsThe authors wish to thank Carrie Pine of AFRL forreviewing and supporting this work.References[Aho & Ullman 1971] Alfred V. Aho and JeffreyD.
Ullman.
Translations on a context-free grammar.Information and Control, 19(5):439?475, 1971.
[Aone & Ramos-Santacruz 1998] REES: ALarge-Scale Relation and Event Extraction System.url: http://acl.ldc.upenn.edu/A/A00/A00-1011.pdf[Chinchor & Marsh 1998] Chinchor, N. & Marsh,E.
1998.
MUC-7 Information Extraction TaskDefinition (version 5.1), Proceedings of MUC-7.
[Cunningham et al2003] Hamish Cunningham etal.
Developing Language Processing Componentswith GATE: A User Guide.http://gate.ac.uk/sale/tao/index.html#annie[Engelfriet et al1999] Joost Engelfriet, HendrikJan Hoogeboom, and Jan-Pascal Van Best.
Trips ontrees.
Acta Cybernetica, 14(1):51?64, 1999.
[G?cseg & Steinby 1997] Ferenc G?cseg andMagnus Steinby.
Tree languages.
In GrzegorzRozenberg and Arto Salomaa, editors, Handbook ofFormal Languages: Beyond Words, volume 3, pages1?68, Berlin, 1997.
Springer[Han 1999] Han, J.
Data Mining.
1999.
In J.Urban and P. Dasgupta (eds.
), Encyclopedia ofDistributed Computing, Kluwer Academic Publishers.
[Hobbs 1993] J. R. Hobbs, 1993.
FASTUS: ASystem for Extracting Information from Text,Proceedings of the DARPA workshop on HumanLanguage Technology?, Princeton, NJ, 133-137.
[Kaplan & Bresnan 1982] Ronald M. Kaplan andJoan Bresnan.
Lexical-Functional Grammar: A formalsystem for grammatical representation.
In JoanBresnan, editor, The Mental Representation ofGrammatical Relations, pages 173?281.
The MITPress, Cambridge, MA, 1982.
[Krupka & Hausman 1998] G. R Krupka and K.Hausman, ?IsoQuest Inc: Description of the NetOwlText Extraction System as used for MUC-7?, MUC-7[Li et al2002] Li, H., R. Srihari, C. Niu, and W. Li(2002).
Localization Normalization for InformationExtraction.
COLING 2002, 549?555, Taipei, Taiwan.
[Li, W & R. Srihari 2000a].
A DomainIndependent Event Extraction Toolkit, FinalTechnical Report, Air Force Research Laboratory,Information Directorate, Rome Research Site, NewYork[Li, W & R. Srihari 2000b].
Flexible InformationExtraction Learning Algorithm, Final TechnicalReport, Air Force Research Laboratory, InformationDirectorate, Rome Research Site, New York[Li & Srihari 2003] Li, W. and R. K. Srihari (2003)Intermediate-Level Event Extraction for Temporaland Spatial Analysis and Visualization, FinalTechnical Report AFRL-IF-RS-TR-2002-245, AirForce Research Laboratory, Information Directorate,Rome Research Site, New York.
[Miller et al1998] Miller, Scott; Crystal, Michael;Fox, Heidi; Ramshaw, Lance; Schwartz, Richard;Stone, Rebecca; Weischedel, Ralph; and AnnotationGroup, the 1998.
Algorithms that Learn to ExtractInformation; BBN: Description of the SIFT System asUsed for MUC-7.
[M?nnich et al2001] Uwe M?nnich, FrankMorawietz, and Stephan Kepser.
A regular query forcontext-sensitive relations.
In Steven Bird, PeterBuneman, and Mark Liberman, editors, IRCSWorkshop Linguistic Databases 2001, pages187?195, 2001[Niu et al2003] Niu, C., W. Li, J. Ding, and R.K.Srihari (to appear 2003).
Orthographic CaseRestoration Using Supervised Learning WithoutManual Annotation.
Proceedings of The 16thFLAIRS, St. Augustine, FL[Riloff 1996] [Automatically GeneratingExtraction Patterns from Untagged Text.
AAAI-96.
[Roche & Schabes 1997] Emmanuel Roche &Yves Schabes, 1997.
Finite-State LanguageProcessing, The MIT Press, Cambridge, MA.
[Silberztein 1999] Max Silberztein, (1999).INTEX: a Finite State Transducer toolbox, inTheoretical Computer Science #231:1, ElsevierScience[Srihari 1998].
A Domain Independent EventExtraction Toolkit, AFRL-IF-RS-TR-1998-152 FinalTechnical Report, Air Force Research Laboratory,Information Directorate, Rome Research Site, NewYork[Srihari et al2000] Srihari, R, C. Niu and W.
Li.(2000).
A Hybrid Approach for Named Entity andSub-Type Tagging.
In Proceedings of ANLP 2000,247?254, Seattle, WA.
