Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333?343,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDocument-level Sentiment Inferencewith Social, Faction, and Discourse ContextEunsol Choi Hannah Rashkin Luke Zettlemoyer Yejin ChoiComputer Science & EngineeringUniversity of Washington{eunsol,hrashkin,lsz,yejin}@cs.washington.eduAbstractWe present a new approach for document-level sentiment inference, where the goalis to predict directed opinions (who feelspositively or negatively towards whom) forall entities mentioned in a text.
To encour-age more complete and consistent predic-tions, we introduce an ILP that jointlymodels (1) sentence- and discourse-levelsentiment cues, (2) factual evidence aboutentity factions, and (3) global constraintsbased on social science theories suchas homophily, social balance, and reci-procity.
Together, these cues allow for richinference across groups of entities, includ-ing for example that CEOs and the com-panies they lead are likely to have simi-lar sentiment towards others.
We evalu-ate performance on new, densely labeleddata that provides supervision for all pairs,complementing previous work that onlylabeled pairs mentioned in the same sen-tence.
Experiments demonstrate that theglobal model outperforms sentence-levelbaselines, by providing more coherent pre-dictions across sets of related entities.1 IntroductionDocuments often present a complex web of factsand opinions that hold among the entities they de-scribe.
Consider the international relations storyin Figure 1.
Representatives from three countriesform factions and create a network of sentiment.While some opinions are relatively directly stated(e.g., Russia criticizes Belarus), many others mustbe inferred based on the factual ties among enti-ties (e.g., Moscow, Gryzlov, and Russia probablyshare the same sentiment towards other entities)and known social context (e.g., Russia probablyRussia criticized Belarus for permitting Georgian Presi-dent Mikheil Saakhashvili to appear on Belorussian tele-vision.
?The appearance was an unfriendly step towardsRussia,?
the speaker of Russian parliament Boris Gry-zlov said.
.
.
.
Saakhashvili announced Thursday thathe did not understand Russia?s claims.
Moscow refusedto have any business with Georgia?s president after thearmed conflict in 2008 .
.
.Figure 1: Example text excerpt paired with the document-level sentiment graph we aim to recover.
The graph includesedges with direct textual support (e.g., from Russian to Be-larus given the verb ?criticized?)
as well as ones that mustbe inferred at the whole-document level (e.g., from Gryzlovto Saakhashvili given the web of relationships and opinionsbetween them, Georgia, Russian, and Belarus).dislikes Saakhashvili since Russia criticized Be-larus for supporting him).
In this paper, we showthat jointly reasoning about all of these factors canprovide more complete and consistent document-level sentiment predictions.More concretely, we present a global modelfor document-level entity-to-entity sentiment, i.e.,who feels positively (or negatively) towards whom.Our goal is to make exhaustive predictions overall entity pairs, including those that require cross-sentence inference.
We present a Integer Lin-ear Programming (ILP) model that combines threecomplementary types of evidence: entity-pair sen-timent classification, template-based faction ex-traction, and sentiment dynamics in social groups.Together, they allow for recovering more completepredictions of both the explicitly stated and im-333Figure 2: Entity subgraphs for the example in Figure 1:(a) shows explicitly stated sentiment, (b) shows faction re-lationships and (c) shows all edges for Georgia and its rep-resentative Saakhashvili.
Through Saakhasvili?s relationshipwith Belarus, Georgia forms an alliance with Belarus, provid-ing evidence for an inferred negative stance towards Russia.Green dotted edges represent positive sentiment, red are neg-ative, and blue dashed lines show faction relationship.plicit sentiment, while preserving consistency.The sentiment dynamics in social groups, moti-vated by social science theories, are encoded assoft ILP constraints.
They include a notion ofhomophily, that entities in the same group tendto have similar opinions (Lazarsfeld and Mer-ton, 1954).
For example, Figure 2b shows di-rected faction edges, where one entity is likelyto agree with the other?s opinions.
They alsoencode dyadic social constraints (i.e., the likelyreciprocity of opinions (Gouldner, 1960)) and tri-adic social dynamics following social balance the-ory (Heider, 1946).
For example, from Russia?scriticism on Belarus and Belarus?
positive attitudetowards Saakhashvilli (in Figure 2a), we can in-fer that Russia is negative towards Saakhashvilli(in Figure 2c).
When considered in aggregate,these constraints can greatly improve the consis-tency over the overall document-level predictions.Our work stands in contrast to previous ap-proaches in three aspects.
First, we apply so-cial dynamics motivated by social science theoriesto entity-entity sentiment analysis in unstructuredtext.
In contrast, most previous studies focusedon social media or dialogue data with overt so-cial network structure when integrating social dy-namics (Tan et al, 2011; Hu et al, 2013; West etal., 2014).
Second, we aim to recover sentimentthat can be inferred through partial evidence thatspans multiple sentences.
This complements priorefforts for accessing implied sentiment where thekey evidence is, by and large, at the sentence level(Zhang and Liu, 2011; Yang and Cardie, 2013;Deng and Wiebe, 2015a).
Finally, we present thefirst approach to model the relationship betweenfactual and subjective relations.We evaluate the approach on a newly gatheredcorpus with dense document-level sentiment la-bels in news articles.1This data includes compre-hensively annotated sentiment between all entitypairs, including those that do not appear togetherin any single sentence.
Experiments demon-strate that the global model significantly improvesperformance over a pairwise classifier and otherstrong baselines.
We also perform a detailed ab-lation and error analysis, showing cases where theglobal constraints contribute and pointing towardsimportant areas for future work.2 A Document-level Sentiment ModelGiven a news document d, and named entitiese1,...,enin d, where each entity eihas mentionsmi1?
?
?
mik, the task is to decide directed senti-ment between all pairs of entities.
We predict thedirected sentiment from eito ejat the documentlevel, i.e., sent(ei?ej) ?
{positive, unbiased, neg-ative}, for all ei, ej?
d where i 6= j, assumingthat sentiment is consistent within the document.We introduce a document-level ILP that in-cludes base models and soft social constraints.ILP has been used successfully for a wide range ofNLP tasks (Roth and Yih, 2004), perhaps becausethey easily support incorporating different types ofglobal constraints.
We use two base models: (1)a learned pairwise sentiment classifier (Sec 3.1)that combines sentence- and discourse-level fea-tures to make predictions for each entity pair and(2) a pattern-based faction extractor (Sec 3.2) thatdetects alliances among a subset of the entities.The ILP is solved by maximizing:F =?social+ ?fact+n?i=1n?j=1?ijwhere F combines soft constraints (?social, ?factdefined in detail in this section) with pairwise po-tentials ?ijdefined as:1All data will be made publicly available.
Youcan browse it at http://homes.cs.washington.edu/?eunsol/project_page/acl16, and downloadit from the author?s webpage.334Sentence i jCanadian Prime Minister Harper.
.
.
Canada Harper.
.
.
Reid, the Democratic leader.
.
.
Reid DemocraticGoldman spokesman DuVally Goldman DuVally.
.
.
Djibouti, a key U.S. ally.
Djibouti U.S.(a) Detection examples(b) Visual representation of common infer-ence patterns.Figure 3: An example sentiment inference from faction relationships.
Pairs in factions are encouraged toshare opinions, and to be positive towards other tied entities.
On the right, sentiment edges can be bothpositive or both negative.?ij=?posij?
posij+ ?negij?
negij+ ?neuij?
neuijEach potential ?ijincludes the sentiment clas-sifier scores (?pos, ?neg, ?neu) with binary vari-ables posij, neuijand negijwhere, for exam-ple, negij=1 indicates that eiis negative towardsej.
Decision variables posijand neuijare definedanalogously for positive and neutral opinion.
Fi-nally, we introduce a hard constraint:?i, j posij+ negij+ neuij= 1to ensure a single prediction is made per pair.2.1 Inference with factionsOur first soft ILP constraint ?factmodels that factthat entities in supportive social relations tend toshare similar sentiment toward others (Lazarsfeldand Merton, 1954), and are often positive towardseach other.
For now, we assume access to a baseextractor to provide such faction relations (Sec.
3.2provides details of our pattern-based extractor).Figure 3a illustrates sample detections.We introduce a binary variable tieij, wheretieij=1 denotes an extracted faction relationship.These variables are tied to the variables regardingsentiment via the variablestie sameijk= tieij?
posik?
posjk+ tieij?
negik?
negjktie diffijk= tieij?
posik?
negjk+ tieij?
negik?
posjkitselfij= tieij?
posij?
tieij?
negijwhich are used in the following objective term:?fact=n?i=1n?j=1(?itself?
itselfij+n?k=1(?fact?
(tie sameijk?
tie diffijk)))This formulation enables the model to predict im-plicit sentiment by jointly considering factual andFigure 4: Balance Theory Constraints.
When iis positive towards j, sharing same sentiment to-wards k define a balanced state.
When i is nega-tive towards j, differing opinions towards k definea balanced state.sentiment relations among other entity pairs, es-sentially drawing a connection between sentimentanalysis and information extraction.
Figure 3 vi-sualizes this inference pattern.2.2 Inference with sentiment relationsWe also include constraints ?socialin the objectivethat model social balance and reciprocity.Balance theory constraints: Social balancetheory (Heider, 1946) models the sentiment dy-namics in an interpersonal network.
In particular,in balanced states, entities on positive terms havesimilar opinions towards other entities and thoseon negative terms have opposing opinions.
We in-troduce a set of variables to capture this insight:for example, the case where eiis positive towardsejis shown below (analogous when negative).pos sameijk= posij?
posik?
posjk+ posij?
negik?
negjkpos diffijk= posij?
negik?
posik+ posij?
posik?
negikand add the term ?blto ?social.?bl=n?i=1n?j=1n?k=1(?bl?
(pos sameijk+ neg diffijk)+ ?badbl?
(pos diffijk+ neg sameijk))A visualization of these constraints is in Figure 4.335Faction Balance ReciprocityPOS 57% 64% 73%NEG 60% 61% 78%Table 1: Percentage of labels where each con-straint holds.
For example, positive on reciprocitymeans when pos(ei, ej) is true, 73% of timespos(ej, ei) is also true.Reciprocity constraint: Reciprocity of senti-ment has been recognized as a key aspect of so-cial stability (Johnston, 1916; Gouldner, 1960).
Tomodel reciprocity among the real world entities,we introduce variables:r sameij= posij?
posji+ negij?
negjir diffij= posij?
negji+ negij?
posji?r=n?i=1n?j=1?r(r sameij) + ?badr(r diffij)and add the term ?rto the ?social.2.3 DiscussionWhile many studies exist on homophily, socialbalance, and reciprocity, no prior work has re-ported quantitative analysis on the sentiment dy-namics among the real world entities that appear inunstructured text.
Thus we report the data statis-tics based on the development set in Table 1.
Wefind that the global constraints hold commonly butare not universal, motivating the use of soft con-straints (see Sec.
6).3 Pairwise Base ModelsThe global model in Sec.
2 uses two base models,one for pairwise sentiment classification and theother for detecting faction relationships.3.1 Sentiment ClassifierThe entity-pair classifier considers a holder en-tity ei, its mentions mi1?
?
?mip, a target entityej, its mentions mj1?
?
?mjq, and document d. Itpredicts sent(ei?ej) ?
{positive, unbiased, nega-tive}.
The input is plain text and no gold labels areassumed; entity detection, dependency parse andco-reference resolution are automatic, and includecommon nouns and pronoun mentions (details inSec.
4.1).
We trained separate classifiers for pairsthat co-occur in a sentence and those that do not,using a linear class-weighted SVM classifier withcrowd-sourced data described in Sec.
4.2.In what follows, we describe three differenttypes of features we developed: dependency fea-tures, document features, and quotation features.Many of the features test the overall sentiment ofa set of words (e.g., the complete document, a de-pendency path, or a quotation).
In each case, wedefine the sentiment label for the text to be pos-itive if it contains more words that appear in thepositive sentiment lexicon than that appear in thenegative one (and similarly for the negative label).We used MPQA sentiment lexicon (Wilson et al,2005) for our study, which contains 2,718 positiveand 4,912 negative lexicons.Dependency Features We consider all depen-dency paths between the head word of eiandejin each sentence, and aggregate over allco-occurring sentences.
The features compute:(1) The sentiment label of the path contain-ing dobj and nsubj rev, up to length three ifthe path contains sentiment lexicon words (e.g.,Olympic hero Skah accuses Norway over custodybattle.)
(2) The sentiment label of the path ei?
nsubj ?
ccomp ?
nsubj ?
ej, when it exists(e.g., McCully said any action against Henry is amatter entirely for TVNZ) (3) The sentiment labelof path when the path does not contain any namedentity (e.g., Nobel winner , Shirin Ebadi) (4) Anindicator for the link nmod:against.Document Features Previous work has shownthat notions related to salience (e.g., proximity tosentiment words) can help to detect sentiment tar-gets (Ben-Ami et al, 2014).
In our data, we foundthat an entity?s occurrence pattern is highly indica-tive of being involved in sentiment, for examplethe most frequently mentioned entity is 3.4 timesmore likely to be polarized and an entity in theheadline is two times more likely to be polarized.Pairwise features include the NER type of eiand ejand the percentage of sentences they co-occur in.
We also use features indicating whethereiand ej(1) are mentioned in the headline and (2)appear only once in the document.
When they arethe two most frequent entities, we add the docu-ment sentiment label as a feature.
For entity pairsthat do not appear together in any sentence, wealso include the rank of holder and target in termsof overall number of mentions in the document.Quotation Features Quotations often involvesubjective opinions towards prominent entities innews articles.
Thus we include document-level336features encoding this intuition.
For example, thesentence ?We?re pleased to put this behind us,?said Michael DuVally implies positive sentimentfrom DuVally.
We extract direct quotations usingregular expressions.
We include the sentiment la-bel of the direct quotation from the speaker to theentities in it, excluding entities that appear lessthan three times in the document.
We add thesentiment label of the quotation as a feature to(speaker, the most frequent entity) pair as well.To extract indirect quotations, we follow stud-ies (Bethard et al, 2004; Lu, 2010) and use a list of20 verbs indicating speech events (e.g., say, speak,and announce) to detect direct quotations and theiropinion holders.
We then add the sentiment labelof words connected to ejvia a dependency path oflength up to two that also includes the subject ofquotation verb to ej(e.g.
Hassanal said that coop-eration between Brunei and China were fruitful).We also include an indicator feature for whether eiis the subject of the quotation verb.3.2 Faction DetectorWe use a simple pattern-based detector that ex-tracts a faction relationship between a pair of enti-ties if the dependency path between them either:1. contains only one link of modifier or com-pound label (nmod, nmod : poss, amod, nn, orcompound).2. or contains less than three links and has a pos-sessive or appositive label (poss or appos).Example extractions for this approach, whichwe adopted for its simplicity and the fact that itworks reasonably well in practice, are shown inFigure 3a.
On average we detect 1.7 ties per doc-ument on a small development set with roughly30% recall and 60% precision.
Improving perfor-mance and adding more relation types is an impor-tant area for future work.24 DataWe collected new datasets that densely label sen-timent among entities in news articles, including:208 documents, 2,226 sentences, and 15,185 en-tity pair labels.
It complements existing datasetssuch as MPQA which provides rich annotations atthe sentence-level (Deng and Wiebe, 2015b) andthe recent KBP challenge which provides sparse2We experimented with using relations from an externalknowledge base (Freebase), but KB sparsity and entity link-ing errors posed major challenges.KBP MPQA CrowdsourcedDocument count 154 54 914Avg.
sentence count 10.0 12.7 14.8Avg.
entity count 7.9 10.6 8.8Avg.
mentions / entity 3.6 2.7 3.5Table 2: Corpus Statisticsannotations at the corpus-level (Ellis et al, 2014),by providing document-level annotations for allentity pairs (see Sec.
7 for discussion).4.1 Document PreprocessingAll-pair annotation can be expensive, as there areN2pairs to annotate for each document with Nentities.
We determined that it would be morecost efficient to cover a large number of short doc-uments than a small number of very long docu-ments.
We therefore selected articles with lessthan eleven entities from KBP and less than fifteenfrom MPQA and took the first 15 sentences for an-notation.
We used Stanford CoreNLP (Manninget al, 2014) for sentence splitting, part-of-speechtagging, named entity recognition, co-referenceresolution and dependency parsing.
We discardedentities of type date, duration, money, time andnumber and merged named entities using severalheuristics, such as merging acronyms, mergingnamed entity of person type with the same lastname (e.g., Tiger Woods to Woods).
We mergednames listed as alias in when there is an exactmatch from Freebase.
We included all mentionsin a co-reference chain with the named entity, dis-carding chains with more than one entity.
The cor-pus statistics are shown in Table 2.4.2 Sentiment Data CollectionWe annotated data using two methods: freelancers($7.6 per article on average) covering all entitypairs and crowd-sourcing ($1.6 per article on av-erage) covering a subset of entity pairs.Evaluation Dataset We provide exhaustive an-notations covering all pairs for the evaluation set.We hired freelancers from UpWork,3after ex-amining performance on five documents.
Theylabeled entity pairs with one of the followingclasses.POS: positive towards the target.NOTNEG: positive or unbiased towards the tar-get.3https://www.upwork.com337Label KBP MPQAPOS 3.93 3.52NOT NEG 5.73 8.06UNBIASED 44.64 91.04NOT POS 2.73 6.70NEG 2.27 2.94Table 3: Sentiment Label Statistics.
Each countrepresents the average number per document.UNB: unbiased towards the targetNOTPOS: negative or unbiased towards the tar-get.NEG: negative towards the target.Here, we introduced the NOTPOS and NOT-NEG classes to mark more subjective cases wherewe expect agreement might be lower.
For exam-ple, one assigned NOTPOS to sentiment(Goldman,FINRA), The FINRA said Goldman lacked ad-equate procedures to .
.
.
and another assignedNOTNEG to sentiment(Macalintal, Arroyo) in thenext example.
.
.
.
Arroyo?s election lawyer, Ro-mulo Macalintal.
Arguments could be made forNEG or POS, respectively, but the decision is in-herently subjective and requires careful reading.4We also asked annotators to mark the label asinferred when not explicitly stated but impliedfrom the context or world knowledge.
Allowingfor inferred labels and finer-grained labels encour-aged annotators to capture implicit sentiment.
Foreach judgement, we acquired two labels.
Inter-annotator agreement, in Table 4, is high for the re-laxed metrics, confirming our intuitions about theambiguity of the NOTNEG and NOTPOS labels.For experiments, we combine the fine grainedlabels as follows: POS or NEG is assigned whenboth marked it as such.
When only one of the an-notators marked it, we assigned the weaker senti-ment (POS to NOTNEG, NEG to NOTPOS).
NOT-NEG and NOTPOS are assigned when either an-notator marked it without ?Inferred?
label.
Whenthe labels contradict in polarity or the labels areinferred weaker sentiment, UNB was assigned.Crowdsourced Dataset We also randomly se-lected news articles from the Gigaword corpus,5and collected labels to train the base sentiment4In the construction of MPQA3.0 dataset, entity-entity/event sentiment corpus, even with iterative expert an-notation, 31% of disagreements are caused by negligence.5LDC2014E13:TAC2014KBP English CorpusExact Strict RelaxedPositive 0.35 0.54 0.67Negative 0.50 0.64 0.74Table 4: Inter-annotator Agreement.
Cohen?skappa score: Exact counts only exact matches,Strict counts allows NOT NEG labels to matchPOS, and Relaxed allows NOT NEG to match POSor UNBIASED (analogously for negative).POS NOT NEG NOT POS NEGKBP 25% 29% 30% 28%MPQA 35% 49% 46% 50%Table 5: Percentage of entity pairs that do not co-occur in a sentence.POS NOTNEG NOTPOS NEGKBP 70% 94% 88% 58%MPQA 68% 74% 83% 66%Table 6: Percentage of labels marked as inferred.classifier (Sec.
3.1).
We designed a pipelined ap-proach, with three steps:1.
Document selection: Is there sentiment amongentities in this document?2.
Entity selection: (1) Select all entities holdingsentiment towards any other entities., and (2)Select all entities which are the target of senti-ment by any other entity.3.
Sentiment label collection: Choose the senti-ment A has towards B, from {Positive, No Sen-timent, Negative}We used CrowdFlower,6where annotators wererandomly presented test questions for quality con-trol.
We collected labels from three annotatorsfor each entity pair, and considered labels whenat least two agreed.
The resulting annotation con-tains total 2,995 labels on 914 documents, 682positive, 836 negative and 474 without sentiment,which we discarded.4.3 Insights Into DataThis data supports the study of sentiment-ladenentity pairs across sentence boundaries and in-ferred labels among entities, as we show here.Sentiment Beyond Sentence Boundary Ap-proximately 25% of polarized sentiment labels arebetween entities that do not co-occur7in a sen-tence (see Table 5).
For example, in the article6http://www.crowdflower.com7This is an estimate due to co-reference resolution errors.338with headline ?Russia heat, smog trigger healthproblems?,.
.
.
?We never care to work with a future per-spective in mind,?
Alexei Skripkov of the Fed-eral Medical and Biological Agency said.
?It?sa big systemic mistake.
?Skripkov never appears together with Russia inany sentence, but he manifests negative sentimenttowards it.
When a document revolves around atheme (in this example Russia), sentiment is oftendirected to it without being explicitly mentioned.Inferred sentiment Annotators marked labelsas inferred frequently, especially on less polarizedsentiment (see Table 6).
Various clues led to sen-timent inference.
For example, in the followingdocument, we can read Sam Lake?s positive atti-tude towards Paul Auster from his ?citing?
action:Ask most video-game designers about their in-spirations .
.
.
Sam Lake cites Paul Auster?s?The Book of Illusions?Sentiment can also be inferred through reasoningover another entity.The U.N. imposed an embargo against Eritreafor helping insurgents opposed to the Somaligovernment.By considering relations with Eritrea, we can inferU.N.
would be positive towards Somalia.5 Experimental SetupData and Metrics We randomly split thedensely labeled KBP document set, using half asa test data and half as a development data.
Onehalf of the development set was used to tune hy-per parameters,8and the other for error analysisand ablations.
After development, we ran on thetest sets composed of KBP documents and MPQAdocuments.
For MPQA we did not create a sep-arate development set and reserved all of the rel-atively modest amount of data for a more reliabletest set.
For the pairwise classifier, we report de-velopment results using five-fold cross validationon the training data.We report macro-averaged precision, recall, andF-measure for both sentiment labels.Comparison Systems We compare per-formance to two simple baselines and twoadaptations of existing sentiment classifiers.
Thebaselines include our base pairwise classifier8We used the following values (?r, ?badr, ?itself, ?faction,?bl, ?badbl) = (0.7, -0.8, 0.4, 0.5, 0.1, -0.5).
(Pair) and randomly assigning labels according totheir empirical distribution (Random).The first existing method adaptation (Sentence)uses the publicly released sentence-level RNNsentiment model from Socher et al(2013).
Foreach entity pair, we collect sentiment labels fromsentences they co-occur in and assign a positivelabel if a positive-labeled sentence exists, negativeif there exists more than one sentence with a neg-ative label and no positives.9We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier.
Here, because we added our newlabels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlapto align the entities (KM Gold).
This provides areasonable upper bound on the performance of anyextractor trained on this data.10Implementation Details We use CPLEX411tosolve the ILP described in Sec.
2.
For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocityand balance theory are introduced only on pairsfor which a high-precision classifier assigned po-larity.
For the pairwise classifier, we use a class-weighted linear SVM.12We include annotatedpairs, and randomly sample negative examplesfrom pairs without a label in the crowd-sourcedtraining dataset.
We made two versions of pair-wise classifiers by tuning weight on polarizedclasses and negative sampling ratio by grid search.One is tuned for high precision to be used as a baseclassifier for ILP (ILP base), and the other is tunedfor the best F1 (Pairwise).136 ResultsTable 7 shows results on the evaluation datasets.The global model achieves the best F1 on both la-bels.
All systems do significantly better than therandom baseline but, overall, we see that entity-entity sentiment detection is challenging, requir-9Due to domain difference, the system predicted negativelabels more (73% of sentences were classified as negative).10We consider this gold evaluation a direct proxy for therecent work Deng and Wiebe (2015a), which is the most re-lated recent entity-entity sentiment model trained on the golddata whose predictions we are evaluating against.11http://tinyurl.com/joccfqy12http://scikit-learn.org/13We use 10 as the weights for the polarized classes.
Pair-wise and base classifier for MPQA sampled 4%, base classi-fier for KBP sampled 10% of unlabeled pairs.339Development Set (KBP) KBP MPQAPositive Negative Positive Negative Positive NegativeP R F1 P R F1 P R F1 P R F1 P R F1 P R F1KM Gold 90.9 2.5 4.8 93.8 8.6 15.8 93.9 4.3 8.3 93.5 6.6 12.4 61.5 1.3 2.5 90.0 5.2 9.8Random 16.6 13.1 14.7 4.9 4.0 4.4 13.3 12.7 13.0 10.1 6.9 8.2 10.9 15.4 12.8 8.9 6.7 7.7Sentence 60.0 16.3 25.7 21.7 43.1 28.8 40.9 20.6 27.4 21.0 31.4 25.2 18.9 3.7 6.2 16.7 18.2 17.4Pairwise 47.3 36.9 41.4 25.6 36.8 30.2 36.2 35.5 35.9 27.6 41.2 33.1 28.7 23.0 25.6 23.2 16.3 19.2Global 58.2 37.9 45.9 37.2 35.1 36.1 45.5 32.7 38.1 34.6 36.8 35.7 25.2 29.3 27.1 17.6 24.4 20.4Table 7: Performance on the evaluation datasets: including implicit and explicit sentiment.Positive NegativeP R F1 P R F1ILP base 56.7 25.2 34.9 36.9 27.6 31.6+ Reci.
53.5 30.0 38.4 33.9 33.9 33.9+ Balance 49.6 30.4 37.7 32.0 32.8 32.4+ Faction 58.9 30.2 39.9 37.6 33.9 35.6Table 8: ILP constraints ablation study.Positive NegativeP R F1 P R F1All 34.5 39.7 36.9 35.7 37.6 36.6- Depend.
32.9 32.1 32.5 31.7 38.5 34.8- Doc.
32.6 41.0 35.8 39.4 23.8 28.0- Quotation 33.6 39.5 36.3 34.5 34.6 34.6Table 9: Pairwise classifier feature ablation study.ing identification of holders, targets, and sentimentjointly.
While the numbers are not directly com-parable, the best performing system for KBP 2014sentiment task achieved F1 score of 25.7.The first row (KM Gold) shows the comparisonagainst gold annotations from different datasets,highlighting the differences between the task def-initions.
Our annotations are much more dense,while KBP focuses on specific query entities andMPQA has a much broader focus with less em-phasis on covering all entity pairs.
The high preci-sion suggests that all of the approaches agree whenconsidering the same entity pairs.The global model also improves performanceover the pairwise classifier (Pairwise) for bothdatasets, but we see very different behavior due tothe different sentiment label distributions (see Ta-ble 3).
The KBP data has many fewer unbiasedpairs and many mistakes are from choosing thewrong polarity.
For the pairwise classifier 17%of all predictions were assigned the opposite po-larity.
After the global inference, it is reduced to11%, contributing to the gain in overall precision.For MPQA the base classifier has a more challeng-ing detection task, due to relatively large amountof the unbaised pairs.
Here, the best base classifiermisses many pairs and the global model helps tofill in some of these gaps in recall.In both cases, the document-level model oftenpropagates correct labels by detecting easier, ex-Sentiment expression detection error 21.0%Missing world knowledge 19.3%Named entity detection error 17.5%Co-reference failure 14.8%Propagation error 12.3%Missing faction 7.0%Table 10: Error Analysis on the development set.plicit expressions.
For example, given the sen-tence Buphavanh said Laos creates favorable con-ditions for Vietnamese companies, the base classi-fier detected positive sentiment from Buphavanhto Vietnam, but not between Vietnam and Laos.By detecting the fact that Buphavanh is the primeminister of Laos, it infers the extra sentiment pairs.We also did ablation studies to measure the con-tributions of different components.
Table 8 showsablations of each soft constraint.
The faction con-straint is the most helpful, improving both preci-sion and recall for both labels.
The reciprocityand social balance constraints tend to improve re-call at the cost of precision.
Table 9 shows ab-lations of the base classifier features.
All featuresare helpful, with dependency features most helpfulfor positive labels, and quotation and document-level features more with negatives.Error Analysis We manually analyzed errorson 20 articles from the development set (Table10).
Our system failed when there were senti-ment words not in the lexicon, or negated senti-ment words.
Capturing subtle sentiment expres-sions beyond sentiment lexicon should improvethe performance.
Preprocessing, as a whole, wasthe largest source of error.
It includes co-referencefailure and named entity error.
Co-reference mis-takes happen as a result of not resolving pro-nouns, referring expressions, as well as named en-tities co-references (e.g., Financial Industry Regu-latory Authority to FINRA), or erroneously merg-ing them.
Lengthy quotations or nested mentionstriggered co-reference error, affecting mostly re-call.
Named entity errors includes incorrect named340entity detection (e.g., pro-Israel) and mention de-tection boundary errors.
For example, we detectednegative sentiment from Mexico to Pakistan fromMexico condemns Pakistan series suicide bomb at-tacks.
While actual sentiment is positive.
Finally,the ILP propagates sentiment labels erroneously attimes.
Our constraints often hold among entitiesof the same type, but are less predictive amongentities of different types.
For example, when aperson supports a peace treaty, the treaty does nothave sentiment towards him/her.
For future workrefining constraints based on entity type shouldhelp performance.7 Related WorkSentiment Inference Our sentiment inferencetask is related to the recent KBP sentiment task,14in that we aim to find opinion target and holder.While we study the complete document-level anal-ysis over all entity pairs, the KBP task is for-mulated as query-focused retrieval of entity sen-timent from a large pool of potentially relevantdocuments.
Thus, their annotations focus onlyon query entities and relatively sparse comparedto ours (see Sec.
6).
Another recent dataset isMPQA 3.0 (Deng and Wiebe, 2015b), which cap-tures various aspects of sentiment.
Their senti-ment pair annotations are only at the sentence-level and are therefore much sparser than we pro-vide (see Sec.
6) for entity-entity relation analysis.Several recent studies focused on various as-pects of implied sentiment (Greene and Resnik,2009; Mohammad and Turney, 2010; Zhang andLiu, 2011; Feng et al, 2013; Deng and Wiebe,2014; Deng et al, 2014).
Deng and Wiebe(2015a) in particular introduced sentiment im-plicature rules relevant for sentence-level entity-entity sentiment.
Our work contributes to these re-cent efforts by presenting a new model and datasetfor document-level sentiment inference over allentity pairs.Document-level Analysis Stoyanov and Claire(2011) also studied document-level sentimentanalysis based on fine-grained detection of di-rected sentiment.
They aggregate sentence-leveldetections to make document-level predictions,while our we model global coherency among en-tities and can discover implied sentiment with-out direct sentence-level evidence.
In the event14http://www.nist.gov/tac/2014/KBP/Sentimentextraction domain, previous research showed theeffectiveness of jointly considering multiple sen-tences.
Yang and Mitchell (2016) proposed jointextraction of entities and events with the documentcontext, improving on the event extraction.
Mostwork focuses on events, while we primarily studysentiment relations.Social Network Analysis While many previousstudies considered the effect of social dynamicsfor social media analysis, most relied on an explic-itly available social network structure or consid-ered dialogues and speech acts for which opinionholders are given (Tan et al, 2011; Hu et al, 2013;Li et al, 2014; West et al, 2014; Krishnan andEisenstein, 2015).
Compared to the recent workthat focused on relationships among fictional char-acters in movie summaries and stories (Chaturvediet al, 2016; Srivastava et al, 2016; Iyyer et al,2016), we consider a broader types of named enti-ties on news domains.8 ConclusionWe presented an approach to interpreting senti-ment among entities in news articles, with globalconstraints provided by social, faction and dis-course context.
Experiments demonstrated that theapproach can infer implied sentiment and pointtoward potential directions for future work, in-cluding joint entity detection and incorporation ofmore varied types of factual relationships.AcknowledgmentsThis research was supported in part by theNSF (IIS-1252835, IIS-1408287, IIS-1524371),DARPA under the DEFT program through theAFRL (FA8750-13-2-0019), an Allen Distin-guished Investigator Award, and a gift fromGoogle.
This material is also based upon worksupported by the National Science FoundationGraduate Research Fellowship Program underGrant No.
DGE-1256082.
The authors thank themembers of UW NLP group for discussions andsupport.
We also thank the anonymous reviewersfor insightful comments.
Finally, we thank the an-notators from the CrowdFlower and UpWork.341ReferencesZvi Ben-Ami, Ronen Feldman, and Binyamin Rosen-feld.
2014.
Entities?
sentiment relevance.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
Auto-matic extraction of opinion propositions and theirholders.
In Proceedings of the AAAI Spring Sympo-sium on Exploring Attitude and Affect in Text: The-ories and Applications.Snigdha Chaturvedi, Shashank Srivastava, Hal Daum?eIII, and Chris Dyer.
2016.
Modeling evolving re-lationships between characters in literary novels.
InProceedings of the National Conference on ArtificialIntelligence.Lingjia Deng and Janyce Wiebe.
2014.
Sentimentpropagation via implicature constraints.
In Proceed-ings of the Conference of the European Chapter ofthe Association for Computational Linguistics.Lingjia Deng and Janyce Wiebe.
2015a.
Joint pre-diction for entity/event-level sentiment analysis us-ing probabilistic soft logic models.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Lingjia Deng and Janyce Wiebe.
2015b.
Mpqa 3.0: Anentity/event-level sentiment corpus.
In Proceedingsof Annual Meeting of the Association for Computa-tional Linguistics.Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.2014.
Joint inference and disambiguation of implicitsentiments via implicature constraints.
In Proceed-ings of International Conference on ComputationalLinguistics.Joe Ellis, Jeremy Getman, and Stephanie M Strassel.2014.
Overview of linguistic resources for the tackbp 2014 evaluations: Planning, execution, and re-sults.
In Proceedings of TAC KBP 2014 Work-shop, National Institute of Standards and Technol-ogy, pages 17?18.Song Feng, Jun Seok Kang, Polina Kuznetsova, andYejin Choi.
2013.
Connotation lexicon: A dash ofsentiment beneath the surface meaning.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Alvin W. Gouldner.
1960.
The norm of reciprocity: Apreliminary statement.
American Sociological Re-view, 25(2).Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings the North American Chapter of theAssociation for Computational Linguistics, Boulder,Colorado, June.
Association for Computational Lin-guistics.Fritz Heider.
1946.
Attitudes and cognitive organiza-tion.
The Journal of psychology, 21(1).Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu.
2013.Exploiting social relations for sentiment analysis inmicroblogging.
In Proceedings of the ACM interna-tional conference on Web search and data mining.ACM.Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jor-dan Boyd-Graber, and Hal Daum?e III.
2016.
Feud-ing families and former friends: Unsupervised learn-ing for dynamic fictional relationships.
In Proceed-ings of North American Association for Computa-tional Linguistics.G.
A. Johnston.
1916. International Journal of Ethics,26(2).Vinodh Krishnan and Jacob Eisenstein.
2015.
?You?reMr.
Lebowski, I?m The Dude?
: Inducing addressterm formality in signed social networks.
In Pro-ceedings of the North American Chapter of the As-sociation for Computational Linguistics.Paul F Lazarsfeld and Robert K Merton.
1954.
Friend-ship as a social process: A substantive and method-ological analysis.
Freedom and control in modernsociety, 18:18?66.Jiwei Li, Alan Ritter, and Eduard Hovy.
2014.
Weaklysupervised user profile extraction from twitter.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.Bin Lu.
2010.
Identifying opinion holders and targetswith dependency parser in chinese news texts.
InProceedings of the NAACL HLT Student ResearchWorkshop.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of AnnualMeeting of the Association for Computational Lin-guistics: System Demonstrations, pages 55?60.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: Us-ing mechanical turk to create an emotion lexicon.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, CAAGET ?10.D.
Roth and W. Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In Proceedings of Conference on NaturalLanguage Learning.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts Potts.
2013.
Recursive deepmodels for semantic compositionality over a sen-timent treebank.
In Proceedings of the EmpiricalMethods in Natural Language Processing.342Shashank Srivastava, Snigdha Chaturvedi, and TomMitchell.
2016.
Inferring interpersonal relations innarrative summaries.
In Proceedings of the NationalConference on Artificial Intelligence.Veselin Stoyanov and Claire Cardie.
2011.
Automati-cally Creating General-Purpose Opinion Summariesfrom Text.
In Proceedings of Recent Advances inNatural Language Processing.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentiment anal-ysis incorporating social networks.
In Proceedingsof Knowldege Discovery and Data Mining.Robert West, Hristo S Paskov, Jure Leskovec, andChristopher Potts.
2014.
Exploiting social networkstructure for person-to-person sentiment analysis.
Inthe Proceedings of Transactions of the Associationfor Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing contextual polarityin phrase-level sentiment analysis.
In Proceed-ings of the Human Language Technologies Confer-ence/Conference on Empirical Methods in NaturalLanguage Processing.Bishan Yang and Claire Cardie.
2013.
Joint inferencefor fine-grained opinion extraction.
In Proceedingsof Association for Computational Linguistics.Bishan Yang and Tom Mitchell.
2016.
Joint extractionof events and entities within a document context.
InNorth American Association for Computational Lin-guistics.Lei Zhang and Bing Liu.
2011.
Identifying noun prod-uct features that imply opinions.
In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics.343
