Concrete Sentence Spaces for Compositional DistributionalModels of MeaningEdward Grefenstette?, Mehrnoosh Sadrzadeh?, Stephen Clark?, Bob Coecke?, Stephen Pulman?
?Oxford University Computing Laboratory, ?University of Cambridge Computer Laboratoryfirstname.lastname@comlab.ox.ac.uk, stephen.clark@cl.cam.ac.ukAbstractCoecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributionalsemantics, in which each word in a sentence has a meaning vector and the distributional meaning of thesentence is a function of the tensor products of the word vectors.
Abstractly speaking, this function is themorphism corresponding to the grammatical structure of the sentence in the category of finite dimensionalvector spaces.
In this paper, we provide a concrete method for implementing this linear meaning map,by constructing a corpus-based vector space for the type of sentence.
Our construction method is basedon structured vector spaces whereby meaning vectors of all sentences, regardless of their grammaticalstructure, live in the same vector space.
Our proposed sentence space is the tensor product of two nounspaces, in which the basis vectors are pairs of words each augmented with a grammatical role.
Thisenables us to compare meanings of sentences by simply taking the inner product of their vectors.1 BackgroundCoecke, Sadrzadeh, and Clark [3] develop a mathematical framework for a compositional distributionalmodel of meaning, based on the intuition that syntactic analysis guides the semantic vector composition.The setting consists of two parts: a formalism for a type-logical syntax and a formalism for vector spacesemantics.
Each word is assigned a grammatical type and a meaning vector in the space corresponding toits type.
The meaning of a sentence is obtained by applying the function corresponding to the grammaticalstructure of the sentence to the tensor product of the meanings of the words in the sentence.
Based on thetype-logic used, some words will have atomic types and some compound function types.
The compoundtypes live in a tensor space where the vectors are weighted sums (i.e.
superpositions) of the pairs of basesfrom each space.
Compound types are ?applied?
to their arguments by taking inner products, in a similarmanner to how predicates are applied to their arguments in Montague semantics.For the type-logic we use Lambek?s Pregroup grammars [7].
The use of pregoups is not essential, butleads to a more elegant formalism, given its proximity to the categorical structure of vector spaces (see [3]).A Pregroup is a partially ordered monoid where each element has a right and left cancelling element, referredto as an adjoint.
It can be seen as the algebraic counterpart of the cancellation calculus of Harris [6].
Theoperational difference between a Pregroup and Lambek?s Syntactic Calculus is that, in the latter, the monoidmultiplication of the algebra (used to model juxtaposition of the types of the words) has a right and a leftadjoint, whereas in the pregroup it is the elements themselves which have adjoints.
The adjoint types areused to denote functions, e.g.
that of a transitive verb with a subject and object as input and a sentence asoutput.
In the Pregroup setting, these function types are still denoted by adjoints, but this time the adjointsof the elements themselves.As an example, consider the sentence ?dogs chase cats?.
We assign the type n (for noun phrase) to ?dog?and ?cat?, and nrsnl to ?chase?, where nr and nl are the right and left adjoints of n and s is the type of a125(declarative) sentence.
The type nrsnl expresses the fact that the verb is a predicate that takes two argumentsof type n as input, on its right and left, and outputs the type s of a sentence.
The parsing of the sentence isthe following reduction:n(nrsnl)n ?
1s1 = sThis parse is based on the cancellation of n and nr, and also nl and n; i.e.
nnr ?
1 and nln ?
1 for 1the unit of juxtaposition.
The reduction expresses the fact that the juxtapositions of the types of the wordsreduce to the type of a sentence.On the semantic side, we assign the vector space N to the type n, and the tensor space N ?S?N to thetype nrsnl.
Very briefly, and in order to introduce some notation, recall that the tensor space A?B has as abasis the cartesian product of a basis of A with a basis of B.
Recall also that any vector can be expressed asa weighted sum of basis vectors; e.g.
if (?
?v1 , .
.
.
,?
?vn) is a basis of A then any vector ?
?a ?
A can be written as?
?a =?i Ci?
?vi where each Ci ?
R is a weighting factor.
Now for (?
?v1 , .
.
.
,?
?vn) a basis of A and (?
?v?1 , .
.
.
,?
?v?n)a basis of B, a vector ?
?c in the tensor space A ?
B can be expressed as follows:?ijCij (?
?vi ??
?v?j )where the tensor of basis vectors ?
?vi ??
?v?j stands for their pair (?
?vi ,?
?v?j ).
In general ?
?c is not separable intothe tensor of two vectors, except for the case when ?
?c is not entangled.
For non-entangled vectors we canwrite ?
?c = ?
?a ??
?b for ?
?a =?i Ci?
?vi and?
?b =?j C ?j?
?v?j ; hence the weighting factor of ?
?c can be obtainedby simply multiplying the weights of its tensored counterparts, i.e.
Cij = Ci ?
C ?j .
In the entangled casethese weights cannot be determined as such and range over all the possibilities.
We take advantage of thisfact to encode meanings of verbs, and in general all words that have compound types and are interpreted aspredicates, relations, or functions.
For a brief discussion see the last paragraph of this section.
Finally, weuse the Dirac notation to denote the dot or inner product of two vectors ??
?a | ?
?b ?
?
R defined by?i Ci?C ?i.Returning to our example, for the meanings of nouns we have???dogs,?
?cats ?
N , and for the meanings ofverbs we have???
?chase ?
N ?
S ?
N , i.e.
the following superposition:?ijkCijk (?
?ni ??
?sj ??
?nk)Here ?
?ni and ?
?nk are basis vectors of N and ?
?sj is a basis vector of S. From the categorical translation methodpresented in [3] and the grammatical reduction n(nrsnl)n ?
s, we obtain the following linear map as thecategorical morphism corresponding to the reduction:N ?
1s ?
N : N ?
(N ?
S ?
N)?
N ?
SUsing this map, the meaning of the sentence is computed as follows:??????????
?dogs chase cats = (N ?
1s ?
N )(??
?dogs ????
?chase ??
?cats)= (N ?
1s ?
N )????
?dogs ????ijkCijk(?
?ni ??
?sj ???nk)?????cats??=?ijkCijk???
?dogs | ??ni??
?sj ??
?nk | ?
?cats?The key features of this operation are, first, that the inner-products reduce dimensionality by ?consuming?tensored vectors and by virtue of the following component function:N : N ?
N ?
R :: ?
?a ??
?b 7?
??
?a | ?
?b ?126Thus the tensored word vectors??
?dogs ?
???
?chase ?
?
?cats are mapped into a sentence space S which is commonto all sentences regardless of their grammatical structure or complexity.
Second, note that the tensor product???dogs?????chase??
?cats does not need to be calculated, since all that is required for computation of the sentencevector are the noun vectors and the Cijk weights for the verb.
Note also that the inner product operationsare simply picking out basis vectors in the noun space, an operation that can be performed in constanttime.
Hence this formalism avoids two problems faced by approaches in the vein of [9, 2], which usethe tensor product as a composition operation: first, that the sentence meaning space is high dimensionaland grammatically different sentences have representations with different dimensionalities, preventing themfrom being compared directly using inner products; and second, that the space complexity of the tensoredrepresentation grows exponentially with the length and grammatical complexity of the sentence.
In constrast,the model we propose does not require the tensored vectors being combined to be represented explicitly.Note that we have taken the vector of the transitive verb, e.g.???
?chase, to be an entangled vector in thetensor space N ?
S ?N .
But why can this not be a separable vector, in which case the meaning of the verbwould be as follows:???
?chase =?iCi?
?ni ?
?jC ?j?
?sj ?
?kC ??k?
?nkThe meaning of the sentence would then become ?1?2?j C ?j?
?sj for ?1 =?i Ci???
?dogs | ??ni?
and ?2 =?k C ?
?k ??
?cats | ??nk?.
The problem is that this meaning only depends on the meaning of the verb and isindependent of the meanings of the subject and object, whereas the meaning from the entangled case,i.e.
?1?2?ijk Cijk?
?sj , depends on the meanings of subject and object as well as the verb.2 From Truth-Theoretic to Corpus-based MeaningThe model presented above is compositional and distributional, but still abstract.
To make it concrete, N andS have to be constructed by providing a method for determining the Cijk weightings.
Coecke, Sadrzadeh,and Clark [3] show how a truth-theoretic meaning can be derived in the compositional framework.
Forexample, assume that N is spanned by all animals and S is the two-dimensional space spanned by ?
?true and???false.
We use the weighting factor to define a model-theoretic meaning for the verb as follows:Cijk?
?sj ={?
?true chase(?
?ni ,?
?nk) = true ,??
?false o.w.The definition of our meaning map ensures that this value propagates to the meaning of the whole sentence.So chase(???dogs,??
?cats) becomes true whenever ?dogs chase cats?
is true and false otherwise.
This is exactlyhow meaning is computed in the model-theoretic view on semantics.
One way to generalise this truth-theoretic meaning is to assume that chase(?
?ni ,?
?nk) has degrees of truth, for instance by defining chase as acombination of run and catch, such as:chase = 23run+13catchAgain, the meaning map ensures that these degrees propagate to the meaning of the whole sentence.
For aworked out example see [3].
But neither of these examples provide a distributional sentence meaning.Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaningfor a sentence based on the meanings of the words derived from a corpus.
But crucially this meaning goesbeyond just composing the meanings of words using a vector operator, such as tensor product, summationor multiplication [8].
Our computation of sentence meaning treats some vectors as functions and others as127function arguments, according to how the words in the sentence are typed, and uses the syntactic structureas a guide to determine how the functions are applied to their arguments.
The intuition behind this approachis that syntactic analysis guides semantic vector composition.The contribution of this paper is to introduce some concrete constructions for a compositional distri-butional model of meaning.
These constructions demonstrate how the mathematical model of [3] can beimplemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of naturallanguage semantics which is closer to the ideas underlying standard distributional models of word meaning.We leave full evaluation to future work, in order to determine whether the following method in conjunctionwith word vectors built from large corpora leads to improved results on language processing tasks, such ascomputing sentence similarity and paraphrase evaluation.Nouns and Transitive Verbs.
We take N to be a structured vector space, as in [4, 5].
The bases of N areannotated by ?properties?
obtained by combining dependency relations with nouns, verbs and adjectives.
Forexample, basis vectors might be associated with properties such as ?arg-fluffy?, denoting the argument ofthe adjective fluffy, ?subj-chase?
denoting the subject of the verb chase, ?obj-buy?
denoting the object of theverb buy, and so on.
We construct the vector for a noun by counting how many times in the corpus a wordhas been the argument of ?fluffy?, the subject of ?chase?, the object of ?buy?, and so on.The framework in [3] offers no guidance as to what the sentence space should consist of.
Here we takethe sentence space S to be N ?
N , so its bases are of the form ?
?sj = (?
?ni ,??nk).
The intuition is that, for atransitive verb, the meaning of a sentence is determined by the meaning of the verb together with its subjectand object.1 The verb vectors Cijk(?
?ni ,?
?nk) are built by counting how many times a word that is ni (e.g.
hasthe property of being fluffy) has been subject of the verb and a word that is nk (e.g.
has the property that it?sbought) has been its object, where the counts are moderated by the extent to which the subject and objectexemplify each property (e.g.
how fluffy the subject is).
To give a rough paraphrase of the intuition behindthis approach, the meaning of ?dog chases cat?
is given by: the extent to which a dog is fluffy and a cat issomething that is bought (for the N ?
N property pair ?arg-fluffy?
and ?obj-buy?
), and the extent to whichfluffy things chase things that are bought (accounting for the meaning of the verb for this particular propertypair); plus the extent to which a dog is something that runs and a cat is something that is cute (for the N ?Npair ?subj-run?
and ?arg-cute?
), and the extent to which things that run chase things that are cute (accountingfor the meaning of the verb for this particular property pair); and so on for all noun property pairs.Adjective Phrases.
Adjectives are dealt with in a similar way.
We give them the syntactic type nnl andbuild their vectors in N ?
N .
The syntactic reduction nnln ?
n associated with applying an adjective to anoun gives us the map 1N ?
N by which we semantically compose an adjective with a noun, as follows:????
?red fox = (1N ?
N )(?
?red ??
?fox) =?ijCij??ni??
?nj |?
?fox?We can view the Cij counts as determining what sorts of properties the arguments of a particular adjectivetypically have (e.g.
arg-red, arg-colourful for the adjective ?red?
).Prepositional Phrases.
We assign the type nrn to the whole prepositional phrase (when it modifies a noun),for example to ?in the forest?
in the sentence ?dogs chase cats in the forest?.
The pregroup parsing is asfollows:n(nrsnl)n(nrn) ?
1snl1n ?
snln ?
s1 = sThe vector space corresponding to the prepositional phrase will thus be the tensor space N ?
N and thecategorification of the parse will be the composition of two morphisms: (1S?lN )?
(rN?1S?1N?rN?1N ).1Intransitive and ditransitive verbs are interpreted in an analagous fashion; see ?4.128The substitution specific to the prepositional phrase happens when computing the vector for ?cats in theforest?
as follows:????????????
?cats in the forest = (rN ?
1N )(?
?cats ?????????
?in the forest)= (rN ?
1N )(?
?cats ??lwClw?
?nl ???nk)=?lwClw??
?cats | ??nl??
?nwHere we set the weights Clw in a similar manner to the cases of adjective phrases and verbs with the countsdetermining what sorts of properties the noun modified by the prepositional phrase has, e.g.
the number oftimes something that has attribute nl has been in the forest.Adverbs.
We assign the type srs to the adverb, for example to ?quickly?
in the sentence ?Dogs chase catsquickly?.
The pregroup parsing is as follows:n(nrsnl)n(srs) ?
1s1srs = ssrs ?
1s = sIts categorification will be a composition of two morphisms (rS ?
1S) ?
(rN ?
1S ?
lN ?
1S ?
1S).
Thesubstitution specific to the adverb happens after computing the meaning of the sentence without it, i.e.
thatof ?Dogs chase cats?, and is as follows:?????????????????
?Dogs chase cats quickly = (rS ?
1S) ?
(rN ?
1S ?
lN ?
1S ?
1S)(??
?Dogs ????
?chase ??
?cats ?????
?quickly)= (rS ?
1S)???ijkCijk???
?dogs | ??ni??
?sj ??
?nk | ??cats?
??????quickly?
?= (rS ?
1S)???ijkCijk???
?dogs | ??ni??
?sj ??
?nk | ??cats?
??lwClw?
?sl ???sw??=?lwClw??ijkCijk???
?dogs | ??ni??
?sj ??
?nk | ??cats?
| ??sl??
?skThe Clw weights are defined in a similar manner to the above cases, i.e.
according to the properties theadverb has, e.g.
which verbs it has modified.
Note that now the basis vectors ?
?sl and ?
?sw are themselves pairsof basis vectors from the noun space, (?
?ni ,??nj).
Hence, Clw(?
?ni ,?
?nj) can be set only for the case when l = iand w = j; these counts determine what sorts of properties the verbs that happen quickly have (or morespecifically what properties the subjects and objects of such verbs have).
By taking the whole sentence intoaccount in the interpretation of the adverb, we are in a better position to semantically distinguish betweenthe meaning of adverbs such as ?slowly?
and ?quickly?, for instance in terms of the properties that the verb?ssubjects have.
For example, it is possible that elephants are more likely to be the subject of a verb which ishappening slowly, e.g.
run slowly, and cheetahs are more likely to be the subject of a verb which is happeningquickly.3 Concrete ComputationsIn this section we first describe how to obtain the relevant counts from a parsed corpus, and then give somesimilarity calculations for some example sentence pairs.129Let Cl be the set of grammatical relations (GRs) for sentence sl in the corpus.
Define verbs(Cl) to bethe function which returns all instances of verbs in Cl, and subj (and similarly obj ) to be the function whichreturns the subject of an instance Vinstance of a verb V , for a particular set of GRs for a sentence:subj(Vinstance) ={noun if Vinstance is a verb with subject noun?n o.w.where ?n is the empty string.
We express Cijk for a verb V as follows:Cijk ={?l?v?verbs(Cl) ?
(v, V )??????
?subj(v) | ??ni??????
?obj(v) | ??nk?
if ?
?sj = (?
?ni ,?
?nk)0 o.w.where ?
(v, V ) = 1 if v = V and 0 otherwise.
Thus we construct Cijk for verb V only for cases wherethe subject property ni and the object property nk are paired in the basis ?
?sj .
This is done by counting thenumber of times the subject of V has property ni and the object of V has property nk, then multiplying them,as prescribed by the inner products (which simply pick out the properties ni and nk from the noun vectorsfor the subjects and objects).The procedure for calculating the verb vectors, based on the formulation above, is as follows:1.
For each GR in a sentence, if the relation is subject and the head is a verb, then find the complementaryGR with object as a relation and the same head verb.
If none, set the object to ?n.2.
Retrieve the noun vectors?????subject,???
?object for the subject dependent and object dependent from previ-ously constructed noun vectors.3.
For each (ni, nk) ?
basis(N)?
basis(N) compute the inner-product of ?
?ni with????
?subject and ?
?nk with???
?object (which involves simply picking out the relevant basis vectors from the noun vectors).
Multiplythe inner-products and add this to Cijk for the verb, with j such that ?
?sj = (?
?ni ,?
?nk).The procedure for other grammatical types is similar, based on the definitions of C weights for the semanticsof these types.We now give a number of example calculations.
We first manually define the distributions for nouns,which in practice would be obtained from a corpus:bankers cats dogs stock kittens1.
arg-fluffy 0 7 3 0 22. arg-ferocious 4 1 6 0 03. obj-buys 0 4 2 7 04. arg-shrewd 6 3 1 0 15. arg-valuable 0 1 2 8 0We aim to make these counts match our intuitions, in that bankers are shrewd and a little ferocious but notfurry, cats are furry but not typically valuable, and so on.We also define the distributions for the transitive verbs ?chase?, ?pursue?
and ?sell?, again manuallyspecified according to our intuitions about how these verbs are used.
Since in the formalism proposed above,Cijk = 0 if ?
?sj 6= (?
?ni ,?
?nk), we can simplify the weight matrices for transitive verbs to two dimensional Cikmatrices as shown below, where Cik corresponds to the number of times the verb has a subject with attributeni and an object with attribute nk.
For example, the matrix below encodes the fact that something ferocious130(i = 2) chases something fluffy (k = 1) seven times in the hypothetical corpus from which we might haveobtained these distributions.Cchase =?????
?1 0 0 0 07 1 2 3 10 0 0 0 02 0 1 0 11 0 0 0 0?????
?Cpursue =?????
?0 0 0 0 04 2 2 2 40 0 0 0 03 0 2 0 10 0 0 0 0?????
?Csell =?????
?0 0 0 0 00 0 3 0 40 0 0 0 00 0 5 0 80 0 1 0 1?????
?These matrices can be used to perform sentence comparisons:???????????
?dogs chase cats | ?????????????
?dogs pursue kittens?
==???
?ijkCchaseijk ???
?dogs | ??ni??
?sj ??
?nk | ??cats???????????
?ijkCpursueijk ???
?dogs | ??ni??
?sj ??
?nk |?????kittens???
?=?ijkCchaseijk Cpursueijk ???
?dogs | ??ni????
?dogs | ??ni???
?nk | ??cats???
?nk |????
?kittens?The raw number obtained from the above calculation is 14844.
Normalising it by the product of the lengthof both sentence vectors gives the cosine value of 0.979.Consider now the sentence comparison ???????????
?dogs chase cats | ??????????
?cats chase dogs?.
The sentences in this paircontain the same words but the different word orders give the sentences very different meanings.
The rawnumber calculated from this inner product is 7341, and its normalised cosine measure is 0.656, which demon-strates the sharp drop in similarity obtained from changing sentence structure.
We expect some similaritysince there is some non-trivial overlap between the properties identifying cats and those identifying dogs(namely those salient to the act of chasing).Our final example for transitive sentences is ???????????
?dogs chase cats | ???????????
?bankers sell stock?, as two sentences thatdiverge in meaning completely.
The raw number for this inner product is 6024, and its cosine measure is0.042, demonstrating the very low semantic similarity between these two sentences.Next we consider some examples involving adjective-noun modification.
The Cij counts for an adjectiveA are obtained in a similar manner to transitive or intransitive verbs:Cij ={?l?a?adjs(Cl) ?(a,A)???????
?arg-of(a) | ??ni?
if ?
?ni = ?
?nj0 o.w.where adjs(Cl) returns all instances of adjectives in Cl; ?
(a,A) = 1 if a = A and 0 otherwise; andarg-of(a) = noun if a is an adjective with argument noun, and ?n otherwise.As before, we stipulate the Cij matrices by hand (and we eliminate all cases where i 6= j since Cij = 0by definition in such cases):Cfluffy = [9 3 4 2 2] Cshrewd = [0 3 1 9 1] Cvaluable = [3 0 8 1 8]We compute vectors for ?fluffy dog?
and ?shrewd banker?
as follows:??????
?fluffy dog = (3 ?
9)??????
?arg-fluffy+ (6 ?
3)????????
?arg-ferocious+ (2 ?
4)?????
?obj-buys+ (5 ?
2)???????
?arg-shrewd+ (2 ?
2)?????????arg-valuable??????????
?shrewd banker = (0 ?
0)??????
?arg-fluffy+ (4 ?
3)????????
?arg-ferocious+ (0 ?
0)?????
?obj-buys+ (6 ?
9)???????
?arg-shrewd+ (0 ?
1)????????
?arg-valuableVectors for??????
?fluffy cat and?????????
?valuable stock are computed similarly.
We obtain the following similarity mea-sures:cosine(??????
?fluffy dog,??????????
?shrewd banker) = 0.389 cosine(??????
?fluffy cat,?????????
?valuable stock) = 0.184131These calculations carry over to sentences which contain the adjective-noun pairings compositionally andwe obtain an even lower similarity measure between sentences:cosine(???????????????????
?fluffy dogs chase fluffy cats,????????????????????????
?shrewd bankers sell valuable stock) = 0.016To summarise, our example vectors provide us with the following similarity measures:Sentence 1 Sentence 2 Degree of similaritydogs chase cats dogs pursue kittens 0.979dogs chase cats cats chase dogs 0.656dogs chase cats bankers sell stock 0.042fluffy dogs chase fluffy cats shrewd bankers sell valuable stock 0.0164 Different Grammatical StructuresSo far we have only presented the treatment of sentences with transitive verbs.
For sentences with intransitiveverbs, the sentence space suffices to be just N .
To compare the meaning of a transitive sentence with anintransitive one, we embed the meaning of the latter from N into the former N ?
N , by taking ??
?n (the?object?
of an intransitive verb) to be?i?
?ni , i.e.
the superposition of all basis vectors of N .Following the method for the transitive verb, we calculate Cijk for an instransitive verb V and basis pair?
?sj = (?
?ni ,?
?nk) as follows, where l ranges over the sentences in the corpus:?l?v?verbs(Cl)?
(v, V )??????
?subj(v) | ??ni??????
?obj(v) | ??nk?
=?l?v?verbs(Cl)?
(v, V )??????
?subj(v) | ??ni????
?n | ?
?nk?and ???
?n | ??ni?
= 1 for any basis vector ni.We can now compare the meanings of transitive and intransitive sentences by taking the inner product oftheir meanings (despite the different arities of the verbs) and then normalising it by vector length to obtainthe cosine measure.
For example:???????????
?dogs chase cats | ???????
?dogs chase?
=????ijkCijk???
?dogs | ??ni??
?sj ??
?nk | ??
?cats ???????????
?ijkC ?ijk???
?dogs | ??ni???sj??
?=?ijkCijkC ?ijk???
?dogs | ??ni????
?dogs | ??ni???
?nk | ?
?cats?The raw number for the inner product is 14092 and its normalised cosine measure is 0.961, indicating highsimilarity (but some difference) between a sentence with a transitive verb and one where the subject remainsthe same, but the verb is used intransitively.Comparing sentences containing nouns modified by adjectives to sentences with unmodified nouns is straight-forward:????????????????????
?fluffy dogs chase fluffy cats | ??????????
?dogs chase cats?
=?ijCfluffyi Cfluffyj Cchaseij Cchaseij ???
?dogs | ??ni?2??
?nj |??
?cats?2 = 2437005132From the above we obtain the following similarity measure:cosine(???????????????????
?fluffy dogs chase fluffy cats,??????????
?dogs chase cats) = 0.971For sentences with ditransitive verbs, the sentence space changes to N ?
N ?
N , on the basis of the verbneeding two objects; hence its grammatical type changes to nrsnlnl.
The transitive and intransitive verbsare embedded in this larger space in a similar manner to that described above; hence comparison of theirmeanings becomes possible.5 Ambiguous WordsThe two different meanings of a word can be distinguished by the different properties that they have.
Theseproperties are reflected in the corpus, by the different contexts in which the words appear.
Consider thefollowing example from [4]: the verb ?catch?
has two different meanings, ?grab?
and ?contract?.
They arereflected in the two sentences ?catch a ball?
and ?catch a disease?.
The compositional feature of our meaningcomputation enables us to realise the different properties of the context words via the grammatical roles theytake in the corpus.
For instance, the word ?ball?
occurs as argument of ?round?, and so has a high weightfor the base ?arg-round?, whereas the word ?disease?
has a high weight for the base ?arg-contagious?
and as?mod-of-heart?.
We extend our example corpus from previously to reflect these differences as follows:ball disease1.
arg-fluffy 1 02. arg-ferocious 0 03. obj-buys 5 04. arg-shrewd 0 05. arg-valuable 1 06. arg-round 8 07. arg-contagious 0 78. mod-of-heart 0 6In a similar way, we build a matrix for the verb ?catch?
as follows:Ccatch =???????????
?3 2 3 3 3 8 6 23 2 3 0 1 4 7 42 4 7 1 1 6 2 23 1 2 0 0 3 6 21 1 1 0 0 2 0 10 0 0 0 0 0 0 00 0 0 0 0 0 0 00 0 0 0 0 0 0 0???????????
?The last three rows are zero because we have assumed that the words that can take these roles are mostlyobjects and hence cannot catch anything.
Given these values, we compute the similarity measure betweenthe two sentences ?dogs catch a ball?
and ?dogs catch a disease?
as follows:????????????
?dogs catch a ball | ?????????????
?dogs catch a disease?
= 0In an idealised case like this where there is very little (or no) overlap between the properties of the objectsassociated with one sense of ?catch?
(e.g.
a disease), and those properties of the objects associated with an-other sense (e.g.
a ball), disambiguation is perfect in that there is no similarity between the resulting phrases.133In practice, in richer vector spaces, we would expect even diseases and balls to share some properties.
How-ever, as long as those shared properties are not those typically held by the object of catch, and as long as theusages of catch play to distinctive properties of diseases and balls, disambiguation will occur by the samemechanism as the idealised case above, and we can expect low similarity measures between such sentences.6 Related WorkMitchell and Lapata introduce and evaluate a multiplicative model for vector composition [8].
The particularconcrete construction of this paper differs from that of [8] in that our framework subsumes truth-theoreticas well as corpus-based meaning, and our meaning construction relies on and is guided by the grammaticalstructure of the sentence.
The approach of [4] is more in the spirit of ours, in that extra information aboutsyntax is used to compose meaning.
Similar to us, they use a structured vector space to integrate lexicalinformation with selectional preferences.
Finally, Baroni and Zamparelli model adjective-noun combinationsby treating an adjective as a function from noun space to noun space, represented using a matrix, as we doin this paper [1].References[1] M. Baroni and R. Zamparelli.
Nouns are vectors, adjectives are matrices: Representing adjective-noun construc-tions in semantic space.
In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),Cambridge, MA, 2010.
[2] S. Clark and S. Pulman.
Combining symbolic and distributional models of meaning.
In Proceedings of AAAISpring Symposium on Quantum Interaction.
AAAI Press, 2007.
[3] B. Coecke, M. Sadrzadeh, and S. Clark.
Mathematical Foundations for a Compositional Dis-tributional Model of Meaning, volume 36.
Linguistic Analysis (Lambek Festschrift), 2010.http://arxiv.org/abs/1003.4394.
[4] K. Erk and S. Pado?.
A structured vector space model for word meaning in context.
In Conference on EmpiricalMethods in Natural Language Processing (EMNLP-08), pages 897?906, Honolulu, Hawaii, 2008.
[5] G. Grefenstette.
Use of syntactic context to produce term association lists for text retrieval.
In Nicholas J. Belkin,Peter Ingwersen, and Annelise Mark Pejtersen, editors, SIGIR, pages 89?97.
ACM, 1992.
[6] Z. Harris.
Mathematical Structures of Language.
Interscience Publishers John Wiley and Sons, 1968.
[7] J. Lambek.
From Word to Sentence.
Polimetrica, 2008.
[8] J. Mitchell and M. Lapata.
Vector-based models of semantic composition.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguistics, pages 236?244, Columbus, OH, 2008.
[9] P. Smolensky and G. Legendre.
The Harmonic Mind: From Neural Computation to Optimality-Theoretic GrammarVol.
I: Cognitive Architecture Vol.
II: Linguistic and Philosophical Implications.
MIT Press, 2005.134
