Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 87?96,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsInteractive Gesture in Dialogue: a PTT ModelHannes RieserBielefeld UniversityHannes.Rieser@uni-Bielefeld.deMassimo PoesioUniversit?
di Trento/University of Essexpoesio@essex.ac.ukAbstractGestures are usually looked at in isola-tion or from an intra-propositional per-spective essentially tied to one speaker.The Bielefeld multi-modal Speech-And-Gesture-Alignment (SAGA) corpus hasmany interactive gestures relevant for thestructure of dialogue (Rieser 2008, 2009).To describe them, a dialogue theory isneeded which can serve as a speech-gesture interface.
PTT (Poesio and Traum1997, Poesio and Rieser submitted a) cando this job in principle, how this can beachieved is the main topic of this paper.As a precondition, the empirical researchprocedure from systematic corpus annota-tion via gesture typology to a partial on-tology for gestures is described.
It is thenexplained how PTT is extended to providean incremental modelling of speech plusgesture in an assertion-acknowledgementadjacency pair where grounding betweendialogue participants is obtained throughgesture.1 Introduction and OverviewWe present work combining experimental meth-ods, body-movement tracking techniques, corpuslinguistics and theoretical modelling in order to in-vestigate the role of iconic gesture in dialogue.
Wepropose to map speech meaning and gesture mean-ing into a single compositional meaning which isthen used in grounding and up-dating of infor-mation states in discourse, using PTT (Poesio &Traum 1997, Poesio & Rieser submitted 2009a)to account for the speech-gesture interface.
Weargue that several design features of PTT are es-sential for this purpose, such as accepting sub-propositional inputs, extracting information fromlinguistic surface, using dynamic semantics, bas-ing the dialogue engine on a theory of grounded-ness and grounding, and allowing for the resolu-tion of anaphora across turns.The structure of the paper is as follows.
Sec-tion 2 looks at the Bielefeld Speech-and-Gesture-Alignment corpus SAGA from which the datacomes.
Section 3 then deals with multi-modal actsusing one example from SAGA (Dial 1 p.??).
Insection 4 a short introduction into PTT is provided.Sections 5 and 6 explain how a gesture typologyand a partial ontology can be extracted from theannotated data.
Both (see Appendix) serve as thebasis for the integration of gesture meaning andverbal meaning.
In section 7 PTT is developedas an interface for verbal and gestural meaning.First a PTT description of Dial 1 is provided using(Poesio and Rieser submitted b, Poesio to appear)dealing inter alia with anaphora resolution (7.1).Secondly, PTTs interface properties are detailed(7.2), the semantic defaults for combining speechand gesture meaning are set up (7.3), and a gestu-ral dialogue act is described (7.4).
Section 8 con-tains some preliminary insights into the groundingof multi-modal content.2 The Multi-modal SAGA CorpusThe SAGA corpus contains 25 route-descriptiondialogues taken from three camera perspectivesusing body tracking technologies.1 The settingcomes with a Router ?riding on a car?
througha virtual landscape passing five landmarks.
Thelandmarks are connected by streets.
Fig.
1a in Ap-pendix B shows the Router, Fig.
1b the site, Fig.1cf.
Bergmann, K. et al (2007, 2008)871c the town hall.
After the ride the Router reportshis trip in detail to a Follower.
We collected audioand body movement data as well as eye-trackingdata from the Router.
The dialogues have all beenannotated, use of functional predicates like IN-DEXING, MODELLING, SHAPING2 etc.
wasrated.3 An Example from the SAGA corpusIn the dialogue passage (Dial 1) the Router usesgestures to explain the looks of the town-hall.We?ll focus on the numbered utterances in this pa-per; utterances omitted in the reconstruction arereported in italics, omitted phrases in brackets.DIAL 1 [ROUTER:] [.
.
.
][.
.
.
]undand[du][you]folgstfollowdannthendemtheStra?enverlaufstreeteinfachsimplynur bisuntilduyouahahvorbeforenemagr??erenlargerGeb?udebuildingstehst.stand.
(2.1) DasThatistisdannthendastheRathaus.townhall.
(2.2) [Ahm][Ahm]dasthatistiseinaU-f?rmigesU-shapedGeb?ude.building.
(2.3) DuYoublickstlook[praktisch][practically]daintorein.it.
(2.4) [Das[Thathei?t]is]esithathasvorneto the frontzweitwoBuchtungenbulgesundandgehtcloseshintenin the rearzusammen dann.then.
[FOLLOWER:] OK.In (Dial 1) Router?s gestures first come withtwo BEATS.3 Shortly after, the BEATs extendinto an ICONIC gesture overlapping town hallin (2.1)(stills in Apendix B), cf.
the still Two-Handed-Prism-Segment-1.
Then the Router?sDRAWN U-shaped gesture (still One-Handed-U-Shape) intersects the word U-shaped.
Nexthis SHAPING the sides of a prism (still Two-Handed-U-Shaped-Prism-Segment) aligns with[look pactically] into it.
The gesture followingis two-handed: one hand SHAPES the U?s leftbranch and the other both the U?s right branchand its rear bend linking up to the left branch(stills Two-Handed-Prism-Segment-2A and 2B).The STROKE overlaps with the words and closesin the rear.
The Follower copies the two-handed2Annotation PREDICATES are written in capital letters.Cf.
also fn.
5.3BEATS largely rest on supra-segmentals and would de-mand a paper of their own.town hall gesture of the Router in his acknowl-edgement (still Two-Handed-Prism-Segment-3).In other words: the Follower?s gesture is alignedto the Router?s.
Being copies of each other, the se-mantics of the Router?s and the Follower?s gesturecan enter the common ground (cf.
7.4 and 8).
Inthe reconstruction we will use the translation withthe English word order standardised.44 A Short Introduction to PTTExplanation of dialogue rests on three things:making clear how the succession of speakers?
con-tributions emerges, stating what the impact of con-tributions on speakers?
minds is and specifyinghow information is extracted incrementally fromthe contributions.
Turning to emerging struc-ture, PTT assumes that participants perform (oftenfragmentary) contributions, discourse units (DUs),which are dynamic propositions (DRSs in thesense of (Muskens, 1996)).
They contain locu-tionary acts, conversational events/dialogue actsplus their propositional contents/DRSs.
DUs maybe sub-propositional micro-conversational events.Dialogue acts are either core speech acts orgrounding acts.
Core speech acts can be related tothe present like assert, towards the past like acceptor towards the future like commit.
Grounding actsare acknowledge or repair (Traum 2009).
Puttingthe distinctions above to work, we obviously canalready model adjacency pairs.
For the problemsat issue we do not need more, cf.
(Dial 1).Which attitudes are assumed in current PTTand which changes of participants?
minds are ac-counted for?
Agents can have individual and pri-vate or common and public intentions.
All sortsof actions, verbal or domain ones, are as a ruleintended, at the outset of changes we have indi-vidual intentions.
Common intentions are for ex-ample needed in order to explain completions andrepairs (Poesio and Rieser submitted a).
Most ofthe cooperation facts investigated in Clark (1996)need common intentions, most prominently, theintention to carry out a communicative task felic-itously.
Frequently, the vehicle for these types ofintentions are (partial) plans.
Plans can also be in-dividual or shared.
In (Dial 1) for example, theRouter has an individual plan how to best map outhis ride and the intention to communicate it to the4We will end up with a mixture of German gestureand English wording here.
However, for didactic purposes(sketch the main ideas) this seems acceptable.
Sometimes wewill simulate German constructions in English.88Follower.
The Follower in turn intends to let theRouter control her beliefs.
Both have the collec-tive intention to enable the Follower to follow theRouter?s route.
Information presupposed or gener-ated is contained in the discourse situation which,in PTT, is just a normal situation with objects andevents, i.e., a DRS.Conversational participants have command overinformation states.
An information state is up-dated whenever a new event is perceived, includ-ing events such as sub-sentential utterances, andnon-verbal events such as gestures or nods.
Hencethe possibility is already implemented in PTT tomodel accumulation of information due to ges-ture.
Information common to the dialogue par-ticipants can be considered as grounded by de-fault.
This assumption connects PTT with otherdialogue theories, for example Clark?s (cf.
Clarkand Marshall, 1981, Clark and Schaefer, 1989)and Traum?s (Traum 2009).
Acknowledged in-formation is at the heart of the grounding pro-cess.
What is grounded is mutually believed ce-teris paribus.
Therefore, grounded information ispart of the pragmatic machinery driving a dialogueforward (Rieser 2009).
Grounding acts are takenas meta-discoursive devices and not included indiscourse units proper.
Besides beliefs and inten-tions we have obligations as mental attitudes.
InPTT every conversational action induces an obli-gation on the participant indicated to address thataction.Information states raise the question of howchanges of information are brought about on thebasic grammatical level, viz.
the interpretationof incrementally produced locutionary acts.
Thegrammar in which syntactic and semantic interpre-tation is implemented is LTAG (Abeille?
& Ram-bow (eds), 2000).
LTAG is a tree-grammar en-coding syntactic projections which do the dutyof, say, HPSGs rules, principles and constraints.Nodes and projecting leaves are decorated with se-mantic information based on Compositional DRTas developed in (Muskens, 1996, 2001).
A spe-cific trait of PTT is working with semantic non-monotonicity at all compositional levels: PTT hy-pothesizes that semantic computation is the resultof defeasible inferences over DRSs obtained con-catenating updates of single contributions.
Thesedefault inference rules have the effect of seman-tic composition rules.
Due to the impact of inter-preted LTAG one can say that PTT is well foundedin a bottom up fashion.
Especially the defaultmechanism of PTT is used to make it a workableinterface for speech and gesture (cf.
7.2 - 7.4).5 Setting up the Speech-gestureInterface: Typology and PartialOntologyAs mentioned, this paper is based on the sys-tematic annotation of SAGA carried out over theyears 2007-2009 (Rieser 2009).
Like many ges-ture researchers we assume that the semantic andpragmatic centre of a gesture is its stroke.
Thestroke overlaps as a rule with part of a com-plex constituent, for example the head or the log-ical subject.
The range of speech-gesture over-lap usually marks the functional position wherethe gestures meaning has to be merged into thespeech content.
Technically, the annotation isan ELAN-grid.
From the annotation, a set ofgesture types has been factored out in the fol-lowing way (Rieser 2009).
AGENCY5 is in-stalled as a root feature dominating the role fea-tures ROUTER and FOLLOWER.
Next come theRouter?s and the Follower?s LEFT and RIGHTHAND and BOTH their HANDS.
HANDEDNESSin turn is mapped onto single annotation fea-tures like HANDSHAPE, WRISTMOVEMENT,PATHOFWRISTMOVEMENT etc.
Bundles offeatures make feature CLUSTERs which yieldclasses of objects like curved, straight etc.
en-tities.
These build up SHAPES of different di-mensions:6 ABSTRACT OBJECTs of 0 DIMEN-SION and LINEs, one-dimensional entities of dif-ferent curvature.
Among the two dimensionalentities are LOCATIONs, RECTANGLEs, CIR-CLEs7 etc.
Then three dimensional sorts comeup: CUBOIDSs CYLINDERs, PRISMs and so on.In the end we get COMPOSITEs of SHAPES,for example a BENT LINE in a SPHERE, andSEQUENCES OF COMPOSITEs.8 The central is-sue of ?How does a gesture acquire meaning??
isanswered in the following way: A gesture type ismapped onto a partial ontology description, a stip-ulation encoding the content attributed to a gestureby raters.
As a rule, gesture content is underspec-5Gesture types, organised in an inheritance hierarchyworking with defaults (cf.
Rieser 2009), are written in CAP-ITAL ITALICS.6In the following geometry terms are used mnemonically.7SHAPEs can in general be fully developed or come inSEGMENTs.
We do not deal with SEGMENTs here.8SEQUENCES encode evolution of SHAPEs in time.89ified and will be completed to some extent wheninterfacing with verbal meaning.
As an exampleof a gesture type and its partial ontology, see e.g.TwoHandedPrismSegment1 and ?Partial Ontolo-gyTwoHandedPrismSegment1?
in Appendix A.6 Setting up the Speech-gestureInterface: Levels of InteractionOur starting point is the hypothesis detailed in(Rieser 2008) that a genuine understanding of di-alogues like (Dial 1) requires integration of multi-modal meaning at different levels of discourse,from fine grained lexical definitions up to rhetor-ical relations.
In the rest of the paper, we willspecify how information from spoken utterancesmerges with information from gestures, using(Dial 1) as an example.
Omitting the two BEATSon that is [then], we have the following gestureson the Router?s side (see stills in Appendix B):6.1 the PRISM SEGMENT covering the town hall; cf.
stillTwo-Handed-Prism-Segment6.2 the DRAWN U-shape overlapping the adjective U-shaped; still One-Handed-U-Shape6.3 the PRISM SEGMENT affiliated to [practically] lookinto it; still Two-Handed-U-Shaped-Prism-Segment6.4 the two-handed U-shaped PRISM SEGMENT goingwith and closes in the rear; stills Two-Handed-Prism-Segment-2A and 2B.The Follower uses a variant of6.5 the Router?s PRISM SEGMENT in (6.4) followed byOK; still Two-Handed-Prism-Segment-3.The key observation from Rieser (2009) is thatgestures interact with verbal contributions at dif-ferent levels.
(6.1) to (6.4) must be integrated atthe level of the semantic interpretation of LTAG.
(6.3) is involved since the stroke covers three con-stituents in the German wording, the modal ad-verb [practically], the pronoun it, and the separa-ble prefix da rein/into of the verb blickst/you look.We will develop a simplified solution here usingthe ?verb?
look-into.
Similarly, in (6.4) the ges-ture contains information relevant for closes in therear, i.e.
for the whole VP.
The gesture informa-tion has to be integrated into the Router?s dialogueacts at the interface points mentioned.
Therefromseveral side issues arise, for example the treatmentof anaphora across Router?s or Follower?s contri-butions.
In (Dial 1) the Follower uses gesturalinformation only to acknowledge.
It is a multi-modal example of acknowledging by imitating theRouter?s multi-modal acts.
Her gesture and theOK form a kind of ?complex acknowledgement?.This way the Router?s contributions (6.2) to (6.4)and the Follower?s contribution (6.5) show the in-teractive role of gesture, more specifically, gesturecontent in its use for grounding.
We will brieflycomment upon that in section 8.7 Using PTT as an Interface for VerbalMeaning and Gestural Meaning7.1 The verbal part of (Dial 1)According to PTT, the discourse situation after theverbal updates brought about by (Dial 1) would beas follows.9 (We only represent one aspect of thecontent of the initial utterances of (Dial 1).
):[DU0, DU1, DU2, DU3, DU4, DU5 |DU0 is [.
.
.
K1, .
.
.
|K1 is [b1 | building(b1), large(b1)],.
.
.
]DU1 is [u2.1, K2, ce2.1 |u2.1: utter(Router,?Das ist das Rathaus?
),sem(u2.1) is K2,K2 is [th1, tnhl |th1 is ?y1.
K1; [ | y1 is b1],tnhl is ?u.
[ | town hall (u)],th1 is tnhl,ce2.1: assert(Router, Follower, K2),generate(u2.1, ce2.1)],DU2 is [u2.2, K4, ce2.2 |u2.2: utter(Router, ?das ist einU-f?rmiges Geb?ude.?
),sem(u2.2) is K4,K4 is [th2 | th2 is ?y2.
K5; [ |s: y2 is b1],building(th2), U-shaped(th2),K5 is K1],ce2.2: assert(Router, Follower, K4),generate(u2.2, ce2.2)],DU3 is [u2.3, K7, ce2.3|u2.3: utter(Router, ?Du blickst da rein?
),sem(u2.3) is K7,K7 is [th3, s1 | th3 is ?y3.
K8; [|s: y3 is b1],s1: look-into(Follower, th3),10K8 is K4],ce2.3: assert(Router, Follower, K7),generate(u2.3, ce2.3)],DU4 is [u2.4, K9, ce2.4|u4: utter(Router, ?es hat vornezwei Buchtungen und geht hinten zus.
dann?
),sem(u2.4) is K9,K9 is [th4,bu1,bu2,s2,s3,s4,s5,s6,re1,re2 |th4 is ?y4.
K10;[ | y4 is th3], K10 isK7,bulge(bu1), bulge(bu2),s2: has(th4, bu1),9Abbreviations used in the PTT-fragment: The prefixesare usually followed by a number n ?
0.
DU = discourseunit, ce = conversational event, K = DRS, u = utterance, sem= semantic function, x, y, z .
.
.
= DRs, e: event, s: = situation.In the DRSs ?,?
stands for conjunction an ?;?
between DRSsfor composition of DRSs.90s3: has(th4, bu2),to-front-of(bu1, th4),to-front-of(bu2, th4),rear(re1), s4: has(bu1,re1), rear(re2)11,s5:has(bu2, re2),s6: meet(re1, re2)],ce2.44: assert(Router, Follower, K9),generate(u2.4, ce2.4)].The model of anaphora resolution accounting forthe anaphoric cases is developed in (Poesio andRieser, submitted 2009 b).
The anaphoric Das/thisin DU1 depends on the discourse entity a largerbuilding introduced at the beginning of the con-versation in DRS K1: K1 is the resource situationfor the anaphoric definite.
The second das/thisstill depends on the same resource situation.
Thepronouns, however, behave differently: Pronounda/there in DU3 takes up the antecedent a U-shaped building, whereas the es/it in DU4 in turnrefers to the it in DU3.
Observe that the verbal partof (Dial 1) alone would already specify the inter-pretation completely: nothing essential is missing.As it will become clear below, what gestures do inthis example is to add details to the verbally deter-mined models and restrict the model set.7.2 Tying in Gestures with UtterancesWhat we have got so far is a PTT-representationof the verbal part of (Dial 1).
We now move onto how the information coming from the Router?sgestures gets integrated with the verbal informa-tion ?
in particular, how this integration can takeplace below the sentential level.
Our accountbuilds on two key ideas from PTT.
First of all,gestures are part of the discourse situation ?
i.e.,the occurrence of gestures is recorded in the infor-mation state?s representation of the discourse sit-uation.
Second, every occurrence of a sentenceconstituent counts as a conversational event ?
aMICRO CONVERSATIONAL EVENT (MCE).With these assumptions in place, the interactionof speech meaning and gesture meaning ?
howthe two types of meanings combine to specify theoverall meaning of a contribution ?
can be spec-ified using the same mechanisms that specify themeaning of MCEs: i.e., with (prioritized) defaultsin the sense of (Reiter, 1980, Brewka 1989).
One10Observe that the town-hall and the U-shaped building arethe same.11Observe that the gesture dynamically shapes two rearswhich meet.example of a default specifying semantic com-position is the BINARY SEMANTIC COMPO-SITION (BSC) developed in (Poesio to appear,Poesio and Rieser submitted a) to specify the de-fault way in which MCEs meanings can be derivedfrom the meanings of their constituents.
(We usethe notation > to indicate defeasible inference, ?to indicate ?dominated by?.
)BSC: u1?u, u2?u, sem(u1) is ?????
sem(u2) is??
, complete(u,u1,u2)> sem(u) is ?(?
)BSC can however be overridden in a numberof circumstances: most notably, when anaphorainterpretation processes identify a referent for adefinite description like uNP1: utter(?the build-ing?
), in which case sem(uNP1) will be the refer-ent as opposed to a set of properties; or in casesof metonymy such as those studied by Nunberg(2004), in which the meaning of a MCE may bederived even more indirectly.
We hypothesize thatthe integration of utterance meaning and gesturemeaning is specified by interface defaults thatmay override the general meaning in a similarway by enriching the normal meaning of MCEs.We provide several examples of interface defaultsbelow.
For reasons of space, we only specifythe results of default inference, without provid-ing full derivations of the multi-modal meanings.For the gestures only the semantics12 is speci-fied, abstracted from the description of the par-tial ontology (cf.
Appendix A for details).
Utter-ance meaning then operates on the partial ontol-ogy information.
MM abbreviates ?multi-modal?;?lex-entry?
means the word-form at stake, ?lex-definition?
means an explict dictionary definitionfor the word, for example in the style of the OED,cast into PL1.7.3 The Interface DefaultsThe general heuristic strategy for setting up inter-face defaults designed to combine verbal mean-ing and gesture meaning is to probe into the PTTstructure as deep as you need in order to fit in thegestural content properly.
Gestures may be rele-vant at any level of discourse, as shown in (Rieser,2008) and demonstrated below; this means thatsometimes gestural content has to be stored ?deep12This is due to the fact that we do not integrate gesturesinto the discourse situation here.
If these are integrated onewill use their type description as syntax in AVM format.
Ges-tures do not have the normal category syntax.91in?
the lexical definition of a word, at other timesone has to remain on the top level of semanticcomposition or even follow up the contributionsproduced so far.
The interface defaults mostly fol-low the general schema:?
-prefix mentioning the open parameters + lex-icon definition + open parameters applied to iconicmeaning = ?
-abstracted partial ontology descrip-tion where the ?
-bound parameters secure bind-ing.An exception to this is (7.3.5.1) which uses thenotion of satisfaction (see stills in Appendix B).7.3.1 The PRISM SEGMENT aligned with[the] town hall (6.1).
To begin with, gesturalmeaning can enrich the meaning of a nominalutterance.
The interface default allowing this iscalled Noun meaning extended (NMExt)13NMExt: Noun(u), sem(u) is ?x lex-definition(x), u?u?, N?(u?
), u overlaps g,gesture(g), iconic-meaning(g) is ?p partialontology(p)>sem(u?)
is ?x (lex-definition(x)) iconic-meaning(g)(x)For instance in the dialogue under considerationlex-definition is the predicate ?large building usedfor the administration of local government?
abbre-viated as ?
?P?x [[ |s: large building(x), used forthe administration of local government(x)]; P(x)]?and the Partial Ontology TwoHandedPrismSeg-ment1 from the Appendix A, resulting in the fol-lowing meaning for the utterance of ?town hall?accompanied by the gesture:(7.3.1.1) ?x [ ls, rs, loc|s: large building(x), usedfor the administration of local government(x),side(ls, x), left(ls, Router), side(rs, x), right(rs,Router), location(loc, x)]Observe that the fine-grained local information isprovided by the gesture.7.3.2 The DRAWN U-shape overlapping the ad-jective U-shaped is an example of gesture enrich-ing an adjectival meaning through the interface de-fault Adjective meaning extended (AdjMExt)AdjMExt: Adjective(u), sem(u) is ?P?x [|lex-entry(x), P(x)], u?u?, N?(u?
), u overlaps g, ges-ture(g), iconic-meaning(g) is ?p partial ontol-ogy(p)> sem(u?)
is ?P?Q?x([|lex-entry(x), P(x)];Q(x)) iconic- meaning(g)(x).13?
?p partial ontology (p)?
in NMExt and the followingdefaults is used in the following way: The expression ?partialontology?
refers to information from the partial ontology listin the Appendix A.
What has to be chosen can be seen fromthe application of the default below.Using AdjMExt and the meaning of the gestureOneHanded-U-shape in the Partial Ontology weobtain (7.3.2.1) as an enriched meaning for ?U-shaped?, ???
denoting mereological composition:(7.3.2.1) ?Q?x([|U-shaped(x), ?us(strai- ght-line(lr, us), arc(lb, us), straight-line(ll, us), us =lr ?
lb ?
ll )(x)]; Q(x))After fitting in the noun modified by the multi-modal content into position ?Q?, the DRs will haveto be correctly bound.Observe that we could apply (NMext) and (Ad-jMext) iteratively to arrive at a complex MMNom-meaning.7.3.3 The PRISM SEGMENT affiliated to[practically] look into it is computed usingthe interface default Verb meaning extended(VMExt).VMExt: VP(u), V(u1), NP(u2), u1?
u, u2?
u,sem(u1) is ?P?x([|s: lex-definition(x), P(x)], uoverlaps g, gesture(g), iconic-meaning(g) is ?ppartial ontology(p)> sem(u) is ?P?x([|s: lex-definition(x), P(x)])iconic-meaning(g)(x)VMExt gives us, again using the informa-tion from the Partial Ontology TwoHanded-U-shapedPrism from the Appendix:(7.3.3.1) ?x([|s: focus(agent, x), space(x),bounded(x), empty(x), ?p[hl, ls, lel, fs, hr, rs,ler, d| prism(p), height(hl, ls), left-side(ls, p),front-side(fs, p), left(ls, Router), height(hr, rs),right-side(rs, p), length(ler, rs), right(rs, Router),length(lel, ls), distance(d, ls, lr), lel = ler](x)])Again we see that fine-grained information isprovided by the gesture, especially the prag-matic anchoring of the space looked into from theRouter?s position.7.3.4 Finally, the two-handed U-shaped PRISMSEGMENT going with and closes in the rearneeds a default VP meaning extended (VPMex-tended).
The gesture information is distributedamong the verb ?closes?
and the PP ?in the rear?,the assumption being that the object closing doesso at a particular location which is part of the ob-ject itself.
So we have:VPMExt: V(u) ?
VP(uph1), P(u) ?
PP(uph2),Det(u) ?
NP(uph3), Nom(uph4) ?
NP(uph3),PP(uph2) ?
VP(uph1), sem(u) is ?P?x([|lex-definition(x)]; P(x)), u overlaps g, gesture(g),iconic-meaning(g) is ?p partial ontology(p)> sem(uph1) = ?P?x([|lex-definition(x), P(x)];iconic-meaning(g)(x)The default using Appendix A, Partial On-tology TwoHanded-U-shapedPrism, generates thefollowing MM meaning:92(7.3.4.1) ?x([ |s: close(x), at(s, loc), prism(leftp),prism(rightp), part(leftp, x), part(rightp, x), sec-tion(sectl, leftp), leftside(lefts, leftp), length(ll,lefts), left(lefts, Router), section(sectr, rightp),rightside(rights, rightp), frontside(fronts, rightp),bent(rightp), meet(lefts, rights, loc), right(rights,Router), parallel(lefts, rights), distance(d, lfts,rhts)]).7.3.5 The Follower?s U-shaped gesture: So far,gesture meaning constrained word meanings orconstituent meanings.
In contrast, the Follower?sU-shaped gesture invades dialogue structure.
TheFollower?s reply has two steps.
Her iconic ges-ture yields a predicate U-shaped in much the sameway as the Router?s contribution in DU2 and DU4does.
This is combined with a DR anaphoricallylinked to the Router?s preceding its and thats.
Thegesture in turn takes up the Router?s U-shapesfrom DU2 and DU4.
So we get an anaphorarelated to antecedent multi-modal information.14Her ?OK?
then simply acknowledges her ownDU5 filled up.
Acknowledgement of the Router?scontributions is achieved indirectly.
In order tomodel all that, we have to Hook up the Gesture?sContent with a DR.
This is simply(7.3.5.1) ?p(iconic-meaning(p))DR for somepreceding discourse referent DR satisfyingiconic-meaning.The relevant iconic meaning is taken from Par-tial Ontology TwoHandedPrismSegment3: sec-tion(sect, p), leftpart(lftp, p), lengthl(lftp),left(leftp, Follower), rightpart(rtp, p), right(rightp,Follower), lengthr(rtp), lftp = rtp, p = lftp ?
rtp.7.4 A Gestural Dialogue Act of AssertionConcerning dialogue structure, we have concen-trated on the verbal part of (Dial 1) in 7.1.
In theSAGA corpus there are many data showing howdialogue structure interfaces with gesture mean-ing.
In 7.3.5 a default for the follower?s U-shapedgesture was given.
Its embedding into the PTT-description of (Dial 1) is shown in DU5 below:(7.4) DU5 is [g1, K10|g1: gesticulate(Follower, Router, U-shape),sem(g1) is K10,K10 is [ |s: th5 is th4, ?p(section(sect, p),leftpart(lftp, p), lengthl(lftp),left(leftp, Follower), rightpart(rtp, p),right(rightp, Follower), lengthr(rtp),lftp is rtp, p is lftp ?
rtp)(th5))]ce5: assert(Follower, Router, K10),generate(g1, ce5)],],14These anaphorical relations are not reconstructed herebut delegated to a follow-up paper.
[ce6, u6|u6: utter(Follower,?OK?
),ce6: ack(Follower, DU5),textbfgenerate(u6, ce6)]]In the multi-modal dialogue passage we have?gesticulate?
instead of ?utter?.
The semantics, us-ing the default (7.3.5.1) ?Hook up the Gesture?sContent with a DR?
and material from AppendixA is provided in the standard way by K10.
It is as-sumed that gestural content can be generated andasserted.
The Follower?s acknowledgement is asort of self-acknowledgement that percolates upthrough anaphora.8 Grounding by Gesture: a GenuineCase of Gestural AlignmentThe different defaults, Noun-meaning ex-tended (NMextended), Adjective meaningextended (AdjMextended), Verb meaning ex-tended (VMextended), VP meaning extended(VPMextended) and Hook up the Gesture?sContent with a DR, clearly indicate that integra-tion of gesture meaning has to operate on levels ofdifferent grain.
Gesture can operate on a sub-wordlevel if one has to attach its meaning to parts of alexical definition, on the word level, on the levelof constituents, and, as a consequence of all that,on specific dialogue acts.
Furthermore, we haveseen gesture at two inter-propositional levels atwork, at the interface among the contributions ofone agent (see Router?s contributions which areall ?united?
by communicating the appearance ofthe town hall) and at the interface among contri-butions of different agents (Router-Follower).TheFollower acknowledges by imitating gestures ofthe Router; this is a genuine case of gestural align-ment.
Alternatively, she could also acknowledgeverbally, uttering ?U-shaped?
but she chooses agestural content.
Obviously, speakers think thatthis works.
Her ?OK?
furthermore shows thatverbal and gestural means can work in tandem.So, in the end, the U-shape of the town hall isrooted in the common ground by default and theRouter can continue with describing the routeleading to the next landmark.AcknowledgementsSupport by the SFB 674, Bielefeld University,is gratefully acknowledged.
We also want tothan three anonymous reviewers for carful reading93and suggestions for improvement.
Hannes Rieserwants to thank Florian Hahn for common work ongesture typology starting in 2007.ReferencesAbeill?, A & Rambow, O.
(eds) 2004.
Tree AdjoiningGrammars.
CSLI Publ.
Stanford, CA.Bergmann, K., Fr?hlich, C., Hahn, F., Kopp, St., L?ck-ing, A. and Rieser, H. June 2007.
Wegbeschreibung-sexperiment: Grobannotationsschema.
Univ.
Biele-feld, MS.Bergmann, K., Damm, O., Fr?hlich, C., Hahn, F.,Kopp, St., L?cking, A., Rieser, H. and Thomas,N.
June 2008.
Annotationsmanual zur Gestenmor-phologie.
Univ.
Bielefeld, MS.Brewka, G. 1989.
Nonmonotonic reasoning: from the-oretical foundation towards efficient computation.Hamburg, Univ., Diss.Clark, H. H. 1996.
Using Language.
Cambridge Uni-versity Press.Clark, H. H. and Marshall, C. R. 1981.
Definite Ref-erence and Mutual Knowledge.
In A. K. Joshi, B.Webber, and I.
A.
Sag (eds.
),Elements of DiscourseUnderstanding.
CUPClark, H. H. and Schaefer, E. F. 1989.
Contributing toDiscourse.
Cognitive Science, 13, 259-294.Muskens, R. 1996.
Combining Montague Semanticsand Discourse Representation.
Linguistics and Phi-losophy 19, pp.
143-186.Muskens, R. 2001.
Talking about Trees and Truth-conditions.
Journal of Logic, Language and Infor-mation, 10(4), pp.
417-455.Nunberg, G. 2004.
The Pragmatics of Deferred Inter-pretation.
In: Horn, L.R.
and Ward, G.: The Hand-book of Pragmatics.
Blackwell Publishing Ltd.Poesio, M. to appear.
Incrementality and underspec-ification in semantic interpretation.
CSLI Publica-tions.Poesio, M. February 2009.
Grounding in PTT.
Talkgiven at Bielefeld Univ.Poesio, M. and Rieser, H. submitted a. Completions,Coordination, and Alignment in Dialogue.Poesio, M. and Rieser, H. submitted b. Anaphora andDirect Reference: Empirical Evidence from Point-ing.Poesio, M. and Traum, D. 1997.
?Conversational Ac-tions and Discourse Situations?, Computational In-telligence, v. 13, n.3, 1997, pp.1- 45.Rieser, H. 2008.
Aligned Iconic Gesture in DifferentStrata of MM Route-description Dialogue.
In Pro-ceedings of LONdial 2008, pp.
167-174Rieser, H. 2009.
On Factoring out a Gesture Typologyfrom the Bielefeld Speech-And- Gesture-AlignmentCorpus.
Talk given at the GW 2009, ZiF Bielefeld,to appear in the Proceedings of GW 2009.Traum, D. 2009.
Computational Models of Ground-ing for Human-Computer Dialogue.
Talk given atBielefeld Univ., February 200994AppendicesAppendix A: Gesture Types and Description of Partial OntologyDue to limited space gesture types and ontology descriptions are only partially characterised.???????????????????????
?TwoHandedPrismSegment 1R.G.Left.HandShapeShape loose B spreadR.G.Left.HandPalmDirection PDN/PTRR.G.Left.BackOfHandDirection BABR.G.Left.Practice grasping-indexingR.G.Left.Perspective speakerR.G.Right.HandShapeShape loose B spreadR.G.Right.HandPalmDirection PDN/PTLR.G.Right.BackOfHandDirection BABR.G.Right.Practice grasping-indexingR.G.Right.Perspective speakerR.Two-handed-configuration TTR.Movement-relative-to-other-hand 0????????????????????????????????
?Partial Ontology TwoHandedPrismSegment 1R.G.Left.HandShapeShape-loose B spread side(ls, p)R.G.Left.HandPalmDirection-PDN/PTR left(ls, Router)R.G.Right.HandShapeShape-loose B spread side(rs, p)R.G.Right.HandPalmDirection-PDN/PTL right(rs, Router)R.Two-handed-configuration-TT location(loc, p)???????????????????????
?OneHanded-U-shapeR.G.Right.HandShapeShape GR.G.Right.PalmDirection PDN/PTL>PDN/PTB>PDNR.G.Right.BackOfHandDirection BAB/BTL>BAB/BDN>BAB/BDN/BTLR.G.Right.PathOfWristLocation ARCR.G.Right.WristLocationMovementDirectio MR>MF>MLR.G.Right.Extent MEDIUMR.G.Right.Practice drawingR.G.Right.Pespective speaker?????????????????????
?Partial Ontology OneHanded-U-shapeR.G.Right.PathOfWristLocation-ARC U-shape(us)R.G.Right.WristLocation straight-line(lr, us) ?MovementDirection-MR>MF>ML bent-line(lb, us) ?straight-line(ll, us)???????????????????????????????????????????
?TwoHandedPrismSegment 2R.G.Left.HandShapeShape B spreadR.G.Left.HandPalmDirection PTRR.G.Left.BackOfHandDirection BAB/BUP > BABR.G.Left.PathOfWristLocation LINER.G.Left.WristLocation MFMovementDirectionR.G.Left.Practice shaping-modellingR.G.Left.Perspective speakerR.G.Right.HandShapeShape B spreadR.G.Right.HandPalmDirection PTLR.G.Right.BackOfHandDirection BAB/BUP > BABR.G.Right.PathOfWristLocation LINER.G.Right.WristLocation MFMovementDirectionR.G.Right.Practice shaping-modellingR.G.Right.Perspective speakerR.Two-handed-configuration PFR.Movement-relative-to-other-hand SYNC???????????????????????????????????????????????????????????????????????
?Partial Ontology TwoHandedPrismSegment 2R.G.Left.HandShapeShape-B spread hight(hl, ls)R.G.Left.HandPalmDirection-PTR leftside(ls, p)?
prism(p)R.G.Left.PathOfWristLocation-LINE length(lel, ls)R.G.Left.WristLocation frontside(fs, p)MovementDirection-MFR.G.Left.Perspective-speaker left(ls, speaker)R.G.Right.HandShapeShape-B spread hight(hr, rs)R.G.Right.HandPalmDirection-PTL rightside(rs, p)?
prism(p)R.G.Right.PathOfWristLocation-LINE length(ler, rs)R.G.Right.WristLocation frontside(fs, p)MovementDirection-MFR.G.Right.Perspective-speaker right(rs, speaker)R.Two-handed-configuration-PF distance(d, ls, lr)R.Movement-relative-to-other-hand-SYNC lel = ler????????????????????????????????????????????????????????????????????????????
?TwoHanded-U-shapedPrismR.G.Left.HandShapeShape small CR.G.Left.HandPalmDirection PABR.G.Left.BackOfHandDirection BAB/BTRR.G.Left.PathOfWristLocation LINER.G.Left.WristLocation MFMovementDirectionR.G.Left.Practice shapingR.G.Left.Perspective speakerR.G.Right.HandShapeShape small CR.G.Right.HandPalmDirection PAB/PTL>PTL>PTB/PTLR.G.Right.BackOfHandDirection BAB/BTR>BAB>BAB/BTLR.G.Right.PathOfWristLocation LINE>LINER.G.Right.WristLocation MF>MLMovementDirectionR.G.Right.Practice shapingR.G.Right.Perspective speakerR.Two-handed-configuration BHAR.Movement-relative-to-other-hand SYNC???????????????????????????????????????????????????????????????????
?Partial Ontology TwoHanded-U-shapedPrismR.G.Left.HandShapeShape-small C section(sectl, leftp)R.G.Left.PathOfWristLocation-LINE leftside(lefts, leftp)R.G.Left.WristLocation length(ll, lefts)MovementDirection -MFR.G.Left.Perspective-speaker left(lefts, speaker)R.G.Right.HandShapeShape-small section(sectr, rightp)R.G.Right.PathOfWristLocation-LINE>LINE rightside(rights, rightp) ?frontside(fronts, rightp)R.G.Right.WristLocation>ML bent(rightp) ?MovementDirection-MF meet(lefts, rights)R.G.Right.Perspective-speaker right(rights, speaker)R.Movement-relative-to-other-hand-SYNC parallel(lefts, rights) ?distance(d, lefts, rights)??????????????????????????95?????????????????????????????????????????
?TwoHandedPrismSegment 3R.G.Left.HandShapeShape CR.G.Right.HandPalmDirection PDN/PTR>PAB/PUPR.G.Left.BackOfHandDirection BAB>BTL/BUPR.G.Left.PathOfWristLocation ARCR.G.Left.WristLocationMovementDirection ML>MBR.G.Left.Practice shapingR.G.Left.Perspective speakerR.G.Right.HandShapeShape CR.G.Right.HandPalmDirection PDN/PTL>PAB/PUPR.G.Right.BackOfHandDirection BAB>BTR/BUPR.G.Right.PathOfWristLocation ARCR.G.Right.WristLocationMovementDirection MR>MBR.G.Right.Practice shapingR.G.Right.Perspective speakerR.Two-handed-configuration BHAR.Movement-relative-to-other-hand Mirror-Sagittal?????????????????????????????????????????????????????????????
?Partial Ontology TwoHandedPrismSegment 3R.G.Left.HandShapeShape section(sect, p)R.G.Left.PathOfWristLocation leftpart(lftp, p)R.G.Left.WristLocationMovementDirection lengthl(lftp)R.G.Left.Perspective left(leftp, speaker)R.G.Right.HandShapeShape section(sect, p)R.G.Right.PathOfWristLocation rightpart(rtp, p)R.G.Right.WristLocationMovementDirection lengthr(rtp)R.G.Right.Perspective speakerR.Two-handed-configuration lftp = rtpR.Movement-relative-to-other-hand p = lftp ?
rtp???????????????????
?Appendix B: Figure 1(a) The Router on his trip.
(b) The site traversed by theRouter.
The U-shaped buildingis the town hall(c) Fig.
1c shows the town hallas described and gestured bythe Router.
(d) Two-Handed-Prism-Segment-1(e) One-Handed-U-Shape (f) Two-Handed-U-Shaped-Prism-Segment(g) Two-Handed-Prism-Segment-2A(h) Two-Handed-Prism-Segment-2B(i) Two-Handed-Prism-Segment-3Figure 1: The SAGA Setting96
