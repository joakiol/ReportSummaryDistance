Proceedings of ACL-08: HLT, pages 986?993,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsBetter Alignments = Better Translations?Kuzman GanchevComputer & Information ScienceUniversity of Pennsylvaniakuzman@cis.upenn.eduJoa?o V. Grac?aL2F INESC-IDLisboa, Portugaljavg@l2f.inesc-id.ptBen TaskarComputer & Information ScienceUniversity of Pennsylvaniataskar@cis.upenn.eduAbstractAutomatic word alignment is a key step intraining statistical machine translation sys-tems.
Despite much recent work on wordalignment methods, alignment accuracy in-creases often produce little or no improve-ments in machine translation quality.
Inthis work we analyze a recently proposedagreement-constrained EM algorithm for un-supervised alignment models.
We attempt totease apart the effects that this simple but ef-fective modification has on alignment preci-sion and recall trade-offs, and how rare andcommon words are affected across several lan-guage pairs.
We propose and extensively eval-uate a simple method for using alignmentmodels to produce alignments better-suitedfor phrase-based MT systems, and show sig-nificant gains (as measured by BLEU score)in end-to-end translation systems for six lan-guages pairs used in recent MT competitions.1 IntroductionThe typical pipeline for a machine translation (MT)system starts with a parallel sentence-aligned cor-pus and proceeds to align the words in every sen-tence pair.
The word alignment problem has re-ceived much recent attention, but improvements instandard measures of word alignment performanceoften do not result in better translations.
Fraser andMarcu (2007) note that none of the tens of paperspublished over the last five years has shown thatsignificant decreases in alignment error rate (AER)result in significant increases in translation perfor-mance.
In this work, we show that by changingthe way the word alignment models are trained andused, we can get not only improvements in align-ment performance, but also in the performance ofthe MT system that uses those alignments.We present extensive experimental results evalu-ating a new training scheme for unsupervised wordalignment models: an extension of the Expecta-tion Maximization algorithm that allows effectiveinjection of additional information about the desiredalignments into the unsupervised training process.Examples of such information include ?one wordshould not translate to many words?
or that direc-tional translation models should agree.
The gen-eral framework for the extended EM algorithm withposterior constraints of this type was proposed by(Grac?a et al, 2008).
Our contribution is a large scaleevaluation of this methodology for word alignments,an investigation of how the produced alignments dif-fer and how they can be used to consistently improvemachine translation performance (as measured byBLEU score) across many languages on training cor-pora with up to hundred thousand sentences.
In 10out of 12 cases we improve BLEU score by at least 14point and by more than 1 point in 4 out of 12 cases.After presenting the models and the algorithm inSections 2 and 3, in Section 4 we examine howthe new alignments differ from standard models, andfind that the newmethod consistently improves wordalignment performance, measured either as align-ment error rate or weighted F-score.
Section 5 ex-plores how the new alignments lead to consistentand significant improvement in a state of the artphrase base machine translation by using posteriordecoding rather than Viterbi decoding.
We proposea heuristic for tuning posterior decoding in the ab-sence of annotated alignment data and show im-provements over baseline systems for six different986language pairs used in recent MT competitions.2 Statistical word alignmentStatistical word alignment (Brown et al, 1994) isthe task identifying which words are translations ofeach other in a bilingual sentence corpus.
Figure2 shows two examples of word alignment of a sen-tence pair.
Due to the ambiguity of the word align-ment task, it is common to distinguish two kinds ofalignments (Och and Ney, 2003).
Sure alignments(S), represented in the figure as squares with bor-ders, for single-word translations and possible align-ments (P), represented in the figure as alignmentswithout boxes, for translations that are either not ex-act or where several words in one language are trans-lated to several words in the other language.
Possi-ble alignments can can be used either to indicatedoptional alignments, such as the translation of anidiom, or disagreement between annotators.
In thefigure red/black dots indicates correct/incorrect pre-dicted alignment points.2.1 Baseline word alignment modelsWe focus on the hidden Markov model (HMM) foralignment proposed by (Vogel et al, 1996).
This isa generalization of IBM models 1 and 2 (Brown etal., 1994), where the transition probabilities have afirst-order Markov dependence rather than a zeroth-order dependence.
The model is an HMM, where thehidden states take values from the source languagewords and generate target language words accordingto a translation table.
The state transitions depend onthe distance between the source language words.
Forsource sentence s the probability of an alignment aand target sentence t can be expressed as:p(t,a | s) =?jpd(aj |aj ?
aj?1)pt(tj |saj ), (1)where aj is the index of the hidden state (source lan-guage index) generating the target language word atindex j.
As usual, a ?null?
word is added to thesource sentence.
Figure 1 illustrates the mapping be-tween the usual HMM notation and the HMM align-ment model.2.2 Baseline trainingAll word alignment models we consider are nor-mally trained using the Expectation Maximizations1 s1s2 s3we knowthe waysabemos       el       camino      nullusual HMM word alignment meaningSi (hidden) source language word iOj (observed) target language word jaij (transition) distortion modelbij (emission) translation modelFigure 1: Illustration of an HMM for word alignment.
(EM) algorithm (Dempster et al, 1977).
The EMalgorithm attempts to maximize the marginal likeli-hood of the observed data (s, t pairs) by repeatedlyfinding a maximal lower bound on the likelihood andfinding the maximal point of the lower bound.
Thelower bound is constructed by using posterior proba-bilities of the hidden alignments (a) and can be opti-mized in closed form from expected sufficient statis-tics computed from the posteriors.
For the HMMalignment model, these posteriors can be efficientlycalculated by the Forward-Backward algorithm.3 Adding agreement constraintsGrac?a et al (2008) introduce an augmentation of theEM algorithm that uses constraints on posteriors toguide learning.
Such constraints are useful for sev-eral reasons.
As with any unsupervised inductionmethod, there is no guarantee that the maximumlikelihood parameters correspond to the intendedmeaning for the hidden variables, that is, more accu-rate alignments using the resulting model.
Introduc-ing additional constraints into the model often re-sults in intractable decoding and search errors (e.g.,IBM models 4+).
The advantage of only constrain-ing the posteriors during training is that the modelremains simple while respecting more complex re-quirements.
For example, constraints might include?one word should not translate to many words?
orthat translation is approximately symmetric.The modification is to add a KL-projection stepafter the E-step of the EM algorithm.
For each sen-tence pair instance x = (s, t), we find the posterior987distribution p?
(z|x) (where z are the alignments).
Inregular EM, p?
(z|x) is used to complete the data andcompute expected counts.
Instead, we find the distri-bution q that is as close as possible to p?
(z|x) in KLsubject to constraints specified in terms of expectedvalues of features f(x, z)argminqKL(q(z) || p?
(z|x)) s.t.
Eq[f(x, z)] ?
b.
(2)The resulting distribution q is then used in placeof p?
(z|x) to compute sufficient statistics for theM-step.
The algorithm converges to a local maxi-mum of the log of the marginal likelihood, p?
(x) =?z p?
(z,x), penalized by the KL distance of theposteriors p?
(z|x) from the feasible set defined bythe constraints (Grac?a et al, 2008):Ex[log p?(x)?
minq:Eq [f(x,z)]?bKL(q(z) || p?
(z|x))],whereEx is expectation over the training data.
Theysuggest how this framework can be used to encour-age two word alignment models to agree duringtraining.
We elaborate on their description and pro-vide details of implementation of the projection inEquation 2.3.1 AgreementMost MT systems train an alignment model in eachdirection and then heuristically combine their pre-dictions.
In contrast, Grac?a et al encourage themodels to agree by training them concurrently.
Theintuition is that the errors that the two models makeare different and forcing them to agree rules outerrors only made by one model.
This is best ex-hibited in the rare word alignments, where one-sided ?garbage-collection?
phenomenon often oc-curs (Moore, 2004).
This idea was previously pro-posed by (Matusov et al, 2004; Liang et al, 2006)although the the objectives differ.In particular, consider a feature that takes on value1 whenever source word i aligns to target word j inthe forward model and -1 in the backward model.
Ifthis feature has expected value 0 under the mixtureof the two models, then the forward model and back-ward model agree on how likely source word i is toalign to target word j.
More formally denote the for-ward model?
?p (z) and backward model?
?p (z) where?
?p (z) = 0 for z /??
?Z and ?
?p (z) = 0 for z /???Z(?
?Z and?
?Z are possible forward and backward align-ments).
Define a mixture p(z) = 12?
?p (z) + 12?
?p (z)for z ??
?Z ??
?Z .
Restating the constraints that en-force agreement in this setup: Eq[f(x, z)] = 0 withfij(x, z) =8><>:1 z ??
?Z and zij = 1?1 z ??
?Z and zij = 10 otherwise.3.2 ImplementationEM training of hidden Markov models for wordalignment is described elsewhere (Vogel et al,1996), so we focus on the projection step:argminqKL(q(z) || p?
(z|x)) s.t.
Eq[f(x, z)] = 0.
(3)The optimization problem in Equation 3 can be effi-ciently solved in its dual formulation:argmin?log?zp?
(z | x) exp {?>f(x, z)} (4)where we have solved for the primal variables q as:q?
(z) = p?
(z | x) exp{?>f(x, z)}/Z, (5)with Z a normalization constant that ensures q sumsto one.
We have only one dual variable per con-straint, and we optimize them by taking a few gra-dient steps.
The partial derivative of the objectivein Equation 4 with respect to feature i is simplyEq?
[fi(x, z)].
So we have reduced the problem tocomputing expectations of our features under themodel q.
It turns out that for the agreement fea-tures, this reduces to computing expectations underthe normal HMM model.
To see this, we have by thedefinition of q?
and p?,q?
(z) =?
?p (z | x) +?
?p (z | x)2exp{?>f(x, z)}/Z=?
?q (z) +?
?q (z)2.
(To make the algorithm simpler, we have assumedthat the expectation of the feature f0(x, z) ={1 if z ??
?Z ; ?1 if z ??
?Z} is set to zero toensure that the two models ?
?q ,?
?q are each properlynormalized.)
For ?
?q , we have: (?
?q is analogous)?
?p (z | x)e?>f(x,z)=?j?
?p d(aj |aj ?
aj?1)?
?p t(tj |saj )?ije?ijfij(x,zij)=?j,i=aj?
?p d(i|i?
aj?1)?
?p t(tj |si)e?ijfij(x,zij)=?j,i=aj?
?p d(i|i?
aj?1)?
?p ?t(tj |si).988Where we have let ?
?p ?t(tj |si) =?
?p t(tj |si)e?ij , andretained the same form for the model.
The final pro-jection step is detailed in Algorithm1.Algorithm 1 AgreementProjection(?
?p ,?
?p )1: ?ij ?
0 ?i, j2: for T iterations do3: ?
?p ?t(j|i)??
?p t(tj |si)e?ij ?i, j4: ?
?p ?t(i|j)??
?p t(si|tj)e?
?ij ?i, j5: ?
?q ?
forwardBackward(?
?p ?t,?
?p d)6: ?
?q ?
forwardBackward(?
?p ?t,?
?p d)7: ?ij ?
?ij ?E?
?q [ai = j] + E?
?q [aj = i] ?i, j8: end for9: return (?
?q ,?
?q )3.3 DecodingAfter training, we want to extract a single alignmentfrom the distribution over alignments allowable forthe model.
The standard way to do this is to findthe most probable alignment, using the Viterbi al-gorithm.
Another alternative is to use posterior de-coding.
In posterior decoding, we compute for eachsource word i and target word j the posterior prob-ability under our model that i aligns to j.
If thatprobability is greater than some threshold, then weinclude the point i?
j in our final alignment.
Thereare two main differences between posterior decod-ing and Viterbi decoding.
First, posterior decod-ing can take better advantage of model uncertainty:when several likely alignment have high probabil-ity, posteriors accumulate confidence for the edgescommon to many good alignments.
Viterbi, by con-trast, must commit to one high-scoring alignment.Second, in posterior decoding, the probability that a0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ?
?
?
?
?
?
?
?
?
0 ?
?
?
?
?
?
?
?
?
it1 ?
?
?
?
?
?
?
?
?
1 ?
?
?
?
?
?
?
?
?
was2 ?
?
?
?
?
?
?
?
?
2 ?
?
?
?
?
?
?
?
?
an3 ?
?
?
?
?
?
?
?
?
3 ?
?
?
?
?
?
?
?
?
animated4 ?
?
?
?
?
?
?
?
?
4 ?
?
?
?
?
?
?
?
?
,5 ?
?
?
?
?
?
?
?
?
5 ?
?
?
?
?
?
?
?
?
very6 ?
?
?
?
?
?
?
?
?
6 ?
?
?
?
?
?
?
?
?
convivial7 ?
?
?
?
?
?
?
?
?
7 ?
?
?
?
?
?
?
?
?
game8 ?
?
?
?
?
?
?
?
?
8 ?
?
?
?
?
?
?
?
?
.jugabande una maneraanimaday muycordial.
jugabande una maneraanimaday muycordial.Figure 2: An example of the output of HMM trained on100k the EPPS data.
Left: Baseline training.
Right: Us-ing agreement constraints.target word aligns to none or more than one word ismuch more flexible: it depends on the tuned thresh-old.4 Word alignment resultsWe evaluated the agreement HMM model on twocorpora for which hand-aligned data are widelyavailable: the Hansards corpus (Och and Ney, 2000)of English/French parliamentary proceedings andthe Europarl corpus (Koehn, 2002) with EPPS an-notation (Lambert et al, 2005) of English/Spanish.Figure 2 shows two machine-generated alignmentsof a sentence pair.
The black dots represent the ma-chine alignments and the shading represents the hu-man annotation (as described in the previous sec-tion), on the left using the regular HMM model andon the right using our agreement constraints.
Thefigure illustrates a problem known as garbage collec-tion (Brown et al, 1993), where rare source wordstend to align to many target words, since the prob-ability mass of the rare word translations can behijacked to fit the sentence pair.
Agreement con-straints solve this problem, because forward andbackward models cannot agree on the garbage col-lection solution.Grac?a et al (2008) show that alignment error rate(Och and Ney, 2003) can be improved with agree-ment constraints.
Since AER is the standard metricfor alignment quality, we reproduce their results us-ing all the sentences of length at most 40.
For theHansards corpus we improve from 15.35 to 7.01 forthe English ?
French direction and from 14.45 to6.80 for the reverse.
For English?
Spanish we im-prove from 28.20 to 19.86 and from 27.54 to 19.18for the reverse.
These values are competitive withother state of the art systems (Liang et al, 2006).Unfortunately, as was shown by Fraser and Marcu(2007) AER can have weak correlation with transla-tion performance as measured by BLEU score (Pa-pineni et al, 2002), when the alignments are usedto train a phrase-based translation system.
Conse-quently, in addition to AER, we focus on precisionand recall.Figure 3 shows the change in precision and re-call with the amount of provided training data forthe Hansards corpus.
We see that agreement con-straints improve both precision and recall when we98965 70 75 80 85 90 95 100  1101001000Thousands of training sentencesAgreement Baseline65 70 75 80 85 90 95 100  1101001000Thousands of training sentencesAgreement BaselineFigure 3: Effect of posterior constraints on precision(left) and recall (right) learning curves for HansardsEn?Fr.10 20 30 40 50 60 70 80 90 100  1101001000Thousands of training sentencesRare Common Agreement Baseline10 20 30 40 50 60 70 80 90 100  1101001000Thousands of training sentencesRare Common  Agreement BaselineFigure 4: Left: Precision.
Right: Recall.
Learning curvesfor Hansards En?Fr split by rare (at most 5 occurances)and common words.use Viterbi decoding, with larger improvements forsmall amounts of training data.
We see a similar im-provement on the EPPS corpus.Motivated by the garbage collection problem, wealso analyze common and rare words separately.Figure 4 shows precision and recall learning curvesfor rare and common words.
We see that agreementconstraints improve precision but not recall of rarewords and improve recall but not precision of com-mon words.As described above an alternative to Viterbi de-coding is to accept all alignments that have probabil-ity above some threshold.
By changing the thresh-old, we can trade off precision and recall.
Figure5 compares this tradeoff for the baseline and agree-ment model.
We see that the precision/recall curvefor agreement is entirely above the baseline curve,so for any recall value we can achieve higher preci-sion than the baseline for either corpus.
In Figure 6we break down the same analysis into rare and nonrare words.Figure 7 shows an example of the same sentence,using the same model where in one case Viterbi de-coding was used and in the other case Posterior de-coding tuned to minimize AER on a development set0 0.2 0.4 0.6 0.8 1  00.2 0.40.6 0.81RecallPrecisionBaseline Agreement0 0.2 0.4 0.6 0.8 1  00.2 0.40.6 0.81RecallPrecisionBaseline AgreementFigure 5: Precision and recall trade-off for posterior de-coding with varying threshold.
Left: Hansards En?Fr.Right: EPPS En?Es.0 0.2 0.4 0.6 0.8 1  00.2 0.40.6 0.81RecallPrecisionBaseline Agreement0 0.2 0.4 0.6 0.8 1  00.2 0.40.6 0.81RecallPrecisionBaseline AgreementFigure 6: Precision and recall trade-off for posterior onHansards En?Fr.
Left: rare words only.
Right: commonwords only.was used.
An interesting difference is that by usingposterior decoding one can have n-n alignments asshown in the picture.A natural question is how to tune the threshold inorder to improve machine translation quality.
In thenext section we evaluate and compare the effects ofthe different alignments in a phrase based machinetranslation system.5 Phrase-based machine translationIn this section we attempt to investigate whether ourimproved alignments produce improved machine0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ?
?
?
?
?
?
?
?
?
0 ?
?
?
?
?
?
?
?
?
firstly1 ?
?
?
?
?
?
?
?
?
1 ?
?
?
?
?
?
?
?
?
,2 ?
?
?
?
?
?
?
?
?
2 ?
?
?
?
?
?
?
?
?
we3 ?
?
?
?
?
?
?
?
?
3 ?
?
?
?
?
?
?
?
?
have4 ?
?
?
?
?
?
?
?
?
4 ?
?
?
?
?
?
?
?
?
a5 ?
?
?
?
?
?
?
?
?
5 ?
?
?
?
?
?
?
?
?
legal6 ?
?
?
?
?
?
?
?
?
6 ?
?
?
?
?
?
?
?
?
framework8 ?
?
?
?
?
?
?
?
?
8 ?
?
?
?
?
?
?
?
?
.en primerolugar, tenemosun marcojur??dico.
en primerolugar, tenemosun marcojur?
?dico.Figure 7: An example of the output of HMM trained on100k the EPPS data using agreement HMM.
Left: Viterbidecoding.
Right: Posterior decoding tuned to minimizeAER.
The addition is en-firstly and tenemos-have.990translation.
In particular we fix a state of the artmachine translation system1 and measure its perfor-mance when we vary the supplied word alignments.The baseline system uses GIZA model 4 alignmentsand the open source Moses phrase-based machinetranslation toolkit2, and performed close to the bestat the competition last year.For all experiments the experimental setup is asfollows: we lowercase the corpora, and train lan-guage models from all available data.
The reason-ing behind this is that even if bilingual texts mightbe scarce in some domain, monolingual text shouldbe relatively abundant.
We then train the com-peting alignment models and compute competingalignments using different decoding schemes.
Foreach alignment model and decoding type we trainMoses and use MERT optimization to tune its pa-rameters on a development set.
Moses is trained us-ing the grow-diag-final-and alignment symmetriza-tion heuristic and using the default distance basedistortion model.
We report BLEU scores using ascript available with the baseline system.
The com-peting alignment models are GIZA Model 4, our im-plementation of the baseline HMM alignment andour agreement HMM.
We would like to stress thatthe fair comparison is between the performance ofthe baseline HMM and the agreement HMM, sinceModel 4 is more complicated and can capture morestructure.
However, we will see that for moderatesized data the agreement HMM performs better thanboth its baseline and GIZA Model 4.5.1 CorporaIn addition to the Hansards corpus and the EuroparlEnglish-Spanish corpus, we used four other corporafor the machine translation experiments.
Table 1summarizes some statistics of all corpora.
The Ger-man and Finnish corpora are also from Europarl,while the Czech corpus contains news commentary.All three were used in recent ACL workshop sharedtasks and are available online3.
The Italian corpusconsists of transcribed speech in the travel domainand was used in the 2007 workshop on spoken lan-guage translation4.
We used the development and1www.statmt.org/wmt07/baseline.html2www.statmt.org/moses/3http://www.statmt.org4http://iwslt07.itc.it/Corpus Train Len Test Rare (%) Unk (%)En, Fr 1018 17.4 1000 0.3, 0.4 0.1, 0.2En, Es 126 21.0 2000 0.3, 0.5 0.2, 0.3En, Fi 717 21.7 2000 0.4, 2.5 0.2, 1.8En, De 883 21.5 2000 0.3, 0.5 0.2, 0.3En, Cz 57 23.0 2007 2.3, 6.6 1.3, 3.9En, It 20 9.4 500 3.1, 6.2 1.4, 2.9Table 1: Statistics of the corpora used in MT evaluation.The training size is measured in thousands of sentencesand Len refers to average (English) sentence length.
Testis the number of sentences in the test set.
Rare and Unkare the percentage of tokens in the test set that are rareand unknown in the training data, for each language.26 28 30 32 34 36  100001000001e+06Training data size(sentences)Agreement Post-pts Model4Baseline ViterbiFigure 8: BLEU score as the amount of training data isincreased on the Hansards corpus for the best decodingmethod for each alignment model.tests sets from the workshops when available.
ForItalian corpus we used dev-set 1 as development anddev-set 2 as test.
For Hansards we randomly chose1000 and 500 sentences from test 1 and test 2 to betesting and development sets respectively.Table 1 summarizes the size of the training corpusin thousands of sentences, the average length of theEnglish sentences as well as the size of the testingcorpus.
We also report the percentage of tokens inthe test corpus that are rare or not encountered in thetraining corpus.5.2 DecodingOur initial experiments with Viterbi decoding andposterior decoding showed that for our agreementmodel posterior decoding could provide better align-ment quality.
When labeled data is available, we cantune the threshold to minimize AER.
When labeleddata is not available we use a different heuristic to991tune the threshold: we choose a threshold that givesthe same number of aligned points as Viterbi decod-ing produces.
In principle, we would like to tunethe threshold by optimizing BLEU score on a devel-opment set, but that is impractical for experimentswith many pairs of languages.
We call this heuristicposterior-points decoding.
As we shall see, it per-forms well in practice.5.3 Training data sizeThe HMM alignment models have a smaller param-eter space than GIZA Model 4, and consequently wewould expect that they would perform better whenthe amount of training data is limited.
We found thatthis is generally the case, with the margin by whichwe beat model 4 slowly decreasing until a crossingpoint somewhere in the range of 105 - 106 sentences.We will see in section 5.3.1 that the Viterbi decodingperforms best for the baseline HMM model, whileposterior decoding performs best for our agreementHMM model.
Figure 8 shows the BLEU score forthe baseline HMM, our agreement model and GIZAModel 4 as we vary the amount of training data from104 - 106 sentences.
For all but the largest data sizeswe outperform Model 4, with a greater margin atlower training data sizes.
This trend continues as welower the amount of training data further.
We see asimilar trend with other corpora.5.3.1 Small to Medium Training SetsOur next set of experiments look at our perfor-mance in both directions across our 6 corpora, whenwe have small to moderate amounts of training data:for the language pairs with more than 100,000 sen-tences, we use only the first 100,000 sentences.
Ta-ble 2 shows the performance of all systems on thesedatasets.
In the table, post-pts and post-aer standfor posterior-points decoding and posterior decod-ing tuned for AER.
With the notable exception ofCzech and Italian, our system performs better thanor comparable to both baselines, even though it usesa much more limited model than GIZA?s Model 4.The small corpora for which our models do not per-form as well as GIZA are the ones with a lot of rarewords.
We suspect that the reason for this is that wedo not implement smoothing, which has been shownto be important, especially in situations with a lot ofrare words.X?
En En?
XBase Agree Base AgreeGIZA M4 23.92 17.89De Viterbi 24.08 23.59 18.15 18.13post-pts 24.24 24.65(+) 18.18 18.45(+)GIZA M4 18.29 11.05Fi Viterbi 18.79 18.38 11.17 11.54post-pts 18.88 19.45(++) 11.47 12.48(++)GIZA M4 33.12 26.90Fr Viterbi 32.42 32.15 25.85 25.48post-pts 33.06 33.09(?)
25.94 26.54(+)post-aer 31.81 33.53(+) 26.14 26.68(+)GIZA M4 30.24 30.09Es Viterbi 29.65 30.03 29.76 29.85post-pts 29.91 30.22(++) 29.71 30.16(+)post-aer 29.65 30.34(++) 29.78 30.20(+)GIZA M4 51.66 41.99It Viterbi 52.20 52.09 41.40 41.28post-pts 51.06 51.14(??)
41.63 41.79(?
)GIZA M4 22.78 12.75Cz Viterbi 21.25 21.89 12.23 12.33post-pts 21.37 22.51(++) 12.16 12.47(+)Table 2: BLEU scores for all language pairs using up to100k sentences.
Results are after MERT optimization.The marks (++)and (+)denote that agreement with poste-rior decoding is better by 1 BLEU point and 0.25 BLEUpoints respectively than the best baseline HMM model;analogously for (??
), (?
); while (?
)denotes smaller dif-ferences.5.3.2 Larger Training SetsFor four of the corpora we have more than 100thousand sentences.
The performance of the sys-tems on all the data is shown in Table 3.
Germanis not included because MERT optimization did notcomplete in time.
We see that even on over a millioninstances, our model sometimes performs better thanGIZA model 4, and always performs better than thebaseline HMM.6 ConclusionsIn this work we have evaluated agreement-constrained EM training for statistical word align-ment models.
We carefully studied its effects onword alignment recall and precision.
Agreementtraining has a different effect on rare and com-mon words, probably because it fixes different typesof errors.
It corrects the garbage collection prob-lem for rare words, resulting in a higher preci-sion.
The recall improvement in common words992X?
En En?
XBase Agree Base AgreeGIZA M4 22.78 14.72Fi Viterbi 22.92 22.89 14.21 14.09post-pts 23.15 23.43 (+) 14.57 14.74 (?
)GIZA M4 35.65 31.15Fr Viterbi 35.19 35.17 30.57 29.97post-pts 35.49 35.95 (+) 29.78 30.02 (?
)post-aer 34.85 35.48 (+) 30.15 30.07 (?
)GIZA M4 31.62 32.40Es Viterbi 31.75 31.84 31.17 31.09post-pts 31.88 32.19 (+) 31.16 31.56 (+)post-aer 31.93 32.29 (+) 31.23 31.36 (?
)Table 3: BLEU scores for all language pairs using allavailable data.
Markings as in Table 2.can be explained by the idea that ambiguous com-mon words are different in the two languages, so theun-ambiguous choices in one direction can force thechoice for the ambiguous ones in the other throughagreement constraints.To our knowledge this is the first extensive eval-uation where improvements in alignment accuracylead to improvements in machine translation per-formance.
We tested this hypothesis on six differ-ent language pairs from three different domains, andfound that the new alignment scheme not only per-forms better than the baseline, but also improvesover a more complicated, intractable model.
In or-der to get the best results, it appears that posteriordecoding is required for the simplistic HMM align-ment model.
The success of posterior decoding us-ing our simple threshold tuning heuristic is fortu-nate since no labeled alignment data are needed:Viterbi alignments provide a reasonable estimate ofaligned words needed for phrase extraction.
The na-ture of the complicated relationship between wordalignments, the corresponding extracted phrases andthe effects on the final MT system still begs forbetter explanations and metrics.
We have investi-gated the distribution of phrase-sizes used in transla-tion across systems and languages, following recentinvestigations (Ayan and Dorr, 2006), but unfortu-nately found no consistent correlation with BLEUimprovement.
Since the alignments we extractedwere better according to all metrics we used, itshould not be too surprising that they yield bettertranslation performance, but perhaps a better trade-off can be achieved with a deeper understanding ofthe link between alignments and translations.AcknowledgmentsJ.
V. Grac?a was supported by a fellowship fromFundac?a?o para a Cie?ncia e Tecnologia (SFRH/ BD/27528/ 2006).
K. Ganchev was partially supportedby NSF ITR EIA 0205448.ReferencesN.
F. Ayan and B. J. Dorr.
2006.
Going beyond AER: Anextensive analysis of word alignments and their impacton MT.
In Proc.
ACL.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J.Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty.1993.
But dictionaries are data too.
In Proc.
HLT.P.
F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1994.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Royal Statistical Society, Ser.
B, 39(1):1?38.A.
Fraser and D. Marcu.
2007.
Measuring word align-ment quality for statistical machine translation.
Com-put.
Linguist., 33(3):293?303.J.
Grac?a, K. Ganchev, and B. Taskar.
2008.
Expecta-tion maximization and posterior constraints.
In Proc.NIPS.P.
Koehn.
2002.
Europarl: A multilingual corpus forevaluation of machine translation.P.
Lambert, A.De Gispert, R. Banchs, and J.
B. Marin?o.2005.
Guidelines for word alignment evaluation andmanual alignment.
In Language Resources and Eval-uation, Volume 39, Number 4.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
HLT-NAACL.E.
Matusov, Zens.
R., and H. Ney.
2004.
Symmetricword alignments for statistical machine translation.
InProc.
COLING.R.
C. Moore.
2004.
Improving IBM word-alignmentmodel 1.
In Proc.
ACL.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In ACL.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Comput.
Lin-guist., 29(1):19?51.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: A Method for Automatic Evaluation of Ma-chine Translation.
In Proc.
ACL.S.
Vogel, H. Ney, and C. Tillmann.
1996.
Hmm-basedword alignment in statistical translation.
In Proc.COLING.993
