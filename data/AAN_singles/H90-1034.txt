Towards Environment-IndependentSpoken Language SystemsAlejandro Acero and Richard M. SternDepartment ofElectrical and Computer EngineeringSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213AbstractIn this paper we discuss recent results from our effortsto make SPHINX, the CMU continuous-speech speaker-independent recognition system, robust to changes in theenvironment.
To deal with differences in noise level andspectral tilt between close-talking and desk-topmicrophones, we describe two novel methods based on ad-ditive corrections in the cepstral domain.
In the first algo-rithm, an additive correction is imposed that depends onthe instantaneous SNR of the signal.
In the second tech-nique, EM techniques are used to best match the cepstralvectors of the input utterances tothe ensemble of codebookentries representing a standard acoustical mbience.
Use ofthese algorithms dramatically improves recognition ac-curacy when the system is tested on a microphone otherthan the one on which it was trained.IntroductionThere are many sources of acoustical distortion that candegrade the accuracy of speech-recognition systems.
Forexample, obstacles to robustness include additive noisefrom machinery, competing talkers, etc., reverberationfrom surface reflections in a room, and spectral shaping bymicrophones and the vocal tracts of individual speakers.These sources of distortion cluster into two complementaryclasses: additive noise (as in the first two examples) anddistortions resulting from the convolution of the speech sig-nal with an unknown linear system (as in the remainingthree).A number of algorithms for speech enhancement havebeen proposed in the literature.
For example, Boll \[3\] andBeroufi et al \[2\] introduced the spectral subtraction ofDFT coefficients, and Porter and Boll \[11\] used MMSEtechniques to estimate the DFT coefficients of corruptedspeech.
Spectral equalization tocompensate for convolveddistortions was introduced by Stockham et al \[13\].
Recentapplications of spectral subtraction and spectral equaliza-tion for speech recognition systems include the work ofVan Compemolle \[5\] and Stem and Acero \[12\].
Althoughrelatively successful, the above methtxls all depend on theassumption of independence of the spectral estimatesacross fr~uencies.
Erell and Weimranb \[6\] demonstratedimproved performance with an MMSE estimator in whichcorrelation among frequencies is modeled explicitly.157Acero and Stem \[1\] proposed an approach to environmentnormalization i  the cepstral domain, going beyond thenoise stripping problem.In this paper we present two algorithms for speech nor-malization based on additive corrections in the cepstraldomain and compare them to techniques that operate in thefrequency domain.
We have chosen the cepstral domainrather than the frequency domain so that can we workdirectly with the parameters that SPHINX uses, and becausespeech can be characterized with a smaller number ofparameters in the cepstral domain than in the frequencydomain.
The first algorithm, SNR-dependent cepstralnormalization (SDCN) is simple and effective, but it can-not be applied to new microphones without microphone-specific training.
The second algorithm,codeword-dependent cepstral normalization (CDCN) usesthe speech knowledge represented in a codebook to es-timate the noise and spectral equalization necessary for theenvironmental normalization.
We also describe an inter-polated SDCN algorithm (iSDCN) which combines thesimplicity of SDCN and the normalization capabilities ofCDCN.
These algorithms are evaluated with a number ofmicrophones using an alphanumeric database in which ut-terances were recorded simultaneously with two differentmicrophones.Experimental ProceduresThe alphanumeric database and system used for theseexperiments has been described previously \[12\] [l\].Briefly, the database contain.q utterances that were recordedsimultaneously in stereo using both the close-talking Sen-nheiser HMD224 microphone (CLSTK), a standard in pre-vious DARPA evaluations, and a desk-top Crown PZM6fsmicrophone (CRPZM).
The recordings with the CRPZMexhibit not only background noise but also key clicks fromworkstations, interference from other talkers, and rever-beration.
The task has a vocabulary of 104 words that arehighly confusable, A simplified version of SPHINX with nogrammar was used.Baseline recognition results obtained by gaining andtesting SPHIlqX using this database are shown in the firsttwo columns of Table 1.
With no processing, training andtesting using the CRPZM degrades recognition accuracy byabout 60 percent relative to that obtained by training andtesting on the CLSTK.
Although most of the "new" errorsintroduced by the CRPZM were confusions of silence ornoise segments with weak phonetic events, a significantpercentage was also due to crosstalk \[12\].
It can also beseen that the "cross conditions" (training on onemicrophone and testing using the other) produce a verylarge degradation i  recognition accuracy.Independent Compensation forNoise and FilteringIn this section we examine the performance of SPHINXunder some of the techniques that have been used in theliterature to combat noise and spectral i k: multi-style train-ing, short-time liftering, spectral subtraction, and spectralequalization.Multi-Style TrainingMulti-style training is a technique in which the trainingset includes data representing different conditions o thatthe resulting HMM models are more robust to thisvariability.
This simple approach as been used success-fully in the field of speech styles \[10\] and speaker indepen-dence \[9\].
The price one must pay for the robustness i adegradation i performance for cases in which the trainingand testing are done with the same condition.with s =1.0 which Jtmqua \[8\] found to be optimum, and theban@ass liftering method efined by Juang \[7\].Unfortunately, we found that application of these tech-niques produced essentially no improvement for cleanspeech and only a very small improvement for corruptedspeech.
Since the frequency-warping transformation iSPHINX alters the variances of the coefficients, ome otherset of weights may prove more effective.Spectral Subtraction and EqualizationIn spectral subtraction and equalization it is assumedthat the speech signal x(t) is degraded by linear filteringand/or uncorrelated additive noise, as depicted in Fig.
1.The goal of the compensation is to reverse the effects ofthese degradations.x(t) ~ h(t)"Clean" Linearspeech Distortion~D~pgra  y(t) dedeechn(t)AdditiveNoiseFigure 1: Model of the degradation.An experiment was carried out in which all the speechrecorded from the CLSTK and the CRPZM microphoneswere used in training (Table 1).
As expected, robustness igained by using multi-style training, but at the expense ofsacrificing performance with respect o the case of trainand test on the same conditions.TRAIN CLSTK CRPZM MULTITest CLSTK 85.3% 36.9% 78.3%Test CRPZM 18.6% 76.5% 6'9.7%Table 1: Comparison of recognition accuracy of SPHINXunder different raining and testing conditions.
CLSTK isthe Sennheiser HMD224, CRPZM is the Crown PZM6sfand MULTI means that the data from both microphoneswere used in trainingLifteringMany studies have examined several potential distortionmeasures for speech recognition in noise.
Most of thesemeasures involve unequal weighthags of the mean-squaredistance between cepstral coefficients of the reference andtest utterances.
The motivation for weighting distances be-tween cepstral vectors is twofold: it provides ome variancenormalization for the coefficients and it makes the systemmore robust o noise and spectral tilt by giving less weightto the low-order cepstral coefficients.
We tried in our sys-tem several weighting measures that have been proposed inthe literature including the inverse of the intra-elustervariance as defined by Tokhura \[14\], the exponential lifter158Using the notation of Fig.
1, we can characterize thepower spectral density (PSD) of the processes involved asPy(f) = Px(f) IH(f)12 + Pn(f) (1)Spectral equalization techniques attempt to compensatefor the filter h(t), while spectral subtraction techniques at-tempt to remove the effects of the noise from the signal.We compare the performance of the following differentimplementations of spectral subtraction and equalizationtechniques in Table 2.?
A spectral equalization algorithm (EQUAL) that issimilar to the approach of \[13\].
It compensates for theeffects of the linear fiJtering, but not the additivenoise, as described in \[12\].?
A direct implementation f the original power spectralsubtraction rule (PSUB) on 32 frequency bands ob-tained via a real DFr  of the cepstmm vector.
Therestored cepstrurn is obtained with an inverse DFT.?
An implementation f BoU's algorithm (MMSE1) \[4\],in which a transformation is applied to all the fre-quency bands of the CRPZM speech that minimizesthe mean squared error relative to the CLSTK speech.The log-power correction in each frequency banddepended only on the instantaneous SNR in that band.?
An implementation f magnitude spectral subtraction(MSUB) described in \[12\] that incorporates over- andunder-subtraction depending on the SNR as suggestedby \[2\].
In \[12\] it was noted that a cascade of theEQUAL and MSUB algorithms did not yield any fur-ther improvement in recognition accuracy becausethey interact nonlinearly.The different criteria used in PSUB, MSUB, producedifferent curves that relate the effective SNR of the inputand output.
Some of these curves are shown in Figure 2.~zo\[ ?..-'~_sl .
Igh SN.
MSUB .
~il~ 10\[" - - -  MMSSE1 .
.
.~ ' /  / "I .
.
.
.
.
PSUe.
.
:~  t " .**..*'.
?y l , , .
.
, .
.
.
..IO~.
: .
.
/  /I.15 ~/  /Figure 2: Input-Output transformation curves for PSUB,MSUB and MMSE1.
The SNR is defined as the log-powerof the signal in a frequency band minus the log-power ofthe noise in that band.
The transformation for MSUB is nota single curve but a family of curves that depend on thetotal SNR for a given frame.TRAIN CLSTK CLSTK CRPZM CRPZMTEST CLSTK CRPZM CLSTK CRPZMBASE 85.3% 18.6% 36.9% 76.5%EQUAL 85.3% 38.3% 50.9% 76.5%PSUB 82.2% 37.2% 62.0% 64.7%MMSE1 85.3% 48.7% 68.7% 71.4%MSUB 82.7% 64.8% 75.1% 72.8%Table 2: Performance of different equalization andspectral subtraction algorithms.
EQUAL and MMSE1 wereapplied only to the CRPZM speech while PSUB andMSUB were applied to both the CLSTK and the CPRPZMspeech.For the most part these algorithms provide increasingdegrees of compensation, but their recognition accuracyunder the "cross" conditions is still much worse than thatobtained even with the system is trained and tested on theCRPZM.
We have found that the above techniques producemany output frames that do not constitute l gitimate speechvectors, especially at low SNR, because they do not takeinto account correlations across frequency.
That problem,along with the nonlinear interaction of the subtraction andnormalization processes motivated us to consider new al-gorithms which jointly compensate for noise and filtering,and with some attention paid to the spectral profile of thecompensated speech.Joint Compensation forNoise and FilteringIn this section we discuss two algorithms that performnoise suppression and spectral-flit compensation jointly inthe cepstmm by means of additive corrections.If we let the cepstral vectors x, n, y and q represent theFourier series expansion of ln ex(f) ,  ln Pn(f), ln Py(f)and in IH(f)12 respectively, (1) can be rewritten asy = x + q + r(x,n,q)  (2)where the correction vector (x, n, q) is given byr(x,n,q)  = IDFT {In (1 + eBb\ [n -  q-x\])} (3)Let z be an estimate of y obtained through our spectralestimation algorithm.
Our goal is to recover the uncor-mpted vectors X = Xo,...x N_ 1 of an utterance given the ob-servations Z = Zo,...zN_ 1 and our knowledge of the en-vironment n and q.SDCN AlgorithmSNR-Dependent Cepstral Normalization (SDCN) is asimple algorithm that applies a fixed additive correctionvector w to the cepstrai coefficients that depends ex-clusively on the instantaneous SNR of the input frame.A x = z - w(SNR) (4)At high SNR, inspection of equations (1), (2) and (3)indicates that x (0) + q(0) >> n(0), r = 0, and y = x + q.On the other hand at low SNR, x(0)+ q(0)<< n(0) andy = n. Hence, the SDCN algorithm performs spectraiequalization at high SNR and noise suppression at lowSNR.SNR is estimated in the SDCN algorithm asz (0) - n (0).
This is not the true signal-to-noise ratio but itis related to it and easier to compute.
The compensationvectors w(SNR) were estimated with an MMSE criterion bycomputing the average difference between cepstral vectorsfor the test condition versus a standard acoustical environ-ment from simultaneous stereo recordings.
We have ob-served that applying a correction to just c o and c 1 yieldsbasically the same results than if all the cepstmm coef-ficients are normalized.For the sake of comparison between algorithms operat-ing in the spectral domain and the cepstral domain, wedeveloped an algorithm called MMSEN that accomplishesnoise suppression and spectral equalization jointly usingdifferent transformations for every frequency band.MMSEN is similar in concept o SDCN except that itoperates in the spectral (rather than cepstral) domain.
As isseen in Table 4, SDCN performs slightly better thanMMSEN, and it is more computationally efficient as well.TRAIN CLSTK CLSTK CRPZM CRPZMTEST CLSTK CRPZM CLSTK CRPZMBASE 85.3% \[ 18.6% 36.9% 76.5%MMSEN 85.3% I 66.4% 75.5% 72.3%SDCN 85.3% 67.2% 76.4% 75.5%Table3: Performance of the MMSEN and SDCN al-gorithrns when compared with the baseline.159Although liftering provided very little improvement forour baseline system, this technique is actually complemen-tary to SDCN: liftering techniques can be viewed as avariance normalization while SDCN is a bias-compensation algorithm.
Using SDCN and the algorithmof Juang \[7\] with p = 12 and values of the parameter L rang-ing from 0 to 6, we observed a modest improvement overpure SDCN (from 67.2% to 72.3%) when training using theCLSTK and testing with the CPRPZM microphone.CDCN AlgorithmAlthough the SDCN technique performs acceptably, ithas the disadvantage that new microphones must be"calibrated" by conecang long-term statistics from a newstereo database.
Since only long-term averages axe used,SDCN is clearly not able to model a non-stationary en-vironment.
The second new algorithm,Codeword-Dependent Cepstral Normalization (CDCN),was proposed to circumvent these problems.The CDCN algorithm attempts to determine the fixedequalization and noise vectors q and n that provide an en-semble of compensated cepstral vectors ~ that are collec-tively closest to the set of locations of legitimate VQcodewords.
The correction vector will be different forevery codebook vector.The q and n are estimated using ML techniques via theEM algorithm since no close-form expression can be ob-tained.
The compensated vectors ~ are estimated usingM/VISE techniques.
The reader is referred to \[1\] for thedetails of this algorithm.Results and DiscussionTable 4 describes the recognition accuracy of theoriginal SPHINX system with no preprocessing, and with theSDCN and CDCN algorithms.
Use of the CDCN algo-rithm brings the performance obtained when training on theCLSTK and testing on the CRPZM to the level observedwhen the system is trained and tested on the CRPZM.Moreover, use of CDCN improves performance obtainedwhen training and testing on the CRPZM to a level greaterthan the baseline performance.
The much simpler SDCNalgorithm also provides considerable improvement in per-formance when the system is trained and tested on twodifferent microphones.Unlike in previous tudies where estimates of the powernorroaliTation factor, spectral equalization function, andnoise are obtained independently, these quantities arejointly estirnated in CDCN using a common maximumlikelihood fiarnework that is based on a priori knowledgeof the speech signal.
Since CDCN only requires a singleutterance in order to estimate noise and spectral tilt, it canbetter captme the non-stationatity of the environment.Moreover, in a real application, long-term averages maynot be available for every speaker and new microphone.In Figures 3, 4, 5 and 6 we show 3-D representations ofTRAIN CLSTK CLSTK CRPZM CRPZMTEST CLSTK CRPZM CLSTK CRPZMBASE 85.3% 18.6% 36.9% 76.5%SDCN 85.3% 67.2% 76. ,0,% 75.5%CDCN 85.3% 74.9% 73.7% 77.9%Table 4: Comparison of recognition accuracy of SPHINXwith no processing, SDCN and CDCN algorithms.
The sys-tem was trained and tested using an combinations of theCLSTK and CRPZM microphones.an utterance with the CLSTK and no processing, theCRPZM with no processing, SDCN, and CDCN respec-tively.
While it can be seen that noise suppression is ach-ieved with both SDCN and CDCN, the CDCN algorithmprovides greater compensation for spectral tilt.~+1?1Figure 3: "Yes" with CLSTK and no processing.l e '- - \mlNl~Figure 4: "Yes" with CRPZ/vl and no processing.Results with other microphonesTo confirm the ability of the CDCN algorithm to adaptto new environmental conditions, a series of tests was per-formed with 5 new stereo speech databases.
The test datawere all collected after development of the CDCN algo-rithm was completed.
In all cases the system was trainedusing the Sennl'~iser HMD224.
The "second" microphones(with which the system was not trained) were:160,*4 im tFigure 5: "Yes" with CRPZM and SDCN.
?+an~/~me-eFigure 6: "Yes" with CRPZM and CDCN.?
The Crown PCC160 desk-top hase-coherent cardioidmicmpbone (CRPCC160).
(This is the new DARPA"standard" desk-top microphone.)?
An independent test set using the Crown PZM6fs.?
The Sennheiser 518 dynamic cardioid, hand-heldmicrophone (SENN518).?
The Sennlaeiser ME80 eleetret supercardioid stand-mounted microphone (SENNME80).?
An H/vIE lavalier microphone that also used an FMreceiver (HME).TEST C I~TK CRPCCI60BASE 82.4% 70.2%CDCN 81.0% 78.5%TEST CLSTK CRPZM6FSBASE 84.8% 41.8%CDCN 83.3% 73.9%In Table 5 we compare results using the CDCN algo-rithm to baseline performance.
With this algorithm greatrobustness i obtained across microphones.
However, thereis a slight drop in performance when training and testing onTEST CLSTK SENNS18BASE 871% 84.5%CDCN 82.2% 83.3%TEST  CLSTK SENNME80BASE 83.7% 71.4%CDCN 81.5% 80.7%TEST l IME CRPCC160BASE 55.9% 56.3%CDCN 81.7% 72.2%Table 5: Analysis of performance of SPHINX for thebasefine and the CDCN algorithm.
Two microphones wererecorded in stereo in each case.
The microphones comparedare the Sennheiser HMD224, 518, MES0, the CrownPZM6FS and PCC160, and the HME microphone.
Trainingwas done with the Sennheiser HMD224 in all cases.the Sennheiser HMD224.
We believe that one cause forthis is that estimates of q and n are not very good for shortutterances.Interpolated SDCNOne of the deficiencies of the SDCN algorithm is theinahif i ty to adapt o new environments since the correctionvectors are derived from a stereo database of our "stan-dard" Sennheiser I-IMD224 and the new microphone.
Byusing an MMSE criterion that included some a priori infor-marion about the distribution of speech (a codebook), theSDCN can estimate the parameters of the environment qand n just as CDCN does.As we have noted above, the correction vector inSDCN, w, has the asymptotic value of the noise vector n atlow SNR and of the equalization vector q at high SNR.
Ininterpolated SDCN (ISDCN) the dependence on SNR ismodelled as follows:wi(SNR ) = n i + (qi-ni)~.
(SNR) (5)wberefi (SNR) is chosen to be the sigmoid functionfi(x) = 1 / \[ 1 + exp(--ct ix + ~i)\] (6)In this evaluation tz i and 15i were set empirically to 3.0 fori > 0 and 6.0 for i= 0.
The vectors n and q were deter-mined by an EM algorithm whose objective function is theminimization of the total VQ distortion.In evaluating the ISDCN algorith m we also varied theamount of speech used for estimation of q and n. Sincethese parameters are normally estimated over the course ofonly a single utterance, the estimates of q and n will ex-hibit a large variance for short utterances.
We believe thisis one of the causes for the slight degradation i  perfor-mance in Table 5 observed when the system was trainedand tested using the CLSTK microphone.161We compared the recognition accuracy with the ISDCNalgorithm using estimates of the model parameters obtainedby considering only one utterance at a rime, and with es-timates obtained using all 14 utterances spoken by a givenspeaker.
Estimating the model parameters from all ut-terances for a speaker produced an accuracy of 85.9%,which is slightly higher than the baseline 85.3%.
(The cor-responding recognition accuracy working an utterance at atime was 84.8%.)
These results lead us to believe thatCDCN could also benefit from a longer estimation time,and will be analyzed in future work.ConclusionsWe described and evaluated two algorithms to makeSPHINX more robust with respect to changes of microphoneand acoustical environment.
With the first algorithm,SNR-dependent cepstral normalization, a correcrion vectoris added that depends exclusively on the instantaneousSNR of the input.
While SDCN is very simple, it providesa cousiderable improvement in performance when the sys-tem is trained and tested on different microphones, whilemaintaining the same performance for the case of trainingand testing on the same microphone.
Two drawbacks ofthe method are that the system must be retrained using astereo database for each new microphone considered, andthat the normalization is based on long-term statisticalmodels.The second algorithm, codeword-dependent cepstralnormalization, uses a maximum likelihood technique to es-tirnate noise and spectral tilt in the context of an iterafivealgorithm similar to the EM algorithm.
With CDCN, thesystem can adapt o new speakers, microphones, and en-vironrnents without he need for collecting statistics aboutthem a priori.
By not relying on long-term apriori infor-marion, the CDCN algorithm can dynamically adapt tochanges in the acoustical environment aswell.Both algorithms provided ramatic improvement inper-forrnance when SPHINX is tra.ined on one microphone andtested on another, without degrading recognition accuracyobtained when the same microhone was used for trainingand testing.AcknowledgmentsThis research was sponsored by the Defense AdvancedResearch Projects Agency (DOD), ARPA Order No.
5167,under contract number N00039-85-C-0163.
The views andconclusions contained in this document are those of theauthors and should not be intelpreted as representing theofficial policies, either expressed or implied, of the DefenseAdvanced Research Projects Agency or the US Govern-ment.
We thank Joel Douglas, Kai-Fu Lee, Robert Weide,Raj Reddy, and the rest of the speech group for their con-tliburions to this work.References1.
A. Acero and R. M. Stem.
Environmental Robustnessin Automaric Speech Recognition.
Proc.
IEFEE Int.
Conf.Acoustics, Speech and Signal Processing, Albuquerque,NM, April, 1990, pp.
849-852.2.
M. Berouti, R. Schwartz and J. Makhoul.
SignalProcessing.
Volume 1: Enhancement of Speech Corruptedby Acoustic Noise.
In Speech Enhancement, J. S.
Lira,Ed., Prentice Hall, Englewood Cliffs, NJ, 1983, pp.
69-73.3.
S. F. Boll.
"Suppression of Acoustic Noise in SpeechUsing Spectral Subtracrion".
IEEE Trans.
Acoustics,Speech and Signal Processing 27, 2 (April 1979), 113-120.4.
S. Boll, J. Porter and L. G. Bahler.
Robust Syntax FreeSpeech Recognition.
Proc.
IEEE Int.
Conf.
Acoustics,Speech and Signal Processing, New York, NY, 1988, pp.179-182.5.
D. Van Compemolle.
Spectral Estimation Using a Log-Distance Error Criterion Applied to Speech Recognition.Proc.
I~Elq Int.
Conf.
Acoustics, Speech and SignalProcessing, Glasgow, UK, May, 1989, pp.
258-261.6.
A. Erefl and M. Weintraub.
Spectral Estimation forNoise Robust Speech Recognition.
Proc.
Speech andNatural Language Workshop, Cape Cod, MA, Oct., 1989.7.
B. H. Juang, L. R. Rabiner and J. G. Wilpon.
"On theUse of Bandpass Liftering in Speech Recognition".
IEEETrans.
Acoustics, Speech and Signal Processing ASSP-35(Jul.
1987), 947-954.8.
J. C. Junqua and H. Wakita.
A Comparative Study ofCepstral Lifters and Distance Measures for All-PoleModels of Speech in Noise.
P,oc.
IEEE Int.
Conf.
Acous-tics, Speech and Signal Processing, Glasgow, OK, May,1989, pp.
476-479.9.
K. F. Lee et al The SPHINX Speech Recognition Sys-tem.
Proc.
IEEE Int.
Conf.
Acoustics, Speech and SignalProcessing, Glasgow, OK, May, 1989, pp.
,445-448.10.
R. P. Lippmann, E. A. Martin and D.B.
Paul.
Mulri-Style Training for Robust Isolated-Word Speech Recog-nirion.
Proc.
IEFEE Int.
Conf.
Acoustics, Speech and SignalProcessing, Dallas, TX, April, 1987, pp.
705-708.11.
J. E. Porter and S. F. Boll.
Optimal Estimators forSpectral Restorarion of Noisy Speech.
Proc.
IEEE Int.Conf.
Acoustics, Speech and Signal Processing, San Diego,CA, May, 1984, pp.
18A.2.1.12.
R. Stem and A. Acero.
Acoustical Pre-processing forRobust Speech Recognition.
Proc.
Speech and NaturalLanguage Workshop, Cape Cod, MA, Oct., 1989, pp.311-318.13.
T. G. Stockham, T. M. Cannon and R. B.
Ingebretsen.
"Blind Deconvolution Through Digital Signal Processing".Proc.
of the IEEE 63, 4 (Apr.
1975), 678-692.14.
Y. Tokhura.
"A Weighted Cepstral Distance Measurefor Speech Recognition".
IEEE Trans.
Acoustics, Speechand Signal Processing ASSP-35 (Oct. 1987), 1414-1422.162
