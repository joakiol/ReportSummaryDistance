Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
952?956,Prague, June 2007. c?2007 Association for Computational LinguisticsMultilingual Dependency Parsing using Global FeaturesTetsuji NakagawaOki Electric Industry Co., Ltd.2?5?7 Honmachi, Chuo-ku, Osaka 541?0053, Japannakagawa378@oki.comAbstractIn this paper, we describe a two-stage multi-lingual dependency parser used for the mul-tilingual track of the CoNLL 2007 sharedtask.
The system consists of two compo-nents: an unlabeled dependency parser us-ing Gibbs sampling which can incorporatesentence-level (global) features as well astoken-level (local) features, and a depen-dency relation labeling module based onSupport Vector Machines.
Experimental re-sults show that the global features are usefulin all the languages.1 IntroductionMaking use of as many informative features as pos-sible is crucial to obtain high performance in ma-chine learning based NLP.
Recently, several meth-ods for incorporating non-local features have beeninvestigated, though such features often make mod-els complex and thus complicate inference.
Collinsand Koo (2005) proposed a reranking method forphrase structure parsing with which any type ofglobal features in a parse tree can be used.
Fordependency parsing, McDonald and Pereira (2006)proposed a method which can incorporate sometypes of global features, and Riedel and Clarke(2006) studied a method using integer linear pro-gramming which can incorporate global linguisticconstraints.
In this paper, we study dependencyparsing using Gibbs sampling which can incorpo-rate any type of global feature in a sentence.
Theparser determines unlabeled dependency structuresonly, and we attach dependency relation labels us-ing Support Vector Machines afterwards.We participated in the multilingual track of theCoNLL 2007 shared task (Nivre et al, 2007), andevaluated the system on data sets of 10 languages(Hajic?
et al, 2004; Aduriz et al, 2003; Mart??
etal., 2007; Chen et al, 2003; Bo?hmova?
et al, 2003;Marcus et al, 1993; Johansson and Nugues, 2007;Prokopidis et al, 2005; Csendes et al, 2005; Mon-temagni et al, 2003; Oflazer et al, 2003).The rest of the paper describes the specification ofthe system and the evaluation results.2 Unlabeled Dependency Parsing usingGlobal Features2.1 Probabilistic ModelRosenfeld et al (2001) proposed whole-sentence ex-ponential language models which can incorporatearbitrary features in a sentence, and we consider herea similar probabilistic model for dependency pars-ing which can incorporate any sentence-level fea-ture.
Let w = w1 ?
?
?w|w| denote an input sentenceconsisting of |w| tokens, and h = h1 ?
?
?h|w| denotethe sequence of the indices of each token?s head.Root nodes of a sentence do not have heads, and weregard the index of a root node?s head as zero, i.e.,hi ?
{0, 1, ?
?
?
, |w|} \ {i}.
We define the probabil-ity distribution of the dependency structure h givena sentence w using exponential models as follows:P?,M(h|w)= 1Z?,M(w)QM(h|w)exp{ K?k=1?kfk(w,h)},(1)Z?,M(w)=?h?
?H(w)QM(h?|w) exp{ K?k=1?kfk(w,h?
)}, (2)where QM(h|w) is an initial distribution, fk(w,h)is the k-th feature function, K is the number of fea-ture functions, and ?k is the weight of the k-th fea-ture.
H(w) is the set of possible configurations ofheads for a given sentence w. Although it is ap-propriate that H(w) is the set of projective trees forprojective languages, and is the set of non-projectivetrees (which is a superset of the set of projectivetrees) for non-projective languages, in this study, wedefine H(w) to be the set of all the possible graphs,which contains |w||w| elements.
P?,M(h|w) andQM(h|w) are defined over H(w)1.
The probabil-ity distribution P?,M(h|w) is a joint distribution ofall the heads conditioned by a sentence, thereforewe call this model sentence-level model.
The fea-ture function fk(w,h) is defined on a sentence wwith heads h, and we can use any information in thesentence without the independence assumption forthe heads of the tokens, therefore we call fk(w,h)1H(w) is a superset of the set of non-projective trees, andis an unnecessarily large set which contains ill-formed depen-dency trees such as trees with cycles.
This issue may causereduction of parsing performance, but we adopt this approachfor computational efficiency.952sentence-level (global) feature.
We define initialdistribution QM(h|w) as the product of qM(h|w, t)which is the probability distribution of the head h ofeach t-th token calculated with maximum entropymodels:QM(h|w)=|w|?t=1qM(ht|w, t), (3)qM(h|w, t)= 1YM(w, t) exp{ L?l=1?lgl(w, t, h)}, (4)YM(w, t)=|w|?h?=0h?
6=texp{ L?l=1?lgl(w, t, h?
)}, (5)where gl(w, t, h) is the l-th feature function, L is thenumber of feature functions, and ?l is the weight ofthe l-th feature.
qM(h|w, t) is a model of the headof a single token, calculated independently fromother tokens, therefore we call qM(h|w, t) token-level model, and gl(w, t, h) token-level (local) fea-ture.2.2 Decoding and Parameter EstimationLet us consider how to find the optimal solutionh?, given a sentence w, parameters of the sentence-level model ?
= {?1, ?
?
?
, ?K}, and parameters ofthe token-level model M = {?1, ?
?
?
, ?L}.
Sincethe probabilistic model contains global features andefficient algorithms such as dynamic programmingcannot be used, we use Gibbs sampling to obtainan approximated solution.
Gibbs sampling can ef-ficiently generate samples from high-dimensionalprobability distributions with complex dependenciesamong variables (Andrieu et al, 2003), and we as-sume that R samples {h(1), ?
?
?
,h(R)} are generatedfrom P?,M(h|w) using Gibbs sampling.
Then, themarginal distribution of the head of the t-th tokengiven w, Pt(h|w), is approximately calculated asfollows:Pt(h|w) =?h1,???,ht?1,ht+1,???,h|w|ht=hP?,M(h|w),=?hP?,M(h|w)?
(h, ht) ' 1RR?r=1?
(h, h(r)t ), (6)where ?
(i, j) is the Kronecker delta.
In order tofind a solution using the marginal distribution, weadopt the maximum spanning tree (MST) frame-work proposed by McDonald et al (2005a).
In thisframework, scores for possible edges in dependencygraphs are defined, and the optimal dependency treeis found as the MST in which the summation of theedge scores is maximized.
Let s(i, j) denote thescore of the edge from a parent node (head) i to achild node (dependent) j.
We define s(i, j) as fol-lows:s(i, j)=logPj(i|w).
(7)We use the logarithm of the marginal distribution be-cause the summation of edge scores is maximizedby the MST search algorithms but the product of themarginal distributions should be maximized.
Thebest projective parse tree is obtained using the Eis-ner algorithm (Eisner, 1996) with the scores, and thebest non-projective one is obtained using the Chu-Liu-Edmonds (CLE) algorithm (McDonald et al,2005b).Although in this method, the factored score s(i, j)is used to measure likelihood of dependency trees,the score is calculated taking a whole sentence intoconsideration using Gibbs sampling.Next, we explain how to estimate the parame-ters of our models, given training data consisting ofN examples {?w1,h1?, ?
?
?
, ?wN ,hN ?}.
In orderto estimate the parameters of the token-level modelM = {?1, ?
?
?
, ?L}, we use maximum a posterioriestimation with Gaussian priors.
We define the fol-lowing objective function M:M=logN?n=1QM(hn|wn)?
12?2L?l=1?2l , (8)where ?
is a hyper parameter of Gaussian priors.The optimal parameters M which maximize M canbe obtained by quasi-Newton methods such as theL-BFGS algorithm with above M and its partialderivatives.
The parameters of the sentence-levelmodel ?
= {?1, ?
?
?
, ?K} can also be estimated ina similar way with the following objective functionL after the parameters of the token-level model areestimated.L=logN?n=1P?,M(hn|wn)?
12??2K?k=1?2k.
(9)This objective function and its partial derivative con-tain summations over all the possible configura-tions which are difficult to calculate.
We approx-imately calculate these values using static MonteCarlo (not MCMC) methods with fixed S samples{hn(1), ?
?
?
,hn(S)} generated from QM(h|wn)2:logZ?,M(wn)'log 1SS?s=1exp{ K?k=1?kfk(wn,hn(s))},(10)?h??H(wn)P?,M(h?|wn)fk(wn,h?)'
1SS?s=1fk(wn,hn(s))Z?,M(wn) exp{ K?k?=1?k?fk?(wn,hn(s))}.
(11)2Static Monte Carlo methods become inefficient when thedimension of the probabilistic distribution is high, and more so-phisticated methods would be used for accurate parameter esti-mation.9532.3 Local FeaturesThe token-level features used in the system are thesame as those used in MSTParser version 0.4.23.The features include lexical forms and (coarse andfine) POS tags of parent tokens, child tokens, theirsurrounding tokens, and tokens between the childand the parent.
The direction and the distance from aparent to its child, and the FEATS fields of the parentand the child which are split into elements and thencombined are also included.
Features that appearedless than 5 times in training data are ignored.2.4 Global FeaturesGlobal features can capture any information in de-pendency trees, and the following nine types ofglobal features are used (In the following, parentnode means a head token, and child node means adependent token):Child Unigram+Parent+Grandparent This fea-ture template is a 4-tuple consisting of (1) achild node, (2) its parent node, (3) the direc-tion from the parent node to the child node, and(4) the grandparent node.Each node in the feature template is expandedto its lexical form and coarse POS tag in or-der to obtain actual features.
Features that ap-peared in four or less sentences are ignored.The same procedure is applied to the followingother features.Child Bigram+Parent This feature template is a 4-tuple consisting of (1) a child node, (2) its par-ent node, (3) the direction from the parent nodeto the child node, and (4) the nearest outer sib-ling node (the nearest sibling node which existson the opposite side of the parent node) of thechild node.
This feature template is almost thesame as the one used by McDonald and Pereira(2006).Child Bigram+Parent+Grandparent This featuretemplate is a 5-tuple.
The first four ele-ments (1)?
(4) are the same as the Child Bi-gram+Parent feature template, and the addi-tional element (5) is the grandparent node.Child Trigram+Parent This feature template is a5-tuple.
The first four elements (1)?
(4) are thesame as the Child Bigram+Parent feature tem-plate, and the additional element (5) is the nextnearest outer sibling node of the child node.3http://sourceforge.net/projects/mstparserParent+All Children This feature template is a tu-ple with more than one element.
The first ele-ment is a parent node, and the other elementsare all of its child nodes.Parent+All Children+Grandparent This featuretemplate is a tuple with more than two ele-ments.
The elements other than the last oneare the same as the Parent+All Children featuretemplate, and the last element is the grandpar-ent node.Child+Ancestor This feature template is a 2-tupleconsisting of (1) a child node, and (2) one of itsancestor nodes.Acyclic This feature type has one of two values,true if the dependency tree is acyclic, or falseotherwise.Projective This feature type has one of two val-ues, true if the dependency tree is projective,or false otherwise.3 Dependency Relation Labeling3.1 ModelDependency relation labeling can be handled as amulti-class classification problem, and we use Sup-port Vector Machines (SVMs) which have been suc-cessfully applied to many NLP tasks.
Solving large-scale multi-class classification problem with SVMsrequires substantial computational resources, so weuse the revision learning method (Nakagawa et al,2002).
The revision learning method combinesa probabilistic model which has smaller computa-tional cost with a binary classifier which has highergeneralization capacity.
In the method, the latterclassifier revises the output of the former model toconduct multi-class classification with higher ac-curacy and reasonable computational cost.
In thisstudy, we use maximum entropy (ME) models asthe probabilistic model and SVMs with the secondorder polynomial kernel as the binary classifier.
Thedependency label of each node is determined inde-pendently of the labeling of other nodes.3.2 FeaturesAs the features for SVMs to predict the dependencyrelation label of the i-th token, we use the lexicalforms, coarse and fine POS tags, and FEATS fieldsof the i-th and the hi-th tokens.
We also use lex-ical forms and POS tags of the tokens surround-ing and in between them (i.e.
the j-th token wherej ?
{j|min{i, hi} ?
1 ?
j ?
max{i, hi} + 1}),the grandparent (hhi-th) token, the sibling tokens ofi (the j?-th token where j?
?
{j?|hj?
= hi, j?
6= i}),954Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish AverageLAS 75.08 72.56 87.90 83.84 80.19 88.41 76.31 76.74 83.61 78.22 80.29UAS 86.09 81.04 92.86 88.88 86.28 90.13 84.08 82.49 87.91 85.77 86.55Table 1: Results of Multilingual Dependency ParsingAlgorithm Features Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian TurkishEisner local 85.15 80.20 91.75 86.75 84.19 88.65 83.31 80.27 86.72 84.82(proj.)
+global 86.09 81.00 92.86 88.88 85.99 90.13 84.08 81.55 87.91 84.82CLE local 84.80 80.39 91.23 86.71 84.21 88.07 83.03 81.15 86.85 85.35(non-proj.)
+global 85.83 81.04 92.64 88.84 86.28 90.05 83.87 82.49 87.97 85.77Table 2: Unlabeled Attachment Scores in Different Settings (underlined values indicate submitted results,and bold values indicate the highest scores)and the child tokens of i (the j?
?-th token wherej??
?
{j??|hj??
= i})4.
As the features for ME mod-els, a subset of them is used since ME models areused just for reducing the search space, and do notneed so many features.4 Results and AnalysisIn order to tune the system, we split each trainingdata set into two parts, and used the first half fortraining and the remaining half for testing in devel-opment.
The CLE algorithm was used for Basque,Czech, Hungarian and Turkish, and the Eisner algo-rithm was used for the others.
We used lemmas forCatalan, Czech, Greek and Italian, and word formsfor all others.
The values of the parameters to befixed were chosen as R = 500, S = 200, ?
= 0.25,and ??
= 0.25.
With these parameter settings, train-ing took 247 hours, and testing took 343 minutes onan Opteron 250 processor.Table 1 shows the evaluation results on the testsets.
Accuracy was measured with the labeled at-tachment score (LAS) and the unlabeled attachmentscore (UAS).
Among the participating systems in theshared task, we obtained the second best averageaccuracy in the labeled attachment score, and thebest average accuracy in the unlabeled attachmentscore.
Compared with other systems, the gap be-tween our labeled and unlabeled scores is relativelybig.
In this study, labeling of dependency relationswas performed in a separate post-processing step,and each label was predicted independently.
The la-beled scores may be improved if the parsing processand the labeling process are performed at the sametime, and dependencies among labels are taken intoaccount.We conducted experiments with different settings.Table 2 shows the results measured with the unla-beled attachment score.
In the table, Eisner and4Although polynomial kernels of SVMs can implicitly han-dle combined features, some of combined features were also in-cluded explicitly because using unnecessarily high order poly-nomial kernels decreases performance.CLE indicate that the Eisner algorithm and theCLE algorithm are used in decoding, and local and+global indicate that local features alone, and localand global features together are used.
The CLE al-gorithm performed better than the Eisner algorithmfor Basque, Czech, Hungarian, Italian and Turkish.All of these data sets except Italian contain relativelya large number of non-projective sentences (the per-centage of sentences with at least one non-projectiverelation in the training data is over 20% (Nivre et al,2007)), though the Greek data set, on which the Eis-ner algorithm performed better, also contains manynon-projective sentences (20.3%).By using the global features, the accuracy wasimproved in all the cases except for Turkish withthe Eisner algorithm (Table 2).
The increase wasrather large in Chinese and Czech.
When the globalfeatures were used in these languages, the depen-dency accuracy for tokens whose heads had con-junctions as parts-of-speech was notably improved;from 80.5% to 86.0% in Chinese (Eisner), and from73.2% to 77.6% in Czech (CLE).
We investigatedthe trained global models, and found that Parent+AllChildren features, whose parents were conjunctionsand whose children had compatible classes, hadlarge positive weights, and those whose children hadincompatible classes had large negative weights.
Afeature with a larger weight is generally more influ-ential.
Riedel and Clarke (2006) suggested to uselinguistic constraints such as ?arguments of a coor-dination must have compatible word classes,?
andsuch constraint seemed to be represented by the fea-tures in our models.5 ConclusionIn this study, we applied a dependency parser us-ing global features to multilingual dependency pars-ing.
Evaluation results showed that the use of globalfeatures was effective to obtain higher accuracy inmultilingual dependency parsing.
Improving depen-dency relation labeling is left for future work.955ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque Dependency Tree-bank.
In Proc.
of the 2nd Workshop on Treebanks andLinguistic Theories (TLT), pages 201?204.C.
Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.2003.
An Introduction to MCMC for Machine Learn-ing.
Machine Learning, 50:5?43.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica Treebank: Design Crite-ria, Representational Issues and Implementation.
InAbeille?
(Abeille?, 2003), chapter 13, pages 231?248.M.
Collins and T. Koo.
2005.
Discriminative Rerank-ing for Natural Language Parsing.
Computational Lin-guistics, 31(1):25?69.D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.J.
Eisner.
1996.
Three New Probabilistic Models for De-pendency Parsing: An Exploration.
In Proc.
of COL-ING ?96, pages 340?345.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
?Snaidauf, and E. Bes?ka.2004.
Prague Arabic Dependency Treebank: Develop-ment in Data and Tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools,pages 110?117.R.
Johansson and P. Nugues.
2007.
ExtendedConstituent-to-Dependency Conversion for English.In Proc.
of the 16th Nordic Conference on Computa-tional Linguistics (NODALIDA).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English:the Penn Treebank.
Computational Linguistics,19(2):313?330.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A Multilingual and MultilevelAnnotated Corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.
InProc.
of EACL 2006, pages 81?88.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line Large-Margin Training of Dependency Parsers.
InProc.
of ACL 2005, pages 91?98.R.
McDonald, F. Pereira, K. Ribarow, and J. Hajic.2005b.
Non-projective dependency parsing usingSpanning Tree Algorithms.
In Proc.
of HLT/EMNLP2005, pages 523?530.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, andR.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(Abeille?, 2003), chap-ter 11, pages 189?210.T.
Nakagawa, T. Kudo, and Y. Matsumoto.
2002.
Re-vision Learning and its Application to Part-of-speechTagging.
In Proc.
of ACL 2002, pages 497?504.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007Shared Task on Dependency Parsing.
In Proc.
ofthe CoNLL 2007 Shared Task.
Joint Conf.
on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.2003.
Building a Turkish Treebank.
In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andPractical Issues in the Construction of a Greek Depen-dency Treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 149?160.S.
Riedel and J. Clarke.
2006.
Incremental Integer LinearProgramming for Non-projective Dependency Parsing.In Proc.
of EMNLP 2006, pages 129?137.R.
Rosenfeld, S. F. Chen, and X. Zhu.
2001.
Whole-Sentence Exponential Language Models: A Vehi-cle For Linguistic-Statistical Integration.
ComputersSpeech and Language, 15(1):55?73.956
