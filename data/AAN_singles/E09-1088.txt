Proceedings of the 12th Conference of the European Chapter of the ACL, pages 772?780,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSequential Labeling with Latent Variables:An Exact Inference Algorithm and Its Efficient ApproximationXu Sun?
Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan?School of Computer Science, University of Manchester, UK?National Centre for Text Mining, Manchester, UK{sunxu, tsujii}@is.s.u-tokyo.ac.jpAbstractLatent conditional models have becomepopular recently in both natural languageprocessing and vision processing commu-nities.
However, establishing an effectiveand efficient inference method on latentconditional models remains a question.
Inthis paper, we describe the latent-dynamicinference (LDI), which is able to producethe optimal label sequence on latent con-ditional models by using efficient searchstrategy and dynamic programming.
Fur-thermore, we describe a straightforwardsolution on approximating the LDI, andshow that the approximated LDI performsas well as the exact LDI, while the speed ismuch faster.
Our experiments demonstratethat the proposed inference algorithm out-performs existing inference methods ona variety of natural language processingtasks.1 IntroductionWhen data have distinct sub-structures, mod-els exploiting latent variables are advantageousin learning (Matsuzaki et al, 2005; Petrov andKlein, 2007; Blunsom et al, 2008).
Actu-ally, discriminative probabilistic latent variablemodels (DPLVMs) have recently become popu-lar choices for performing a variety of tasks withsub-structures, e.g., vision recognition (Morencyet al, 2007), syntactic parsing (Petrov and Klein,2008), and syntactic chunking (Sun et al, 2008).Morency et al (2007) demonstrated that DPLVMmodels could efficiently learn sub-structures ofnatural problems, and outperform several widely-used conventional models, e.g., support vector ma-chines (SVMs), conditional random fields (CRFs)and hidden Markov models (HMMs).
Petrov andKlein (2008) reported on a syntactic parsing taskthat DPLVM models can learn more compact andaccurate grammars than the conventional tech-niques without latent variables.
The effectivenessof DPLVMs was also shown on a syntactic chunk-ing task by Sun et al (2008).DPLVMs outperform conventional learningmodels, as described in the aforementioned pub-lications.
However, inferences on the latent condi-tional models are remaining problems.
In conven-tional models such as CRFs, the optimal label pathcan be efficiently obtained by the dynamic pro-gramming.
However, for latent conditional mod-els such as DPLVMs, the inference is not straight-forward because of the inclusion of latent vari-ables.In this paper, we propose a new inference al-gorithm, latent dynamic inference (LDI), by sys-tematically combining an efficient search strategywith the dynamic programming.
The LDI is anexact inference method producing the most prob-able label sequence.
In addition, we also proposean approximated LDI algorithm for faster speed.We show that the approximated LDI performs aswell as the exact one.
We will also discuss apost-processing method for the LDI algorithm: theminimum bayesian risk reranking.The subsequent section describes an overviewof DPLVM models.
We discuss the probabilitydistribution of DPLVM models, and present theLDI inference in Section 3.
Finally, we reportexperimental results and begin our discussions inSection 4 and Section 5.772y1 y2 ymxmx2x1h1 h2 hmxmx2x1ymy2y1CRF DPLVMFigure 1: Comparison between CRF models andDPLVM models on the training stage.
x representsthe observation sequence, y represents labels andh represents the latent variables assigned to the la-bels.
Note that only the white circles are observedvariables.
Also, only the links with the current ob-servations are shown, but for both models, longrange dependencies are possible.2 Discriminative Probabilistic LatentVariable ModelsGiven the training data, the task is to learn a map-ping between a sequence of observations x =x1, x2, .
.
.
, xm and a sequence of labels y =y1, y2, .
.
.
, ym.
Each yj is a class label for the j?thtoken of a word sequence, and is a member of aset Y of possible class labels.
For each sequence,the model also assumes a sequence of latent vari-ables h = h1, h2, .
.
.
, hm, which is unobservablein training examples.The DPLVM model is defined as follows(Morency et al, 2007):P (y|x,?)
=?hP (y|h,x,?
)P (h|x,?
), (1)where ?
represents the parameter vector of themodel.
DPLVM models can be seen as a naturalextension of CRF models, and CRF models canbe seen as a special case of DPLVMs that employonly one latent variable for each label.To make the training and inference efficient, themodel is restricted to have disjointed sets of latentvariables associated with each class label.
Eachhj is a member in a set Hyj of possible latent vari-ables for the class label yj .
H is defined as the setof all possible latent variables, i.e., the union of allHyj sets.
Since sequences which have any hj /?Hyj will by definition have P (y|hj ,x,?)
= 0,the model can be further defined as:P (y|x,?)
=?h?Hy1?...
?HymP (h|x,?
), (2)where P (h|x,?)
is defined by the usual condi-tional random field formulation:P (h|x,?)
= exp??f(h,x)?
?h exp?
?f(h,x), (3)in which f(h,x) is a feature vector.
Given a train-ing set consisting of n labeled sequences, (xi,yi),for i = 1 .
.
.
n, parameter estimation is performedby optimizing the objective function,L(?)
=n?i=1logP (yi|xi,?)?R(?).
(4)The first term of this equation represents a condi-tional log-likelihood of a training data.
The sec-ond term is a regularizer that is used for reducingoverfitting in parameter estimation.3 Latent-Dynamic InferenceOn latent conditional models, marginalizing la-tent paths exactly for producing the optimal la-bel path is a computationally expensive prob-lem.
Nevertheless, we had an interesting observa-tion on DPLVM models that they normally had ahighly concentrated probability mass, i.e., the ma-jor probability are distributed on top-n ranked la-tent paths.Figure 2 shows the probability distribution ofa DPLVM model using a L2 regularizer with thevariance ?2 = 1.0.
As can be seen, the probabil-ity distribution is highly concentrated, e.g., 90%of the probability is distributed on top-800 latentpaths.Based on this observation, we propose an infer-ence algorithm for DPLVMs by efficiently com-bining search and dynamic programming.3.1 LDI InferenceIn the inference stage, given a test sequence x, wewant to find the most probable label sequence, y?:y?
= argmaxyP (y|x,??).
(5)For latent conditional models like DPLVMs, they?
cannot directly be produced by the Viterbialgorithm because of the incorporation of latentvariables.In this section, we describe an exact inferencealgorithm, the latent-dynamic inference (LDI),for producing the optimal label sequence y?
onDPLVMs (see Figure 3).
In short, the algorithm7730204060801000.4K 0.8K 1.2K 1.6K 2KTop-nProbabilityMass(%)nFigure 2: The probability mass distribution of la-tent conditional models on a NP-chunking task.The horizontal line represents the n of top-n latentpaths.
The vertical line represents the probabilitymass of the top-n latent paths.generates the best latent paths in the order of theirprobabilities.
Then it maps each of these to its as-sociated label paths and uses a method to computetheir exact probabilities.
It can continue to gener-ate the next best latent path and the associated la-bel path until there is not enough probability massleft to beat the best label path.In detail, an A?
search algorithm1 (Hart et al,1968) with a Viterbi heuristic function is adoptedto produce top-n latent paths, h1,h2, .
.
.hn.
Inaddition, a forward-backward-style algorithm isused to compute the exact probabilities of theircorresponding label paths, y1,y2, .
.
.yn.
Themodel then tries to determine the optimal labelpath based on the top-n statistics, without enumer-ating the remaining low-probability paths, whichcould be exponentially enormous.The optimal label path y?
is ready when the fol-lowing ?exact-condition?
is achieved:P (y1|x,?)?(1?
?yk?LPnP (yk|x,?))
?
0, (6)where y1 is the most probable label sequencein current stage.
It is straightforward to provethat y?
= y1, and further search is unnecessary.This is because the remaining probability mass,1?
?yk?LPn P (yk|x,?
), cannot beat the currentoptimal label path in this case.1A?
search and its variants, like beam-search, are widelyused in statistical machine translation.
Compared to othersearch techniques, an interesting point of A?
search is that itcan produce top-n results one-by-one in an efficient manner.Definition:Proj(h) = y ??
hj ?
Hyj for j = 1 .
.
.m;P (h) = P (h|x,?
);P (y) = P (y|x,?
).Input:weight vector ?, and feature vector F (h,x).Initialization:Gap = ?1; n = 0; P (y?)
= 0; LP0 = ?.Algorithm:while Gap < 0 don = n+ 1hn = HeapPop[?, F (h,x)]yn = Proj(hn)if yn /?
LPn?1 thenP (yn) = DynamicProg?h:Proj(h)=yn P (h)LPn = LPn?1 ?
{yn}if P (yn) > P (y?)
theny?
= ynGap = P (y?)?(1?
?yk?LPn P (yk))elseLPn = LPn?1Output:the most probable label sequence y?.Figure 3: The exact LDI inference for latent condi-tional models.
In the algorithm, HeapPop meanspopping the next hypothesis from the A?
heap; Bythe definition of the A?
search, this hypothesis (onthe top of the heap) should be the latent path withmaximum probability in current stage.3.2 Implementation IssuesWe have presented the framework of the LDI in-ference.
Here, we describe the details on imple-menting its two important components: designingthe heuristic function, and an efficient method tocompute the probabilities of label path.As described, the A?
search can produce top-nresults one-by-one using a heuristic function (thebackward term).
In the implementation, we usethe Viterbi algorithm (Viterbi, 1967) to computethe admissible heuristic function for the forward-style A?
search:Heui(hj) = maxh?i=hj?h?
?HP|h|iP ?(h?
|x,??
), (7)where h?i = hj represents a partial latent pathstarted from the latent variable hj .
HP|h|i rep-resents all possible partial latent paths from the774position i to the ending position, |h|.
As de-scribed in the Viterbi algorithm, the backwardterm, Heui(hj), can be efficiently computed byusing dynamic programming to reuse the terms(e.g., Heui+1(hj)) in previous steps.
Because thisViterbi heuristic is quite good in practice, this waywe can produce the exact top-n latent paths effi-ciently (see efficiency comparisons in Section 5),even though the original problem is NP-hard.The probability of a label path, P (yn) in Fig-ure 3, can be efficiently computed by a forward-backward algorithm with a restriction on the targetlabel path:P (y|x,?)
=?h?Hy1?...
?HymP (h|x,?).
(8)3.3 An Approximated Version of the LDIBy simply setting a threshold value on the searchstep, n, we can approximate the LDI, i.e., LDI-Approximation (LDI-A).
This is a quite straight-forward method for approximating the LDI.
Infact, we have also tried other methods for approx-imation.
Intuitively, one alternative method is todesign an approximated ?exact condition?
by us-ing a factor, ?, to estimate the distribution of theremaining probability:P (y1|x,?)??(1?
?yk?LPnP (yk|x,?))
?
0.
(9)For example, if we believe that at most 50% of theunknown probability, 1 ?
?yk?LPn P (yk|x,?
),can be distributed on a single label path, we canset ?
= 0.5 to make a loose condition to stop theinference.
At first glance, this seems to be quitenatural.
However, when we compared this alter-native method with the aforementioned approxi-mation on search steps, we found that it workedworse than the latter, in terms of performance andspeed.
Therefore, we focus on the approximationon search steps in this paper.3.4 Comparison with Existing InferenceMethodsIn Matsuzaki et al (2005), the Best Hidden Pathinference (BHP) was used:yBHP = argmaxyP (hy|x,??
), (10)where hy ?
Hy1 ?
.
.
.
?Hym .
In other words,the Best Hidden Path is the label sequencewhich is directly projected from the optimal la-tent path h?.
The BHP inference can be seenas a special case of the LDI, which replaces themarginalization-operation over latent paths withthe max-operation.In Morency et al (2007), y?
is estimated by theBest Point-wise Marginal Path (BMP) inference.To estimate the label yj of token j, the marginalprobabilities P (hj = a|x,?)
are computed forall possible latent variables a ?
H. Then themarginal probabilities are summed up accordingto the disjoint sets of latent variables Hyj and theoptimal label is estimated by the marginal proba-bilities at each position i:yBMP (i) = argmaxyi?YP (yi|x,??
), (11)whereP (yi = a|x,?)
=?h?Ha P (h|x,?
)?h P (h|x,?).
(12)Although the motivation is similar, the exactLDI (LDI-E) inference described in this paper is adifferent algorithm compared to the BLP inference(Sun et al, 2008).
For example, during the search,the LDI-E is able to compute the exact probabilityof a label path by using a restricted version of theforward-backward algorithm, also, the exact con-dition is different accordingly.
Moreover, in thispaper, we more focus on how to approximate theLDI inference with high performance.The LDI-E produces y?
while the LDI-A, theBHP and the BMP perform estimation on y?.
Wewill compare them via experiments in Section 4.4 ExperimentsIn this section, we choose Bio-NER and NP-chunking tasks for experiments.
First, we describethe implementations and settings.We implemented DPLVMs by extending theHCRF library developed by Morency et al (2007).We added a Limited-Memory BFGS optimizer(L-BFGS) (Nocedal and Wright, 1999), and re-implemented the code on training and inferencefor higher efficiency.
To reduce overfitting, weemployed a Gaussian prior (Chen and Rosenfeld,1999).
We varied the the variance of the Gaussianprior (with values 10k, k from -3 to 3), and wefound that ?2 = 1.0 is optimal for DPLVMs onthe development data, and used it throughout theexperiments in this section.775The training stage was kept the same asMorency et al (2007).
In other words, thereis no need to change the conventional parameterestimation method on DPLVM models for adapt-ing the various inference algorithms in this paper.For more information on training DPLVMs, referto Morency et al (2007) and Petrov and Klein(2008).Since the CRF model is one of the most success-ful models in sequential labeling tasks (Lafferty etal., 2001; Sha and Pereira, 2003), in this paper, wechoosed CRFs as a baseline model for the compar-ison.
Note that the feature sets were kept the samein DPLVMs and CRFs.
Also, the optimizer andfine tuning strategy were kept the same.4.1 BioNLP/NLPBA-2004 Shared Task(Bio-NER)Our first experiment used the data from theBioNLP/NLPBA-2004 shared task.
It is a biomed-ical named-entity recognition task on the GENIAcorpus (Kim et al, 2004).
Named entity recogni-tion aims to identify and classify technical termsin a given domain (here, molecular biology) thatrefer to concepts of interest to domain experts.The training set consists of 2,000 abstracts fromMEDLINE; and the evaluation set consists of 404abstracts from MEDLINE.
We divided the origi-nal training set into 1,800 abstracts for the trainingdata and 200 abstracts for the development data.The task adopts the BIO encoding scheme, i.e.,B-x for words beginning an entity x, I-x forwords continuing an entity x, and O for words be-ing outside of all entities.
The Bio-NER task con-tains 5 different named entities with 11 BIO en-coding labels.The standard evaluation metrics for this task areprecision p (the fraction of output entities match-ing the reference entities), recall r (the fractionof reference entities returned), and the F-measuregiven by F = 2pr/(p+ r).Following Okanohara et al (2006), we usedword features, POS features and orthography fea-tures (prefix, postfix, uppercase/lowercase, etc.
),as listed in Table 1.
However, their globally depen-dent features, like preceding-entity features, werenot used in our system.
Also, to speed up thetraining, features that appeared rarely in the train-ing data were removed.
For DPLVM models, wetuned the number of latent variables per label from2 to 5 on preliminary experiments, and used theWord Features:{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi,wiwi+1}?
{hi, hi?1hi}POS Features:{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti,titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1,titi+1ti+2}?
{hi, hi?1hi}Orth.
Features:{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi,oioi+1, oi+1oi+2}?
{hi, hi?1hi}Table 1: Feature templates used in the Bio-NERexperiments.
wi is the current word, ti is the cur-rent POS tag, oi is the orthography mode of thecurrent word, and hi is the current latent variable(for the case of latent models) or the current label(for the case of conventional models).
No globallydependent features were used; also, no external re-sources were used.Word Features:{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi,wiwi+1}?
{hi, hi?1hi}Table 2: Feature templates used in the NP-chunking experiments.
wi and hi are defined fol-lowing Table 1.number 4.Two sets of experiments were performed.
First,on the development data, the value of n (the searchstep, see Figure 3 for its definition) was varied inthe LDI inference; the corresponding F-measure,exactitude (the fraction of sentences that achievedthe exact condition, Eq.
6), #latent-path (num-ber of latent paths that have been searched), andinference-time were measured.
Second, the ntuned on the development data was employed forthe LDI on the test data, and experimental com-parisons with the existing inference methods, theBHP and the BMP, were made.4.2 NP-Chunking TaskOn the Bio-NER task, we have studied the LDIon a relatively rich feature-set, including wordfeatures, POS features and orthographic features.However, in practice, there are many tasks with776Models S.A. Pre.
Rec.
F1 TimeLDI-A 40.64 68.34 66.50 67.41 0.4K sLDI-E 40.76 68.36 66.45 67.39 4K sBMP 39.10 65.85 66.49 66.16 0.3K sBHP 39.93 67.60 65.46 66.51 0.1K sCRF 37.44 63.69 64.66 64.17 0.1K sTable 3: On the test data of the Bio-NER task, ex-perimental comparisons among various inferencealgorithms on DPLVMs, and the performance ofCRFs.
S.A. signifies sentence accuracy.
As canbe seen, at a much lower cost, the LDI-A (A signi-fies approximation) performed slightly better thanthe LDI-E (E signifies exact).only poor features available.
For example, in POS-tagging task and Chinese/Japanese word segmen-tation task, there are only word features available.For this reason, it is necessary to check the perfor-mance of the LDI on poor feature-set.
We choseanother popular task, the NP-chunking, for thisstudy.
Here, we used only poor feature-set, i.e.,feature templates that depend only on words (seeTable 2 for details), taking into account 200K fea-tures.
No external resources were used.The NP-chunking data was extracted from thetraining/test data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz, 2000).
Inthis task, the non-recursive cores of noun phrasescalled base NPs are identified.
The training setconsists of 8,936 sentences, and the test set con-sists of 2,012 sentences.
Our preliminary exper-iments in this task suggested the use of 5 latentvariables for each label on latent models.5 Results and Discussions5.1 Bio-NERFigure 4 shows the F-measure, exactitude, #latent-path and inference inference time of the DPLVM-LDI model, against the parameter n (the searchstep, see Table 3), on the development dataset.
Ascan be seen, there was a dramatic climbing curveon the F-measure, from 68.78% to 69.73%, whenwe increased the number of the search step from1 to 30.
When n = 30, the F-measure has al-ready reached its plateau, with the exactitude of83.0%, and the inference time of 80 seconds.
Inother words, the F-measure approached its plateauwhen n went to 30, with a high exactitude and alow inference time.6869700K 2K 4K 6K 8K 10KF-measure(%)657075808590950K 2K 4K 6K 8K 10KExactitude(%)01002003004005006007000K 2K 4K 6K 8K 10K#latent-path00.20.40.60.811.21.40K 2K 4K 6K 8K 10KTime(Ks)n6869700 50 100 150 200 250657075808590950 50 100 150 200 25001002003004005006000 50 100 150 200 25000.20.40.60.811.21.40 50 100 150 200 250nFigure 4: (Left) F-measure, exactitude, #latent-path (averaged number of latent paths beingsearched), and inference time of the DPLVM-LDImodel, against the parameter n, on the develop-ment dataset of the Bio-NER task.
(Right) En-largement of the beginning portion of the left fig-ures.
As can be seen, the curve of the F-measureapproached its plateau when n went to 30, with ahigh exactitude and a low inference time.Our significance test based on McNemar?s test(Gillick and Cox, 1989) shows that the LDI withn = 30 was significantly more accurate (P <0.01) than the BHP inference, while the inferencetime was at a comparable level.
Further growthof n after the beginning point of the plateau in-creases the inference time linearly (roughly), butachieved only very marginal improvement on F-measure.
This suggests that the LDI inference canbe approximated aggressively by stopping the in-ference within a small number of search steps, n.This can achieve high efficiency, without an obvi-ous degradation on the performance.Table 3 shows the experimental comparisonsamong the LDI-Approximation, the LDI-Exact(here, exact means the n is big enough, e.g., n =10K), the BMP, and the BHP on DPLVM mod-777Models S.A. Pre.
Rec.
F1 TimeLDI-A 60.98 91.76 90.59 91.17 42 sLDI-E 60.88 91.72 90.61 91.16 1K sBHP 59.34 91.54 90.30 90.91 25 sCRF 58.37 90.92 90.33 90.63 18 sTable 4: Experimental comparisons among differ-ent inference algorithms on DPLVMs, and the per-formance of CRFs using the same feature set onthe word features.els.
The baseline was the CRF model with thesame feature set.
On the LDI-A, the parameter ntuned on the development data was employed, i.e.,n = 30.To our surprise, the LDI-A performed slightlybetter than the LDI-E even though the perfor-mance difference was marginal.
We expected thatLDI-A would perform worse than the LDI-E be-cause LDI-A uses the aggressive approximationfor faster speed.
We have not found the exactcause of this interesting phenomenon, but remov-ing latent paths with low probabilities may resem-ble the strategy of pruning features with low fre-quency in the training phase.
Further analysis isrequired in the future.The LDI-A significantly outperformed the BHPand the BMP, with a comparable inference time.Also, all models of DPLVMs significantly outper-formed CRFs.5.2 NP-ChunkingAs can be seen in Figure 5, compared to Figure 4of the Bio-NER task, very similar curves were ob-served in the NP-chunking task.
It is interestingbecause the tasks are different, and their featuresets are very different.The F-measure reached its plateau when n wasaround 30, with a fast inference speed.
Thisechoes the experimental results on the Bio-NERtask.
Moreover, as can be seen in Table 4, at amuch lower cost on inference time, the LDI-A per-formed as well as the LDI-E.
The LDI-A outper-forms the BHP inference.
All the DPLVM mod-els outperformed CRFs.
The experimental resultsdemonstrate that the LDI also works well on poorfeature-set.8989.289.489.689.80K 2K 4K 6K 8K 10KF-measure(%)657075808590950K 2K 4K 6K 8K 10KExactitude(%)02004006008000K 2K 4K 6K 8K 10K#latent-path00.20.40.60.80K 2K 4K 6K 8K 10KTime(Ks)n8989.289.489.689.80 50 100 150 200 250657075808590950 50 100 150 200 25002004006008000 50 100 150 200 25000.20.40.60.80 50 100 150 200 250nFigure 5: (Left) F-measure, exactitude, #latent-path, and inference time of the DPLVM-LDImodel against the parameter n on the NP-chunking development dataset.
(Right) Enlarge-ment of the beginning portion of the left figures.The curves echo the results on the Bio-NER task.5.3 Post-Processing of the LDI: MinimumBayesian Risk RerankingAlthough the label sequence produced by the LDIinference is indeed the optimal label sequence bymeans of probability, in practice, it may be benefi-cial to use some post-processing methods to adaptthe LDI towards factual evaluation metrics.
Forexample, in practice, many natural language pro-cessing tasks are evaluated by F-measures basedon chunks (e.g., named entities).We further describe in this section the MBRreranking method for the LDI.
Here MBR rerank-ing can be seen as a natural extension of the LDIfor adapting it to various evaluation criterions,EVAL:yMBR=argmaxy?y?
?LPnP (y?)fEVAL(y|y?).
(13)The intuition behind our MBR reranking is the778Models Pre.
Rec.
F1 TimeLDI-A 91.76 90.59 91.17 42 sLDI-A + MBR 92.22 90.40 91.30 61 sTable 5: The effect of MBR reranking on the NP-chunking task.
As can be seen, MBR-rerankingimproved the performance of the LDI.?voting?
by those results (label paths) produced bythe LDI inference.
Each label path is a voter, andit gives another one a ?score?
(the score depend-ing on the reference y?
and the evaluation met-ric EVAL, i.e., fEVAL(y|y?))
with a ?confidence?
(the probability of this voter, i.e., P (y?)).
Finally,the label path with the highest value, combiningscores and confidences, will be the optimal result.For more details of the MBR technique, refer toGoel & Byrne (2000) and Kumar & Byrne (2002).An advantage of the LDI over the BHP and theBMP is that the LDI can efficiently produce theprobabilities of the label sequences in LPn.
Suchprobabilities can be used directly for performingthe MBR reranking.
We will show that it is easyto employ the MBR reranking for the LDI, be-cause the necessary statistics (e.g., the probabili-ties of the label paths, y1,y2, .
.
.yn) are alreadyproduced.
In other words, by using LDI infer-ence, a set of possible label sequences has beencollected with associated probabilities.
Althoughthe cardinality of the set may be small, it accountsfor most of the probability mass by the definitionof the LDI.
Eq.13 can be directly applied on thisset to perform reranking.In contrast, the BHP and the BMP inference areunable to provide such information for the rerank-ing.
For this reason, we can only report the resultsof the reranking for the LDI.As can be seen in Table 5, MBR-reranking im-proved the performance of the LDI on the NP-chunking task with a poor feature set.
The pre-sented MBR reranking algorithm is a general so-lution for various evaluation criterions.
We cansee that the different evaluation criterion, EVAL,shares the common framework in Eq.
13.
In prac-tice, it is only necessary to re-implement the com-ponent of fEVAL(y,y?)
for a different evaluationcriterion.
In this paper, the evaluation criterion isthe F-measure.6 Conclusions and Future WorkIn this paper, we propose an inference method, theLDI, which is able to decode the optimal label se-quence on latent conditional models.
We studythe properties of the LDI, and showed that it canbe approximated aggressively for high efficiency,with no loss in the performance.
On the two NLPtasks, the LDI-A outperformed the existing infer-ence methods on latent conditional models, and itsinference time was comparable to that of the exist-ing inference methods.We also briefly present a post-processingmethod, i.e., MBR reranking, upon the LDIalgorithm for various evaluation purposes.
Itdemonstrates encouraging improvement on theNP-chunking tasks.
In the future, we plan to per-form further experiments to make a more detailedstudy on combining the LDI inference and theMBR reranking.The LDI inference algorithm is not necessarilylimited in linear-chain structure.
It could be ex-tended to other latent conditional models with treestructure (e.g., syntactic parsing with latent vari-ables), as long as it allows efficient combinationof search and dynamic-programming.
This couldalso be a future work.AcknowledgmentsWe thank Xia Zhou, Yusuke Miyao, Takuya Mat-suzaki, Naoaki Okazaki and Galen Andrew for en-lightening discussions, as well as the anonymousreviewers who gave very helpful comments.
Thefirst author was partially supported by Universityof Tokyo Fellowship (UT-Fellowship).
This workwas partially supported by Grant-in-Aid for Spe-cially Promoted Research (MEXT, Japan).ReferencesPhillip Blunsom, Trevor Cohn, and Miles Osborne.2008.
A discriminative latent variable model for sta-tistical machine translation.
Proceedings of ACL?08.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical Report CMU-CS-99-108, CMU.L.
Gillick and S. Cox.
1989.
Some statistical issuesin the comparison of speech recognition algorithms.International Conference on Acoustics Speech andSignal Processing, v1:532?535.V.
Goel and W. Byrne.
2000.
Minimum bayes-risk au-tomatic speech recognition.
Computer Speech andLanguage, 14(2):115?135.779P.E.
Hart, N.J. Nilsson, and B. Raphael.
1968.
Aformal basis for the heuristic determination of mini-mum cost path.
IEEE Trans.
On System Science andCybernetics, SSC-4(2):100?107.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,and Yuka Tateisi.
2004.
Introduction to the bio-entity recognition task at JNLPBA.
Proceedings ofJNLPBA?04, pages 70?75.S.
Kumar and W. Byrne.
2002.
Minimum bayes-risk alignment of bilingual texts.
Proceedings ofEMNLP?02.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
Proceedings of ICML?01, pages 282?289.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-jii.
2005.
Probabilistic CFG with latent annotations.Proceedings of ACL?05.Louis-Philippe Morency, Ariadna Quattoni, and TrevorDarrell.
2007.
Latent-dynamic discriminative mod-els for continuous gesture recognition.
Proceedingsof CVPR?07, pages 1?8.Jorge Nocedal and Stephen J. Wright.
1999.
Numeri-cal optimization.
Springer.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?chi Tsujii.
2006.
Improving the scal-ability of semi-markov conditional random fields fornamed entity recognition.
Proceedings of ACL?06.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT-NAACL?07), pages 404?411, Rochester, New York, April.
Association forComputational Linguistics.Slav Petrov and Dan Klein.
2008.
Discriminativelog-linear grammars with latent variables.
In J.C.Platt, D. Koller, Y.
Singer, and S. Roweis, editors,Advances in Neural Information Processing Systems20 (NIPS), pages 1153?1160, Cambridge, MA.
MITPress.Erik Tjong Kim Sang and Sabine Buchholz.
2000.
In-troduction to the CoNLL-2000 shared task: Chunk-ing.
Proceedings of CoNLL?00, pages 127?132.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
Proceedings ofHLT/NAACL?03.Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,and Jun?ichi Tsujii.
2008.
Modeling latent-dynamicin shallow parsing: A latent conditional model withimproved inference.
Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING?08), pages 841?848.Andrew J. Viterbi.
1967.
Error bounds for convolu-tional codes and an asymptotically optimum decod-ing algorithm.
IEEE Transactions on InformationTheory, 13(2):260?269.780
