Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDynamic Programming for Linear-Time Incremental ParsingLiang HuangUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292lhuang@isi.eduKenji SagaeUSC Institute for Creative Technologies13274 Fiji WayMarina del Rey, CA 90292sagae@ict.usc.eduAbstractIncremental parsing techniques such asshift-reduce have gained popularity thanksto their efficiency, but there remains amajor problem: the search is greedy andonly explores a tiny fraction of the wholespace (even with beam search) as op-posed to dynamic programming.
We showthat, surprisingly, dynamic programmingis in fact possible for many shift-reduceparsers, by merging ?equivalent?
stacksbased on feature values.
Empirically, ouralgorithm yields up to a five-fold speedupover a state-of-the-art shift-reduce depen-dency parser with no loss in accuracy.
Bet-ter search also leads to better learning, andour final parser outperforms all previouslyreported dependency parsers for Englishand Chinese, yet is much faster.1 IntroductionIn terms of search strategy, most parsing al-gorithms in current use for data-driven parsingcan be divided into two broad categories: dy-namic programming which includes the domi-nant CKY algorithm, and greedy search which in-cludes most incremental parsing methods such asshift-reduce.1 Both have pros and cons: the for-mer performs an exact search (in cubic time) overan exponentially large space, while the latter ismuch faster (in linear-time) and is psycholinguis-tically motivated (Frazier and Rayner, 1982), butits greedy nature may suffer from severe search er-rors, as it only explores a tiny fraction of the wholespace even with a beam.Can we combine the advantages of both ap-proaches, that is, construct an incremental parser1McDonald et al (2005b) is a notable exception: the MSTalgorithm is exact search but not dynamic programming.that runs in (almost) linear-time, yet searches overa huge space with dynamic programming?Theoretically, the answer is negative, as Lee(2002) shows that context-free parsing can be usedto compute matrix multiplication, where sub-cubicalgorithms are largely impractical.We instead propose a dynamic programming al-ogorithm for shift-reduce parsing which runs inpolynomial time in theory, but linear-time (withbeam search) in practice.
The key idea is to mergeequivalent stacks according to feature functions,inspired by Earley parsing (Earley, 1970; Stolcke,1995) and generalized LR parsing (Tomita, 1991).However, our formalism is more flexible and ouralgorithm more practical.
Specifically, we makethe following contributions:?
theoretically, we show that for a large classof modern shift-reduce parsers, dynamic pro-gramming is in fact possible and runs in poly-nomial time as long as the feature functionsare bounded and monotonic (which almost al-ways holds in practice);?
practically, dynamic programming is up tofive times faster (with the same accuracy) asconventional beam-search on top of a state-of-the-art shift-reduce dependency parser;?
as a by-product, dynamic programming canoutput a forest encoding exponentially manytrees, out of which we can draw better andlonger k-best lists than beam search can;?
finally, better and faster search also leads tobetter and faster learning.
Our final parserachieves the best (unlabeled) accuracies thatwe are aware of in both English and Chi-nese among dependency parsers trained onthe Penn Treebanks.
Being linear-time, it isalso much faster than most other parsers,even with a pure Python implementation.1077input: w0 .
.
.
wn?1axiom 0 : ?0, ??
: 0sh?
: ?j, S?
: c?
+ 1 : ?j + 1, S|wj?
: c + ?j < nrex?
: ?j, S|s1|s0?
: c?
+ 1 : ?j, S|s1xs0?
: c + ?rey?
: ?j, S|s1|s0?
: c?
+ 1 : ?j, S|s1ys0?
: c + ?goal 2n?
1 : ?n, s0?
: cwhere ?
is the step, c is the cost, and the shift cost ?and reduce costs ?
and ?
are:?
= w ?
fsh(j, S) (1)?
= w ?
frex (j, S|s1|s0) (2)?
= w ?
frey (j, S|s1|s0) (3)Figure 1: Deductive system of vanilla shift-reduce.For convenience of presentation and experimen-tation, we will focus on shift-reduce parsing fordependency structures in the remainder of this pa-per, though our formalism and algorithm can alsobe applied to phrase-structure parsing.2 Shift-Reduce Parsing2.1 Vanilla Shift-ReduceShift-reduce parsing performs a left-to-right scanof the input sentence, and at each step, choose oneof the two actions: either shift the current wordonto the stack, or reduce the top two (or more)items at the end of the stack (Aho and Ullman,1972).
To adapt it to dependency parsing, we splitthe reduce action into two cases, rex and rey, de-pending on which one of the two items becomesthe head after reduction.
This procedure is knownas ?arc-standard?
(Nivre, 2004), and has been en-gineered to achieve state-of-the-art parsing accu-racy in Huang et al (2009), which is also the ref-erence parser in our experiments.2More formally, we describe a parser configura-tion by a state ?j, S?
where S is a stack of treess0, s1, ... where s0 is the top tree, and j is the2There is another popular variant, ?arc-eager?
(Nivre,2004; Zhang and Clark, 2008), which is more complicatedand less similar to the classical shift-reduce algorithm.input: ?I saw Al with Joe?step action stack queue0 - I ...1 sh I saw ...2 sh I saw Al ...3 rex Ixsaw Al ...4 sh Ixsaw Al with ...5a rey IxsawyAl with ...5b sh Ixsaw Al with JoeFigure 2: A trace of vanilla shift-reduce.
Afterstep (4), the parser branches off into (5a) or (5b).queue head position (current word q0 is wj).
Ateach step, we choose one of the three actions:1. sh: move the head of queue, wj , onto stack Sas a singleton tree;2. rex: combine the top two trees on the stack,s0 and s1, and replace them with tree s1xs0.3.
rey: combine the top two trees on the stack,s0 and s1, and replace them with tree s1ys0.Note that the shorthand notation txt?
denotes anew tree by ?attaching tree t?
as the leftmost childof the root of tree t?.
This procedure can be sum-marized as a deductive system in Figure 1.
Statesare organized according to step ?, which denotesthe number of actions accumulated.
The parserruns in linear-time as there are exactly 2n?1 stepsfor a sentence of n words.As an example, consider the sentence ?I saw Alwith Joe?
in Figure 2.
At step (4), we face a shift-reduce conflict: either combine ?saw?
and ?Al?
ina rey action (5a), or shift ?with?
(5b).
To resolvethis conflict, there is a cost c associated with eachstate so that we can pick the best one (or few, witha beam) at each step.
Costs are accumulated ineach step: as shown in Figure 1, actions sh, rex,and rey have their respective costs ?, ?, and ?,which are dot-products of the weights w and fea-tures extracted from the state and the action.2.2 FeaturesWe view features as ?abstractions?
or (partial) ob-servations of the current state, which is an im-portant intuition for the development of dynamicprogramming in Section 3.
Feature templatesare functions that draw information from the fea-ture window (see Tab.
1(b)), consisting of thetop few trees on the stack and the first fewwords on the queue.
For example, one such fea-ture templatef100 = s0.w ?
q0.t is a conjunction1078of two atomic features s0.w and q0.t, capturingthe root word of the top tree s0 on the stack, andthe part-of-speech tag of the current head word q0on the queue.
See Tab.
1(a) for the list of featuretemplates used in the full model.
Feature templatesare instantiated for a specific state.
For example, atstep (4) in Fig.
2, the above template f100 will gen-erate a feature instance(s0.w = Al) ?
(q0.t = IN).More formally, we denote f to be the feature func-tion, such that f(j, S) returns a vector of featureinstances for state ?j, S?.
To decide which actionis the best for the current state, we perform a three-way classification based on f(j, S), and to do so,we further conjoin these feature instances with theaction, producing action-conjoined instances like(s0.w = Al) ?
(q0.t = IN) ?
(action = sh).We denote fsh(j, S), frex (j, S), and frey (j, S) tobe the conjoined feature instances, whose dot-products with the weight vector decide the best ac-tion (see Eqs.
(1-3) in Fig.
1).2.3 Beam Search and Early UpdateTo improve on strictly greedy search, shift-reduceparsing is often enhanced with beam search(Zhang and Clark, 2008), where b states developin parallel.
At each step we extend the states inthe current beam by applying one of the three ac-tions, and then choose the best b resulting statesfor the next step.
Our dynamic programming algo-rithm also runs on top of beam search in practice.To train the model, we use the averaged percep-tron algorithm (Collins, 2002).
Following Collinsand Roark (2004) we also use the ?early-update?strategy, where an update happens whenever thegold-standard action-sequence falls off the beam,with the rest of the sequence neglected.3 The intu-ition behind this strategy is that later mistakes areoften caused by previous ones, and are irrelevantwhen the parser is on the wrong track.
Dynamicprogramming turns out to be a great fit for earlyupdating (see Section 4.3 for details).3 Dynamic Programming (DP)3.1 Merging Equivalent StatesThe key observation for dynamic programmingis to merge ?equivalent states?
in the same beam3As a special case, for the deterministic mode (b=1), up-dates always co-occur with the first mistake made.
(a) Features Templates f(j, S) qi = wj+i(1) s0.w s0.t s0.w ?
s0.ts1.w s1.t s1.w ?
s1.tq0.w q0.t q0.w ?
q0.t(2) s0.w ?
s1.w s0.t ?
s1.ts0.t ?
q0.t s0.w ?
s0.t ?
s1.ts0.t ?
s1.w ?
s1.t s0.w ?
s1.w ?
s1.ts0.w ?
s0.t ?
s1.w s0.w ?
s0.t ?
s1 ?
s1.t(3) s0.t ?
q0.t ?
q1.t s1.t ?
s0.t ?
q0.ts0.w ?
q0.t ?
q1.t s1.t ?
s0.w ?
q0.t(4) s1.t ?
s1.lc.t ?
s0.t s1.t ?
s1.rc.t ?
s0.ts1.t ?
s0.t ?
s0.rc.t s1.t ?
s1.lc.t ?
s0s1.t ?
s1.rc.t ?
s0.w s1.t ?
s0.w ?
s0.lc.t(5) s2.t ?
s1.t ?
s0.t(b) ?
stack queue?...
s2...s1s1.lc...... s1.rc...s0s0.lc...... s0.rc...q0 q1 ...(c) Kernel features for DPef(j, S) = (j, f2(s2), f1(s1), f0(s0))f2(s2) s2.tf1(s1) s1.w s1.t s1.lc.t s1.rc.tf0(s0) s0.w s0.t s0.lc.t s0.rc.tj q0.w q0.t q1.tTable 1: (a) feature templates used in this work,adapted from Huang et al (2009).
x.w and x.t de-notes the root word and POS tag of tree (or word)x. and x.lc and x.rc denote x?s left- and rightmostchild.
(b) feature window.
(c) kernel features.
(i.e., same step) if they have the same featurevalues, because they will have the same costs asshown in the deductive system in Figure 1.
Thuswe can define two states ?j, S?
and ?j?, S??
to beequivalent, notated ?j, S?
?
?j?, S?
?, iff.j = j?
and f(j, S) = f(j?, S?).
(4)Note that j = j?
is also needed because thequeue head position j determines which word toshift next.
In practice, however, a small subset ofatomic features will be enough to determine thewhole feature vector, which we call kernel fea-tures f?
(j, S), defined as the smallest set of atomictemplates such thatf?
(j, S) = f?
(j?, S?)
?
?j, S?
?
?j?, S?
?.For example, the full list of 28 feature templatesin Table 1(a) can be determined by just 12 atomicfeatures in Table 1(c), which just look at the rootwords and tags of the top two trees on stack, aswell as the tags of their left- and rightmost chil-dren, plus the root tag of the third tree s2, and fi-nally the word and tag of the queue head q0 and the1079state form ?
: ?i, j, sd...s0?
: (c, v, ?)
?
: step; c, v: prefix and inside costs; ?
: predictor statesequivalence ?
: ?i, j, sd...s0?
?
?
: ?i, j, s?d...s?0?
iff.
f?
(j, sd...s0) = f?
(j, s?d...s?0)ordering ?
: : (c, v, ) ?
?
: : (c?, v?, ) iff.
c < c?
or (c = c?
and v < v?
).axiom (p0) 0 : ?0, 0, ??
: (0, 0, ?
)shstate p:?
: ?
, j, sd...s0?
: (c, , )?
+ 1 : ?j, j + 1, sd?1...s0, wj?
: (c + ?, 0, {p})j < nrexstate p:: ?k, i, s?d...s?0?
: (c?, v?, ??
)state q:?
: ?i, j, sd...s0?
: ( , v, ?)?
+ 1 : ?k, j, s?d...s?1, s?0xs0?
: (c?
+ v + ?, v?
+ v + ?, ??
)p ?
?goal 2n?
1 : ?0, n, sd...s0?
: (c, c, {p0})where ?
= w ?
fsh(j, sd...s0), and ?
= ??
+ ?, with ??
= w ?
fsh(i, s?d...s?0) and ?
= w ?
frex (j, sd...s0).Figure 3: Deductive system for shift-reduce parsing with dynamic programming.
The predictor state set ?is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995).The rey case is similar, replacing s?0xs0 with s?0ys0, and ?
with ?
= w ?
frey (j, sd...s0).
Irrelevantinformation in a deduction step is marked as an underscore ( ) which means ?can match anything?.tag of the next word q1.
Since the queue is staticinformation to the parser (unlike the stack, whichchanges dynamically), we can use j to replace fea-tures from the queue.
So in general we writef?
(j, S) = (j, fd(sd), .
.
.
, f0(s0))if the feature window looks at top d + 1 treeson stack, and where fi(si) extracts kernel featuresfrom tree si (0 ?
i ?
d).
For example, for the fullmodel in Table 1(a) we havef?
(j, S) = (j, f2(s2), f1(s1), f0(s0)), (5)where d = 2, f2(x) = x.t, and f1(x) = f0(x) =(x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).3.2 Graph-Structured Stack and DeductionNow that we have the kernel feature functions, itis intuitive that we might only need to rememberthe relevant bits of information from only the last(d + 1) trees on stack instead of the whole stack,because they provide all the relevant informationfor the features, and thus determine the costs.
Forshift, this suffices as the stack grows on the right;but for reduce actions the stack shrinks, and in or-der still to maintain d + 1 trees, we have to knowsomething about the history.
This is exactly whywe needed the full stack for vanilla shift-reduceparsing in the first place, and why dynamic pro-gramming seems hard here.To solve this problem we borrow the ideaof ?graph-structured stack?
(GSS) from Tomita(1991).
Basically, each state p carries with it a set?
(p) of predictor states, each of which can becombined with p in a reduction step.
In a shift step,if state p generates state q (we say ?p predicts q?in Earley (1970) terms), then p is added onto ?
(q).When two equivalent shifted states get merged,their predictor states get combined.
In a reductionstep, state q tries to combine with every predictorstate p ?
?
(q), and the resulting state r inheritsthe predictor states set from p, i.e., ?
(r) = ?
(p).Interestingly, when two equivalent reduced statesget merged, we can prove (by induction) that theirpredictor states are identical (proof omitted).Figure 3 shows the new deductive system withdynamic programming and GSS.
A new state hasthe form?
: ?i, j, sd...s0?where [i..j] is the span of the top tree s0, andsd..s1 are merely ?left-contexts?.
It can be com-bined with some predictor state p spanning [k..i]??
: ?k, i, s?d...s?0?to form a larger state spanning [k..j], with theresulting top tree being either s1xs0 or s1ys0.1080This style resembles CKY and Earley parsers.
Infact, the chart in Earley and other agenda-basedparsers is indeed a GSS when viewed left-to-right.In these parsers, when a state is popped up fromthe agenda, it looks for possible sibling statesthat can combine with it; GSS, however, explicitlymaintains these predictor states so that the newly-popped state does not need to look them up.43.3 Correctness and Polynomial ComplexityWe state the main theoretical result with the proofomitted due to space constraints:Theorem 1.
The deductive system is optimal andruns in worst-case polynomial time as long as thekernel feature function satisfies two properties:?
bounded: f?
(j, S) = (j, fd(sd), .
.
.
, f0(s0))for some constant d, and each |ft(x)| alsobounded by a constant for all possible tree x.?
monotonic: ft(x) = ft(y) ?
ft+1(x) =ft+1(y), for all t and all possible trees x, y.Intuitively, boundedness means features canonly look at a local window and can only extractbounded information on each tree, which is alwaysthe case in practice since we can not have infinitemodels.
Monotonicity, on the other hand, says thatfeatures drawn from trees farther away from thetop should not be more refined than from thosecloser to the top.
This is also natural, since the in-formation most relevant to the current decision isalways around the stack top.
For example, the ker-nel feature function in Eq.
5 is bounded and mono-tonic, since f2 is less refined than f1 and f0.These two requirements are related to grammarrefinement by annotation (Johnson, 1998), whereannotations must be bounded and monotonic: forexample, one cannot refine a grammar by onlyremembering the grandparent but not the parentsymbol.
The difference here is that the annotationsare not vertical ((grand-)parent), but rather hori-zontal (left context).
For instance, a context-freerule A ?
B C would become DA ?
DB BCfor some D if there exists a rule E ?
?DA?.This resembles the reduce step in Fig.
3.The very high-level idea of the proof is thatboundedness is crucial for polynomial-time, whilemonotonicity is used for the optimal substructureproperty required by the correctness of DP.4In this sense, GSS (Tomita, 1988) is really not a new in-vention: an efficient implementation of Earley (1970) shouldalready have it implicitly, similar to what we have in Fig.
3.3.4 Beam Search based on Prefix CostThough the DP algorithm runs in polynomial-time, in practice the complexity is still too high,esp.
with a rich feature set like the one in Ta-ble 1.
So we apply the same beam search ideafrom Sec.
2.3, where each step can accommodateonly the best b states.
To decide the ordering ofstates in each beam we borrow the concept of pre-fix cost from Stolcke (1995), originally developedfor weighted Earley parsing.
As shown in Fig.
3,the prefix cost c is the total cost of the best actionsequence from the initial state to the end of state p,i.e., it includes both the inside cost v (for Viterbiinside derivation), and the cost of the (best) pathleading towards the beginning of state p. We saythat a state p with prefix cost c is better than a statep?
with prefix cost c?, notated p ?
p?
in Fig.
3, ifc < c?.
We can also prove (by contradiction) thatoptimizing for prefix cost implies optimal insidecost (Nederhof, 2003, Sec.
4).
5As shown in Fig.
3, when a state q with costs(c, v) is combined with a predictor state p withcosts (c?, v?
), the resulting state r will have costs(c?
+ v + ?, v?
+ v + ?
),where the inside cost is intuitively the combinedinside costs plus an additional combo cost ?
fromthe combination, while the resulting prefix costc?
+ v + ?
is the sum of the prefix cost of the pre-dictor state q, the inside cost of the current state p,and the combo cost.
Note the prefix cost of q is ir-relevant.
The combo cost ?
= ??
+ ?
consists ofshift cost ??
of p and reduction cost ?
of q.The cost in the non-DP shift-reduce algorithm(Fig.
1) is indeed a prefix cost, and the DP algo-rithm subsumes the non-DP one as a special casewhere no two states are equivalent.3.5 Example: Edge-Factored ModelAs a concrete example, Figure 4 simulates anedge-factored model (Eisner, 1996; McDonald etal., 2005a) using shift-reduce with dynamic pro-gramming, which is similar to bilexical PCFGparsing using CKY (Eisner and Satta, 1999).
Herethe kernel feature function isf?
(j, S) = (j, h(s1), h(s0))5Note that using inside cost v for ordering would be abad idea, as it will always prefer shorter derivations like inbest-first parsing.
As in A* search, we need some estimateof ?outside cost?
to predict which states are more promising,and the prefix cost includes an exact cost for the left outsidecontext, but no right outside context.1081sh?
: ?
, h...j?
: (c, )?
+ 1 : ?h, j?
: (c, 0) j < nrex: ?h?
?, h?k...i?
: (c?, v?)
?
: ?h?, hi...j?
: ( , v)?
+ 1 : ?h?
?, hh?k...ii...j?
: (c?
+ v + ?, v?
+ v + ?
)where rex cost ?
= w ?
frex(h?, h)Figure 4: Example of shift-reduce with dynamicprogramming: simulating an edge-factored model.GSS is implicit here, and rey case omitted.where h(x) returns the head word index of tree x,because all features in this model are based on thehead and modifier indices in a dependency link.This function is obviously bounded and mono-tonic in our definitions.
The theoretical complexityof this algorithm is O(n7) because in a reductionstep we have three span indices and three head in-dices, plus a step index ?.
By contrast, the na?
?veCKY algorithm for this model is O(n5) which canbe improved to O(n3) (Eisner, 1996).6 The highercomplexity of our algorithm is due to two factors:first, we have to maintain both h and h?
in onestate, because the current shift-reduce model cannot draw features across different states (unlikeCKY); and more importantly, we group states bystep ?
in order to achieve incrementality and lin-ear runtime with beam search that is not (easily)possible with CKY or MST.4 ExperimentsWe first reimplemented the reference shift-reduceparser of Huang et al (2009) in Python (hence-forth ?non-DP?
), and then extended it to do dy-namic programing (henceforth ?DP?).
We evalu-ate their performances on the standard Penn Tree-bank (PTB) English dependency parsing task7 us-ing the standard split: secs 02-21 for training, 22for development, and 23 for testing.
Both DP andnon-DP parsers use the same feature templates inTable 1.
For Secs.
4.1-4.2, we use a baseline modeltrained with non-DP for both DP and non-DP, sothat we can do a side-by-side comparison of search6Or O(n2) with MST, but including non-projective trees.7Using the head rules of Yamada and Matsumoto (2003).quality; in Sec.
4.3 we will retrain the model withDP and compare it against training with non-DP.4.1 Speed ComparisonsTo compare parsing speed between DP and non-DP, we run each parser on the development set,varying the beam width b from 2 to 16 (DP) or 64(non-DP).
Fig.
5a shows the relationship betweensearch quality (as measured by the average modelscore per sentence, higher the better) and speed(average parsing time per sentence), where DPwith a beam width of b=16 achieves the samesearch quality with non-DP at b=64, while being 5times faster.
Fig.
5b shows a similar comparisonfor dependency accuracy.
We also test with anedge-factored model (Sec.
3.5) using feature tem-plates (1)-(3) in Tab.
1, which is a subset of thosein McDonald et al (2005b).
As expected, this dif-ference becomes more pronounced (8 times fasterin Fig.
5c), since the less expressive feature setmakes more states ?equivalent?
and mergeable inDP.
Fig.
5d shows the (almost linear) correlationbetween dependency accuracy and search quality,confirming that better search yields better parsing.4.2 Search Space, Forest, and OraclesDP achieves better search quality because it ex-pores an exponentially large search space ratherthan only b trees allowed by the beam (see Fig.
6a).As a by-product, DP can output a forest encodingthese exponentially many trees, out of which wecan draw longer and better (in terms of oracle) k-best lists than those in the beam (see Fig.
6b).
Theforest itself has an oracle of 98.15 (as if k ?
?
),computed a` la Huang (2008, Sec.
4.1).
These can-didate sets may be used for reranking (Charniakand Johnson, 2005; Huang, 2008).84.3 Perceptron Training and Early UpdatesAnother interesting advantage of DP over non-DPis the faster training with perceptron, even whenboth parsers use the same beam width.
This is dueto the use of early updates (see Sec.
2.3), whichhappen much more often with DP, because a gold-standard state p is often merged with an equivalent(but incorrect) state that has a higher model score,which triggers update immediately.
By contrast, innon-DP beam search, states such as p might still8DP?s k-best lists are extracted from the forest using thealgorithm of Huang and Chiang (2005), rather than those inthe final beam as in the non-DP case, because many deriva-tions have been merged during dynamic programming.10822370237323762379238223852388239123940  0.05  0.1  0.15  0.2  0.25  0.3  0.35avg.modelscoreb=16 b=64DPnon-DP92.292.392.492.592.692.792.892.99393.10  0.05  0.1  0.15  0.2  0.25  0.3  0.35dependencyaccuracyb=16 b=64DPnon-DP(a) search quality vs. time (full model) (b) parsing accuracy vs. time (full model)22902295230023052310231523202325233023350  0.05  0.1  0.15  0.2  0.25  0.3  0.35avg.modelscoreb=16b=64DPnon-DP88.58989.59090.59191.59292.59393.52280  2300  2320  2340  2360  2380  2400dependencyaccuracyfull, DPfull, non-DPedge-factor, DPedge-factor, non-DP(c) search quality vs. time (edge-factored model) (d) correlation b/w parsing (y) and search (x)Figure 5: Speed comparisons between DP and non-DP, with beam size b ranging 2?16 for DP and 2?64for non-DP.
Speed is measured by avg.
parsing time (secs) per sentence on x axis.
With the same levelof search quality or parsing accuracy, DP (at b=16) is ?4.8 times faster than non-DP (at b=64) with thefull model in plots (a)-(b), or ?8 times faster with the simplified edge-factored model in plot (c).
Plot (d)shows the (roughly linear) correlation between parsing accuracy and search quality (avg.
model score).100102104106108101010120  10  20  30  40  50  60  70number of treesexploredsentence lengthDP forestnon-DP (16)9394959697989964 32 16 8 4 1oracleprecisionkDP forest (98.15)DP k-best in forestnon-DP k-best in beam(a) sizes of search spaces (b) oracle precision on devFigure 6: DP searches over a forest of exponentially many trees, which also produces better and longerk-best lists with higher oracles, while non-DP only explores b trees allowed in the beam (b = 16 here).108390.59191.59292.59393.50  4  8  12  16  20  24accuracyondev(eachround)hours17th18thDPnon-DPFigure 7: Learning curves (showing precision ondev) of perceptron training for 25 iterations (b=8).DP takes 18 hours, peaking at the 17th iteration(93.27%) with 12 hours, while non-DP takes 23hours, peaking at the 18th (93.04%) with 16 hours.survive in the beam throughout, even though it isno longer possible to rank the best in the beam.The higher frequency of early updates resultsin faster iterations of perceptron training.
Table 2shows the percentage of early updates and the timeper iteration during training.
While the number ofupdates is roughly comparable between DP andnon-DP, the rate of early updates is much higherwith DP, and the time per iteration is consequentlyshorter.
Figure 7 shows that training with DP isabout 1.2 times faster than non-DP, and achieves+0.2% higher accuracy on the dev set (93.27%).Besides training with gold POS tags, we alsotrained on noisy tags, since they are closer to thetest setting (automatic tags on sec 23).
In thatcase, we tag the dev and test sets using an auto-matic POS tagger (at 97.2% accuracy), and tagthe training set using four-way jackknifing sim-ilar to Collins (2000), which contributes another+0.1% improvement in accuracy on the test set.Faster training also enables us to incorporate morefeatures, where we found more lookahead features(q2) results in another +0.3% improvement.4.4 Final Results on English and ChineseTable 3 presents the final test results of our DPparser on the Penn English Treebank, comparedwith other state-of-the-art parsers.
Our parserachieves the highest (unlabeled) dependency ac-curacy among dependency parsers trained on theTreebank, and is also much faster than most otherparsers even with a pure Python implementationit update early% time update early% time1 31943 98.9 22 31189 87.7 295 20236 98.3 38 19027 70.3 4717 8683 97.1 48 7434 49.5 6025 5715 97.2 51 4676 41.2 65Table 2: Perceptron iterations with DP (left) andnon-DP (right).
Early updates happen much moreoften with DP due to equivalent state merging,which leads to faster training (time in minutes).word L time comp.McDonald 05b 90.2 Ja 0.12 O(n2)McDonald 05a 90.9 Ja 0.15 O(n3)Koo 08 base 92.0 ?
?
O(n4)Zhang 08 single 91.4 C 0.11 O(n)?this work 92.1 Py 0.04 O(n)?Charniak 00 92.5 C 0.49 O(n5)?Petrov 07 92.4 Ja 0.21 O(n3)Zhang 08 combo 92.1 C ?
O(n2)?Koo 08 semisup 93.2 ?
?
O(n4)Table 3: Final test results on English (PTB).
Ourparser (in pure Python) has the highest accuracyamong dependency parsers trained on the Tree-bank, and is also much faster than major parsers.
?converted from constituency trees.
C=C/C++,Py=Python, Ja=Java.
Time is in seconds per sen-tence.
Search spaces: ?linear; others exponential.
(on a 3.2GHz Xeon CPU).
Best-performing con-stituency parsers like Charniak (2000) and Berke-ley (Petrov and Klein, 2007) do outperform ourparser, since they consider more information dur-ing parsing, but they are at least 5 times slower.Figure 8 shows the parse time in seconds for eachtest sentence.
The observed time complexity of ourDP parser is in fact linear compared to the super-linear complexity of Charniak, MST (McDonaldet al, 2005b), and Berkeley parsers.
Additionaltechniques such as semi-supervised learning (Kooet al, 2008) and parser combination (Zhang andClark, 2008) do achieve accuracies equal to orhigher than ours, but their results are not directlycomparable to ours since they have access to ex-tra information like unlabeled data.
Our techniqueis orthogonal to theirs, and combining these tech-niques could potentially lead to even better results.We also test our final parser on the Penn Chi-nese Treebank (CTB5).
Following the set-up ofDuan et al (2007) and Zhang and Clark (2008), wesplit CTB5 into training (secs 001-815 and 1001-108400.20.40.60.811.21.40  10  20  30  40  50  60  70parsingtime(secs)sentence lengthChaBerkMSTDPFigure 8: Scatter plot of parsing time against sen-tence length, comparing with Charniak, Berkeley,and the O(n2) MST parsers.word non-root root compl.Duan 07 83.88 84.36 73.70 32.70Zhang 08?
84.33 84.69 76.73 32.79this work 85.20 85.52 78.32 33.72Table 4: Final test results on Chinese (CTB5).
?The transition parser in Zhang and Clark (2008).1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137-1147) sets,assume gold-standard POS-tags for the input, anduse the head rules of Zhang and Clark (2008).
Ta-ble 4 summarizes the final test results, where ourwork performs the best in all four types of (unla-beled) accuracies: word, non-root, root, and com-plete match (all excluding punctuations).
9,105 Related WorkThis work was inspired in part by Generalized LRparsing (Tomita, 1991) and the graph-structuredstack (GSS).
Tomita uses GSS for exhaustive LRparsing, where the GSS is equivalent to a dy-namic programming chart in chart parsing (seeFootnote 4).
In fact, Tomita?s GLR is an in-stance of techniques for tabular simulation of non-deterministic pushdown automata based on deduc-tive systems (Lang, 1974), which allow for cubic-time exhaustive shift-reduce parsing with context-free grammars (Billot and Lang, 1989).Our work advances this line of research in twoaspects.
First, ours is more general than GLR in9Duan et al (2007) and Zhang and Clark (2008) did notreport word accuracies, but those can be recovered given non-root and root ones, and the number of non-punctuation words.10Parser combination in Zhang and Clark (2008) achievesa higher word accuracy of 85.77%, but again, it is not directlycomparable to our work.that it is not restricted to LR (a special case ofshift-reduce), and thus does not require building anLR table, which is impractical for modern gram-mars with a large number of rules or features.
Incontrast, we employ the ideas behind GSS moreflexibly to merge states based on features values,which can be viewed as constructing an implicitLR table on-the-fly.
Second, unlike previous the-oretical results about cubic-time complexity, weachieved linear-time performance by smart beamsearch with prefix cost inspired by Stolcke (1995),allowing for state-of-the-art data-driven parsing.To the best of our knowledge, our work is thefirst linear-time incremental parser that performsdynamic programming.
The parser of Roark andHollingshead (2009) is also almost linear time, butthey achieved this by discarding parts of the CKYchart, and thus do achieve incrementality.6 ConclusionWe have presented a dynamic programming al-gorithm for shift-reduce parsing, which runs inlinear-time in practice with beam search.
Thisframework is general and applicable to a large-class of shift-reduce parsers, as long as the featurefunctions satisfy boundedness and monotonicity.Empirical results on a state-the-art dependencyparser confirm the advantage of DP in many as-pects: faster speed, larger search space, higher ora-cles, and better and faster learning.
Our final parseroutperforms all previously reported dependencyparsers trained on the Penn Treebanks for bothEnglish and Chinese, and is much faster in speed(even with a Python implementation).
For futurework we plan to extend it to constituency parsing.AcknowledgmentsWe thank David Chiang, Yoav Goldberg, JonathanGraehl, Kevin Knight, and Roger Levy for help-ful discussions and the three anonymous review-ers for comments.
Mark-Jan Nederhof inspired theuse of prefix cost.
Yue Zhang helped with Chinesedatasets, and Wenbin Jiang with feature sets.
Thiswork is supported in part by DARPA GALE Con-tract No.
HR0011-06-C-0022 under subcontract toBBN Technologies, and by the U.S. Army Re-search, Development, and Engineering Command(RDECOM).
Statements and opinions expresseddo not necessarily reflect the position or the policyof the United States Government, and no officialendorsement should be inferred.1085ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
TheTheory of Parsing, Translation, and Compiling, vol-ume I: Parsing of Series in Automatic Computation.Prentice Hall, Englewood Cliffs, New Jersey.S.
Billot and B. Lang.
1989.
The structure of sharedforests in ambiguous parsing.
In Proceedings of the27th ACL, pages 143?151.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminativereranking.
In Proceedings of the 43rd ACL, Ann Ar-bor, MI.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of ACL.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of ICML,pages 175?182.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof EMNLP.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic models for action-based chinese dependencyparsing.
In Proceedings of ECML/PKDD.Jay Earley.
1970.
An efficient context-free parsing al-gorithm.
Communications of the ACM, 13(2):94?102.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head-automaton grammars.
In Proceedings of ACL.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Pro-ceedings of COLING.Lyn Frazier and Keith Rayner.
1982.
Making and cor-recting errors during sentence comprehension: Eyemovements in the analysis of structurally ambigu-ous sentences.
Cognitive Psychology, 14(2):178 ?210.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies (IWPT-2005).Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of EMNLP.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofthe ACL: HLT, Columbus, OH, June.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24:613?632.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL.B.
Lang.
1974.
Deterministic techniques for efficientnon-deterministic parsers.
In Automata, Languagesand Programming, 2nd Colloquium, volume 14 ofLecture Notes in Computer Science, pages 255?269,Saarbru?cken.
Springer-Verlag.Lillian Lee.
2002.
Fast context-free grammar parsingrequires fast Boolean matrix multiplication.
Journalof the ACM, 49(1):1?15.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proc.
of HLT-EMNLP.Mark-Jan Nederhof.
2003.
Weighted deductive pars-ing and Knuth?s algorithm.
Computational Linguis-tics, pages 135?143.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Incremental Parsing: Bring-ing Engineering and Cognition Together.
Workshopat ACL-2004, Barcelona.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL.Brian Roark and Kristy Hollingshead.
2009.
Linearcomplexity context-free parsing pipelines via chartconstraints.
In Proceedings of HLT-NAACL.Andreas Stolcke.
1995.
An efficient probabilis-tic context-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165?201.Masaru Tomita.
1988.
Graph-structured stack and nat-ural language parsing.
In Proceedings of the 26thannual meeting on Association for ComputationalLinguistics, pages 249?257, Morristown, NJ, USA.Association for Computational Linguistics.Masaru Tomita, editor.
1991.
Generalized LR Parsing.Kluwer Academic Publishers.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of IWPT.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of EMNLP.1086
