Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99?103,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsEvaluation without references:IBM1 scores as evaluation metricsMaja Popovic?, David Vilar, Eleftherios Avramidis, Aljoscha BurchardtGerman Research Center for Artificial Intelligence (DFKI)Language Technology (LT), Berlin, Germanyname.surname@dfki.deAbstractCurrent metrics for evaluating machine trans-lation quality have the huge drawback thatthey require human-quality reference transla-tions.
We propose a truly automatic evalua-tion metric based on IBM1 lexicon probabili-ties which does not need any reference transla-tions.
Several variants of IBM1 scores are sys-tematically explored in order to find the mostpromising directions.
Correlations betweenthe new metrics and human judgments are cal-culated on the data of the third, fourth and fifthshared tasks of the Statistical Machine Trans-lation Workshop.
Five different European lan-guages are taken into account: English, Span-ish, French, German and Czech.
The resultsshow that the IBM1 scores are competitivewith the classic evaluation metrics, the mostpromising being IBM1 scores calculated onmorphemes and POS-4grams.1 IntroductionCurrently used evaluation metrics such as BLEU (Pa-pineni et al, 2002), METEOR (Banerjee and Lavie,2005), etc.
are based on the comparison betweenhuman reference translations and the automaticallygenerated hypotheses in the target language to beevaluated.
While this scenario helps in the designof machine translation systems, it has two majordrawbacks.
The first one is the practical criticismthat using reference translations is inefficient and ex-pensive: in real-life situations, the quality of ma-chine translation must be evaluated without havingto pay humans for producing reference translationsfirst.
The second criticism is methodological: inusing reference translation, the problem of evalu-ating translation quality (e.g., completeness, order-ing, domain fit, etc.)
is transformed into a kind ofparaphrase evaluation in the target language, whichis a very difficult problem itself.
In addition, theset of selected references always represents only asmall subset of all good translations.
To remedythese drawbacks, we propose a truly automatic eval-uation metric which is based on the IBM1 lexiconscores (Brown et al, 1993).The inclusion of IBM1 scores in translation sys-tems has shown experimentally to improve transla-tion quality (Och et al, 2003).
They also have beenused for confidence estimation for machine transla-tion (Blatz et al, 2003).
To the best of our knowl-edge, these scores have not yet been used as an eval-uation metric.We carry out a systematic comparison betweenseveral variants of IBM1 scores.
The Spearman?srank correlation coefficients on the document (sys-tem) level between the IBM1 metrics and the hu-man ranking are computed on the English, French,Spanish, German and Czech texts generated by var-ious translation systems in the framework of thethird (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al,2010) shared translation tasks.2 IBM1 scoresThe IBM1 model is a bag-of-word translation modelwhich gives the sum of all possible alignment proba-bilities between the words in the source sentence andthe words in the target sentence.
Brown et al (1993)defined the IBM1 probability score for a translation99pair fJ1 and eI1 in the following way:P (fJ1 |eI1) =1(I + 1)JJ?j=1I?i=0p(fj |ei) (1)where fJ1 is the source language sentence of lengthJ and eI1 is the target language sentence of length I .As it is a conditional probability distribution, weinvestigated both directions as evaluation metrics.
Inorder to avoid frequent confusions about what is thesource and what the target language, we defined ourscores in the following way:?
source-to-hypothesis (sh) IBM1 score:IBM1sh =1(H + 1)SS?j=1H?i=0p(sj|hi) (2)?
hypothesis-to-source (hs) IBM1 score:IBM1hs =1(S + 1)HH?i=1S?j=0p(hi|sj) (3)where sj are the words of the original source lan-guage sentence, S is the length of this sentence, hiare the words of the target language hypothesis, andH is the length of this hypothesis.In addition to the standard IBM1 scores calculatedon words, we also investigated:?
MIBM1 scores ?
IBM1 scores of word mor-phemes in each direction;?
PnIBM1 scores ?
IBM1 scores of POS n-gramsin each direction.A parallel bilingual corpus for the desired lan-guage pair and a tool for training the IBM1 modelare required in order to obtain IBM1 probabilitiesp(fj|ei).
For the POS n-gram scores, appropriatePOS taggers for each of the languages are necessary.The POS tags cannot be only basic but must haveall details (e.g.
verb tenses, cases, number, gender,etc.).
For the morpheme scores, a tool for splittingwords into morphemes is necessary.3 Experiments on WMT 2008, WMT 2009and WMT 2010 test data3.1 Experimental set-upThe IBM1 probabilities necessary for the IBM1scores are learnt using the WMT 2010 NewsCommentary bilingual corpora consisting of theSpanish-English, French-English, German-Englishand Czech-English parallel texts.
Spanish, French,German and English POS tags were produced usingthe TreeTagger1, and the Czech texts are tagged us-ing the COMPOST tagger (Spoustova?
et al, 2009).The morphemes for all languages are obtained us-ing the Morfessor tool (Creutz and Lagus, 2005).The tool is corpus-based and language-independent:it takes a text as input and produces a segmenta-tion of the word forms observed in the text.
Theobtained results are not strictly linguistic, howeverthey often resemble a linguistic morpheme segmen-tation.
Once a morpheme segmentation has beenlearnt from some text, it can be used for segment-ing new texts.
In our experiments, the splitting arelearnt from the training corpus used for the IBM1lexicon probabilities.
The obtained segmentation isthen used for splitting the corresponding source textsand hypotheses.
Detailed corpus statistics are shownin Table 1.Using the obtained IBM1 probabilities of words,morphemes and POS n-grams, the scores de-scribed in Section 2 are calculated for theSpanish-English, French-English, German-Englishand Czech-English translation outputs from eachtranslation direction.
For each of the IBM1 scores,the system level Spearman correlation coefficients ?with the human ranking are calculated for each doc-ument.
In total, 32 correlation coefficients are ob-tained for each score ?
four English outputs fromthe WMT 2010 task, four from the WMT 2009 andeight from the WMT 2008 task, together with six-teen outputs in other four target languages.
The ob-tained correlation results were then summarised intothe following three values:?
meana correlation coefficient averaged over all trans-lation outputs;1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/100Spanish English French English German English Czech Englishsentences 97122 83967 100222 94693running words 2661344 2338495 2395141 2042085 2475359 2398780 2061422 2249365vocabulary:words 69620 53527 56295 50082 107278 54270 125614 52081morphemes 14178 13449 12004 12485 22211 13499 18789 12961POS tags 69 44 33 44 54 44 611 44POS-2grams 2459 1443 826 1443 1611 1454 27835 1457POS-3grams 27350 20474 10409 19838 19928 20769 209481 20522POS-4grams 135166 121182 62177 114555 114314 123550 637337 120646Table 1: Statistics of the corpora for training IBM1 lexicon models.?
rank>percentage of documents where the particularscore has better correlation than the other IBM1scores;?
rank?percentage of documents where the particularscore has better or equal correlation than theother IBM1 scores.3.2 Comparison of IBM1 scoresThe first step towards deciding which IBM1 scoreto submit to the WMT 2011 evaluation task was acomparison of the average correlations i.e.
meanvalues.
These values for each of the IBM1 scoresare presented in Table 2.
The left column showsaverage correlations of the source-hypothesis (sh)scores, and the right one of the hypothesis-source(hs) scores.mean IBM1sh IBM1hswords 0.066 0.308morphemes 0.227 0.445POS tags 0.006 0.337POS-2grams 0.058 0.337POS-3grams 0.172 0.376POS-4grams 0.196 0.442Table 2: Average correlations of source-hypothesis (leftcolumn) and hypothesis-source (right column) IBM1scores.It can be seen that the morpheme, POS-3gram andPOS-4gram scores have the best correlations in bothdirections.
Apart from that, it can be observed thatall the hs scores have better correlations than shscores.
Therefore, all the further experiments willdeal only with the hs scores, and the subscript hs isomitted.In the next step, all the hs scores are sorted ac-cording to each of the three values described inSection 3.1, i.e.
average correlation mean, rank>and rank?, and the results are shown in Table 3.The most promising scores according to each ofthe three values are morpheme score MIBM1, POS-3gram score P3IBM1 and POS-4gram score P4IBM1.3.2.1 Combined IBM1 scoresThe last experiment was to combine the mostpromising IBM1 scores in order to see if the correla-tion with human rankings can be further improved.In general, a combined IBM1 score is defined asarithmetic mean of various individual IBM1hs scoresdescribed in Section 2:COMBIBM1 =K?k=1wk ?
IBM1k (4)The following combinations were investigated:?
P1234IBM1combination of all POS n-gram scores;?
MP1234IBM1combination of all POS n-gram scores and themorpheme score;?
MP34IBM1combination of the most promising individualscores, i.e.
POS-3gram, POS-4gram and mor-pheme scores;101mean rank> rank?0.445 morphemes 60.6 POS-4grams 71.3 POS-4grams0.442 POS-4grams 54.4 morphemes 61.3 POS-3grams0.376 POS-3grams 50.6 POS-3grams 56.3 morphemes0.337 POS-2grams 39.4 POS tags 48.1 POS tags0.337 POS tags 36.3 words 43.7 POS-2grams0.308 words 35.6 POS-2grams 42.5 wordsTable 3: IBM1hs scores sorted by average correlation (column 1), rank> value (column 2) and rank?
value (column3).
The most promising scores are those calculated on morphemes (MIBM1), POS-3grams (P3IBM1) and POS-4grams(P4IBM1).?
MP4IBM1combination of the two most promising indi-vidual scores, i.e.
POS-4gram score and mor-pheme score.For each of the scores, two variants were investi-gated, with and without (i.e.
with uniform) weightswk.
The weigths were choosen proportionally tothe average correlation of each individual score.
Ta-ble 4 contains average correlations for all combinedscores, together with the weight values.combined score meanP1234IBM1 0.403+weights (0.15, 0.15, 0.3, 0.4) 0.414MP1234IBM1 0.466+weights (0.2, 0.05, 0.05, 0.2, 0.5) 0.486MP34IBM1 0.480+weights (0.25, 0.25, 0.5) 0.498MP4IBM1 0.494+weights (0.4, 0.6) 0.496Table 4: Average correlations of the investigated IBM1hscombinations.
The weight values are choosen accord-ing to the average correlation of the particular individualIBM1 score.The POS n-gram combination alone does not yieldany improvement over the best individual scores.Introduction of the morpheme score increases theaverage correlation, especially when only the bestn-gram scores are chosen.
Apart from that, intro-ducing weights improves the average correlation foreach of the combined scores.The final step in our experiments consists of rank-ing the weighted combined scores.
The rank> andrank?
values for these scores are presented in Ta-ble 5.
According to the rank> values, the MP4IBM1score clearly outperforms all other scores.
Thisscore also has the highest mean value together withthe MP34IBM1 score.
As for rank?
values, allmorpheme-POS scores have similar values signifi-cantly outperforming the P1234IBM1 score.combined score rank> rank?P1234IBM1 25.0 36.4MP1234IBM1 44.8 68.7MP34IBM1 39.6 64.6MP4IBM1 55.2 65.7Table 5: rank> (column 1) and rank?
(column 2) valuesof the weighted IBM1hs combinations.Following all these observations, we decided tosubmit the MP4IBM1 score to the WMT 2011 evalu-ation task.4 Conclusions and outlookThe results presented in this article show that theIBM1 scores have the potential to be used as replace-ment of current evaluation metrics based on refer-ence translations.
Especially the scores abstractingaway from word surface particularities (i.e.
vocabu-lary, domain) based on morphemes, POS-3grams and4grams show a high average correlation of about 0.5(the average correlation of the BLEU score on thesame data is 0.566).An important point for future optimisation is toinvestigate effects of the selection of training datafor the IBM1 models (and its similarity to the train-ing data of the involved statistical translation sys-tems).
Furthermore, investigation of how to assignthe weights for combining the corresponding indi-102vidual scores, as well as of the possible impact ofdifferent morpheme splittings should be carried out.Other direction for future work is combination withother features (i.e.
POS language models).This method is currently being tested and fur-ther developed in the framework of the TARAX ?Uproject2.
In this project, three industry and one re-search partners develop a hybrid machine transla-tion architecture that satisfies current industry needs,which includes a number of large-scale evalua-tion rounds involving various languages: English,French, German, Czech, Spanish, Russian, Chineseand Japanese.
By the time of writing this article, thefirst human evaluation round in TARAX ?U on a pilotset of about 7000 sentences is running.
The metricsproposed in this paper will be tested on the TARAX ?Udata as soon as they are available.
First results willbe reported in the presentation of this paper.AcknowledgmentsThis work has been partly developed within theTARAX ?U project financed by TSB Technologies-tiftung Berlin ?
Zukunftsfonds Berlin, co-financedby the European Union ?
European fund for regionaldevelopment.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements.
In Pro-ceedings of the ACL 05 Workshop on Intrinsic and Ex-trinsic Evaluation Measures for MT and/or Summa-rization, pages 65?72, Ann Arbor, MI, June.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence estimation formachine translation.
Final report, JHU/CLSP SummerWorkshop.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
FurtherMeta-Evaluation of Machine Translation.
In Proceed-ings of the 3rd ACL 08 Workshop on Statistical Ma-2http://taraxu.dfki.de/chine Translation (WMT 08), pages 70?106, Colum-bus, Ohio, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR(WMT 10), pages 17?53, Uppsala, Sweden, July.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using morfessor 1.0.
Technical Re-port Report A81, Computer and Information Science,Helsinki University of Technology, Helsinki, Finland,March.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.
Syn-tax for statistical machine translation.
Technical re-port, Johns Hopkins University 2003 Summer Work-shop on Language Engineering, Center for Languageand Speech Processing, Baltimore, MD, USA, August.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL 02), pages 311?318, Philadel-phia, PA, July.Drahom?
?ra ?Johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervised train-ing for the averaged perceptron POS tagger.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 763?771,Athens, Greece, March.103
