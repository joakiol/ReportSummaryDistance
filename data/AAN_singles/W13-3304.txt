Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27?32,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsAssociative Texture is Lost in TranslationBeata Beigman Klebanov and Michael FlorEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541{bbeigmanklebanov,mflor}@ets.orgAbstractWe present a suggestive finding regardingthe loss of associative texture in the pro-cess of machine translation, using com-parisons between (a) original and back-translated texts, (b) reference and systemtranslations, and (c) better and worse MTsystems.
We represent the amount of as-sociation in a text using word associationprofile ?
a distribution of pointwise mu-tual information between all pairs of con-tent word types in a text.
We use the av-erage of the distribution, which we termlexical tightness, as a single measure ofthe amount of association in a text.
Weshow that the lexical tightness of human-composed texts is higher than that of themachine translated materials; human ref-erences are tighter than machine trans-lations, and better MT systems producelexically tighter translations.
While thephenomenon of the loss of associative tex-ture has been theoretically predicted bytranslation scholars, we present a measurecapable of quantifying the extent of thisphenomenon.1 IntroductionWhile most current approaches to machine trans-lation concentrate on single sentences, there isemerging interest in phenomena that go beyond asingle sentence and pertain to the whole text beingtranslated.
For example, Wong and Kit (2012)demonstrated that repetition of content words isa predictor of translation quality, with poorertranslations failing to repeat words appropriately.Gong et al(2011) and Tiedemann (2010) presentcaching of translations from earlier sections of adocument to facilitate the translation of its latersections.In scholarship that deals with properties of hu-man translation of literary texts, translation is of-ten rendered as a process that tends to deformthe original, and a number of particular aspectsof deformation have been identified.
Specifically,Berman (2000) discusses the problem of quantita-tive impoverishment thus:This refers to a lexical loss.
Everywork in prose presents a certain pro-liferation of signifiers and signifyingchains.
Great novelist prose is ?abun-dant.?
These signifiers can be describedas unfixed, especially as a signified mayhave a multiplicity of signifiers.
Forthe signified visage (face) Arlt employssemblante, rosto and cara without jus-tifying a particular choice in a particu-lar sentence.
The essential thing is thatvisage is marked as an important real-ity in his work by the use of three sig-nifiers.
The translation that does not re-spect this multiplicity renders the ?vis-age?
of an unrecognizable work.
Thereis a loss, then, since the translation con-tains fewer signifiers than the original.
?1While Berman?s remarks refer to literary trans-lation, recent work demonstrates its relevance formachine translation, showing that MT systemstend to under-use linguistic devices that are com-monly used for repeated reference, such as super-ordinates or meronyms, although the pattern withsynonyms and near-synonyms was not clear cut(Wong and Kit, 2012).
Studying a complemen-tary phenomenon of translation of same-lemmalexical items in the source document into a targetlanguage, Carpuat and Simard (2012) found thatwhen MT systems produce different target lan-guage translations, they are stylistically, syntac-tically, or semantically inadequate in most cases1italics in the original27(see upper panel of Table 5 therein), that is, diver-sifying the signifiers appropriately is a challeng-ing task.
For recent work on biasing SMT systemstowards consistent translations of repeated words,see Ture et al(2012) and Xiao et al(2011).Moving beyond single signifieds, or concepts,Berman faults translations for ?the destruction ofunderlying networks of signification?, wherebygroups of related words are translated withoutpreserving the relatedness in the target language.While these might be unavoidable in any trans-lation, we show below that machine translationspecifically indeed suffers from such a loss (sec-tion 3) and that machine translation suffers from itmore than the human translations (section 4).2 MethodologyWe define WAPT ?
a word association profileof a text T ?
as the distribution of PMI(x, y) forall pairs of content2 word types (x, y) ?T.3 We es-timate PMIs using same-paragraph co-occurrencecounts from a large and diverse corpus of about 2.5billion words: 2 billion words come from the Gi-gaword 2003 corpus (Graff and Cieri, 2003); anadditional 500 million words come from an in-house corpus containing popular science and fic-tion texts.
We further define LTT ?
the lexicaltightness of a text T ?
as the average value of theword association profile.
All pairs of words in Tfor which the corpus had no co-occurrence dataare excluded from the calculations.
We note thatthe database has very good coverage with respectto the datasets in sections 3-5, with 94%-96%of pairs on average having co-occurrence countsin the database.
A more detailed exposition ofthe notion of a word association profile, includ-ing measurements on a number of corpora, can befound in Beigman Klebanov and Flor (2013).Our prediction is that translated texts would beless lexically tight than originals, and that bettertranslations ?
either human or machine ?
would betighter than worse translations, incurring a smalleramount of association loss.3 Experiment 1: Back-translationFor the experiment, we selected 20 editorials onthe topic of baseball from the New York Times2We part-of-speech tag a text using OpenNLP tagger(http://opennlp.apache.org) and only take into account com-mon and proper nouns, verbs, adjectives, and adverbs.3PMI = Pointwise Mutual InformationAnnotated Corpus.4 The selected articles hadbaseball annotated as their sole topic, and rangedfrom 250 to 750 words in length.
We expectthese articles to contain a large group of wordsthat reflects vocabulary that is commonly used indiscussing baseball and no other systematic sub-topics.
All articles were translated into French,Spanish, Arabic, and Swedish, and then translatedback to English, using the Google automatic trans-lation service.
Our goal is to observe the effect ofthe two layers of translation (out of English andback) on the lexical tightness of the resulting texts.Since baseball is not a topic that is commonlydiscussed in the European languages or in Ara-bic, this is a case where culturally foreign materialneeds to be rendered in a host (or target) language.This is exactly the kind of situation where we ex-pect deformation to occur ?
the material is eitheraltered so that is feels more ?native?
in the hostlanguage (domestication) or its foreigness is pre-served (foreignization) in that the material lacksassociative support in the host language (Venuti,1995).
In the first case, the translation might beassociatively adequate in the host language, but,being altered, it would produce less culturally pre-cise result when translated back into English.
Inthe second case, the result of translating out of En-glish might already be associatively impoverishedby the standards of the host language.The italicized phrases in the previous paragraphunderscore the theoretical and practical difficultyin diagnozing domestication or foreignization intranslating out of English ?
an associative modelfor each of the host languages will be needed,as well as some benchmark of the lexical tight-ness of native texts written on the given topicagainst which translations from English could bejudged.
While the technique of back-translationcannot identify the exact path of association loss?
through domestication or foreignization ?
it canhelp establish that association loss has occurredin at least one or both of the translation processesinvolved, since the original native English versionprovides a natural benchmark against which theresulting back-translations can be measured.To make the phenomenon of association lossmore concrete, consider the following sentence:Original Dave Magadan, the hard-hitting rookiethird baseman groomed to replace Knight,has been hospitalized.4LDC2008T19 in LDC catalogue28Arabic Dave Magadan, the stern rookie 3 base-man groomed to replace Knight, is in the hos-pital.5Spanish Dave Magadan, the strong rookie thirdbaseman who managed to replace Knight,has been hospitalized.French Dave Magadan, the hitting third rookieplayer prepared to replace Knight, was hos-pitalized.Swedish Dave Magadan, powerful rookie thirdbaseman groomed to replace Knight, hasbeen hospitalized.Observe the translations of the phrase ?hard-hitting rookie third baseman.?
While substitutingstrong and powerful for hard-hitting might seemacceptable semantically, these terms are not asso-ciated with the other baseball terms in the text,whereas hitting is highly associated with them:6Table 1 shows PMI scores for each of hitting,stern, strong, powerful with the baseball termsrookie and baseman.
The French translation gotthe hitting, but substituted the more generic termplayer instead of the baseball-specific baseman.As the bottom panel of Table 1 makes clear, whileplayer is associated with other baseball terms, theassociations are lower than those of baseman.rookie baseman hittinghitting 3.54 5.29stern 0.35 -1.60strong 0.54 -0.08powerful -0.62 -0.63player 3.95 2.73baseman 5.11 5.29Table 1: PMI associations of words introduced inback-translations with baseball terms rookie, base-man, and hitting.Table 2 shows the average lexical tightnessvalues across 20 texts for the original version aswell as for the back translated versions.
The origi-nal version is statistically significantly tighter thaneach of the back translated versions, using 4 ap-plications of t-test for correlated samples, n=20,p<0.05 in each case.5We corrected the syntax of all back-translations whilepreserving the content-word vocabulary choices.6Our tokenizer splits words on hyphens, therefore exam-ples are shown for hitting rather than for hard-hitting.
Thepoint still holds, since hitting is a baseball term on its own.Version Av.
Std.
Min.
Max.LT LT LT LTOriginal .953 .092 .832 1.144Via Arabic .875 .093 .747 1.104Via Spanish .909 .081 .801 1.069Via French .912 .087 .786 1.123Via Swedish .931 .099 .796 1.131Table 2: Average lexical tightness (Av.
LT) for theoriginal vs back translated versions, on 20 base-ball texts from the New York Times.
Standard de-viation, minimum, and maximum values are alsoshown.4 Experiment 2: Reference vs MachineTranslationWe use a part of the dataset used in the NIST OpenMT 2008 Evaluation.7 Our set contains transla-tions of 120 news and web articles from Arabic toEnglish.
For each document, there are 4 humanreference translations and 17 machine translationsby various systems that participated in the bench-mark.
Table 3 shows the average and standard de-viation of lexical tightness values across the 120texts for each of the four reference translations,each of the 17 MT systems, as well as an averageacross the four reference translations, and an aver-age across the 17 MT systems.
Each of the 17 MTsystems is statistically significantly less tight thanthe average reference human translation (17 appli-cations of the t-test for correlated samples, n=120,p<0.05); 12 of the 17 MT systems are statisticallysignificantly less tight than the least tight humanreference (reference translation #3) at p<0.05; theaverage system translation is statistically signifi-cantly less tight that the average human translationat p<0.05.To exemplify a large gap in associative texturebetween reference and machine translations, con-sider the following extracts.8 As the raw MT ver-sion (MT-raw) is barely readable, we provide aversion where words are re-arranged for readabil-ity (MT-read), preserving most of the vocabulary.Since lexical tightness operates on content wordtypes, adding or removing repetitions and functionwords does not impact the calculation, so we re-moved or inserted those for the sake of readability7LDC2010T018The first paragraph of arb-WL-1-154489-7725312#Arabic#system21#c.xml vs arb-WL-1-154489-7725312#Arabic#reference 1#r.xml.29Translation Av.
Std.
Min.
Max.LT LT LT LTRef.
1 .873 .140 .590 1.447Ref.
2 .851 .124 .636 1.256Ref.
3 .838 .121 .657 1.177Ref.
4 .865 .131 .639 1.429Av.
Ref.
.857 .124 .641 1.317MT 1 .814 .110 .670 1.113MT 2 .824 .109 .565 1.089MT 3 .818 .113 .607 1.137MT 4 .836 .116 .615 1.144MT 5 .803 .097 .590 1.067MT 6 .824 .116 .574 1.173MT 7 .819 .115 .576 1.162MT 8 .810 .104 .606 1.157MT 9 .827 .114 .546 1.181MT 10 .827 .122 .569 1.169MT 11 .814 .116 .606 1.131MT 12 .826 .112 .607 1.119MT 13 .823 .115 .619 1.116MT 14 .826 .115 .630 1.147MT 15 .820 .107 .655 1.124MT 16 .827 .112 .593 1.147MT 17 .835 .117 .642 1.169Av.
MT .822 .107 .623 1.106Table 3: Average lexical tightness (Av.
LT) forthe reference vs machine translations, on the NISTOpen MT 2008 Evaluation Arabic to English cor-pus.
Standard deviation, minimum, and maximumvalues across the 120 texts are also shown.in the MT-read version.MT-raw vision came to me on dream in view ofher dream: Arab state to travel to and groupof friends on my mission and travel quicklyI was with one of the girls seem close to theremaining more than I was happy and you?reraised ended === known nowMT-read A vision came to me in a dream.
I wasto travel quickly to an Arab state with a groupof friends on a mission.
I was with one ofthe girls who seemed close to the remainingones.
I was happy and you are raised.
Itended.
It is known now.Ref A Dream.
My sister came to tell me about adream she had while she slept.
She was say-ing: I saw you preparing to travel to an Arabcountry, myself and a group of girlfriends.You were sent on a scholarship abroad, andyou were preparing to travel quickly.
Youwere with one of the girls, who appeared tobe closer to you than the others, and I washappy and excited because you were travel-ing.
The end.
I now know !The use of vision instead of dream, state in-stead of country, friends instead of girlfriends,mission instead of scholarship, raised instead ofexcited, along with the complete disapperanceof slept, sister, preparing, abroad, all contributeto a dramatic loss of associative texture in theMT version.
Highly associated pairs like dream-slept, tell-saying, girlfriends-girls, travel-abroad,sister-girls, happy-excited, travel-traveling are allmissed in the machine translation, while the newlyintroduced word raised is quite unrelated to therest of the vocabulary in the extract.5 Experiment 3: Quality of MachineTranslation5.1 System-Level ComparisonIn this experiment, we address the following ques-tion: Is it the case that when a worse MT system Aand a better MT system B translate the same set ofmaterials, B tends to provide more lexically tighttranslations?To address this question, we use the Metrics-MATR 2008 development set (Przybocki et al2009) from NIST Open MT 2006 evaluation.Eight MT systems were used to translate 25 newsarticles from Arabic to English, and humans pro-vided scores for translation adequacy on a 1-7scale.
We calculated the average lexical tightnessover 25 texts for each of the eigth MT systems, aswell as the average translation score for each of thesystems.
We note that human scores are availableper text segments (roughly equivalent to a sen-tence, 249 segments in total for 25 texts), ratherthan for whole texts.
We first derive a human scorefor the whole text for a given system by averagingthe scores of the system?s translations of the differ-ent segments of the text.
We then derive a humanscore for an MT system by averaging the scores ofits translations of the 25 texts.
We found that theaverage adequacy score of a system is statisticallysignificantly positively correlated with the averagelexical tightness that the system?s translations ex-hibit: r=0.630, n=8, df = 6, p<0.05.305.2 Translation-Level ComparisonThe same data could be used to answer the ques-tion: Is it the case that better translations arelexically tighter?
Experiment 2 demonstrated thathuman reference translations are tighter than ma-chine translations; does the same relationship holdfor better vs worse machine translations?
To ad-dress this question, 25 x 8 = 200 instances of (sys-tem, text) pairs can be used, where each has ahuman score for translation adequacy and a lexi-cal tightness value.
Human scores and lexicaltightness of a translated text are significantly pos-itively correlated, r=0.178, n=200, p<0.05.
Note,however, that this analysis is counfounded by thevariation in lexical tightness that exists betweentexts: As standard deviations and ranges in Ta-bles 2 and 3 make clear, original human texts, aswell as reference human translation for differenttexts, vary in their lexical tightness.
Therefore, alower lexical tightness value can be expected forcertain texts even for adequate translations, whilefor other texts low values of lexical tightness sig-nal a low quality translation.
System-level anal-ysis as presented in section 5.1 avoids this con-founding, since all systems translated the same setof texts, therefore average tightness values per sys-tem are directly comparable.6 Discussion and ConclusionWe presented a suggestive finding regarding theloss of associative texture in the process of ma-chine translation, using comparisons between (a)original and back-translated texts, (b) referenceand system translations, (c) better and worse ma-chine translations.
We represented the amount ofassociation in a text using word association pro-file ?
a distribution of point wise mutual infor-mation between all pairs of content word typesin a text.
We used the average of the distribu-tion, which we term lexical tightness ?
as a sin-gle measure of the amount of association in a text.We showed that the lexical tightness of human-composed texts is higher than that of the machinetranslated materials.
While the phenomenon of theloss of associative texture has been theoreticallypredicted by translation scholars, lexical tightnessis a computational measure capable of quantifyingthe extent of this phenomenon.Our work complements that of Wong andKit (2012) in demonstrating the potential utilityof discourse-level phenomena to assess machinetranslations.
First, we note that our findings areorthogonal to the main finding in Wong and Kit(2012) regarding loss of cohesion through insuffi-cient word repetition, since our measure looks atpairs of word types, hence disregards repetitions.Second, the notion of pairwise word associationgeneralizes the notion of lexical cohesive devicesby looking not only at repeated reference with dif-ferent lexical items or at words standing in cer-tain semantic relations to each other, but at thewhole of the lexical network of the text.
Third, dif-ferently from the cohesion measure proposed byWong and Kit (2012), the lexical tightness mea-sure does not depend on lexicographic resourcessuch as WordNet that do not exist in many lan-guages.ReferencesBeata Beigman Klebanov and Michael Flor.
2013.Word Association Profiles and their Use for Auto-mated Scoring of Essays.
In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics, Sofia, Bulgaria, August.Antoine Berman.
2000.
Translation and the Trials ofthe Foreign (translated from 1985 French original byL.
Venuti).
In Lawrence Venuti, editor, The Trans-lation Studies Reader, pages 276?289.
New York:Routledge.Marine Carpuat and Michel Simard.
2012.
The Trou-ble with SMT Consistency.
In Proceedings of the7th Workshop on Statistical Machine Translation,pages 442?449, Montre?al, Canada, June.
Associa-tion for Computational Linguistics.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 909?919, Edinburgh, Scotland,UK., July.
Association for Computational Linguis-tics.David Graff and Christopher Cieri.
2003.
English Gi-gaword LDC2003T05.
Linguistic Data Consortium,Philadelphia.Mark Przybocki, Kay Peterson, and Sebastien Bron-sart.
2009.
2008 NIST metrics for machine transla-tion (MetricsMATR08) development data.Jo?rg Tiedemann.
2010.
Context adaptation in statisti-cal machine translation using models with exponen-tially decaying cache.
In Proceedings of the 2010Workshop on Domain Adaptation for Natural Lan-guage Processing, pages 8?15, Uppsala, Sweden,July.
Association for Computational Linguistics.31Ferhan Ture, Douglas W. Oard, and Philip Resnik.2012.
Encouraging consistent translation choices.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 417?426, Montre?al, Canada, June.
Associa-tion for Computational Linguistics.Lawrence Venuti.
1995.
The Translator?s Invisibi-ilty: A History of Translation.
London & New York:Routledge.Billy Tak-Ming Wong and Chunyu Kit.
2012.
Extend-ing machine translation evaluation metrics with lexi-cal cohesion to document level.
In EMNLP-CoNLL,pages 1060?1068.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level consistency verification inmachine translation.
In Proceedings of the MachineTranslation Summit XIII.32
