Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78?86,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsCoupling hierarchical word reordering and decoding in phrase-basedstatistical machine translationMaxim Khalilov and Jos?
A.R.
FonollosaUniversitat Polit?cnica de CatalunyaCampus Nord UPC, 08034,Barcelona, Spain{khalilov,adrian}@gps.tsc.upc.eduMark DrasMacquarie UniversityNorth Ryde NSW 2109,Sydney, Australiamadras@ics.mq.edu.auAbstractIn this paper, we start with the existing idea oftaking reordering rules automatically derivedfrom syntactic representations, and applyingthem in a preprocessing step before translationto make the source sentence structurally morelike the target; and we propose a new approachto hierarchically extracting these rules.
Weevaluate this, combined with a lattice-baseddecoding, and show improvements over state-of-the-art distortion models.1 IntroductionOne of the big challenges for the MT community isthe problem of placing translated words in a naturalorder.
This issue originates from the fact that dif-ferent languages are characterized by different wordorder requirements.
The problem is especially im-portant if the distance between words which shouldbe reordered is high (global reordering); in this casethe reordering decision is very difficult to take basedon statistical information due to dramatic expansionof the search space with the increase in number ofwords involved in the search process.Classically, statistical machine translation (SMT)systems do not incorporate any linguistic analysisand work at the surface level of word forms.
How-ever, more recently MT systems are moving towardsincluding additional linguistic and syntactic infor-mative sources (for example, source- and/or target-side syntax) into word reordering process.
In this pa-per we propose using a syntactic reordering systemoperating with fully, partially and non- lexicalizedreordering patterns, which are applied on the stepprior to translation; the novel idea in this paper is inthe derivation of these rules in a hierarchical manner,inspired by Imamura et al(2005).
Furthermore, wepropose generating a word lattice from the bilingualcorpus with the reordered source side, extending thesearch space on the decoding step.
A thorough studyof the combination of syntactical and word lattice re-ordering approaches is another novelty of the paper.2 Related workMany reordering algorithms have appeared over thepast few years.
Word class-based reordering was apart of Och?s Alignment Template system (Och etal., 2004); the main criticism of this approach is thatit shows bad performance for the pair of languageswith very distinct word order.
The state-of-the-artSMT system Moses implements a distance-based re-ordering model (Koehn et al, 2003) and a distor-tion model, operating with rewrite patterns extractedfrom a phrase alignment table (Tillman, 2004).Many SMT models implement the brute force ap-proach, introducing several constrains for the re-ordering search as described in Kanthak et al (2005)and Crego et al (2005).
The main criticism of suchsystems is that the constraints are not lexicalized.Recently there has been interest in SMT exploitingnon-monotonic decoding which allow for extensionof the search space and linguistic information in-volvement.
The variety of such models includes aconstrained distance-based reordering (Costa-juss?et al, 2006); and a constrained version of distortionmodel where the reordering search problem is tack-led through a set of linguistically motivated rulesused during decoding (Crego and Mari?o, 2007).78A quite popular class of reordering algorithms isa monotonization of the source part of the parallelcorpus prior to translation.
The first work on thisapproach is described in Nie?en and Ney (2004),where morpho-syntactic information was used to ac-count for the reorderings needed.
A representativeset of similar systems includes: a set of hand-craftedreordering patterns for German-to-English (Collinset al, 2005) and Chinese-English (Wang et al,2007) translations, emphasizing the distinction be-tween German/Chinese and English clause struc-ture; and statistical machine reordering (SMR) tech-nique where a monotonization of the source wordssequence is performed by translating them into thereordered one using well established SMT mecha-nism (Costa-juss?
and Fonollosa, 2006).
Couplingof SMR algorithm and the search space extensionvia generating a set of weighted reordering hypothe-ses has demonstrated a significant improvement, asshown in Costa-juss?
and Fonollosa (2008).The technique proposed in this study is mostsimilar to the one proposed for French-to-Englishtranslation task in Xia and McCord (2004), wherethe authors present a hybrid system for French-English translation based on the principle of auto-matic rewrite patterns extraction using a parse treeand phrase alignments.
We propose using a worddistortion model not only to monotonize the sourcepart of the corpus (using a different approach torewrite rule organization from Xia and McCord), butalso to extend the search space during decoding.3 Baseline phrase-based SMT systemsThe reference system which was used as a transla-tion mechanism is the state-of-the-art Moses-basedSMT (Koehn et al, 2007).
The training and weightstuning procedures can be found on the Moses webpage1.Classical phrase-based translation is consideredas a three step algorithm: (1) the source sequenceof words is segmented into phrases, (2) each phraseis translated into the target language using a transla-tion table, (3) the target phrases are reordered to fitthe target language.
The probabilities of the phrasesare estimated by relative frequencies of their appear-ance in the training corpus.1http://www.statmt.org/moses/In baseline experiments we used a phrase depen-dent lexicalized reordering model, as proposed inTillmann (2004).
According to this model, mono-tonic or reordered local orientations enriched withprobabilities are learned from training data.
Duringdecoding, translation is viewed as a monotone blocksequence generation process with the possibility toswap a pair of neighbor blocks.4 Syntax-based reordering coupled withword graphOur syntax-based reordering system requires accessto source and target language parse trees and wordalignments intersections.4.1 NotationSyntax-based reordering (SBR) operates with sourceand target parse trees that represent the syntacticstructure of a string in source and target languagesaccording to a Context-Free Grammar (CFG).We call this representation "CFG form".
Weformally define a CFG in the usual way as G =?N,T,R, S?, where N is a set of nonterminal sym-bols (corresponding to source-side phrase and part-of-speech tags); T is a set of source-side terminals(the lexicon), R is a set of production rules of theform ?
?
?, with ?
?
N and ?, which is a sequenceof terminal and nonterminal symbols; and S ?
N isthe distinguished symbol.The reordering rules then have the form?0@0 .
.
.
?k@k ?
?d0@d0 .
.
.
?dk@dk|Lexicon|p1 (1)where ?i ?
N for all 0 ?
i ?
k; (do .
.
.
dk) isa permutation of (0 .
.
.
k); Lexicon comes from thesource-side set of words for each ?i; and p1 is a prob-ability associated with the rule.
Figure 1 gives twoexamples of the rule format.4.2 Rules extractionConcept.
Inspired by the ideas presented in Imamuraet al (2005), where monolingual correspondences ofsyntactic nodes are used during decoding, we extracta set of bilingual patterns allowing for reordering asdescribed below:79(1) align the monotone bilingual corpus withGIZA++ (Och and Ney, 2003) and findthe intersection of direct and inverse wordalignments, resulting in the constructionof the projection matrix P (see below));(2) parse the source and the target parts of theparallel corpus;(3) extract reordering patterns from the par-allel non-isomorphic CFG-trees based onthe word alignment intersection.Step 2 is straightforward; we explain aspects ofSteps 1 and 3 in more detail below.
Figures 1 and 2show an example of the extraction of two lexicalizedrules for a parallel Arabic-English sentence:Arabic:English:h*AthishWisfndqyour+khotelWe use this below in our explanations.Figure 2: Example of subtree transfer and reorderingrules extraction.Projection matrix.
Bilingual content can be rep-resented in the form of words or sequences of wordsdepending on the syntactic role of the correspondinggrammatical element (constituent or POS).Given two parse trees and a word alignment in-tersection, a projection matrix P is defined as anM ?N matrix such that M is the number of wordsin the target phrase; N is the number of words inthe source phrase; and a cell (i, j) has a value basedon the alignment intersection ?
this value is zeroif word i and word j do not align, and is a uniquenon-zero link number if they do.For the trees in Figure 2,P =???
?1 0 0 00 2 0 00 0 0 30 0 4 0???
?Unary chains.
Given an unary chain of the formX ?
Y , rules are extracted for each level in thischain.
For example given a ruleNP@0ADV P@1 ?
ADV P@1NP@0and a unary chain "ADV P ?
AD", a followingequivalent rule will be generatedNP@0AD@1 ?
AD@1NP@0.The role of target-side parse tree.
Although re-ordering is performed on the source side only, thetarget-side tree is of great importance: the reorder-ing rules can be only extracted if the words coveredby the rule are entirely covered by both a node inthe source and in the target trees.
It allows the moreaccurate determination of the covering and limits ofthe extracted rules.4.3 Rules organizationOnce the list of fully lexicalized reordering patternsis extracted, all the rules are progressively processedreducing the amount of lexical information.
Theseinitial rules are iteratively expanded such that eachelement of the pattern is generalized until all the lex-ical elements of the rule are represented in the formof fully unlexicalized categories.
Hence, from eachNN@0 NP@1 ?
NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >> | pNN@0 NNP@1 ?
NNP@1 NN@0 | NN@0 << fndq >> NNP@1 << +k >> | p?Figure 1: Directly extracted rules.80initial pattern with N lexical elements, 2N ?
2 par-tially lexicalized rules and 1 general rule are gener-ated.
An example of the process of delexicalizationcan be found in Figure 3.Thus, finally three types of rules are available: (1)fully lexicalized (initial) rules, (2) partially lexical-ized rules and (3) unlexicalized (general) rules.On the next step, the sets are processed separately:patterns are pruned and ambiguous rules are re-moved.
All the rules from the fully lexicalized, par-tially lexicalized and general sets that appear fewerthan k times are directly discarded (k is a shorthandfor kful, kpart and kgener).
The probability of apattern is estimated based on relative frequency oftheir appearance in the training corpus.
Only onethe most probable rule is stored.
Fully lexicalizedrules are not pruned (kful = 0); partially lexicalizedrules that have been seen only once were discarded(kpart = 1); the thresholds kgener was set to 3: itlimits the number of general patterns capturing raregrammatical exceptions which can be easily foundin any language.Only the one-best reordering is used in otherstages of the algorithm, so the rule output function-ing as an input to the next rule can lead to situa-tions reverting the change of word order that thepreviously applied rule made.
Therefore, the rulesthat can be ambiguous when applied sequentiallyduring decoding are pruned according to the higherprobability principle.
For example, for the pair ofpatterns with the same lexicon (which is empty fora general rule leading to a recurring contradictionNP@0 VP@1 ?
VP@1 NP@0 p1, VP@0 NP@1?
NP@1 VP@0 p2 ), the less probable rule is re-moved.Finally, there are three resulting parameter tablesanalogous to the "r-table" as stated in (Yamada andKnight, 2001), consisting of POS- and constituent-based patterns allowing for reordering and mono-tone distortion (examples can be found in Table 5).4.4 Source-side monotonizationRule application is performed as a bottom-up parsetree traversal following two principles:(1) the longest possible rule is applied, i.e.
amonga set of nested rules, the rule with a longest left-sidecovering is selected.
For example, in the case of theappearance of an NN JJ RB sequence and presenceof the two reordering rulesNN@0 JJ@1 ?
... andNN@0 JJ@1 RB@2 ?
...the latter pattern will be applied.
(2) the rule containing the maximum lexical infor-mation is applied, i.e.
in case there is more than onealternative pattern from different groups, the lexical-ized rules have preference over the partially lexical-ized, and partially lexicalized over general ones.Figure 4: Reordered source-side parse tree.Once the reordering of the training corpus isready, it is realigned and new more monotonic align-ment is passed to the SMT system.
In theory, theword links from the original alignment can be used,however, due to our experience, running GIZA++again results in a better word alignment since it iseasier to learn on the modified training example.Example of correct local reordering done with theSBR model can be found in Figure 4.Initial rule: NN@0 NP@1 ?
NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >> | p1Part.
lexic.
rules: NN@0 NP@1 ?
NP@1 NN@0 | NN@0 << fndq >> NP@1 << - >> | p2NN@0 NP@1 ?
NP@1 NN@0 | NN@0 << - >> NP@1 << +k >> | p3General rule: NN@0 NP@1 ?
NP@1 NN@0 | p4Figure 3: Example of a lexical rule expansion.814.5 Coupling with decodingIn order to improve reordering power of the transla-tion system, we implemented an additional reorder-ing as described in Crego and Mari?o (2006).Multiple word segmentations is encoded in a lat-tice, which is then passed to the input of the de-coder, containing reordering alternatives consistentwith the previously extracted rules.
The decodertakes the n-best reordering of a source sentencecoded in the form of a word lattice.
This approachis in line with recent research tendencies in SMT, asdescribed for example in (Hildebrand et al, 2008;Xu et al, 2005).
Originally, word lattice algorithmsdo not involve syntax into reordering process, there-fore their reordering power is limited at representinglong-distance reordering.
Our approach is designedin the spirit of hybrid MT, integrating syntax trans-fer approach and statistical word lattice methods toachieve better MT performance on the basis of thestandard state-of-the-art models.During training a set of word permutation patternsis automatically learned following given word-to-word alignment.
Since the original and monotonized(reordered) alignments may vary, different sets ofreordering patterns are generated.
Note that no in-formation about the syntax of the sentence is used:the reordering permutations are motivated by thecrossed links found in the word alignment and, con-S 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 L> n+ h+ h> nm T E m m T E m* wE r y q> n* wE r y qt A r y xm T E mE r y q* wE r y qt A r y x* w* wE r y qm T E mt A r y xE r y q* wS 1 2 3 4 5 6 7 8 9> n+ h+ h> nm T E m m T E m> n* wt A r y x* wm T E mt A r y x1 0 LE r y qm T E mE r y qt A r y x> n  + h  m T E m  * w  t A r y x  E r y q  W o r d  l a t t i c e ,  p l a i n  t e x t :W o r d  l a t t i c e ,  r e o r d e r e d  t e x t : > n  + h  m T E m  * w  E r y q  t A r y x  ( c )( b )S 1 2 3 4 5> n + h m T E m * w Lt A r y x E r y q> n  + h  m T E m  * w  t A r y x   E r y qM o n o t o n i c  s e a r c h ,  p l a i n  t e x t :( a )Figure 5: Comparative example of a monotone search (a), word lattice for a plain (b) and reordered (c) sourcesentences.82sequently, the generalization power of this frame-work is limited to local permutations.On the step prior to decoding, the system gen-erates word reordering graph for every source sen-tence, expressed in the form of a word lattice.
Thedecoder processes word lattice instead of only oneinput hypothesis, extending the monotonic searchgraph with alternative paths.Original sentence in Arabic, the English gloss andreference translation are:Ar.:Gl.
:>n +hthismTEmrestaurant*whasEryqhistorytAryxillustriousRef: ?this restaurant has an illustrious history?The monotonic search graph (a) is extended witha word lattice for the monotonic train set (b) and re-ordered train sets (c).
Figure 5 shows an exampleof the input word graph expressed in the form of aword lattice.
Lattice (c) differ from the graph (b) innumber of edges and provides more input options tothe decoder.
The decision about final translation istaken during decoding considering all the possiblepaths, provided by the word lattice.5 Experiments and results5.1 DataThe experiments were performed on two Arabic-English corpora: the BTEC?08 corpus from thetourist domain and the 50K first-lines extractionfrom the corpus that was provided to the NIST?08evaluation campaign and belongs to the news do-main (NIST50K).
The corpora differ mainly in theaverage sentence length (ASL), which is the key cor-pus characteristic in global reordering studies.A training set statistics can be found in Table 1.BTEC NIST50KAr En Ar EnSentences 24.9 K 24.9 K 50 K 50 KWords 225 K 210 K 1.2 M 1.35 MASL 9.05 8.46 24.61 26.92Voc 11.4 K 7.6 K 55.3 36.3Table 1: Basic statistics of the BTEC training corpus.The BTEC development dataset consists of 489sentences and 3.8 K running words, with 6 human-made reference translations per sentence; the datasetused to test the translation quality has 500 sentences,4.1 K words and is also provided with 6 referencetranslations.The NIST50K development set consists of 1353sentences and 43 K words; the test data contains1056 sentences and 33 K running words.
Bothdatasets have 4 reference translations per sentence.5.2 Arabic data preprocessingWe took a similar approach to that shown in Habashand Sadat (2006), using the MADA+TOKAN sys-tem for disambiguation and tokenization.
For dis-ambiguation only diacritic unigram statistics wereemployed.
For tokenization we used the D3 schemewith -TAGBIES option.
The scheme splits the fol-lowing set of clitics: w+, f+, b+, k+, l+, Al+ andpronominal clitics.
The -TAGBIES option producesBies POS tags on all taggable tokens.5.3 Experimental setupWe used the Stanford Parser (Klein and Man-ning, 2003) for both languages, Penn English Tree-bank (Marcus et al, 1993) and Penn Arabic Tree-bank set (Kulick et al, 2006).
The English Treebankis provided with 48 POS and 14 syntactic tags, theArabic Treebank has 26 POS and 23 syntactic cate-gories.As mentioned above, specific rules are not prunedaway due to a limited amount of training material weset the thresholds kpart and kgener to relatively lowvalues, 1 and 3, respectively.Evaluation conditions were case-insensitive andwith punctuation marks considered.
The target-side 4-gram language model was estimated usingthe SRILM toolkit (Stolcke, 2002) and modifiedKneser-Ney discounting with interpolation.
Thehighest BLEU score (Papineni et al, 2002) was cho-sen as the optimization criterion.
Apart from BLEU,a standard automatic measure METEOR (Banerjeeand Lavie, 2005) was used for evaluation.5.4 ResultsThe scores considered are: BLEU scores obtainedfor the development set as the final point of theMERT procedure (Dev), and BLEU and METEORscores obtained on test dataset (Test).We present BTEC results (Tables 2), character-ized by relatively short sentence length, and the re-83sults obtained on the NIST corpus (Tables 3) withmuch longer sentences and much need of global re-ordering.Dev TestBLEU BLEU METEORPlain 48.31 45.02 65.98BL 48.46 47.10 68.10SBR 48.75 47.52 67.33SBR+lattice 48.90 48.78 68.85Table 2: Summary of BTEC experimental results.Dev TestBLEU BLEU METEORPlain 41.83 43.80 62.03BL 42.68 43.52 62.17SBR 42.71 44.01 63.29SBR+lattice 43.05 44.89 63.30Table 3: Summary of NIST50K experimental results.Four SMT systems are contrasted: BL refers tothe Moses baseline system: the training data is notreordered, lexicalized reordering model (Tillman,2004) is applied; SBR refers to the monotonic sys-tem configuration with reordered (SBR) source part;SBR+lattice is the run with reordered source part, onthe translation step the input is represented as a wordlattice.We also compare the proposed approach with amonotonic system configuration (Plain).
It showsthe effect of source-reordering and lattice input, alsodecoded monotonically.Automatic scores obtained on the test datasetevolve similarly when the SBR and word lattice rep-resentation applied to BTEC and NIST50K tasks.The combined method coupling two reorderingtechniques was more effective than the techniquesapplied independently and shows an improvementin terms of BLEU for both corpora.
The METEORscore is only slightly better for the SBR configura-tions in case of BTEC task; in the case of NIST50Kthe METEOR improvement is more evident.
Thegeneral trend is that automatic scores evaluated onthe test set increase with the reordering model com-plexity.Application of the SBR algorithm only (withouta word lattice decoding) does not allow achievingstatistical significance threshold for a 95% confi-dence interval and 1000 resamples (Koehn, 2004)for either of considered corpora.
However, theSBR+lattice system configuration outperforms theBL by about 1.7 BLEU points (3.5%) for BTEC taskand about 1.4 BLEU point (3.1%) for NIST task.These differences is statistically significant.Figure 6 demonstrates how two reordering tech-niques interact within a sentence with a need forboth global and local word permutations.5.5 Syntax-based rewrite rulesAs mentioned above, the SBR operates with threegroups of reordering rules, which are the productof complete or partial delexicalization of the origi-nally extracted patterns.
The groups are processedand pruned independently.
Basic rules statistics forboth translation tasks can be found in Table 4.The major part of reordering rules consists oftwo or three elements (for BTEC task there areno patterns including more than three nodes).
ForNIST50K there are a few rules with higher size inwords of the move (up to 8).
In addition, there aresome long lexicalized rules (7-8), generating a highnumber of partially lexicalized patterns.Table 5 shows the most frequent reordering ruleswith non-monotonic right part from each group.Ar.
plain.:En.
gloss:AElntannouncedAjhzppressAlAElAmreleaselbybEvpmissionAlAmm AlmtHdpnations unitedfyinsyrAlywnsierra leoneAnthat......En.
ref.
: ?a press release by the united nations mission to sierra leone announced that ...?Ar.
reord.
: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An ...Figure 6: Example of SBR application (highlited bold) and local reordering error corrected with word lattice reorder-ing (underlined).846 ConclusionsIn this study we have shown how the translationquality can be improved, coupling (1) SBR al-gorithm and (2) word alignment-based reorderingframework applied during decoding.
The systemautomatically learns a set of syntactic reorderingpatterns that exploit systematic differences betweenword order of source and target languages.Translation accuracy is clearly higher when al-lowing for SBR coupled with word lattice input rep-resentation than standard Moses SMT with existing(lexicalized) reordering models within the decoderand one input hypothesis condition.
We have alsocompared the reordering model a monotonic system.The method was tested translating from Arabic toEnglish.
Two corpora and tasks were considered:the BTEC task with much need of local reorderingand the NIST50K task requiring long-distance per-mutations caused by longer sentences.The reordering approach can be expanded for anyother pair of languages with available parse tools.We also expect that the method scale to a large train-ing set, and that the improvement will still be kept,however, we plan to confirm this assumption exper-imentally in the near future.AcknowledgmentsThis work has been funded by the Spanish Gov-ernment under grant TEC2006-13964-C03 (AVI-VAVOZ project) and under a FPU grant.Group # of rules Voc 2-element 3-element 4-element [5-8]-elementBTEC experimentsSpecific rules 703 413 406 7 0 0Partially lexicalized rules 1,306 432 382 50 0 0General rules 259 5 259 0 0 0NIST50K experimentsSpecific rules 517 399 193 109 72 25Partially lexicalized rules 17,897 14,263 374 638 1,010 12,241General rules 489 372 180 90 72 30Table 4: Basic reordering rules statistics.Specific rulesNN@0 NP@1 -> NP@1 NN@0 | NN@0 ?
Asm ?
NP@1 ?
+y ?
| 0.0270DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0 ?
AlAmm ?DTJJ@1 ?
AlmtHdp ?
| 0.0515Partially lexicalized rulesDTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0 ?
NON ?DTJJ@1 ?
AlmtHdp ?
| 0.0017NN@0 NNP@1 -> NNP@1 NN@0 | NN@0 ?
NON ?NNP@1 ?
$rm ?
| 0.0017General rulesPP@0 NP@1 -> PP@0 NP@1 | 0.0432NN@0 DTNN@1 DTJJ@2 -> NN@0 DTJJ@2 DTNN@1 |0.0259Table 5: Examples of Arabic-to-English reordering rules.85ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion, pages 65?72.M.
Collins, Ph.
Koehn, and I. Kuc?erov?.
2005.
Clauserestructuring for statistical machine translation.
InProceedings of the 43rd Annual Meeting on ACL 2005,pages 531?540.M.R.
Costa-juss?
and J.A.R.
Fonollosa.
2006.
Sta-tistical machine reordering.
In Proceedings of theHLT/EMNLP 2006.M.R.
Costa-juss?
and J.A.R.
Fonollosa.
2008.
Comput-ing multiple weighted reordering hypotheses for a sta-tistical machine translation phrase-based system.
In InProc.
of the AMTA?08, Honolulu, USA, October.M.R.
Costa-juss?, J.M.
Crego, A. de Gispert, P. Lambert,M.
Khalilov, J.
A. Fonollosa, J.B. Mari no, and R.E.Banchs.
2006.
TALP phrase-based system and TALPsystem combination for IWSLT 2006.
In Proceedingsof the IWSLT 2006, pages 123?129.J.M.
Crego and J.
B Mari?o.
2006.
Reordering experi-ments for N-gram-based SMT.
In SLT?06, pages 242?245.J.M.
Crego and J.B. Mari?o.
2007.
Syntax-enhanced N-gram-based smt.
In Proceedings of MT SUMMIT XI.J.M.
Crego, J.
B. Mari?o, and A. de Gispert.
2005.
Re-ordered search and tuple unfolding for ngram-basedsmt.
In In Proc.
of MT SUMMIT X, pages 283?289,September.S.
Nie?en and H. Ney.
2004.
Statistical machine transla-tion with scarce resources using morpho-syntactic in-formation.
volume 30, pages 181?204.N.
Habash and F. Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In Pro-ceedings of the Human Language Technology Confer-ence of the NAACL, pages 49?52.A.S.
Hildebrand, K. Rottmann, M. Noamany, Q. Gao,S.
Hewavitharana, N. Bach, and S. Vogel.
2008.
Re-cent improvements in the cmu large scale chinese-english smt system.
In Proceedings of ACL-08: HLT(Companion Volume), pages 77?80.K.
Imamura, H. Okuma, and E. Sumita.
2005.
Practicalapproach to syntax-based statistical machine transla-tion.
In Proceedings of MT Summit X, pages 267?274.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.2005.
Novel reordering approaches in phrase-basedstatistical machine translation.
In In Proc.
of the ACLWorkshop on Building and Using Parallel Texts, pages167?174, June.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of the 41st Annual Meeting ofthe ACL 2003, pages 423?430.Ph.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based machine translation.
In Proceedings ofthe HLT-NAACL 2003, pages 48?54.Ph.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: open-source toolkit forstatistical machine translation.
In Proceedings of ACL2007, pages 177?180.Ph.
Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395.S.
Kulick, R. Gabbard, and M. Marcus.
2006.
Parsing theArabic Treebank: Analysis and improvements.
Tree-banks and Linguistic Theories.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.J.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A Smorgasbord ofFeatures for Statistical Machine Translation.
In Pro-ceedings of HLT/NAACL04, pages 161?168.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL 2002, pages 311?318.A.
Stolcke.
2002.
SRILM: an extensible language mod-eling toolkit.
In Proceedings of the Int.
Conf.
on Spo-ken Language Processing, pages 901?904.C.
Tillman.
2004.
A unigram orientation model for sta-tistical machine translation.
In Proceedings of HLT-NAACL?04.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinese syn-tactic reordering for statistical machine translation.
InProceedings of the Joint Conference on EMNLP.F.
Xia and M. McCord.
2004.
Improving a statistical mtsystem with automatically learned rewrite patterns.
InProceedings of the COLING 2004.J.
Xu, E. Matusov, R. Zens, and H. Ney.
2005.
In-tegrated chinese word segmentation in statistical ma-chine translation.
In Proc.
of IWSLT 2005.K.
Yamada and K. Knight.
2001.
A syntax-based statis-tical translation model.
In Proceedings of ACL 2001,pages 523?530.86
