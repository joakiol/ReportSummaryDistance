Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1137?1146,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsGenerating Aspect-oriented Multi-Document Summarization withEvent-aspect modelPeng Li1 and Yinglin Wang1 and Wei Gao2and Jing Jiang31 Department of Computer Science and Engineering, Shanghai Jiao Tong University2 Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong3 School of Information Systems, Singapore Management University{lipeng, ylwang@sjtu.edu.cn} {wgao@se.cuhk.edu.hk} {jingjiang@smu.edu.sg}AbstractIn this paper, we propose a novel approach toautomatic generation of aspect-oriented sum-maries from multiple documents.
We first de-velop an event-aspect LDA model to clustersentences into aspects.
We then use extend-ed LexRank algorithm to rank the sentencesin each cluster.
We use Integer Linear Pro-gramming for sentence selection.
Key featuresof our method include automatic grouping ofsemantically related sentences and sentenceranking based on extension of random walkmodel.
Also, we implement a new sentencecompression algorithm which use dependencytree instead of parser tree.
We compare ourmethod with four baseline methods.
Quantita-tive evaluation based on Rouge metric demon-strates the effectiveness and advantages of ourmethod.1 IntroductionIn recent years, there has been much interest inthe task of multi-document summarization.
In thispaper, we study the task of automatically generat-ing aspect-oriented summaries from multiple docu-ments.
The goal of aspect-oriented summarizationis to present the most important content to the us-er in a condensed form and a well-organized struc-ture to satisfy the user?s needs.
A summary shouldfollow a readable structure and cover all the aspect-s users are interested in.
For example, a summaryabout natural disasters should include aspects aboutwhat happened, when/where it happened, reasons,damages, rescue efforts, etc.
and these aspects maybe scattered in multiple articles written by differentnews agencies.
Our goal is to automatically collectaspects and construct summaries from multiple doc-uments.Aspect-oriented summarization can be used inmany scenarios.
First of all, it can be used to gener-ateWikipedia-like summary articles, especially usedto generate introduction sections that summarizesthe subject of articles before the table of contentsand other elaborate sections.
Second, opinionat-ed text often contains multiple viewpoints about anissue generated by different people.
Summarizingthese multiple opinions can help people easily di-gest them.
Furthermore, combined with search en-gines and question&answering systems, we can bet-ter organize the summary content based on aspectsto improve user experience.Despite its usefulness, the problem of modelingdomain specific aspects for multi-document summa-rization has not been well studied.
The most relevantwork is by (Haghighi and Vanderwende, 2009) onexploring content models for multi-document sum-marization.
They proposed a HIERSUM model forfinding the subtopics or aspects which are combinedby using KL-divergence criterion for selecting rel-evant sentences.
They introduced a general con-tent distribution and several specific content distri-butions to discover the topic and aspects for a s-ingle document collection.
However, the aspectsmay be shared not only across documents in a sin-gle collection, but also across documents in differenttopic-related collections.
Their model is conceptual-ly inadequate for simultaneously summarizing mul-tiple topic-related document collections.
Further-more, their sentence selection method based on KL-divergence cannot prevent redundancy across differ-ent aspects.In this paper, we study how to overcome these1137limitations.
We hypothesize that comparativelysummarizing topics across similar collections canimprove the effectiveness of aspect-oriented multi-document summarization.
We propose a novelextraction-based approach which consists of fourmain steps listed below:Sentence Clustering: Our goal in this step is toautomatically identify the different aspects and clus-ter sentences into aspects (See Section 2).
We sub-stantially extend the entity-aspect model in (Li et al,2010) for generating general sentence clusters.Sentence Ranking: In this step, we use an exten-sion of LexRank algorithm proposed by (Paul et al,2010) to score representative sentences in each clus-ter (See Section 3).Sentence Compression: In this step, we aim toimprove the linguistic quality of the summaries bysimplifying the sentence expressions.
We prune sen-tences using grammatical relations defined on de-pendency trees for recognizing important clausesand removing redundant subtrees (See Section 4).Sentence Selection: Finally, we select one com-pressed version of the sentences from each aspec-t cluster.
We use Integer Linear Programming(ILP) algorithm, which optimizes a global objectivefunction, for sentence selection (McDonald, 2007;Gillick and Favre, 2009; Sauper and Barzilay, 2009)(See Section 5).We evaluate our method using TAC2010 GuidedSummarization task data sets1 (Section 6).
Our eval-uation shows that our method obtains better ROUGErecall score compared with four baseline methods,and it also achieve reasonably high-quality aspec-t clusters in terms of purity.2 Sentence ClusteringIn this step, our goal is to discover event aspects con-tained in a document set and cluster sentences in-to aspects.
Here we substantially extend the entity-aspect model in Li et al (2010) and refer to it asevent-aspect model.
The main difference betweenour event-aspect model and entity-aspect model isthat we introduce an additional layer of event topicsand the separation of general and specific aspects.1http://www.nist.gov/tac/2010/Summarization/Our extension is based upon the following ob-servations.
For example, specific events like?Columbine Massacre?
and ?Malaysia Resort Ab-duction?
can be related to the ?Attack?
topic.
Eachevent consists of multiple articles written by dif-ferent news agencies.
Interesting aspects may in-clude ?what happened, when, where, perpetrators,reasons, who affected, damages and countermea-sures,?
etc2.
We compared the ?Columbine Mas-sacre?
and ?Malaysia Resort Abduction?
data set-s and found 5 different kinds of words in the text:(1) stop words that occur frequently in any docu-ment collection; (2) general content words describ-ing ?damages?
or ?countermeasures?
aspect of at-tacks; (3) specific content words describing ?whathappened?, ?who affected?
or ?where?
aspect of theconcrete event; (4) background words describing thegeneral topic of ?Attack?
; (5) words that are local toa single document and do not appear across differentdocuments.
Table 1 shows four sentences related totwo major aspects.
We found that the entity-aspectmodel does not have enough capacity to cluster sen-tences into aspects (See Section 6).
So we introduceadditional layer to improve the effectiveness of sen-tence clustering.
We also found that their one aspectper sentence assumption is not very strong in thisscenario.
Although a sentence may belong to a sin-gle general aspect, it still contains multiple specificaspect words like second sentence in Table 1.
There-fore, We assume that each sentence belongs to botha general aspect and a specific aspect.2.1 Event-Aspect ModelStop words can be ignored by LDA model becausethey can be easily identified using a standard stopword list.
Suppose that for a given event topic, thereare in total C specific events for which we need tosimultaneously generate summaries.
We can assumefour kinds of unigram language models (i.e.
multi-nomial word distributions).
For each event topic,there is a background model ?B that generates wordscommonly used in all documents, and there are AGgeneral aspect models ?ga (1 ?
ga ?
AG), whereAG is the number of general aspects.
For each spe-cific event in a topic, there are AS specific aspect2http://www.nist.gov/tac/2010/Summarization/Guided-Summ.2010.guidelines.html1138countermeasuresPolice/GA are/S close/B to/S identifying/GA someone/B responsible/GAfor/S the/S attack/B .Investigators/GA do/S not/S know/B how/S many/S suspects/SAthey/S are/S looking/B for/S, but/S reported/B progress/B toward/Sidentifying/GA one/S of/S the/S bombers/SA .what happened, when, whereDuring/S the/S morning/SA rush/D hour/D on/S July/SA 7/SA terrorists/Bexploded/SA bombs/SA on/S three/D London/SA subway/D trains/SA and/S a/Sdouble-decker/D bus/SA .Four/D coordinated/B bombings/SA struck/B central/B London/SA on/SAJuly/SA 7/SA, three/D in/S subway/D cars/SA and/S one/D on/S a/S bus/SA .Table 1: Four sentences on ?COUNTERMEASURES?
and ?What, When, Where?
aspects from the ?Attack?
topic.
S:stop word.
B: background word.
GA: general aspect word.
SA: specific aspect word.
D: document word.models ?sa (1 ?
sa ?
AS), where AS is the num-ber of specific aspects, and also there are D doc-ument models ?d (1 ?
d ?
D), where D is thenumber of documents in this collection.
We assumethat these word distributions have a uniform Dirich-let prior with parameter ?.We introduce a level distribution ?
that control-s whether we choose a word from ?ga or ?sa.
?is sampled from Beta(?0, ?1) distribution.
We alsointroduce an aspect distribution ?
that controls howoften a general or a specific aspect occurs in the col-lection, where ?
is sampled from another Dirichletprior with parameter ?.
There is also a multinomi-al distribution ?
that controls in each sentence howoften we encounter a background word, a documentword, or an aspect word.
?
has a Dirichlet prior withparameter ?.Let Sd denote the number of sentences in docu-ment d, Nd,s denote the number of words (after stopword removal) in sentence s of document d, andwd,s,n denote the n?th word in this sentence.
Weintroduce hidden variables zgad,s and zsad,s to indicatethat a sentence s of document d belongs to whichgeneral or specific aspects .
We introduce hiddenvariables yd,s,n for each word to indicate whether aword is generated from the background model, thedocument model, or the aspect model.
We also intro-duce hidden variables ld,s,n to indicate whether then?th word in sentence s of document d is generatedfrom the general aspect model.
Figure 1 describesthe process of generating the whole document col-lection.
The plate notation of the model is shown inFigure 2.
Note that the values of ?0, ?1, ?1, ?2, ?and ?
are fixed.
The number of general and specificaspects AG and AS are also empirically set.Given a document collection, i.e.
the set of allwd,s,n, our goal is to find the most likely assignmen-t of zgad,s, zsad,s, yd,s,n and ld,s,n that maximizes dis-tribution p(z,y, l|w;?, ?, ?, ?
), where z, y, l and wrepresent the set of all z, y, l andw variables, respec-tively.
With the assignment, sentences are naturallyclustered into aspects, and words are labeled as ei-ther a background word, a document word, a generalaspect word or a specific aspect word.Inference can be done with Gibbs sampling,which is commonly used in LDA models (Griffithsand Steyvers, 2004).In our experiments, we set ?1 = 5, ?2 = 3,?
= 0.01, ?
= 20, ?1 = 10 and ?2 = 10.
Werun 100 burn-in iterations through all documents ina collection to stabilize the distribution of z and ybefore collecting samples.
We take 10 samples witha gap of 10 iterations between two samples, and av-erage over these 10 samples to get the estimation forthe parameters.After estimating all the distributions, we can findthe values of each zgad,s and zsad,s that gives us sen-tences clustered into general and specific aspects.3 Sentence RankingIn this step, we want to order the clustered sen-tences so that the representative sentences can beranked higher in each aspect.
Inspired by Paul etal.
(2010), we use an extended LexRank algorithmto obtain top ranked sentences.
LexRank (Erkan andRadev, 2004) algorithm defines a random walk mod-11391.
Draw ?1 ?
Dir(?1), ?2 ?
Dir(?2), ?
?
Dir(?
)Draw ?
?
Beta(?0, ?1)2.
For each event topic, there is a background model?B, and there are general aspect ga, where 1 ?ga ?
AG(a) draw ?B ?
Dir(?
)(b) draw ?ga ?
Dir(?)3.
For each document collection, there are specificaspect sa, where 1 ?
sa ?
AS(a) draw ?sa ?
Dir(?)4.
For each document d = 1, .
.
.
, D,(a) draw ?d ?
Dir(?
)(b) for each sentence s = 1, .
.
.
, Sdi.
draw zga ?
Multi(?1)ii.
draw zsa ?
Multi(?2)iii.
for each word n = 1, .
.
.
, Nd,sA.
draw ld,s,n ?
Binomial(?)B.
draw yd,s,n ?
Multi(?)C.
drawwd,s,n ?
Multi(?B) if yd,s,n =1, wd,s,n ?
Multi(?d) if yd,s,n = 2,wd,s,n ?
Multi(?zsad,s) if yd,s,n =3 and ld,s,n = 1 or wd,s,n ?Multi(?zgad,s) if yd,s,n = 3 andld,s,n = 0Figure 1: The document generation process.el on top of a graph that represents sentences to besummarized as nodes and their similarities as edges.The LexRank score of a sentence gives the expectedprobability that a random walk will visit that sen-tence in the long run.
A variant is called continu-ous LexRank improved LexRank by making use ofthe strength of the similarity links.
The continuousLexRank score can be computed using the followingformula:L(u) = dN + (1 ?
d)?v?adj[u]p(u|v)L(v)whereL(u) is the LexRank value of sentence u,N isthe total number of nodes in the graph, d is a damp-ing factor for the convergence of the method, andp(u|v) is the jumping probability between sentenceu and its neighboring sentence v. p(u|v) is definedusing content similarity function sim(u, v) betweentwo sentences:Tyd?
dSD sd ,wSA?
?C?pi ?
?gaz sazl?GAB?1?
2?1?
2?Figure 2: The event-aspect model.p(u|v) = sim(u, v)?z?adj[v] sim(z, v)The major extension is to modify this jumpingprobability so as to favor visiting representative sen-tences.
More specifically, we scale sim(u, v) by thelikelihood that the two sentences represent the samegeneral aspect ga or specific aspect sa:sim?
(u, v) = sim(u, v)[AG?ga=1P (ga|u)P (ga|v)+AS?sa=1P (sa|u)P (sa|v)]where the value P (ga|u) and P (sa|u) can becomputed by our event-aspect model.
We definesim(u, v) as the tf ?
idf weighted cosine similar-ity between two sentences.We found that sentence ranking is better con-ducted before the compression because the pre-compressed sentences are more informative and thesimilarity function in LexRank can be better off withthe complete information.4 Sentence CompressionIt has been shown that sentence compression canimprove linguistic quality of summaries (Zajic etal., 2007; Gillick et al, 2010).
Commonly used?Syntactic parse and trim?
approach may producepoor compression results.
For example, given thesentence ?We have friends whose children go toColumbine, the freshman said?, the procedure triesto remove the clause ?the freshman said?
from theparse tree by using the ?SBAR?
label to locate the1140clause, and will result in ?whose children go toColumbine?, which is not adequate.
Furthermore,some important temporal modifier, numeric modifierand clausal complement need to be retained becausethey reflect content aspects of the summary.
There-fore, we propose the ?dependency parse and trim?approach, which prunes sentences based on depen-dency tree representations, using English grammati-cal relations to recognize clauses and remove redun-dant structures.
Table 2 shows two examples by re-moving redundant auxiliary clauses.
Below is thesentence compression procedure:1.
Select possible subtree root nodes using gram-matical relations, such as clausal complement,complementizer, or parataxis 3.2.
Decide which subtree root node can be the rootof clause.
If this root contains maximum num-ber of child nodes and the collection of all childedges include object or auxiliary relations, it isselected as the root node.3.
Remove redundant modifiers such as adverbial-s, relative clause modifiers and abbreviations,participials and infinitive modifiers.4.
Traverse the subtrees and generate all possiblecompression alternatives using the subtree rootnode, then keep the top two longest sub sen-tences.5.
Drop the sub sentences shorter than 5 words.5 Sentence SelectionAfter sentence pruning, we prepare for the finalevent summary generation process.
In this step, weselect one compressed version of the sentence fromeach aspect cluster.
To avoid redundancy betweenaspects, we use Integer Linear Programming to opti-mize a global objective function for sentence selec-tion.
Inspired by (Sauper and Barzilay, 2009), weformulate the optimization problem based on sen-tence ranking information.
More specifically, we3The parataxis relation is a relation between the main verbof a clause and other sentential elements, such as a sententialparenthetical, colon, or semicolonOriginal CompressedWhen rescue workersarrived, they said, on-ly one of his limbs wasvisible.When rescue workersarrived, only one of hislimbs was visible.Two days earlier, amassacre by two s-tudents at ColumbineHigh, whose teams arecalled the Rebels, left15 people dead anddozens wounded.Two days earlier, amassacre by two stu-dents at ColumbineHigh, left 15 peo-ple dead and dozenswounded.Table 2: Example compressed sentences.would like to select exactly one compressed sen-tence which receives the highest possible ranking s-core from each aspect cluster subject to a series ofconstraints, such as redundancy and length.
We em-ployed lp solver 4, an efficient mixed integer pro-gramming solver using the Branch-and-Bound algo-rithm to select sentences.Assume that there are in total K aspects in anevent topic.
For each aspect j, there are in total Rranked sentences.
The variables Sjl is a binary indi-cator of the sentence.
That is, Sjl= 1 if the sentenceis included in the final summary, and Sjl = 0 other-wise.
l is the ranked position of the sentence in thisaspect cluster.Objective FunctionTop ranked sentences are the most relevant corre-sponding to the related aspects which we want to in-clude in the final summary.
Thus we try to minimizethe ranks of the sentences to improve the overall re-sponsiveness.min(K?j=1Rj?l=1l ?
Sjl)Exclusivity ConstraintsTo prevent redundancy in each aspect, we justchoose one sentence from each general or specificaspect cluster.
The constraint is formulated as fol-lows:Rj?l=1Sjl = 1 ?j ?
{1 .
.
.K}4http://lpsolve.sourceforge.net/5.5/1141Redundancy ConstraintsWe also want to prevent redundancy across differ-ent aspects.
If sentence-similarity sim(sjl, sj?l?)
be-tween sentence sjl and sj?l?
is above 0.5, then wedrop the pair and choose one sentence ranked higherfrom the pair otherwise.
This constraint is formulat-ed as follows:(Sjl + Sj?l?)
?
sim(sjl, sj?l?)
?
0.5?j, j?
?
{1 .
.
.K}?l ?
{1 .
.
.
Rj}?l?
?
{1 .
.
.
Rj?
}Length ConstraintsWe add this constraint to ensure that the length ofthe final summary is limited to L words.K?j=1Rj?l=1lenjl ?
Sjl ?
Lwhere lenjl is the length of Sjl.6 EvaluationIn order to systematically evaluate our method, wewant to check (1) whether the whole system is effec-tive, which means to quantitatively evaluate summa-ry quality, and (2) whether individual componentslike clustering and compression algorithms are use-ful.6.1 DataWe use TAC2010 Summarization task data set forthe summary content evaluation.
This data set pro-vides 46 events.
Each event falls into a predefinedevent topic.
Each specific event includes an even-t statement and 20 relevant newswire articles whichhave been divided into 2 sets: Document Set A andDocument Set B.
Each document set has 10 docu-ments, and all the documents in Set A chronologi-cally precede the documents in Set B.
We just usedocument Set A for our task.
Assessors wrote mod-el summaries for each event, so we can compareour automatic generated summaries with the modelsummaries.
We combine topic related data sets to-gether, then these data sets simultaneously annotatedby our Event-aspect model.
After labeling process,we run sentence ranking, compression and selectionmodule to get final aspect-oriented summarizations.6.2 Quality of summaryWe use the ROUGE (Lin and Hovy, 2003) metric formeasuring the summarization system performance.Ideally, a summarization criterion should be morerecall oriented.
So the average recall of ROUGE-1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 andROUGE-L were computed by running ROUGE-1.5.5 with stemming but no removal of stop word-s. We compare our method with the following fourbaseline methods.Baseline 1In this baseline, we try to compare different sen-tence clustering algorithms in the multi-documentsummarization scenario.
First, we use CLUTO 5 todo K-means clustering.
Then we try entity-aspectmodel proposed by Li et al (2010) to do sentenceclustering.
Entity-aspect model is similar with ?HI-ERSUM?
content model proposed by Haghighi andVanderwende (2009).
We use the same ranking,compression, and selection components to generateaspect-oriented summaries for comparison.Baseline 2In this baseline, we compare our method withtraditional ranking and selection summary genera-tion framework (Erkan and Radev, 2004; Nenkovaand Vanderwende, 2005) to show that our sentenceclustering component is necessary in aspect-orientedsummarization system.
Also we want check whethersentence ranking combined with greedy based sen-tence selection can prevent redundancy effective-ly.
We follow LexRank based sentence rankingcombined with greedy sentence selection methods.We implement two greedy algorithms (Zhang et al,2008; Paul et al, 2010).
One is to select the topranked sentence simultaneously by removing 10 re-dundant neighbor sentences from the sentence sim-ilarity graph if the summary length is less then 100words.
This is repeated until the graph cannot bepartitioned.
The similarity graph building thresholdis 0.3, damping factor is 0.2 and error tolerance forPower Method in LexRank is 0.1.
The other is to se-lect top ranked sentences as long as the redundancyscore (similarity) between a candidate sentence and5http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview1142current summary is under 0.5.
This is repeated untilthe summary reaches a 100 word length limit.Baseline 3In this baseline, we compare our ILP based sen-tence selection with KL-divergence based sentenceselection.
The KL-divergence formula we use is be-low,KL(PS ||QD) =?wP (w) log P (w)Q(w)where P (S) is the empirical unigram distribution ofthe candidate summary S, and Q(D) is the unigramdistribution of document collection D. We only re-placed our selection method with the KL-divergenceselection method.
Other parts are the same.
Afterranking sentences for each aspect, we add the sen-tence with the highest ranking score from each as-pect sentence cluster as long as the KL-divergencebetween candidate and current summary does notdecrease.
This is repeated until the summary reach-es a 100 word length limit.
To our knowledge, thisis the first work to directly compare Integer Lin-ear Programming based sentence selection with KL-divergence based sentence selection in summariza-tion generation framework.Baseline 4In this baseline, we directly compare our methodwith ?HIERSUM?
proposed by (Haghighi and Van-derwende, 2009).
As in Baseline 1, we use entity-aspect model to approximate ?HIERSUM?
mod-el.
We replace unigram distribution of P (w) inKL-divergence with learned distribution estimatedby ?HIERSUM?
model.
The KL-divergence basedgreedy sentence selection algorithm is similar toBaseline 3.For fair comparison, Baselines 1, 2, 3 and 4 usethe same sentence compression algorithm and havethe summary length no more then 100 words.
InTable 3, we show the average ROUGE recall of 46summaries generated by our method and four base-line methods.
We can see that our method givesbetter Rouge recall measures then the four baselinemethods.
For BL-1, we can see that LDA-based sen-tence clustering is better then k-means.
For BL-2,we can see that traditional ranking plus greedy selec-tion summary generation framework is not suitablefor the aspect-oriented summarization task.
Morespecifically, greedy-based sentence selection can notprevent redundancy effectively.
BL-3 evaluation re-sults showed that ILP-based sentence selection isbetter then KL-divergence selection in terms of pre-venting redundancy across different aspects.
Themeasurement performance between BL-3 and BL-4 is close.
They use the same KL-divergence basedsentence selection, but topic model they use are d-ifferent, and also BL-3 has a sentence ranking pro-cess.
The Rouge recall of our method is better thanBL-4.
It is expected because our event-aspect mod-el can better find the aspects and also prove thatour LexRank based sentence ranking combined withILP-based sentence selection can prevent redundan-cy.Due to TAC2010 summarization community justcompute ROUGE-2 and ROUGE-SU4 metrics forparticipants, our ROUGE-2 metric ranked 11 outof 23, ROUGE-SU4 metric ranked 12 out of 23.They use MEAD6 as their baseline approach.
TheROUGE-2 score of our approach achieve 0.06508higher than MEAD?s 0.05929.
The ROUGE-SU4 s-core of our approach achieve 0.10146 higher thanMEAD?s 0.09112.
Many systems that get high-er performances leverage domain knowledge baseslike Wikipedia or training data, but we didn?t.
Theadvantage of our method is that we generate sum-maries with totally unsupervised framework and thisapproach is domain adaptive.6.3 Quality of aspect-oriented sentence clustersTo judge the quality of the aspect-oriented sentenceclusters, we ask the human judges to group theground truth sentences based on the aspect related-ness in each event topic.
We then compute the pu-rity of the automatically generated clusters againstthe human judged clusters.
The results are shownin Table 4.
In our experiments, we set the numberof general aspect clusters AG is 5 and specific as-pect clusters AS is 3.
We can see from Table 4 thatour generated aspect clusters can achieve reasonablygood performance.6http://www.summarization.com/mead/1143Rouge Average RecallMethod ROUGE-1 ROUGE-2 ROUGE-SU4 ROUGE-W-1.2 ROUGE-LBL-1 k-means 0.21895 0.03689 0.06644 0.06683 0.19208entity-aspect 0.26082 0.05082 0.08286 0.08055 0.22976BL-2 greedy 1 0.27802 0.04872 0.08302 0.08488 0.24426greedy 2 0.27898 0.04723 0.08275 0.08500 0.24430BL-3 KL-Div 0.29286 0.05369 0.09117 0.08827 0.25100BL-4 HIERSUM 0.28736 0.05502 0.08932 0.08923 0.25285Without compression 0.30563 0.05983 0.09513 0.09468 0.25487Our Method 0.32641 0.06508 0.10146 0.09998 0.28610Table 3: ROUGE evaluation results on TAC2010 Summarization data setsCategory A PurityAccidents and Natural Disasters 7 0.613Attacks 8 0.658Health and Safety 5 0.724Endangered Resources 4 0.716Investigations and Trials 6 0.669Table 4: The true numbers of aspects as judged by thehuman annotator (A), and the purity of the clusters.Category Average ScoreAccidents and Natural Disasters 2.4Attacks 2.3Health and Safety 2.6Endangered Resources 2.5Investigations and Trials 2.4Table 5: The average score of each event topic.6.4 Quality of sentence compressionTo judge the quality of the dependency tree basedsentence compression algorithm, we ask the humanjudges to choose 20 sentences from each event top-ic then score them.
The judges follow 3-point scaleto score each compressed sentence: 1 means poor,2 means barely acceptable, and 3 means good.
Wethen compute the average scores.
The results areshown in Table 5.
To evaluate the effectiveness ofsentence compression component, we conduct thesystem without sentence compression component,then compare it with our system.
In Table 3, wecan see that sentence compression can improve thesystem performance.7 Related WorkOur event-aspect model is related to a number ofprevious extensions of LDA models.
Chemudugun-ta et al (2007) proposed to introduce a backgroundtopic and document-specific topics.
Our backgroundand document language models are similar to theirs.However, they still treat documents as bags of wordsrather then sets of sentences as in our models.
Titovand McDonald (2008) exploited the idea that a shortparagraph within a document is likely to be aboutthe same aspect.
The way we separate words in-to stop words, background words, document word-s and aspect words bears similarity to that usedin (Daume?
III and Marcu, 2006; Haghighi and Van-derwende, 2009).
Paul and Girju (2010) proposed atopic-aspect model for simultaneously finding topic-s and aspects.
The most related extension is entity-aspect model proposed by Li et al (2010).
The maindifference between event-aspect model and entity-aspect model is our model further consider aspectgranularity and add a layer to model topic-relatedevents.Filippova and Strube (2008) proposed a depen-dency tree based sentence compression algorithm.Their approach need a large corpus to build languagemodel for compression, whereas we prune depen-dency tree using grammatical rules.Paul et al (2010) proposed to modify LexRankalgorithm using their topic-aspect model.
But theirtask is to summarize contrastive viewpoints in opin-ionated text.
Furthermore, they use a simple greedyapproach for constructing summary.McDonald (2007) proposed to use Integer LinearProgramming framework in multi-document sum-1144marization.
And Sauper and Barzilay (2009) use in-teger linear programming framework to automatical-ly generate Wikipedia articles.
There is a fundamen-tal difference between their method and ours.
Theyused trained perceptron algorithm for ranking ex-cerpts, whereas we give an extended LexRank withinteger linear programming to optimize sentence se-lection for our aspect-oriented multi-document sum-marization.8 Conclusions and Future WorkIn this paper, we study the task of automaticallygenerating aspect-oriented summary from multipledocuments.
We proposed an event-aspect modelthat can automatically cluster sentences into aspect-s. We then use an extension of the LexRank algo-rithm to rank sentences.
We took advantage of theoutput generated by the event-aspect model to mod-ify jumping probabilities so as to favor visiting rep-resentative sentence.
We also proposed dependen-cy tree compression algorithm to prune sentence forimproving linguistic quality of the summaries.
Fi-nally we use Integer Linear Programming Frame-work to select aspect relevant sentences.
We con-ducted quantitative evaluation using standard testdata sets.
We found that our method gave overal-l better ROUGE scores than four baseline methods,and the new sentence clustering and compression al-gorithm are robust.There are a number of directions we plan to pur-sue in the future in order to improve our method.First, we can possibly apply more linguistic knowl-edge to improve the quality of sentence compres-sion.
Currently the sentence compression algorith-m may generate meaningless subtrees.
It is rela-tively hard to decide which clause is redundant interms of summarization.
Second, we may exploremore domain knowledge to improve the quality ofaspect-oriented summaries.
For example, we knowthat the ?who-affected?
aspect is related to person,and ?when, where?
are related to Time and Location.we can import Name Entity Recognition to anno-tate these phrases and then help locate relevant sen-tences.
Third, we want to extend our event-aspectmodel to simultaneously find topics and aspects.AcknowledgmentsThis work was supported by the National Nat-ural Science Foundation of China (NSFC No.60773088), the National High-tech R&D Programof China (863 Program No.
2009AA04Z106), andthe Key Program of Basic Research of ShanghaiMunicipal S&T Commission (No.
08JC1411700).ReferencesChaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2007.
Modeling general and specific aspectsof documents with a probabilistic topic model.
In Ad-vances in Neural Information Processing Systems 19,pages 241?248.Hal.
Daume?
III and Daniel.
Marcu.
2006.
Bayesianquery-focused summarization.
In Proceedings of the21st International Conference on Computational Lin-guistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, pages 305?312.Association for Computational Linguistics.Gu?nes.
Erkan and Dragomir Radev.
2004.
LexRank:Graph-based lexical centrality as salience in text sum-marization.
Journal of Artificial Intelligence Re-search, 22(1):457?479.K.
Filippova and M. Strube.
2008.
Dependencytree based sentence compression.
In Proceedings ofthe Fifth International Natural Language GenerationConference, pages 25?32.
Association for Computa-tional Linguistics.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of the Work-shop on Integer Linear Programming for Natural Lan-gauge Processing, pages 10?18.Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet,Y.
Liu, and S. Xie.
2010.
The icsi/utd summarizationsystem at tac 2009.
In Proceedings of the Second TextAnalysis Conference, Gaithersburg, Maryland, USA:National Institute of Standards and Technology.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the National A-cademy of Sciences of the United States of America,101(Suppl.
1):5228?5235.A.
Haghighi and L. Vanderwende.
2009.
Exploring con-tent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics onZZZ, pages 362?370.
Association for ComputationalLinguistics.1145Peng Li, Jing Jiang, and Yinglin Wang.
2010.
Gen-erating templates of entity summaries with an entity-aspect model and pattern mining.
In Proceedings ofthe Joint Conference of the 48th Annual Meeting of theACL.
Association for Computational Linguistics.C.Y.
Lin and E. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statistics.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 71?78.
Association for Computation-al Linguistics.RyanMcDonald.
2007.
A study of global inference algo-rithms in multi-document summarization.
Advances inInformation Retrieval, pages 557?564.A.
Nenkova and L. Vanderwende.
2005.
The impactof frequency on summarization.
Microsoft Research,Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.Michael J. Paul and Roxana Girju.
2010.
A two-dimensional topic-aspect model for discovering multi-faceted topics.
In In AAAI-2010: Twenty-Fourth Con-ference on Artificial Intelligence.Michael J. Paul, ChengXiang Zhai, and Roxana Girju.2010.
Summarizing contrastive viewpoints in opin-ionated text.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?10, pages 66?76, Morristown, NJ, USA.Association for Computational Linguistics.Christina Sauper and Regina Barzilay.
2009.
Automati-cally generating wikipedia articles: A structure-awareapproach.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP, pages 208?216, Suntec, Singa-pore, August.
Association for Computational Linguis-tics.Ivan Titov and Ryan McDonald.
2008.
Modeling onlinereviews with multi-grain topic models.
In Proceedingof the 17th International Conference on World WideWeb, pages 111?120.D.
Zajic, B.J.
Dorr, J. Lin, and R. Schwartz.
2007.
Multi-candidate reduction: Sentence compression as a toolfor document summarization tasks.
Information Pro-cessing & Management, 43(6):1549?1570.Jin.
Zhang, Xueqi.
Cheng, and Hongbo.
Xu.
2008.
GSP-Summary: a graph-based sub-topic partition algorith-m for summarization.
In Proceedings of the 4th Asi-a information retrieval conference on Information re-trieval technology, pages 321?334.
Springer-Verlag.1146
