Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 38?45,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsReliability and Type of Consumer Health Documents on the World Wide Web: an Annotation Study    Melanie J. Martin California State University, Stanislaus One University Circle Turlock, CA 95382 mmartin@cs.csustan.edu    Abstract In this paper we present a detailed scheme for annotating medical web pages de-signed for health care consumers.
The annotation is along two axes: first, by re-liability or the extent to which the medical information on the page can be trusted, second, by the type of page (patient leaf-let, commercial, link, medical article, tes-timonial, or support).
We analyze inter-rater agreement among three judges for each category.
Inter-rater agreement was moderate (0.77 accuracy, 0.62 F-measure, 0.49 Kappa) on the reliability axis and good (0.81 accuracy, 0.72 F-measure, 0.73 Kappa) along the type axis.1 Introduction With the explosive growth of the World Wide Web has come, not just an explosion of information, but also the explosion of false, misleading and unsup-ported information.
At the same time, the web is increasingly used for tasks where information qual-ity and reliability are vital, from legal and medical research by both professionals and lay people, to fact checking by journalists and research by gov-ernment policy makers.
In particular, there has been a proliferation of web pages in the medical domain for health care consumers.
At the first sign of illness or injury more and more people go to the web before con-sulting medical professionals.
The quality and reli-ability of the information on consumer medical web pages has been of concern for some time tomedical professionals and policy makers.
(For ex-ample see Eysenbach et al, 2002, Impicciatore et al, 1997.)
Our goal is to create a system that can automati-cally measure the reliability of web pages in the medical domain (Martin, 2004).
More specifically, given a web page resulting from a user query on a medical topic, we would like to automatically pro-vide an estimate of the extent to which the infor-mation on the page can be trusted.
In order to make use of supervised natural language processing and machine learning algorithms to create such a sys-tem, and to ultimately evaluate the performance of the system, it is necessary to have human anno-tated data.
It is important to note the varied uses of the term ?reliability?
in the computer and information sci-ences.
In the current context we use it to refer to an intrinsic property of a web page: essentially the trustworthiness of the information it contains.
This sense of reliability is distinct from its meaning in measurement theory as an indicator of repeatabil-ity.
It also excludes measures such as credibility that are based on user beliefs or understanding.
In this paper we report results of an annotation study of medical web pages designed for health care consumers.
Three humans annotated a corpus of web pages along two axes.
The first axis is the reliability of the information contained in the page.
The second axis is the type, or kind, of page.
Inter-coder agreement was moderate (0.77 accuracy, 0.62 F-measure, 0.49 Kappa) on the reliability axis and good (0.81 accuracy, 0.72 F-measure, 0.73 Kappa) along the type axis.
In our materials and methods section we discuss the data, definitions, annotation study and the re-sults.
We follow with a discussion section and a conclusion.382 Materials and Methods In this section we will discuss the data and definitions for the annotation task.
We also describe the annotation study and the testing and analysis.
2.1 Data The data to be annotated consists of two corpora of web pages created by the author: IBS70 and MMED100.
The MMED100 corpus is a subset of a larger corpus (MMED1000).
Both corpora are de-scribed below.
2.1.1 IBS70 Corpus The IBS70 corpus was created as an exploratory corpus for use in system development.
It was originally the top 50 Google hits for "irritable bowel syndrome" downloaded automatically though the Google API on July 1, 2004.
The query was chosen to provide a range of quality and types of pages which one would expect to see more gen-erally in the medical domain on the web: patient information from both traditional and alternative sources, support groups, medical articles, commer-cial pages from drug companies and quacks.
Dur-ing system development we determined that it would be useful to have additional pages at both ends of the reliability spectrum, possibly to use as seeds for clustering.
On September 15, 2004, twenty documents were added to the corpus to create the IBS70.
Ten highly reliable documents were added based on web searches to find documents judged as meeting the standards of Evidence Based Medicine.
Ten docu-ments judged unreliable were added by taking the first ten relevant ?Sponsored Links?
resulting from a Google search on ?irritable bowel syndrome?.
There are two important things to note about this process: first, the high quality pages added were disproportionately from the U.K.; second, the low quality pages tend toward the crassly commercial and are more extreme than one would likely find in this proportion of the top 100 (or even 200) of the results of a Google query for a medical condition.
2.1.2 MMED100 Corpus The MMED1000 corpus was created on November 5th and 8th, 2004 by automatically downloadingfrom Google the top 100 search results for each of the following 10 queries:  ?
Adrenoleukodystrophy ?
Alzheimer's ?
Endometriosis ?
Fibromyalgia ?
Obesity ?
Pancreatic cancer ?
Colloidal Silver ?
Irritable Bowel Syndrome ?
Late Lyme Disease ?
Lower Back Pain  The queries were chosen to provide a broad range of what might be typical queries for health con-sumers on the web and the types of pages that would result from these queries.
Colloidal Silver was chosen in the hopes of pro-viding a sufficient number of pages of questionable reliability.
Adrenoleukodystrophy, Pancreatic Can-cer, Alzheimer?s and Obesity were chosen because there is general agreement in the medical commu-nity that these are diseases or health issues, and on diagnostic techniques.
They also cover a spectrum of occurrence rates, with Adrenoleukodystrophy being relatively rare and Obesity being relatively common.
The other five queries were chosen be-cause there is less agreement in both the medical community and the general population about the existence, frequency, severity and treatment of these conditions.
In particular, Fibromyalgia and IBS can be exclusionary diagnoses without clear and successful treatment options, which can open the door to web pages with a range of questionable treatments.
For annotation purposes a subset of this corpus, MMED100, with 100 pages, was created by ran-domly selecting ten documents from each of the ten queries.
At this time neither corpus is publicly available.
However they can be provided on request and it is anticipated that they will be made publicly avail-able once a viable standard is established for the annotations.
2.2 Definitions The primary classification task is to classify pages based on their reliability (quality or trustworthiness of the information they contain).
The secondary39classification task is to classify pages based on their type (e.g.
commercial, patient leaflet, link).
The classification by type emerged from the hy-pothesis that different types of pages may need to be treated differently to classify them based on their reliability.
For example, if the primary pur-pose of a page is to provide links to information, determining the reliability of the page may require determining the reliability of the pages to which it links.
However, in the current study, annotators are provided only the given web page and not allowed to follow links, so their reliability determination was made based on the apparent balance and ob-jectivity of the links on the page.
For both tasks, only one tag was allowed, so an-notators were instructed to consider the main pur-pose or intent of the page.
2.2.1 Reliability Reliability of web pages is annotated based on a five level scale.
Probably Reliable (PrR) The information on these pages appears to be com-plete and correct, meeting the standards of Evi-dence-Based Medicine where appropriate.
Information is presented in a balanced and objec-tive manner, with the full range of options dis-cussed (where appropriate).
The page and author appear reputable, with no obvious conflicts of in-terest.
The appropriate disclaimers, policies, and contact information are present.
Where appropri-ate, sources are cited.
An example of a page in this category would be a patient leaflet from a reputa-ble source that adheres to the standards of Evi-dence-Based Medicine.
Possibly Reliable (PoR) The information on the page is generally good and without obvious false or outdated statements, but may not be sufficiently complete and balanced or may not conform to evidence-based standards.
An example of a page in this category would be a pa-tient leaflet that contains only a brief description of diagnostic procedures or suggests a treatment op-tion that is generally accepted, but not supported by evidence.Unable to determine (N) For these pages it is difficult or impossible to de-termine the reliability, generally because there is not enough information.
For example, the page may be blank, only contain login information, or be the front page of a medical journal.
Possibly Unreliable (PoU) These pages may contain some reliable informa-tion, but either have some that is outdated, false or misleading, or the information is sufficiently un-balanced so as to be somewhat misleading.
An ex-ample of a page that might fall into this category is a practitioner commercial pages, which has valid information about an illness, but only discuss the preferred treatment offered by the practitioner.
Probably Unreliable (PrU) These pages contain false or misleading informa-tion, or present an unbalanced or biased viewpoint on the topic.
Examples of pages in this category would include: testimonials (unsupported view-points or opinions of a single individual) or pages that are clearly promoting and selling a single treatment option.
2.2.2 Type of Page We found six types of pages that frequently come up in search results for queries in the medical do-main: Commercial, Patient Leaflet, Link, Medical Articles, Support, and Testimonials.
There are also pages which are not relevant, or do not contain sufficient information to make a determination.
Below we discuss each of these types.
When a page seems to overlap categories the annotation is based on the primary purpose of the page.
Commercial (C) The primary purpose of these pages is to sell some-thing, for example, pages about an ailment spon-sored by a drug (also more general treatment or equipment) company, which sells a drug to treat it.
Given the desire to sell, these pages might not pre-sent complete or balanced information (making them less likely to be reliable).
Practitioner pages with no real (substantial) information, which are designed to get people to make an appointment, as opposed to patient leaflets (designed to supplement information that patients receive in the office or clinic), might also fall into this category40Link  (L) The primary purpose of these pages is to provide links to other pages or sites (external), which will provide information about a certain illness or medical condition.
These links may or may not be annotated, and the degree of annotation may vary considerably.
Since the reliability of these pages depends on the reliability of the pages they link to (possibly also on the text in the annotations), with-out following the links a reliability estimate can be based on the range and apparent objectivity of the links.
Patient Leaflet, Brochure, Fact Sheet or FAQ (P) The primary purpose of these pages is to provide information to patients about a specific illness or medical condition.
Generally, these pages will be produced by a clinic, medical center, physician, or government agency, etc.
The primary purpose is to provide information.
This class needs to be distin-guished from medical articles, especially in ency-clopedias or the Merck Manual, etc.
These pages will tend to have headings like: symptoms, diagno-sis, treatment, etc.
These headings can take the form of links to specific parts of the same page or to other pages on the same site (internal).
The reli-ability of these pages is based on their content and determined by factors including Evidence-Based Medicine, completeness, and the presence of incor-rect or outdated information.
Medical Article (practitioner or consumer) (MA) The primary purpose of these pages is to discuss an aspect of a specific illness or medical condition, or a specific illness or medical condition.
These can be divided into two main categories: articles aimed at consumers and articles aimed at health practitio-ners.
Articles aimed at health practitioners, particu-larly doctors, may be scientific research articles.
The reliability of these pages is based on their con-tent and determined by factors including Evidence Based Medicine, completeness, and the presence of incorrect or outdated information.
Note: Medline search results may be considered a links page to medical articles.
Articles aimed at consumers may come from a variety of sources including mainstream and alter-native media sources.
Reliability is determinedbased on the content as with articles for practitio-ners.
Testimonial (T) The primary purpose of these pages is to provide testimonial(s) of individuals about their experience with an illness, condition, or treatment.
While in-dividuals may be considered reliable when discuss-ing their own personal experiences, these pages tend to be unreliable, because they are generally not objective or balanced.
There is a tendency for readers to generalize from very specific informa-tion or experiences provided by the testimonial, which can be misleading.
Support (S) The primary purpose of these pages is to provide support of sufferers (or their loved ones or care-givers) of a particular illness or condition.
The pages may contain information, similar to that found in a patient leaflet; links to other sites, simi-lar to a links page; and testimonials.
In addition they may contain facilities such as chat rooms, newsletters, and email lists.
Activities may include lobbying for funding for research, generally put up by individuals or non-profit organizations.
For re-liability, one may need to look at the agenda of the authors or group.
It may be in their interest (politi-cally) to overstate the problem or make things out to be worse then they are to secure increased fund-ing or sympathy for their cause.
Not Relevant (N) These pages are blank or not relevant and include: login pages, conditions of use pages, and medical journal front pages.
2.3 Annotation Study In order to get started with system development, a single annotator, M, who was involved with devel-opment of both the classifications and the system, tagged the IBS70 and MMED100.
Then in Spring 2008 two senior undergraduate science majors (chemistry and biology), L and E, were hired for the annotation study.
The annotation study con-sisted to two primary phases: training and testing.
Each phase is described below.412.3.1 Training Phase The two student annotators, L and E, received cop-ies of the draft annotation instructions.
They each met individually with M to discuss the instructions and any questions they had.
For each of three training runs, ten randomly chosen web pages from the IBS70 corpus were posted on a private web site.
The students anno-tated the pages for reliability and type and then met individually to discuss their annotations with M. As questions and issues arose, the instructions were amended to reflect clarifications.
For exam-ple, L needed additional instructions on the distinc-tion between Link and Patient Leaflet pages; a separate category for FAQs was collapsed into the Patient Leaflet category.
2.3.2 Testing Phase Once the student annotators seemed to be achiev-ing reasonable levels of agreement (Cohen?s Kappa above 0.4) on each task, there was a three-part testing phase.
The remaining 40 pages in the IBS70 corpus were randomly divided into two test corpora and finally the MMED100 corpus was an-notated.
During the testing phase, one of the students, L, seemed to annotate less carefully.
(Possibly be-cause the timing coincided with graduation and summer vacation.)
For example, on the MMED100 corpus L tagged 30% as N (unable to determine the reliability, compared to 12% for E and 10% for M. L was asked to go back and reconsider the web pages tagged as N. We report results with L?s re-considered tags here for completeness, but further discussion will focus on agreement between M and E. 2.4 Testing and Analysis We report inter-rater agreement using accuracy, Cohen?s Kappa statistic (Cohen, 1960) for chance corrected agreement and F-Measure (Hripcsak and Rothschild, 2005).
We consider each annotation axis separately.
2.4.1 Page Reliability We can estimate a baseline distribution of the cate-gories R (reliable), N (unable to determine), and U (unreliable) based on an average of the tags acrossall training and test sets to be approximately: 68% R; 13% N; 19% U.
Table 1 shows the results for the Accuracy (per-cent agreement) and Kappa statistic on the five reliability classes across all the corpora.
It became immediately clear the annotators were not able to make the more fine-grained distinctions between ?probably?
and ?possibly?
for either the reliable or unreliable classes, given the current instructions and timeline.
The classes were then collapsed to three: R (reliable), N (unable to determine) and U (unreliable) and the results are shown in Table 2.
Accuracy/ Kappa 5 Classes Reliability   Set\Raters M-E M-L E-L IBS train 0.47/0.30 0.33/0.12 0.40/0.19 IBS test 0.33/0.11 0.40/0.25 0.43/0.28 MMed100  0.51/0.32 0.35/0.12 0.38/0.14 Table 1.
Inter-rater agreement for 5-class reliability.
Accuracy/ Kappa  3 Classes Reliability  Set\Raters M-E M-L E-L IBS train 0.70/0.44 0.60/0.25 0.67/0.33 IBS test 0.70/0.43 0.65/0.42 0.75/0.59 MMed100 0.77/0.49 0.66/0.30 0.62/0.22 Table 2.
Inter-rater agreement for 3-classs reliability.
The results in Table 2 for M-E show improved agreement after training and consistent moderate agreement on the test corpora based on the Kappa statistic.
Accuracy (percent agreement) for M-E is 70% for both IBS testing and training and 77% for the MMED100.
Further analysis of L?s reliability tags showed a bias toward the  ?U?
tag.
For example, in the MMED100 corpus, L tagged 28% as U, compared to 19% and 17% for M and E, respectively.
Hripcsak and Rothschild (2005) suggest use of the F-measure (harmonic average of precision ?
equivalent to positive predictive value - and recall ?
equivalent to sensitivity - commonly used in In-formation Retrieval) to calculate inter-rater agree-ment in the absence of a gold standard.
In Table 3 we report the average F-measure between each pair of raters and the F-measure by class.
A higher F-measure indicates better agreement, so these re-sults show that the ?Can?t Tell?
class is the most42difficult to agree on, followed by the ?Unreliable?
class.
MMED100 F-Measure  3 Classes Reliability  Class\Raters M-E M-L E-L Reliable 0.87 0.78 0.76 Can?t Tell 0.45 0.22 0.30 Unreliable 0.55 0.46 0.36 Average 0.62 0.49 0.47 Table 3.
F-measure by class for 3-classs reliability.
In order to look for patterns of agreement be-tween the raters we looked at agreement by query in the MMED100 corpus.
In Table 4 we show the agreement for M and E by query.
Although it ap-pears that some queries were easier to annotate than others, since there are only 10 pages per query, the sample may be too small to draw defi-nite conclusions.
Query Accuracy Kappa Endometriosis 1 1 Pancreatic Cancer 1 1 Late Lyme 1 1 Adrenoleukodystrophy 0.8 0.412 Obesity 0.8 0.655 Alzheimer?s 0.7 -0.154 Fibromyalgia 0.7 0.444 Lower Back Pain 0.7 -0.154 Colloidal Silver 0.6 0.13 Irritable Bowel Syn-drome 0.4 -0.053 Table 4.
Inter-rater reliability agreement for M-E by query.
Possible ways to improve these results are pre-sented in the ?Discussion?
section.
2.4.2 Page Type The dominant page types are P (patient leaflets), L (link), C (commercial) and MA (medical article).
The baseline distribution based on averages across the training and test sets is approximately: 39% P; 15% L; 18% C; and 13% MA.
The other three classes S (support), T (testimonial), and N (unable to determine) making up only 15% of the pages in the corpus.Table 5 shows the results for Accuracy and the Kappa statistic on the seven type classes across all the corpora.
Collapsing categories for the type an-notation task did not appreciably increase Kappa scores (M-E Kappa was 0.742 on the MMED100 corpus when the P and MA classes were col-lapsed), so it seems preferable to keep the original classes.
Accuracy/ Kappa   Type   Set\Raters M-E M-L E-L IBS train 0.57/0.42 0.83/0.78 0.47/0.28 IBS test 0.73/0.64 0.65/0.55 0.73/0.64 MMed100  0.81/0.73 0.48/0.29 0.50/0.31 Table 5.
Inter-rater agreement for type annotation.
Again we see with annotators M and E, the im-proved agreement from training to testing, as dis-tinctions between classes were clarified (for example, between Link and Patient Leaflets, and between Patient Leaflets and Medical Articles).
We also computed F-measure by type for the MMED100 corpus, as shown in Table 6.
Of the three most common types of pages (Patient Leaflet, Link, Commercial), the Link type was the most difficult for M-E to agree on.
MMED100 F-Measure   Type  Class\Raters M-E M-L E-L P 0.893 0.593 0.625 L 0.625 0.480 0.435 C 0.727 0.323 0.414 S 0.769 0.222 0.250 T 0.500 0.000 0.800 MA 0.667 0.593 0.455 N 0.857 0.143 0.118 Average 0.720 0.336 0.442 Table 6.
F-measure by class for page type.
We further analyzed the page type annotations by query for raters M and E (Table 7).
We found a negative correlation between the variance of the types in a query to the Kappa statistic of agreement for the query  (r2 = -0.62).43Query Accuracy Kappa Endometriosis 0.9 0.851 Fibromyalgia 0.9 0.846 Alzheimer?s 0.8 0.75 Irritable Bowel Syn-drome 0.8 0.73 Obesity 0.8 0.697 Pancreatic Cancer 0.8 0.63 Colloidal Silver 0.8 0.63 Adrenoleukodystrophy 0.8 0.512 Lower Back Pain 0.8 0.512 Late Lyme 0.7 0.483 Table 7.
Inter-rater type agreement for M-E by query.
3 Discussion Librarians, scholars, and information scientists have done significant work on the quality (reliabil-ity) of print, and more recently, web information (for example, see Cook 2001, Alexander and Tate 1999).
It is important to distinguish quality (reli-ability) from credibility (e.g.
Danielson 2005), which is based on the users view of the informa-tion.
Here we are interested in the quality of the information itself.
In a relatively early study, Impicciatore et al (1997) sampled web documents relating to fever in children and found the quality of the information provided to be very low.
In 2002, Eysenbach et al conducted a review of studies assessing the quality of consumer health information on the web.
Of the 79 studies meeting their inclusion criteria (essen-tially appropriate scope and quantitative analysis), they found that 70% of the studies concluded that reliability of medical information on the Web is a problem.
To address the question of how to determine the quality of medical information on the web, Fallis and Frick?
(2002) empirically tested several pro-posed indicators and found that the standard indi-cators of quality for print media could not be directly translated to consumer medical informa-tion on the Web.
Price and Hersh (1999) developed a semi-automated system to filter out low quality consumer medical web pages based on approxi-mately 30 criteria.
Annotation studies have been discussed and conducted in the computational linguistics com-munity for a variety of annotation tasks, including subjectivity (e.g.
Weibe et al 1999) and opinion (e.g.
Somasundaran et al 2008).
Artstein and Poe-sio (2008) surveyed inter-coder agreement in com-putational linguistics, including Cohen?s Kappa.
To ensure a ?gold standard?
for training ma-chine learning algorithms to do automatic classifi-cation a number of approaches could be pursued: the production of bias-corrected tags as described by Weibe et al (1999); a new study with ?expert?
annotators ?
having a stronger medical background ?
and additional training; ask annotators to use ex-isting web tools (e.g.
American Accreditation HealthCare Commission) to assess the page qual-ity; systematically assess whether the noise intro-duced by moderate agreement levels will create problems for machine learning with this data (Beigman Klebanov and Beigman 2009).
The agreement on the type annotation task could still be improved, possibly by additional clarifica-tion to the definitions.
However, it is still to be de-termined if noise levels are low enough and sufficiently random to be used successfully in su-pervised learning.
This task is easier than the reli-ability task and requires less expertise of the annotators.
4 Conclusion  There is a demonstrated need to provide tools to health care consumers to automatically filter web pages by the reliability, quality, or trustworthiness of the medical information the pages contain.
We have shown promising results in this study that appropriate classes of pages can be developed.
These classes can be used by human annotators to annotate web pages with reasonable to good agreement.
Thus we have laid a foundation for future anno-tation studies to create a gold standard data set of consumer medical web pages.
The corpora in this study are currently being used to create an auto-mated system to estimate the reliability of medical web pages.
Acknowledgments This work was supported in part by a CSU Stanis-laus Naraghi Faculty Research Enhancement Grant.
I am grateful to Elizabeth Jimenez and Luis Adalco for participating in the annotation study and to the anonymous reviews for their comments and suggestions.
I would also like to thank Roger Hartley my dissertation advisor and Peter Foltz for discussions during the formulation and develop-44ment of the system, and Tom Carter for helpful and insightful comments leading to the improvement of this paper.
References  Janet E. Alexander and Marsha Ann Tate.
1999.
Web Wisdom: How to Evaluate and Create Information Quality on the Web.
Lawrence Erlbaum and Associ-ates, New Jersey.
American Accreditation HealthCare Commission.
Health information on the internet:  A checklist to help you judge which websites to trust.
Retrieved February 28, 2010, from http://www.urac.org R. Artstein and M. Poesio.
2008.
Inter-coder agreement for computational linguistics.
Comput.
Linguist.
34, 4 (Dec. 2008), 555-596.
B. Beigman Klebanov, and E. Beigman.
2009.
From annotator agreement to noise models.
Comput.
Lin-guist.
35, 4 (Dec. 2009), 495-503.
J. Cohen.
1960.
A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20, pages 34-46.
Alison Cooke.
2001.
A Guide to Finding Quality Infor-mation on the Internet: Selection and Evaluation Strategies, Second Edition.
Library Association Pub-lishing, London.
D.R.
Danielson.
2005.
Web credibility.
C. Ghaoui (Ed.
), Encyclopedia of Human-Computer Interaction.
Her-shey, PA: Idea Group, 713-721.
Gunther Eysenbach, John Powell, Oliver Kuss, and Eun-Ryoung Sa.
2002.
Empirical Studies Assessing the Quality of Health Information for Consumers on the World Wide Web: A Systematic Review.
JAMA, May 22, 2002; 287(20): 2691 - 2700.
Don Fallis and Martin Frick?.
2002.
Indicators of Accu-racy of Consumer Health Information on the Internet.
Journal of the American Medical Informatics Asso-ciation, 9, 1, (2002): 73-79.
George Hripcsak and Adam S. Rothschild.
2005.
Agreement, the F-Measure, and Reliability in Infor-mation Retrieval.
J Am Med Inform Assoc.
2005 May?Jun; 12(3): 296?298.
Piero Impicciatore, Chiara Pandolfini, Nicola Casella, and Maurizio Bonati.
1997.
Reliability of Health In-formation for the Public on the World Wide Web: Systematic Survey of Advice on Managing Fever in Children at Home.
BMJ 1997; 314:1875 (28 June).
Melanie J. Martin.
2004.
Reliability and Verification of Natural Language Text on the World Wide Web.
Pa-per at ACM-SIGIR Doctoral Consortium, July 25, 2004, Sheffield, England.
Susan L. Price and William R. Hersh.
1999.
Filtering Web Pages for Quality Indicators: An Empirical Ap-proach to Finding High Quality Consumer Health In-formation.
American Medical Informatics Association 1999.
Swapna Somasundaran, Josef Ruppenhofer and Janyce Wiebe.
2008.
Discourse Level Opinion Relations: An Annotation Study.
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue Columbus, Ohio, June 19-20, 2008, pp.
129?137.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O'Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.
In Proceedings of the 37th Annual Meeting of the Asso-ciation For Computational Linguistics on Computa-tional Linguistics (College Park, Maryland, June 20 - 26, 1999).
Annual Meeting of the ACL.
Association for Computational Linguistics, Morristown, NJ, 246-253.45
