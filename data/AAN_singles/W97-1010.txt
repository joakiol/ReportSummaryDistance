Learning Stochast ic  Categorial  GrammarsMiles Osborne and Ted BriscoeComputer Laboratory, Cambridge UniversityCambridge CB2 3QG, UK{Miles.Osborne,Ted.Briscoe}@cl.cam.ac.ukAbstractStochastic ategorial grammars (SCGs) areintroduced as a more appropriate formal-ism for statistical language learners to es-timate than stochastic ontext free gram-mars.
As a vehicle for demonstrating SCGestimation, we show, in terms of crossingrates and in coverage, that when trainingmaterial is limited, SCG estimation usingthe Minimum Description Length Princi-ple is preferable to SCG estimation usingan indifferent prior.1 IntroductionStochastic context free grammars (SCFGs), whichare standard context free grammars extended witha probabilistic interpretation of the generation ofstrings, have been shown to model some sourceswith hidden branching processes more efficientlythan stochastic regular grammars (Lari and Young,1990).
Furthermore, SCFGs can be automaticallyestimated using the Inside-Outside algorithm, whichis guaranteed to produce a SCFG that is (locally)optimal (Baker, 1990).
Hence, SCFGs appear tobe suitable formalisms for the estimation of wide-covering rammars, capable of being used as part ofa system that assigns logical forms to sentences.Unfortunately, from a Natural Language Process-ing perspective, SCFGs are not appropriate gram-mars to learn.
Firstly, as Collins demonstrates(Collins, 1996), accurate parse selection, which isimportant for ambiguity resolution, requires lexicalstatistics.
SCFGs, as standardly used in the Inside-Outside algorithm, are in Chomsky Normal Form(CNF), which restricts rules to being at most bi-nary branching.
Such rules are not lexicalised, andhence, to lexicalise (CNF) CFGs requires adding acomplex statistical model that simulates the projec-tion of head items up the parse tree.
Given theembryonic status of grammatical statistical modelsand the difficulties of accurately estimating the pa-rameters of such a model, it seems more prudentto prefer whenever possible simpler statistical mod-els with fewer parameters, and treat lexicalisationas part of the grammatical formalism, and not aspart of the statistical framework (for example (Sch-abes, 1992)).
Secondly, (stochastic) CFGs are well-known as being linguistically inadequate formalismsfor problems uch as non-constituent coordination.Hence, a learner using a SCFG will not have an ap-propriate formalism with which to construct an ad-equate grammar.Stochastic ategorial grammars (SCGs), which areclassical categorial grammars extended with a prob-abilistic component, by contrast, have a grammat-ical component that is naturally lexicalised.
Fur-thermore, Combinatory Categorial Grammars havebeen shown to account elegantly for problematic ar-eas of syntax such as non-constituent co-ordination(Steedman, 1989), and so it seems likely that SCGs,when suitably extended, will be able to inherit thislinguistic adequacy.
We therefore believe that SCGsare more useful formalisms for statistical anguagelearning than SCFGs.
Future work will reinforcethe differences between SCFGs and SCGS, but inthis paper, we instead concentrate upon the estima-tion of SCGs.Stochastic grammars (of all varieties) are usuallyestimated using the Maximum Likelihood Principle,which assumes an indifferent prior probability dis-tribution.
When there is sufficient training mate-rial, Maximum Likelihood Estimation (MLE) pro-duces good results.
More usually however, withmany thousands of parameters to estimate, therewill be insufficient training material for MLE to pro-duce an optimal solution.
If, instead, an informativeprior is used in place of the indifferent prior, betterresults can be achieved.
In this paper we show howusing an informative prior probability distributionOsborne 8~ Briscoe 80 Stochastic Categorial GrammarsMiles Osborne and Ted Briscoe (1997) Learning Stochastic Categorial Grammars.
In T.M.
Ellison (ed.
)CoNLL97: Computational Natural Language Learning, ACL pp 80-87.
(~) 1997 Association for Computational Linguisticsleads to the estimation of a SCG that is more accu-rate than a SCG estimated using an indifferent prior.We use the Minimum Description Length Principle(MDL) as the basis of our informative prior.
To ourknowledge, we know of no other papers comparingMDL to MLE using naturally occurring data andlearning probabilistic grammars.
For example, Stol-cke's MDL-based learner was trained using artificialdata (Stolcke, 1984); Chen's similar learner mixessmoothing techniques with MDL, thereby obfuscat-ing the difference between MDL and MLE (Chen,1996).The structure of the rest of this paper is as follows.In section 2 we introduce SCGs.
We then in section3 present a problem facing most statistical learnersknown as over\]itting.
Section 4 gives an overview ofthe MDL principle, which we use to deal with over-fitting1; in section 5 we present our learner.
Fol-lowing this, in section 6 we give some experimentscomparing use of MDL, with a MLE-style learner.The paper ends with some brief comments.2 Grammar  fo rmal i sm ands ta t i s t i ca l  mode lsAn SCG is a classical categorial grammar (one usingjust functional application, see, for example, Wood(Wood, 1993)) such that each category is augmentedwith a probability, which is used to model the choicesmade when constructing a parse.
Categories are con-ditioned on the lexical item they are assigned to.More formally, a categorial lexicon G is the tuple(A, C, V, L), where:?
A is a non-empty set of atomic categories.?
C is a non-empty set of complex categories.?
V is a non-empty set of lexical items (words).?
L is the function L: Vv E V ~-~ 2 c. That is, Lassigns sets of categories to lexical items.Complex categories are defined as follows:?
Any member of A is a complex category.?
If a and b are complex categories, then so is a\b.
* If a and b are complex categories, then so is a/b.?
Nothing else is a complex category.1A fuller discussion of MDL and statistical languagelearning can be found in (Rissanen and Ristad, 1994, deMarcken, 1996).A categorial grammar consists of a categorial lexiconaugmented with the rule of left functional applica-tion: a b\a ~ b and the rule of right functionalapplication: b/a a ~-+ b.A probabilistic ategorial grammar is a categorialgrammar such that the sum of the probabilities ofall derivations is one.
Since in our variant of a cate-gorial grammar, where there are no variables in cat-egories, directional information is encoded into eachcategory, and we only use functional application,the actual derivation of any sentence mechanicallyfollows from the assignment of categories to lexicalitems, and so it follows that the choices availablewhen parsing with a categorial grammar arise fromthe particular assignment of categories to any givenlexical item.
Within a stochastic process, probabili-ties model these choices, so in a stochastic ategorialgrammar, we need to ensure that the probabilitiesof all categories assigned to a particular lexical itemsum to one.
That  is, for all categories c in lexicon Cassigned to lexical item w:E P(c \[ w) = 1 (1)cECWe estimate P(c \] w) as being:f(c) P(c I w) E ev (l)for some distinct Category c, occurring with fre-quency f(c), that can be assigned to lexical item w,and for all categories x, with frequency f(x),  thatcan also be assigned to w.For the derivation space to actually sum to one,all possible assignments of categories to lexical itemsmust be legal.
Clearly, only assignments of lexicalitems that combine to form a valid parse constitutelegal category assignments, and so there will be aprobability loss.
That is, the sum of all derivationswill be less than, or equal to one.
We can either scalethe probabilities o that the derivations do sum toone, or alternatively, we can assume that (illegal)assignments of categories are never seen, with therelative probabilities between the legal category as-signments being unaffected, and so give a zero prob-ability to the illegal category assignments 2Because categories are normalised with respect othe lexical item they are associated with, the re-sulting statistical model is lexicalised.
However, inthis paper, we learn lexica of part-of-speech tag se-quences, and not lexica for actual words.
That  is,the set of simple categories i taken as being a part-of-speech tag set and the set of words is Mso a set of2Thanks to Eirik Hektoen for pointing this point out.Osborne ~ Briscoe 81 Stochastic Categorial Grammarspart-of-speech tags.
This greatly reduces the num-ber of parameters to be acquired, than would be thecase if the lexicon contained a set of words, but in-curs the obvious cost of a loss of accuracy.
In futureexperiments, we plan to learn fully-lexicalised SCGs.Having now introduced SCGs, we now turn to theproblem of overfitting.3 Over f i t t ingBayesian inference forms the basis of many popu-lar language learning systems, examples of whichinclude the Baum-Welch algorithm for estimatinghidden Markov models (Baum, 1972) and the Inside-Outside algorithm for estimating CFGs (Baker,1990).
As is well known, Bayes' theorem takes thefollowing form:P(H I D) = P(H)P(D I H)P(D) (3)Here, the term P(H) is the prior probability, P(D IH) is the likelihood probability, and P(H I D) is theposterior probability.
The prior probability of H canbe interpreted as quantifying one's belief in H. Ifthe prior is accurate, hypotheses that are closer tothe target hypothesis will have a higher prior proba-bility assigned to them than hypotheses that are fur-ther away from the target hypothesis.
The likelihoodprobability describes how well the training materialcan be encoded in the hypothesis.
For example, onewould hope that the training corpus would receive ahigh likelihood probability, but a set of ungrammat-ical sentences would receive a low likelihood proba-bility.
Finally, the posterior probability can be con-sidered to be the combination of these two probabil-ity distributions: we prefer hypotheses that accordwith our prior belief in them (have a high prior prob-ability) and model the training material well (havea high likelihood probability).
When learning in aBayesian framework, we try to find some hypothesisthat maximises the posterior probability.
For exam-ple, we might try to find some maximally probablegrammar H given some corpus D.The usual setting is for the learner to assumean uninformative (indifferent) prior, yielding MLE.Usually, with sufficient data, MLE give good results.However, with insufficient data, which is the stan-dard case when there are many thousands of param-eters to estimate, MLE, unless checked, will lead tothe estimation of a large theory whose probabilitymass is concentrated upon the training set, witha consequential poor prediction of future, unseenevents.
This problem is known as over-fitting.
Over-fitting affects all Bayesian learners that assume anuninformative prior and are given insufficient train-ing data.
An over-fitted theory poorly predicts fu-ture events not seen in the training set.
Clearly,good prediction of unseen events is the central taskof language learners, and so steps need to be takento avoid over-fitting.Over-fitting is generally tackled in two ways:Restrict he learner such that it cannot expressthe maximally likely hypothesis, given some hy-pothesis language.Smooth the resulting parameters in the hopethat they back-off rom the training data andapportion more of the probability mass to ac-count for unseen material.Examples of the first approach can be seen mostclearly with the usage of CNF grammars by theInside-Outside algorithm (Pereira and Schabes,1992, Lari and Young, 1990).
A grammar in CNFdoes not contain rules of an arbitrary arity, and sowhen learning CNF grammars, the Inside-Outsidealgorithm cannot find the maximal likelihood es-timation of some training set.
The problem withthis language restriction is that there is no a pri-ori reason why one should settle with any particularlimit on rule arity; some grammars mainly containbinary rules, but others (for example those implic-itly within tree-banks) sometimes contain rules withmany right-hand side categories.
Any language re-striction, in lieu of some theory of rule arity, mustremain ad hoc.
Note that SCGs, whilst assigning bi-nary branching trees to sentences, contain categoriesthat may naturally be of an arbitrary length, with-out violating linguistic intuitions about what consti-tutes a plausible analysis of some sentence.Examples of the second approach can be foundin language modelling (for example (Church andGale, 1991, Katz, 1987)).
Smoothing a probabil-ity distribution tends to make it 'closer' (reduces theKullback-Liebler distance) to some other probabilitydistribution (for example, the uniform distribution).Unfortunately, there is no guarantee that this otherdistribution is closer to the target probability distri-bution than was the original, un-smoothed distribu-tion, and so smoothing cannot be relied upon alwaysto improve upon the un-smoothed theory.
Smooth-ing is also a post-hoc operation, unmotivated by de-tails of what is actually being learnt, or with prop-erties (problems) of the estimation process.
Insteadof selecting some language restriction or resorting tosmoothing, a better solution to the over-fitting prob-lem would be to use an informative prior.
One suchprior is in terms of theory minimisation, the pursuitOsborne ~4 Briscoe 82 Stochastic Categorial Grammarsof which leads to the Minimum Description LengthPrinciple (MDL) (Rissanen, 1989).In this paper we demonstrate that using MDLgives better results than when using an uninforma-tive prior.
Elsewhere, we demonstrated that (Good-Turing) smoothing does improve upon the accuracyof a SCG estimated using MLE, but still, the bestresults were obtained when using MDL (Osborne,1997).4 The  MDL PrincipleLearning can be viewed as compression f the train-ing data in terms of a compact hypothesis.
It can beshown that, under very general assumptions, the hy-pothesis with the minimal, or nearly minimal com-plexity, which is consistent with the training data,will with high probability predict future observationswell (Blumer et al, 1987).
One way of finding a goodhypothesis is to use a prior that favours hypothe-ses that are consistent with the training data, buthave minimal complexity.
That is, the prior shouldbe construed in terms of how well the hypothesiscan be compressed (since significant compression isequivalent to a low stochastic omplexity).We can compress the hypothesis by replacing itwith code words, such that when measured in bits ofinformation, the total length of the encoding is lessthan, or equal to, the length of the hypothesis, alsowhen measured in bits.
To achieve this aim, objectsin the hypothesis that occur frequently should beassigned shorter length code words than objects thatoccur infrequently.
Let l(H) be the total length ofthe code words for some set of objects H, as assignedby some optimal coding scheme.
It turns out that:2 -~(H) (4)can be used as a prior probability for H. The smallerl(H), the greater the compression, and so the higherthe prior probability.There is an equivalence between descriptionlengths, as measured in bits, and probabilities: theShannon Complexity of some object x, with proba-bility P(x), is - log(P(x)) (all logarithms are to thebase 2).
This gives the minimal number of bits re-quired to encode some object.
Hence, we can give adescription length to both the prior and likelihoodprobabilities.
Using these description lengths, wehave the MDL Principle: we should select some hy-pothesis H that:?
Minimises the length of the hypothesis (whenmeasured in bits) and?
Minimises the length of the data encoded in thehypothesis (measured in bits).Osborne ~ BriscoeThe first part says prefer hypotheses that are com-pact; the second part says prefer hypotheses that fitthe data well.
Both aspects of a theory are taken intoconsideration to arrive at a proper balance betweenoverly favouring a compact hypothesis (which willmodel the training data badly) and overly favour-ing the likelihood probability (which leads to over-fitting).To use the MDL principle when learning gram-mar, we need to compute the prior and likelihoodprobabilities.
One way to compute the prior is asfollows.We give each category r in lexicon H an encodingprobability P(r) .
If r was used f(r) times in theparse trees of the training set,f(r) (5)P( r )  - f (x )That is, categories used frequently in the trainingset have a high probability, and categories used in-frequently have a low probability.The intuition behind this particular codingscheme is to imagine that we are transmitting, inthe shortest possible way, a set of parse trees acrosssome channel.
We conceptually use a two-part,dictionary-based coding scheme: one part for word-category pairs with their associated code words, andanother part for an encoding of the trees in terms ofthe code words.
Since the total length of the en-coding of the trees will be much larger than the to-tal length of the word-category pairs and associatedcode words, we can assume the dictionary length isjust a constant, smaller than the total length of theencoded parse trees, and just consider, without anundue loss in accuracy, the cost of transmitting thetrees.
Hence, when we evaluate various lexica, we de-termine how much it costs to transmit he trainingmaterial in terms of the particular dictionary-basedencoding of the lexicon in question.
Equation 5 isused to give the length, in bits, of the code word wewould assign to each category in a parse tree.Our encoding scheme treats each category as be-ing independent and clearly, we could have usedmore of the context within the parse trees to con-struct a more efficient encoding scheme (see, for ex-ample (Ristad and Thomas, 1995)).
For the pur-poses of this paper, our simple encoding scheme issufficient.The length of a lexicon is the sum of the lengthsof all the categories used in the grammar:l(H) = ~ - log(P (x ) )  (6)xEH83 Stochastic Categorial GrammarsThe prior is therefore:P(g) = 2 -t(H) (7)The likelihood probability, P(D \[ H), is definedas simply the product of the probabilities of the cat-egories used to parse the corpus.We approximate the probability of the data,P(D), using a linear interpolated trigram model(Jelinek, 1990).
Our trigram model is used to as-sign probabilities to substrings: substrings denotingphrases will be assigned higher probabilities thansubstrings that do not form natural phrases.
Itshould be pointed out that most work in statisticallanguage learning ignores P(D).
However, the im-plementation reported in this paper is greedy, andtries to build parse trees for sentences incrementally.Hence, we need to determine if the substring dom-inated by a local tree forms a phrase (has a highP(D)), and is not some non-phrasal word grouping(has a low P(D)).
Clearly, using trigrams as an ap-proximation of P(D) may undermine the estimationprocess.
In our more recent, non-greedy work, wecan, and do, ignore P(D),  and so do not resort tousing the trigram model.5 ImplementationHaving shown how MDL can be applied to the es-timation of SCG, we now turn to a description ofan implemented system.
We learn categorial gram-mars in a greedy, bottom-up, incremental manner.In summary:?
For each part-of-speech tag sequence in somecorpus, we create a labelled binary tree span-ning that sequence.?
We then read-off rom the tree those categoriesthat would have generated that tree in the firstplace, placing them in the lexicon for subse-quent usage.In more detail, to create a labelled binary tree, wefirstly assign unary trees to each tag in the tag se-quence.
As far as the current implementation is con-cerned, the only element in a unary local tree is thetag.
For example, assuming the following taggedsentence:We_prp love_vbp categorial_jj grammars_nnswe would generate the forest of local trees:(prp) (vbp) ( j  j )  (nns)We ignore words and only work with the part-of-speech tags.Next, we consider all pairwise ways of joining ad-jacent local trees together.
For example, given theprevious forest of local trees, we would consider join-ing the following local trees together:(prp) (vbp)(vbp) (j  j )and(j j )  (nns)Each putative local tree is evaluated using Bayes'theorem: the prior is taken as being the probabilityassigned to an encoding of just the categories con-tained within the local tree (with respect o all thecategories in the lexicon)3; the likelihood is taken asbeing the geometric mean of the probabilities of thecategories contained within the local tree 4; the prob-ability of the data is taken as being the probabilityassigned by the ngram model to the tag sequencedominated by that local tree.
The mother of a localtree is defined using a small table of what constitutesa mother given possible heads.
Mothers are alwayseither the left or right daughter, representing eitherleft or right functional application.After evaluating each putative local tree, the treewith the highest posterior probability is chosen.This tree replaces the two local trees from whichit was created.Continuing our example, if we assume the putativelocal tree:(nns ( j j)  (nns))has a higher posterior probability than the putativelocal tree:(vbp (vbp) ( j j ) )we would replace the local trees:( j j)  (nns)with the local tree:(nns ( j j)  (nns))The whole process of tree evaluation, selection andreplacement is then repeated until a single tree re-mains.To read categories off a labelled local tree, thefollowing recursive process is applied:?
The category of the root of a tree is the categorydominating that tree.?
Given a local tree of the form (A (A S)), thecategory assigned to the daughter node labelledA is a/B, where a is the category assigned tothe root of the tree.
The category assigned tonode B is B.3This differs from taking the length of all the cate-gories in the lexicon.
We do this for efficiency purposes.4We take the geometric mean, and not the product,as this normalises the likelihood probability of arbitrarynumbers of categories.Osborne 8J Briscoe 84 Stochastic Categorial Grammars?
Given a local tree of the form (A (B A)), thecategory assigned to the daughter node labelledA is o~\B, where ~ is the category assigned tothe root of the tree.
The category assigned toB isB .Note other methods of reading categories off a treemight exist.
We make no claim that this is neces-sarily the best method.So, if we assume the following tree:(vbp (prp) (vbp (vbp) (nns (jj) (nns))))we would extract he following categories:Tag Categorynns nns~jjJJ JJvbp vbp\prp/nnsprp prpOur categories are With each category, we also keepa frequency count of the number of times that cat-egory was added to the lexicon.
This frequency in-formation is used to estimate the probabilities of thelexicon.Finally, when learning, we ignore sentencesshorter than three words (these are likely to be un-grammatical fragments), or, for computational rea-sons, sentences longer than 50 words.6 Exper imentsHere, we report on a number of experiments show-ing that when there is a danger of overfitting takingplace, MDL produces a quantitatively better SCGthan does MLE.To evaluate the various lexica produced, we usedthe following metrics:?
To measure a grammar's coverage, we note thenumber of tag sequences, drawn from a corpusof naturally occurring language, some grammargenerates.
The higher the number, the betterthe grammar.?
To measure a grammar's overgeneration, wenote the number of ungrammatical strings,drawn from a source that generates all stringsup to some length randomly, a grammar gen-erates.
The lower the number, the better thegrammar.
That is, random sequences of tags,of a sufficient length, will have a low probabil-ity of being grammatically well-formed.?
To measure the accuracy of the parses pro-.
duced, we use the Grammar Evaluation Inter-est Group scheme (GEIG) (Harrison et al, 19).This compares unlabelled, manually producedparses with automatically produced parses interms of recall (the ratio of matched brack-ets over all brackets in the manually producedparses), precision (the ratio of matched bracketsin the manually produced parse over all brack-ets found by the parser) and crossing rates (thenumber of times a bracketed sequence producedby the parser overlaps with one in the man-ually produced parse, but neither is properlycontained in the other).
The higher the preci-sion and recall, and the lower the crossing rates,the better the grammar.Throughout our experiments, we used the Brillpart-of-speech tagger to create testing and train-ing material (Brill, 1993).
Our trigram model wascreated using seven million words of tagged mate-rial drawn from the British National Corpus (BNC);training material consisted of 43,000 tagged sen-tences also taken from the BNC.
For test material,we took 429 sentences taken from the Spoken En-glish Corpus (SEC).
To compute crossing rates, re-call and precision figures, we used a program calledParseval to compare most probable parses with man-ually produced parses (232 trees in total taken fromthe SEC) (Harrison et al, 19).
To measure overgen-eration, we randomly generated 250 strings.
From amanual inspection, these do appear to be ungram-matical.
Here is an example randomly generated tagsequence:1 NP MD WP$ LS POS POS VBD NN WDT SYMWe started with no initial lexica.Training constructed the lexica outlined in figure1.
Note the difference in the size of the lexica.
AllLexicon How learnt ~ Size (categories)A MDL 24829B MLE 31091Figure 1: Sizes of various lexicathings being equal, we prefer the lexicon to be assmall as possible.
The larger the lexicon, the slowerthe parsing.
As predicted by theory, the lexiconlearnt using MLE is larger than the one learnt usingMDL.Testing for coverage, we produced the resultsshown in figure 2.
Again as predicted, lexicon A isLexicon Percentage generatedA 95B 93Figure 2: UndergenerationO~borne 8J Briscoe 85 Stochastic Categorial Grammarscloser to convergence (better coverage) than lexiconB.Lexicon Percentage generatedA 0B 0Figure 3: OvergenerationTurning now to figure 3, we see that, with respectto the test set, neither lexicon overgenerates.Lexicon Crossing rate Precision RecallA 2.84 51.13 36.04B 3.39 46.46 32.05Figure 4: Crossing ratesFigure 4 shows the crossing rate results.
Again,MDL has lead to the estimation of a better lexiconthan has MLE.
Note that the actual figures are notas great as they might be.
This follows from the factthat although categorial grammars assigned binary-branching trees to sentences, the test parses used tocompute crossing rates were not restricted to beingbinary branching.
Also, our learner used virtuallyno supervision (for example parsed corpora), anddid not start with a given lexicon: learning usingparsed corpora is substantiMly easier than learningfrom just a tagged text, whilst starting with a given,manually constructed lexicon is equivalent to learn-ing with a good initial estimation of the target lexi-con, which greatly increases the chance of successfullearning.
However, the figures are sufficient for thepurposes of our demonstration.7 DiscussionIn this paper, we introduced SCGs, and argued thatthey are more appropriate formalisms with whichto estimate grammars than are SCFGs.
We thenshowed how the Minimum Description Length Prin-ciple provides away of reducing the problem of over-fitting when estimating SCGs.In more recent work, we are using a versionof the Expectation-Maximisation algorithm to es-timate SCGs.
We use bracketed training material,and an MDL-style prior to aid in the estimation pro-cess.
A later publication will report on this research.The current state-of-the-art parsers trained ontreebank data using fully lexicalised probabilisticmodels achieve crossing rates of around 1.0 persentence.
We achieve 2.84 using more heteroge-neous and unannotated training material and learn-ing SCGs from scratch.
In future work we will at-Osborne 64 Briscoe 86tempt o rival the state-of-the-art through full lexi-calisation and utilising bracketed training material.AcknowledgmentsWe would like to thank the two anonymous reviewersfor valuable comments on this paper.
This work wassupported by the EU Project Sparkle LE-21ReferencesJ.
K. Baker.
Trainable grammars for speech recog-nition.
In D. H. Klatt and J. J. Wolf, editors,Speech Communication Papers for the 97 thMeeting of the Acoustical Society of America,pages 547-550.
1979.L.
E. Baum.
An inequality and associated max-imization technique in statistical estimationfor probabilistic functions of Markov processes.Inequalities, III:l-8, 1972.A.
Blumer, A. Ehrenfeucht, D. Haussler, andM.
Warmuth.
Occam's Razor.
InformationProcessing Letters, 24:377-380, 1987.Eric Brill.
A Corpus-Based Approach to LanguageLearning.
PhD thesis, University of Pennsyl-vania, 1993.Stanley F. Chen.
Building Probabilistic LanguageModels for Natural Language.
PhD thesis, Har-vard University, 1996.Kenneth W. Church and William A. Gale.
Acomparison of the enhanced Good-Turing anddeleted estimation methods for estimatingprobabilities of English bigrams.
ComputerSpeech and Language, 5:19-54, 1991.Michael John Collins.
A new statistical parser basedon bigram lexical dependencies.
In 34 th An-nual Meeting of the Association for Compu-tational Linguistics.
University of California,Santa Cruz, California, USA, June 1996.Carl de Marcken.
Unsupervised Language Acquisi-tion.
PhD thesis, MIT, 1996.Philip Harrison, Steven Abney, Ezra Black, DanFlickinger, Ralph Grishman Claudia Gdaniec,Donald Hindle, Robert Ingria, Mitch Mar-cus, Beatrice Santorini, and Tomek Strza-lkowski.
Evaluating Syntax Performance ofParser/Grammars of English.
In Jeannette G.Neal and Sharon M. Walter, editors, Natu-ral Language Processing Systems EvaluationWorkshop, Technical Report RL-TR-91-362,1991.Stochastic Categorial GrammarsFred Jelinek.
Self-organised language modelling forspeech recognition.
In Alex Waibel and Kai-FuLee, editors, Readings in Speech Recognition,pages 450-560.
Morgan-Kaufmann, 1990.Slava M. Katz.
Estimation of probabilities fromsparse data for the language model componentof a speech recognizer.
In IEEE Transactionson Acoustics, Speech and Signal Processing,volume 35, pages 400-401, March 1987.K.
Lari and S. J.
Young.
The estimation of stochas-tic context-free grammars using the Inside-Outside Algorithm.
Computer Speech andLanguage, 4:35-56, 1990.Miles Osborne.
Minimisation, indifference andstatistical language learning.
In Empiri-cal Learning of Natural Language ProcessingTasks, pages 113-124, Prague, Czech Repub-lic, April 1997.
ECML-97 MLNET Familiari-sation Workshop.Fernando Pereira and Yves Schabes.
Inside-outsidereestimation from partially bracketed corpora.In Proceedings off the 30 th A CL, University ofDelaware, Newark, Delaware, pages 128-135,1992.Jorma Rissanen.
Stochastic Complexity in StatisticalInquiry.
Series in Computer Science -Volume15.
World Scientific, 1989.Jorma Rissanen and Eric Sven Ristad.
Lan-guage Acquisition in the MDL Framework.In Eric Sven Ristad, editor, Language Com-putation.
American Mathemtatical Society,Philedelphia, 1994.EricYvesSven Ristad and Robert G. Thomas, III.
Con-text Models in the MDL Framework.
In Pro-ceedings of the Data Compression Conference,pages 62-71, Snowbird, Utah, March 1995.IEEE, IEEE Computer Society Press.Schabes.
Stochastic lexicalized tree-adjoininggrammars.
In Proceedings of the 14 th Interna-tional Conference on Computational Linguis-tics, 1992.M.
J. Steedman.
Dependency and Coordination inthe Grammar of Dutch and English.
Language,61:523-568, 1989.Andreas Stolcke.
Bayesian Learning of ProbabilisticLanguage Models.
PhD thesis, University ofCalifornia, Berkley, 1994.Osborne ~ Briscoe 87Mary McGee Wood.
Categorial Grammars.
Rout-ledge, 1993.
Linguistic Theory Guides.Stochastic Categorial Grammars
