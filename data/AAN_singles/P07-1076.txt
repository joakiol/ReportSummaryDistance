Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 600?607,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsUsing Corpus Statistics on Entities to Improve Semi-supervisedRelation Extraction from the WebBenjamin RosenfeldInformation SystemsHU School of Business,Hebrew University, Jerusalem, Israelgrurgrur@gmail.comRonen FeldmanInformation SystemsHU School of Business,Hebrew University, Jerusalem, Israelronen.feldman@huji.ac.ilAbstractMany errors produced by unsupervised andsemi-supervised relation extraction (RE)systems occur because of wrong recogni-tion of entities that participate in the rela-tions.
This is especially true for systemsthat do not use separate named-entity rec-ognition components, instead relying ongeneral-purpose shallow parsing.
Such sys-tems have greater applicability, becausethey are able to extract relations thatcontain attributes of unknown types.However, this generality comes with thecost in accuracy.
In this paper we showhow to use corpus statistics to validate andcorrect the arguments of extracted relationinstances, improving the overall REperformance.
We test the methods onSRES ?
a self-supervised Web relationextraction system.
We also compare theperformance of corpus-based methods tothe performance of validation and correc-tion methods based on supervised NERcomponents.1 IntroductionInformation Extraction (IE) is the task of extract-ing factual assertions from text.
Most IE systemsrely on knowledge engineering or on machinelearning to generate the ?task model?
that is subse-quently used for extracting instances of entities andrelations from new text.
In the knowledge engi-neering approach the model (usually in the form ofextraction rules) is created manually, and in themachine learning approach the model is learnedautomatically from a manually labeled training setof documents.
Both approaches require substantialhuman effort, particularly when applied to thebroad range of documents, entities, and relationson the Web.
In order to minimize the manual ef-fort necessary to build Web IE systems, semi-supervised and completely unsupervised systemsare being developed by many researchers.The task of extracting facts from the Web hassignificantly different aims than the regular infor-mation extraction.
The goal of regular IE is toidentify and label all mentions of all instances ofthe given relation type inside a document or insidea collection of documents.
Whereas, in the WebExtraction (WE) tasks we are only interested inextracting relation instances and not interested inparticular mentions.This difference in goals leads to a difference inthe methods of performance evaluation.
The usualmeasures of performance of regular IE systems areprecision, recall, and their combinations ?
thebreakeven point and F-measure.
Unfortunately, thetrue recall usually cannot be known for WE tasks.Consequently, for evaluating the performance ofWE systems, the recall is substituted by the num-ber of extracted instances.WE systems usually order the extracted in-stances by the system?s confidence in their cor-rectness.
The precision of top-confidence extrac-tions is usually very high, but it gets progressivelylower when lower-confidence candidates are con-sidered.
The curve that plots the number of extrac-tions against precision level is the best indicator ofsystem?s quality.
Naturally, for a comparision be-600tween different systems to be meaningful, theevaluations must be performed on the same corpus.In this paper we are concerned with Web REsystems that extract binary relations betweennamed entities.
Most of such systems utilize sepa-rate named entity recognition (NER) components,which are usualy trained in a supervised way on aseparate set of manually labeled documents.
TheNER components recognize and extract the valuesof relation attributes (also called arguments, orslots), while the RE systems are concerned withpatterns of contexts in which the slots appear.However, good NER components only exist forcommon and very general entity types, such asPerson, Organization, and Location.
For some re-lations, the types of attributes are less common,and no ready NER components (or ready labeledtraining sets) exist for them.
Also, some Web REsystems (e.g., KnowItAll (Etzioni, Cafarella et al2005)) do not use separate NER components evenfor known entity types, because such componentsare usually domain-specific and may performpoorly on cross-domain text collections extractedfrom the Web.In such cases, the values for relation attributesmust be extracted by generic methods ?
shallowparsing (extracting noun phrases), or even simplesubstring extraction.
Such methods are naturallymuch less precise and produce many entity-recognition errors (Feldman and Rosenfeld 2006).In this paper we propose several methods of us-ing corpus statistics to improve Web RE precisionby validating and correcting the entities extractedby generic methods.
The task of Web Extraction isparticularly suited for the corpus statistics-basedmethods because of very large size of the corporainvolved, and because the system is not required toidentify individual mentions of the relations.Our methods of entity validation and correctionare based on the following two observations:First, the entities that appear in target relationswill often also appear in many other contexts,some of which may strongly discriminate in favorof entities of specific type.
For example, assumethe system encounters a sentence ?Oracle boughtPeopleSoft.?
If the system works without a NERcomponent, it only knows that ?Oracle?
and ?Peo-pleSoft?
are proper noun phrases, and its confi-dence in correctness of a candidate relation in-stance  Acquisition(Oracle, PeopleSoft)  cannot bevery high.
However, both entities occur manytimes elsewhere in the corpus, sometimes instrongly discriminating contexts, such as ?Oracleis a company that??
or ?PeopleSoft Inc.?
If thesystem somehow learned that such contexts indi-cate entities of the correct type for the Acquisitionrelation (i.e., companies), then the system wouldbe able to boost its confidence in both entities(?Oracle?
and ?PeopleSoft?)
being of correct typesand, consequently, in (Oracle, PeopleSoft) being acorrect instance of the Acquisition relation.Another observation that we can use is the factthat the entities, in which we are interested, usuallyhave sufficient frequency in the corpus for statisti-cal term extraction methods to perform reasonablywell.
These methods may often correct a wronglyplaced entity boundary, which is a common mis-take of general-purpose shallow parsers.In this paper we show how to use these observa-tions to supplement a Web RE system with an en-tity validation and correction component, which isable to significantly improve the system?s accu-racy.
We evaluate the methods using SRES(Feldman and Rosenfeld 2006) ?
a Web RE sys-tem, designed to extend and improve KnowItAll(Etzioni, Cafarella et al 2005).
The contributionsof this paper are as follows:?
We show how to automatically generatethe validating patterns for the target relationarguments, and how to integrate the resultsproduced by the validating patterns into thewhole relation extraction system.?
We show how to use corpus statistics andterm extraction methods to correct theboundaries of relation arguments.?
We experimentally compare the improve-ment produced by the corpus-based entityvalidation and correction methods with theimprovements produced by two alternativevalidators ?
a CRF-based NER systemtrained on a separate labeled corpus, and asmall manually-built rule-based NER com-ponent.The rest of the paper is organized as follows:Section 2 describes previous work.
Section 3 out-lines the general design principles of SRES andbriefly describes its components.
Section 4 de-scribes in detail the different entity validation andcorrection methods, and Section 5 presents their601experimental evaluation.
Section 6 contains con-clusions and directions for future work.2 Related WorkWe are not aware of any work that deals specifi-cally with validation and/or correction of entityrecognition for the purposes of improving relationextraction accuracy.
However, the backgroundtechniques of our methods are relatively simpleand known.
The validation is based on the sameideas that underlie semi-supervised entity extrac-tion (Etzioni, Cafarella et al 2005), and uses asimplified SRES code.
The boundary correctionprocess utilizes well-known term extraction meth-ods, e.g., (Su, Wu et al 1994).We also recently became aware of the work byDowney, Broadhead and Etzioni (2007) that dealswith locating entities of arbitrary types in largecorpora using corpus statistics.The IE systems most similar to SRES are basedon bootstrap learning: Mutual Bootstrapping(Riloff and Jones 1999), the DIPRE system (Brin1998), and the Snowball system (Agichtein andGravano 2000).
Ravichandran and Hovy(Ravichandran and Hovy 2002) also use bootstrap-ping, and learn simple surface patterns for extract-ing binary relations from the Web.Unlike these systems, SRES surface patterns al-low gaps that can be matched by any sequences oftokens.
This makes SRES patterns more general,and allows to recognize instances in sentences in-accessible to the simple surface patterns of systemssuch as (Brin 1998; Riloff and Jones 1999; Ravi-chandran and Hovy 2002).Another direction for unsupervised relationlearning was taken in (Hasegawa, Sekine et al2004; Chen, Ji et al 2005).
These systems use aNER system to identify frequent pairs of entitiesand then cluster the pairs based on the types of theentities and the words appearing between the enti-ties.
The main benefit of this approach is that allrelations between two entity types can be discov-ered simultaneously and there is no need for theuser to supply the relations definitions.3 Description of SRESThe goal of SRES is extracting instances of speci-fied relations from the Web without human super-vision.
Accordingly, the supervised input to thesystem is limited to the specifications of the targetrelations.
A specification for a given relation con-sists of the relation schema and a small set of seeds?
known true instances of the relation.
In the full-scale SRES, the seeds are also generated automati-cally, by using a set of generic patterns instantiatedwith the relation schema.
However, the seed gen-eration is not relevant to this paper.A relation schema specifies the name of the rela-tion, the names and types of its arguments, and thearguments ordering.
For example, the schema ofthe Acquisition relationAcquisition(Buyer=ProperNP,Acquired=ProperNP)  orderedspecifies that Acquisition has two slots, namedBuyer and Acquired, which must be filled with en-tities of type ProperNP.
The order of the slots isimportant (as signified by the word ?ordered?, andas opposed to relations like Merger, which are?unordered?
or, in binary case, ?symmetric?
).The baseline SRES does not utilize a named en-tity recognizer, instead using a shallow parser forexracting the relation slots.
Thus, the only allowedentity types are ProperNP, CommonNP, andAnyNP, which mean the heads of, respectively,proper, common, and arbitrary noun phrases.
In theexperimental section we compare the baselineSRES to its extensions containing additional NERcomponents.
When using those components weallow further subtypes of ProperNP, and the rela-tion schema above becomes?
(Buyer=Company, Acquired=Company) ?The main components of SRES are the PatternLearner, the Instance Extractor, and the Classifier.The Pattern Learner uses the seeds to learn likelypatterns of relation occurrences.
Then, the InstanceExtractor uses the patterns to extract the candidateinstances from the sentences.
Finally, the Classifierassigns the confidence score to each extraction.
Weshall now briefly describe these components.3.1 Pattern LearnerThe Pattern Learner receives a relation schemaand a set of seeds.
Then it finds the occurences ofseeds inside a large (unlabeled) text corpus, ana-lyzes their contexts, and extracts common patternsamong these contexts.
The details of the patternslanguage and the process of pattern learning arenot significant for this paper, and are describedfully in (Feldman and Rosenfeld 2006).6023.2 Instance ExtractorThe Instance Extractor applies the patterns gener-ated by the Pattern Learner to the text corpus.
Inorder to be able to match the slots of the patterns,the Instance Extractor utilizes an external shallowparser from the OpenNLP package(http://opennlp.sourceforge.net/), which is able tofind all proper and common noun phrases in a sen-tence.
These phrases are matched to the slots of thepatterns.
In other respects, the pattern matchingand extraction process is straightforward.3.3 ClassifierThe goal of the final classification stage is to filterthe list of all extracted instances, keeping the cor-rect extractions, and removing mistakes that wouldalways occur regardless of the quality of the pat-terns.
It is of course impossible to know which ex-tractions are correct, but there exist properties ofpatterns and pattern matches that increase or de-crease the confidence in the extractions that theyproduce.These properties are turned into a set of binaryfeatures, which are processed by a linear feature-rich classifier.
The classifier receives a feature vec-tor for a candidate, and produces a confidencescore between 0 and 1.The set of features is small and is not specific toany particular relation.
This allows to train a modelusing a small amount of labeled data for one rela-tion, and then use the model for scoring the candi-dates of all other relations.
Since the supervisedtraining stage needs to be run only once, it is a partof the system development, and the complete sys-tem remains unsupervised, as demonstrated in(Feldman and Rosenfeld 2006).4 Entity Validation and CorrectionIn this paper we describe three different methodsof validation and correction of relation argumentsin the extracted instances.
Two of them are ?classi-cal?
and are based, respectively, on the knowledge-engineering, and on the statistical supervised ap-proaches to the named entity recognition problems.The third is our novel approach, based on redun-dancy and corpus statistics.The methods are implemented as componentsfor SRES, called Entity Validators, inserted be-tween the Instance Extractor and the Classifier.The result of applying Entity Validator to a candi-date instance is an (optionally) fixed instance, withvalidity values attached to all slots.
There are threevalidity values: valid, invalid, and uncertain.The Classifier uses the validity values by con-verting them into two additional binary features,which are then able to influence the confidence ofextractions.We shall now describe the three different valida-tors in details.4.1 Small Rule-based NER validatorThis validator is a small Perl script that checkswhether a character string conforms to a set ofsimple regular expression patterns, and whether itappears inside lists of known named entities.
Thereare two sets of regular expression patterns ?
forPerson and for Company entity types, and threelarge lists ?
for known personal names, knowncompanies, and ?other known named entities?, cur-rently including locations, universities, and gov-ernment agencies.The manually written regular expression repre-sent simple regularities in the internal structure ofthe entity types.
For example, the patterns for Per-son include:Person = KnownFirstName  [Initial]  LastNamePerson = Honorific [FirstName] [Initial] LastNameHonorific = (?Mr?
| ?Ms?
| ?Dr?
|?)
[?.?
]Initial = CapitalLetter [?.?
]KnownFirstName = member ofKnownPersonalNamesListFirstName = CapitalizedWordLastName = CapitalizedWordLastName = CapitalizedWord [??
?CapitalizedWord]LastName = (?o?
| ?de?
| ?)
?`?CapitalizedWord?while the patterns for Company include:Company = KnownCompanyNameCompany = CompanyName CompanyDesignatorCompany = CompanyName FrequentCompanySfxKnownCompanyName = member ofKnownCompaniesListCompanyName = CapitalizedWord +CompanyDesignator = ?inc?
| ?corp?
| ?co?
| ?FrequentCompanySfx = ?systems?
| ?software?
| ?
?The validator works in the following way: it re-ceives a sentence with a labeled candidate entity ofa specified entity type (which can be either Personor Company).
It then applies all of the regular ex-pression patterns to the labeled text and to its en-603closing context.
It also checks for membership inthe lists of known entities.
If a boundary is incor-rectly placed according to the patterns or to thelists, it is fixed.
Then, the following result is re-turned:Valid, if some pattern/list of the right entity typematched the candidate entity, while therewere no matches for patterns/lists of otherentity types.Invalid, if no pattern/list of the right entity typematched the candidate entity, while therewere matches for patterns/lists of other entitytypes.Uncertain, otherwise, that is either if there wereno matches at all, or if both correct and in-correct entity types matched.The number of patterns is relatively small, andthe whole component consists of about 300 lines inPerl and costs several person-days of knowledgeengineering work.
Despite its simplicity, we willshow in the experimental section that it is quiteeffective, and even often outperforms the CRF-based NER component, described below.4.2 CRF-based NER validatorThis validator is built using a feature-rich CRF-based sequence classifier, trained upon an Englishdataset of the CoNLL 2003 shared task (Rosenfeld,Fresko et al 2005).
For the gazetteer lists it usesthe same large lists as the rule-based componentdescribed above.The validator receives a sentence with a labeledcandidate entity of a specified entity type (whichcan be either Person or Company).
It then sendsthe sentence to the CRF-based classifier, whichlabels all named entities it knows ?
Dates, Times,Percents, Persons, Organizations, and Locations.If the CRF classifier places the entity boundariesdifferently, they are fixed.
Then, the following re-sult is returned:Valid, if CRF classification of the entity accordswith the expected argument type.Invalid, if CRF classification of the entity is dif-ferent from the expected argument type.Uncertain, otherwise, that is if the CRF classi-fier didn?t recognize the entity at all.4.3 Corpus-based NER validatorThe goal of building the corpus-based NER valida-tor is to provide the same level of performance asthe supervised NER components, while requiringneither additional human supervision nor addi-tional labeled corpora or other resources.
There areseveral important facts that help achieve this goal.First, the relation instances that are used as seedsfor the pattern learning are known to contain cor-rect instances of the right entity type.
These in-stances can be used as seeds in their own right, forlearning the patterns of occurrence of the corre-sponding entity types.
Second, the entities in whichwe are interested usually appear in the corpus witha sufficient frequency.
The validation is based onthe first observation, while the boundary fixing onthe second.Corpus-based entity validationThere is a preparation stage, during which theinformation required for validation is extractedfrom the corpus.
This information is the lists of allentities of every type that appears in the target rela-tions.
In order to extract these lists we use a simpli-fied SRES.
The entities are considered to be unaryrelations, and the seeds for them are taken from theslots of the target binary relations seeds.
We don?tuse the Classifier on the extracted entity instances.Instead, for every extracted instance we record thenumber of different sentences the entity was ex-tracted from.During the validation process, the validator?stask is to evaluate a given candidate entity in-stance.
The validator compares the number oftimes the instance was extracted (during the prepa-ration stage) by the patterns for the correct entitytype, and by the patterns for all other entity types.The validator then returnsValid, if the number of times the entity was ex-tracted for the specified entity type is at least5, and at least two times bigger than thenumber of times it was extracted for all otherentity types.Invalid, if the number of times the instance wasextracted for the specified entity type is lessthan 5, and at least 2 times smaller than thenumber of times it was extracted for all otherentity types.604Uncertain, otherwise, that is if it was never ex-tracted at all, or extracted with similar fre-quency for both correct and wrong entitytypes.Corpus-based correction of entity boundariesOur entity boundaries correction mechanism issimilar to the known statistical term extractiontechniques (Su, Wu et al 1994).
It is based on theassumption that the component words of a term (anentity in our case) are more tightly bound to eachother than to the context.
In the statistical sense,this fact is expressed by a high mutual informationbetween the adjacent words belonging to the sameterm.There are two possible boundary fixes: remov-ing words from the candidate entity, or addingwords from the context to the entity.
There is asignificant practical difference between the twocases.Assume that an entity boundary was placed toobroadly, and included extra words.
If this was achance occurrence (and only such cases can befound by statistical methods), then the resultingsequence of tokens will be very infrequent, whileits parts will have relatively high frequency.
Forexample, consider a sequence ?Formerly MicrosoftCorp.
?, which is produced by mistakenly labeling?Formerly?
as a proper noun by the PoS tagger.While it is easy to know from the frequencies thata boundary mistake was made, it is unclear (to thesystem) which part is the correct entity.
But sincethe entity (one of the parts of the candidate) has ahigh frequency, there is a chance that the relationinstance, in which the entity appears, will be re-peated elsewhere in the corpus and will be ex-tracted correctly there.
Therefore, in such case, thesimplest recourse is to simply label the entity asInvalid, and not to try fixing the boundaries.On the other hand, if a word was missed from anentity (e.g., ?Beverly O?, instead of ?Beverly O 'Neill?
), the resulting sequence will be frequent.Moreover, it is quite probable that the sameboundary mistake is made in many places, becausethe same sequence of tokens is being analyzed inall those places.
Therefore, it makes sense to try tofix the bounary in this case, especially since it canbe done simply and  reliably: a word (or severalwords) is attached to the entity string if both theirfrequencies and their mutual information are abovea threshold.5 Experimental EvaluationThe experiments described in this paper aim toconfirm the effectiveness of the proposed corpus-based relation argument validation and correctionmethod, and to compare its performance with theclassical knowledge-engineering-based and super-vised-training-based methods.
The experimentswere performed with five relations:Acquisition(BuyerCompany, AcquiredCompany),Merger(Company1, Company2),CEO_Of(Company, Person),MayorOf(City, Person),InventorOf(Person, Invention).The data for the experiments were collected by theKnowItAll crawler.
The data for the Acquisitionand Merger consist of about 900,000 sentences foreach of the two relations.
The data for the boundrelations consist of sentences, such that each con-tains one of a hundred values of the first (bound)attribute.
Half of the hundred are frequent entities(>100,000 search engine hits), and another half arerare (<10,000 hits).For evaluating the validators we randomly se-lected a set of 10000 sentences from the corporafor each of the relations, and manually evaluatedthe SRES results generated from these sentences.Four sets of results were evaluated: the baselineresults produced without any NER validator, andthree sets of results produced using three differentNER validators.
For the InventorOf relation, onlythe corpus-based validator results can be produced,since the other two NER components cannot beadapted to validate/correct entities of type Inven-tion.The results for the five relations are shown inthe Figure 1.
Several conclusions can be drawnfrom the graphs.
First, all of the NER validatorsimprove over the baseline SRES, sometimes asmuch as doubling the recall at the same level ofprecision.
In most cases the three validators showroughly similar levels of performance.
A notabledifference is the CEO_Of relation, where the sim-ple rule-based component performs much betterthan CRF, which performs yet better than the cor-pus-based component.
The CEO_Of relation istested as bound, which means that only the secondrelation argument, of type Person, is validated.
ThePerson entities have much more rigid internalstructure than the other entities ?
Companies andInventions.
Consequently, the best performing of605Acquisition0.500.600.700.800.901.000 50 100 150Correct ExtractionsPrecisionBaseline RB-NER CRF CorpusMerger0.500.600.700.800.901.000 50 100 150Correct ExtractionsPrecisionBaseline RB-NER CRF CorpusCeoOf0.500.600.700.800.901.000 20 40 60 80 100 120Correct ExtractionsPrecisionBaseline RB-NER CRF CorpusInventorOf0.500.600.700.800.901.000 20 40 60 80 100 120Correct ExtractionsPrecisionBaseline CorpusFigure 1.
Comparison between Baseline-SRES and its extensions with three different NER validators:  asimple Rule-Based one, a CRF-based statistical one, and a Corpus-based one.the three validators is the rule-based, which di-rectly tests this internal structure.
The CRF-basedvalidator is also able to take advantage of the struc-ture, although in a weaker manner.
The Corpus-based validator, however, works purely on the ba-sis of context, entirely disregarding the internalstructure of entities, and thus performs worst of allin this case.
On the other hand, the Corpus-basedvalidator is able to improve the results for the In-ventor relation, which the other two validators arecompletely unable to do.It is also of interest to compare the performanceof CRF-based and the rule-based NER componentsin other cases.
As can be seen, in most cases therule-based component, despite its simplicity, out-performs the CRF-based one.
The possible reasonfor this is that relation extraction setting is signifi-cantly different from the classical named entityrecognition setting.
A classical NER system is setto maximize  the F1 measure of all mentions of allentities in the corpus.
A relation argument extrac-tor, on the other hand, should maximize its per-formance on relation arguments, and apparentlytheir statistical properties are often significantlydifferent.6 ConclusionsWe have presented a novel method for validationand correction of relation arguments for the state-of-the-art unsupervised Web relation extractionsystem SRES.
The method is based on corpus sta-tistics and requires no human supervision and noadditional corpus resources beyond the corpus thatis used for relation extraction.We showed experimentally the effectiveness ofour method, which performed comparably to bothsimple rule-based NER and a statistical CRF-basedNER in the task of validating Companies, andsomewhat worse in the task of validating Persons,606due to its complete disregard of internal structureof entities.
The ways to learn and use this structurein an unsupervised way are left for future research.Our method also successfully validated theInvention entities, which are inaccessible to theother methods due to the lack of training data.In our experiments we made use of a unique fea-ture of SRES system ?
a feature-rich classifier thatassigns confidence score to the candidate in-stances, basing its decisions on various features ofthe patterns and of the contexts from which thecandidates were extracted.
This architecture allowseasy integration of the entity validation compo-nents as additional feature generators.
We believe,however, that our results have greater applicability,and that the corpus statistics-based components canbe added to RE systems with other architectures aswell.ReferencesAgichtein, E. and L. Gravano (2000).
Snowball: Ex-tracting Relations from Large Plain-Text Collections.Proceedings of the 5th ACM International Confer-ence on Digital Libraries (DL).Brin, S. (1998).
Extracting Patterns and Relations fromthe World Wide Web.
WebDB Workshop at 6th In-ternational Conference on Extending Database Tech-nology, EDBT?98, Valencia, Spain.Chen, J., D. Ji, C. L. Tan and Z. Niu (2005).
Unsuper-vised Feature Selection for Relation Extraction.IJCNLP-05, Jeju Island, Korea.Downey, D., M. Broadhead and O. Etzioni (2007).
Lo-cating Complex Named Entities in Web Text.
IJCAI-07.Etzioni, O., M. Cafarella, D. Downey, A. Popescu, T.Shaked, S. Soderland, D. Weld and A. Yates (2005).Unsupervised named-entity extraction from the Web:An experimental study.
Artificial Intelligence 165(1):91-134.Feldman, R. and B. Rosenfeld (2006).
Boosting Unsu-pervised Relation Extraction by Using NER.EMNLP-06, Sydney, Australia.Feldman, R. and B. Rosenfeld (2006).
Self-SupervisedRelation Extraction from the Web.
ISMIS-2006, Bari,Italy.Hasegawa, T., S. Sekine and R. Grishman (2004).
Dis-covering Relations among Named Entities fromLarge Corpora.
ACL 2004.Ravichandran, D. and E. Hovy (2002).
Learning Sur-face Text Patterns for a Question Answering System.40th ACL Conference.Riloff, E. and R. Jones (1999).
Learning Dictionariesfor Information Extraction by Multi-level Boot-strapping.
AAAI-99.Rosenfeld, B., M. Fresko and R. Feldman (2005).
ASystematic Comparison of Feature-Rich Probabilis-tic Classifiers for NER Tasks.
PKDD.Su, K.-Y., M.-W. Wu and J.-S. Chang (1994).
A Cor-pus-based Approach to Automatic Compound Ex-traction.
Meeting of the Association for Computa-tional Linguistics: 242-247.607
