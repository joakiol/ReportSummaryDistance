Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025?1035,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Unified Model for Word Sense Representation and DisambiguationXinxiong Chen, Zhiyuan Liu, Maosong SunState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and TechnologyTsinghua University, Beijing 100084, Chinacxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cnAbstractMost word representation methods assumethat each word owns a single semantic vec-tor.
This is usually problematic becauselexical ambiguity is ubiquitous, which isalso the problem to be resolved by wordsense disambiguation.
In this paper, wepresent a unified model for joint wordsense representation and disambiguation,which will assign distinct representation-s for each word sense.1The basic idea isthat both word sense representation (WS-R) and word sense disambiguation (WS-D) will benefit from each other: (1) high-quality WSR will capture rich informa-tion about words and senses, which shouldbe helpful for WSD, and (2) high-qualityWSD will provide reliable disambiguat-ed corpora for learning better sense rep-resentations.
Experimental results showthat, our model improves the performanceof contextual word similarity compared toexisting WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD, and achieves competitiveperformance on coarse-grained all-wordsWSD.1 IntroductionWord representation aims to build vectors for eachword based on its context in a large corpus, usuallycapturing both semantic and syntactic informationof words.
These representations can be used asfeatures or inputs, which are widely employed ininformation retrieval (Manning et al., 2008), doc-ument classification (Sebastiani, 2002) and otherNLP tasks.1Our sense representations can be downloaded at http://pan.baidu.com/s/1eQcPK8i.Most word representation methods assume eachword owns a single vector.
However, this is usual-ly problematic due to the homonymy and polyse-my of many words.
To remedy the issue, Reisingerand Mooney (2010) proposed a multi-prototypevector space model, where the contexts of eachword are first clustered into groups, and then eachcluster generates a distinct prototype vector for aword by averaging over all context vectors with-in the cluster.
Huang et al.
(2012) followed thisidea, but introduced continuous distributed vectorsbased on probabilistic neural language models forword representations.These cluster-based models conduct unsuper-vised word sense induction by clustering wordcontexts and, thus, suffer from the following is-sues:?
It is usually difficult for these cluster-basedmodels to determine the number of cluster-s. Huang et al.
(2012) simply cluster wordcontexts into static K clusters for each word,which is arbitrary and may introduce mis-takes.?
These cluster-based models are typically off-line , so they cannot be efficiently adapted tonew senses, new words or new data.?
It is also troublesome to find the sense thata word prototype corresponds to; thus, thesecluster-based models cannot be directly usedto perform word sense disambiguation.In reality, many large knowledge bases havebeen constructed with word senses availableonline, such as WordNet (Miller, 1995) andWikipedia.
Utilizing these knowledge bases tolearn word representation and sense representationis a natural choice.
In this paper, we present a uni-fied model for both word sense representation anddisambiguation based on these knowledge basesand large-scale text corpora.
The unified model1025can (1) perform word sense disambiguation basedon vector representations, and (2) learn continu-ous distributed vector representation for word andsense jointly.The basic idea is that, the tasks of word senserepresentation (WSR) and word sense disam-biguation (WSD) can benefit from each other: (1)high-quality WSR will capture rich semantic andsyntactic information of words and senses, whichshould be helpful for WSD; (2) high-quality WS-D will provide reliable disambiguated corpora forlearning better sense representations.By utilizing these knowledge bases, the prob-lem mentioned above can be overcome:?
The number of senses of a word can be de-cided by the expert annotators or web users.?
When a new sense appears, our model can beeasily applied to obtain a new sense represen-tation.?
Every sense vector has a corresponding sensein these knowledge bases.We conduct experiments to investigate the per-formance of our model for both WSR and WS-D. We evaluate the performance of WSR using acontextual word similarity task, and results showthat out model can significantly improve the cor-relation with human judgments compared to base-lines.
We further evaluate the performance onboth domain-specific WSD and coarse-grained all-words WSD, and results show that our modelyields performance competitive with state-of-the-art supervised approaches.2 MethodologyWe describe our method as a 3-stage process:1.
Initializing word vectors and sense vectors.Given large amounts of text data, we first usethe Skip-gram model (Mikolov et al., 2013),a neural network based language model, tolearn word vectors.
Then, we assign vectorrepresentations for senses based on their def-initions (e.g, glosses in WordNet).2.
Performing word sense disambiguation.Given word vectors and sense vectors, wepropose two simple and efficient WSD algo-rithms to obtain more relevant occurrencesfor each sense.3.
Learning sense vectors from relevant oc-currences.
Based on the relevant occur-rences of ambiguous words, we modify thetraining objective of Skip-gram to learn wordvectors and sense vectors jointly.
Then, weobtain the sense vectors directly from themodel.Before illustrating the three stages of ourmethod in Sections 2.2, 2.3 and 2.4, we brieflyintroduce our sense inventory, WordNet, in Sec-tion 2.1.
Note that, although our experiments willuse the WordNet sense inventory, our model is notlimited to this particular lexicon.
Other knowledgebases containing word sense distinctions and defi-nitions can also serve as input to our model.2.1 WordNetWordNet (Miller, 1995) is the most widely usedcomputational lexicon of English where a concep-t is represented as a synonym set, or synset.
Thewords in the same synset share a common mean-ing.
Each synset has a textual definition, or gloss.Table 1 shows the synsets and the correspondingglosses of the two common senses of bank.Before introducing the method in detail, we in-troduce the notations.
The unlabeled texts are de-noted as R, and the vocabulary of the texts is de-noted as W .
For a word w in W , wsiis the ithsense in WordNet WN.
Each sense wsihas a glossgloss(wsi) in WN.
The word embedding of w isdenoted as vec(w), and the sense embedding of itsith sense wsiis denoted as vec(wsi).2.2 Initializing Word Vectors and SenseVectorsInitializing word vectors.
First, we use Skip-gram to train the word vectors from large amountsof text data.
We choose Skip-gram for its sim-plicity and effectiveness.
The training objective ofSkip-gram is to train word vector representationsthat are good at predicting its context in the samesentence (Mikolov et al., 2013).More formally, given a sequence of trainingwords w1, w2, w3,...,wT, the objective of Skip-gram is to maximize the average log probability1TT?t=1(??k?
j?k, j 6=0log p(wt+ j|wt))(1)where k is the size of the training window.
Theinner summation spans from ?k to k to compute1026Sense Synset Glossbanks1(sloping land (especially the slope beside a body of water))bank ?they pulled the canoe up on the bank?
;?he sat on the bank of the river and watched the currents?banks2depository institution, (a financial institution that accepts deposits and channels thebank, money into lending activities)banking concern, ?he cashed a check at the bank?
;banking company ?that bank holds the mortgage on my home?Table 1: Example of a synset in WordNet.the log probability of correctly predicting the wordwt+ jgiven the word in the middle wt.
The outersummation covers all words in the training data.The prediction task is performed via softmax, amulticlass classifier.
There, we havep(wt+ j|wt) =exp(vec?
(wt+ j)>vec(wt))?Ww=1exp(vec?
(w)>vec(wt))(2)where vec(w) and vec?
(w) are the ?input?
and?output?
vector representations of w. This formu-lation is impractical because the cost of comput-ing p(wt+ j|wt) is proportional to W , which is oftenlarge( 105?107terms).Initializing sense vectors.
After learning theword vectors using the Skip-gram model, we ini-tialize the sense vectors based on the glosses ofsenses.
The basic idea of the sense vector initial-ization is to represent the sense by using the sim-ilar words in the gloss.
From the content wordsin the gloss, we select those words whose cosinesimilarities with the original word are larger thana similarity threshold ?
.
Formally, for each sensewsiin WN, we first define a candidate set fromgloss(wsi)cand(wsi) = {u|u ?
gloss(wsi),u 6= w,POS(u) ?CW,cos(vec(w),vec(u)) > ?}
(3)where POS(u) is the part-of-speech tagging of theword u and CW is the set of all possible part-of-speech tags that content words could have.
In thispaper, CW contains the following tags: noun, verb,adjective and adverb.Then the average of the word vectors incand(wsi) is used as the initialization value of thesense vector vec(wsi).vec(wsi) =1|cand(wsi)|?u?cand(wsi)vec(u) (4)For example, in WordNet, the gloss of the sensebanks1is ?sloping land (especially the slope besidea body of water)) they pulled the canoe up on thebank; he sat on the bank of the river and watchedthe currents?.
The gloss contains a definition ofthe sense and two examples of the sense.
Thecontent words and the cosine similarities with theword ?bank?
are listed as follows: (sloping, 0.12),(land, 0.21), (slope, 0.17), (body, 0.01), (water,0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06),(river, 0.43), (watch, -0.11), (currents, 0.01).
Ifthe threshold, ?
, is set to 0.05, then cand(banks1)is {sloping, land, slope, water, canoe, sat, riv-er}.
Then the average of the word vectors incand(banksi) is used as the initialization value ofvec(banksi).2.3 Performing Word Sense Disambiguation.One of the state-of-the-art WSD results can beobtained using exemplar models, i.e., the wordmeaning is modeled by using relevant occurrencesonly, rather than merging all of the occurrences in-to a single word vector (Erk and Pado, 2010).
In-spired by this idea, we perform word sense disam-biguation to obtain more relevant occurrences.Here, we perform knowledge-based word sensedisambiguation for training data on an all-wordssetting, i.e., we will disambiguate all of the con-tent words in a sentence.
Formally, the sentence Sis a sequence of words (w1,w2,...,wn), and we willidentify a mapping M from words to senses suchthat M(i) ?
SensesWN(wi), where SensesWN(wi) isthe set of senses encoded in the WN for word wi.For sentence S, there are?ni=1|SenseWN(wi)| pos-sible mapping answers, which are impractical tocompute.
Thus, we design two simple algorithms,L2R (left to right) algorithm and S2C (simple tocomplex) algorithm, for word sense disambigua-tion based on the sense vectors.The main difference between L2R and S2C is1027the order of words when performing word sensedisambiguation.
When given a sentence, the L2Ralgorithm disambiguates the words from left toright (the natural order of a sentence), whereas theS2C algorithm disambiguates the words with few-er senses first.
The main idea of S2C algorithmis that the words with fewer senses are easier todisambiguate, and the disambiguation result canbe helpful to disambiguate the words with moresenses.
Both of the algorithms have three steps:Context vector initialization.
Similar to the ini-tialization of sense vectors, we use the average ofall of the content words?
vectors in a sentence asthe initialization vector of context.vec(context) =1|cand(S)|?u?cand(S)vec(u) (5)where cand(S) is the set of content wordscand(S) = {u|u ?
S,POS(u) ?CW}.Ranking words.
For L2R, we do nothing in thisstep.
For S2C, we rank the words based on theascending order of |SensesWN(wi)|.Word sense disambiguation.
For both L2R andS2C, we denote the order of words as L and per-form word sense disambiguation according to L.First, we skip a word if the word is nota content word or the word is monosemous(|SensesWN(wi)| = 1).
Then, for each word inL, we can compute the cosine similarities be-tween the context vector and its sense vectors.
Wechoose the sense that yields the maximum cosinesimilarity as its disambiguation result.
If the s-core margin between the maximum and the sec-ond maximum is larger than the threshold ?
, weare confident with the disambiguation result of wiand then use the sense vector to replace the wordvector in the context vector.
Thus, we obtain amore accurate context vector for other words thatare still yet to be disambiguated.For example, given a sentence ?He sat on thebank of the lake?, we first explain how S2C work-s.
In the sentence, there are three content word-s, ?sat?, ?bank?
and ?lake?, to be disambiguated.First, the sum of the three word vectors is used asthe initialization of the context vector.
Then werank the words by |SensesWN(wi)|, in ascendingorder, that is, lake (3 senses), bank (10 senses), sat(10 senses).
We first disambiguate the word ?lake?based on the similarities between its sense vectorsand context vector.
If the score margin is largerbankinputprojectionoutput sat on the of the lakesit lake1 1Figure 1: The architecture of our model.
Thetraining objective of Skip-gram is to train wordvector representations that are not only good atpredicting its context words but are also good atpredicting its context words?
senses.
The centerword ?bank?
is used to predict not only its contextwords but also the sense ?sit1?
and ?lake1?.than the threshold ?
, then we are confident withthis disambiguation result and replace the wordvector with the sense vector to update the contex-t vector.
It would be helpful to disambiguate thenext word, ?bank?.
We repeat this process until allthree words are disambiguated.For L2R, the order of words to be disambiguat-ed will be ?sat?, ?bank?
and ?lake?.
In this time,when disambiguating ?bank?
(10 senses), we stilldon?t know the sense of ?lake?
(3 senses).2.4 Learning Sense Vectors from RelevantOccurrences.Based on the disambiguation result, we modify thetraining objective of Skip-gram and train the sensevectors directly from the large-scale corpus.
Ourtraining objective is to train the vector representa-tions that are not only good at predicting its con-text words but are also good at predicting its con-text words?
senses.
The architecture of our modelis shown in Figure 1.More formally, given the disambiguation resultM(w1), M(w2), M(w3),...,M(wT), the training ob-jective is modified to1TT?t=1(k?j=?klog{p(wt+ j|wt)p(M(wt+ j)|wt)})(6)where k is the size of the training window.
Theinner summation spans from ?k to k to computethe log probability of correctly predicting the wordwt+ jand the log probability of correctly predicting1028the sense M(wt+ j) given the word in the middlewt.
The outer summation covers all words in thetraining data.Because not all of the disambiguation results arecorrect, we only disambiguate the words that weare confident in.
Similar to step 3 of our WSDalgorithm, we only disambiguate words under thecondition that the score margin between the max-imum and the second maximum is larger than thescore margin threshold, ?
.We also use the softmax function to definep(wt+ j|wt) and p(M(wt+ j)|wt).
Then, we use hi-erarchical softmax (Morin and Bengio, 2005) togreatly reduce the computational complexity andlearn the sense vectors directly from the relevantoccurrences.3 ExperimentsIn this section, we first present the nearest neigh-bors of some words and their senses, showing thatour sense vectors can capture the semantics ofwords.
Then, we use three tasks to evaluate our u-nified model: a contextual word similarity task toevaluate our sense representations, and two stan-dard WSD tasks to evaluate our knowledge-basedWSD algorithm based on the sense vectors.
Ex-perimental results show that our model not onlyimproves the correlation with human judgmentson the contextual word similarity task but also out-performs state-of-the-art supervised WSD system-s on domain-specific datasets and competes withthem in a coarse-grained all-words setting.We choose Wikipedia as the corpus to trainthe word vectors because of its wide coverageof topics and words usages.
We use an EnglishWikipedia database dump from October 20132,which includes roughly 3 million articles and 1billion tokens.
We use Wikipedia Extractor3topreprocess the Wikipedia pages and only save thecontent of the articles.We use word2vec4to train Skip-gram.
We usethe default parameters of word2vec and the dimen-sion of the vector representations is 200.We use WordNet5as our sense inventory.
Thedatasets for different tasks are tagged with differ-ent versions of WordNet.
The version of WordNet2http://download.wikipedia.org.3The tool is available from http://medialab.di.unipi.it/wiki/Wikipedia_Extractor.4The code is available from https://code.google.com/p/word2vec/.5http://wordnet.princeton.edu/.Word or sense Nearest neighborsbank banks, IDBI, CitiBankbanks1river, slope, Sooesbanks2mortgage, lending, loansstar stars, stellar, trekstars1photosphere, radiation,gamma-raysstars2someone, skilled, genuinelyplant plants, glavaticevo, herbaceousplants1factories, machinery,manufacturingplants2locomotion, organism,organismsTable 2: Nearest neighbors of word vectors andsense vectors learned by our model based on co-sine similarity.
The subscript of each sense labelcorresponds to the index of the sense in Word-Net.
For example, banks2is the second sense ofthe word bank in WordNet.is 1.7 for the domain-specific WSD task and 2.1for the coarse-grained WSD task.We use the S2C algorithm described in Section2.3 to perform word sense disambiguation to ob-tain more relevant occurrences for each sense.
Wecompare S2C and L2R on the coarse-grained WS-D task in a all-words setting.The experimental results of our model are ob-tained by setting the similarity threshold as ?
= 0and the score margin threshold as ?
= 0.1.
The in-fluence of parameters on our model can be foundin Section 3.5.3.1 Examples for Sense VectorsTable 2 shows the nearest neighbors of word vec-tors and sense vectors based on cosine similari-ty.
We see that our sense representations can i-dentify different meanings of a word, allowing ourmodel to capture more semantic and syntactic re-lationships between words and senses.
Note thateach sense vector in our model corresponds to asense in WordNet; thus, our sense vectors can beused to perform knowledge-based word sense dis-ambiguation, whereas the vectors of cluster-basedmodels cannot.3.2 Contextual Word SimilarityExperimental setting.
A standard dataset for e-valuating a vector-space model is the WordSim-353 dataset (Finkelstein et al., 2001), which con-1029Model ?
?100C&W-S 57.0Huang-S 58.6Huang-M AvgSim 62.8Huang-M AvgSimC 65.7Our Model-S 64.2Our Model-M AvgSim 66.2Our Model-M AvgSimC 68.9Table 3: Spearman?s ?
on the SCWS dataset.
OurModel-S uses one representation per word to com-pute similarities, while Our Model-M uses onerepresentation per sense to compute similarities.AvgSim calculates the similarity with each sensecontributing equally, while AvgSimC weighs thesense according to the probability of the wordchoosing that sense in context c.sists of 353 pairs of nouns.
However, each pair ofnouns in WordSim-353 is presented without con-text.
This is problematic because the meaningsof homonymous and polysemous words dependhighly on the words?
contexts.
Thus we choose theStanford?s Contextual Word Similarities (SCWS)dataset from (Huang et al., 2012)6.
The SCWSdataset contains 2003 pairs of words and each pairis associated with 10 human judgments on similar-ity on a scale from 0 to 10.
In the SCWS dataset,each word in a pair has a sentential context.In our experiments, the similarity between apair of words (w, w?)
is computed as follows:AvgSimC(w,w?)
=1MNM?i=1N?j=1p(i|w,c)p( j|w?,c?
)d(vec(wsi),vec(w?sj)) (7)where p(i|w,c) is the likelihood that word wchooses its ith sense given context c.
d(vec,vec?
)is a function computing the similarity between twovectors, and here we use cosine similarity.Results and discussion.
For evaluation, wecompute the Spearman correlation between amodel?s computed similarity scores and humanjudgements.
Table 3 shows our results com-pared to previous methods, including (Collobertand Weston, 2008)?s language model (C&W), andHuang?s model which utilize the global contextand multi-prototype to improve the word represen-tations.6The dataset can be downloaded at http://ai.stanford.edu/?ehhuang/.From Table 3, we observe that:?
Our single-vector version outperformsHuang?s single-vector version.
This indi-cates that, by training the word vectors andsense vectors jointly, our model can bettercapture the semantic relationships betweenwords and senses.?
With one representation per sense, our mod-el can outperform the single-vector versionwithout using context (66.2 vs.
64.2).?
Our model obtains the best performance(68.9) by using AvgSimC, which takes con-text into account.3.3 Domain-Specific WSDExperimental setting.
We use Wikipedia astraining data because of its wide coverage for spe-cific domains.
To test our performance on do-main word sense disambiguation, we evaluatedour system on the dataset published in (Koelinget al., 2005).
This dataset consists of examplesretrieved from the Sports and Finance sections ofthe Reuters corpus.
41 words related to the Sportsand Finance domains were selected, with an aver-age polysemy of 6.7 senses, ranging from 2 to 13senses.Approximately 100 examples for each wordwere annotated with senses from WordNet v.1.7by three reviewers, yielding an inter-tagger agree-ment of 65%.
(Koeling et al., 2005) did not clarifyhow to select the ?correct?
sense for each word, sowe followed the work of (Agirre et al., 2009) and,used the sense chosen by the majority of taggersas the correct answer.Baseline methods.
As a baseline, we use themost frequent WordNet sense (MFS), as well asa random sense assignment.
We also compare ourresults with four systems7: Static PageRank (A-girre et al., 2009), the k nearest neighbor algorith-m (k-NN), Degree (Navigli and Lapata, 2010) andPersonalized PageRank (Agirre et al., 2009).Static PageRank applies traditional PageRankover the semantic graph based on WordNet andobtains a context-independent ranking of wordsenses.k-NN is a widely used classification method,where neighbors are the k labeled examples most7We compare only with those systems performing token-based WSD, i.e., disambiguating each instance of a targetword separately.1030AlgorithmSports FinanceRecall RecallRandom BL 19.5 19.6MFS BL 19.6 37.1k-NN 30.3 43.4Static PR 20.1 39.6Personalized PR 35.6 46.9Degree 42.0 47.8Our Model 57.3 60.6Table 4: Performance on the Sports and Financesections of the dataset from (Koeling et al., 2005).similar to the test example.
The k-NN system istrained on SemCor (Miller et al., 1993), the largestpublicly available annotated corpus.Degree and Personalized PageRank are state-of-the-art systems that exploit WordNet to builda semantic graph and exploit the structural proper-ties of the graph in order to choose the appropriatesenses of words in context.Results and discussion.
Similar to other workon this dataset, we use recall (the ratio of correctsense labels to the total labels in the gold standard)as our evaluation measure.
Table 4 shows the re-sults of different WSD systems on the dataset, andthe best results are shown in bold.
The differencesbetween other results and the best result in eachcolumn of the table are statistically significant atp < 0.05.The results show that:?
Our model outperforms k-NN on the t-wo domains by a large margin, support-ing the findings from (Agirre et al., 2009)that knowledge-based systems perform bet-ter than supervised systems when evaluatedacross different domains.?
Our model also achieves better results thanthe state-of-the-art system (+15.3% recall onSports and +12.8% recall on Finance againstDegree).
The reason for this is that whendealing with short sentences or context wordsthat are not in WordNet, our model can stillcompute similarity based on the context vec-tor and sense vectors, whereas Degree willhave difficulty building the semantic graph.?
Moreover, our model achieves the best per-formance by only using the unlabeled text da-ta and the definitions of senses, whereas otherAlgorithm TypeNouns only All wordsF1F1Random BL U 63.5 62.7MFS BL Semi 77.4 78.9SUSSX-FR Semi 81.1 77.0NUS-PT S 82.3 82.5SSI Semi 84.1 83.2Degree Semi 85.5 81.7Our ModelL2RU 79.2 73.9Our ModelS2CU 81.6 75.8Our ModelL2RSemi 82.5 79.6Our ModelS2CSemi 85.3 82.6Table 5: Performance on Semeval-2007 coarse-grained all-words WSD.
In the type column,U, Semi and S stand for unsupervised, semi-supervised and supervised, respectively.
The dif-ferences between the results in bold in each col-umn of the table are not statistically significant atp < 0.05.methods rely greatly on high-quality seman-tic relations or annotated data, which are hardto acquire.3.4 Coarse-grained WSDExperimental setting.
We also evaluate ourWSD model on the Semeval-2007 coarse-grainedall-words WSD task (Navigli et al., 2007).
Thereare multiple reasons that we perform experimentsin a coarse-grained setting: first, it has been ar-gued that the fine granularity of WordNet is oneof the main obstacles to accurate WSD (cf.
thediscussion in (Navigli, 2009)); second, the train-ing corpus of word representations is Wikipedia,which is quite different from WordNet.Baseline methods.
We compare our model withthe best unsupervised system SUSSX-FR (Koel-ing and McCarthy, 2007), and the best supervisedsystem, NUS-PT (Chan et al., 2007), participat-ing in the Semeval-2007 coarse-grained all-wordstask.
We also compare with SSI (Navigli and Ve-lardi, 2005) and the state-of-the-art system De-gree (Navigli and Lapata, 2010).
We use differentbaseline methods for the two WSD tasks becausewe want to compare our model with the state-of-the-art systems that are applicable to differentdatasets and show that our WSD method can per-form robustly in these different WSD tasks.1031Results and discussion.
We report our results interms of F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al., 2007).Table 5 reports the results for nouns (1,108 words)and all words (2,269 words).
The difference be-tween unsupervised and semi-supervised methodsis whether the method uses MFS as a back-off s-trategy.We can see that the S2C algorithm outperformsthe L2R algorithm no matter on the nouns subsetor on the entire set.
This indicates that words withfewer senses are easier to disambiguate, and it canbe helpful to disambiguate the words with moresenses.On the nouns subset, our model yields compa-rable performance to SSI and Degree, and it out-performs NUS-PT and SUSSX-FR.
Moreover, ourunsupervised WSD method (S2C) beats the MF-S baseline, which is notably a difficult competitorfor knowledge-based systems.On the entire set, our semi-supervised model issignificantly better than SUSSX-FR, and it is com-parable with SSI and Degree.
In contrast to SSI,our model is simple and does not rely on a cost-ly annotation effort to engineer the set of semanticrelations.Overall, our model achieves state-of-the-art per-formance on the Semeval-2007 coarse-grained all-words dataset compared to other systems, with asimple WSD algorithm that only relies on a large-scale unlabeled text corpora and a sense inventory.3.5 Parameter InfluenceWe investigate the influence of parameters on ourmodel with coarse-grained all-words WSD task.The parameters include the similarity threshold, ?
,and the score margin threshold, ?
.Similarity threshold.
In Table 6, we show theperformance of domain WSD when the similari-ty threshold ?
ranges from ?0.1 to 0.3.
The co-sine similarity interval is [-1, 1], and we focus onthe performance in the interval [-0.1, 0.3] for tworeasons: first, no words are removed from glosseswhen ?
< ?0.1; second, nearly half of the word-s are removed when ?
> 0.3 and the performancedrops significantly for the WSD task.
From table6, we can see that our model achieves the best per-formance when ?
= 0.0.Score margin threshold.
In Table 7, we showthe performance on the coarse-grained all-wordsParameter Nouns only All words?
=?0.10 79.8 74.3?
=?0.05 81.0 74.6?
= 0.00 81.6 75.8?
= 0.05 81.3 75.4?
= 0.10 80.8 75.2?
= 0.15 80.0 75.0?
= 0.20 77.1 73.3?
= 0.30 75.0 72.1Table 6: Evaluation results on the coarse-grainedall-words WSD when the similarity threshold ?ranges from ?0.1 to 0.3.Parameter Nouns only All words?
= 0.00 78.2 72.9?
= 0.05 79.5 74.5?
= 0.10 81.6 75.8?
= 0.15 81.2 74.7?
= 0.20 80.9 75.1?
= 0.25 80.2 74.8?
= 0.30 80.4 74.9Table 7: Evaluation results on the coarse-grainedall-words WSD when the score margin threshold?
ranges from 0.0 to 0.3.WSD when the score margin threshold ?
rangesfrom 0.0 to 0.3.
When ?
= 0.0, we use everydisambiguation result to update the context vec-tor.
When ?
6= 0, we only use the confident disam-biguation results to update the context vector if thescore margin is larger than ?
.
Our model achievesthe best performance when ?
= 0.1.4 Related Work4.1 Word RepresentationsDistributed representations for words were pro-posed in (Rumelhart et al., 1986) and have beensuccessfully used in language models (Bengio etal., 2006; Mnih and Hinton, 2008) and many nat-ural language processing tasks, such as word rep-resentation learning (Mikolov, 2012), named enti-ty recognition (Turian et al., 2010), disambigua-tion (Collobert et al., 2011), parsing and tag-ging (Socher et al., 2011; Socher et al., 2013).They are very useful in NLP tasks because theycan be used as inputs to learning algorithms or asextra word features in NLP systems.
Hence, manyNLP applications, such as keyword extraction (Li-1032u et al., 2010; Liu et al., 2011b; Liu et al., 2012),social tag suggestion (Liu et al., 2011a) and textclassification (Baker and McCallum, 1998), mayalso potentially benefit from distributed word rep-resentation.
The main advantage is that the rep-resentations of similar words are close in vectorspace, which makes generalization to novel pat-terns easier and model estimation more robust.Word representations are hard to train due to thecomputational complexity.
Recently, (Mikolov etal., 2013) proposed two particular models, Skip-gram and CBOW, to learn word representations inlarge amounts of text data.
The training objectiveof the CBOW model is to combine the representa-tions of the surrounding words to predict the wordin the middle, while the Skip-gram model?s is tolearn word representations that are good at predict-ing its context in the same sentence (Mikolov etal., 2013).
Our paper uses the model architectureof Skip-gram.Most of the previous vector-space models useone representation per word.
This is problematicbecause many words have multiple senses.
Themulti-prototype approach has been widely stud-ied.
(Reisinger and Mooney, 2010) proposed themulti-prototype vector-space model.
(Huang etal., 2012) used the multi-prototype models to learnthe vector for different senses of a word.
All ofthese models use the clustering of contexts as aword sense and can not be directly used in wordsense disambiguation.After our paper was submitted, we perceive thefollowing recent advances: (Tian et al., 2014) pro-posed a probabilistic model for multi-prototypeword representation.
(Guo et al., 2014) exploredbilingual resources to learn sense-specific wordrepresentation.
(Neelakantan et al., 2014) pro-posed an efficient non-parametric model for multi-prototype word representation.4.2 Knowledge-based WSDThe objective of word sense disambiguation (WS-D) is to computationally identify the meaning ofwords in context (Navigli, 2009).
There are t-wo approaches of WSD that assign meaning ofwords from a fixed sense inventory, supervised andknowledge-based methods.
Supervised approach-es require large labeled training sets, which aretime consuming to create.
In this paper, we on-ly focus on knowledge-based word sense disam-biguation.Knowledge-based approaches exploit knowl-edge resources (such as dictionaries, thesauri, on-tologies, collocations, etc.)
to determine thesenses of words in context.
However, it hasbeen shown in (Cuadros and Rigau, 2006) thatthe amount of lexical and semantic informationcontained in such resources is typically insuf-ficient for high-performance WSD.
Much workhas been presented to automatically extend ex-isting resources, including automatically linkingWikipedia to WordNet to include full use of thefirst WordNet sense heuristic (Suchanek et al.,2008), a graph-based mapping of Wikipedia cat-egories to WordNet synsets (Ponzetto and Nav-igli, 2009), and automatically mapping Wikipediapages to WordNet synsets (Ponzetto and Navigli,2010).It was recently shown that word representation-s can capture semantic and syntactic informationbetween words (Mikolov et al., 2013).
Some re-searchers tried to incorporate WordNet senses in aneural model to learn better word representation-s (Bordes et al., 2011).
In this paper, we have pro-posed a unified method for word sense representa-tion and disambiguation to extend the informationcontained in the vector representations to the ex-isting resources.
Our method only requires a largeamount of unlabeled text to train sense representa-tions and a dictionary to provide the definitions ofword meanings, which makes it easily applicableto other resources.5 ConclusionIn this paper, we present a unified model for wordsense representation and disambiguation that us-es one representation per sense.
Experimental re-sults show that our model improves the perfor-mance of contextual word similarity compared toexisting WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD,and achieves competitive performance on coarse-grained all-words WSD.
Our model only requireslarge-scale unlabeled text corpora and a sense in-ventory for WSD, thus it can be easily applied toother corpora and tasks.There are still several open problems thatshould be investigated further:1.
Because the senses of words change overtime (new senses appear), we will incorpo-rate cluster-based methods in our model tofind senses that are not in the sense inventory.10332.
We can explore other WSD methods basedon sense vectors to improve our performance.For example, (Li et al., 2010) used LDA toperform data-driven WSD in a manner simi-lar to our model.
We may integrate the advan-tages of these models and our model togetherto build a more powerful WSD system.3.
To learn better sense vectors, we can exploitthe semantic relations (such as the hypernymand hyponym relations defined in WordNet)between senses in our model.AcknowledgmentsThis work is supported by National Key Ba-sic Research Program of China (973 Program2014CB340500) and National Natural ScienceFoundation of China (NSFC 61133012).ReferencesEneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, andInformatika Fakultatea.
2009.
Knowledge-basedwsd and specific domains: Performing better thangeneric supervised wsd.
In Proceedings of IJCAI,pages 1501?1506.L Douglas Baker and Andrew Kachites McCallum.1998.
Distributional clustering of words for textclassification.
In Proceedings of SIGIR, pages 96?103.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
In In-novations in Machine Learning, pages 137?186.Antoine Bordes, Jason Weston, Ronan Collobert,Yoshua Bengio, et al.
2011.
Learning structuredembeddings of knowledge bases.
In Proceedings ofAAAI, pages 301?306.Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong.
2007.Nus-pt: exploiting parallel texts for word sense dis-ambiguation in the english all-words tasks.
In Pro-ceedings of SemEval, pages 253?256.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
JMLR, 12:2493?2537.Montse Cuadros and German Rigau.
2006.
Qualityassessment of large scale knowledge resources.
InProceedings of EMNLP, pages 534?541.Katrin Erk and Sebastian Pado.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof ACL, pages 92?97.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-hud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The con-cept revisited.
In Proceedings of WWW, pages 406?414.Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Li-u.
2014.
Learning sense-specific word embeddingsby exploiting bilingual resources.
In Proceedings ofCOLING, pages 497?507.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of ACL, pages 873?882.Rob Koeling and Diana McCarthy.
2007.
Sussx: Ws-d using automatically acquired predominant senses.In Proceedings of SemEval, pages 314?317.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proceedings of HLT-EMNLP, pages 419?426.Linlin Li, Benjamin Roth, and Caroline Sporleder.2010.
Topic models for word sense disambiguationand token-based idiom detection.
In Proceedings ofACL, pages 1138?1147.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, andMaosong Sun.
2010.
Automatic keyphrase extrac-tion via topic decomposition.
In Proceedings ofEMNLP, pages 366?376.Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.2011a.
A simple word trigger method for social tagsuggestion.
In Proceedings of EMNLP, pages 1577?1588.Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, andMaosong Sun.
2011b.
Automatic keyphrase extrac-tion by bridging vocabulary gap.
In Proceedings ofCoNLL, pages 135?144.Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.
2012.Mining the interests of chinese microbloggers viakeyword extraction.
Frontiers of Computer Science,6(1):76?87.Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informationretrieval.
Cambridge University Press Cambridge.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
Proceedings of ICLR.Tomas Mikolov.
2012.
Statistical Language Model-s Based on Neural Networks.
Ph.D. thesis, Ph.
D.thesis, Brno University of Technology.1034George A Miller, Claudia Leacock, Randee Tengi, andRoss T Bunker.
1993.
A semantic concordance.
InProceedings of the workshop on HLT, pages 303?308.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Andriy Mnih and Geoffrey E Hinton.
2008.
A s-calable hierarchical distributed language model.
InProceedings of NIPS, pages 1081?1088.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on artifi-cial intelligence and statistics, pages 246?252.Roberto Navigli and Mirella Lapata.
2010.
An ex-perimental study of graph connectivity for unsu-pervised word sense disambiguation.
IEEE PAMI,32(4):678?692.Roberto Navigli and Paola Velardi.
2005.
Structuralsemantic interconnections: a knowledge-based ap-proach to word sense disambiguation.
IEEE PAMI,27(7):1075?1086.Roberto Navigli, Kenneth C Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grained english all-words task.
In Proceedings ofSemEval, pages 30?35.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
CSUR, 41(2):10.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of EMNLP.Simone Paolo Ponzetto and Roberto Navigli.
2009.Large-scale taxonomy mapping for restructuringand integrating wikipedia.
In Proceedings of IJCAI,volume 9, pages 2083?2088.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of ACL, pages1522?1531.Joseph Reisinger and Raymond J Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Proceedings of HLT-NAACL, pages 109?117.David E Rumelhart, Geoffrey E Hintont, and Ronald JWilliams.
1986.
Learning representations by back-propagating errors.
Nature, 323(6088):533?536.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
CSUR, 34(1):1?47.Richard Socher, Cliff C Lin, Andrew Ng, and ChrisManning.
2011.
Parsing natural scenes and naturallanguage with recursive neural networks.
In Pro-ceedings of ICML, pages 129?136.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of ACL.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
Yago: A large ontology fromwikipedia and wordnet.
Web Semantics: Sci-ence, Services and Agents on the World Wide Web,6(3):203?217.Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,Enhong Chen, and Tie-Yan Liu.
2014.
A probabilis-tic model for learning multi-prototype word embed-dings.
In Proceedings of COLING, pages 151?160.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of A-CL, pages 384?394.1035
