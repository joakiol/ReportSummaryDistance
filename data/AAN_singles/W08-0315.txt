Proceedings of the Third Workshop on Statistical Machine Translation, pages 127?130,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsThe TALP-UPC Ngram-based statistical machine translation system forACL-WMT 2008Maxim Khalilov, Adolfo Hern?ndez H., Marta R. Costa-juss?,Josep M. Crego, Carlos A. Henr?quez Q., Patrik Lambert,Jos?
A. R. Fonollosa, Jos?
B. Mari?o and Rafael E. BanchsDepartment of Signal Theory and CommunicationsTALP Research Center (UPC)Barcelona 08034, Spain(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.eduAbstractThis paper reports on the participation of the TALPResearch Center of the UPC (Universitat Polit?cnicade Catalunya) to the ACL WMT 2008 evaluationcampaign.This year?s system is the evolution of the one we em-ployed for the 2007 campaign.
Main updates andextensions involve linguistically motivated word re-ordering based on the reordering patterns technique.In addition, this system introduces a target languagemodel, based on linguistic classes (Part-of-Speech),morphology reduction for an inflectional language(Spanish) and an improved optimization procedure.Results obtained over the development and test setson Spanish to English (and the other way round)translations for both the traditional Europarl anda challenging News stories tasks are analyzed andcommented.1 IntroductionOver the past few years, the Statistical Machine Transla-tion (SMT) group of the TALP-UPC has been develop-ing the Ngram-based SMT system (Mari?o et al, 2006).In previous evaluation campaigns the Ngram-based ap-proach has proved to be comparable with the state-of-the-art phrase-based systems, as shown in Koehn andMonz(2006), Callison-Burch et al (2007).We present a summary of the TALP-UPC Ngram-based SMT system used for this shared task.
We dis-cuss the system configuration and novel features, namelylinguistically motivated reordering technique, which isapplied on the decoding step.
Additionally, the reorder-ing procedure is supported by an Ngram language model(LM) of reordered source Part-of-Speech tags (POS).In this year?s evaluation we submitted systems forSpanish-English and English-Spanish language pairs forthe traditional (Europarl) and challenging (News) tasks.In each case, we used only the supplied data for each lan-guage pair for models training and optimization.This paper is organized as follows.
Section 2 brieflyoutlines the 2008 system, including tuple definition andextraction, translation model and additional feature mod-els, decoding tool and optimization procedure.
Section 3describes the word reordering problem and presents theproposed technique of reordering patterns learning andapplication.
Later on, Section 4 reports on the experi-mental setups of the WMT 2008 evaluation campaign.
InSection 5 we sum up the main conclusions from the pa-per.2 Ngram-based SMT SystemOur translation system implements a log-linear model inwhich a foreign language sentence fJ1 = f1, f2, ..., fJis translated into another language eI1 = f1, f2, ..., eI bysearching for the translation hypothesis e?I1 maximizing alog-linear combination of several feature models (Brownet al, 1990):e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}where the feature functions hm refer to the system modelsand the set of ?m refers to the weights corresponding tothese models.The core part of the system constructed in that wayis a translation model, which is based on bilingual n-grams.
It actually constitutes an Ngram-based LM ofbilingual units (called tuples), which approximates thejoint probability between the languages under consider-ation.
The procedure of tuples extraction from a word-to-word alignment according to certain constraints is ex-plained in detail in Mari?o et al (2006).The Ngram-based approach differs from the phrase-based SMT mainly by distinct representating of the bilin-gual units defined by word alignment and using a higher127order HMM of the translation process.
While regularphrase-based SMT considers context only for phrase re-ordering but not for translation, the N-gram based ap-proach conditions translation decisions on previous trans-lation decisions.The TALP-UPC 2008 translation system, besides thebilingual translation model, which consists of a 4-gramLM of tuples with Kneser-Ney discounting (estimatedwith SRI Language Modeling Toolkit1), implements alog-linear combination of five additional feature models:?
a target language model (a 4-gram model of words,estimated with Kneser-Ney smoothing);?
a POS target language model (a 4-gram model oftags with Good-Turing discounting (TPOS));?
a word bonus model, which is used to compensatethe system?s preference for short output sentences;?
a source-to-target lexicon model and a target-to-source lexicon model, these models use word-to-word IBM Model 1 probabilities (Och and Ney,2004) to estimate the lexical weights for each tuplein the translation table.Decisions on the particular LM configuration andsmoothing technique were taken on the minimal-perplexity and maximal-BLEU bases.The decoder (called MARIE), an open source tool2,implementing a beam search strategy with distortion ca-pabilities was used in the translation system.Given the development set and references, the log-linear combination of weights was adjusted using a sim-plex optimization method (with the optimization criteriaof the highest BLEU score ) and an n-best re-rankingjust as described in http://www.statmt.org/jhuws/.
Thisstrategy allows for a faster and more efficient adjustmentof model weights by means of a double-loop optimiza-tion, which provides significant reduction of the numberof translations that should be carried out.3 Reordering frameworkFor a great number of translation tasks a certain reorder-ing strategy is required.
This is especially importantwhen the translation is performed between pairs of lan-guages with non-monotonic word order.
There are var-ious types of distortion models, simplifying bilingualtranslation.
In our system we use an extended monotonereordering model based on automatically learned reorder-ing rules.
A detailed description can be found in Cregoand Mari?o (2006).1http://www.speech.sri.com/projects/srilm/2http://gps-tsc.upc.es/veu/soft/soft/marie/Apart from that, tuples were extracted by an unfold-ing technique: this means that the tuples are broken intosmaller tuples, and these are sequenced in the order of thetarget words.3.1 Reordering patternsWord movements are realized according to the reorderingrewrite rules, which have the form of:t1, ..., tn 7?
i1, ..., inwhere t1, ..., tn is a sequence of POS tags (relating asequence of source words), and i1, ..., in indicates whichorder of the source words generate monotonically the tar-get words.Patterns are extracted in training from the crossed linksfound in the word alignment, in other words, found intranslation tuples (as no word within a tuple can be linkedto a word out of it (Crego and Mari?o, 2006)).Having all the instances of rewrite patterns, a score foreach pattern on the basis of relative frequency is calcu-lated as shown below:p(t1, ..., tn 7?
i1, ..., in) =N(t1, ..., tn 7?
i1, ..., in)NN(t1, ..., tn)3.2 Search graph extension and source POS modelThe monotone search graph is extended with reorderingsfollowing the patterns found in training.
Once the searchgraph is built, the decoder traverses the graph looking forthe best translation.
Hence, the winning hypothesis iscomputed using all the available information (the wholeSMT models).Figure 1: Search graph extension.
NC, CC and AQ stand re-spectively for name, conjunction and adjective.The procedure identifies first the sequences of wordsin the input sentence that match any available pattern.Then, each of the matchings implies the addition of an arcinto the search graph (encoding the reordering learned inthe pattern).
However, this addition of a new arc is not128Task BL BL+SPOSEuroparl News Europarl Newses2en 32.79 36.09 32.88 36.36en2es 32.05 33.91 32.10 33.63Table 1: BLEU comparison demonstrating the impact of thesource-side POS tags model.performed if a translation unit with the same source-sidewords already exists in the training.
Figure 1 shows howtwo rewrite rules applied over an input sentence extendthe search graph given the reordering patterns that matchthe source POS tag sequence.The reordering strategy is additionally supported bya 4-gram language model (estimated with Good-Turingsmoothing) of reordered source POS tags (SPOS).
Intraining, POS tags are reordered according with the ex-tracted reordering patterns and word-to-word links.
Theresulting sequence of source POS tags is used to train theNgram LM.Table 1 presents the effect of the source POS LM in-troduction to the reordering module of the Ngram-basedSMT.
As it can be seen, the impactya le h of the source-side POS LM is minimal, however we decided to considerthe model aiming at improving it in future.
The reportedresults are related to the Europarl and News Commen-tary (News) development sets.
BLEU calculation is caseinsensitive and insensitive to tokenization.
BL (baseline)refers to the presented Ngram-based system consideringall the features, apart from the target and source POSmodels.4 WMT 2008 Evaluation Framework4.1 CorpusAn extraction of the official transcriptions of the 3rd re-lease of the European Parliament Plenary Sessions3 wasprovided for the ACL WMT 2008 shared translation task.About 40 times smaller corpus from news domain (calledNews Commentary) was also available.
For both tasks,our training corpus was the catenation of the Europarl andNews Commentary corpora.TALP UPC participated in the constraint to theprovided training data track for Spanish-English andEnglish-Spanish translation tasks.
We used the sametraining material for the traditional and challenging tasks,while the development sets used to tune the system weredistinct (2000 sentences for Europarl task and 1057for News Commentary, one reference translation foreach of them).
A brief training and development corporastatistics is presented in Table 2.3http://www.statmt.org/wmt08/shared-task.htmlSpanish EnglishTrainSentences 1.3 M 1.3 MWords 38.2 M 35.8 KVocabulary 156 K 120 KDevelopment EuroparlSentences 2000 2000Words 61.8 K 58.7 KVocabulary 8 K 6.5 KDevelopment News CommentarySentences 1057 1057Words 29.8 K 25.8 KVocabulary 5.4 K 4.9 KTable 2: Basic statistics of ACL WMT 2008 corpus.4.2 Processing detailsThe training data was preprocessed by using providedtools for tokenizing and filtering.POS tagging.
POS information for the source and thetarget languages was considered for both translation tasksthat we have participated.
The software tools availablefor performing POS-tagging were Freeling (Carreras etal., 2004) for Spanish and TnT (Brants, 2000) for En-glish.
The number of classes for English is 44, whileSpanish is considered as a more inflectional language,and the tag set contains 376 different tags.Word Alignment.
The word alignment is automati-cally computed by using GIZA++4(Och and Ney, 2000)in both directions, which are symmetrized by using theunion operation.
Instead of aligning words themselves,stems are used for aligning.
Afterwards case sensitivewords are recovered.Spanish Morphology Reduction.
We implemented amorphology reduction of the Spanish language as a pre-processing step.
As a consequence, training data sparse-ness due to Spanish morphology was reduced improvingthe performance of the overall translation system.
In par-ticular, the pronouns attached to the verb were separatedand contractions as del or al were splitted into de el ora el.
As a post-processing, in the En2Es direction weused a POS target LM as a feature (instead of the targetlanguage model based on classes) that allowed to recoverthe segmentations (de Gispert, 2006).4.3 Experiments and ResultsIn contrast to the last year?s system where statisticalclasses were used to train the target-side tags LM, thisyear we used linguistically motivated word classes4http://code.google.com/p/giza-pp/129Task BL+SPOS BL+SPOS+TPOS(UPC 2008)Europarl News Europarl Newses2en 32.88 36.36 32.89 36.31en2es 31.52 34.13 30.72 32.72en2es "clean"5 32.10 33.63 32.09 35.04Table 3: BLEU scores for Spanish-English and English-Spanish2008 development corpora (Europarl and News Commentary).Task UPC 2008Europarl Newses2en 32.80 19.61en2es 31.31 19.28en2es "clean"5 32.34 20.05Table 4: BLEU scores for official tests 2008.
(POS) which were considered to train the POS target LMand extract the reordering patterns.
Other characteristicsof this year?s system are:?
reordering patterns technique;?
source POS model, supporting word reordering;?
no LM interpolation.
For this year?s evaluation, wetrained two separate LMs for each domain-specificcorpus (i.e., Europarl and News Commentary tasks).It is important to mention that 2008 training material isidentical to the one provided for the 2007 shared transla-tion task.Table 3 presents the BLEU score obtained for the 2008development data sets and shows the impact of the target-side POS LM introduction, which can be characterized ashighly corpus- and language-dependent feature.
BL refersto the same system configuration as described in subsec-tion 3.2.
The computed BLEU scores are case insensitive,insensitive to tokenization and use one translation refer-ence.After submitting the systems we discovered a bug re-lated to incorrect implementation of the target LMs ofwords and tags for Spanish, it caused serious reductionof translation quality (1.4 BLEU points for developmentset in case of English-to-Spanish Europarl task and 2.3points in case of the corresponding News Commentarytask).
The last raw of table 3 (en2es "clean") repre-sents the results corresponding to the UPC 2008 post-evaluation system, while the previous one (en2es) refersto the "bugged" system submitted to the evaluation.The experiments presented in Table 4 correspond to the2008 test evaluation sets.5Corrected post-evaluation results (see subsection 4.3.
)5 ConclusionsIn this paper we introduced the TALP UPC Ngram-basedSMT system participating in the WMT08 evaluation.Apart from briefly summarizing the decoding and opti-mization processes, we have presented the feature mod-els that were taken into account, along with the bilingualNgram translation model.
A reordering strategy based onlinguistically-motivated reordering patterns to harmonizethe source and target word order has been presented inthe framework of the Ngram-based system.6 AcknowledgmentsThis work has been funded by the Spanish Governmentunder grant TEC2006-13964-C03 (AVIVAVOZ project).The authors want to thank Adri?
de Gispert (CambridgeUniversity) for his contribution to this work.ReferencesT.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
InProceedings of the 6th Applied Natural Language Processing(ANLP-2000).P.
Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,J.
D. Lafferty, R. Mercer, and P. S. Roossin.
1990.
A sta-tistical approach to machine translation.
Computational Lin-guistics, 16(2):79?85.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machine trans-lation.
In Proceedings of the ACL 2007 Workshop on Statis-tical and Hybrid methods for Machine Translation (WMT),pages 136?158.X.
Carreras, I. Chao, L.
Padr?, and M. Padr?.
2004.
Freeling:An open-source suite of language analyzers.
In Proceedingsof the 4th Int.
Conf.
on Language Resources and Evaluation(LREC?04).J.
M. Crego and J.
B. Mari?o.
2006.
Improving statistical MTby coupling reordering and decoding.
Machine Translation,20(3):199?215.A.
de Gispert.
2006.
Introducing linguistic knowledge intostatistical machine translation.
Ph.D. thesis, UniversitatPolit?cnica de Catalunya, December.P.
Koehn and C. Monz.
2006.
Manual and automatic eval-uation of machine translation between european languages.In Proceedings of the ACL 2006 Workshop on Statistical andHybrid methods for Machine Translation (WMT), pages 102?121.J.
B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-bert, J.
A. R. Fonollosa, and M. R. Costa-juss?.
2006.
N-gram based machine translation.
Computational Linguistics,32(4):527?549, December.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proceedings of the the 38th Annual Meetingon Association for Computational Linguistics (ACL), pages440?447.F.
Och and H. Ney.
2004.
The alignment template approach tostatistical machine translation.
30(4):417 ?
449, December.130
