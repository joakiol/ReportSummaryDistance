CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 89?96Manchester, August 2008An Incremental Bayesian Model for Learning Syntactic CategoriesChristopher Parisien, Afsaneh Fazly and Suzanne StevensonDepartment of Computer ScienceUniversity of TorontoToronto, ON, Canada[chris,afsaneh,suzanne]@cs.toronto.eduAbstractWe present an incremental Bayesian model forthe unsupervised learning of syntactic cate-gories from raw text.
The model draws infor-mation from the distributional cues of wordswithin an utterance, while explicitly bootstrap-ping its development on its own partially-learned knowledge of syntactic categories.Testing our model on actual child-directeddata, we demonstrate that it is robust to noise,learns reasonable categories, manages lexicalambiguity, and in general shows learning be-haviours similar to those observed in children.1 IntroductionAn important open problem in cognitive science andartificial intelligence is how children successfullylearn their native language despite the lack of explicittraining.
A key challenge in the early stages of lan-guage acquisition is to learn the notion of abstractsyntactic categories (e.g., nouns, verbs, or determin-ers), which is necessary for acquiring the syntacticstructure of language.
Indeed, children as young astwo years old show evidence of having acquired agood knowledge of some of these abstract categories(Olguin and Tomasello, 1993); by around six years ofage, they have learned almost all syntactic categories(Kemp et al, 2005).
Computational models help toelucidate the kinds of learning mechanisms that maybe capable of achieving this feat.
Such studies shedlight on the possible cognitive mechanisms at workin human language acquisition, and also on potentialmeans for unsupervised learning of complex linguis-tic knowledge in a computational system.Learning the syntactic categories of words hasbeen suggested to be based on the morphological andphonological properties of individual words, as wellc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.as on the distributional information about the con-texts in which they appear.
Several computationalmodels have been proposed that draw on one or moreof the above-mentioned properties in order to groupwords into discrete unlabeled categories.
Most ex-isting models only intend to show the relevance ofsuch properties to the acquisition of adult-like syn-tactic categories such as nouns and verbs; hence, theydo not necessarily incorporate the types of learningmechanisms used by children (Schu?tze, 1993; Red-ington et al, 1998; Clark, 2000; Mintz, 2003; Onnisand Christiansen, 2005).
For example, in contrast tothe above models, children acquire their knowledgeof syntactic categories incrementally, processing theutterances they hear one at a time.
Moreover, chil-dren appear to be sensitive to the fact that syntacticcategories are partially defined in terms of other cat-egories, e.g., nouns tend to follow determiners, andcan be modified by adjectives.We thus argue that a computational model shouldbe incremental, and should use more abstract cate-gory knowledge to help better identify syntactic cat-egories.
Incremental processing also allows a modelto incorporate its partially-learned knowledge of cat-egories, letting the model bootstrap its development.To our knowledge, the only incremental model ofcategory acquisition that also incorporates bootstrap-ping is that of Cartwright and Brent (1997).
Theirtemplate-based model, however, draws on very spe-cific linguistic constraints and rules to learn cate-gories.
Moreover, their model has difficulty with thevariability of natural language data.We address these shortcomings by developing anincremental probabilistic model of syntactic categoryacquisition that uses a domain-general learning algo-rithm.
The model also incorporates a bootstrappingmechanism, and learns syntactic categories by look-ing only at the general patterns of distributional sim-ilarity in the input.
Experiments performed on actual(noisy) child-directed data show that an explicit boot-strapping component improves the model?s ability to89learn adult-like categories.
The model?s learning tra-jectory resembles some relevant behaviours seen inchildren, and we also show that the categories thatour model learns can be successfully used in a lexicaldisambiguation task.2 Overview of the Computational ModelWe adapt a probabilistic incremental model of un-supervised categorization (i.e., clustering) proposedby Anderson (1991).
The original model has beenused to simulate human categorization in a varietyof domains, including the acquisition of verb argu-ment structure (Alishahi and Stevenson, 2008).
Ouradaptation of the model incorporates an explicit boot-strapping mechanism and a periodic merge of clus-ters, both facilitating generalization over input data.Here, we explain the input to our model (Section 2.1),the categorization model itself (Section 2.2), how weestimate probabilities to facilitate bootstrapping (Sec-tion 2.3), and our approach for merging similar clus-ters (Section 2.4).2.1 Input FramesWe aim to learn categories of words, and we do thisby looking for groups of similar word usages.
Thus,rather than categorizing a word alone, we categorize aword token with its context from that usage.
The ini-tial input to our model is a sequence of unannotatedutterances, that is, words separated by spaces.
Beforebeing categorized by the model, each word usage inthe input is processed to produce a frame that con-tains the word itself (the head word of the frame) andits distributional context (the two words before andafter it).
For example, in the utterance ?I gave Josiea present,?
when processing the head word Josie, wecreate the following frame for input to the categoriza-tion system:feature w?2w?1w0w+1w+2I gave Josie a presentwhere w0denotes the head word feature, and w?2,w?1, w+1, w+2are the context word features.
A con-text word may be ?null?
if there are fewer than twopreceding or following words in the utterance.2.2 CategorizationUsing Anderson?s (1991) incremental Bayesian cat-egorization algorithm, we learn clusters of word us-ages (i.e., the input frames) by drawing on the overallsimilarity of their features (here, the head word andthe context words).
The clusters themselves are notpredefined, but emerge from similarities in the input.More formally, for each successive frame F in theinput, processed in the order of the input words, weplace F into the most likely cluster, either from theK existing clusters, or a new one:BestCluster(F ) = argmaxkP (k|F ) (1)where k = 0, 1, ..,K, including the new clusterk = 0.
Using Bayes?
rule, and dropping P (F ) fromthe denominator, which is constant for all k, we find:P (k|F ) =P (k)P (F |k)P (F )?
P (k)P (F |k) (2)The prior probability of k, P (k), is given by:P (k) =cnk(1?
c) + cn, 1 ?
k ?
K (3)P (0) =1?
c(1?
c) + cn(4)where nkis the number of frames in k, and n isthe total number of frames observed at the time ofprocessing frame F .
Intuitively, a well-entrenched(large) cluster should be a more likely candidate forcategorization than a small one.
We reserve a smallprobability for creating a new cluster (Eq.
4).
As themodel processes more input overall, it should becomeless necessary to create new clusters to fit the data, soP (0) decreases with large n. In our experiments, weset c to a large value, 0.95, to further increase thelikelihood of using existing clusters.1The probability of a frame F given a cluster k,P (F |k), depends on the probabilities of the featuresin F given k. We assume that the individual fea-tures in a frame are conditionally independent givenk, hence:P (F |k) = PH(w0|k)?i?
{?2,?1,+1,+2}P (wi|k) (5)where PHis the head word probability, i.e., the like-lihood of seeing w0as a head word among the framesin cluster k. The context word probability P (wi|k) isthe likelihood of seeing wiin the ith context positionof the frames in cluster k. Next, we explain how weestimate each of these probabilities from the input.2.3 Probabilities and BootstrappingFor the head word probability PH(w0|k), we use asmoothed maximum likelihood estimate (i.e., the pro-portion of frames in cluster k with head word w0).For the context word probability P (wi|k), we canform two estimates.
The first is a simple maximumlikelihood estimate, which enforces a preference forcreating clusters of frames with the same contextwords.
That is, head words in the same cluster will1The prior P (k) is equivalent to the prior in a Dirichlet pro-cess mixture model (Sanborn et al, 2006), commonly used forsampling clusters of objects.90tend to share the same adjacent words.
We call thisword-based estimate Pword.Alternatively, we may consider the likelihood ofseeing not just the context word wi, but similar wordsin that position.
For example, if wican be used as anoun or a verb, then we want the likelihood of seeingother nouns or verbs in position i of frames in clusterk.
Here, we use the partial knowledge of the learnedclusters.
That is, we look over all existing clustersk?, estimate the probability that wiis the head wordof frames in k?, then estimate the probability of usingthe head words from those other clusters in position iin cluster k. We refer to this category-based estimateas Pcat:Pcat(wi|k) =?k?PH(wi|k?
)Pi(k?|k) (6)where Pi(k?|k) is the probability of finding usagesfrom cluster k?
in position i given cluster k. To sup-port this we record the categorization decisions themodel has made.
When we categorize the frames ofan utterance, we get a sequence of clusters for thatutterance, which gives additional information to sup-plement the frame.
We use this information to esti-mate Pi(k?|k) for future categorizations, again usinga smoothed maximum likelihood formula.In contrast to the Pwordestimate, the estimate inEq.
(6) prefers clusters of frames that use the samecategories as context.
While some of the results ofthese preferences will be the same, the latter approachlets the model make second-order inferences aboutcategories.
There may be no context words in com-mon between the current frame and a potential clus-ter, but if the context words in the cluster have beenfound to be distributionally similar to those in theframe, it may be a good cluster for that frame.We equally weight the word-based and thecategory-based estimates for P (wi|k) to get the like-lihood of a context word; that is:P (wi|k) ?12Pword(wi|k) +12Pcat(wi|k) (7)This way, the model sees an input utterance simulta-neously as a sequence of words and as a sequence ofcategories.
It is the Pcatcomponent, by using devel-oping category knowledge, that yields the bootstrap-ping abilities of our model.2.4 GeneralizationOur model relies heavily on the similarity of wordcontexts in order to find category structure.
In nat-ural language, these context features are highly vari-able, so it is difficult to draw consistent structure fromthe input in the early stages of an incremental model.When little information is available, there is a risk ofincorrectly generalizing, leading to clustering errorswhich may be difficult to overcome.
Children facea similar problem in early learning, but there is ev-idence that they may manage the problem by usingconservative strategies (see, e.g., Tomasello, 2000).Children may form specific hypotheses about eachword type, only later generalizing their knowledge tosimilar words.
Drawing on this observation, we formearly small clusters specific to the head word type,then later aid generalization by merging these smallerclusters.
By doing this, we ensure that the model onlygroups words of different types when there is suffi-cient evidence for their contextual similarity.Thus, when a cluster has been newly created, werequire that all frames put into the cluster share thesame head word type.2 When clusters are small, thisprevents the model from making potentially incorrectgeneralizations to different words.
Periodically, weevaluate a set of reasonably-sized clusters, and mergepairs of clusters that have highly similar contexts (seebelow for details).
If the model decides to merge twoclusters with different head word types?e.g., onecluster with all instances of dog, and another withcat?it has in effect made a decision to generalize.Intuitively, the model has learned that the contextsin the newly merged cluster apply to more than oneword type.
We now say that any word type could bea member of this cluster, if its context is sufficientlysimilar to that of the cluster.
Thus, when categoriz-ing a new word token (represented as a frame F ),our model can choose from among the clusters witha matching head word, and any of these ?generalized?clusters that contain mixed head words.Periodically, we look through a subset of the clus-ters to find similar pairs to merge.
In order to limitthe number of potential merges to consider, we onlyexamine pairs of clusters in which at least one clusterhas changed since the last check.
Thus, after pro-cessing every 100 frames of input, we consider theclusters used to hold those recent 100 frames as can-didates to be merged with another cluster.
We onlyconsider clusters of reasonable size (here, at least 10frames) as candidates for merging.
For each candi-date pair of clusters, k1and k2, we first evaluate aheuristic merge score that determines if the pair isappropriate to be merged, according to some localcriteria, i.e., the size and the contents of the candi-date clusters.
For each suggested merge (a pair whosemerge score exceeds a pre-determined threshold), wethen look at the set of all clusters, the global evidence,to decide whether to accept the merge.The merge score combines two factors: the en-trenchment of the two clusters, and the similarity of2However, a word type may exist in several clusters (e.g., fordistinct noun and verb usages), thus handling lexical ambiguity.91their context features.
The entrenchment measureidentifies clusters that contain enough frames to showa significant trend.
We take a sigmoid function overthe number of frames in the clusters, giving a softthreshold approaching 0 for small clusters and 1 forlarge clusters.
The similarity measure identifies pairsof clusters with similar distributions of word and cat-egory contexts.
Given two clusters, we measure thesymmetric Kullback-Leibler divergence for each cor-responding pair of context feature probabilities (in-cluding the category contexts Pi(k?|k), 8 pairs in to-tal), then place the sum of those measures on anothersigmoid function.
The merge score is the sum of theentrenchment and similarity measures.Since it is only a local measure, the merge score isnot sufficient on its own for determining if a mergeis appropriate.
For each suggested merge, we thusexamine the likelihood of a sample of input frames(here, the last 100 frames) under two states: the setof clusters before the merge, and the set of clusters ifthe merge is accepted.
We only accept a merge if itresults in an increase in the likelihood of the sampledata.
The likelihood of a sample set of frames, S ,over a set of clusters, K, is calculated as in:P (S) =?F?S?k?KP (F |k)P (k) (8)3 Evaluation MethodologyTo test our proposed model, we train it on a sample oflanguage representative of what children would hear,and evaluate its categorization abilities.
We havemultiple goals in this evaluation.
First, we determinethe model?s ability to discover adult-level syntacticcategories from the input.
Since this is intended to bea cognitively plausible learning model, we also com-pare the model?s qualitative learning behaviours withthose of children.
In the first experiment (Section 4),we compare the model?s categorization with a goldstandard of adult-level syntactic categories and exam-ine the effect of the bootstrapping component.
Thesecond experiment (Section 5) examines the model?sdevelopment of three specific parts of speech.
De-velopmental evidence suggests that children acquiredifferent syntactic categories at different ages, so wecompare the model?s learning rates of nouns, verbs,and adjectives.
Lastly, we examine our model?s abil-ity to handle lexically ambiguous words (Section 6).English word forms commonly belong to more thanone syntactic category, so we show how our modeluses context to disambiguate a word?s category.In all experiments, we train and test the model us-ing the Manchester corpus (Theakston et al, 2001)from the CHILDES database (MacWhinney, 2000).The corpus contains transcripts of mothers?
conver-sations with 12 British children between the ages of1;8 (years;months) and 3;0.
There are 34 one-hoursessions per child over the course of a year.
The agerange of the children roughly corresponds with theages at which children show the first evidence of syn-tactic categories.We extract the mothers?
speech from each of thetranscripts, then concatenate the input of all 12 chil-dren (all of Anne?s sessions, followed by all of Aran?ssessions, and so on).
We remove all punctuation.
Wespell out contractions, so that each token in the inputcorresponds to only one part-of-speech (PoS) label(noun, verb, etc.).
We also remove single-word ut-terances and utterances with a single repeated wordtype, since they contain no distributional informa-tion.
We randomly split the data into developmentand evaluation sets, each containing approximately683,000 tokens.
We use the development set to fine-tune the model parameters and develop the experi-ments, then use the evaluation set as a final test ofthe model.
We further split the development set intoabout 672,000 tokens (about 8,000 types) for trainingand 11,000 tokens (1,300 types) for validation.
Wesplit the evaluation set comparably, into training andtest subsets.
All reported results are for the evaluationset.
A conservative estimate suggests that childrenare exposed to at least 1.5 million words of child-directed speech annually (Redington et al, 1998), sothis corpus represents only a small portion of a child?savailable input.4 Experiment 1: Adult Categories4.1 MethodsWe use three separate versions of the categorizationmodel, in which we change the components used toestimate the context word probability, P (wi|k) (asused in Eq.
(5), Section 2.2).
In the word-basedmodel, we estimate the context probabilities usingonly the words in the context window, by directlyusing the maximum-likelihood Pwordestimate.
Thebootstrap model uses only the existing clusters to es-timate the probability, directly using the Pcatesti-mate from Eq.
(6).
The combination model uses anequally-weighted combination of the two probabili-ties, as presented in Eq.
(7).We run the model on the training set, categoriz-ing each of the resulting frames in order.
After every10,000 words of input, we evaluate the model?s cate-gorization performance on the test set.
We categorizeeach of the frames of the test set as usual, treating thetext as regular input.
So that the test set remains un-seen, the model does not record these categorizations.4.2 EvaluationThe PoS tags in the Manchester corpus are too fine-grained for our evaluation, so for our gold standard92we map them to the following 11 tags: noun, verb,auxiliary, adjective, adverb, determiner, conjunction,negation, preposition, infinitive to, and ?other.?
Whenwe evaluate the model?s categorization performance,we have two different sets of clusters of the words inthe test set: one set resulting from the gold standard,and another as a result of the model?s categorization.We compare these two clusterings, using the adjustedRand index (Hubert and Arabie, 1985), which mea-sures the overall agreement between two clusteringsof a set of data points.
The measure is ?corrected forchance,?
so that a random grouping has an expectedscore of zero.
This measure tends to be very con-servative, giving values much lower than an intuitivepercentage score.
However, it offers a useful relativecomparison of overall cluster similarity.4.3 ResultsFigure 1 gives the adjusted Rand scores of the threemodel variants, word-based, bootstrap, and combi-nation.
Higher values indicate a better fit with thegold-standard categorization scheme.
The adjustedRand score is corrected for chance, thus providing abuilt-in baseline measure.
Since the expected scorefor a random clustering is zero, all three model vari-ants operate at above-baseline performance.As seen in Figure 1, the word-based model gainsan early advantage in the comparison, but its per-formance approaches a plateau at around 200,000words of input.
This suggests that while simpleword distributions provide a reliable source of infor-mation early in the model?s development, the infor-mation is not sufficient to sustain long-term learn-ing.
The bootstrap model learns much more slowly,which is unsurprising, given that it depends on hav-ing some reasonable category knowledge in order todevelop its clusters?leading to a chicken-and-eggproblem.
However, once started, its performance im-proves well beyond the word-based model?s plateau.These results suggest that on its own, each compo-nent of the model may be effectively throwing awayuseful information.
By combining the two models,the combination model appears to gain complemen-tary benefits from each component, outperformingboth.
The word-based component helps to create abase of reliable clusters, which the bootstrap compo-nent uses to continue development.After all of the training text, the combinationmodel uses 411 clusters to categorize the test tokens(compared to over 2,000 at the first test point).
Whilethis seems excessive, we note that 92.5% of the testtokens are placed in the 25 most populated clusters.33See www.cs.toronto.edu/?chris/syncat for examples.0 1 2 3 4 5 6x 10500.050.10.150.2Training set size (words)R adjCombinationWord?basedBootstrapFigure 1: Adjusted Rand Index of each of three mod-els?
clusterings of the test set, as compared with thePoS tags of the test data.5 Experiment 2: Learning TrendsA common trend observed in children is that differ-ent syntactic categories are learned at different rates.Children appear to have learned the category of nounsby 23 months of age, verbs shortly thereafter, andadjectives relatively late (Kemp et al, 2005).
Ourgoal in this experiment is to look for these specifictrends in the behaviour of our model.
We thus simu-late an experiment where a child uses a novel word?slinguistic context to infer its syntactic category (e.g.,Tomasello et al, 1997).
For our experiment, we ran-domly generate input frames with novel head wordsusing contexts associated with nouns, verbs, and ad-jectives, then examine the model?s categorization ineach case.
We expect that our model should approxi-mate the developmental trends of children, who tendto learn the category of ?noun?
before ?verb,?
and bothof these before ?adjective.
?5.1 MethodsWe generate new input frames using the most com-mon syntactic patterns in the training data.
For eachof the noun, verb, and adjective categories (from thegold standard), we collect the five most frequent PoSsequences in which these are used, bounded by theusual four-word context window.
For example, theAdjective set includes the sequence ?V Det Adj Nnull?, where the sentence ends after the N. For eachof the three categories, we generate each of 500 inputframes by sampling one of the five PoS sequences,weighted by frequency, then sampling words of theright PoS from the lexicon, also weighted by fre-quency.
We replace the head word with a novel word,forcing the model to use only the context for cluster-ing.
Since the context words are chosen at random,most of the word sequences generated will be novel.This makes the task more difficult, rather than sim-ply sampling utterances from the corpus, where rep-93etitions are common.
While a few of the sequencesmay exist in the training data, we expect the modelto mostly use the underlying category information tocluster the frames.We intend to show that the model uses context tofind the right category for a novel word.
To evaluatethe model?s behaviour, we let it categorize each ofthe randomly generated frames.
We score each frameas follows: if the frame gets put into a new cluster,it earns score zero.
Otherwise, its score is the pro-portion of frames in the chosen cluster matching thecorrect part of speech (we use a PoS-tagged versionof the training corpus; for example, a noun frame putinto a cluster with 60% nouns would get 0.6).
We re-port the mean score for each of the noun, verb, andadjective sets.
Intuitively, the matching score indi-cates how well the model recognizes that the givencontexts are similar to input it has seen before.
If themodel clusters the novel word frame with others ofthe right type, then it has formed a category for thecontextual information in that frame.We use the full combination model (Eq.
(7)) toevaluate the learning rates of individual parts ofspeech.
We run the model on the training subset ofthe evaluation corpus.
After every 10,000 words ofinput, we use the model to categorize the 1,500 con-text frames with novel words (500 frames each fornoun, verb, and adjective).
As in experiment 1, themodel does not record these categorizations.5.2 ResultsFigure 2 shows the mean matching scores for eachof the tested parts of speech.
Recall that since theframes each use a novel head word, a higher match-ing score indicates that the model has learned to cor-rectly recognize the contexts in the frames.
This doesnot necessarily mean that the model has learned sin-gle, complete categories of ?noun,?
?verb,?
and ?ad-jective,?
but it does show that when the head wordgives no information, the model can generalize basedon the contextual patterns alone.
The model learnsto categorize novel nouns better than verbs until latein training, which matches the trends seen in children.Adjectives progress slowly, and show nearly no learn-ing ability by the end of the trial.
Again, this appearsto reflect natural behaviour in children, although theeffect we see here may simply be a result of the over-all frequency of the PoS types.
Over the entire corpus(development and evaluation), 35.4% of the word to-kens are nouns and 24.3% are verbs, but only 2.9%are tagged as adjectives.
The model, and similarly achild, may need much more data to learn adjectivesthan is available at this stage.The scores in Figure 2 tend to fluctuate, partic-ularly for the noun contexts.
This fluctuation cor-responds to periods of overgeneralization, followed0 1 2 3 4 5 6x 10500.050.10.150.20.25Training set size (words)MatchingscoreNounsVerbsAdjectivesFigure 2: Comparative learning trends of noun, verb,and adjective patterns.by recovery (also observed in children; see, e.g.,Tomasello, 2000).
When the model merges two clus-ters, the contents of the resulting cluster can initiallybe quite heterogeneous.
Furthermore, the new clusteris much larger, so it becomes a magnet for new cate-gorizations.
This results in overgeneralization errors,giving the periodic drops seen in Figure 2.
While ourformulation in Section 2.4 aims to prevent such er-rors, they are likely to occur on occasion.
Eventually,the model recovers from these errors, and it is worthnoting that the fluctuations diminish over time.
As themodel gradually improves with more input, the dom-inant clusters become heavily entrenched, and incon-sistent merges are less likely to occur.6 Experiment 3: DisambiguationThe category structure of our model allows a singleword type to be a member of multiple categories.
Forexample, kiss could belong to a category of predom-inantly noun usages (Can I have a kiss?)
and alsoto a category of verb usages (Kiss me!).
As a result,the model easily represents lexical ambiguity.
In thisexperiment, inspired by disambiguation work in psy-cholinguistics (see, e.g., MacDonald, 1993), we ex-amine the model?s ability to correctly disambiguatecategory memberships.6.1 MethodsGiven a word that the model has previously seen asvarious different parts of speech, we examine howwell the model can use that ambiguous word?s con-text to determine its category in the current usage.For example, by presenting the word kiss in sepa-rate noun and verb contexts, we expect that the modelshould categorize kiss as a noun, then as a verb, re-spectively.
We also wish to examine the effect of thetarget word?s lexical bias, that is, the predominance ofa word type to be used as one category over another.As with adults, if kiss is mainly used as a noun, weexpect the model to more accurately categorize the94N V N V N V N V N V N V00.10.20.30.4PoSproportioninchosenclustersNounsVerbsContext:Noun only Noun biased Equibiased Verb biased Verb only Novel wordWord bias:Figure 3: Syntactic category disambiguation.
Shown are the proportions of nouns and verbs in the chosenclusters for ambiguous words used in either noun (N) or verb (V) contexts.word in a noun context than in a verb context.We focus on noun/verb ambiguities.
We artificiallygenerate input frames for noun and verb contexts asin experiment 2, with the following exceptions.
Tomake the most use of the context information, we al-low no null words in the input frames.
We also ensurethat the contexts are distinctive enough to guide dis-ambiguation.
For each PoS sequence surrounding anoun (e.g., ?V Det head Prep Det?
), we ensure thatover 80% of the instances of that pattern in the cor-pus are for nouns, and likewise for verbs.We test the model?s disambiguation in six con-ditions, with varying degrees of lexical bias.
Un-ambiguous (?noun/verb only?)
conditions test wordsseen in the corpus only as nouns or verbs (10 wordseach).
?Biased?
conditions test words with a clearbias (15 with average 93% noun bias; 15 with aver-age 84% verb bias).
An ?equibiased?
condition uses 4words of approximately equal bias, and a novel wordcondition provides an unbiased case.For the six sets of test words, we measure the ef-fect of placing each of these words in both noun andverb contexts.
That is, each word in each conditionwas used as the head word in each of the 500 nounand 500 verb disambiguating frames.
For example,we create 500 frames where book is used as a noun,and 500 frames where it is used as a verb.
We thenuse the fully-trained ?combination?
model (Eq.
(7)) tocategorize each frame.
Unlike in the previous experi-ment, we do not let the model create new clusters.
Foreach frame, we choose the best-fitting existing clus-ter, then examine that cluster?s contents.
As in ex-periment 2, we measure the proportions of each PoSof the frames in this cluster.
We then average thesemeasures over all tested frames in each condition.6.2 ResultsFigure 3 presents the measured PoS proportions foreach of the six conditions.
For both the equibias andnovel word conditions, we see that the clusters cho-sen for the noun context frames (labeled N) containmore nouns than verbs, and the clusters chosen forthe verb context frames (V) contain more verbs thannouns.
This suggests that although the model?s pastexperience with the head word is not sufficiently in-formative, the model can use the word?s context todisambiguate its category.
In the ?unambiguous?
andthe ?biased?
conditions, the head words?
lexical biasesare too strong for the model to overcome.However, the results show a realistic effect of thelexical bias.
Note the contrasts from the ?noun only?condition, to the ?noun biased?
condition, to ?equibi-ased?
(and likewise for the verb biases).
As the lex-ical bias weakens, the counter-bias contexts (e.g., anoun bias with a verb context) show a stronger ef-fect on the chosen clusters.
This is a realistic effectof disambiguation seen in adults (MacDonald, 1993).Strongly biased words are more difficult to categorizein conflict with their bias than weakly biased words.7 Related WorkSeveral existing computational models use distribu-tional cues to find syntactic categories.
Schu?tze(1993) employs co-occurrence statistics for commonwords, while Redington et al (1998) build word dis-tributional profiles using corpus bigram counts.
Clark(2000) also builds distributional profiles, introducingan iterative clustering method to better handle am-biguity and rare words.
Mintz (2003) shows thateven very simple three-word templates can effec-tively define syntactic categories.
Each of these mod-els demonstrates that by using the kinds of simple in-formation to which children are known to be sensi-tive, syntactic categories are learnable.
However, thespecific learning mechanisms they use, such as thehierarchical clustering methods of Redington et al(1998), are not intended to be cognitively plausible.In contrast, Cartwright and Brent (1997) propose95an incremental model of syntactic category acquisi-tion that uses a series of linguistic preferences to findcommon patterns across sentence-length templates.Their model presents an important incremental al-gorithm which is very effective for discovering cat-egories in artificial languages.
However, the model?sreliance on templates limits its applicability to tran-scripts of actual spoken language data, which containhigh variability and noise.Recent models that apply Bayesian approachesto PoS tagging are not incremental and assume afixed number of tags (Goldwater and Griffiths, 2007;Toutanova and Johnson, 2008).
In syntactic cate-gory acquisition, the true number of categories is un-known, and must be inferred from the input.8 Conclusions and Future DirectionsWe have developed a computational model of syn-tactic category acquisition in children, and demon-strated its behaviour on a corpus of naturalistic child-directed data.
The model is based on domain-generalproperties of feature similarity, in contrast to earlier,more linguistically-specific methods.
The incremen-tal nature of the algorithm contributes to a substantialimprovement in psychological plausibility over pre-vious models of syntactic category learning.
Further-more, due to its probabilistic framework, our modelis robust to noise and variability in natural language.Our model successfully uses a syntactic bootstrap-ping mechanism to build on the distributional proper-ties of words.
Using its existing partial knowledgeof categories, the model applies a second level ofanalysis to learn patterns in the input.
By makingfew assumptions about prior linguistic knowledge,the model develops realistic syntactic categories fromthe input data alone.
The explicit bootstrapping com-ponent improves the model?s ability to learn adult cat-egories, and its learning trajectory resembles relevantbehaviours seen in children.
Using the contextualpatterns of individual parts of speech, we show dif-ferential learning rates across nouns, verbs, and ad-jectives that mimic child development.
We also showan effect of a lexical bias in category disambiguation.The algorithm is currently only implemented as anincremental process.
However, comparison with abatch version of the algorithm, such as by using aGibbs sampler (Sanborn et al, 2006), would help usfurther understand the effect of incrementality on lan-guage fidelity.While we have only examined the effects of learn-ing categories from simple distributional information,the feature-based framework of our model could eas-ily be extended to include other sources of informa-tion, such as morphological and phonological cues.Furthermore, it would also be possible to include se-mantic features, thereby allowing the model to drawon correlations between semantic and syntactic cate-gories in learning.AcknowledgmentsWe thank Afra Alishahi for valuable discussions,and the anonymous reviewers for their comments.We gratefully acknowledge the financial support ofNSERC of Canada and the University of Toronto.ReferencesAlishahi, A. and S. Stevenson 2008.
A computationalmodel for early argument structure acquisition.
Cog-nitive Science, 32(5).Anderson, J. R. 1991.
The adaptive nature of human cate-gorization.
Psychological Review, 98(3):409?429.Cartwright, T. A. and M. R. Brent 1997.
Syntactic catego-rization in early language acquisition: formalizing therole of distributional analysis.
Cognition, 63:121?170.Clark, A.
2000.
Inducing syntactic categories by contextdistribution clustering.
In CoNLL2000, pp.
91?94.Goldwater, S. and T. L. Griffiths 2007.
A fully bayesianapproach to unsupervised part-of-speech tagging.
InProc.
of ACL2007, pp.
744?751.Hubert, L. and P. Arabie 1985.
Comparing partitions.Journal of Classification, 2:193?218.Kemp, N., E. Lieven, and M. Tomasello 2005.
Young chil-dren?s knowledge of the ?determiner?
and ?adjective?categories.
J.
Speech Lang.
Hear.
R., 48:592?609.MacDonald, M. C. 1993.
The interaction of lexical andsyntactic ambiguity.
J. Mem.
Lang., 32:692?715.MacWhinney, B.
2000.
The CHILDES Project: Tools foranalyzing talk, volume 2: The Database.
Lawrence Erl-baum, Mahwah, NJ, 3 edition.Mintz, T. H. 2003.
Frequent frames as a cue for gram-matical categories in child directed speech.
Cognition,90:91?117.Olguin, R. and M. Tomasello 1993.
Twenty-five-month-old children do not have a grammatical category of verb.Cognitive Development, 8:245?272.Onnis, L. and M. H. Christiansen 2005.
New beginningsand happy endings: psychological plausibility in com-putational models of language acquisition.
CogSci2005.Redington, M., N. Chater, and S. Finch 1998.
Distribu-tional information: A powerful cue for acquiring syn-tactic categories.
Cognitive Science, 22(4):425?469.Sanborn, A. N., T. L. Griffiths, and D. J. Navarro 2006.
Amore rational model of categorization.
CogSci2006.Schu?tze, H. 1993.
Part of speech induction from scratch.In Proc.
of ACL1993, pp.
251?258.Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-land 2001.
The role of performance limitations in theacquisition of verb-argument structure: an alternativeaccount.
J.
Child Lang., 28:127?152.Tomasello, M. 2000.
Do young children have adult syn-tactic competence?
Cognition, 74:209?253.Tomasello, M., N. Akhtar, K. Dodson, and L. Rekau 1997.Differential productivity in young children?s use ofnouns and verbs.
J.
Child Lang., 24:373?387.Toutanova, K. and M. Johnson 2008.
A Bayesian LDA-based model for semi-supervised part-of-speech tag-ging.
In NIPS2008.96
