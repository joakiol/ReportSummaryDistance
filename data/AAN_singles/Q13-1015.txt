Transactions of the Association for Computational Linguistics, 1 (2013) 179?192.
Action Editor: Johan Bos.Submitted 1/2013; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Combined Distributional and Logical SemanticsMike LewisSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKmike.lewis@ed.ac.ukMark SteedmanSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKsteedman@inf.ed.ac.ukAbstractWe introduce a new approach to semanticswhich combines the benefits of distributionaland formal logical semantics.
Distributionalmodels have been successful in modelling themeanings of content words, but logical se-mantics is necessary to adequately representmany function words.
We follow formal se-mantics in mapping language to logical rep-resentations, but differ in that the relationalconstants used are induced by offline distri-butional clustering at the level of predicate-argument structure.
Our clustering algorithmis highly scalable, allowing us to run on cor-pora the size of Gigaword.
Different senses ofa word are disambiguated based on their in-duced types.
We outperform a variety of ex-isting approaches on a wide-coverage questionanswering task, and demonstrate the ability tomake complex multi-sentence inferences in-volving quantifiers on the FraCaS suite.1 IntroductionMapping natural language to meaning representa-tions is a central challenge of NLP.
There has beenmuch recent progress in unsupervised distributionalsemantics, in which the meaning of a word is in-duced based on its usage in large corpora.
This ap-proach is useful for a range of key applications in-cluding question answering and relation extraction(Lin and Pantel, 2001; Poon and Domingos, 2009;Yao et al 2011).
Because such a semantics can beautomically induced, it escapes the limitation of de-pending on relations from hand-built training data,knowledge bases or ontologies, which have provedof limited use in capturing the huge variety of mean-ings that can be expressed in language.However, distributional semantics has largely de-veloped in isolation from the formal semantics liter-ature.
Whilst distributional semantics has been ef-fective in modelling the meanings of content wordssuch as nouns and verbs, it is less clear that it can beapplied to the meanings of function words.
Semanticoperators, such as determiners, negation, conjunc-tions, modals, tense, mood, aspect, and plurals areubiquitous in natural language, and are crucial forhigh performance on many practical applications?but current distributional models struggle to captureeven simple examples.
Conversely, computationalmodels of formal semantics have shown low recallon practical applications, stemming from their re-liance on ontologies such as WordNet (Miller, 1995)to model the meanings of content words (Bobrow etal., 2007; Bos and Markert, 2005).For example, consider what is needed to answera question like Did Google buy YouTube?
from thefollowing sentences:1.
Google purchased YouTube2.
Google?s acquisition of YouTube3.
Google acquired every company4.
YouTube may be sold to Google5.
Google will buy YouTube or Microsoft6.
Google didn?t takeover YouTubeAll of these require knowledge of lexical seman-tics (e.g.
that buy and purchase are synonyms), butsome also need interpretation of quantifiers, nega-tives, modals and disjunction.
It seems unlikely that179distributional or formal approaches can accomplishthe task alone.We propose a method for mapping natural lan-guage to first-order logic representations capable ofcapturing the meanings of function words such asevery, not and or, but which also uses distributionalstatistics to model the meaning of content words.Our approach differs from standard formal seman-tics in that the non-logical symbols used in the log-ical form are cluster identifiers.
Where standard se-mantic formalisms would map the verb write to awrite?
symbol, we map it to a cluster identifier suchas relation37, which the noun author may also mapto.
This mapping is learnt by offline clustering.Unlike previous distributional approaches, weperform clustering at the level of predicate-argumentstructure, rather than syntactic dependency struc-ture.
This means that we abstract away from manysyntactic differences that are not present in the se-mantics, such as conjunctions, passives, relativeclauses, and long-range dependencies.
This signifi-cantly reduces sparsity, so we have fewer predicatesto cluster and more observations for each.Of course, many practical inferences rely heavilyon background knowledge about the world?suchknowledge falls outside the scope of this work.2 BackgroundOur approach is based on Combinatory CategorialGrammar (CCG; Steedman, 2000), a strongly lexi-calised theory of language in which lexical entriesfor words contain all language-specific information.The lexical entry for each word contains a syntacticcategory, which determines which other categoriesthe word may combine with, and a semantic inter-pretation, which defines the compositional seman-tics.
For example, the lexicon may contain the entry:write ` (S\NP)/NP : ?y?x.write?
(x,y)Crucially, there is a transparent interface betweenthe syntactic category and the semantics.
For ex-ample the transitive verb entry above defines theverb syntactically as a function mapping two noun-phrases to a sentence, and semantically as a bi-nary relation between its two argument entities.This means that it is relatively straightforward todeterministically map parser output to a logicalform, as in the Boxer system (Bos, 2008).
ThisEvery dog barksNP?/N N S\NP?
p?q.
?x[p(x) =?
q(x)] ?x.dog?
(x) ?x.bark?(x)>NP??q.?x[dog?
(x) =?
q(x)]>S?x[dog?
(x) =?
bark?
(x)]Figure 1: A standard logical form derivation using CCG.The NP?
notation means that the subject is type-raised,and taking the verb-phrase as an argument?so is an ab-breviation of S/(S\NP).
This is necessary in part to sup-port a correct semantics for quantifiers.Input SentenceShakespeare wrote Macbeth?Intial semantic analysiswritearg0,arg1(shakespeare, macbeth)?Entity Typingwritearg0:PER,arg1:BOOK(shakespeare:PER,macbeth:BOOK)?Distributional semantic analysisrelation37(shakespeare:PER, macbeth:BOOK)Figure 2: Layers used in our model.form of semantics captures the underlying predicate-argument structure, but fails to license many impor-tant inferences?as, for example, write and authordo not map to the same predicate.In addition to the lexicon, there is a small set ofbinary combinators and unary rules, which have asyntactic and semantic interpretation.
Figure 1 givesan example CCG derivation.3 Overview of ApproachWe attempt to learn a CCG lexicon which mapsequivalent words onto the same logical form?forexample learning entries such as:author ` N/PP[o f ] : ?x?y.relation37(x,y)write ` (S\NP)/NP : ?x?y.relation37(x,y)The only change to the standard CCG derivation isthat the symbols used in the logical form are arbi-trary relation identifiers.
We learn these by first map-ping to a deterministic logical form (using predicates180such as author?
and write?
), using a process simi-lar to Boxer, and then clustering predicates based ontheir arguments.
This lexicon can then be used toparse new sentences, and integrates seamlessly withCCG theories of formal semantics.Typing predicates?for example, determining thatwriting is a relation between people and books?has become standard in relation clustering (Schoen-mackers et al 2010; Berant et al 2011; Yao etal., 2012).
We demonstate how to build a typingmodel into the CCG derivation, by subcategorizingall terms representing entities in the logical formwith a more detailed type.
These types are also in-duced from text, as explained in Section 5, but forconvenience we describe them with human-readablelabels, such as PER, LOC and BOOK.A key advantage of typing is that it allows us tomodel ambiguous predicates.
Following Berant etal.
(2011), we assume that different type signaturesof the same predicate have different meanings, butgiven a type signature a predicate is unambiguous.For example a different lexical entry for the verbborn is used in the contexts Obama was born inHawaii and Obama was born in 1961, reflecting adistinction in the semantics that is not obvious in thesyntax1.
Typing also greatly improves the efficiencyof clustering, as we only need to compare predicateswith the same type during clustering (for example,we do not have to consider clustering a predicatebetween people and places with predicates betweenpeople and dates).In this work, we focus on inducing binary rela-tions.
Many existing approaches have shown howto produce good clusterings of (non-event) nouns(Brown et al 1992), any of which could be sim-ply integrated into our semantics?but relation clus-tering remains an open problem (see Section 9).N-ary relations are binarized, by creating a bi-nary relation between each pair of arguments.
Forexample, for the sentence Russia sold Alaska tothe United States, the system creates three binaryrelations?
corresponding to sellToSomeone(Russia,Alaska), buyFromSomeone(US, Alaska), sellSome-thingTo(Russia, US).
This transformation does not1Whilst this assumption is very useful, it does not always hold?for example, the genitive in Shakespeare?s book is ambigu-ous between ownership and authorship relations even given thetypes of the arguments.exactly preserve meaning, but still captures the mostimportant relations.
Note that this allows us tocompare semantic relations across different syntac-tic types?for example, both transitive verbs andargument-taking nouns can be seen as expressing bi-nary semantic relations between entities.Figure 2 shows the layers used in our model.4 Initial Semantic AnalysisThe initial semantic analysis maps parser outputonto a logical form, in a similar way to Boxer.
Thesemantic formalism is based on Steedman (2012).The first step is syntactic parsing.
We use theC&C parser (Clark and Curran, 2004), trained onCCGBank (Hockenmaier and Steedman, 2007), us-ing the refined version of Honnibal et al(2010)which brings the syntax closer to the predicate-argument structure.
An automatic post-processingstep makes a number of minor changes to the parseroutput, which converts the grammar into one moresuitable for our semantics.
PP (prepositional phrase)and PR (phrasal verb complement) categories aresub-categorised with the relevant preposition.
Nouncompounds with the same MUC named-entity type(Chinchor and Robinson, 1997) are merged into asingle non-compositional node2 (we otherwise ig-nore named-entity types).
All argument NPs andPPs are type-raised, allowing us to represent quanti-fiers.
All prepositional phrases are treated as core ar-guments (i.e.
given the category PP, not adjunct cat-egories like (N\N)/NP or ((S\NP)\(S\NP))/NP),as it is difficult for the parser to distinguish argu-ments and adjuncts.Initial semantic lexical entries for almost allwords can be generated automatically from thesyntactic category and POS tag (obtained fromthe parser), as the syntactic category captures theunderlying predicate-argument structure.
We usea Davidsonian-style representation of arguments(Davidson, 1967), which we binarize by creating aseparate predicate for each pair of arguments of aword.
These predicates are labelled with the lemmaof the head word and a Propbank-style argument key(Kingsbury and Palmer, 2002), e.g.
arg0, argIn.
Wedistinguish noun and verb predicates based on POS2For example, this allows us to give Barack Obama the seman-tics ?x.barack obama(x) instead of ?x.barack(x)?
obama(x),which is more convenient for collecting distributional statistics.181Word Category SemanticsAutomatic author N/PP[o f ] ?x?y.authorarg0,argOf (y,x)write (S\NP)/NP ?x?y.writearg0,arg1(y,x)Manual every NP?/N ?
p?q.?x[p(x)?
q(x)]not (S\NP)/(S\NP) ?
p?x.
?p(x)Figure 3: Example initial lexical entriestag?so, for example, we have different predicatesfor effect as a noun or verb.This algorithm can be overridden with man-ual lexical entries for specific closed-class functionwords.
Whilst it may be possible to learn thesefrom data, our approach is pragmatic as there arerelatively few such words, and the complex logicalforms required would be difficult to induce from dis-tributional statistics.
We add a small number of lexi-cal entries for words such as negatives (no, not etc.
),and quantifiers (numbers, each, every, all, etc.
).Some example initial lexical entries are shown inFigure 3.5 Entity Typing ModelOur entity-typing model assigns types to nouns,which is useful for disambiguating polysemouspredicates.
Our approach is similar to O?Seaghdha(2010) in that we aim to cluster entities based onthe noun and unary predicates applied to them (itis simple to convert from the binary predicatesto unary predicates).
For example, we want thepair (bornargIn, 1961) to map to a DAT type, and(bornargIn, Hawaii) to map to a LOC type.
This isnon-trivial, as both the predicates and arguments canbe ambiguous between multiple types?but topicmodels offer a good solution (described below).5.1 Topic ModelWe assume that the type of each argument of a pred-icate depends only on the predicate and argument,although Ritter et al(2010) demonstrate an advan-tage of modelling the joint probability of the typesof multiple arguments of the same predicate.
We usethe standard Latent Dirichlet Allocation model (Bleiet al 2003), which performs comparably to morecomplex models proposed in O?Seaghdha (2010).In topic-modelling terminology, we construct adocument for each unary predicate (e.g.
bornargIn),based on all of its argument entities (words).
We as-sume that these arguments are drawn from a smallnumber of types (topics), such as PER, DAT orLOC3.
Each type j has a multinomial distribution?
j over arguments (for example, a LOC type is morelikely to generate Hawaii than 1961).
Each unarypredicate i has a multinomial distribution ?i overtopics, so the bornargIn predicate will normally gen-erate a DAT or LOC type.
Sparse Dirichlet priors?
and ?
on the multinomials bias the distributionsto be peaky.
The parameters are estimated by Gibbssampling, using the Mallet implementation (McCal-lum, 2002).The generative story to create the data is:For every type k:Draw the p(arg|k) distribution ?k from Dir(?
)For every unary predicate i:Draw the p(type|i) distribution ?i from Dir(?
)For every argument j:Draw a type zi j from Mult(?i)Draw an argument wi j from Mult(?
?i)5.2 Typing in Logical FormIn the logical form, all constants and variables repre-senting entities x can be assigned a distribution overtypes px(t) using the type model.
An initial typedistribution is applied in the lexicon, using the ?distributions for the types of nouns, and the ?i dis-tributions for the type of arguments of binary predi-cates (inverted using Bayes?
rule).
Then at each ?
-reduction in the derivation, we update probabilitiesof the types to be the product of the type distribu-tions of the terms being reduced.
If two terms x and3Types are induced from the text, but we give human-readablelabels here for convenience.182file a suit(S\NP)/NP NP?
?y :{ DOC=0.5LEGAL=0.4CLOTHES=0.01...}?x :{PER = 0.7ORG = 0.2...}.
f ilearg0,arg1(x,y) ?
p.?y :{CLOTHES = 0.6LEGAL = 0.3DOC=0.001...}[suit ?(y)?
p(y)]<S\NP?x :{ PER = 0.7ORG = 0.2...}?y :{ LEGAL = 0.94CLOTHES = 0.05DOC = 0.004...})[suit ?(y)?
f ilearg0,arg1(x,y)]Figure 4: Using the type model for disambiguation in the derivation of file a suit.
Type distributions are shown afterthe variable declarations.
Both suit and the object of file are lexically ambiguous between different types, but after the?
-reduction only one interpretation is likely.
If the verb were wear, a different interpretation would be preferred.y combine to a term z:pz(t) = px(t)py(t)?
t ?
px(t ?
)py(t ?
)For example, in wore a suit and file a suit, the vari-able representing suit may be lexically ambiguousbetween CLOTHES and LEGAL types, but the vari-ables representing the objects of wear and f ile willhave preferences that allow us to choose the correcttype when the terms combine.
Figure 4 shows anexample derivation using the type model for disam-biguation4.6 Distributional Relation ClusteringModelThe typed binary predicates can be groupedinto clusters, each of which represents a dis-tinct semantic relation.
Note that because wecluster typed predicates, bornarg0:PER,argIn:LOC andbornarg0:PER,argIn:DAT can be clustered separately.6.1 Corpus statisticsTyped binary predicates are clustered based on theexpected number of times they hold between eachargument-pair in the corpus.
This means we cre-ate a single vector of argument-pair counts for eachpredicate (not a separate vector for each argument).For example, the vector for the typed predicatewritearg0:PER,arg1:BOOK may contain non-zero countsfor entity-pairs such as (Shakespeare, Macbeth),(Dickens, Oliver Twist) and (Rowling, Harry Potter).4Our implementation follows Steedman (2012) in using Gener-alized Skolem Terms rather than existential quantifiers, in orderto capture quantifier scope alternations monotonically, but weomit these from the example to avoid introducing new notation.The entity-pair counts for authorarg0:PER,argOf :BOOKmay be similar, on the assumption that both are sam-ples from the same underlying semantic relation.To find the expected number of occurrences ofargument-pairs for typed binary predicates in a cor-pus, we first apply the type model to the derivationof each sentence, as described in Section 5.2.
Thisoutputs untyped binary predicates, with distributionsover the types of their arguments.
The type of thepredicate must match the type of its arguments, sothe type distribution of a binary predicate is simplythe joint distribution of the two argument type dis-tributions.For example, if the arguments in abornarg0,argIn(obama,hawaii) derivation have therespective type distributions (PER=0.9, LOC=0.1)and (LOC=0.7, DAT=0.3), the distribution over bi-nary typed predicates is (bornarg0:PER,argIn:LOC=0.63,bornarg0:PER,argIn:DAT =0.27, etc.)
The expectedcounts for (obama,hawaii) in the vectors forbornarg0:PER,argIn:LOC and bornarg0:PER,argIn:DAT arethen incremented by these probabilities.6.2 ClusteringMany algorithms have been proposed for cluster-ing predicates based on their arguments (Poon andDomingos, 2009; Yao et al 2012).
The number ofrelations in the corpus is unbounded, so the cluster-ing algorithm should be non-parametric.
It is alsoimportant that it remains tractable for very largenumbers of predicates and arguments, in order togive us a greater coverage of language than can beachieved by hand-built ontologies.We cluster the typed predicate vectors using theChinese Whispers algorithm (Biemann, 2006)?183although somewhat ad-hoc, it is both non-parametricand highly scalable5.
This has previously been usedfor noun-clustering by Fountain and Lapata (2011),who argue it is a cognitively plausible model forlanguage acquisition.
The collection of predicatesand arguments is converted into a graph with onenode per predicate, and edge weights representingthe similarity between predicates.
Predicates withdifferent types have zero-similarity, and otherwisesimilarity is computed as the cosine-similarity of thetf-idf vectors of argument-pairs.
We prune nodes oc-curring fewer than 20 times, edges with weights lessthan 10?3, and a short list of stop predicates.The algorithm proceeds as follows:1.
Each predicate p is assigned to a different se-mantic relation rp2.
Iterate over the predicates p in a random order3.
Set rp = argmaxr ?p?
1r=rp?
sim(p, p?
), wheresim(p, p?)
is the distributional similarity be-tween p and p?, and 1r=r?
is 1 iff r=r?
and 0otherwise.4.
Repeat (2.)
to convergence.7 Semantic Parsing using RelationClustersThe final phase is to use our relation clusters in thelexical entries of the CCG semantic derivation.
Thisis slightly complicated by the fact that our predi-cates are lexically ambiguous between all the pos-sible types they could take, and hence the relationsthey could express.
For example, the system can-not tell whether born in is expressing a birthplaceor birthdate relation until later in the derivation,when it combines with its arguments.
However, allthe possible logical forms are identical except forthe symbols used, which means we can produce apacked logical form capturing the full distributionover logical forms.
To do this, we make the predi-cate a function from argument types to relations.For each word, we first take the lexical semanticdefinition produced by the algorithm in Section 4.For binary predicates in this definition (which will5We also experimented with a Dirichlet Process Mixture Model(Neal, 2000), but even with the efficient A* search algorithmsintroduced by Daume?
III (2007), the cost of inference was foundto be prohibitively high when run at large scale.be untyped), we perform a deterministic lookup inthe cluster model learned in Section 6, using all pos-sible corresponding typed predicates.
This allows usto represent the binary predicates as packed predi-cates: functions from argument types to relations.For example, if the clustering mapsbornarg0:PER,argIn:LOC to rel49 (?birthplace?
)and bornarg0:PER,argIn:DAT to rel53 (?birthdate?
), ourlexicon contains the following packed lexical entry(type-distributions on the variables are suppressed):born ` (S\NP)/PP[in] :?y?x.
{(x : PER,y :LOC)?rel49(x : PER,y :DAT)?rel53}(x,y)The distributions over argument types then implya distribution over relations.
For example, if thepacked-predicate for bornarg0,argIn is applied to ar-guments Obama and Hawaii, with respective typedistributions (PER=0.9, LOC=0.1) and (LOC=0.7,DAT=0.3)6, the distribution over relations will be(rel49=0.63, rel53=0.27, etc.
).If 1961 has a type-distribution (LOC=0.1,DAT=0.9), the output packed-logical form forObama was born in Hawaii in 1961 will be:???rel49=0.63rel53=0.27...???(ob,hw)????rel49=0.09rel53=0.81...???
(ob,1961)The probability of a given logical form can be readfrom this packed logical form.8 ExperimentsOur approach aims to offer a strong model of bothformal and lexical semantics.
We perform two eval-uations, aiming to target each of these separately, butusing the same semantic representations in each.We train our system on Gigaword (Graff et al2003), which contains around 4 billion words ofnewswire.
The type-model is trained using 15types7, and 5,000 iterations of Gibbs sampling (us-ing the distributions from the final sample).
Table 16These distributions are composed from the type-distributionsfor both the predicate and argument, as explained in Section 57This number was chosen by examination of models trainedwith different numbers of types.
The algorithm produces se-mantically coherent clusters for much larger numbers of types,but many of these are fine-grained categories of people, whichintroduces sparsity in the relation clustering.184Type Top Words1 suspect, assailant, fugitive, accomplice2 author, singer, actress, actor, dad5 city, area, country, region, town, capital8 subsidiary, automaker, airline, Co., GM10 musical, thriller, sequel, specialTable 1: Most probable terms in some clusters inducedby the Type Model.shows some example types.
The relation clusteringuses only proper nouns, to improve precision (spar-sity problems are partly offset by the large input cor-pus).
Aside from parsing, the pipeline takes arounda day to run using 12 cores.8.1 Question Answering ExperimentsAs yet, there is no standard way of evaluating lexicalsemantics.
Existing tasks like Recognising TextualEntailment (RTE; Dagan et al 2006) rely heavily onbackground knowledge, which is beyond the scopeof this work.
Intrinsic evaluations of entailment rela-tions have low inter-annotator agreement (Szpektoret al 2007), due to the difficulty of evaluating rela-tions out of context.Our evaluation is based on that performed byPoon and Domingos (2009).
We automatically con-struct a set of questions by sampling from text,and then evaluate how many answers can be foundin a different corpus.
From dependency-parsednewswire, we sample either Xnsub j?
verbdob j?
Y, Xnsub j?verbpob j?
Y or Xnsub j?
be dob j?
nounpob j?
Y patterns,where X and Y are proper nouns and the verb isnot on a list of stop verbs, and deterministically con-vert these to questions.
For example, from Googlebought YouTube we create the questions What didGoogle buy?
and What bought YouTube?.
The taskis to find proper-noun answers to these questions ina different corpus, which are then evaluated by hu-man annotators based on the sentence the answerwas retrieved from8.
Systems can return multiple8Common nouns are filtered automatically.
To focus on evalu-ating the semantics, annotators ignored garbled sentences dueto errors pre-processing the corpus (these are excluded fromthe results).
We also automatically exclude weekday andmonth answers, which are overwhelmingly syntax errors forall systems?e.g.
treating Tuesday as an object in Obama an-nounced Tuesday that...answers to the same question (e.g.
What did Googlebuy?
may have many valid answers), and all ofthese contribute to the result.
As none of the systemsmodel tense or temporal semantics, annotators wereinstructed to annotate answers as correct if they weretrue at any time.
This approach means we evaluateon relations in proportion to corpus frequency.
Wesample 1000 questions from the New York Timessubset of Gigaword from 2010, and search for an-swers in the New York Times from 2009.We evaluate the following approaches:?
CCG-Baseline The logical form produced byour CCG derivation, without the clustering.?
CCG-WordNet The CCG logical form, plusWordNet as a model of lexical semantics.?
CCG-Distributional The logical form includ-ing the type model and clusters.?
Relational LDA An LDA based model forclustering dependency paths (Yao et al 2011).We train on New York Times subset of Giga-word9, using their setup of 50 iterations with100 relation types.?
Reverb A sophisticated Open Information Ex-traction system (Fader et al 2011).Unsupervised Semantic Parsing (USP; Poon andDomingos, 2009; USP; Poon and Domingos, 2010;USP; Titov and Klementiev, 2011) would be anotherobvious baseline.
However, memory requirementsmean it is not possible to run at this scale (our systemis trained on 4 orders of magnitude more data thanthe USP evaluation).
Yao et al(2011) found it hadcomparable performance to Relational LDA.For the CCG models, rather than performing fullfirst-order inference on a large corpus, we simplytest whether the question predicate subsumes a can-didate answer predicate, and whether the argumentsmatch10.
In the case of CCG-Distributional, we cal-culate the probability that the two packed-predicates9This is around 35% of Gigaword, and was the largest scalepossible on our resources.10We do this as it is much more efficient than full first-ordertheorem-proving.
We could in principle make additional in-ferences with theorem-proving, such as answering What didGoogle buy?
from Google bought the largest video website andYouTube is the largest video website.185System Answers CorrectRelational-LDA 7046 11.6%Reverb 180 89.4%CCG-Baseline 203 95.8%CCG-WordNet 211 94.8%CCG-Distributional@250 250 94.1%CCG-Distributional@500 500 82.0%Table 2: Results on wide-coverage Question Answer-ing task.
CCG-Distributional ranks question/answer pairsby confidence?
@250 means we evaluate the top 250 ofthese.
It is not possible to give a recall figure, as the totalnumber of correct answers in the corpus is unknown.are in the same cluster, marginalizing over their ar-gument types.
Answers are ranked by this proba-bility.
For CCG-WordNet, we check if the ques-tion predicate is a hypernym of the candidate answerpredicate (using any WordNet sense of either term).Results are shown in Table 2.
Relational-LDA in-duces many meaningful clusters, but predicates mustbe assigned to one of 100 relations, so results aredominated by large, noisy clusters (it is not possi-ble to take the N-best answers as the cluster assign-ments do not have a confidence score).
The CCG-Baseline errors are mainly caused by parser errors,or relations in the scope of non-factive operators.CCG-WordNet adds few answers to CCG-Baseline,reflecting the limitations of hand-built ontologies.CCG-Distributional substantially improves recallover other approaches whilst retaining good preci-sion, demonstrating that we have learnt a powerfulmodel of lexical semantics.
Table 3 shows somecorrectly answered questions.
The system improvesover the baseline by mapping expressions such asmerge with and acquisition of to the same relationcluster.
Many of the errors are caused by conflatingpredicates where the entailment only holds in onedirection, such as was elected to with ran for.
Hier-archical clustering could be used to address this.8.2 Experiments on the FraCaS SuiteWe are also interested in evaluating our approachas a model of formal semantics?demonstrating thatit is possible to integrate the formal semantics ofSteedman (2012) with our distributional clusters.The FraCaS suite (Cooper et al 1996)11 containsa hand-built set of entailment problems designed tobe challenging in terms of formal semantics.
Weuse Section 1, which contains 74 problems requiringan understanding of quantifiers12.
They do not re-quire any knowledge of lexical semantics, meaningwe can evaluate the formal component of our systemin isolation.
However, we use the same representa-tions as in our previous experiment, even though theclusters provide no benefit on this task.
Figure 5gives an example problem.The only previous work we are aware of onthis dataset is by MacCartney and Manning (2007).This approach learns the monotonicity propertiesof words from a hand-built training set, and usesthis to transform a sentence into a polarity anno-tated string.
The system then aims to transform thepremise string into a hypothesis.
Positively polar-ized words can be replaced with less specific ones(e.g.
by deleting adjuncts), whereas negatively po-larized words can be replaced with more specificones (e.g.
by adding adjuncts).
Whilst this is high-precision and often useful, this logic is unable to per-form inferences with multiple premise sentences (incontrast to our first-order logic).Development consists of adding entries to our lex-icon for quantifiers.
For simplicity, we treat multi-word quantifiers like at least a few, as being multi-word expressions?although a more compositionalanalysis may be possible.
Following MacCartneyand Manning (2007), we do not use held-out data?each problem is designed to test a different issue, soit is not possible to generalize from one subset of thesuite to another.
As we are interested in evaluatingthe semantics, not the parser, we manually supplygold-standard lexical categories for sentences withparser errors (any syntactic mistake causes incorrectsemantics).
Our derivations produce a distributionover logical forms?we license the inference if itholds in any interpretation with non-zero probabil-ity.
We use the Prover9 (McCune, 2005) theoremprover for inference, returning yes if the premise im-plies the hypothesis, no if it implies the negation ofthe hypothesis, and unknown otherwise.Results are shown in Table 4.
Our system im-11We use the version converted to machine readable format byMacCartney and Manning (2007)12Excluding 6 problems without a defined solution.186Question Answer SentenceWhat did Delta merge with?
Northwest The 747 freighters came with Delta?s acquisition of NorthwestWhat spoke with Hu Jintao?
Obama Obama conveyed his respect for the Dalai Lama to China?spresident Hu Jintao during their first meeting.
.
.What arrived in Colorado?
Zazi Zazi flew back to Colorado.
.
.What ran for Congress?
Young .
.
.
Young was elected to Congress in 1972Table 3: Example questions correctly answered by CCG-Distributional.Premises: Every European has the right to live in Europe.Every European is a person.Every person who has the right to live in Europe can travel freely within Europe.Hypothesis: Every European can travel freely within EuropeSolution: YesFigure 5: Example problem from the FraCaS suite.System Single MultiplePremise PremisesMacCartney&Manning 07 84% -MacCartney&Manning 08 98% -CCG-Dist (parser syntax) 70% 50%CCG-Dist (gold syntax) 89% 80%Table 4: Accuracy on Section 1 of the FraCaS suite.Problems are divided into those with one premise sen-tence (44) and those with multiple premises (30).proves on previous work by making multi-sentenceinferences.
Causes of errors include missing a dis-tinct lexical entry for plural the, only taking existen-tial interpretations of bare plurals, failing to inter-pret mass-noun determiners such as a lot of, and notproviding a good semantics for non-monotone de-terminers such as most.
We believe these problemswill be surmountable with more work.
Almost all er-rors are due to incorrectly predicting unknown ?
thesystem makes just one error on yes or no predictions(with or without gold syntax).
This suggests thatmaking first-order logic inferences in applicationswill not harm precision.
We are less robust thanMacCartney and Manning (2007) to syntax errorsbut, conversely, we are able to attempt more of theproblems (i.e.
those with multi-sentence premises).Other approaches based on distributional semanticsseem unable to tackle any of these problems, as theydo not represent quantifiers or negation.9 Related WorkMuch work on semantics has taken place in a su-pervised setting?for example the GeoQuery (Zelleand Mooney, 1996) and ATIS (Dahl et al 1994) se-mantic parsing tasks.
This approach makes sense forgenerating queries for a specific database, but meansthe semantic representations do not generalize toother datasets.
There have been several attemptsto annotate larger corpora with semantics?such asOntonotes (Hovy et al 2006) or the GroningenMeaning Bank (Basile et al 2012).
These typicallymap words onto senses in ontologies such as Word-Net, VerbNet (Kipper et al 2000) and FrameNet(Baker et al 1998).
However, limitations of theseontologies mean that they do not support inferencessuch as X is the author of Y ?
X wrote Y.Given the difficulty of annotating large amountsof text with semantics, various approaches have at-tempted to learn meaning without annotated text.Distant Supervision approaches leverage existingknowledge bases, such as Freebase (Bollacker et al2008), to learn semantics (Mintz et al 2009; Krish-namurthy and Mitchell, 2012).
Dependency-basedCompositional Semantics (Liang et al 2011) learnsthe meaning of questions by using their answers asdenotations?but this appears to be specific to ques-tion parsing.
Such approaches can only learn thepre-specified relations in the knowledge base.The approaches discussed so far in this sectionhave all attempted to map language onto some pre-187specified set of relations.
Various attempts havebeen made to instead induce relations from text byclustering predicates based on their arguments.
Forexample, Yao et al(2011) propose a series of LDA-based models which cluster relations between en-tities based on a variety of lexical, syntactic andsemantic features.
Unsupervised Semantic Pars-ing (Poon and Domingos, 2009) recursively clustersfragments of dependency trees based on their argu-ments.
Although USP is an elegant model, it is toocomputationally expensive to run on large corpora.It is also based on frame semantics, so does not clus-ter equivalent predicates with different frames.
Toour knowledge, our work is the first such approachto be integrated within a linguistic theory supportingformal semantics for logical operators.Vector space models represent words by vectorsbased on co-occurrence counts.
Recent work hastackled the problem of composing these matricesto build up the semantics of phrases or sentences(Mitchell and Lapata, 2008).
Another strand (Co-ecke et al 2010; Grefenstette et al 2011) hasshown how to represent meanings as tensors, whoseorder depends on the syntactic category, allowingan elegant correspondence between syntactic andsemantic types.
Socher et al(2012) train a com-position function using a neural network?howevertheir method requires annotated data.
It is also notobvious how to represent logical relations such asquantification in vector-space models.
Baroni et al(2012) make progress towards this by learning aclassifier that can recognise entailments such as alldogs =?
some dogs, but this remains some wayfrom the power of first-order theorem proving of thekind required by the problem in Figure 5.An alternative strand of research has attemptedto build computational models of linguistic theoriesbased on formal compositional semantics, such asthe CCG-based Boxer (Bos, 2008) and the LFG-based XLE (Bobrow et al 2007).
Such approachesconvert parser output into formal semantic repre-sentations, and have demonstrated some ability tomodel complex phenomena such as negation.
Forlexical semantics, they typically compile lexical re-sources such as VerbNet and WordNet into inferencerules?but still achieve only low recall on open-domain tasks, such as RTE, mostly due to the lowcoverage of such resources.
Garrette et al(2011)use distributional statistics to determine the proba-bility that a WordNet-derived inference rule is validin a given context.
Our approach differs in thatwe learn inference rules not present in WordNet.Our lexical semantics is integrated into the lexicon,rather than being implemented as additional infer-ence rules, meaning that inference is more efficient,as equivalent statements have the same logical form.Natural Logic (MacCartney and Manning, 2007)offers an interesting alternative to symbolic logics,and has been shown to be able to capture complexlogical inferences by simply identifying the scope ofnegation in text.
This approach achieves similar pre-cision and much higher recall than Boxer on the RTEtask.
Their approach also suffers from such limita-tions as only being able to make inferences betweentwo sentences.
It is also sensitive to word order, socannot make inferences such as Shakespeare wroteMacbeth =?
Macbeth was written by Shakespeare.10 Conclusions and Future WorkThis is the first work we are aware of that combines adistributionally induced lexicon with formal seman-tics.
Experiments suggest our approach comparesfavourably with existing work in both areas.Many potential areas for improvement remain.Hierachical clustering would allow us to capturehypernym relations, rather than the synonyms cap-tured by our flat clustering.
There is much potentialfor integrating existing hand-built resources, suchas Ontonotes and WordNet, to improve the accu-racy of clustering.
There are cases where the ex-isting CCGBank grammar does not match the re-quired predicate-argument structure?for examplein the case of light verbs.
It may be possible to re-bank CCGBank, in a way similar to Honnibal et al(2010), to improve it on this point.AcknowledgementsWe thank Christos Christodoulopoulos, TejaswiniDeoskar, Mark Granroth-Wilding, Ewan Klein, Ka-trin Erk, Johan Bos and the anonymous reviewersfor their helpful comments, and Limin Yao for shar-ing code.
This work was funded by ERC AdvancedFellowship 249520 GRAMPLUS and IP EC-FP7-270273 Xperience.188ReferencesC.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.
Theberkeley framenet project.
In Proceedings of the36th Annual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics-Volume 1, pages 86?90.Association for Computational Linguistics.M.
Baroni, R. Bernardi, N.Q.
Do, and C. Shan.
2012.Entailment above the word level in distributional se-mantics.
In Proceedings of EACL, pages 23?32.
Cite-seer.V.
Basile, J. Bos, K. Evang, and N. Venhuizen.
2012.Developing a large semantically annotated corpus.
InProceedings of the Eighth International Conferenceon Language Resources and Evaluation (LREC12).
Toappear.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 610?619.
Association for Computational Linguistics.C.
Biemann.
2006.
Chinese whispers: an efficient graphclustering algorithm and its application to natural lan-guage processing problems.
In Proceedings of theFirst Workshop on Graph Based Methods for NaturalLanguage Processing, pages 73?80.
Association forComputational Linguistics.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alcation.
the Journal of machine Learningresearch, 3:993?1022.D.
G. Bobrow, C. Condoravdi, R. Crouch, V. De Paiva,L.
Karttunen, T. H. King, R. Nairn, L. Price, andA.
Zaenen.
2007.
Precision-focused textual inference.In Proceedings of the ACL-PASCAL Workshop on Tex-tual Entailment and Paraphrasing, RTE ?07, pages16?21.
Association for Computational Linguistics.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Management ofdata, SIGMOD ?08, pages 1247?1250, New York, NY,USA.
ACM.J.
Bos and K. Markert.
2005.
Recognising textual en-tailment with logical inference.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 628?635.
Association for Computational Lin-guistics.Johan Bos.
2008.
Wide-coverage semantic analysis withboxer.
In Johan Bos and Rodolfo Delmonte, editors,Semantics in Text Processing.
STEP 2008 ConferenceProceedings, Research in Computational Semantics,pages 277?286.
College Publications.P.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.D.
Pietra, andJ.C.
Lai.
1992.
Class-based n-gram models of naturallanguage.
Computational linguistics, 18(4):467?479.N.
Chinchor and P. Robinson.
1997.
Muc-7 named entitytask definition.
In Proceedings of the 7th Conferenceon Message Understanding.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, ACL ?04.
Association forComputational Linguistics.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a compositionaldistributional model of meaning.
Linguistic Analysis:A Festschrift for Joachim Lambek, 36(1-4):345?384.Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,Johan Van Genabith, Jan Jaspars, Hans Kamp, DavidMilward, Manfred Pinkal, Massimo Poesio, et al1996.
Using the framework.
FraCaS Deliverable D,16.Ido Dagan, O. Glickman, and B. Magnini.
2006.
ThePASCAL recognising textual entailment challenge.Machine Learning Challenges.
Evaluating PredictiveUncertainty, Visual Object Classification, and Recog-nising Tectual Entailment, pages 177?190.D.A.
Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-Smith, D. Pallett, C. Pao, A. Rudnicky, andE.
Shriberg.
1994.
Expanding the scope of the ATIStask: The ATIS-3 corpus.
In Proceedings of the work-shop on Human Language Technology, pages 43?48.Association for Computational Linguistics.Hal Daume?
III.
2007.
Fast search for dirichlet processmixture models.
In Proceedings of the Eleventh In-ternational Conference on Artificial Intelligence andStatistics (AIStats), San Juan, Puerto Rico.D.
Davidson.
1967.
6. the logical form of action sen-tences.
Essays on actions and events, 1(9):105?149.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open informationextraction.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?11, pages 1535?1545.
Association for Com-putational Linguistics.T.
Fountain and M. Lapata.
2011.
Incremental models ofnatural language category acquisition.
In Proceedingsof the 32st Annual Conference of the Cognitive ScienceSociety.D.
Garrette, K. Erk, and R. Mooney.
2011.
Integratinglogical representations with probabilistic informationusing markov logic.
In Proceedings of the Ninth In-ternational Conference on Computational Semantics,189pages 105?114.
Association for Computational Lin-guistics.D.
Graff, J. Kong, K. Chen, and K. Maeda.
2003.
Englishgigaword.
Linguistic Data Consortium, Philadelphia.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2011.
Con-crete sentence spaces for compositional distributionalmodels of meaning.
Computational Semantics IWCS2011, page 125.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the penn treebank.
Compu-tational Linguistics, 33(3):355?396.M.
Honnibal, J.R. Curran, and J. Bos.
2010.
RebankingCCGbank for improved NP interpretation.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 207?215.
Associa-tion for Computational Linguistics.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: the 90% solution.In Proceedings of the Human Language TechnologyConference of the NAACL, Companion Volume: ShortPapers, pages 57?60.
Association for ComputationalLinguistics.P.
Kingsbury and M. Palmer.
2002.
From treebank topropbank.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 1989?1993.
Citeseer.K.
Kipper, H.T.
Dang, and M. Palmer.
2000.
Class-basedconstruction of a verb lexicon.
In Proceedings of theNational Conference on Artificial Intelligence, pages691?696.
Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999.Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, EMNLP-CoNLL ?12, pages 754?765.
Association for Compu-tational Linguistics.P.
Liang, M.I.
Jordan, and D. Klein.
2011.
Learningdependency-based compositional semantics.
In Proc.Association for Computational Linguistics (ACL).Dekang Lin and Patrick Pantel.
2001.
DIRT - Discoveryof Inference Rules from Text.
In In Proceedings of theACM SIGKDD Conference on Knowledge Discoveryand Data Mining, pages 323?328.Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, RTE ?07, pages 193?200.
Associa-tion for Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.W.
McCune.
2005.
Prover9 and Mace4.http://cs.unm.edu/?mccune/mace4/.G.A.
Miller.
1995.
Wordnet: a lexical database for en-glish.
Communications of the ACM, 38(11):39?41.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.
Dis-tant supervision for relation extraction without labeleddata.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 2-Volume 2, pages 1003?1011.
Association for Computational Linguistics.J.
Mitchell and M. Lapata.
2008.
Vector-based models ofsemantic composition.
proceedings of ACL-08: HLT,pages 236?244.R.M.
Neal.
2000.
Markov chain sampling methods fordirichlet process mixture models.
Journal of compu-tational and graphical statistics, 9(2):249?265.D.O.
O?Seaghdha.
2010.
Latent variable models of se-lectional preference.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 435?444.
Association for Compu-tational Linguistics.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 1 - Volume 1, EMNLP ?09,pages 1?10.
Association for Computational Linguis-tics.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontology induction from text.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 296?305.
As-sociation for Computational Linguistics.A.
Ritter, O. Etzioni, et al2010.
A latent dirichlet al-cation method for selectional preferences.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 424?434.
Associa-tion for Computational Linguistics.Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,and Jesse Davis.
2010.
Learning first-order hornclauses from web text.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?10, pages 1088?1098.Association for Computational Linguistics.R.
Socher, B. Huval, C.D.
Manning, and A.Y.
Ng.
2012.Semantic compositionality through recursive matrix-vector spaces.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 1201?1211.
Association forComputational Linguistics.190Mark Steedman.
2000.
The Syntactic Process.
MITPress.Mark Steedman.
2012.
Taking Scope: The Natural Se-mantics of Quantifiers.
MIT Press.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In In Proceedings of ACL 2007, volume 45, page456.Ivan Titov and Alexandre Klementiev.
2011.
A bayesianmodel for unsupervised semantic parsing.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1445?1455, Portland, Oregon, USA,June.
Association for Computational Linguistics.Limin Yao, Aria Haghighi, Sebastian Riedel, and AndrewMcCallum.
2011.
Structured relation discovery usinggenerative models.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?11, pages 1456?1466.
Association forComputational Linguistics.Limin Yao, Sebastian Riedel, and Andrew McCallum.2012.
Unsupervised relation discovery with sense dis-ambiguation.
In ACL (1), pages 712?720.J.M.
Zelle and R.J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic programming.In Proceedings of the National Conference on Artifi-cial Intelligence, pages 1050?1055.191192
