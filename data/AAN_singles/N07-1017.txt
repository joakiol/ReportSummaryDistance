Proceedings of NAACL HLT 2007, pages 131?138,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsThe Domain Restriction Hypothesis:Relating Term Similarity and Semantic ConsistencyAlfio Massimiliano GliozzoITC-irstTrento, Italygliozzo@itc.itMarco PennacchiottiUniversity of Rome Tor VergataRome, Italypennacchiotti@info.uniroma2.itPatrick PantelUSC, Information Sciences InstituteMarina del Rey, CApantel@isi.eduAbstractIn this paper, we empirically demonstratewhat we call the domain restriction hy-pothesis, claiming that semantically re-lated terms extracted from a corpus tendto be semantically coherent.
We applythis hypothesis to define a post-processingmodule for the output of Espresso, a stateof the art relation extraction system, show-ing that irrelevant and erroneous relationscan be filtered out by our module, in-creasing the precision of the final output.Results are confirmed by both quantita-tive and qualitative analyses, showing thatvery high precision can be reached.1 IntroductionRelation extraction is a fundamental step inmany natural language processing applications suchas learning ontologies from texts (Buitelaar etal., 2005) and Question Answering (Pasca andHarabagiu, 2001).The most common approach for acquiring con-cepts, instances and relations is to harvest semanticknowledge from texts.
These techniques have beenlargely explored and today they achieve reasonableaccuracy.
Harvested lexical resources, such as con-cept lists (Pantel and Lin, 2002), facts (Etzioni etal., 2002) and semantic relations (Pantel and Pen-nacchiotti, 2006) could be then successfully used indifferent frameworks and applications.The state of the art technology for relation extrac-tion primarily relies on pattern-based approaches(Snow et al, 2006).
These techniques are based onthe recognition of the typical patterns that expressa particular relation in text (e.g.
?X such as Y?usually expresses an is-a relation).
Yet, text-basedalgorithms for relation extraction, in particularpattern-based algorithms, still suffer from a numberof limitations due to complexities of natural lan-guage, some of which we describe below.Irrelevant relations.
These are valid relationsthat are not of interest in the domain at hand.
Forexample, in a political domain, ?Condoleezza Riceis a football fan?
is not as relevant as ?CondoleezzaRice is the Secretary of State of the United States?.Irrelevant relations are ubiquitous, and affect ontol-ogy reliability, if used to populate it, as the relationdrives the wrong type of ontological knowledge.Erroneous or false relations.
These are particu-larly harmful, since they directly affect algorithmprecision.
A pattern-based relation extractionalgorithm is particularly likely to extract erroneousrelations if it uses generic patterns, which aredefined in (Pantel and Pennacchiotti, 2006) asbroad coverage, noisy patterns with high recall andlow precision (e.g.
?X of Y?
for part-of relation).Harvesting algorithms either ignore generic patterns(Hearst, 1992) (affecting system recall) or use man-ually supervised filtering approaches (Girju et al,2006) or use completely unsupervised Web-filteringmethods (Pantel and Pennacchiotti, 2006).
Yet,these methods still do not sufficiently mitigate theproblem of erroneous relations.Background knowledge.
Another aspect thatmakes relation harvesting difficult is related to the131nature of semantic relations: relations among enti-ties are mostly paradigmatic (de Saussure, 1922),and are usually established in absentia (i.e., they arenot made explicit in text).
According to Eco?s posi-tion (Eco, 1979), the background knowledge (e.g.
?persons are humans?)
is often assumed by thewriter, and thus is not explicitly mentioned in text.In some cases, such widely-known relations can becaptured by distributional similarity techniques butnot by pattern-based approaches.Metaphorical language.
Even when paradigmaticrelations are explicitly expressed in texts, it canbe very difficult to distinguish between facts andmetaphoric usage (e.g.
the expression ?My mind isa pearl?
occurs 17 times on the Web, but it is clearthat mind is not a pearl, at least from an ontologicalperspective).The considerations above outline some of the dif-ficulties of taking a purely lexico-syntactic approachto relation extraction.
Pragmatic issues (backgroundknowledge and metaphorical language) and onto-logical issues (irrelevant relation) can not be solvedat the syntactic level.
Also, erroneous relations canalways arise.
These considerations lead us to theintuition that extraction can benefit from imposingsome additional constraints.In this paper, we integrate Espresso with a lex-ical distribution technique modeling semantic co-herence through semantic domains (Magnini et al,2002).
These are defined as common discourse top-ics which demonstrate lexical coherence, such asECONOMICS or POLITICS.
We explore whether se-mantic domains can provide the needed additionalconstraints to mitigate the acceptance of erroneousrelations.
At the lexical level, semantic domainsidentify clusters of (domain) paradigmatically re-lated terms.
We believe that the main advantage ofadopting semantic domains in relation extraction isthat relations are established mainly among terms inthe same Domain, while concepts belonging to dif-ferent fields are mostly unrelated (Gliozzo, 2005),as described in Section 2.
For example, in a chem-istry domain, an is-a will tend to relate only terms ofthat domain (e.g., nitrogen is-a element), while out-of-domain relations are likely to be erroneous e.g.,driver is-a element.By integrating pattern-based and distributional ap-proaches we aim to capture the two characteristicproperties of semantic relations:?
Syntagmatic properties: if two terms X andY are in a given relation, they tend to co-occur in texts, and are mostly connected by spe-cific lexical-syntactic patterns (e.g., the patter?X is a Y ?
connects terms in is-a relations).This aspect is captured using a pattern-basedapproach.?
Domain properties: if a semantic relationamong two terms X and Y holds, both Xand Y should belong to the same semanticdomain (i.e.
they are semantically coherent),where semantic domains are sets of termscharacterized by very similar distributionalproperties in a (possibly domain specific)corpus.In Section 2, we develop the concept of semantic do-main and an automatic acquisition procedure basedon Latent Semantic Analysis (LSA) and we provideempirical evidence of the connection between rela-tion extraction and domain modelling.
Section 3 de-scribes the Espresso system.
Section 4 concerns ourintegration of semantic domains and Espresso.
InSection 5, we evaluate the impact of our LSA do-main restriction module on improving a state of theart relation extraction system.
In Section 6 we drawsome interesting research directions opened by ourwork.2 Semantic DomainsSemantic domains are common areas of humandiscussion, which demonstrate lexical coherence,such as ECONOMICS, POLITICS, LAW, SCIENCE,(Magnini et al, 2002).
At the lexical level, se-mantic domains identify clusters of (domain) relatedlexical-concepts, i.e.
sets of highly paradigmaticallyrelated words also known as Semantic Fields.In the literature, semantic domains have beeninferred from corpora by adopting term clusteringmethodologies (Gliozzo, 2005), and have been usedfor several NLP tasks, such as Text Categorizationand Ontology Learning (Gliozzo, 2006).Semantic domains can be described by DomainModels (DMs) (Gliozzo, 2005).
A DM is a com-132putational model for semantic domains, that repre-sents domain information at the term level, by defin-ing a set of term clusters.
Each cluster represents aSemantic Domain, i.e.
a set of terms that often co-occur in texts having similar topics.
A DM is repre-sented by a k ?
k?
rectangular matrix D, containingthe domain relevance for each term with respect toeach domain, as illustrated in Table 1.MEDICINE COMPUTER SCIENCEHIV 1 0AIDS 1 0virus 0.5 0.5laptop 0 1Table 1: Example of a Domain ModelDMs can be acquired from texts in a completelyunsupervised way by exploiting a lexical coherenceassumption.
To this end, term clustering algorithmscan be used with each cluster representing a Se-mantic Domain.
The degree of association amongterms and clusters, estimated by the learning algo-rithm, provides a domain relevance function.
Forour experiments we adopted a clustering strategybased on LSA (Deerwester et al, 1990), followingthe methodology described in (Gliozzo, 2005).
Theinput of the LSA process is a term-by-document ma-trix T reporting the term frequencies in the wholecorpus for each term.
The matrix is decomposed bymeans of a Singular Value Decomposition (SVD),identifying the principal components of T. This op-eration is done off-line, and can be efficiently per-formed on large corpora.
SVD decomposes T intothree matrixes T ' V?k?UT where ?k?
is the di-agonal k ?
k matrix containing the highest k?
?
keigenvalues of T on the diagonal, and all the re-maining elements are 0.
The parameter k?
is thedimensionality of the domain and can be fixed inadvance1.
Under this setting we define the domainmatrix DLSA2 asDLSA = INV??k?
(1)where IN is a diagonal matrix such that iNi,i =1q?
~w?i, ~w?i?and ~w?i is the ith row of the matrix V??k?
.1It is not clear how to choose the right dimensionality.
Inour experiments we used 100 dimensions.2Details of this operation can be found in (Gliozzo, 2005).Once a DM has been defined by the matrix D, theDomain Space is a k?
dimensional space, in whichboth texts and terms are associated to Domain Vec-tors (DVs), i.e.
vectors representing their domainrelevancies with respect to each domain.
The DV~t?i for the term ti ?
V is the ith row of D, whereV = {t1, t2, .
.
.
, tk} is the vocabulary of the corpus.The domain similarity ?d(ti, tj) among terms is thenestimated by the cosine among their correspondingDVs in the Domain Space, defined as follows:?d(ti, tj) = ?~ti, ~tj??
?~ti, ~ti?
?~tj , ~tj?
(2)Figure 1: Probability of finding paradigmatic rela-tionsThe main advantage of adopting semantic do-mains for relation extraction is that they allow us toimpose a domain restriction on the set of candidatepairs of related terms.
In fact, semantic relations canbe established mainly among terms in the same Se-mantic Domain, while concepts belonging to differ-ent fields are mostly unrelated.To show the validity of the domain restriction weconducted a preliminary experiment, contrasting theprobability for two words to be related in Word-Net (Magnini and Cavaglia`, 2000) with their domainsimilarity, measured in the Domain Space inducedfrom the British National Corpus.
In particular, foreach couple of words, we estimated the domain sim-ilarity, and we collected word pairs in sets charac-terized by different ranges of similarity (e.g.
all thepairs between 0.8 and 0.9).
Then we estimated the133probability of each couple of words in different setsto be linked by a semantic relation in WordNet, suchas synonymy, hyperonymy, co-hyponymy and do-main in WordNet Domains (Magnini et al, 2002).Results in Figure 1 show a monotonic crescent rela-tion between these two quantities.
In particular theprobability for two words to be related tends to 0when their similarity is negative (i.e., they are notdomain related), supporting the basic hypothesis ofthis work.
In Section 4 we will show that this prop-erty can be used to improve the overall performancesof the relation extraction algorithm.3 The pattern-based Espresso systemEspresso (Pantel and Pennacchiotti, 2006) is acorpus-based general purpose, broad, and accuraterelation extraction algorithm requiring minimal su-pervision, whose core is based on the frameworkadopted in (Hearst, 1992).
Espresso introduces twomain innovations that guarantee high performance:(i) a principled measure for estimating the reliabil-ity of relational patterns and instances; (ii) an algo-rithm for exploiting generic patterns.
Generic pat-terns are broad coverage noisy patterns (high recalland low precision), e.g.
?X of Y?
for the part-of re-lation.
As underlined in the introduction, previousalgorithms either required significant manual workto make use of generic patterns, or simply ignorethem.
Espresso exploits an unsupervised Web-basedfiltering method to detect generic patterns and to dis-tinguish their correct and incorrect instances.Given a specific relation (e.g.
is-a) and a POS-tagged corpus, Espresso takes as input few seedinstances (e.g.
nitrogen is-a element) or seed surfacepatterns (e.g.
X/NN such/JJ as/IN Y/NN).
It thenincrementally learns new patterns and instancesby iterating on the following three phases, until aspecific stop condition is met (i.e., new patterns arebelow a pre-defined threshold of reliability).Pattern Induction.
Given an input set of seedinstances I , Espresso infers new patterns connectingas many instances as possible in the given corpus.To do so, Espresso uses a slight modification of thestate of the art algorithm described in (Ravichandranand Hovy, 2002).
For each instance in input, thesentences containing it are first retrieved and thengeneralized, by replacing term expressions with aterminological label using regular expressions onthe POS-tags.
This generalization allows to easethe problem of data sparseness in small corpora.Unfortunately, as patterns become more generic,they are more prone to low precision.Pattern Ranking and Selection.
Espresso ranksall extracted patterns using a reliability measure rpiand discards all but the top-k P patterns, where k isset to the number of patterns from the previous iter-ation plus one.
rpi captures the intuition that a reli-able pattern is one that is both highly precise and onethat extracts many instances.
rpi is formally definedas the average strength of association between a pat-tern p and each input instance i in I , weighted by thereliability r?
of the instance i (described later):rpi(p) =?i?I(pmi(i,p)maxpmi ?
r?
(i))|I|where pmi(i, p) is the pointwise mutual information(pmi) between i and p (estimated with MaximumLikelihood Estimation), and maxpmi is the maxi-mum pmi between all patterns and all instances.Instance Extraction, Ranking, Selection.Espresso extracts from the corpus the set of in-stances I matching the patterns in P .
In this phasegeneric patterns are detected, and their instancesare filtered, using a technique described in detail in(Pantel and Pennacchiotti, 2006).
Instances are thenranked using a reliability measure r?, similar to thatadopted for patterns.
A reliable instance should behighly associated with as many reliable patterns aspossible:r?
(i) =?p?P(pmi(i,p)maxpmi ?
rpi(i))|P |Finally, the best scoring instances are selected forthe following iteration.
If the number of extractedinstances is too low (as often happens in smallcorpora) Espresso enters an expansion phase, inwhich instances are expanded by using web basedand syntactic techniques.134The output Espresso is a list of instancesi = (X,Y ) ?
I , ranked according to r?(i).
Thisscore accounts for the syntagmatic similarity be-tween X and Y , i.e., how strong is the co-occurrenceof X and Y in texts with a given pattern p.A key role in the Espresso algorithm is playedby the reliability measures.
The accuracy of thewhole extraction process is in fact highly sensitiveto the ranking of patterns and instances because, ateach iteration, only the best scoring entities are re-tained.
For instance, if an erroneous instance is se-lected after the first iteration, it could in theory af-fect the following pattern extraction phase and causedrift in consequent iterations.
This issue is criti-cal for generic patterns (where precision is still aproblem, even with Web-based filtering), and couldsometimes also affect non-generic patterns.It would be then useful to integrate Espresso witha technique able to retain only very precise in-stances, without compromising recall.
As syntag-matic strategies are already in place, another strategyis needed.
In the next Section, we show how this canbe achieved using instance domain information.4 Integrating syntagmatic and domaininformationThe strategy of integrating syntagmatic and do-main information has demonstrated to be fruitful inmany NLP tasks, such as Word Sense Disambigua-tion and open domain Ontology Learning (Gliozzo,2006).
According to the structural view (de Saus-sure, 1922), both aspects contribute to determinethe linguistic value (i.e.
the meaning) of words:the meaning of lexical constituents is determinedby a complex network of semantic relations amongwords.
This suggests that relation extraction canbenefit from accounting for both syntagmatic anddomain aspects at the same time.To demonstrate the validity of this claim we canexplore many different integration schemata.
For ex-ample we can restrict the search space (i.e.
the set ofcandidate instances) to the set of all those terms be-longing to the same domain.
Another possibility isto exploit a similarity metric for domain relatednessto re-rank the output instances I of Espresso, hopingthat the top ranked ones will mostly be those whichare correct.
One advantage of this latter method-ology is that it can be applied to the output of anyrelation extraction system without any modificationto the system itself.
In addition, this methodologycan be evaluated by adopting standard InformationRetrieval (IR) measures, such as mean average pre-cision (see Section 5).
Because of these advantages,we decided to adopt the re-ranking procedure.The procedure is defined as follows: each in-stance extracted by Espresso is assigned a DomainSimilarity score ?d(X,Y ) estimated in the domainspace according to Equation 2; a higher score isthen assigned to the instances that tend to co-occurin the same documents in the corpus.
For exam-ple, the candidate instances ethanol is-a nonaro-matic alcohol has a higher score than ethanol is-asomething, as ethanol and alcohol are both from thechemistry domain, while something is a generic termand is thus not associated to any domain.Instances are then re-ranked according to?d(X,Y ), which is used as the new index ofreliability instead of the original reliability scoresof Espresso.
In Subsection 5.2 we will show thatthe re-ranking technique improves the originalreliability scores of Espresso.5 EvaluationIn this Section we evaluate the benefits of applyingthe domain information to relation extraction (ESP-LSA), by measuring the improvements of Espressodue to domain based re-ranking.5.1 Experimental SettingsAs a baseline system, we used the ESP- implemen-tation of Espresso described in (Pantel and Pennac-chiotti, 2006).
ESP- is a fully functioning Espressosystem, without the generic pattern filtering module(ESP+).
We decided to use ESP- for two main rea-sons.
First, the manual evaluation process wouldhave been too time consuming, as ESP+ extractsthousands of relations.
Also, the small scale experi-ment for EXP- allows us to better analyse and com-pare the results.To perform the re-ranking operation, we acquireda Domain Model from the input corpus itself.
To thisaim we performed a SVD of the term by documentmatrix T describing the input corpus, indexing allthe candidate terms recognized by Espresso.135As an evaluation benchmark, we adopted thesame instance sets extracted by ESP- in the ex-periment described in (Pantel and Pennacchiotti,2006).
We used an input corpus of 313,590 words,a college chemistry textbook (Brown et al 2003),pre-processed using the Alembic Workbench POS-tagger (Day et al 1997).
We considered the fol-lowing relations: is-a, part-of, reaction (a relationof chemical reaction among chemical entities) andproduction (a process or chemical element/objectproducing a result).
ESP- extracted 200 is-a, 111part-of, 40 reaction and 196 production instances.5.2 Quantitative AnalysisThe experimental evaluation compared the accuracyof the ranked set of instances extracted by ESP- withthe re-ranking produced on these instances by ESP-LSA.
By analogy to IR, we are interested in ex-tracting positive instances (i.e.
semantically relatedwords).
Accordingly, we utilize the standard defi-nitions of precision and recall typically used in IR .Table 2 reports the Mean Average Precision obtainedby both ESP- and ESP-LSA on the extracted rela-tions, showing the substantial improvements on allthe relations due to domain based re-ranking.ESP- ESP-LSAis-a 0.54 0.75 (+0.21)part-of 0.65 0.82 (+0.17)react 0.75 0.82 (+0.07)produce 0.55 0.62 (+0.07)Table 2: Mean Average Precision reported by ESP-and ESP-LSAFigures 2, 3, 4 and 5 report the precision/recallcurves obtained for each relation, estimated by mea-suring the precision / recall at each point of theranked list.
Results show that precision is very highespecially for the top ranked relations extracted byESP-LSA.
Precision reaches the upper bound for thetop ranked part of the part-of relation, while it isclose to 0.9 for the is-a relation.
In all cases, theprecision reported by the ESP-LSA system surpassthose of the ESP- system at all recall points.5.3 Qualitative AnalysisTable 3 shows the best scoring instances for ESP-and ESP-LSA on the evaluated relations.
ResultsFigure 2: Syntagmatic vs. Domain ranking for theis-a relationFigure 3: Syntagmatic vs. Domain ranking for theproduce relationshow that ESP-LSA tends to assign a much lowerscore to erroneous instances, as compared to theoriginal Espresso reliability ranking.
For exam-ple for the part-of relation, the ESP- ranks the er-roneous instance geometry part-of ion in 23th po-sition, while ESP-LSA re-ranks it in 92nd.
Inthis case, a lower score is assigned because ge-ometry is not particularly tied to the domain ofchemistry.
Also, ESP-LSA tends to penalize in-stances derived from parsing/tokenization errors:136Figure 4: Syntagmatic vs. Domain ranking for thepart-of relationFigure 5: Syntagmatic vs. Domain ranking for thereact relation] binary hydrogen compounds hydrogen react ele-ments is 16th for ESP-, while in the last tenth ofthe ESP-LSA.
In addition, out-of-domain relationsare successfully interpreted by ESP-LSA.
For ex-ample, the instance sentences part-of exceptions isa possibly correct relation, but unrelated to the do-main, as an exception in chemistry has nothing todo with sentences.
This instance lies at the bottomof the ESP-LSA ranking, while is in the middle ofESP- list.
Also, low ranked and correct relations ex-tracted by ESP- emerge with ESP-LSA.
For exam-ple, magnesium metal react elemental oxygen lies atthe end of ESP- rank, as there are not enough syntag-matic evidence (co-occurrences) that let the instanceemerge.
The domain analysis of ESP-LSA promotesthis instance to the 2nd rank position.
However, infew cases, the strategy adopted by ESP-LSA tendsto promote erroneous instances (e.g.
high voltageproduce voltage).
Yet, results show that these areisolated cases.6 Conclusion and future workIn this paper, we propose the domain restriction hy-pothesis, claiming that semantically related termsextracted from a corpus tend to be semantically co-herent.
Applying this hypothesis, we presented anew method to improve the precision of pattern-based relation extraction algorithms, where the inte-gration of domain information allows the system tofilter out many irrelevant relations, erroneous can-didate pairs and metaphorical language relationalexpressions, while capturing the assumed knowl-edge required to discover paradigmatic associationsamong terms.
Experimental evidences supports thisclaim both qualitatively and quantitatively, openinga promising research direction, that we plan to ex-plore much more in depth.
In the future, we planto compare LSA to other term similarity measures,to train the LSA model on large open domain cor-pora and to apply our technique to both generic andspecific corpora in different domains.
We want alsoto increase the level of integration of the LSA tech-nique in the Espresso algorithm, by using LSA as analternative reliability measure at each iteration.
Wewill also explore the domain restriction property ofsemantic domains to develop open domain ontologylearning systems, as proposed in (Gliozzo, 2006).The domain restriction hypothesis has potentialto greatly impact many applications where match-ing textual expressions is a primary component.
It isour hope that by combining existing ranking strate-gies in applications such as information retrieval,question answering, information extraction and doc-ument classification, with knowledge of the coher-ence of the underlying text, one will see significantimprovements in matching accuracy.137Relation ESP- ESP - LSAX is-a Y Aluminum ; metal F ; electronegative atomsnitride ion ; strong Br O ; electronegative atomsheat flow ; calorimeter NaCN ; cyanide saltcomplete ionic equation ; spectator NaCN ; cyanide saltsX part-of Y elements ; compound amino acid building blocks ; tripeptidecomposition ; substance acid building blocks ; tripeptideblocks ; tripeptide powdered zinc metal ; batteryelements ; sodium chloride building blocks ; tripeptideX react Y hydrazine ; water magnesium metal ; elemental oxygenmagnesium metal ; hydrochloric acid nitrogen ; ammoniamagnesium ; oxygen sodium metal ; chloridemagnesium metal ; acid carbon dioxide ; methaneX produce Y bromine ; bromide high voltage ; voltageoxygen ; oxide reactions ; reactionscommon fuels ; dioxide dr jekyll ; hydekidneys ; stones yellow pigments ; green pigmentTable 3: Top scoring relations extracted by ESP- and ESP-LSA.AcknowledgmentsThanks to Roberto Basili for his precious comments,suggestions and support.
Alfio Gliozzo was sup-ported by the OntoText project, funded by the Au-tonomous Province of Trento under the FUP-2004,and the FIRB founded project N.RBIN045PXH.ReferencesP.
Buitelaar, P. Cimiano, and B. Magnini.
2005.
On-tology learning from texts: methods, evaluation andapplications.
IOS Press.F.
de Saussure.
1922.
Cours de linguistique ge?ne?rale.Payot, Paris.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society of InformationScience.U.
Eco.
1979.
Lector in fabula.
Bompiani.O.
Etzioni, M.J. Cafarella, D. Downey, A.-MA.M.
Popescu, T. Shaked, S. Soderland, D.S.Weld, and A. Yates.
2002.
Unsupervised named-entity extraction from the web: An experimentalstudy.
Artificial Intelligence, 165(1):91?143.R.
Girju, A. Badulescu, and D. Moldovan.
2006.
Learn-ing semantic constraints for the automatic discovery ofpart-whole relations.
In Proceedings of HLT/NAACL-03, pages 80?87, Edmonton, Canada, July.A.
Gliozzo.
2005.
Semantic Domains in ComputationalLinguistics.
Ph.D. thesis, University of Trento.A.
Gliozzo.
2006.
The god model.
In Proceedings ofEACL.M.A.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th In-ternational Conference on Computational Linguistics.Nantes, France.B.
Magnini and G. Cavaglia`.
2000.
Integrating subjectfield codes into WordNet.
In Proceedings of LREC-2000, pages 1413?1418, Athens, Greece, June.B.
Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.2002.
The role of domain information in wordsense disambiguation.
Natural Language Engineer-ing, 8(4):359?373.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proceedings of ACM Conference onKnowledge Discovery and Data Mining, pages 613?619.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging generic patterns for automatically harvesting se-mantic relations.
In ACL-COLING-06, pages 113?120, Sydney, Australia.M.
Pasca and S. Harabagiu.
2001.
The informative roleof wordnet in open-domain question answering.
InProceedings of NAACL-01 Workshop on WordNet andOther Lexical Resources, pages 138?143, Pittsburgh,PA.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
In Pro-ceedings of ACL-02, pages 41?47, Philadelphia, PA.R.
Snow, D. Jurafsky, and A.Y.
Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InProceedings of the ACL/COLING-06, pages 801?808,Sydney, Australia.138
