Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1586?1597,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsIt Takes Three to Tango: Triangulation Approach to Answer Rankingin Community Question AnsweringPreslav Nakov, Llu?
?s Ma`rquez and Francisco Guzma?nArabic Language Technologies Research GroupQatar Computing Research Institute, HBKU{pnakov,lmarquez,fguzman}@qf.org.qaAbstractWe address the problem of answering newquestions in community forums, by selectingsuitable answers to already asked questions.We approach the task as an answer rankingproblem, adopting a pairwise neural networkarchitecture that selects which of two compet-ing answers is better.
We focus on the util-ity of the three types of similarities occurringin the triangle formed by the original ques-tion, the related question, and an answer tothe related comment, which we call relevance,relatedness, and appropriateness.
Our pro-posed neural network models the interactionsamong all input components using syntac-tic and semantic embeddings, lexical match-ing, and domain-specific features.
It achievesstate-of-the-art results, showing that the threesimilarities are important and need to be mod-eled together.
Our experiments demonstratethat all feature types are relevant, but the mostimportant ones are the lexical similarity fea-tures, the domain-specific features, and thesyntactic and semantic embeddings.1 IntroductionIn recent years, community Question Answering(cQA) forums, such as StackOverflow, Quora, QatarLiving, etc., have gained a lot of popularity as asource of knowledge and information.
These forumstypically organize their content in the form of multi-ple topic-oriented question?comment threads, wherea question posed by a user is followed by a list ofother users?
comments, which intend to answer thequestion.Many of such on-line forums are not moderated,which often results in (a) noisy and (b) redundantcontent, as users tend to deviate from the questionand start asking new questions or engage in conver-sations, fights, etc.Web forums try to solve problem (a) in variousways, most often by allowing users to up/down-vote answers according to their perceived useful-ness, which makes it easier to retrieve useful an-swers in the future.
Unfortunately, this negativelypenalizes recent comments, which might be the mostrelevant and updated ones.
This is due to the time ittakes for a comment to accumulate votes.
Moreover,voting is prone to abuse by forum trolls (Mihaylovet al, 2015; Mihaylov and Nakov, 2016a).Problem (b) is harder to solve, as it requires thatusers verify that their question has not been askedbefore, possibly in a slightly different way.
Thissearch can be hard, especially for less experiencedusers as most sites only offer basic search, e.g., a sitesearch by Google.
Yet, solving problem (b) automat-ically is important both for site owners, as they wantto prevent question duplication as much as possible,and for users, as finding an answer to their ques-tions without posting means immediate satisfactionof their information needs.In this paper, we address the general problemof finding good answers to a given new ques-tion (referred to as original question) in one suchcommunity-created forum.
More specifically, weuse a pairwise deep neural network to rank com-ments retrieved from different question-commentthreads according to their relevance as answers tothe original question being asked.1586A key feature of our approach is that we inves-tigate the contribution of the edges in the trian-gle formed by the pairwise interactions between theoriginal question, the related question, and the re-lated comments to rank comments in a unified fash-ion.
Additionally, we use three different sets of fea-tures that capture such similarity: lexical, distributed(semantics/syntax), and domain-specific knowledge.The experimental results show that addressing theanswer ranking task directly, i.e., modelling onlythe similarity between the original question andthe answer-candidate comments, yields very lowresults.
The other two edges of the triangle areneeded to obtain good results, i.e., the similarity be-tween the original question and the related questionand the similarity between the related question andthe related comments.
Both aspects add significantand cumulative improvements to the overall perfor-mance.
Finally, we show that the full network, in-cluding the three pairs of similarities, outperformsthe state-of-the-art on a benchmark dataset.The rest of the paper is organized as follows: Sec-tion 2 discusses the similarity triangle in answerranking for cQA, Section 3 presents our pairwiseneural network model for answering new questionsin community forums, which integrates multiple lev-els of interaction, Section 4 describes the features weused, Section 5 presents our evaluation setup, the ex-periments and the results, Section 6 discusses somerelated work, and Section 7 wraps up the paper witha brief summary of the contributions and some pos-sible directions for future work.2 The Similarity Triangle in cQAFigure 1 presents an example illustrating the simi-larity triangle that we use when solving the answerranking problem in cQA.
In the figure, q stands forthe new question, q?
is an existing related question,and c is a comment within the thread of question q?.The edge qc relates to the main cQA task ad-dressed in this paper, i.e., deciding whether a com-ment for a potentially related question is a good an-swer to the original question.
We will say that therelation captures the relevance of c for q.The edge qq?
represents the similarity between theoriginal and the related questions.
We will call thisrelation relatedness.Can I drive with an Australian driver?s license in Qatar?
q:q?
: How long can i drive in Qatar with my international driver's permit before I'm forced to change my Australian license to a Qatari one?
When I do change over to a Qatar license do I actually lose my Australian license?
I'd prefer to keep it if possible... c: depends on the insurer, Qatar Insurance Company said this in email to me:?Thank you for your email!
With regards to your query below, a foreigner is valid to drive in Doha with the following conditions: Foreign driver with his country valid driving license allowed driving only for one week from entry date Foreign driver with international valid driving license allowed driving for 6 months from entry date Foreign driver with GCC driving license allowed driving for 3 months from entry?.
As an Aussie your driving licence should be transferable to a Qatar one with only the eyetest (temporary, then permanent once RP sorted).Figure 1: The similarity triangle in cQA.Finally, the edge q?c represents the decision ofwhether c is a good answer for the question from itsthread, q?.
We will call this relation appropriateness.In this particular example, q and q?
are indeed re-lated, and c is a good answer for both q?
and q.1In the past, the approaches to cQA were focusedon using information from the new question q, anexisting related question q?, and a comment c withinthe thread of q?, to solve different cQA sub-tasks.For example, answer selection, which selects themost appropriate comment c within the thread q?,was addressed in SemEval-2015 Task 3 (Nakov etal., 2015).
Similarly, question?question similarity,which looks for the most related questions to a givenquestion, was addressed by many authors (Jeon etal., 2005; Duan et al, 2008; Li and Manandhar,2011; Zhou et al, 2015; dos Santos et al, 2015).In this paper, we solve the cQA task problem2 ina novel way by using the three types of similaritiesjointly.
Our main hypothesis is that relevance, ap-propriateness, and relatedness are essential to find-ing the best answer in a community Question An-swering setting.
Below we present experimental re-sults that support this hypothesis.1The essence of this triangle is also described in SemEval2016 Task 3 to motivate a three-subtask setting for cQA (Nakovet al, 2016).
In that evaluation exercise, q?c and qq?
are pre-sented as subtask A and subtask B, respectively.
In this paper,we mainly use them as similarity relations to be modeled in thelearning architecture to solve the answer ranking task.2We use the task setup and the datasets from SemEval-2016Task 3, focusing on subtask C (Nakov et al, 2016).15873 Neural Model for Answer RankingAs explained above, we tackle answer ranking asa three-way similarity problem, exploring similar-ity features that capture lexical, distributed (seman-tics and syntax), and domain-specific knowledge.
Toachieve this, we propose a pairwise neural network(NN) approach for the cQA task, which is inspiredby our NN framework for machine translation eval-uation (Guzma?n et al, 2015).3 The input of the NNconsists of the original question q, two competingcomments, c1 and c2, and the questions from thethreads of the two comments, q?1 and q?2.
The out-put of the network is a decision about which of thetwo comments is a better answer to q.The main properties of our NN approach can besummarized as follows: (i) it works in a pairwisefashion, which is appropriate for the ranking natureof the cQA problem; (ii) it allows for an easy in-corporation of rich syntactic and semantic embed-ded representations of the input texts; (iii) it modelsnon-linear relationships between all input elements(q, c1, c2, q?1 and q?2), which allows us to study theinteractions and the impact of the three types of sim-ilarity (relevance, relatedness and appropriateness)when solving the answer ranking task.3.1 ArchitectureOur full NN model for pairwise answer ranking isdepicted in Figure 2.
We have a binary classificationtask with input x = (q, q?1, c1, q?2, c2), which shouldoutput 1 if c1 is a better answer to the original ques-tion q than c2, and 0 otherwise.4 In this setting, q?1and q?2 are questions related to q, whose threads con-tain the comments c1 and c2, respectively.
They pro-vide useful information to link the two comments tothe original question.
On the one hand, they allowto predict whether the comments are good answerswithin their respective threads.
On the other hand,they allow to infer whether the questions for whichthe comments were produced are closely related tothe original question.
The pair of comments can be-long to the same thread (i.e., q?1 ?
q?2) or they cancome from different threads.3Also, we previously used a similar framework for findinggood answers in a question-comment thread (Guzma?n et al,2016a; Guzma?n et al, 2016b).4In this work, we do not learn to predict ties, and ties areexcluded from our training data.f(q,q'1,c1,q'2,c2)?(q,q'1)?
(q,c1)hq1hq2h12vxc1xc2xqqc1c2sentences  embeddings pairwise nodes pairwise featuresoutput layerq'1xq'2q'2xq'1?(q'1,c1)?
(q,q'2) ?
(q,c2) ?
(q'2,c2)Figure 2: The overall architecture of our neural net-work model for pairwise answer ranking in commu-nity question answering.The feed-forward neural network com-putes a sigmoid function f(q, q?1, c1, q?2, c2) =sig(wTv ?
(q, q?1, c1, q?2, c2) + bv), where ?(.
)transforms the input through the hidden layer,wv are the weights from the hidden layerto the output layer, and bv is a bias term.The function ?(.)
is actually a concatenationof three subfunctions: ?
(q, q?1, c1, q?2, c2) =[?1(q, q?1, c1), ?2(q, q?2, c2), ?1,2(q?1, c1, q?2, c2)].We first map the question and the comments toa fixed-length vector [xq,xq?1 ,xc1 ,xq?2 ,xc2 ] usingsyntactic and semantic embeddings.
Then, we feedthis vector as input to the neural network, whichmodels several types of interactions, using differentgroups of nodes in the hidden layer.
Overall, wemake use of three different groups of nodes in thehidden layer.The first two groups include the relevance nodeshq1 and hq2.
These groups of hidden nodes modelhow relevant comment cj is to the original questionq given that it belongs to the thread of the relatedquestion q?j .
In these hidden nodes, we model com-plex non-linear interactions between the distributedrepresentations of q, q?j and cj .
Intuitively, thesenodes are designed to learn to distinguish a relevantcomment by extracting features from the distributedrepresentations of a comment and of the question itis supposed to answer.1588The last group of nodes in the hidden layer is thesimilarity node h12.
It measures the similarity be-tween c1 and c2 and their respective questions q?1and q?2.
This node is designed to compute the non-linear interactions between the syntactic and seman-tic representations of comment-comment, comment-question and question-question pairs.
Intuitively,this can help disambiguate when comments are verysimilar or were generated from the same or fromvery similar questions.The model further allows to incorporate exter-nal sources of information in the form of skiparcs that go directly from the input to the outputlayer, skipping the hidden layer.
These arcs rep-resent pairwise similarity feature vectors inspiredby the edges of the triangle in Figure 1.
In Fig-ure 2, we indicate these pairwise external featuresets as: ?
(q, q?1), ?
(q, q?2) for relatedness; ?
(q?1, c1),?
(q?2, c2) for appropriateness; and ?
(q, c1), ?
(q, c2)for relevance.
When including the skip-arc features,the activation at the output is f(q, q?1, c1, q?2, c2) =sig(wTv [?
(q, q?1, c1, q?2, c2), ?
(q, q?1), ?
(q, q?2),?
(q?1, c1), ?
(q?2, c2), ?
(q, c1), ?
(q, c2)] + bv).We use these feature vectors to encode ma-chine translation evaluation measures, componentsthereof, cQA task-specific features, etc.
The nextsection gives more detail about these features.4 FeaturesWe experiment with three kinds of features: (i) lexi-cal features that measure similarity at a word, wordn-gram, and paraphrase level, (ii) distributed repre-sentations that measure similarity at a syntactic andsemantic level, (iii) domain-specific knowledge fea-tures, which capture similarity using thread-level in-formation and other features that have proven valu-able to solve similar tasks (Nicosia et al, 2015).4.1 Lexical similarity featuresThese types of features measure similarity at a sur-face level between the following pairs: (q,q?1), (q,q?2),(q?1, c1), (q?2, c2), (q1, c1), and (q2, c2).
They are in-spired by our previous work on Machine Transla-tion Evaluation (MTE) (Guzma?n et al, 2015), andwe previously found them useful for finding goodanswers in a question-comment thread (Guzma?n etal., 2016a; Guzma?n et al, 2016b).MTFEATS We use (as pairwise features) the fol-lowing six machine translation evaluation features:(i) BLEU: This is the most commonly used mea-sure for machine translation evaluation, which isbased on n-gram overlap and length ratios (Papineniet al, 2002).
(ii) NIST: This measure is similarto BLEU, and is used at evaluation campaigns runby NIST (Doddington, 2002).
(iii) TER: Trans-lation error rate; it is based on the edit distancebetween a translation hypothesis and the reference(Snover et al, 2006).
(iv) METEOR: A complexmeasure, which matches the hypothesis and the ref-erence using synonyms and paraphrases (Lavie andDenkowski, 2009).
(v) Unigram PRECISION andRECALL.BLEUCOMP Following (Guzma?n et al, 2015),we further use as features various components thatare involved in the computation of BLEU: n-gramprecisions, n-gram matches, total number of n-grams (n=1,2,3,4), lengths of the hypotheses andof the reference, length ratio between them, andBLEU?s brevity penalty.
Again, these are computedover the same six pairs of vectors as before.4.2 Distributed representationsWe use the following vector-based embeddings ofall input components: q, c1, c2, q?1, and q?2.GOOGLE VEC We use the pre-trained, 300-dimensional embedding vectors from WORD2VEC(Mikolov et al, 2013).
We compute a vector rep-resentation of the text by simply averaging over theembeddings of all words in the text.QL VEC We train in-domain word embeddingsusing WORD2VEC on all available QatarLiving data.Again, we use these embeddings to compute 100-dimensional vector representations for all inputcomponents by averaging over all words in the texts.SYNTAX VEC We parse the entire ques-tion/comment using the Stanford neural parser(Socher et al, 2013), and we use the final 25-dimensional vector that is produced internally as aby-product of parsing.Moreover, we use the above vectors to calcu-late pairwise similarity features, i.e., the cosine be-tween the following six vector pairs: (q, c1), (q, c2),(q?1, c1), (q?2, c2), (q, q?1) and (q, q?2).15894.3 Domain-specific featuresWe extract various domain-specific features that usethread-level and other useful information known tocapture relatedness and appropriateness.SAME AUTHOR We have a thread-level meta-feature, which we apply to the pairs (q?1, c1), (q?2, c2).It checks whether the person answering the questionis also the one who asked it, i.e., do the related ques-tion and the comment have the same author.
Theidea is that the person asking a question is unlikelyto answer his/her own question, but s/he could aska clarification question or thank another person whohas provided a useful answer earlier in the thread.CQ?RANK FEAT We further have two thread-level meta-features related to the rank of the com-ment in the thread, which we apply to the pairs(q?1, c1) and (q?2, c2): (i) reciprocal rank of the com-ment in the thread, i.e., 1/?, where ?
is the rank ofthe comment; (ii) percentile of the number of com-ments in the thread, calculated as follows: the firstcomment gets the score of 1.0, the second one gets0.9, and so on.
Note that in our dataset, there areexactly ten comments per thread.QQ?RANK FEAT We also have three featuresmodeling the rank of the related question in the listof related questions for the original question, whichwe apply to the pairs (q, q?1) and (q, q?2).In total, use the following six features: (i) the re-ciprocal rank of q?1 or q?2 in the list of related ques-tions for q; (ii) the reciprocal ordinal rank5 of q?1 orq?2 in the list of related questions for q; (iii) the per-centile of the q?1 or q?2 in the list of related questionsfor q, calculated as for the comments.CQRANK FEAT.
Finally, we have features for therank of the comment in the list of 100 comments forthe original question, which we apply to the pairs(q, c1) and (q, c2): (i) reciprocal rank of the commentin the list; (ii) percentile of the comment in the list.5The related questions are obtained using a query to a searchengine (using words from the original question), with resultslimited to QatarLiving.
However, some of the returned resultspointed to the wrong (non-forum) sections of the website or toquestions with less than ten comments, and these were skipped.Suppose that the surviving top ten related questions were atranks 3, 7, 18, ... in the original list.
Now, we can use theseranks ?, or we can use instead the ordinal ranks r: 1, 2, 3, ...TASK FEAT.
We further have features that havebeen proven useful in the answer selection taskfrom SemEval 2015 Task 3 (Nakov et al, 2015).This includes some comment-specific features,which refer to c1 and c2 only, but which weapply twice, to generate features for the pairs(q?1, c1), (q?2, c2), (q1, c1), and (q2, c2): numberof URLs/images/emails/phone numbers; number ofoccurrences of the string thank;6 number of to-kens/sentences; average number of tokens; numberof nouns/verbs/adjectives/adverbs/pronouns; num-ber of positive/negative smileys; number of sin-gle/double/triple exclamation/ interrogation sym-bols; number of interrogative sentences (based onparsing); number of words that are not in word2vec?sGoogle News vocabulary.7And also some question-comment pair fea-tures, which we apply to the pairs (q?1, c1),(q?2, c2), (q1, c1), and (q2, c2): (i) question to com-ment count ratio in terms of sentences/tokens/nouns/verbs/adjectives/adverbs/pronouns; (ii) ques-tion to comment count ratio of words that are not inword2vec?s Google News vocabulary.5 Experiments and ResultsWe experimented with the data from SemEval-2016 Task 3 on ?Community Question Answering?.More precisely, the problem addressed is subtask C(Question?External Comment Similarity), which isthe primary cQA task.
For a given new question (re-ferred to as the original question), the task providesthe set of the first ten related questions (retrievedby a search engine), each associated with the firstten comments appearing in the question-commentthread.
The goal then is to rank the total of 100comments according to their appropriateness withrespect to the original question.In this framework, the retrieval part of the task isdone as a pre-processing step, and the challenge isto learn to rank all good comments above all badones.
All the data comes from the QatarLiving fo-rum, and the related questions are obtained usingGoogle search with the original question?s text lim-ited to the www.qatarliving.com domain.6When an author thanks somebody, this post is typically abad answer to the original question.7Can detect slang, foreign language, etc., which would indi-cate a bad answer.1590The task offers a higher quality training datasetTRAIN-PART1, which includes 200 original ques-tions, 1,999 related questions and 19,990 comments,and a lower-quality TRAIN-PART2, which we didnot use.
Additionally, it provides a developmentset (DEV, with 50 original questions, 500 relatedquestions and 5,000 related comments) and a TESTset (70 original questions, 700 related questions and7,000 related comments).
Apart from the class la-bels for subtask C, the datasets also offer class labelsfor subtask A (i.e., whether a comment is a good an-swer to the question in the thread) and subtask B(i.e., whether the related questions is relevant for theoriginal question).5.1 Settingwe use Theano (Bergstra et al, 2010) to train ourmodel on TRAIN-PART1 with hidden layers of size3 for 100 epochs with minibatches of size 30, regu-larization of 0.05, and a learning rate of 0.01, usingstochastic gradient descent with adagrad (Duchi etal., 2011).
We normalize the input feature values tothe [?1; 1] interval using minmax, and we initializethe NN weights by sampling from a uniform distri-bution as in (Bengio and Glorot, 2010).We evaluate the model on DEV after each epoch,and ultimately we keep the model that achieves thehighest accuracy;8 in case of a tie, we prefer the pa-rameters from a later epoch.
We selected the aboveparameter values on the DEV dataset using the fullmodel, and we use them for all experiments in Sec-tion 5.3, where we evaluate on the TEST dataset.Note that, we train the NN using all pairs of(Good, Bad) comments, in both orders, ignoringties.
At test time, we compute the full ranking ofcomments by scoring all possible pairs, and by thenaccumulating the scores at the comment level.5.2 Evaluation and baselinesThe results are calculated with the official scorerfrom the SemEval-2016 Task 3.
We report threeranking-based measures that are commonly ac-cepted in the IR community: Mean average preci-sion (MAP), which is the official evaluation mea-sure of the task, average recall (AvgRec), and meanreciprocal rank (MRR).8We tried Kendall?s Tau (?
), but it performed slightly worse.For comparison purposes, we report the results fortwo baselines.
One corresponds to a random order-ing of the comments, assuming zero knowledge ofthe task.
The second one is a more realistic baseline,which keeps the question ranking from the searchengine (Google search) and the chronological or-der of the comments within the thread of teh relatedquestion.
Although this may be considered a veryna?
?ve baseline, it is actually notably informed.
Thequestion ranking from Google search takes into ac-count the relevance of the entire thread (question andcomments) to the original question.
Moreover, thereis a natural concentration of the best answers in thefirst comments of the threads.5.3 Main resultsTable 1 shows the evaluation results on the TESTdataset for several variants of our pairwise neuralnetwork architecture.
Regarding our network con-figurations, we present the results from simpler tomore complex.Relevance The ?Relevance only?
network con-tains only the relevance relations and features cor-responding to q, c1 and c2.
The rest of the com-ponents are deactivated in the network.
This corre-sponds to solving the task without any informationabout the related questions and the appropriatenessof the comments in their threads, i.e., just by com-paring the texts of the comments and of the originalquestion.
In some sense, this setup is largely lessinformed than the IR baseline.
The results are verylow, being only ?7 MAP points higher than the ran-dom baseline.Relevance + appropriateness Adding the appro-priateness interactions between c1 and q?1, and be-tween c2 and q?2 improves MAP by ?9 points.
Al-though more informed, as some information fromthe related questions is taken indirectly, the resultsof this system are still below the IR baseline.Relevance + relatedness Adding the relatednessinteractions and features between q and q?1, and q andq?2, turns out to be crucial.
When added to the ?Rel-evance only?
basic system, the MAP score jumpsto 52.43, significantly above the IR baseline.
Thisshows that question?question similarity plays an im-portant role in solving the cQA task.1591System MAP AvgRec MRRRelevance relations only 21.78 20.66 22.59+ Appropriateness 30.94 29.86 35.02+ Relatedness 52.43 57.05 60.14Full Network 54.51 60.93 62.94Baseline 1 (random) 15.01 11.44 15.19Baseline 2 (IR+chron.)
40.36 45.97 45.83Table 1: Results on the answer ranking task of ourfull NN vs. variants using partial information.Full Network Adding both appropriateness andrelatedness interactions yields an improvement ofanother two MAP points absolute (to 54.51), whichshows that appropriateness features encode infor-mation that is complementary to the informationmodeled by relevance and relatedness.
Note thatthe results with the other evaluation metrics (Av-gRec and MRR) follow exactly the same pattern.
Insummary, we can conclude that in order to solve thecommunity question answering problem, we need to(i) find the best related questions, and (ii) judge therelevance of individual comments with respect to thenew question.5.4 Features in perspectiveTable 2 shows the results of an ablation study whenremoving some groups of features.9 More specif-ically, we drop lexical similarities, domain-specificfeatures, and the complex semantic-syntactic inter-actions modeled in the hidden layer between the em-beddings and the domain-specific features.We can see that the lexical similarity features(which we modeled by MT evaluation metrics), havea large impact: excluding them from the networkyields a decrease of over eight MAP points.
This canbe explained as the strong dependence that related-ness has over strict word matching.
Since questionsare relatively short, a better related question will beone that matches better the original question.9Note that here we only show the impact of groups of fea-tures, e.g., we do not consider experiments with different em-beddings such as GOOGLE VEC, QL VEC, and SYNTAX VEC,which all belong to the lexical similarity group of features.
Thisis because in previous work (which was limited to subtask A),our ablation study has shown that all features in a group clearlycontribute to the overall performance (Guzma?n et al, 2016a;Guzma?n et al, 2016b).System MAP AvgRec MRR ?MAPFull Network 54.51 60.93 62.94?
Lexical similarity 45.89 51.54 53.29 -8.62?
Domain-specific 48.48 50.46 53.78 -6.03?
Distributed rep. 51.17 56.63 56.91 -3.34No hidden layer 52.19 58.23 59.95 -2.32Table 2: Results of the ablation study.As expected, eliminating the domain-specific fea-tures also hurts the performance greatly: by sixMAP points absolute.
Eliminating the use of dis-tributed representation has a lesser impact: 3.3 MAPpoints absolute.
This is in line with our previousfindings (Guzma?n et al, 2015; Guzma?n et al, 2016a;Guzma?n et al, 2016b) that semantic and syntacticembeddings are useful to make a fine-grained dis-tinction between comments (relevance, appropriate-ness), which are usually longer.We have also found that there is an interaction be-tween features and similarity relations.
For example,for relatedness, lexical similarity is 2.6 MAP pointsmore informative10 than distributed representations.In contrast, for relevance, distributed representationsare 0.7 MAP points more informative than lexicalsimilarities.5.5 Impact of the hidden layerTable 2 also presents the results of a system thathas the full set of features, but eliminates the hid-den layer from the neural network.
This is equiva-lent to training a Maximum Entropy classifier withthe complete set of features.
This simplified sys-tem performs consistently worse than the full NNmodel (?2.32 MAP,?2.7 AvgRec, and?2.99 MRRpoints), which shows that using the hidden layer tomodel the non-linear interactions between informa-tion sources has a decent overall contribution.5.6 Making appropriateness more usefulSince the SemEval-2016 Task 3 datasets also pro-vide labeled examples for the so called ?subtask A?
(q?c; appropriateness) and ?subtask B?
(qq?
; related-ness), one could use this supervision to help trainthe neural network for the primary cQA task.
Weobserved that relatedness has proven quite informa-tive.
However, the improvements observed from us-ing appropriateness were more modest.10As measured by the relative drop in MAP performance.1592System MAP AvgRec MRRFull Network 54.51 60.93 62.94Full + appr.
preds.
55.82 61.63 62.39Table 3: Using appropriateness predictions.We present here a stacked experiment in which anadditional neural network trained to predict appro-priateness is used to inform the full network model.More concretely, we train a feed-forward pairwiseneural network for subtask A, which is a simplifi-cation of the architecture from Figure 2.
The inputis reduced to three elements (q?, c1, c2), where q?
isthe thread question and c1 and c2 are a pair of com-ments in the thread.
The output consists of decidingwhether c1 is a better answer to q?
than c2.
All thepairwise interactions between input components areincluded in the hidden layer, and we use the samefeatures to train the network as the ones described inSection 4 (obviously, this time the input and the fea-tures are reduced to those involving q?, c1 and c2).We used this exact setting in previous work for solv-ing subtask A (Guzma?n et al, 2016a; Guzma?n et al,2016b).We used the network to classify all subtask A ex-amples in TRAIN-PART1, DEV and TEST, and weused the resulting scores at the comment level asskip-arc features for the full NN model: (a) alone,included in ?
(q?1, c1) and ?
(q?2, c2), and (b) multi-plied by each of the QQ?Rank feat features, includedin ?
(q, c1) and ?
(q, c2).In Table 3, we observe that using the pre-trainednetwork to incorporate subtask A predictions as fea-tures yields another sizable improvement to a finalMAP of 55.82 (the increase is smaller for AvgRec,and MRR is slightly hurt), which suggests that pre-training parts of the NN with labeled examples toperform a dedicated task, is a promising directionfor future work.5.7 Results in perspectiveNext, in order to put our results in perspective, wecompare them to the state of the art for this prob-lem, represented by the systems that participated inSemEval-2016 Task 3, subtask C. The comparisonis shown in Table 4, where we list the top-3 systems,as well as the average and the worst scores for theofficial runs of all participating teams.System MAP AvgRec MRRFull Network + subtask A preds.
55.82 61.63 62.39* 1st (Mihaylova et al, 2016) 55.41 60.66 61.48Full Network 54.51 60.93 62.94* 2nd (Filice et al, 2016) 52.95 59.27 59.23* 3rd (Mihaylov and Nakov, 2016b) 51.68 53.43 55.96. .
.
.
.
.
.
.
.
.
.
.SemEval Average 49.30 53.74 54.39. .
.
.
.
.
.
.
.
.
.
.SemEval Worst 43.20 47.96 47.79Baseline 2 (IR+chron.)
40.36 45.97 45.83Table 4: Comparative results with the state of the art,i.e., the top-3 systems that participated in SemEval-2016 Task 3, subtask C.We can see that all systems in the competition per-formed over the IR baseline with MAP scores rang-ing from 43.20 to 55.41.
We can further see that ourfull network with subtask A predictions achieves thebest results with 55.82 MAP.
The margin over thebest SemEval system is small in terms of MAP butmore noticeable in terms of AvgRec and MRR.
Notethat, even without the Subtask A predictions, ourpairwise neural network still produces results thatare on par with the state of the art (with improve-ments slightly over one point in both cases).6 Related WorkRecently, a variety of neural network models havebeen applied to community question answeringtasks such as question-question similarity (Zhou etal., 2015; dos Santos et al, 2015; Lei et al, 2015)and answer selection (Severyn and Moschitti, 2015;Wang and Nyberg, 2015; Feng et al, 2015; Tanet al, 2015; Filice et al, 2016; Barro?n-Ceden?o etal., 2016; Mohtarami et al, 2016).
Most of thesepapers concentrate on constructing advanced neuralnetwork architectures in order to model the problemat hand better.For instance, dos Santos et al (2015) propose aneural network approach combining a convolutionalneural network and a bag-of-words representationfor modeling question-question similarity.
Simi-larly, Tan et al (2015) adopt a neural attention mech-anism over bidirectional long short-term memory(LSTM) neural network to generate better answerrepresentations given the questions.1593Similarly, Lei et al (2015) use a combination ofrecurrent and convolutional neural models to mapquestions to semantic representations.
The mod-els are pre-trained within an encoder-decoder frame-work (from body to title) in order to de-noise thelong question body from irrelevant text.The main objective of our work here is different:we focus on studying the impact of the different in-put components in a novel cQA setting of rankinganswers for new questions, and we use a more stan-dard neural network.The setting of cQA as a triangle of three inter-related subtasks, which we use here, has been re-cently proposed in SemEval-2016 Task 3 on Com-munity Question Answering (Nakov et al, 2016).Above, we empirically compared our results to thoseof the best participating systems.
Unfortunately,most of the systems that took part in the compe-tition, including the winning system of the SUperteam (Mihaylova et al, 2016), approached the taskindirectly by solving subtask A at the thread leveland then using these predictions together with thereciprocal rank of the related questions to produce afinal ranking for subtask C.One exception is the Kelp system (Filice et al,2016), which was ranked second in the competition.Their approach is most similar to ours, as it also triesto combine information from different subtasks andfrom all input components.
It does so in a modu-lar kernel function, including stacking from inde-pendent subtask A and B classifiers, and it appliesSVMs to train a Good vs. Bad classifier (Filice etal., 2016).
In contrast, our approach here proceedsin a pairwise setting, it is lighter in terms of featuresengineering, and presents a direct way to combinethe relations between the different subtasks in an in-tegrated neural network model.Finally, our model uses lexical features derivedfrom machine translation evaluation.
Some previouswork also used MT model(s) as a feature(s) (Bergeret al, 2000; Echihabi and Marcu, 2003; Jeon et al,2005; Soricut and Brill, 2006; Riezler et al, 2007; Liand Manandhar, 2011; Surdeanu et al, 2011; Tranet al, 2015; Hoogeveen et al, 2016; Wu and Zhang,2016), e.g., a variation of IBM model 1 (Brown etal., 1993), to compute the probability that the ques-tion is a ?translation?
of the candidate answer.7 ConclusionWe presented a neural-based approach to a novelproblem in cQA, where given a new question, thetask is to rank comments from related question-threads according to their relevance as answers tothe original question.
We explored the utility ofthree types of similarities between the original ques-tion, the related question, and the related comment.We adopted a pairwise feed-forward neural net-work architecture, which takes as input the origi-nal question and two comments together with theircorresponding related questions.
This allowed usto study the impact and the interaction effects ofthe question-question relatedness and comment-to-related question appropriateness relations whensolving the primary cQA relevance task.
The largeperformance gains obtained from using relatednessfeatures show that question-question similarity playsa crucial role in finding relevant comments (+30MAP points).
Yet, including appropriateness re-lations is needed to achieve state-of-the-art results(+3.3 MAP) on benchmark datasets.We also studied the impact of several types of fea-tures, especially domain-specific features, but alsolexical features and syntactic embeddings.
We ob-served that lexical similarity MTE features provethe most important, followed by domain-specificfeatures, and syntactic and semantic embeddings.Overall, they all showed to be necessary to achievestate-of-the-art results.In future work, we plan to use the labels for sub-tasks A and B, which are provided in the datasetsin order to pre-train the corresponding componentsof the full network for answer ranking.
We furtherwant to apply a similar network to other semanticsimilarity problems, such as textual entailment.AcknowledgmentsThis research was performed by the Arabic Lan-guage Technologies (ALT) group at the Qatar Com-puting Research Institute (QCRI), HBKU, part ofQatar Foundation.
It is part of the InteractivesYstems for Answer Search (Iyas) project, which isdeveloped in collaboration with MIT-CSAIL.Last but not least, we would also like to thankthe anonymous reviewers for their constructive com-ments, which have helped us improve the paper.1594ReferencesAlberto Barro?n-Ceden?o, Giovanni Da San Martino,Shafiq Joty, Alessandro Moschitti, Fahad A. AlObaidli, Salvatore Romeo, Kateryna Tymoshenko, andAntonio Uva.
2016.
ConvKN at SemEval-2016 Task3: Answer and question selection for question answer-ing on Arabic and English fora.
In Proceedings of the10th International Workshop on Semantic Evaluation,SemEval ?16, pages 896?903, San Diego, CA.Yoshua Bengio and Xavier Glorot.
2010.
Understandingthe difficulty of training deep feedforward neural net-works.
In Proceedings of Artificial Intelligence andStatistics, AISTATS ?10, pages 249?256, Chia LagunaResort, Sardinia, Italy.Adam Berger, Rich Caruana, David Cohn, Dayne Freitag,and Vibhu Mittal.
2000.
Bridging the lexical chasm:Statistical approaches to answer-finding.
In Proceed-ings of the 23rd Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR ?00, pages 192?199, Athens,Greece.James Bergstra, Olivier Breuleux, Fre?de?ric Bastien, Pas-cal Lamblin, Razvan Pascanu, Guillaume Desjardins,Joseph Turian, David Warde-Farley, and Yoshua Ben-gio.
2010.
Theano: a CPU and GPU math expressioncompiler.
In Proceedings of the Python for ScientificComputing Conference, SciPy ?10, Austin, TX.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Comput.
Linguist., 19(2):263?311.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the Second Interna-tional Conference on Human Language TechnologyResearch, HLT ?02, pages 138?145, San Diego, CA.Cicero dos Santos, Luciano Barbosa, Dasha Bogdanova,and Bianca Zadrozny.
2015.
Learning hybrid rep-resentations to retrieve semantically equivalent ques-tions.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing, ACL-IJCNLP ?15, pages 694?699,Beijing, China.Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and YongYu.
2008.
Searching questions by identifying questiontopic and question focus.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, ACL ?08, pages 156?164, Columbus, OH.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
Journal of Machine LearningResearch, 12:2121?2159.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of the 41st Annual Meeting of the Associ-ation for Computational Linguistics, ACL ?03, pages16?23, Sapporo, Japan.Minwei Feng, Bing Xiang, Michael R. Glass, LidanWang, and Bowen Zhou.
2015.
Applying deep learn-ing to answer selection: A study and an open task.
InProceedings of the 2015 IEEE Workshop on AutomaticSpeech Recognition and Understanding, ASRU ?15,pages 813?820, Scottsdale, AZ.Simone Filice, Danilo Croce, Alessandro Moschitti, andRoberto Basili.
2016.
KeLP at SemEval-2016 Task 3:Learning semantic relations between questions and an-swers.
In Proceedings of the 10th International Work-shop on Semantic Evaluation, SemEval ?16, pages1116?1123, San Diego, CA.Francisco Guzma?n, Shafiq Joty, Llu?
?s Ma`rquez, andPreslav Nakov.
2015.
Pairwise neural machine trans-lation evaluation.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics and the 7th International Joint Conference on Nat-ural Language Processing, ACL-IJCNLP ?15, pages805?814, Beijing, China.Francisco Guzma?n, Llu?
?s Ma`rquez, and Preslav Nakov.2016a.
Machine translation evaluation meets commu-nity question answering.
In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?16, pages 460?466, Berlin, Ger-many.Francisco Guzma?n, Preslav Nakov, and Llu?
?s Ma`rquez.2016b.
MTE-NN at SemEval-2016 Task 3: Canmachine translation evaluation help community ques-tion answering?
In Proceedings of the 10th In-ternational Workshop on Semantic Evaluation, Se-mEval ?16, pages 887?895, San Diego, CA.Doris Hoogeveen, Yitong Li, Huizhi Liang, Bahar Salehi,Timothy Baldwin, and Long Duong.
2016.
UniMelbat SemEval-2016 Task 3: Identifying similar questionsby combining a CNN with string similarity measures.In Proceedings of the 10th International Workshop onSemantic Evaluation, SemEval ?16, pages 851?856,San Diego, CA.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM Inter-national Conference on Information and KnowledgeManagement, CIKM ?05, pages 84?90, Bremen, Ger-many.Alon Lavie and Michael Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.Jaakkola, Kateryna Tymoshenko, Alessandro Mos-1595chitti, and Llu?
?s Ma`rquez.
2015.
Denoising bodies totitles: Retrieving similar questions with recurrent con-volutional models.
arXiv preprint arXiv:1512.05726.Shuguang Li and Suresh Manandhar.
2011.
Improv-ing question recommendation by exploiting informa-tion need.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, ACL ?11, pages 1425?1434, Portland, OR.Todor Mihaylov and Preslav Nakov.
2016a.
Hunting fortroll comments in news community forums.
In Pro-ceedings of the 54th Annual Meeting of the Associationfor Computational Linguistics, ACL ?16, pages 399?405, Berlin, Germany.Todor Mihaylov and Preslav Nakov.
2016b.
SemanticZat SemEval-2016 Task 3: Ranking relevant answers incommunity question answering using semantic simi-larity based on fine-tuned word embeddings.
In Pro-ceedings of the 10th International Workshop on Se-mantic Evaluation, SemEval ?16, pages 879?886, SanDiego, CA.Todor Mihaylov, Georgi Georgiev, and Preslav Nakov.2015.
Finding opinion manipulation trolls in newscommunity forums.
In Proceedings of the Nine-teenth Conference on Computational Natural Lan-guage Learning, CoNLL ?15, pages 310?314, Beijing,China.Tsvetomila Mihaylova, Pepa Gencheva, Martin Boyanov,Ivana Yovcheva, Todor Mihaylov, Momchil Hardalov,Yasen Kiprov, Daniel Balchev, Ivan Koychev, PreslavNakov, Ivelina Nikolova, and Galia Angelova.
2016.SUper Team at SemEval-2016 Task 3: Building afeature-rich system for community question answer-ing.
In Proceedings of the 10th International Work-shop on Semantic Evaluation, SemEval ?16, pages836?843, San Diego, CA.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous space wordrepresentations.
In Proceedings of the 2013 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, NAACL-HLT ?13, pages 746?751, At-lanta, GA.Mitra Mohtarami, Yonatan Belinkov, Wei-Ning Hsu,Yu Zhang, Tao Lei, Kfir Bar, Scott Cyphers, andJim Glass.
2016.
SLS at SemEval-2016 Task 3:Neural-based approaches for ranking in communityquestion answering.
In Proceedings of the 10th In-ternational Workshop on Semantic Evaluation, Se-mEval ?16, pages 828?835, San Diego, CA.Preslav Nakov, Llu?
?s Ma`rquez, Walid Magdy, Alessan-dro Moschitti, Jim Glass, and Bilal Randeree.
2015.SemEval-2015 Task 3: Answer selection in commu-nity question answering.
In Proceedings of the 9thInternational Workshop on Semantic Evaluation, Se-mEval ?15, pages 269?281, Denver, CO.Preslav Nakov, Llu?
?s Ma`rquez, Alessandro Moschitti,Walid Magdy, Hamdy Mubarak, Abed Alhakim Frei-hat, Jim Glass, and Bilal Randeree.
2016.
SemEval-2016 task 3: Community question answering.
In Pro-ceedings of the 10th International Workshop on Se-mantic Evaluation, SemEval ?16, pages 525?545, SanDiego, CA.Massimo Nicosia, Simone Filice, Alberto Barro?n-Ceden?o, Iman Saleh, Hamdy Mubarak, Wei Gao,Preslav Nakov, Giovanni Da San Martino, AlessandroMoschitti, Kareem Darwish, Llu?
?s Ma`rquez, ShafiqJoty, and Walid Magdy.
2015.
QCRI: Answer selec-tion for community question answering - experimentsfor Arabic and English.
In Proceedings of the 9thInternational Workshop on Semantic Evaluation, Se-mEval ?2015, pages 203?209, Denver, CO.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?02, pages 311?318, Philadelphia,PA.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisti-cal machine translation for query expansion in answerretrieval.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,ACL ?07, pages 464?471, Prague, Czech Republic.Aliaksei Severyn and Alessandro Moschitti.
2015.Learning to rank short text pairs with convolutionaldeep neural networks.
In Proceedings of the 38thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR ?15,pages 373?382, Santiago, Chile.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Americas,AMTA ?06, pages 223?231, Cambridge, MA.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing with compositionalvector grammars.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics, ACL ?13, pages 455?465, Sofia, Bulgaria.Radu Soricut and Eric Brill.
2006.
Automatic questionanswering using the web: Beyond the factoid.
Inf.Retr., 9(2):191?206.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Comput.
Lin-guist., 37(2):351?383.1596Ming Tan, Bing Xiang, and Bowen Zhou.
2015.
LSTM-based deep learning models for non-factoid answer se-lection.
arXiv preprint arXiv:1511.04108.Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen,and Son Bao Pham.
2015.
JAIST: Combiningmultiple features for answer selection in communityquestion answering.
In Proceedings of the 9th In-ternational Workshop on Semantic Evaluation, Se-mEval ?15, pages 215?219, Denver, CO.Di Wang and Eric Nyberg.
2015.
A long short-termmemory model for answer sentence selection in ques-tion answering.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics and the 7th International Joint Conference on Nat-ural Language Processing, ACL-IJCNLP ?15, pages707?712, Beijing, China.Yunfang Wu and Minghua Zhang.
2016.
ICL00at SemEval-2016 Task 3: Translation-based methodfor CQA system.
In Proceedings of the 10th In-ternational Workshop on Semantic Evaluation, Se-mEval ?16, pages 857?860, San Diego, CA.Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.2015.
Learning continuous word embedding withmetadata for question retrieval in community questionanswering.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on NaturalLanguage Processing, ACL-IJCNLP ?15, pages 250?259, Beijing, China.1597
