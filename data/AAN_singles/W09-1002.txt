Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 5?6,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsOn bootstrapping of linguistic features for bootstrapping grammarsDamir C?avarUniversity of ZadarZadar, Croatiadcavar@unizd.hrAbstractWe discuss a cue-based grammar induc-tion approach based on a parallel theory ofgrammar.
Our model is based on the hy-potheses of interdependency between lin-guistic levels (of representation) and in-ductability of specific structural propertiesat a particular level, with consequencesfor the induction of structural properties atother linguistic levels.
We present the re-sults of three different cue-learning exper-iments and settings, covering the induc-tion of phonological, morphological, andsyntactic properties, and discuss potentialconsequences for our general grammar in-duction model.11 IntroductionWe assume that individual linguistic levels of nat-ural languages differ with respect to their for-mal complexity.
In particular, the assumption isthat structural properties of linguistic levels likephonology or morphology can be characterizedfully by Regular grammars, and if not, at least alarge subset can.
Structural properties of naturallanguage syntax on the other hand might be char-acterized by Mildly context-free grammars (Joshiet al, 1991), where at least a large subset could becharacterized by Regular and Context-free gram-mars.21This article is builds on joint work and articles with K.Elghamri, J.
Herring, T. Ikuta, P. Rodrigues, G. Schrementiand colleagues at the Institute of Croatian Language and Lin-guistics and the University of Zadar.
The research activitieswere partially funded by several grants over a couple of years,at Indiana University and from the Croatian Ministry of Sci-ence, Education and Sports of the Republic of Croatia.2We are abstracting away from concrete linguistic modelsand theories, and their particular complexity, as discussed e.g.in (Ristad, 1990) or (Tesar and Smolensky, 2000).Ignoring for the time being extra-linguistic con-ditions and cues for linguistic properties, and in-dependent of the complexity of specific linguis-tic levels for particular languages, we assumethat specific properties at one particular linguisticlevel correlate with properties at another level.
Innatural languages certain phonological processesmight be triggered at morphological boundariesonly, e.g.
(Chomsky and Halle, 1968), or prosodicproperties correlate with syntactic phrase bound-aries and semantic properties, e.g.
(Inkelas andZec, 1990).
Similarly, lexical properties, as forexample stress patterns and morphological struc-ture tend to be specific to certain word types (e.g.substantives, but not function words).
i.e.
corre-late with the lexical morpho-syntactic propertiesused in grammars of syntax.
Other more informalcorrelations that are discussed in linguistics, thatrather lack a formal model or explanation, are forexample the relation between morphological rich-ness and the freedom of word order in syntax.Thus, it seems that specific regularities andgrammatical properties at one linguistic levelmight provide cues for structural properties at an-other level.
We expect such correlations to be lan-guage specific, given that languages qualitativelysignificantly differ at least at the phonetic, phono-logical and morphological level, and at least quan-titatively also at the syntactic level.Thus in our model of grammar induction, wefavor the view expressed e.g.
in (Frank, 2000)that complex grammars are bootstrapped (or grow)from less complex grammars.
On the other hand,the intuition that structural or inherent proper-ties at different linguistic levels correlate, i.e.
theyseem to be used as cues in processing and acquisi-tion, might require a parallel model of languagelearning or grammar induction, as for examplesuggested in (Jackendoff, 1996) or the Competi-tion Model (MacWhinney and Bates, 1989).In general, we start with the observation that5natural languages are learnable.
In principle, thestudy of how this might be modeled, and what theminimal assumptions about the grammar proper-ties and the induction algorithm could be, couldstart top-down, by assuming maximal knowledgeof the target grammar, and subsequently eliminat-ing elements that are obviously learnable in an un-supervised way, or fall out as side-effects.
Alter-natively, a bottom-up approach could start with thequestion about how much supervision has to beadded to an unsupervised model in order to con-verge to a concise grammar.Here we favor the bottom-up approach, and askhow simple properties of grammar can be learnedin an unsupervised way, and how cues could beidentified that allow for the induction of higherlevel properties of the target grammar, or other lin-guistic levels, by for example favoring some struc-tural hypotheses over others.In this article we will discuss in detail sev-eral experiments of morphological cue inductionfor lexical classification (C?avar et al, 2004a) and(C?avar et al, 2004b) using Vector Space Modelsfor category induction and subsequent rule for-mation.
Furthermore, we discuss structural cohe-sion measured via Entropy-based statistics on thebasis of distributional properties for unsupervisedsyntactic structure induction (C?avar et al, 2004c)from raw text, and compare the results with syn-tactic corpora like the Penn Treebank.
We ex-pand these results with recent experiments in thedomain of unsupervised induction of phonotacticregularities and phonological structure (C?avar andC?avar, 2009), providing cues for morphologicalstructure induction and syntactic phrasing.ReferencesDamir C?avar and Ma?gorzata E. C?avar.
2009.
On theinduction of linguistic categories and learning gram-mars.
Paper presented at the 10th Szklarska PorebaWorkshop, March.Damir C?avar, Joshua Herring, Toshikazu Ikuta, PaulRodrigues, and Giancarlo Schrementi.
2004a.Alignment based induction of morphology grammarand its role for bootstrapping.
In Gerhard Ja?ger,Paola Monachesi, Gerald Penn, and Shuly Wint-ner, editors, Proceedings of Formal Grammar 2004,pages 47?62, Nancy.Damir C?avar, Joshua Herring, Toshikazu Ikuta, PaulRodrigues, and Giancarlo Schrementi.
2004b.
Onstatistical bootstrapping.
In William G. Sakas, ed-itor, Proceedings of the First Workshop on Psycho-computational Models of Human Language Acqui-sition, pages 9?16.Damir C?avar, Joshua Herring, Toshikazu Ikuta, PaulRodrigues, and Giancarlo Schrementi.
2004c.
Syn-tactic parsing using mutual information and relativeentropy.
Midwest Computational Linguistics Collo-quium (MCLC), June.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Harper & Row, New York.Robert Frank.
2000.
From regular to context free tomildly context sensitive tree rewriting systems: Thepath of child language acquisition.
In A. Abeille?and O. Rambow, editors, Tree Adjoining Gram-mars: Formalisms, Linguistic Analysis and Process-ing, pages 101?120.
CSLI Publications.Sharon Inkelas and Draga Zec.
1990.
The Phonology-Syntax Connection.
University Of Chicago Press,Chicago.Ray Jackendoff.
1996.
The Architecture of the Lan-guage Faculty.
Number 28 in Linguistic InquiryMonographs.
MIT Press, Cambridge, MA.Aravind Joshi, K. Vijay-Shanker, and David Weird.1991.
The convergence of mildly context-sensitivegrammar formalisms.
In Peter Sells, Stuart Shieber,and Thomas Wasow, editors, Foundational Issues inNatural Language Processing, pages 31?81.
MITPress, Cambridge, MA.Brian MacWhinney and Elizabeth Bates.
1989.
TheCrosslinguistic Study of Sentence Processing.
Cam-bridge University Press, New York.Eric S. Ristad.
1990.
Computational structure of gen-erative phonology and its relation to language com-prehension.
In Proceedings of the 28th annual meet-ing on Association for Computational Linguistics,pages 235?242.
Association for Computational Lin-guistics.Bruce Tesar and Paul Smolensky.
2000.
Learnabilityin Optimality Theory.
MIT Press, Cambridge, MA.6
