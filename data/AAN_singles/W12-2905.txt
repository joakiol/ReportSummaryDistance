NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 28?36,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsNon-Syntactic Word Prediction for AACKarl WiegandNortheastern University360 Huntington AveBoston, MA 02115, USAwiegand@ccs.neu.eduRupal Patel, Ph.D.Northeastern University360 Huntington AveBoston, MA 02115, USAr.patel@neu.eduAbstractMost icon-based augmentative and alternativecommunication (AAC) devices require usersto formulate messages in syntactic order inorder to produce syntactic utterances.
Re-liance on syntactic ordering, however, maynot be appropriate for individuals with lim-ited or emerging literacy skills.
Some of theseusers may benefit from unordered messageformulation accompanied by automatic mes-sage expansion to generate syntactically cor-rect messages.
Facilitating communication viaunordered message formulation, however, re-quires new methods of prediction.
This pa-per describes a novel approach to word predic-tion using semantic grams, or ?sem-grams,?which provide relational information aboutmessage components regardless of word or-der.
Performance of four word-level predic-tion algorithms, two based on sem-grams andtwo based on n-grams, were compared on acorpus of informal blogs.
Results showedthat sem-grams yield accurate word predic-tion, but lack prediction coverage.
Hybridmethods that combine n-gram and sem-gramapproaches may be viable for unordered pre-diction in AAC.1 IntroductionMany individuals with severe speech impairmentsrely on augmentative and alternative communica-tion (AAC) devices to convey their thoughts anddesires.
Those with limited or emerging literacyskills may use icon-based systems, which often re-quire that vocabulary items be selected in syntac-tic order to generate syntactically well-formed mes-sages; however, selecting vocabulary items seriallyand in syntactic order can be physically and cogni-tively arduous depending on the icon organizationscheme (Udwin and Yule, 1990).
Moreover, AACproductions are often syntactically incomplete or in-correct (Van Balkom and Welle Donker-Gimbrere,1996), perhaps for efficiency or due to limited lin-guistic abilities.
For many users, unordered vocabu-lary selection may alleviate the physical and cogni-tive demands of message formulation and shift theonus of generating syntactically complete and ac-curate messages onto the AAC device.
Althoughunordered message formulation schemes have beenproposed (Karberis and Kouroupetroglou, 2002; Pa-tel et al, 2004) and techniques have been devel-oped for expanding incomplete input (McCoy et al,1998), prediction has not been incorporated.
Thispaper presents an initial step toward text predictionfrom a set of unordered vocabulary selections.Rate enhancement is a commonly cited issue inAAC because aided message formulation rates arean order of magnitude slower than spoken interac-tion (Beukelman and Mirenda, 1998).
Predictionis a common rate enhancement technique.
Textprediction for AAC has primarily focused on well-ordered, syntactic input and has leveraged both se-mantic characteristics (Demasco and McCoy, 1992;Li and Hirst, 2005; Nikolova et al, 2010) and vari-ations of n-grams (Lesher et al, 1998; Trnka et al,2006).
For example, semantic networks and linguis-tic rules have been used to predict missing functionwords and to apply affixes to content words (McCoyet al, 1998).
The use of n-grams to predict text en-try has been extensively studied at both the level of28letters (Broerse and Zwaan, 1966; Suen, 1979; Howand Kan, 2005) and words (Bickel et al, 2005).
Forexample, memory based language models have beenused to predict missing content words using trigrams(Van Den Bosch, 2006).
Although some recent workhas attempted to loosen syntactic requirements byincluding either left or right context, some direc-tional context has historically been required (VanDen Bosch and Berck, 2009).
Furthermore, wordprediction approaches in AAC have typically beenimplemented for letter-by-letter message formula-tion (Koester and Levine, 1996; Koester and Levine,1997; Lesher and Rinkus, 2002; Higginbotham etal., 2009).
The current work is fundamentally novelin that: (1) no syntactic order is implied or requiredduring either training or testing; and (2) the predic-tion is implemented at word level to accommodateicon-based interaction.Previous work in information retrieval has ex-plored relationships between words with regard todistance (Lin and Hovy, 2003; Lv and Zhai, 2009),grammatical purpose (Tzoukermann et al, 1997; Al-lan and Raghavan, 2002), and semantic characteris-tics (Westerman and Cribbin, 2000; Fang and Zhai,2006; Hemayati et al, 2007), particularly for re-trieving highly relevant documents or passages.
Onestudy in this area resulted in an approach called s-grams, a generalization of n-grams, in which thedistance between words directly affects the strengthof their semantic relationship (Ja?rvelin et al, 2007).Another approach to predicting semantically relatedwords is to use collocation to indicate topic changeswithin a moving window of fixed length (Matiasekand Baroni, 2003).
Rather than relying on distanceto indicate relationship strength, the current workcombines frequency analysis with syntactic indica-tions of semantic coherence.1.1 Semantic GramsSemantic grams, or ?sem-grams,?
provide an alter-native approach to quantifying the relationship be-tween co-occurring words.
A sem-gram is definedas a multiset of words that can appear together in asentence (Table 1).
In English, a sentence is one ofthe smallest units of language that is typically bothcoherent, in terms of semantic content, and cohesive,in that the semantic content is inter-related.
Addi-tionally, because sentences are demarcated with syn-Table 1: Example of Sem-Grams of Length 2Sentence: ?I like to play chess with my brother.
?Filtered Words: i, like, play, chess, brotherSem-grams and Counts:brother, chess (1) brother, i (1)brother, like (1) brother, play (1)chess, i (1) chess, like (1)chess, play (1) i, like (1)i, play (1) like, play (1)tactic cues such as punctuation, semantically relateditems can be efficiently identified using sentenceboundary detection (Kiss and Strunk, 2006).
Thus,sem-grams leverage sentence-level co-occurrence toextract semantic content at different levels of gran-ularity, depending on the allowable lengths of mul-tisets.
Sem-grams can be viewed as non-directionals-grams with a uniform weight applied to all rela-tionships between any words in a given sentence.In a sentence of length L (in words), the numberof n-grams of length n (in words), where L ?
n, isgiven by the expression L ?
n + 3, which includesthe beginning and ending n-grams that contain nullelements.
By contrast, the number of sem-grams oflength n is given by the expression(Ln).
Thus, therewill typically be many more sem-grams of length nin a single sentence than n-grams of the same length.Unlike n-grams, it is not necessary for sem-grams tocontain null elements because a sem-gram of lengthS with a null element is equivalent to a sem-gram oflength S ?
1 without null elements.
Sem-grams oflength one, containing a single word, are equivalentto the prior probability of that word.1.2 Prediction AlgorithmsUnordered word prediction poses the followingproblem: given a multiset of existing words E thathave already been selected by a user and a set of can-didate words C that the user may select from, whichcandidate word c ?
C is the user most likely to se-lect in order to complete the message?
As an initialstep toward addressing this problem, the followingfour algorithms, two based on sem-grams and twobased on n-grams, were compared:S1: Naive Bayesian Sem-grams Given existingwords E, rank all candidate words c ?
C in de-29scending order of probability according to:P (c|E) = P (c)?w?EP (w|c)S1 is a modification of the Bayesian ranking ofsem-grams in that it assumes independence of ex-isting words to each other, conditional on the givencandidate word.
Using true Bayesian probabili-ties for sem-grams, the probability of a candidateword could be represented as the following for eachP (c|E), given w ?
E and |E| = 3:P (c)P (w1|c, w2, w3)P (w2|c, w3)P (w3|c)P (w1, w2, w3)The exact form of this equation depends on the or-dering branch chosen, but it also requires joint prob-abilities for sem-grams of different lengths.
Assum-ing conditional independence of the existing wordsto each other, S1 only requires sem-grams of lengthtwo.S2: Independent Sem-grams Given existingwords E, rank all candidate words c ?
C in de-scending order of probability according to:P (c|E) =?w?EP (w, c)The approach of S2 is a ?hand of cards?
approachthat treats the message formulation task as a randomdrawing of sem-grams from a pool.
While the for-mula above is specified for sem-grams of length 2, itcan be extended to support sem-grams of any length.N1: Naive Bayesian N-grams Given existingwords E, rank all candidate words c ?
C in de-scending order of probability according to:P (c|E) = P (c)?w?EP (w|c)N1 is a copy of S1, except that the definition of thejoint probability P (w, c) includes the counts for n-grams that contain both w and c, regardless of order.This algorithm was designed to compare whether theinformation provided by n-grams can be used to ap-proximate the information provided by sem-grams.N1 assigns high ranks to candidate words that arelikely to appear adjacent to all other words in thesentence.N2: Applied N-grams Given existing words E,rank all candidate words c ?
C in descending orderof probability according to:P (c|E) =?w?EP (w, c)N2 is designed to leverage the strength of n-gramsand rank candidate words based on the probabilityof them appearing adjacent to any of the existingwords.
N2 uses the same definition of joint prob-ability as N1, where P (w, c) includes the counts forn-grams that contain both w and c, irrespective oforder.2 Method2.1 Corpus Selection and PreparationGiven the lack of large corpora of AAC message for-mulations (Lesher and Sanelli, 2000), approxima-tions have often been used (Wandmacher and An-toine, 2006; Trnka and McCoy, 2007).
Despite re-cent efforts to create AAC-like corpora (Vertanenand Kristensson, 2011), statistical prediction is of-ten more effective with larger data sets.
The BlogAuthorship Corpus (Schler et al, 2006) was se-lected because it is freely available and tends to bewritten in an informal style, such as might be seenin diary entries or personal emails.
The corpus isboth large and diverse, comprising over 140 millionwords written by 19,320 bloggers in August 2004.The bloggers ranged in age from 13 - 48 and wereequally divided between males and females.To prepare the corpus, all blog posts were ex-tracted as ASCII text.
Every blog post was split intosentences using the PunktSentenceTokenizer (Kissand Strunk, 2006) of the Natural Language Toolkit(NLTK) (Bird et al, 2009) and then split into wordsusing the following regular expression:\w+(\w*([\-\?\.
]\w+)*)*English stop words were removed according toa popular list (Ranks, 2012) and remaining wordswere stemmed using the NLTK?s PorterStemmer,which is a modified implementation of the originalPorter stemming algorithm (Porter, 1997).
Finally,all stemmed words were examined for membershipin a stemmed American-English dictionary (Ward,30Table 2: Sample Test Results for N1 and S1Original Sentence: ?but i went to church yesterday with the fam.
?Target Stem: wentInput Stems: yesterday, churchN1 Candidate List: went, morn, today, go, attend, work, afternoon, church, got, day, back, ...S1 Candidate List: went, go, church, today, got, day, like, time, just, well, one, get, peopl, ...Original Sentence: ?You never see signs like that in cities.
?Target Stem: likeInput Stems: never, see, sign, citiN1 Candidate List: just, show, sign, realli, say, want, go, seen, thought, hall, citi, live, ...S1 Candidate List: never, will, like, can, go, love, one, just, know, want, get, live, time, ...Original Sentence: ?This semester Im taking six classes.
?Target Stem: classInput Stems: take, semest, sixN1 Candidate List: next, month, class, hour, last, second, week, year, first, five, flag, ...S1 Candidate List: class, month, year, last, time, one, go, day, get, school, will, first, ...Original Sentence: ?Hey, they?re in first, by a game and a half over the Yankees.
?Target Stem: gameInput Stems: yanke, hey, first, halfN1 Candidate List: game, stadium, like, hour, time, year, day, guy, hey, fan, say, one, two, ...S1 Candidate List: game, got, like, red, time, play, team, sox, hour, go, fan, one, get, day, ...Note: Uncommon spelling (e.g.
semest) is due to stemming.2002).
Any stemmed words not found in the dictio-nary were removed to further constrain the vocabu-lary and account for spelling errors and nonsensicaltext.The corpus was then randomly split into a train-ing and testing set based on authorship, with 80%of the authors (15,451) being placed in the trainingset and 20% of the authors (3,871) being placed inthe testing set.
The training set comprised over 7million sentences written by 7,682 males and 7,768females with a combined average age of 22 years.All n-gram and sem-gram statistics, with plus-onesmoothing, were gathered using only sentences inthe training set and both n-grams and sem-gramswere limited to a word length of 2 (bigrams).2.2 EvaluationTesting was conducted on 2,000 sentences that wererandomly selected from the test corpus.
The sameprocessing steps used during training were per-formed on the test sentences: stop words were re-moved, the remaining words were stemmed, and allstems not in the dictionary were filtered out.
Toavoid run-on sentences and sentence boundary de-tection errors, all test sentences were also truncatedto a maximum of 20 words.
The words in each testsentence were then shuffled and one word was re-moved at random and designated as the target word.Each of the four algorithms were provided the shuf-fled words as input; as output, each algorithm at-tempted to identify the target word by generating aranked list of candidates (Table 2).In addition to the shuffled multiset of input words,each algorithm required a seed list of candidatewords.
Ideally, all known words in the corpus wouldbe used as candidate words.
To constrain the com-putational requirements, the two algorithms basedon n-grams (N1 and N2) were provided with the listof most frequently co-occurring words that appearedas n-grams with any of the multiset of input words,limited to the top 10 n-grams for a given input word.Similarly, each sem-gram algorithm (S1 and S2) re-ceived a list of most frequently co-occurring wordsthat appeared as sem-grams with any of the multisetof input words, limited to the top 10 sem-grams fora given input word.
With a limit of 19 input words(20 minus the target word), each algorithm received31at most 190 unique candidate words to rank.Two evaluation metrics were used to quantify theperformance of each algorithm: (1) a boolean valuethat was true if the output list contained the targetword in any position, indicating that the target wordhad been successfully predicted; (2) if the algorithmsuccessfully predicted the target word, the algorithmreceived a positive integer score corresponding tothe position of the target word in the output list,with lower scores indicating more accurate predic-tion.
For example, if an algorithm suggested the tar-get word as the first item in its ranked list, it receiveda score of 1; if it suggested the target word as thesecond item in its ranked list, it received a score of2.
For computational convenience, the output lists ofeach algorithm were truncated to the first 100 items;thus, if an algorithm?s output list contained the tar-get word in a position after 100, it was marked asfailing to predict the target word.3 ResultsThe n-gram algorithms successfully predicted 32%of the 2,000 test sentences while the sem-gram al-gorithms successfully predicted 22% (Table 3).
Al-though both n-gram algorithms performed similarly,N1 consistently predicted the target word more ac-curately than N2.
On average, N1 suggested the tar-get word as the 16th word in its ranked list, whereN2 suggested the target word as the 20th word in itslist.
While the sem-gram algorithms predicted fewersentences than the n-gram algorithms, they were al-most twice as accurate on sentences that they didpredict.
On average, S1 suggested the target wordas the 9th word in its ranked list; for S2, the targetword was the 13th item.To further compare the effectiveness of sem-grams and n-grams, sentences were grouped accord-ing to their input length, from 1 to 19 words, andstatistics were gathered for each algorithm on eachsentence length (Table 4).
For test sentences inwhich the algorithms were only given a single in-put word, both n-gram algorithms ranked the tar-get word at least one full ranking higher than ei-ther sem-gram algorithm, thus giving more accu-rate predictions.
For all other sentence lengths, thesem-gram algorithms were more accurate.
Betweenthe n-gram algorithms, N1 consistently predicted theTable 3: Summary of ResultsN1 N2 S1 S2Sentences 2000 2000 2000 2000# Predicted 647 649 435 435% Predicted 32% 32% 22% 22%Avg Score 16.26 19.70 9.04 12.67target word more accurately and more often than N2.Similarly, S1 consistently predicted the target wordmore accurately and more often than S2.For every input sentence length greater than one,S1 outperformed N1 in all gathered metrics.
Whencomparing the prediction accuracy of N1 and S1,S1?s prediction accuracy was also more stable, withN1?s prediction accuracy continuing to degrade asthe length of the input sentence increased (Figure 1).4 DiscussionMessage formulation using AAC devices has histor-ically relied on serial selection of letters or words(icons).
To produce syntactically correct messagesfor icon-based AAC, selection is often required toproceed in syntactic order.
The current work aimedto facilitate unordered vocabulary selection throughthe use of text prediction.
Results indicate that wordprediction for unordered message formulation is vi-able using statistical approaches.
Although the n-gram algorithms predicted a larger number of testsentences than the sem-gram algorithms, evalua-tion of the ranked output indicated that the sem-gram approaches were more accurate.
Because n-grams assume that adjacent words are strongly re-lated, it was expected that n-grams would providemore accurate prediction for shorter sentences; how-ever, this advantage was not maintained as sentencelength increased beyond two words.
Prediction ac-curacy is likely to be more important in AAC de-vices because the cognitive demands of choosingfrom prediction lists can sometimes outweigh rateenhancements (Koester and Levine, 1996; Koesterand Levine, 1997).The use of bigrams may have resulted in poor ac-curacy of the n-gram algorithms because there weremany more sem-grams than n-grams of length 2.
In-creasing n-gram length, up to a cardinality equal tothe number of sem-grams of length 2, could allow n-32Table 4: Prediction Coverage (%) and Average Scores by Sentence Length# Words N1 % N1 Avg S1 % S1 Avg N2 % N2 Avg S2 % S2 Avg1 20.88% 3.44 12.05% 4.47 20.88% 3.42 12.05% 4.472 26.55% 6.07 19.47% 5.89 26.55% 6.32 19.47% 6.233 22.22% 7.64 16.89% 6.87 22.22% 9.82 16.89% 9.844 32.11% 10.46 22.94% 7.62 32.11% 11.91 22.94% 9.945 31.25% 12.13 21.88% 6.14 31.25% 14.02 21.88% 9.146 38.18% 15.25 26.67% 8.75 38.18% 17.68 26.67% 12.117 42.86% 16.17 29.46% 9.52 42.86% 21.77 29.46% 12.738 39.60% 18.08 25.74% 11.15 39.60% 22.00 25.74% 15.739 29.11% 19.13 20.25% 11.31 29.11% 23.48 20.25% 17.8810 44.74% 24.47 35.53% 10.52 44.74% 23.56 35.53% 16.2211 38.46% 28.55 26.92% 15.21 38.46% 26.80 26.92% 17.9312 46.00% 23.39 14.00% 13.71 46.00% 41.26 14.00% 9.1413 38.46% 24.47 25.64% 14.30 38.46% 34.07 25.64% 15.9014 29.41% 26.30 14.71% 10.80 29.41% 39.10 14.71% 26.2015 46.67% 32.14 20.00% 16.17 46.67% 36.79 20.00% 15.1716 47.62% 25.70 28.57% 12.83 47.62% 30.50 28.57% 12.6717 53.85% 23.14 38.46% 12.20 53.85% 35.14 38.46% 21.4018 40.95% 38.35 25.71% 13.56 42.86% 43.07 25.71% 25.1119 38.46% 23.80 38.46% 11.00 38.46% 52.40 38.46% 32.00gram algorithms to potentially match or surpass theprediction accuracy of sem-grams.
For unorderedword prediction, this larger set of n-grams wouldneed to be indexed in an order-independent man-ner, which would further increase computational de-mands.
Such prediction lags, however, are unlikelyto be tolerated by users as they engage in interactivetasks (Higginbotham et al, 2009).Of the two n-gram algorithms, N1 outperformedN2 on both prediction coverage and accuracy.
It washypothesized, however, that N2 would yield moreaccurate predictions because the target word was de-fined to be adjacent to at least one of the input words.It was expected that N1 would unfairly reward can-didate words that had appeared adjacent to each in-put word in the training set, while punishing moredesirable candidate words that had not appeared ad-jacent to some of the input words.
Perhaps this biaswas not evident in the current corpus because plus-one smoothing removed all zero probabilities foradjacency likelihoods.
Additionally, N1 may havebeen more successful because it favored candidatesthat were related to all input words rather than can-didates that were strongly related to just a subset ofthe input words.Despite the encouraging prediction coverage ofn-grams and the prediction accuracy of sem-grams,approximately two-thirds of the test sentences werenot predicted by any of the algorithms.
One possibleexplanation may relate to the decision to seed eachalgorithm with only the top 10 most frequent wordsthat co-occurred with each input word.
Ideally,each algorithm would have considered all words inthe vocabulary as candidate words; however, be-cause there were almost 40,000 unique stems in theprocessed corpus, the computational requirementswere prohibitive for this initial implementation.
Anopen empirical question is whether increasing theseed values to include a larger set of co-occurringwords would result in greater prediction coverage.It should be noted, however, that while seeding sem-grams with more candidate words may improve pre-diction coverage, it is unlikely to increase predictionaccuracy for the n-gram approaches.Icon-based AAC devices typically have activevocabularies with much fewer than 40,000 words,which may negate the need for seeding candidatewords.
For example, two commonly used icon33Figure 1: Average score per sentence length for both N1and S1 (lower scores indicate more accurate prediction).sets, the Widgit Symbol Set and the Mayer-JohnsonPicture Communication Symbol collection, eachcontain approximately 11,000 icons (Widgit, 2012;Mayer-Johnson, 2012).
While a large dictionary wasused in this work to provide a conservative estimateof prediction performance, it is possible that usinga smaller and more representative AAC vocabularywould improve prediction coverage and accuracy.Additionally, restricting vocabulary size would alsoreduce computational demands, making it more fea-sible to use all vocabulary words as candidates.5 Conclusion and Future DirectionsThe current work provides a promising approach toword prediction for AAC users who may benefitfrom unordered message formulation.
Sem-gramsmake use of co-occurrence between words within asentence to improve prediction accuracy.
While n-grams have historically provided a strong founda-tion for word prediction in letter-by-letter systems,results indicate that they can also be used for un-ordered word prediction, although they are not as ac-curate as sem-grams.
A hybrid approach that seedsboth types of algorithms with a superset of can-didate words and merges the prediction lists maysimultaneously exhibit the wide prediction cover-age of n-grams and the high prediction accuracy ofsem-grams.
Such a hybrid approach could enhancethe speed of unordered message formulation and in-crease social engagement.Additional improvements to this work may bepossible using the breadth of information availablewithin well-documented and comprehensive cor-pora.
For example, while the Blog Authorship Cor-pus included age and gender information about eachblogger, this information was not used in the presentstudy.
To tailor prediction to individual users, itmay be possible to limit the available vocabulary andgram-based statistics to information gathered fromusers of similar age and gender.
This may improveprediction accuracy for both n-gram and sem-gramalgorithms, as well as provide an approach to de-signing icon-based AAC devices that can evolve andadapt to users as their needs and abilities mature, po-tentially even suggesting new vocabulary words asthe users age.AcknowledgmentsThe authors would like to thank the creators ofthe Blog Authorship Corpus for making the corpusfreely available for non-commercial research pur-poses.
The material in this paper is based upon worksupported by the National Science Foundation underGrant No.
0914808.
Any opinions, findings, con-clusions, or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of the National Science Foundation.ReferencesJ.
Allan and H. Raghavan.
2002.
Using part-of-speechpatterns to reduce query ambiguity.
In Proceedingsof the 25th annual international ACM SIGIR confer-ence on Research and development in information re-trieval, SIGIR ?02, pages 307?314, New York, NY,USA.
ACM.D.
Beukelman and P. Mirenda.
1998.
Augmentationand alternative communication: Management of se-vere communication disorders in children and adults.Paul H. Brookes, Baltimore.S.
Bickel, P. Haider, and T. Scheffer.
2005.
Predictingsentences using n-gram language models.
In Proceed-ings of the conference on Human Language Technol-ogy and Empirical Methods in Natural Language Pro-cessing, HLT ?05, pages 193?200, Stroudsburg, PA,USA.
Association for Computational Linguistics.S.
Bird, E. Klein, and E. Loper.
2009.
Natural LanguageProcessing with Python.
O?Reilly Media, 1 edition,July.A.
C. Broerse and E. J. Zwaan.
1966.
The infor-mation value of initial letters in the identification ofwords.
Journal of Verbal Learning and Verbal Behav-ior, 5(5):441?446, October.34P.
Demasco and K. McCoy.
1992.
Generating text fromcompressed input: an intelligent interface for peo-ple with severe motor impairments.
Commun.
ACM,35(5):68?78, May.H.
Fang and C. Zhai.
2006.
Semantic term matching inaxiomatic approaches to information retrieval.
In Pro-ceedings of the 29th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, SIGIR ?06, pages 115?122, New York,NY, USA.
ACM.R.
Hemayati, W. Meng, and C. Yu.
2007.
Semantic-based grouping of search engine results using Word-Net.
In Proceedings of the joint 9th Asia-Pacific weband 8th international conference on web-age informa-tion management conference on Advances in data andweb management, APWeb/WAIM?07, pages 678?686,Berlin, Heidelberg.
Springer-Verlag.J.
Higginbotham, A. Bisantz, M. Sunm, K. Adams,and F. Yik.
2009.
The effect of context primingand task type on augmentative communication perfor-mance.
Augmentative and Alternative Communica-tion, 25(1):19?31.Y.
How and M. Kan. 2005.
Optimizing predictive textentry for short message service on mobile phones.
InHuman Computer Interfaces International (HCII 05).A.
Ja?rvelin, A. Ja?rvelin, and K. Ja?rvelin.
2007. s-grams: Defining generalized n-grams for informationretrieval.
Information Processing & Management,43(4):1005?1019, July.G.
Karberis and G. Kouroupetroglou.
2002.
Transform-ing spontaneous telegraphic language to Well-Formedgreek sentences for alternative and augmentative com-munication.
In Proceedings of the Second HellenicConference on AI: Methods and Applications of Artifi-cial Intelligence, SETN ?02, pages 155?166, London,UK, UK.
Springer-Verlag.T.
Kiss and J. Strunk.
2006.
Unsupervised multilin-gual sentence boundary detection.
Comput.
Linguist.,32(4):485?525, December.H.
Koester and S. Levine.
1996.
Effect of a word predic-tion feature on user performance.
Augmentative andAlternative Communication, 12(3):155?168.H.
Koester and S. Levine.
1997.
Keystroke-level modelsfor user performance with word prediction.
Augmen-tative and Alternative Communication, 13(4).G.
Lesher and G. Rinkus.
2002.
Domain-Specific wordprediction for augmentative communication.
In Pro-ceedings of the RESNA 2002 Annual Conference.G.
Lesher and C. Sanelli.
2000.
A Web-Based system forautonomous text corpus generation.
In Proceedings ofISAAC.G.
Lesher, B. Moulton, and J. Higginbotham.
1998.Techniques for augmenting scanning communica-tion.
Augmentative and Alternative Communication,14(2):81?101, January.J.
Li and G. Hirst.
2005.
Semantic knowledge in wordcompletion.
In Proceedings of the 7th internationalACM SIGACCESS conference on Computers and ac-cessibility, Assets ?05, pages 121?128, New York, NY,USA.
ACM.C.
Lin and E. Hovy.
2003.
Automatic evaluation of sum-maries using n-gram co-occurrence statistics.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 71?78, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Y.
Lv and C. Zhai.
2009.
Positional language mod-els for information retrieval.
In Proceedings of the32nd international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?09, pages 299?306, New York, NY, USA.
ACM.J.
Matiasek and M. Baroni.
2003.
Exploiting long dis-tance collocational relations in predictive typing.
InProceedings of the 2003 EACL Workshop on Lan-guage Modeling for Text Entry Methods, TextEntry?03, pages 1?8, Stroudsburg, PA, USA.
Association forComputational Linguistics.Mayer-Johnson.
2012.
Picture communication symbolscollections (http://www.mayer-johnson.com).
March.K.
McCoy, C. Pennington, and A. Badman.
1998.
Com-pansion: From research prototype to practical integra-tion.
Natural Language Engineering, 4(01):73?95.S.
Nikolova, M. Tremaine, and P. Cook.
2010.
Clickon bake to get cookies: guiding word-finding with se-mantic associations.
In Proceedings of the 12th inter-national ACM SIGACCESS conference on Computersand accessibility, ASSETS ?10, pages 155?162, NewYork, NY, USA.
ACM.R.
Patel, S. Pilato, and D. Roy.
2004.
Beyond linearsyntax: An Image-Oriented communication aid.
Jour-nal of Assistive Technology Outcomes and Benefits,(1):57?66.M.
F. Porter.
1997.
Readings in information retrieval.chapter An algorithm for suffix stripping, pages 313?316.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA.Ranks.
2012.
English stopwords (http://www.ranks.nl),March.J.
Schler, M. Koppel, S. Argamon, and J. Pennebaker.2006.
Effects of age and gender on blogging.
In Pro-ceedings of 2006 AAAI Spring Symposium on Compu-tational Approaches for Analyzing Weblogs.C.
Suen.
1979. n-Gram statistics for natural language un-derstanding and text processing.
Pattern Analysis andMachine Intelligence, IEEE Transactions on, PAMI-1(2):164?172, April.35K.
Trnka and K. McCoy.
2007.
Corpus studies in wordprediction.
In Proceedings of the 9th internationalACM SIGACCESS conference on Computers and ac-cessibility, Assets ?07, pages 195?202, New York, NY,USA.
ACM.K.
Trnka, D. Yarrington, K. McCoy, and C. Pennington.2006.
Topic modeling in fringe word prediction forAAC.
In Proceedings of the 11th international con-ference on Intelligent user interfaces, IUI ?06, pages276?278, New York, NY, USA.
ACM.E.
Tzoukermann, J. Klavans, and C. Jacquemin.
1997.Effective use of natural language processing tech-niques for automatic conflation of multi-word terms:the role of derivational morphology, part of speech tag-ging, and shallow parsing.
SIGIR Forum, 31(SI):148?155, July.O.
Udwin and W. Yule.
1990.
Augmentative com-munication systems taught to cerebral palsied chil-dren - a longitudinal study:I. the acquisition of signsand symbols, and syntactic aspects of their use overtime.
British Journal of Disorders of Communication,25(3):295?309, January.H.
Van Balkom and M. Welle Donker-Gimbrere.
1996.A psycholinguistic approach to graphic language use.Augmentative and alternative communication: Euro-pean Perspectives, pages 153?170.A.
Van Den Bosch and P. Berck.
2009.
Memory-basedmachine translation and language modeling.
In ThePrague Bulletin of Mathematical Linguistics.A.
Van Den Bosch.
2006.
Scalable classification-basedword prediction and confusible correction.
TraitementAutomatique des Langues, 46(2):39?63.K.
Vertanen and P. O. Kristensson.
2011.
The imagina-tion of crowds: Conversational AAC language model-ing using crowdsourcing and large data sources.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages700?711.
ACL.T.
Wandmacher and J. Antoine.
2006.
Training languagemodels without appropriate language resources: Ex-periments with an AAC system for disabled people.
InProceedings of LREC.G.
Ward.
2002.
Moby word list: American english(http://www.gutenberg.org/ebooks/3201).
Public do-main in the USA.S.
J. Westerman and T. Cribbin.
2000.
Mapping seman-tic information in virtual space: dimensions, varianceand individual differences.
International Journal ofHuman-Computer Studies, 53(5):765?787, November.Widgit.
2012.
About symbols (http://www.widgit.com),March.36
