Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 406?411,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning Non-linear Features for Machine Translation Using GradientBoosting MachinesKristina ToutanovaMicrosoft ResearchRedmond, WA 98502kristout@microsoft.comByung-Gyu Ahn?Johns Hopkins UniversityBaltimore, MD 21218bahn@cs.jhu.eduAbstractIn this paper we show how to auto-matically induce non-linear features formachine translation.
The new featuresare selected to approximately maximizea BLEU-related objective and decomposeon the level of local phrases, which guar-antees that the asymptotic complexity ofmachine translation decoding does not in-crease.
We achieve this by applying gra-dient boosting machines (Friedman, 2000)to learn newweak learners (features) in theform of regression trees, using a differen-tiable loss function related to BLEU.
Ourresults indicate that small gains in perfor-mance can be achieved using this methodbut we do not see the dramatic gains ob-served using feature induction for otherimportant machine learning tasks.1 IntroductionThe linear model for machine translation (Och andNey, 2002) has become the de-facto standard inthe field.
Recently, researchers have proposed alarge number of additional features (TaroWatan-abe et al, 2007; Chiang et al, 2009) and param-eter tuning methods (Chiang et al, 2008b; Hop-kins and May, 2011; Cherry and Foster, 2012)which are better able to scale to the larger pa-rameter space.
However, a significant feature en-gineering effort is still required from practition-ers.
When a linear model does not fit well, re-searchers are careful to manually add importantfeature conjunctions, as for example, (Daume?
IIIand Jagarlamudi, 2011; Clark et al, 2012).
In therelated field of web search ranking, automaticallylearned non-linear features have brought dramaticimprovements in quality (Burges et al, 2005; Wu?This research was conducted during the author?s intern-ship at Microsoft Researchet al, 2010).
Here we adapt the main insights ofsuch work to the machine translation setting andshare results on two language pairs.Some recent works have attempted to relax thelinearity assumption on MT features (Nguyen etal., 2007), by defining non-parametric models oncomplete translation hypotheses, for use in an n-best re-ranking setting.
In this paper we developa framework for inducing non-linear features inthe form of regression decision trees, which de-compose locally and can be integrated efficientlyin decoding.
The regression trees encode non-linear feature combinations of the original fea-tures.
We build on the work by Friedman (2000)which shows how to induce features to minimizeany differentiable loss function.
In our applica-tion the features are regression decision trees, andthe loss function is the pairwise ranking log-lossfrom the PRO method for parameter tuning (Hop-kins and May, 2011).
Additionally, we show howto design the learning process such that the in-duced features are local on phrase-pairs and theirlanguage model and reordering context, and thuscan be incorporated in decoding efficiently.Our results using re-ranking on two languagepairs show that the feature induction approach canbring small gains in performance.
Overall, eventhough the method shows some promise, we donot see the dramatic gains that have been seen forthe web search ranking task (Wu et al, 2010).
Fur-ther improvements in the original feature set andthe induction algorithm, as well as full integrationin decoding are needed to potentially result in sub-stantial performance improvements.2 Feature learning using gradientboosting machinesIn the linear model for machine translation, thescores of translation hypotheses are weightedsums of a set of input features over the hypotheses.406Figure 1: A Bulgarian source sentence (meaning ?theconference in Bulgaria?, together with a candidate transla-tion.
Local and global features for the translation hypoth-esis are shown.
f0=smoothed relative frequency estimateof log p(s|t); f1=lexical weighting estimate of log p(s|t);f2=joint count of the phrase-pair; f3=sum of language modellog-probabilities of target phrase words given context.For a set of features f1(h), .
.
.
, fL(h) and weightsfor these features ?1, .
.
.
, ?L, the hypothesisscores are defined as: F (h) = ?l=1...L ?lfl(h).In current state-of-the-art models, the featuresfl(h) decompose locally on phrase-pairs (withlanguage model and reordering context) inside thehypotheses.
This enables hypothesis recombina-tion during machine translation decoding, leadingto faster and more accurate search.
As an exam-ple, Figure 1 shows a Bulgarian source sentence(spelled phonetically in Latin script) and a can-didate translation.
Two phrase-pairs are used tocompose the translation, and each phrase-pair hasa set of local feature function values.
A mini-mal set of four features is shown, for simplicity.We can see that the hypothesis-level (global) fea-ture values are sums of phrase-level (local) featurevalues.
The score of a translation given featureweights ?
can be computed either by scoring thephrase-pairs and adding the scores, or by scoringthe complete hypothesis by computing its globalfeature values.
The local feature values do look atsome limited context outside of a phrase-pair, tocompute language model scores and re-orderingscores; therefore we say that the features are de-fined on phrase-pairs in context.We start with such a state-of-the-art linearmodel with decomposable features and show howwe can automatically induce additional features.The new features are also locally decomposable,so that the scores of hypotheses can be computedas sums of phrase-level scores.
The new localphrase-level features are non-linear combinationsof the original phrase-level features.Figure 2: Example of two decision tree features.
The leftdecision tree has linear nodes and the right decision tree hasconstant nodes.2.1 Form of induced featuresWe will use the example in Figure 1 to introducethe form of the new features we induce and to givean intuition of why such features might be useful.The new features are expressed by regression de-cision trees; Figure 2 shows two examples.One intuition we might have is that, if a phrasepair has been seen very few times in the trainingcorpus (for example, the first phrase pair P1 in theFigure has been seen only one time f2 = 1), wewould like to trust its lexical weighting channelmodel score f1 more than its smoothed relative-frequency channel estimate f0.
The first regres-sion tree feature h1 in Figure 2 captures this in-tuition.
The feature value for a phrase-pair ofthis feature is computed as follows: if f2 ?2, then h1(f0, f1, f2, f3) = 2 ?
f1; otherwise,h1(f0, f1, f2, f3) = f1.
The effect of this newfeature h1 is to boost the importance of the lexi-cal weighting score for phrase-pairs of low jointcount.
More generally, the regression tree fea-tures we consider have either linear or constantleaf nodes, and have up to 8 leaves.
Deeper treescan capture more complex conditions on severalinput feature values.
Each non-leaf node performsa comparison of some input feature value to athreshold and each leaf node (for linear nodes) re-turns the value of some input feature multipliedby some factor.
For a given regression tree withlinear nodes, all leaf nodes are expressions of thesame input feature but have different coefficientsfor it (for example, both leaf nodes of h1 returnaffine functions of the input feature f1).
A deci-sion tree feature with constant-valued leaf nodesis illustrated by the right-hand-side tree in Figure2.
For these decision trees, the leaf nodes containa constant, which is specific to each leaf.
Thesekinds of trees can effectively perform conjunctionsof several binary-valued input feature functions; orthey can achieve binning of real-values features to-gether with conjunctions over binned values.407Having introduced the form of the new featureswe learn, we now turn to the methodology for in-ducing them.
We apply the framework of gradientboosting for decision tree weak learners (Fried-man, 2000).
To define the framework, we needto introduce the original input features, the differ-entiable loss function, and the details of the treegrowing algorithm.
We discuss these in turn next.2.2 Initial featuresOur baseline MT system uses relative frequencyand lexical weighting channel model weights, oneor more language models, distortion penalty, wordcount, phrase count, and multiple lexicalized re-ordering weights, one for each distortion type.
Wehave around 15 features in this base feature set.We further expand the input set of features to in-crease the possibility that useful feature combi-nations could be found by our feature inductionmethod.
The large feature set contains around190 features, including source and target wordcount features, joint phrase count, lexical weight-ing scores according to alternative word-alignmentmodel ran over morphemes instead of words, in-dicator lexicalized features for insertion and dele-tion of the top 15 words in each language, cluster-based insertion and deletion indicators using hardword clustering, and cluster based signatures ofphrase-pairs.
This is the feature set we use as abasis for weak learner induction.2.3 Loss functionWe use a pair-wise ranking log-loss as in thePRO parameter tuning method (Hopkins and May,2011).
The loss is defined by comparing the modelscores of pairs of hypotheses hi and hj wherethe BLEU score of the first hypothesis is greaterthan the BLEU score of the second hypothesis bya specified threshold.
1We denote the sentences in a corpus ass1, s2, .
.
.
, sN .
For each sentence sn, we de-note the ordered selected pairs of hypotheses as[hni1 , hnj1 ], .
.
.
, [hniK , hnjK ].
The loss-function ?
isdefined in terms of the hypothesis model scores1In our implementation, for each sentence, we sample10, 000 pairs of translations and accept a pair of transla-tions for use with probability proportional to the BLEU scoredifference, if that difference is greater than the threshold of0.04.
The top K = 100 or K = 300 hypothesis pairs withthe largest BLEU difference are selected for computation ofthe loss.
We compute sentence-level BLEUscores by add-?smoothing of the match counts for computation of n-gramprecision.
The ?
and K parameters are chosen via cross-validation.1: F0(x) = argmin?
?
(F (x, ?
))2: for m = 1toM do3: yr = ?[??
(F (x))?F (xr) ]F (x)=Fm?1(x), r =1 .
.
.
R4: ?m = argmin?,?
?Rr=1[yr ?
?h(xi;?
)]25: ?m = argmin?
?
(Fm?1(x) + ?h(x;?m)6: Fm(x) = Fm?1(x) + ?mh(x;?m)7: end forFigure 3: A gradient boosting algorithm for localfeature functions.F (h) as follows: ?n=1...N?k=1...K log(1 +eF (hnjk)?F (hnik )).The idea of the gradient boosting method is toinduce additional features by computing a func-tional gradient of the target loss function and itera-tively selecting the next weak learner (feature) thatis most parallel to the negative gradient.
Since wewant to induce features such that the hypothesisscores decompose locally, we need to formulateour loss function as a function of local phrase-pairin context scores.
Having the model scores de-compose locally means that the scores of hypothe-ses F (h) decompose as F (h) = ?pr?h F (pr)),where by pr ?
h we denote the enumeration overphrase pairs in context that are parts of h. If xr de-notes the input feature vector for a phrase-pair incontext pr, the score of this phrase-pair can be ex-pressed as F (xr).
Appendix A expresses the pair-wise log-loss as a function of the phrase scores.We are now ready to introduce the gradientboosting algorithm, summarized in Figure 3.
Inthe first step of the algorithm, we start by set-ting the phrase-pair in context scoring functionF0(x) as a linear function of the input feature val-ues, by selecting the feature weights ?
to min-imize the PRO loss ?
(F0(x)) as a function of?.
The initial scores have the form F0(x) =?l=1...L ?lfl(x).This is equivalent to using the(Hopkins and May, 2011) method of parametertuning for a fixed input feature set and a linearmodel.
We used LBFGS for the optimization inLine 1.
Then we iterate and induce a new de-cision tree weak learner h(x;?m) like the exam-ples in Figure 2 at each iteration.
The parame-ter vectors ?m encode the topology and parame-ters of the decision trees, including which featurevalue is tested at each node, what the compari-son cutoffs are, and the way to compute the val-ues at the leaf nodes.
After a new decision tree408Language Train Dev-Train Dev-Select TestChs-En 999K NIST02+03 2K NIST05Fin-En 2.2M 12K 2K 4.8KTable 1: Data sets for the two language pairs Chinese-English and Finnish-English.Chs-En Fin-EnFeatures Tune Dev-Train Test Dev-Train Testbase MERT 31.3 30.76 49.8 51.31base PRO 31.1 31.16 49.7 51.56large PRO 31.8 31.44 49.8 51.77boost-global PRO 31.8 31.30 50.0 51.87boost-local PRO 31.8 31.44 50.1 51.95Table 2: Results for the two language pairs using differentweight tuning methods and feature sets.h(x;?m) is induced, it is treated as new featureand a linear coefficient ?m for that feature is setby minimizing the loss as a function of this pa-rameter (Line 5).
The new model scores are set asthe old model scores plus a weighted contributionfrom the new feature (Line 6).
At the end of learn-ing, we have a linear model over the input featuresand additional decision tree features.
FM (x) =?l=1...L ?lfl(x) +?m=1...M ?mh(x;?m).
Themost time-intensive step of the algorithm is the se-lection of the next decision tree h. This is doneby first computing the functional gradient of theloss with respect to the phrase scores F (xr) at thepoint of the current model scores Fm?1(xr).
Ap-pendix A shows a derivation of this gradient.
Wethen induce a regression tree using mean-square-error minimization, setting the direction given bythe negative gradient as a target to be predicted us-ing the features of each phrase-pair in context in-stance.
This is shown as the setting of the ?m pa-rameters by mean-squared-error minimization inLine 4 of the algorithm.
The minimization is doneapproximately by a standard greedy tree-growingalgorithm (Breiman et al, 1984).
When we tuneweights to minimize the loss, such as the weights?
of the initial features, or the weights ?m of in-duced learners, we also include an L2 penalty onthe parameters, to prevent overfitting.3 ExperimentsWe report experimental results on two languagepairs: Chinese-English, and Finnish-English.
Ta-ble 1 summarizes statistics about the data.
Foreach language pair, we used a training set (Train)for extracting phrase tables and language models,a Dev-Train set for tuning feature weights and in-ducing features, a Dev-Select set for selecting hy-perparameters of PRO tuning and selecting a stop-ping point and other hyperparameters of the boost-ing method, and a Test set for reporting final re-sults.
For Chinese-English, the training corpusconsists of approximately one million sentencepairs from the FBIS and HongKong portions ofthe LDC data for the NIST MT evaluation and theDev-Train and Test sets are from NIST competi-tions.
The MT system is a phrasal system with a 4-gram language model, trained on the Xinhua por-tion of the English Gigaword corpus.
The phrasetable has maximum phrase length of 7 words oneither side.
For Finnish-English we used a data-set from a technical domain of software manuals.For this language pair we used two language mod-els: one very large model trained on billions ofwords, and another language model trained fromthe target side of the parallel training set.
We re-port performance using the BLEU-SBP metric pro-posed in (Chiang et al, 2008a).
This is a vari-ant of BLEU (Papineni et al, 2002) with strictbrevity penalty, where a long translation for onesentence can not be used to counteract the brevitypenalty for another sentence with a short transla-tion.
Chiang et al (2008a) showed that this metricovercomes several undesirable properties of BLEUand has better correlation with human judgements.In our experiments with different feature sets andhyperparameters we observed more stable resultsand better correlation of Dev-Train, Dev-Select,and Test results using BLEU-SBP.
For our exper-iments, we first trained weights for the base fea-ture sets described in Section 2.2 using MERT.
Wethen decoded the Dev-Train, Dev-Select, and Testdatasets, generating 500-best lists for each set.
Allresults in Table 2 report performance of re-rankingon these 500-best lists using different feature setsand parameter tuning methods.The baseline (base feature set) performance us-ing MERT and PRO tuning on the two languagepairs is shown on the first two lines.
In line withprior work, PRO tuning achieves a bit lower scoreson the tuning set but higher scores on the test set,compared to MERT.
The large feature set addi-tionally contains over 170 manually specified fea-tures, described in Section 2.2.
It was infeasibleto run MERT training on this feature set.
The testset results using PRO tuning for the large set areabout a quarter of a BLEU-SBP point higher thanthe results using the base feature set on both lan-guage pairs.
Finally, the last two rows show theperformance of the gradient boosting method.
In409addition to learning locally decomposable featuresboost-local, we also implemented boost-global,where we are learning combinations of the globalfeature values and lose decomposability.
The fea-tures learned by boost-global can not be com-puted exactly on partial hypotheses in decodingand thus this method has a speed disadvantage, butwe wanted to compare the performance of boost-local and boost-global on n-best list re-rankingto see the potential accuracy gain of the two meth-ods.
We see that boost-local is slightly better inperformance, in addition to being amenable to ef-ficient decoder integration.The gradient boosting results are mixed; forFinnish-English, we see around .2 gain of theboost-local model over the large feature set.There is no improvement on Chinese-English, andthe boost-global method brings slight degrada-tion.
We did not see a large difference in perfor-mance among models using different decision treeleaf node types and different maximum numbersof leaf nodes.
The selected boost-local modelfor FIN-ENU used trees with maximum of 2 leafnodes and linear leaf values; 25 new features wereinduced before performance started to degradeon the Dev-Select set.
The induced features forFinnish included combinations of language modeland channel model scores, combinations of wordcount and channel model scores, and combina-tions of channel and lexicalized reordering scores.For example, one feature increases the contribu-tion of the relative frequency channel score forphrases with many target words, and decreases thechannel model contribution for shorter phrases.The best boost-local model for Chs-Enu usedtrees with a maximum of 2 constant-values leafnodes, and induced 24 new tree features.
The fea-tures effectively promoted and demoted phrase-pairs in context based on whether an input fea-ture?s value was smaller than a determined cutoff.In conclusion, we proposed a new method toinduce feature combinations for machine transla-tion, which do not increase the decoding complex-ity.
There were small improvements on one lan-guage pair in a re-ranking setting.
Further im-provements in the original feature set and the in-duction algorithm, as well as full integration in de-coding are needed to result in substantial perfor-mance improvements.This work did not consider alternative waysof generating non-linear features, such as takingproducts of two or more input features.
It wouldbe interesting to compare such alternatives to theregression tree features we explored.ReferencesLeo Breiman, Jerome Friedman, Charles J.
Stone, andR.A.
Olshen.
1984.
Classification and RegressionTrees.
Chapman and Hall.Chris Burges, Tal Shaked, Erin Renshaw, Matt Deeds,Nicole Hamilton, and Greg Hullender.
2005.
Learn-ing to rank using gradient descent.
In ICML.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
InHLT-NAACL.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008a.
Decomposability of trans-lation metrics for improved evaluation and efficientalgorithms.
In EMNLP.David Chiang, Yuval Marton, and Philp Resnik.
2008b.Online large margin training of syntactic and struc-tural translation features.
In EMNLP.D.
Chiang, W. Wang, and K. Knight.
2009.
11,001new features for statistical machine translation.
InNAACL.Jonathan Clark, Alon Lavie, and Chris Dyer.
2012.One system, many domains: Open-domain statisti-cal machine translation via feature augmentation.
InAMTA.Hal Daume?
III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by miningunseen words.
In ACL.Jerome H. Friedman.
2000.
Greedy function approx-imation: A gradient boosting machine.
Annals ofStatistics, 29:1189?1232.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In EMNLP.Patrick Nguyen, Milind Mahajan, and Xiaodong He.2007.
Training non-parametric features for statis-tical machine translation.
In Second Workshop onStatistical Machine Translation.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In ACL.TaroWatanabe, Jun Suzuki, Hajime Tsukuda, andHideki Isozaki.
2007.
Online large-margin trainingfor statistical machine translation.
In EMNLP.410QiangWu, Christopher J. Burges, Krysta M. Svore, andJianfeng Gao.
2010.
Adapting boosting for infor-mation retrieval measures.
Information Retrieval,13(3), June.4 Appendix A: Derivation of derivativesHere we express the loss as a function of phrase-level in context scores and derive the derivative ofthe loss with respect to these scores.Let us number all phrase-pairs in context inall hypotheses in all sentences as p1, .
.
.
, pR anddenote their input feature vectors as x1, .
.
.
,xR.We will use F (pr) and F (xr) interchange-ably, because the score of a phrase-pair incontext is defined by its input feature vec-tor.
The loss ?
(F (xr)) is expressed as follows:?Nn=1?Kk=1 log(1 + e?pr?hnjkF (xr)?
?pr?hnikF (xr)).Next we derive the derivatives of the loss?
(F (x)) with respect to the phrase scores.
Intu-itively, we are treating the scores we want to learnas parameters for the loss function; thus the lossfunction has a huge number of parameters, onefor each instance of each phrase pair in context ineach translation.
We ask the loss function if thesescores could be set in an arbitrary way, what di-rection it would like to move them in to be mini-mized.
This is the direction given by the negativegradient.Each phrase-pair in context pr occurs in exactlyone hypothesis h in one sentence.
It is possiblethat two phrase-pairs in context share the same setof input features, but for ease of implementationand exposition, we treat these as different train-ing instances.
To express the gradient with respectto F (xr) we therefore need to focus on the termsof the loss from a single sentence and to take intoaccount the hypothesis pairs [hj,k, hi,k] where theleft or the right hypothesis is the hypothesis h con-taining our focus phrase pair pr.
??
(F (x))?F (xr) is ex-pressed as:= ?k:h=hik ?e?pr?hnjkF (xr)?
?pr?hnikF (xr)1+e?pr?hnjkF (xr)?
?pr?hnikF (xr)+ ?k:h=hjke?pr?hnjkF (xr)?
?pr?hnikF (xr)1+e?pr?hnjkF (xr)?
?pr?hnikF (xr)Since in the boosting step we induce a deci-sion tree to fit the negative gradient, we can seethat the feature induction algorithm is trying to in-crease the scores of phrases that occur in betterhypotheses (the first hypothesis in each pair), andit increases the scores more if weaker hypotheseshave higher advantage; it is also trying to decreasethe scores of phrases in weaker hypotheses that arecurrently receiving high scores.411
