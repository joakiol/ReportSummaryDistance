Automatic Construction of Japanese KATAKANA Variant Listfrom Large CorpusTakeshi MasuyamaInformation Technology CenterUniversity of Tokyo7-3-1, Hongo, BunkyoTokyo 113-0023Japantak@r.dl.itc.u-tokyo.ac.jpSatoshi SekineComputer Science DepartmentNew York University715 Broadway, 7th floorNew York NY 10003USAsekine@cs.nyu.eduHiroshi NakagawaInformation Technology CenterUniversity of Tokyo7-3-1, Hongo, BunkyoTokyo 113-0023Japannakagawa@dl.itc.u-tokyo.ac.jpAbstractThis paper presents a method to constructJapanese KATAKANA variant list fromlarge corpus.
Our method is useful forinformation retrieval, information extrac-tion, question answering, and so on, becauseKATAKANA words tend to be used as?loan words?
and the transliteration causesseveral variations of spelling.
Our methodconsists of three steps.
At step 1, our sys-tem collects KATAKANA words from largecorpus.
At step 2, our system collects can-didate pairs of KATAKANA variants fromthe collected KATAKANA words using aspelling similarity which is based on the editdistance.
At step 3, our system selects vari-ant pairs from the candidate pairs using asemantic similarity which is calculated bya vector space model of a context of eachKATAKANA word.
We conducted exper-iments using 38 years of Japanese news-paper articles and constructed JapaneseKATAKANA variant list with the perfor-mance of 97.4% recall and 89.1% precision.Estimating from this precision, our systemcan extract 178,569 variant pairs from thecorpus.1 Introduction?Loan words?
in Japanese are usually writ-ten by a phonogram type of Japanese charac-ter set, KATAKANA.
Because of loan words,the transliteration causes several variations ofspelling.
Therefore, Japanese KATAKANAwords sometimes have several different or-thographies for each original word.
For exam-ple, we found at least six different spellings of?spaghetti?
in 38 years of Japanese newspaperarticles, such as ???????,?
????????,?
???????,?
??????,?
???????,?
and ??????.?
The different ex-pression causes problems when we use searchengines, question answering systems, and so on(Yamamoto et al, 2003).
For example, when weinput ????????
as a query for a search en-gine or a query for a question answering system,we may not be able to find the web pages or theanswers for which we are looking, if a differentorthography for ????????
is used.We investigated how many documents wereretrieved by Google 1 when each JapaneseKATAKANA variant of ?spaghetti?
was usedas a query.
The result is shown as Table 1.For example, when we inputted ????????
as a query of Google, 104,000 documentswere retrieved and the percentage was 34.6%,calculated by 104,000 divided by 300,556.
FromTable 1, we see that each of six variants appearsfrequently and thus we may not be able to findthe web pages for which we are looking.Although we can manually create JapaneseKATAKANA variant list, it is a labor-intensivetask.
In order to solve the problem, we proposean automatic method to construct JapaneseKATAKANA variant list from large corpus.Variant # of retrieved documents??????
104,000 (34.6%)???????
25,400 (8.5%)??????
1,570 (0.5%)?????
131,000 (43.6%)??????
37,700 (12.5%)?????
886 (0.3%)Total 300,556 (100%)Table 1: Number of retrieved documents whenwe inputted each Japanese KATAKANA vari-ant of ?spaghetti?
as a query of Google.Our method consists of three steps.
First,we collect Japanese KATAKANA words fromlarge corpus.
Then, we collect candidate pairs ofKATAKANA variants based on a spelling simi-larity from the collected Japanese KATAKANAwords.
Finally, we select variant pairs using1http://www.google.co.jp/a semantic similarity based on a vector spacemodel of a context of each KATAKANA word.This paper is organized as follows.
Section2 describes related work.
Section 3 presentsour method to construct Japanese KATAKANAvariant list from large corpus.
Section 4 showssome experimental results using 38 years ofJapanese newspaper articles, which we call ?theCorpus?
from now on, followed by evaluationand discussion.
Section 5 describes future work.Section 6 offers some concluding remarks.2 Related WorkThere are some related work for the problemswith Japanese spelling variations.
In (Shishi-bori and Aoe, 1993), they have proposed amethod for generating Japanese KATAKANAvariants by using replacement rules, such as?
(be) ?
??
(ve) and ?
(chi) ?
??
(tsi).Here, ???
represents ?substitution.?
For ex-ample, when we apply these rules to ?????(Venezia),?
three different spellings are gener-ated as variants, such as ??????,?
??????,?
and ???????.
?Kubota et al have extracted JapaneseKATAKANA variants by first transformingKATAKANA words to directed graphs based onrewrite rules and by then checking whether thedirected graphs contain the same labeled pathor not (Kubota et al, 1993).
A part of theirrewrite rules is shown in Table 2.
For exam-ple, when applying these rules to ??????(Kuwait),?
??
a?c,?
??
b?c,?
???
d?c?
aregenerated as variants.KATAKANA String ?
Symbol??
(we), ?
(e) ?
a??
(we), ??
(ue) ?
b??
(twu), ?
(to), ?
(tsu) ?
c?
(macron) ?
??
(small e), ?
(e) ?
dTable 2: A part of rewrite rules.In (Shishibori and Aoe, 1993) and (Kubotaet al, 1993), they only paid attention to apply-ing their replacement or rewrite rules to wordsthemselves and didn?t pay attention to theircontexts.
Therefore, they wrongly decide that??????
is a variant of ????.?
Here, ??????
represents ?wave?
and ?????
repre-sents ?web.?
In our method, we will decide if ??????
and ?????
convey the same mean-ing or not using a semantic similarity based ontheir contexts.3 Construct Japanese KATAKANAVariant List from Large CorpusOur method consists of the following threesteps.1.
Collect Japanese KATAKANA words fromlarge corpus.2.
Collect candidate pairs of KATAKANAvariants from the collected KATAKANAwords using a spelling similarity.3.
Select variant pairs from the candidatepairs based on a semantic similarity.3.1 Collect KATAKANA Words fromLarge CorpusAt the first step, we collected JapaneseKATAKANA words which consist of aKATAKANA character, ?
(bullet), ?
(macron-1), and ?
(macron-2), which arecommonly used as a part of KATAKANAwords, using pattern matching.
For example,our system collects three KATAKANA words ?????????????
(Ludwig Erhard-1),???
(Soviet),?
??????????????
(Ludwig Erhard-2),?
????
(Germany)?from the following sentences.
Note that twomentions of ?Ludwig Erhard?
have differentorthographies.?
???????????????????????????????
(Defunct LudwigErhard-1 is called ?Father of The Mirac-ulous Economic Revival.?)?
?????????????????????????????????????????????????????????????????????????????????????
(If Soviet and East Eu-ropean countries give up their controllingconcepts and pursue the economic deregu-lation which Ludwig Erhard-2 of West Ger-many did in 1948, they may achieve themiraculous revival like West Germany.
)3.2 Spelling SimilarityAt the second step, our system collects candi-date pairs of two KATAKANA words, whichare similar in spelling, from the collectedKATAKANA words described in Section 3.1.We used ?string penalty?
to collect candidatepairs.
String penalty is based on the edit dis-tance (Hall and DOWLING, 1980) which is asimilarity measure between two strings.
Weused the following three types of operations.?
SubstitutionReplace a character with another charac-ter.?
DeletionDelete a character.?
InsertionInsert a character.We also added some scoring heuristics to theoperations based on a pronunciation similaritybetween characters.
The rules are tuned byhand using randomly selected training data.Some examples are shown in Table 3.
Here,???
represents ?substitution?
and lines with-out ?
represent ?deletion?
or ?insertion.?
Notethat ?Penalty?
represents a score of the stringpenalty from now on.For example, we give penalty 1 between ???????
and ??????,?
because the stringsbecome the same when we replace ???
with ???
and its penalty is 1 as shown in Table 3.Rules Penalty?
(a) ?
?
(small a) 1?
(zi) ?
?
(di) 1?
(macron) 1?
(ha) ?
?
(ba) 2?
(u) ?
?
(vu) 2?
(a) ?
?
(ya) 3?
(tsu) ?
?
(small tsu) 3Table 3: A part of our string penalty rules.We analyzed hundreds of candidate pairsof training data and figured out that mostKATAKANA variations occur when the stringpenalties were less than a certain threshold.
Inthis paper, we set 4 for the threshold and regardKATAKANA pairs as candidate pairs when thestring penalties are less than 4.
The thresh-old was tuned by hand using randomly selectedtraining data.For example, from the collected KATAKANAwords described in Section 3.1, our system col-lects the pair of ????????????
and????????????
?, since the stringpenalty is 3.3.3 Context SimilarityAt the final step, our system selects variantpairs from the candidate pairs described in Sec-tion 3.2 based on a semantic similarity.
We useda vector space model as a semantic similarity.In the vector space model, we treated 10 ran-domly selected articles from the Corpus as acontext of each KATAKANA word.We divided sentences of the articles intowords using JUMAN2 (Kurohashi and Nagao,1999) which is the Japanese morphological an-alyzer, and then extracted content words whichconsist of nouns, verbs, adjectives, adverbs, andunknown words except stopwords.
Stopwordsare composed of Japanese HIRAGANA charac-ters, punctuations, numerals, common words,and so on.We used a cosine measure to calculate a se-mantic similarity of two KATAKANA words.Suppose that one KATAKANA word makes acontext vector a and the other one makes b.The semantic similarity between two vectors aand b is calculated as follows.sim(a,b) = cos?
= a ?
b|a||b|(1)The cosine measure tends to overscore fre-quently appeared words.
Therefore, in order toavoid the problem, we treated log(N + 1) as ascore of a word appeared in a context.
Here, Nrepresents the frequency of a word in a context.We set 0.05 for the threshold of the seman-tic similarity, i.e.
we regard candidate pairs asvariant pairs when the semantic similarities aremore than 0.05.
The threshold was tuned byhand using randomly selected training data.In the case of ?????????????
(Lud-wig Erhard-1)?
and ??????????????
(Ludwig Erhard-2)?, the semantic similaritybecomes 0.17 as shown in Table 4.
Therefore,we regard them as a variant pair.Note that in Table 4, a decimal number rep-resents a score of a word appeared in a contextcalculated by log(N+1).
For example, the scoreof ??
(miracle) in the first context is 0.7.4 Experiments4.1 Data Preprocessing andPerformance MeasuresWe conducted the experiments using the Cor-pus.
The number of documents in the Cor-2http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.htmlWord ??????????????
(miracle):0.7??
(economy):1.9Context ?
(father):0.7??
(revival):0.7?
?
?Word ???????????????
(miracle):1.1???
(liberalization):1.4Context ??
(economy):2.4??
(revival):1.1?
?
?Similarity 0.17Table 4: Semantic similarity between ??????????????
and ??????????????.
?pus was 4,678,040 and the distinct numberof KATAKANA words in the Corpus was1,102,108.As for a test set, we collected candidatepairs whose string penalties range from 1 to12.
The number of collected candidate pairswas 2,590,240.
In order to create sample correctKATAKANA variant data, 500 out of 2,590,240were randomly selected and we evaluated themmanually by checking their contexts.
Throughthe evaluation, we found that no correct vari-ant pairs appeared from 10 to 12.
Thus, wethink that treating candidate pairs whose stringpenalties range from 1 to 12 can cover almostall of correct variant pairs.To evaluate our method, we used recall (Re),precision (Pr), and F measure (F ).
These per-formance measures are calculated by the follow-ing formulas:Re = number of pairs found and correcttotal number of pairs correct ,Pr = number of pairs found and correcttotal number of pairs found ,F = 2RePrRe + Pr .4.2 Experiment-1We conducted the first experiment based ontwo settings; one method uses only the spellingsimilarity and the other method uses both thespelling similarity and the semantic similarity.Henceforth, we use ?Methodp,??Methodp&s,?
?Ext,?
and ?Cor?
as thefollowing meanings.Methodp: The method using only the spellingsimilarityMethodp&s: The method using both thespelling similarity and the semanticsimilarityExt: The number of extracted candidate pairsCor: The number of correct variant pairs amongthe extracted candidate pairsNote that in Methodp&s, we ignored candi-date pairs whose string penalties ranged from4 to 12, since we set 4 for the threshold of thestring penalty as described in Section 3.2.The result is shown in Table 5.
For example,when the penalty was 2, 81 out of 117 were se-lected as correct variant pairs in Methodpandthe precision was 69.2%.
Also, 80 out of 98 wereselected as correct variant pairs in Methodp&sand the precision was 81.6%.As for Penalty 1-12 of Methodp, i.e.
we fo-cused on the string penalties between 1 and 12,the recall was 100%, because we regarded 269out of 500 as correct variant pairs and Methodpextracted all of them.
Also, the precision was53.8%, calculated by 269 divided by 500.
Com-paring Methodp&sto Methodp, the recall andthe precision of Methodp&swere well-balanced,since the recall was 97.4% and the precision was89.1%.In the same way, for Penalty 1-3, i.e.
thestring penalties between 1 and 3, the recall ofMethodpwas 98.1%, since five correct variantpairs between 4 and 12 were ignored and theremaining 264 out of 269 were found.
The preci-sion of Methodpwas 77.2%.
It was 23.4% higherthan the one of Penalty 1-12.
Thus, F measurealso improved 16.4%.
This result indicates thatsetting 4 for the threshold works well to improveoverall performance.Now, comparing Methodp&sto Methodpwhen the string penalties ranged from 1 to 3, therecall of Methodp&swas 0.7% lower.
This wasbecause Methodp&scouldn?t select two correctvariant pairs when the penalties were 1 and 2.However, the precision of Methodp&swas 16.2%higher.
Thus, F measure of Methodp&sim-proved 6.7% compared to the one of Methodp.From this result, we think that taking the se-mantic similarity into account is a better strat-egy to construct Japanese KATAKANA variantlist.Penalty MethodpMethodp&sCor/Ext (%) Cor/Ext (%)1 130/134 (97.0) 129/129 (100)2 81/117 (69.2) 80/98 (81.6)3 53/91 (58.2) 53/67 (79.1)4 2/14 (14.3)5 0/30 (0.0)6 1/14 (7.1)7 1/20 (5.0)8 0/14 (0.0)9 1/12 (8.3)10 0/16 (0.0)11 0/17 (0.0)12 0/21 (0.0)Re 264/269 (98.1) 262/269 (97.4)1-3 Pr 264/342 (77.2) 262/294 (89.1)F 86.4% 93.1%Re 269/269 (100)1-12 Pr 269/500 (53.8)F 70.0%Table 5: Comparison of MethodpandMethodp&s.4.3 Experiment-2We investigated how many variant pairs wereextracted in the case of six different spellingsof ?spaghetti?
described in Section 1.
Table 6shows the result of all combination pairs whenwe applied Methodp&s.For example, when the penalty was 1,Methodp&sselected seven candidate pairs andall of them were correct.
Thus, the recall was100%.
From Table 6, we see that the stringpenalties of all combination pairs ranged from1 to 3 and our system selected all of them bythe semantic similarity.Penalty Methodp&s1 7/7 (100%)2 6/6 (100%)3 2/2 (100%)Total 15/15 (100%)Table 6: A result of six different spellings of?spaghetti?
described in Section 1.4.4 Estimation of expected correctvariant pairsWe estimated how many correct variant pairscould be selected from the Corpus based on theprecision of Methodp&sas shown in Table 5.The result is shown in Table 7.
We find thatthe number of candidate pairs in the Corpuswas 100,746 for the penalty of 1, and 56,569 forthe penalty of 2, and 40,004 for the penalty of3.For example, when the penalty was 2, we esti-mate that 46,178 out of 56,569 could be selectedas correct variant pairs, since the precision was81.6% as shown in Table 5.
In total, we estimatethat 178,569 out of 197,319 could be selected ascorrect variant pairs from the Corpus.Penalty # of expected variant pairs1 100,746/100,746 (100%)2 46,178/56,569 (81.6%)3 31,645/40,004 (79.1%)Total 178,569/197,319 (90.5%)Table 7: Estimation of expected correct variantpairs.4.5 Error Analysis-1As shown in Table 5, our system couldn?t selecttwo correct variant pairs using semantic sim-ilarity when the penalties were 1 and 2.
Weinvestigated the reason from the training data.The problem was caused because the contextsof the pairs were diffrent.
For example, in thecase of ?????????
and ?????????,?
which represent the same building materialcompany ?Aroc Sanwa?
of Fukui prefecture inJapan, their contexts were completely differentbecause of the following reason.?
??????
?, ???????????????
(Aroc Sanwa): Thisword appeared with the name ofan athlete who took part in thenational athletic meet held in Toyamaprefecture in Japan, and the companysponsored the athlete.????????
(Aroc?Sanwa): Thisword was used to introduce thecompany in the article.Note that each context of these words wascomposed of only one article.4.6 Error Analysis-2From Table 5, we see that the numbers of incor-rect variant pairs selected by Methodp&swere18 and 14 for each penalty of 2 and 3.
We in-vestigated such cases in the training data.
Theexample of ????
(Cart, Kart)?
and ????(Card)?
is shown as follows.?
??
?, ??????
(Cart, Kart): This word wasused as the abbreviation of ?ShoppingCart,?
?Racing Kart,?
or ?SportKart.????
(Card): This word was used asthe abbreviation of ?Credit Card?
or?Cash Card?
and was also used as themeaning of ?Schedule of Games.
?Although these were not a variant pair, oursystem regarded the pair as the variant pair,because their contexts were similar.
In bothcontexts, ???
(utilization),?
???
(record),???
(guest),?
????
(aim),?
????
(team),????
(victory),?
???
(high, expensive),?
???
(success),?
???
(entry),?
and so on wereappeared frequently and therefore the semanticsimilarity became high.5 Future WorkIn this paper, we have used newspaper articlesto construct Japanese KATAKANA variant list.We are planning to apply our method on differ-ent types of corpus, such as patent documentsand Web data.
We think that more variationscan be found from Web data.
In the case of?spaghetti?
described in Section 1, we found atleast seven more different spellings, such as ???????,?
????????,?
???????,?
?????????
??????,?
???????,?
and ????????.
?Although we have manually tuned scoringrules of the string penalty using training data,we are planning to introduce an automaticmethod for learning the rules.We will also have to consider other charac-ter types of Japanese, i.e.
KANJI variationsand HIRAGANA variations, though we have fo-cused on only KATAKANA variations in thispaper.
For example, both ?????
and ??????
mean ?move?
in Japanese.6 ConclusionWe have described the method to constructJapanese KATAKANA variant list from largecorpus.
Unlike the previous work, we focusednot only on the similarity in spelling but alsoon the semantic similarity.From the experiments, we found thatMethodp&sperforms better than Methodp,since it constructed Japanese KATAKANAvariant list with high performance of 97.4% re-call and 89.1% precision.Estimating from the precision, we found that178,569 out of 197,319 could be selected as cor-rect variant pairs from the Corpus.
The resultcould be helpful to solve the variant problemsof information retrieval, information extraction,question answering, and so on.ReferencesPatrick A. V. Hall and GEOFF R. DOWLING.1980.
Approximate string matching.
Com-puting Surveys, 12(4):381?402.Jun?ichi Kubota, Yukie Shoda, MasahiroKawai, Hirofumi Tamagawa, and RyoichiSugimura.
1993.
A method of detectingKATAKANA variants in a document.
IPSJNL97-16, pages 111?117.Sadao Kurohashi and Makoto Nagao.
1999.Japanese morphological analysis system JU-MAN version 3.61.
Department of Informat-ics, Kyoto University.Masami Shishibori and Jun?ichi Aoe.
1993.
Amethod for generation and normalization ofkatakana variant notations.
IPSJ NL94-5,pages 33?40.Eiko Yamamoto, Yoshiyuki Takeda, and KyojiUmemura.
2003.
An IR similarity measurewhich is tolerant for morphological variation.Natural Language Processing, 10(1):63?80.
