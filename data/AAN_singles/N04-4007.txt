Advances in Children?s Speech Recognitionwithin an Interactive Literacy TutorAndreas Hagen, Bryan Pellom, Sarel Van Vuuren, and Ronald ColeCenter for Spoken Language ResearchUniversity of Colorado at Boulderhttp://cslr.colorado.eduAbstract1In this paper we present recent advances inacoustic and language modeling that improverecognition performance when children readout loud within digital books.
First we extendprevious work by incorporating cross-utterance word history information and dy-namic n-gram language modeling.
By addi-tionally incorporating Vocal Tract LengthNormalization (VTLN), Speaker-AdaptiveTraining (SAT) and iterative unsupervisedstructural maximum a posteriori linear regres-sion (SMAPLR) adaptation we demonstrate a54% reduction in word error rate.
Next, weshow how data from children?s read-aloudsessions can be utilized to improve accuracyin a spontaneous story summarization task.An error reduction of 15% over previous pub-lished results is shown.
Finally we describe anovel real-time implementation of our re-search system that incorporates time-adaptiveacoustic and language modeling.1 IntroductionPioneering research by MIT and CMU as well as morerecent work by the IBM Watch-me-Read Project havedemonstrated that speech recognition can play an effec-tive role in systems designed to improve children?sreading abilities (Mostow et al, 1994; Zue et al, 1996).In CMU?s Project LISTEN, for example, the tutor oper-ates by prompting children to read individual sentencesout loud.
The tutor listens to the child using speechrecognition and extracts features that can be used todetect oral reading miscues (Mostow et al, 2002; Tamet al 2003).
Upon detecting reading miscues, the tutorprovides appropriate feedback to the child.
Recent re-1This work was supported in part by grants from the National ScienceFoundation's Information Technology Research (ITR) Program andthe Interagency Educational Research Initiative (IERI) under grantsNSF/ITR: REC-0115419, NSF/IERI: EIA-0121201, NSF/ITR: IIS-0086107, NSF/IERI: 1R01HD-44276.01, NSF: INT-0206207; and theColeman Institute for Cognitive Disabilities.
The views expressed inthis paper do not necessarily represent the views of the NSF.sults show that such automated reading tutors can im-prove student achievement (Mostow et al 2003).
Pro-viding real time feedback by highlighting words as theare read out loud is the basis of at least one commercialproduct today (http://www.soliloquy.com).Cole et al (2003) and Wise et al (in press) describea new scientifically-based literacy program, Founda-tions to Fluency, in which a virtual tutor?a lifelike 3Dcomputer model?interacts with children in multimodallearning tasks to teach them to read.
A key componentof this program is the Interactive Book, which combinesreal-time speech recognition, facial animation, and natu-ral language understanding capabilities to teach childrento read and comprehend text.
Interactive Books aredesigned to improve student achievement by helpingstudents to learn to read fluently, to acquire new knowl-edge through deep understanding of what they read, tomake connections to other knowledge, and to expresstheir ideas concisely through spoken or written summa-ries.
Transcribed spoken summaries can be gradedautomatically to provide feedback to the student abouttheir comprehension.During reading out loud activities in InteractiveBooks, the goal is to design a computer interface andspeech recognizer that combine to teach the student toread fluently and naturally.
Here, speech recognition isused to track a child?s position within the text duringread-aloud sessions in addition to providing timing andconfidence information which can be used for readingassessment.
The speech recognizer must follow the stu-dents verbal behaviors accurately and quickly, so thecursor (or highlighted word) appears at the right placeand right time when the student is reading fluently, andpauses when the student hesitates to sound out a word.The recognizer must also score mispronounced wordsaccurately so that the student can revisit these wordsand receive feedback about their pronunciation aftercompleting a paragraph or page (since highlighting hy-pothesized mispronounced words when reading out loudmay disrupt fluent reading behavior).In this paper we focus on the problem of speech rec-ognition to track and provide feedback during readingout loud and to transcribe spoken summaries of text.Specifically, we describe several new methods for in-corporating language modeling knowledge into the readaloud task.
In addition, through use of speaker adapta-tion, we also demonstrate the potential for significantgains in recognition accuracy.
Finally, we leverageimprovements in speech recognition for read aloudtracking to improve performance for spoken story sum-marization.
Work reported here extends previous workin several important ways: by integrating the researchadvances into a real time system, and by including time-adaptive language modeling and time-adaptive acousticmodeling of the child?s voice into the system.The paper is organized as follows.
Sect.
2 describesour baseline speech recognition system and readingtracking method.
Sect.
3 presents our rationale for usingword-error-rate as a measure of performance.
Sect.
4describes the read aloud and story summarization cor-pora used in this work.
Sect.
5 describes and evaluatesproposed improvements in a read aloud speech recogni-tion task.
Sect.
6 describes how these improvementstranslate to improved recognition of story summariesproduced by a child.
Sect.
7 details our real-time systemimplementation.2 Baseline SystemFor this work we use the SONIC speech recognitionsystem (Pellom, 2001; Pellom and Hacioglu, 2003).The recognizer implements an efficient time-synchronous, beam-pruned Viterbi token-passing searchthrough a static re-entrant lexical prefix tree whileutilizing continuous density mixture Gaussian HMMs.For children?s speech, the recognizer has been trainedon 46 hours of data from children in grades K through 9extracted from the CU Read and Prompted speechcorpus (Hagen et al, 2003) and the OGI Kids?
speechcorpus (Shobaki et al, 2000).
Further, the baselinesystem utilizes PMVDR cepstral coefficients (Yapaneland Hansen, 2003) for improved noise robustness.During read-aloud operation, the speech recognizermodels the story text using statistical n-gram languagemodels.
This approach gives the recognizer flexibilityto insert/delete/substitute words based on acoustics andto provide accurate confidence information from theword-lattice.
The recognizer receives packets of audioand automatically detects voice activity.
When thechild speaks, the partial hypotheses are sent to a readingtracking module.
The reading tracking module deter-mines the current reading location by aligning each par-tial hypothesis with the book text using a DynamicProgramming search.
In order to allow for skipping ofwords or even skipping to a different place within thetext, the search finds words that when strung togetherminimize a weighted cost function of adjacent word-proximity and distance from the reader's last activereading location.
The Dynamic Programming searchadditionally incorporates constraints to account forboundary effects at the ends of each partial phrase.3 Evaluation MethodologyThere are many different ways in which speech recogni-tion can be used to serve children.
In computer-basedliteracy tutors, speech recognition can be used to meas-ure children's ability to read fluently and pronouncewords while reading out loud, to engage in spoken dia-logues with an animated agent to assess and train com-prehension, or to transcribe spoken summaries of storiesthat can be graded automatically.
Because of the varietyof ways of using speech recognition systems, it is criti-cally important to establish common metrics that areused by the research community so that progress can bemeasured both within and across systems.For this reason, we argue that word error rate calcu-lations using the widely accepted NIST scoring softwareprovides the most widely accepted, easy to use andhighly valid metric.
In this scoring procedure, worderror rate is computed strictly by comparing the speechrecognizer output against a known human transcription(or the text in a book).
Of course, authors are free todefine and report other measures, such as detection/falsealarm curves for useful events such as reading miscues.However, such analyses should always supplement re-ports of word error rates using a single standardizedmeasure.
Adopting this strategy enables fair and bal-anced comparisons within and across systems for anyspeech data given a known word-level transcription.4 Experimental DataFor all experiments in this paper we use speech data andassociated transcriptions from 106 children (grade 3: 17speakers, grade 4: 28 speakers, and grade 5: 61 speak-ers) who were asked to read one of ten stories and toprovide a spoken story summary.
The 16 kHz audiodata contains an average of 1054 words (min 532words; max 1926 words) with an average of 413 uniquewords per story.
The resulting summaries spoken bychildren contain an average of 168 words.5 Improved Read-Aloud RecognitionBaseline: Our baseline read-aloud system utilizes atrigram language model constructed from a normalizedversion of the story text.
Text normalization consistsprimarily of punctuation removal and determination ofsentence-like units.
For example,It was the first day of summer vacation.
Sue and Billy wereeating breakfast.
?What can we do today??
Billy asked.is normalized as:<s> IT WAS THE FIRST DAY OF SUMMERVACATION</s><s> SUE AND BILLY WERE EATING BREAKFAST</s><s> WHAT CAN WE DO TODAY </s><s> BILLY ASKED </s>The resulting text is used to estimate a back-off trigramlanguage model.
We stress that only the story text isused to construct the language model.
Details on thestory texts are provided in Hagen et al (2003).
Note thatthe sentence markers (<s> and </s>) are used to repre-sent positions of expected speaker pause.
This baselinesystem is shown in Table 1(A) to produce a 17.4% worderror rate.Improved Sentence Context Modeling: It is impor-tant in the context of this research to note that childrendo not pause between each estimated sentence bound-ary.
Instead, many children read fluently across phrasesand sentences, where more experienced readers wouldpause.
For this reason, we improved upon our baselinesystem by estimating language model parameters usinga combined text material that is generated both with andwithout the contextual sentence markers (<s> and </s>).Results of this modification are shown in Table 1(B)and show a reduction in error from 17.4% to 13.5%.Improved Word History Modeling:  Most speechrecognition systems operate on the utterance as a pri-mary unit of recognition.
Word history informationtypically is not maintained across segmented utterances.However, in our text example, the words ?do today?should provide useful information to the recognizer that?Billy asked?
may follow.
We therefore modify therecognizer to incorporate knowledge of previous utter-ance word history.
During token-passing search, theinitial word-history tokens are modified to account forthe fact that the incoming sentence may be either thebeginning of a new sentence or a direct extension of theprevious utterance?s word-end history.
Incorporatingthis constraint lowers the word error rate from 13.5% to12.7% as shown in Table 1(C).Dynamic n-gram Language Modeling:  During storyreading we can anticipate words that are likely to bespoken next based upon the words in the text that arecurrently being read aloud.
To account for this knowl-edge, we estimate a series of position-sensitive n-gramlanguage models by partitioning the story into overlap-ping regions containing at most 150 words (i.e., eachregion is centered on 50 words of text with 50 wordsbefore and 50 words after).
For each partition, we con-struct an n-gram language model by using the entirenormalized story text in addition to a 10x weighting oftext within the partition.
Each position-sensitive lan-guage model therefore contains the entire story vocabu-lary.
We also compute a general language modelestimated solely from the entire story text (similar toTable 1(C)).
At run-time, the recognizer implements aword-history buffer containing the most recent 15 rec-ognized words.
After decoding each utterance, theprobability of the text within the word history buffer iscomputed using each of the position-sensitive languagemodels.
The language model with the highest probabil-ity is selected for the first-pass decoding of the subse-quent utterance.
This modification decreases the worderror rate from 12.7% to 10.7% (Table 1(D)).Vocal Tract Normalization and Acoustic Adaptation:We further extend on our baseline system by incorporat-ing the Vocal Tract Length Normalization (VTLN)method described in Welling et al (1999).
Based onresults shown in Table 1(E), we see that VTLN providesonly a marginal gain (0.1% absolute).
Our final set ofacoustic models for the read aloud task are both VTLNnormalized and estimated using Speaker AdaptiveTraining (SAT).
The SAT models are determined byestimating a single linear feature space transform foreach training speaker (Gales, 1997).
The means andvariances of the VTLN/SAT models are then iterativelyadapted using the SMAPLR algorithm (Siohan, 2002) toyield a final recognition error rate of 8.0% absolute (Ta-ble 1(G)).
By combining all of these techniques, weachieved a 54% reduction in word error rate relative tothe baseline system.Word Error Rate (%) Experimental Configuration MFCC PMVDR(A) Baseline: single n-gramlanguage model 17.7% 17.4%(B) (A) + Begin/End SentenceContext Modeling 14.0% 13.5%(C) (B) + between utteranceword history modeling 13.0% 12.7%(D) (C) + dynamicn-gram language model 11.0% 10.7%(E) (D) + VTLN 10.9% 10.6%(F) (E) + VTLN/SAT +SMAPLR (iteration 1) 8.2% 8.2%(G) (E) + VTLN/SAT +SMAPLR (iteration 2) 8.0% 8.0%Table 1: Recognition of children?s read out-loud data.6 Improved Story Summary RecognitionOne of the unique and powerful features of our interac-tive books is the notion of assessing and training com-prehension by providing feedback to the student about atyped summary of text that the student has just read(Cole et al, 2003).
Verbal input is especially importantfor younger children who often can not type well.
Util-izing summaries from the children?s speech corpus,Hagen et al (2003) showed that an error rate of 42.6%could be achieved.
The previous work, however, didnot consider utilizing the read story material to provideimproved initial acoustic models for the summarizationtask.
In Table 2 we demonstrate several findings usinga language model trained on story text and examplesummaries produced by children (leaving out data fromthe child under test).
Without any adaptation the errorrate is 47.1%.
However, utilizing adapted models fromthe read stories (see Table 1(G)) provides an initial per-formance gain of nearly 10% absolute.
Further useSMAPLR adaptation reduces the error rate to 36.1%.Word Error Rate (%) Experimental Configuration MFCC PMVDR(A) Baseline / no adaptation 47.0% 47.1%(B) Read-aloud adapted models(VTLN/SAT) 37.2% 38.0%(C) (B) + SMAPLRadaptation iteration #1 36.0% 36.6%(D) (C) + SMAPLRadaptation iteration #2 35.1% 36.1%Table 2:  Recognition of spontaneous story summaries7 Practical Real-Time ImplementationThe research systems described in Sect.
5 and 6 do notoperate in real-time since multiple adaptation passesover the data are required.
To address this issue, wehave implemented a real-time system that operates onsmall pipelined audio segments (250ms on average).When evaluated on the read-aloud task (Sect.
5), theinitial baseline system achieves an error rate of 19.5%.This system has a real-time factor of 0.56 on a 2.4 GHzIntel Pentium 4 PC with 512MB of RAM.
When inte-grated, the proposed methods show the error rate can bereduced from 19.5% to 12.7% (compare with 10.7%error research system in Table 1(D)).
The revised sys-tem which incorporates dynamic language modelingoperates 35% faster than the single language modelmethod while also reducing the variance in real-timefactor for each processed chunk of audio.
Further gainsare possible by incorporating adaptation in an incre-mental manner.
For example, in Table 3(C) a real-timesystem that incorporates incremental unsupervisedmaximum likelihood linear regression (MLLR) adapta-tion of the Gaussian means is shown.
This final real-time system simultaneously adapts both language andacoustic model parameters during system use.
The sys-tem is now being refined for deployment in classroomswithin the CLT project.
We were able to further im-prove the system after the submission deadline.
Thecurrent WER on the story read aloud task improved to7.6%; while a WER of 32.2% was achieved on thesummary recognition task.
The improvements are due tothe inclusion of a breath model and the additional use ofaudio data from 103 second graders for more accurateacoustic modeling.PMVDR Front-End System Description WER (%) RTF(A) Baseline: single LM 19.5% 0.56  (  2=0.11)(B) Proposed System 12.7% 0.36 (  2=0.06)(C) (B) + IncrementalMLLR adaptation 11.5%0.80(  2=0.33)Table 3:  Evaluation of real-time read out-loud system.ReferencesV.
Zue, S. Seneff, J. Polifroni, H. Meng, J.
Glass(1996).
?Multilingual Human-Computer Interactions:From Information Acess to Language Learning,?ICSLP-96, Philadelphia, PAJ.
Mostow, S. Roth, A. G. Hauptmann, and M. Kane(1994).
"A Prototype Reading Coach that Listens",AAAI-94, Seattle, WA, pp.
785-792.Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee (2003).
?Training a Confidence Measure for a Reading Tutorthat Listens?.
Eurospeech, Geneva, Switzerland,3161-3164.J.
Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin(2002).
?Predicting oral reading miscues?
ICSLP-02,Denver, Colorado.J.
Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo,S.
Eitelman, C. Huang, B. Junker, M. B. Sklar, andB.
Tobin (2003).
?Evaluation of an automated Read-ing Tutor that listens:  Comparison to human tutoringand classroom instruction?.
Journal of EducationalComputing Research, 29(1), 61-117R.
Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma,J.
Movellan, S. Schwartz, D. Wade-Stein, W. Ward,J.
Yan (2003).
?Perceptive Animated Interfaces: FirstSteps Toward a New Paradigm for Human ComputerInteraction,?
Proceedings of the IEEE, Vol.
91, No.9, pp.
1391-1405A.
Hagen, B. Pellom, and R. Cole (2003).
"Children?sSpeech Recognition with Application to InteractiveBooks and Tutors", ASRU-2003, St. Thomas, USAB.
Pellom (2001).
"SONIC: The University of ColoradoContinuous Speech Recognizer", Technical ReportTR-CSLR-2001-01, University of Colorado.B.
Pellom and K. Hacioglu (2003).
"Recent Improve-ments in the CU Sonic ASR System for NoisySpeech: The SPINE Task", ICASSP-2003, HongKong, China.U.
Yapanel, J. H.L.
Hansen (2003).
"A New Perspectiveon Feature Extraction for Robust In-vehicle SpeechRecognition" Eurospeech, Geneva, Switzerland.K.
Shobaki, J.-P. Hosom, and R. Cole (2000).
"The OGIKids' Speech Corpus and Recognizers", Proc.ICSLP-2000, Beijing, China.L.
Welling, S. Kanthak, and H. Ney.
(1999) "ImprovedMethods for Vocal Tract Length Normalization",ICASSP, Phoenix, Arizona.M.
Gales (1997).
Maximum Likelihood Linear Trans-formations for HMM-Based Speech Recognition",Tech.
Report, CUED/F-INFENG/TR291, CambridgeUniversity.O.
Siohan, T. Myrvoll, and C.-H. Lee (2002) "StructuralMaximum a Posteriori Linear Regression for FastHMM Adaptation", Computer, Speech and Lan-guage, 16, pp.
5-24.
