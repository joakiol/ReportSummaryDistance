Coling 2010: Demonstration Volume, pages 33?36,Beijing, August 2010A High-Performance Syntactic and Semantic Dependency ParserAnders Bjo?rkelund?
Bernd Bohnet?
Love Hafdell?
Pierre Nugues?
?Department of Computer science ?Institute for Natural Language ProcessingLund University University of Stuttgartanders.bjorkelund@cs.lth.se bohnet@ims.uni-stuttgart.delove.hafdell@cs.lth.sepierre.nugues@cs.lth.seAbstractThis demonstration presents a high-performance syntactic and semantic de-pendency parser.
The system consists of apipeline of modules that carry out the to-kenization, lemmatization, part-of-speechtagging, dependency parsing, and seman-tic role labeling of a sentence.
The sys-tem?s two main components draw on im-proved versions of a state-of-the-art de-pendency parser (Bohnet, 2009) and se-mantic role labeler (Bjo?rkelund et al,2009) developed independently by the au-thors.The system takes a sentence as input andproduces a syntactic and semantic anno-tation using the CoNLL 2009 format.
Theprocessing time needed for a sentence typ-ically ranges from 10 to 1000 millisec-onds.
The predicate?argument structuresin the final output are visualized in theform of segments, which are more intu-itive for a user.1 Motivation and OverviewSemantic analyzers consist of processingpipelines to tokenize, lemmatize, tag, and parsesentences, where all the steps are crucial to theiroverall performance.
In practice, however, whilecode of dependency parsers and semantic rolelabelers is available, few systems can be run asstandalone applications and even fewer with aprocessing time per sentence that would allow a?Authors are listed in alphabetical order.user interaction, i.e.
a system response rangingfrom 100 to 1000 milliseconds.This demonstration is a practical semanticparser that takes an English sentence as inputand produces syntactic and semantic dependencygraphs using the CoNLL 2009 format.
It buildson lemmatization and POS tagging preprocessingsteps, as well as on two systems, one dealing withsyntax and the other with semantic dependenciesthat reported respectively state-of-the-art resultsin the CoNLL 2009 shared task (Bohnet, 2009;Bjo?rkelund et al, 2009).
The complete system ar-chitecture is shown in Fig.
1.The dependency parser is based on Carreras?salgorithm (Carreras, 2007) and second order span-ning trees.
The parser is trained with the margininfused relaxed algorithm (MIRA) (McDonald etal., 2005) and combined with a hash kernel (Shi etal., 2009).
In combination with the system?s lem-matizer and POS tagger, this parser achieves anaverage labeled attachment score (LAS) of 89.88when trained and tested on the English corpusof the CoNLL 2009 shared task (Surdeanu et al,2008).The semantic role labeler (SRL) consists of apipeline of independent, local classifiers that iden-tify the predicates, their senses, the arguments ofthe predicates, and the argument labels.
The SRLmodule achieves an average labeled semantic F1of 80.90 when trained and tested on the Englishcorpus of CoNLL 2009 and combined with thesystem?s preprocessing steps and parser.2 The DemonstrationThe demonstration runs as a web application andis available from a server located at http://33		Figure 1: The overall system architecture.barbar.cs.lth.se:8081/.
Figure 2 showsthe input window, where the user can write orpaste a sentence, here Speculators are calling fora degree of liquidity that is not there in the market.Figure 3 shows the system output.
It visual-izes the end results as a list of predicates and theirrespective arguments in the form of colored seg-ments.
It also details the analysis as tabulateddata using the CoNLL 2009 format (Surdeanu etal., 2008; Hajic?
et al, 2009), where the columnscontain for each word, its form, lemma, POS tag,syntactic head, grammatical function, whether itis a predicate, and, if yes, the predicate sense.Then, columns are appended vertically to the tableto identify the arguments of each predicate (onecolumn per predicate).
Figure 3 shows that thesentence contains two predicates, call.03 and de-gree.01 and the two last columns of the table showtheir respective arguments.
Clicking on a predi-cate in the first column shows the description ofits arguments in the PropBank or NomBank dic-tionaries.
For call.03, this will open a new win-dow that will show that Arg0 is the demander,Arg1, the thing being demanded, and Arg2, thedemandee.3 Preprocessing StepsThe preprocessing steps consist of the tokeniza-tion, lemmatization, and part-of-speech taggingof the input sentence.
We use first OpenNLP1to tokenize the sentence.
Then, the lemmatizeridentifies the lemmas for each token and the tag-ger assigns the part-of-speech tags.
The lemma-tizer and the tagger use a rich feature set thatwas optimized for all languages of the CoNLL2009 shared task (Hajic?
et al, 2009).
Our lemma-tizer uses the shortest edit script (SES) betweenthe lemmas and the forms and we select a scriptwithin an SES list using a MIRA classifier (Chru-1http://opennlp.sourceforge.net/Figure 2: The input window, where the user en-tered the sentence Speculators are calling for adegree of liquidity that is not there in the market.Clicking on the Parse button starts the parser.pala, 2006).
The English lemmatizer has an ac-curacy of 99.46.
This is 0.27 percentage pointlower than the predicted lemmas of the Englishcorpus in CoNLL 2009, which had an accuracyof 99.73.
The German lemmatizer has an accu-racy of 98.28.
The accuracy of the predicted lem-mas in the German corpus was 68.48.
The valueis different because some closed-class words areannotated differently (Burchardt et al, 2006).
Wealso employed MIRA to train the POS classifiers.Compared to the predicted POS tags in the sharedtask, we could increase the accuracy by 0.15 from97.48 to 97.63 for English and by 1.55 from 95.68to 97.23 for German.4 Dependency ParsingThe dependency parser of this demonstration isa further development of Carreras (2007) and Jo-hansson and Nugues (2008).
We adapted it to ac-count for the multilingual corpus of the CoNLL2009 shared task ?
seven languages ?
and to im-prove the speed of the computationally expensivehigher order decoder (Bohnet, 2009).
The parser34Figure 3: The output window.
The predicates and their arguments are shown in the upper part of thefigure, respectively call.03 with A0 and A1 and degree.01 with A1, while the results in the CoNLL 2008format are shown in the lower part.reached the best accuracies in CoNLL 2009 forEnglish and German, and was ranked second inaverage over all the languages in the task.The parser in this demonstration is an enhance-ment of the CoNLL 2009 version with a hashkernel, a parallel parsing algorithm, and a paral-lel feature extraction to improve the accuracy andparsing speed.
The hash kernel enables the parserto reach a higher accuracy.
The introduction ofthis kernel entails a modification of MIRA, whichis simple to carry out: We replaced the feature-index mapping that mapped the features to indicesof the weight vector by a random function.
Usu-ally, the feature-index mapping in a support vectormachine has two tasks: It maps the features to anindex in the weight vector and filters out the fea-tures not collected in the first step.
The parser isabout 12 times faster than a baseline parser with-out hash kernel and without parallel algorithms.The parsing time is about 0.077 seconds per sen-tence in average for the English test set.5 Semantic Role Labeling PipelineThe pipeline of classifiers used in the seman-tic role labeling consists of four steps: predi-cate identification, predicate disambiguation, ar-gument identification, and argument classifica-tion, see Fig.
1.
In each step, we used differentclassifiers for the nouns and the verbs.
We buildall the classifiers using the L2-regularized linearlogistic regression from the LIBLINEAR package(Fan et al, 2008).
To speed up processing, we dis-abled the reranker used in the CoNLL 2009 sys-tem (Bjo?rkelund et al, 2009).Predicate Identification is carried out using abinary classifier that determines whether anoun or verb is a predicate or not.Predicate Disambiguation is carried out for allthe predicates that had multiple senses in thetraining corpus.
We trained one classifier perlemma.
For lemmas that could be both a verbor a noun (e.g.
plan), we trained one classi-fier per part of speech.
We considered lem-35mas with a unique observed sense as unam-biguous.Argument Identification and Classification.Similarly to the two previous steps, a binaryclassifier first identifies the arguments andthen a multiclass classifier assigns them alabel.
In both steps, we used separate modelsfor the nouns and the verbs.Features.
For the predicate identification, weused the features suggested by Johansson andNugues (2008).
For the other modules ofthe pipeline, we used the features outlinedin Bjo?rkelund et al (2009).
The feature setswere originally selected using a greedy for-ward procedure.
We first built a set of sin-gle features and, to improve the separabilityof our linear classifiers, we paired features tobuild bigrams.6 Results and DiscussionThe demonstration system implements a completesemantic analysis pipeline for English, where wecombined two top-ranked systems for syntacticand semantic dependency parsing of the CoNLL2009 shared task.
We trained the classifiers on thesame data sets and we obtained a final semanticF1 score of 80.90 for the full system.
This scoreis lower than the best scores reported in CoNLL2009.
It is not comparable, however, as the pred-icates had then been manually marked up.
Oursystem includes a predicate identification stageto carry out a fully automatic analysis.
This ex-plains a part of the performance drop.
To pro-vide comparable figures, we replaced the predi-cate identification classifier with an oracle read-ing the gold standard.
We reached then a scoreof 85.58.
To reach a higher speed and providean instantaneous response to the user (less than1 sec.
), we also removed the global reranker fromthe pipeline which accounts for an additional lossof about 1.2 percentage point.
This would put theupper-bound semantic F1 value to about 86.80,which would match the CoNLL 2009 top figures.Acknowledgments.
The research leading tothese results has received funding from the Eu-ropean community?s seventh framework programFP7/2007-2013, challenge 2, cognitive systems,interaction, robotics, under grant agreement No230902?ROSETTA.ReferencesBjo?rkelund, Anders, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of CoNLL-2009.Bohnet, Bernd.
2009.
Efficient parsing of syntacticand semantic dependency structures.
In Proceed-ings of CoNLL-09.Burchardt, Aljoscha, Katrin Erk, Anette Frank, An-drea Kowalski, Sebastian Pado?, and Manfred Pinkal.2006.
The SALSA corpus: a German corpus re-source for lexical semantics.
In Proceedings of the5th LREC-2006.Carreras, Xavier.
2007.
Experiments with a higher-order projective dependency parser.
In Proceedingsof CoNLL-2007.Chrupala, Grzegorz.
2006.
Simple data-drivencontext-sensitive lemmatization.
In Proceedings ofSEPLN.Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofCoNLL-2009.Johansson, Richard and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In ProceedingsCoNLL-2008.McDonald, Ryan, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL-2005.Shi, Qinfeng, JamesPetterson, Gideon Dror, JohnLangford, Alex Smola, and S.V.N.
Vishwanathan.2009.
Hash kernels for structured data.
Journal ofMachine Learning, 15(1):143?172.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL?2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In Proceedingsof CoNLL?2008.36
