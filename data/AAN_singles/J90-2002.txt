A STATISTICAL APPROACH TO MACHINE TRANSLATIONPeter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek,John D. Lafferty, Robert L. Mercer, and Paul S. RoossinIBMThomas J. Watson Research CenterYorktown Heights, NYIn this paper, we present a statistical approach to machine translation.
We describe the application of ourapproach to translation from French to English and give preliminary results.1 INTRODUCTIONThe field of machine translation is almost as old as themodern digital computer.
In 1949 Warren Weaver sug-gested that the problem be attacked with statistical meth-ods and ideas from information theory, an area which he,Claude Shannon, and others were developing at the time(Weaver 1949).
Although researchers quickly abandonedthis approach, advancing numerous theoretical objections,we believe that the true obstacles lay in the relative impo-tence of the available computers and the dearth of machine-readable text from which to gather the statistics vital tosuch an attack.
Today, computers are five orders of magni-tude faster than they were in 1950 and have hundreds ofmillions of bytes of storage.
Large, machine-readable cor-pora are readily available.
Statistical methods have proventheir value in automatic speech recognition (Bahl et al1983) and have recently been applied to lexicography(Sinclair 1985) and to natural anguage processing (Baker1979; Ferguson 1980; Garside et al 1987; Sampson 1986;Sharman et al 1988).
We feel that it is time to give them achance in machine translation.The job of a translator is to render in one language themeaning expressed by a passage of text in another lan-guage.
This task is not always straightforward.
For exam-ple, the translation of a word may depend on words quitefar from it.
Some English translators of Proust's sevenvolume work A la Recherche du Temps Perdu have strivento make the first word of the first volume the same as thelast word of the last volume because the French originalbegins and ends with the same word (Bernstein 1988).Thus, in its most highly developed form, translation in-volves a careful study of the original text and may evenencompass a detailed analysis of the author's life andcircumstances.
We, of course, do not hope to reach thesepinnacles of the translator's art.In this paper, we consider only the translation of individ-ual sentences.
Usually, there are many acceptable transla-tions of a particular sentence, the choice among them beinglargely a matter of taste.
We take the view that everysentence in one language is a possible translation of anysentence in the other.
We assign to every pair of sentences(S, T) a probability, Pr(TIS),  to be interpreted as theprobability that a translator will produce T in the targetlanguage when presented with S in the source language.We expect Pr(TIS) to be very small for pairs like (Lematin je me brosse les dents lPresident Lincoln was a goodlawyer) and relatively large for pairs like (Le presidentLincoln btait un bon avocat l President Lincoln was a goodlawyer).
We view the problem of machine translation thenas follows.
Given a sentence T in the target language, weseek the sentence S from which the translator produced T.We know that our chance of error is minimized by choosingthat sentence S that is most probable given T. Thus, wewish to choose S so as to maximize Pr(SI T).
Using Bayes'theorem, we can writePr (S) Pr (T IS)Pr (SI T) = Pr (T)The denominator on the right of this equation does notdepend on S, and so it suffices to choose the S that maxi-mizes the product Pr(S)Pr(TIS).
Call the first factor inthis product he language model probability of S and thesecond factor the translation probability of T given S.Although the interaction of these two factors can be quiteprofound, it may help the reader to think of the translationprobability as suggesting words from the source languagethat might have produced the words that we observe in thetarget sentence and to think of the language model proba-bility as suggesting an order in which to place these sourcewords.Thus, as illustrated in Figure 1, a statistical translationsystem requires a method for computing language modelprobabilities, a method for computing translation probabil-ities, and, finally, a method for searching among possiblesource sentences S for the one that gives the greatest valuefor Pr(S)Pr( TIS).In the remainder of this paper we describe a simpleversion of such a system that we have implemented.
In theComputational Linguistics Volume 16, Number 2, June 1990 79Peter F. Brown et al A Statistical Approach to Machine TranslationSourceLanguageModelS t TranslatiOnModelTP r (S )  x P r (T IS )  = Pr (S ,T )A Source Language Model and a Translation Model furnish a probabilitydistribution over source-target sentence pairs (S,T) .
The joint probabilityPz (S, T) of the pair (S, T) is the product of the probability Pr (S) computedby the language model and the conditional probability Pr (T I S) computedby the translation model.
The parameters of these models are estimatedautomatically f~om a large database ofsource-target s ntence pairs using astatistical igoritlim which optimizes, in an appropriate s nse, the fit betweenthe models and the data.T ~ Decoder= argmaxPr (S  I T ) = argmaxPr  (S ,T)s sA Decoder performs the actual translation.
Given a sentence T in the targetlanguage, the decoder chooses a viable translation by selecting that sentencein the source langnage for which the probability Pr (S \[ T) is maximum.Figure 1 A Statistical Machine TranslationSystem.next section we describe our language model for Pr (S) ,  andin Section 3 we describe our translation model for P r (T \ [S ) .In Section 4 we describe our search procedure.
In Section 5we explain how we estimate the parameters of our modelsfrom a large database of translated text.
In Section 6 wedescribe the results of two experiments we performed usingthese models.
Finally, in Section 7 we conclude with adiscussion of some improvements hat we intend to imple-ment.2 THE LANGUAGE MODELGiven a word string, sis 2 .
.
.
s n, we can, without loss ofgenerality, writePr (&s2 .
.
.
s,)= Pr (Sl) Pr ( s2 \ [s l ) .
.
.
Pr (sn ls l s2 .
.
.
s~-l).Thus, we can recast he language modeling problem as oneof computing the probabil ity of a single word given all ofthe words that precede it in a sentence.
At any point in thesentence, we must know the probabil ity of an object word,s i, given a history, s~s2.
?
?
Si_l.
Because there are so manyhistories, we cannot simply treat each of these probabil it iesas a separate parameter.
One way to reduce the number ofparameters i to place each of the histories into an equiva-lence class in some way and then to allow the probabil ity ofan object word to depend on the history only through theequivalence class into which that history falls.
In an n-grammodel, two histories are equivalent if they agree in theirfinal n -  1 words.
Thus, in a bigram model, two histories areequivalent if they end in the same word and in a tr igrammodel, two histories are equivalent if they end in the sametwo words.Whi le n-gram models are linguistically simpleminded,they have proven quite valuable in speech recognition andhave the redeeming feature that they are easy to make andto use.
We can see the power of a tr igram model byapplying it to something that we call bag translation fromEnglish into English.
In bag translation we take a sentence,cut it up into words, place the words in a bag, and then tryto recover the sentence given the bag.
We use the n-grammodel to rank different arrangements of the words in thebag.
Thus, we treat an arrangement S as better thananother arrangement S '  if P r (S )  is greater than Pr (S ' ) .We tried this scheme on a random sample of sentences.From a collection of 100 sentences, we considered the 38sentences with fewer than 11 words each.
We had torestrict he length of the sentences because the number ofpossible rearrangements grows exponentially with sentencelength.
We used a tr igram language model that had beenconstructed for a speech recognition system.
We were ableto recover 24 (63%) of the sentences exactly.
Sometimes,the sentence that we found to be most probable was not anexact reproduction of the original, but conveyed the samemeaning.
In other cases, of course, the most probablesentence according to our model was just garbage.
I f  wecount as correct all of the sentences that retained themeaning of the original, then 32 (84%) of the 38 werecorrect.
Some examples of the original sentences and thesentertces recovered from the bags are shown in Figure 2.We :have no doubt that if we had been able to handle longersentertces, the results would have been worse and that theprobabil ity of error grows rapidly with sentence length.3 THE TRANSLATION MODELFor simple sentences, it is reasonable to think of the Frenchtranslation of an English sentence as being generated fromthe English sentence word by word.
Thus, in the sentencepair (,lean aime Mar ie  I John loves Mary)  we feel that Johnproduces Jean, loves produces aime, and Mary  producesExact reconstruction (24 of 38)Please give me your response as soon as possible.Please give me your response as soon as possible.Reconstruction preserving meaning (8 of 38)Now let me mention some of the disadv'antages.=~ Let me mention some of the disadvantages now.Garbage reconstruction (6 of 38)In our organization research as two missions.=~ In oar missions research organization has two.Figure 2 Bag Model Examples.80 Computational Linguistics Volume 16, Number 2, June 1990Peter F. Brown et al A Statistical Approach to Machine TranslationMarie.
We say that a word is aligned with the word that itproduces.
Thus John is aligned with Jean in the pair thatwe just discussed.
Of course, not all pairs of sentences areas simple as this example.
In the pair (Jean n'aimepersonne\[John loves nobody), we can again align Johnwith Jean and loves with aime, but now, nobody aligns withboth n' and personne.
Sometimes, words in the Englishsentence of the pair align with nothing in the Frenchsentence, and similarly, occasionally words in the Frenchmember of the pair do not appear to go with any of thewords in the English sentence.
We refer to a picture such asthat shown in Figure 3 as an alignment.
An alignmentindicates the origin in the English sentence of each of thewords in the French sentence.
We call the number ofFrench words that an English word produces in a givenalignment i s fertility in that alignment.If we look at a number of pairs, we find that words nearthe beginning of the English sentence tend to align withwords near the beginning of the French sentence and thatwords near the end of the English sentence tend to alignwith words near the end of the French sentence.
But this isnot always the case.
Sometimes, a French word will appearquite far from the English word that produced it.
We callthis effect distortion.
Distortions will, for example, allowadjectives to precede the nouns that they modify in Englishbut to follow them in French.It is convenient to introduce the following notation foralignments.
We write the French sentence followed by theEnglish sentence and enclose the pair in parentheses.
Weseparate the two by a vertical bar.
Following each of theEnglish words, we give a parenthesized list of the positionsof the words in the French sentence with which it is aligned.If an English word is aligned with no French words, then weomit the list.
Thus (Jean aime MarielJohn(1) loves(2)Mary(3) ) is the simple alignment with which we began thisdiscussion.
In the alignment (Le chien est battu parJean\[John(6) does beat(3,4) the(l) dog(2) ), John producesJean, does produces nothing, beat produces est battu, theproduces Le, dog produces chien, and par is not producedby any of the English words.Rather than describe our translation model formally, wepresent it by working an example.
To compute the probabil-ity of the alignment (Le chien est battu par Jean\[John(6)does beat(3,4) the(l) dog(2)), begin by multiplying theprobability that John has fertility 1 by Pr(Jean\[John).The proposal willLes propositionsSne seront pas raises en applicationFigure 3 Alignment Example.not now be implementedmalntenantThen multiply by the probability that does has fertility 0.Next, multiply by the probability that beat has fertility 2times Pr(estlbeat)Pr(battulbeat), nd so on.
The word paris produced from a special English word which is denotedby (null).
The result isPr(fertility = 1 \[John) x Pr(Jean\[John) xPr(fertility = O ldoes) xPr(fertility --- 2\[beat) x Pr(est\[beat)Pr(battulbeat) xPr(fertility = l\[the) x Pr(Le\[the) ?Pr(fertility = 1 \[dog) x Pr(chien\[dog) xPr(fertility = l \[(nul l))  x Pr(par(null)) .Finally, factor in the distortion probabilities.
Our model fordistortions is, at present, very simple.
We assume that theposition of the target word depends only on the length of thetarget sentence and the position of the source word.
There-fore, a distortion probability has the form Pr(i\[j, 1) where iis a target position, j a source position, and 1 the targetlength.In summary, the parameters ofour translation model area set of fertility probabilities Pr(n\[e) for each English worde and for each fertility n from 0 to some moderate limit, inour case 25; a set of translation probabilities Pr ( f ie ) ,  onefor each element f of the French vocabulary and eachmember eof the English vocabulary; and a set of distortionprobabilities Pr(i\[j, l) for each target position i, sourceposition j ,  and target length l. We limit i, j, and l to therange 1 to 25.4 SEARCHINGIn searching for the sentence S that maximizesPr(S) Pr(T\[S), we face the difficulty that there are simplytoo many sentences to try.
Instead, we must carry out asuboptimal search.
We do so using a variant of the stacksearch that has worked so well in speech recognition (Bahlet al 1983).
In a stack search, we maintain a list of partialalignment hypotheses.
Initially, this list contains only oneentry corresponding to the hypothesis that the target sen-tence arose in some way from a sequence of source wordsthat we do not know.
In the alignment notation introducedearlier, this entry might be (Jean aime Marie I *) where theasterisk is a place holder for an unknown sequence ofsource words.
The search proceeds by iterations, each ofwhich extends ome of the most promising entries on thelist.
An entry is extended by adding one or more additionalwords to its hypothesis.
For example, we might extend theinitial entry above to one or more of the following entries:(Jean aime Marie I John(I)*),(Jean aime Marie\[ *loves (2)*),(Jean aime Marie l *Mary(3)),(Jean airne Marie l Jeans(l)*).The search ends when there is a complete alignment onthe list that is significantly more promising than any of theincomplete alignments.Sometimes, the sentence S' that is found in this wayis not the same as the sentence S that a translator mightComputational Linguistics Volume 16, Number 2, June 1990 81Peter F. Brown et al A Statistical Approach to Machine Translationhave been working on.
When S' itself is not an accept-able translation, then there is clearly a problem.
IfPr(S')Pr(T\[S') is greater than Pr(S)Pr(TIS), then theproblem lies in our modeling of the language or of thetranslation process.
If, however, Pr(S')Pr(T\[ S') is less thanPr(S)Pr(TIS), then our search has failed to find the mostlikely sentence.
We call this latter type of failure a searcherror.
In the case of a search error, we can be sure that oursearch procedure has failed to find the most probablesource sentence, but we cannot be sure that were we tocorrect the search we would also correct the error.
Wemight simply find an even more probable sentence thatnonetheless i incorrect.
Thus, while a search error is aclear indictment of the search procedure, it is not anacquittal of either the language model or the translationmodel.5 PARAMETER ESTIMATIONBoth the language model and the translation model havemany parameters that must be specified.
To estimate theseparameters accurately, we need a large quantity of data.For the parameters of the language model, we need onlyEnglish text, which is available in computer-readable formfrom many sources; but for the parameters of the transla-tion model, we need pairs of sentences that are translationsof one another.By law, the proceedings of the Canadian parliament arekept in both French and English.
As members rise toaddress a question before the house or otherwise xpressthemselves, their remarks are jotted clown in whichever ofthe two languages i  used.
After the meeting adjourns, acollection of translators begins working to produce a com-plete set of the proceedings in both French and English.?
These proceedings are called Hansards, in remembrance ofthe publisher of the proceedings of the British parliamentin the early 1800s.
All of these proceedings are available incomputer-readable form, and we have been able to obtainabout 100 million words of English text and the correspond-ing French text from the Canadian government.
Althoughthe translations are not made sentence by sentence, we havebeen able to extract about three million pairs of sentencesby using a statistical algorithm based on sentence length.Approximately 99% of these pairs are made up of sentencesthat are actually translations of one another.
It is thiscollection of sentence pairs, or more properly various sub-sets of this collection, from which we have estimated theparameters of the language and translation models.In the experiments we describe later, we use a bigramlanguage model.
Thus, we have one parameter for everypair of words in the source language.
We estimate theseparameters from the counts of word pairs in a large sampleof text from the English part of our Hansard data using amethod escribed by Jelinek and Mercer (1980).In Section 3 we discussed alignments of sentence pairs.
Ifwe had a collection of aligned pairs of sentences, then wecould estimate the parameters of the translation model bycounting, just as we do for the language model.
However,we do not have alignments but only the unaligned pairs ofsentences.
This is exactly analogous to the situation inspeech recognition where one has the script of a sentenceand the time waveform corresponding to an utterance of it,but no indication of just what in the time waveform corre-sponds to what in the script.
In speech recognition, thisproblem is attacked with the EM algorithm (Baum 1972;Dempster et al 1977).
We have adapted this algorithm toour problem in translation.
In brief, it works like this: givensome :initial estimate of the parameters, we can computethe probability of any particular alignment.
We can thenre-estimate he parameters by weighing each possible align-ment according to its probability as determined by theinitial guess of the parameters.
Repeated iterations of thisprocess lead to parameters that assign ever greater probabil-ity to the set of sentence pairs that we actually observe.This algorithm leads to a local maximum of the probabilityof the observed pairs as a function of the parameters of themodel.
There may be many such local maxima.
The partic-ular one at which we arrive will, in general, depend on theinitial choice of parameters.6 Two PILOT EXPERIMENTSIn our first experiment, we test our ability to estimateparameters for the translation model.
We chose as ourEnglish vocabulary the 9,000 most common words in theEnglish part of the Hansard data, and as our Frenchvocabulary the 9,000 most common French words.
For thepurposes of this experiment, we replaced all other wordswith either the unknown English word or the unknownFrenc.h word, as appropriate.
We applied the iterativealgorithm discussed above in order to estimate some 81millJion parameters from 40,000 pairs of sentences compris-ing a total of about 800,000 words in each language.
Thealgorithm requires an initial guess of the parameters.
Weassumted that each of the 9,000 French words was equallyprobable as a translation of any of the 9,000 English words;we assumed that each of the fertilities from 0 to 25 wasequally probable for each of the 9,000 English words; andfinally, we assumed that each target position was equallyprobable given each source position and target length.Thus, our initial choices contained very little informationabout either French or English.Fi\[gure 4 shows the translation and fertility probabilitieswe estimated for the English word the.
We see that, accord-ing to the model, the translates most frequently into theFrench articles le and la.
This is not surprising, of course,but we emphasize that it is determined completely automat-ically by the estimation process.
In some sense, this corre-spondence is inherent in the sentence pairs themselves.Figure 5 shows these probabilities for the English word not.As expected, the French word pas appears as a highlyprobable translation.
Also, the fertility probabilities indi-cate that not translates most often into two French words, asituation consistent with the fact that negative Frenchsentences contain the auxiliary word ne in addition to aprimary negative word such as pas or rien.82 Computational Linguistics Volume 16, Number 2, June 1990Peter F. Brown et al A Statistical Approach to Machine TranslationEngl ish:  theF rench  Probab i l i tyle .610la .1781' .083les .023ce .013il .012de .009.007que .007Fer t i l i ty  P robab i l i ty1 .8710 .1242 .004Figure 4 Probabilities for "the.
"For both of these words, we could easily have discoveredthe same information from a dictionary.
In Figure 6, we seethe trained parameters for the English word hear.
As wewould expect, various forms of the French word entendreappear as possible translations, but the most probabletranslation is the French word bravo.
When we look at thefertilities here, we see that the probability is about equallydivided between fertility 0 and fertility 1.
The reason forthis is that the English speaking members of parliamentexpress their approval by shouting Hear, hear/, while theFrench speaking ones say Bravo/The translation model haslearned that usually two hears produce one bravo by havingone of them produce the bravo and the other producenothing.A given pair of sentences has many possible alignments,since each target word can be aligned with any sourceword.
A translation model will assign significant probabil-ity only to some of the possible alignments, and we can gainfurther insight about the model by examining the align-ments that it considers most probable.
We show one suchalignment in Figure 3.
Observe that, quite reasonably, notis aligned with ne and pas, while implemented is alignedwith the phrase mises en application.
We can also see hereEngl ish:  notF rench  Probab i l i typ~ .469ne .460non .024pa, du tout .003faux .003plus .002ce .002que .002jamais .002Fer t i l i ty  P robab i l i ty2 .7580 .1331 .106Figure 5 Probabilities for "not.
"Engl ish:  hearF rench  Probab i l i ty  Fer t i l i ty  P robab i l i tybravo .992 0 .584entendre .005 1 .416entendu .002entends .001Figure 6 Probabilities for "hear.
"a deficiency of the model since intuitively we feel that willand be act in concert o produce seront while the modelaligns will with seront but aligns be with nothing.In our second experiment, we used the statistical ap-proach to translate from French to English.
To have amanageable task, we limited the English vocabulary to the1,000 most frequently used words in the English part of theHansard corpus.
We chose the French vocabulary to be the1,700 most frequently used French words in translations ofsentences that were completely covered by the 1,000-wordEnglish vocabulary.
We estimated the 17 million parame-ters of the translation model from 117,000 pairs of sen-tences that were completely covered by both our Frenchand English vocabularies.
We estimated the parameters ofthe bigram language model from 570,000 sentences fromthe English part of the Hansard data.
These sentencescontain about 12 million words altogether and are notrestricted to sentences completely covered by our vocabu-lary.We used our search procedure to decode 73 new Frenchsentences from elsewhere in the Hansard data.
We as-signed each of the resulting sentences a category accordingto the following criteria.
If the decoded sentence wasexactly the same as the actual Hansard translation, weassigned the sentence to the exact category.
If it conveyedthe same meaning as the Hansard translation but in slightlydifferent words, we assigned it to the alternate category.
Ifthe decoded sentence was a legitimate translation of theFrench sentence but did not convey the same meaning asthe Hansard translation, we assigned it to the differentcategory.
If it made sense as an English sentence but couldnot be interpreted as a translation of the French sentence,we assigned it to the wrong category.
Finally, if the decodedsentence was grammatically deficient, we assigned it to theungrammatical category.
An example from each categoryis shown in Figure 7, and our decoding results are summa-rized in Figure 8.Only 5% of the sentences fell into the exact category.However, we feel that a decoded sentence that is in any ofthe first three categories (exact, alternate, or different)represents a reasonable translation.
By this criterion, thesystem performed successfully 48% of the time.As an alternate measure of the system's performance,one of us corrected each of the sentences in the last threecategories (different, wrong, and ungrammatical) to eitherthe exact or the alternate category.
Counting one stroke forComputational Linguistics Volume 16, Number 2, June 1990 83Peter F. Brown et al A Statistical Approach to Machine TranslationExactHansard:Decoded as:AlternateHansard:Decoded as:DifferentHansard:Decoded as:WrongHansard:Decoded as:UngrammaticalHansard:Decoded as:Ces ammendements sont certainement ~cessaires.These amendments are certainly necessary.These amendments are certainly necessary.C'est pourtant r~s imple.Yet it is very simple.It is still very simple.J'al re~u cette demande n effet.Such a request was made.I have received this request in effect.Permettez que je donne un example ~, la Chambre.Let me give the House one example.Let me give an example in the House.Vous avez besoin de toute l'~de disponible.You need all the help you can get.You need of the whole benefits available.Figure 7 Translation Examples.each letter that must be deleted and one stroke for eachletter that must be inserted, 776 strokes were needed torepair all of the decoded sentences.
This compares with the1,916 strokes required to generate all of the Hansardtranslations from scratch.
Thus, to the extent that transla-tion time can be equated with key strokes, the systemreduces the work by about 60%.7 PLANSThere are many ways in which the simple models describedin this paper can be improved.
We expect some improve-ment from estimating the parameters on more data.
For theexperiments described above, we estimated the parametersof the models from only a small fraction of the data we haveCategory Number of sentences PercentExact 4 5Alternate 18 25Different 13 18Wrong 11 15Ungramatical 27 37Total 73Figure 8 Translation Results.available: for the translation model, we used only about onepercent of our data, and for the language model, only aboutten percent.We Mve serious problems in sentences in which thetranslation of certain source words depends on the transla-tion of other source words.
For example, the translationmodel produces aller from to go by producing aller from goand nothing from to.
Intuitively we feel that to go functionsas a unit to produce aller.
While our model allows manytarget words to come from the same source word, it does notallow several source words to work together to produce asingle target word.
In the future, we hope to address theproblem of identifying groups of words in the source lan-guage that function as a unit in translation.
This may takethe form of a probabilistic division of the source sentenceinto groups of words.At present, we assume in our translation model thatwords, are placed into the target sentence independently ofone another.
Clearly, a more realistic assumption mustaccoun~: for the fact that words form phrases in the targetsentence that are translations of phrases in the sourcesentence and that the target words in these phrases willtend to stay together even if the phrase itself is movedaround.
We are working on a model in which the positionsof the: target words produced by a particular source worddepend on the identity of the source word and on thepositions of the target words produced by the previoussource ,word.We are preparing a trigram language model that wehope will substantially improve the performance of thesystem.
A useful information-theoretic measure of thecomplexity of a language with respect o a model is theperplexity as defined by Bahl et al (1983).
With thebigrana model that we are currently using, the source textfor our 1,000-word translation task has a perplexity ofabout 78.
With the trigram model that we are preparing,the perplexity of the source text is about 9.
In addition toshowing the strength of a trigram model relative to abigram model, this also indicates that the 1,000-word taskis very simple.We: treat words as unanalyzed wholes, recognizing noconnection, for example, between va, vais, and vont, orbetween tall, taller, and tallest.
As a result, we cannotimprove our statistical characterization f va, say, by obser-vation of sentences involving vont.
We are working onmorplhologies for French and English so that we can profitfrom sl;atistical regularities that our current word-basedapproach must overlook.Finally, we treat the sentence as a structureless equenceof words.
Sharman et al discuss a method for deriving aprobabilistic phrase structure grammar automatically froma sample of parsed sentences (1988).
We hope to applytheir method to construct grammars for both French andEnglish and to base future translation models on the gram-matical constructs thus defined.84 Computational Linguistics Volume 16, Number 2, June 1990Peter F. Brown et al A Statistical Approach to Machine TranslationREFERENCESBahl, L. R.; Jelinek, F.; and Mercer, R. L. 1983 A Maximum LikelihoodApproach to Continuous Speech Recognition.
IEEE Transactions onPattern Analysis and Machine Intelligence PAMI-5(2): 179-190.Baker, J. K. 1979 Stochastic Modeling for Automatic Speech Understand-ing.
In: Reddy, R. A.
(ed.)
Speech Recognition.
Academic Press, NewYork, NY.Baum, L. E. 1972 An Inequality and Associated Maximization Techniquein Statistical Estimation of Probabilistic Functions of a Markov Pro-cess.
Inequalities 3:1-8.Bernstein, R. 1988 Howard's Way.
The New York Times Magazine138(47639): pp 40-44, 74, 92.Dempster, A. P.; Laird, N. M.; and Rubin, D. B.
1977 MaximumLikelihood from Incomplete Data via the EM Algorithm.
Journal ofthe Royal Statistical Society 39(B):1-38.Ferguson, J. D. 1980 Hidden Markov Analysis: An Introduction.
In:Ferguson, J. D.
(ed.
), Hidden Markov Models for Speech.
IDA-CRD,Princeton, NJ.Garside, R. G.; Leech, G. N.; and Sampson, G. R. 1987 The Computa-tional Analysis of English: A Corpus-Based Approach.
Longman, NY.Jelinck, F. and Mercer, R. L. 1980 Interpolated Estimation of MarkovSource Parameters from Sparse Data.
In: Proceedings of the Workshopon Pattern Recognition in Practice.
North-Holland, Amsterdam, TheNetherlands.Sampson, G. R. 1986 A Stochastic Approach to Parsing.
Proceedings ofthe I I th International Conference on Computational Linguistics.
151-155.Sharman, R. A.; Jelinck, F.; and Mercer, R. L. 1988 Generating aGrammar for Statistical Training.
In: Proceedings of the IBM Confer-ence on Natural Language Processing, Thornwood, NY.Sinclair, J. M. 1985 Lcxicographic Evidence.
In: Ilson, R.
(ed.)
Dictionar-ies, Lexicography and Language Learning.
Pergamon Press, NewYork, NY.Weaver, W. 1955 Translation (1949).
In: Machine Translation of Lan-guages, MIT  Press, Cambridge, MA.Computational Linguistics Volume 16, Number 2, June 1990 85
