Proceedings of the SIGDIAL 2013 Conference, pages 369?371,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsA Robotic Agent in a Virtual Environment that Performs SituatedIncremental Understanding of Navigational UtterancesTakashi YamauchiSeikei University3-3-1 Kichijoji-KitamachiMusashino, Tokyo, Japandm126222@cc.seikei.ac.jpMikio NakanoHonda Research InstituteJapan Co., Ltd.8-1 Honcho, WakoWako, Saitama, Japannakano@jp.honda-ri.comKotaro FunakoshiHonda Research InstituteJapan Co., Ltd.8-1 Honcho, WakoWako, Saitama, Japanfunakoshi@jp.honda-ri.comAbstractWe demonstrate a robotic agent in a 3Dvirtual environment that understands hu-man navigational instructions.
Such anagent needs to select actions based on notonly instructions but also situations.
Itis also expected to immediately react tothe instructions.
Our agent incrementallyunderstands spoken instructions and im-mediately controls a mobile robot basedon the incremental understanding resultsand situation information such as the lo-cations of obstacles and moving history.
Itcan be used as an experimental system forcollecting human-robot interactions in dy-namically changing situations.1 IntroductionMovable robots are ones that can execute tasksby moving around.
If such robots can understandspoken language navigational instructions, theywill become more useful and will be widely used.However, spoken language instructions are some-times ambiguous in that their meanings differ de-pending on the situations such as robot and obsta-cle locations, so it is not always easy to make themunderstand spoken language instructions.
More-over, when they receive instructions while they aremoving and they understand instructions only af-ter they finish, accurate understanding is not easysince the situation may change during the instruc-tion utterances.Although there have been several pieces ofwork on robots that receive linguistic navigationalinstructions (Marge and Rudnicky, 2010; Tellex etal., 2011), they try to understand instructions be-fore moving and they do not deal with instructionswhen situations dynamically change.We will demonstrate a 3D virtual robotic systemthat understands spoken language navigational in-structions in a situation-dependent way.
It incre-mentally understands instructions so that it can un-derstand them based on the situation at that pointin time when the instructions are made.2 A Mobile Robot in a 3D VirtualEnvironmentWe use a robotic system that works in a virtualenvironment built on top of SIROS (Raux, 2010),which was originally developed for collecting di-alogues between two participants who are engag-ing in an online video game.
As an example, aconvenience store environment was developed anda corpus of interaction was collected (Raux andNakano, 2010).
One of the participants, the oper-ator, controls a (simulated) humanoid robot whoserole is to answer all customer requests.
The otherparticipant plays the role of a remote manager whosees the whole store but can only interact withthe operator through speech.
The operator has therobot view (whose field of view and depth are lim-ited to simulate a robot?s vision) and the managerhas a birds-eye view of the store (Figure 1).
Cus-tomers randomly visit the store and make requestsat various locations.
The manager guides the op-erator towards customers needing attention.
Theoperator then answers the customer?s requests andgets points for each satisfied request.Using the virtual environment described above,we have developed a system that operates the robotaccording to the human manager?s instructions.Currently we deal with only navigational instruc-tions for moving the robot to a customer.Figure 2 depicts the architecture for our system.We use Sphinx-4 (Lamere et al 2003) for speechrecognition.
Its acoustic model is trained on theWall Street Journal Corpus and its trigram lan-guage model was trained on 1,616 sentences in thehuman-human dialogue corpus described above.Its vocabulary size is 275 words.
We use Festival(Black et al 2001) for speech synthesis.369Robot ClerkCustomersFigure 1: The manager?s view of the conveniencestore.Speech Recognitionglobal contextNavigation Expertrobot moving history,robot & obstacle locationsTask Execution Expertrobot location,task info.
task execution commandrobot & obstacle locations,task info.Dialogue and Behavior Controller (HRIME)speech recognition resultManager ViewDisplaySIROS ServerSpeech Synthesisutterance textutterance textFigure 2: System architecture.We use HRIME (HRI Intelligence Platformbased on Multiple Experts) (Nakano et al 2008)for dialogue and behavior control.
In an HRIMEapplication, experts, which are modules dedicatedto specific tasks, are activated at appropriate timesand perform tasks.
The navigation expert is ac-tivated when the system receives a navigationalinstruction.
There are seven semantic categoriesof instructions; they are turn-right, turn-left, go-forward, go-back, repeat-the-previous-action, do-the-opposite-action-of-the-previous-one, and stop.Utterances that do not fall into any of these are ig-nored.
We assume that there are rules that matchlinguistic patterns and those semantic categories.For example, ?right?
corresponds to turn-right,and ?more?
corresponds to repeat-the-previous-action.
The navigation expert sends the SIROSserver navigation commands based on the rec-ognized semantic categories.
Those commandsmove the robot in the same way as a human op-erator operates the robot using the keyboard, andthe results are shown on the display the manageris watching.
When the robot starts moving and itcannot move because of an obstacle, it reports it tothe manager by sending its utterance to the speechsynthesizer.When the robot has approached a customer whois requesting help, the task is automatically per-formed by the task execution expert.The global context in the dialogue and behaviorcontroller stores information on the environmentwhich is obtained from the SIROS server, and itcan be used by the experts.
As in the same way inthe human-human interaction, it holds informationonly on customers and obstacles close to the robotso that restricted robot vision can be simulated.3 Situated Incremental UnderstandingSometimes manager utterances last without pauseslike ?right, right, more right, stop?, and the sit-uation changes during the utterances because therobot and the customers can move.
So our sys-tem employs incremental speech recognition andmoves the robot if a navigational instruction pat-tern is found in the incremental output.
To obtainincremental speech recognition outputs, we em-ployed InproTK (Baumann et al 2010), which isan extension to Sphinx-4.
It enables the systemto receive tentative results every 10ms, which is ahypothesis for the interval from the beginning ofspeech to the point in time.However, since incremental outputs are some-times unstable and the instructions are ambiguousin that the amount of movement is not specified,not only incremental speech recognition outputsbut also obstacle locations and moving history isused to determine the navigation commands.In our system, the robot navigation expert re-ceives incremental recognition results and if itfinds a navigational instruction pattern, it consultsthe situation information in the global context,and issues a navigation command based on sev-eral situation-dependent understanding rules thatare manually written.
Below are examples.?
If there is an obstacle in the direction that therecognized instruction indicates, ignore therecognized instruction.
For example, when?go forward?
is recognized but there is an ob-stacle ahead, it is guessed that the recognitionresult was an error.370Turn left Turn to the left Go straightI?m turning left I?m turning left I?m going forward9.95 11.135.873.41 4.39 4.779.80 12.139.132.43 3.12 4.323.45 7.54 16.159.81Initial position Turn to the left Make the orientation parallel to an obstacle Go forwardLeft turn Going forwardManager?sutteranceRobot?sutteranceRobot?sactionGo straight3.46 12.13 14.0912.93I?m going forwardRobotorientationFigure 3: Interaction example.?
When rotating, adjust the degree of rotationso that the resulting orientation becomes par-allel to obstacles such as a display shelf.
Thisenables the robot to smoothly go down theaisles.Figure 3 shows an example interaction.
In thedemonstration, we will show how the robot movesaccording to the spoken instructions by a humanlooking at the manager display.
We will compareour system with its non-incremental version and aversion that does not use situation-dependent un-derstanding rules to show how incremental situ-ated understanding is effective.4 Future WorkWe are using this system for collecting a corpusof human-robot interaction in dynamically chang-ing situations so that we can analyze how hu-mans make utterances in such situations.
Fu-ture work includes to make the system understandmore complicated utterances such as ?turn a lit-tle bit to the left?.
We are also planning to workon automatically learning the situation-dependentaction selection rules from such a corpus (Vogeland Jurafsky, 2010) to navigate the robot moresmoothly.AcknowledgmentsWe thank Antoine Raux and Shun Sato for theircontribution to building the previous versions ofthis system.
Thanks also go to Timo BaumannOkko Bu?, and David Schlangen for making theirInproTK available.ReferencesTimo Baumann, Okko Bu?, and David Schlangen.2010.
InproTK in Action: Open-Source Softwarefor Building German-Speaking Incremental SpokenDialogue Systems.
In Proc.
of ESSV.Alan Black, Paul Taylor, Richard Caley, Rob Clark,Korin Richmond, Simon King, Volker Strom,and Heiga Zen.
2001.
The Festival SpeechSynthesis System, Version 1.4.2.
Unpublisheddocument available via http://www.
cstr.
ed.
ac.uk/projects/festival.
html.Paul Lamere, Philip Kwok, William Walker, EvandroGouvea, Rita Singh, Bhiksha Raj, and Peter Wolf.2003.
Design of the CMU Sphinx-4 decoder.
InProc.
of Eurospeech-2003.Matthew Marge and Alexander I. Rudnicky.
2010.Comparing spoken language route instructions forrobots across environment representations.
In Proc.of SIGDIAL-10.Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, andHiroshi Tsujino.
2008.
A framework for build-ing conversational agents based on a multi-expertmodel.
In Proc.
of SIGDIAL-08, pages 88?91.Antoine Raux and Mikio Nakano.
2010.
The dynam-ics of action corrections in situated interaction.
InProc.
of SIGDIAL-10, pages 165?174.Antoine Raux.
2010.
SIROS: A framework for human-robot interaction research in virtual worlds.
In Proc.of the AAAI 2010 Fall Symposium on Dialog withRobots.Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew R. Walter, Ashis Gopal Banerjee, SethTeller, and Nicholas Roy.
2011.
Understanding nat-ural language commands for robotic navigation andmobile manipulation.
In Proc.
of AAAI-2011.Adam Vogel and Dan Jurafsky.
2010.
Learning to fol-low navigational directions.
In Proc.
of ACL-2010,pages 806?814.371
