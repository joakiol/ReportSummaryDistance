Proceedings of the ACL Student Research Workshop, pages 9?15,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCrawling microblogging services to gather language-classified URLsWorkflow and case studyAdrien BarbaresiICAR LabENS Lyon & University of Lyon15 parvis Rene?
Descartes, 69007 Lyon, Franceadrien.barbaresi@ens-lyon.frAbstractWe present a way to extract links frommessages published on microbloggingplatforms and we classify them accordingto the language and possible relevance oftheir target in order to build a text cor-pus.
Three platforms are taken into con-sideration: FriendFeed, identi.ca and Red-dit, as they account for a relative diver-sity of user profiles and more importantlyuser languages.
In order to explore them,we introduce a traversal algorithm basedon user pages.
As we target lesser-knownlanguages, we try to focus on non-Englishposts by filtering out English text.
Us-ing mature open-source software from theNLP research field, a spell checker (as-pell) and a language identification sys-tem (langid.py), our case study andour benchmarks give an insight into thelinguistic structure of the considered ser-vices.1 Introduction1.1 The ?Web as Corpus?
paradigmThe state of the art tools of the ?Web as Corpus?framework rely heavily on URLs obtained fromsearch engines.
As a matter of fact, the approachfollowed by the most researchers of this field con-sists in querying search engines (e.g.
by tuples)to gather links that are crawled in order to build acorpus (Baroni et al 2009).This method could be used in free corpus build-ing approach until recently, when it was made im-possible because of increasing limitations on thesearch engines?
APIs, which make the gatheringprocess on a low budget very slow or impossible.All in all, the APIs may be too expensive and/ortoo unstable in time to support large-scale corpusbuilding projects.Moreover, the question whether the methodused so far, i.e.
randomizing keywords, providesa good overview of a language is still open.
Othertechnical difficulties include diverse and partly un-known search biases due, in part, to search en-gine optimization tricks as well as undocumentedPageRank adjustments.
Using diverse sources ofseed URLs could at least ensure that there is not asingle bias, but several ones.The crawling method using these seeds for cor-pus building may then yield better results, e.g.ensure better randomness in a population of webdocuments as described by (Henzinger et al2000).1.2 User-based URL gatheringOur hypothesis is that microblogging services area good alternative to overcome the limitations ofseed URL collections and the biases implied bysearch engine optimization techniques, PageRankand link classification.It is a user-based language approach.
Its obvi-ous limits are the amount of spam and advertise-ment.
Its obvious bias consists in the technology-prone users who are familiar with these platformsand account for numerous short messages whichin turn over-represent their own interests and hob-bies.However, user-related biases also have advan-tages, most notably the fact that documents thatare most likely to be important are being shared,which has benefits when it comes to gather linksin lesser-known languages, below the English-speaking spammer?s radar.1.3 InterestThe main goal is to provide well-documented,feature-rich software and databases relevant forlinguistic studies.
More specifically, we wouldlike to be able to cover languages which are morerarely seen on the Internet, which implies the gath-9ering of higher proportions of URLs leading tolesser-known languages.
We think that social net-works and microblogging services may be of greathelp when it comes to focus on them.In fact, the most engaged social networking na-tions do arguably not use English as a first com-municating language1.
In addition, crawling theseservices gives an opportunity to perform a casestudy of existing tools and platforms.Finally, the method presented here could beused in other contexts : microtext collections, userlists and relations could prove useful for microtextcorpus building, network visualization or socialnetwork sampling purposes (Gjoka et al 2011).2 Data SourcesFriendFeed, identi.ca and Reddit are taken intoconsideration for this study.
These services pro-vide a good overview of the peculiarities of socialnetworks.
At least by the last two of them a crawlappears to be manageable in terms of both API ac-cessibility and corpus size, which is not the caseconcerning Twitter for example.2.1 identi.caidenti.ca is a social microblogging service built onopen source tools and open standards, which is thereason why we chose to crawl it at first.The advantages compared to Twitter include theCreative Commons license of the content, the ab-sence of limitations on the total number of pagesseen (to our knowledge) and the relatively smallamount of messages, which can also be a prob-lem.
A full coverage of the network is theoreti-cally possible, where all the information may bepublicly available.
Thus, all interesting informa-tion is collected and no language filtering is usedconcerning this website.2.2 FriendFeedTo our knowledge, FriendFeed is the most activeof the three microblogging services consideredhere.
It is also the one which seems to have beenstudied the most by the research community.
Theservice works as an aggregator (Gupta et al 2009)that offers a broader spectrum of retrieved infor-mation.
Technically, FriendFeed and identi.ca canoverlap, as the latter is integrated in the former.1http://www.comscore.com/Press Events/Press Releases/2011/12/Social Networking Leads as Top OnlineActivity GloballyBut the size difference between the two platformsmakes this hypothesis unlikely.The API of FriendFeed is somewhat liberal, asno explicit limits are enforced.
Nonetheless, ourtests showed that after a certain number of suc-cessful requests with little or no sleep, the serversstart dropping most of the inbound connections.All in all, the relative tolerance of this websitemakes it a good candidate to gather a lot of textin a short period of time.2.3 RedditReddit is a social bookmarking and a microblog-ging platform, which ranks to the 7th place world-wide in the news category according to Alexa.2The entries are organized into areas of interestcalled ?reddits?
or ?subreddits?.
The users accountfor the linguistic relevance of their channel, themoderation processes are mature, and since thechannels (or subreddits) have to be hand-picked,they ensure a certain stability.There are 16 target languages so far, whichcan be accessed via so-called ?multi-reddit ex-pressions?, i.e.
compilations of subreddits: Croa-tian, Czech, Danish, Finnish, French, German,Hindi, Italian, Norwegian, Polish, Portuguese, Ro-manian, Russian, Spanish, Swedish and Turkish3.Sadly, it is currently not possible to go back intime further than the 500th oldest post due to APIlimitations, which severely restricts the number oflinks one may crawl.3 MethodologyThe following workflow describes how the resultsbelow are obtained:1.
URL harvesting: social network traversal,obvious spam and non-text documents filter-ing, optional spell check of the short messageto see if it could be English text, optionalrecord of user IDs for later crawls.2.
Operations on the URL queue: redirectionchecks, sampling by domain name.3.
Download of the web documents and analy-sis: HTML code stripping, document validitycheck, language identification.2http://www.alexa.com/topsites/category/Top/News3Here is a possible expression to target Norwegian users:http://www.reddit.com/r/norge+oslo+norskenyheter10The only difference between FriendFeed andReddit on one hand and identi.ca on the other handis the spell check performed on the short messagesin order to target non-English ones.
Indeed, allnew messages can be taken into consideration onthe latter, making a selection unnecessary.Links pointing to media documents, which rep-resent a high volume of links shared on microblog-ging services, are excluded from this study, as itsfinal purpose is to be able to build a text corpus.
Asa page is downloaded or a query is executed, linksare filtered on the fly using a series of heuristicsdescribed below, and finally the rest of the links isstored.3.1 TRUC: an algorithm for TRaversal andUser-based CrawlsStarting from a publicly available homepage, thecrawl engine selects users according to their lin-guistic relevance based on a language filter (seebelow), and then retrieves their messages, eventu-ally discovering friends of friends and expandingits scope and the size of the network it traverses.As this is a breadth-first approach its applicabilitydepends greatly on the size of the network.In this study, the goal is to concentrate on non-English speaking messages in the hope of find-ing non-English links.
The main ?timeline?
fos-ters a users discovery approach, which then be-comes user-centered as the spider focuses on a listof users who are expected not to post messages inEnglish and/or spam.
The messages are filtered ateach step to ensure relevant URLs are collected.This implies that a lot of subtrees are pruned, sothat the chances of completing the traversal in-crease.
In fact, experience shows that a relativelysmall fraction of users and URLs is selected.This approach is ?static?, as it does not rely onany long poll requests (which are for instance usedto capture a fraction of Twitter?s messages as theyare made public), it actively fetches the requiredpages.3.2 Check for redirection and samplingFurther work on the URL queue before the lan-guage identification task ensures an even smallerfraction of URLs really goes through the resource-expensive process of fetching and analyzing webdocuments.The first step of preprocessing consists in find-ing those URLs that lead to a redirect, which isdone using a list comprising all the major URLshortening services and adding all intriguinglyshort URLs, i.e.
less than 26 characters in length,which according to our FriendFeed data occurs ata frequency of about 3%.
To deal with shortenedURLs, one can perform HTTP HEAD requests foreach member of the list in order to determine andstore the final URL.The second step is a sampling that reduces boththe size of the list and the probable impact of anoverrepresented domain names in the result set.
Ifseveral URLs contain the same domain name, thegroup is reduced to a randomly chosen URL.Due to the overlaps of domain names and theamount of spam and advertisement on social net-works such an approach is very useful when itcomes to analyze a large list of URLs.3.3 Language identificationMicrotext has characteristics that make it hard for?classical?
NLP approaches like web page lan-guage identification based on URLs (Baykan etal., 2008) to predict with certainty the languagesof the links.
That is why mature NLP tools have tobe used to filter the incoming messages.A similar work on language identification andFriendFeed is described in (Celli, 2009), who usesa dictionary-based approach: the software triesto guess the language of microtext by identifyingvery frequent words.However, the fast-paced evolution of the vocab-ulary used on social networks makes it hard torely only on lists of frequent terms, so that our ap-proach seems more complete.A first dictionary-based filter First, a quick testis used in order to guess whether a microtext is En-glish or not.
Indeed, this operation cuts the amountof microtexts in half and enables to select the usersor the friends which feature the desired response,thus directing the traversal in a more fruitful direc-tion.The library used, enchant4, allows the use ofa variety of spell-checking backends, like aspell,hunspell or ispell, with one or several locales5.Basically, this approach can be used with otherlanguages as well, even if they are not used asdiscriminating factors in this study.
We considerthis option to be a well-balanced solution betweenprocessing speed on one hand and coverage on4http://www.abisource.com/projects/enchant/5All software mentioned here is open-source.11the other.
Spell checking algorithms benefit fromyears of optimization in both areas.This first filter uses a threshold to discriminatebetween short messages, expressed as a percent-age of tokens which do not pass the spell check.The filter also relies on software biases, like Uni-code errors, which make it nearly certain that thegiven input microtext is not English.langid.py A language identification tool isused to classify the web documents and to bench-mark the efficiency of the test mentioned above.langid.py (Lui and Baldwin, 2011; Lui andBaldwin, 2012) is open-source, it incorporatesa pre-trained model and it covers 97 languages,which is ideal to tackle the diversity of the web.Its use as a web service makes it a fast solutionenabling distant or distributed work.The server version of langid.py was used,the texts were downloaded, all the HTML markupwas stripped and the resulting text was discardedif it was less than 1,000 characters long.
Accord-ing to its authors, langid.py could be used di-rectly on microtexts.
However, this feature wasdiscarded because it did not prove as efficient asthe approach used here when it comes to a sub-stantial amounts of short messages.4 ResultsThe surface crawl dealing with the main time-line and one level of depth has been performedon the three platforms6.
In the case of identi.ca,a deep miner was launched to explore the net-work.
FriendFeed proved too large to start such abreadth-first crawler so that other strategies oughtto be used (Gjoka et al 2011), whereas the multi-reddit expressions used did not yield enough users.FriendFeed is the biggest link provider on a reg-ular basis (about 10,000 or 15,000 messages perhour can easily be collected), whereas Reddit isthe weakest, as the total figures show.The total number of English websites may bea relevant indication when it comes to establisha baseline for finding possibly non-English docu-ments.
Accordingly, English accounts for about55 % of the websites7, with the second most-used content-language, German, only representing6Several techniques are used to keep the number of re-quests as low as possible, most notably user profiling accord-ing to the tweeting frequency.
In the case of identi.ca thisresults into approximately 300 page views every hour.7http://w3techs.com/technologies/overview/contentlanguage/allabout 6 % of the web pages.
So, there is a gap be-tween English and the other languages, and thereis also a discrepancy between the number of Inter-net users and the content languages.4.1 FriendFeedTo test whether the first language filter was ef-ficient, a testing sample of URLs and users wascollected randomly.
The first filter was emu-lated by selecting about 8% of messages (basedon a random function) in the spam and media-filtered posts of the public timeline.
Indeed, themessages selected by the algorithm approximatelyamount to this fraction of the total.
At the sametime, the corresponding users were retrieved, ex-actly as described above, and then the user-basedstep was run, keeping one half of the user?s mes-sages, which is also realistic according to real-world data.The datasets compared here were both of anorder of magnitude of at least 105 unique URLsbefore the redirection checks.
At the end of thetoolchain, the randomly selected benchmark setcomprises 7,047 URLs and the regular set 19,573URLs8.
The first was collected in about 30 hoursand the second one in several weeks.
Accordingto the methodology used, this phenomenon maybe explained by the fact that the domain names inthe URLs tend to be mentioned repeatedly.Language URLs %English 4,978 70.6German 491 7.0Japanese 297 4.2Spanish 258 3.7French 247 3.5Table 1: 5 most frequent languages of URLs takenat random on FriendFeedAccording to the language identification system(langid.py), the first language filter beats therandom function by nearly 30 points (see Table2).
The other top languages are accordingly betterrepresented.
Other noteworthy languages are to befound in the top 20, e.g.
Indonesian and Persian(Farsi).8The figures given describe the situation at the end, afterthe sampling by domain name and after the selection of doc-uments based on a minimum length.
The word URL is usedas a shortcut for the web documents they link to.12Language URLs %English 8,031 41.0Russian 2,475 12.6Japanese 1,757 9.0Turkish 1,415 7.2German 1,289 6.6Spanish 954 4.9French 703 3.6Italian 658 3.4Portuguese 357 1.8Arabic 263 1.3Table 2: 10 most frequent languages of spell-check-filtered URLs gathered on FriendFeed4.2 identi.caThe results of the two strategies followed onidenti.ca led to a total of 1,113,783 URLs checkedfor redirection, which were collected in about aweek (the deep crawler reached 37,485 user IDs).A large majority of the 192,327 total URLs ap-parently lead to English texts (64.9 %), since nolanguage filter was used but only a spam filter.Language URLs %English 124,740 64.9German 15,484 8.1Spanish 15,295 8.0French 12,550 6.5Portuguese 5,485 2.9Italian 3,384 1.8Japanese 1,758 0.9Dutch 1,610 0.8Indonesian 1,229 0.6Polish 1,151 0.6Table 3: 10 most frequent languages of URLsgathered on identi.ca4.3 RedditThe figures presented here are the results of a sin-gle crawl of all available languages altogether, butregular crawls are needed to compensate for the500 posts limit.
English accounted for 18.1 % ofthe links found on channel pages (for a total of4,769 URLs) and 55.9 % of the sum of the linksfound on channel and on user pages (for a total of20,173 URLs).The results in Table 5 show that the first filterwas nearly sufficient to discriminate between theLanguage URLs % Comb.
%English 863 18.1 55.9Spanish 798 16.7 9.7German 519 10.9 6.3French 512 10.7 7.2Swedish 306 6.4 2.9Romanian 265 5.6 2.5Portuguese 225 4.7 2.1Finnish 213 4.5 1.6Czech 199 4.2 1.4Norwegian 194 4.1 2.1Table 4: 10 most frequent languages of filteredURLs gathered on Reddit channels and on a com-bination of channels and user pageslinks.
Indeed, the microtexts that were under thethreshold led to a total of 204,170 URLs.
28,605URLs remained at the end of the toolchain and En-glish accounted for 76.7 % of the documents theylinked to.Language URLs % of totalEnglish 21,926 76.7Spanish 1,402 4.9French 1,141 4.0German 997 3.5Swedish 445 1.6Table 5: 5 most frequent languages of links seenon Reddit and rejected by the primary languagefilterThe threshold was set at 90 % of the words forFriendFeed and 33% for Reddit, each time aftera special punctuation strip to avoid the influenceof special uses of punctuation on social networks.Yet, the lower filter achieved better results, whichmay be explained by the moderation system of thesubreddits as well as by the greater regularity inthe posts of this platform.5 DiscussionThree main technical challenges had to be ad-dressed, which resulted in a separate workflow:the shortened URLs are numerous, yet they oughtto be resolved in order to enable the use of heuris-tics based on the nature of the URLs or a propersampling of the URLs themselves.
The con-frontation with the constantly increasing numberof URLs to analyze and the necessarily limited re-13sources make a website sampling by domain nameuseful.
Finally, the diversity of the web documentsput the language recognition tools to a test, so thata few tweaks are necessary to correct the results.The relatively low number of results for Russianmay be explained by weaknesses of langid.pywith deviations of encoding standards.
Indeed, afew tweaks are necessary to correct the biases ofthe software in its pre-trained version, in particularregarding texts falsely considered as being writtenin Chinese, although URL-based heuristics indi-cate that the website is most probably hosted inRussia or in Japan.
A few charset encodings foundin Asian countries are also a source of classifica-tion problems.
The low-confidence responses aswell as a few well-delimited cases were discardedin this study, they account for no more than 2 % ofthe results.
Ideally, a full-fledged comparison withother language identification software may be nec-essary to identify its areas of expertise.A common practice known as cloaking has notbeen addressed so far: a substantial fraction ofweb pages show a different content to crawler en-gines and to browsers.
This Janus-faced behaviortends to alter the language characteristics of theweb page in favor of English results.Regarding topics, a major user bias was not ad-dressed either: among the most frequently sharedlinks on identi.ca for example, many are related totechnology, IT or software and are mostly writtenin English.
The social media analyzed here tendto be dominated by English-speaking users, eithernative speakers or second-language learners.In general, there is room for improvement con-cerning the first filter, the threshold could be testedand adapted to several scenarios.
This may involvelarger datasets for testing purposes and machinelearning techniques relying on feature extraction.The contrasted results on Reddit shed a differentlight on the exploration of user pages: in all like-lihood, users mainly share links in English whenthey are not posting them on a language-relevantchannel.
The results on FriendFeed are better fromthis point of view, which may suggest that Englishis not used equally on all platforms by users whospeak other languages than English.
Nonetheless,the fact that the microblogging services studiedhere are mainly English-speaking seems to be astrong tendency.Last but not least, the adequateness of the webdocuments shared on social networks has yet tobe thoroughly assessed.
From the output of thistoolchain to a full-fledged web corpus, other fine-grained instruments (Scha?fer and Bildhauer, 2012)as well as further decisions processes (Scha?fer etal., 2013) are needed along the way.6 ConclusionWe presented a methodology to gather multilin-gual URLs on three microblogging platforms.
Inorder to do so, we perform traversals of the plat-forms and use already available tools to filter theURLs accordingly and identify their language.We provide open source software to access theAPIs (FriendFeed and Reddit) and the HTML ver-sion of identi.ca, as an authentication is mandatoryfor the API.
The TRUC algorithm is fully imple-mented.
All the operations described in this papercan be reproduced using the same tools, which arepart of repositories currently hosted on the GitHubplatform9.The main goal is achieved, as hundreds if notthousands of URLs for lesser-known languagessuch as Romanian or Indonesian can be gatheredon social networks and microblogging services.When it comes to filter out English posts, a firststep using an English spell checker gives betterresults than the baseline established using micro-texts selected at random.
However, the discrep-ancy between the languages one would expect tofind based on demographic indicators and the re-sults of the study is remarkable.
English websitesstay numerous even when one tries to filter themout.This proof of concept is usable, but a better fil-tering process and longer crawls may be necessaryto unlock the full potential of this approach.
Last,a random-walk crawl using these seeds and a stateof the art text categorization may provide more in-formation on what is really shared on microblog-ging platforms.Future work perspectives include dealing withlive tweets (as Twitter and FriendFeed can bequeried continuously), exploring the depths ofidenti.ca and FriendFeed and making the directoryof language-classified URLs collected during thisstudy publicly available.9https://github.com/adbar/microblog-explorer147 AcknowledgmentsThis work has been partially funded by an inter-nal grant of the FU Berlin (COW project at theGerman Grammar Dept.).
Many thanks to RolandScha?fer and two anonymous reviewers for theiruseful comments.ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky WideWeb: A collection of very large linguistically pro-cessed web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.Eda Baykan, Monika Henzinger, and Ingmar Weber.2008.
Web Page Language Identification Basedon URLs.
Proceedings of the VLDB Endowment,1(1):176?187.Fabio Celli.
2009.
Improving Language identifica-tion performance with FriendFeed data.
Technicalreport, CLIC, University of Trento.Minas Gjoka, Maciej Kurant, Carter T. Butts, andAthina Markopoulou.
2011.
Practical recom-mendations on crawling online social networks.IEEE Journal on Selected Areas in Communica-tions, 29(9):1872?1892.Trinabh Gupta, Sanchit Garg, Niklas Carlsson, AnirbanMahanti, and Martin Arlitt.
2009.
Characterizationof FriendFeed ?
A Web-based Social AggregationService.
In Proceedings of the AAAI ICWSM, vol-ume 9.Monika R. Henzinger, Allan Heydon, Michael Mitzen-macher, and Marc Najork.
2000.
On near-uniformURL sampling.
In Proceedings of the 9th Inter-national World Wide Web conference on ComputerNetworks: The International Journal of Computerand Telecommunications Networking, pages 295?308.
North-Holland Publishing Co.Marco Lui and Timothy Baldwin.
2011.
Cross-domainFeature Selection for Language Identification.
InProceedings of the Fifth International Joint Con-ference on Natural Language Processing (IJCNLP2011), pages 553?561, Chiang Mai, Thailand.Marco Lui and Timothy Baldwin.
2012. langid.py: AnOff-the-shelf Language Identification Tool.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2012),Jeju, Republic of Korea.Roland Scha?fer and Felix Bildhauer.
2012.
Buildinglarge corpora from the web using a new efficienttool chain.
In Proceedings of the Eight InternationalConference on Language Resources and Evaluation(LREC?12), pages 486?493.Roland Scha?fer, Adrien Barbaresi, and Felix Bildhauer.2013.
The Good, the Bad, and the Hazy: DesignDecisions in Web Corpus Construction.
In Proceed-ings of the 8th Web as Corpus Workshop (WAC8).To appear.15
