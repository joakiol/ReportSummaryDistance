Finite-State Chart Constraints for ReducedComplexity Context-Free Parsing PipelinesBrian Roark?Oregon Health & Science UniversityKristy Hollingshead?
?University of MarylandNathan Bodenstab?Oregon Health & Science UniversityWe present methods for reducing the worst-case and typical-case complexity of a context-freeparsing pipeline via hard constraints derived from finite-state pre-processing.
We perform O(n)predictions to determine if each word in the input sentence may begin or end a multi-wordconstituent in chart cells spanning two or more words, or allow single-word constituents inchart cells spanning the word itself.
These pre-processing constraints prune the search spacefor any chart-based parsing algorithm and significantly decrease decoding time.
In many casescell population is reduced to zero, which we term chart cell ?closing.?
We present methods forclosing a sufficient number of chart cells to ensure provably quadratic or even linear worst-casecomplexity of context-free inference.
In addition, we apply high precision constraints to achievelarge typical-case speedups and combine both high precision and worst-case bound constraintsto achieve superior performance on both short and long strings.
These bounds on processingare achieved without reducing the parsing accuracy, and in some cases accuracy improves.We demonstrate that our method generalizes across multiple grammars and is complementaryto other pruning techniques by presenting empirical results for both exact and approximateinference using the exhaustive CKY algorithm, the Charniak parser, and the Berkeley parser.
Wealso report results parsing Chinese, where we achieve the best reported results for an individualmodel on the commonly reported data set.1.
IntroductionAlthough there have been great advances in the statistical modeling of hierarchicalsyntactic structure over the past 15 years, exact inference with suchmodels remains verycostly andmost rich syntactic modeling approaches resort to heavy pruning, pipelining,?
Center for Spoken Language Understanding, Oregon Health & Science University, Beaverton, Oregon,97006 USA.
E-mails: roarkbr@gmail.com, bodenstab@gmail.com.??
Some of the work in this paper was done while Kristy Hollingshead was at OHSU.
She is currently at theUniversity of Maryland Institute for Advanced Computer Studies, College Park, Maryland, 20740 USA.E-mail: hollingk@gmail.com.Submission received: 9 August 2011; revised submission received: 30 November 2011; accepted forpublication: 4 January 2012.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 4or both.
Pipeline systems make use of simpler models with more efficient inference toreduce the search space of the full model.
For example, the well-known Ratnaparkhi(1999) parser used a part-of-speech (POS)-tagger and a finite-state noun phrase (NP)chunker as initial stages of a multi-stageMaximum Entropy parser.
The Charniak (2000)parser uses a simple probalistic context-free grammar (PCFG) to sparsely populate achart for a richer model, and Charniak and Johnson (2005) added a discriminativelytrained reranker to the end of that pipeline.Finite-state pre-processing for context-free parsing is very common as a meansof reducing the amount of search required in the later stage.
As mentioned earlier,the Ratnaparkhi pipeline used a finite-state POS-tagger and a finite-state NP-chunkerto reduce the search space at the parsing stage, and achieved linear observed-timeperformance.
Other recent examples of the utility of finite-state constraints for parsingpipelines include Glaysher and Moldovan (2006), Djordjevic, Curran, and Clark (2007),and Hollingshead and Roark (2007).
Similar hard constraints have been applied fordependency parsing, as will be outlined in Section 2.
Note that by making use of pre-processing constraints, such approaches are no longer performing full exact inference?these are approximate inference methods, as are the methods presented in this article.Using finite-state chunkers early in a syntactic parsing pipeline has shown both anefficiency (Glaysher and Moldovan 2006) and an accuracy (Hollingshead and Roark2007) benefit for parsing systems.
Glaysher and Moldovan (2006) demonstrated anefficiency gain by explicitly disallowing constituents that cross chunk boundaries.Hollingshead and Roark (2007) demonstrated that high-precision constraints onearly stages of the Charniak and Johnson (2005) pipeline (in the form of base phraseconstraints derived either from a chunker or from later stages of an earlier iterationof the same pipeline) achieved significant accuracy improvements, by moving thepipeline search away from unlikely areas of the search space.
All of these approaches(as with Ratnaparkhi earlier) achieve improvements by ruling out parts of the searchspace, and the gain can either be realized in efficiency (same accuracy, less time) and/oraccuracy (same time, greater accuracy).Rather than extracting constraints from taggers or chunkers built for differentpurposes, in this study we have trained prediction models to more directly reduce thenumber of entries stored in cells of a dynamic programming chart during parsing?evento the point of ?closing?
chart cells to all entries.
We demonstrate results using threefinite-state taggers that assign each word position in the sequence with a binary classlabel.
The first tagger decides if the word can begin a constituent of span greater thanone word; the second tagger decides if the word can end a constituent of span greaterthan oneword; and the third tagger decides if a chart cell spanning a single word shouldcontain phrase-level non-terminals, or only POS tags.
Following the prediction of eachword, chart cells spanning multiple words can be completely closed as follows: Given achart cell (b, e) spanning words wb .
.
.we where b < e, we can ?close?
cell (b, e) if the firsttagger decides that wb cannot be the first word of a multi-word constituent (MWC) or ifthe second tagger decides thatwe cannot be the last word in aMWC.
Completely closingsufficient chart cells allows us to impose worst-case complexity bounds on the overallpipeline, a bound that none of the other previously mentioned methods for finite-statepreprocessing can guarantee.To complement closing multi-word constituent chart cells, our third tagger restrictsthe population of span-1 chart cells.
We note that all span-1 chart cells must containat least one POS tag and can therefore never be closed completely.
Instead, our taggerrestricts unary productions with POS tags on their right-hand side that span a singleword.
We term these single word constituents (SWCs).
Disallowing SWCs alters span-1720Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingcell population from potentially containing all non-terminals to just POS non-terminals.In practice, this decreases the number of entries in span-1 chart cells by 70% during ex-haustive parsing, significantly reducing the number of allowable constituents in largerspans (Bodenstab, Hollingshead, and Roark 2011).
Span-1 chart cells are also the mostfrequently queried cells in the Cocke-Younger-Kasami (CKY) algorithm.
The search overpossible midpoints will always include two cells spanning a single word?one as thefirst left child and one as the last right child.
It is therefore beneficial to minimize thenumber of entries in these span-1 cells.The pre-processing framework we have outlined is straightforward to incorporateinto most existing context-free constituent parsers, a task we have already done for sev-eral state-of-the art parsers.
In the following sections we formally define our approachto finite-state chart constraints and analyze the accuracy of each of the three taggersand their impact on parsing efficiency and accuracy when used to prune the searchspace of a constituent parser.
We apply our methods to exhaustive CYK parsing withsimple grammars, as well as to high-accuracy parsing approaches such as the Charniakand Johnson (2005) parsing pipeline and the Berkeley parser (Petrov and Klein 2007a,2007b).
Various methods for applying finite-state chart constraints are investigated,including methods that guarantee quadratic or linear complexity of the context-freeparser.2.
Related WorkHard constraints are ubiquitous within parsing pipelines.
One of the most basic andstandard techniques is the use of a POS-tag dictionary, whereby words are only allowedto be assigned one of a subset of the POS-tag vocabulary, often based on what has beenobserved in the training data.
This will overly constrain polysemous word types thathappen not to have been observedwith one of their possible tags; yet the large efficiencygain of so restricting the tags is typically seen as outweighing the loss in coverage.
POS-tag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi1999) and dependency parsing (McDonald et al 2005).
Richer tag sets can also be usedto further constrain the parser, such as supertags (Bangalore and Joshi 1999), whichcontain information about how the word will syntactically integrate with other wordsin the sequence.
Supertagging has been widely used to make parsing algorithms effi-cient, particularly those making use of context-sensitive grammars (Clark and Curran2004).By applying finite-state chart constraints to constituent parsing, the approachespursued in this article constrain the possible shapes of unlabeled trees, eliminating fromconsideration trees with constituents over specific spans.
There is thus some similaritywith other tagging approaches (e.g., supertagging) that dictate how words combinewith the rest of the sentence via specific syntactic structures.
Supertagging is generallyused to enumerate which sorts of structures are licensed, whereas the constraints inthis article indicate unlabeled tree structures that are proscribed.
Along the same lines,there is a very general similarity with coarse-to-fine search methods, such as thoseused in the Berkeley (Petrov and Klein 2007a) and Charniak (2000) parsers, and moregeneral structured prediction cascades (Weiss, Sapp, and Taskar 2010; Weiss and Taskar2010).
Our approach also uses simpler models that reduce the search space for largerdownstream models.Dependency parsing involves constructing a graph of head/dependent relations,andmanymethods for constraining the space of possible dependency graphs have been721Computational Linguistics Volume 38, Number 4investigated, such as requiring that each word have a single head or that the graph beacyclic.
Nivre (2006) investigated the impact of such constraints on coverage and thenumber of candidate edges in the search space.
Most interestingly, that paper foundthat constraining the degree of non-projectivity that is allowed can greatly reduce thenumber of arcs that must be considered during search, and, as long as some degreeof non-projectivity is allowed, coverage is minimally impacted.
Of course, the totalabsence of projectivity constraints allows for the use of spanning tree algorithms thatcan be quadratic complexity for certain classes of statistical models (McDonald et al2005), so the ultimate utility of such constraints varies depending on the model beingused.Other hard constraints have been applied to dependency parsing, including con-straints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith,and Smith 2006), which is known as vine parsing.
Such vine parsers can be furtherconstrained using taggers to determine the directionality and distance of each word?shead in a way similar to our use of taggers (Sogaard and Kuhn 2009).
More general arcfiltering approaches, using a variety of features (including some inspired by previouslypublished results of methods presented in this article) have been proposed to reducethe number of arcs considered for the dependency graph (Bergsma and Cherry 2010;Cherry and Bergsma 2011), resulting in large parsing speedups.In a context-free constituent parsing pipeline, constraints on the final parse struc-ture can be made in stages preceding the CYK algorithm.
For example, base phrasechunking (Hollingshead and Roark 2007) involves identifying a span as a base phraseof some category, often NP.
A base phrase constituent has no children other thanpre-terminal POS-tags, which all have a single terminal child (i.e., there is no in-ternal structure in the base phrase involving non-POS non-terminals).
This has anumber of implications for the context-free parser.
First, there is no need to buildinternal structure within the identified base phrase constituent.
Second, constituentsthat cross brackets with the base phrase cannot be part of the final tree structure.This second constraint on possible trees can be thought of as a constraint on chartcells, as pointed out in Glaysher and Moldovan (2006): No multi-word constituentcan begin at a word falling within a base-phrase chunk, other than the first wordof that chunk.
Similarly, no multi-word constituent can end at a word falling withina base-phrase chunk, other than the last word of that chunk.
These constraints ruleout many possible structures that the full context-free parser would have otherwiseconsidered.These begin and end constraints can be extracted from the output of the chunker,but the chunker is most often trained to optimize chunking accuracy, not parsingaccuracy (or parsing precision).
Further, these constraints can apply even for wordsthat fall outside of typical chunks.
For example, in English, verbs and prepositions tendto occur before their arguments, hence are often unlikely to end constituents, yet verbsand prepositions are rarely inside a typically defined base phrase.
Instead of imposingparsing constraints from NLP pre-processing steps such as chunking, we propose thatbuilding specific prediction models to constrain the search space within the CYK chartwill more directly optimize efficiency within a parsing pipeline.In this article, we focus on linear complexity finite-state methods for derivingconstraints on the chart.
Recent work has also examined methods for constraining eachof theO(N2) chart cell independently (Bodenstab et al 2011), permitting a finer-grainedpruning (e.g., not just ?open?
or ?closed?
but an actual beam width prediction) and theuse of features beyond the scope of our tagger.
We discuss this and other extensions ofthe current methods in our concluding remarks.722Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing3.
Preliminaries3.1 Dynamic Programming ChartDynamic programming for context-free inference generally makes use of a chart struc-ture, as shown in Figure 1c for the left-binarized gold parse tree in Figure 1b.
Each cellin the chart represents a collection of possible constituents covering a substring, whichis identified by the indices of the first and last words of the substring.
Let w1 .
.
.wn bea string of n words to be parsed.
The cell identified with (b, e) will contain possibleconstituents spanning the substring wb .
.
.we, where the span of a constituent or a chartcell is defined as the number of words it covers, or e?
b+ 1.
As an example, in thisFigure 1Gold parse tree (a), left-binarized representation of the same tree (b), and correspondingdynamic programming chart (c) for the sentence As usual, the real-estate market had overreacted.
(sentence 1094 in WSJ Section 24).
Each cell in the chart spans a unique substring of the inputsentence.
Non-terminals preceded with the symbol ?@?
are created through binarization(see Section 3.3).723Computational Linguistics Volume 38, Number 4article we will occasionally refer to span-1 chart cells, meaning all chart cells covering asingle word.Context-free inference using dynamic programming over a chart structure buildslonger-span constituents by combining smaller span constituents, guided by rules in acontext-free grammar.
A context-free grammar G = (V,T,S?,P) consists of: a set of non-terminal symbols V, including a special start symbol S?
; a set of terminal symbols T;and a set of rule productions P of the form A ?
?
for A ?
V and ?
?
(V ?
T)?, i.e., asingle non-terminal on the left-hand side of the rule production, and a sequence of 0 ormore terminals or non-terminals on the right-hand side of the rule.
If we have a ruleproduction A ?
B C ?
P, a completed B entry in chart cell (b,m) and a completed Centry in chart cell (m+1, e), we can place a completed A entry in chart cell (b, e).
Sucha chart cell entry is sometimes called an ?edge?
and can be represented by the tuple(A ?
B C, b,m, e).Context-free inference has cubic complexity in the length of the string N, due to theO(N2) number of chart cells and O(N) possible child configurations at each cell.
As anexample, a cell spanning (b, e) must consider all possible configurations of two childconstituents (child cells) that span a proper prefix (b, m) and a proper suffix (m+1, e)where b ?
m < e, leading to O(N) possible midpoints.3.2 The CYK AlgorithmAlgorithm 1 contains pseudocode for the CYK algorithm (Kasami 1965; Younger 1967;Cocke and Schwartz 1970), where the context-free grammar G is assumed to be bina-rized.
The function ?
maps each grammar production in P to a probability.
Lines 1?3Algorithm 1 CYKPseudocode of the CYK algorithm using a binarized PCFG.
Unary processing is sim-plified to allow only chains of length one (excluding lexical unary productions).
Back-pointer storage is omitted.Input:w1 .
.
.wn: Input sentenceG: Left-binarized PCFGOutput:?
: Viterbi-max score for all non-terminals over every spanCYK(w1 .
.
.wn, G = (V,T,S?,P,?
))1: for s = 1 to n do  Span width: bottom-up traversal2: for b = 1 to n?
s+ 1 do  Begin word position3: e ?
b+s?14: for Ai ?
V do5: if s = 1 then  Add lexical productions6: ?i(b, e)?
?
(Ai ?
wb)7: else8: ?i(b, e)?
maxb?m<e(maxj,k?
(Ai ?
Aj Ak) ?j(b,m) ?k(m+ 1, e))9: for Ai ?
V do  Add unary productions10: ?i(b, e)?
max(?i(b, e) , maxj?
(Ai ?
Aj) ?j(b, e))11: ?
(b, e)?
?
(b, e)12: return ?724Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingiterate over all O(N2) chart cells in a bottom?up traversal.
Line 6 initializes span-1 cellswith all possible part-of-speech tags, and line 8 introduces the cubic complexity of thealgorithm: maximization over all midpoints m, which is O(N).
The variable ?
stores theViterbi-max score for each non-terminalAi ?
V at each cell (b, e), representing the span?smost probable derivation rooted in Ai.
Backpointers indicating which argument(s)maximize each ?i(b, e) can be optionally recorded to efficiently extract the maximumlikelihood solution at the end of inference, but we omit these for clarity; they are easilyrecoverable by storing the argmax for each max in lines 8 and 10.We have also included pseudocode in Algorithm 1 to process unary productions inlines 9 and 10.
Unary processing is a necessary step to recover the gold-standard trees ofthe Penn treebanks (Marcus, Marcinkiewicz, and Santorini 1993; Xue et al 2005), but isoften ignored in the presentation of the CYK algorithm.
Because there is little discussionabout unary processing in the literature, implementation details often differ from parserto parser.
In Algorithm 1 we present a simplified version of unary processing that onlyallows unary chains of length 1 per span (excluding lexical productions).
This approachis efficient and can be iterated as needed to represent the length of observed unarychain in the treebank.
Note that line 10 uses the temporary variable ?
to store theaccumulated Viterbi-max scores ?.
This temporary variable is necessary due to theiterative nature in which we update ?.
If we were to write the result of line 10 directly to?i(b, e), then the subsequent maximization of ?i+1(b, e) would use an unstable version of?, some of which would already be updated with unary productions, and some whichwould not.3.3 Incomplete Edges: Chomsky Normal Form and Dotted-RulesA key feature to the efficiency of the CYK algorithm is that all productions in thegrammar G are assumed to have no more than two right-hand-side children.
Ratherthan trying to combine an arbitrary number of smaller substrings (child cells), the CYKalgorithm exploits shared structure between rules and only needs to consider pairwisecombination.
To conform to this requirement, incomplete edges are needed to representthat further combination is required to achieve a complete edge.
This can either beperformed in advance, for example, by transforming a grammar into Chomsky NormalForm resulting in ?incomplete?
non-terminals created by the transform, or incompleteedges can be represented through so-called dotted rules, as with the Earley (1970)algorithm, in which transformation is essentially performed on the fly.
For example,if we have a rule production A ?
B C D ?
P, a completed B entry in chart cell (b,m1)and a completed C entry in chart cell (m1+1,m2), then we can place an incomplete edgeA ?
B C ?D in chart cell (b,m2).
The dot signifies the division between what has alreadybeen combined (left of the dot), and what remains to be combined.
Then, if we have anincomplete edge A ?
B C ?D in chart cell (b,m2) and a complete D in cell (m2+1, e), wecan place a completed A entry in chart cell (b, e).Transforming a grammar into ChomskyNormal Form (CNF) is an off-line operationthat converts rules with more than two children on the right-hand side into multiplebinary rules.
To do this, composite non-terminals are created during the transformation,which represent incomplete constituents (i.e., those edges that require further combina-tion to be made complete).1 For example, if we have a rule production A ?
B C D in1 In this section we assume that edges are extended from left-to-right, which requires a left-binarization ofthe grammar, but everything carries over straightforwardly to the right-binarized case.725Computational Linguistics Volume 38, Number 4the context-free grammar G, then a new composite non-terminal would be created (e.g.,@A:BC) and two binary rules would replace the previous ternary rule: A ?
@A:BC Dand @A:BC ?
B C. The @A:BC non-terminal represents part of a rule expansion thatneeds to be combined with something else to produce a complete non-terminal fromthe original set of non-terminals.2In addition to binarization, one frequent modification to the grammar is to createa Markov model of the dependencies on the right-hand side of the rule.
One way todo this is to reduce the number of children categories annotated on our new compositenon-terminals introduced by binarization.
For example, if instead of @A:BC we assignthe label @A:C to our new non-terminal?with the semantics ?an incomplete A con-stituent with rightmost child C?
?then the two rules that result from binarization are:A ?
@A:C D and @A:C ?
B C. Probabilistically, the result of this is that the childrennon-terminals B and D are conditionally independent of each other given A and C.This approach will provide probability mass to combinations of children of the originalcategory A that may not have been observed together, hence it should be seen as a formof smoothing.
One can go further and remove all children categories from the new non-terminals (i.e., replacing @A:BC with just @A).
This is the binarization pursued in theBerkeley parser, and is shown in Figure 1b.In this article, we explicitly discuss unary productions of type A ?
B where B is anon-terminal, and include these productions in our grammar.
These productions violatethe definition of a CNF grammar, and therefore we will use the term ?binarized gram-mar?
for the remainder of the article to indicate a grammar in CNF with the additionof unary productions.
We will assume that all of our grammars have been previouslybinarized and we define V?
to be the set of non-terminals that are created throughbinarization, and denote edges where A ?
V?
as incomplete edges.
Note that categoriesA ?
V?
are only used in binary productions, not unary productions, a consideration thatwill be used in our constrained parsing algorithm.4.
Finite-State Chart ConstraintsIn this section, we will explicitly define our chart constraints, and present methods forusing the constraints to constrain parsing.
We begin with constraints on beginning orending multi-word constituents, then move to constraining span-1 chart cells.4.1 Constituent Begin and End ConstraintsOur task is to learn which words (in the appropriate context) can begin (B) or end (E)multi-word constituents.
We will treat this as a pre-processing step to parsing and usethese constraints to either completely or partially close chart cells during execution ofthe CYK algorithm.First, let us introduce notation.
Given a set of labeled pairs (S,T) where S is a stringof n words w1 .
.
.wn and T is the target constituent parse tree for S, we say that wordwb ?
B if there is a constituent spanning wb .
.
.we for some e > b and wb ?
B otherwise.Similarly, we say that word we ?
E if there is a constituent spanning wb .
.
.we for someb < e and we ?
E otherwise.
Recovery of these labels will be treated as two separatebinary tagging tasks (B/B and E/E).2 In this example, the symbol ?@?
indicates that the new non-terminal is incomplete, and the symbol ?
:?separates the original parent non-terminal from the children.726Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingFigure 2Begin (a), End (b), Unary (c), and the combination of the three (d) type of constraints for thedynamic programming chart used to parse As usual, the real-estate market had overreactedwitha left-binarized grammar (see Figure 1).
Black denotes ?closed?
cells, white cells are ?open,?and gray cells are only open to a restricted population (gray cells closed by E constraints onlyallow incomplete edges; gray cells closed by U constraints only allow POS tags).Although it may be obvious that we can rule out multi-word constituents withparticular begin and end positions, there may be incomplete structures within the parserthat should not be ruled out by these same constraints.
Hence the notion of ?closing?a chart cell is slightly more complicated than it may initially seem (which accountsfor our use of quotes around the term).
Consider the chart representation in Figure 2with the following constraints, where B is the set of words disallowed from beginning amulti-word constituent and E is the set of words disallowed from ending a multi-wordconstituent3B : {?usual?, ?, ?, ?real-estate?, ?market?, ?overreacted?
}E : {?, ?, ?the?, ?real-estate?, ?had?
}Given our constraints, suppose that wb is in class B and we is in class E, for b < e.We can ?close?
all cells (b,m1) such that m1 > b and all cells (m2, e) such that m2 < e,based on the fact that multi-word constituents cannot begin with word wb and cannotend with we.
In Figure 2 this is depicted by the black and gray diagonals through thechart, ?closing?
those chart cells.3 In this example we list the actual words from the sentence for clarity with Figure 2, but in practice weclassify word positions as a specific word may occur multiple times in the same sentence with potentiallydifferent B or E labels.727Computational Linguistics Volume 38, Number 4If a chart cell (b, e) has been ?closed?
due to begin or end constraints then it is clearthat complete edges should not be permitted in the cell since these represent preciselythe multi-word constituents that are being ruled out.
But what about incomplete edgesthat are introduced through grammar binarization or dotted rule parsing?
To the extentthat an incomplete edge can be extended to a valid complete edge, it must be allowed.There are two cases where this is possible.
If wb ?
B, then under the assumption thatincomplete edges are extended from left-to-right (see footnote 1), the incomplete edgeshould be discarded, because any completed edges that could result from extending thatincomplete edge would have the same begin position.
Stated another way, if wb ?
Bfor chart cell (b, e) then all chart cells (b, i) for i > b must also be closed.
For example,in Figure 2, the cell associated with the two-word substring real-estate market can beclosed to both complete and incomplete edges, since real-estate ?
B, and any completeedge built from entries in that cell would also have to start with the same word andhence would be discarded.
Thus, the whole diagonal is closed.
If, however, wb ?
B andwe ?
E, such as the cell associated with the two-word substring the real-estate in Figure 2,a complete edge?achieved by extending the incomplete edge?may end at wi for i > e,and cell (b, i) may be open (the real-estate market), hence the incomplete edge should beallowed in cell (b, e).In Section 4.4 we discuss limitations on how such incomplete edges arise in closedcells, which has consequences for the worst-case complexity under certain conditions.4.2 Unary ConstraintsIn addition to begin and end constraints, we also introduce unary constraints in span-1cells.
Although we cannot close span-1 cells entirely because each of these cells mustcontain at least one POS tag, we can reduce the population of these cells by restrictingthe type of constituents they contain.We define a single-word constituent (SWC) as anyunary production A ?
B in the grammar such that B is a non-terminal (not a lexiconentry) and the production spans a single word.
The productions ADJP ?
JJ and VP ?VBN in Figure 1a are examples of SWCs.
Note that TOP ?
S and JJ ?usual in Figure 1aare also unary productions, but by definition they are not SWC unary productions.
Wetrain a distinct tagger, as is done for B and E constraints, to label each word position aseither in U or U, indicating that the word position may or may not be extended by aSWC, respectively.Because the search over possible grammar extension from two child cells in theCYK algorithm is analogous to a database JOIN operation, the efficiency of this cross-product hinges on the population of the two child cells that are intersected.
We focus onconstraining the population of span-1 chart cells for three reasons.
First, the begin/endconstituent constraints only affect chart cells spanning more than one word and leavespan-1 chart cells completely unpruned.
By pruning entries in these span-1 cells, wecomplement multi-word constituent pruning so that all chart cells are now candidatesfor finite-state tagging constraints.
Second, span-1 chart cells are the most frequentlyqueried cells in the CYK algorithm.
The search over possible midpoints will alwaysinclude two cells spanning a single word?one as the first left child and one as thelast right child.
It is therefore important that the number of entries in these cells beminimized to make bottom?up CYK processing efficient.
Finally, as we will show inSection 5, only 11.2% of words in the WSJ treebank are labeled with SWC productions.With oracle unary constraints, the possibility of constraining nearly 90% of span-1 chartcells has promising efficiency benefits to downstream processing.728Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingTo reiterate, unary constraints in span-1 chart cells never close the cell completelybecause each span-1 cell must contain at least one POS non-terminal.
Instead, if wi ?
Uthen we simply do not add phrase-level non-terminals to chart cell (i, i).
Similar to chartcells spanning multiple words that cannot be closed completely, these span-1 chart cellspartially restrict the population of the cell, which we will empirically show to reduceprocessing time over unconstrained CYK parsing.
These partially closed chart cells arerepresented as gray in Figure 2.4.3 The Constrained CYK AlgorithmOur discussion of cell closing in Section 4.1 highlighted different conditions for closingcells for complete and incomplete edges.
We will refer to specific conditions in the pseu-docode, which we enumerate here.
The constraints determine three possible conditionsfor cell (b, e) spanning multiple words:1. wb ?
B: cell is closed to all constituents, both complete and incomplete2.
wb ?
B and we ?
E: cell is closed to complete constituents3.
wb ?
B and we ?
E: cell is open to all constituentsand two possible conditions for cell (i, i) spanning a single word:4. wi ?
U: cell is closed to unary phrase-level constituents5.
wi ?
U: cell is open to all constituentsWe will refer to a chart cell (b, e) that matches the above condition as ?case 1 cells,?
?case 2 cells,?
and so on, throughout the remainder of this article.
Figure 2 representsthese five cases pictorially, where case 1 cells are black, case 2 and 4 cells are gray, andcase 3 and 5 cells are white.Algorithm 2 contains pseudocode of our modified CYK algorithm that takes intoaccount B, E, and U constraints.
Line 4 of Algorithm 2 is the first modification fromthe standard CYK processing of Algorithm 1: We now consider only words in set B tobegin multi-word constituents.
Chart cells excluded from this criteria fall into case 1and require no work.
Line 5 determines if chart cell (b, e) is of case 2 (partially open)or case 3 (completely open).
If we ?
E, then we skip to lines 16?17 and only incompleteedges are permitted.
Note that there is only one possible midpoint for case 2 chart cells,which results in constant-time work (see proof in Section 4.4) and unary productionsare not considered because all entries in the cell are incomplete constituents, which onlyparticipate in binary productions.
Otherwise, if we ?
E on Line 5, then the cell is opento all constituents and processing occurs as in the standard CYK algorithm (lines 6?10of Algorithm 2 are identical to lines 4?8 of Algorithm 1).
Finally, unary productions areadded in lines 12?14, but restricted to multi-word spanning chart cells, or span-1 cellswhere wb ?
U.4.4 Proofs to Bound Worst-Case ComplexityAll of the proofs in this section rely on constraints imposed by B and E. Pruningprovided by the U constraints reduces decoding time in practice, but does not provideadditional reductions in complexity.
Our proofs of complexity bounds will rest onthe number of cells that fall in cases 1?3 outlined in the Section 4.3, and the amount729Computational Linguistics Volume 38, Number 4Algorithm 2 CONSTRAINEDCYKPseudocode of a modified CYK algorithm, with constituent begin/end and unary con-straints.
Unary processing is simplified to allow only chains of length one (excludinglexical unary productions).
Backpointer storage is omitted.Input:w1 .
.
.wn: Input sentenceG: Left-binarized PCFGV?
: Set of binarized non-terminals from VB,E,U: Begin, End, and Unary chart constraintsOutput:?
: Viterbi-max scores for all non-terminals over every spanCONSTRAINEDCYK(w1 .
.
.wn, G = (V,T, S?,P,?
),V?,B,E,U)1: for s = 1 to n do  Span width: bottom-up traversal2: for b = 1 to n?s+1 do  Begin word position3: e ?
b+s?14: if wb ?
B or s = 1 then  Case 1 cells excluded5: if we ?
E or s = 1 then6: for Ai ?
V do7: if s = 1 then  Add lexical productions8: ?i(b, b)?
?
(Ai ?
wb)9: else  Case 3: cell open10: ?i(b, e)?
maxb?m<e(maxj,k?
(Ai ?
Aj Ak) ?j(b,m) ?k(m+ 1, e))11: if s > 1 or wb ?
U then  Case 5 for span-1 cells12: for Ai ?
V do  Add unary productions13: ?i(b, e)?
max(?i(b, e) , maxj?
(Ai ?
Aj) ?j(b, e))14: ?
(b, e)?
?
(b, e)15: else  Case 2: closed to complete constituents16: for Ai ?
V?
do  Only consider binarized non-terminals17: ?i(b, e)?
maxj,k?
(Ai ?
Aj Ak) ?j(b, e?
1) ?k(e, e)18: return ?of work in each case.
The amount of work for each case is related to how the CYKand CONSTRAINEDCYK algorithms performs their search.
Each cell (b, e) in the chartspans the substring wb .
.
.we, and building non-terminal categories in that cell involvescombining non-terminal categories (via rules in the context-free grammar) found in cellsspanning adjacent substrings wb .
.
.wm and wm+1 .
.
.we.
The substring wb .
.
.we can beas large as the entire sentence, requiring a search overO(N) possiblemidpointwordswm.This accounts for the linear amount of work in these case 3, open, cells.More formally, we can define an upper bound on the work required in a chart cellas W = |P| ?
|M| where |P| is the number grammar productions bounded above by theconstant size of the grammar, and |M| is the number ofmidpoints considered.We denotethe upper bound on the amount of work done in case 1 cells byW1 and the total numberof case 1 cells considered by C1 (similarly for case 2 and 3).
With this decomposition, thecomplexity of the CONSTRAINEDCYK algorithm can be written as O(|C1|W1 + |C2|W2 +|C3|W3).
Because there is no work for case 1 cells (W1 = 0), these can be ignored.
In fact,we can reduce the complexity even further.730Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingTheorem 1W2 is constant and CONSTRAINEDCYK has complexity O(|C2|+ |C3|W3).ProofWithout loss of generality, we assume a left-binarized grammar.
The upper bound on re-quiredwork for cell (b, e) is defined asW = |P| ?
|M|where |P| is the number of grammarproductions bounded above by the constant size of the grammar, and |M| is the num-ber of midpoints considered.
For some (b, e) where 1 ?
b < e ?
n, assume wb ?
B andwe ?
E, that is, (b, e) ?
C2.
The possible child cells of (b, e) are (b,m) and (m+ 1, e) whereb ?
m < e. Because we assume a left-binarized grammar, the production?s right childmust be a complete non-terminal and (m+ 1, e) ?
C3 must hold.
But we ?
E so (m+ 1, e)cannot be in C3 unless m = e?
1 (span 1 cell).
Therefore, the only valid midpoint ism = e?
1 andW2 = |P|?1 is constant, hence O(|C2|W2 + |C3|W3) = O(|C2|+ |C3|W3).
In summary, we find that case 2 chart cells (gray in Figure 2) are surprisingly cheapto process because the right child cell must also be in case 2, resulting in only onepossible midpoint to consider.
Next, we show that bounding the number of open cellsand applying Theorem 1 leads to quadratically bounded parsing.Theorem 2If |C3| < ?N for some constant ?
then CONSTRAINEDCYK has complexity O(N2).ProofThe total number of cells in the chart for a string of length N is N(N + 1)/2, therefore|C2| < N2.
Further, the number of midpoints for any cell is less than N, hence W3 < N.If we bound |C3| < ?N, then it follows directly from Theorem 1 that CONSTRAINEDCYKhas complexity O(N2 + ?N ?N) = O(N2).
We have just proved in Theorem 2 that we can reduce the complexity of constituentparsing from O(N3) to O(N2) by restricting the number of open cells in the chart viaconstraints B and E. Next, we will show that this bound can be reduced even furtherto O(N) when the number of words in B is held constant.
We call this approach ?linearconstraints.?
There are two things to note when considering these constraints.
First,the proof relies only on the size of set B, leaving E potentially empty and limitingthe efficiency gains on short sentence compared to high-precision and quadraticallybounded constraints.
Second, limiting the size of B to a constant value is a restrictivecriterion.
When applied to a long sentence, this will over-constrain the search space andimpose a branching bias on possible trees.
In Section 7.2 we show that if the branchingbias from linear constraints matches the branching bias of the treebank, then accuracyis not severely affected and linear constraints can provide a useful bound on longsentences that often dominate computational resources.We first prove limits on the total number of open chart cells (case 3) and workrequired in these cells, which directly leads to the proof bounding the complexity ofthe algorithm.Lemma 1If |B| ?
?
for some ?, thenW3 ?
?|P|.731Computational Linguistics Volume 38, Number 4ProofIt has been defined thatW1 is zero and shown in Theorem 1 thatW2 is constant.
For chartcell (b, e) ?
C3 there are e?
b possiblemidpoints to consider.
For somemidpointmwhereb ?
m < e, ifwm ?
B then (m, e) ?
C1 and contains no complete or incomplete constituententries.
Therefore midpoint m is not a valid midpoint for cell (b, e).
Because |B| ?
?,there are at most ?
midpoints to consider and the work required in (b, e) is boundedabove by the constant number of productions in the grammar (|P|) and ?
midpoints,thusW3 ?
?|P|.
Lemma 2If |B| ?
?
for some ?, then |C2|+ |C3| ?
?N.ProofFor a string of length N and some wb ?
B, there are at most N substrings wb .
.
.we toconsider, hence O(N) chart cells for each word wb ?
B.
Because |B| ?
?, then there areat most ?N cells (b, e) ?
C1 and |C2|+ |C3| ?
?N.
Theorem 3If |B| ?
?
for some ?
then CONSTRAINEDCYK has complexity O(N).ProofFrom Theorem 1 we know that CONSTRAINEDCYK has complexity O(|C2|+ |C3|W3).Given that |B| ?
?, it follows from Lemmas 1 and 2 that CONSTRAINEDCYK has com-plexity O(?2N|P|) = O(N).
In this section, we have been discussing worst-case complexity bounds, but there isa strong expectation for large typical-case complexity savings that aren?t explicitlyaccounted for here.
To provide some intuition as to why this might be, consider againthe chart structure in Figure 2d.
The black cells in the chart represent the cells that havebeen closed when wj ?
B (case 1 cells).
Because there is no work required for these cells,the total amount of work required to parse the sentence is reduced.
The quadratic bounddoes not include any potential reduction of work for the remaining open cells, however.Thus the amount of work to parse the sentence will be less than the worst-case quadraticbound because of this reduction in processing.
In Section 7.2 we compute the empiricalcomplexity of each constraint method and compare that with its theoretical bound.5.
Tagging Chart ConstraintsTo better understand the proposed tagging tasks and their likely utility, we will firstlook at the distribution of classes and our ability to automatically assign them correctly.Note that we do not consider the first word w1 and the last word wN during thebegin-constituent and end-constituent prediction tasks because they are unambiguousin terms of whether they begin or end constituents of span greater than one.
The firstwordw1 must begin a constituent spanning the whole string, and the last wordwN mustend that same constituent.
The first word w1 cannot end a constituent of length greaterthan 1; similarly, the last word wN cannot begin a constituent of length greater than 1.We therefore omit B and E at these two word positions from prediction, leading toN ?
2 begin-constituent and N ?
2 end-constituent ambiguous predictions for a stringof length N.732Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingTable 1 displays word statistics from the training sections of the Penn Englishtreebank (Marcus, Marcinkiewicz, and Santorini 1993) and the Penn Chinese treebank(Xue et al 2005), and the positive and negative class label frequency of all words inthe data.From the nearly 1 million words in the English corpus, just over 870,000 are neitherthe first nor the last word in the string, therefore possible members of the sets B or E (i.e.,neither beginning a multi-word constituent (B) nor ending a multi-word constituent(E)).
Of these 870,000 words, over half (50.5%) do not begin multi-word constituents,and nearly three quarters (74.3%) do not end multi-word constituents.
The skewed dis-tribution of E to E reflects the right-branching structure of English.
Finally, almost 90%of words are not labeled with a single-word constituent, demonstrating the infrequencyat which these productions occur in the English treebank.The Chinese treebank is approximately half of the size of the English treebankin terms of the number of sentences and word count.
Again, we see a bias towardsright-branching trees (|B| > |E|), but the skew of the distributions is much smallerthan it is for English.
The most significant difference between the two corpora is thepercentage of single-word constituents in Chinese compared with English.
Due to theannotation guidelines of the Chinese treebank, nearly 40% of words contain single-wordconstituent unary productions.
Because these occur with such high frequency, we canassume that unary constraints may have a smaller impact on efficiency for Chinese thanfor English, because an accurate tagger will constrain fewer cells.
We still have theexpectation, though, that accurate tagging of SWC productions will increase parsingaccuracy for both English and Chinese.To automatically predict the class of each word position, we train a binary predictorfrom supervised data for each language/word-class pair, tuning performance on therespective development sets (WSJ Section 24 for English and Penn Chinese Treebankarticles 301-325 for Chinese).
Word classes are extracted from the treebank trees byobserving constituents beginning or ending at each word position, or by observingsingle word constituents.
We use the tagger from Hollingshead, Fisher, and Roark(2005) to train six linear models (three for English, three for Chinese) with the averagedperceptron algorithm (Collins 2002).Table 2 summarizes the features implemented in our tagger for B, E, and U identifi-cation.
In the table, the?
features are instantiated as POS-tags (provided by a separatelytrained log-linear POS-tagger) and ?
features are instantiated as constituent tags (B,E, and U class labels).
The feature set used in the tagger includes the n-grams ofsurrounding words, the n-grams of surrounding POS-tags, and the constituent tags ofTable 1Statistics on extracted word classes for English (Sections 2?21 of the Penn WSJ treebank) andChinese (articles 1?270 and 400?1151 of the Penn Chinese treebank).Corpus totals Begin class End class Unary classStrings Words B B E E U UEnglishCount 39,832 950,028 430,841 439,558 223,544 646,855 105,973 844,055Percent 49.5 50.5 25.7 74.3 11.2 88.8ChineseCount 18,086 493,708 188,612 269,000 165,591 292,021 196,732 296,976Percent 41.2 58.8 36.2 63.8 39.9 60.1733Computational Linguistics Volume 38, Number 4Table 2Tagger features for B, E, and U.LEX ORTHO POS?i ?i,wi ?i,wi[0] ?i,?i?i?1, ?i ?i,wi?1 ?i,wi[0..1] ?i,?i?1?i?2, ?i ?i,wi+1 ?i,wi[0..2] ?i,?i?1,?i?i?2, ?i?1, ?i ?i,wi?2 ?i,wi[0..3] ?i,?i+1?i,wi+2 ?i,wi[n] ?i,?i,?i+1?i,wi?1,wi ?i,wi[n-1..n] ?i,?i?1,?i,?i+1?i,wi,wi+1 ?i,wi[n-2..n] ?i,?i?2?i,wi[n-3..n] ?i,?i?2,?i?1?i,wi ?
Digit ?i,?i?2,?i?1,?i?i,wi ?
UpperCase ?i,?i+2?i,wi ?
Hyphen ?i,?i+1,?i+2?i,?i,?i+1,?i+2All lexical (LEX), orthographic (ORTHO), and part-of-speech (POS) features are duplicated to also occurwith ?i?1; e.g., {?i?1, ?i,wi} as a LEX feature.the preceding words.
The n-gram features are represented by the words within a three-wordwindow of the current word.
The tag features are represented as unigram, bigram,and trigram tags (i.e., constituent tags from the current and two previous words).
Thesefeatures are based on the feature set implemented by Sha and Pereira (2003) for NPchunking.
Additional orthographical features are used for unknown and rare words(words that occur fewer than five times in the training data), such as the prefixes andsuffixes of the word (up to the first and last four characters of the word), and the pres-ence of a hyphen, a digit, or a capitalized letter, following the features implemented byRatnaparkhi (1999).
Note that the orthographic feature templates, including the prefix(e.g., wi[0..1]) and suffix (e.g., wi[n-2..n]) templates, are only activated for unknown andrare words.
When applying our tagging model to Chinese data, all feature functionswere left in the model as-is, and not tailored to the specifics of the language.We ran various tagging experiments on the development set and report accuracyresults in Table 3 for all three predictions tasks, using Viterbi decoding.
We trainedTable 3Tagging accuracy on the respective development sets (WSJ Section 24 for English and PennChinese Treebank articles 301?325 for Chinese) for binary classes B, E, and U, for variousMarkov orders.Tagging Task Markov order0 1 2EnglishB (no multi-word constituent begin) 96.7 96.9 96.9E (no multi-word constituent end) 97.3 97.3 97.3U (no span-1 unary constituent) 98.3 98.3 98.3ChineseB (no multi-word constituent begin) 94.8 95.4 95.2E (no multi-word constituent end) 96.2 96.4 96.6U (no span-1 unary constituent) 95.9 96.2 96.3734Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingmodels with Markov order-0 (constituent tags predicted for each word independently),order-1 (features with constituent tag pairs), and order-2 (features with constituent tagtriples).
In general, tagging accuracy for English is higher than for Chinese, especiallyfor the U and B tasks.
Given the consistent improvement from Markov order-0 toMarkov order-1 (particularly on the Chinese data), and for the sake of consistency, wehave chosen to perform Markov order-1 prediction for all results in the remainder ofthis article.6.
Experimental Set-upIn the sections that follow, we present empirical trials to examine the behavior of chartconstraints under a variety of conditions.
First, we detail the data, evaluation, andparsers used in these experiments.6.1 Data Sets and EvaluationFor English, all stochastic grammars are induced from the Penn WSJ Treebank (Marcus,Marcinkiewicz, and Santorini 1993).
Sections 2?21 of the treebank are used as training,Section 00 as held-out (for determining stopping criteria during training and someparameter tuning), Section 24 as development, and Section 23 as test set.
For Chinese,we use the Penn Chinese Treebank (Xue et al 2005).
Articles 1?270 and 400?1151 areused for training, articles 301?325 for both held-out and development, and articles 271?300 for testing.
Supervised class labels are extracted from the non-binarized treebanktrees for B, E, and U (as well as their complements).All results report F-measure labeled bracketing accuracy (harmonic mean of labeledprecision and labeled recall) for all sentences in the data set (Black et al 1991), andtiming is reported using an Intel 3.00GHz processor with 6MB of cache and 16GB ofmemory.
Timing results include both the pre-processing time to tag the chart constraintsas well as the subsequent context-free inference, but tagging time is relatively negligibleas it takes less than three seconds to tag the entire development corpus.6.2 Tagging Methods and Closing Chart CellsWe have three separate tagging tasks, each with two possible tags for every word wi inthe input string: (1) B or B; (2) E or E; and (3) U or U.
Our taggers are as described inSection 5.Within a pipeline system that leverages hard constraints, one may want to choosea tagger operating point that favors precision of constraints over recall to avoid over-constraining the downstream parser.
We have two methods for trading recall for preci-sion that will be detailed later in this section, both relying on calculating the cumulativescore Si for each of the binary tags at each word position wi.
That is, (using B as theexample tag):Si(B | w1 .
.
.wn) = log??1...?n?
(?i,B) e?(w1...wn,?1...
?n )?w (1)where?sums over all possible tag sequences for sentence w1 .
.
.wn; ?
(?i,B) = 1 if ?i =B and 0 otherwise; ?
(w1 .
.
.wn, ?1 .
.
.
?n) maps the word string and particular tag stringto a d-dimensional (global) feature vector; andw is the d-dimensional parameter vector735Computational Linguistics Volume 38, Number 4estimated by the averaged perceptron algorithm.
Note that this cumulative score overall tag sequences that have B in position i can be calculated efficiently using the forward?backward algorithm.
We can compare the cumulative scores Si(B) and Si(B) to decidehow to tag word wi, and define the cumulative score ratio (CSR) as follows:CSR(wi) = Si(B)?
Si(B) (2)If we want to tag Bwith high precision, and thus avoid over-constraining the parser, wecan change our decision criterion to produce fewer such tags.
We present two differentselection criteria in the next two subsections.To show the effect of precision-oriented decision criteria, Figure 3 shows the preci-sion/recall tradeoff at various operating points, using the global high-precision methoddetailed herein, for all three tags on both the English and the Chinese developmentsets.
As expected, we see that the English B curve is significantly lower than theEnglish E and U curves.
This is due to the near-uniform prior on B in the data (E andU are much higher-frequency classes).
Still, we can achieve 99% precision for B withrecall above 70%.
We do much better with E and U and see that when precision is 99%for these two tags, recall does not drop below 90%.
For the Chinese tagging task, B, E,and U all have similar performance as we trade precision for recall.
Here, as with theEnglish B tag, we achieve 99% precision with recall still above 70%.We can see from these results that our finite-state tagging approach yields veryhigh accuracy on these tasks, as well as the ability to provide high precision (above99%) operating points with a tolerable loss in recall.
In what follows, we present twoapproaches to adjusting precision: first by adjusting the overall precision and recall ofthe tagger directly, as shown here; and second, by adjusting the precision and recallof tagging results on a per-sentence basis.
We then discuss how complexity-boundedconstraints are implemented in our experiments.
In Section 7.1 we discuss empiricalresults showing how adjusting the tagger in these ways affects parsing performance.Figure 3Tagger precision/recall tradeoff of B, E, and U on the development set for English (a) andChinese (b).736Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing6.2.1 Global High Precision.
Global high precision constraints (GHP) are implementedusing the cumulative score ratios directly to determine if wb ?
B or B.4 Given thecumulative score ratio as defined herein, we define the set B under global high precisionconstraints as:GHP?
(wi .
.
.wN ) = {wi ?
B : CSR(wi) > ?}
(3)The default decision boundary ?
= 0 provides high tagging accuracy, but as men-tioned in Section 6.2 we may want to increase the precision of the tagger so thatthe subsequent parser is not over constrained.
To do this, we increase the thresholdparameter ?
towards positive infinity, potentially moving some words in the corpusfrom B to B.
The same procedure is applied to E and U tags.6.2.2 Sentence-Level High Precision.
Global high precision constraints, as defined here,affect the precision of the tagger over the entire corpus, but may or may not affectindividual sentences.
Because parsing operates on a sentence-by-sentence basis, it maybe beneficial to modify the precision of the tagger based on the current sentence beingprocessed.
We call this approach sentence-level high precision (HP).5To increase tagging precision at the sentence level, we compute the cumulativescores Si and the cumulative scores ratio (CSR) at all word positions as described inthis article.
We next tag all word positions with a CSR(wi) score less than zero withB, and rank the remaining word positions according to their CSR score.
The lowest-ranking ?
fraction of this sorted list are tagged as B, and the rest default to B.
When?
= 0, zero percent of words are tagged with B (i.e., no constraints are applied); andwhen ?
= 1, 100 percent of the words with CSR(wi) > 0 are tagged with B.
This ensuresthat the high-precision threshold is adapted to each sentence, even if its absolute CSRscores are not similar to others in the corpus.6.2.3 Quadratic Bounds.We impose quadratic bounds on context-free parsing by restrict-ing the number of open cells (case 3 in Section 4.3) to be less than or equal to ?N for sometuning parameter ?
and sentence lengthN.
This only involves the sets B and E, as unaryconstraints are restricted to span-1 cells.
Given gold parse trees t?
and a function T(B,E)to generate the set of all valid parse trees satisfying constraints B and E, the optimalquadratically bounded constraints would be:Quad?
(wi .
.
.wN ) = argmaxB,E( maxt?T(B,E)F1(t?, t) s.t.
|C3| ?
?N) (4)that is, the set of constraints that provides the optimal F-score of the best remainingcandidate in the chart while still imposing the required bound on the number of C3cells.
This equation is an instance of combinatorial optimization and solving it exactlyis NP-complete.
Furthermore, we do not have access to gold parse trees t?
duringparsing and must rely on the posterior scores of the tagger.We instead use a greedy approach to approximate Equation (4).
To accomplish this,at every sentence we first sort both the B and E CSR scores for each word position4 In our experiments these scores are in the range ?103.5 As a historical note, we referred to this approach as simply ?high precision constraints?
in Roark andHollingshead (2008) and Roark and Hollingshead (2009).737Computational Linguistics Volume 38, Number 4into a single list.
Next, we assume all word positions are in B and E, then startingfrom the top of the sorted list (highest posterior probability for set inclusion), wecontinue to add word positions to their respective open set and compute the numberof open cells with the given constraints while |C3| < ?N.
By doing this, we guaranteethat only a linear number of case 3 cells are open in the chart, which leads to quadraticworst-case parsing complexity.6.2.4 Linear Bounds.
Imposing O(N) complexity bounds requires constraining thesize of the set B such that |B| ?
?
for some constant ?.
As with quadratic complexitybounds, we wish to find the optimal set B to fulfill these requirements:Linear?
(wi .
.
.wN ) = argmaxB( maxt?T(B)F1(t?, t) such that |B| ?
?)
(5)We again resort to a greedy method to determine the set B, as this optimizationproblem is still NP-complete and gold trees are not available.
B is constructed by sortingall word positions by their CSR scores for B, and then adding only the highest-scoring?
entries to the inclusion set.
All other word positions are closed and in the set B.Because this method does not consider the set E to impose limits on processing,it will be shown in Section 7.2 that O(N) complexity bounding is not as effective asa stand-alone method when compared to the quadratic complexity or high precisionconstraints presented here.6.3 ParsersWe will present results constraining several different parsers.
We first present resultsfor exhaustive parsing using both the CYK and the CONSTRAINEDCYK algorithms.
Weuse a basic CYK exhaustive parser, termed the BUBS parser,6 to parse with a simplePCFG model that uses non-terminal node-labels as provided by the Penn Treebank,after removing empty nodes, node indices, and function tags.
The results presentedhere replicate and extend the results presented in Roark and Hollingshead (2009), usinga different CYK parser.7 The BUBS parser is an open-source high-efficiency parserthat is grammar agnostic and can be run in either exhaustive mode or with variousapproximate inference options (detailed more fully in Section 8.1).
It has been shown toparse exhaustively with very competitive or superior efficiency compared with otherhighly optimized CYK parsers (Dunlop, Bodenstab, and Roark 2011).
In contrast tothe results in Roark and Hollingshead (2009), here we present results with both left-and right-binarized PCFGs induced using a Markov order-2 transform, as detailed inSection 3.3, and also present results for parsing Chinese.We will then present results applying finite-state chart constraints to state-of-the-art parsers, and evaluate the additional efficiency gains these constraints provide, evenwhen these parsers are already heavily pruned.
To simplify the presentation of these6 http://code.google.com/p/bubs-parser.7 Note that there are several differences between the two parsers, including the way in which grammarsare induced, leading to different baseline accuracies and parsing times.
Most notably, the parser fromRoark and Hollingshead (2009) relied upon POS-tagger output, and collapsed unary productions in away that effectively led to parent annotation in certain unary chain constructions.
The current resultsdo not exploit those additional annotations, hence the baseline F-measure accuracy is a couple ofpercentage points lower.738Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingresults, we concentrate in Section 7.1 on parsing with the simple Markov order-2grammars, and then move to higher accuracy parsers in Section 8.7.
CYK Parsing with Finite-State Chart Constraints7.1 High-Precision ConstraintsAs mentioned in Section 6.2, we can adjust the severity of pruning to favor preci-sion over recall.
Figure 4a shows parse time versus F-measure labeled parse accuracyon the English development set for the baseline (unconstrained) exact-inference CYKparser, and for various parameterizations of global high precision (GHP), sentence-levelhigh precision (HP), and unary constraints.
Note that we see accuracy increasing overthe baseline in Figure 4a with the imposition of these constraints at some operatingpoints.
This is not surprising, though, as the finite-state tagger makes use of lexicalFigure 4English development set results (WSJ Section 24) applying global high precision (GHP),sentence-level high precision (HP), and unary constraints with the CONSTRAINEDCYKalgorithm.
We sweep over multiple values of ?
in (a) and plot results in (b), (c), and (d) with theoptimal value for each constraint found in (a).
F-measure accuracy in (b) is computed overbinned sentence lengths.
Figure (d) plots the same data as (c), but zoomed in.739Computational Linguistics Volume 38, Number 4information that the simple PCFG does not, hence there is complementary informationadded that improves the model.
The best operating points?fast parsing and relativelyhigh accuracy?are achieved for GHP constraints at ?
= 40, for sentence-level HP at?
= 0.95, and for unary constraints at ?
= 40.
These high-precision parameterizationsachieve roughly an order of magnitude speed-up and between 0.2 absolute (for unaryconstraints) and 3.7 absolute (for high-precision constraints) F-measure improvementover the baseline unconstrained parser.In order to analyze how these constraints affect accuracy with respect to sentencelength, we turn to Figure 4b.
In this plot, F-measure accuracy is computed over binnedsentence lengths in increments of ten.
All sentences of length greater than 60 are in-cluded in the final bin.
As one might expect, accuracy declines with sentence lengthfor all models because the number of possible trees is exponential in the sentence lengthand errors in one portion of the tree can adversely affect prediction of other constituentsin nearby structures.
We see that all constraints provide accuracy gains over the baselineat all sentence lengths, but point out that the gains by GHP B and E constraints are largerfor longer sentences.
As stated earlier, this is due to themodel-correcting behavior of cellconstraints: Lexical features are leveraged to prune trees that the PCFG may favor butare not syntactically correct with respect to the entire sentence.
Because longer sentencesare poorly modeled by the PCFG, cell constraints play a larger role in restricting thespace of possible trees considered and correcting modeling errors.We can get a different perspective of how these constraints affect parsing time byconsidering the scatter plots in Figures 4c and 4d, which plot each sentence accordingto its length and parsing time at four operating points: baseline (unconstrained); globalhigh precision at ?
= 40; sentence-level high precision at ?
= 0.95; and unary at ?
=40.
Figure 4c shows data points with up to 80 words and 400 msec of parsing time.Figure 4d zooms in to under 200 msec and up to 40 words.
It can be seen in each graphthat unconstrained CYK parsing quickly leaves the plotted area via a steep cubic curve(least-squares fit is N2.9).
Unary constraints operate at the same cubic complexity asthe baseline, but with a constant factor decrease in parsing time (least-squares fit isalso N2.9).
The plots for GHP and HP show dramatic decreases in typical-case runtimecompared with the baseline.
We again run a least-squares fit to the data and find thatboth GHP and HP constraints follow a N2.5 trajectory.The empirical complexity and run-time performance of GHP and HP are nearlyidentical relative to all other chart constraints.
But looking a little closer, we see inFigures 4c and 4d that that GHP constraints are slightly faster than HP for somesentences, and accumulated over the entire development set, this leads to both higheraccuracy (75.1 vs. 74.6 F-measure) and faster parsing time (46 vs. 50 seconds).8 Thisleads to the conclusion that when optimizing parameters for high-precision constraints,we can better tune the overall pipeline by choosing an operating point based on thecorpus-level tagger performance as opposed to tuning for sentence-specific precisiongoals.
Consequently, other than Table 4, we only apply GHP constraints on test setresults in the remainder of this section.Although Figure 4a displays the constrained F-measure parse accuracy as a functionof parsing time, one may also be interested in how tagger precision directly affects parseaccuracy.
To answer this question, we apply high precision constraints to sets B and E inisolation and plot results in Figure 5.
Note that when only constraints on E are applied,no chart cells can be completely closed and parsing time does not significantly decrease.8 See Table 4 for additional performance comparisons.740Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingLikewise, when we apply constraints on B, a chart cell is either open to all constituents(case 3) or closed to all constituents (case 1).
When constraining either B or E, we arepredicting the structure of the final parse tree, which (as can be seen in Figure 5) haslarge effects on parse F-measure.We point out in Figure 5 that to achieve optimal parsing F-measure, tagger precisionmust be above 97% for both English and Chinese.
For both languages, B constraintsare more forgiving and do not adversely affect parsing F-measure as quickly as E con-straints.
These results may lead one to tune two separate high-precision parameters?one to constrain B and one to constrain E. We ran such experiments but found nosignificant gains compared to tying these parameters together.7.2 Complexity-Bounded ConstraintsFigure 6a plots F-measure accuracy versus time to parse the entire development set fortwo complexity bounded constraints: O(N2) and O(N).
We also include the previouslyplotted global high-precision constraints for comparison.
The large asterisk in the plotdepicts the baseline accuracy and efficiency of standard CYK parsing without con-straints.
We sweep over various parameterizations for each method, from very lightlyconstrained to very heavily constrained.
The complexity-bounded constraints are notcombined with the high-precision constraints for this plot (but are later in the article).As can be seen in Figure 6a, the linear-bounded method does not, as applied,achieve a favorable accuracy/efficiency tradeoff curve compared with the quadraticbound or high-precision constraints.
This is not surprising, given that no words areexcluded from the set E for this method, hence far fewer constraints overall are appliedwith the linear-bounded constraints.In addition, we see in Figure 6b that unlike high precision and quadratic constraints,the linear method hurts accuracy on longer sentences over the baseline unconstrainedalgorithm, notably for sentences greater than 50 words.
We attribute this to the factthat for longer sentences, say length 65, only ?
= 16 words are allowed in B for linearFigure 5The effects of tagger precision on parsing F-measure.
B and E constraints are applied in isolation;no other constraints are used during parsing.
Results on the English development set in (a) andChinese in (b).
Baseline unconstrained F-measure accuracy is indicated with the horizontalblack line.741Computational Linguistics Volume 38, Number 4Figure 6English development set results (WSJ Section 24), applying complexity-bounding constraintswith the CONSTRAINEDCYK algorithm.
We sweep over multiple values of ?
in (a) and plotresults in (b), (c), and (d) with the optimal value for each constraint found in (a).
F-measureaccuracy in (b) is computed over binned sentence lengths.
Figure (d) plots the same data as (c),but zoomed in.constraints, which severely limits the search space for these sentences to the point ofpruning gold constituents and decreasing overall accuracy.Next we turn to the scatter plots of Figures 6c and 6d.
Fitting an exponential curveto the data for each constraint via least-squares, we find that global high-precisionconstraints follow a N2.5 trajectory, quadratic at N1.6, and linear at N1.4.
It is interestingthat quadratic constraints actually perform at sub-quadratic run-time complexity.
Thisis because the quadric complexity proof in Section 4.4 assumes that the linear numberof open cells each process O(N) midpoints.
But in practice, many midpoints are notconsidered due to the heavily constrained chart, decreasing the average-case runtimeof CONSTRAINEDCYK with quadratically bounded constraints.Also interesting is that linear constraints perform worse than O(N) at N1.4.
Weattribute this to the nature of the data set.
When parsing with linear constraints, wesee that for short sentences parsing complexity is actually cubic.
Because we allowa constant number of word positions in the open set B, sentences with length less742Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingthan ?
are completely unconstrained and all chart cells remain open.
In Figure 6d itlooks as if parsing becomes linear after the sentence length notably exceeds ?, aroundN = 20.
Assuming our corpus contained a disproportionate number of long sentences,empirical complexity of linear constraints should approach O(N), but due to the largenumber of short sentences, we see an empirical complexity greater than O(N).Three important points can be taken away from Figure 6.
First, although we canprovably constrain parsing to linear speeds, in practice this method is inferior toboth quadratically bounded constraints and high-precision constraints.
Second, high-precision constraints are more accurate (see Figure 6b) and more efficient (see Fig-ure 6d) for shorter strings than the quadratically bound constraints; yet with longerstrings the quadratic constraints better control parsing time than the high-precisionconstraints (see Figure 6c).
Finally, at the ?crossover?
point, where quadratic constraintsstart becoming more efficient than high-precision constraints (roughly 50?60 words,see Figure 4c), there is a larger variance in the parsing time with high-precision con-straints versus those with quadratic bounds.
This illustrates the difference betweenthe two methods of selecting constraints: High-precision constraints can provide verystrong typical-case gains, but there is no guarantee of worst-case performance.
In thisway, the high-precision constraints are similar to other tagging-derived constraints likePOS-tags or chunks.7.3 Combining ConstraintsDepending on the length of the string, the complexity-bound constraints may closemore or fewer chart cells than the high-precision constraints?more for long strings,fewer for short strings.
We can achieve worst-case bounds along with superior typical-case speed-ups by combining both methods.
This is accomplished by taking the unionof high-precision B, E, andU constraints with their respective complexity-bounded sets.When combining complexity-bound constraints with high-precision constraints, wefirst chose operating parameters for each complexity-bounded method at the pointwhere efficiency is greatest and accuracy has yet to decline.
These operating points canbe seen as the ?knee?
of the curves in Figure 6a.
For the quadratic complexity method,we set ?
= 4, limiting the number of open cells to 4N.
For the linear complexity method,we set ?
= 12, limiting the number of word positions in B to a maximum of 12 members.Table 4 displays F-measure accuracy and parsing time (in seconds) for many indi-vidual and combined constraints on the development set: unconstrained CYK parsing;unary constraints; global high-precision (GHP) and sentence-level high-precision (HP)constraints; and O(N2) and O(N) complexity-bounded constraints (Quad and Linear,respectively).
We present all parsing results in Table 4 using both a left- and right-binarized Markov order-2 grammar so that the effects of grammar binarization onfinite-state constraints can be evaluated.
Pre-processing the grammar with a right orleft binarization alters the nature and distribution of child non-terminals for grammarproductions.
Because B and E constraints prune the chart differently depending on thegrammar binarization, we suspect that one method may outperform the other due tothe branching bias of the language being parsed.We find three general trends in Table 4.
First, the efficiency benefits of combin-ing constraints are relatively small.
We suspect this is because the data set containsmostly shorter sentences.
Global high-precision constraints outperform the complexitybounded constraints on sentences of length 10 to 50, which makes up the majority ofthe development set.
It is not until we parse longer sentences that the trends start todiffer and exhibit characteristics of the complexity bounds.
Thus by combining high-743Computational Linguistics Volume 38, Number 4Table 4English development set results (WSJ Section 24) for the CONSTRAINEDCYK algorithm with bothleft- and right-binarized Markov order-2 grammars under various individual and combinedconstraints.Constraints F1 Precision Recall Seconds Speed-upRight-binarizedgrammarNone (baseline CYK) 71.5 74.5 68.8 451GHP(40) 75.3 78.6 72.3 46 9.8xHP(0.95) 74.6 77.8 71.7 50 8.9xQuad(4) 73.8 77.0 70.9 70 6.4xLinear(12) 72.4 75.4 69.6 106 4.3xUnary(40) 72.0 75.4 68.9 386 1.2xHP(0.95) + Quad(4) 74.6 77.8 71.7 48 9.5xHP(0.95) + Linear(12) 74.4 77.6 71.5 48 9.5xGHP(40) + Quad(4) 75.3 78.5 72.3 45 10.0xGHP(40) + Linear(12) 75.1 78.4 72.1 44 10.2xGHP(40) + Unary(40) 75.7 79.4 72.4 34 13.4xGHP(40) + Unary(40) + Quad(4) 75.8 79.5 72.4 33 13.9xLeft-binarizedgrammarNone (baseline CYK) 71.7 74.5 69.1 774GHP(40) 75.4 78.5 72.5 66 11.2xHP(0.95) 74.9 77.9 72.1 75 10.3xQuad(4) 74.0 77.0 71.2 99 7.8xLinear(24) 71.7 74.5 69.1 448 1.7xUnary(40) 71.9 75.2 69.0 607 1.3xHP(0.95) + Quad(4) 74.9 78.0 72.1 69 11.2xHP(0.95) + Linear(24) 74.7 77.8 71.9 71 11.0xGHP(40) + Quad(4) 75.4 78.5 72.5 62 12.6xGHP(40) + Linear(24) 75.2 78.4 72.3 62 12.6xGHP(40) + Unary(40) 75.7 79.3 72.5 37 20.9xGHP(40) + Unary(40) + Quad(4) 75.7 79.3 72.5 37 20.9xprecision and complexity constraints, we attain the typical-case efficiency benefits ofhigh-precision constraints with worst-case complexity bounds.Second, we see that the efficiency gain combining unary and high-precision con-straints is more than additive.
Unary constraints alone for the right-binarized grammardecreased parsing time by 1.3x, but in conjunction with high-precision constraints,parsing time is decreased from 66 seconds to 37 seconds, an additional 1.8x speed-up.We suspect that this additional gain comes from cache efficiencies (due to the heavilypruned nature of the chart, the population of commonly-queried span-1 chart cells havea higher likelihood of remaining in high-speed cache, decreasing overall parsing time),but we leave empirical verification of this theory to future work.The third trend we see in Table 4 is that there are significant efficiency differenceswhen parsing with a right- or left-binarized grammar.
The difference in baseline per-formance has been previously studied (Song, Ding, and Lin 2008; Dunlop, Bodenstab,and Roark 2010), and our results confirm that a right-binarized grammar is superior forparsing the WSJ treebank due to the right-branching bias of parse trees in this corpus.Furthermore, linear constraints were far less effective with a left-binarized grammar,744Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingTable 5English test set results (WSJ Section 23) for the CONSTRAINEDCYK algorithm with both left- andright-binarized Markov order-2 grammars.Constraints F1 Precision Recall Seconds Speed-upRight None (baseline CYK) 71.7 74.6 69.0 628Unary(40) 72.1 75.5 69.0 525 1.2xGHP(40) 75.8 79.0 72.8 75 8.4xGHP(40) + Unary(40) 76.1 79.8 72.7 57 11.0xLeftNone (baseline CYK) 72.0 74.8 69.4 1,063Unary(40) 72.4 75.8 69.4 910 1.2xGHP(40) 76.1 79.3 73.2 106 10.1xGHP(40) + Unary(40) 76.4 80.0 73.1 60 17.9xTable 6Chinese test set results (PCTB Sections 271?300) for the CONSTRAINEDCYK algorithm with bothleft- and right-binarized Markov order-2 grammars.Constraints F1 Precision Recall Seconds Speed-upRight None (baseline CYK) 60.5 64.6 56.9 157Unary(20) 62.3 67.6 57.8 141 1.1xGHP(20) 66.9 71.4 63.0 17 9.1xGHP(20) + Unary(20) 68.9 74.4 64.1 15 10.2xLeftNone (baseline CYK) 60.4 64.5 56.9 269Unary(20) 62.1 67.2 57.8 234 1.2xGHP(20) 66.0 70.5 62.1 33 8.2xGHP(20) + Unary(20) 68.0 73.5 63.2 23 11.7xrequiring a tuning parameter of ?
= 24 such that accuracy was not adversely affected(compare with ?
= 12 for right-binarized results).
This is also caused by the branchingbias inherent in the treebank.
For example, consider a left-binarized grammar andlinear constraints; in the extreme case where ?
= 0, only w1 will be in the open set B,forcing all constituents in the final tree to start at w1.
This results in a completely left-branching tree.
With a right-binarized grammar, only the last word position will be inE, resulting in a completely right-branching tree.
Thus, an overly constrained linear-bounded parse will favor one branching direction over the other.
Because the treebankis biased towards right-branching trees, a right-binarized grammar is more favorablewhen linear constraints are applied.Finally, we note that after all constraints have been applied, the accuracy andefficiency differences between the two binarization strategies nearly disappear.To validate the selected operating points on unseen data, we present results on thetest sets for English in Table 5 and Chinese in Table 6.9 We parse individually with globalhigh-precision and unary constraints, then present the combined result as this gave thebest performance on the development set.
In all cases, we see efficiency improvementsgreater than 10-fold and accuracy improvements of 4.4 absolute F-measure for English,9 We used the development set (articles 301?325 of the Penn Chinese treebank) to tune chart constraintparameters for Chinese, exactly as was done for English.745Computational Linguistics Volume 38, Number 4and 8.4 absolute F-measure for Chinese.
Note that these efficiency improvements areachieved with no additional techniques for speeding up search; modulo the cell closingmechanism, the CYK parsing is exhaustive?it explores all possible category combina-tions from all open child cells.
Techniques such as coarse-to-fine, A?
parsing, or beam-search are all orthogonal to the current approach, and could be applied in conjunctionto achieve additional speedups.In the next section, we investigate the use of chart constraints with a number ofhigh-accuracy parsers, and empirically evaluate the combination of our chart constraintmethods with popular heuristic search methods for parsing.
These parsers include theCharniak parser, the Berkeley parser, and an in-house beam-search parser.8.
High-Accuracy Parsing with Finite-State Chart ConstraintsIn this section we evaluate the additive efficiency gains provided by finite-state con-straints to state-of-the-art parsers.
We apply B, E, and U constraints to three parsers, allof which already prune the search space to make decoding time with large grammarsmore practical.
Each high-accuracy parser that we evaluate also prunes the search spacein a different way?agenda-based search, coarse-to-fine pruning, and beam-search.Applying finite-state constraints to these three distinct parsers demonstrates how ourconstraints interact with other well-known pruning algorithms.
In what follows wedescribe the pruning inherent in the three parsers and how we apply finite-state con-straints within each framework.8.1 Parsers8.1.1 Charniak Parser.
The Charniak (2000) parser is a multi-stage, agenda-driven parserthat can be constrained by pruning edges before they are placed on the agenda.
The firststage of the Charniak parser uses an agenda and a simple PCFG to build a sparse chart,which is used to limit the search in later stages with the full model.
We focus on thisfirst stage, because it is here that we will be constraining the parser.
The edges on theagenda and in the chart are dotted rules, as described in Section 3.3.
When edges arecreated, they are pushed onto the agenda.
Edges that are popped from the agenda areplaced in the chart, and then combined with other chart entries to create new edges thatare pushed onto the agenda.
Once a complete edge spanning the whole string is placedin the chart, at least one full solution must exist.
Instead of terminating the initial chartpopulation at this point, a technique called ?over-parsing?
is used that continues addingedges to the chart (and agenda) until a parameterized number of additional edges havebeen added.
A small over-parsing value will heavily constrain the search space of thelater stages within the pipeline, and a large value will often increase accuracy at theexpense of efficiency.
Upon reaching the desired number of edges, the next stage ofthe pipeline receives the chart as input and any edges remaining on the agenda arediscarded.We constrain the first stage of the Charniak parser by restricting agenda edges.When an edge is created for cell (b, e), where b < e, it is not placed on the agenda ifeither of the following two conditions hold: 1) wb ?
B; or 2) the edge is complete andwe ?
E. With these constraints, a large number of edges that would have previouslybeen considered in the first stage of this pipeline will now be ignored.
This allows usto either reduce the amount of over-parsing, which will increase efficiency, or maintainthe over-parsing threshold and expand the search space in more promising directionsaccording to the chart constraints.
In this article, we have chosen to do the latter.
Notethat speed-ups are still observed, presumably due to the parser finding a complete edge746Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingspanning the whole sentence more quickly, thus leading to slight reductions in totaledges added to the chart.8.1.2 Berkeley Parser.
The Berkeley parser (Petrov and Klein 2007a) is a multi-level coarse-to-fine parser that operates over a set of coarse-to-fine grammars, G1 .
.
.Gt.
At eachgrammar level, the inside and outside constituent probabilities are computed withcoarse grammarGi and used to prune the subsequent search withinGi+1.
This continuesuntil the sentence is parsed with the target grammar Gt.
The initial grammar G1 isa Markov order-0 grammar, and the target grammar Gt is a latent-variable grammarinduced through multiple iterations of splitting and merging non-terminals to maxi-mize the likelihood of the training data (Petrov and Klein 2007b).
This explicit targetgrammar is very large, consisting of 4.3 million productions, 2.4 million of which arelexical productions.
We refer to Gt as the Berkeley grammar.We apply chart constraints to the Berkeley parser during the initial inside passwith grammar G1.
Inside scores are computed via the CONSTRAINEDCYK algorithmof Algorithm 2 modulo the fact that the standard inside/outside sum over scores isused in lines 10, 13, and 17 instead of the Viterbi-max.
It is unnecessary to constraineither the subsequent outside pass or the search with the following grammars G2 .
.
.Gt,as the coarse-to-fine algorithm only considers constituents remaining in the chart fromthe previous coarse grammar.
Reducing the population of chart cells during the initialG1 inside pass consequently prunes the search space for all levels of the coarse-to-finesearch.
We also note that even though there are t = 6 grammar levels in our experimentswith the Berkeley parser, exhaustive parsing with G1 consumes nearly 50% of the totalparse time (Petrov, personal communication).
Applying chart constraints during thisinitial pass is where we see the largest efficiency gain?much more than when chartconstraints supplement coarse-to-fine pruning in subsequent passes.8.1.3 BUBS Parser.
The bottom?up beam-search parser (BUBS) is a variation of the CYKalgorithm where, at each chart cell, all possible edges are sorted by a Figure-of-Merit(FOM) and only the k-best edges are retained (Bodenstab et al 2011).
We follow this set-up and use the Boundary FOM (Caraballo and Charniak 1997), but do not apply beam-width prediction in these experiments as chart constraints and beam-width predictionprune the search space in similar ways (see Bodenstab et al [2011] for a comparison ofthe two methods).
The BUBS parser is grammar agnostic, so to achieve high accuracywe parse with the Berkeley latent variable grammar (Gt described in the previoussubsection), yet only require a single pass over the chart.
The BUBS parser performsViterbi decoding and does not marginalize over the latent variables or compute themax-rule solution as is done in the Berkeley parser.
This leads to a lower F-measurescore in the final results even though both parsers use the same grammar.In this article, we apply finite-state constraints to the BUBS parser in a fashionalmost identical to the CONSTRAINEDCYK algorithm.
Because the BUBS parser is abeam-search parser, the difference is that instead of retaining the max score for all non-terminals Ai at each chart cell, we only retain the max score for the k-best non-terminals.Otherwise, B, E, and U constraints are used to prune the search space in the same way.8.2 High-Accuracy Parsing ResultsFigure 7 displays accuracy and efficiency results of applying three independentconstraints to the three parsers: high precision, quadratically bounded, and unaryconstraints.
We sweep over possible tuning parameters from unconstrained (baseline747Computational Linguistics Volume 38, Number 4Figure 7English accuracy and efficiency results of applying high precision, quadratic, and unaryconstraints at multiple values of ?
to the Charniak, Berkeley, and BUBS parsers, all of whichalready heavily prune the search space.asterisk) to overly constrained such that accuracy is adversely affected.
We also plot theoptimal combination of high precision and quadratic constraints (diamond) and thecombination of all three constraints (circle) for each parser which was computed via agrid search over the joint parameter space.There are many interesting patterns in Figure 7.
First, all three constraints inde-pendently improve the accuracy and efficiency of all three parsers, with the exceptionof accuracy in the Berkeley parser.
This is a powerful result considering each of theseparsers is simultaneously performing various alternative forms of pruning, which were(presumably) tuned for optimal accuracy and efficiency on this same data set.
We alsonote that the efficiency gains from all three constraints are not identical.
In particular,high precision and quadratic constraints outperforms unary constraints in isolation.
Butthis should be expected as unary constraints only partially closeO(n) chart cells whereasboth high precision and quadratic constraints affect O(n2) chart cells.
Nevertheless,looking at the optimal point combining all three constraints, we see that adding unaryconstraints to begin/end constraints does provide additional gains (in both accuracyand efficiency) for the BUBS and Charniak parsers.The Berkeley parser appears to benefit from B and E constraints, but sees almostno gain from unary constraints.
The reason for this has to do with the implementationdetails of combining (joining) two child cells within the inner loop of the CYK algorithm(line 8 in Algorithm 1).
In bottom?up CYK parsing, to extend derivations of adjacentsubstrings into new constituents spanning the combined string, one can either iterateover all binary productions in the grammar and test if the new derivation is valid (wecall this ?grammar loop?
), or one can take the cross-product of active entries in the cellsspanning the substrings and poll the grammar for possible derivations (we call this?cross-product?).
With the cross-product approach, fewer active entries in either childcell leads to fewer grammar access operations.
Thus, pruning constituents in smaller-span cells directly affects the overall efficiency of parsing.
On the other hand, with the748Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsinggrammar loop method there is a constant number of grammar access operations (i.e.,the number of grammar productions) and the number of active entries in each childcell has little impact on efficiency.
Therefore, with the grammar loop implementationof the CYK algorithm, pruning techniques such as unary constraints will have verylittle impact on the final run-time efficiency of the parser unless the list of grammarproductions is modified given the constraints.
For example, alternate iterators overgrammar productions could be created such that they only consider a subset of allpossible productions.
If the left child is a span-1 chart cell in U, then only grammarproductions with a POS tag as the left child need to be considered.
Looping overthis smaller list, opposed to all possible grammar productions, can reduce the overallruntime.
The Berkeley parser contains the grammar-loop implementation of the CYKalgorithm.
Although all grammar productions are iterated over at each cell, the parsermaintains meta-information indicating where non-terminals have been placed in thechart, allowing it to quickly skip over a subset of the productions that are incompatiblewith state of the chart.
This optimization improves efficiency, but does not take fulladvantage of restricted cell population imposed by unary constraints, and thus doesnot benefit as greatly when compared to the BUBS or Charniak parsers.The second trend we see in Figure 7 is that accuracy actually improves with ad-ditional constraints.
This is expected with the Charniak parser as we keep the amountof search space fixed in hopes of pursuing more accurate global paths, but both theBerkeley and BUBS parsers are simply eliminating search paths they would haveotherwise considered.
Although it is unusual that pruning leads to higher accuracyduring search, it is not wholly unexpected here as our finite-state tagger makes use oflexical relationships that the PCFG does not (i.e., lexical relationships based on the linearstring rather than over the syntactic structure).
By leveraging this new information toconstrain the search space, we are indirectly improving the quality of the model.
Wealso suspect that the Berkeley parser sees less of an accuracy improvement than theBUBS parsers because the coarse-to-fine pruning within the Berkeley parser is more?globally informed?
than the beam-search within the BUBS parser.
By leveraging thecoarse-grained inside/outside distribution of trees over the input sentence, the Berkeleyparser can more intelligently prune the search space with respect to the target grammarand may not benefit from the additional information inherent in the finite-state taggingmodel.The third observation we point out in Figure 7 is that we see no additional gainsfrom combining high-precision constraints with quadratic complexity constraints.
Withall three parsers, high-precision constraints are empirically superior to quadratic con-straints, even though high-precision constraints come with no guarantee on worst-casecomplexity reduction.
It is our hypothesis that the additional pruning provided byquadratic constraints for exhaustive CYK parsing is already removed by the internalpruning of each of the three high-accuracy parsers.
We therefore report testing resultsusing only high-precision and unary constraints for these high-accuracy parsers.We apply models tuned on the development set to unseen English test data (WSJSection 23) in Table 7, and Chinese test data (PCTB articles 271?300) in Table 8.
ForEnglish, we see similar trends as we did on the development set results: Decoding timeis nearly halved when chart constraints are applied to these already heavily constrainedparsers, without any loss in accuracy.
We also see independent gains from both unaryand high-precision constraints, and additive efficiency gains when combined.Applying chart constraints to Chinese parsing in Table 8 gives substantially largeraccuracy and efficiency gains than English for both the BUBS and Berkeley parser.
Inparticular, the accuracy of the BUBS parser increases by 2.3 points absolute (p = 0.0002),749Computational Linguistics Volume 38, Number 4Table 7English test set results (WSJ Section 23) applying sentence-level high precision and unaryconstraints to three parsers with parameter settings tuned on development data.Parser F1 Precision Recall Seconds Speed-upBUBS (2010) 88.4 88.5 88.3 586+ Unary(100) 88.5 88.7 88.3 486 1.2x+ HP(0.9) 88.7 88.9 88.6 349 1.7x+ HP(0.9) + Unary(100) 88.7 89.0 88.4 283 2.1xCharniak (2000) 89.7 89.7 89.6 1,116+ Unary(100) 89.8 89.8 89.7 900 1.2x+ HP(0.8) 89.8 90.0 89.6 716 1.6x+ HP(0.8) + Unary(100) 89.7 90.0 89.5 679 1.6xBerkeley (2007) 90.2 90.3 90.0 564+ Unary(125) 90.1 90.3 89.9 495 1.1x+ HP(0.7) 90.2 90.4 90.0 320 1.8x+ HP(0.7) + Unary(125) 90.2 90.4 89.9 289 2.0xTable 8Chinese test set results (PCTB articles 271?300) applying sentence-level high-precision and unaryconstraints to two parsers with parameter settings tuned on development data.Parser F1 Precision Recall Seconds Speed-upBUBS (2010) 79.5 79.5 79.1 169+ Unary(50) 80.7 82.1 79.4 153 1.1x+ HP(0.8) 81.1 81.5 80.7 75 2.3x+ HP(0.8) + Unary(50) 81.8 83.1 80.5 44 3.8xBerkeley (2007) 83.9 84.5 83.3 141+ Unary(50) 84.5 85.9 83.0 125 1.1x+ HP(0.7) 84.5 85.1 83.8 64 2.2x+ HP(0.7) + Unary(50) 84.7 86.1 83.4 57 2.5xand the Berkeley parser increases by 0.8 points absolute to 84.7 (p = 0.0119), the highestaccuracy we are aware of for an individual model on this data set.10,11 These increasesrelative to English may be surprising as chart constraint tagging accuracy for Chineseis worse than English (see Table 3).
We attribute this large gain to the lower baselineaccuracy of parsing with the Chinese treebank, allowing our method to contributeadditional syntactic constraints that were otherwise unmodeled by the PCFG.9.
Conclusion and Future WorkWe have presented finite-state pre-processing methods to constrain context-freeparsing that reduce both the worst-case complexity and overall run time.
Four unique10 Significance was tested using stratified shuffling.11 Zhang et al (2009) report an F-measure of 85.5 with a k-best combination of parsers, and Burkett, Blitzer,and Klein (2010) report an F-measure of 86.0 by leveraging parallel English data for training, but ourmodel is trained from the Chinese treebank alone and is integrated into the Berkeley parser, making itvery efficient.750Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsingbounding methods were presented, with high-precision constraints showing superiorperformance in empirical trials.
Applying these constraints to context-free parsingincreased efficiency by over 20 times for exhaustive CYK parsing, and nearly doubledthe speed of the Charniak and Berkeley parsers?both of which have been previouslytuned for optimal accuracy/efficiency performance.
We have shown that our methodgeneralizes across multiple grammars and languages, and that constraining bothmulti-word chart cells and single-word chart cells produces additive efficiency gains.In future work we plan to investigate additional methods to make context-freeparsing more efficient.
In particular, we believe the dynamic programming chart couldbe constrained even further if we take into account the population and structure ofchart cells at a finer level.
For example, the B and E constraints we have presented heredo not take into account the height of the constituent they are predicting.
Instead, theysimply open or close the entire diagonal in the chart.
Refining this structured predictionto bins, as is done in Zhang et al (2010), or classifying chart cells directly, as is donein Bodenstab et al (2011), has shown promise, but we believe that further research inthis area would yield addition efficiency gains.
In particular, those two papers do notdifferentiate between non-terminals when opening or closing cells, and we hypothesizethat learning separate finite-state classifiers for automatically derived clusters of non-terminals may increase performance.AcknowledgmentsPortions of this paper have appeared inconference papers: Roark and Hollingshead(2008), Roark and Hollingshead (2009), andBodenstab, Hollingshead, and Roark (2011).We thank three anonymous reviewers fortheir insightful comments and suggestions.Also thanks to Aaron Dunlop for being soswell.
This research was supported in partby NSF grants IIS-0447214 and IIS-0811745,and DARPA grant HR0011-09-1-0041.Any opinions, findings, conclusions,or recommendations expressed in thispublication are those of the authors anddo not necessarily reflect the views of theNSF or DARPA.ReferencesBangalore, Srinivas and Aravind K. Joshi.1999.
Supertagging: an approach toalmost parsing.
Computational Linguistics,25:237?265.Bergsma, Shane and Colin Cherry.
2010.Fast and accurate arc filtering fordependency parsing.
In Proceedingsof the 23rd International Conference onComputational Linguistics (Coling 2010),pages 53?61, Beijing.Black, E., S. Abney, S. Flickenger,C.
Gdaniec, C. Grishman, P. Harrison,D.
Hindle, R. Ingria, F. Jelinek, J. Klavans,M.
Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.Procedure for quantitatively comparingthe syntactic coverage of Englishgrammars.
In Proceedings of the Workshopon Speech and Natural Language, HLT ?91,pages 306?311, Pacific Grove, CA.Bodenstab, Nathan, Aaron Dunlop, KeithHall, and Brian Roark.
2011.
Beam-widthprediction for efficient context-freeparsing.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics, pages 440?449, Portland, OR.Bodenstab, Nathan, Kristy Hollingshead,and Brian Roark.
2011.
Unary constraintsfor efficient context-free parsing.
InProceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics,pages 676?681, Portland, OR.Burkett, David, John Blitzer, and Dan Klein.2010.
Joint parsing and alignment withweakly synchronized grammars.
In HumanLanguage Technologies: The 2010 AnnualConference of the North American Chapterof the Association for ComputationalLinguistics, HLT ?10, pages 127?135,Los Angeles, CA.Caraballo, Sharon A. and Eugene Charniak.1997.
New figures of merit for best-firstprobabilistic chart parsing.
ComputationalLinguistics, 24:275?298.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofthe 1st Conference of the North AmericanChapter of the Association for ComputationalLinguistics, pages 132?139, Seattle, WA.Charniak, Eugene and Mark Johnson.
2005.Coarse-to-fine n-best parsing and MaxEnt751Computational Linguistics Volume 38, Number 4discriminative reranking.
In Proceedings ofthe 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 173?180, Sydney.Cherry, Colin and Shane Bergsma.
2011.Joint training of dependency parsingfilters through latent support vectormachines.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies,pages 200?205, Toulouse.Clark, Stephen and James R. Curran.2004.
The importance of supertaggingfor wide-coverage CCG parsing.
InProceedings of COLING, pages 282?288,Geneva.Cocke, John and Jacob T. Schwartz.
1970.Programming languages and theircompilers: Preliminary notes.
CourantInstitute of Mathematical Sciences,New York University.Collins, Michael.
2002.
Discriminativetraining methods for hidden Markovmodels: Theory and experiments withperceptron algorithms.
In Proceedingsof the Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 1?8, Philadelphia, PA.Djordjevic, Bojan, James R. Curran, andStephen Clark.
2007.
Improving theefficiency of a wide-coverage CCG parser.In Proceedings of the 10th InternationalConference on Parsing Technologies,IWPT ?07, pages 39?47, Prague.Dreyer, Markus, David A. Smith, andNoah A. Smith.
2006.
Vine parsing andminimum risk reranking for speed andprecision.
In Proceedings of the TenthConference on Computational NaturalLanguage Learning (CoNLL-X),pages 201?205, New York, NY.Dunlop, Aaron, Nathan Bodenstab, andBrian Roark.
2010.
Reducing the grammarconstant: an analysis of CYK parsingefficiency.
Technical report CSLU-2010-02,Oregon Health & Science University,Beaverton, OR.Dunlop, Aaron, Nathan Bodenstab,and Brian Roark.
2011.
Efficientmatrix-encoded grammars and lowlatency parallelization strategies for CYK.In Proceedings of the 12th InternationalConference on Parsing Technologies (IWPT),pages 163?174, Dublin.Earley, Jay.
1970.
An efficient context-freeparsing algorithm.
Communications of theACM, 6(8):451?455.Eisner, Jason and Noah A. Smith.
2005.Parsing with soft and hard constraints ondependency length.
In Proceedings of theNinth International Workshop on ParsingTechnology (IWPT), pages 30?41,Vancouver.Glaysher, Elliot and Dan Moldovan.
2006.Speeding up full syntactic parsing byleveraging partial parsing decisions.
InProceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 295?300,Sydney.Hollingshead, Kristy, Seeger Fisher, andBrian Roark.
2005.
Comparing andcombining finite-state and context-freeparsers.
In Proceedings of the HumanLanguage Technology Conference and theConference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP),pages 787?794, Vancouver.Hollingshead, Kristy and Brian Roark.2007.
Pipeline iteration.
In Proceedingsof the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 952?959, Prague.Kasami, Tadao.
1965.
An efficientrecognition and syntax analysisalgorithm for context-free languages.Technical report AFCRL-65-758, Air ForceCambridge Research Lab, Bedford, MA.Marcus, Mitchell P., Mary AnnMarcinkiewicz, and Beatrice Santorini.1993.
Building a large annotated corpus ofEnglish: The Penn treebank.
ComputationalLinguistics, 19:313?330.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic.
2005.Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings ofHuman Language Technology Conference andConference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP),pages 523?530, Vancouver.Nivre, Joakim.
2006.
Constraints onnon-projective dependency parsing.In Proceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL),pages 73?80, Trento.Petrov, Slav and Dan Klein.
2007a.
Improvedinference for unlexicalized parsing.
InProceedings of Human Language Technologies2007: The Conference of the North AmericanChapter of the Association for ComputationalLinguistics (HLT-NAACL), pages 404?411,Rochester, NY.Petrov, Slav and Dan Klein.
2007b.
Learningand inference for hierarchically splitPCFGs.
In Proceedings of the 22nd NationalConference on Artificial Intelligence -Volume 2, pages 1663?1666, Vancouver.752Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity ParsingRatnaparkhi, Adwait.
1999.
Learning toparse natural language with maximumentropy models.Machine Learning,34(1-3):151?175.Roark, Brian and Kristy Hollingshead.2008.
Classifying chart cells for quadraticcomplexity context-free inference.
InProceedings of the 22nd InternationalConference on Computational Linguistics(COLING), pages 745?752, Manchester.Roark, Brian and Kristy Hollingshead.
2009.Linear complexity context-free parsingpipelines via chart constraints.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (HLT-NAACL),pages 647?655, Boulder, CO.Sha, Fei and Fernando Pereira.
2003.
Shallowparsing with conditional random fields.
InProceedings of HLT-NAACL, pages 134?141,Edmonton.Sogaard, Anders and Jonas Kuhn.
2009.Using a maximum entropy-based taggerto improve a very fast vine parser.In Proceedings of the 11th InternationalConference on Parsing Technologies,IWPT ?09, pages 206?209, Paris.Song, Xinying, Shilin Ding, and Chin-YewLin.
2008.
Better binarization for the CKYparsing.
In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 167?176,Honolulu, HI.Weiss, David, Benjamin Sapp, and BenTaskar.
2010.
Sidestepping intractableinference with structured ensemblecascades.
In Proceedings of NIPS,pages 2415?2423, Vancouver.Weiss, David and Benjamin Taskar.
2010.Structured prediction cascades.
Journal ofMachine Learning Research - ProceedingsTrack, 9:916?923.Xue, Naiwen, Fei Xia, Fu-dong Chiou, andMarta Palmer.
2005.
The Penn Chinesetreebank: Phrase structure annotationof a large corpus.
Natural LanguageEngingeering, 11:207?238.Younger, Daniel H. 1967.
Recognition andparsing of context-free languages in timen3.
Information and Control, 10(2):189?208.Zhang, Hui, Min Zhang, Chew Lim Tan, andHaizhou Li.
2009.
K-best combination ofsyntactic parsers.
In Proceedings of the 2009Conference on Empirical Methods in NaturalLanguage Processing: Volume 3, EMNLP ?09,pages 1552?1560, Singapore.Zhang, Yue, Byung Gyu Ahn, Stephen Clark,Curt Van Wyk, James R. Curran, andLaura Rimell.
2010.
Chart pruning forfast lexicalised-grammar parsing.
InProceedings of the 23rd InternationalConference on Computational Linguistics,pages 1472?1479, Beijing.753
