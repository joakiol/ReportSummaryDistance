Word Sense Discrimination by Clustering Contextsin Vector and Similarity SpacesAmruta Purandare and Ted PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812 USA{pura0010,tpederse}@d.umn.eduhttp://senseclusters.sourceforge.netAbstractThis paper systematically compares unsuper-vised word sense discrimination techniquesthat cluster instances of a target word that oc-cur in raw text using both vector and similarityspaces.
The context of each instance is repre-sented as a vector in a high dimensional fea-ture space.
Discrimination is achieved by clus-tering these context vectors directly in vectorspace and also by finding pairwise similaritiesamong the vectors and then clustering in sim-ilarity space.
We employ two different repre-sentations of the context in which a target wordoccurs.
First order context vectors representthe context of each instance of a target wordas a vector of features that occur in that con-text.
Second order context vectors are an indi-rect representation of the context based on theaverage of vectors that represent the words thatoccur in the context.
We evaluate the discrim-inated clusters by carrying out experiments us-ing sense?tagged instances of 24 SENSEVAL-2 words and the well known Line, Hard andServe sense?tagged corpora.1 IntroductionMost words in natural language have multiple possiblemeanings that can only be determined by consideringthe context in which they occur.
Given a target wordused in a number of different contexts, word sense dis-crimination is the process of grouping these instances ofthe target word together by determining which contextsare the most similar to each other.
This is motivated by(Miller and Charles, 1991), who hypothesize that wordswith similar meanings are often used in similar contexts.Hence, word sense discrimination reduces to the problemof finding classes of similar contexts such that each classrepresents a single word sense.
Put another way, contextsthat are grouped together in the same class represent aparticular word sense.While there has been some previous work in sense dis-crimination (e.g., (Schu?tze, 1992), (Pedersen and Bruce,1997), (Pedersen and Bruce, 1998), (Schu?tze, 1998),(Fukumoto and Suzuki, 1999)), by comparison it is muchless than that devoted to word sense disambiguation,which is the process of assigning a meaning to a wordfrom a predefined set of possibilities.
However, solutionsto disambiguation usually require the availability of anexternal knowledge source or manually created sense?tagged training data.
As such these are knowledge inten-sive methods that are difficult to adapt to new domains.By contrast, word sense discrimination is an unsuper-vised clustering problem.
This is an attractive methodol-ogy because it is a knowledge lean approach based on ev-idence found in simple raw text.
Manually sense taggedtext is not required, nor are specific knowledge rich re-sources like dictionaries or ontologies.
Instances are clus-tered based on their mutual contextual similarities whichcan be completely computed from the text itself.This paper presents a systematic comparison of dis-crimination techniques suggested by Pedersen and Bruce((Pedersen and Bruce, 1997), (Pedersen and Bruce,1998)) and by Schu?tze ((Schu?tze, 1992), (Schu?tze,1998)).
This paper also proposes and evaluates severalextensions to these techniques.We begin with a summary of previous work, and thena discussion of features and two types of context vec-tors.
We summarize techniques for clustering in vectorversus similarity spaces, and then present our experimen-tal methodology, including a discussion of the data usedin our experiments.
Then we describe our approach tothe evaluation of unsupervised word sense discrimina-tion.
Finally we present an analysis of our experimentalresults, and conclude with directions for future work.2 Previous Work(Pedersen and Bruce, 1997) and (Pedersen and Bruce,1998) propose a (dis)similarity based discrimination ap-proach that computes (dis)similarity among each pair ofinstances of the target word.
This information is recordedin a (dis)similarity matrix whose rows/columns repre-sent the instances of the target word that are to be dis-criminated.
The cell entries of the matrix show the de-gree to which the pair of instances represented by thecorresponding row and column are (dis)similar.
The(dis)similarity is computed from the first order contextvectors of the instances which show each instance as avector of features that directly occur near the target wordin that instance.
(Schu?tze, 1998) introduces second order context vec-tors that represent an instance by averaging the featurevectors of the content words that occur in the context ofthe target word in that instance.
These second order con-text vectors then become the input to the clustering algo-rithm which clusters the given contexts in vector space,instead of building the similarity matrix structure.There are some significant differences in the ap-proaches suggested by Pedersen and Bruce and bySchu?tze.
As yet there has not been any systematic studyto determine which set of techniques results in bettersense discrimination.
In the sections that follow, we high-light some of the differences between these approaches.2.1 Context RepresentationPedersen and Bruce represent the context of each test in-stance as a vector of features that directly occur near thetarget word in that instance.
We refer to this representa-tion as the first order context vector.
Schu?tze, by contrast,uses the second order context representation that averagesthe first order context vectors of individual features thatoccur near the target word in the instance.
Thus, Schu?tzerepresents each feature as a vector of words that occurin its context and then computes the context of the targetword by adding the feature vectors of significant contentwords that occur near the target word in that context.2.2 FeaturesPedersen and Bruce use a small number of local featuresthat include co?occurrence and part of speech informa-tion near the target word.
They select features from thesame test data that is being discriminated, which is a com-mon practice in clustering in general.
Schu?tze representscontexts in a high dimensional feature space that is cre-ated using a separate large corpus (referred to as the train-ing corpus).
He selects features based on their frequencycounts or log-likelihood ratios in this corpus.In this paper, we adopt Schu?tze?s approach and selectfeatures from a separate corpus of training data, in partbecause the number of test instances may be relativelysmall and may not be suitable for selecting a good featureset.
In addition, this makes it possible to explore varia-tions in the training data while maintaining a consistenttest set.
Since the training data used in unsupervised clus-tering does not need to be sense tagged, in future work weplan to develop methods of collecting very large amountsof raw corpora from the Web and other online sourcesand use it to extract features.Schu?tze represents each feature as a vector of wordsthat co?occur with that feature in the training data.
Thesefeature vectors are in fact the first order context vectorsof the feature words (and not target word).
The wordsthat co?occur with the feature words form the dimensionsof the feature space.
Schu?tze reduces the dimensional-ity of this feature space using Singular Value Decompo-sition (SVD), which is also employed by related tech-niques such as Latent Semantic Indexing (Deerwester etal., 1990) and Latent Semantic Analysis (Landauer et al,1998).
SVD has the effect of converting a word levelfeature space into a concept level semantic space thatsmoothes the fine distinctions between features that rep-resent similar concepts.2.3 Clustering SpacePedersen and Bruce represent instances in a(dis)similarity space where each instance can be seen asa point and the distance between any two points is a func-tion of their mutual (dis)similarities.
The (dis)similaritymatrix showing the pair-wise (dis)similarities amongthe instances is given as the input to the agglomerativeclustering algorithm.
The context group discriminationmethod used by Schu?tze, on the other hand, operates onthe vector representations of instances and thus worksin vector space.
Also he employs a hybrid clusteringapproach which uses both an agglomerative and theEstimation Maximization (EM) algorithm.3 First Order Context VectorsFirst order context vectors directly indicate which fea-tures make up a context.
In all of our experiments, thecontext of the target word is limited to 20 surroundingcontent words on either side.
This is true both when weare selecting features from a set of training data, or whenwe are converting test instances into vectors for cluster-ing.
The particular features we are interested in are bi-grams and co?occurrences.Co-occurrences are words that occur within five po-sitions of the target word (i.e., up to three interveningwords are allowed).
Bigrams are ordered pairs of wordsthat co?occur within five positions of each other.
Thus,co?occurrences are unordered word pairs that include thetarget word, whereas bigrams are ordered pairs that mayor may not include the target.
Both the co?occurrencesand the bigrams must occur in at least two instances inthe training data, and the two words must have a log?likelihood ratio in excess of 3.841, which has the effectof removing co?occurrences and bigrams that have morethan 95% chance of being independent of the target word.After selecting a set of co-occurrences or bigrams froma corpus of training data, a first order context representa-tion is created for each test instance.
This shows howmany times each feature occurs in the context of the tar-get word (i.e., within 20 positions from the target word)in that instance.4 Second Order Context VectorsA test instance can be represented by a second order con-text vector by finding the average of the first order contextvectors that are associated with the words that occur nearthe target word.
Thus, the second order context represen-tation relies on the first order context vectors of featurewords.
The second order experiments in this paper usetwo different types of features, co?occurrences and bi-grams, defined as they are in the first order experiments.Each co?occurrence identified in training data is as-signed a unique index and occupies the correspondingrow/column in a word co?occurrence matrix.
This isconstructed from the co?occurrence pairs, and is a sym-metric adjacency matrix whose cell values show the log-likelihood ratio for the pair of words representing thecorresponding row and column.
Each row of the co?occurrence matrix can be seen as a first order context vec-tor of the word represented by that row.
The set of wordsforming the rows/columns of the co?occurrence matrixare treated as the feature words.Bigram features lead to a bigram matrix such thatfor each selected bigram WORDi<>WORDj, WORDirepresents a single row, say the ith row, and WORDjrepresents a single column, say the jth column, ofthe bigram matrix.
Then the value of cell (i,j) indi-cates the log?likelihood ratio of the words in the bigramWORDi<>WORDj.
Each row of the bigram matrix canbe seen as a bigram vector that shows the scores of allbigrams in which the word represented by that row oc-curs as the first word.
Thus, the words representing therows of the bigram matrix make the feature set while thewords representing the columns form the dimensions ofthe feature space.5 ClusteringThe objective of clustering is to take a set of instancesrepresented as either a similarity matrix or context vec-tors and cluster together instances that are more like eachother than they are to the instances that belong to otherclusters.Clustering algorithms are classified into three maincategories, hierarchical, partitional, and hybrid methodsthat incorporate ideas from both.
The algorithm acts as asearch strategy that dictates how to proceed through theinstances.
The actual choice of which clusters to splitor merge is decided by a criteria function.
This sectiondescribes the clustering algorithms and criteria functionsthat have been employed in our experiments.5.1 HierarchicalHierarchical algorithms are either agglomerative or divi-sive.
They both proceed iteratively, and merge or divideclusters at each step.
Agglomerative algorithms start witheach instance in a separate cluster and merge a pair ofclusters at each iteration until there is only a single clus-ter remaining.
Divisive methods start with all instancesin the same cluster and split one cluster into two duringeach iteration until all instances are in their own cluster.The most widely known criteria functions used with hi-erarchical agglomerative algorithms are single link, com-plete link, and average link, also known as UPGMA.
(Schu?tze, 1998) points out that single link clusteringtends to place all instances into a single elongated clus-ter, whereas (Pedersen and Bruce, 1997) and (Purandare,2003) show that hierarchical agglomerative clusteringusing average link (via McQuitty?s method) fares well.Thus, we have chosen to use average link/UPGMA as ourcriteria function for the agglomerative experiments.In similarity space, each instance can be viewed as anode in a weighted graph.
The weights on edges joiningtwo nodes indicate their pairwise similarity as measuredby the cosine between the context vectors that representthe pair of instances.When agglomerative clustering starts, each node is inits own cluster and is considered to be the centroid of thatcluster.
At each iteration, average link selects the pairof clusters whose centroids are most similar and mergesthem into a single cluster.
For example, suppose the clus-ters I and J are to be merged into a single cluster IJ .
Theweights on all other edges that connect existing nodes tothe new node IJ must now be revised.
Suppose that Q issuch a node.
The new weight in the graph is computed byaveraging the weight on the edge between nodes I and Qand that on the edge between J and Q.
In other words:W?
(IJ,Q) =W (I,Q) +W (J,Q)2(1)In vector space, average link starts by assigning eachvector to a single cluster.
The centroid of each cluster isfound by calculating the average of all the context vec-tors that make up the cluster.
At each iteration, averagelink selects the pair of clusters whose centroids are clos-est with respect to their cosines.
The selected pair of clus-ters is merged and a centroid is computed for this newlycreated cluster.5.2 PartitionalPartitional algorithms divide an entire set of instancesinto a predetermined number of clusters (K) without go-ing through a series of pairwise comparisons.
As suchthese methods are somewhat faster than hierarchical al-gorithms.For example, the well known K-means algorithm ispartitional.
In vector space, each instance is representedby a context vector.
K-means initially selects K randomvectors to serve as centroids of these initial K clusters.
Itthen assigns every other vector to one of the K clusterswhose centroid is closest to that vector.
After all vectorsare assigned, it recomputes the cluster centroids by av-eraging all of the vectors assigned to that cluster.
Thisrepeats until convergence, that is until no vector changesits cluster across iterations and the centroids stabilize.In similarity space, each instance can be viewed as anode of a fully connected weighted graph whose edges in-dicate the similarity between the instances they connect.K-means will first select K random nodes that representthe centroids of the initial K clusters.
It will then assignevery other node I to one of the K clusters such that theedge joining I and the centroid of that cluster has maxi-mum weight among the edges joining I to all centroids.5.3 Hybrid MethodsIt is generally believed that the quality of clustering bypartitional algorithms is inferior to that of the agglom-erative methods.
However, a recent study (Zhao andKarypis, 2002) has suggested that these conclusions arebased on experiments conducted with smaller data sets,and that with larger data sets partitional algorithms arenot only faster but lead to better results.In particular, Zhao and Karypis recommend a hybridapproach known as Repeated Bisections.
This overcomesthe main weakness with partitional approaches, which isthe instability in clustering solutions due to the choice ofthe initial random centroids.
Repeated Bisections startswith all instances in a single cluster.
At each iteration itselects one cluster whose bisection optimizes the chosencriteria function.
The cluster is bisected using standardK-means method with K=2, while the criteria functionmaximizes the similarity between each instance and thecentroid of the cluster to which it is assigned.
As such thisis a hybrid method that combines a hierarchical divisiveapproach with partitioning.6 Experimental DataWe use 24 of the 73 words in the SENSEVAL-2 sense?tagged corpus, and the Line, Hard and Serve sense?tagged corpora.
Each of these corpora are made up ofinstances that consist of 2 or 3 sentences that include asingle target word that has a manually assigned sense tag.However, we ignore the sense tags at all times exceptduring evaluation.
At no point do the sense tags enter intothe clustering or feature selection processes.
To be clear,we do not believe that unsupervised word sense discrim-ination needs to be carried out relative to a pre-existingset of senses.
In fact, one of the great advantages of un-supervised technique is that it doesn?t need a manuallyannotated text.
However, here we employ sense?taggedtext in order to evaluate the clusters that we discover.The SENSEVAL-2 data is already divided into trainingand test sets, and those splits were retained for these ex-periments.
The SENSEVAL-2 data is relatively small, inthat each word has approximately 50-200 training andtest instances.
The data is particularly challenging forunsupervised algorithms due to the large number of finegrained senses, generally 8 to 12 per word.
The smallvolume of data combined with large number of possiblesenses leads to very small set of examples for most of thesenses.As a result, prior to clustering we filter the trainingand test data independently such that any instance thatuses a sense that occurs in less than 10% of the availableinstances for a given word is removed.
We then elimi-nate any words that have less than 90 training instancesafter filtering.
This process leaves us with a set of 24SENSEVAL-2 words, which includes the 14 nouns, 6 ad-jectives and 4 verbs that are shown in Table 1.In creating our evaluation standard, we assume thateach instance will be assigned to at most a single clus-ter.
Therefore if an instance has multiple correct sensesassociated with it, we treat the most frequent of these asthe desired tag, and ignore the others as possible correctanswers in the test data.The Line, Hard and Serve corpora do not have a stan-dard training?test split, so these were randomly dividedinto 60?40 training?test splits.
Due to the large numberof training and test instances for these words, we filteredout instances associated with any sense that occurred inless than 5% of the training or test instances.We also randomly selected five pairs of words fromthe SENSEVAL-2 data and mixed their instances together(while retaining the training and test distinction that al-ready existed in the data).
After mixing, the data wasfiltered such that any sense that made up less than 10%in the training or test data of the new mixed sample wasremoved; this is why the total number of instances for themixed pairs is not the same as the sum of those for theindividual words.
These mix-words were created in orderto provide data that included both fine grained and coarsegrained distinctions.Table 1 shows all words that were used in our exper-iments along with their parts of speech.
Thereafter weshow the number of training (TRN) and test instances(TST) that remain after filtering, and the number ofsenses found in the test data (S).
We also show the per-centage of the majority sense in the test data (MAJ).
Thisis particularly useful, since this is the accuracy that wouldbe attained by a baseline clustering algorithm that puts alltest instances into a single cluster.7 Evaluation TechniqueWhen we cluster test instances, we specify an upper limiton the number of clusters that can be discovered.
In theseexperiments that value is 7.
This reflects the fact thatwe do not know a?priori the number of possible senses aword will have.
This also allows us to verify the hypothe-sis that a good clustering approach will automatically dis-cover approximately same number of clusters as sensesfor that word, and the extra clusters (7?#actual senses)will contain very few instances.
As can be seen from col-umn S in Table 1, most of the words have 2 to 4 senses onan average.
Of the 7 clusters created by an algorithm, wedetect the significant clusters by ignoring (throwing out)clusters that contain less than 2% of the total instances.The instances in the discarded clusters are counted as un-clustered instances and are subtracted from the total num-ber of instances.Our basic strategy for evaluation is to assign availablesense tags to the discovered clusters such that the assign-ment leads to a maximally accurate mapping of senses toclusters.
The problem of assigning senses to clusters be-comes one of reordering the columns of a confusion ma-trix that shows how senses and clusters align such that thediagonal sum is maximized.
This corresponds to severalwell known problems, among them the Assignment Prob-lem in Operations Research, or determining the maximalmatching of a bipartite graph in Graph Theory.During evaluation we assign one sense to at most onecluster, and vice versa.
When the number of discoveredclusters is the same as the number of senses, then thereis a one to one mapping between them.
When the num-ber of clusters is greater than the number of actual senses,then some clusters will be left unassigned.
And when thenumber of senses is greater than the number of clusters,some senses will not be assigned to any cluster.
The rea-son for not assigning a single sense to multiple clustersor multiple senses to one cluster is that, we are assumingone sense per instance and one sense per cluster.We measure the precision and recall based on this max-imally accurate assignment of sense tags to clusters.
Pre-cision is defined as the number of instances that are clus-tered correctly divided by the number of instances clus-tered, while recall is the number of instances clusteredcorrectly over the total number of instances.
From that wecompute the F?measure, which is two times the precisionand recall, divided by the sum of precision and recall.8 Experimental ResultsWe present the discrimination results for six configura-tions of features, context representations and clusteringalgorithms.
These were run on each of the 27 targetwords, and also on the five mixed words.
What follows isa concise description of each configuration. PB1 : First order context vectors, using co?occurrence features, are clustered in similarity spaceusing the UPGMA technique. PB2 : Same as PB1, except that the first order con-text vectors are clustered in vector space using Re-peated Bisections. PB3: Same as PB1, except the first order con-text vectors used bigram features instead of co?occurrences.All of the PB experiments use first order context repre-sentations that correspond to the approach suggested byPedersen and Bruce. SC1: Second order context vectors of instances wereclustered in vector space using the Repeated Bisec-tions technique.
The context vectors were createdfrom the word co?occurrence matrix whose dimen-sions were reduced using SVD. SC2: Same as SC1 except that the second order con-text vectors are converted to a similarity matrix andclustered using the UPGMA method. SC3: Same as SC1, except the second order contextvectors were created from the bigram matrix.All of the SC experiments use second order contextvectors and hence follow the approach suggested bySchu?tze.Experiment PB2 clusters the Pedersen and Bruce style(first order) context vectors using the Schu?tze like cluster-ing scheme, while SC2 tries to see the effect of using thePedersen and Bruce style clustering method on Schu?tzestyle (second order) context vectors.
The motivation be-hind experiments PB3 and SC3 is to try bigram featuresin both PB and SC style context vectors.The F?measure associated with the discrimination ofeach word is shown in Table 1.
Any score that is sig-nificantly greater than the majority sense (according to apaired t?test) is shown in bold face.9 Analysis and DiscussionWe employ three different types of data in our experi-ments.
The SENSEVAL-2 words have a relatively smallnumber of training and test instances (around 50-200).However, the Line, Hard and Serve data is much larger,word.pos TRN TST S PB1 SC1 PB2 SC2 PB3 SC3 MAJart.n 159 83 4 37.97 45.52 45.46 46.15 43.03 55.34 46.32authority.n 168 90 4 38.15 51.25 43.93 53.01 41.86 34.94 37.76bar.n 220 119 5 34.63 37.23 50.66 40.87 41.05 58.26 45.93channel.n 135 67 6 40.63 37.21 40.31 41.54 36.51 39.06 31.88child.n 116 62 2 45.04 46.85 51.32 50.00 55.17 53.45 56.45church.n 123 60 2 57.14 49.09 48.21 55.36 52.73 46.43 59.02circuit.n 129 75 8 25.17 34.72 32.17 33.33 27.97 25.35 30.26day.n 239 128 3 60.48 46.15 55.65 45.76 62.65 55.65 62.94facility.n 110 56 3 40.00 58.00 38.09 58.00 38.46 64.76 48.28feeling.n 98 45 2 58.23 51.22 52.50 56.10 46.34 53.66 61.70grip.n 94 49 5 45.66 43.01 58.06 53.76 49.46 49.46 46.67material.n 111 65 5 32.79 40.98 41.32 47.54 32.79 47.54 42.25mouth.n 106 55 4 54.90 47.53 60.78 43.14 43.14 47.06 46.97post.n 135 72 5 32.36 37.96 48.17 30.88 30.88 32.36 32.05blind.a 97 53 3 53.06 61.18 63.64 58.43 76.29 79.17 82.46cool.a 102 51 5 35.42 39.58 38.71 34.78 33.68 38.71 42.86fine.a 93 59 5 47.27 47.71 47.71 33.93 38.18 47.71 41.10free.a 105 64 3 48.74 49.54 52.54 55.46 45.00 52.99 49.23natural.a 142 75 4 34.72 35.21 33.56 30.99 32.40 38.03 35.80simple.a 126 64 4 38.33 50.00 47.06 38.33 38.33 47.06 50.75begin.v 507 255 3 59.36 40.46 40.40 43.66 70.12 42.55 64.31leave.v 118 54 5 43.14 38.78 27.73 40.00 46.00 53.47 38.18live.v 112 59 4 37.83 40.00 48.21 45.45 36.37 41.82 57.63train.v 116 56 5 28.57 33.96 28.57 34.28 26.67 32.08 33.93line.n 1615 1197 3 72.67 26.77 62.00 55.47 68.40 37.97 72.10hard.a 2365 1592 2 86.75 67.42 41.18 73.22 87.06 63.41 87.44serve.v 2365 1752 4 40.50 33.20 36.82 34.37 45.66 31.46 40.53cool.a-train.v 197 102 8 22.34 39.00 25.25 40.61 22.57 41.00 22.86fine.a-cool.a 185 104 7 27.86 42.36 33.83 47.72 35.00 42.05 24.79fine.a-grip.n 177 99 7 36.84 49.48 33.50 45.02 31.41 49.48 24.19leave.v-post.n 204 113 8 29.36 48.18 32.11 41.44 23.85 41.82 21.01post.n-grip.n 208 117 8 28.44 43.67 28.44 41.05 26.55 34.21 20.90Table 1: F-measureswhere each contains around 4200 training and test in-stances combined.
Mixed word are unique because theycombined the instances of multiple target words andthereby have a larger number of senses to discriminate.Each type of data brings with it unique characteristics,and sheds light on different aspects of our experiments.9.1 Senseval-2 dataTable 2 compares PB1 against PB3, and SC1 againstSC3, when these methods are used to discriminate the 24SENSEVAL-2 words.
Our objective is to study the effectof using bigram features against co?occurrences in first(PB) and second (SC) order context vectors while usingrelatively small amounts of training data per word.
Notethat PB1 and SC1 use co?occurrence features, while PB3and SC3 rely on bigram features.This table shows the number of nouns (N), adjec-tives (A) and verbs (V) where bigrams were more effec-tive than co-occurrences (bigram>co-occur), less effec-tive (bigram<co-occur), and had no effect (bigram=co-occur).Table 2 shows that there is no clear advantage to us-ing either bigrams or co?occurrence features in first or-der context vectors (PB).
However, bigram features showclear improvement in the results of second order contextvectors (SC).Our hypothesis is that first order context vectors (PB)represent a small set of bigram features since they areselected from the relatively small SENSEVAL-2 words.These features are very sparse, and as such most instancesdo not share many common features with other instances,making first order clustering difficult.N A V7 1 2 bigram>co-occurPB 6 4 2 bigram<co-occur1 1 0 bigram=co-occur9 3 3 bigram>co-occurSC 4 1 1 bigram<co-occur1 2 0 bigram=co-occurTable 2: Bigrams vs. Co-occurrencesN A VPB 9 4 1 rbr>upgma4 0 2 rbr<upgma1 2 1 rbr=upgmaSC 8 1 3 rbr>upgma2 5 0 rbr<upgma4 0 1 rbr=upgmaTable 3: Repeated Bisections vs. UPGMAHowever, second order context vectors indirectly rep-resent bigram features, and do not require an exact matchbetween vectors in order to establish similarity.
Thus,the poor performance of bigrams in the case of first or-der context vectors suggests that when dealing with smallamounts of data, we need to boost or enrich our bigramfeature set by using some other larger training source likea corpus drawn from the Web.Table 3 shows the results of using the Repeated Bi-sections algorithm in vector space (PB) against that ofusing UPGMA method in similarity space.
This ta-ble shows the number of Nouns, Adjectives and VerbsSENSEVAL-2 words that performed better (rbr>upgma),worse (rbr<upgma), and equal (rbr=upgma) when usingRepeated Bisections clustering versus the UPGMA tech-nique, on first (PB) and second (SC) order vectors.In short, Table 3 compares PB1 against PB2 and SC1against SC2.
From this, we observe that with both firstorder and second order context vectors Repeated Bisec-tions is more effective than UPGMA.
This suggests that itis better suited to deal with very small amounts of sparsedata.Table 4 summarizes the overall performance of each ofthese experiments compared with the majority class.
Thistable shows the number of words for which an experi-ment performed better than the the majority class, brokendown by part of speech.
Note that SC3 and SC1 are mostoften better than the majority class, followed closely byPB2 and SC2.
This suggests that the second order con-text vectors (SC) have an advantage over the first ordervectors for small training data as is found among the 24SENSEVAL-2 words.We believe that second order methods work better onN A V TOTALSC3 > MAJ 8 3 1 12SC1 > MAJ 6 2 2 10PB2 > MAJ 7 2 0 9SC2 > MAJ 6 1 2 9PB1 > MAJ 4 1 1 6PB3 > MAJ 3 0 2 5Table 4: All vs.
Majority Classsmaller amounts of data, in that the feature spaces arequite small, and are not able to support the degree of ex-act matching of features between instances that first ordervectors require.
Second order context vectors succeed insuch cases because they find indirect second order co?occurrences of feature words and hence describe the con-text more extensively than the first order representations.With smaller quantities of data, there is less possibil-ity of finding instances that use exactly the same set ofwords.
Semantically related instances use words that areconceptually the same but perhaps not lexically.
Sec-ond order context vectors are designed to identify suchrelationships, in that exact matching is not required, butrather words that occur in similar contexts will have sim-ilar vectors.9.2 Line, Hard and Serve dataThe comparatively good performance of PB1 and PB3 inthe case of the Line, Hard and Serve data (see Table 1)suggests that first order context vectors when clusteredwith UPGMA perform relatively well on larger samplesof data.Moreover, among the SC experiments on this data, theperformance of SC2 is relatively high.
This further sug-gests that UPGMA performs much better than RepeatedBisections with larger amounts of training data.These observations correspond with the hypothesisdrawn from the SENSEVAL-2 results.
That is, a largeamount of training data will lead to a larger feature spaceand hence there is a greater chance of matching more fea-tures directly in the context of the test instances.
Hence,the first order context vectors that rely on the immedi-ate context of the target word succeed as the contexts aremore likely to use similar sets of words that in turn areselected from a large feature collection.9.3 Mix-Word ResultsNearly all of the experiments carried out with the 6 dif-ferent methods perform better than the majority sense inthe case of the mix-words.
This is partially due to the factthat these words have a large number of senses, and there-fore have low majority classifiers.
In addition, recall thatthis data is created by mixing instances of distinct targetwords, which leads to a subset of coarse grained (distinct)senses within the data that are easier to discover than thesenses of a single word.Table 1 shows that the top 3 experiments for each ofthe mixed-words are all second order vectors (SC).
Webelieve that this is due to the sparsity of the feature spacesof this data.
Since there are so many different senses, thenumber of first order features that would be required tocorrectly discriminate them is very high, leading to betterresults for second order vectors.10 Future DirectionsWe plan to conduct experiments that compare the ef-fect of using very large amounts of training data versussmaller amounts where each instance includes the tar-get word (as is the case in this paper).
We will drawour large corpora from a variety of sources, includingthe British National Corpus, the English GigaWord Cor-pus, and the Web.
Our motivation is that the larger cor-pora will provide more generic co?occurrence informa-tion about words without regard to a particular targetword.
However, the data specific to a given target wordwill capture the word usages in the immediate context ofthe target word.
Thus, we will test the hypothesis thata smaller sample of data where each instance includesthe target word is more effective for sense discriminationthan a more general corpus of training data.We are also planning to automatically attach descrip-tive labels to the discovered clusters that capture the un-derlying word sense.
These labels will be created fromthe most characteristic features used by the instances be-longing to the same cluster.
By comparing such descrip-tive features of each cluster with the words that occur inactual dictionary definitions of the target word, we planto carry out fully automated word sense disambiguationthat does not rely on any manually annotated text.11 ConclusionsWe present an extensive comparative analysis of wordsense discrimination techniques using first order and sec-ond order context vectors, where both can be employed insimilarity and vector space.
We conclude that for largeramounts of homogeneous data such as the Line, Hard andServe data, the first order context vector representationand the UPGMA clustering algorithm are the most effec-tive at word sense discrimination.
We believe this is thecase because in a large sample of data, it is very likely thatthe features that occur in the training data will also occurin the test data, making it possible to represent test in-stances with fairly rich feature sets.
When given smalleramounts of data like SENSEVAL-2, second order contextvectors and a hybrid clustering method like Repeated Bi-sections perform better.
This occurs because in small andsparse data, direct first order features are seldom observedin both the training and the test data.
However, the in-direct second order co?occurrence relationships that arecaptured by these methods provide sufficient informationfor discrimination to proceed.12 AcknowledgmentsThis research is supported by a National Science Foun-dation Faculty Early CAREER Development Award(#0092784).All of the experiments in this paper were carried outwith version 0.47 of the SenseClusters package, freelyavailable from the URL shown on the title page.ReferencesS.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41:391?407.F.
Fukumoto and Y. Suzuki.
1999.
Word sense disam-biguation in untagged text based on term weight learn-ing.
In Proceedings of the Ninth Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 209?216, Bergen.T.K.
Landauer, P.W.
Foltz, and D. Laham.
1998.
An in-troduction to latent semantic analysis.
Discourse Pro-cesses, 25:259?284.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsenses in untagged text.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natural Lan-guage Processing, pages 197?207, Providence, RI,August.T.
Pedersen and R. Bruce.
1998.
Knowledge lean wordsense disambiguation.
In Proceedings of the FifteenthNational Conference on Artificial Intelligence, pages800?805, Madison, WI, July.A.
Purandare.
2003.
Discriminating among word sensesusing McQuitty?s similarity analysis.
In Proceedingsof the HLT-NAACL 2003 Student Research Workshop,pages 19?24, Edmonton, Alberta, Canada, May.H.
Schu?tze.
1992.
Dimensions of meaning.
In Pro-ceedings of Supercomputing ?92, pages 787?796, Min-neapolis, MN.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.Y.
Zhao and G. Karypis.
2002.
Evaluation of hierar-chical clustering algorithms for document datasets.
InProceedings of the 11th Conference of Information andKnowledge Management (CIKM), pages 515?524.
