Proceedings of NAACL HLT 2007, pages 340?347,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAn Integrated Approach to Measuring Semantic Similarity between WordsUsing Information available on the WebDanushka BollegalaThe University of Tokyo7-3-1, Hongo, Tokyo,113-8656, Japandanushka@mi.ci.i.u-tokyo.ac.jpYutaka MatsuoNational Institute of AdvancedIndustrial Science andTechnology1-18-13, Sotokanda, Tokyo,101-0021, Japany.matsuo@aist.go.jpMitsuru IshizukaThe University of Tokyo7-3-1, Hongo, Tokyo,113-8656, Japanishizuka@i.u-tokyo.ac.jpAbstractMeasuring semantic similarity betweenwords is vital for various applicationsin natural language processing, such aslanguage modeling, information retrieval,and document clustering.
We propose amethod that utilizes the information avail-able on the Web to measure semantic sim-ilarity between a pair of words or entities.We integrate page counts for each word inthe pair and lexico-syntactic patterns thatoccur among the top ranking snippets forthe AND query using support vector ma-chines.
Experimental results on Miller-Charles?
benchmark data set show that theproposed measure outperforms all the ex-isting web based semantic similarity mea-sures by a wide margin, achieving a cor-relation coefficient of 0.834.
Moreover,the proposed semantic similarity measuresignificantly improves the accuracy (F -measure of 0.78) in a named entity cluster-ing task, proving the capability of the pro-posed measure to capture semantic simi-larity using web content.1 IntroductionThe study of semantic similarity between words hasbeen an integral part of natural language processingand information retrieval for many years.
Semanticsimilarity measures are vital for various applicationsin natural language processing such as word sensedisambiguation (Resnik, 1999), language model-ing (Rosenfield, 1996), synonym extraction (Lin,1998a) and automatic thesaurus extraction (Curran,2002).Pre-compiled taxonomies such as WordNet 1 andtext corpora have been used in previous work on se-mantic similarity (Lin, 1998a; Resnik, 1995; Jiangand Conrath, 1998; Lin, 1998b).
However, seman-tic similarity between words change over time asnew senses and associations of words are constantlycreated.
One major issue behind taxonomies andcorpora oriented approaches is that they might notnecessarily capture similarity between proper namessuch as named entities (e.g., personal names, loca-tion names, product names) and the new uses of ex-isting words.
For example, apple is frequently asso-ciated with computers on the Web but this sense ofapple is not listed in the WordNet.
Maintaining anup-to-date taxonomy of all the new words and newusages of existing words is costly if not impossible.The Web can be regarded as a large-scale, dy-namic corpus of text.
Regarding the Web as a livecorpus has become an active research topic recently.Simple, unsupervised models have shown to per-form better when n-gram counts are obtained fromthe Web rather than from a large corpus (Keller andLapata, 2003; Lapata and Keller, 2005).
Resnik andSmith (2003) extract bilingual sentences from theWeb to create parallel corpora for machine trans-lation.
Turney (2001) defines a point wise mutualinformation (PMI-IR) measure using the number ofhits returned by a Web search engine to recognizesynonyms.
Matsuo et.
al, (2006b) follows a similar1http://wordnet.princeton.edu/340approach to measure the similarity between wordsand apply their method in a graph-based word clus-tering algorithm.Due to the huge number of documents and thehigh growth rate of the Web, it is difficult to di-rectly analyze each individual document separately.Search engines provide an efficient interface to thisvast information.
Page counts and snippets are twouseful information sources provided by most Websearch engines.
Page count of a query is the numberof pages that contain the query words 2.
A snippet isa brief window of text extracted by a search enginearound the query term in a document.
Snippets pro-vide useful information about the immediate contextof the query term.This paper proposes a Web-based semantic simi-larity metric which combines page counts and snip-pets using support vector machines.
We extractlexico-syntactic patterns from snippets.
For exam-ple, X is a Y indicates there is a high semantic sim-ilarity between X and Y.
Automatically extractedlexico-syntactic patterns have been successfully em-ployed in various term extraction tasks (Hearst,1992).Our contributions are summarized as follows:?
We propose a lexico-syntactic patterns-basedapproach to compute semantic similarity usingsnippets obtained from a Web search engine.?
We integrate different Web-based similarityscores using WordNet synsets and support vec-tor machines to create a robust semantic sim-ilarity measure.
The integrated measure out-performs all existing Web-based semantic sim-ilarity measures in a benchmark dataset and anamed entity clustering task.
To the best ofour knowledge, this is the first attempt to com-bine both WordNet synsets and Web content toleverage a robust semantic similarity measure.2 Previous WorkGiven a taxonomy of concepts, a straightforwardmethod for calculating similarity between two words(concepts) is to find the length of the shortest path2page count may not necessarily be equal to the word fre-quency because the queried word may appear many times in apageconnecting the two words in the taxonomy (Radaet al, 1989).
If a word is polysemous (i.e., havingmore than one sense) then multiple paths may ex-ist between the two words.
In such cases only theshortest path between any two senses of the words isconsidered for the calculation of similarity.
A prob-lem frequently acknowledged with this approach isthat it relies on the notion that all links in the taxon-omy represent uniform distances.Resnik (1995) proposes a similarity measurebased on information content.
He defines the sim-ilarity between two concepts C1 and C2 in the tax-onomy as the maximum of the information contentof all concepts C that subsume both C1 and C2.Then the similarity between two words are definedas the maximum of the similarity between any con-cepts that the words belong to.
He uses WordNet asthe taxonomy and information content is calculatedusing the Brown corpus.Li et al, (2003) combines structural semantic in-formation from a lexical taxonomy and informa-tion content from a corpus in a non-linear model.They propose a similarity measure that uses shortestpath length, depth and local density in a taxonomy.Their experiments using WordNet and the Browncorpus reports a Pearson correlation coefficient of0.8914 on the Miller and Charles?
(1998) bench-mark dataset.
They do not evaluate their method onsimilarities between named entities.
Recently, somework has been carried out on measuring semanticsimilarity using web content.
Matsuo et al, (2006a)propose the use of Web hits for the extraction ofcommunities on the Web.
They measure the associ-ation between two personal names using the overlapcoefficient, calculated based on the number of Webhits for each individual name and their conjunction.Sahami et al, (2006) measure semantic similaritybetween two queries using the snippets returned forthose queries by a search engine.
For each query,they collect snippets from a search engine and rep-resent each snippet as a TF-IDF weighted term vec-tor.
Each vector is L2 normalized and the centroidof the set of vectors is computed.
Semantic similar-ity between two queries is then defined as the innerproduct between the corresponding centroid vectors.They do not compare their similarity measure withtaxonomy based similarity measures.Chen et al, (2006) propose a web-based double-341checking model to compute semantic similarity be-tween words.
For two words P and Q, they col-lect snippets for each word from a web search en-gine.
Then they count the number of occurrences ofword P in the snippets for word Q and the numberof occurrences of word Q in the snippets for wordP .
These values are combined non-linearly to com-pute the similarity between P and Q.
This methodheavily depends on the search engine?s ranking al-gorithm.
Although two words P and Q may be verysimilar, there is no reason to believe that one can findQ in the snippets for P , or vice versa.
This observa-tion is confirmed by the experimental results in theirpaper which reports 0 similarity scores for manypairs of words in the Miller and Charles (1998) dataset.3 MethodIn this section we will describe the various similarityfeatures we use in our model.
We utilize page countsand snippets returned by the Google 3 search enginefor simple text queries to define various similarityscores.3.1 Page Counts-based Similarity ScoresFor the rest of this paper we use the notation H(P )to denote the page count for the query P in a searchengine.
Terra and Clarke (2003) compare varioussimilarity scores for measuring similarity betweenwords in a corpus.
We modify the traditional Jac-card, overlap (Simpson), Dice and PMI measuresfor the purpose of measuring similarity using pagecounts.
WebJaccard coefficient between words (orphrases) P and Q, WebJaccard(P,Q), is definedby,WebJaccard(P,Q)={ 0 if H(P ?Q) ?
cH(P?Q)H(P )+H(Q)?H(P?Q) otherwise.
(1)Here, P ?
Q denotes the conjunction query P ANDQ.
Given the scale and noise in the Web, some wordsmight occur arbitrarily, i.e.
by random chance, onsome pages.
Given the scale and noise in web data, itis a possible that two words man order to reduce theadverse effect due to random co-occurrences, we set3http://www.google.comthe WebJaccard coefficient to zero if the page countsfor the query P ?Q is less than a threshold c. 4Likewise, we define WebOverlap coefficient,WebOverlap(P,Q), as,WebOverlap(P,Q)={ 0 if H(P ?Q) ?
cH(P?Q)min(H(P ),H(Q)) otherwise.
(2)We define WebDice as a variant of Dice coeffi-cient.
WebDice(P,Q) is defined as,WebDice(P,Q)={ 0 if H(P ?Q) ?
c2H(P?Q)H(P )+H(Q) otherwise.
(3)We define WebPMI as a variant form of PMI usingpage counts by,WebPMI(P,Q)=??
?0 if H(P ?Q) ?
clog2(H(P?Q)NH(P )NH(Q)N) otherwise .
(4)Here, N is the number of documents indexed by thesearch engine.
Probabilities in Formula 4 are esti-mated according to the maximum likelihood princi-ple.
In order to accurately calculate PMI using For-mula 4, we must know N , the number of documentsindexed by the search engine.
Although estimatingthe number of documents indexed by a search en-gine (Bar-Yossef and Gurevich, 2006) is an interest-ing task itself, it is beyond the scope of this work.
Inthis work, we set N = 1010 according to the numberof indexed pages reported by Google.3.2 Snippets-based Synonymous WordPatternsPage counts-based similarity measures do not con-sider the relative distance between P and Q in a pageor the length of the page.
Although P and Q occurin a page they might not be related at all.
Therefore,page counts-based similarity measures are prone tonoise and are not reliable when H(P ?Q) is low.
Onthe other hand snippets capture the local context ofquery words.
We propose lexico-syntactic patternsextracted from snippets as a solution to the problemswith page counts-based similarity measures.4we set c = 5 in our experiments342To illustrate our pattern extraction algorithm con-sider the following snippet from Google for thequery jaguar AND cat.
?The Jaguar is the largest cat in Western Hemi-sphere and can subdue a larger prey than can thepuma?Here, the phrase is the largest indicates a hy-pernymic relationship between Jaguar and the cat.Phrases such as also known as, is a, part of, is an ex-ample of all indicate various of semantic relations.Such indicative phrases have been successfully ap-plied in various tasks such as synonym extraction,hyponym extraction (Hearst, 1992) and fact extrac-tion (Pasca et al, 2006).We describe our pattern extraction algorithm inthree steps.Step 1We replace the two query terms in a snippet by twowildcards X and Y.
We extract all word n-grams thatcontain both X and Y.
In our experiments we ex-tracted n-grams for n = 2 to 5.
For example, fromthe previous snippet we extract the pattern, X is thelargest X.
In order to leverage the pattern extractionprocess, we randomly select 5000 pairs of synony-mous nouns from WordNet synsets.
We ignore thenouns which do not have synonyms in the WordNet.For nouns with more than one sense, we select syn-onyms from its dominant sense.
For each pair ofsynonyms (P,Q), we query Google for ?P?
AND?Q?
and download the snippets.
Let us call this col-lection of snippets as the positive corpus.
We applythe above mentioned n-gram based pattern extrac-tion procedure and count the frequency of each validpattern in the positive corpus.Step 2Pattern extraction algorithm described in step 1yields 4, 562, 471 unique patterns.
80%of these pat-terns occur less than 10 times in the positive corpus.It is impossible to learn with such a large number ofsparse patterns.
Moreover, some patterns might oc-cur purely randomly in a snippet and are not goodindicators of semantic similarity.
To measure thereliability of a pattern as an indicator of semanticsimilarity we employ the following procedure.
Wecreate a set of non-synonymous word-pairs by ran-domly shuffling the words in our data set of synony-Table 1: Contingency tablev other than v AllFreq.
in positive corpus pv P ?
pv PFreq.
in negative corpus nv N ?
nv Nmous word-pairs.
We check each pair of words inthis newly created data set against WordNet and con-firm that they do not belong to any of the synsetsin the WordNet.
From this procedure we created5000 non-synonymous pairs of words.
For eachnon-synonymous word-pair, we query Google forthe conjunction of its words and download snippets.Let us call this collection of snippets as the nega-tive corpus.
For each pattern generated in step 1, wecount its frequency in the negative corpus.Step 3We create a contingency table as shown in Table 1for each pattern v extracted in step 1 using its fre-quency pv in positive corpus and nv in negative cor-pus.
In Table 1, P denotes the total frequency of allpatterns in the positive corpus and N denotes that inthe negative corpus.Using the information in Table 1, we calculate?2 (Manning and Schu?tze, 2002) value for each pat-tern as,?2 = (P +N)(pv(N ?
nv)?
nv(P ?
pv))2PN(pv + nv)(P +N ?
pv ?
nv) .
(5)We selected the top ranking 200 patterns experimen-tally as described in section 4.2 according to their ?2values.
Some of the selected patterns are shown inTable 2.3.3 TrainingFor each pair of synonymous and non-synonymouswords in our datasets, we count the frequency ofoccurrence of the patterns selected in Step 3.
Wenormalize the frequency count of each pattern bydividing from the total frequency of all patterns.Moreover, we compute the page counts-based fea-tures as given by formulae (1-4).
Using the 200pattern features and the 4 page counts-based fea-tures we create 204 dimensional feature vectors foreach training instance in our synonymous and non-synonymous datasets.
We train a two class supportvector machine (SVM) (Vapnik, 1998), where class343+1 represents synonymous word-pairs and class?1 represents non-synonymous word-pairs.
Finally,SVM outputs are converted to posterior probabilities(Platt, 2000).
We consider the posterior probabilityof a given pair of words belonging to class +1 as thesemantic similarity between the two words.4 ExperimentsTo evaluate the performance of the proposed se-mantic similarity measure, we conduct two sets ofexperiments.
Firstly, we compare the similarityscores produced by the proposed measure againstthe Miller-Charles?
benchmark dataset.
We analyzethe performance of the proposed measure with thenumber of snippets and the size of the training dataset.
Secondly, we apply the proposed measure in areal-world named entity clustering task and measureits performance.4.1 The Benchmark DatasetWe evaluated the proposed method against Miller-Charles (1998) dataset, a dataset of 30 5 word-pairsrated by a group of 38 human subjects.
Word-pairs are rated on a scale from 0 (no similarity) to4 (perfect synonymy).
Miller-Charles?
dataset isa subset of Rubenstein-Goodenough?s (1965) orig-inal dataset of 65 word-pairs.
Although Miller-Charles?
experiment was carried out 25 yearslater than Rubenstein-Goodenough?s, two sets ofratings are highly correlated (Pearson correlationcoefficient=0.97).
Therefore, Miller-Charles ratingscan be considered as a reliable benchmark for eval-uating semantic similarity measures.4.2 Pattern SelectionWe trained a linear kernel SVM with top N patternfeatures (ranked according to their ?2 values) andcalculated the Pearson correlation coefficient againstthe Miller-Charles?
benchmark dataset.
Experimen-tal results are shown in Figure 1.
From Figure 1we select N = 200, where correlation maximizes.Features with the highest linear kernel weights areshown in Table 2 alongside with their ?2 values.
Theweight of a feature in the linear kernel can be consid-ered as a rough estimate of the influence it has on the5Due to the omission of two word-pairs in earlier versionsof WordNet most researchers had used only 28 pairs for evalu-ations0 200 400 600 800 1000120014001600180020000.7800.7820.7840.7860.7880.7900.7920.7940.7960.7980.800CorrelationCoefficient (r)Number of pattern features (N)Figure 1: Correlation vs No of pattern featuresTable 2: Features with the highest SVM linear ker-nel weightsfeature ?2 SVM weightWebDice N/A 8.19X/Y 33459 7.53X, Y : 4089 6.00X or Y 3574 5.83X Y for 1089 4.49X .
the Y 1784 2.99with X ( Y 1819 2.85X=Y 2215 2.74X and Y are 1343 2.67X of Y 2472 2.56final SVM output.
WebDice has the highest linearkernel weight followed by a series of patterns-basedfeatures.
WebOverlap (rank=18, weight=2.45), We-bJaccard (rank=66, weight=0.618) and WebPMI(rank=138, weight=0.0001) are not shown in Table 2due to space limitations.
It is noteworthy that thepattern features in Table 2 agree with the intuition.Lexical patterns (e.g., X or Y, X and Y are, X of Y) aswell as syntactic patterns (e.g., bracketing, commausage) are extracted by our method.4.3 Semantic SimilarityWe score the word-pairs in Miller-Charles datasetusing the page counts-based similarity measures,previous work on web-based semantic similaritymeasures (Sahami (2006), Chen (2006)) and theproposed method (SVM).
Results are shown in Ta-ble 4.3.
All figures except for the Miller-Charlesratings are normalized into [0, 1] range for the easeof comparison 6.
Proposed method (SVM) re-6Pearson correlation coefficient is invariant against a lineartransformation344Table 3: Semantic Similarity of Human Ratings and baselines on Miller-Charles datasetWord Pair Miller- Web Web Web Web Sahami Chen (CODC) ProposedCharles Jaccard Dice Overlap PMI (2006) (2006) (SVM)cord-smile 0.13 0.102 0.108 0.036 0.207 0.090 0 0rooster-voyage 0.08 0.011 0.012 0.021 0.228 0.197 0 0.017noon-string 0.08 0.126 0.133 0.060 0.101 0.082 0 0.018glass-magician 0.11 0.117 0.124 0.408 0.598 0.143 0 0.180monk-slave 0.55 0.181 0.191 0.067 0.610 0.095 0 0.375coast-forest 0.42 0.862 0.870 0.310 0.417 0.248 0 0.405monk-oracle 1.1 0.016 0.017 0.023 0 0.045 0 0.328lad-wizard 0.42 0.072 0.077 0.070 0.426 0.149 0 0.220forest-graveyard 0.84 0.068 0.072 0.246 0.494 0 0 0.547food-rooster 0.89 0.012 0.013 0.425 0.207 0.075 0 0.060coast-hill 0.87 0.963 0.965 0.279 0.350 0.293 0 0.874car-journey 1.16 0.444 0.460 0.378 0.204 0.189 0.290 0.286crane-implement 1.68 0.071 0.076 0.119 0.193 0.152 0 0.133brother-lad 1.66 0.189 0.199 0.369 0.644 0.236 0.379 0.344bird-crane 2.97 0.235 0.247 0.226 0.515 0.223 0 0.879bird-cock 3.05 0.153 0.162 0.162 0.428 0.058 0.502 0.593food-fruit 3.08 0.753 0.765 1 0.448 0.181 0.338 0.998brother-monk 2.82 0.261 0.274 0.340 0.622 0.267 0.547 0.377asylum-madhouse 3.61 0.024 0.025 0.102 0.813 0.212 0 0.773furnace-stove 3.11 0.401 0.417 0.118 1 0.310 0.928 0.889magician-wizard 3.5 0.295 0.309 0.383 0.863 0.233 0.671 1journey-voyage 3.84 0.415 0.431 0.182 0.467 0.524 0.417 0.996coast-shore 3.7 0.786 0.796 0.521 0.561 0.381 0.518 0.945implement-tool 2.95 1 1 0.517 0.296 0.419 0.419 0.684boy-lad 3.76 0.186 0.196 0.601 0.631 0.471 0 0.974automobile-car 3.92 0.654 0.668 0.834 0.427 1 0.686 0.980midday-noon 3.42 0.106 0.112 0.135 0.586 0.289 0.856 0.819gem-jewel 3.84 0.295 0.309 0.094 0.687 0.211 1 0.686Correlation 1 0.259 0.267 0.382 0.548 0.579 0.693 0.834ports the highest correlation of 0.8129 in our ex-periments.
Our implementation of Co-occurrenceDouble Checking (CODC) measure (Chen et al,2006) reports the second best correlation of 0.6936.However, CODC measure reports zero similarity formany word-pairs.
This is because for a word-pair(P,Q), we might not necessarily find Q among thetop snippets for P (and vice versa).
CODC mea-sure returns zero under these conditions.
Sahamiet al (2006) is ranked third with a correlation of0.5797.
Among the four page counts based mea-sures WebPMI reports the highest correlation (r =0.5489).
Overall, the results in Table 4.3 suggestthat snippet-based measures are more accurate thanpage counts-based measures in capturing semanticsimilarity.
This is evident for word-pairs where atleast one of the words is a polysemous word (e.g.,pairs that include cock, brother).
Page counts-basedmeasures do not consider the context in which thewords appear in a page, thus cannot disambiguateTable 4: Comparison with taxonomy based methodsMethod correlationHuman replication 0.901Resnik (1995) 0.745Lin (1998) 0.822Li et al(2003) 0.891Edge-counting 0.664Information content 0.745Jiang & Conrath (1998) 0.848proposed (SVM) 0.834the multiple senses.As summarized in Table 4.3, proposed methodis comparable with the WordNet based methods.In fact, the proposed method outperforms simpleWordNet based approaches such as Edge-Countingand Information Content measures.
However, con-sidering the high correlation between human sub-jects (0.9), there is still room for improvement.Figure 2 illustrates the effect of the numberof snippets on the performance of the proposed3450 100 200 300 400 500 600 700 800 900 10000.700.710.720.730.740.750.760.770.780.790.80CorrelationCoefficientNumber of snippetsFigure 2: Correlation vs No of snippets500  1000  1500  2000  2500  3000  3500  4000negative examples  5001000 15002000 25003000 35004000positive examples0.45 0.50.55 0.60.65 0.70.75 0.80.85correlationFigure 3: Correlation vs No of positive and negativetraining instancesmethod.
Correlation coefficient steadily improveswith the number of snippets used for extracting pat-terns.
When few snippets are processed only a fewpatterns are found, thus the feature vector becomessparse, resulting in poor performance.
Figure 3 de-picts the correlation with human ratings for variouscombinations of positive and negative training in-stances.
Maximum correlation coefficient of 0.834is achieved with 1900 positive training examples and2400 negative training examples.
Moreover, Fig-ure 3 reveals that correlation does not improve be-yond 2500 positive and negative training examples.Therefore, we can conclude that 2500 examples aresufficient to leverage the proposed semantic similar-ity measure.4.4 Named Entity ClusteringMeasuring semantic similarity between named en-tities is vital in many applications such as queryexpansion (Sahami and Heilman, 2006) and com-munity mining (Matsuo et al, 2006a).
Since mostnamed entities are not covered by WordNet, simi-larity measures based on WordNet alne cannot beTable 5: Performance of named entity clusteringMethod Precision Recall F MeasureWebJaccard 0.5926 0.712 0.6147WebOverlap 0.5976 0.68 0.5965WebDice 0.5895 0.716 0.6179WebPMI 0.2649 0.428 0.2916Sahami (2006) 0.6384 0.668 0.6426Chen (2006) 0.4763 0.624 0.4984Proposed 0.7958 0.804 0.7897used in such tasks.
Unlike common English words,named entities are constantly being created.
Manu-ally maintaining an up-to-date taxonomy of namedentities is costly, if not impossible.
The proposedsemantic similarity measure is appealing as it doesnot require pre-compiled taxonomies.
In order toevaluate the performance of the proposed measurein capturing the semantic similarity between namedentities, we set up a named entity clustering task.We selected 50 person names from 5 categories :tennis players, golfers, actors, politicians and scien-tists, (10 names from each category) from the dmozdirectory 7.
For each pair of names in our dataset,we measure the association between the two namesusing the proposed method and baselines.
We usegroup-average agglomerative hierarchical clusteringto cluster the names in our dataset into five clusters.We employed the B-CUBED metric (Bagga andBaldwin, 1998) to evaluate the clustering results.
Assummarized in Table 5 the proposed method outper-forms all the baselines with a statistically significant(p ?
0.01 Tukey HSD) F score of 0.7897.5 ConclusionWe propose an SVM-based approach to combinepage counts and lexico-syntactic patterns extractedfrom snippets to leverage a robust web-based seman-tic similarity measure.
The proposed similarity mea-sure outperforms existing web-based similarity mea-sures and competes with models trained on Word-Net.
It requires just 2500 synonymous word-pairs,automatically extracted from WordNet synsets, fortraining.
Moreover, the proposed method provesuseful in a named entity clustering task.
In future,we intend to apply the proposed method to automat-ically extract synonyms from the web.7http://dmoz.org346ReferencesA.
Bagga and B. Baldwin.
1998.
Entity-based cross doc-ument coreferencing using the vector space model.
InProc.
of 36th COLING-ACL, pages 79?85.Z.
Bar-Yossef and M. Gurevich.
2006.
Random sam-pling from a search engine?s index.
In Proceedings of15th International World Wide Web Conference.H.
Chen, M. Lin, and Y. Wei.
2006.
Novel associationmeasures using web search with double checking.
InProc.
of the COLING/ACL 2006, pages 1009?1016.J.
Curran.
2002.
Ensemble menthods for automatic the-saurus extraction.
In Proc.
of EMNLP.M.A.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
of 14th COLING,pages 539?545.J.J.
Jiang and D.W. Conrath.
1998.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProc.
of the International Conference on Research inComputational Linguistics ROCLING X.F.
Keller and M. Lapata.
2003.
Using the web to ob-tain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3):459?484.M.
Lapata and F. Keller.
2005.
Web-based models ofrnatural language processing.
ACM Transactions onSpeech and Language Processing, 2(1):1?31.D.
Lin.
1998a.
Automatic retreival and clustering of sim-ilar words.
In Proc.
of the 17th COLING, pages 768?774.D.
Lin.
1998b.
An information-theoretic definition ofsimilarity.
In Proc.
of the 15th ICML, pages 296?304.C.
D. Manning and H. Schu?tze.
2002.
Foundations ofStatistical Natural Language Processing.
The MITPress, Cambridge, Massachusetts.Y.
Matsuo, J. Mori, M. Hamasaki, K. Ishida,T.
Nishimura, H. Takeda, K. Hasida, and M. Ishizuka.2006a.
Polyphonet: An advanced social network ex-traction system.
In Proc.
of 15th International WorldWide Web Conference.Y.
Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.2006b.
Graph-based word clustering using web searchengine.
In Proc.
of EMNLP 2006.G.
Miller and W. Charles.
1998.
Contextual correlatesof semantic similarity.
Language and Cognitive Pro-cesses, 6(1):1?28.M.
Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Organizing and searching the world wide webof facts - step one: the one-million fact extraction chal-lenge.
In Proc.
of AAAI-2006.J.
Platt.
2000.
Probabilistic outputs for support vec-tor machines and comparison to regularized likelihoodmethods.
Advances in Large Margin Classifiers, pages61?74.R.
Rada, H. Mili, E. Bichnell, and M. Blettner.
1989.Development and application of a metric on semanticnets.
IEEE Transactions on Systems, Man and Cyber-netics, 9(1):17?30.P.
Resnik and N. A. Smith.
2003.
The web as a parallelcorpus.
Computational Linguistics, 29(3):349?380.P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proc.
of14th International Joint Conference on Aritificial In-telligence.P.
Resnik.
1999.
Semantic similarity in a taxonomy: Aninformation based measure and its application to prob-lems of ambiguity in natural language.
Journal of Ar-itificial Intelligence Research, 11:95?130.R.
Rosenfield.
1996.
A maximum entropy approach toadaptive statistical modelling.
Computer Speech andLanguage, 10:187?228.H.
Rubenstein and J.B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Communications of the ACM,8:627?633.M.
Sahami and T. Heilman.
2006.
A web-based ker-nel function for measuring the similarity of short textsnippets.
In Proc.
of 15th International World WideWeb Conference.E.
Terra and C.L.A.
Clarke.
2003.
Frequency estimatesfor statistical word similarity measures.
In Proc.
of theNAACL/HLT, pages 165?172.P.
D. Turney.
2001.
Minning the web for synonyms:Pmi-ir versus lsa on toefl.
In Proc.
of ECML-2001,pages 491?502.V.
Vapnik.
1998.
Statistical Learning Theory.
Wiley,Chichester, GB.D.
McLean Y. Li, Zuhair A. Bandar.
2003.
An approchfor measuring semantic similarity between words us-ing multiple information sources.
IEEE Transactionson Knowledge and Data Engineering, 15(4):871?882.347
