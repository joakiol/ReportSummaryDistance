Noun-Phrase Analysis in Unrestricted Text for Information RetrievalDavid A. Evans, Chengxiang ZhaiLaboratory for Computational LinguisticsCarnegie Mellon UniveristyPittsburgh, PA 15213dae@cmu.edu, cz25@andrew.cmu.eduAbstractInformation retrieval is an important ap-plication area of natural-language pro-cessing where one encounters the gen-uine challenge of processing large quanti-ties of unrestricted natural-language t xt.This paper reports on the application of afew simple, yet robust and efficient noun-phrase analysis techniques to create bet-ter indexing phrases for information re-trieval.
In particular, we describe a hy-brid approach to the extraction of mean-ingful (continuous or discontinuous) ub-compounds from complex noun phrasesusing both corpus statistics and linguisticheuristics.
Results of experiments showthat indexing based on such extracted sub-compounds improves both recall and pre-cision in an information retrieval system.The noun-phrase analysis techniques arealso potentially useful for book indexingand automatic thesaurus extraction.1 Introduct ion1.1 Information RetrievalInformation retrieval (IR) is an important applica-tion area of naturaManguage processing (NLP).
1The IR (or perhaps more accurately "text retrieval")task may be characterized as the problem of select-ing a subset of documents (from a document col-lection) whose content is relevant o the informa-tion need of a user as expressed by a query.
Thedocument collections involved in IR are often gi-gabytes of unrestricted natural-language t xt.
Auser's query may be expressed in a controlled lan-guage (e.g., a boolean expression of keywords) or,more desirably, anatural anguage, such as English.A typical IR system works as follows.
The doc-uments to be retrieved are processed to extract in-dexing terms or content carriers, which are usually(Evans, 1990; Evans et al, 1993; Smeaton, 1992; Lewis& Sparck Jones, 1996)single words or (less typically) phrases.
The index-ing terms provide a description of the document'scontent.
Weights are often assigned to terms to in-dicate how well they describe the document.
A(natural-language) query is processed in a similarway to extract query terms.
Query terms are thenmatched against he indexing terms of a documentto determine the relevance of each document to thequer3aThe ultimate goal of an IR system is to increaseboth precision, the proportion of retrieved docu-ments that are relevant, as well as recall, the propor-tion of relevant document that are retrieved.
How-ever, the real challenge is to understand and rep-resent appropriately the content of a document andquer~ so that the relevance decision can be made ef-ficiently, without degrading precision and recall.
Atypical solution to the problem of making relevancedecisions efficient is to require exact matching of in-dexing terms and query terms, with an evaluationof the 'hits' based on a scoring metric.
Thus, forinstance, in vector-space models of relevance rank-ing, both the indexing terms of a document and thequery terms are treated as vectors (with individualterm weights) and the similarity between the twovectors is given by a cosine-distance measure, es-sentially the angle between any two vectors?1.2 Natural-Language Processing for IROne can regard almost any IR system as perform-ing an NLP task: text is 'parsed" for terms andterms are used to express 'meaning'--to capturedocument content.
Clearly, most traditional IR sys-tems do not attempt to find structure in the natural-language text in the 'parsing' process; they merelyextract word-like strings to use in indexing.
Ide-ally, however, extracted structure would directly re-flect the encoded linguistic relations among terms--captuing the conceptual content of the text betterthan simple word-strings.There are several prerequisites for effective NLPin an IR application, including the following.2 (Salton & McGill, 1983)171.
Ability to process large amounts of textThe amount of text in the databases accessed bymodem IR systems i  typically measured in gi-gabytes.
This requires that the NLP used mustbe extraordinarily efficient in both its time andspace requirements.
It would be impracticalto use a parser with the speed of one or twosentences per second.2.
Ability to process unrestricted textThe text database for an IR task is generallyunrestricted natural-language text possibly en-compassing many different domains and top-ics.
A parser must be able to manage the manykinds of problems one sees in natural-languagecorpora, including the processing of unknownwords, proper names, and unrecognized struc-tures.
Often more is required, as when spelling,transcription, or OCR errors occur.
Thus, theNLP used must be especially robust.3.
Need for shallow understandingWhile the large amount of unrestricted textmakes NLP more difficult for IR, the fact thata deep and complete understanding of the textmay not be necessary for IR makes NLP for IRrelatively easier than other NLP tasks such asmachine translation.
The goal of an IR systemis essentially to classify documents (as relevantor irrelevant) vis-a-vis a query.
Thus, it maysuffice to have a shallow and partial represen-tation of the content of documents.Information retrieval thus poses the genuine chal-lenge of processing large volumes of unrestrictednatural-language text but not necessarily at a deeplevel.1.3 Our WorkThis paper reports on our evaluation of the use ofsimple, yet robust and efficient noun-phrase analy-sis techniques to enhance phrase-based IR.
In par-ticular, we explored an extension of the ~phrase-based indexing in the CLARIT TM system ?
usinga hybrid approach to the extraction of meaning-ful (continuous or discontinuous) subcompoundsfrom complex noun phrases exploiting both corpus-statistics and linguistic heuristics.
Using such sub-compounds rather than whole noun phrases as in-dexing terms helps a phrase-based IR system solvethe phrase normalization problem, that is, the prob-lem of matching syntactically different, but semanti-cally similar phrases.
The results of our experimentsshow that both recall and precision are improved byusing extracted subcompounds for indexing.2 Phrase-Based IndexingThe selection of appropriate indexing terms is criti-cal to the improvement of both precision and recallin an IR task.
The ideal indexing terms would di-rectly represent the concepts in a document.
Since'concepts' are difficult to represent and extract (aswell as to define), concept-based indexing is anelusive goal.
Virtually all commercial IR systems(with the exception of the CLARIT system) indexonly on "words', since the identification of words intexts is typically easier and more efficient han theidentification of more complex structures.
How-ever, single words are rarely specific enough to sup-port accurate discrimination and their groupingsare often accidental.
An often cited example is thecontrast between "junior college" and "college ju-nior".
Word-based indexing cannot distinguish thephrases, though their meanings are quite different.Phrase-based indexing, on the other hand, as a steptoward the ideal of concept-based indexing, can ad-dress such a case directly.Indeed, it is interesting to note that the useof phrases as index terms has increased ramat-ically among the systems that participate in theTREC evaluations.
~ Even relatively traditionalword-based systems are exploring the use of multi-word terms by supplementing words with sta-tistical phrases--selected high frequency adjacentword pairs (bigrams).
And a few systems, suchas CLARIT--which uses simplex noun phrases,attested subphrases, and contained words as in-dex terms--and New York University's TRECsystemS--which uses "head-modifier pairs" de-rived from identified noun phrases--have demon-strated the practicality and effectiveness of thor-ough NLP in IR tasks.The experiences of the CLAR1T system are in-structive.
By using selective NLP to identify sim-plex NPs, CLARIT generates phrases, subphrases,and individual words to use in indexing documentsand queries.
Such a first-order analysis of the lin-guistic structures in texts approximates conceptsand affords us alternative methods for calculatingthe fit between documents and queries.
In particu-lar, we can choose to treat some phrasal structuresas atomic units and others as additional informa-tion about (or representations of)content.
There areimmediate ffects in improving precision:1.
Phrases can replace individual indexing words.For example, if both "dog" and "hot" are usedfor indexing, they will match any query inwhich both words occur.
But if only the phrase"hot dog" is used as an index term, then it willonly match the same phrase, not any of the in-dividual words.3(Evans et al, 1991; Evans et al, 1993; Evans et al,1995; Evans et al, 1996)4 (Harman, 1995; Harman, 1996)5 (Strzalkowski, 1994)182.
Phrases can supplement word-level matches.For example, if only the individual words "ju-nior" and "college" are used for indexing, both"junior college" and "college junior" will matcha query with the phrase "junior college" equallywell.
But if we also use the phrase "junior col-lege" for indexing, then "junior college" willmatch better than "college junior", even thoughthe latter also will receive some credit as amatch at the word level.We can see, then, that it is desirable to distinquish--and, if possible, extract--two kinds of phrases:those that behave as lexical atoms and those that re-flect more general linguistic relations.Lexical atoms help us by obviating the possibilityof extraneous word matches that have nothing todo with true relevance.
We do not want "hot" or"dog" to match on "hot dog".
In essence, we want toeliminate the effect of the independence assumptionat the word level by creating new words--the lexicalatoms--in which the individual word dependenciesare explicit (structural).More general phrases help us by adding detail.Indeed, all possible phrases (or paraphrases) of ac-tual content in a document are potentially valuablein indexing.
In practice, of course, the indexingterm space has to be limited, so it is necessary to se-lect a subset of phrases for indexing.
Short phrases(often nominal compounds) are preferred over longcomplex phrases, because short phrases have bet-ter chances for matching short phrases in queriesand will still match longer phrases owing to theshort phrases they have in common.
Using onlyshort phrases also helps solve the phrase normal-ization problem of matching syntactically differentlong phrases (when they share similar meaning).
6Thus, lexical atoms and small nominal com-pounds should make good indexing phrases.While the CLARIT system does index at the levelof phrases and subphrases, it does not currentlyindex on lexical atoms or on the small compoundsthat can be derived from complex NPs, in particular,reflecting cross-simplex NP dependency relations.Thus, for example, under normal CLARIT process-ing the phrase "the quality of surface of treatedstainless teel strip "7 would yield index terms suchas "treated stainless teel strip", "treated stainlesssteel", "stainless teel strip", and "stainless teel"(as a phrase, not lexical atom), along with all therelevant single-word terms in the phrase.
But theprocess would not identify "stainless teel" as a po-tential exical atom or find terms such as "surfacequality", "strip surface", and "treated strip".To achieve more complete (and accurate) phrase-based indexing, we propose to use the following6 (Smeaton, 1992)ZThis is an actual example from a U.S. patentdocument.four kinds of phrases as indexing terms:1.
Lexical atoms (e.g., "hot dog" or2.3.4.perhaps"stainless teel" in the example above)Head modifier pairs (e.g., "treated strip" and"steel strip" in the example above)Subcompounds (e.g., "stainless teel strip" inthe example above)Cross-preposition modification pairs (e.g.,"surface quality" in the example above)In effect, we aim to augment CLARIT indexing withlexical atoms and phrases capturing additional (dis-continuous) modification relations than those thatcan be found within simplex NPs.It is clear that a certain level of robust and effi-cient noun-phrase analysis is needed to extract heabove four kinds of small compounds from a largeunrestricted corpus.
In fact, the set of small com-pounds extracted from a noun phrase can be re-garded as a weak representation f the meaning ofthe noun phrase, since each meaningful small com-pound captures a part of the meaning of the nounphrase.
In this sense, extraction of such small com-pounds is a step toward a shallow interpretationof noun phrases.
Such weak interpretation is use-ful for tasks like information retrieval, documentclassification, and thesaurus extraction, and indeedforms the basis in the CLARIT system for automatedthesaurus discovery.3 MethodologyOur task is to parse text into NPs, analyze the nounphrases, and extract he four kinds of small com-pounds given above.
Our emphasis is on robustand efficient NLP techniques to support large-scaleapplications.For our purposes, we need to be able to identifyall simplex and complex NPs in a text.
ComplexNPs are defined as a sequence of simplex NPs thatare associated with one another via prepositionalphrases.
We do not consider simplex NPs joined byrelative clauses.Our approach to NLP involves a hybrid use ofcorpus statistics upplemented bylinguistic heuris-tics.
We assume that there is no training data (mak-ing the approach more practically useful) and, thus,rely only on statistical information in the documentdatabase itself.
This is different from many cur-rent statistical NLP techniques that require a train-ing corpus.
The volume of data we see in IR tasksalso makes it impractical to use sophisticated statis-tical computations.The use of linguistic heuristics can assist statis-tical analysis in several ways.
First, it can focusthe use of statistics by helping to eliminate irrele-vant structures from consideration.
For example,syntactic ategory analysis can filter out impossible19word modification pairs, such as \[adjective, adjec-tive\] and \[noun, adjective\].
Second, it may improvethe reliability of statistical decisions.
For example,the counting ofbigrams that occur only within nounphrases is more reliable for lexical atom discoverythan the counting of all possible bigrams that occurin the corpus.
In addition, syntactic ategory anal-ysis is also helpful in adjusting cutoff parametersfor statistics.
For example, one useful heuristic isthat we should use a higher threshold of reliability(evidence) for accepting the pair \[adjective, noun\]as a lexical atom than for the pair \[noun, noun\]: anoun-noun pair is much more likely to be a lexicalatom than an adjective-noun o e.The general process of phrase generation is illus-trated in Figure 1.
We used the CLARIT NLP mod-ule as a preprocessor toproduce NPs with syntacticcategories attached to words.
We did not attemptto utilize CLARIT complex-NP generation or sub-phrase analysis, since we wanted to focus on thespecific techniques for subphrase discovery that wedescribe in this paper.I Raw Text~Np CLARITExtractor INPsNP Parser ~I ' ~(  Lexical Atoms 9 / Structured/~k Attested Terms,NPs / ~Subcompound /Generator /Meaningful SubcompoundsFigure 1: General Processing for Phrase GenerationAfter preprocessing, the system works in twostages--parsing and generation.
In the parsingstage, each simplex noun phrase in the corpus isparsed.
In the generation stage, the structured nounphrase is used to generate candidates for all fourkinds of small compounds, which are further testedfor occurrence (validity) in the corpus.Parsing of simplex noun phrases is done in mul-tiple phases.
At each phase, noun phrases are par-tially parsed, then the partially parsed structures areused as input to start another phase of partial pars-ing.
Each phase of partial parsing is completed byconcatenating those most reliable modification pairstogether to form a single unit.
The reliability of amodification pair is determined by a score basedon frequency statistics and category analysis andis further tested via local optimum phrase analysis(described below).
Lexical atoms are discovered atthe same time, during simplex noun phrase parsing.Phrase generation is quite simple.
Once the struc-ture of a noun phrase (with marked lexical atoms)is known, the four kinds of small compounds canbe easily produced.
Lexical atoms are already avail-able.
Head-modifier pairs can be extracted based onthe modification relations implied by the structure.Subcompounds are just the substructures of the NP.Cross-preposition pairs are generated by enumerat-ing all possible pairs of the heads of each simplexNP within a complex NP in backward order.
8To validate discontinuous compounds uch asnon-sequential head-modifier pairs and cross-preposition pairs, we use a standard technique ofCLARIT processing, viz., we test any nominatedcompounds against he corpus itself.
If we findindependently attested (whole) simplex NPs thatmatch the candidate compounds, we accept thecandidates as index terms.
Thus for the NP "thequality of surface of treated stainless teel strip",the head-modifier pairs "treated strip", "stain-less steel", "stainless trip", and "steel strip", andthe cross-preposition pairs "strip surface", "surfacequality", and "strip quality", would be generatedas index terms only if we found independent evi-dence of such phrases in the corpus in the form offree-standing simplex NPs.3.1 Lexical Atom DiscoveryA lexical atom is a semantically coherent phraseunit.
Lexical atoms may be found among propernames, idioms, and many noun-noun compounds.Usually they are two-word phrases, but sometimesthey can consist of three or even more words, asin the case of proper names and technical terms.Examples of lexical atoms (in general English) are"hot dog", "tear gas", "part of speech", and "yonNeumann".However, recognition of lexical atoms in free textis difficult.
In particular, the relevant lexical atomsfor a corpus of text will reflect he various discoursedomains encompassed by the text.
In a collectionof medical documents, for example, "Wilson's dis-ease" (an actual rheumatological disorder) may beused as a lexical atom, whereas in a collection ofgeneral news stories, "Wilson's disease" (referenceto the disease that Wilson has) may not be a lexi-cal atom.
Note that in the case of the medical us-age, we would commonly find "Wilson's disease"as a bigram and we would not find, for example,8 (Schwarz, 1990) reports asimilar strategy.2O"Wilson's severe disease" as a phrase, though thelatter might well occur in the general news corpus.This example serves to illustrate the essential obser-vation that motivates our heuristics for identityinglexical atoms in a corpus: (1) words in lexical atomshave strong association, and thus tend to co-occuras a phrase and (2) when the words in a lexical atomco-occur in a noun phrase, they are never or rarelyseparated.The detection of lexical atoms, like the parsingof simplex noun phrases, is also done in multiplephases.
At each phase, only two adjacent unitsare considered.
So, initiall~ only two-word lexicalatoms can be detected.
But, once a pair is deter-mined to be a lexical atom, it will behave exactlylike a single word in subsequent processing, so, inlater phases, atoms with more than two words canbe detected.Suppose the pair to test is \[W1, W2\].
The firstheuristic is implemented by requiring the frequencyof the pair to be higher than the frequency of anyother pair that is formed by either word with otherwords in common contexts (within a simplex nounphrase).
The intuition behind the test is that (1) ingeneral, the high frequency of a bigram in a simplenoun phrase indicates trong association and (2) wewant to avoid the case where \[W1, W2\] has a highfrequency, but \[W1, W2, W\] (or \[W, W1, W2\]) has aneven higher frequency, which implies that W2 (orW1) has a stronger association with W than withW1 (or W2, respectively).
More precisely, we re-quire the following:F(W~, W2) > Maa:LDF(W~, W2)andF(W~, W2) > Ma3:RDF(W1, W2)Where,MaxLDF(W1, W2) =Maxw( U in( F(W, W1), DF(W, W2)))andMaxRDF(W1, W2) =Maxw( U in( DF(W1, W), F(W2, W) ) )W is any context word in a noun phrase and F(X, Y)and DF(X, Y) are the continuous and discontin-uous frequencies of \[X, Y\], respective135 within asimple noun phrase, i.e., the frequency of patterns\[...X, Y...\] and patterns \[...X, ..., Y...\], respectively.The second heuristic requires that we record allcases where two words occur in simplex NPs andcompare the number of times the words occur asa strictly adjacent pair with the number of timesthey are separated.
The second heuristic is simplyimplemented by requiring that F(W1, W2) be muchhigher than DF(W1, W2) (where 'higher' is deter-mined by some threshold).Syntactic ategory analysis also helps filter outimpossible lexical atoms and establish the thresh-21old for passing the second test.
Only the follow-ing category combinations are allowed for lexicalatoms: \[noun, noun\], \[noun, lexatom\], \[lexatom,noun\], \[adjective, noun\], and \[adjective, lexatom\],where "lexatom" is the category for a detected lexi-cal atom.
For combinations other than \[noun, noun\],the threshold for passing the second test is high.In practice, the process effectively nominatesphrases that are true atomic concepts (in a par-ticular domain of discourse) or are being usedso consistently as unit concepts that they can besafely taken to be lexical atoms.
For example, thelexical atoms extracted by this process from theCACM corpus (about 1 MB) include "operatingsystem", "data structure", "decision table", "database", "real time", "natural anguage", "on line","least squares", "numerical integration", and "fi-nite state automaton", among others.3.2 Bottom-Up Association-Based ParsingExtended simplex noun-phrase parsing as devel-oped in the CLARIT system, which we exploit in ourprocess, works in multiple phases.
At each phase,the corpus is parsed using the most specific (i.e.,recently created) lexicon of lexical atoms.
New lex-ical atoms (results) are added to the lexicon and arereused as input to start another phase of parsinguntil a complete parse is obtained for all the nounphrases.The idea of association-based parsing is that bygrouping words together (based on association)many times, we will eventually discover the mostrestrictive (and informative) structure of a nounphrase.
For example, if we have evidence from thecorpus that "high performance" is a more reliableassociation and "general purpose" a less reliableone, then the noun phrase "general purpose highperformance computer" (an actual example fromthe CACM corpus) would undergo the followinggrouping process:general purpose high performance omputer =~general purpose \[high=performance\] computer =~\[general=purpose\] \[high=performance\] computer =~\[general=purpose\] \[\[high=performance\]=computer\] =~\[\[general=purpose\]=\[\[high=performance\]=computer\]\]Word pairs are given an association score (S) ac-cording to the following rules.
Scores provide ev-idence for groupings in our parsing process.
Notethat a smaller score means a stronger association.1.
Lexical atoms are given score 0.
This gives thehighest priority to lexical atoms.2.
The combination of an adverb with an adjec-tive, past participle, or progressive verb is givenscore 0.3.
Syntactically impossible pairs are given score100.
This assigns the lowest priority to thosepairs filtered out by syntactic ategory analysis.The 'impossible' combinations include pairssuch as \[noun, adjective\], [noun, adverb\], \[ad-jective, adjective\], \[past-participle, adjective\],\[past-participle, adverb\], and \[past-participle,past-participle\], among others.4.
Other pairs are scored according to the formu-las given in Figure 2.
Note the following effectsof the formulas:When /;'(W1,W2) increases, S(W1,W2) de-creases;When DF(W1, W2) increases, S(Wx, W2) de-creases;When AvgLDF(W~, W2) or AvgRDF(W~, W2)increases, S(W1, W2) increases; andWhen F(Wx) -  F(W1,W2) or F(W2) -F(W1, W2) increases, S(W1, W2) decreases.S(W1 W2)= I+LDF(W,,W2)+RDF(W1,W=) A(W1,W2) XlxF(W1,W2)+DF(W1,W,~) XMin(F(W, W1),DF(W,W',)) AvgLDF(Wa, W2) = ~-..,WeLD ILD\[5-" Min( F( W2,W),D F( W1,W)) AvgRDF(W1, W2) = ~-..,WCRD IRDIA(W1, W2 ) = ~ F(W1)+F(W2)--2?F(WI,W2)+X2Where?
F(W) is frequency of word W?
F(W1, W2) is frequency of adjacent bigram \[W1,W2\](i.e ..... W1 W2 ...)?
DF(W1, W2) is frequency of discontinuous bigram\[W1,W21 (i.e ..... W1...W2...)?
LD is all left dependents, i.e.,{W\]min(F(W, l), DF(W, W2)) ~ 0}?
RD is all right dependents, i.e.,{WJmin( D F(W1, W), F(W2, W) ) ?
0}?
),1 is the parameter indicating the relative contribu-tion of F(W1,W2) to the score (e.g., 5 in the actualexperiment)?
A2 is the parameter to control the contribution ofword frequency (e.g., 1000 in the actual experiment)Figure 2: Formulas for ScoringThe association score (based principally on fre-quency) can sometimes be unreliable.
For example,if the phrase "computer aided design" occurs fre-quently in a corpus, "aided design" may be judgeda good association pair, even though "computeraided" might be a better pair.
A problem may arisewhen processing a phrase such as "program aideddesign": if "program aided" does not occur fre-quently in the corpus and we use frequency as theprincipal statistic, we may (incorrectly) be led toparse the phrase as "\[program (aided design)\]".One solution to such a problem is to recomputethe bigram occurrence statistics after making eachround of preferred associations.
Thus, using the ex-ample above, if we first make the association "com-puter aided" everywhere it occurs, many instancesof "aided design" will be removed from the corpus.Upon recalculation of the (free) bigram statistics,"aided design" will be demoted in value and thefalse evidence for "aided design" as a preferred as-sociation in some contexts will be eliminated.The actual implementation of such a scheme re-quires multiple passes over the corpus to generatephrases.
The first phrases chosen must always bethe most reliable.
To aid us in making such decisionswe have developed a metric for scoring preferredassociations in their local NP contexts.To establish a preference metric, we use two statis-tics: (1) the frequency of the pair in the corpus,F(W1, W2), and (2) the number of the times thatthe pair is locally dominant in any NP in which thepair occurs.
A pair is locally dominant in an NPiff it has a higher association score than either ofthe pairs that can be formed from contiguous otherwords in the NP.
For example, in an NP with the se-quence \[X, Y, g\], we compare S(X, Y) with S(Y, g);whichever is higher is locally dominant.
The prefer-ence score (PS) for a pair is determined by the ratioof its local dominance count (LDC)--the total num-ber of cases in which the pair is locally dominant--toits frequency:LDC(WI 1W2) PS(W1, W2) = r(Wl,W~)By definition all two-word NIPs score their pairsas locally dominant.In general, in each processing phase we make onlythose associations in the corpus where a pair's PSis above a specified threshold.
If more than one as-sociation is possible (above theshold) in a particularNP, we make all possible associations, but in orderof PS: the first grouping oes to the pair with high-est PS, and so on.
In practice, we have used 0.7 asthe threshold for most processing phases.
94 ExperimentWe tested the phrase extraction system (PES) by us-ing it to index documents in an actual retrieval task.In particular, we substituted the PES for the defaultNLP module in the CLARIT system and then in-dexed a large corpus using the terms nominated bythe PES, essentially the extracted small compoundsand single words (but not words within a lexi-cal atom).
All other normal CLARIT processing--weighting of terms, division of documents intosubdocuments (passages), vector-space modeling,etc.--was used in its default mode.
As a baseline?When the phrase data becomes sparse, e.g., after sixor seven iterations of processing, it is desirable to reducethe threshold.22for comparison, we used standard CLARIT process-ing of the same corpus, with the NLP module set toreturn full NPs and their contained words (and nofurther subphrase analysis).l 0The corpus used is a 240-megabyte collectionof Associated Press newswire stories from 1989(AP89), taken from the set of TREC corpora.
Thereare about 3-million simplex NPs in the corpus andabout 1.5-million complex NPs.
For evaluation,we used TREC queries 51-100, ll each of whichis a relatively long description of an informationneed.
Queries were processed by the PES and nor-mal CLARIT NLP modules, respectively, to gener-ate query terms, which were then used for CLARITretrieval.To quantify the effects of PES processing, we usedthe standard IR evaluation measures of recall andprecision.
Recall measures how many of the rele-vant documents have been actually retrieved.
Pre-cision measures how many of the retrieved ocu-ments are indeed relevant.
For example, if the totalnumber of relevant documents i N and the systemreturns M documents of which K are relevant, then,Recall = K IVandPrecision = ~-.We used the judged-relevant documents from theTREC evaluations as the gold standard in scoringthe performance of the two processes.suggests that the PES could be used to support otherIR enhancements, such as automatic feedback of thetop-returned documents to expand the initial queryfor a second retrieval step)  2CLARIT Retrieved-Rel Total-Rel RecallBaseline 2,668 3,304 80.8%PES 2,695 3,304 81.6%Table 1: Recall ResultsBaseline Rel.Improvement0.6819 4%Recall PES0.00 0.70990.10 0.5535 0.57300.20 0.4626 0.49270.30 0.4098 0.43290.40 0.3524 0.37820.50 0.3289 0.33170.60 0.2999 0.30260.70 0.2481 0.24580.80 0.1860 0.19660.90 0.1190 0.14481.00 0.0688 0.06533.5%6.5%5.6%7.0%0.5%0.9%--0.9%5.7%21.7%-5.0%Table 2: Interpolated Precision Results5 ResultsThe results of the experiment are given in Tables 1,2, and 3.
In general, we see improvement in bothrecall and precision.Recall improves lightly (about 1%), as shown inTable 1.
While the actual improvement is not sig-nificant for the run of fifty queries, the increase inabsolute numbers of relevant documents returnedindicates that the small compounds upported bet-ter matches in some cases.Interpolated precision improves ignificantly5 asshown in Table 2.
The general improvement inprecision indicates that small compounds providemore accurate (and effective) indexing terms thanfull NPs.Precision improves at various returned-docu-ment levels, as well, as shown in Table 3.
Initialprecision, in particular, improves ignificantly.
This1?Note that he CLARIT process used as a baseline doesnot reflect optimum CLARIT performance, .g., as ob-tained in actual TREC evaluations, ince we did not use avariety of standard CLARIT techniques that significantlyimprove performance, such as automatic query expan-sion, distractor space generation, subterm indexing, ordifferential query-term weighting.
Cf.
(Evans et al, 1996)for details.1 ~ (Harman, 1993)Do, c-Level5 docs10 docs15 docs20 docs30 docs100 docs200 docs500 docs1000 docsBaseline PES Rel.Improvement0.4255 0.4809 13%0.4170 0.4426 6%0.3943 0.4227 7%0.3819 0.3957 4%0.3539 0.3603 2%0.2526 0.2553 1%0.1770 0.1844 4%0.0973 0.0994 2%0.0568 0.0573 1%Table 3: Precision at Various Document LevelsThe PES, which was not optimized for pro-cessing, required approximately 3.5 hours per 20-megabyte subset of AP89 on a 133-MHz DEC alphaprocessor) 3 Most processing time (more than 2 ofevery 3.5 hours) was spent on simplex NP parsing.Such speed might be acceptable in some, smaller-scale IR applications, but it is considerably slowerthan the baseline speed of CLARIT noun-phraseidentification (viz., 200 megabytes per hour on a100-MIPS processor).l~ (Evans et al, 1995; Evans et al, 1996)13Note that the machine was not dedicated to the PESprocessing; other processes were running simultaneously.236 ConclusionsThe notion of association-based parsing dates atleast from (Marcus, 1980) and has been exploredagain recently by a number of researchers.
TM Themethod we have developed iffers from previouswork in that it uses linguistic heuristics and local-ity scoring along with corpus statistics to generatephrase associations.The experiment contrasting the PES with baselineprocessing in a commercial IR system demonstratesa direct, positive effect of the use of lexical atoms,subphrases, and other pharase associations acrosssimplex NPs.
We believe the use of N-P-substructureanalysis can lead to more effective information man-agement, including more precise IR, text summa-rization, and concept clustering.
Our future workwill explore such applications of the techniques wehave described in this paper.7 AcknowledgementsWe received helpful comments from Bob Carpen-ter, Christopher Manning, Xiang Tong, and SteveHanderson, who also provided us with a hash tablemanager that made the implementation easier.
Theevaluation of the experimental results would havebeen impossible without he help of Robert Leffertsand Nata~a Mili4-Frayling at CLARITECH Corpo-ration.
Finally, we thank the anonymous reviewersfor their useful comments.ReferencesDavid A. Evans.
1990.
Concept management i  text vianatural-language processing: The CLARIT approach.In: Working Notes of the 1990 AAAI Symposium on "Text-Based Intelligent Systems", Stanford University, March,27-29, 1990, 93-95.David A. Evans, Kimberly Ginther-Webster, Mary Hart,Robert G. Lefferts, Ira A. Monarch.
1991.
Automaticindexing using selective NLP and first-order thesauri.In: A. Lichnerowicz (ed.
), Intelligent Text and Image Han-dling.
Proceedings ofa Conference, RIAO "91.
Amsterdam,NL: Elsevier, pp.
624-644.David A. Evans, Robert G. Lefferts, Gregory Grefenstette,Steven K. Handerson, William R. Hersh, and Armar A.Archbold.
1993.
CLARIT TREC design, experiments,and results.
In: Donna K. Harman (ed.
), The First TextREtrieval Conference (TREC-1).
NIST Special Publication500-207.
Washington, DC: U.S. Government PrintingOffice, pp.
251-286; 494-501.David A. Evans, and Robert G. Lefferts.
1995.
CLARIT-TREC experiments Information Processing and Manage-ment, Vol.
31, No.
3, 385-395.David A. Evans, Nata~a Mili4-Frayling, Robert G. Lef-ferts.
1996.
CLARIT TREC-4 experiments.
In: Donna14 (Liberman et al, 1992; Pustejovsky et al, 1993; Resniket al, 1993; Lauer, 1995)K. Harman (ed.
), The Fourth Text REtrieval Conference(TREC-4).
NIST Special Publication.
Washington, DC:U.S. Government Printing Office.Donna K. Harman, ed.
1993.
The First Text REtrievalConference (TREC-1) NIST Special Publication 500-207.Washington, DC: U.S. Government Printing Office.Donna K. Harman, ed.
1995.
Overview of the Third TextREtrieval Conference (TREC-3 ), NIST Special Publication500-225.
Washington, DC: U.S. Government PrintingOffice.Donna K. Harman, ed.
1996.
Overview of the Fourth TextREtrieval Conference (TREC-4), NIST Special Publica-tion.
Washington, DC: U.S. Government Printing Of-fice.Mark Lauer.
1995.
Corpus statistics meet with the nouncompound: Some empirical results.
In: Proceedings ofthe 33th Annual Meeting of the Association for Computa-tional Linguistics.David Lewis and K. Sparck Jones.
1996.
Natural languageprocessing for information retrieval.
Communications ofthe ACM, January, Vol.
39, No.
1, 92-101.Mark Liberman and Richard Sproat.
1992.
The stress andstructure of modified noun phrases in English.
In: I.Sag and A. Szabolcsi (eds.
), Lexical Matters, CSLI Lec-ture Notes No.
24.
Chicago, IL: University of ChicagoPress, pp.
131-181.Mitchell Maucus.
1980.
A Theory of Syntactic Recognitionfor Natural Language.
Cambridge, MA: MIT Press.J.
Pustejovsky, S. Bergler, and P. Anick.
1993.
Lexicalsemantic techniques for corpus analysis.
In: Compu-tational Linguistics, Vol.
19(2), Special Issue on UsingLarge Corpora II, pp.
331-358.P.
Resnik, and M. Hearst.
1993.
Structural Ambiguity andConceptual Relations.
In: Proceedings ofthe Workshop onVery Large Corpora: Academic and Industrial Perspectives,June 22, Ohio State University, pp.
58-64.Gerard Salton and Michael McGill.
1983.
Introduction toModern Information Retrieval, New York, NY: McGraw-Hill.Christoph Schwarz.
1990.
Content based text handling.Information Processing and Management, Vol.
26(2),pp.
219-226.Alan F. Smeaton.
1992.
Progress in application of natu-ral language processing to information retrieval.
TheComputer Journal, Vol.
35, No.
3, pp.
268-278.T.
Strzalkowski and J. Carballo.
1994.
Recent develop-ments in natural anguage text retrieval.
In: DonnaK.
Harman (ed.
), The Second Text REtrieval Conference(TREC-2).
NIST Special Publication 500-215.
Washing-ton, DC: U.S. Government Printing Office, pp.
123-136.24
