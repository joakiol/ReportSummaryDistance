Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 66?70,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsIntermediary Semantic Representationthrough Proposition StructuresGabriel Stanovsky?, Jessica Ficler?, Ido Dagan, Yoav GoldbergComputer Science Department, Bar-Ilan University?Both authors equally contributed to this paper{gabriel.satanovsky,jessica.ficler,yoav.goldberg}@gmail.comdagan@cs.biu.ac.ilAbstractWe propose an intermediary-level seman-tic representation, providing a higher levelof abstraction than syntactic parse trees,while not committing to decisions in casessuch as quantification, grounding or verb-specific roles assignments.
The proposalis centered around the proposition struc-ture of the text, and includes also im-plicit propositions which can be inferredfrom the syntax but are not transparent inparse trees, such as copular relations intro-duced by appositive constructions.
Otherbenefits over dependency-trees are ex-plicit marking of logical relations betweenpropositions, explicit marking of multi-word predicate such as light-verbs, and aconsistent representation for syntactically-different but semantically-similar struc-tures.
The representation is meant toserve as a useful input layer for semantic-oriented applications, as well as to providea better starting point for further levels ofsemantic analysis such as semantic-role-labeling and semantic-parsing.1 IntroductionParsers for semantic formalisms (such as Neo-davidsonian (Artzi and Zettlemoyer, 2013) andDRT (Kamp, 1988)) take unstructured natural lan-guage text as input, and output a complete seman-tic representation, aiming to capture the mean-ing conveyed by the text.
We suggest that thistask may be effectively separated into a sequentialcombination of two different tasks.
The first ofthese tasks is syntactic abstraction over phenom-ena such as expression of tense, negation, modal-ity, and passive versus active voice, which are alleither expressed or implied from syntactic struc-ture.
The second task is semantic interpretationover the syntactic abstraction, deriving quantifi-cation, grounding, etc.
Current semantic parsers(such as Boxer (Bos, 2008)) tackle these tasks si-multaneously, mixing syntactic and semantic is-sues in a single framework.
We believe that sepa-rating semantic parsing into two well defined taskswill help to better target and identify challengesin syntactic and semantic domains.
Challengeswhich are often hidden due to the one-step archi-tecture of current parsers.Many of today?s semantic parsers, and semanticapplications in general, leverage dependency pars-ing (De Marneffe and Manning, 2008a) as an ab-straction layer, since it directly represents syntac-tic dependency relations between predicates andarguments.
Some systems exploit Semantic RoleLabeling (SRL) (Carreras and M?arquez, 2005),where predicate-argument relationships are cap-tured at a thematic (rather than syntactic) level,though current SRL technology is less robust andaccurate for open domains than syntactic pars-ing.
While dependency structures and semanticroles capture much of the proposition structure ofsentences, there are substantial aspects which arenot covered by these representations and thereforeneed to be handled by semantic applications ontheir own (or they end up being ignored).Such aspects, as detailed in Section 3, includepropositions which are not expressed directly assuch but are rather implied by syntactic struc-ture, like nominalizations, appositions and pre-modifying adjectives.
Further, the same proposi-tion structure may be expressed in many differ-ent ways by the syntactic structure, forcing sys-tems to recognize this variability and making thetask of recognizing semantic roles harder.
Otheraspects not addressed by common representationsinclude explicit marking of links between propo-sitions within a sentence, which affect their asser-tion or truth status, and the recognition of multi-word predicates (e.g., considering ?take a deci-66Figure 1: Proposed representation for the sentence: ?If youleave the park, you will find the Peak Tram terminal?sion?
as a single predicate, rather than consideringdecision as an argument).In this position paper we propose an intermedi-ary representation level for the first syntactic ab-straction phase described above, intended to re-place syntactic parsing as a more abstract repre-sentation layer.
It is designed to capture the fullproposition structure which is expressed, eitherexplicitly or implicitly, by the syntactic structureof sentences.
Thus, we aim to both extract im-plicit propositions as well as to abstract away syn-tactic variations which yield the same propositionstructure.
At the same time, we aim to remain ata representation level that corresponds to syntac-tic properties and relationships, while avoiding se-mantic interpretations, to be targeted by systemsimplementing the further step of semantic inter-pretation, as discussed above.In addition, we suggest our representation as auseful input for semantic applications which needto recognize the proposition structure of sentencesin order to identify targeted information, such asQuestion Answering(QA), Information Extraction(IE) and multidocument summarization.
We ex-pect that our representation may be more usefulin comparison with current popular use of depen-dency parsing, in such applications.2 Representation SchemeOur representation is centered around proposi-tions, where a proposition is a statement for whicha truth-value can be assigned.
We propose to rep-resent sentences as a set of inter-linked proposi-tions.
Each proposition is composed of one pred-icate and a set of arguments.
An example rep-resentation can be seen in Figure 1.
Predicatesare usually centered around verbs, and we con-sider multi-word verbs (e.g., ?take apart?)
as sin-gle predicates.
Both the predicates and argumentsare represented as sets of feature-value pairs.
Eachargument is marked with a relation to its predicate,and the same argument can appear in differentpropositions.
The relation-set we use is syntacticin nature, including relations such as Subject,Object, and Preposition with, in contrastto semantic relations such as instrument.Canonical Representation The same proposi-tion can be realized syntactically in many forms.An important goal of our proposal is abstractingover idiosyncrasies in the syntactic structure andpresenting unified structures when possible.
Wecanonicalize on two levels:?
We canonicalize each predicate and argumentby representing each predicate as its mainlemma, and indicating other aspects of thepredication (e.g., tense, negation and time) asfeatures; Similarly, we mark arguments withfeatures such as definiteness and plurality.?
We canonicalize the argument structure byabstracting away over word order and phe-nomena such as topicalization and pas-sive/active voice, and present a unified rep-resentation in terms of the argument roles (sothat, for example, in the sentence ?the doorwas opened?
the argument ?door?
will re-ceive the object role, with the passive be-ing indicated as a feature of the predicate).Relations Between Propositions Some propo-sitions must be interpreted taking into accounttheir relations to other propositions.
These in-clude conditionals (?if congress does nothing,President Bush will have won?
(wsj 0112));temporal relations (?UAL?s announcement cameafter the market closed yesterday?
(wsj 0112));and conjunctions (?They operate ships andbanks.?
(wsj 0083)).We model such relations as typed links betweenextracted propositions.
Figure 1 presents an exam-ple of handling a conditional relation: the depen-dence between the propositions is made explicit bythe Cond(if) relation.3 Implicit PropositionsCrucially, our proposal aims to capture not onlyexplicit but also implicit propositions ?
proposi-tions that can be inferred from the syntactic struc-67ture but which are not explicitly marked in syn-tactic dependency trees, as we elaborate below.Some of these phenomena are relatively easy toaddress by post-processing over syntactic parsers,and could thus be included in a first implemen-tation that produces our proposed representations.Other phenomena are more subtle and would re-quire further research, yet they seem importantwhile not being addressed by current techniques.The syntactic structures giving rise to implicitpropositions include:Copular sentences such as ?This is not a triv-ial issue.?
(wsj 0108) introduces a proposition bylinking between a non-verbal predicate and its ar-gument.
We represent this by making ?not a triv-ial issue?
a predicate, and ?this?
an argument oftype Predication.Appositions, we distinguish between co-referenceand predicative appositions.
In Co-reference in-dication appositions (?The company, RandomHouse, doesn?t report its earnings.?
(adaption ofwsj 0111)) we produce a proposition to indicatethe co-reference between two lexical items.
Otherpropositions relating to the entity use the mainclause as the referent for this entity.
In this ex-ample, we will produce:1.
Random House == the company.2.
The company doesn?t report its earnings.In Predicative appositions (?Pierre Vinken, 61years old, will join the board as a nonexecutive di-rector Nov. 29.?
(wsj 0001)) an apposition is usedin order to convey knowledge about an entity.
Inour representation this will produce:1.
Pierre Vinken is 61 years old (which is canoni-calized to the representation of copular sentences)2.
Pierre Vinken will join the board as a nonexec-utive director Nov. 29.Adjectives, as in the sentence ?you emphasizedthe high prevalence of mental illness?
(wsj 0105).Here an adjective is used to describe a definite sub-ject and introduces another proposition, namelythe high prevalence of mental illness.Nominalizations, for instance in the sentence?Googles acquisition of Waze occurred yester-day?, introduce the implicit proposition that?Google acquired Waze?.
Such propositions werestudied and annotated in the NOMLEX (Macleodet al., 1998) and NOMBANK (Meyers et al., 2004)resources.
It remains an open issue how to repre-sent or distinguish cases in which nominalizationintroduce an underspecified proposition.
For ex-ample, consider ?dancing?
in ?I read a book aboutdancing?.Possessives, such as ?John?s book?
introduce theproposition that John has a book.
Similarly, ex-amples such as ?John?s Failure?
combine a pos-sessive construction with nominalization and in-troduce the proposition that John has failed.Conjunctions - for example in ?They operateships and banks.?
(wsj 0083), introduce severalpropositions in one sentence:1.
They operate ships2.
They operate banksWe mark that they co-refer to the same lexical unitin the original sentence.
Such cases are alreadyrepresented explicitly in the ?collapsed?
versionof Stanford-dependencies (De Marneffe and Man-ning, 2008a).1Implicit future tense indication, for instancein ?I?m going to vote for it?
(wsj 0098) and?The economy is about to slip into recession.?
(wsj 0036), verbs like ?going to?
and ?about to?are used as future-tense markers of the proposi-tion following them, rather than predicates on theirown.
We represent these as a single predicate(?vote?)
in which the tense is marked as a fea-ture.2Other phenomena, omitted for lack of space,include propositional modifiers (e.g., relativeclause modifiers), propositional arguments (suchas ?John asserted that he will go home?
), condi-tionals, and the canonicalization of passive andactive voice.4 Relation to Other RepresentationsOur proposed representation is intended to serveas a bridging layer between purely syntactic rep-resentations such as dependency trees, and seman-tic oriented applications.
In particular, we explic-itly represent many semantic relations expressedin a sentence that are not captured by contempo-rary proposition-directed semantic representations(Baker et al., 1998; Kingsbury and Palmer, 2003;Meyers et al., 2004; Carreras and M`arquez, 2005).Compared to dependency-based representationssuch as Stanford-dependency trees (De Marneffe1A case of conjunctions requiring special treatment is in-troduced by reciprocals, in which the entities roles are ex-changeable.
For example: ?John and Mary bet against eachother on future rates?
(adaption of wsj 0117).2Care needs to be taken to distinguish from cases such as?going to Italy?
in which ?going to?
is not followed by averbal predicate.68and Manning, 2008b), we abstract away overmany syntactic details (e.g., the myriad of waysof expressing tense, negation and modality, or thedifference between passive and active) which arenot necessary for semantic interpretation and markthem instead using a unified set of features and ar-gument types.
We make explicit many relationsthat can be inferred from the syntax but whichare not directly encoded in dependency relations.We directly connect predicates with all of their ar-guments in e.g., conjunctions and embedded con-structions, and we do not commit to a tree struc-ture.
We also explicitly mark predicate and argu-ment boundaries, and explicitly mark multi-wordpredicates such as light-verb constructions.Compared to proposition-based semantic rep-resentations, we do not attempt to assign frame-specific thematic roles, nor do we attempt to dis-ambiguate or interpret word meanings.
We restrictourselves to representing predicates by their (lem-matized) surface forms, and labeling argumentsbased on a ?syntactic?
role inventory, similar to thelabel-sets available in dependency representations.This design choice makes our representation mucheasier to assign automatically to naturally occur-ring text (perhaps pre-annotated using a syntacticparser) than it is to assign semantic roles.
At thesame time, as described in Section 3, we capturemany relations that are currently not annotated inresources such as FrameNet, and provide a com-prehensive set of propositions present in the sen-tence (either explicitly or implicitly) as well as therelations between them ?
an objective which is nottrivial even when presented with full semantic rep-resentation.Compared to more fine-grained semantic repre-sentations used in semantic-parsers (i.e.
lambda-calculus (Zettlemoyer and Collins, 2005), neo-davidsonian semantics (Artzi and Zettlemoyer,2013), DRT (Kamp, 1988) or the DCS represen-tation of Liang (2011)), we do not attempt totackle quantification, nor to ground the argumentsand predicates to a concrete domain-model or on-tology.
These important tasks are orthogonal toour representation, and we believe that semantic-parsers can benefit from our proposal by using itas input in addition to or instead of the raw sen-tence text ?
quantification, binding and groundingare hard enough without needing to deal with thesubtleties of syntax or the identification of implicitpropositions.5 Conclusion and Future WorkWe proposed an intermediate semantic repre-sentation through proposition extraction, whichcaptures both explicit and implicit propositions,while staying relatively close to the syntacticlevel.
We believe that this kind of representationwill serve not only as an advantageous input forsemantically-centered applications, such as ques-tion answering, summarization and informationextraction, but also serve as a rich representationlayer that can be used as input for systems aimingto provide a finer level of semantic analysis, suchas semantic-parsers.We are currently at the beginning of our in-vestigation.
In the near future we plan to semi-automatically annotate the Penn Tree Bank (Mar-cus et al., 1993) with these structures, as well asto provide software for deriving (some of) the im-plicit and explicit annotations from automaticallyproduced parse-trees.
We believe such resourceswill be of immediate use to semantic-oriented ap-plications.
In the longer term, we plan to inves-tigate dedicated algorithms for automatically pro-ducing such representation from raw text.The architecture we describe can easily accom-modate additional layers of abstraction, by en-coding these layers as features of propositions,predicates or arguments.
Such layers can includethe marking of named entities, the truth status ofpropositions and author commitment.In the current version infinitive constructionsare treated as nested propositions, similar to theirrepresentation in syntactic parse trees.
Providinga consistent, useful and transparent representationfor infinitive constructions is a challenging direc-tion for future research.Other extensions of the proposed representa-tion are also possible.
One appealing directionis going beyond the sentence level and represent-ing discourse level relations, including impliedpropositions and predicate - argument relation-ships expressed by discourse (Stern and Dagan,2014; Ruppenhofer et al., 2010; Gerber and Chai,2012).
Such an extension may prove useful as anintermediary representation for parsers of seman-tic formalisms targeted at the discourse level (suchas DRT).6 AcknowledgmentsThis work was partially supported by the Eu-ropean Community?s Seventh Framework Pro-69gramme (FP7/2007-2013) under grant agreementno.
287923 (EXCITEMENT).ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly supervisedlearning of semantic parsers for mapping instructions toactions.
Transactions of the Association for Computa-tional Linguistics, 1(1):49?62.Collin F Baker, Charles J Fillmore, and John B Lowe.
1998.The berkeley framenet project.
In Proceedings of ACL,pages 86?90.
Association for Computational Linguistics.Johan Bos.
2008.
Wide-coverage semantic analysis withboxer.
In Proceedings of the 2008 Conference on Seman-tics in Text Processing, pages 277?286.
Association forComputational Linguistics.Xavier Carreras and Llu?
?s M`arquez.
2005.
Introduction tothe conll-2005 shared task: Semantic role labeling.
InProceedings of CONLL, pages 152?164.Marie-Catherine De Marneffe and Christopher D Manning.2008a.
Stanford typed dependencies manual.
Technicalreport, Stanford University.Marie-Catherine De Marneffe and Christopher D Manning.2008b.
The stanford typed dependencies representation.In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages1?8.Matthew Gerber and Joyce Y Chai.
2012.
Semantic role la-beling of implicit arguments for nominal predicates.
Com-putational Linguistics, 38(4):755?798.Hans Kamp.
1988.
Discourse representation theory.
In Nat-ural Language at the computer, pages 84?111.
Springer.Paul Kingsbury and Martha Palmer.
2003.
Propbank: thenext level of treebank.
In Proceedings of Treebanks andlexical Theories, volume 3.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learningdependency-based compositional semantics.
In Proceed-ings of ACL, pages 590?599.Catherine Macleod, Ralph Grishman, Adam Meyers, LeslieBarrett, and Ruth Reeves.
1998.
Nomlex: A lexiconof nominalizations.
In Proceedings of EURALEX, vol-ume 98, pages 187?193.Mitchell P Marcus, Mary Ann Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpus ofenglish: The penn treebank.
Computational linguistics,19(2):313?330.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and Ralph Gr-ishman.
2004.
The nombank project: An interim report.In HLT-NAACL 2004 workshop: Frontiers in corpus an-notation, pages 24?31.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
Semeval-2010task 10: Linking events and their participants in discourse.In Proceedings of the 5th International Workshop on Se-mantic Evaluation, pages 45?50.
Association for Compu-tational Linguistics.Asher Stern and Ido Dagan.
2014.
Recognizing impliedpredicate-argument relationships in textual inference.
InProceedings of ACL.
Association for Computational Lin-guistics.Luke S. Zettlemoyer and Michael Collins.
2005.
Learningto map sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In UAI, pages658?666.
AUAI Press.70
