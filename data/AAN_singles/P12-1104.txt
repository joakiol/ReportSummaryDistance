Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 988?996,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsEcological Evaluation of Persuasive Messages Using Google AdWordsMarco GueriniTrento-RiseVia Sommarive 18, PovoTrento ?
Italymarco.guerini@trentorise.euCarlo StrapparavaFBK-IrstVia Sommarive 18, PovoTrento ?
Italystrappa@fbk.euOliviero StockFBK-IrstVia Sommarive 18, PovoTrento ?
Italystock@fbk.euAbstractIn recent years there has been a growing in-terest in crowdsourcing methodologies to beused in experimental research for NLP tasks.In particular, evaluation of systems and theo-ries about persuasion is difficult to accommo-date within existing frameworks.
In this paperwe present a new cheap and fast methodologythat allows fast experiment building and eval-uation with fully-automated analysis at a lowcost.
The central idea is exploiting existingcommercial tools for advertising on the web,such as Google AdWords, to measure messageimpact in an ecological setting.
The paper in-cludes a description of the approach, tips forhow to use AdWords for scientific research,and results of pilot experiments on the impactof affective text variations which confirm theeffectiveness of the approach.1 IntroductionIn recent years there has been a growing interest infinding new cheap and fast methodologies to be usedin experimental research, for, but not limited to, NLPtasks.
In particular, approaches to NLP that rely onthe use of web tools - for crowdsourcing long andtedious tasks - have emerged.
Amazon Mechani-cal Turk, for example, has been used for collectingannotated data (Snow et al, 2008).
However ap-proaches a la Mechanical Turk might not be suitablefor all tasks.In this paper we focus on evaluating systems andtheories about persuasion, see for example (Fogg,2009) or the survey on persuasive NL generationstudies in (Guerini et al, 2011a).
Measuring theimpact of a message is of paramount importance inthis context, for example how affective text varia-tions can alter the persuasive impact of a message.The problem is that evaluation experiments repre-sent a bottleneck: they are expensive and time con-suming, and recruiting a high number of human par-ticipants is usually very difficult.To overcome this bottleneck, we present a specificcheap and fast methodology to automatize large-scale evaluation campaigns.
This methodology al-lows us to crowdsource experiments with thousandsof subjects for a few euros in a few hours, by tweak-ing and using existing commercial tools for adver-tising on the web.
In particular we make referenceto the AdWords Campaign Experiment (ACE) toolprovided within the Google AdWords suite.
Oneimportant aspect of this tool is that it allows for real-time fully-automated data analysis to discover sta-tistically significant phenomena.
It is worth notingthat this work originated in the need to evaluate theimpact of short persuasive messages, so as to assessthe effectiveness of different linguistic choices.
Still,we believe that there is further potential for openingan interesting avenue for experimentally exploringother aspects of the wide field of pragmatics.The paper is structured as follows: Section 2 dis-cusses the main advantages of ecological approachesusing Google ACE over traditional lab settings andstate-of-the-art crowdsourcing methodologies.
Sec-tion 3 presents the main AdWords features.
Section4 describes how AdWords features can be used fordefining message persuasiveness metrics and whatkind of stimulus characteristics can be evaluated.
Fi-nally Sections 5 and 6 describe how to build up an988experimental scenario and some pilot studies to testthe feasibility of our approach.2 Advantages of Ecological ApproachesEvaluation of the effectiveness of persuasive sys-tems is very expensive and time consuming, as theSTOP experience showed (Reiter et al, 2003): de-signing the experiment, recruiting subjects, makingthem take part in the experiment, dispensing ques-tionnaires, gathering and analyzing data.Existing methodologies for evaluating persuasionare usually split in two main sets, depending on thesetup and domain: (i) long-term, in the field eval-uation of behavioral change (as the STOP examplementioned before), and (ii) lab settings for evaluat-ing short-term effects, as in (Andrews et al, 2008).While in the first approach it is difficult to take intoaccount the role of external events that can occurover long time spans, in the second there are stillproblems of recruiting subjects and of time consum-ing activities such as questionnaire gathering andprocessing.In addition, sometimes carefully designed exper-iments can fail because: (i) effects are too subtle tobe measured with a limited number of subjects or(ii) participants are not engaged enough by the taskto provoke usable reactions, see for example whatreported in (Van Der Sluis and Mellish, 2010).
Es-pecially the second point is awkward: in fact, sub-jects can actually be convinced by the message towhich they are exposed, but if they feel they do notcare, they may not ?react?
at all, which is the case inmany artificial settings.
To sum up, the main prob-lems are:1.
Time consuming activities2.
Subject recruitment3.
Subject motivation4.
Subtle effects measurements2.1 Partial Solution - Mechanical TurkA recent trend for behavioral studies that is emerg-ing is the use of Mechanical Turk (Mason and Suri,2010) or similar tools to overcome part of these limi-tations - such as subject recruitment.
Still we believethat this poses other problems in assessing behav-ioral changes, and, more generally, persuasion ef-fects.
In fact:1.
Studies must be as ecological as possible, i.e.conducted in real, even if controlled, scenarios.2.
Subjects should be neither aware of being ob-served, nor biased by external rewards.In the case of Mechanical Turk for example, sub-jects are willingly undergoing a process of beingtested on their skills (e.g.
by performing annota-tion tasks).
Cover stories can be used to soften thisawareness effect, nonetheless the fact that subjectsare being paid for performing the task renders theapproach unfeasible for behavioral change studies.It is necessary that the only reason for behavior in-duction taking place during the experiment (fillinga form, responding to a questionnaire, clicking onan item, etc.)
is the exposition to the experimentalstimuli, not the external reward.
Moreover, Mechan-ical Turk is based on the notion of a ?gold standard?to assess contributors reliability, but for studies con-cerned with persuasion it is almost impossible to de-fine such a reference: there is no ?right?
action thecontributor can perform, so there is no way to assesswhether the subject is performing the action becauseinduced to do so by the persuasive strategy, or justin order to receive money.
On the aspect of how tohandle subject reliability in coding tasks, see for ex-ample the method proposed in (Negri et al, 2010).2.2 Proposed Solution - Targeted Ads on theWebEcological studies (e.g.
using Google AdWords) of-fer a possible solution to the following problems:1.
Time consuming activities: apart from experi-mental design and setup, all the rest is automat-ically performed by the system.
Experimentscan yield results in a few hours as compared toseveral days/weeks.2.
Subject recruitment: the potential pool of sub-jects is the entire population of the web.3.
Subject motivation: ads can be targeted exactlyto those persons that are, in that precise mo-ment throughout the world, most interested inthe topic of the experiment, and so potentiallymore prone to react.4.
Subject unaware, unbiased: subjects are totallyunaware of being tested, testing is performedduring their ?natural?
activity on the web.9895.
Subtle effects measurements: if the are notenough subjects, just wait for more ads to bedisplayed, or focus on a subset of even moreinterested people.Note that similar ecological approaches are begin-ning to be investigated: for example in (Aral andWalker, 2010) an approach to assessing the social ef-fects of content features on an on-line community ispresented.
A previous approach that uses AdWordswas presented in (Guerini et al, 2010), but it crowd-sourced only the running of the experiment, not datamanipulation and analysis, and was not totally con-trolled for subject randomness.3 AdWords FeaturesGoogle AdWords is Google?s advertising program.The central idea is to let advertisers display theirmessages only to relevant audiences.
This is doneby means of keyword-based contextualization on theGoogle network, divided into:?
Search network: includes Google search pages,search sites and properties that display searchresults pages (SERPs), such as Froogle andEarthlink.?
Display network: includes news pages, topic-specific websites, blogs and other properties -such as Google Mail and The New York Times.When a user enters a query like ?cruise?
in theGoogle search network, Google displays a variety ofrelevant pages, along with ads that link to cruise tripbusinesses.
To be displayed, these ads must be asso-ciated with relevant keywords selected by the adver-tiser.Every advertiser has an AdWords account that isstructured like a pyramid: (i) account, (ii) campaignand (iii) ad group.
In this paper we focus on adgroups.
Each grouping gathers similar keywords to-gether - for instance by a common theme - aroundan ad group.
For each ad group, the advertiser sets acost-per-click (CPC) bid.
The CPC bid refers to theamount the advertiser is willing to pay for a click onhis ad; the cost of the actual click instead is basedon its quality score (a complex measure out of thescope of the present paper).For every ad group there could be multiple adsto be served, and there are many AdWords measure-ments for identifying the performance of each singlead (its persuasiveness, from our point of view):?
CTR, Click Through Rate: measures the num-ber of clicks divided by the number of impres-sions (i.e.
the number of times an ad has beendisplayed in the Google Network).?
Conversion Rate: if someone clicks on an ad,and buys something on your site, that click isa conversion from a site visit to a sale.
Con-version rate equals the number of conversionsdivided by the number of ad clicks.?
ROI: Other conversions can be page views orsignups.
By assigning a value to a conversionthe resulting conversions represents a return oninvestment, or ROI.?
Google Analytics Tool: Google Analytics is aweb analytics tool that gives insights into web-site traffic, like number of visited pages, timespent on the site, location of visitors, etc.So far, we have been talking about text ads, -Google?s most traditional and popular ad format -because they are the most useful for NLP analysis.In addition there is also the possibility of creatingthe following types of ads:?
Image (and animated) ads?
Video ads?
Local business ads?
Mobile adsThe above formats allow for a greater potentialto investigate persuasive impact of messages (otherthan text-based) but their use is beyond the scope ofthe present paper1.4 The ACE ToolAdWords can be used to design and develop vari-ous metrics for fast and fully-automated evaluationexperiments, in particular using the ACE tool.This tool - released in late 2010 - allows testing,from a marketing perspective, if any change made toa promotion campaign (e.g.
a keyword bid) had astatistically measurable impact on the campaign it-self.
Our primary aim is slightly different: we are1For a thorough description of the AdWords tool see:https://support.google.com/adwords/990interested in testing how different messages impact(possibly different) audiences.
Still the ACE toolgoes exactly in the direction we aim at, since it in-corporates statistically significant testing and allowsavoiding many of the tweaking and tuning actionswhich were necessary before its release.The ACE tool also introduces an option that wasnot possible before, that of real-time testing of sta-tistical significance.
This means that it is no longernecessary to define a-priori the sample size for theexperiment: as soon as a meaningful statisticallysignificant difference emerges, the experiment canbe stopped.Another advantage is that the statistical knowl-edge to evaluate the experiment is no longer nec-essary: the researcher can focus only on setting upproper experimental designs2.The limit of the ACE tool is that it only allowsA/B testing (single split with one control and one ex-perimental condition) so for experiments with morethan two conditions or for particular experimentalsettings that do not fit with ACE testing bound-aries (e.g.
cross campaign comparisons) we suggesttaking (Guerini et al, 2010) as a reference model,even if the experimental setting is less controlled(e.g.
subject randomness is not equally guaranteedas with ACE).Finally it should be noted that even if ACE allowsonly A/B testing, it permits the decomposition of al-most any variable affecting a campaign experimentin its basic dimensions, and then to segment suchdimensions according to control and experimentalconditions.
As an example of this powerful option,consider Tables 3 and 6 where control and experi-mental conditions are compared against every singlekeyword and every search network/ad position usedfor the experiments.5 Evaluation and Targeting with ACELet us consider the design of an experiment with 2conditions.
First we create an ad Group with 2 com-peting messages (one message for each condition).Then we choose the serving method (in our opin-ion the rotate option is better than optimize, since it2Additional details about ACE features and statistics can befound at http://www.google.com/ads/innovations/ace.htmlguarantees subject randomness and is more transpar-ent) and the context (language, network, etc.).
Thenwe activate the ads and wait.
As soon as data beginsto be collected we can monitor the two conditionsaccording to:?
Basic Metrics: the highest CTR measure in-dicates which message is best performing.
Itindicates which message has the highest initialimpact.?
Google Analytics Metrics: measures how muchthe messages kept subjects on the site and howmany pages have been viewed.
Indicates inter-est/attitude generated in the subjects.?
Conversion Metrics: measures how much themessages converted subjects to the final goal.Indicates complete success of the persuasivemessage.?
ROI Metrics: by creating specific ROI valuesfor every action the user performs on the land-ing page.
The more relevant (from a persuasivepoint of view) the action the user performs, thehigher the value we must assign to that action.In our view combined measurements are better:for example, there could be cases of messageswith a lower CTR but a higher conversion rate.Furthermore, AdWords allows very complex tar-geting options that can help in many different evalu-ation scenarios:?
Language (see how message impact can vary indifferent languages).?
Location (see how message impact can vary indifferent cultures sharing the same language).?
Keyword matching (see how message impactcan vary with users having different interests).?
Placements (see how message impact can varyamong people having different values - e.g.
thesame message displayed on Democrat or Re-publican web sites).?
Demographics (see how message impact canvary according to user gender and age).5.1 Setting up an ExperimentTo test the extent to which AdWords can be ex-ploited, we focused on how to evaluate lexical varia-tions of a message.
In particular we were interested991in gaining insights about a system for affective varia-tions of existing commentaries on medieval frescoesfor a mobile museum guide that attracts the attentionof visitors towards specific paintings (Guerini et al,2008; Guerini et al, 2011b).
The various steps forsetting up an experiment (or a series of experiments)are as follows:Choose a Partner.
If you have the opportunityto have a commercial partner that already has the in-frastructure for experiments (website, products, etc.
)many of the following steps can be skipped.
We as-sume that this is not the case.Choose a scenario.
Since you may not beequipped with a VAT code (or with the commercialpartner that furnishes the AdWords account and in-frastructure), you may need to ?invent something topromote?
without any commercial aim.
If a ?socialmarketing?
scenario is chosen you can select ?per-sonal?
as a ?tax status?, that do not require a VATcode.
In our case we selected cultural heritage pro-motion, in particular the frescoes of Torre Aquila(?Eagle Tower?)
in Trento.
The tower contains agroup of 11 frescoes named ?Ciclo dei Mesi?
(cy-cle of the months) that represent a unique exampleof non-religious medieval frescoes in Europe.Choose an appropriate keyword on which toadvertise, ?medieval art?
in our case.
It is betterto choose keywords with enough web traffic in or-der to speed up the experimental process.
In ourcase the search volume for ?medieval art?
(in phrasematch) was around 22.000 hits per month.
Anothersuggestion is to restrict the matching modality onKeywords in order to have more control over thesituations in which ads are displayed and to avoidpossible extraneous effects (the order of controlfor matching modality is: [exact match], ?phrasematch?
and broad match).Note that such a technical decision - which key-word to use - is better taken at an early stage of de-velopment because it affects the following steps.Write messages optimized for that keyword (e.g.including it in the title or the body of the ad).
Suchoptimization must be the same for control and exper-imental condition.
The rest of the ad can be designedin such a way to meet control and experimental con-dition design (in our case a message with slightlyaffective terms and a similar message with more af-fectively loaded variations)Build an appropriate landing page, accordingto the keyword and populate the website pages withrelevant material.
This is necessary to create a ?cred-ible environment?
for users to interact with.Incorporate meaningful actions in the website.Users can perform various actions on a site, and theycan be monitored.
The design should include ac-tions that are meaningful indicators of persuasive ef-fect/success of the message.
In our case we decidedto include some outbound links, representing:?
general interest: ?Buonconsiglio Castle site??
specific interest: ?Eagle Tower history??
activated action: ?Timetable and venue??
complete success: ?Book a visit?Furthermore, through new Google Analytics fea-tures, we set up a series of time spent on site andnumber of visited pages thresholds to be monitoredin the ACE tool.5.2 Tips for Planning an ExperimentThere are variables, inherent in the Google AdWordsmechanism, that from a research point of view weshall consider ?extraneous?.
We now propose tipsfor controlling such extraneous variables.Add negative matching Keywords: To add morecontrol, if in doubt, put the words/expressions of thecontrol and experimental conditions as negative key-words.
This will prevent different highlighting be-tween the two conditions that can bias the results.
Itis not strictly necessary since one can always controlwhich queries triggered a click through the reportmenu.
An example: if the only difference betweencontrol and experimental condition is the use of theadjectives ?gentle knights?
vs. ?valorous knights?,one can use two negative keyword matches: -gentleand -valorous.
Obviously if you are using a key-word in exact matching to trigger your ads, such as[knight], this is not necessary.Frequency capping for the display network: ifyou are running ads on the display network, you canuse the ?frequency capping?
option set to 1 to addmore control to the experiment.
In this way it is as-sured that ads are displayed only one time per useron the display network.Placement bids for the search network: unfor-tunately this option is no longer available.
Basicallythe option allowed to bid only for certain positions992on the SERPs to avoid possible ?extraneous vari-ables effect?
given by the position.
This is best ex-plained via an example: if, for whatever reason, oneof the two ads gets repeatedly promoted to the pre-mium position on the SERPs, then the CTR differ-ence between ads would be strongly biased.
Froma research point of view ?premium position?
wouldthen be an extraneous variable to be controlled (i.e.either both ads get an equal amount of premium po-sition impressions, or both ads get no premium po-sition at all).
Otherwise the difference in CTR is de-termined by the ?premium position?
rather than bythe independent variable under investigation (pres-ence/absence of particular affective terms in the textad).
However even if it is not possible to rule out this?position effect?
it is possible to monitor it by usingthe report (Segment > Top vs. other + Experiment)and checking how many times each ad appeared ina given position on the SERPs, and see if the ACEtool reports any statistical difference in the frequen-cies of ads positions.Extra experimental time: While planning an ex-periment, you should also take into account the adsreviewing time that can take up to several days, inworst case scenarios.
Note that when ads are in eli-gible status, they begin to show on the Google Net-work, but they are not approved yet.
This means thatthe ads can only run on Google search pages and canonly show for users who have turned off SafeSearchfiltering, until they are approved.
Eligible ads cannotrun on the Display Network.
This status will providemuch less impressions than the final ?approved?
sta-tus.Avoid seasonal periods: for the above reason,and to avoid extra costs due to high competition,avoid seasonal periods (e.g.
Christmas time).Delivery method: if you are planning to use theAccelerated Delivery method in order to get the re-sults as quick as possible (in the case of ?quick anddirty?
experiments or ?fast prototyping-evaluationcycles?)
you should consider monitoring your ex-periment more often (even several times per day) toavoid running out of budget during the day.6 ExperimentsWe ran two pilot experiments to test how affectivevariations of existing texts alter their persuasive im-pact.
In particular we were interested in gaininginitial insights about an intelligent system for affec-tive variations of existing commentaries on medievalfrescoes.We focused on adjective variations, using aslightly biased adjective for the control conditionsand a strongly biased variation for the experimen-tal condition.
In these experiments we took it forgranted that affective variations of a message workbetter than a neutral version (Van Der Sluis and Mel-lish, 2010), and we wanted to explore more finelygrained tactics that involve the grade of the vari-ation (i.e.
a moderately positive variation vs. anextremely positive variation).
Note that this is amore difficult task than the one proposed in (VanDer Sluis and Mellish, 2010), where they were test-ing long messages with lots of variations and withpolarized conditions, neutral vs. biased.
In additionwe wanted to test how quickly experiments could beperformed (two days versus the two week sugges-tion of Google).Adjectives were chosen according to MAX bi-gram frequencies with the modified noun, using theWeb 1T 5-gram corpus (Brants and Franz, 2006).Deciding whether this is the best metric for choosingadjectives to modify a noun or not (e.g.
also point-wise mutual-information score can be used with adifferent rationale) is out of the scope of the presentpaper, but previous work has already used this ap-proach (Whitehead and Cavedon, 2010).
Top rankedadjectives were then manually ordered - according toaffective weight - to choose the best one (we used astandard procedure using 3 annotators and a recon-ciliation phase for the final decision).6.1 First ExperimentThe first experiment lasted 48 hour with a total of 38thousand subjects and a cost of 30 euros (see Table1 for the complete description of the experimentalsetup).
It was meant to test broadly how affectivevariations in the body of the ads performed.
The twovariations contained a fragment of a commentary ofthe museum guide; the control condition contained?gentle knight?
and ?African lion?, while in the ex-perimental condition the affective loaded variationswere ?valorous knight?
and ?indomitable lion?
(seeFigure 1, for the complete ads).
As can be seen fromTable 2, the experiment did not yield any significant993result, if one looks at the overall analysis.
But seg-menting the results according to the keyword thattriggered the ads (see Table 3) we discovered thaton the ?medieval art?
keyword, the control conditionperformed better than the experimental one.Starting Date: 1/2/2012Ending Date: 1/4/2012Total Time: 48 hoursTotal Cost: 30 eurosSubjects: 38,082Network: Search and DisplayLanguage: EnglishLocations: Australia; Canada; UK; USKeyWords: ?medieval art?, pictures middle agesTable 1: First Experiment SetupACE split Clicks Impr.
CTRControl 31 18,463 0.17%Experiment 20 19,619 0.10%Network Clicks Impr.
CTRSearch 39 4,348 0.90%Display 12 34,027 0.04%TOTAL 51 38,082 0.13%Table 2: First Experiment ResultsKeyword ACE split Impr.
CTR?medieval art?
Control 657 0.76%?medieval art?
Experiment 701 0.14%*medieval times history Control 239 1.67%medieval times history Experiment 233 0.86%pictures middle ages Control 1114 1.35%pictures middle ages Experiment 1215 0.99%Table 3: First Experiment Results Detail.
* indicates astatistically significant difference with ?
< 0.01Discussion.
As already discussed, user moti-vation is a key element for success in such fine-grained experiments: while less focused keywordsdid not yield any statistically significant differences,the most specialized keyword ?medieval art?
was theone that yielded results (i.e.
if we display messageslike those in Figure 1, that are concerned with me-dieval art frescoes, only those users really interestedin the topic show different reaction patterns to the af-fective variations, while those generically interestedin medieval times behave similarly in the two con-ditions).
In the following experiment we tried to seewhether such variations have different effects whenmodifying a different element in the text.Figure 1: Ads used in the first experiment6.2 Second ExperimentThe second experiment lasted 48 hours with a to-tal of one thousand subjects and a cost of 17 euros(see Table 4 for the description of the experimen-tal setup).
It was meant to test broadly how affec-tive variations introduced in the title of the text Adsperformed.
The two variations were the same as inthe first experiment for the control condition ?gentleknight?, and for the experimental condition ?valor-ous knight?
(see Figure 2 for the complete ads).
Ascan be seen from Table 5, also in this case the experi-ment did not yield any significant result, if one looksat the overall analysis.
But segmenting the resultsaccording to the search network that triggered theads (see Table 6) we discovered that on the searchpartners at the ?other?
position, the control conditionperformed better than the experimental one.
Unlikethe first experiment, in this case we segmented ac-cording to the ad position and search network typol-ogy since we were running our experiment only onone keyword in exact match.Starting Date: 1/7/2012Ending Date: 1/9/2012Total Time: 48 hoursTotal Cost: 17.5 eurosSubjects: 986Network: SearchLanguage: EnglishLocations: Australia; Canada; UK; USKeyWords: [medieval knights]Table 4: Second Experiment Setup994Figure 2: Ads used in the second experimentACE split Clicks Impr.
CTRControl 10 462 2.16%Experiment 8 524?
1.52%TOTAL 18 986 1.82%Table 5: Second Experiment Results.
?
indicates a statis-tically significant difference with ?
< 0.05Top vs. Other ACE split Impr.
CTRGoogle search: Top Control 77 6.49%Google search: Top Experiment 68 2.94%Google search: Other Control 219 0.00%Google search: Other Experiment 277* 0.36%Search partners: Top Control 55 3.64%Search partners: Top Experiment 65 6.15%Search partners: Other Control 96 3.12%Search partners: Other Experiment 105 0.95%?Total - Search ?
986 1.82%Table 6: Second Experiment Results Detail.
?
indicates astatistical significance with ?
< 0.05, * indicates a sta-tistical significance with ?
< 0.01Discussion.
From this experiment we can confirmthat at least under some circumstances a mild af-fective variation performs better than a strong varia-tion.
This mild variations seems to work better whenuser attention is high (the difference emerged whenads are displayed in a non-prominent position).
Fur-thermore it seems that modifying the title of the adrather than the content yields better results: 0.9% vs.1.83% CTR (?2 = 6.24; 1 degree of freedom; ?
<0,01) even if these results require further assessmentwith dedicated experiments.As a side note, in this experiment we can seethe problem of extraneous variables: according toAdWords?
internal mechanisms, the experimentalcondition was displayed more often in the Googlesearch Network on the ?other?
position (277 vs. 219impressions - and overall 524 vs. 462), still from aresearch perspective this is not a interesting statisti-cal difference, and ideally should not be present (i.e.ads should get an equal amount of impressions foreach position).Conclusions and future workAdWords gives us an appropriate context for evalu-ating persuasive messages.
The advantages are fastexperiment building and evaluation, fully-automatedanalysis, and low cost.
By using keywords with alow CPC it is possible to run large-scale experimentsfor just a few euros.
AdWords proved to be very ac-curate, flexible and fast, far beyond our expectations.We believe careful design of experiments will yieldimportant results, which was unthinkable before thisopportunity for studies on persuasion appeared.The motivation for this work was exploration ofthe impact of short persuasive messages, so to assessthe effectiveness of different linguistic choices.
Theexperiments reported in this paper are illustrative ex-amples of the method proposed and are concernedwith the evaluation of the role of minimal affectivevariations of short expressions.
But there is enor-mous further potential in the proposed approach toecological crowdsourcing for NLP: for instance, dif-ferent rhetorical techniques can be checked in prac-tice with large audiences and fast feedback.
The as-sessment of the effectiveness of a change in the titleas opposed to the initial portion of the text body pro-vides a useful indication: one can investigate if vari-ations inside the given or the new part of an expres-sion or in the topic vs. comment (Levinson, 1983)have different effects.
We believe there is potentialfor a concrete extensive exploration of different lin-guistic theories in a way that was simply not realisticbefore.AcknowledgmentsWe would like to thank Enrique Alfonseca andSteve Barrett, from Google Labs, for valuable hintsand discussion on AdWords features.
The presentwork was partially supported by a Google ResearchAward.995ReferencesP.
Andrews, S. Manandhar, and M. De Boni.
2008.
Ar-gumentative human computer dialogue for automatedpersuasion.
In Proceedings of the 9th SIGdial Work-shop on Discourse and Dialogue, pages 138?147.
As-sociation for Computational Linguistics.S.
Aral and D. Walker.
2010.
Creating social contagionthrough viral product design: A randomized trial ofpeer influence in networks.
In Proceedings of the 31thAnnual International Conference on Information Sys-tems.T.
Brants and A. Franz.
2006.
Web 1t 5-gram corpusversion 1.1.
Linguistic Data Consortium.BJ Fogg.
2009.
Creating persuasive technologies: Aneight-step design process.
Proceedings of the 4th In-ternational Conference on Persuasive Technology.M.
Guerini, O.
Stock, and C. Strapparava.
2008.Valentino: A tool for valence shifting of natural lan-guage texts.
In Proceedings of LREC 2008, Mar-rakech, Morocco.M.
Guerini, C. Strapparava, and O.
Stock.
2010.
Evalu-ation metrics for persuasive nlp with google adwords.In Proceedings of LREC-2010.M.
Guerini, O.
Stock, M. Zancanaro, D.J.
O?Keefe,I.
Mazzotta, F. Rosis, I. Poggi, M.Y.
Lim, andR.
Aylett.
2011a.
Approaches to verbal persuasion inintelligent user interfaces.
Emotion-Oriented Systems,pages 559?584.M.
Guerini, C. Strapparava, and O.
Stock.
2011b.
Slant-ing existing text with Valentino.
In Proceedings of the16th international conference on Intelligent user inter-faces, pages 439?440.
ACM.S.C.
Levinson.
1983.
Pragmatics.
Cambridge Univer-sity Press.W.
Mason and S. Suri.
2010.
Conducting behavioralresearch on amazon?s mechanical turk.
Behavior Re-search Methods, pages 1?23.M.
Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, andA.
Marchetti.
2010.
Divide and conquer: Crowd-sourcing the creation of cross-lingual textual entail-ment corpora.
Proc.
of EMNLP 2011.E.
Reiter, R. Robertson, and L. Osman.
2003.
Lessonfrom a failure: Generating tailored smoking cessationletters.
Artificial Intelligence, 144:41?58.R.
Snow, B. O?Connor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expertannotations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263.
Association forComputational Linguistics.I.
Van Der Sluis and C. Mellish.
2010.
Towards empir-ical evaluation of affective tactical nlg.
In Empiricalmethods in natural language generation, pages 242?263.
Springer-Verlag.S.
Whitehead and L. Cavedon.
2010.
Generating shiftingsentiment for a conversational agent.
In Proceedingsof the NAACL HLT 2010 Workshop on ComputationalApproaches to Analysis and Generation of Emotion inText, pages 89?97, Los Angeles, CA, June.
Associa-tion for Computational Linguistics.996
