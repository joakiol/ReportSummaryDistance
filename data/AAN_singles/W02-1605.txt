Improving Translation Quality of Rule-based Machine TranslationPaisarn Charoenpornsawat, Virach Sornlertlamvanich and Thatsanee CharoenpornInformation Research and Development DivisionNational Electronics and Computer Technology Center112 Thailand Science Park, Paholyothin Rd.,Klong 1, Klong Luang, Pathumthani 12120THAILAND{paisarn, virach, thatsanee}@nectec.or.thAbstractThis paper proposes machine learningtechniques, which help disambiguate wordmeaning.
These methods focus on consideringthe relationship between a word and itssurroundings, described as context informationin the paper.
Context information is producedfrom rule-based translation such as part-of-speech tags, semantic concept, case relations andso on.
To automatically extract the contextinformation, we apply machine learningalgorithms which are C4.5, C4.5rule andRIPPER.
In this paper, we test on ParSit, whichis an interlingual-based machine translation forEnglish to Thai.
To evaluate our approach, anverb-to-be is selected because it has increased infrequency and it is quite difficult to be translatedinto Thai by using only linguistic rules.
Theresult shows that the accuracy of C4.5, C4.5ruleand RIPPER are 77.7%, 73.1% and 76.1%respectively whereas ParSit give accuracy only48%.IntroductionMachine translation has been developed formany decades.
Many approaches have beenproposed such as rule-based, statistic-based [5],and example-based approaches [3, 6, 11].However, there is no machine learning techniquethat meets human?s requirement.
Each techniquehas its own advantages and disadvantages.Statistic-based, example-based and corpus-basedapproaches were recently proposed.
A rule-based approach is the first strategy pursued byresearch in the field of machine translation.Rules are written from linguistic knowledge byhuman.
The strength is that it can deeply analyzein both syntax and semantic levels.
However, theweak points of this model are 1) it requires muchlinguistic knowledge.
2) it is impossible to writerules that cover all a language.
In many yearsago, a statistic-based and an example-based wereproposed.
These approaches do not requirelinguistic knowledge, but they need large size ofbilingual corpus.
A statistic-based approach usesstatistic of bilingual corpus and language model.The advantage is that it may be able to producesuitable translations even if a given sentence isnot similar to any sentences in a training corpus.In contrast, an example-based can produceappropriate translations in case of a givensentence must similar to any sentences in atraining data.
Nevertheless, a statistic-basedapproach cannot translate idioms and phrasesthat reflect long-distance dependency.To improve quality of a rule-basedmachine translation, we have to modify/addsome generation rules or analysis rules.
Thismethod requires much linguistic knowledge andwe cannot guarantee that accuracy will be better.For example, in case of modifying some rules, itdoes not only change incorrect sentences tocorrect sentences furthermore they may effect oncorrect sentences too.
The common errors ofmachine translation can be classified into twomain groups.
One is choosing incorrect meaningand the other is incorrect ordering.
In ourexperiments, we select ParSit in evaluation.ParSit is English-to-Thai machine translation byusing an interlingual-based approach [8].
Aninterlingual-based approach is a kind of rule-based machine translation.
The statistics ofincorrect meaning and incorrect ordering inParSit are 81.74% and 18.26% respectively.Therefore, in this paper, we address on choosinga correct meaning.
We use context information,words and part-of-speech tags, in classifying thecorrect meaning.
This paper, we apply machinelearning algorithms, C4.5, C4.5rule, andRIPPER, to automatically extract words andpart-of-speech tags.We develop a computer system for sentence translationSyntax & Semantic analysis for EnglishParsit1.
A Rule-Based Approach: Case StudyParSit: English to Thai MachineTranslation.
develop agent proposeobject In this section, we will briefly describe a rule-based machine translation.
Each rule-basedmachine translation has its own mythology intranslation.
Hence in this paper, we select ParSitas a case study.
ParSit is English to Thaimachine translation using an interlingual-basedapproach.
ParSit consists of four modules thatare a syntax analysis module, a semanticanalysis module, a semantic generation module,and a syntax generation module.
An example ofParSit translation is shown in figure 1.system translationwemodifier objectcomputer sentenceInterlingual treeSyntax & Semantic generation for ThaiIn figure 1, the English sentence, ?Wedevelop a computer system for sentencetranslation.
?, input into ParSit.
Both syntax andsemantic analysis modules analyze the sentenceand then transform into the interlingual treewhich is shown in Figure 1.
In the interlingualtree shows the relationship between words suchas 1) ?We?
is an agent of ?develop?
2) ?system?is an object of ?develop?
3)  ?computer?
ismodifier of  ?system?
and so on.
Finally, Thaisentence, ??????????????????????????????????????????
?, is generated from the interlingual treeby the syntax and semantic generation modules.??????
?????
????
???????????
?????
??????
?????
?Figure 1: ParSit translation process.o Generating over words.This is the house in which she lives.Incorrect: ?????????????????????????????????
?Correct:  ????????????????????????
?o Using an incorrect word.The news that she died was a greatshock.The errors of translation from ParSit canbe classified into two main groups.
One isincorrect meaning and the other is incorrectordering.
The incorrect meaning also can bereclassified into three categories; 1).
missingsome words 2).
generating over words 3).
usingincorrect word The examples of errors areshown below.Incorrect: ?????????????????????????????????????
?Correct:  ??????????????????????????????????????????
Incorrect ordering errors.
?
Incorrect meaning errors.
He is wrong to leave.o Missing some words.
Incorrect: ?????????????
?The city is not far from hereCorrect:     ??????????????
Incorrect: ????????????????
?Correct:      ????????????????????
?We evaluated ParSit by using 770-English-sentence corpus that is designed byJapan Electronic Industry DevelopmentAssociation (JEIDA).
This corpus has thecharacteristics for testing in word level suchas concept mismatching, word absence andetc.
and sentence level such as grammar andmodifier misplacement.
The statistics ofParSit errors are shown in Table 1.Table 1.
Statistics of ParSit ErrorIncorrect meaning errorsM (%) G (%) U (%)Incorrect orderingerrors (%)16.71 13.31 51.42 18.26In table 1, M, G and U mean missingsome word errors, generating over word errorsand using incorrect word errors respectively.According to Table 1, ParSit makes manyerrors in choosing incorrect meaning (81.74%).In this paper, we focus on solving the problemof choosing incorrect meaning.
To decide whatis the correct meaning of a word, we propose touse context information around that word.Context information that we use will bedescribed in the next section.2 Applying Machine Learning Technique2.1  Context InformationThere are many kinds of context informationthat useful to decide the appropriate meaning ofa word such as grammatical rules, collocationwords, context words, semantic concept and etc.Context information is derived from a rule-basemachine translation.
Words and their part-of-speech tags are the simplest information, whichare produced from English analysis module.
Inthis paper, we use words and/or part-of-speechtags around a target word in deciding a wordmeaning.2.2 Machine LearningIn this section, we will briefly descript threemachine leaning techniques, C4.5, C4.5rule andRIPPER.2.2.1 C4.5 & C4.5RuleC4.5, decision tree, is a traditional classifyingtechnique that proposed by Quinlan [7].
C4.5have been successfully applied in many NLPproblems such as word extraction [9] andsentence boundary disambiguation [2].
So in thispaper, we employ C4.5 in our experiments.The induction algorithm proceeds byevaluation content of series of attributes anditeratively building a tree from the attributevalues with the leaves of the decision tree beingthe valued of the goal attribute.
At each step oflearning procedure, the evolving tree is branchedon the attribute that partitions the data itemswith the highest information gain.
Branches willbe added until all items in the training set areclassified.
To reduce the effect of overfitting,C4.5 prunes the entire decision tree constructed.It recursively examines each subtree todetermine whether replacing it with a leaf orbranch would reduce expected error rate.
Thispruning makes the decision tree better in dealingwith the data different from training data.In C4.5 version 8, it provides the othertechnique, which is extended from C4.5 calledC4.5rule.
C4.5rule extracts production rulesfrom an unpruned decision tree produced byC4.5, and then improves process by greedilydeletes or adds single rules in an effort to reducedescription length.
So in this paper we alsoemploy both techniques of C4.5 and C4.5rule.2.2.2 RIPPERRIPPER [10] is the one of the famous machinelearning techniques applying in NLP problems[4], which was  proprosed by William W.Cohen.
On his experiment [10] shows thatRIPPER is more efficient than C4.5 on noisydata and it scales nearly linearly with thenumber of examples in a dataset.
So we decideto choose RIPPER in evaluating and comparingresults with C4.5 and C4.5rule.RIPPER is a propositional rule learningalgorithm that constructs a ruleset whichclassifies the training data [11].
A rule in theconstructed ruleset is represented in the form ofa conjunction of conditions:if T1 and T2 and ... Tn then class Cx.T1 and T2 and ... Tn is called the body of the rule.Cx is a target class to be learned; it can be apositive or negative class.
A condition Ti testsfor a particular value of an attribute, and it takesone of four forms: An = v,  Ac ?
?, Ac ?
?
and  v3 Overview of The SystemFigure 2 : Overview of the systemMachine learningTranslated sentence withimproving qualityRules or treefrom trainingdataRules or tree from training dataInput sentenceRule-based MT(ParSit)Translated sentence Context information(words and POS)Correct the translatedsentence by humanMachine learningRule-based MT(ParSit)Context information(words and POS)Translated sentenceInput sentence In this section, we will describe the process ofour system in Figure 2.
First, input a sourcesentence into rule-based MT and then use syntaxand semantic rules for analysing the sentence.
Atthis step, rule-based MT gives various kinds ofword information.
In this experiment we usedonly words and part-of-speech tags.
Afteranalysing, rule-based MT generates a sentenceinto target language.
Next, the translatedsentence from rule-based MT and the contextinformation are parsed into machine learning.Machine learning requires a rule set or adecision tree, which are generated from atraining set, to decide what is the appropriatemeaning of a word.In training module (Figure 3), we parseEnglish sentences with part-of-speech tags,which are given by ParSit, and assign the correctmeaning by linguists into machine learningmodule.
The machine learning will learn andproduces a rule set or a decision tree fordisambiguating word meaning.
The process oftraining is shown in Figure 3.4 Preliminary Experiments & Results.To evaluate our approach, we should test on aword, which frequently occurred in normal textand has several meanings.
According to thestatistics of word usage from 100M-word BritishNational Corpus, verb-to-be occurred more thanthee million times, and translation of verb-to-beinto Thai is quite difficult by using onlylinguistic rules.
Therefore our experiment, wetest our approach on verb-to-be.?
As, where An is a nominal attribute and v is alegal value for An; or Ac is a continuous variableand ?
is some value for Ac that occurs in thetraining data; or As is a set-value attribute and vis a value that is an element of As.
In fact, acondition can include negation.
A set-valuedattribute is an attribute whose value is a set ofstrings.
The primitive tests on a set-valuedattribute As are of the form ?v ?
As?.
Whenconstructing a rule, RIPPER finds the test thatmaximizes information gain for a set ofexamples S efficiently, making only a singlepass over S for each attribute.
All symbols v, thatappear as elements of attribute A for sometraining examples, are considered by RIPPER.Figure 3 : The training moduleIn the experiment, we use 3,200 Englishsentences from Japan Electronic DictionaryResearch Institute (EDR).
EDR corpus iscollected from news, novel and journal.
Thenour linguists manually assigned the suitablemeaning of verb-to-be in Thai.
In training andtesting steps, we divided data into two groups.The first is 700 sentences for testing and theother is for training.
We use various sizes of atraining data set and different sizes of contextinformation.Table 2, 3 and 4 are the result fromC4.5, C4.5rule and RIPPER respectively.
Theseries in columns represent the number oftraining sentences.
The row headers show thetypes of context information that Pos?n,Word?n and P&W?n mean part-of-speech tags,words and part-of-speech tags and words withthe window size is n.Table 2.
The results from C4.5100 500 1 K 1.5K 2K 2.5KPos?1 67.1 69.8 69.8 69.8 69.8 69.8Pos?2 67.1 69.8 69.8 69.8 69.8 69.8Pos?3 67.1 69.8 69.8 69.8 69.8 69.8Word?1 55.5 63.2 73.1 74.2 75.5 75.4Word?2 57.7 64.6 71.7 72.7 75.5 77.3Word?3 57.8 65.3 71.3 73.1 75.4 77.7P&W?1 55.5 68.6 71.1 71.3 71.8 71.8P&W?2 57.7 68.6 71.3 70.4 71.8 71.8P&W?3 57.8 68.6 71.3 69.6 71.3 71.9Table 3: The results from C4.5rule100 500 1 K 1.5K 2K 2.5KPos?1 69.8 71.3 76.3 77.3 76.0 73.1Pos?2 69.8 77.5 76.7 76.9 76.3 73.1Pos?3 69.2 77.2 76.2 76.8 70.1 73.1Word?1 54.9 73.1 63.4 63.6 67.2 71.1Word?2 56.3 73.5 73.5 72.5 64.7 70.6Word?3 56.3 72.2 72.5 72.3 76.8 70.6P&W?1 54.9 77.2 63.4 68.4 69.2 71.1P&W?2 56.8 76.7 73.5 68.0 70.5 70.6P&W?3 56.8 69.6 64.3 61.8 71.5 71.1Table 4: The results from RIPPER.100 500 1 K 1.5K 2K 2.5KPos?1 70.2 70.9 73.3 71.7 72.1 76.1Pos?2 69.4 71.0 69.2 70.2 70.8 72.1Pos?3 69.2 71.0 69.6 71.3 76.9 70.6Word?1 63.1 69.8 67.2 72.1 72.9 71.1Word?2 55.3 67.7 66.8 74.0 72.2 70.6Word?3 58.0 70.5 66.8 71.7 72.3 70.6P&W?1 72.7 73.9 73.3 73.5 73.4 76.1P&W?2 57.7 72.3 69.2 73.5 72.2 72.1P&W?3 62.0 70.4 69.6 72.1 72.6 70.6According to the result from C4.5 inTable 2, with data size is not more than 500sentences, C4.5 makes good accuracy by usingonly part-of-speech tags with any window sizes.In case of a training data set is equal or morethan 1000 sentences, considering only wordsgive the best accuracy and the suitable windowsize is depend on the size of training data set.
InTable 3, C4.5rule gives high accuracies onconsidering only part-of-speech tags with anywindow sizes.
In table 4, RIPPER produces highaccuracies by investigating only one word andone part-of-speech tag before and after verb-to-be words.ConclusionC4.5, C4.5rule and RIPPER have efficiency inextracting context information from a trainingcorpus.
The accuracy of these three machinelearning techniques is not quite different, andRIPPER gives the better results than C4.5 andC4.5rule do in a small train set.
The appropriatecontext information depends on machinelearning algorithms.
The suitable contextinformation giving high accuracy in C4.5,C4.5rule and RIPPER are ?3 words around atarget word, part-of-speech tags with anywindow sizes and ?1 word and part-of-speechtag respectivelyThis can prove that our approach has asignificant in improving a quality of translation.The advantages of our method are 1) adaptivemodel, 2) it can apply to another languages, and3).
It is not require linguistic knowledge.In future experiment, we will includeother machine learning techniques such asWinnow[1] and increase other contextinformation such as semantic, grammar.AcknowledgementsSpecial thanks to Mr. Sittha Phaholphinyo formarking up the correct meaning of verb-to-bewords and Mr. Danoopon Nanongkhai, internstudent from Computer EngineeringDepartment, Kasertsart University, for his helpin testing the experiments.References[1] Andrew R. Golding and Dan Roth.
1999.A Winnow-Based Approach to Context SensitiveSpelling Correction, Machine Learning, Specialissue on Machine Learning and Natural LanguageProcessing, Volume 34, pp.
107-130.
[2] David D. Palmer Marti A. Hearst 1994.
AdaptiveSentence Boundary Disambiguation.
In theProceedings of the Fourth ACL Conference onApplied Natural Language Processing, Stuttgart.
[3] Michael Carl.
1999: Inducing TranslationTemplates for Example-Based Machine Translation,In the Proceeding of MT-Summit VII, Singapore.
[4] Paisarn Charoenpornsawat., Boonserm Kijsirikul.and Surapant Meknavin.
1998.
Feature-based ThaiUnknown Word Boundary Identification UsingWinnow.
In Proceedings of the 1998 IEEE Asia-Pacific Conference on Circuits and Systems(APCCAS?98).
[5] Peter F. Brown, John Cocke and etc.
statiscalapproach to machine translation.
Computationallinguistics 16, 1990[6] Ralf D. Brown 1996.
Example-Based MachineTranslation  in the PanGloss System.
InProceedings of the Sixteenth InternationalConference on Computational Linguistics, Page169-174, Copenhagen, Denmark.
[7] Ross Quinlan.
1993.
C4.5: Programs for MachineLearning Morgan Kauffman.
[8] Virach Sornlertlamvanich and WantaneePhantachat 1993.
Interlingual Expression for ThaiLanguage.
Technical report.
Linguistic andKnowledge Engineering Laboratory, NationalElectronics and Computer Technology Center,Thailand.
[9] Virach sornlertlamvanich, Tanapong Potipiti andThatsanee Charoenporn.
2000.
Automatic Corpus-Based Thai Word Extraction with the C4.5 LeaningAlgorithm.
Proceedings of the 18th InternationalConference on Computational Linguistics(COLING2000), Saarbrucken, Germany.
[10] William W. Cohen.
1995 Fast effective ruleinduction, In Proceedings of the TwelfthInternational Conference on Machine Learning,Lake Taho, California, Morgan Kauffman.
[11] Ying Zhang, Ralf D. Brown, and Robert E.Frederking, 2001.
Adapting an Example-BasedTranslation System to Chinese.
In Proceedings ofHuman Language Technology Conference 2001 p.7-10.
San Diego, California, March 18-21, 2001.Computational Linguistics, ComputationalLinguistics, 11/1, pp.
18?27.
