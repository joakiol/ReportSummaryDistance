Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154?162,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTraining Paradigms for Correcting Errors in Grammar and UsageAlla Rozovskaya and Dan RothUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{rozovska,danr}@illinois.eduAbstractThis paper proposes a novel approach to theproblem of training classifiers to detect andcorrect grammar and usage errors in text byselectively introducing mistakes into the train-ing data.
When training a classifier, we wouldlike the distribution of examples seen in train-ing to be as similar as possible to the one seenin testing.
In error correction problems, suchas correcting mistakes made by second lan-guage learners, a system is generally trainedon correct data, since annotating data for train-ing is expensive.
Error generation methodsavoid expensive data annotation and createtraining data that resemble non-native datawith errors.We apply error generation methods and trainclassifiers for detecting and correcting arti-cle errors in essays written by non-native En-glish speakers; we show that training on datathat contain errors produces higher accuracywhen compared to a system that is trained onclean native data.
We propose several train-ing paradigms with error generation and showthat each such paradigm is superior to traininga classifier on native data.
We also show thatthe most successful error generation methodsare those that use knowledge about the arti-cle distribution and error patterns observed innon-native text.1 IntroductionThis paper considers the problem of training clas-sifiers to detect and correct errors in grammar andword usage in text.
Both native and non-nativespeakers make a variety of errors that are not alwayseasy to detect.
Consider, for example, the problemof context-sensitive spelling correction (e.g., (Gold-ing and Roth, 1996; Golding and Roth, 1999; Carl-son et al, 2001)).
Unlike spelling errors that result innon-words and are easy to detect, context-sensitivespelling correction task involves correcting spellingerrors that result in legitimate words, such as confus-ing peace and piece or your and you?re.
The typicaltraining paradigm for these context-sensitive ambi-guities is to use text assumed to be error free, replac-ing each target word occurrence (e.g.
peace) with aconfusion set consisting of, say {peace, piece}, thusgenerating both positive and negative examples, re-spectively, from the same context.This paper proposes a novel error generation ap-proach to the problem of training classifiers for thepurpose of detecting and correcting grammar andusage errors in text.
Unlike previous work (e.g.,(Sjo?bergh and Knutsson, 2005; Brockett et al, 2006;Foster and Andersen, 2009)), we selectively intro-duce mistakes in an appropriate proportion.
In par-ticular, to create training data that closely resembletext with naturally occurring errors, we use error fre-quency information and error distribution statisticsobtained from corrected non-native text.
We applythe method to the problem of detecting and correct-ing article mistakes made by learners of English asa Second Language (ESL).The problem of correcting article errors is gener-ally viewed as that of article selection, cast as a clas-sification problem and is trained as described above:a machine learning algorithm is used to train a clas-sifier on native English data, where the possible se-lections are used to generate positive and negative154examples (e.g., (Izumi et al, 2003; Han et al, 2006;De Felice and Pulman, 2008; Gamon et al, 2008)).The classifier is then applied to non-native text topredict the correct article in context.
But the articlecorrection problem differs from the problem of ar-ticle selection in that we know the original (source)article that the writer used.
When proposing a cor-rection, we would like to use information about theoriginal article.
One reason for this is that about 90%of articles are used correctly by ESL learners; this ishigher than the performance of state-of-the-art clas-sifiers for article selection.
Consequently, not us-ing the writer?s article, when making a prediction,may result in making more mistakes than there arein the data.
Another reason is that statistics on ar-ticle errors (e.g., (Han et al, 2006; Lee and Sen-eff, 2008)) and in the annotation performed for thepresent study reveal that non-native English speak-ers make article mistakes in a consistent manner.The system can consider the article used by thewriter at evaluation time, by proposing a correctiononly when the confidence of the classifier is highenough, but the article cannot be used in trainingif the classifier is trained on clean native data thatdo not have errors.
Learning Theory says that thedistribution of examples seen in testing should beas similar as possible to the one seen in training, soone would like to train on errors similar to those ob-served in testing.
Ideally, we would like to train us-ing corrected non-native text.
In that case, the orig-inal article of the writer can be used as a feature forthe classifier and the correct article, as judged bya native English speaker, will be viewed as the la-bel.
However, obtaining annotated data for trainingis expensive and, since the native training data donot contain errors, we cannot use the writer?s articleas a feature for the classifier.This paper compares the traditional trainingparadigm that uses native data to training paradigmsthat use data with artificial mistakes.
We proposeseveral methods of generating mistakes in nativetraining data and demonstrate that they outperformthe traditional training paradigm.
We also show thatthe most successful error generation methods useknowledge about the article distribution and errorpatterns observed in the ESL data.The rest of the paper is organized as follows.First, we discuss the baseline on the error correc-tion task and show why the baselines used in selec-tion tasks are not relevant for the error correctiontask.
Next, we describe prior work in error genera-tion and show the key difference of our approach.Section 4 presents the ESL data that we use andstatistics on article errors.
Section 5 describes train-ing paradigms that employ error generation.
In Sec-tions 6 and 7 we present the results and discuss theresults.
The key findings are summarized in Table 7in Section 6.
We conclude with a brief discussion ofdirections for future work.2 Measuring Success in Error CorrectionTasksThe distinction between the selection and the errorcorrection tasks alluded to earlier is important notonly for training but also in determining an appro-priate evaluation method.The standard baseline used in selection tasks isthe relative frequency of the most common class.For example, in word sense disambiguation, thebaseline is the most frequent sense.
In the taskof article selection, the standard baseline used isto predict the article that occurs most frequently inthe data (usually, it is the zero article, whose fre-quency is 60-70%).
In this context, the performanceof a state-of-the-art classifier (Knight and Chander,1994; Minnen et al, 2000; Turner and Charniak,2007; Gamon et al, 2008) whose accuracy is 85-87% is a significant improvement over the base-line.
The majority has been used as the baseline alsoin the context-sensitive spelling task (e.g., (Goldingand Roth, 1999)).However, in article correction, spelling correc-tion, and other text correction applications the splitof the classes is not an appropriate baseline since themajority of the words in the confusion set are usedcorrectly in the text.
Han et al (2006) report an av-erage error rate of 13% on article data from TOEFLessays, which gives a baseline of 87%, versus thebaseline of 60-70% used in the article selection task.Statistics on article mistakes in our data suggest abaseline of about 90%, depending on the source lan-guage of the writer.
So the real baseline on the taskis ?do nothing?.
Therefore, to determine the base-line for a correction task, one needs to consider theerror rate in the data.155Using the definitions of precision and recall andthe ?real?
baseline, we can also relate the resultingaccuracy of the classifier to the precision and recallon an error correction task as follows: Let P and Rdenote the precision and recall, respectively, of thesystem on an error correction task, and Base denotethe error rate in the data.
Then the task baseline (i.e.,accuracy of the data before running the system) is:Baseline = 1?BaseIt can be shown that the error rate after running theclassifier is:Error =Base ?
(P + R?
2RP )PIt follows that the accuracy of the system on the taskis 1?
Error.For example, we can obtain a rough estimate onthe accuracy of the system in Han et al (2006), us-ing precision and recall numbers by error type.
Ex-cluding the error type of category other, we can esti-mate that Base = 0.1, so the baseline is 0.9, averageprecision and recall are 0.85 and 0.25, respectively,and the resulting overall accuracy of the system is92.2%.3 Related Work3.1 Generating Errors in TextIn text correction, adding mistakes in training hasbeen explored before.
Although the general ap-proach has been to produce errors similar to thoseobserved in the data to be corrected, mistakes wereadded in an ad-hoc way, without respecting the er-ror frequencies and error patterns observed in non-native text.
Izumi et al (2003) train a maxi-mum entropy model on error-tagged data from theJapanese Learners of English corpus (JLE, (Izumi etal., 2004)) to detect 8 error types in the same cor-pus.
They show improvement when the training setis enhanced with sentences from the same corpusto which artificial article mistakes have been added.Though it is not clear in what proportion mistakeswere added, it is also possible that the improvementwas due to a larger training set.
Foster and Ander-sen (2009) attempt to replicate naturally occurringlearner mistakes in the Cambridge Learner Corpus(CLC)1, but show a drop in accuracy when the orig-inal error-tagged data in training are replaced withcorrected CLC sentences containing artificial errors.Brockett et al (2006) generate mass noun er-rors in native English data using relevant exam-ples found in the Chinese Learners English Cor-pus (CLEC, (Gui and Yang, 2003)).
Training dataconsist of an equal number of correct and incor-rect sentences.
Sjo?bergh and Knutsson (2005) in-troduce split compound and agreement errors intonative Swedish text: agreement errors are added inevery sentence and for compound errors, the train-ing set consists of an equal number of negative andpositive examples.
Their method gives higher recallat the expense of lower precision compared to rule-based grammar checkers.To sum up, although the idea of using data with ar-tificial mistakes is not new, the advantage of trainingon such data has not been investigated.
Moreover,training on error-tagged data is currently unrealisticin the majority of error correction scenarios, whichsuggests that using text with artificial mistakes is theonly alternative to using clean data.
However, it hasnot been shown whether training on data with artifi-cial errors is beneficial when compared to utilizingclean data.
More importantly, error statistics havenot been considered for error correction tasks.
Leeand Seneff (2008) examine statistics on article andpreposition mistakes in the JLE corpus.
While theydo not suggest a specific approach, they hypothesizethat it might be helpful to incorporate this knowl-edge into a correction system that targets these twolanguage phenomena.3.2 Approaches to Detecting Article MistakesAutomated methods for detecting article mistakesgenerally use a machine learning algorithm.
Ga-mon et al (2008) use a decision tree model and a5-gram language model trained on the English Giga-word corpus (LDC2005T12) to correct errors in En-glish article and preposition usage.
Han et al (2006)and De Felice and Pulman (2008) train a maximumentropy classifier.
Yi et al (2008) propose a webcount-based system to correct determiner errors.
Inthe above approaches, the classifiers are trained onnative data.
Therefore the classifiers cannot use the1http://www.cambridge.org/elt156original article that the writer used as a feature.
Hanet al (2006) use the source article at evaluation timeand propose a correction only when the score of theclassifier is high enough, but the source article is notused in training.4 Article Errors in ESL DataArticle errors are one of the most common mistakesthat non-native speakers make, especially thosewhose native language does not have an article sys-tem.
For example, Han et al (2006) report that inthe annotated TOEFL data by Russian, Chinese, andJapanese speakers 13% of all noun phrases have anincorrect article.
It is interesting to note that articleerrors are present even with very advanced speakers.While the TOEFL data include essays by students ofdifferent proficiency levels, we use data from veryadvanced learners and find that error rates on articlesare similar to those reported by Han et al (2006).We use data from speakers of three first languagebackgrounds: Chinese, Czech, and Russian.
Noneof these languages has an article system.
The Czechand the Russian data come from the ICLE corpus(Granger et al, 2002), which is a collection of es-says written by advanced learners of English.
TheChinese data is a part of the CLEC corpus that con-tains essays by students of all levels of proficiency.4.1 Data AnnotationA portion of data for each source language was cor-rected and error-tagged by native speakers.
The an-notation was performed at the sentence level: a sen-tence was presented to the annotator in the contextof the entire essay.
Essay context can become nec-essary, when an article is acceptable in the contextof a sentence, but is incorrect in the context of theessay.
Our goal was to correct all article errors, in-cluding those that, while acceptable in the context ofthe sentence, were not correct in the context of theessay.
The annotators were also encouraged to pro-pose more than one correction, as long as all of theirsuggestions were consistent with the essay context.The annotators were asked to correct all mistakesin the sentence.
The annotation schema includedthe following error types: mistakes in article andpreposition usage, errors in noun number, spelling,verb form, and word form2.
All other correctionswere marked as word replacement, word deletion,and word insertion.
For details about annotation anddata selection, please refer to the companion paper(Rozovskaya and Roth, 2010).4.2 Statistics on Article ErrorsTraditionally, three article classes are distinguished:the, a(an)3 and None (no article).
The training andthe test data are thus composed of two types ofevents:1.
All articles in the data2.
Spaces in front of a noun phrase if that nounphrase does not start with an article.
To identifythe beginning of a noun phrase, we ran a part-of-speech tagger and a phrase chunker4 and ex-cluded all noun phrases not headed5 by a per-sonal or demonstrative pronoun.Table 1 shows the size of the test data by sourcelanguage, proportion of errors and distribution of ar-ticle classes before and after annotation and com-pares these distributions to the distribution of articlesin English Wikipedia.
The distribution before anno-tation shows statistics on article usage by the writersand the distribution after annotation shows statisticsafter the corrections made by the annotators wereapplied.
As the table shows, the distribution of arti-cles is quite different for native data (Wikipedia) andnon-native text.
In particular, non-native data have alower proportion of the.The annotation statistics also reveal that learn-ers do not confuse articles randomly.
From Table2, which shows the distribution of article errors bytype, we observe that the majority of mistakes areomissions and extraneous articles.
Table 3 showsstatistics on corrections by source and label, wheresource refers to the article used by the writer, andlabel refers to the article chosen by the annotator.Each entry in the table indicates Prob(source =2Our classification, was inspired by the classification pre-sented in Tetreault and Chodorow (2008)3Henceforth, we will use a to refer to both a and an4The tagger and the chunker are available at http://L2R.cs.uiuc.edu/?cogcomp/software.php5We assume that the last word of the noun phrase is its head.157Source Number of Proportion of Errors Article Classeslanguage test examples errors total distribution a the NoneChinese 1713 9.2% 158Before annotation 8.5 28.2 63.3After annotation 9.9 24.9 65.2Czech 1061 9.6% 102Before annotation 9.1 22.9 68.0After annotation 9.9 22.3 67.8Russian 2146 10.4% 224Before annotation 10.5 21.7 67.9After annotation 12.5 20.1 67.4English Wikipedia 9.6 29.1 61.4Table 1: Statistics on articles in the annotated data before and after annotation.Source Proportion of Errors total Errors by Typelanguage errors in the data Extraneous Missing a Missing the ConfusionChinese 9.2% 158 57.0% 13.3% 22.8% 7.0%Czech 9.6% 102 45.1% 14.7% 33.3% 6.9%Russian 10.4% 224 41.5% 20.1% 25.5% 12.3%Table 2: Distribution of article errors in the annotated data by error type.
Extraneous refers to using a or the whereNone (no article) is correct.
Confusion is using a instead of the or vice versa.Label Source Sourcelanguage a the NoneaChinese 81.7% 5.9% 12.4%Czech 81.0% 4.8% 14.3%Russian 75.3% 7.9% 16.9%theChinese 0.2% 91.3% 8.5%Czech 0.9% 84.7% 14.4%Russian 1.9% 84.9% 13.2%NoneChinese 0.6% 7.4%% 92.0%Czech 1.3% 5.2% 93.6%Russian 1.0% 5.4%% 93.6%Table 3: Statistics on article corrections by the originalarticle (source) and the annotator?s choice (label).
Eachentry in the table indicates Prob(source = s|label = l)for each article pair.s|label = l) for each article pair.
We can also ob-serve specific error patterns.
For example, the ismore likely than a to be used superfluously.5 Introducing Article Errors into TrainingDataThis section describes experiments with error gener-ation methods.
We conduct four sets of experiments.Each set differs in how article errors are generated inthe training data.
We now give a description of errorgeneration paradigms in each experimental set.5.1 Methods of error generationWe refer to the article that the writer used in the ESLdata as source, and label refers to the article thatthe annotator chose.
Similarly, when we introduceerrors into the training data, we refer to the originalarticle as label and to the replacement as source.This is because the original article is the correctarticle choice, and the replacement that the classifierwill see as a feature can be an error.
We call thisfeature source feature.
In other words, both fortraining (native data) and test (ESL data), sourcedenotes the form that the classifier sees as a feature(which could be an error) and label denotes thecorrect article.
Below we describe how errors aregenerated in each set of experiments.Method 1: General With probability x each ar-ticle in the training data is replaced witha different article uniformly at random, andwith probability (1 ?
x) it remains un-changed.
We build six classifiers, where x?
{5%, 10%, 12%, 14%, 16%, 18%}.
We callthis method general since it uses no informa-tion about article distribution in the ESL data.Method 2: ArticleDistrBeforeAnnot We use thedistribution of articles in the ESL data beforethe annotation to change the distribution of ar-ticles in the training.
Specifically, we changethe articles so that their distribution approxi-mates the distribution of articles in the ESLdata.
For example, the relative frequency ofthe in English Wikipedia data is 29.1%, whilein the writing by Czech speakers it is 22.3%.It should be noted that this method changesthe distribution only of source articles, but the158distribution of labels is not affected.
An ad-ditional constraint that we impose is the mini-mum error rate r for each article class, so thatProb(s|l) ?
r ?l ?
labels.
In this fashion, foreach source language we train four classifiers,where we use article distribution from Chinese,Czech, and Russian, and where we set the min-imum error rate r to be ?
{2%, 3%, 4%, 5%}.Method 3: ArticleDistrAfterAnnot This methodis similar to the one above but we use the dis-tribution of articles in the ESL data after thecorrections have been made by the annotators.Method 4: ErrorDistr This method uses informa-tion about error patterns in the annotated ESLdata.
For example, in the Czech annotated sub-corpus, label the corresponds to source the in85% of the cases and corresponds to sourceNone in 14% of the cases.
In other words, in14% of the cases where the article the shouldhave been used, the writer used no article at all.Thus, with probability 14% we change the inthe training data to None.6 Experimental ResultsIn this section, we compare the quality of the sys-tem trained on clean native English data to the qual-ity of the systems trained on data with errors.
Theerrors were introduced into the training data usingerror generation methods presented in Section 5.In each training paradigm, we follow a discrimi-native approach, using an online learning paradigmand making use of the Averaged Perceptron Al-gorithm (Freund and Schapire, 1999) implementedwithin the Sparse Network of Winnow framework(Carlson et al, 1999) ?
we use the regularizedversion in Learning Based Java6 (LBJ, (Rizzoloand Roth, 2007)).
While classical Perceptroncomes with generalization bound related to the mar-gin of the data, Averaged Perceptron also comeswith a PAC-like generalization bound (Freund andSchapire, 1999).
This linear learning algorithm isknown, both theoretically and experimentally, tobe among the best linear learning approaches andis competitive with SVM and Logistic Regression,6LBJ code is available at http://L2R.cs.uiuc.edu/?cogcomp/asoftware.php?skey=LBJwhile being more efficient in training.
It also hasbeen shown to produce state-of-the-art results onmany natural language applications (Punyakanok etal., 2008).Since the methods of error generation described inSection 5 rely on the distribution of articles and ar-ticle mistakes and these statistics are specific to thefirst language of the writer, we conduct evaluationseparately for each source language.
Thus, for eachlanguage group, we train five system types: one sys-tem is trained on clean English data without errors(the same classifier for the three language groups)and four systems are trained on data with errors,where errors are produced using the four methodsdescribed in Section 5.
Training data are extractedfrom English Wikipedia.All of the five systems employ the same set of fea-tures based on three tokens to the right and to the leftof the target article.
For each context word, we useits relative position, its part-of-speech tag and theword token itself.
We also use the head of the nounphrase and the conjunctions of the pairs and triplesof the six tokens and their part-of-speech tags7.
Inaddition to these features, the classifiers trained ondata with errors also use the source article as a fea-ture.
The classifier that is trained on clean Englishdata cannot use the source feature, since in trainingthe source always corresponds to the label.
By con-trast, when the training data contain mistakes, thesource is not always the same as the label, the situa-tion that we also have with the test (ESL) data.We refer to the classifier trained on clean dataas TrainClean.
We refer to the classifiers trainedon data with mistakes as TWE (TrainWithErrors).There are four types of TWE systems for each lan-guage group, one for each of the methods of errorgeneration described in Section 5.
All results are theaveraged results of training on three random sam-ples from Wikipedia with two million training ex-amples on each round.
All five classifiers are trainedon exactly the same set of Wikipedia examples, ex-cept that we add article mistakes to the data usedby the TWE systems.
The TrainClean systemachieves an accuracy of 87.10% on data from En-glish Wikipedia.
This performance is state-of-the-7Details about the features are given in the paper?s web page,accessible from http://L2R.cs.uiuc.edu/?cogcomp/159art compared to other systems reported in the lit-erature (Knight and Chander, 1994; Minnen et al,2000; Turner and Charniak, 2007; Han et al, 2006;De Felice and Pulman, 2008).
The best resultsof 92.15% are reported by De Felice and Pulman(2008).
But their system uses sophisticated syntac-tic features and they observe that the parser does notperform well on non-native data.As mentioned in Section 4, the annotation of theESL data consisted of correcting all errors in the sen-tence.
We exclude from evaluation examples thathave spelling errors in the 3-word window aroundthe target article and errors on words that immedi-ately precede or immediately follow the article, assuch examples would obscure the evaluation of thetraining paradigms.Tables 4, 5 and 6 show performance by languagegroup.
The tables show the accuracy and the er-ror reduction on the test set.
The results of systemsTWE (methods 2 and 3) that use the distribution ofarticles before and after annotation are merged andappear as ArtDistr in the tables, since, as shownin Table 1, these distributions are very similar andthus produce similar results.
Each table comparesthe performance of the TrainClean system to theperformance of the four systems trained on data witherrors.For all language groups, all classifiers of typeTWE outperform the TrainClean system.
Thereduction in error rate is consistent when the TWEclassifiers are compared to the TrainClean system.Table 7 shows results for all three languages, com-paring for each language group the TrainCleanclassifier to the best performing system of typeTWE.Training Errors in Accuracy Errorparadigm training reductionTrainClean 0.0% 91.85% -2.26%TWE(General) 10.0% 92.57% 6.78%TWE(ArtDistr) 13.2% 92.67% 8.33%TWE(ErrorDistr) 9.2% 92.31% 3.51%Baseline 92.03%Table 4: Chinese speakers: Performance of theTrainClean system (without errors in training) and ofthe best classifiers of type TWE.
Rows 2-4 show theperformance of the systems trained with error generationmethods described in 5.
Error reduction denotes the per-centage reduction in the number of errors when comparedto the number of errors in the ESL data.Training Errors in Accuracy Errorparadigm training reductionTrainClean 0.0% 91.82% 10.31%TWE(General) 18.0% 92.22% 14.69%TWE(ArtDistr) 21.6% 92.00% 12.28%TWE(ErrorDistr) 10.2% 92.15% 13.93%Baseline 90.88%Table 5: Czech speakers: Performance of theTrainClean system (without errors in training) and ofthe best classifiers of type TWE.
Rows 2-4 show theperformance of the systems trained with error generationmethods described in 5.
Error reduction denotes the per-centage reduction in the number of errors when comparedto the number of errors in the ESL data.Training Errors in Accuracy Errorparadigm training reductionTrainClean 0.0% 90.62% 5.92%TWE(General) 14.0% 91.25% 12.24%TWE(ArtDistr) 18.8% 91.52% 14.94%TWE(ErrorDistr) 10.7% 91.63% 16.05%Baseline 90.03%Table 6: Russian speakers: Performance of theTrainClean system (without errors in training) and ofthe best classifiers of type TWE.
Rows 2-4 show theperformance of the systems trained with error generationmethods described in 5.
Error reduction denotes the per-centage reduction in the number of errors when comparedto the number of errors in the ESL data.7 DiscussionAs shown in Section 6, training a classifier ondata that contain errors produces better results whencompared to the TrainClean classifier trained onclean native data.
The key results for all languagegroups are summarized in Table 7.
It should benoted that the TrainClean system also makes useof the article chosen by the author through a confi-dence threshold8; it prefers to keep the article chosenby the user.
The difference is that the TrainCleansystem does not consider the author?s article in train-ing.
The results of training with error generationare better, which shows that training on automati-cally corrupted data indeed helps.
While the per-formance is different by language group, there is anobservable reduction in error rate for each languagegroup when TWE systems are used compared toTrainClean approach.
The reduction in error rate8The decision threshold is found empirically on a subset ofthe ESL data set aside for development.160achieved by the best performing TWE system whencompared to the error rate of the TrainClean sys-tem is 10.06% for Chinese, 4.89% for Czech and10.77% for Russian, as shown in Table 7.
We alsonote that the best performing TWE systems for Chi-nese and Russian speakers are those that rely on thedistribution of articles (Chinese) and the distributionof errors (Russian), but for Czech it is the GeneralTWE system that performs the best, maybe becausewe had less data for Czech speakers, so their statis-tics are less reliable.There are several additional observations to bemade.
First, training paradigms that use error gen-eration methods work better than the training ap-proach of using clean data.
Every system of typeTWE outperforms the TrainClean system, as ev-idenced by Tables 4, 5, and 6.
Second, the propor-tion of errors in the training data should be similarto the error rate in the test data.
The proportion oferrors in training is shown in Tables 4, 5 and 6 in col-umn 2.
Furthermore, TWE systems ArtDistr andErrorDistr that use specific knowledge about arti-cle and error distributions, respectively, work betterfor Russian and Chinese groups than the Generalmethod that adds errors to the data uniformly at ran-dom.
Since ArtDistr and ErrorDistr depend onthe statistics of learner mistakes, the success of thesystems that use these methods for error generationdepends on the accuracy of these statistics, and weonly have between 100 and 250 errors for each lan-guage group.
It would be interesting to see whetherbetter results can be achieved with these methods ifmore annotated data are available.
Finally, for thesame reason, there is no significant difference in theperformance of methods ArtDistrBeforeAnnotand ArtDistrAfterAnnot: With small sizes of an-notated data there is no difference in article distribu-tions before and after annotation.8 Conclusion and Future WorkWe have shown that error correction trainingparadigms that introduce artificial errors are supe-rior to training classifiers on clean data.
We pro-posed several methods of error generation that ac-count for error frequency information and error dis-tribution statistics from non-native text and demon-strated that the methods that work best are those thatSource Accuracy Errorlanguage Train TWE reductionCleanChinese 91.85% 92.67% 10.06%Czech 91.82% 92.22% 4.89%Russian 90.62% 91.63% 10.77%Table 7: Improvement due to training with errors.
Foreach source language, the last column of the table showsthe reduction in error rate achieved by the best perform-ing TWE system when compared to the error rate of theTrainClean system.
The error rate for each system iscomputed by subtracting the accuracy achieved by thesystem, as shown in columns 2 and 3.result in a training corpus that statistically resemblesthe non-native text.
Adding information about arti-cle distribution in non-native data and statistics onspecific error types is even more helpful.We have also argued that the baselines used ear-lier in the relevant literature ?
all based on the major-ity of the most commonly used class ?
suit selectiontasks, but are inappropriate for error correction.
In-stead, the error rate in the data should be taken intoaccount when determining the baseline.The focus of the present study was on trainingparadigms.
While it is quite possible that the articlecorrection system presented here can be improved?
we would like to explore improving the systemby using a more sophisticated feature set ?
we be-lieve that the performance gap due to the error driventraining paradigms shown here will remain.
The rea-son is that even with better features, some of the fea-tures that hold in the native data will not be active inin the ESL writing.Finally, while this study focused on the problemof correcting article mistakes, we plan to apply theproposed training paradigms to similar text correc-tion problems.AcknowledgmentsWe thank Nick Rizzolo for helpful discussions onLBJ.
We also thank Peter Chew and the anonymousreviewers for their insightful comments.
This re-search is partly supported by a grant from the U.S.Department of Education.161ReferencesC.
Brockett, W. B. Dolan, and M. Gamon.
2006.
Cor-recting ESL errors using phrasal SMT techniques.
InProceedings of the 21st COLING and the 44th ACL,Sydney.A.
Carlson, C. Cumby, J. Rosen, and D. Roth.
TheSNoW learning architecture.
Technical report.A.
J. Carlson and J. Rosen and D. Roth.
2001.
ScalingUp Context Sensitive Text Correction.
IAAI, 45?50.R.
De Felice and S. Pulman.
2008.
A Classifier-BasedApproach to Preposition and Determiner Error Correc-tion in L2 English.
In Proceedings of COLING-08.J.
Foster and ?.
Andersen.
2009.
GenERRate: Gener-ating Errors for Use in Grammatical Error Detection.In Proceedings of the NAACL Workshop on InnovativeUse of NLP for Building Educational Applications.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277-296.M.
Gamon, J. Gao, C. Brockett, A. Klementiev, W.Dolan, D. Belenko and L. Vanderwende.
2008.
UsingContextual Speller Techniques and Language Model-ing for ESL Error Correction.
Proceedings of IJCNLP.A.
R. Golding and D. Roth.
1996.
Applying Winnowto Context-Sensitive Spelling Correction.
ICML, 182?190.A.
R. Golding and D. Roth.
1999.
A Winnow based ap-proach to Context-Sensitive Spelling Correction.
Ma-chine Learning, 34(1-3):107?130.S.
Granger, E. Dagneaux and F. Meunier 2002.
Interna-tional Corpus of Learner English.S.
Gui and H. Yang.
2003.
Zhongguo Xuexizhe YingyuYuliaohu.
(Chinese Learner English Corpus).
Shang-hai Waiyu Jiaoyu Chubanshe.
(In Chinese).N.
Han, M. Chodorow and C. Leacock.
2006.
De-tecting Errors in English Article Usage by Non-nativeSpeakers.
Journal of Natural Language Engineering,12(2):115?129.E.
Izumi, K. Uchimoto, T. Saiga and H. Isahara.
2003.Automatic Error Detection in the Japanese LeanersEnglish Spoken Data.
ACL.E.
Izumi, K. Uchimoto and H. Isahara.
2004.
TheNICT JLE Corpus: Exploiting the Language Learner?sSpeech Database for Research and Education.
Inter-national Journal of the Computer, the Internet andManagement, 12(2):119?125.K.
Knight and I. Chander.
1994.
Automatic Posteditingof Documents.
In Proceedings of the American Asso-ciation of Artificial Intelligence, pp 779?784.J.
Lee and S. Seneff.
2008.
An analysis of grammaticalerrors in non-native speech in English.
In Proceedingsof the 2008 Spoken Language Technology Workshop,Goa.G.
Minnen, F. Bond and A. Copestake 2000.
Memory-Based Learning for Article Generation.
In Proceed-ings of the Fourth Conference on Computational Nat-ural Language Learning and of the Second LearningLanguage in Logic Workshop, pp 43?48.V.
Punyakanok, D. Roth, and W. Yih.
The importance ofsyntactic parsing and inference in semantic role label-ing.
Computational Linguistics, 34(2).N.
Rizzolo and D. Roth 2007.
Modeling DiscriminativeGlobal Inference.
In Proceedings of the First Interna-tional Conference on Semantic Computing (ICSC), pp597?604.A.
Rozovskaya and D. Roth 2010.
Annotating ESL Er-rors: Challenges and Rewards.
In Proceedings of theNAACL Workshop on Innovative Use of NLP for Build-ing Educational Applications.J.
Sjo?bergh and O. Knutsson.
2005.
Faking errors toavoid making errors.
In Proceedings of RANLP 2005,Borovets.J.
Tetreault and M. Chodorow.
2008.
Native Judgmentsof Non-Native Usage: Experiments in Preposition Er-ror Detection.
COLING Workshop on Human Judg-ments in Computational Linguistics, Manchester, UK.J.
Turner and E. Charniak.
2007.
Language Modelingfor Determiner Selection.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Companion Volume, Short Papers, pp 177?180.J.
Wagner, J.
Foster, and J. van Genabith.
2009.
Judg-ing grammaticality: Experiments in sentence classifi-cation.
CALICO Journal.
Special Issue on the 2008Automatic Analysis of Learner Language CALICOWorkshop.Y.
Xing, J. Gao, and W. Dolan.
2009.
A web-based En-glish proofing system for ESL users.
In Proceedingsof IJCNLP.162
