NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 58?64,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsComparing human versus automatic feature extraction for fine-grainedelementary readability assessmentYi Ma, Ritu Singh, Eric Fosler-LussierDept.
of Computer Science & EngineeringThe Ohio State UniversityColumbus, OH 43210, USAmay,singhri,fosler@cse.ohio-state.eduRobert LofthusXerox CorporationRochester, NY 14604, USARobert.Lofthus@xerox.comAbstractEarly primary children?s literature poses someinteresting challenges for automated readabil-ity assessment: for example, teachers oftenuse fine-grained reading leveling systems fordetermining appropriate books for children toread (many current systems approach read-ability assessment at a coarser whole gradelevel).
In previous work (Ma et al, 2012),we suggested that the fine-grained assess-ment task can be approached using a rankingmethodology, and incorporating features thatcorrespond to the visual layout of the pageimproves performance.
However, the previ-ous methodology for using ?found?
text (e.g.,scanning in a book from the library) requireshuman annotation of the text regions and cor-rection of the OCR text.
In this work, we askwhether the annotation process can be auto-mated, and also experiment with richer syntac-tic features found in the literature that can beautomatically derived from either the human-corrected or raw OCR text.
We find that auto-mated visual and text feature extraction workreasonably well and can allow for scaling tolarger datasets, but that in our particular exper-iments the use of syntactic features adds littleto the performance of the system, contrary toprevious findings.1 IntroductionKnowing the reading level of a children?s bookis an important task in the educational setting.Teachers want to have leveling for books in theschool library; parents are trying to select appro-priate books for their children; writers need guid-ance while writing for different literacy needs (e.g.text simplification)?reading level assessment is re-quired in a variety of contexts.
The history of as-sessing readability using simple arithmetic metricsdates back to the 1920s when Thorndike (1921) hasmeasured difficulty of texts by tabulating words ac-cording to the frequency of their use in general lit-erature.
Most of the traditional readability formulaswere also based on countable features of text, suchas syllable counts (Flesch, 1948).More advanced machine learning techniques suchas classification and regression have been appliedto the task of reading level prediction (Collins-Thompson and Callan, 2004; Schwarm and Osten-dorf, 2005; Petersen and Ostendorf, 2009; Feng etal., 2010); such works are described in further de-tail in the next Section 2.
In recent work (Ma et al,2012), we approached the problem of fine-grainedleveling of books, demonstrating that a ranking ap-proach to predicting reading level outperforms bothclassification and regression approaches in that do-main.
A further finding was that visually-orientedfeatures that consider the visual layout of the page(e.g.
number of text lines per annotated text region,text region area compared to the whole page areaand font size etc.)
play an important role in predict-ing the reading levels of children?s books in whichpictures and textual layout dominate the book con-tent over text.However, the data preparation process in our pre-vious study involves human intervention?we askhuman annotators to draw rectangle markups aroundtext region over pages.
Moreover, we only use avery shallow surface level text-based feature set to58compare with the visually-oriented features.
Hencein this paper, we assess the effect of using com-pletely automated annotation processing within thesame framework.
We are interested in exploringhow much performance will change by completelyeliminating manual intervention.
At the same time,we have also extended our previous feature set by in-troducing a richer set of automatically derived text-based features, proposed by Feng et al (2010),which capture deeper syntactic complexities of thetext.
Unlike our previous work, the major goal ofthis paper is not trying to compare different machinelearning techniques used in readability assessmenttask, but rather to compare the performance differ-ences between with and without human labor in-volved within our previous proposed system frame-work.We begin the paper with the description of re-lated work in Section 2, followed by detailed ex-planation regarding data preparation and automaticannotations in Section 3.
The extended features willbe covered in Section 4, followed by experimentalanalysis in Section 5, in which we will compare theresults between human annotations and automaticannotations.
We will also report the system per-formance after incorporating the rich text features(structural features).
Conclusions follow in Section6.2 Related WorkSince 1920, approximately 200 readability formulashave been reported in the literature (DuBay, 2004);statistical language processing techniques have re-cently entered into the fray for readability assess-ment.
Si and Callan (2001) and Collins-Thompsonand Callan (2004) have demonstrated the use of lan-guage models is more robust for web documentsand passages.
Heilman et al (2007) studied theimpact of grammar-based features combined withlanguage modeling approach for readability assess-ment of first and second language texts.
They ar-gued that grammar-based features are more perti-nent for second language learners than for the firstlanguage readers.
Schwarm and Ostendorf (2005)and Petersen and Ostendorf (2009) both used a sup-port vector machine to classify texts based on thereading level.
They combined traditional methodsof readability assessment and the features from lan-guage models and parsers.
Aluisio et al (2010)have developed a tool for text simplification for theauthoring process which addresses lexical and syn-tactic phenomena to make text readable but their as-sessment takes place at more coarse levels of liter-acy instead of finer-grained levels used for children?sbooks.A detailed analysis of various features for auto-matic readability assessment has been done by Fenget al (2010).
Most of the previous work has usedweb page documents, short passages or articles fromeducational newspapers as their datasets; typicallythe task is to assess reading level at a whole-gradelevel.
In contrast, early primary children?s literatureis typically leveled in a more fine-grained manner,and the research question we pursued in our previ-ous study was to investigate appropriate methods ofpredicting what we suspected was a non-linear read-ing level scale.Automating the process of readability assessmentis crucial for eventual widespread acceptance.
Pre-vious studies have looked at documents that werealready found in electronic form, such as web texts.While e-books are certainly on the rise (and wouldhelp automated processing) it is unlikely that paperbooks will be completely eliminated from the pri-mary school classroom soon.
Our previous study re-quired both manual scanning of the books and man-ual annotation of the books to extract the locationand content of text within the book ?
the necessityof which we evaluate in this study by examining theeffects of errors from the digitization process.3 Data Preparation and Book AnnotationOur previous study was based on a corpus of 36scanned children?s books; in this study we have ex-panded the set to 97 books which range from lev-els A to N in Fountas and Pinnell Benchmark As-sessment System 1 (Fountas and Pinnell, 2010); theFountas and Pinnell level serves as our gold stan-dard.
The distribution of number of books per read-ing level is shown in Table 1.
Levels A to N,in increasing difficulty, corresponds to the primarygrade books from roughly kindergarten throughthird grade.
The collection of children?s books cov-ers a large diversity of genres, series and publishers.59Reading # of Reading # ofLevel Books Level BooksA 6 H 7B 9 I 6C 5 J 11D 8 K 6E 11 L 3F 10 M 6G 7 N 2Table 1: Distribution of books over Fountas and Pinnellreading levelsOur agreement with the books?
publishers onlyallows access to physical copies of books ratherthan electronic versions; we scan each book intoa PDF version.
This situation would be similar tothat of a contemporary classroom teacher who is se-lecting books from the classroom or school libraryfor evaluating a child?s literacy progress.1 We thenuse Adobe Acrobat to run OCR (Optical CharacterRecognition) on the PDF books.
Following our pre-vious work, we first begin our process of annotat-ing each book using Adobe Acrobat before convert-ing them into corresponding XML files.
Featuresfor each book are extracted from their correspond-ing XMLs which contain all the text information andbook layout contents necessary to calculate the fea-tures.
Each book is manually scanned, and then an-notated in two different ways: we use human anno-tators (Section 3.1) and a completely automated pro-cess (Section 3.2).
The job of human annotators isprimarily to eliminate the errors made by OCR soft-ware, as well as correctly identifying text regions oneach page.
We encountered three types of typicalOCR errors for the children?s books in our set:1.
False alarms: some small illustration picturesegments (e.g.
flower patterns on a little girl?spajama or grass growing in bunches on theground) are recognized as text.2.
False negatives: this is more likely to occur fortext on irregular background such as white text1While it is clear that publishers will be moving toward elec-tronic books which would avoid the process of scanning (andlikely corresponding OCR problems), it is also clear that phys-ical books and documents will be present in the classroom foryears to come.OCR Correct Exampleoutput word1 I 1 ?
I!
I !
?
I[ f [or ?
forO 0 1OO ?
100nn rm wann ?
warmrn m horne ?
homeIT!
m aIT!
?
am1n m tilne ?
timen1.
m n1.y ?
my1V W 1Ve ?
Wevv w vvhen ?
whenTable 2: Some common OCR errorson black background or text overlapped withillustrations.3.
OCR could misread the text.
These are mostcommon errors.
Some examples of this type oferror are shown in Table 2.The two different annotation processes are explainedin the following Subsections 3.1 and 3.2.3.1 Human AnnotationAnnotators manually draw a rectangular box overthe text region on each page using Adobe Acrobatmarkup drawing tools.
The annotators also correctthe type 2 and 3 of OCR errors which are mentionedabove.
In human annotation process, the false alarm(type 1) errors are implicitly prevented since the an-notators will only annotate the regions where texttruly exists on the page (no matter whether the OCRrecognized or not).3.2 Automatic AnnotationFor automatic annotation, we make use of JavaScriptAPI provided by Adobe Acrobat.
The automatic an-notation tool is implemented as a JavaScript pluginmenu item within Adobe Acrobat.
The JavaScriptAPI can return the position of every single recog-nized word on the page.
Based on the position cuesof each word, we design a simple algorithm to auto-matically cluster the words into separate groups ac-cording to certain spatial distance thresholds.2 In-2A distance threshold of 22 pixels was used in practice.60tuitively, one could imagine the words as smallfloating soap bubbles on the page?where smallerbubbles (individual words) which are close enoughwill merge together to form bigger bubbles (text re-gions) automatically.
For each detected text region,a bounding rectangle box annotation is drawn onthe page automatically.
Beyond this point, the restof the data preparation process is identical to hu-man annotation, in which the corresponding XMLswill be generated from the annotated versions ofthe PDF books.
However, unlike human annota-tion, automating the annotation process can intro-duce noise into the data due to uncorrected OCR er-rors.
In correspondence to the three types of OCRerrors, automatic annotation could also draw extrabounding rectangle boxes on non-text region (whereOCR thinks there is text there but there is not), failsto draw bounding rectangle boxes on text region(where OCR should have recognized text there butit does not) and accepts many mis-recognized non-word symbols as text content (where OCR misreadswords).3.3 Generating XMLs From Annotated PDFBooksThis process is also implemented as anotherJavaScript plugin menu item within Adobe Acrobat.The plugin is run on the annotated PDFs and is de-signed to be agnostic to the annotation types?it willwork on both human-annotated and auto-annotatedversions of PDFs.
Once the XMLs for each chil-dren?s book are generated, we could proceed to thefeature extraction step.
The set of features we use inthe experiments are described in the following Sec-tion 4.4 FeaturesFor surface-level features and visual features, weutilize similar features proposed in our previousstudy.3 For completeness?
sake, we list these twosets of features as follows in Section 4.1:3We discard two visual features in both the human and au-tomatic annotation that require the annotation of the locationof images on the page, as these were features that the AdobeAcrobat JavaScript API could not directly access.4.1 Surface-level Features andVisually-oriented Features?
Surface-level Features1.
Number of words2.
Number of letters per word3.
Number of sentences4.
Average sentence length5.
Type-token ratio of the text content.?
Visually-oriented Features1.
Page count2.
Number of words per page3.
Number of sentences per page4.
Number of text lines per page5.
Number of words per text line6.
Number of words per annotated text rect-angle7.
Number of text lines per annotated textrectangle8.
Average ratio of annotated text rectanglearea to page area9.
Average font size4.2 Structural FeaturesSince our previous work only uses surface level oftext features, we are interested in investigating thecontribution of high-level structural features to thecurrent system.
Feng et al (2010) found severalparsing-based features and part-of-speech based fea-tures to be useful.
We utilize the Stanford Parser(Klein and Manning, 2003) to extract the followingfeatures from the XML files based on those used in(Feng et al, 2010):?
Parsed Syntactic Features for NPs and VPs1.
Number of the NPs/VPs2.
Number of NPs/VPs per sentence3.
Average NP/VP length measured by num-ber of words4.
Number of non-terminal nodes per parsetree5.
Number of non-terminal ancestors perword in NPs/VPs?
POS-based Features611.
Fraction of tokens labeled asnoun/preposition2.
Fraction of types labeled asnoun/preposition3.
Number of noun/preposition tokens persentence4.
Number of noun/preposition types persentence5 ExperimentsIn the experiments, we look at how much the perfor-mance dropped by switching to zero human inputs.We also investigate the impact of using a richer setof text-based features.
We apply the ranking-basedbook leveling algorithm proposed by our previousstudy (Ma et al, 2012) and use the SVMrank ranker(Joachims, 2006) for our experiments.
In this sys-tem, the ranker learns to sort the training books intoleveled order.
The unknown test book is insertedinto the ordering of the training books by the trainedranking model, and the predicted reading level iscalculated by averaging over the levels of the knownbooks above and below the test book.
Following theprevious study, each book is uniformly partitionedinto 4 parts, treating each sub-book as an individ-ual entity.
A leave-n-out procedure is utilized forevaluation: during each iteration of the training, thesystem leaves out all n partitions (sub-books) cor-responding to one book.
In the testing phase, thetrained ranking model tests on all partitions corre-sponding to the held-out book.
We obtain a singlepredicted reading level for the held-out book by av-eraging the results for all its partitions; averagingproduces a more robust result.
Two separate experi-ments are carried out on human-annotated and auto-annotated PDF books respectively.We use two metrics to determine quality: first, theaccuracy of the system is computed by claiming itis correct if the predicted book level is within ?1 ofthe true reading level.4 The second scoring metric isthe absolute error of number of levels away from thekey reading level, averaged over all of the books.4We follow our previous study to use ?1 accuracy evalu-ation metric in order to generate consistent results and alloweasy comparison.
Another thing to notice is that this is stillrather fine-grained since multiple reading levels correspond toone single grade level.We report the experiment results on differentcombinations of feature sets: surface level featuresplus visually-oriented features, surface level featuresonly, visually-oriented features only, structural fea-tures only and finally combining all the features to-gether.5.1 Human Annotation vs. AutomaticAnnotationAs we can observe from Table 3,5 overall the humanannotation gives higher accuracy than automatic an-notation across different feature sets.
The perfor-mance difference between human annotation and au-tomatic annotation could be attributed to the OCRerrors (described in Section 3.2) which are intro-duced in the automatic annotation process.
How-ever, to our surprise, the best performance of humanannotation is not significantly better than automaticannotation even at p < 0.1 level (figures in bold).6Only for the experiment using all features does hu-man annotation outperform the automatic annota-tion at p < 0.1 level (still not significantly betterat p < 0.05 level, figures with asterisks).
There-fore, we believe that the extra labor involved in theannotation step could be replaced by the automaticprocess without leading to a significant performancedrop.
While the process does still require manualscanning of each book (which can be time consum-ing depending on the kind of scanner), the automaticprocessing can reduce the labor per book from ap-proximately twenty minutes per book to just a fewseconds.5.2 Incorporating Structural FeaturesOur previous study demonstrated that combin-ing surface features with visual features producespromising results.
As mentioned above, the sec-ond aim of this study is to see how much benefitwe can get from incorporating high-level structuralfeatures, such as those used in (Feng et al, 2010)(described in Section 4.2), with the features in ourprevious study.Table 3 shows that for both human and automatic5In three of the books, the OCR completely failed; thus only94 books are available for evaluation of the automatic annota-tion.6One-tailed Z-test was used with each book taken as an in-dependent sample.62Annotation type Human Automatic?1 Accuracy %Surface+Visual features 76.3 70.2Surface level features 69.1 64.9Visual features 63.9 58.5Structural features 63.9 58.5All features 76.3?
66.0?Average leveling error ?
standard deviationSurface+Visual features 0.99 ?
0.87 1.16 ?
0.83Surface level features 1.24 ?
1.05 1.16 ?
0.97Visual features 1.24 ?
1.00 1.37 ?
0.89Structural features 1.30 ?
0.89 1.33 ?
0.91All features 1.05 ?
0.78 1.15 ?
0.90Table 3: Results on 97 books using human annotations vs. automatic annotations, reporting accuracy within one leveland average error for 4 partitions per book.annotation under the ?1 accuracy metric, the vi-sual features and the structural features have thesame performance, whose accuracy are both slightlylower than that of surface level features.
By combin-ing the surface level features with the visual features,the system obtains the best performance.
How-ever, by combining all three feature sets, the sys-tem performance does not change for human annota-tion whereas it hurts the performance for automaticannotation?it is likely that the OCR errors existingin the automatic annotations give rise to erroneousstructural features (e.g.
the parser would produceless robust parses for sentences which have out ofvocabulary words).
Overall, we did not observe bet-ter performance by incorporating structural features.Using structural features on their own also did notproduce noteworthy results.
Although among thethree kinds of features (surface, visual and struc-tural), structural features have the highest computa-tional cost, it exhibits no significant improvement tosystem results.
In the average leveling error metric,the best performance is again obtained at the com-bination of surface level features and visual featuresfor human annotation, whereas the performance re-mains almost the same after incorporating structuralfeatures for automatic annotation.6 ConclusionIn this paper, we explore the possibility of reducinghuman involvement in the specific task of predictingreading levels of scanned children?s books by elimi-nating the need for human annotation.
Clearly thereis a trade off between the amount of human laborinvolved and the accuracy of the reading level pre-dicted.
Based on the experimental results, we didnot observe significant performance drop by switch-ing from human annotation to automatic annotationin the task of predicting reading levels for scannedchildren?s books.We also study the effect of incorporating struc-tural features into the proposed ranking system.
Theexperimental results showed that structural featuresexhibit no significant effect to the system perfor-mance.
We conclude for the simply structured, shorttext that appears in most children?s books, a deeplevel analysis of the text properties may be overkillfor the task and produced unsatisfactory results at ahigh computational cost for our task.In the future, we are interested in investigating theimportance of each individual feature as well as ap-plying various feature selection methods to furtherimprove the overall performance of the system?inthe hope that making the ranking system more ro-bust to OCR errors introduced by automatic annota-tion processing.
Another interesting open questionis that how many scanned book pages are needed tomake a good prediction.7 Such analysis would bevery helpful for practical purposes, since a teacher7We thank an anonymous reviewer of the paper for this sug-gestion.63could just scan few sample pages instead of a fullbook for a reliable prediction.ReferencesS.
Aluisio, L. Specia, C. Gasperin, and C. Scarton.
2010.Readability assessment for text simplification.
In Pro-ceedings of the NAACL HLT 2010 Fifth Workshop onInnovative Use of NLP for Building Educational Ap-plications, pages 1?9.
Association for ComputationalLinguistics.K.
Collins-Thompson and J. Callan.
2004.
A languagemodeling approach to predicting reading difficulty.
InProceedings of HLT / NAACL 2004, volume 4, pages193?200, Boston, USA.W.H.
DuBay.
2004.
The principles of readability.
Im-pact Information, pages 1?76.L.
Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.2010.
A comparison of features for automatic read-ability assessment.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(COLING 2010), pages 276?284, Beijing, China.
As-sociation for Computational Linguistics.R.
Flesch.
1948.
A new readability yardstick.
Journal ofapplied psychology, 32(3):221?233.I.
Fountas and G. Pinnell.
2010.
Fountasand pinnell benchmark assessment system 1.http://www.heinemann.com/products/E02776.aspx.M.
Heilman, K. Collins-Thompson, J. Callan, and M. Es-kenazi.
2007.
Combining lexical and grammaticalfeatures to improve readability measures for first andsecond language texts.
In Proceedings of NAACLHLT, pages 460?467.T.
Joachims.
2006.
Training linear SVMs in linear time.In Proceedings of the 12th ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, pages 217?226.
ACM.D.
Klein and C. Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Meeting ofthe Association for Computational Linguistics, pages423?430.Y.
Ma, E. Fosler-Lussier, and R. Lofthus.
2012.Ranking-based readability assessment for early pri-mary children?s literature.
In Proceedings of NAACLHLT.S.
Petersen and M. Ostendorf.
2009.
A machine learn-ing approach to reading level assessment.
ComputerSpeech & Language, 23(1):89?106.S.
Schwarm and M. Ostendorf.
2005.
Reading level as-sessment using support vector machines and statisticallanguage models.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 523?530.
Association for ComputationalLinguistics.L.
Si and J. Callan.
2001.
A statistical model for scien-tific readability.
In Proceedings of the tenth interna-tional conference on Information and knowledge man-agement, pages 574?576.
ACM.E.L.
Thorndike.
1921.
The teacher?s word book, volume134.
Teachers College, Columbia University NewYork.64
