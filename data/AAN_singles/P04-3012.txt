Corpus representativeness for syntactic information acquisitionN?ria BELIULA, Universitat Pompeu FabraLa Rambla 30-3208002 BarcelonaSpainnuria.bel@upf.eduAbstractThis paper refers to part of our research in thearea of automatic acquisition of computationallexicon information from corpus.
The presentpaper reports the ongoing research on corpusrepresentativeness.
For the task of inducinginformation out of text, we wanted to fix acertain degree of confidence on the size andcomposition of the collection of documents tobe observed.
The results show that it ispossible to work with a relatively small corpusof texts if it is tuned to a particular domain.Even more, it seems that a small tuned corpuswill be more informative for real parsing thana general corpus.1 IntroductionThe coverage of the computational lexicon usedin deep Natural Language Processing (NLP) iscrucial for parsing success.
But rather frequently,the absence of particular entries or the fact that theinformation encoded for these does not cover veryspecific syntactic contexts --as those found intechnical texts?
make high informative grammarsnot suitable for real applications.
Moreover, thisposes a real problem when porting a particularapplication from domain to domain, as the lexiconhas to be re-encoded in the light of the newdomain.
In fact, in order to minimize ambiguitiesand possible over-generation, application basedlexicons tend to be tuned for every specific domainaddressed by a particular application.
Tuning oflexicons to different domains is really a delayingfactor in the deployment of NLP applications, as itraises its costs, not only in terms of money, butalso, and crucially, in terms of time.A desirable solution would be a ?plug and play?system that, given a collection of documentssupplied by the customer, could induce a tunedlexicon.
By ?tuned?
we mean full coverage both interms of: 1) entries: detecting new items andassigning them a syntactic behavior pattern; and 2)syntactic behavior pattern: adapting the encodingof entries to the observations of the corpus, so as toassign a class that accounts for the occurrences ofthis particular word in that particular corpus.
Thequestion we have addressed here is to define thesize and composition of the corpus we would needin order to get necessary and sufficient informationfor Machine Learning techniques to induce thattype of information.Representativeness of a corpus is a topic largelydealt with, especially in corpus linguistics.
One ofthe standard references is Biber (1993) where theauthor offers guidelines for corpus design tocharacterize a language.
The size and compositionof the corpus to be observed has also been studiedby general statistical NLP (Lauer 1995), and inrelation with automatic acquisition methods(Zernick, 1991, Yang & Song 1999).
But most ofthese studies focused in having a corpus thatactually models the whole language.
However, wewill see in section 3 that for inducing informationfor parsing we might want to model just aparticular subset of a language, the one thatcorresponds to the texts that a particularapplication is going to parse.
Thus, the research wereport about here refers to aspects related to thequantity and optimal composition of a corpus thatwill be used for inducing syntactic information.In what follows, we first will briefly describethe observation corpus.
In section 3, we introducethe phenomena observed and the way we got anobjective measure.
In Section 4, we report onexperiments done in order to check the validity ofthis measure in relation with word frequency.
Insection 5 we address the issue of corpus size andhow it affects this measure.2 Experimental corpus descriptionWe have used a corpus of technical specializedtexts, the CT.
The CT is made of subcorporabelonging to 5 different areas or domains:Medicine, Computing, Law, Economy,Environmental sciences and what is called aGeneral subcorpus made basically of news.
Thesize of the subcorpora range between 1 and 3million words per domain.
The CT corpus covers 3different languages although for the time being wehave only worked on Spanish.
For Spanish, thesize of the subcorpora is stated in Table 1.
All textshave been processed and are annotated withmorphosyntactic information.The CT corpus has been compiled as a test-bedfor studying linguistic differences between generallanguage and specialized texts.
Nevertheless, forour purposes, we only considered it as documentsthat represent the language used in particularknowledge domains.
In fact, we use them tosimulate the scenario where a user supplies acollection of documents with no specific samplingmethodology behind.3 Measuring syntactic behavior: the case ofadjectivesWe shall first motivate the statement thatparsing lexicons require tuning for a full coverageof  a particular domain.
We use the term ?fullcoverage?
to describe the ideal case where wewould have correct information for all the wordsused in the (unknown a priori) set of texts we wanta NLP application to handle.
Note that fullcoverage implies two aspects.
First, type coverage:all words that are used in a particular domain are inthe lexicon.
Second, that the information containedin the lexicon is the information needed by thegrammar to parse every word occurrence asintended.Full coverage is not guaranteed by working with?general language?
dictionaries.
Grammardevelopers know that the lexicon must be tuned tothe application?s domain, because general languagedictionaries either contain too much information,causing overgeneration, or do not cover everypossible syntactic context, some of them becausethey are specific of a particular domain.
The keypoint for us was to see whether texts belonging to adomain justify this practice.In order to obtain objective data about thedifferences among domains that motivate lexicontuning, we have carried out an experiment to studythe syntactic behavior (syntactic contexts) of a listof about 300 adjectives in technical texts of fourdifferent domains.
We have chosen adjectivesbecause their syntactic behavior is easy to becaptured by bigrams, as we will see below.Nevertheless, the same methodology could havebeen applied to other open categories.The first part of the experiment consisted ofcomputing different contexts for adjectivesoccurring in texts belonging to 4 different domains.We wanted to find out how significant coulddifferent uses be; that is, different syntacticcontexts for the same word depending on thedomain.
We took different parameters tocharacterize what we call ?syntactic behavior?.For adjectives, we defined 5 different parametersthat were considered to be directly related withsyntactic patterns.
These were the followingcontexts: 1) pre-nominal position, e.g.
?importantedecisi?n?
(important decision) 2) post-nominalposition, e.g.
?decisi?n importante?
3) ?ser?
copula1predicative position, e.g.
?la decisi?n esimportante?
(the decision is important) 4) ?estar?copula predicative position, e.g.
?la decisi?n est?interesante/*importante?
(the decision isinteresting/important) 5) modified by a quantityadverb, e.g.
?muy interesante?
(very interesting).Table 1 shows the data gathered for the adjective?paralelo?
(parallel) in the 4 different domainsubcorpora.
Note the differences in the position 3(?ser?
copula) when observed in texts oncomputing, versus the other domains.Corpora/n.of occurrences 1 2 3 4 5general (3.1 M words) 1 61 29 3 0computing (1.2 M words) 4 30 0 0 0medecine (3.7 M words) 3 67 22 1 0economy (1 M words) 0 28 6 0 0Table 1: Computing syntactic contexts asbehaviourThe observed occurrences (as in Table 1) wereused as parameters for building a vector for everylemma for each subcorpus.
We used cosinedistance2 (CD) to measure differences among theoccurrences in different subcorpora.
The closer to0, the more significantly different, the closer to 1,the more similar in their syntactic behavior in aparticular subcorpus with respect to the generalsubcorpus.
Thus, the CD values for the case of?paralelo?
seen in Table 1 are the following:Corpus Cosine Distancecomputing 0.7920economy 0.9782medecine 0.9791Table 2: CD for ?paralelo?
compared to thegeneral corpus1 Copulative sentences are made of 2 different basic copulative verbs ?ser?and ?estar?.
Most authors tend to express as ?lexical idyosincracy?
preferencesshown by particular adjectives as to go with one of them or even with bothalthough with different meaning.2 Cosine distance shows divergences that have to do with  large differences inquantity between parameters in the same position, whether small quantitiesspread along the different parameters does not compute significantly.
Cosinedistance was also considered to be interesting because it computes relativeweight of parameters within the vector.
Thus we are not obliged to take intoaccount relative frequency, which is actually different according to the differentdomains.What we were interested in was identifyingsignificant divergences, like, in this case, thecomplete absence of predicative use of theadjective ?paralelo?
in the computing corpus.
TheCD measure has been sensible to the fact that nopredicative use has been observed in texts oncomputing, the CD going down to 0.7.
Cosinedistance takes into account significant distancesamong the proportionality of the quantities in thedifferent features of the vector.
Hence we decidedto use CD to measure the divergence in syntacticbehavior of the observed adjectives.
Figure 1 plotsCD for the 4 subcorpora (Medicine, Computing,Economy) compared each one with the generalsubcorpus.
It corresponds to the observations forabout 300 adjectives, which were present in all thecorpora.
More than a half for each corpus is in factbelow the 0.9 of similarity.
Recall also that thismark holds for the different corpora, independentlyof the number of tokens (Economy is made of 1million words and Medicine of 3).-0,200,20,40,60,811,2125497397121145169193217241265289313The data of figure 1 would allow us to concludethat for lexicon tuning, the sample has to be rich indomain dependent texts.4 Frequency and CD measureFor being sure that CD was a good measure, wechecked to what extent what we called syntacticbehavior differences measured by a low CD couldbe due to a different number of occurrences in eachof the observed subcorpora.
It would have beenreasonable to think that when something is seenmore times, more different contexts can beobserved, while when something is seen only a fewtimes, variations are not that significant.-500050010001500200025000 0,2 0,4 0,6 0,8 1 1,2Figure 2: Difference in n. of observationsin 2 corpora and CDFigure 2 relates the obtained CD and thefrequency for every adjective.
For being able to doit, we took the difference of occurrences in twosubcorpora as the frequency measure, that is, thenumber resulting of subtracting the occurrences inthe computing subcorpus from the number ofoccurrences in the general subcorpus.
It clearlyshows that there is no regular relation betweendifferent number of occurrences in the two corporaand the observed divergence in syntactic behavior.Those elements that have a higher CD (0.9) rangeover all ranking positions: those that are 100 timesmore frequent in one than in other, etc.
Thus wecan conclude that CD do capture syntacticbehavior differences that are not motivated byfrequency related issues.5 Corpus size and syntactic behaviorWe also wanted to see the minimum corpus sizefor observing syntactic behavior differencesclearly.
The idea behind was to measure when CDgets stable, that is, independent of the number ofoccurrences observed.
This measure would help usin deciding the minimum corpus size we need tohave a reasonable representation for our inducedlexicon.
In fact our departure point was to checkwhether syntactic behavior could be comparedwith the figures related to number of types(lemmas) and number of tokens in a corpus.
Biber1993, S?nchez and Cantos, 1998, demonstrate thatthe number of new types does not increaseproportionally to the number of words once acertain quantity of texts has been observed.Figure 1: Cosine distance for the 4different subcorpusIn our experiment, we split the computingcorpus in 3 sets of 150K, 350K and 600K words inorder to compare the CD?s obtained.
In Figure 3, 1represents the whole computing corpus of 1,200Kfor the set of 300 adjectives we had worked withbefore.00,20,40,60,811,214181121161201241281105K351K603K3M GENAs shown in Figure 3, the results of thiscomparison were conclusive: for the computingcorpus, with half of the corpus, that is around600K, we already have a good representation ofthe whole corpus.
The CD being superior to 0.9 forall adjectives (mean is 0.97 and 0.009 of standarddeviation).
Surprisingly, the CD of the generalcorpus, the one that is made of 3 million words ofnews, is lower than the CD achieved for thesmallest computing subcorpus.
Table 3 shows themean and standard deviation for all de subcorpora(CC is Computing Corpus).Corpus size mean st. deviationCC 150K 0.81 0.04CC   360K 0.93 0.01CC 600K 0.97 0.009CC 1.2 M 1 0General 3M 0.75 0.03Table 3: Comparing corpus size and CDWhat Table 3 suggests is that according to CD,measured as shown here, the corpus to be used forinducing information about syntactic behavior doesnot need to be very large, but made of textsrepresentative of a particular domain.
It is part ofour future work to confirm that Machine LearningTechniques can really induce syntactic informationfrom such a corpus.ReferencesBiber, D. 1993.
Representativeness in corpusdesign.
Literary and Linguistic Computing 8:243-257.Lauer, M. 1995.
?How much is enough?
Datarequirements for Statistical NLP?.
In 2nd.Conference of the Pacific Association forComputational Linguistics.
Brisbane, Australia.S?nchez, A.
& Cantos P., 1997, ?Predictability ofWord Forms (Types) and Lemmas in LinguisticCorpora, A Case Study Based on the Analysis ofthe CUMBRE Corpus: An 8-Million-WordCorpus of Contemporary Spanish,?
InInternational Journal of Corpus Linguistics Vol.2, No.
2.Schone, P & D. Jurafsky.
2001.
Language-Independent induction of part of speech classlabels using only language universals.Proceedings IJCAI, 2001.Figure 3: CD of 300 adjs.
in differentsize subcorpora and general corpusYang, D-H and M. Song.
1999.
?The Estimate ofthe Corpus Size for Solving Data Sparseness?.Journal of KISS, 26(4): 568-583.Zernik, U. Lexical Acquisition.
1991.
ExploitingOn-Line Resources to Build a Lexicon.Lawrence Erlbaum Associates: 1-26.
