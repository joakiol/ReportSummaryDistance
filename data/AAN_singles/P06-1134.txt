Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1065?1072,Sydney, July 2006. c?2006 Association for Computational LinguisticsWord Sense and SubjectivityJanyce WiebeDepartment of Computer ScienceUniversity of Pittsburghwiebe@cs.pitt.eduRada MihalceaDepartment of Computer ScienceUniversity of North Texasrada@cs.unt.eduAbstractSubjectivity and meaning are both impor-tant properties of language.
This paper ex-plores their interaction, and brings empir-ical evidence in support of the hypothesesthat (1) subjectivity is a property that canbe associated with word senses, and (2)word sense disambiguation can directlybenefit from subjectivity annotations.1 IntroductionThere is growing interest in the automatic extrac-tion of opinions, emotions, and sentiments in text(subjectivity), to provide tools and support for var-ious NLP applications.
Similarly, there is continu-ous interest in the task of word sense disambigua-tion, with sense-annotated resources being devel-oped for many languages, and a growing num-ber of research groups participating in large-scaleevaluations such as SENSEVAL.Though both of these areas are concerned withthe semantics of a text, over time there has beenlittle interaction, if any, between them.
In this pa-per, we address this gap, and explore possible in-teractions between subjectivity and word sense.There are several benefits that would motivatesuch a joint exploration.
First, at the resourcelevel, the augmentation of lexical resources suchas WordNet (Miller, 1995) with subjectivity labelscould support better subjectivity analysis tools,and principled methods for refining word sensesand clustering similar meanings.
Second, at thetool level, an explicit link between subjectivity andword sense could help improve methods for each,by integrating features learned from one into theother in a pipeline approach, or through joint si-multaneous learning.In this paper we address two questions aboutword sense and subjectivity.
First, can subjectiv-ity labels be assigned to word senses?
To addressthis question, we perform two studies.
The first(Section 3) investigates agreement between anno-tators who manually assign the labels subjective,objective, or both to WordNet senses.
The secondstudy (Section 4) evaluates a method for automaticassignment of subjectivity labels to word senses.We devise an algorithm relying on distributionallysimilar words to calculate a subjectivity score, andshow how it can be used to automatically assessthe subjectivity of a word sense.Second, can automatic subjectivity analysis beused to improve word sense disambiguation?
Toaddress this question, the output of a subjectivitysentence classifier is input to a word-sense disam-biguation system, which is in turn evaluated on thenouns from the SENSEVAL-3 English lexical sam-ple task (Section 5).
The results of this experimentshow that a subjectivity feature can significantlyimprove the accuracy of a word sense disambigua-tion system for those words that have both subjec-tive and objective senses.A third obvious question is, can word sense dis-ambiguation help automatic subjectivity analysis?However, due to space limitations, we do not ad-dress this question here, but rather leave it for fu-ture work.2 BackgroundSubjective expressions are words and phrasesbeing used to express opinions, emotions, evalu-ations, speculations, etc.
(Wiebe et al, 2005).
Ageneral covering term for such states is privatestate, ?a state that is not open to objective obser-1065vation or verification?
(Quirk et al, 1985).1 Thereare three main types of subjective expressions:2(1) references to private states:His alarm grew.He absorbed the information quickly.He was boiling with anger.
(2) references to speech (or writing) events ex-pressing private states:UCC/Disciples leaders roundly con-demned the Iranian President?s verbalassault on Israel.The editors of the left-leaning paper at-tacked the new House Speaker.
(3) expressive subjective elements:He would be quite a catch.What?s the catch?That doctor is a quack.Work on automatic subjectivity analysis fallsinto three main areas.
The first is identifyingwords and phrases that are associated with sub-jectivity, for example, that think is associated withprivate states and that beautiful is associated withpositive sentiments (e.g., (Hatzivassiloglou andMcKeown, 1997; Wiebe, 2000; Kamps and Marx,2002; Turney, 2002; Esuli and Sebastiani, 2005)).Such judgments are made for words.
In contrast,our end task (in Section 4) is to assign subjectivitylabels to word senses.The second is subjectivity classification of sen-tences, clauses, phrases, or word instances in thecontext of a particular text or conversation, ei-ther subjective/objective classifications or posi-tive/negative sentiment classifications (e.g.,(Riloffand Wiebe, 2003; Yu and Hatzivassiloglou, 2003;Dave et al, 2003; Hu and Liu, 2004)).The third exploits automatic subjectivity anal-ysis in applications such as review classification(e.g., (Turney, 2002; Pang and Lee, 2004)), min-ing texts for product reviews (e.g., (Yi et al, 2003;Hu and Liu, 2004; Popescu and Etzioni, 2005)),summarization (e.g., (Kim and Hovy, 2004)), in-formation extraction (e.g., (Riloff et al, 2005)),1Note that sentiment, the focus of much recent work in thearea, is a type of subjectivity, specifically involving positiveor negative opinion, emotion, or evaluation.2These distinctions are not strictly needed for this paper,but may help the reader appreciate the examples given below.and question answering (e.g., (Yu and Hatzivas-siloglou, 2003; Stoyanov et al, 2005)).Most manual subjectivity annotation researchhas focused on annotating words, out of context(e.g., (Heise, 2001)), or sentences and phrases inthe context of a text or conversation (e.g., (Wiebeet al, 2005)).
The new annotations in this pa-per are instead targeting the annotation of wordsenses.3 Human Judgment of Word SenseSubjectivityTo explore our hypothesis that subjectivity maybe associated with word senses, we developed amanual annotation scheme for assigning subjec-tivity labels to WordNet senses,3 and performedan inter-annotator agreement study to assess itsreliability.
Senses are classified as S(ubjective),O(bjective), or B(oth).
Classifying a sense as Smeans that, when the sense is used in a text or con-versation, we expect it to express subjectivity; wealso expect the phrase or sentence containing it tobe subjective.We saw a number of subjective expressions inSection 2.
A subset is repeated here, along withrelevant WordNet senses.
In the display of eachsense, the first part shows the synset, gloss, andany examples.
The second part (marked with =>)shows the immediate hypernym.His alarm grew.alarm, dismay, consternation ?
(fear resulting from the aware-ness of danger)=> fear, fearfulness, fright ?
(an emotion experienced inanticipation of some specific pain or danger (usually ac-companied by a desire to flee or fight))He was boiling with anger.seethe, boil ?
(be in an agitated emotional state; ?The cus-tomer was seething with anger?
)=> be ?
(have the quality of being; (copula, used with anadjective or a predicate noun); ?John is rich?
; ?This is nota good answer?
)What?s the catch?catch ?
(a hidden drawback; ?it sounds good but what?s thecatch??
)=> drawback ?
(the quality of being a hindrance; ?hepointed out all the drawbacks to my plan?
)That doctor is a quack.quack ?
(an untrained person who pretends to be a physicianand who dispenses medical advice)=> doctor, doc, physician, MD, Dr., medicoBefore specifying what we mean by an objec-tive sense, we give examples.3All our examples and data used in the experiments arefrom WordNet 2.0.1066The alarm went off.alarm, warning device, alarm system ?
(a device that signalsthe occurrence of some undesirable event)=> device ?
(an instrumentality invented for a particu-lar purpose; ?the device is small enough to wear on yourwrist?
; ?a device intended to conserve water?
)The water boiled.boil ?
(come to the boiling point and change from a liquid tovapor; ?Water boils at 100 degrees Celsius?
)=> change state, turn ?
(undergo a transformation or achange of position or action; ?We turned from Socialismto Capitalism?
; ?The people turned against the Presidentwhen he stole the election?
)He sold his catch at the market.catch, haul ?
(the quantity that was caught; ?the catch wasonly 10 fish?
)=> indefinite quantity ?
(an estimated quantity)The duck?s quack was loud and brief.quack ?
(the harsh sound of a duck)=> sound ?
(the sudden occurrence of an audible event;?the sound awakened them?
)While we expect phrases or sentences contain-ing subjective senses to be subjective, we do notnecessarily expect phrases or sentences containingobjective senses to be objective.
Consider the fol-lowing examples:Will someone shut that damn alarm off?Can?t you even boil water?While these sentences contain objective sensesof alarm and boil, the sentences are subjectivenonetheless.
But they are not subjective due toalarm and boil, but rather to punctuation, sentenceforms, and other words in the sentence.
Thus, clas-sifying a sense as O means that, when the sense isused in a text or conversation, we do not expectit to express subjectivity and, if the phrase or sen-tence containing it is subjective, the subjectivity isdue to something else.Finally, classifying a sense as B means it coversboth subjective and objective usages, e.g.
:absorb, suck, imbibe, soak up, sop up, suck up, draw, take in,take up ?
(take in, also metaphorically; ?The sponge absorbswater well?
; ?She drew strength from the minister?s words?
)Manual subjectivity judgments were added toa total of 354 senses (64 words).
One annotator,Judge 1 (a co-author), tagged all of them.
A sec-ond annotator (Judge 2, who is not a co-author)tagged a subset for an agreement study, presentednext.3.1 Agreement StudyFor the agreement study, Judges 1 and 2 indepen-dently annotated 32 words (138 senses).
16 wordshave both S and O senses and 16 do not (accordingto Judge 1).
Among the 16 that do not have bothS and O senses, 8 have only S senses and 8 haveonly O senses.
All of the subsets are balanced be-tween nouns and verbs.
Table 1 shows the contin-gency table for the two annotators?
judgments onthis data.
In addition to S, O, and B, the annotationscheme also permits U(ncertain) tags.S O B U TotalS 39 O O 4 43O 3 73 2 4 82B 1 O 3 1 5U 3 2 O 3 8Total 46 75 5 12 138Table 1: Agreement on balanced set (Agreement:85.5%, ?
: 0.74)Overall agreement is 85.5%, with a Kappa (?
)value of 0.74.
For 12.3% of the senses, at leastone annotator?s tag is U.
If we consider these casesto be borderline and exclude them from the study,percent agreement increases to 95% and ?
rises to0.90.
Thus, annotator agreement is especially highwhen both are certain.Considering only the 16-word subset with bothS and O senses (according to Judge 1), ?
is .75,and for the 16-word subset for which Judge 1 gaveonly S or only O senses, ?
is .73.
Thus, the twosubsets are of comparable difficulty.The two annotators also independently anno-tated the 20 ambiguous nouns (117 senses) of theSENSEVAL-3 English lexical sample task used inSection 5.
For this tagging task, U tags were notallowed, to create a definitive gold standard for theexperiments.
Even so, the ?
value for them is 0.71,which is not substantially lower.
The distributionsof Judge 1?s tags for all 20 words can be found inTable 3 below.We conclude this section with examples ofdisagreements that illustrate sources of uncer-tainty.
First, uncertainty arises when subjec-tive senses are missing from the dictionary.The labels for the senses of noun assault are(O:O,O:O,O:O,O:UO).4 For verb assault there isa subjective sense:attack, round, assail, lash out, snipe, assault (attack in speechor writing) ?The editors of the left-leaning paper attacked thenew House Speaker?However, there is no corresponding sense for4I.e., the first three were labeled O by both annotators.
Forthe fourth sense, the second annotator was not sure but wasleaning toward O.1067noun assault.
A missing sense may lead an anno-tator to try to see subjectivity in an objective sense.Second, uncertainty can arise in weighing hy-pernym against sense.
It is fine for a synset toimply just S or O, while the hypernym impliesboth (the synset specializes the more general con-cept).
However, consider the following, whichwas tagged (O:UB).attack ?
(a sudden occurrence of an uncontrollable condition;?an attack of diarrhea?
)=> affliction ?
(a cause of great suffering and distress)While the sense is only about the condition, thehypernym highlights subjective reactions to thecondition.
One annotator judged only the sense(giving tag O), while the second considered thehypernym as well (giving tag UB).4 Automatic Assessment of Word SenseSubjectivityEncouraged by the results of the agreement study,we devised a method targeting the automatic an-notation of word senses for subjectivity.The main idea behind our method is that we canderive information about a word sense based on in-formation drawn from words that are distribution-ally similar to the given word sense.
This idea re-lates to the unsupervised word sense ranking algo-rithm described in (McCarthy et al, 2004).
Note,however, that (McCarthy et al, 2004) used the in-formation about distributionally similar words toapproximate corpus frequencies for word senses,whereas we target the estimation of a property ofa given word sense (the ?subjectivity?
).Starting with a given ambiguous word w, wefirst find the distributionally similar words usingthe method of (Lin, 1998) applied to the automat-ically parsed texts of the British National Corpus.Let DSW = dsw1, dsw2, ..., dswn be the list oftop-ranked distributionally similar words, sortedin decreasing order of their similarity.Next, for each sense wsi of the word w, we de-termine the similarity with each of the words in thelist DSW , using a WordNet-based measure of se-mantic similarity (wnss).
Although a large num-ber of such word-to-word similarity measures ex-ist, we chose to use the (Jiang and Conrath, 1997)measure, since it was found both to be efficientand to provide the best results in previous exper-iments involving word sense ranking (McCarthyet al, 2004)5.
For distributionally similar words5Note that unlike the above measure of distributional sim-Algorithm 1 Word Sense Subjectivity ScoreInput: Word sense wiInput: Distributionally similar words DSW = {dswj |j =1..n}Output: Subjectivity score subj(wi)1: subj(wi) = 02: totalsim = 03: for j = 1 to n do4: Instsj = all instances of dswj in the MPQA corpus5: for k in Instsj do6: if k is in a subj.
expr.
in MPQA corpus then7: subj(wi) += sim(wi,dswj)8: else if k is not in a subj.
expr.
in MPQA corpusthen9: subj(wi) -= sim(wi,dswj)10: end if11: totalsim += sim(wi,dswj)12: end for13: end for14: subj(wi) = subj(wi) / totalsimthat are themselves ambiguous, we use the sensethat maximizes the similarity score.
The similar-ity scores associated with each word dswj are nor-malized so that they add up to one across all possi-ble senses of w, which results in a score describedby the following formula:sim(wsi, dswj) = wnss(wsi,dswj)?i??senses(w)wnss(wsi?
,dswj)wherewnss(wsi, dswj) = maxk?senses(dswj)wnss(wsi, dswkj )A selection process can also be applied so thata distributionally similar word belongs only toone sense.
In this case, for a given sense wi weuse only those distributionally similar words withwhom wi has the highest similarity score across allthe senses of w. We refer to this case as similarity-selected, as opposed to similarity-all, which refersto the use of all distributionally similar words forall senses.Once we have a list of similar words associatedwith each sense wsi and the corresponding simi-larity scores sim(wsi, dswj), we use an annotatedcorpus to assign subjectivity scores to the senses.The corpus we use is the MPQA Opinion Corpus,which consists of over 10,000 sentences from theworld press annotated for subjective expressions(all three types of subjective expressions describedin Section 2).6ilarity which measures similarity between words, rather thanword senses, here we needed a similarity measure that alsotakes into account word senses as defined in a sense inven-tory such as WordNet.6The MPQA corpus is described in (Wiebe et al, 2005)and available at www.cs.pitt.edu/mpqa/databaserelease/.1068Algorithm 1 is our method for calculating sensesubjectivity scores.
The subjectivity score is avalue in the interval [-1,+1] with +1 correspond-ing to highly subjective and -1 corresponding tohighly objective.
It is a sum of sim scores, wheresim(wi,dswj) is added for each instance of dswjthat is in a subjective expression, and subtractedfor each instance that is not in a subjective expres-sion.Note that the annotations in the MPQA corpusare for subjective expressions in context.
Thus, thedata is somewhat noisy for our task, because, asdiscussed in Section 3, objective senses may ap-pear in subjective expressions.
Nonetheless, wehypothesized that subjective senses tend to appearmore often in subjective expressions than objec-tive senses do, and use the appearance of words insubjective expressions as evidence of sense sub-jectivity.
(Wiebe, 2000) also makes use of an annotatedcorpus, but in a different approach: given a wordw and a set of distributionally similar words DSW,that method assigns a subjectivity score to w equalto the conditional probability that any member ofDSW is in a subjective expression.
Moreover, theend task of that work was to annotate words, whileour end task is the more difficult problem of anno-tating word senses for subjectivity.4.1 EvaluationThe evaluation of the algorithm is performedagainst the gold standard of 64 words (354 wordsenses) using Judge 1?s annotations, as describedin Section 3.For each sense of each word in the set of 64ambiguous words, we use Algorithm 1 to deter-mine a subjectivity score.
A subjectivity label isthen assigned depending on the value of this scorewith respect to a pre-selected threshold.
While athreshold of 0 seems like a sensible choice, we per-form the evaluation for different thresholds rang-ing across the [-1,+1] interval, and correspond-ingly determine the precision of the algorithm atdifferent points of recall7.
Note that the wordsenses for which none of the distributionally sim-ilar words are found in the MPQA corpus are not7Specifically, in the list of word senses ranked by theirsubjectivity score, we assign a subjectivity label to the top Nword senses.
The precision is then determined as the numberof correct subjectivity label assignments out of all N assign-ments, while the recall is measured as the correct subjectivesenses out of all the subjective senses in the gold standarddata set.
By varying the value of N from 1 to the total num-ber of senses in the corpus, we can derive precision and recallcurves.included in this evaluation (excluding 82 senses),since in this case a subjectivity score cannot becalculated.
The evaluation is therefore performedon a total of 272 word senses.As a baseline, we use an ?informed?
random as-signment of subjectivity labels, which randomlyassigns S labels to word senses in the data set,such that the maximum number of S assignmentsequals the number of correct S labels in the goldstandard data set.
This baseline guarantees a max-imum recall of 1 (which under true random condi-tions might not be achievable).
Correspondingly,given the controlled distribution of S labels acrossthe data set in the baseline setting, the precisionis equal for all eleven recall points, and is deter-mined as the total number of correct subjective as-signments divided by the size of the data set8.Number Break-evenAlgorithm of DSW pointsimilarity-all 100 0.41similarity-selected 100 0.50similarity-all 160 0.43similarity-selected 160 0.50baseline - 0.27Table 2: Break-even point for different algorithmand parameter settingsThere are two aspects of the sense subjectivityscoring algorithm that can influence the label as-signment, and correspondingly their evaluation.First, as indicated above, after calculating thesemantic similarity of the distributionally similarwords with each sense, we can either use all thedistributionally similar words for the calculationof the subjectivity score of each sense (similarity-all), or we can use only those that lead to the high-est similarity (similarity-selected).
Interestingly,this aspect can drastically affect the algorithm ac-curacy.
The setting where a distributionally simi-lar word can belong only to one sense significantlyimproves the algorithm performance.
Figure 1plots the interpolated precision for eleven points ofrecall, for similarity-all, similarity-selected, andbaseline.
As shown in this figure, the precision-recall curves for our algorithm are clearly abovethe ?informed?
baseline, indicating the ability ofour algorithm to automatically identify subjectiveword senses.Second, the number of distributionally similarwords considered in the first stage of the algo-rithm can vary, and might therefore influence the8In other words, this fraction represents the probability ofmaking the correct subjective label assignment by chance.106900.10.20.30.40.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallPrecision recall curvesselectedallbaselineFigure 1: Precision and recall for automatic sub-jectivity annotations of word senses (DSW=160).output of the algorithm.
We experiment with twodifferent values, namely 100 and 160 top-rankeddistributionally similar words.
Table 2 shows thebreak-even points for the four different settingsthat were evaluated,9 with results that are almostdouble compared to the informed baseline.
Asit turns out, for weaker versions of the algorithm(i.e., similarity-all), the size of the set of distribu-tionally similar words can significantly impact theperformance of the algorithm.
However, for the al-ready improved similarity-selected algorithm ver-sion, this parameter does not seem to have influ-ence, as similar results are obtained regardless ofthe number of distributionally similar words.
Thisis in agreement with the finding of (McCarthy etal., 2004) that, in their word sense ranking method,a larger set of neighbors did not influence the al-gorithm accuracy.5 Automatic Subjectivity Annotations forWord Sense DisambiguationThe final question we address is concerned withthe potential impact of subjectivity on the qualityof a word sense classifier.
To answer this ques-tion, we augment an existing data-driven wordsense disambiguation system with a feature re-flecting the subjectivity of the examples where theambiguous word occurs, and evaluate the perfor-mance of the new subjectivity-aware classifier ascompared to the traditional context-based senseclassifier.We use a word sense disambiguation systemthat integrates both local and topical features.9The break-even point (Lewis, 1992) is a standard mea-sure used in conjunction with precision-recall evaluations.
Itrepresents the value where precision and recall become equal.Specifically, we use the current word and its part-of-speech, a local context of three words to theleft and right of the ambiguous word, the parts-of-speech of the surrounding words, and a global con-text implemented through sense-specific keywordsdetermined as a list of at most five words occurringat least three times in the contexts defining a cer-tain word sense.
This feature set is similar to theone used by (Ng and Lee, 1996), as well as by anumber of SENSEVAL systems.
The parametersfor sense-specific keyword selection were deter-mined through cross-fold validation on the train-ing set.
The features are integrated in a NaiveBayes classifier, which was selected mainly forits performance in previous work showing that itcan lead to a state-of-the-art disambiguation sys-tem given the features we consider (Lee and Ng,2002).The experiments are performed on the set ofambiguous nouns from the SENSEVAL-3 Englishlexical sample evaluation (Mihalcea et al, 2004).We use the rule-based subjective sentence classi-fier of (Riloff and Wiebe, 2003) to assign an S,O, or B label to all the training and test examplespertaining to these ambiguous words.
This sub-jectivity annotation tool targets sentences, ratherthan words or paragraphs, and therefore the tool isfed with sentences.
We also include a surroundingcontext of two additional sentences, because theclassifier considers some contextual information.Our hypothesis motivating the use of asentence-level subjectivity classifier is that in-stances of subjective senses are more likely to bein subjective sentences, and thus that sentence sub-jectivity is an informative feature for the disam-biguation of words having both subjective and ob-jective senses.For each ambiguous word, we perform two sep-arate runs: one using the basic disambiguationsystem described earlier, and another using thesubjectivity-aware system that includes the addi-tional subjectivity feature.
Table 3 shows the re-sults obtained for these 20 nouns, including wordsense disambiguation accuracy for the two differ-ent systems, the most frequent sense baseline, andthe subjectivity/objectivity split among the wordsenses (according to Judge 1).
The words in thetop half of the table are the ones that have both Sand O senses, and those in the bottom are the onesthat do not.
If we were to use Judge 2?s tags in-stead of Judge 1?s, only one word would change:source would move from the top to the bottom ofthe table.1070Sense Data ClassifierWord Senses subjectivity train test Baseline basic + subj.Words with subjective sensesargument 5 3-S 2-O 221 111 49.4% 51.4% 54.1%atmosphere 6 2-S 4-O 161 81 65.4% 65.4% 66.7%difference 5 2-S 3-O 226 114 40.4% 54.4% 57.0%difficulty 4 2-S 2-O 46 23 17.4% 47.8% 52.2%image 7 2-S 5-O 146 74 36.5% 41.2% 43.2%interest 7 1-S 5-O 1-B 185 93 41.9% 67.7% 68.8%judgment 7 5-S 2-O 62 32 28.1% 40.6% 43.8%plan 3 1-S 2-O 166 84 81.0% 81.0% 81.0%sort 4 1-S 2-O 1-B 190 96 65.6% 66.7% 67.7%source 9 1-S 8-O 64 32 40.6% 40.6% 40.6%Average 46.6% 55.6% 57.5%Words with no subjective sensesarm 6 6-O 266 133 82.0% 85.0% 84.2%audience 4 4-O 200 100 67.0% 74.0% 74.0%bank 10 10-O 262 132 62.6% 62.6% 62.6%degree 7 5-O 2-B 256 128 60.9% 71.1% 71.1%disc 4 4-O 200 100 38.0% 65.6% 66.4%organization 7 7-O 112 56 64.3% 64.3% 64.3%paper 7 7-O 232 117 25.6% 49.6% 48.0%party 5 5-O 230 116 62.1% 62.9% 62.9%performance 5 5-O 172 87 26.4% 34.5% 34.5%shelter 5 5-O 196 98 44.9% 65.3% 65.3%Average 53.3% 63.5% 63.3%Average for all words 50.0% 59.5% 60.4%Table 3: Word Sense Disambiguation with andwithout subjectivity information, for the set of am-biguous nouns in SENSEVAL-3For the words that have both S and O senses,the addition of the subjectivity feature alone canbring a significant error rate reduction of 4.3%(p < 0.05 paired t-test).
Interestingly, no improve-ments are observed for the words with no subjec-tive senses; on the contrary, the addition of thesubjectivity feature results in a small degradation.Overall for the entire set of ambiguous words, theerror reduction is measured at 2.2% (significant atp < 0.1 paired t-test).In almost all cases, the words with both S and Osenses show improvement, while the others showsmall degradation or no change.
This suggests thatif a subjectivity label is available for the words ina lexical resource (e.g.
using Algorithm 1 fromSection 4), such information can be used to decideon using a subjectivity-aware system, thereby im-proving disambiguation accuracy.One of the exceptions is disc, which had a smallbenefit, despite not having any subjective senses.As it happens, the first sense of disc is phonographrecord.phonograph record, phonograph recording, record, disk, disc,platter ?
(sound recording consisting of a disc with continu-ous grooves; formerly used to reproduce music by rotatingwhile a phonograph needle tracked in the grooves)The improvement can be explained by observ-ing that many of the training and test sentencescontaining this sense are labeled subjective by theclassifier, and indeed this sense frequently occursin subjective sentences such as ?This is anyway astunning disc.
?Another exception is the noun plan, which didnot benefit from the subjectivity feature, althoughit does have a subjective sense.
This can perhapsbe explained by the data set for this word, whichseems to be particularly difficult, as the basic clas-sifier itself could not improve over the most fre-quent sense baseline.The other word that did not benefit from thesubjectivity feature is the noun source, for whichits only subjective sense did not appear in thesense-annotated data, leading therefore to an ?ob-jective only?
set of examples.6 Conclusion and Future WorkThe questions posed in the introduction concern-ing the possible interaction between subjectivityand word sense found answers throughout the pa-per.
As it turns out, a correlation can indeed beestablished between these two semantic propertiesof language.Addressing the first question of whether subjec-tivity is a property that can be assigned to wordsenses, we showed that good agreement (?=0.74)can be achieved between human annotators la-beling the subjectivity of senses.
When uncer-tain cases are removed, the ?
value is even higher(0.90).
Moreover, the automatic subjectivity scor-ing mechanism that we devised was able to suc-cessfully assign subjectivity labels to senses, sig-nificantly outperforming an ?informed?
baselineassociated with the task.
While much work re-mains to be done, this first attempt has provedthe feasibility of correctly assigning subjectivitylabels to the fine-grained level of word senses.The second question was also positively an-swered: the quality of a word sense disambigua-tion system can be improved with the additionof subjectivity information.
Section 5 providedevidence that automatic subjectivity classificationmay improve word sense disambiguation perfor-mance, but mainly for words with both subjectiveand objective senses.
As we saw, performancemay even degrade for words that do not.
Tyingthe pieces of this paper together, once the sensesin a dictionary have been assigned subjectivity la-bels, a word sense disambiguation system couldconsult them to decide whether it should consideror ignore the subjectivity feature.There are several other ways our results couldimpact future work.
Subjectivity labels wouldbe a useful source of information when manuallyaugmenting the lexical knowledge in a dictionary,1071e.g., when choosing hypernyms for senses or de-ciding which senses to eliminate when defining acoarse-grained sense inventory (if there is a sub-jective sense, at least one should be retained).Adding subjectivity labels to WordNet couldalso support automatic subjectivity analysis.
First,the input corpus could be sense tagged and thesubjectivity labels of the assigned senses could beexploited by a subjectivity recognition tool.
Sec-ond, a number of methods for subjectivity or sen-timent analysis start with a set of seed words andthen search through WordNet to find other subjec-tive words (Kamps and Marx, 2002; Yu and Hatzi-vassiloglou, 2003; Hu and Liu, 2004; Kim andHovy, 2004; Esuli and Sebastiani, 2005).
How-ever, such searches may veer off course down ob-jective paths.
The subjectivity labels assigned tosenses could be consulted to keep the search trav-eling along subjective paths.Finally, there could be different strategiesfor exploiting subjectivity annotations and wordsense.
While the current setting considered apipeline approach, where the output of a subjec-tivity annotation system was fed to the input of amethod for semantic disambiguation, future workcould also consider the role of word senses as apossible way of improving subjectivity analysis,or simultaneous annotations of subjectivity andword meanings, as done in the past for other lan-guage processing problems.Acknowledgments We would like to thankTheresa Wilson for annotating senses, and theanonymous reviewers for their helpful comments.This work was partially supported by ARDAAQUAINT and by the NSF (award IIS-0208798).ReferencesK.
Dave, S. Lawrence, and D. Pennock.
2003.
Min-ing the peanut gallery: Opinion extraction and se-mantic classification of product reviews.
In Proc.WWW-2003, Budapest, Hungary.
Available athttp://www2003.org.A.
Esuli and F. Sebastiani.
2005.
Determining the se-mantic orientation of terms through gloss analysis.In Proc.
CIKM-2005.V.
Hatzivassiloglou and K. McKeown.
1997.
Predict-ing the semantic orientation of adjectives.
In Proc.ACL-97, pages 174?181.D.
Heise.
2001.
Project magellan: Collecting cross-cultural affective meanings via the internet.
Elec-tronic Journal of Sociology, 5(3).M.
Hu and B. Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of ACMSIGKDD.J.
Jiang and D. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical tax onomy.
InProceedings of the International Conference on Re-search in Computational Linguistics, Taiwan.J.
Kamps and M. Marx.
2002.
Words with attitude.
InProc.
1st International WordNet Conference.S.M.
Kim and E. Hovy.
2004.
Determining the senti-ment of opinions.
In Proc.
Coling 2004.Y.K.
Lee and H.T.
Ng.
2002.
An empirical evaluationof knowledge sources and learning algo rithms forword sense disambiguation.
In Proc.
EMNLP 2002.D.
Lewis.
1992.
An evaluation of phrasal and clus-tered representations on a text categorization task.In Proceedings of ACM SIGIR.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL,Montreal, Canada.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Finding predominant senses in untagged text.In Proc.
ACL 2004.R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.The Senseval-3 English lexical sample task.
In Proc.ACL/SIGLEX Senseval-3.G.
Miller.
1995.
Wordnet: A lexical database.
Com-munication of the ACM, 38(11):39?41.H.T.
Ng and H.B.
Lee.
1996.
Integrating multipleknowledge sources to disambiguate word se nse: Anexamplar-based approach.
In Proc.
ACL 1996.B.
Pang and L. Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-riza tion based on minimum cuts.
In Proc.
ACL2004.A.
Popescu and O. Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Proc.
ofHLT/EMNLP 2005.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.1985.
A Comprehensive Grammar of the EnglishLanguage.
Longman, New York.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proc.
EMNLP2003.E.
Riloff, J. Wiebe, and W. Phillips.
2005.
Exploitingsubjectivity classification to improve information extraction.
In Proc.
AAAI 2005.V.
Stoyanov, C. Cardie, and J. Wiebe.
2005.
Multi-perspective question answering using the opqa cor-pus.
In Proc.
HLT/EMNLP 2005.P.
Turney.
2002.
Thumbs up or thumbs down?
Seman-tic orientation applied to unsupervised classificationof reviews.
In Proc.
ACL 2002.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.Language Resources and Evaluation, 1(2).J.
Wiebe.
2000.
Learning subjective adjectives fromcorpora.
In Proc.
AAAI 2000.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment analyzer: Extracting sentiments about agiven topic using natu ral language processing tech-niques.
In Proc.
ICDM 2003.H.
Yu and V. Hatzivassiloglou.
2003.
Towards an-swering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proc.
EMNLP 2003.1072
