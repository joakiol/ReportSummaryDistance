Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763?771,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUnsupervised Neural Dependency Parsing?Yong Jiang, Wenjuan Han and Kewei Tu{jiangyong,hanwj,tukw}@shanghaitech.edu.cnSchool of Information Science and TechnologyShanghaiTech University, Shanghai, ChinaAbstractUnsupervised dependency parsing aims tolearn a dependency grammar from text anno-tated with only POS tags.
Various featuresand inductive biases are often used to incorpo-rate prior knowledge into learning.
One use-ful type of prior information is that there existcorrelations between the parameters of gram-mar rules involving different POS tags.
Pre-vious work employed manually designed fea-tures or special prior distributions to encodesuch information.
In this paper, we proposea novel approach to unsupervised dependen-cy parsing that uses a neural model to predictgrammar rule probabilities based on distribut-ed representation of POS tags.
The distributedrepresentation is automatically learned fromdata and captures the correlations betweenPOS tags.
Our experiments show that ourapproach outperforms previous approaches u-tilizing POS correlations and is competitivewith recent state-of-the-art approaches on ninedifferent languages.1 IntroductionUnsupervised structured prediction from data is animportant problem in natural language processing,with applications in grammar induction, POS tag in-duction, word alignment and so on.
Because thetraining data is unannotated in unsupervised struc-tured prediction, learning is very hard.
In this pa-per, we focus on unsupervised dependency parsing,which aims to identify the dependency trees of sen-tences in an unsupervised manner.
?This work was supported by the National Natural ScienceFoundation of China (61503248).Previous work on unsupervised dependency pars-ing is mainly based on the dependency model withvalence (DMV) (Klein and Manning, 2004) and it-s extension (Headden III et al, 2009; Gillenwateret al, 2010).
To effectively learn the DMV mod-el for better parsing accuracy, a variety of induc-tive biases and handcrafted features have been pro-posed to incorporate prior information into learning.One useful type of prior information is that thereexist correlations between the parameters of gram-mar rules involving different POS tags.
Cohen andSmith (2009; 2010) employed special prior distribu-tions to encourage learning of correlations betweenPOS tags.
Berg-Kirkpatrick et al (2010) encodedthe relations between POS tags using manually de-signed features.In this work, we propose a neural based ap-proach to unsupervised dependency parsing.
Weincorporate a neural model into the DMV modelto predict grammar rule probabilities based on dis-tributed representation of POS tags.
We learn theneural network parameters as well as the distribut-ed representations from data using the expectation-maximization algorithm.
The correlations betweenPOS tags are automatically captured in the learnedPOS embeddings and contribute to the improvemen-t of parsing accuracy.
In particular, probabilities ofgrammar rules involving correlated POS tags are au-tomatically smoothed in our approach without theneed for manual features or additional smoothingprocedures.Our experiments show that on the Wall StreetJournal corpus our approach outperforms the pre-vious approaches that also utilize POS tag correla-763tions, and achieves a comparable result with recentstate-of-the-art grammar induction systems.
On thedatasets of eight additional languages, our approachis able to achieve better performance than the base-line methods without any parameter tuning.2 Related work2.1 Dependency Model with ValenceThe dependency model with valence (DMV) (Kleinand Manning, 2004) is the first model to outperformthe left-branching baseline in unsupervised depen-dency parsing of English.
The DMV model is agenerative model of a sentence and its parse tree.
Itgenerates a dependency parse from the root in a re-cursive top-down manner.
At each step, a decision isfirst made as to whether a new child POS tag shall begenerated from the current head tag; if the decisionis yes, then a new child POS tag is sampled; other-wise, the existing child tags are recursively visited.There are three types of grammar rules in the mod-el: CHILD, DECISION and ROOT, each with a setof multinomial parameters PCHILD(c|h, dir, val),PDECISION (dec|h, dir, val) and PROOT (c|root),where dir is a binary variable indicating the genera-tion direction (left or right), val is a boolean variableindicating whether the current head POS tag alreadyhas a child in the current direction or not, c indicatesthe child POS tag, h indicates the head POS tag, anddec indicates the decision of either STOP or CON-TINUE.
A CHILD rule indicates the probability ofgenerating child c given head h on direction dir andvalence val.
A DECISION rule indicates the proba-bility of STOP or CONTINUE given the head, direc-tion and valence.
A ROOT rule is the probability ofa child c generated by the root.
The probability of adependency tree is the product of probabilities of allthe grammar rules used in generating the dependen-cy tree.
The probability of a sentence is the sum ofprobabilities of all the dependency trees consistentwith the sentence.The basic DMV model has the limitation of beingoversimplified and unable to capture certain linguis-tic structures.
Headden et al (2009) incorporatedmore types of valence and lexicalized information inthe DMV model to increase its representation powerand achieved better parsing accuracy than the basicDMV model.2.2 DMV-based Learning Algorithms forUnsupervised Dependency ParsingTo learn a DMV model from text, the ExpectationMaximization (EM) algorithm (Klein and Manning,2004) can be used.
In the E step, the model calcu-lates the expected number of times each grammarrule is used in parsing the training text by using theinside-outside algorithm.
In the M-step, these ex-pected counts are normalized to become the proba-bilities of the grammar rules.There have been many more advanced learning al-gorithms of the DMV model beyond the basic EMalgorithm.
In the work of Cohen and Smith (2008),a logistic normal prior was used in the DMV modelto capture the similarity between POS tags.
In thework of Berg-Kirkpatrick et al (2010), features thatgroup various morphological variants of nouns andverbs are used to predict the DECISION and CHILDparameters.
These two approaches both utilize thecorrelations between POS tags to obtain better prob-ability estimation of grammar rules involving suchcorrelated POS tags.
In the work of Tu and Honavar(2012), unambiguity of parse trees is incorporatedinto the training objective function of DMV to ob-tain a better performance.2.3 Other Approaches to UnsupervisedDependency ParsingThere are many other approaches to unsuperviseddependency parsing that are not based on DMV.Daum?
III (2009) proposed a stochastic search basedmethod to do unsupervised Shift-Reduce transitionparsing.
Rasooli and Faili (2012) proposed a transi-tion based unsupervised dependency parser togetherwith "baby-step" training (Spitkovsky et al, 2010) toimprove parsing accuracy.
Le and Zuidema (2015)proposed a complicated reranking based unsuper-vised dependency parsing system and achieved thestate-of-the-art performance on the Penn Treebankdataset.2.4 Neural based Supervised DependencyParserThere exist several previous approaches on usingneural networks for supervised dependency pars-ing.
Garg and Henderson (2011) proposed a Tem-poral Restricted Boltzmann Machine to do transition764Head POS TagValencyInputs:Continous Representation:Hidden Layer:?Softmax Layer: Outputs (CHILD or DECISION)Wdirp = Softmax(Wf)Wf = ReLU(Wdir[vh; vval])[vh; vval]Figure 1: Structure of the neural network.
Both CHILD andDECISION use the same architecture for the calculation of dis-tributions.based dependency parsing.
Stenetorp (2013) appliedrecursive neural networks to transitional based de-pendency parsing.
Chen and Manning (2014) built aneural network based parser with dense features in-stead of sparse indicator features.
Dyer et al (2015)proposed a stack long short-term memory approachto supervised dependency parsing.
To our knowl-edge, our work is the first attempt to incorporateneural networks into a generative grammar for un-supervised dependency parsing.3 Neural DMVIn this section, we introduce our neural based gram-mar induction approach.
We describe the model insection 3.1 and the learning method in section 3.2.3.1 ModelOur model is based on the DMV model (section 2.1),except that the CHILD and DECISION probabilitiesare calculated through two neural networks.
We donot compute the ROOT probabilities using a neuralnetwork because doing that complicates the mod-el while leads to no significant improvement in theparsing accuracy.
Parsing a sentence using our mod-el can be done in the same way as using DMV.Below we show how the CHILD rule probabilitiesare computed in our neural based DMV model.
De-note the set of all possible POS tags by T .
We builda neural network to compute the probabilities of pro-ducing child tag c ?
T conditioned on the head, di-rection and valence (h, dir, val).The full architecture of the neural network isshown in Figure 1.
First, we represent each headtag h as a d dimensional vector vh ?
Rd, representeach value of valence val as a d?
dimensional vectorvval ?
Rd?
.
We concatenate vh and vval as the in-put embedding vector.
Then we map the input layerto a hidden layer with weight matrix Wdir through aReLU activation function.
We have two versions ofweight matrix Wdir for the direction dir being leftand right respectively.f(h, dir, val) = ReLU(Wdir[vh; vval])We then take the inner product of f and all the childPOS tag vectors and apply a softmax function to ob-tain the rule probabilities:[pc1 , pc2 , ..., pc?T? ]
= Softmax(WTf)whereW = [vc1 , vc2 , ..., vc?T? ]
is an embedding ma-trix composed of all the child POS tag vectors.We use the same neural architecture to predict theprobabilities of DECISION rules.
The difference isthat the neural network for DECISION has only t-wo outputs (STOP and CONTINUE).
Note that thetwo networks share parameters such as head POStag embeddings and direction weight matricesWleftand Wright.
Valence embeddings are either sharedor distinct between the two networks depending onthe variant of DMV we use (i.e., whether the max-imal valences for CHILD and DECISION are thesame).The parameters of our neural based model in-clude the weights of the neural network andall the POS and valence embeddings, denotedby a set ?
= {vh, vc, vval, vdec,Wdir;h, c ?T, val ?
{0, 1, ...}, dir ?
{left, right}, dec ?
{STOP,CONTINUE}}.3.2 LearningIn this section, we describe an approach based on theEM algorithm to learn our neural DMV model.
Tolearn the parameters, given a set of unannotated sen-tences x1, x2, ..., xN , our objective function is thelog-likelihood function.L(?)
=N?
?=1log P(x?
; ?
)765PEWDynamic ProgrammingNeural Network TrainingForward Evaluating Count NormalizingFigure 2: Learning procedure of our neural based DMV model.Green dashed lines represent the EM algorithm for learning tra-ditional DMV.
Red solid lines represent the learning procedureof our model.
P represents the rule probabilities of DMV, Erepresents the expected counts of rules, and W represents theparameters of the neural networks.
In the traditional EM algo-rithm, the expected counts are directly used to re-estimate therule probabilities.
In our approach, parameter re-estimation isdivided into two steps: training the neural networks from theexpected counts and forward evaluation of the neural networksto produce the rule probabilities.The approach is visualized in the Figure 2.
The E-step computes the expected number of times eachgrammar rule used in parsing each training sentencexi, denoted by ec(xi) for CHILD rule c, ed(xi) forDECISION rule d, and er(xi) for ROOT rule r. Inthe M-step of traditional DMV learning, these ex-pected counts are normalized to re-estimate the pa-rameters of DMV.
This maximizes the expected loglikelihood (ELL) with respect to the DMV modelparameters.ELL(?)
=N?
?=1(?cec(xi) log pc+?ded(xi) log pd +?rer(xi) log pr)In our model, however, we do not directly as-sign the optimal rule probabilities of CHILD andDECISION; instead, we train the neural networks tooutput rule probabilities that optimize ELL, which isequivalent to a weighted cross-entropy loss functionfor each neural network.
Note that while the tradi-tional M-step produces the global optimum of ELL,our neural-based M-step does not.
This is because aneural network tends to produce similar outputs forcorrelated inputs.
In our case, the neural networkis able to capture the correlations between differentPOS tags as well as different valence values and s-mooth the probabilities involving correlated tags andvalences.
In other words, our M-step can be seen asoptimizing the ELL with a regularization term tak-ing into account the input correlations.
We use mo-mentum based batch stochastic gradient descent al-gorithm to train the neural network and learn all theembeddings and weight matrices.In addition to standard EM, we can also learn ourneural based DMV model based on the Viterbi EMalgorithm.
The difference from standard EM is thatin the E-step, we compute the number of times eachgrammar rule is used in the best parse of a trainingsentence instead of considering all possible parses.4 Experiments4.1 SetupWe used the Wall Street Journal corpus (with section2-21 for training, section 22 for validation and sec-tion 23 for testing) in section 4.2 and 4.3.
Then wereported the results on eight additional languages insection 4.4.
In each experiment, we trained our mod-el on gold POS tags with sentences of length lessthan 10 after punctuation has been stripped off.
Asthe EM algorithm is very sensitive to initializations,we used the informed initialization method proposedin (Klein and Manning, 2004).The length of embeddings is set to 10 for bothPOS tags and valence.
We trained the neural net-works with batch size 10 and used the change ofthe validation set loss function as the stop criteria.We ran our model for five times and reported the av-eraged directed dependency accuracy (DDA) of thelearned grammars on the test sentences with lengthless than 10 and all sentences.4.2 Comparisons of Approaches based on POSCorrelationsWe first evaluated our approach in learning the basicDMV model and compared the results against (Co-hen and Smith, 2009) and (Berg-Kirkpatrick et al,2010), both of which have very similar motivation asours in that they also utilize the correlation betweenPOS tags to learn the basic DMV model.
Table 1766Methods WSJ10 WSJStandard EM 46.2 34.9Viterbi EM 58.3 39.4LN (Cohen et al, 2008) 59.4 40.5Shared LN (Cohen and Smith, 2009) 61.3 41.4Feature DMV (Berg-Kirkpatrick et al, 2010) 63.0 -Neural DMV (Standard EM) 51.3 37.1Neural DMV (Viterbi EM) 65.9 47.0Table 1: Comparisons of Approaches based on POS Correla-tionsshows the results.
It can be seen that our approachwith Viterbi EM significantly outperforms the EMand viterbi EM baselines and also outperforms thetwo previous approaches.4.3 Results on the extended DMV modelWe directly apply our neural approach to learningthe extended DMV model (Headden III et al, 2009;Gillenwater et al, 2010) (with the maximum va-lence value set to 2 for both CHILD and DECISIONrules).
As shown in Table 2, we achieve comparableaccuracy with recent state-of-the-art systems.
If weinitialize our model with the grammar learned by Tuand Honavar (2012), the accuracy of our approachcan be further improved.Most of the recent state-of-the-art systems em-ploy more complicated models and learning algo-rithms.
For example, Spitkovsky et al (2013) takeseveral grammar induction techniques as modulesand connect them in various ways; Le and Zuide-ma (2015) use a neural-based supervised parser andreranker that make use of high-order features andlexical information.
We expect that the performanceof our approach can be further improved when thesemore advanced techniques are incorporated.4.4 Results on other languagesWe also applied our approach on datasets of eightadditional languages from the PASCAL Challengeon Grammar Induction (Gelling et al, 2012).
Weran our approach using the hyper-parameters fromexperiment 4.2 on the new datasets without any fur-ther tuning.
We tested three versions of our ap-proach based on standard EM, softmax EM (Tu andHonavar, 2012) and Viterbi EM respectively.
Theresults are shown in Table 3 for test sentence lengthno longer than ten and Table 4 for all test sentences.Methods WSJ10 WSJSystems with Basic SetupEVG (Headden III et al, 2009) 65.0 -TSG-DMV (Blunsom and Cohn, 2010) 65.9 53.1PR-S (Gillenwater et al, 2010) 64.3 53.3UR-A E-DMV (Tu and Honavar, 2012) 71.4 57.0Neural E-DMV 69.7 52.5Neural E-DMV (Good Init) 72.5 57.6Systems Using Extra InfoLexTSG-DMV (Blunsom and Cohn, 2010) 67.7 55.7L-EVG (Headden III et al, 2009) 68.8 -CS (Spitkovsky et al, 2013) 72.0 64.4MaxEnc (Le and Zuidema, 2015) 73.2 65.8Table 2: Comparison of recent unsupervised dependency pars-ing systems.
Basic setup means learning from POS tags withsentences of length ?
10 and punctuation stripped off.
Extrainformation may contain punctuations, longer sentences, lexi-cal information, etc.
For Neural E-DMV, ?Good Init?
meansusing the learned DMV model from Tu and Honavar (2012) asour initialization.Our neural based methods achieve better results thantheir corresponding baselines in 75.0% of the casesfor test sentences no longer than 10 and 77.5% forall test sentences.
The good performance of our ap-proach without data-specific hyper-parameter tuningdemonstrates the robustness of our approach.
Care-fully tuned hyper-parameters on validation datasets,in our experience, can further improve the perfor-mance of our approach, in some cases by a largemargin.4.5 Effects of Hyper-parametersWe examine the influence of hyper-parameters onthe performance of our approach with the same ex-perimental setup as in section 4.3.Activation function We compare different linearand non-linear functions: ReLU, Leaky ReLU, Tan-h, Sigmoid.
The results are shown in Table 5.
Non-linear activation functions can be seen to significant-ly outperform linear activation functions.Length of the embedding vectors The dimen-sion of the embedding space is an important hyper-parameter in our system.
As Figure 3 illustrates,when the dimension is too low (such as dim = 5),the performance is bad probably because the embed-ding vectors cannot effectively discriminate between767Arabic Basque Czech Danish Dutch Portuguese Slovene SwedishStandard EMDMV 45.8 41.1 31.3 50.8 47.1 36.7 36.7 43.5Neural DMV 43.4 46.5 33.1 55.6 49.0 30.4 42.2 44.3Softmax EM ?
= 0.25DMV 49.3 45.6 30.4 43.6 46.1 33.5 29.8 50.3Neural DMV 54.2 46.3 36.8 44.0 39.9 35.8 31.2 49.7Softmax EM ?
= 0.5DMV 54.2 47.6 43.2 38.8 38.0 33.7 23.0 37.2Neural DMV 44.6 48.9 33.4 50.3 37.5 35.3 32.2 43.3Softmax EM ?
= 0.75DMV 42.2 48.6 22.7 41.0 33.8 33.5 23.2 41.6Neural DMV 56.7 45.3 31.6 41.3 33.7 34.7 22.9 42.0Viterbi EMDMV 32.5 47.1 27.1 39.1 37.1 32.3 23.7 42.6Neural DMV 48.2 48.1 28.6 39.8 37.2 36.5 39.9 47.9Table 3: DDA results (on sentences no longer than 10) on eight additional languages.
Our neural based approaches are comparedwith traditional approaches using standard EM, softmax EM (parameterized by ?)
and Viterbi EM.Activation function WSJ10ReLU 69.7Leaky ReLU 67.0Tanh 66.2Sigmoid 62.5Linear 55.1Table 5: Comparison between activation functions.Length of POS embedding6 8 10 12 16 246264666870ParsingaccuracyFigure 3: Parsing accuracy vs. length of POS embeddingdifferent POS tags.
On the other hand, when the di-mension is too high (such as dim = 30), since wehave only 35 POS tags, the neural network is proneto overfitting.Shared parameters An alternative to our neuralnetwork architecture is to have two separate neuralnetworks to compute CHILD and DECISION ruleprobabilities respectively.
The embeddings of thehead POS tag and the valence are not shared be-tween the two networks.
As can be seen in TableWSJ10 WSJSeparate Networks 68.6 52.1Merged Network 69.7 52.5Table 6: Comparison between using two separate networks andusing a merged network.6, sharing POS tags embeddings attribute to betterperformance.5 Model AnalysisIn this section, we investigate what information ourneural based DMV model captures and analyze howit contributes to better parsing performance.5.1 Correlation of POS Tags Encoded inEmbeddingsA main motivation of our approach is to encode cor-relation between POS tags in their embeddings soas to smooth the probabilities of grammar rules in-volving correlated POS tags.
Here we want to ex-amine whether the POS embeddings learned by ourapproach successfully capture such correlation.We collected the POS embeddings learned in theexperiment described in section 4.3 and visualizedthem on a 2D plane using the t-SNE algorithm(Van der Maaten and Hinton, 2008).
t-SNE is adimensionality reduction algorithm that maps datafrom a high dimensional space to a low dimensionalone (2 or 3) while maintaining the distances between768Arabic Basque Czech Danish Dutch Portuguese Slovene SwedishStandard EMDMV 28.0 31.2 28.1 40.3 44.2 23.5 25.2 32.0Neural DMV 30.6 38.5 29.3 46.1 46.2 16.2 36.6 32.8Softmax EM ?
= 0.25DMV 30.0 38.1 27.1 35.1 42.5 27.4 23.1 41.6Neural DMV 31.5 40.5 32.6 38.0 35.7 26.7 24.2 41.3Softmax EM ?
= 0.5DMV 32.3 41.0 33.0 32.2 33.9 27.6 15.0 29.6Neural DMV 22.5 42.6 30.6 40.8 37.5 28.6 25.0 33.7Softmax EM ?
= 0.75DMV 30.1 43.0 15.6 33.9 29.9 25.8 15.2 32.7Neural DMV 34.9 37.4 24.7 34.2 29.5 28.9 15.1 33.3Viterbi EMDMV 23.9 40.9 20.4 32.6 33.0 26.9 16.5 36.2Neural DMV 31.0 41.8 23.8 34.2 33.6 29.4 30.8 40.2Table 4: DDA results (on all the sentences) on eight additional languages.
Our neural based approaches are compared withtraditional approaches using standard EM, softmax EM (parameterized by ?)
and viterbi EM.the data points in the high dimensional space.
The"perplexity" hyper-parameter of the algorithm wasset to 20.0 and the distance metric we used is theEuclidean distance.Figure 4 shows the visualization result.
It can beseen that in most cases, nearby POS tags in the figureare indeed similar.
For example, VBP (Verb, non-3rd person singular present), VBD (Verb, past tense)and VBZ (Verb, 3rd person singular present) can beseen to be close to each other, and they indeed havevery similar syntactic behavior.
Similar observationcan be made to NN (Noun, singular or mass ), NNPS(Proper noun, plural) and NNS (Noun, plural).5.2 Smoothing of Grammar Rule ProbabilitiesBy using similar embeddings to represent correlat-ed POS tags, we hope to smooth the probabilities ofrules involving correlated POS tags.
Here we an-alyze whether our neural networks indeed predictmore similar probabilities for rules with correlatedPOS tags.We conducted a case study on all types of verb-s: VBP (Verb, non-3rd person singular present),VBZ (Verb, 3rd person singular present), VBD (Ver-b, past tense), VBN (Verb, past participle), VB (Verb,base form), VBG (Verb, gerund or present participle).We used the neural networks in our N-DMV modellearned in the experiment described in section 4.2 topredict the probabilities of all the CHILD rules head-ed by a verb.
For each pair of verb tags, we com--500 -400 -300 -200 -100 0 100 200 300 400 500-500-400-300-200-1000100200300400500JJMDWDTUHT0RPPRP$PRPPOSPDT$CCCDDTEXFWINWPWP$WRBVBZVBVBDVBGVBNVBPNNSNNPSNNPNNJJSJJRRBSRBRRBAdjAdvNounOthersVerbFigure 4: A visualization of the distances between embeddingsof different POS tags.puted the total variation distance between the multi-nomial distributions of CHILD rules headed by thetwo verb tags.
We also computed the total variationdistances between CHILD rules of verb tags in thebaseline DMV model learned by EM.In Figure 5, We report the differences between thetotal variation distances computed from our modeland from the baseline.
A positive value means thedistance is reduced in our model compared with thatin the baseline.
It can be seen that overall the dis-tances between CHILD rules of different verb tags7699%P 9%= 9%' 9%1 9% 9%*9%P9%=9%'9%19%9%*00000411450300012201220410000000025-022-0121450000000552053053030000250552000004000200122-02205304000000110122-01205300200110000-022 1459%P 9%= 9%' 9%1 9% 9%*9%P9%=9%'9%19%9%*00000411450300012201220410000000025-022-0121450000000552053053030000250552000004000200122-02205304000000110122-01205300200110000-022 145Figure 5: The change of the total variation distances betweenprobabilities of CHILD rules headed by different verb tags inour model vs. the baseline.
A positive value means the distanceis reduced in our model compared with that in the baseline.become smaller in our model.
This verifies that ourapproach smooths the probabilities of rules involv-ing correlated POS tags.
From the figure one cansee that the distance that reduces the most is be-tween VBG and VBN.
These two verb tags indeedhave very similar syntactic behaviors and thus havesimilar embeddings as shown in figure 4.
One theother hand, the distances between VB and VBZ/VBPbecome larger.
This is reasonable since VB is syn-tactically different from VBZ/VBP in that it is verylikely to generate a child tag TO to the left whileVBZ/VBP always generate a subject (e.g., a noun ora pronoun) to the left.6 ConclusionWe propose a neural based DMV model to do unsu-pervised dependency parsing.
Our approach learn-s neural networks with continuous representationsof POS tags to predict the probabilities of grammarrules, thus automatically taking into account the cor-relations between POS tags.
Our experiments showthat our approach outperforms previous approachesutilizing POS correlations and is competitive withrecent state-of-the-art approaches on nine differentlanguages.For future work, we plan to extend our approachin learning lexicalized DMV models.
In addition,we plan to apply our approach to other unsupervisedtasks such as word alignment and sentence cluster-ing.ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590.
Association forComputational Linguistics.Phil Blunsom and Trevor Cohn.
2010.
Unsupervised in-duction of tree substitution grammars for dependencyparsing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 1204?1213.
Association for Computational Lin-guistics.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural network-s.
In EMNLP, pages 740?750.Shay B Cohen and Noah A Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in unsu-pervised grammar induction.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics, pages 74?82.
As-sociation for Computational Linguistics.Shay B Cohen and Noah A Smith.
2010.
Covariancein unsupervised learning of probabilistic grammars.The Journal of Machine Learning Research, 11:3017?3051.Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008.Logistic normal priors for unsupervised probabilisticgrammar induction.
In Advances in Neural Informa-tion Processing Systems, pages 321?328.Hal Daum?
III.
2009.
Unsupervised search-based struc-tured prediction.
In Proceedings of the 26th AnnualInternational Conference on Machine Learning, pages209?216.
ACM.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A Smith.
2015.
Transition-baseddependency parsing with stack long short-term memo-ry.
arXiv preprint arXiv:1505.08075.Nikhil Garg and James Henderson.
2011.
Temporal re-stricted boltzmann machines for dependency parsing.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, pages11?17.
Association for Computational Linguistics.770Douwe Gelling, Trevor Cohn, Phil Blunsom, and JoaoGra?a.
2012.
The pascal challenge on grammar in-duction.
In Proceedings of the NAACL-HLT Workshopon the Induction of Linguistic Structure, pages 64?80.Association for Computational Linguistics.Jennifer Gillenwater, Kuzman Ganchev, Joao Gra?a, Fer-nando Pereira, and Ben Taskar.
2010.
Sparsity in de-pendency grammar induction.
In Proceedings of theACL 2010 Conference Short Papers, pages 194?199.Association for Computational Linguistics.William P Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages101?109.
Association for Computational Linguistics.Dan Klein and Christopher D Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, page 478.
Association for Compu-tational Linguistics.Phong Le and Willem Zuidema.
2015.
Unsuperviseddependency parsing: Let?s use supervised parsers.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 651?661, Denver, Colorado, May?June.
Asso-ciation for Computational Linguistics.Mohammad Sadegh Rasooli and Heshaam Faili.
2012.Fast unsupervised dependency parsing with arc-standard transitions.
In Proceedings of the Joint Work-shop on Unsupervised and Semi-Supervised Learn-ing in NLP, pages 1?9.
Association for ComputationalLinguistics.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2010.
From baby steps to leapfrog: How less ismore in unsupervised dependency parsing.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 751?759.
Associa-tion for Computational Linguistics.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2013.
Breaking out of local optima with coun-t transforms and model recombination: A study ingrammar induction.
In EMNLP, pages 1983?1995.Pontus Stenetorp.
2013.
Transition-based dependen-cy parsing using recursive neural networks.
In NIPSWorkshop on Deep Learning.Kewei Tu and Vasant Honavar.
2012.
Unambiguity reg-ularization for unsupervised learning of probabilisticgrammars.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 1324?1334.
Association for ComputationalLinguistics.Laurens Van der Maaten and Geoffrey Hinton.
2008.
Vi-sualizing data using t-sne.
Journal of Machine Learn-ing Research, 9(2579-2605):85.771
