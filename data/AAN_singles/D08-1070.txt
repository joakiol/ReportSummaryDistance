Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670?679,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLearning with Probabilistic Features for Improved Pipeline ModelsRazvan C. BunescuSchool of EECSOhio UniversityAthens, OH 45701bunescu@ohio.eduAbstractWe present a novel learning framework forpipeline models aimed at improving the com-munication between consecutive stages in apipeline.
Our method exploits the confidencescores associated with outputs at any givenstage in a pipeline in order to compute prob-abilistic features used at other stages down-stream.
We describe a simple method of in-tegrating probabilistic features into the linearscoring functions used by state of the art ma-chine learning algorithms.
Experimental eval-uation on dependency parsing and named en-tity recognition demonstrate the superiority ofour approach over the baseline pipeline mod-els, especially when upstream stages in thepipeline exhibit low accuracy.1 IntroductionMachine learning algorithms are used extensivelyin natural language processing.
Applications rangefrom fundamental language tasks such as part ofspeech (POS) tagging or syntactic parsing, to higherlevel applications such as information extraction(IE), semantic role labeling (SRL), or question an-swering (QA).
Learning a model for a particular lan-guage processing problem often requires the outputfrom other natural language tasks.
Syntactic pars-ing and dependency parsing usually start with a tex-tual input that is tokenized, split in sentences andPOS tagged.
In information extraction, named en-tity recognition (NER), coreference resolution, andrelation extraction (RE) have been shown to benefitfrom features that use POS tags and syntactic depen-dencies.
Similarly, most SRL approaches assumea parse tree representation of the input sentences.The common practice in modeling such dependen-cies is to use a pipeline organization, in which theoutput of one task is fed as input to the next taskin the sequence.
One advantage of this model isthat it is very simple to implement; it also allowsfor a modular approach to natural language process-ing.
The key disadvantage is that errors propagatebetween stages in the pipeline, significantly affect-ing the quality of the final results.
One solutionis to solve the tasks jointly, using the principledframework of probabilistic graphical models.
Sut-ton et al (2004) use factorial Conditional RandomFields (CRFs) (Lafferty et al, 2001) to jointly pre-dict POS tags and segment noun phrases, improvingon the cascaded models that perform the two tasksin sequence.
Wellner et al (2004) describe a CRFmodel that integrates the tasks of citation segmen-tation and citation matching.
Their empirical resultsshow the superiority of the integrated model over thepipeline approach.
While more accurate than theirpipeline analogues, probabilistic graphical modelsthat jointly solve multiple natural language tasks aregenerally more demanding in terms of finding theright representations, the associated inference algo-rithms and their computational complexity.
Recentnegative results on the integration of syntactic pars-ing with SRL (Sutton and McCallum, 2005) provideadditional evidence for the difficulty of this generalapproach.
When dependencies between the taskscan be formulated in terms of constraints betweentheir outputs, a simpler approach is to solve the tasksseparately and integrate the constraints in a linearprogramming formulation, as proposed by Roth and670Yih (2004) for the simultaneous learning of namedentities and relations between them.
More recently,Finkel et al (2006) model the linguistic pipelinesas Bayesian networks on which they perform MonteCarlo inference in order to find the most likely out-put for the final stage in the pipeline.In this paper, we present a new learning methodfor pipeline models that mitigates the problem of er-ror propagation between the tasks.
Our method ex-ploits the probabilities output by any given stage inthe pipeline as weights for the features used at otherstages downstream.
We show a simple method ofintegrating probabilistic features into linear scoringfunctions, which makes our approach applicable tostate of the art machine learning algorithms such asCRFs and Support Vector Machines (Vapnik, 1998;Scho?lkopf and Smola, 2002).
Experimental resultson dependency parsing and named entity recogni-tion show useful improvements over the baselinepipeline models, especially when the basic pipelinecomponents exhibit low accuracy.2 Learning with Probabilistic FeaturesWe consider that the task is to learn a mapping frominputs x ?
X to outputs y ?
Y(x).
Each inputx is also associated with a different set of outputsz ?
Z(x) for which we are given a probabilisticconfidence measure p(z|x).
In a pipeline model, zwould correspond to the annotations performed onthe input x by all stages in the pipeline other thanthe stage that produces y.
For example, in the caseof dependency parsing, x is a sequence of words, yis a set of word-word dependencies, z is a sequenceof POS tags, and p(z|x) is a measure of the confi-dence that the POS tagger has in the output z.
Let?
be a representation function that maps an exam-ple (x, y, z) to a feature vector ?
(x, y, z) ?
Rd, andw ?
Rd a parameter vector.
Equations (1) and (2)below show the traditional method for computingthe optimal output y?
in a pipeline model, assuminga linear scoring function defined by w and ?.y?
(x) = argmaxy?Y(x)w ?
?
(x, y, z?
(x)) (1)z?
(x) = argmaxz?Z(x)p(z|x) (2)The weight vector w is learned by optimizing a pre-defined objective function on a training dataset.In the model above, only the best annotation z?produced by upstream stages is used for determiningthe optimal output y?.
However, z?
may be an incor-rect annotation, while the correct annotation may beignored because it was assigned a lower confidencevalue.
We propose exploiting all possible annota-tions and their probabilities as illustrated in the newmodel below:y?
(x) = argmaxy?Y(x)w ?
?
(x, y) (3)?
(x, y) =?z?Z(x)p(z|x) ?
?
(x, y, z) (4)In most cases, directly computing ?
(x, y) is unfeasi-ble, due to a large number of annotations inZ(x).
Inour dependency parsing example, Z(x) contains allpossible POS taggings of sentence x; consequentlyits cardinality is exponential in the length of the sen-tence.
A more efficient way of computing ?
(x, y)can be designed based on the observation that mostcomponents ?i of the original feature vector ?
utilizeonly a limited amount of evidence from the example(x, y, z).
We define (x?, y?, z?)
?
Fi(x, y, z) to cap-ture the actual evidence from (x, y, z) that is used byone instance of feature function ?i.
We call (x?, y?, z?
)a feature instance of ?i in the example (x, y, z).Correspondingly, Fi(x, y, z) is the set of all fea-ture instances of ?i in example (x, y, z).
Usually,?i(x, y, z) is set to be equal with the number of in-stances of ?i in example (x, y, z), i.e.
?i(x, y, z) =|Fi(x, y, z)|.
Table 1 illustrates three feature in-stances (x?, y?, z?)
generated by three typical depen-dency parsing features in the example from Figure 1.Because the same feature may be instantiated multi-?1 : DT?
NN ?2 : NNS?
thought ?3 : be?
iny?
10?11 2?4 7?9z?
DT10 NN11 NNS2x?
thought4 be7 in9|Fi| O(|x|2) O(|x|) O(1)Table 1: Feature instances.ple times in the same example, the components ofeach feature instance are annotated with their po-sitions relative to the example.
Given these defi-nitions, the feature vector ?
(x, y) from (4) can be671Figure 1: Dependency Parsing Example.rewritten in a component-wise manner as follows:?
(x, y) = [?1(x, y) .
.
.
?d(x, y)] (5)?i(x, y) =?z?Z(x)p(z|x) ?
?i(x, y, z)=?z?Z(x)p(z|x) ?
|Fi(x, y, z)|=?z?Z(x)p(z|x)?(x?,y?,z?)?Fi(x,y,z)1=?z?Z(x)?(x?,y?,z?)?Fi(x,y,z)p(z|x)=?(x?,y?,z?
)?Fi(x,y,Z(x))?z?Z(x),z?z?p(z|x)where Fi(x, y,Z(x)) stands for:Fi(x, y,Z(x)) =?z?Z(x)Fi(x, y, z)We introduce p(z?|x) to denote the expectation:p(z?|x) =?z?Z(x),z?z?p(z|x)Then ?i(x, y) can be written compactly as:?i(x, y) =?(x?,y?,z?
)?Fi(x,y,Z(x))p(z?|x) (6)The total number of terms in (6) is equal with thenumber of instantiations of feature ?i in the exam-ple (x, y) across all possible annotations z ?
Z(x),i.e.
|Fi(x, y,Z(x))|.
Usually this is significantlysmaller than the exponential number of terms in (4).The actual number of terms depends on the particu-lar feature used to generate them, as illustrated in thelast row of Table 1 for the three features used in de-pendency parsing.
The overall time complexity forcalculating ?
(x, y) also depends on the time com-plexity needed to compute the expectations p(z?|x).When z is a sequence, p(z?|x) can be computed ef-ficiently using a constrained version of the forward-backward algorithm (to be described in Section 3).When z is a tree, p(z?|x) will be computed using aconstrained version of the CYK algorithm (to be de-scribed in Section 4).The time complexity can be further reduced if in-stead of ?
(x, y) we use its subcomponent ??
(x, y)that is calculated based only on instances that appearin the optimal annotation z?:??
(x, y) = [?
?1(x, y) .
.
.
?
?d(x, y)] (7)?
?i(x, y) =?(x?,y?,z?)?Fi(x,y,z?
)p(z?|x) (8)The three models are summarized in Table 2 below.In the next two sections we illustrate their applica-y?
(x) = argmaxy?Y(x)w ?
?
(x, y)M1 ?
(x, y) = ?
(x, y, z?(x))z?
(x) = argmaxz?Z(x)p(z|x)y?
(x) = argmaxy?Y(x)w ?
?
(x, y)M2 ?
(x, y) = [?1(x, y) .
.
.
?d(x, y)]?i(x, y) =?(x?,y?,z?)?Fi(x,y,Z(x))p(z?|x)y?
(x) = argmaxy?Y(x)w ?
??
(x, y)M3 ??
(x, y) = [?
?1(x, y) .
.
.
?
?d(x, y)]?
?i(x, y) =?(x?,y?,z?)?Fi(x,y,z?
)p(z?|x)Table 2: Three Pipeline Models.tion to two common tasks in language processing:dependency parsing and named entity recognition.3 Dependency Parsing PipelineIn a traditional dependency parsing pipeline (modelM1 in Table 2), an input sentence x is first aug-672mented with a POS tagging z?
(x), and then pro-cessed by a dependency parser in order to obtaina dependency structure y?(x).
To evaluate the newpipeline models we use MSTPARSER1, a linearlyscored dependency parser developed by McDonaldet al (2005).
Following the edge based factorizationmethod of Eisner (1996), the score of a dependencytree in the first order version is defined as the sum ofthe scores of all edges in the tree.
Equivalently, thefeature vector of a dependency tree is defined as thesum of the feature vectors of all edges in the tree:M1: ?
(x, y) =?u?v?y?
(x, u?v, z?
(x))M2: ?
(x, y) =?u?v?y?
(x, u?v)M3: ??
(x, y) =?u?v?y??
(x, u?v)For each edge u?
v ?
y, MSTPARSER generatesfeatures based on a set of feature templates that takeinto account the words and POS tags at positions u,v, and their left and right neighbors u?1, v?1.
Forexample, a particular feature template T used insideMSTPARSER generates the following POS bigramfeatures:?i(x, u?v, z) ={1, if ?zu, zv?
= ?t1, t2?0, otherwisewhere t1, t2 ?
P are the two POS tags associatedwith feature index i.
By replacing y with u?
v inthe feature expressions from Table 2, we obtain thefollowing formulations:M1:?i(x, u?v) ={1, if ?z?u, z?v?=?t1, t2?0, otherwiseM2:?i(x, u?v) = p(z?=?t1, t2?|x)M3: ?
?i(x, u?v) ={p(z?=?t1, t2?|x), if ?z?u, z?v?=?t1, t2?0, otherwisewhere, following the notation from Section 2,z?
= ?zu, zv?
is the actual evidence from z that isused by feature i, and z?
is the top scoring annotationproduced by the POS tagger.
The implementation inMSTPARSER corresponds to the traditional pipelinemodel M1.
Given a method for computing feature1URL: http://sourceforge.net/projects/mstparserprobabilities p(z?
= ?t1, t2?|x), it is straightforwardto modify MSTPARSER to implement models M2and M3 ?
we simply replace the feature vectors ?with ?
and ??
respectively.
As mentioned in Sec-tion 2, the time complexity of computing the fea-ture vectors ?
in model M2 depends on the com-plexity of the actual evidence z?
used by the fea-tures.
For example, the feature template T usedabove is based on the POS tags at both ends of a de-pendency edge, consequently it would generate |P|2features in model M2 for any given edge u ?
v.There are however feature templates used in MST-PARSER that are based on the POS tags of up to 4tokens in the input sentence, which means that foreach edge they would generate |P|4 ?
4.5M fea-tures.
Whether using all these probabilistic featuresis computationally feasible or not also depends onthe time complexity of computing the confidencemeasure p(z?|x) associated with each feature.3.1 Probabilistic POS featuresThe new pipeline models M2 and M3 require anannotation model that, at a minimum, facilitatesthe computation of probabilistic confidence valuesfor each output.
We chose to use linear chainCRFs (Lafferty et al, 2001) since CRFs can be eas-ily modified to compute expectations of the typep(z?|x), as needed by M2 and M3.The CRF tagger was implemented in MAL-LET (McCallum, 2002) using the original featuretemplates from (Ratnaparkhi, 1996).
The modelwas trained on sections 2?21 from the English PennTreebank (Marcus et al, 1993).
When tested on sec-tion 23, the CRF tagger obtains 96.25% accuracy,which is competitive with more finely tuned systemssuch as Ratnaparkhi?s MaxEnt tagger.We have also implemented in MALLET a con-strained version of the forward-backward procedurethat allows computing feature probabilities p(z?|x).If z?
= ?ti1ti2 ...tik?
specifies the tags at k positionsin the sentence, then the procedure recomputes the?
parameters for all positions between i1 and ik byconstraining the state transitions to pass through thespecified tags at the k positions.
A similar approachwas used by Culotta et al in (2004) in order to asso-ciate confidence values with sequences of contigu-ous tokens identified by a CRF model as fields in aninformation extraction task.
The constrained proce-673dure requires (ik ?
i1)|P|2 = O(N |P|2) multipli-cations in an order 1 Markov model, where N is thelength of the sentence.
Because MSTPARSER usesan edge based factorization of the scoring function,the constrained forward procedure will need to berun for each feature template, for each pair of tokensin the input sentence x.
If the evidence z?
required bythe feature template T constrains the tags at k posi-tions, then the total time complexity for computingthe probabilistic features p(z?|x) generated by T is:O(N3|P|k+2)=O(N |P|2) ?O(N2) ?O(|P|k) (9)As mentioned earlier, some feature templates usedin the dependency parser constrain the POS tags at 4positions, leading to a O(N3|P|6) time complexityfor a length N sentence.
Experimental runs on thesame machine that was used for CRF training showthat such a time complexity is not yet feasible, espe-cially because of the large size of P (46 POS tags).In order to speed up the computation of probabilis-tic features, we made the following two approxima-tions:1.
Instead of using the constrained forward-backward procedure, we enforce an indepen-dence assumption between tags at different po-sitions and rewrite p(z?
= ?ti1ti2 ...tik?|x) as:p(ti1ti2 ...tik |x) ?k?j=1p(tij |x)The marginal probabilities p(tij |x) are easilycomputed using the original forward and back-ward parameters as:p(tij |x) =?ij (tij |x)?ij (tij |x)Z(x)This approximation eliminates the factorO(N |P|2) from the time complexity in (9).2.
If any of the marginal probabilities p(tij |x) isless than a predefined threshold (?
|P|)?1, weset p(z?|x) to 0.
When ?
?
1, the method isguaranteed to consider at least the most proba-ble state when computing the probabilistic fea-tures.
Looking back at Equation (4), this isequivalent with summing feature vectors onlyover the most probable annotations z ?
Z(x).The approximation effectively replaces the fac-tor O(|P|k) in (9) with a quasi-constant factor.The two approximations lead to an overall time com-plexity of O(N2) for computing the probabilisticfeatures associated with any feature template T , plusO(N |P|2) for the unconstrained forward-backwardprocedure.
We will use M ?2 to refer to the modelM2 that incorporates the two approximations.
Theindependence assumption from the first approxima-tion can be relaxed without increasing the asymp-totic time complexity by considering as independentonly chunks of contiguous POS tags that are at leasta certain number of tokens apart.
Consequently,the probability of the tag sequence will be approxi-mated with the product of the probabilities of the tagchunks, where the exact probability of each chunkis computed in constant time with the constrainedforward-backward procedure.
We will use M ?
?2 torefer to the resulting model.3.2 Experimental ResultsMSTPARSER was trained on sections 2?21 from theWSJ Penn Treebank, using the gold standard POStagging.
The parser was then evaluated on section23, using the POS tagging output by the CRF tagger.For model M1 we need only the best output fromthe POS tagger.
For models M ?2 and M ?
?2 we com-pute the probability associated with each feature us-ing the corresponding approximations, as describedin the previous section.
In model M ?
?2 we consideras independent only chunks of POS tags that are 4tokens or more apart.
If the distance between thechunks is less than 4 tokens, the probability for theentire tag sequence in the feature is computed ex-actly using the constrained forward-backward pro-cedure.
Table 3 shows the accuracy obtained bymodels M1, M ?2(?)
and M ?
?2 (?)
for various valuesof the threshold parameter ?
.
The accuracy is com-M1 M ?2(1) M ?2(2) M ?2(4) M ?
?2 (4)88.51 88.66 88.67 88.67 88.70Table 3: Dependency parsing results.puted over unlabeled dependencies i.e.
the percent-age of words for which the parser has correctly iden-tified the parent in the dependency tree.
The pipeline674Figure 2: Named Entity Recognition Example.model M ?2 that uses probabilistic features outper-forms the traditional pipeline model M1.
As ex-pected, M ?
?2 performs slightly better than M ?2, dueto a more exact computation of feature probabilities.Overall, only by using the probabilities associatedwith the POS features, we achieve an absolute er-ror reduction of 0.19%, in a context where the POSstage in the pipeline already has a very high accu-racy of 96.25%.
We expect probabilistic features toyield a more substantial improvement in cases wherethe pipeline model contains less accurate upstreamstages.
Such a case is that of NER based on a com-bination of POS and dependency parsing features.4 Named Entity Recognition PipelineIn Named Entity Recognition (NER), the task is toidentify textual mentions of predefined types of en-tities.
Traditionally, NER is modeled as a sequenceclassification problem: each token in the input sen-tence is tagged as being either inside (I) or outside(O) of an entity mention.
Most sequence taggingapproaches use the words and the POS tags in alimited neighborhood of the current sentence posi-tion in order to compute the corresponding features.We augment these flat features with a set of treefeatures that are computed based on the words andPOS tags found in the proximity of the current to-ken in the dependency tree of the sentence.
Weargue that such dependency tree features are betterat capturing predicate-argument relationships, espe-cially when they span long stretches of text.
Figure 2shows a sentence x together with its POS tagging z1,dependency links z2, and an output tagging y. As-suming the task is to recognize mentions of people,the word sailors needs to be tagged as inside.
If weextracted only flat features using a symmetric win-dow of size 3, the relationship between sailors andthought would be missed.
This relationship is use-ful, since an agent of the predicate thought is likelyto be a person entity.
On the other hand, the nodessailors and thought are adjacent in the dependencytree of the sentence.
Therefore, their relationshipcan be easily captured as a dependency tree featureusing the same window size.For every token position, we generate flat featuresby considering all unigrams, bigrams and trigramsthat start with the current token and extend either tothe left or to the right.
Similarly, we generate treefeatures by considering all unigrams, bigrams andtrigrams that start with the current token and extendin any direction in the undirected version of the de-pendency tree.
The tree features are also augmentedwith the actual direction of the dependency arcs be-tween the tokens.
If we use only words to createn-gram features, the token sailors will be associatedwith the following features:?
Flat: sailors, the sailors, ?S?
the sailors,sailors mistakenly, sailors mistakenly thought.?
Tree: sailors, sailors ?
the, sailors ?thought, sailors?
thought?
must, sailors?thought?
mistakenly.We also allow n-grams to use word classes such asPOS tags and any of the following five categories:?1C?
for tokens consisting of one capital letter, ?AC?for tokens containing only capital letters, ?FC?
fortokens that start with a capital letter, followed bysmall letters, ?CD?
for tokens containing at least onedigit, and ?CRT?
for the current token.The set of features can then be defined as a Carte-sian product over word classes, as illustrated in Fig-ure 3 for the original tree feature sailors?
thought?
mistakenly.
In this case, instead of one com-pletely lexicalized feature, the model will consider12 different features such as sailors?
VBD?
RB,NNS?
thought?
RB, or NNS?
VBD?
RB.675???CRT?NNSsailors???[?]?[VBDthought]?[?]?
[RBmistakenly]Figure 3: Dependency tree features.The pipeline model M2 uses features that appearin all possible annotations z = ?z1, z2?, where z1and z2 are the POS tagging and the dependencyparse respectively.
If the corresponding evidence isz?
= ?z?1, z?2?, then:p(z?|x) = p(z?2|z?1, x)p(z?1|x)For example, NNS2 ?
thought4 ?
RB3 is a featureinstance for the token sailors in the annotations fromFigure 2.
This can be construed as having been gen-erated by a feature template T that outputs the POStag ti at the current position, the word xj that is theparent of xi in the dependency tree, and the POS tagtk of another dependent of xj (i.e.
ti ?
xj ?
tk).The probability p(z?|x) for this type of features canthen be written as:p(z?|x) = p(i?j?k|ti, tk, x) ?
p(ti, tk|x)The two probability factors can be computed exactlyas follows:1.
The M2 model for dependency parsing fromSection 3 is used to compute the probabilisticfeatures ?
(x, u?
v|ti, tk) by constraining thePOS annotations to pass through tags ti and tkat positions i and k. The total time complexityfor this step is O(N3|P|k+2).2.
Having access to ?
(x, u?
v|ti, tk), the factorp(i?j?k|ti, tk, x) can be computed in O(N3)time using a constrained version of Eisner?s al-gorithm, as will be explained in Section 4.1.3.
As described in Section 3.1, computing theexpectation p(ti, tk|x) takes O(N |P2|) timeusing the constrained forward-backward algo-rithm.The current token position i can have a total of Nvalues, while j and k can be any positions otherthan i.
Also, ti and tk can be any POS tag fromP .
Consequently, the feature template T inducesO(N3|P|2) feature instances.
Overall, the timecomplexity for computing the feature instances gen-erated by T is O(N6|P|k+4), as results from:O(N3|P|2) ?
(O(N3|P|k+2) +O(N3) +O(N |P|2))While still polynomial, this time complexity is fea-sible only for small values ofN .
In general, the timecomplexity for computing probabilistic features inthe full model M2 increases with both the numberof stages in the pipeline and the complexity of thefeatures.Motivated by efficiency, we decided to use thepipeline model M3 in which probabilities are com-puted only over features that appear in the top scor-ing annotation z?
= ?z?1, z?2?, where z?1 and z?2 repre-sent the best POS tagging, and the best dependencyparse respectively.
In order to further speed up thecomputation of probabilistic features, we made thefollowing approximations:1.
We consider the POS tagging and the depen-dency parse independent and rewrite p(z?|x) as:p(z?|x) = p(z?1, z?2|x) ?
p(z?1|x)p(z?2|x)2.
We enforce an independence assumption be-tween POS tags.
Thus, if z?1 = ?ti1ti2 ...tik?specifies the tags at k positions in the sentence,then p(z?1|x) is rewritten as:p(ti1ti2 ...tik |x) ?k?j=1p(tij |x)3.
We also enforce a similar independence as-sumption between dependency links.
Thus, ifz?2 = ?u1 ?
v1...uk ?
vk?
specifies k depen-dency links, then p(z?2|x) is rewritten as:p(u1?v1...uk?vk|x) ?k?l=1p(ul?vl|x)For example, the probability p(z?|x) of the featureinstance NNS2 ?
thought4 ?
RB3 is approximatedas:p(z?|x) ?
p(z?1|x) ?
p(z?2|x)p(z?1|x) ?
p(t2 =NNS|x) ?
p(t3 =RB|x)p(z?2|x) ?
p(2?4|x) ?
p(3?4|x)We will use M ?3 to refer to the resulting model.6764.1 Probabilistic Dependency FeaturesThe probabilistic POS features p(ti|x) are computedusing the forward-backward procedure in CRFs, asdescribed in Section 3.1.
To completely specify thepipeline model for NER, we also need an efficientmethod for computing the probabilistic dependencyfeatures p(u?
v|x), where u?
v is a dependencyedge between positions u and v in the sentence x.MSTPARSER is a large-margin method that com-putes an unbounded score s(x, y) for any given sen-tence x and dependency structure y ?
Y(x) usingthe following edge-based factorization:s(x, y) =?u?v?ys(x, u?v) = w?u?v?y?
(x, u?v)The following three steps describe a general methodfor associating probabilities with output substruc-tures.
The method can be applied whenever a struc-tured output is associated a score value that is un-bounded in R, assuming that the score of the entireoutput structure can be computed efficiently basedon a factorization into smaller substructures.S1.
Map the unbounded score s(x, y) from Rinto [0, 1] using the softmax function (Bishop, 1995):n(x, y) = es(x,y)?y?Y(x) es(x,y)The normalized score n(x, y) preserves the rankinggiven by the original score s(x, y).
The normaliza-tion constant at the denominator can be computed inO(N3) time by replacing the max operator with thesum operator inside Eisner?s chart parsing algorithm.S2.
Compute a normalized score for the sub-structure by summing up the normalized scores ofall the complete structures that contain it.
In ourmodel, dependency edges are substructures, whiledependency trees are complete structures.
The nor-malized score will then be computed as:n(x, u?v) =?y?Y(x),u?v?yn(x, y)The sum can be computed in O(N3) time using aconstrained version of the algorithm that computesthe normalization constant in step S1.
This con-strained version of Eisner?s algorithm works in asimilar manner with the constrained forward back-ward algorithm by restricting the dependency struc-tures to contain a predefined edge or set of edges.S3.
Use the isotonic regression method ofZadrozny and Elkan (2002) to map the normalizedscores n(x, u?
v) into probabilities p(u?
v|x).
Apotential problem with the softmax function is that,depending on the distribution of scores, the expo-nential transform could dramatically overinflate thehigher scores.
Isotonic regression, by redistributingthe normalized scores inside [0, 1], can alleviate thisproblem.4.2 Experimental ResultsWe test the pipeline model M ?3 versus the traditionalmodel M1 on the task of detecting mentions of per-son entities in the ACE dataset2.
We use the standardtraining ?
testing split of the ACE 2002 dataset inwhich the training dataset is also augmented with thedocuments from the ACE 2003 dataset.
The com-bined dataset contains 674 documents for trainingand 97 for testing.
We implemented the CRF modelin MALLET using three different sets of features:Tree, Flat, and Full corresponding to the union ofall flat and tree features.
The POS tagger and the de-pendency parser were trained on sections 2-21 of thePenn Treebank, followed by an isotonic regressionstep on section 23 for the dependency parser.
Wecompute precision recall (PR) graphs by varying athreshold on the token level confidence output by theCRF tagger, and summarize the tagger performanceusing the area under the curve.
Table 4 shows the re-sults obtained by the two models under the three fea-ture settings.
The model based on probabilistic fea-Model Tree Flat FullM ?3 76.78 77.02 77.96M1 74.38 76.53 77.02Table 4: Mention detection results.tures consistently outperforms the traditional model,especially when only tree features are used.
Depen-dency parsing is significantly less accurate than POStagging.
Consequently, the improvement for the treebased model is more substantial than for the flat2URL: http://www.nist.gov/speech/tests/ace67700.10.20.30.40.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallProbabilisticTraditionalFigure 4: PR graphs for tree features.model, confirming our expectation that probabilis-tic features are more useful when upstream stages inthe pipeline are less accurate.
Figure 4 shows the PRcurves obtained for the tree-based models, on whichwe see a significant 5% improvement in precisionover a wide range of recall values.5 Related WorkIn terms of the target task ?
improving the perfor-mance of linguistic pipelines ?
our research is mostrelated to the work of Finkel et al (2006).
In theirapproach, output samples are drawn at each stagein the pipeline conditioned on the samples drawnat previous stages, and the final output is deter-mined by a majority vote over the samples fromthe final stage.
The method needs very few sam-ples for tasks such as textual entailment, where thefinal outcome is binary, in agreement with a theo-retical result on the rate of convergence of the vot-ing Gibbs classifier due to Ng and Jordan (2001).While their sampling method is inherently approx-imate, our full pipeline model M2 is exact in thesense that feature expectations are computed exactlyin polynomial time whenever the inference step ateach stage can be done in polynomial time, irrespec-tive of the cardinality of the final output space.
Also,the pipeline models M2 and M3 and their more effi-cient alternatives propagate uncertainty during bothtraining and testing through the vector of probabilis-tic features, whereas the sampling method takes ad-vantage of the probabilistic nature of the outputsonly during testing.
Overall, the two approachescan be seen as complementary.
In order to be ap-plicable with minimal engineering effort, the sam-pling method needs NLP researchers to write pack-ages that can generate samples from the posterior.Similarly, the new pipeline models could be easilyapplied in a diverse range of applications, assum-ing researchers develop packages that can efficientlycompute marginals over output substructures.6 Conclusions and Future WorkWe have presented a new, general method for im-proving the communication between consecutivestages in pipeline models.
The method relies onthe computation of probabilities for count features,which translates in adding a polynomial factor to theoverall time complexity of the pipeline whenever theinference step at each stage is done in polynomialtime, which is the case for the vast majority of infer-ence algorithms used in practical NLP applications.We have also shown that additional independenceassumptions can make the approach more practicalby significantly reducing the time complexity.
Ex-isting learning based models can implement the newmethod by replacing the original feature vector witha more dense vector of probabilistic features3.
It isessential that every stage in the pipeline producesprobabilistic features, and to this end we have de-scribed an effective method for associating proba-bilities with output substructures.We have shown for NER that simply using theprobabilities associated with features that appearonly in the top annotation can lead to useful im-provements in performance, with minimal engineer-ing effort.
In future work we plan to empiricallyevaluate NER with an approximate version of thefull model M2 which, while more demanding interms of time complexity, could lead to even moresignificant gains in accuracy.
We also intend to com-prehensively evaluate the proposed scheme for com-puting probabilities by experimenting with alterna-tive normalization functions.AcknowledgementsWe would like to thank Rada Mihalcea and theanonymous reviewers for their insightful commentsand suggestions.3The Java source code will be released on my web page.678ReferencesChristopher M. Bishop.
1995.
Neural Networks for Pat-tern Recogntion.
Oxford University Press.Aron Culotta and Andrew McCallum.
2004.
Confidenceestimation for information extraction.
In Proceed-ings of Human Language Technology Conference andNorth American Chapter of the Association for Com-putational Linguistics (HLT-NAACL), Boston, MA.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of the 16th Conference on Computational linguis-tics, pages 340?345, Copenhagen, Denmark.Jenny R. Finkel, Christopher D. Manning, and Andrew Y.Ng.
2006.
Solving the problem of cascading errors:Approximate Bayesian inference for linguistic annota-tion pipelines.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, pages 618?626, Sydney, Australia.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of 18th International Conference onMachine Learning (ICML-2001), pages 282?289,Williamstown, MA.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn treebank.
Computational Linguistics,19(2):313?330.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics (ACL-05), pages 91?98, Ann Arbor, Michigan.Andrew Y. Ng and Michael I. Jordan.
2001.
Conver-gence rates of the Voting Gibbs classifier, with appli-cation to bayesian feature selection.
In Proceedings of18th International Conference on Machine Learning(ICML-2001), pages 377?384, Williamstown, MA.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part of speech tagging.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-96), pages 133?141, Philadel-phia, PA.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.In Proceedings of the Eighth Conference on Compu-tational Natural Language Learning (CoNLL-2004),pages 1?8, Boston, MA.Bernhard Scho?lkopf and Alexander J. Smola.
2002.Learning with kernels - support vector machines, regu-larization, optimization and beyond.
MIT Press, Cam-bridge, MA.Charles Sutton and Andrew McCallum.
2005.
Joint pars-ing and semantic role labeling.
In CoNLL-05 SharedTask.Charles Sutton, Khashayar Rohanimanesh, and AndrewMcCallum.
2004.
Dynamic conditional randomfields: Factorized probabilistic models for labeling andsegmenting sequence data.
In Proceedings of 21st In-ternational Conference on Machine Learning (ICML-2004), pages 783?790, Banff, Canada, July.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley & Sons.Ben Wellner, Andrew McCallum, Fuchun Peng, andMichael Hay.
2004.
An integrated, conditional modelof information extraction and coreference with appli-cation to citation matching.
In Proceedings of 20thConference on Uncertainty in Artificial Intelligence(UAI-2004), Banff, Canada, July.Bianca Zadrozny and Charles Elkan.
2002.
Trans-forming classifier scores into accurate multiclass prob-ability estimates.
In Proceedings of the EighthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD-2002), Ed-monton, Alberta.679
