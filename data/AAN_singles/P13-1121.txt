Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1233?1242,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTowards Robust Abstractive Multi-Document Summarization: ACaseframe Analysis of Centrality and DomainJackie Chi Kit CheungUniversity of Toronto10 King?s College Rd., Room 3302Toronto, ON, Canada M5S 3G4jcheung@cs.toronto.eduGerald PennUniversity of Toronto10 King?s College Rd., Room 3302Toronto, ON, Canada M5S 3G4gpenn@cs.toronto.eduAbstractIn automatic summarization, centrality isthe notion that a summary should containthe core parts of the source text.
Cur-rent systems use centrality, along with re-dundancy avoidance and some sentencecompression, to produce mostly extrac-tive summaries.
In this paper, we investi-gate how summarization can advance pastthis paradigm towards robust abstractionby making greater use of the domain ofthe source text.
We conduct a series ofstudies comparing human-written modelsummaries to system summaries at the se-mantic level of caseframes.
We show thatmodel summaries (1) are more abstrac-tive and make use of more sentence aggre-gation, (2) do not contain as many topi-cal caseframes as system summaries, and(3) cannot be reconstructed solely fromthe source text, but can be if texts fromin-domain documents are added.
Theseresults suggest that substantial improve-ments are unlikely to result from betteroptimizing centrality-based criteria, butrather more domain knowledge is needed.1 IntroductionIn automatic summarization, centrality has beenone of the guiding principles for content selectionin extractive systems.
We define centrality to bethe idea that a summary should contain the partsof the source text that are most similar or repre-sentative of the source text.
This is most trans-parently illustrated by the Maximal Marginal Rel-evance (MMR) system of Carbonell and Goldstein(1998), which defines the summarization objectiveto be a linear combination of a centrality term anda non-redundancy term.Since MMR, much progress has been made onmore sophisticated methods of measuring central-ity and integrating it with non-redundancy (SeeNenkova and McKeown (2011) for a recent sur-vey).
For example, term weighting methods suchas the signature term method of Lin and Hovy(2000) pick out salient terms that occur more oftenthan would be expected in the source text based onfrequencies in a background corpus.
This methodis a core component of the most successful sum-marization methods (Conroy et al, 2006).While extractive methods based on centralityhave thus achieved success, there has long beenrecognition that abstractive methods are ultimatelymore desirable.
One line of work is in text simpli-fication and sentence fusion, which focus on theability of abstraction to achieve a higher compres-sion ratio (Knight and Marcu, 2000; Barzilay andMcKeown, 2005).
A less examined issue is that ofaggregation and information synthesis.
A key partof the usefulness of summaries is that they providesome synthesis or analysis of the source text andmake a more general statement that is of direct rel-evance to the user.
For example, a series of relatedevents can be aggregated and expressed as a trend.The position of this paper is that centrality isnot enough to make substantial progress towardsabstractive summarization that is capable of thistype of semantic inference.
Instead, summariza-tion systems need to make more use of domainknowledge.
We provide evidence for this in a se-ries of studies on the TAC 2010 guided summa-rization data set that examines how the behaviourof automatic summarizers can or cannot be dis-tinguished from human summarizers.
First, weconfirm that abstraction is a desirable goal, and1233provide a quantitative measure of the degree ofsentence aggregation in a summarization system.Second, we show that centrality-based measuresare unlikely to lead to substantial progress towardsabstractive summarization, because current top-performing systems already produce summariesthat are more ?central?
than humans do.
Third, weconsider how domain knowledge may be useful asa resource for an abstractive system, by showingthat key parts of model summaries can be recon-structed from the source plus related in-domaindocuments.Our contributions are novel in the following re-spects.
First, our analyses are performed at thelevel of caseframes, rather at the level of words orsyntactic dependencies as in previous work.
Case-frames are shallow approximations of semanticroles which are well suited to characterizing a do-main by its slots.
Furthermore, we take a devel-opmental rather than evaluative perspective?ourgoal is not to develop a new evaluation measure asdefined by correlation with human responsivenessjudgments.
Instead, our studies reveal useful cri-teria with which to distinguish human-written andsystem summaries, helping to guide the develop-ment of future summarization systems.2 Related WorkDomain-dependent template-based summariza-tion systems have been an alternative to extractivesystems which make use of rich knowledge abouta domain and information extraction techniques togenerate a summary, possibly using a natural lan-guage generation system (Radev and McKeown,1998; White et al, 2001; McKeown et al, 2002).This paper can be seen as a first step towardsreconciling the advantages of domain knowledgewith the resource-lean extraction approaches pop-ular today.As noted above, Lin and Hovy?s (2000) sig-nature terms have been successful in discoveringterms that are specific to the source text.
Theseterms are identified by a log-likelihood ratio testbased on their relative frequencies in relevant andirrelevant documents.
They were originally pro-posed in the context of single-document summa-rization, where they were calculated using in-domain (relevant) vs. out-of-domain (irrelevant)text.
In multi-document summarization, the in-domain text has been replaced by the source textcluster (Conroy et al, 2006), thus they are nowused as a form of centrality-based features.
Inthis paper, we use guided summarization data asan opportunity to reopen the investigation into theeffect of domain, because multiple document clus-ters from the same domain are available.Summarization evaluation is typically done bycomparing system output to human-written modelsummaries, and are validated by their correlationwith user responsiveness judgments.
The compar-ison can be done at the word level, as in ROUGE(Lin, 2004), at the syntactic level, as in BasicElements (Hovy et al, 2006), or at the level ofsummary content units, as in the Pyramid method(Nenkova and Passonneau, 2004).
There are alsoautomatic measures which do not require modelsummaries, but compare against the source text in-stead (Louis and Nenkova, 2009; Saggion et al,2010).Several studies complement this paper by ex-amining the best possible extractive system us-ing current evaluation measures, such as ROUGE(Lin and Hovy, 2003; Conroy et al, 2006).
Theyfind that the best possible extractive systems scorehigher or as highly than human summarizers, butit is unclear whether this means the oracle sum-maries are actually as useful as human ones inan extrinsic setting.
Genest et al (2009) ask hu-mans to create extractive summaries, and find thatthey score in between current automatic systemsand human-written abstracts on responsiveness,linguistic quality, and Pyramid score.
In the lec-ture domain, He et al (1999; 2000) find thatlecture transcripts that have been manually high-lighted with key points improve students?
quizscores more than when using automated summa-rization techniques or when providing only thelecture transcript or slides.Jing and McKeown (2000) manually analyzed30 human-written summaries, and find that 19%of sentences cannot be explained by cut-and-pasteoperations from the source text.
Saggion and La-palme (2002) similarly define a list of transfor-mations necessary to convert source text to sum-mary text, and manually analyzed their frequen-cies.
Copeck and Szpakowicz (2004) find thatat most 55% of vocabulary items found in modelsummaries occur in the source text, but they donot investigate where the other vocabulary itemsmight be found.1234Sentence:At one point, two bomb squad trucks sped tothe school after a backpack scare.Dependencies:num(point, one) prep at(sped, point)num(trucks, two) nn(trucks, bomb)nn(trucks, squad) nsubj(sped, trucks)root(ROOT, sped) det(school, the)prep to(sped, school) det(scare, a)nn(scare, backpack) prep after(sped, scare)Caseframes:(speed, prep at) (speed, nsubj)(speed, prep to) (speed, prep after)Table 1: A sentence decomposed into its depen-dency edges, and the caseframes derived fromthose edges that we consider (in black).3 Theoretical basis of our analysisMany existing summarization evaluation methodsrely on word or N-gram overlap measures, butthese measures are not appropriate for our anal-ysis.
Word overlap can occur due to shared propernouns or entity mentions.
Good summaries shouldcertainly contain the salient entities in the sourcetext, but when assessing the effect of the domain,different domain instances (i.e., different docu-ment clusters in the same domain) would be ex-pected to contain different salient entities.
Also,the realization of entities as noun phrases dependsstrongly on context, which would confound ouranalysis if we do not also correctly resolve corefer-ence, a difficult problem in its own right.
We leavesuch issues to other work (Nenkova and McKe-own, 2003, e.g.
).Domains would rather be expected to share slots(a.k.a.
aspects), which require a more semanticlevel of analysis that can account for the variousways in which a particular slot can be expressed.Another consideration is that the structures to beanalyzed should be extracted automatically.
Basedon these criteria, we selected caseframes to be theappropriate unit of analysis.
A caseframe is a shal-low approximation of the semantic role structureof a proposition-bearing unit like a verb, and arederived from the dependency parse of a sentence1.1Note that caseframes are distinct from (though directlyRelation Caseframe Pair Sim.Degree (kill, dobj) (wound, dobj) 0.82Causal (kill, dobj) (die, nsubj) 0.80Type (rise, dobj) (drop, prep to) 0.81Figure 1: Sample pairs of similar caseframes byrelation type, and the similarity score assigned tothem by our distributional model.In particular, they are (gov, role) pairs, where govis a proposition-bearing element, and role is anapproximation of a semantic role with gov as itshead (See Figure 1 for examples).
Caseframes donot consider the dependents of the semantic roleapproximations.The use of caseframes is well grounded in a va-riety of NLP tasks relevant to summarization suchas coreference resolution (Bean and Riloff, 2004),and information extraction (Chambers and Juraf-sky, 2011), where they serve the central unit of se-mantic analysis.
Related semantic representationsare popular in Case Grammar and its derivativeformalisms such as frame semantics (Fillmore,1982).We use the following algorithm to extract case-frames from dependency parses.
First, we extractthose dependency edges with a relation type ofsubject, direct object, indirect object, or prepo-sitional object (with the preposition indicated),along with their governors.
The governor must bea verb, event noun (as defined by the hyponymsof the WordNet EVENT synset), or nominal or ad-jectival predicate.
Then, a series of deterministictransformations are applied to the syntactic rela-tions to account for voicing alternations, control,raising, and copular constructions.3.1 Caseframe SimilarityDirect caseframe matches account for some vari-ation in the expression of slots, such as voicingalternations, but there are other reasons differentcaseframes may indicate the same slot (Figure 1).For example, (kill, dobj) and (wound, dobj) bothindicate the victim of an attack, but differ bythe degree of injury to the victim.
(kill, dobj)and (die, nsubj) also refer to a victim, but arelinked by a causal relation.
(rise, dobj) andinspired by) the similarly named case frames of Case Gram-mar (Fillmore, 1968).1235(drop, prep to) on the other hand simply share anamed entity type (in this case, numbers).
To ac-count for these issues, we measure caseframe sim-ilarity based on their distributional similarity in alarge training corpus.First, we construct vector representations ofeach caseframe, where the dimensions of the vec-tor correspond to the lemma of the head word thatfills the caseframe in the training corpus.
For ex-ample, kicked the ball would result in a count of1 added to the caseframe (kick, dobj) for the con-text word ball.
Then, we rescale the counts intopointwise mutual information values, which hasbeen shown to be more effective than raw countsat detecting semantic relatedness (Turney, 2001).Similarity between caseframes can then be com-pared by cosine similarity between the their vectorrepresentations.For training, we use the AFP portion of theGigaword corpus (Graff et al, 2005), which weparsed using the Stanford parser?s typed depen-dency tree representation with collapsed conjunc-tions (de Marneffe et al, 2006).
For reasons ofsparsity, we only considered caseframes that ap-pear at least five times in the guided summariza-tion corpus, and only the 3000 most common lem-mata in Gigaword as context words.3.2 An ExampleTo illustrate how caseframes indicate the slots in asummary, we provide the following fragment of amodel summary from TAC about the Unabombertrial:(1) In Sacramento, Theodore Kaczynski faces a10-count federal indictment for 4 of the 16mail bomb attacks attributed to theUnabomber in which two people were killed.If found guilty, he faces a death penalty.
...He has pleaded innocent to all charges.
U.S.District Judge Garland Burrell Jr. presidesin Sacramento.All of the slots provided by TAC for the Inves-tigations and Trials domain can be identified byone or more caseframes.
The DEFENDANT can beidentified by (face, nsubj), and (plead, nsubj);the CHARGES by (face, dobj); the REASONby (indictment, prep for); the SENTENCE by(face, dobj); the PLEAD by (plead, dobj); andthe INVESTIGATOR by (preside, nsubj).4 ExperimentsWe conducted our experiments on the data and re-sults of the TAC 2010 summarization workshop.This data set contains 920 newspaper articles in46 topics of 20 documents each.
Ten are used inan initial guided summarization task, and ten areused in an update summarization task, in whicha summary must be produced assuming that theoriginal ten documents had already been read.
Allsummaries have a word length limit of 100 words.We analyzed the results of the two summarizationtasks separately in our experiments.The 46 topics belong to five different cate-gories or domains: Accidents and natural dis-asters, Criminal or terrorist attacks, Health andsafety, Endangered resources, and Investigationsand trials.
Each domain is associated with a tem-plate specifying the type of information that is ex-pected in the domain, such as the participants inthe event or the time that the event occurred.In our study, we compared the characteristics ofsummaries generated by the eight human summa-rizers with those generated by the peer summaries,which are basically extractive systems.
Thereare 43 peer summarization systems, including twobaselines defined by NIST.
We refer to systemsby their ID given by NIST, which are alphabeticalfor the human summarizers (A to H), and numericfor the peer summarizers (1 to 43).
We removedtwo peer systems (systems 29 and 43) which didnot generate any summary text in the workshop,presumably due to software problems.
For eachmeasure that we consider, we compare the averageamong the human-written summaries to the threeindividual peer systems, which we chose in orderto provide a representative sample of the averageand best performance of the automatic systems ac-cording to current evaluation methods.
These sys-tems are all primarily extractive, like most of thesystems in the workshop:Peer average The average of the measureamong the 41 peer summarizers.Peer 16 This system scored the highest in re-sponsiveness scores on the original summarizationtask and in ROUGE-2, responsiveness, and Pyra-mid score in the update task.Peer 22 This system scored the highest inROUGE-2 and Pyramid score in the original sum-marization task.1236142825121015313618428143037331319241621404132738173523393422762026113259G2FBEADCHSystem IDs0.00.51.01.52.0Numberofsentences(a) Initial guided summarization task281311842410153624251213163034273981437334071139381935262317222163241520FG2ECBAHDSystem IDs0.00.20.40.60.81.01.21.41.61.8Numberofsentences(b) Update summarization taskFigure 2: Average sentence cover size: the average number of sentences needed to generate the case-frames in a summary sentence (Study 1).
Model summaries are shown in darker bars.
Peer systemnumbers that we focus on are in bold.Condition Initial UpdateModel average 1.58 1.57Peer average 1.06 1.06Peer 1 1.00 1.00Peer 16 1.04 1.04Peer 22 1.08 1.09Table 2: The average number of source text sen-tences needed to cover a summary sentence.
Themodel average is statistically significantly differ-ent from all the other conditions p < 10?7(Study 1).Peer 1 The NIST-defined baseline, which is theleading sentence baseline from the most recentdocument in the source text cluster.
This systemscored the highest on linguistic quality in bothtasks.4.1 Study 1: Sentence aggregationWe first confirm that human summarizers are moreprone to sentence aggregation than system sum-marizers, showing that abstraction is indeed a de-sirable goal.
To do so, we propose a measure toquantify the degree of sentence aggregation exhib-ited by a summarizer, which we call average sen-tence cover size.
This is defined to be the min-imum number of sentences from the source textneeded to cover all of the caseframes found in asummary sentence (for those caseframes that canbe found in the source text at all), averaged over allof the summary sentences.
Purely extractive sys-tems would thus be expected to score 1.0, as wouldsystems that perform text compression by remov-ing constituents of a source text sentence.
Humansummarizers would be expected to score higher, ifthey actually aggregate information from multiplepoints in the source text.To illustrate, suppose we assign arbitrary in-dices to caseframes, a summary sentence con-tains caseframes {1, 2, 3, 4, 5}, and the sourcetext contains three sentences with caseframes,which can be represented as a nested set{{1, 3, 4}, {2, 5, 6}, {1, 4, 7}}.
Then, the sum-mary sentence can be covered by two sentencesfrom the source text, namely {{1, 3, 4}, {2, 5, 6}}.This problem is actually an instance of the min-imum set cover problem, in which sentences aresets, and caseframes are set elements.
Minimumset cover is NP-hard in general, but the standardinteger programming formulation of set cover suf-ficed for our data set; we used ILOG CPLEX12.4?s mixed integer programming mode to solveall the set cover problems optimally.Results Figure 2 shows the ranking of the sum-marizers by this measure.
Most peer systems havea low average sentence cover size of close to 1,which reflects the fact that they are purely or al-most purely extractive.
Human model summariz-ers show a higher degree of aggregation in theirsummaries.
The averages of the tested condi-tions are shown in Table 2, and are statisticallysignificant.
Peer 2 shows a relatively high levelof aggregation despite being an extractive system.Upon inspection of its summaries, it appears thatPeer 2 tends to select many datelines, and withoutpunctuation to separate them from the rest of thesummary, our automatic analysis tools incorrectlymerged many sentences together, resulting in in-correct parses and novel caseframes not found in1237A32B1242273733G15287392EFH352615CD11209361419401316830461031841213424172531222338System IDs0.000.020.040.060.080.100.12Perworddensity(a) Initial guided summarization taskEAGB37133C122726423911H28F152D322035540741081914303641183921243413222516311762338System IDs0.000.020.040.060.080.10Perworddensity(b) Update summarization taskFigure 3: Density of signature caseframes (Study 2).Topic: Unabomber trial(charge, dobj), (kill, dobj),(trial, prep of), (bombing, prep in)Topic: Mangrove forests(beach, prep of), (save, dobj)(development, prep of), (recover, nsubj)Topic: Bird Flu(infect, prep with), (die, nsubj)(contact, dobj), (import, prep from)Figure 4: Examples of signature caseframes foundin Study 2.the source text.4.2 Study 2: Signature caseframe densityStudy 1 shows that human summarizers are moreabstractive in that they aggregate information frommultiple sentences in the source text, but how isthis aggregation performed?
One possibility isthat human summary writers are able to pack agreater number of salient caseframes into theirsummaries.
That is, humans are fundamentally re-lying on centrality just as automatic summarizersdo, and are simply able to achieve higher compres-sion ratios by being more succinct.
If this is true,then sentence fusion methods over the source textalone might be able to solve the problem.
Unfor-tunately, we show that this is false and that systemsummaries are actually more central than modelones.To extract topical caseframes, we use Lin andHovy?s (2000) method of calculating signatureterms, but extend the method to apply it at thecaseframe rather than the word level.
We fol-low Lin and Hovy (2000) in using a significanceCondition Initial UpdateModel average 0.065 0.052Peer average 0.080?
0.072?Peer 1 0.066 0.050Peer 16 0.083?
0.085?Peer 22 0.101?
0.084?Table 3: Signature caseframe densities for differ-ent sets of summarizers, for the initial and updateguided summarization tasks (Study 2).
?
: p <0.005.threshold of 0.001 to determine signature case-frames2.
Figure 4 shows examples of signaturecaseframes for several topics.
Then, we calculatethe signature caseframe density of each of thesummarization systems.
This is defined to be thenumber of signature caseframes in the set of sum-maries divided by the number of words in that setof summaries.Results Figure 3 shows the density for all of thesummarizers, in ascending order of density.
Ascan be seen, the human abstractors actually tend touse fewer signature caseframes in their summariesthan automatic systems.
Only the leading baselineis indistinguishable from the model average.
Ta-ble 3 shows the densities for the conditions thatwe described earlier.
The differences in densitybetween the human average and the non-baselineconditions are highly statistically significant, ac-cording to paired two-tailed Wilcoxon signed-ranktests for the statistic calculated for each topic clus-ter.These results show that human abstractors do2We tried various other thresholds, but the results weremuch the same.1238Threshold 0.9 0.8Condition Init.
Up.
Init.
Up.Model average 0.066 0.052 0.062 0.047Peer average 0.080 0.071 0.071 0.063Peer 1 0.068 0.050 0.060 0.044Peer 16 0.083 0.086 0.072 0.077Peer 22 0.100 0.086 0.084 0.075Table 4: Density of signature caseframes aftermerging to various threshold for the initial (Init.
)and update (Up.)
summarization tasks (Study 2).not merely repeat the caseframes that are indica-tive of a topic cluster or use minor grammaticalalternations in their summaries.
Rather, a genuinesort of abstraction or distillation has taken place,either through paraphrasing or semantic inference,to transform the source text into the final informa-tive summary.Merging Caseframes We next investigatewhether simple paraphrasing could account forthe above results; it may be the case that humansummarizers simply replace words in the sourcetext with synonyms, which can be detected withdistributional similarity.
Thus, we merged similarcaseframes into clusters according to the distribu-tional semantic similarity defined in Section 3.1,and then repeated the previous experiment.
Wechose two relatively high levels of similarity (0.8and 0.9), and used complete-link agglomerative(i.e., bottom-up) clustering to merge similarcaseframes.
That is, each caseframe begins as aseparate cluster, and the two most similar clustersare merged at each step until the desired similaritythreshold is reached.
Cluster similarity is definedto be the minimum similarity (or equivalently,maximum distance) between elements in thetwo clusters; that is, maxc?C1,c?
?C2 ?sim(c, c?
).Complete-link agglomerative clustering tends toform coherent clusters where the similarity be-tween any pair within a cluster is high (Manninget al, 2008).Cluster Results Table 4 shows the results afterthe clustering step, with similarity thresholds of0.9 and 0.8.
Once again, model summaries containa lower density of signature caseframes.
The sta-tistical significance results are unchanged.
This in-dicates that simple paraphrasing alone cannot ac-count for the difference in the signature caseframedensities, and that some deeper abstraction or se-mantic inference has occurred.Note that we are not claiming that a lower den-sity of signature caseframes necessarily correlateswith a more informative summary.
For example,some automatic summarizers are comparable tothe human abstractors in their relatively low den-sity of signature caseframes, but these turn out tobe the lowest performing summarization systemsby all measures in the workshop, and they are un-likely to rival human abstractors in any reasonableevaluation of summary informativeness.
It does,however, appear that further optimizing centrality-based measures alone is unlikely to produce bet-ter informative summaries, even if we analyze thesummary at a syntactic/semantic rather than lexi-cal level.4.3 Study 3: Summary ReconstructionThe above studies show that the higher degreeof abstraction in model summaries cannot be ex-plained by better compression of topically salientcaseframes alone.
We now switch perspectives toask how model summaries might be automaticallygenerated at all.
We will show that they cannotbe reconstructed solely from the source text, ex-tending Copeck and Szpakowicz (2004)?s result tocaseframes.
However, we also show that if articlesfrom the same domain are added, reconstructionthen becomes possible.
Our measure of whethera model summary can be reconstructed is case-frame coverage.
We define this to be the propor-tion of caseframes in a summary that is containedby some reference set.
This is thus a score be-tween 0 and 1.
Unlike in the previous study, weuse the full set of caseframes, not just signaturecaseframes, because our goal now to create a hy-pothesis space from which it is in principle possi-ble to generate the model summaries.Results We first calculated caseframe coveragewith respect to the source text alone (Figure 5).As expected, automatic systems show close to per-fect coverage, because of their basically extractivenature, while model summaries show much lowercoverage.
These statistics are summarized by Ta-ble 5.
These results present a fundamental limitto extractive systems, and also text simplificationand sentence fusion methods based solely on thesource text.The Impact of Domain Knowledge How mightautomatic summarizers be able to acquire these1239AGEBHFCD3817232206394059341423351973341121137264221273242810481316313025221151836System IDs0.00.20.40.60.81.0Coverage(a) Initial guided summarization taskGABEHCFD2381732114139203519262152314374027421225463378302231102413341528139161836System IDs0.00.20.40.60.81.0Coverage(b) Update summarization taskFigure 5: Coverage of summary text caseframes in source text (Study 3).Condition Initial UpdateModel average 0.77 0.75Peer average 0.99 0.99Peer 1 1.00 1.00Peer 16 1.00 1.00Peer 22 1.00 1.00Table 5: Coverage of caseframes in summarieswith respect to the source text.
The model aver-age is statistically significantly different from allthe other conditions p < 10?8 (Study 3).caseframes from other sources?
Traditional sys-tems that perform semantic inference do so from aset of known facts about the domain in the form ofa knowledge base, but as we have seen, most ex-tractive summarization systems do not make muchuse of in-domain corpora.
We examine addingin-domain text to the source text to see how thiswould affect coverage.Recall that the 46 topics in TAC 2010 are cat-egorized into five domains.
To calculate the im-pact of domain knowledge, we add all the docu-ments that belong in the same domain to the sourcetext to calculate coverage.
To ensure that coveragedoes not increase simply due to increasing the sizeof the reference set, we compare to the baseline ofadding the same number of documents that belongto another domain.
As shown in Table 6, the ef-fect of adding more in-domain text on caseframecoverage is substantial, and noticeably more thanusing out-of-domain text.
In fact, nearly all case-frames can be found in the expanded set of arti-cles.
The implication of this result is that it maybe possible to generate better summaries by min-ing in-domain text for relevant caseframes.Reference corpus Initial UpdateSource text only 0.77 0.75+out-of-domain 0.91 0.91+in-domain 0.98 0.97Table 6: The effect on caseframe coverage ofadding in-domain and out-of-domain documents.The difference between adding in-domain and out-of-domain text is significant p < 10?3 (Study 3).5 ConclusionWe have presented a series of studies to distin-guish human-written informative summaries fromthe summaries produced by current systems.
Ourstudies are performed at the level of caseframes,which are able to characterize a domain in terms ofits slots.
First, we confirm that model summariesare more abstractive and aggregate informationfrom multiple source text sentences.
Then, weshow that this is not simply due to summary writ-ers packing together source text sentences contain-ing topical caseframes to achieve a higher com-pression ratio, even if paraphrasing is taken intoaccount.
Indeed, model summaries cannot be re-constructed from the source text alone.
How-ever, our results are also positive in that we findthat nearly all model summary caseframes can befound in the source text together with some in-domain documents.Current summarization systems have beenheavily optimized towards centrality and lexical-semantical reasoning, but we are nearing the bot-tom of the barrel.
Domain inference, on the otherhand, and a greater use of in-domain documentsas a knowledge source for domain inference, arevery promising indeed.
Mining useful caseframes1240for a sentence fusion-based approach has the po-tential, as our experiments have shown, to deliverresults in just the areas where current approachesare weakest.AcknowledgementsThis work is supported by the Natural Sciencesand Engineering Research Council of Canada.ReferencesRegina Barzilay and Kathleen R. McKeown.
2005.Sentence fusion for multidocument news summa-rization.
Computational Linguistics, 31(3):297?328.David Bean and Ellen Riloff.
2004.
Unsupervisedlearning of contextual role knowledge for corefer-ence resolution.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: HLT-NAACL 2004.Jaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedingsof the 21st Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval, pages 335?336.
ACM.Nathanael Chambers and Dan Jurafsky.
2011.Template-based information extraction without thetemplates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 976?986, Portland, Oregon, USA, June.
Association forComputational Linguistics.John M. Conroy, Judith D. Schlesinger, and Dianne P.O?Leary.
2006.
Topic-focused multi-documentsummarization using an approximate oracle score.In Proceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 152?159, Syd-ney, Australia, July.
Association for ComputationalLinguistics.Terry Copeck and Stan Szpakowicz.
2004.
Vocabu-lary agreement among model summaries and sourcedocuments.
In Proceedings of the 2004 DocumentUnderstanding Conference (DUC).Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InIn LREC 2006.Charles Fillmore.
1968.
The case for case.
In E. Bachand R. T. Harms, editors, Universals in LinguisticTheory, pages 1?88.
Holt, Reinhart, and Winston,New York.Charles J. Fillmore.
1982.
Frame semantics.
Linguis-tics in the Morning Calm, pages 111?137.Pierre-Etienne Genest, Guy Lapalme, and MehdiYousfi-Monod.
2009.
Hextac: the creation of amanual extractive run.
In Proceedings of the SecondText Analysis Conference, Gaithersburg, Maryland,USA.
National Institute of Standards and Technol-ogy.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2005.
English gigaword second edition.Linguistic Data Consortium, Philadelphia.Liwei He, Elizabeth Sanocki, Anoop Gupta, andJonathan Grudin.
1999.
Auto-summarization ofaudio-video presentations.
In Proceedings of theSeventh ACM International Conference on Multime-dia.
ACM.Liwei He, Elizabeth Sanocki, Anoop Gupta, andJonathan Grudin.
2000.
Comparing presentationsummaries: slides vs. reading vs. listening.
In Pro-ceedings of the SIGCHI Conference on Human Fac-tors in Computing Systems, CHI ?00, pages 177?184, New York, NY, USA.
ACM.Eduard Hovy, Chin-Yew Lin, Liang Zhou, and JunichiFukumoto.
2006.
Automated summarization evalu-ation with Basic Elements.
In Proceedings of the 5thInternational Conference on Language Resourcesand Evaluation (LREC), pages 899?902.IBM.
IBM ILOG CPLEX Optimization Studio V12.4.Hongyan Jing and Kathleen R. McKeown.
2000.
Cutand paste based text summarization.
In Proceed-ings of the 1st North American Chapter of the As-sociation for Computational Linguistics Conference,pages 178?185.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization-step one: Sentence compres-sion.
In Proceedings of the National Conference onArtificial Intelligence.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for text sum-marization.
In Proceedings of the 18th Conferenceon Computational Linguistics - Volume 1, COLING?00, pages 495?501, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
The potentialand limitations of automatic sentence extraction forsummarization.
In Proceedings of the HLT-NAACL03 on Text Summarization Workshop.
Associationfor Computational Linguistics.Chin Y. Lin.
2004.
ROUGE: A package for automaticevaluation of summaries.
In Stan Szpakowicz andMarie-Francine Moens, editors, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.Annie Louis and Ani Nenkova.
2009.
Automaticallyevaluating content selection in summarization with-out human models.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural Language1241Processing.
Association for Computational Linguis-tics.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze, 2008.
Introduction to InformationRetrieval, chapter 17.
Cambridge University Press.Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing newson a daily basis with Columbia?s Newsblaster.
InProceedings of the Second International Conferenceon Human Language Technology Research, pages280?285.
Morgan Kaufmann Publishers Inc.Ani Nenkova and Kathleen McKeown.
2003.
Refer-ences to named entities: a corpus study.
In Com-panion Volume of the Proceedings of HLT-NAACL2003 - Short Papers.
Association for ComputationalLinguistics.Ani Nenkova and Kathleen McKeown.
2011.
Auto-matic summarization.
Foundations and Trends inInformation Retrieval, 5(2):103?233.Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: HLT-NAACL 2004, volume 2004, pages145?152.Dragomir R. Radev and Kathleen R. McKeown.
1998.Generating natural language summaries from mul-tiple on-line sources.
Computational Linguistics,24(3):470?500.Horacio Saggion and Guy Lapalme.
2002.
Generat-ing indicative-informative summaries with SumUM.Computational linguistics, 28(4):497?526.Horacio Saggion, Juan-Manuel Torres-Moreno, IriaCunha, and Eric SanJuan.
2010.
Multilingual sum-marization evaluation without human models.
InProceedings of the 23rd International Conferenceon Computational Linguistics: Posters, pages 1059?1067.
Association for Computational Linguistics.Peter Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe Twelth European Conference on Machine Learn-ing (ECML-2001), pages 491?502.Michael White, Tanya Korelsky, Claire Cardie, VincentNg, David Pierce, and Kiri Wagstaff.
2001.
Mul-tidocument summarization via information extrac-tion.
In Proceedings of the First International Con-ference on Human Language Technology Research.Association for Computational Linguistics.1242
