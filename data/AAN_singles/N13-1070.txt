Proceedings of NAACL-HLT 2013, pages 617?626,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDown-stream effects of tree-to-dependency conversionsJakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi?,Hector Martinez, Anders S?gaardCenter for Language Technology, University of Copenhagen?Institute for Informatics, University of OsloAbstractDependency analysis relies on morphosyntac-tic evidence, as well as semantic evidence.In some cases, however, morphosyntactic ev-idence seems to be in conflict with seman-tic evidence.
For this reason dependencygrammar theories, annotation guidelines andtree-to-dependency conversion schemes oftendiffer in how they analyze various syntacticconstructions.
Most experiments for whichconstituent-based treebanks such as the PennTreebank are converted into dependency tree-banks rely blindly on one of four-five widelyused tree-to-dependency conversion schemes.This paper evaluates the down-stream effect ofchoice of conversion scheme, showing that ithas dramatic impact on end results.1 IntroductionAnnotation guidelines used in modern depen-dency treebanks and tree-to-dependency conversionschemes for converting constituent-based treebanksinto dependency treebanks are typically based ona specific dependency grammar theory, such as thePrague School?s Functional Generative Description,Meaning-Text Theory, or Hudson?s Word Grammar.In practice most parsers constrain dependency struc-tures to be tree-like structures such that each wordhas a single syntactic head, limiting diversity be-tween annotation a bit; but while many dependencytreebanks taking this format agree on how to an-alyze many syntactic constructions, there are stillmany constructions these treebanks analyze differ-ently.
See Figure 1 for a standard overview of clearand more difficult cases.The difficult cases in Figure 1 are difficult forthe following reason.
In the easy cases morphosyn-tactic and semantic evidence cohere.
Verbs gov-ern subjects morpho-syntactically and seem seman-tically more important.
In the difficult cases, how-ever, morpho-syntactic evidence is in conflict withthe semantic evidence.
While auxiliary verbs havethe same distribution as finite verbs in head positionand share morpho-syntactic properties with them,and govern the infinite main verbs, main verbs seemsemantically superior, expressing the main predi-cate.
There may be distributional evidence that com-plementizers head verbs syntactically, but the verbsseem more important from a semantic point of view.Tree-to-dependency conversion schemes usedto convert constituent-based treebanks intodependency-based ones also take different stands onthe difficult cases.
In this paper we consider four dif-ferent conversion schemes: the Yamada-Matsumotoconversion scheme yamada,1 the CoNLL 2007format conll07,2 the conversion scheme ewt used inthe English Web Treebank (Petrov and McDonald,2012),3 and the lth conversion scheme (Johansson1The Yamada-Matsumoto scheme can bereplicated by running penn2malt.jar available athttp://w3.msi.vxu.se/?nivre/research/Penn2Malt.html.
Weused Malt dependency labels (see website).
The Yamada-Matsumoto scheme is an elaboration of the Collins scheme(Collins, 1999), which is not included in our experiments.2The CoNLL 2007 conversion scheme can beobtained by running pennconverter.jar available athttp://nlp.cs.lth.se/software/treebank converter/with the?conll07?
flag set.3The EWT conversion scheme can be repli-cated using the Stanford converter available athttp://nlp.stanford.edu/software/stanford-dependencies.shtml617Clear cases Difficult casesHead Dependent ?
?Verb Subject Auxiliary Main verbVerb Object Complementizer VerbNoun Attribute Coordinator ConjunctsVerb Adverbial Preposition NominalPunctuationFigure 1: Clear and difficult cases in dependency annotation.and Nugues, 2007).4 We list the differences inFigure 2.
An example of differences in analysis ispresented in Figure 3.In order to access the impact of these conversionschemes on down-stream performance, we need ex-trinsic rather than intrinsic evaluation.
In generalit is important to remember that while researchersdeveloping learning algorithms for part-of-speech(POS) tagging and dependency parsing seem ob-sessed with accuracies, POS sequences or depen-dency structures have no interest on their own.
Theaccuracies reported in the literature are only inter-esting insofar they correlate with the usefulness ofthe structures predicted by our systems.
Fortunately,POS sequences and dependency structures are use-ful in many applications.
When we consider tree-to-dependency conversion schemes, down-stream eval-uation becomes particularly important since someschemes are more fine-grained than others, leadingto lower performance as measured by intrinsic eval-uation metrics.Approach in this workIn our experiments below we apply a state-of-the-artparser to five different natural language processing(NLP) tasks where syntactic features are known tobe effective: negation resolution, semantic role la-beling (SRL), statistical machine translation (SMT),sentence compression and perspective classification.In all five tasks we use the four tree-to-dependencyconversion schemes mentioned above and evaluatethem in terms of down-stream performance.
We alsocompare our systems to baseline systems not rely-4The LTH conversion scheme can be ob-tained by running pennconverter.jar available athttp://nlp.cs.lth.se/software/treebank converter/ with the?oldLTH?
flag set.ing on syntactic features, when possible, and to re-sults in the literature, when comparable results exist.Note that negation resolution and SRL are not endapplications.
It is not easy to generalize across fivevery different tasks, but the tasks will serve to showthat the choice of conversion scheme has significantimpact on down-stream performance.We used the most recent release of the Mate parserfirst described in Bohnet (2010),5 trained on Sec-tions 2?21 of the Wall Street Journal section of theEnglish Treebank (Marcus et al 1993).
The graph-based parser is similar to, except much faster, andperforms slightly better than the MSTParser (Mc-Donald et al 2005), which is known to performwell on long-distance dependencies often importantfor down-stream applications (McDonald and Nivre,2007; Galley and Manning, 2009; Bender et al2011).
This choice may of course have an effect onwhat conversion schemes seem superior (Johanssonand Nugues, 2007).
Sentence splitting was done us-ing splitta,6, and the sentences were then tokenizedusing PTB-style tokenization7 and tagged using thein-built Mate POS tagger.Previous workThere has been considerable work on down-streamevaluation of syntactic parsers in the literature, butmost previous work has focused on evaluating pars-ing models rather than linguistic theories.
No onehas, to the best of our knowledge, compared theimpact of choice of tree-to-dependency conversionscheme across several NLP tasks.Johansson and Nugues (2007) compare the im-pact of yamada and lth on semantic role labeling5http://code.google.com/p/mate-tools/6http://code.google.com/p/splitta/7http://www.cis.upenn.edu/?treebank/tokenizer.sed618FORM1 FORM2 yamada conll07 ewt lthAuxiliary Main verb 1 1 2 2Complementizer Verb 1 2 2 2Coordinator Conjuncts 2 1 2 2Preposition Nominal 1 1 1 2Figure 2: Head decisions in conversions.
Note: yamada also differ from CoNLL 2007 in proper names.Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.performance, showing that lth leads to superior per-formance.Miyao et al(2008) measure the impact of syntac-tic parsers in an information extraction system iden-tifying protein-protein interactions in biomedical re-search articles.
They evaluate dependency parsers,constituent-based parsers and deep parsers.Miwa et al(2010) evaluate down-stream per-formance of linguistic representations and parsingmodels in biomedical event extraction, but do notevaluate linguistic representations directly, evaluat-ing representations and models jointly.Bender et al(2011) compare several parsersacross linguistic representations on a carefully de-signed evaluation set of hard, but relatively frequentsyntactic constructions.
They compare dependencyparsers, constituent-based parsers and deep parsers.The authors argue in favor of evaluating parsers ondiverse and richly annotated data.
Others have dis-cussed various ways of evaluating across annotationguidelines or translating structures to a common for-mat (Schwartz et al 2011; Tsarfaty et al 2012).Hall et al(2011) discuss optimizing parsers forspecific down-stream applications, but consider onlya single annotation scheme.Yuret et al(2012) present an overview of theSemEval-2010 Evaluation Exercises on SemanticEvaluation track on recognition textual entailmentusing dependency parsing.
They also compare sev-eral parsers using the heuristics of the winning sys-tem for inference.
While the shared task is anexample of down-stream evaluation of dependencyparsers, the evaluation examples only cover a subsetof the textual entailments relevant for practical ap-plications, and the heuristics used in the experimentsassume a fixed set of dependency labels (ewt labels).Finally, Schwartz et al(2012) compare theabove conversion schemes and several combinationsthereof in terms of learnability.
This is very differentfrom what is done here.
While learnability may bea theoretically motivated parameter, our results indi-cate that learnability and downstream performancedo not correlate well.2 ApplicationsDependency parsing has proven useful for a widerange of NLP applications, including statistical ma-chine translation (Galley and Manning, 2009; Xu etal., 2009; Elming and Haulrich, 2011) and sentimentanalysis (Joshi and Penstein-Rose, 2009; Johanssonand Moschitti, 2010).
This section describes the ap-plications and experimental set-ups included in thisstudy.In the five applications considered below we619use syntactic features in slightly different ways.While our statistical machine translation and sen-tence compression systems use dependency rela-tions as additional information about words and ona par with POS, our negation resolution system usesdependency paths, conditioning decisions on bothdependency arcs and labels.
In perspective classifi-cation, we use dependency triples (e.g.
SUBJ(John,snore)) as features, while the semantic role labelingsystem conditions on a lot of information, includingthe word form of the head, the dependent and the ar-gument candidates, the concatenation of the depen-dency labels of the predicate, and the labeled depen-dency relations between predicate and its head, itsarguments, dependents or siblings.2.1 Negation resolutionNegation resolution (NR) is the task of finding nega-tion cues, e.g.
the word not, and determining theirscope, i.e.
the tokens they affect.
NR has recentlyseen considerable interest in the NLP community(Morante and Sporleder, 2012; Velldal et al 2012)and was the topic of the 2012 *SEM shared task(Morante and Blanco, 2012).The data set used in this work, the Conan Doylecorpus (CD),8 was released in conjunction with the*SEM shared task.
The annotations in CD extendon cues and scopes by introducing annotations forin-scope events that are negated in factual contexts.The following is an example from the corpus show-ing the annotations for cues (bold), scopes (under-lined) and negated events (italicized):(1) Since we have been sounfortunate as to miss him [.
.
.
]CD-style scopes can be discontinuous and overlap-ping.
Events are a portion of the scope that is se-mantically negated, with its truth value reversed bythe negation cue.The NR system used in this work (Lapponi et al2012), one of the best performing systems in the*SEM shared task, is a CRF model for scope resolu-tion that relies heavily on features extracted from de-pendency graphs.
The feature model contains tokendistance, direction, n-grams of word forms, lemmas,POS and combinations thereof, as well as the syntac-tic features presented in Figure 4.
The results in our8http://www.clips.ua.ac.be/sem2012-st-neg/data.htmlSyntacticconstituentdependency relationparent head POSgrand parent head POSword form+dependency relationPOS+dependency relationCue-dependentdirected dependency distancebidirectional dependency distancedependency pathlexicalized dependency pathFigure 4: Features used to train the conditional randomfield modelsexperiments are obtained from configurations thatdiffer only in terms of tree-to-dependency conver-sions, and are trained on the training set and testedon the development set of CD.
Since the negationcue classification component of the system does notrely on dependency features at all, the models aretested using gold cues.Table 1 shows F1 scores for scopes, events andfull negations, where a true positive correctly as-signs both scope tokens and events to the rightfulcue.
The scores are produced using the evaluationscript provided by the *SEM organizers.2.2 Semantic role labelingSemantic role labeling (SRL) is the attempt to de-termine semantic predicates in running text and la-bel their arguments with semantic roles.
In ourexperiments we have reproduced the second best-performing system in the CoNLL 2008 shared taskin syntactic and semantic parsing (Johansson andNugues, 2008).9The English training data for the CoNLL 2008shared task were obtained from PropBank andNomBank.
For licensing reasons, we usedOntoNotes 4.0, which includes PropBank, but notNomBank.
This means that our system is onlytrained to classify verbal predicates.
We usedthe Clearparser conversion tool10 to convert theOntoNotes 4.0 and subsequently supplied syntac-tic dependency trees using our different conversionschemes.
We rely on gold standard argument identi-fication and focus solely on the performance metricsemantic labeled F1.9http://nlp.cs.lth.se/software/semantic parsing: propbanknombank frames10http://code.google.com/p/clearparser/6202.3 Statistical machine translationThe effect of the different conversion schemes wasalso evaluated on SMT.
We used the reorderingby parsing framework described by Elming andHaulrich (2011).
This approach integrates a syn-tactically informed reordering model into a phrase-based SMT system.
The model learns to predict theword order of the translation based on source sen-tence information such as syntactic dependency re-lations.
Syntax-informed SMT is known to be use-ful for translating between languages with differentword orders (Galley and Manning, 2009; Xu et al2009), e.g.
English and German.The baseline SMT system is created as describedin the guidelines from the original shared task.11Only modifications are that we use truecasing in-stead of lowercasing and recasing, and allow train-ing sentences of up to 80 words.
We used datafrom the English-German restricted task: ?3M par-allel words of news, ?46M parallel words of Eu-roparl, and ?309M words of monolingual Europarland news.
We use newstest2008 for tuning, new-stest2009 for development, and newstest2010 fortesting.
Distortion limit was set to 10, which isalso where the baseline system performed best.
Thephrase table and the lexical reordering model istrained on the union of all parallel data with a maxphrase length of 7, and the 5-gram language modelis trained on the entire monolingual data set.We test four different experimental systems thatonly differ with the baseline in the addition of a syn-tactically informed reordering model.
The baselinesystem was one of the tied best performing systemin the WMT 2011 shared task on this dataset.
Thefour experimental systems have reordering modelsthat are trained on the first 25,000 sentences of theparallel news data that have been parsed with eachof the tree-to-dependency conversion schemes.
Thereordering models condition reordering on the wordforms, POS, and syntactic dependency relations ofthe words to be reordered, as described in Elmingand Haulrich (2011).
The paper shows that whilereordering by parsing leads to significant improve-ments in standard metrics such as BLEU (Papineniet al 2002) and METEOR (Lavie and Agarwal,2007), improvements are more spelled out with hu-11 http://www.statmt.org/wmt11/translation-task.htmlman judgements.
All SMT results reported beloware averages based on 5 MERT runs following Clarket al(2011).2.4 Sentence compressionSentence compression is a restricted form of sen-tence simplification with numerous usages, includ-ing text simplification, summarization and recogniz-ing textual entailment.
The most commonly useddataset in the literature is the Ziff-Davis corpus.12 Awidely used baseline for sentence compression ex-periments is Knight and Marcu (2002), who intro-duce two models: the noisy-channel model and a de-cision tree-based model.
Both are tree-based meth-ods that find the most likely compressed syntactictree and outputs the yield of this tree.
McDonald etal.
(2006) instead use syntactic features to directlyfind the most likely compressed sentence.Here we learn a discriminative HMM model(Collins, 2002) of sentence compression usingMIRA (Crammer and Singer, 2003), comparable topreviously explored models of noun phrase chunk-ing.
Our model is thus neither tree-based norsentence-based.
Instead we think of sentence com-pression as a sequence labeling problem.
We com-pare a model informed by word forms and predictedPOS with models also informed by predicted depen-dency labels.
The baseline feature model conditionsemission probabilities on word forms and POS us-ing a ?2 window and combinations thereoff.
Theaugmented syntactic feature model simply adds de-pendency labels within the same window.2.5 Perspective classificationFinally, we include a document classification datasetfrom Lin and Hauptmann (2006).13 The dataset con-sists of blog posts posted at bitterlemons.org by Is-raelis and Palestinians.
The bitterlemons.org web-site is set up to ?contribute to mutual understandingthrough the open exchange of ideas.?
In the dataset,each blog post is labeled as either Israeli or Pales-tinian.
Our baseline model is just a standard bag-of-words model, and the system adds dependencytriplets to the bag-of-words model in a way similarto Joshi and Penstein-Rose (2009).
We do not re-move stop words, since perspective classification is12LDC Catalog No.
: LDC93T3A.13https://sites.google.com/site/weihaolinatcmu/data621bl yamada conll07 ewt lthDEPRELS - 12 21 47 41PTB-23 (LAS) - 88.99 88.52 81.36?
87.52PTB-23 (UAS) - 90.21 90.12 84.22?
90.29Neg: scope F1 - 81.27 80.43 78.70 79.57Neg: event F1 - 76.19 72.90 73.15 76.24Neg: full negation F1 - 67.94 63.24 61.60 64.31SentComp F1 68.47 72.07 64.29 71.56 71.56SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11SRL-22-gold - 81.35 83.22 84.72 84.01SRL-23-gold - 79.09 80.85 80.39 82.01SRL-22-pred - 74.41 76.22 78.29 66.32SRL-23-pred - 73.42 74.34 75.80 64.06bitterlemons.org 96.08 97.06 95.58 96.08 96.57Table 1: Results.
?
: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and theOntonotes 4.0 release of the English Treebank.similar to authorship attribution, where stop wordsare known to be informative.
We evaluate perfor-mance doing cross-validation over the official train-ing data, setting the parameters of our learning algo-rithm for each fold doing cross-validation over theactual training data.
We used soft-margin supportvector machine learning (Cortes and Vapnik, 1995),tuning the kernel (linear or polynomial with degree3) and C = {0.1, 1, 5, 10}.3 Results and discussionOur results are presented in Table 1.
The parsingresults are obtained relying on predicted POS ratherthan, as often done in the dependency parsing liter-ature, relying on gold-standard POS.
Note that theycomply with the result in Schwartz et al(2012) thatYamada-Matsumoto-style annotation is more easilylearnable.The negation resolution results are significantlybetter using syntactic features in yamada annota-tion.
It is not surprising that a syntactically ori-ented conversion scheme performs well in this task.Since Lapponi et al(2012) used Maltparser (Nivreet al 2007) with the freely available pre-trainedparsing model for English,14 we decided to alsorun that parser with the gold-standard cues, in ad-14http://www.maltparser.org/mco/english parser/engmalt.htmldition to Mate.
The pre-trained model was trainedon Sections 2?21 of the Wall Street Journal sec-tion of the English Treebank (Marcus et al 1993),augmented with 4000 sentences from the Question-Bank,15 which was converted using the Stanfordconverter and thus similar to the ewt annotationsused here.
The results were better than using ewtwith Mate trained on Sections 2?21 alone, but worsethan the results obtained here with yamada conver-sion scheme.
F1 score on full negation was 66.92%.The case-sensitive BLEU evaluation of theSMT systems indicates that choice of conversionscheme has no significant impact on overall perfor-mance.
The difference to the baseline system issignificant (p < 0.01), showing that the reorder-ing model leads to improvement using any of theschemes.
However, the conversion schemes lead tovery different translations.
This can be seen, forexample, by the fact that the relative tree edit dis-tance between translations of different syntacticallyinformed SMT systems is 12% higher than withineach system (across different MERT optimizations).The reordering approach puts a lot of weight onthe syntactic dependency relations.
As a conse-quence, the number of relation types used in theconversion schemes proves important.
Consider the15http://www.computing.dcu.ie/?jjudge/qtreebank/622REFERENCE: Zum Glu?ck kam ich beim Strassenbahnfahren an die richtige Stelle .SOURCE: Luckily , on the way to the tram , I found the right place .yamada: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .conll07: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .ewt: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .lth: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .BASELINE: Zum Glu?ck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .Figure 5: Examples of SMT output.ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answeringmachine and voice-message handler that links a macintosh to touch-tone phones .BASELINE: 68000 sweden ab introduced the teleserve an integrated answeringmachine and voice-message handler .yamada 68000 sweden ab introduced the teleserve integrated answeringmachine and voice-message handler .conll07 68000 sweden ab sweden introduced the teleserve integrated answeringmachine and voice-message handler .ewt 68000 sweden ab introduced the teleserve integrated answeringmachine and voice-message handler .lth 68000 sweden ab introduced the teleserve an integrated answeringmachine and voice-message handler .HUMAN: 68000 sweden ab introduced the teleserve integrated answeringmachine and voice-message handler .Figure 6: Examples of sentence compression output.example in Figure 5.
German requires the verb insecond position, which is obeyed in the much bet-ter translations produced by the ewt and lth sys-tems.
Interestingly, the four schemes produce virtu-ally identical structures for the source sentence, butthey differ in their labeling.
Where conll07 and ya-mada use the same relation for the first two con-stituents (ADV and vMOD, respectively), ewt andlth distinguish between them (ADVMOD/PREP andADV/LOC).
This distinction may be what enablesthe better translation, since the model may learn tomove the verb after the sentence adverbial.
In theother schemes, sentence adverbials are not distin-guished from locational adverbials.
Generally, ewtand lth have more than twice as many relation typesas the other schemes.The schemes ewt and lth lead to better SRLperformance than conll07 and yamada when re-lying on gold-standard syntactic dependency trees.This supports the claims put forward in Johanssonand Nugues (2007).
These annotations also hap-pen to use a larger set of dependency labels, how-ever, and syntactic structures may be harder to re-construct, as reflected by labeled attachment scores(LAS) in syntactic parsing.
The biggest drop inSRL performance going from gold-standard to pre-dicted syntactic trees is clearly for the lth scheme,at an average 17.8% absolute loss (yamada 5.8%;conll07 6.8%; ewt 5.5%; lth 17.8%).The ewt scheme resembles lth in most respects,but in preposition-noun dependencies it marks thepreposition as the head rather than the noun.
Thisis an important difference for SRL, because seman-tic arguments are often nouns embedded in preposi-tional phrases, like agents in passive constructions.It may also be that the difference in performance issimply explained by the syntactic analysis of prepo-sitional phrases being easier to reconstruct.The sentence compression results are generallymuch better than the models proposed in Knight andMarcu (2002).
Their noisy channel model obtainsan F1 compression score of 14.58%, whereas thedecision tree-based model obtains an F1 compres-sion score of 31.71%.
While F1 scores should becomplemented by human judgements, as there aretypically many good sentence compressions of anysource sentence, we believe that error reductions ofmore than 50% indicate that the models used here623ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD0.000.050.100.150.200.250.300.350.40LabelssrlnegFigure 7: Distributions of dependency labels in theYamada-Matsumoto scheme(though previously unexplored in the literature) arefully competitive with state-of-the-art models.We also see that the models using syntactic fea-tures perform better than our baseline model, exceptfor the model using conll07 dependency annotation.This may be surprising to some, since distributionalinformation is often considered important in sen-tence compression (Knight and Marcu, 2002).
Someoutput examples are presented in Figure 6.
Un-surprisingly, it is seen that the baseline model pro-duces grammatically incorrect output, and that mostof our syntactic models correct the error leading toungrammaticality.
The model using ewt annotationis an exception.
We also see that conll07 introducesanother error.
We believe that this is due to the waythe conll07 tree-to-dependency conversion schemehandles coordination.
While the word Sweden is notcoordinated, it occurs in a context, surrounded bycommas, that is very similar to coordinated items.In perspective classification we see that syntacticfeatures based on yamada and lth annotations leadto improvements, with yamada leading to slightlybetter results than lth.
The fact that a syntacticallyoriented conversion scheme leads to the best resultsmay reflect that perspective classification, like au-thorship attribution, is less about content than stylis-tics.While lth seems to lead to the overall best re-sults, we stress the fact that the five tasks consideredhere are incommensurable.
What is more interest-ing is that, task to task, results are so different.
Thesemantically oriented conversion schemes, ewt andlth, lead to the best results in SRL, but with a signif-icant drop for lth when relying on predicted parses,while the yamada scheme is competitive in the otherfour tasks.
This may be because distributional infor-mation is more important in these tasks than in SRL.The distribution of dependency labels seems rel-atively stable across applications, but differences indata may of course also affect the usefulness of dif-ferent annotations.
Note that conll07 leads to verygood results for negation resolution, but bad resultsfor SRL.
See Figure 7 for the distribution of labelsin the conll07 conversion scheme on the SRL andnegation scope resolution data.
Many differencesrelate to differences in sentence length.
The nega-tion resolution data is literary text with shorter sen-tences, which therefore uses more punctuation andhas more root dependencies than newspaper articles.On the other hand we do see very few predicate de-pendencies in the SRL data.
This may affect down-stream results when classifying verbal predicates inSRL.
We also note that the number of dependencylabels have less impact on results in general than wewould have expected.
The number of dependencylabels and the lack of support for some of them mayexplain the drop with predicted syntactic parses inour SRL results, but generally we obtain our best re-sults with yamada and lth annotations, which have12 and 41 dependency labels, respectively.4 ConclusionsWe evaluated four different tree-to-dependency con-version schemes, putting more or less emphasis onsyntactic or semantic evidence, in five down-streamapplications, including SMT and negation resolu-tion.
Our results show why it is important to beprecise about exactly what tree-to-dependency con-version scheme is used.
Tools like pennconverter.jargives us a wide range of options when convertingconstituent-based treebanks, and even small differ-ences may have significant impact on down-streamperformance.
The small differences are also impor-tant for more linguistic comparisons that also tend togloss over exactly what conversion scheme is used,e.g.
Ivanova et al(2012).AcknowledgementsHector Martinez is funded by the ERC grantCLARA No.
238405, and Anders S?gaard isfunded by the ERC Starting Grant LOWLANDSNo.
313695.624ReferencesEmily Bender, Dan Flickinger, Stephan Oepen, andYi Zhang.
2011.
Parser evaluation over local and non-local dependencies in a large corpus.
In EMNLP.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In COLING.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: controlling for optimizer instabil-ity.
In ACL.Mike Collins.
1999.
Head-driven statistical models fornatural language parsing.
Ph.D. thesis, University ofPennsylvania.Michael Collins.
2002.
Discriminative training methodsfor Hidden Markov Models.
In EMNLP.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive algorithms for multiclass problems.
In JMLR.Jakob Elming and Martin Haulrich.
2011.
Reorderingby parsing.
In Proceedings of International Workshopon Using Linguistic Information for Hybrid MachineTranslation (LIHMT-2011).Michel Galley and Christopher Manning.
2009.Quadratic-time dependency parsing for machine trans-lation.
In ACL.Keith Hall, Ryan McDonald, Jason Katz-Brown, andMichael Ringgaard.
2011.
Training dependencyparsers by jointly optimizing multiple objectives.
InEMNLP.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, andDan Flickinger.
2012. Who did what to whom?
a con-trastive study of syntactico-semantic dependencies.
InLAW.Richard Johansson and Alessandro Moschitti.
2010.Syntactic and semantic structure for opinion expres-sion detection.
In CoNLL.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InNODALIDA.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic-semantic analysiswith propbank and nombank.
In CoNLL.Mahesh Joshi and Carolyn Penstein-Rose.
2009.
Gen-eralizing dependency features for opinion mining.
InACL.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139:91?107.Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, andJonathon Read.
2012.
UiO2: Sequence-labeling nega-tion using dependency features.
In *SEM.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: an au-tomatic metric for mt evaluation with high levels ofcorrelation with human judgments.
In WMT.Wei-Hao Lin and Alexander Hauptmann.
2006.
Arethese documents written from different perspectives?In COLING-ACL.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Lin-guistics, 19(2):313?330.Ryan McDonald and Joakim Nivre.
2007.
Characteriz-ing the errors of data-driven dependency parsers.
InEMNLP-CoNLL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofthe Conference on Human Language Technology andEmpirical Methods in Natural Language Processing2005, pages 523?530, Vancouver, British Columbia.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In EACL.Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010.
Evaluating dependency repre-sentation for event extraction.
In COLING.Yusuke Miyao, Rune S?
tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented eval-uation of syntactic parsers and their representations.
InACL.Roser Morante and Eduardo Blanco.
2012.
*sem 2012shared task: Resolving the scope and focus of nega-tion.
In *SEM.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special issue.Computational linguistics, 38(2):223?260.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: a language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 Shared Task on Parsing the Web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Roy Schwartz, and Omri Abend, Roi Reichart, and AriRappoport.
2011.
Neutralizing linguistically prob-lematic annotations in unsupervised dependency pars-ing evaluation.
In ACL.Roy Schwartz, Omri Abend, and Ari Rappoport.
2012.Learnability-based syntactic annotation design.
InCOLING.625Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012.
Cross-framework evaluation for statistical pars-ing.
In EACL.Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers, and the role of synta.
Computational linguis-tics, 38(2):369?410.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In NAACL-HLT, Boulder, Colorado.Deniz Yuret, Laura Rimell, and Aydin Han.
2012.
Parserevaluation using textual entailments.
Language Re-sources and Evaluation, Published online 31 October2012.626
