Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 142?145,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPTowards a Methodology for Named Entities AnnotationKar?n FortINIST / LIPN2 all?e du Parc de Brabois,54500 Vandoeuvre-l?s-Nancy, Francekaren.fort@inist.frMaud EhrmannXRCE6 Chemin de Maupertuis,38240 Meylan, FranceEhrmann@xrce.xerox.comAdeline NazarenkoLIPN, Universit?
Paris 13 & CNRS99 av.
J.B. Cl?ment,93430 Villetaneuse, Francenazarenko@lipn.univ-paris13.frAbstractToday, the named entity recognition task isconsidered as fundamental, but it involvessome specific difficulties in terms of anno-tation.
Those issues led us to ask the fun-damental question of what the annotatorsshould annotate and, even more important,for which purpose.
We thus identify theapplications using named entity recogni-tion and, according to the real needs ofthose applications, we propose to seman-tically define the elements to annotate.
Fi-nally, we put forward a number of method-ological recommendations to ensure a co-herent and reliable annotation scheme.1 IntroductionNamed entity (NE) extraction appeared in the mid-dle of the 1990s with the MUC conferences (Mes-sage Understanding Conferences).
It has now be-come a successful Natural Language Processing(NLP) task that cannot be ignored.
However, theunderlying corpus annotation is still little studied.The issues at stake in manual annotation are cru-cial for system design, be it manual design, ma-chine learning, training or evaluation.
Manual an-notations give a precise description of the expectedresults of the target system.
Focusing on manualannotation issues led us to examine what namedentities are and what they are used for.2 Named Entities Annotation: practiceand difficultiesNamed entity recognition is a well-establishedtask (Nadeau and Sekine, 2007).
One can recall itsevolution according to three main directions.
Thefirst corresponds to work in the ?general?
field,0This work was partly realised as part of the Quaero Pro-gramme, funded by OSEO, French State agency for innova-tion.with the continuation of the task defined by MUCfor languages other than English, with a revised setof categories, mainly with journalistic corpora1.The second direction relates to work in ?special-ized?
domains, with the recognition of entities inmedicine, chemistry or microbiology, like geneand protein names in specialized literature2.
Thelast direction, spanning the two previous ones, isdisambiguation.For each of those evaluation campaigns, cor-pora were built and annotated manually.
Theyare generally used to develop automatic annotationtools.
?To Develop?
is to be understood in a broadsense: the goal is to describe what automatic sys-tems should do, to help writing the symbolic rulesthey are based on, to learn those rules or decisioncriteria automatically, and, finally, to evaluate theresults obtained by comparing them with a goldstandard.
The annotation process brings into playtwo actors, an annotator and a text.
The text anno-tation must follow precise guidelines, satisfy qual-ity criteria and support evaluation.In the general field, the MUC, CoNLL andACE evaluation campaigns seem to have paid at-tention to the process of manual NE annotation,with the definition of annotation guidelines andthe calculation of inter-annotator (but not intra-annotator) agreement, using a back-and-forth pro-cess between annotating the corpus and definingthe annotation guidelines.
Nevertheless, some as-pects of the annotation criteria remained problem-atic, caused mainly by ?different interpretationsof vague portions of the guidelines?
(Sundheim,1995).
In the fields of biology and medicine, textsfrom specialized databases (PubMed and Med-Line3) were annotated.
Annotation guidelines1See the evaluation campaigns MET, IREX, CoNNL,ACE, ESTER and HAREM (Ehrmann, 2008, pp.
19-21).2See the evaluation campaigns BioCreAtIvE (Kim et al,2004) and JNLPBA (Hirschman et al, 2005).3www.ncbi.nlm.nih.gov/pubmed, http://medline.cos.com142were vague about the annotation of NEs 4, and fewstudies measured annotation quality.
For the GE-NIA (Kim et al, 2003), PennBioIE (Kulick et al,2004) or GENETAG (Tanabe et al, 2005) corpora,no inter- or intra-annotator agreement is reported.If NE annotation seems a well-established prac-tice, it involves some difficulties.As regards general language corpora, those dif-ficulties are identified (Ehrmann, 2008).
The firstone relates to the choice of annotation categoriesand the determination of what they encompass.Indeed, beyond the ?universal?
triad defined bythe MUC conferences (ENAMEX, NUMEX andTIMEX), the inventory of categories is difficult tostabilize.
For ENAMEX, although it may be ob-vious that the name of an individual such as KofiAnnan is to be annotated using this category, whatto do with the Kennedys, Zorro, the Democrats orSanta Claus?
For the other categories, it is justas difficult to choose the granularity of the cat-egories and to determine what they encompass.Another type of difficulty relates to the selectionof the mentions to be annotated as well as the de-limitation of NE boundaries.
Let us consider theNE ?Barack Obama?
and the various lexemes thatcan refer to it: Barack Obama, Mr Obama, thePresident of the United States, the new president,he.
Should we annotate proper nouns only, or alsodefinite descriptions that identify this person, evenpronouns which, contextually, could refer to thisNE?
And what to do with the various attributesthat go with this NE (Mr and president)?
Coordi-nation and overlapping phenomena can also raiseproblems for the annotators.
Finally, another dif-ficulty results from phenomena of referential plu-rality, with homonyms NEs (Java place and Javalanguage) and metonyms (England as a geograph-ical place, a government or sport team).Our experience in microbiology shows thatthese difficulties are even more acute in special-ized language.
We carried out an annotation ex-periment on an English corpus of PubMed notices.The main difficulty encountered related to thedistinction required between proper and commonnouns, the morphological boundary between thetwo being unclear in those fields where commonnouns are often reclassified as ?proper nouns?, asis demonstrated by the presence of these names4(Tanabe et al, 2005) notes that ?a more detailed defi-nition of a gene/protein name, as well as additional annota-tion rules, could improve inter-annotator agreement and helpsolve some of the tagging inconsistencies?.in nomenclatures (small, acid-soluble spore pro-tein A is an extreme case) or acronymisation phe-nomena (one finds for example across the outermembrane (OM)).
In those cases, annotators wereinstructed to refer to official lists, such as Swiss-Prot5, which requires a significant amount of time.Delimiting the boundaries of the elements to beannotated also raised many questions.
One canthus choose to annotate nifh messenger RNA if it isconsidered that the mention of the state messengerRNA is part of the determination of the reference,or only nifh, if it is considered that the proper nounis enough to build the determination.
Selecting se-mantic types was also a problem for the annota-tors, in particular for mobile genetic elements, likeplasmids or transposons.
Indeed, those were to beannotated in taxons but not in genes whereas theyare chunks of DNA, therefore parts of genome.
Aparticularly confusing directive for the annotatorswas to annotate the acronym KGFR as a propernoun and the developed form keratinocyte growthFactor receptor as a common noun.
This kind ofinstruction is difficult to comprehend and shouldhave been documented better.These problems result in increased annotationcosts, too long annotation guidelines and, aboveall, a lot of indecision for the annotators, whichinduces inconsistencies and lower-quality annota-tion.
This led us to consider the issue of what theannotators must annotate (semantic foundations ofNE) and, above all, why.3 What to Annotate?3.1 Various Defining CriteriaEhrmann (2008) proposes a linguistic analysisof the notion of NE, which is presented as anNLP ?creation?.
In the following paragraphs, wetake up the distinction introduced in LDC (2004):NE are ?mentions?
refering to domain ?entities?,those mentions relate to different linguistic cate-gories: proper nouns (?Rabelais?
), but also pro-nouns (?he?
), and in a broader sense, definite de-scriptions (?the father of Gargantua?).
Severaldefining criteria for NE can be identified.Referential Unicity One of the main charac-teristics of proper nouns is their referential be-haviour: a proper noun refers to a unique refer-ential entity, even if this unicity is contextual.
Weconsider that this property is essential in the usageof NEs in NLP.5http://www.expasy.org/sprot/143Referential Autonomy NEs are also au-tonomous from the referential point of view.
Itis obvious in the case of proper nouns, which areself-sufficient to identify the referent, at least in agiven communication situation (Eurotunnel).
Thecase of definite descriptions (The Channel Tunneloperator) is a bit different: they can be used toidentify the referent thanks to external knowledge.Denominational Stability Proper nouns arealso stable denominations.
Even if some varia-tions may appear (A. Merkel/Mrs Merkel), theyare more regular and less numerous than for othernoun phrases6.Referential Relativity Interpretation is alwayscarried out relatively to a domain model, that canbe implicit in simple cases (for example, a countryor a person) but has to be made explicit when thediversity in entities to consider increases.3.2 Different Annotation PerspectivesThe defining criteria do not play the same rolein all applications.
In some cases (indexing andknowledge integration), we focus on referentialentities which are designated by stable and non-ambiguous descriptors.
In those cases, the NEsto use are proper nouns or indexing NEs and theyshould be normalized to identify variations thatcan appear despite their referential stability.
Forthis type of application, the main point is not tohighlight all the mentions of an entity in a doc-ument, but to identify which document mentionswhich entity.
Therefore, precision has to be fa-vored over recall.
On the other hand, in the tasksof information extraction and domain modelling, itis important to identify all the mentions, includingdefinite descriptions (therefore, coreference rela-tions between mentions that are not autonomousenough from a referential point of view are alsoimportant to identify).As it is impossible to identify the mentions of allthe referential entities, the domain model defineswhich entities are ?of interest?
and the boundarybetween what has to be annotated or not.
Forinstance, when a human resources director is in-terested in the payroll in the organization, s/hethinks in terms of personnel categories and notin terms of the employees as individuals.
Thisappears in the domain model: the different cate-gories of persons (technicians, engineers, etc.)
are6A contrario, this explains the importance of synonymsidentification in domains where denominations are not stable(like, for instance, in genomics).modelled as instances attached to the concept CAT-OF-EMPLOYEES and the individuals are not rep-resented.
On the opposite, when s/he deals withemployees?
paychecks and promotion, s/he is in-terested in individuals.
In this case, the modelshould consider the persons as instances and thecategories of personnel as concepts.Domain modelling implies making explicitchoices where texts can be fuzzy and mix pointsof view.
It is therefore impossible to annotate theNEs of a text without refering to a model.
In thecase of the above experiment, as it is often thecase, the model was simply described by a list ofconcepts: the annotators had to name genes andproteins, but also their families, compositions andcomponents.4 Annotation methodologyAnnotation guidelines As the targeted annota-tion depends on what one wants to annotate andhow it will be exploited, it is important to provideannotators with guidelines that explain what mustbe annotated rather than how it should be anno-tated.
Very often, feasibility constraints overcomesemantic criteria,7 which confuses annotators.
Be-sides, it is important to take into consideration thecomplexity of the annotation task, without exclud-ing the dubious annotations or those which wouldbe too difficult to reproduce automatically.
On thecontrary, one of the roles of manual annotationis to give a general idea of the task complexity.The annotators must have a clear view of the tar-get application.
This view must be based on anexplicit reference model, as that of GENIA, withprecise definitions and explicit modelling choices.Examples can be added for illustration but theyshould not replace the definition of the goal.
Itis important that annotators understand the under-lying logic of annotation.
It helps avoiding mis-understandings and giving them a sense of beinginvolved and committed.Annotation tools Although there exists manyannotation tools, few are actually available, free,downloadable and usable.
Among those tools areCallisto, MMAX2, Knowtator or Cadixe8 whichwas used in the reported experiment.
The features7"In [src homology 2 and 3], it seems excessive to requirean NER program to recognize the entire fragment, however,3 alone is not a valid gene name."
(Tanabe et al, 2005).8http://callisto.mitre.org, http://mmax2.sourceforge.net,http://knowtator.sourceforge.net, http://caderige.imag.fr144and the annotation language expressivity must beadapted to the targeted annotation task: is it suf-ficient to type the textual segments or should theyalso be related?
is it possible/necessary to haveconcurrent or overlapping annotations?
In our ex-periment on biology, for instance, although the an-notators had the possibility to mention their un-certainty by adding an attribute to the annotations,they seldom did so, because it was not easy to dousing the provided interface.Annotation evaluation Gut and Bayerl (2004)distinguishes the inter-annotator agreement, whichmeasures the annotation stability, and the intra-annotation agreement that gives an idea on howreproducible an annotation is.
The inter- and intra-annotator agreements do not have to be measuredon the whole corpus, but quite early in the annota-tion process, so that the annotation guidelines canbe modified.
Another way to evaluate annotationrelies on annotator introspection.
Annotators areasked to auto-evaluate the reliability of their an-notations and their (un)certainty attributes can beused afterwards to evaluate the overall quality ofthe work.
Since we did not have several anno-tators working independently on our biology cor-pus, we asked them to indicate the uncertainty oftheir annotations on a carefully selected samplecorpus.
25 files were extracted out of the 499 textsof our corpus (5%).
This evaluation required onlyfew hours of work and it enabled to better qualifyand quantity annotation confidence.
The annota-tors declared that around 20% of the total numberof annotation tags were "uncertain".
We observedthat more than 75% of these uncertain tags wereassociated to common nouns of type bacteria andthat uncertainty was very often (77%) linked to thefact that distinguishing common and proper nounswas difficult.More generally, a good annotation methodologyconsists in having several annotators working in-dependently on the same sample corpus very earlyin the process.
It allows to quickly identify the dis-agreement causes.
If they can be solved, new rec-ommendations are added to the annotation guide-lines.
If not, the annotation task might be simpli-fied and the dubious cases eliminated.5 Conclusion and ProspectsIn the end, two main points must be considered fora rigorous and efficient NE annotation in corpus.First, as for the content, it is important to focus,not on how to annotate, but rather on what to anno-tate, according to the final application.
Once spec-ified what is to be annotated, one has to be cau-tious in terms of methodology and consider fromthe very beginning of the campaign, the evaluationof the produced annotation.We intend to apply this methodology to otherannotation campaigns of the project we participatein.
As those campaigns cover terminology and se-mantic relations extraction, we will have to adaptour method to those applications.ReferencesMaud Ehrmann.
2008.
Les entit?s nomm?es, de lalinguistique au TAL : statut th?orique et m?thodesde d?sambigu?sation.
Ph.D. thesis, Univ.
Paris 7.Ulrike Gut and Petra Saskia Bayerl.
2004.
Measuringthe reliability of manual annotations of speech cor-pora.
In Proc.
of Speech Prosody, pages 565?568,Nara, Japan.Lynette Hirschman, Alexander Yeh, ChristianBlaschke, and Alfonso Valencia.
2005.
Overviewof biocreative: critical assessment of informationextraction for biology.
BMC Bioinformatics, 6(1).J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
Ge-nia corpus?a semantically annotated corpus for bio-textmining.
Bioinformatics, 19:180?182.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introduc-tion to the bio-entity recognition task at JNLPBA.In Proc.
of JNLPBA COLING 2004 Workshop, pages70?75.Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,Ryan McDonald, Martha Palmer, Andrew Schein,and Lyle Ungar.
2004.
Integrated annotation forbiomedical information extraction.
In HLT-NAACL2004 Workshop: Biolink.
ACL.LDC.
2004.
ACE (Automatic Content Extraction)english annotation guidelines for entities.
Livrableversion 5.6.1 2005.05.23, Linguistic Data Consor-tium.David Nadeau and Satoshi Sekine.
2007.
A surveyof named entity recognition and classification.
Lin-guisticae Investigaciones, 30(1):3?26.B.
Sundheim.
1995.
Overview of results of the MUC-6evaluation.
In Proc.
of the 6th Message Understand-ing Conference.
Morgan Kaufmann Publishers.Lorraine Tanabe, Natalie Xie, Lynne Thom, WayneMatten, and John Wilbur1.
2005.
Genetag: a taggedcorpus for gene/protein named entity recognition.Bioinformatics, 6.145
