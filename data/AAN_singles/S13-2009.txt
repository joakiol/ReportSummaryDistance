Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 53?57, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsManTIME: Temporal expression identification and normalization in theTempEval-3 challengeMichele Filannino, Gavin Brown, Goran NenadicThe University of ManchesterSchool of Computer ScienceManchester, M13 9PL, UK{m.filannino, g.brown, g.nenadic}@cs.man.ac.ukAbstractThis paper describes a temporal expressionidentification and normalization system, Man-TIME, developed for the TempEval-3 chal-lenge.
The identification phase combinesthe use of conditional random fields alongwith a post-processing identification pipeline,whereas the normalization phase is carried outusing NorMA, an open-source rule-based tem-poral normalizer.
We investigate the perfor-mance variation with respect to different fea-ture types.
Specifically, we show that the useof WordNet-based features in the identifica-tion task negatively affects the overall perfor-mance, and that there is no statistically sig-nificant difference in using gazetteers, shal-low parsing and propositional noun phraseslabels on top of the morphological features.On the test data, the best run achieved 0.95(P), 0.85 (R) and 0.90 (F1) in the identifica-tion phase.
Normalization accuracies are 0.84(type attribute) and 0.77 (value attribute).
Sur-prisingly, the use of the silver data (alone or inaddition to the gold annotated ones) does notimprove the performance.1 IntroductionTemporal information extraction (Verhagen et al2007; Verhagen et al 2010) is pivotal for many Nat-ural Language Processing (NLP) applications suchas question answering, text summarization and ma-chine translation.
Recently the topic aroused in-creasing interest also in the medical domain (Sun etal., 2013; Kovac?evic?
et al 2013).Following the work of Ahn et al(2005), thetemporal expression extraction task is now conven-tionally divided into two main steps: identificationand normalization.
In the former step, the effortis concentrated on how to detect the right bound-ary of temporal expressions in the text.
In the nor-malization step, the aim is to interpret and repre-sent the temporal meaning of the expressions usingTimeML (Pustejovsky et al 2003) format.
In theTempEval-3 challenge (UzZaman et al 2012) thenormalization task is focused only on two temporalattributes: type and value.2 System architectureManTIME mainly consists of two components, onefor the identification and one for the normalization.2.1 IdentificationWe tackled the problem of identification as a se-quencing labeling task leading to the choice of Lin-ear Conditional Random Fields (CRF) (Lafferty etal., 2001).
We trained the system using both human-annotated data (TimeBank and AQUAINT corpora)and silver data (TE3Silver corpus) provided by theorganizers of the challenge in order to investigate theimportance of the silver data.Because the silver data are far more numerous(660K tokens vs. 95K), our main goal was to rein-force the human-annotated data, under the assump-tion that they are more informative with respect tothe training phase.
Similarly to the approach pro-posed by Adafre and de Rijke (2005), we developeda post-processing pipeline on top of the CRF se-quence labeler to boost the results.
Below we de-scribe each component in detail.532.1.1 Conditional Random FieldsThe success of applying CRFs mainly depends onthree factors: the labeling scheme (BI, BIO, BIOEor BIOEU), the topology of the factor graph andthe quality of the features used.
We used the BIOformat in all the experiments performed during thisresearch.
The factor graph has been generated us-ing the following topology: (w0), (w?1), (w?2),(w+1), (w+2), (w?2?w?1), (w?1?w0), (w0?w+1),(w?1?w0?w+1), (w0?w+1?w+2), (w+1?w+2),(w?2 ?w?1 ?w0), (w?1 ?w+1) and (w?2 ?w+2).The system tokenizes each document in the cor-pus and extracts 94 features.
These belong to thefollowing four disjoint categories:?
Morphological: This set includes a compre-hensive list of features typical of Named En-tity Recognition (NER) tasks, such as the wordas it is, lemma, stem, pattern (e.g.
?Jan-2003?:?Xxx-dddd?
), collapsed pattern (e.g.
?Jan-2003?
: ?Xx-d?
), first 3 characters, last 3 charac-ters, upper first character, presence of ?s?
as lastcharacter, word without letters, word withoutletters or numbers, and verb tense.
For lemmaand POS tags we use TreeTagger (Schmid,1994).
Boolean values are included, indicatingif the word is lower-case, alphabetic, digit, al-phanumeric, titled, capitalized, acronym (cap-italized with dots), number, decimal number,number with dots or stop-word.
Additionally,there are features specifically crafted to han-dle temporal expressions in the form of regu-lar expression matching: cardinal and ordinalnumbers, times, dates, temporal periods (e.g.morning, noon, nightfall), day of the week, sea-sons, past references (e.g.
ago, recent, before),present references (e.g.
current, now), futurereferences (e.g.
tomorrow, later, ahead), tem-poral signals (e.g.
since, during), fuzzy quan-tifiers (e.g.
about, few, some), modifiers, tem-poral adverbs (e.g.
daily, earlier), adjectives,conjunctions and prepositions.?
Syntactic: Chunks and propositional nounphrases belong to this category.
Both areextracted using the shallow parsing softwareMBSP1.1http://www.clips.ua.ac.be/software/mbsp-for-python?
Gazetteers: These features are expressed us-ing the BIO format because they can includeexpressions longer than one word.
The inte-grated gazetteers are: male and female names,U.S.
cities, nationalities, world festival namesand ISO countries.?
WordNet: For each word we use the number ofsenses associated to the word, the first and thesecond sense name, the first 4 lemmas, the first4 entailments for verbs, the first 4 antonyms,the first 4 hypernyms and the first 4 hyponyms.Each of them is defined as a separate feature.The features mentioned above have been com-bined in 4 different models:?
Model 1: Morphological only?
Model 2: Morphological + syntactic?
Model 3: Morphological + gazetteers?
Model 4: Morphological + gazetteers + Word-NetAll the experiments have been carried out usingCRF++ 0.572 with parameters C = 1, ?
= 0.0001and L2-regularization function.2.1.2 Model selectionThe model selection was performed over theentire training corpus.
Silver data and human-annotated data were merged, shuffled at sentence-level (seed = 490) and split into two sets: 80% ascross-validation set and 20% as real-world test set.The cross-validation set was shuffled 5 times, andfor each of these, the 10-fold cross validation tech-nique was applied.The analysis is statistically significant (p =0.0054 with ANOVA test) and provides two impor-tant outcomes: (i) the set of WordNet features nega-tively affects the overall classification performance,as suggested by Rigo et al(2011).
We believe this isdue to the sparseness of the labels: many tokens didnot have any associated WordNet sense.
(ii) Thereis no statistically significant difference among thefirst three models, despite the presence of apparentlyimportant information such as chunks, propositional2https://code.google.com/p/crfpp/54Figure 1: Differences among models using 5x10-foldcross-validationnoun phrases and gazetteers.
The Figure 1 shows thebox plots for each model.In virtue of this analysis, we opted for the smallestfeature set (Model 1) to prevent overfitting.In order to get a reliable estimation of the perfor-mance of the selected model on the real world data,we trained it on the entire cross-validation set andtested it against the real-word test set.
The resultsfor all the models are shown in the following table:System Pre.
Rec.
F?=1Model 1 83.20 85.22 84.50Model 2 83.57 85.12 84.33Model 3 83.51 85.12 84.31Model 4 83.15 84.44 83.79Precision, Recall and F?=1 score are computedusing strict matching.The models used for the challenge have beentrained using the entire training set.2.1.3 Post-processing identification pipelineAlthough CRFs already provide reasonable per-formance, equally balanced in terms of precisionand recall, we focused on boosting the baseline per-formance through a post-processing pipeline.
Forthis purpose, we introduced 3 different modules.Probabilistic correction module averages theprobabilities from the trained CRFs model with theones extracted from human-annotated data only.
Foreach token, we extracted: (i) the conditional proba-bility for each label to be assigned (B, I or O), and(ii) the prior probability of the labels in the human-annotated data only.
The two probabilities are aver-aged for every label of each token.
The list of tokensextracted in the human-annotated data was restrictedto those that appeared within the span of temporalexpressions at least twice.
The application of thismodule in some cases has the effect of changing themost likely label leading to an improvement of re-call, although its major advantage is making CRFspredictions less strict.BIO fixer fixes wrong label sequences.
For theBIO labeling scheme, the sequence O-I is necessar-ily wrong.
We identified B-I as the appropriate sub-stitution.
This is the case in which the first tokenhas been incorrectly annotated (e.g.
?Three/O days/Iago/I ./O?
is converted into ?Three/B days/I ago/I./O?).
We also merged close expressions such as B-B or I-B, because different temporal expressions aregenerally divided at least by a symbol or a punctu-ation character (e.g.
?Wednesday/B morning/B?
isconverted into ?Wednesday/B morning/I?
).Threshold-based label switcher uses the prob-abilities extracted from the human-annotated data.When the most likely label (in the human-annotateddata) has a prior probability greater than a certainthreshold, the module changes the CRFs predictedlabel to the most likely one.
This leads to forcethe probabilities learned from the human-annotateddata.Through repeated empirical experiments on asmall sub-set of the training data, we found anoptimal threshold value (0.87) and an optimal se-quence of pipeline components (Probabilistic cor-rection module, BIO fixer, Threshold-based labelswitcher, BIO fixer).We analyzed the effectiveness of the post-processing identification pipeline using a 10-foldcross-validation over the 4 models.
The differencebetween CRFs and CRFs + post-processing pipelineis statistically significant (p = 3.51 ?
10?23 withpaired T-test) and the expected average increment is2.27% with respect to the strict F?=1 scores.2.2 NormalizationThe normalization component is an updated versionof NorMA (Filannino, 2012), an open-source rule-based system.55# Training dataIdentification NormalizationOverallStrict matching Lenient matching Accuracyrun (post-processing) Pre.
Rec.
F?=1 Pre.
Rec.
F?
?=1 Type Value score1 Human&Silver (no) 78.57 63.77 70.40 97.32 78.99 87.20 88.99 77.06 67.202 Human&Silver (yes) 79.82 65.94 72.22 97.37 80.43 88.10 87.38 75.68 66.673 Human (no) 76.07 64.49 69.80 94.87 80.43 87.06 87.39 77.48 67.454 Human (yes) 78.86 70.29 74.33 95.12 84.78 89.66 86.31 76.92 68.975 Silver (no) 77.68 63.04 69.60 97.32 78.99 87.20 88.99 77.06 67.206 Silver (yes) 81.98 65.94 73.09 98.20 78.99 87.55 90.83 77.98 68.27Table 1: Performance on the TempEval-3 test set.3 Results and DiscussionWe submitted six runs as combinations of differenttraining sets and the use of the post-processing iden-tification pipeline.
The results are shown in Table 1where the overall score is computed as multiplica-tion between lenient F?=1 score and the value accu-racy.In all the runs, recall is lower than precision.
Thisis an indication of a moderate lexical difference be-tween training data and test data.
The relatively lowtype accuracy testifies the normalizer?s inability torecognize new lexical patterns.
Among the correctlytyped temporal expressions, there is still about 10%of them for which an incorrect value is provided.The normalization task is proved to be challenging.The training of the system by using human-annotated data only, in addition to the post-processing pipeline, provided the best results, al-though not the highest normalization accuracy.
Sur-prisingly, the silver data do not improve the per-formance, both when used alone or in additionto human-annotated data (regardless of the post-processing pipeline usage).The post-processing pipeline produces the high-est precision when applied to the silver data only.In this case, the pipeline acts as a reinforcement ofthe human-annotated data.
As expected, the post-processing pipeline boosts the performance of bothprecision and recall.
We registered the best improve-ment with the human-annotated data.Due to the small number of temporal expressionsin the test set (138), further analysis is required todraw more general conclusions.4 ConclusionsWe described the overall architecture of ManTIME,a temporal expression extraction pipeline, in thecontext of TempEval-3 challenge.This research shows, in the limits of its general-ity, the primary and exhaustive importance of mor-phological features to the detriment of syntactic fea-tures, as well as gazetteer and WordNet-related ones.In particular, while syntactic and gazetteer-relatedfeatures do not affect the performance, WordNet-related features affect it negatively.The research also proves the use of a post-processing identification pipeline to be promisingfor both precision and recall enhancement.Finally, we found out that the silver data do notimprove the performance, although we consider thetest set too small for this result to be generalizable.To aid replicability of this work, the systemcode, machine learning pre-trained models, statis-tical validation details and an online DEMO areavailable at: http://www.cs.man.ac.uk/?filannim/projects/tempeval-3/AcknowledgmentsWe would like to thank the organizers of theTempEval-3 challenge.
The first author would likealso to acknowledge Marilena Di Bari, Joseph Mel-lor and Daniel Jamieson for their support and the UKEngineering and Physical Science Research Coun-cil for its support in the form of a doctoral traininggrant.56ReferencesSisay Fissaha Adafre and Maarten de Rijke.
2005.
Fea-ture engineering and post-processing for temporal ex-pression recognition using conditional random fields.In Proceedings of the ACL Workshop on Feature En-gineering for Machine Learning in Natural LanguageProcessing, FeatureEng ?05, pages 9?16, Stroudsburg,PA, USA.
Association for Computational Linguistics.David Ahn, Sisay Fissaha Adafre, and Maarten de Ri-jke.
2005.
Towards task-based temporal extractionand recognition.
In Graham Katz, James Pustejovsky,and Frank Schilder, editors, Annotating, Extractingand Reasoning about Time and Events, number 05151in Dagstuhl Seminar Proceedings, Dagstuhl, Germany.Internationales Begegnungs- und Forschungszentrumfu?r Informatik (IBFI), Schloss Dagstuhl, Germany.Michele Filannino.
2012.
Temporal expressionnormalisation in natural language texts.
CoRR,abs/1206.2010.Aleksandar Kovac?evic?, Azad Dehghan, Michele Filan-nino, John A Keane, and Goran Nenadic.
2013.
Com-bining rules and machine learning for extraction oftemporal expressions and events from clinical narra-tives.
Journal of American Medical Informatics.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.James Pustejovsky, Jose?
Castan?o, Robert Ingria, RoserSaur?
?, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003.
Timeml: Robust specification of eventand temporal expressions in text.
In in Fifth Interna-tional Workshop on Computational Semantics (IWCS-5.Stefan Rigo and Alberto Lavelli.
2011.
Multisex - amulti-language timex sequential extractor.
In Tempo-ral Representation and Reasoning (TIME), 2011 Eigh-teenth International Symposium on, pages 163?170.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, Manchester, UK.Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner.
2013.Evaluating temporal relations in clinical text: 2012i2b2 challenge.
Journal of the American Medical In-formatics Association.Naushad UzZaman, Hector Llorens, James F. Allen,Leon Derczynski, Marc Verhagen, and James Puste-jovsky.
2012.
Tempeval-3: Evaluating events,time expressions, and temporal relations.
CoRR,abs/1206.5333.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 75?80, Prague.Marc Verhagen, Roser Saur?
?, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, SemEval?10, pages 57?62, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.57
