Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 72?80,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsModeling the Noun Phrase versus Sentence Coordination Ambiguity inDutch: Evidence from Surprisal TheoryHarm BrouwerUniversity of GroningenGroningen, the Netherlandsharm.brouwer@rug.nlHartmut FitzUniversity of GroningenGroningen, the Netherlandsh.fitz@rug.nlJohn C. J. HoeksUniversity of GroningenGroningen, the Netherlandsj.c.j.hoeks@rug.nlAbstractThis paper investigates whether surprisaltheory can account for differential pro-cessing difficulty in the NP-/S-coordina-tion ambiguity in Dutch.
Surprisal is es-timated using a Probabilistic Context-FreeGrammar (PCFG), which is induced froman automatically annotated corpus.
Wefind that our lexicalized surprisal modelcan account for the reading time data froma classic experiment on this ambiguity byFrazier (1987).
We argue that syntacticand lexical probabilities, as specified in aPCFG, are sufficient to account for what iscommonly referred to as an NP-coordina-tion preference.1 IntroductionLanguage comprehension is incremental in thatmeaning is continuously assigned to utterancesas they are encountered word-by-word (Altmannand Kamide, 1999).
Not all words, however, areequally easy to process.
A word?s processing dif-ficulty is affected by, for instance, its frequency orits effect on the syntactic and semantic interpreta-tion of a sentence.
A recent theory of sentence pro-cessing, surprisal theory (Hale, 2001; Levy, 2008),combines several of these aspects into one singleconcept, namely the surprisal of a word.
A word?ssurprisal is proportional to its expectancy, i.e., theextent to which that word is expected (or pre-dicted).
The processing difficulty a word causesduring comprehension is argued to be related lin-early to its surprisal; the higher the surprisal valueof a word, the more difficult it is to process.In this paper we investigate whether surprisaltheory can account for the processing difficultyinvolved in sentences containing the noun phrase(NP) versus sentence (S) coordination ambiguity.The sentences in (1), from a self-paced reading ex-periment by Frazier (1987), exemplify this ambi-guity:(1) a. PietPietkustekissedMarieMarieenand//haar zusjeher sister//ooktoo[1,222ms; NP-coordination]b. PietPietkustekissedMarieMarieenand//haar zusjeher sister//lachtelaughed[1,596ms; S-coordination]Both sentences are temporarily ambiguous in theboldface region.
Sentence (1-a) is disambiguatedas an NP-coordination by the sentence-final ad-verb ook.
Sentence (1-b), on the other hand, is dis-ambiguated as an S-coordination by the sentence-final verb lachte.
Frazier found that the verb lachtein sentence (1-b) takes longer to process (1,596ms) than the adverb ook (1,222 ms) in (1-a).Frazier (1987) explained these findings by as-suming that the human language processor ad-heres to the so-called minimal attachment prin-ciple.
According to this principle, the sentenceprocessor projects the simplest syntactic struc-ture which is compatible with the material readat any point in time.
NP-coordination is syntac-tically simpler than S-coordination in that it re-quires less phrasal nodes to be projected.
Hence,the processor is biased towards NP- over S-coor-dination.
Processing costs are incurred when thisinitial preference has to be revised in the disam-biguating region, as in sentence (1-b), resulting inlonger reading times.
Hoeks et al (2006) haveshown that the NP-coordination preference can bereduced, but not entirely eliminated, when poorthematic fit between the verb and a potential objectmake an NP-coordination less likely (e.g., Jaspersanded the board and the carpenter laughed).
Weargue here that this residual preference for NP-coordination can be explained in terms of syntac-tic and lexical expectation within the frameworkof surprisal theory.
In contrast to the minimal at-tachment principle, surprisal theory does not pos-72tulate specific kinds of syntactic representations orrely on a metric of syntactic complexity to predictprocessing behavior.This paper is organized as follows.
In section2 below, we briefly sketch basic surprisal theory.Then we describe how we induced a grammarfrom a large annotated Dutch corpus and how sur-prisal was estimated from this grammar (section3).
In section 4, we describe Frazier?s experimenton the NP-/S-coordination ambiguity in more de-tail, and present our surprisal-based simulations ofthis data.
We conclude with a discussion of our re-sults in section 5.2 Surprisal TheoryAs was mentioned in the introduction, languageprocessing is highly incremental, and proceeds ona more or less word-by-word basis.
This suggeststhat a person?s difficulty with processing a sen-tence can be modeled on a word level as proposedby Attneave (1959).
Furthermore, it has recentlybeen suggested that one of the characteristics ofthe comprehension system that makes it so fast,is its ability to anticipate what a speaker will saynext.
In other words, the language comprehensionsystem works predictively (Otten et al, 2007; vanBerkum et al, 2005).
Surprisal theory is a modelof differential processing difficulty which accom-modates both these properties of the comprehen-sion system, incremental processing and word pre-diction (Hale, 2001; Levy, 2008).
In this theory,the processing difficulty of a sentence is a func-tion of word processing difficulty.
A word?s dif-ficulty is inversely proportional to its expectancy,i.e., the extent to which the word was expected orpredicted in the context in which it occurred.
Thelower a word?s expectancy, the more difficult it isto process.
A word?s surprisal is linearly related toits difficulty.
Consequently, words with lower con-ditional probabilities (expectancy) lead to highersurprisal than words with higher conditional prob-abilities.Surprisal theory is, to some extent, indepen-dent of the language model that generates condi-tional word probabilities.
Different models canbe used to estimate these probabilities.
For allsuch models, however, a clear distinction can bemade between lexicalized and unlexicalized sur-prisal.
In lexicalized surprisal, the input to the lan-guage model is a sequence of words (i.e., a sen-tence).
In unlexicalized surprisal, the input is asequence of word categories (i.e., part-of-speechtags).
While previous studies have used unlexical-ized surprisal to predict reading times, evidencefor lexicalized surprisal is rather sparse.
Smithand Levy (2008) investigated the relation betweenlexicalized surprisal and reading time data for nat-uralistic texts.
Using a trigram language model,they showed that there was a linear relationshipbetween the two measures.
Demberg and Keller(2008) examined whether this relation extendedbeyond transitional probabilities and found no sig-nificant effects.
This state of affairs is somewhatunfortunate for surprisal theory since input to thehuman language processor consists of sequencesof words, not part-of-speech tags.
In our study wetherefore used lexicalized surprisal to investigatewhether it can account for reading time data fromthe NP-/S-coordination ambiguity in Dutch.
Lex-icalized surprisal furthermore allows us to studyhow syntactic expectations might be modulated oreven reversed by lexical expectations in temporar-ily ambiguous sentences.2.1 Probabilistic Context Free GrammarsBoth Hale (2001) and Levy (2008) used a Prob-abilistic Context Free Grammar (PCFG) as a lan-guage model in their implementations of surprisaltheory.
A PCFG consists of a set of rewrite ruleswhich are assigned some probability (Charniak,1993):S ?
NP, VP 1.0NP ?
Det, N 0.5NP ?
NP, VP 0.5. .
.
?
.
.
.
.
.
.In this toy grammar, for instance, a noun phraseplaceholder can be rewritten to a determiner fol-lowed by a noun symbol with probability 0.5.From such a PCFG, the probability of a sentencecan be estimated as the product of the probabili-ties of all the rules used to derive the sentence.
Ifa sentence has multiple derivations, its probabil-ity is the sum of the probabilities for each deriva-tion.
For our purpose, we also needed to obtain theprobability of partial sentences, called prefix prob-abilities.
The prefix probability P (w1...wi) of apartial sentence w1...wi is the sum of the probabil-ities of all sentences generated by the PCFG whichshare the initial segment w1...wi.
Hale (2001)pointed out that the ratio of the prefix probabilitiesP (w1 .
.
.
wi) and P (w1 .
.
.
wi?1) equals preciselythe conditional probability of word wi.
Given a73PCFG, the difficulty of word wi can therefore bedefined as:difficulty(wi) ?
?log2[P (w1 .
.
.
wi)P (w1 .
.
.
wi?1)].Surprisal theory requires a probabilistic lan-guage model that generates some form of wordexpectancy.
The theory itself, however, is largelyneutral with respect to which model is employed.Models other than PCFGs can be used to esti-mate surprisal.
Nederhof et al (1998), for in-stance, show that prefix probabilities, and there-fore surprisal, can be estimated from Tree Adjoin-ing Grammars.
This approach was taken in Dem-berg and Keller (2009).
Other approaches haveused trigram models (Smith and Levy, 2008), Sim-ple Recurrent Networks of the Elman type (Frank,2009), Markov models and Echo-state Networks(Frank and Bod, 2010).
This illustrates that sur-prisal theory is not committed to specific claimsabout the structural representations that languagetakes in the human mind.
It rather functions as a?causal bottleneck?
between the representations ofa language model, and expectation-based compre-hension difficulty (Levy, 2008).
In other words,comprehension difficulty does not critically de-pend on the structural representations postulatedby the language model which is harnessed to gen-erate word expectancy.The use of PCFGs raises some important ques-tions on parallelism in language processing.
Aprefix probability can be interpreted as a prob-ability distribution over all analyses compatiblewith a partial sentence.
Since partial sentencescan sometimes be completed in an indefinite num-ber of ways, it seems both practically and psycho-logically implausible to implement this distribu-tion as an enumeration over complete structures.Instead, prefix probabilities should be estimatedas a by-product of incremental processing, as inStolcke?s (1995) parser (see section 3.2).
Thisapproach, however, still leaves open how manyanalyses are considered in parallel; does the hu-man sentence processor employ full or limited par-allelism?
Jurafsky (1996) showed that full par-allelism becomes more and more unmanageablewhen the amount of information used for disam-biguation increases.
Levy, on the other hand, ar-gued that studies of probabilistic parsing revealthat typically a small number of analyses are as-signed the majority of probability mass (Roark,2001).
Thus, even when assuming full parallelism,only a small number of ?relevant?
analyses wouldbe considered in parallel.3 Grammar and Parser3.1 Grammar InductionIn our simulations, we used a PCFG to modelthe phrase structure of natural language.
To in-duce such a grammar, an annotated corpus wasrequired.
We used Alpino (van Noord, 2006)?a robust and wide-coverage dependency parserfor Dutch?to automatically generate such a cor-pus, annotated with phrase structure, for 204.000sentences, which were randomly extracted fromDutch newspapers.
These analyses were thenused to induce a PCFG consisting of 650 gram-mar rules, 89 non-terminals, and 208.133 termi-nals (lexical items).1 Moreover, 29 of the 89 non-terminals could result in epsilon productions.The Alpino parser constructed the phrase struc-ture analyses automatically.
Despite Alpino?s highaccuracy, some analyses might not be entirely cor-rect.
Nonetheless, the overall quality of Alpino?sanalyses is sufficient for corpus studies, and sincesurprisal theory relies largely on corpus features,we believe the small number of (partially) incor-rect analyses should not affect the surprisal esti-mates computed from our PCFG.3.2 Earley-Stolcke ParserTo compute prefix probabilities in our model weimplemented Stolcke?s (1995) probabilistic modi-fication of Earley?s (1970) parsing algorithm.
AnEarley-Stolcke parser is a breadth-first parser.
Ateach point in processing, the parser maintains acollection of states that reflect all possible analy-ses of a partial sentence thus far.
A state is a recordthat keeps track of:(a) the position up to which a sentence has beenprocessed,(b) the grammar rule that is applied,(c) a ?dot position?
indicating which part of therule has been processed thus far, and(d) the leftmost edge of the partial string gener-ated by the rule.1A PCFG can be induced by estimating the relative fre-quency of each CFG rule A?
?
:P (A?
?)
= count(A??)??count(A??
).74The collection of states is constantly expanded bythree operations.
First upcoming structural andlexical material is predicted.
For all predictions,new states are added with the ?dot?
placed onthe leftmost side of the rule.
Then it is deter-mined whether there is a state that predicts the nextword in the input sentence.
If this is the case, anew state is added with the ?dot?
placed right tothe predicted word.
A third operation looks forstates with the ?dot?
rightmost to a grammar rule,and then tries to find states which have the com-pleted state as their leftmost edge.
If such statesare found, the ?dot?
in these states is moved tothe right of this edge.
This step is repeated untilno more new states are added.
These three op-erations are cyclically performed until the entiresentence is processed.
Our grammar contained29 non-terminals that could result in epsilon pro-ductions.
Due to the way epsilon productions arehandled within the Earley-Stolcke parser (i.e., bymeans of ?spontaneous dot shifting?
), having alarge number of epsilon productions leads to alarge number of predicted and completed edges.As a consequence, pursuing all possible analysesmay become computationally infeasible.
To over-come this problem, we modified the Earley-Stol-cke parser with a beam ?.
In prediction and com-pletion, only the ?-number of states with the high-est probabilities are added.2 This constrains thenumber of states generated by the parser and en-forces limited parallelism.4 NP-/S-coordination ambiguities4.1 Frazier?s experimentOur aim was to determine to what extent lexi-calized surprisal theory can account for readingtime data for the NP-/S-coordination ambiguity inDutch.
This type of ambiguity was investigatedby Frazier (1987) using a self-paced reading ex-periment.
The sentences in (2) are part of Fra-zier?s materials.
Sentence (2-a) and (2-b) exem-plify an NP-/S-coordination ambiguity.
The sen-tences are identical and temporarily ambiguous upto the NP haar zusje (her sister).
In (2-a) thisNP is followed by the adverb ook, and thereforedisambiguated to be part of an NP-coordination;Marie and haar zusje are conjoined.
In (2-b), onother hand, the same NP is followed by the verblachte, and therefore disambiguated as the sub-2A similar approach was used in Roark (2001) andFrank (2009).ject of a conjoined sentence; Piet kuste Marie andhaar zusje lachte are conjoined.
(2) a. PietPetekustekissedMarieMarieenandhaarherzusjesisterooktoo(Ambiguous; NP-coordination)b. PietPetekustekissedMarieMarieenandhaarherzusjesisterlachtelaughed(Ambiguous; S-coordination)c. AnnieAnniezagsawhaarherzusjesisterooktoo(Unambiguous; NP-control)d. AnnieAnniezagsawdatthathaarherzusjesisterlachtelaughed(Unambiguous; S-control)Sentence (2-c) and (2-d) functioned as unambigu-ous controls.
These sentences are identical up tothe verb zag.
In (2-c), the verb is followed bythe single NP haar zusje, and subsequently the ad-verb ook.
The adverb eliminates the possibility ofan NP-coordination.
In (2-d), on the other hand,the same verb is followed by the complementizerdat, indicating that the clause her sister laughed isa subordinate clause (the complementizer is oblig-atory in Dutch).Frazier constructed twelve sets consisting offour of such sentences each.
The 48 sentenceswere divided into three frames.
The first frameincluded all the material up to the critical NPhaar zusje in (2).
The second frame contained onlythe critical NP itself, and the third frame containedall the material that followed this NP.40 native Dutch speakers participated in the ex-periment.
Reading times for the final frames werecollected using a self-paced reading task.
Figure 1depicts the mean reading times for each of the fourconditions.Frazier found a significant interaction betweenType of Coordination (NP- versus S-coordination)and Ambiguity (ambiguous versus control) indi-cating that the effect of disambiguation was largerfor S-coordinations (ambiguous: 1596 ms; con-trol: 1141 ms) than for NP-coordinations (ambigu-ous: 1222 ms; control: 1082 ms).4.2 SimulationsWe simulated Frazier?s experiment in our model.Since one set of sentences contained a word thatwas not covered by our lexicon (set 11; ?Lor-raine?
), we used only eleven of the twelve setsof test items from her study.
The remaining 44sentences were successfully analyzed.
In our first75NP?coord/control S?coord/controltype of coordinationmeanreadingtimes(ms)40080012001600ambiguousunambiguousFigure 1: Reading time data for the NP-/S-coordi-nation ambiguity (Frazier, 1987).simulation we fixed a beam of ?
= 16.
Figure 2depicts surprisal values in the sentence-final frameas estimated by our model.
When final framescontained multiple words, we averaged the sur-prisal values for these words.
As Figure 2 shows,NP?coord/control S?coord/controltype of coordinationmeansurprisal500055006000650070007500ambiguousunambiguousFigure 2: Mean surprisal values for the final framein the model (?
= 16).our model successfully replicated the effects re-ported in Frazier (1987): In both types of coordi-nations there was a difference in mean surprisalbetween the ambiguous sentences and the con-trols, but in the S-coordinations this effect waslarger than in the sentences with NP-coordination.Statistical analyses confirmed our findings.
AnANOVA on surprisal values per item revealed aninteraction between Type of Coordination (NP- vs.S-coordination) and Ambiguity (ambiguous vs.control), which was marginally significant (p =0.06), most probably due to the small number ofbeamdifferenceinmeans(NP*?
S*)?600?400?200020032 16 8 4NP?/S?controlNP?/S?coordinationFigure 3: Differences between NP versus S sur-prisal for different beam sizes (?s).items (i.e., 11) available for this statistical test (re-call that the test in the original experiment wasbased on 40 participants).
Follow-up analyses re-vealed that the difference between S-coordinationand S-control was significant (p < 0.05), whereasthe difference between NP-coordination and NP-control was not (p = 0.527).To test the robustness of these findings, we re-peated the simulation with different beam sizes(?s) by iteratively halving the beam, starting with?
= 32.
Figure 3 shows the differences inmean surprisal between NP-coordination and S-coordination, and NP-control and S-control.
Withthe beam set to four (?
= 4), we did not obtain fullanalyses for all test items.
Consequently, two setsof items had to be disregarded (sets 8 and 9).
Forthe remaining items, however, we obtained an NP-coordination preference for all beam sizes.
Thelargest difference occurred for ?
= 16.
Whenthe beam was set to ?
?
8, the difference stabi-lized.
Taking everything into account, the modelwith ?
= 16 led to the best overall match withFrazier?s reading time data.As for the interaction, Figure 4 depicts the dif-ferences in mean surprisal between NP-coordina-tion and NP-control, and S-coordination and S-control.
These results indicate that we robustlyreplicated the interaction between coordinationtype and ambiguity.
For all beam sizes, S-co-ordination benefited more from disambiguationthan NP-coordination, i.e., the difference in meansbetween S-coordination and S-control was larger76beamdifferenceinmeans(*?coord.?
*?control)05001000150032 16 8 4NP?coordination/NP?controlS?coordination/S?controlFigure 4: Differences in coordination versus con-trol surprisal for different beam sizes (?s).than the difference in means between NP-coordi-nation and NP-control.In our simulations, we found that surprisal the-ory can account for reading time data from a clas-sic experiment on the NP-/S-coordination ambigu-ity in Dutch reported by Frazier (1987).
This sug-gests that the interplay between syntactic and lex-ical expectancy might be sufficient to explain anNP-coordination preference in human subjects.
Inthe remainder of this section, we analyze our re-sults and explain how this preference arises in themodel.4.3 Model AnalysisTo determine what caused the NP-preference inour model, we inspected surprisal differencesitem-by-item.
Whether the NP-coordination pref-erence was syntactic or lexical in nature shouldbe reflected in the grammar.
If it was syntactic,NP-coordination would have a higher probabilitythan S-coordination according to our PCFG.
If, onthe other hand, it was lexical, NP- and S-coor-dination should be equally probable syntactically.Another possibility, however, is that syntactic andlexical probabilities interacted.
If this was thecase, we should expect NP-coordinations to leadto lower surprisal values on average only, but notnecessarily on every item.
Figure 5 shows the es-timated surprisal values per sentence-final framefor the ambiguous condition and Figure 6 for theunambiguous condition.
Figure 5 indicates thatalthough NP-coordination led to lower surprisalsentencessurprisals50006000700080001 2 3 4 5 6 7 8 9 10 12NP?coordinationS?coordinationFigure 5: Surprisal per sentence for final frames inthe ambiguous condition.sentencessurprisals5000600070001 2 3 4 5 6 7 8 9 10 12NP?controlS?controlFigure 6: Surprisal per sentence for final frames inthe unambiguous condition.overall (see Figure 2), this was not the case for alltested items.
A similar pattern was found for theNP-control versus S-control items in Figure 6.
S-controls led to lower surprisal overall, but not forall items.
Manual inspection of the grammar re-vealed a bias towards NP-coordination.
A total of115 PCFG rules concerned coordination (?
18%of the entire grammar).
As these rules expandedthe same grammatical category, their probabilitiessummed to 1.
A rule-by-rule inspection showedthat approximately 48% of the probability masswas assigned to rules that dealt with NP-coordi-nations, 22% to rules that dealt with S-coordina-tions, and the remaining 30% to rules that dealtwith coordination in other structures.
In other77words, there was a clear preference for NP-coordi-nation in the grammar.
Despite this bias, for sometested items the S-coordination received lower sur-prisal than the NP-coordination (Figure 5).
In-dividual NP-coordination rules might have lowerprobability than individual S-coordination rules,so the overall preference for NP-coordination inthe grammar therefore does not have to be re-flected in every test item.
Secondly, syntacticprobabilities could be modified by lexical proba-bilities.
Suppose for a pair of test items that NP-coordination was syntactically preferred over S-coordination.
If the sentence was disambiguatedas an NP-coordination by a highly improbable lex-ical item, and disambiguated as an S-coordinationby a highly probable lexical item, surprisal for theNP-coordination might turn out higher than sur-prisal for the S-coordination.
In this way, lexicalfactors could override the NP-coordination bias inthe grammar, leading to a preference for S-coordi-nation in some items.To summarize, the PCFG displayed an over-all NP-coordination preference when surprisal wasaveraged over the test sentences and this result isconsistent with the findings of Frazier (1987).
TheNP-coordination preference, however, was not in-variably reflected on an item-by-item basis.
SomeS-coordinations showed lower surprisal than thecorresponding NP-coordinations.
This reversal ofprocessing difficulty can be explained in terms ofdifferences in individual rules, and in terms of in-teractions between syntactic and lexical probabil-ities.
This suggests that specific lexical expecta-tions might have a much stronger effect on disam-biguation preferences than supposed by the min-imal attachment principle.
Unfortunately, Frazier(1987) only reported mean reading times for thetwo coordination types.3 It would be interesting tocompare the predictions from our surprisal modelwith human data item-by-item in order to validatethe magnitude of lexical effects we found in themodel.5 DiscussionIn this paper we have shown that a model of lex-icalized surprisal, based on an automatically in-duced PCFG, can account for the NP-/S-ambiguityreading time data of Frazier (1987).
We found3Thus it was not possible to determine the strength of thecorrelation between reading times in Frazier?s study and sur-prisal in our model.these results to be robust for a critical model pa-rameter (beam size), which suggests that syntac-tic processing in human comprehension might bebased on limited parallelism only.
Surprisal the-ory models processing difficulty on a word level.A word?s difficulty is related to the expectationsthe language processor forms, given the structuraland lexical material that precedes it.
The modelshowed a clear preference for NP-coordinationwhich suggests that structural and lexical expec-tations as estimated from a corpus might be suffi-cient to explain the NP-coordination bias in humansentence processing.Our account of this bias differs considerablyfrom the original account proposed by Frazier(minimal attachment principle) in a number ofways.
Frazier?s explanation is based on a met-ric of syntactic complexity which in turn dependson quite specific syntactic representations of alanguage?s phrase structure.
Surprisal theory, onthe other hand, is largely neutral with respect tothe form syntactic representations take in the hu-man mind.4 Moreover, differential processing insurprisal-based models does not require the speci-fication of a notion of syntactic complexity.
Boththese aspects make surprisal theory a parsimo-nious explanatory framework.
The minimal at-tachment principle postulates that the bias towardsNP-coordination is an initial processing primitive.In contrast, the bias in our simulations is a func-tion of the model?s input history and linguisticexperience from which the grammar is induced.It is further modulated by the immediate contextfrom which upcoming words are predicted dur-ing processing.
Consequently, the model?s prefer-ence for one structural type can vary across sen-tence tokens and even be reversed on occasion.We argued that our grammar showed an over-all preference for NP-coordination but this pref-erence was not necessarily reflected on each andevery rule that dealt with coordinations.
Some S-coordination rules could have higher probabilitythan NP-coordination rules.
In addition, syntac-tic expectations were modified by lexical expec-tations.
Thus, even when NP-coordination wasstructurally favored over S-coordination, highlyunexpected lexical material could lead to moreprocessing difficulty for NP-coordination than for4This is not to say, of course, that the choice of languagemodel to estimate surprisal is completely irrelevant; differ-ent models will yield different degrees of fit, see Frank andBod (2010).78S-coordination.
Surprisal theory allows us to builda formally precise computational model of read-ing time data which generates testable, quantita-tive predictions about the differential processingof individual test items.
These predictions (Figure5) indicate that mean reading times for a set of NP-/S-coordination sentences may not be adequate totap the origin of differential processing difficulty.Our results are consistent with the findings ofHoeks et al (2002), who also found evidencefor an NP-coordination preference in a self-pacedreading experiment as well as in an eye-trackingexperiment.
They suggested that NP-coordinationmight be easier to process because it has a sim-pler topic structure than S-coordination.
The for-mer only has one topic, whereas the latter has two.Hoeks et al (2002) argue that having more thanone topic is unexpected.
Sentences with more thanone topic will therefore cause more processing dif-ficulty.
This preference for simple topic-structurethat was evident in language comprehension mayalso be present in language production, and hencein language corpora.
Thus, it may very well bethe case that the NP-coordination preference thatwas present in our training corpus may have hada pragmatic origin related to topic-structure.
Theoutcome of our surprisal model is also compati-ble with the results of Hoeks et al (2006) whofound that thematic information can strongly re-duce but not completely eliminate the NP-coordi-nation preference.
Surprisal theory is explicitlybuilt on the assumption that multiple sources ofinformation can interact in parallel at any point intime during sentence processing.
Accordingly, wesuggest here that the residual preference for NP-coordination found in the study of Hoeks et al(2006) might be explained in terms of syntacticand lexical expectation.
And finally, our approachis consistent with a large body of evidence indi-cating that language comprehension is incremen-tal and makes use of expectation-driven word pre-diction (Pickering and Garrod, 2007).
It remainsto be tested whether our model can explain behav-ioral data from the processing of ambiguities otherthan the Dutch NP- versus S-coordination case.ReferencesG.
Altmann and Y. Kamide.
1999.
Incremental inter-pretation at verbs: Restricting the domain of subse-quent reference.
Cognition, 73:247?264.F.
Attneave.
1959.
Applications of Information Theoryto Psychology: A summary of basic concepts, meth-ods, and results.
Holt, Rinehart and Winston.E.
Charniak.
1993.
Statistical Language Learning.MIT Press.V.
Demberg and F. Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109:193?210.V.
Demberg and F. Keller.
2009.
A computationalmodel of prediction in human parsing: Unifying lo-cality and surprisal effects.
In Proceedings of the31st Annual Conference of the Cognitive Science So-ciety, Amsterdam, the Netherlands.J.
Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 6:451?455.S.
Frank and R. Bod.
2010.
The irrelevance of hi-erarchical structure to human sentence processing.Unpublished manuscript.S.
Frank.
2009.
Surprisal-based comparison between asymbolic and a connectionist model of sentence pro-cessing.
In Proceedings of the 31th Annual Confer-ence of the Cognitive Science Society, pages 1139?1144, Amsterdam, the Netherlands.L.
Frazier.
1987.
Syntactic processing: Evidence fromDutch.
Natural Langauge and Linguistic Theory,5:519?559.J.
Hale.
2001.
A probabilistic Earley parser as a psy-cholinguistic model.
In Proceedings of the 2nd Con-ference of the North American Chapter of the As-sociation for Computational Linguistics, volume 2,pages 159?166.J.
Hoeks, W. Vonk, and H. Schriefers.
2002.
Process-ing coordinated structures in context: The effect oftopic-structure on ambiguity resolution.
Journal ofMemory and Language, 46:99?119.J.
Hoeks, P. Hendriks, W. Vonk, C. Brown, and P. Ha-goort.
2006.
Processing the noun phrase versus sen-tence coordination ambiguity: Thematic informa-tion does not completely eliminate processing dif-ficulty.
The Quarterly Journal of Experimental Psy-chology, 59:1581?1599.D.
Jurafsky.
1996.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience, 20:137?147.R.
Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106:1126?1177.M.
Nederhof, A. Sarkar, and G. Satta.
1998.
Prefixprobabilities from stochastic tree adjoining gram-mar.
In Proceedings of COLING-ACL ?98, pages953?959, Montreal.M.
Otten, M. Nieuwland, and J. van Berkum.
2007.Great expectations: Specific lexical anticipation in-fluences the processing of spoken language.
BMCNeuroscience.79M.
Pickering and S. Garrod.
2007.
Do people use lan-guage production to make predictions during com-prehension?
Trends in Cognitive Sciences, 11:105?110.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27:249?276.N.
Smith and R. Levy.
2008.
Optimal processing timesin reading: A formal model and empirical investi-gation.
In Proceedings of the 30th annual confer-ence of the Cognitive Science Society, pages 595?600, Austin, TX.A.
Stolcke.
1995.
An efficient probabilistic context-free parsing algorithm that computes prefix proba-bilities.
Computational linguistics, 21:165?201.J.
van Berkum, C. Brown, P. Zwitserlood, V. Kooij-man, and P. Hagoort.
2005.
Anticipating upcom-ing words in discourse: Evidence from ERPs andreading times.
Journal of Experimental Psychology:Learning, Memory, and Cognition, 31:443?467.G.
van Noord.
2006.
At last parsing is now op-erational.
In Verbum Ex Machina.
Actes de la13e confe?rence sur le traitement automatique deslangues naturelles, pages 20?42.
Presses universi-taires de Louvain.80
