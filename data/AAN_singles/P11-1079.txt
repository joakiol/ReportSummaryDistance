Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 783?792,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsBootstrapping Coreference Resolution Using Word AssociationsHamidreza Kobdani, Hinrich Schu?tze, Michael Schiehlen and Hans KampInstitute for Natural Language ProcessingUniversity of Stuttgartkobdani@ims.uni-stuttgart.deAbstractIn this paper, we present an unsupervisedframework that bootstraps a complete corefer-ence resolution (CoRe) system from word as-sociations mined from a large unlabeled cor-pus.
We show that word associations are use-ful for CoRe ?
e.g., the strong association be-tween Obama and President is an indicatorof likely coreference.
Association informationhas so far not been used in CoRe because it issparse and difficult to learn from small labeledcorpora.
Since unlabeled text is readily avail-able, our unsupervised approach addresses thesparseness problem.
In a self-training frame-work, we train a decision tree on a corpus thatis automatically labeled using word associa-tions.
We show that this unsupervised systemhas better CoRe performance than other learn-ing approaches that do not use manually la-beled data.1 IntroductionCoreference resolution (CoRe) is the process of find-ing markables (noun phrases) referring to the samereal world entity or concept.
Until recently, most ap-proaches tried to solve the problem by binary classi-fication, where the probability of a pair of markablesbeing coreferent is estimated from labeled data.
Al-ternatively, a model that determines whether a mark-able is coreferent with a preceding cluster can beused.
For both pair-based and cluster-based models,a well established feature model plays an importantrole.
Typical systems use a rich feature space basedon lexical, syntactic and semantic knowledge.
Mostcommonly used features are described by Soon et al(2001).Most existing systems are supervised systems,trained on human-labeled benchmark data sets forEnglish.
These systems use linguistic features basedon number, gender, person etc.
It is a challenge toadapt these systems to new domains, genres and lan-guages because a significant human labeling effort isusually necessary to get good performance.To address this challenge, we pursue an unsuper-vised self-training approach.
We train a classifieron a corpus that is automatically labeled using asso-ciation information.
Self-training approaches usu-ally include the use of some manually labeled data.In contrast, our self-trained system is not trained onany manually labeled data and is therefore a com-pletely unsupervised system.
Although training onautomatically labeled data can be viewed as a formof supervision, we reserve the term supervised sys-tem for systems that are trained on manually labeleddata.The key novelty of our approach is that we boot-strap a competitive CoRe system from associationinformation that is mined from an unlabeled cor-pus in a completely unsupervised fashion.
Whilethis method is shallow, it provides valuable informa-tion for CoRe because it considers the actual iden-tity of the words in question.
Consider the pair ofmarkables (Obama, President).
It is a likely coref-erence pair, but this information is not accessibleto standard CoRe systems because they only usestring-based features (often called lexical features),named entity features and semantic word class fea-tures (e.g., from WordNet) that do not distinguish,783say, Obama from Hawking.In our approach, word association information isused for clustering markables in unsupervised learn-ing.
Association information is calculated as asso-ciation scores between heads of markables as de-scribed below.
We view association information asan example of a shallow feature space which con-trasts with the rich feature space that is generallyused in CoRe.Our experiments are conducted using theMCORE system (?Modular COreference REso-lution?
).1 MCORE can operate in three differentsettings: unsupervised (subsystem A-INF), super-vised (subsystem SUCRE (Kobdani and Schu?tze,2010)), and self-trained (subsystem UNSEL).
Theunsupervised subsystem A-INF (?AssociationINFormation?)
uses the association scores betweenheads as the distance measure when clusteringmarkables.
SUCRE (?SUpervised CoreferenceREsolution?)
is trained on a labeled corpus(manually or automatically labeled) similar tostandard CoRe systems.
Finally, the unsupervisedself-trained subsystem UNSEL (?UNsupervisedSELf-trained?)
uses the unsupervised subsystemA-INF to automatically label an unlabeled corpusthat is then used as a training set for SUCRE.Our main contributions in this paper are as fol-lows:1.
We demonstrate that word association informa-tion can be used to develop an unsupervisedmodel for shallow coreference resolution (sub-system A-INF).2.
We introduce an unsupervised self-trainedmethod (UNSEL) that takes a two-learner two-feature-space approach.
The two learners areA-INF and SUCRE.
The feature spaces are theshallow and rich feature spaces.3.
We show that the performance of UNSEL isbetter than the performance of other unsuper-vised systems when it is self-trained on the au-tomatically labeled corpus and uses the lever-aging effect of a rich feature space.4.
MCORE is a flexible and modular frameworkthat is able to learn from data with different1MCORE can be downloaded from ifnlp.org/?schuetze/mcore.quality and domain.
Not only is it able to dealwith shallow information spaces (A-INF), butit can also deliver competitive results for richfeature spaces (SUCRE and UNSEL).This paper is organized as follows.
Related workis discussed in Section 2.
In Section 3, we presentour system architecture.
Section 4 describes the ex-periments and Section 5 presents and discusses ourresults.
The final section presents our conclusions.2 Related WorkThere are three main approaches to CoRe: super-vised, semi-supervised (or weakly supervised) andunsupervised.
We use the term semi-supervised forapproaches that use some amount of human-labeledcoreference pairs.Mu?ller et al (2002) used co-training for coref-erence resolution, a semi-supervised method.
Co-training puts features into disjoint subsets whenlearning from labeled and unlabeled data and triesto leverage this split for better performance.
Ng andCardie (2003) use self-training in a multiple-learnerframework and report performance superior to co-training.
They argue that the multiple learner ap-proach is a better choice for CoRe than the multi-ple view approach of co-training.
Our self-trainedmodel combines multiple learners (A-INF and SU-CRE) and multiple views (shallow/rich informa-tion).
A key difference to the work by Mu?ller et al(2002) and Ng and Cardie (2003) is that we do notuse any human-labeled coreference pairs.Our basic idea of self-training without human la-bels is similar to (Kehler et al, 2004), but we ad-dress the general CoRe problem, not just pronouninterpretation.Turning to unsupervised CoRe, Haghighi andKlein (2007) proposed a generative Bayesian modelwith good performance.
Poon and Domingos (2008)introduced an unsupervised system in the frameworkof Markov logic.
Ng (2008) presented a generativemodel that views coreference as an EM clusteringprocess.
We will show that our system, which issimpler than prior work, outperforms these systems.Haghighi and Klein (2010) present an ?almost un-supervised?
CoRe system.
In this paper, we onlycompare with completely unsupervised approaches,784not with approaches that make some limited use oflabeled data.Recent work by Haghighi and Klein (2009), Klen-ner and Ailloud (2009) and Raghunathan et al(2010) challenges the appropriateness of machinelearning methods for CoRe.
These researchers showthat a ?deterministic?
system (essentially a rule-based system) that uses a rich feature space includ-ing lexical, syntactic and semantic features can im-prove CoRe performance.
Almost all CoRe systems,including ours, use a limited number of rules or fil-ters, e.g., to implement binding condition A that re-flexives must have a close antecedent in some senseof ?close?.
In our view, systems that use a few ba-sic filters are fundamentally different from carefullytuned systems with a large number of complex rules,some of which use specific lexical information.
Alimitation of complex rule-based systems is that theyrequire substantial effort to encode the large numberof deterministic constraints that guarantee good per-formance.
Moreover, these systems are not adapt-able (since they are not machine-learned) and mayhave to be rewritten for each new domain, genreand language.
Consequently, we do not compare ourperformance with deterministic systems.Ponzetto (2010) extracts metadata fromWikipedia for supervised CoRe.
Using suchadditional resources in our unsupervised systemshould further improve CoRe performance.
Elsneret al (2009) present an unsupervised algorithmfor identifying clusters of entities that belong tothe same named entity (NE) class.
Determiningcommon membership in an NE class like person isan easier task than determining coreference of twoNEs.3 System ArchitectureFigure 1 illustrates the system architecture of ourunsupervised self-trained CoRe system (UNSEL).Oval nodes are data, box nodes are processes.
Wetake a self-training approach to coreference resolu-tion: We first label the corpus using the unsuper-vised model A-INF and then train the supervisedmodel SUCRE on this automatically labeled train-ing corpus.
Even though we train on a labeled cor-pus, the labeling of the corpus is produced in a com-pletely automatic fashion, without recourse to hu-Unlabeled DataUnsupervised Model (A-INF)Automatically Labeled DataSupervised Model (SUCRE)Figure 1: System Architecture of UNSEL (UnsupervisedSelf-Trained Model).man labeling.
Thus, it is an unsupervised approach.The MCORE architecture is very flexible; in par-ticular, as will be explained presently, it can be eas-ily adapted for supervised as well as unsupervisedsettings.The unsupervised and supervised models have anidentical top level architecture; we illustrate this inFigure 2.
In preprocessing, tokens (words), mark-ables and their attributes are extracted from the inputtext.
The key difference between the unsupervisedand supervised approaches is in how pair estimationis accomplished ?
see Sections 3.1 & 3.2 for de-tails.The main task in chain estimation is clustering.Figure 3 presents our clustering method, which isused for both supervised and unsupervised CoRe.We search for the best predicted antecedent (withcoreference probability p ?
0.5) from right to leftstarting from the end of the document.
McEnery etal.
(1997) showed that in 98.68% of cases the an-tecedent is within a 10-sentence window; hence weuse a window of 10 sentences for search.
We havefound that limiting the search to a window increasesboth efficiency and effectiveness.Filtering.
We use a feature definition languageto define the templates according to which the fil-ters and features are calculated.
These templatesare hard constraints that filter out all cases that areclearly disreferent, e.g., (he, she) or (he, they).
Weuse the following filters: (i) the antecedent of a re-flexive pronoun must be in the same sentence; (ii)the antecedent of a pronoun must occur at a distanceof at most 3 sentences; (iii) a coreferent pair of anoun and a pronoun or of two pronouns must not785Input Text Preprocessing Markables Pair EstimationMarkable Chains Chain Estimation Markable PairsFigure 2: Common architecture of unsupervised (A-INF) and supervised (SUCRE) models.Chain Estimation (M1, M2, .
.
.
, Mn)1. t?
12.
For each markable Mi: Ci ?
{Mi}3.
Proceed through the markables from the endof the document.
For each Mj , consider eachpreceding Mi within 10 sentences:If Pair Estimation(Mi, Mj)>=t: Ci ?
Ci?Cj4.
t?
t?
0.015.
If t >= 0.5: go to step 3Pair Estimation (Mi, Mj):If Filtering(Mi, Mj)==FALSE then return 0;else return the probability p (or associationscore N ) of markable pair (Mi, Mj) beingcoreferent.Filtering (Mi, Mj):return TRUE if all filters for (Mi, Mj) areTRUE else FALSEFigure 3: MCORE chain estimation (clustering) algo-rithm (test).
t is the clustering threshold.
Ci refers tothe cluster that Mi is a member of.disagree in number; (iv) a coreferent pair of two pro-nouns must not disagree in gender.
These four filtersare used in supervised and unsupervised modes ofMCORE.3.1 Unsupervised Model (A-INF)Figure 4 (top) shows how A-INF performs pair esti-mation.
First, in the pair generation step, all possiblepairs inside 10 sentences are generated.
Other stepsare separately explained for train and test as follows.Train.
In addition to the filters (i)?
(iv) describedabove, we use the following filter: (v) If the headof markable M2 matches the head of the precedingmarkable M1, then we ignore all other pairs for M2in the calculation of association scores.This additional filter is necessary because an ap-proach without some kind of string matching con-straint yields poor results, given the importance ofstring matching for CoRe.
As we will show below,even the simple filters (i)?
(v) are sufficient to learnhigh-quality association scores; this means that wedo not need the complex features of ?determinis-tic?
systems.
However, if such complex features areavailable, then we can use them to improve perfor-mance in our self-trained setting.To learn word association information from anunlabeled corpus (see Section 4), we compute mu-tual information (MI) scores between heads of mark-ables.
We defineMI as follows: (Cover and Thomas,1991)MI(a, b) =?i?{a?,a}?j?
{b?,b}P (i, j) log2P (i, j)P (i)P (j)E.g., P (a, b?)
is the probability of a pair whose twoelements are a and a word not equal to b.Test.
A key virtue of our approach is that in theclassification of pairs as coreferent/disreferent, thecoreference probability p estimated in supervisedlearning plays exactly the same role as the associ-ation information score N (defined below).
For p, itis important that we only consider pairs with p ?
0.5as potentially coreferent (see Figure 3).
To be able toimpose the same constraint on N , we normalize theMI scores by the maximum values of the two wordsand take the average:N(a, b) =12(MI(a, b)argmaxxMI(a, x)+MI(a, b)argmaxxMI(x, b))In the above equation, the value of N indicates howstrongly two words are associated.
N is normalizedto ensure 0 ?
N ?
1.
If a or b did not occur, thenwe set N =0.In filtering for test, we use filters (i)?(iv).
We thenfetch the MI values and calculate N values.
Theclustering algorithm described in Figure 3 uses theseN values in exactly the same way as p: we search forthe antecedent with the maximum association score786N greater than 0.5 from right to left starting fromthe end of the document.As we will see below, using N scores acquiredfrom an unlabeled corpus as the only source of in-formation for CoRe performs surprising well.
How-ever, the weaknesses of this approach are (i) the fail-ure to cover pairs that do not occur in the unlabeledcorpus (negatively affecting recall) and (ii) the gen-eration of pairs that are not plausible candidates forcoreference (negatively affecting precision).
To ad-dress these problems, we train a model on a corpuslabeled by A-INF in a self-training approach.3.2 Supervised Model (SUCRE)Figure 4 (bottom) presents the architecture of pairestimation for the supervised approach (SUCRE).In the pair generation step for train, we take eachcoreferent markable pair (Mi, Mj) without inter-vening coreferent markables and use (Mi, Mj) as apositive training instance and (Mi, Mk), i < k < j,as negative training instances.
For test, we generateall possible pairs within 10 sentences.
After filter-ing, we then calculate a feature vector for each gen-erated pair that survived filters (i)?
(iv).Our basic features are similar to those describedby Soon et al (2001): string-based features, dis-tance features, span features, part-of-speech fea-tures, grammatical features, semantic features, andagreement features.
These basic features are engi-neered with the goal of creating a feature set thatwill result in good performance.
For this purposewe used the relational feature engineering frame-work which has been presented in (Kobdani et al,2010).
It includes powerful and flexible methods forimplementing and extracting new features.
It allowssystematic and fast search of the space of featuresand thereby reduces the time and effort needed fordefining optimal features.
We believe that the goodperformance of our supervised system SUCRE (ta-bles 1 and 2) is the result of our feature engineeringapproach.2As our classification method, we use a decision2While this is not the focus of this paper, SUCRE has per-formance comparable to other state-of-the-art supervised sys-tems.
E.g., B3/MUC F1 are 75.6/72.4 on ACE-2 and 69.4/70.6on MUC-6 compared to 78.3/66.0 on ACE-2 and 70.9/68.5 onMUC-6 for Reconcile (Stoyanov et al, 2010)tree3 (Quinlan, 1993) that is trained on the trainingset to estimate the coreference probability p for apair and then applied to the test set.
Note that, asis standard in CoRe, filtering and feature calculationare exactly the same for training and test, but thatpair generation is different as described above.4 Experimental Setup4.1 Data SetsFor computing word association, we used a cor-pus of about 63,000 documents from the 2009 En-glish Wikipedia (the articles that were larger than200 bytes).
This corpus consists of more than 33.8million tokens; the average document length is 500tokens.
The corpus was parsed using the Berkeleyparser (Petrov and Klein, 2007).
We ignored all sen-tences that had no parse output.
The number of de-tected markables (all noun phrases extracted fromparse trees) is about 9 million.We evaluate unsupervised, supervised and self-trained models on ACE (Phase 2) (Mitchell et al,2003).4 This data set is one of the most widelyused CoRe benchmarks and was used by the sys-tems that are most comparable to our approach; inparticular, it was used in most prior work on unsu-pervised CoRe.
The corpus is composed of threedata sets from three different news sources.
We givethe number of test documents for each: (i) BroadcastNews (BNEWS): 51.
(ii) Newspaper (NPAPER):17.
(iii) Newswire (NWIRE): 29.
We report re-sults for true markables (markables extracted fromthe answer keys) to be able to compare with othersystems that use true markables.In addition, we use the recently publishedOntoNotes benchmark (Recasens et al, 2010).OntoNotes is an excerpt of news from the OntoNotesCorpus Release 2.0 (Pradhan et al, 2007).
The ad-vantage of OntoNotes is that it contains two parallelannotations: (i) a gold setting, gold standard manualannotations of the preprocessing information and (ii)an automatic setting, automatically predicted anno-tations of the preprocessing information.
The au-tomatic setting reflects the situation a CoRe system3We also tried support vector machines and maximum en-tropy models, but they did not perform better.4We used two variants of ACE (Phase 2): ACE-2 andACE2003787Markable Pairs Filtering Association CalculationPair Generation Filter Templates Association Information Train/TestMarkable Pairs Filtering Feature Calculation Feature VectorsPair Generation Filter Templates Feature Templates Train/TestFigure 4: Pair estimation in the unsupervised model A-INF (top) and in the supervised model SUCRE (bottom).faces in reality; in contrast, the gold setting shouldbe considered less realistic.The issue of gold vs. automatic setting is directlyrelated to a second important evaluation issue: theinfluence of markable detection on CoRe evaluationmeasures.
In a real application, we do not have ac-cess to true markables, so an evaluation on systemmarkables (markables automatically detected by thesystem) reflects actual expected performance better.However, reporting only CoRe numbers (even forsystem markables) is not sufficient either since ac-curacy of markable detection is necessary to inter-pret CoRe scores.
Thus, we need (i) measures ofthe quality of system markables (i.e., an evaluationof the markable detection subtask) and CoRe per-formance on system markables as well as (ii) a mea-sure of CoRe performance on true markables.
Weuse OntoNotes in this paper to perform such a, inour view, complete and realistic evaluation of CoRe.The two evaluations correspond to the two evalua-tions performed at SemEval-2010 (Recasens et al,2010): the automatic setting with system markablesand the gold setting with true markables.
Test setsize is 85 documents.In the experiments with A-INF we use Wikipediato compute association information and then evalu-ate the model on the test sets of ACE and OntoNotes.For the experiments with UNSEL, we use its unsu-pervised subsystem A-INF (which uses Wikipediaassociation scores) to automatically label the train-ing sets of ACE and OntoNotes.
Then for each dataset, the supervised subsystem of UNSEL (i.e., SU-CRE) is trained on its automatically labeled trainingset and then evaluated on its test set.
Finally, forthe supervised experiments, we use the manually la-beled training sets and evaluate on the correspondingtest sets.4.2 Evaluation MetricsWe report recall, precision, and F1 for MUC (Vilainet al, 1995), B3 (Bagga and Baldwin, 1998), andCEAF (Luo, 2005).
We selected these three met-rics because a single metric is often misleading andbecause we need to use metrics that were used inprevious unsupervised work.It is well known that MUC by itself is insuffi-cient because it gives misleadingly high scores to the?single-chain?
system that puts all markables intoone chain (Luo et al, 2004; Finkel and Manning,2008).
However, B3 and CEAF have a differentbias: they give high scores to the ?all-singletons?system that puts each markable in a separate chain.On OntoNotes test, we get B3 = 83.2 and CEAF= 71.2 for all-singletons, which incorrectly sug-gests that performance is good; but MUC F1 is 0 inthis case, demonstrating that all-singletons performspoorly.
With the goal of performing a complete eval-uation, one that punishes all-singletons as well assingle-chain, we use one of the following two com-binations: (i) MUC and B3 or (ii) MUC and CEAF.Recasens et al (2010) showed that B3 and CEAFare highly correlated (Pearson?s r = 0.91).
There-fore, either combination (i) or combination (ii) fairlycharacterizes CoRe performance.5 Results and DiscussionTable 1 compares our unsupervised self-trainedmodel UNSEL and unsupervised model A-INF to788MUC B3 CEAFBNEWS-ACE-2 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F11 P&D 68.3 66.6 67.4 70.3 65.3 67.7 ?
?
?2 A-INF 60.8 61.4 61.1 55.5 69.0 61.5 52.6 52.0 52.33 UNSEL 72.5 65.6 68.9 72.5 66.4 69.3 56.7 64.8 60.54 SUCRE 86.6 60.3 71.0 87.6 64.6 74.4 56.1 81.6 66.5NWIRE-ACE-2 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F15 P&D 67.7 67.3 67.4 74.7 68.8 71.6 ?
?
?6 A-INF 62.4 57.4 59.8 59.2 62.4 60.7 46.8 52.5 49.57 UNSEL 76.2 61.5 68.1 81.5 67.6 73.9 61.5 77.1 68.48 SUCRE 82.5 65.7 73.1 85.4 72.3 78.3 63.5 80.6 71.0NPAPER-ACE-2 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F19 P&D 69.2 71.7 70.4 70.0 66.5 68.2 ?
?
?10 A-INF 60.6 56.0 58.2 52.4 60.3 56.0 38.9 44.0 41.311 UNSEL 78.6 65.7 71.6 74.0 68.0 70.9 57.6 73.2 64.512 SUCRE 82.5 67.0 73.9 80.7 69.5 74.6 58.8 77.1 66.7BNEWS-ACE2003 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F113 H&K 68.3 56.8 62.0 ?
?
?
59.9 53.9 56.714 Ng 71.4 56.1 62.8 ?
?
?
60.5 53.3 56.715 A-INF 60.9 64.9 62.8 50.9 72.5 59.8 53.8 49.4 51.516 UNSEL 69.5 65.0 67.1 70.2 65.9 68.0 58.5 64.2 61.217 SUCRE 73.9 68.5 71.1 75.4 69.6 72.4 60.1 66.6 63.2NWIRE-ACE2003 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F118 H&K 66.2 46.8 54.8 ?
?
?
62.8 49.6 55.419 Ng 68.3 47.0 55.7 ?
?
?
60.7 49.2 54.420 A-INF 62.7 60.5 61.6 54.8 66.1 59.9 47.7 50.2 49.021 UNSEL 64.8 68.6 66.6 61.5 73.6 67.0 59.8 55.1 57.322 SUCRE 77.6 69.3 73.2 78.8 75.2 76.9 65.1 74.4 69.5Table 1: Scores for MCORE (A-INF, SUCRE and UNSEL) and three comparable systems on ACE-2 and ACE2003.P&D (Poon and Domingos, 2008) on ACE-2; andto Ng (Ng, 2008) and H&K5 (Haghighi and Klein,2007) on ACE2003.
To our knowledge, these threepapers are the best and most recent evaluation resultsfor unsupervised learning and they all report resultson ACE-2 and ACE-2003.
Results on SUCRE willbe discussed later in this section.A-INF scores are below some of the earlier unsu-pervised work reported in the literature (lines 2, 6,10) although they are close to competitive on twoof the datasets (lines 15 and 20: MUC scores areequal or better, CEAF scores are worse).
Given thesimplicity of A-INF, which uses nothing but asso-5We report numbers for the better performing Pronoun-onlySalience variant of H&K proposed by Ng (2008).ciations mined from a large unannotated corpus, itsperformance is surprisingly good.Turning to UNSEL, we see that F1 is always bet-ter for UNSEL than for A-INF, for all three mea-sures (lines 3 vs 2, 7 vs 6, 11 vs 10, 16 vs 15, 21vs 20).
This demonstrates that the self-training stepof UNSEL is able to correct many of the errors thatA-INF commits.
Both precision and recall are im-proved with two exceptions: recall of B3 decreasesfrom line 2 to 3 and from 15 to 16.When comparing the unsupervised system UN-SEL to previous unsupervised results, we find thatUNSEL?s F1 is higher in all runs (lines 3 vs 1, 7 vs5, 11 vs 9, 16 vs 13&14, 21 vs 18&19).
The differ-ences are large (up to 11%) compared to H&K and789Ng.
The difference to P&D is smaller, ranging from2.7% (B3, lines 11 vs 9) to 0.7% (MUC, lines 7 vs5).
Given that MCORE is a simpler and more ef-ficient system than this prior work on unsupervisedCoRe, these results are promising.In contrast to F1, there is no consistent trend forprecision and recall.
For example, P&D is betterthan UNSEL on MUC recall for BNEWS-ACE-2(lines 1 vs 3) and H&K is better than UNSEL onCEAF precision for NWIRE-ACE2003 (lines 18 vs21).
But this higher variability for precision and re-call is to be expected since every system trades thetwo measures off differently.These results show that the application of self-training significantly improves performance.
As dis-cussed in Section 3.1, self-training has positive ef-fects on both recall and precision.
We now presenttwo simplified examples that illustrate this point.Example for recall.
Consider the markable pair(Novoselov6,he) in the test set.
Its N score is 0 be-cause our subset of 2009 Wikipedia sentences hasno occurrence of Novoselov.
However, A-INF findsmany similar pairs like (Einstein,he) and (Hawk-ing,he), pairs that have high N scores.
Supposewe represent pairs using the following five fea-tures: <sentence distance, string match, type offirst markable, type of second markable, numberagreement>.
Then (Einstein,he), (Hawking,he) and(Novoselov,he) will all be assigned the feature vector<1, No, Proper Noun, Personal Pronoun, Yes>.
Wecan now automatically label Wikipedia using A-INF?
this will label (Einstein,he) and (Hawking,he) ascoreferent ?
and train SUCRE on the resulting train-ing set.
SUCRE can then resolve the coreference(Novoselov,he) correctly.
We call this the better re-call effect.Example for precision.
Using the same repre-sentation of pairs, suppose that for the sequence ofmarkables Biden, Obama, President the markablepairs (Biden,President) and (Obama,President) areassigned the feature vectors <8, No, Proper Noun,Proper Noun, Yes> and <1, No, Proper Noun,Proper Noun, Yes>, respectively.
Since both pairshave N scores > 0.5, A-INF incorrectly puts thethree markables into one cluster.
But as we wouldexpect, A-INF labels many more markable pairs6The 2010 physics Nobel laureate.1020304050607080100  20000  40000  60000Prec.,Rec.
andF1Number of input Wikipedia articlesMUC-Prec.MUC-Rec.MUC-F1Figure 5: MUC learning curve for A-INF.with the second feature vector (distance=1) as coref-erent than with the first one (distance=8) in the en-tire automatically labeled training set.
If we nowtrain SUCRE on this training set, it can resolve suchcases in the test set correctly even though they areso similar: (Biden,President) is classified as disref-erent and (Obama,President) as coreferent.
We callthis the better precision effect.Recall that UNSEL has better recall and precisionthan A-INF in almost all cases (discussion of Ta-ble 1).
This result shows that better precision andbetter recall effects do indeed benefit UNSEL.To summarize, the advantages of our self-trainingapproach are: (i) We cover cases that do not occurin the unlabeled corpus (better recall effect); and (ii)we use the leveraging effect of a rich feature spaceincluding distance, person, number, gender etc.
toimprove precision (better precision effect).Learning curve.
Figure 5 presents MUC scoresof A-INF as a function of the number of Wikipediaarticles used in unsupervised learning.
We can seethat a small number of input articles (e.g., 100) re-sults in low recall and high precision.
When we in-crease the number of input articles, recall rapidly in-creases and precision rapidly decreases up to about10,000 articles.
Increase and decrease continuemore slowly after that.
F1 increases throughout be-cause lower precision is compensated by higher re-call.
This learning curve demonstrates the impor-tance of the size of the corpus for A-INF.Comparison of UNSEL with SUCRETable 2 compares our unsupervised self-trained(UNSEL) and supervised (SUCRE) models withthe recently published SemEval-2010 OntoNotes re-790Gold setting + True markablesSystem MD MUC B3 CEAFRelax 100 33.7 84.5 75.6SUCRE2010 100 60.8 82.4 74.3SUCRE 100 64.3 87.0 80.1UNSEL 100 63.0 86.9 79.7Automatic setting + System markablesSystem MD MUC B3 CEAFSUCRE2010 80.7 52.5 67.1 62.7Tanl-1 73.9 24.6 61.3 57.3SUCRE 80.9 55.7 69.7 66.6UNSEL 80.9 55.0 69.8 66.3Table 2: F1 scores for MCORE (SUCRE and UNSEL)and the best comparable systems in SemEval-2010.
MD:Markable Detection F1 (Recasens et al, 2010).sults (gold and automatic settings).
We comparewith the scores of the two best systems, Relax andSUCRE20107 (for the gold setting with true mark-ables) and SUCRE2010 and Tanl-1 (for the automaticsetting with system markables, 89.9% markable de-tection (MD) F1).
It is apparent from this table thatour supervised and unsupervised self-trained mod-els outperform Relax, SUCRE2010 and Tanl-1.
Weshould make clear that we did not use the test set fordevelopment to ensure a fair comparison with theparticipant systems at SemEval-2010.Table 1 shows that the unsupervised self-trainedsystem (UNSEL) does a lot worse than the su-pervised system (SUCRE) on ACE.8 In contrast,UNSEL performs almost as well as SUCRE onOntoNotes (Table 2), for both gold and automaticsettings: F1 differences range from +.1 (Auto-matic, B3) to ?1.3 (Gold, MUC).
We suspect thatthis is partly due to the much higher proportionof singletons in OntoNotes than in ACE-2: 85.2%(OntoNotes) vs. 60.2% (ACE-2).
The low recall ofthe automatic labeling by A-INF introduces a biasfor singletons when UNSEL is self-trained.
Anotherreason is that the OntoNotes training set is about4 times larger than each of BNEWS, NWIRE and7It is the first version of our supervised system that took partin SemEval-2010.
We call it SUCRE2010.8A reviewer observes that SUCRE?s performance is betterthan the supervised system of Ng (2008).
This may indicatethat part of our improved unsupervised performance in Table 1is due to better feature engineering implemented in SUCRE.NPAPER training sets.
With more training data,UNSEL can correct more of its precision and re-call errors.
For an unsupervised approach, whichonly needs unlabeled data, there is little cost to cre-ating large training sets.
Thus, this comparison ofACE-2/Ontonotes results is evidence that in a realis-tic scenario using association information in an un-supervised self-trained system is almost as good asa system trained on manually labeled data.It is important to note that the comparison ofSUCRE to UNSEL is the most direct comparisonof supervised and unsupervised CoRe learning weare aware of.
The two systems are identical with thesingle exception that they are trained on manual vs.automatic coreference labels.6 ConclusionIn this paper, we have demonstrated the utility ofassociation information for coreference resolution.We first developed a simple unsupervised model forshallow CoRe that only uses association informationfor finding coreference chains.
We then introducedan unsupervised self-trained approach where a su-pervised model is trained on a corpus that was auto-matically labeled by the unsupervised model basedon the association information.
The results of the ex-periments indicate that the performance of the unsu-pervised self-trained approach is better than the per-formance of other unsupervised learning systems.
Inaddition, we showed that our system is a flexible andmodular framework that is able to learn from datawith different quality (perfect vs noisy markable de-tection) and domain; and is able to deliver good re-sults for shallow information spaces and competitiveresults for rich feature spaces.
Finally, our frame-work is the first CoRe system that is designed to sup-port three major modes of machine learning equallywell: supervised, self-trained and unsupervised.AcknowledgmentsThis research was funded by DFG (grant SCHU2246/4).We thank Aoife Cahill, Alexander Fraser, ThomasMu?ller and the anonymous reviewers for their help-ful comments.791ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In LREC Workshop onLinguistics Coreference ?98, pages 563?566.Thomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
Wiley.Micha Elsner, Eugene Charniak, and Mark Johnson.2009.
Structured generative models for unsupervisednamed-entity clustering.
In HLT-NAACL ?09, pages164?172.Jenny Rose Finkel and Christopher D. Manning.
2008.Enforcing transitivity in coreference resolution.
InHLT ?08, pages 45?48.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In ACL ?07, pages 848?855.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.
InEMNLP ?09, pages 1152?1161.Aria Haghighi and Dan Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In NAACL-HLT ?10, pages 385?393.Andrew Kehler, Douglas E. Appelt, Lara Taylor, andAleksandr Simma.
2004.
Competitive Self-TrainedPronoun Interpretation.
In HLT-NAACL ?04, pages33?36.Manfred Klenner and ?Etienne Ailloud.
2009.
Opti-mization in coreference resolution is not needed: Anearly-optimal algorithm with intensional constraints.In EACL, pages 442?450.Hamidreza Kobdani and Hinrich Schu?tze.
2010.
Sucre:A modular system for coreference resolution.
In Se-mEval ?10, pages 92?95.Hamidreza Kobdani, Hinrich Schu?tze, Andre Burkovski,Wiltrud Kessler, and Gunther Heidemann.
2010.
Re-lational feature engineering of natural language pro-cessing.
In CIKM ?10.
ACM.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
AMention-Synchronous Coreference Resolution Algo-rithm Based on the Bell Tree.
In ACL ?04, pages 135?142.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In HLT ?05, pages 25?32.A.
McEnery, I. Tanaka, and S. Botley.
1997.
Corpusannotation and reference resolution.
In ANARESOLU-TION ?97, pages 67?74.Alexis Mitchell, Stephanie Strassel, Mark Przybocki,JK Davis, George Doddington, Ralph Grishman,Adam Meyers, Ada Brunstein, Lisa Ferro, and BethSundheim.
2003.
ACE-2 version 1.0.
Linguistic DataConsortium, Philadelphia.Christoph Mu?ller, Stefan Rapp, and Michael Strube.2002.
Applying co-training to reference resolution.
InACL ?02, pages 352?359.Vincent Ng and Claire Cardie.
2003.
Bootstrappingcoreference classifiers with multiple machine learningalgorithms.
In EMNLP ?03, pages 113?120.Vincent Ng.
2008.
Unsupervised models for coreferenceresolution.
In EMNLP ?08, pages 640?649.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL ?07, pages404?411.Simone Paolo Ponzetto.
2010.
Knowledge Acquisi-tion from a Collaboratively Generated Encyclopedia,volume 327 of Dissertations in Artificial Intelligence.Amsterdam, The Netherlands: IOS Press & Heidel-berg, Germany: AKA Verlag.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with markov logic.
InEMNLP ?08, pages 650?659.Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, MarthaPalmer, Lance Ramshaw, and Ralph Weischedel.2007.
Ontonotes: A unified relational semantic rep-resentation.
In ICSC ?07, pages 517?526.J.
Ross Quinlan.
1993.
C4.5: Programs for machinelearning.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In EMNLP ?10,pages 492?501.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.Anto`nia Mart?
?, Mariona Taule?, Ve?ronique Hoste,Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 Task 1: Coreference resolution inmultiple languages.
In SemEval ?10, pages 70?75.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to coref-erence resolution of noun phrases.
In CL ?01, pages521?544.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence resolution with reconcile.
In ACL ?10, pages156?161.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In MUC6 ?95,pages 45?52.792
