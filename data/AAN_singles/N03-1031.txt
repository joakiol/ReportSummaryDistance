Example Selection for Bootstrapping Statistical ParsersMark Steedman?, Rebecca Hwa?, Stephen Clark?, Miles Osborne?, Anoop Sarkar?Julia Hockenmaier?, Paul Ruhlen?
Steven Baker?, Jeremiah Crim?
?School of Informatics, University of Edinburgh{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk?Institute for Advanced Computer Studies, University of Marylandhwa@umiacs.umd.edu?School of Computing Science, Simon Fraser Universityanoop@cs.sfu.ca?Center for Language and Speech Processing, Johns Hopkins Universityjcrim@jhu.edu,ruhlen@cs.jhu.edu?Department of Computer Science, Cornell Universitysdb22@cornell.eduAbstractThis paper investigates bootstrapping for statis-tical parsers to reduce their reliance on manu-ally annotated training data.
We consider botha mostly-unsupervised approach, co-training,in which two parsers are iteratively re-trainedon each other?s output; and a semi-supervisedapproach, corrected co-training, in which ahuman corrects each parser?s output beforeadding it to the training data.
The selection oflabeled training examples is an integral part ofboth frameworks.
We propose several selectionmethods based on the criteria of minimizing er-rors in the data and maximizing training util-ity.
We show that incorporating the utility cri-terion into the selection method results in betterparsers for both frameworks.1 IntroductionCurrent state-of-the-art statistical parsers (Collins, 1999;Charniak, 2000) are trained on large annotated corporasuch as the Penn Treebank (Marcus et al, 1993).
How-ever, the production of such corpora is expensive andlabor-intensive.
Given this bottleneck, there is consider-able interest in (partially) automating the annotation pro-cess.To overcome this bottleneck, two approaches from ma-chine learning have been applied to training parsers.
Oneis sample selection (Thompson et al, 1999; Hwa, 2000;Tang et al, 2002), a variant of active learning (Cohn et al,1994), which tries to identify a small set of unlabeled sen-tences with high training utility for the human to label1.Sentences with high training utility are those most likelyto improve the parser.
The other approach, and the fo-cus of this paper, is co-training (Sarkar, 2001), a mostly-unsupervised algorithm that replaces the human by hav-ing two (or more) parsers label training examples for eachother.
The goal is for both parsers to improve by boot-strapping off each other?s strengths.
Because the parsersmay label examples incorrectly, only a subset of their out-put, chosen by some selection mechanism, is used in or-der to minimize errors.
The choice of selection methodsignificantly affects the quality of the resulting parsers.We investigate a novel approach of selecting trainingexamples for co-training parsers by incorporating the ideaof maximizing training utility from sample selection.
Theselection mechanism is integral to both sample selectionand co-training; however, because co-training and sam-ple selection have different goals, their selection methodsfocus on different criteria: co-training typically favors se-lecting accurately labeled examples, while sample selec-tion typically favors selecting examples with high train-ing utility, which often are not sentences that the parsersalready label accurately.
In this work, we investigate se-lection methods for co-training that explore the trade-offbetween maximizing training utility and minimizing er-rors.Empirical studies were conducted to compare selectionmethods under both co-training and a semi-supervisedframework called corrected co-training (Pierce andCardie, 2001), in which the selected examples are man-ually checked and corrected before being added to the1In the context of training parsers, a labeled example is asentence with its parse tree.
Throughout this paper, we use theterm ?label?
and ?parse?
interchangeably.Edmonton, May-June 2003Main Papers , pp.
157-164Proceedings of HLT-NAACL 2003training data.
For co-training, we show that the benefit ofselecting examples with high training utility can offset theadditional errors they contain.
For corrected co-training,we show that selecting examples with high training util-ity reduces the number of sentences the human annotatorhas to check.
For both frameworks, we show that selec-tion methods that maximize training utility find labeledexamples that result in better trained parsers than thosethat only minimize error.2 Co-trainingBlum and Mitchell (1998) introduced co-training tobootstrap two classifiers with different views of the data.The two classifiers are initially trained on a small amountof annotated seed data; then they label unannotated datafor each other in an iterative training process.
Blum andMitchell prove that, when the two views are conditionallyindependent given the label, and each view is sufficientfor learning the task, co-training can boost an initialweak learner using unlabeled data.The theory underlying co-training has been extendedby Dasgupta et al (2002) to prove that, by maximizingtheir agreement over the unlabeled data, the two learn-ers make few generalization errors (under the same in-dependence assumption adopted by Blum and Mitchell).Abney (2002) argues that this assumption is extremelystrong and typically violated in the data, and he proposesa weaker independence assumption.Goldman and Zhou (2000) show that, through care-ful selection of newly labeled examples, co-training canwork even when the classifiers?
views do not satisfythe independence assumption.
In this paper we investi-gate methods for selecting labeled examples produced bytwo statistical parsers.
We do not explicitly maximizeagreement (along the lines of Abney?s algorithm (2002))because it is too computationally intensive for trainingparsers.The pseudocode for our co-training framework is givenin Figure 1.
It consists of two different parsers and a cen-tral control that interfaces between the two parsers andthe data.
At each co-training iteration, a small set of sen-tences is drawn from a large pool of unlabeled sentencesand stored in a cache.
Both parsers then attempt to labelevery sentence in the cache.
Next, a subset of the newlylabeled sentences is selected to be added to the train-ing data.
The examples added to the training set of oneparser (referred to as the student) are only those producedby the other parser (referred to as the teacher), althoughthe methods we use generalize to the case in which theparsers share a single training set.
During selection, oneparser first acts as the teacher and the other as the student,and then the roles are reversed.A and B are two different parsers.M iA and M iB are the models of A and B at step i.U is a large pool of unlabeled sentences.U i is a small cache holding a subset of U at step i.L is the manually labeled seed data.LiA and LiB are the labeled training examples for A and Bat step i.Initialize:L0A ?
L0B ?
L.M0A ?
Train(A,L0A)M0B ?
Train(B,L0B)Loop:U i ?
Add unlabeled sentences from U .M iA and M iB parse the sentences in U i andassign scores to them according to their scoringfunctions fA and fB .Select new parses {PA} and {PB} according to someselection method S, which uses the scoresfrom fA and fB .Li+1A is LiA augmented with {PB}Li+1B is LiB augmented with {PA}M i+1A ?
Train(A,Li+1A )M i+1B ?
Train(B,Li+1B )Figure 1: The pseudo-code for the co-training algorithm3 Selecting Training ExamplesIn each iteration, selection is performed in two steps.First, each parser uses some scoring function, f , to assessthe parses it generated for the sentences in the cache.2Second, the central control uses some selection method,S, to choose a subset of these labeled sentences (based onthe scores assigned by f ) to add to the parsers?
trainingdata.
The focus of this paper is on the selection phase, butto more fully investigate the effect of different selectionmethods we also consider two possible scoring functions.3.1 Scoring functionsThe scoring function attempts to quantify the correctnessof the parses produced by each parser.
An ideal scor-ing function would give the true accuracy rates (e.g., F-score, the combined labeled precision and recall rates).In practice, accuracy is approximated by some notionof confidence.
For example, one easy-to-compute scor-ing function measures the conditional probability of the(most likely) parse.
If a high probability is assigned, theparser is said to be confident in the label it produced.In our experimental studies, we considered the selec-tion methods?
interaction with two scoring functions: anoracle scoring function fF-score that returns the F-scoreof the parse as measured against a gold standard, and a2In our experiments, both parsers use the same scoring func-tion.practical scoring function fprob that returns the condi-tional probability of the parse.33.2 Selection methodsBased on the scores assigned by the scoring function,the selection method chooses a subset of the parser la-beled sentences that best satisfy some selection criteria.One such criterion is the accuracy of the labeled exam-ples, which may be estimated by the teacher parser?s con-fidence in its labels.
However, the examples that theteacher correctly labeled may not be those that the stu-dent needs.
We hypothesize that the training utility ofthe examples for the student parser is another importantcriterion.Training utility measures the improvement a parserwould make if that sentence were correctly labeled andadded to the training set.
Like accuracy, the utility ofan unlabeled sentence is difficult to quantify; therefore,we approximate it with values that can be computed fromfeatures of the sentence.
For example, sentences contain-ing many unknown words may have high training util-ity; so might sentences that a parser has trouble parsing.Under the co-training framework, we estimate the train-ing utility of a sentence for the student by comparing thescore the student assigned to its parse (according to itsscoring function) against the score the teacher assignedto its own parse.To investigate how the selection criteria of utility andaccuracy affect the co-training process, we considered anumber of selection methods that satisfy the requirementsof accuracy and training utility to varying degrees.
Thedifferent selection methods are shown below.
For eachmethod, a sentence (as labeled by the teacher parser) isselected if:?
above-n (Sabove-n): the score of the teacher?s parse(using its scoring function) ?
n.?
difference (Sdiff-n): the score of the teacher?s parseis greater than the score of the student?s parse bysome threshold n.?
intersection (Sint-n): the score of the teacher?s parseis in the set of the teacher?s n percent highest-scoring labeled sentences, and the score of the stu-dent?s parse for the same sentence is in the set ofthe student?s n percent lowest-scoring labeled sen-tences.Each selection method has a control parameter, n, thatdetermines the number of labeled sentences to add at eachco-training iteration.
It also serves as an indirect control3A nice property of using conditional probability,Pr(parse|sentence), as the scoring function is that itnormalizes for sentence length.of the number of errors added to the training set.
For ex-ample, the Sabove-n method would allow more sentencesto be selected if n was set to a low value (with respect tothe scoring function); however, this is likely to reduce theaccuracy rate of the training set.The above-n method attempts to maximize the accu-racy of the data (assuming that parses with higher scoresare more accurate).
The difference method attempts tomaximize training utility: as long as the teacher?s label-ing is more accurate than that of the student, it is cho-sen, even if its absolute accuracy rate is low.
The inter-section method attempts to maximize both: the selectedsentences are accurately labeled by the teacher and incor-rectly labeled by the student.4 ExperimentsExperiments were performed to compare the effect ofthe selection methods on co-training and corrected co-training.
We consider a selection method, S1, superiorto another, S2, if, when a large unlabeled pool of sen-tences has been exhausted, the examples selected by S1(as labeled by the machine, and possibly corrected by thehuman) improve the parser more than those selected byS2.
All experiments shared the same general setup, asdescribed below.4.1 Experimental SetupFor two parsers to co-train, they should generate com-parable output but use independent statistical models.In our experiments, we used a lexicalized context freegrammar parser developed by Collins (1999), and a lex-icalized Tree Adjoining Grammar parser developed bySarkar (2002).
Both parsers were initialized with someseed data.
Since the goal is to minimize human annotateddata, the size of the seed data should be small.
In this pa-per we used a seed set size of 1, 000 sentences, taken fromsection 2 of the Wall Street Journal (WSJ) Penn Tree-bank.
The total pool of unlabeled sentences was the re-mainder of sections 2-21 (stripped of their annotations),consisting of about 38,000 sentences.
The cache size isset at 500 sentences.
We have explored using differentsettings for the seed set size (Steedman et al, 2003).The parsers were evaluated on unseen test sentences(section 23 of the WSJ corpus).
Section 0 was used asa development set for determining parameters.
The eval-uation metric is the Parseval F-score over labeled con-stituents: F-score = 2?LR?LPLR+LP , where LP and LRare labeled precision and recall rate, respectively.
Bothparsers were evaluated, but for brevity, all results reportedhere are for the Collins parser, which received higher Par-seval scores.8080.58181.58282.58383.5840 2000 4000 6000 8000 10000 12000Parsing Accuracy on TestData (Fscore)Number of Training Sentencesabove-70%diff-10%int-60%No selection(Human annotated)8080.58181.58282.58383.5840 2000 4000 6000 8000 10000 12000Parsing Accuracy on TestData (Fscore)Number of Training Sentencesabove-90%diff-10%int-30%No selection(Human annotated)(a) (b)Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the labelquality of the training data.
(a) The average accuracy rates are about 85%.
(b) The average accuracy rates (except forthose selected by Sdiff-10%) are about 95%.4.2 Experiment 1: Selection Methods andCo-TrainingWe first examine the effect of the three selection meth-ods on co-training without correction (i.e., the chosenmachine-labeled training examples may contain errors).Because the selection decisions are based on the scoresthat the parsers assign to their outputs, the reliability ofthe scoring function has a significant impact on the per-formance of the selection methods.
We evaluate the ef-fectiveness of the selection methods using two scoringfunctions.
In Section 4.2.1, each parser assesses its out-put with an oracle scoring function that returns the Par-seval F-score of the output (as compared to the humanannotated gold-standard).
This is an idealized conditionthat gives us direct control over the error rate of the la-beled training data.
By keeping the error rates constant,our goal is to determine which selection method is moresuccessful in finding sentences with high training utility.In Section 4.2.2 we replace the oracle scoring functionwith fprob, which returns the conditional probability ofthe best parse as the score.
We compare how the selectionmethods?
performances degrade under the realistic con-dition of basing selection decisions on unreliable parseroutput assessment scores.4.2.1 Using the oracle scoring function, fF-scoreThe goal of this experiment is to evaluate the selectionmethods using a reliable scoring function.
We thereforeuse an oracle scoring function, fF-score, which guaran-tees a perfect assessment of the parser?s output.
This,however, may be too powerful.
In practice, we expecteven a reliable scoring function to sometimes assign highscores to inaccurate parses.
We account for this effect byadjusting the selection method?s control parameter to af-fect two factors: the accuracy rate of the newly labeledtraining data, and the number of labeled sentences addedat each training iteration.
A relaxed parameter settingadds more parses to the training data, but also reducesthe accuracy of the training data.Figure 2 compares the effect of the three selectionmethods on co-training for the relaxed (left graph) andthe strict (right graph) parameter settings.
Each curve inthe two graphs charts the improvement in the parser?s ac-curacy in parsing the test sentences (y-axis) as it is trainedon more data chosen by its selection method (x-axis).The curves have different endpoints because the selectionmethods chose a different number of sentences from thesame 38K unlabeled pool.
For reference, we also plottedthe improvement of a fully-supervised parser (i.e., trainedon human-annotated data, with no selection).For the more relaxed setting, the parameters are chosenso that the newly labeled training data have an averageaccuracy rate of about 85%:?
Sabove-70% requires the labels to have an F-score ?70%.
It adds about 330 labeled sentences (out of the500 sentence cache) with an average accuracy rateof 85% to the training data per iteration.?
Sdiff-10% requires the score difference between theteacher?s labeling and the student?s labeling to be atleast 10%.
It adds about 50 labeled sentences withan average accuracy rate of 80%.?
Sint-60% requires the teacher?s parse to be in thetop 60% of its output and the student?s parse for thesame sentence to be in its bottom 60%.
It adds about150 labeled sentences with an average accuracy rateof 85%.Although none rivals the parser trained on human an-notated data, the selection method that improves theparser the most is Sdiff-10%.
One interpretation is thatthe training utility of the examples chosen by Sdiff-10%outweighs the cost of errors introduced into the trainingdata.
Another interpretation is that the other two selectionmethods let in too many sentences containing errors.
Inthe right graph, we compare the same Sdiff-10% with theother two selection methods using stricter control, suchthat the average accuracy rate for these methods is nowabout 95%:?
Sabove-90% now requires the parses to be at least90% correct.
It adds about 150 labeled sentencesper iteration.?
Sint-30% now requires the teacher?s parse to be inthe top 30% of its output and the student?s parse forthe same sentence in its bottom 30%.
It adds about15 labeled sentences.The stricter control on Sabove-90% improved theparser?s performance, but not enough to overtakeSdiff-10% after all the sentences in the unlabeled poolhad been considered, even though the training data ofSdiff-10% contained many more errors.
Sint-30% has afaster initial improvement4, closely tracking the progressof the fully-supervised parser.
However, the stringent re-quirement exhausted the unlabeled data pool before train-ing the parser to convergence.
Sint-30% might continueto help the parser to improve if it had access to more un-labeled data, which is easier to acquire than annotateddata5.Comparing the three selection methods under bothstrict and relaxed control settings, the results suggest thattraining utility is an important criterion in selecting train-ing examples, even at the cost of reduced accuracy.4.2.2 Using the fprob scoring functionTo determine the effect of unreliable scores on the se-lection methods, we replace the oracle scoring function,fF-score, with fprob, which approximates the accuracyof a parse with its conditional probability.
Although thisis a poor estimate of accuracy (especially when computedfrom a partially trained parser), it is very easy to compute.The unreliable scores also reduce the correlation betweenthe selection control parameters and the level of errors inthe training data.
In this experiment, we set the parame-ters for all three selection methods so that approximately4A fast improvement rate is not a central concern here, butit will be more relevant for corrected co-training.5This oracle experiment is bounded by the size of the anno-tated portion of the WSJ corpus.79.88080.280.480.680.88181.21000 1500 2000 2500 3000 3500 4000 4500 5000Parsing Accuracy on TestData (Fscore)Number of Training Sentencesabove-70%diff-30%int-30%Figure 3: A comparison of selection methods using theconditional probability scoring function, fprob.30-50 sentences were added to the training data per iter-ation.
The average accuracy rate of the training data forSabove-70% was about 85%, and the rate for Sdiff-30%and Sint-30% was about 75%.As expected, the parser performances of all three selec-tion methods using fprob (shown in Figure 3) are lowerthan using fF-score (see Figure 2).
However, Sdiff-30%and Sint-30% helped the co-training parsers to improvewith a 5% error reduction (1% absolute difference) overthe parser trained only on the initial seed data.
In con-trast, despite an initial improvement, using Sabove-70%did not help to improve the parser.
In their experiments onNP identifiers, Pierce and Cardie (2001) observed a sim-ilar effect.
They hypothesize that co-training does notscale well for natural language learning tasks that requirea huge amount of training data because too many errorsare accrued over time.
Our experimental results suggestthat the use of training utility in the selection process canmake co-training parsers more tolerant to these accumu-lated errors.4.3 Experiment 2: Selection Methods andCorrected Co-trainingTo address the problem of the training data accumulatingtoo many errors over time, Pierce and Cardie proposeda semi-supervised variant of co-training called correctedco-training, which allows a human annotator to reviewand correct the output of the parsers before adding it tothe training data.
The main selection criterion in theirco-training system is accuracy (approximated by confi-dence).
They argue that selecting examples with nearlycorrect labels would require few manual interventionsfrom the annotator.We hypothesize that it may be beneficial to considerthe training utility criterion in this framework as well.We perform experiments to determine whether select-ing fewer (and possibly less accurately labeled) exam-80818283848586872000 4000 6000 8000 10000 12000Parsing Accuracy on TestData (Fscore)Number of Training Sentencesabove-90%diff-10%int-30%No selection80818283848586870 5000 10000 15000 20000 25000 30000 35000 40000 45000Parsing Accuracy on TestData (Fscore)Number of Constituents to Correct in the Training Dataabove-90%diff-10%int-30%No selection(a) (b)Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number ofsentences added to the training data; (b) in terms of the number of manually corrected constituents.ples with higher training utility would require less effortfrom the annotator.
In our experiments, we simulatedthe interactive sample selection process by revealing thegold standard.
As before, we compare the three selectionmethods using both fF-score and fprob as scoring func-tions.64.3.1 Using the oracle scoring function, fF-scoreFigure 4 shows the effect of the three selection meth-ods (using the strict parameter setting) on corrected co-training.
As a point of reference, we plot the improve-ment rate for a fully supervised parser (same as the onein Figure 2).
In addition to charting the parser?s perfor-mance in terms of the number of labeled training sen-tences (left graph), we also chart the parser?s performancein terms of the the number of constituents the machinemislabeled (right graph).
The pair of graphs indicates theamount of human effort required: the left graph showsthe number of sentences the human has to check, and theright graph shows the number of constituents the humanhas to correct.Comparing Sabove-90% and Sdiff-10%, we see thatSdiff-10% trains a better parser than Sabove-90% when allthe unlabeled sentences have been considered.
It also im-proves the parser using a smaller set of training exam-ples.
Thus, for the same parsing performance, it requiresthe human to check fewer sentences than Sabove-90% andthe reference case of no selection (Figure 4(a)).
On theother hand, because the labeled sentences selected bySdiff-10% contain more mistakes than those selected bySabove-90%, Sdiff-10% requires slightly more corrections6The selection control parameters are the same as the previ-ous set of experiments, using the strict setting (i.e., Figure 2(b))for fF-score.than Sabove-90% for the same level of parsing perfor-mance; though both require fewer corrections than thereference case of no selection (Figure 4(b)).
Becausethe amount of effort spent by the annotator depends onthe number of sentences checked as well as the amountof corrections made, whether Sdiff-10% or Sabove-90% ismore effort reducing may be a matter of the annotator?spreference.The selection method that improves the parser at thefastest rate is Sint-30%.
For the same parser performancelevel, it selects the fewest number of sentences for a hu-man to check and requires the human to make the leastnumber of corrections.
However, as we have seen in theearlier experiment, very few sentences in the unlabeledpool satisfy its stringent criteria, so it ran out of data be-fore the parser was trained to convergence.
At this pointwe cannot determine whether Sint-30% might continue toimprove the parser if we used a larger set of unlabeleddata.4.3.2 Using the fprob scoring functionWe also consider the effect of unreliable scores in thecorrected co-training framework.
A comparison betweenthe selection methods using fprob is reported in Figure5.
The left graph charts parser performance in terms ofthe number of sentences the human must check; the rightcharts parser performance in terms of the number of con-stituents the human must correct.
As expected, the unreli-able scoring function degrades the effectiveness of the se-lection methods; however, compared to its unsupervisedcounterpart (Figure 3), the degradation is not as severe.In fact, Sdiff-30% and Sint-30% still require fewer train-ing data than the reference parser.
Moreover, consistentwith the other experiments, the selection methods that at-tempt to maximize training utility achieve better parsingperformance than Sabove-70%.
Finally, in terms of reduc-ing human effort, the three selection methods require thehuman to correct comparable amount of parser errors forthe same level of parsing performance, but for Sdiff-30%and Sint-30%, fewer sentences need to be checked.4.3.3 DiscussionCorrected co-training can be seen as a form of activelearning, whose goal is to identify the smallest set of un-labeled data with high training utility for the human tolabel.
Active learning can be applied to a single learner(Lewis and Catlett, 1994) and to multiple learners (Fre-und et al, 1997; Engelson and Dagan, 1996; Ngai andYarowsky, 2000).
In the context of parsing, all previ-ous work (Thompson et al, 1999; Hwa, 2000; Tang etal., 2002) has focussed on single learners.
Corrected co-training is the first application of active learning for mul-tiple parsers.
We are currently investigating comparisonsto the single learner approaches.Our approach is similar to co-testing (Muslea et al,2002), an active learning technique that uses two classi-fiers to find contentious examples (i.e., data for which theclassifiers?
labels disagree) for a human to label.
There isa subtle but significant difference, however, in that theirgoal is to reduce the total number of labeled training ex-amples whereas we also wish to reduce the number ofcorrections made by the human.
Therefore, our selectionmethods must take into account the quality of the parseproduced by the teacher in addition to how different itsparse is from the one produced by the student.
The inter-section method precisely aims at selecting sentences thatsatisfy both requirements.
Exploring different selectionmethods is part of our on-going research effort.5 ConclusionWe have considered three selection methods that have dif-ferent priorities in balancing the two (often competing)criteria of accuracy and training utility.
We have em-pirically compared their effect on co-training, in whichtwo parsers label data for each other, as well as correctedco-training, in which a human corrects the parser labeleddata before adding it to the training set.
Our results sug-gest that training utility is an important selection criterionto consider, even at the cost of potentially reducing the ac-curacy of the training data.
In our empirical studies, theselection method that aims to maximize training utility,Sdiff-n, consistently finds better examples than the onethat aims to maximize accuracy, Sabove-n. Our resultsalso suggest that the selection method that aims to maxi-mize both accuracy and utility, Sint-n, shows promise inimproving co-training parsers and in reducing human ef-fort for corrected co-training; however, a much larger un-labeled data set is needed to verify the benefit of Sint-n.The results of this study indicate the need for scor-ing functions that are better estimates of the accuracy ofthe parser?s output than conditional probabilities.
Ouroracle experiments show that, by using effective selec-tion methods, the co-training process can improve parserpeformance even when the newly labeled parses arenot completely accurate.
This suggests that co-trainingmay still be beneficial when using a practical scoringfunction that might only coarsely distinguish accurateparses from inaccurate parses.
Further avenues to ex-plore include the development of selection methods toefficiently approximate maximizing the objective func-tion of parser agreement on unlabeled data, following thework of Dasgupta et al (2002) and Abney (2002).
Also,co-training might be made more effective if partial parseswere used as training data.
Finally, we are conducting ex-periments to compare corrected co-training with other ac-tive learning methods.
We hope these studies will revealways to combine the strengths of co-training and activelearning to make better use of unlabeled data.AcknowledgmentsThis work has been supported, in part, by NSF/DARPAfunded 2002 Human Language Engineering Workshopat JHU, EPSRC grant GR/M96889, the Department ofDefense contract RD-02-5700, and ONR MURI Con-tract FCPO.810548265.
We would like to thank ChrisCallison-Burch, Michael Collins, John Henderson, Lil-lian Lee, Andrew McCallum, and Fernando Pereira forhelpful discussions; to Ric Crabbe, Adam Lopez, the par-ticipants of CS775 at Cornell University, and the review-ers for their comments on this paper.ReferencesSteven Abney.
2002.
Bootstrapping.
In Proceedings of the40th Annual Meeting of the Association for ComputationalLinguistics, pages 360?367, Philadelphia, PA.Avrim Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedings of the11th Annual Conference on Computational Learning Theory,pages 92?100, Madison, WI.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Annual Meeting of the NAACL.David Cohn, Les Atlas, and Richard Ladner.
1994.
Improv-ing generalization with active learning.
Machine Learning,15(2):201?221.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University of Penn-sylvania.Sanjoy Dasgupta, Michael Littman, and David McAllester.2002.
PAC generalization bounds for co-training.
In T. G.Dietterich, S. Becker, and Z. Ghahramani, editors, Advances80818283848586872000 4000 6000 8000 10000 12000Parsing Accuracy on TestData (Fscore)Number of Training Sentencesabove-70%diff-30%int-30%No selection80818283848586870 5000 10000 15000 20000 25000 30000 35000 40000 45000Parsing Accuracy on TestData (Fscore)Number of Constituents to Correct in the Training Dataabove-70%diff-30%int-30%No selection(a) (b)Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number ofsentences added to the training data; (b) in terms of the number of manually corrected constituents.in Neural Information Processing Systems 14, Cambridge,MA.
MIT Press.Sean P. Engelson and Ido Dagan.
1996.
Minimizing manualannotation cost in supervised training from copora.
In Pro-ceedings of the 34th Annual Meeting of the ACL, pages 319?326.Yoav Freund, H. Sebastian Seung, Eli Shamir, and NaftaliTishby.
1997.
Selective sampling using the query by com-mittee algorithm.
Machine Learning, 28(2-3):133?168.Sally Goldman and Yan Zhou.
2000.
Enhancing supervisedlearning with unlabeled data.
In Proceedings of the 17th In-ternational Conference on Machine Learning, Stanford, CA.Rebecca Hwa.
2000.
Sample selection for statistical grammarinduction.
In Proceedings of the 2000 Joint SIGDAT Confer-ence on EMNLP and VLC, pages 45?52, Hong Kong, China,October.David D. Lewis and Jason Catlett.
1994.
Heterogeneous un-certainty sampling for supervised learning.
In Proceedingsof the Eleventh International Conference on Machine Learn-ing, pages 148?156.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Ion Muslea, Steve Minton, and Craig Knoblock.
2002.
Selec-tive sampling with redundant views.
In Proceedings of theSeventeenth National Conference on Artificial Intelligence,pages 621?626.Grace Ngai and David Yarowsky.
2000.
Rule writing or an-notation: Cost-efficient resource usage for base noun phrasechunking.
In Proceedings of the 38th Annual Meeting of theACL, pages 117?125, Hong Kong, China, October.David Pierce and Claire Cardie.
2001.
Limitations of co-training for natural language learning from large datasets.
InProceedings of the Empirical Methods in NLP Conference,Pittsburgh, PA.Anoop Sarkar.
2001.
Applying co-training methods to statisti-cal parsing.
In Proceedings of the 2nd Annual Meeting of theNAACL, pages 95?102, Pittsburgh, PA.Anoop Sarkar.
2002.
Statistical Parsing Algorithms for Lexi-calized Tree Adjoining Grammars.
Ph.D. thesis, Universityof Pennsylvania.Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, StevenBaker, and Jeremiah Crim.
2003.
Bootstrapping statisticalparsers from small datasets.
In The Proceedings of the An-nual Meeting of the European Chapter of the ACL.
To ap-pear.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.
Activelearning for statistical natural language parsing.
In Proceed-ings of the 40th Annual Meeting of the ACL, pages 120?127,July.Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.Mooney.
1999.
Active learning for natural language pars-ing and information extraction.
In Proceedings of ICML-99,pages 406?414, Bled, Slovenia.
