Some statistical methods for evaluating information extraction systemsWill LoweComputer Science DepartmentBath Universitywlowe@latte.harvard.eduGary KingCenter for Basic Researchin the Social SciencesHarvard Universityking@harvard.eduAbstractWe present new statistical methods forevaluating information extraction sys-tems.
The methods were developedto evaluate a system used by polit-ical scientists to extract event infor-mation from news leads about inter-national politics.
The nature of thisdata presents two problems for evalu-ators: 1) the frequency distribution ofevent types in international event datais strongly skewed, so a random sampleof newsleads will typically fail to con-tain any low frequency events.
2) Man-ual information extraction necessary tocreate evaluation sets is costly, and mosteffort is wasted coding high frequencycategories .We present an evaluation scheme thatovercomes these problems with consid-erably less manual effort than traditionalmethods, and also allows us to interpretan information extraction system as anestimator (in the statistical sense) and toestimate its bias.1 IntroductionThis paper introduces a statistical approach wedeveloped to evaluate information extraction sys-tems used to study international relations.
Eventextraction is a form of categorization, but thehighly skewed frequency profile of internationalevent categories in real data generates severe prob-lems for evaluators.
We discuss these problems insection 3, show how to circumvent using a novelsampling scheme in section 4, and briefly describeour application.
Finally we discuss the advantagesand disadvantages of the methods, and their rela-tions to standard evaluation procedure.
We startwith a brief review of information extraction in in-ternational relations.2 Event Analysis in InternationalRelationsResearchers in quantitative international relationshave been performing manual information ex-traction since the mid-1970s (McClelland, 1978;Azar, 1982).
The information extracted has re-mained fairly simple; a researcher fills a ?who didwhat to whom?
template, usually from historicaldocuments, a list of countries and internationalorganizations to describe the actors, and a moreor less articulated ontology of international eventsto describe what occurred (McClelland, 1978).In the early 1990s automated information extrac-tion tools mostly replaced manual coding efforts(Schrodt et al, 1994).
Information extraction sys-tems in international relations perform a similartask to those competing in early Message Under-standing Competitions (Sundheim, 1991, 1992).With machine extracted events data it is now pos-sible to do near real-time conflict forecasting withdata based on newswire leads, and detailed politi-cal analysis afterwards.3 Event Category DistributionsWe wanted to evaluate an information extractionsystem from Virtual Research Associates1 .
Thissystem bundles extraction and visualization soft-ware with a custom event ontology containing, atlast count, about 200 categories of internationalevent.We found two problems with the nature of inter-national events data.
First, the frequency distribu-tion over the system?s ontology, or indeed severalother ontologies we considered, is heavily skewed.A handful of mostly diplomatic event types pre-dominate, and the frequency of other event typesfalls of very sharply: we ran the system over allthe newsleads in Reuters?
coverage of the Bosniaconflict, and of the approximately 45,000 eventsit extracted, 10,605 were in the category of ?neu-tral comment?, 4 of ?apology?
and 35 of ?threat offorce?.
Thus the relative frequencies of event cat-egories in this data can be 2,500 to 1.Also, as these figures suggest, the more inter-esting and politically relevant events tend to be oflow frequency.
This problem is quite general incategorization systems with reasonably articulatedcategory systems, and not specific to internationalrelations.
But any dataset with these propertiescauses an immediate problem for evaluation.Ideally we would choose a random subset ofleads whose events are known with certainty (be-cause we have coded them manually beforehand),run the system over them, and then compute var-ious sample statistics such as precision and re-call2.
However, a small randomly chosen subsetis very unlikely contain instances of most interest-ing events, and so the system?s performance willnot be evaluated on them.
Given the possible fre-quency ratios above, the size of subset necessaryto ensure reasonable coverage of lower frequencyevent categories is enormous.
Put more concretely,to construct a test set of news leads the evaluatorwill on average have to code around 2,500 com-ments to reach a single apology and about 300comments to find a single threat of force.1http://www.vranet.com2This paper only evaluates extraction performance onevent types, though there would seem to be no reason whya similar approach would not work for actors etc.3.1 Standard Evalution MethodsThe standard evaluation methods developed overthe course of the Message Understanding Compe-titions consist mainly in sample statistics to com-pute over the evaluation materials e.g.
precisionand recall, but do not give any guidance for choos-ing the materials themselves (Cowie and Lehnert,1996; Grishman, 1997).
This is just done by handby the judges.
Perhaps because the selection ques-tion is neglected, it is seldom clear what largerpopulation the test materials are from (save that itis the same one as the training examples), and as aconsequence it is unclear what the implications forgeneralization are when a system obtains a partic-ular set of scores for precision and recall (Lehnertand Sundheim, 1991).Since this literature did not help us generatea suitable evaluation sample, we approached theproblem from scratch, and developed a statisticalframework specific to our needs.4 MethodOne reasonable-sounding but wrong way to ad-dress the problem of creating a test set withouthaving to code tens of thousands of irrelevant sto-ries is the following:1.
Use the extraction system itself to perform aninitial coding,2.
Take a sample of the output that covers all theevent types in reasonable quantities,3.
Examine each coding to see whether the sys-tem assigned the correct event code.This looks like it can guarantee a good sample oflow frequency events at much lower cost to themanual coder; we can just pick a fixed numberof events from each category and evaluate them.However, this method exhibits selection bias.
Tosee this, let M and T be variables indicating whichevent category the Machine (that is, the informa-tion extraction system) codes an event into, andthe True category to which the event actually be-longs.
Statistically, the quantity of interest to us isthe probability that the machine is correct:P(M = i | T = i) (1)This is the probability that the machine classifiesan event into category i given that the true eventcoding is indeed i.
A full characterization of thesuccess of the machine requires knowing P(M = i |T = i) for i = 0, .
.
.
,J, which includes all J eventcategories and where i = 0 denotes the situationwhere the machine is unable to classify an eventinto any category.
In short, the quantity of interestis the full probability density P(M | T ).In statistical terms, this distribution is a likeli-hood function for the information extraction sys-tem.
This observation allows us to treat the systemlike any other statistical estimator and offers theinteresting possibility of analyzing generalizationvia its sampling properties, e.g.
its bias, variance,mean squared error, or risk.Unfortunately, the problem with the reasonable-sounding approach described above is that it doesnot in fact allow us to estimate P(M | T ) becauseit is implicitly conditioning on M, not T .
In par-ticular, the proportion of events that are actually incategory i among those the machine put in cate-gory i gives us instead an estimate ofP(T | M) (2)which is not the quantity of interest.
(2) is theprobability of the truth being in some event cate-gory rather than the machine?s response whereasin fact the true event category is fixed and it isthe machine?s response that is uncertain3 .
Worse,P(T | M) is a systematically biased estimate ofP(M | T ) because these two quantities are relatedby Bayes theorem:P(M | T ) = P(M,T )P(T ) =P(T | M)P(M)P(T ) , (3)and the only circumstances under which theywould be equal is when P(M) is uniform.
Butthe figures in section 3 suggest that P(M) is highlyskewed.However this last observation suggests a bettermethod for unbiased estimation of (1).1.
Estimate P(T | M) as described above3This is due to changes in the journalist?s choice of vocab-ulary and syntactic construction that are uncorrelated with theidentity of the event being described.2.
Compute P(M) by running the system overthe entire data set and normalizing the fre-quency histogram of event categories3.
Estimate P(M | T ) by correcting P(T | M)with P(M) using Bayes theoremOur implementation of this scheme was to firstrun the system over 45,000 leads about the Bosniaconflict, and normalize the frequency histogram ofevents extracted to create P(M).
Then, randomlychoose 5 leads assigned to each event category,and manually determine which event type the in-stantiate.
Then normalize to estimate P(T | M).And finally, use (3) to create P(M | T ).
We chosefour times as many uncategorized leads as fromeach true category in addition.
A larger samplehere is advisable to see what sort of categories thesystem misses.
These sample sizes are fixed, butit may also be possible to use active learning tech-niques to tune them (as in e.g.
Argamon-Engelsonand Dagan, 1999) for even more efficient sam-pling.The advantage of this roundabout route to (1) isthat it requires many fewer events to be manuallycoded.
We ran the system over 45,000 leads butonly manually coded a handful of events for eachcategory.
This guaranteed us even coverage of thelowest frequency event categories whilst not bias-ing the end result ?
for an ontology with about 200categories this is a substantial decrease in evalua-tor effort.This method works by making use of the ex-traction system itself to produce one importantmarginal: P(M).
If we assume that the aim is toevaluate the system on the Bosnia conflict, P(M)is not estimated, but is rather an exact populationmarginal4 .
Then we can guarantee that our esti-mate of P(M | T ) is unbiased because the methodfor estimating P(T | M) is clearly unbiased, andP(M) adds no error.4.1 Summary MeasuresP(M | T ) allows the computation of a numberof useful summary measures5.
For example, we4We might consider the Bosnian conflict to be a samplepoint from the larger population of all wars, but that popula-tion ?
if it exists at all ?
is certainly difficult to quantify.5Detailed discussion of several summary measures for thesystem we evaluated can be found in King and Lowe (2002).can easily compute P(M,T ) from quantities al-ready available, so ?J P(M = i,T = i) is the pro-portion of time the system extracts the correctcategory.
Alternatively, if it is more importantto extract some categories than others, then var-ious weighted measures can be constructed e.g.
?J P(M = i | T = i)wi where ws are non-negativeand sum to 1, representing the relative importanceof extracting each category.
Some more graphi-cal methods of evaluation using P(M | T ) are pre-sented below.4.2 Estimator PropertiesGiven a likelihood function for the extraction sys-tem we can investigate its properties as an esti-mator.
It is particularly useful to know the biasof an estimator, defined in this case as the dif-ference between the expected category responsefrom the system when the true event category isi, and i itself, where the expectation is taken of re-peated information extraction tasks that instantiatethe same event categories.
We do not examine thecorresponding variance here, and a more completeevaluation might also address the question of con-sistency.4.2.1 Conflict and CooperationThe machines response and the true categoryis best seen as a set of multinomial probabilities(with a unit vector with the value 1 at the indexof the system?s extracted category or the true cate-gory respectively.
Estimator properties are cum-bersome to represent in this format, so here wemap the system?s response to a single real valuecorresponding to the level of conflict or coopera-tion of the event category.
This re-representationis usual in international relations and allows stan-dard econometric time series methods to be ap-plied (Schrodt and Gerner, 1994; Goldstein andFreeman, 1990; Goldstein and Pevehouse, 1997).For our purposes it also allows the straightfor-ward graphical presentation of the main ideas.
Wedefine the level of conflict or cooperation levelof an event category i as Gi, a real number be-tween -10 (most conflictual) to 10 (most coopera-tive) (see Goldstein, 1992, for the full mapping).For example, according to this scheme, when idenotes the event category ?extending economic?10 ?8 ?6 ?4 ?2 0 2 4 6 8 10?10?8?6?4?20246810Gig iFigure 1: Expected (gi) versus true (Gi) conflict-cooperation level for each event category.aid?, Gi = 7.4, ?policy endorsement?
maps to 3.6,?halt negotiations?
maps to -3.8, and a ?military en-gagement?
maps to -10, the maximally conflictualevent.
The mapping allows univariate, and polit-ically relevant comparison between the true con-flict level and that of the event categories the sys-tem extracts.The expected system response when the truecategory has conflict/cooperation level Gi is:gi =J?G jP(M = j | T = i,M 6= 0) (4)whereP(M = j | T = i,M 6= 0) = P(M | T )1(M 6= 0)P(M 6= 0 | T ) .and 1(M 6= 0) is an indicator function equaling 1if M 6= 0 and 0 otherwise.A plot of Gi against gi for each event category isshown in Figure 1.
An unbiased estimator wouldshow expected values on the main diagonal.
Esti-mator bias for event category i is simply gi ?Gi.Estimator variance is simply the spread around thediagonal.4.3 ComparisonWe also compared the system?s performance to3 undergraduate coders (U1-3) working on thesame data set.
To examine undergraduate perfor-mance requires first P(U,T ), from which we canget P(U | T ).
However, we cannot simply countthe proportion of times each undergraduate assignsa lead to category i when it is in fact in category ibecause this ignores the fact that we have sampledthe leads themselves using the system, and musttherefore condition on M. On the other hand wedo have access to the relevant conditional distri-bution P(U,T | M = i).
This is the distribution ofundergraduate and true categories, conditioned onthe fact the the system assigns an event to cate-gory i.
The desired P(U,T ) is a weighted averageof these distributions:P(U,T ) = ?iP(U,T | M = i)P(M = i).P(U | T ) is then obtained by marginalization6 .Clearly these calculations can also be used to com-pare other systems with the same ontology usingthe same materials.Summary statistics similar to those describedabove can be easily computed (King and Lowe,2002).
Here we provide graphical results: Fig-ure 2 plots the bias of the system and that of the un-dergraduates over the category set (with smoothedestimates superimposed).
In the figure, the biasGi ?
gi is plotted against Gi, so deflections fromthe horizontal are systematic bias.
In almost allcases we find that more conflictual (negative val-ued) categories are mistaken for more cooperativeones, with some suggestion of a similar effect atthe cooperative end too.
Of most interest is the ba-sic similarity in performance between undergrad-uates and the information extraction system.It would be helpful if the bias that appears inthese plots were systematically related to the ex-pected system response.
If this was the case, infuture use we could simply adjust the system?sresponse up or down by some coefficient deter-mined in the evaluation process and remove thebias.
However, figure 3 shows that there is nosystematic relation between the expected reponsesand the level of bias, so no such coefficient canbecomputed.
This is a rather pessimistic result forthis system, suggesting a level of bias that can-not be straightforwardly removed.
On the other6We would normally expect to use P(U | T,U 6= 0), butthe undergraduates never failed to assign categories.
?10 ?5 0 5 10?15?10?50510GiGi?g iMachine?10 ?5 0 5 10?10?50510GiGi?g iU1?10 ?5 0 5 10?15?10?50510GiGi?g iU2?10 ?5 0 5 10?15?10?50510GiGi?g iU3Figure 2: System (M) versus undergraduate coder(U1-3) bias.
Connected lines are generated bysmoothing Gi ?gi.
?10 ?5 0 5 10?15?10?50510giGi?g iMachine?10 ?5 0 5 10?10?50510giGi?g iU1?10 ?5 0 5 10?15?10?50510giGi?g iU2?10 ?5 0 5 10?15?10?50510giGi?g iU3Figure 3: Bias plotted against expected system andundergraduate response.
Deviations from the hori-zontal suggest the possibility of a post-output cor-rection to correct for bias in subsequent applica-tion.
?10 ?5 0 5 1000.20.40.60.81Gip(null)Machine?10 ?5 0 5 1000.20.40.60.81Gip(null)U1?10 ?5 0 5 1000.20.40.60.81Gip(null)U2?10 ?5 0 5 1000.20.40.60.81Gip(null)U3Figure 4: The probability that the system, or un-dergraduate fails to assign an event to a category,plotted against the level of conflict/cooperation ofthat category.hand, one of the advantages of the methods pre-sented here is that this bias is now estimated, and,since bias estimates are available on a category-by-category basis, redesigning effort can be di-rected in a way that maximizes generalization per-formance.Finally, figure 4 plots the probability that themachine failed to assign an event category, P(M =0 | T = i) (denoted p(null) in the figure), as a func-tion of that category?s conflict/cooperation value,Gi.
Our interest in Gi reflects the use this data istypically put to, since we are most concerned witherrors that make the world look systematicallymore (or less) cooperative than it really is.
Butwe might equally have plotted P(M = 0 | T = i)against i itself, or any other property of events thatmight be suspected to generate difficult to catego-rize event descriptions.Like the previous figures, plotting P(M = 0 |T = i) against other quantities is a useful diagnos-tic, indicating where future work should best beapplied.
In this case there appears to be no sys-tematic relationship between the true level of con-flict/cooperation and the probability that either thesystem or the undergraduates will fail to assign theevent to a category.5 ConclusionWe have presented a set of statistical methodsfor evaluating an information extraction systemwithout unreasonable manual labour when the dis-tribution of categories to be extracted is heav-ily skewed.
The scheme uses a form of biasedsampling and subsequent correction to estimatea probability distribution of system responses foreach true category in the data.
This distributioncostitutes a likelihood function for the system.
Wethen show how functions of this distribution canbe used for evaluation, and estimate the system?sstatistical bias.The two main ideas: using estimates of P(M |T ) as the basis for evaluation, and using a non-standard sampling scheme for the estimation, areseparate.
Emphasis on using P(M | T ) comes fromstandard statistical theory, and if correct, suggestshow evaluation in information extraction might beintegrated in to that body of theory.
When a sam-ple of leads is randomly chosen and can be ex-pected to be reasonably representative, then thesampling machinery described above, the compu-tation of P(M), and the application of Bayes the-orem will not be necessary.
But when the distri-bution of categories to be extracted is so highlyskewed then our method is the only one that willmake it feasible to evaluate a system on all of itscategories in an unbiased way.The principle difference between these andstandard evaluation methods is in our explicitlystatistical framework, and our consideration ofhow to sample in a representative way, and meth-ods to get around cases where we cannot.
The ex-act relationship to precision, recall etc.
is the topicof current research.
In the meantime we hope thatthe methods presented might advance understand-ing of effective evaluation methods in computa-tional linguistics.AcknowledgmentsWe thank Doug Bond, Craig Jenkins, DylanBalch-Lindsay, Phil Schrodt, and two anonymousreviewers for helpful comments, and the NationalScience Foundation (IIS-9874747), the NationalInstitutes of Aging (P01 AG17625-01), the Weath-erhead Center for International Relations, and theWorld Health Organization for research support.ReferencesArgamon-Engelson, S. and Dagan, I.
(1999).Committee-based sample selection for proba-bilistic classifiers.
Journal of Articial Intelli-gence Research, 11:335?360.Azar, E. E. (1982).
Codebook of the Conict andPeace Databank.
Center for International De-velopment, University of Maryland.Cowie, J. and Lehnert, W. (1996).
Informa-tion extraction.
Communications of the ACM,39(1):80?91.Goldstein, J. S. (1992).
A conflict-cooperationscale for WEIS events data.
Journal of ConictResolution, 36(2).Goldstein, J. S. and Freeman, J. R. (1990).
Three-Way Street: Strategic Reciprocity in World Pol-itics.
Chicago University Press.Goldstein, J. S. and Pevehouse, J. C. (1997).Reciprocity, bullying and international conflict:Time-series analysis of the Bosnia conflict.American Political Science Review, 91(3):515?529.Grishman, R. (1997).
Information extraction:Techniques and challenges.
In Pazienza, M. T.,editor, Information Extraction: A Multidisci-plinary Approach to an Emerging InformationTechnology, volume 1299 of Lecture Notes inArticial Intelligence, chapter 2, pages 10?27.Springer Verlag.King, G. and Lowe, W. (2002).
An automated in-formation extraction tool for international con-flict data with performance as good as hu-man coders: A rare events evaluation design.http://gking.harvard.edu/infoex.pdf.Lehnert, W. and Sundheim, B.
(1991).
A perfor-mance evaluation of text-analysis technologies.AI Magazine, pages 81?95.McClelland, C. (1978).
World Event / Interac-tion Survey (WEIS) 1966-1978.
Inter-UniversityConsortium for Political and Social Research,University of Southern California.Schrodt, P. A., Davis, S. G., and Weddle, J. L.(1994).
Political science: KEDS ?
a programfor the machine coding of event data.
SocialScience Computer Review, 12.Schrodt, P. A. and Gerner, D. J.
(1994).
Validityassessment of a machine-coded event data setfor the Middle East, 1982-92.
American Journalof Political Science, 38(3).Sundheim, B.
(1992).
Overview of the fourth mes-sage understanding evaluation and conference.In Proceedings of the Fourth Message Under-standing Conference, pages 3?22.Sundheim, S., editor (1991).
Proceedings of theThird Message Understanding Conference, SanMateo, CA.
Morgan Kaufmann.
