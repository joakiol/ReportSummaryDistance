Task Tolerance of MT Output in Integrated Text ProcessesJohn S. White, Jennifer B. Doyon, and Susan W. TalbottLitton PRC1500 PRC DriveMcLean, VA 22102, USA{white_john, doyon__jennifer, talbott_susan}@prc.comAbstractThe importance of machine translation (MT)in the stream of text-handling processes hasbecome readily apparent in many currentproduction settings as well as in researchprograms such as the TranslingualInformation Detection, Extraction, andSummarization (TIDES) program.
The MTProficiency Scale project has developed ameans of baselining the inherent "tolerance"that a text-handling task has for raw MToutput, and thus how good the output must bein order to be of use to that task.
This methodallows for a prediction of how useful aparticular system can be in a text-handlingprocess stream, whether in integrated, MT-embedded processes, or less integrated user-intensive processes.1 IntroductionIssues of evaluation have been pre-eminent inMT since its beginning, yet there are nomeasures or metrics which are universallyaccepted as standard or adequate.
This is inpart because, at present, different evaluationmethods are required to measure differentattributes of MT, depending on what aparticular stakeholder needs to know (e.g.,Arnold 1993).
A venture capitalist who wantsto invest in an MT start-up needs to know adifferent set of attributes about he system thandoes a developer who needs to see if the mostrecent software changes improved (ordegraded) the system.
Users need to knowanother set of metrics, namely those associatedwith whether the MT system in situ improvesor degrades the other tasks in their overallprocess.
Task-based evaluation of this sort isof particular value because of the recentlyenvisioned role of MT as an embedded part ofproduction processes rather than a stand-alonetranslator's tool.
In this context, MT can bemeasured in terms of its effect on the"downstream" tasks, i.e., the tasks that a useror system performs on the output of the MT.
'The assertion that usefulness could begauged by tasks to which output might beapplied has been used for systems and forprocesses (JEIDA 1992, Albisser 1993), andalso particular theoretical approaches (Churchand Hovy 1991).
However, the potential forrapidly adaptable systems for which MT couldbe expected to run without humanintervention, and to interact flexibly withautomated extraction, summarization, filtering,and document detection calls for an evaluationmethod that measures usefulness across severaldifferent downstream tasks.The U.S. government MT FunctionalProficiency Scale project has conductedmethodology research that has resulted in aranking of text-handling tasks by theirtolerance to MT output.
When an MTsystem's output is mapped onto this scale, theset of tasks for which the output is useful, ornot useful, can be predicted.
The method usedto develop the scale can also be used to map aparticular system onto the scale.Development of the scale required theidentification of the text-handling tasksmembers of a user community perform, andthen the development of exercises to testoutput from several MT systems (Japanese-to-English).
The level of ease users can performthese exercises on the corpus reflects thetolerance that the tasks have for MT output ofvarying quality.
The following sections detailthe identification of text-handling tasks, theevaluation corpus, exercise development, andinference of the proficiency scale .from theapparent tolerance of the downstream text-handling tasks.92 Proficiency ScaleDevelopmentIn order to determine the suitability of MToutput for text-handling tasks, it was necessaryto interview users of text-handling tools toidentify the tasks they actually perform withtranslated material.
It was necessary also tocompile a corpus of translations and createexercises to measure the usefulness of thetranslations.2.1 Task IdentificationExpert user judgments were needed to ensureconfidence in the resulting proficiency scale.The users who provided these judgments workmonolingually on document collections thatinclude translated material.
Preliminaryinterviews were conducted with 17 users.During the preliminary interviews, userscompleted questionnaires providinginformation identifying the text-handling tasksthat ultimately formed the proficiency scale.2.2 Corpus CompositionFor a 1994 evaluation effort, the DefenseAdvanced Research Projects Agency (DARPA)Machine Translation Initiative developed acorpus of 100 general news texts taken fromJapanese newswires.
These texts weretranslated into English and were incorporatedinto what is now known as the "3Q94"evaluation.
A subset of these translations wasused for the MT Functional Proficiency Scaleproject.The 100 3Q94 Japanese source texts weretranslated into six English output versions,four from commercial and research MTsystems (Systran (SY), Pivot (P), Lingstat (L),and Pangloss (PN)), and two from professionalexpert translations (E) used as baseline andcontrol for the 3Q94 evaluations.Translations were selected from all of thesesets for the proficiency scale corpus.
For thepurpose of validating the project's results, twoadditional systems' translations were added toits corpus.
These included translations from acurrent version of Systran (SY2) and TyphoonflY).2.3 Exercise DefinitionsThe user exercises were designed to determineif users could successfully accomplish theirregular tasks with translations of varyingqualities, by eliciting judgments that indicatedthe usefulness of these translations.
A varietyof human factors issues were relevant to thedevelopment of the exercise sets.
Since thetexts to be seen by the users were general newstexts, it was unlikely they would be relevant othe users' usual domains of interest (White andTaylor, 1998 and Taylor and White, 1998).This issue was handled by selecting textsrelated to domains that were thought to besimilar, but broader, than those typicallyhandled by users (White and Taylor, 1998 andTaylor and White, 1998).
Additionally, thesimple elicitation of a judgment (to a questionsuch as "can you do your job with this text")is possibly biased by a predisposition tocooperate (Taylor and White 1998).Therefore, it was necessary to develop twocomplementary sets of exercises: the snapjudgment exercise and the task-specificexercises.
Detailed definitions of these twoexercises can be found in Kathryn B. Taylorand John S. White's paper "Predicting WhatMT is Good for: User Judgments and TaskPerformance" in the Proceedings of the ThirdConference of the Association for MachineTranslation in the Americas, AMTA '98.3 Results3.1 Compilation of ResponsesThe user responses for the snap judgmentexercise are shown in Exhibit 1.
In the snapjudgment exercise, the users were asked tolook at 15 translations and categorize ach asbeing of a good enough quality tosuccessfully complete their text-handling task,i.e., "YES" or "Y," or if they could not usethe translation to perform their task, i.e.,"NO" or "N." The top row of Exhibit 1 liststhe 15 translations by their documentidentification codes.
Each documentidentification code includes a documentnumber followed by the code of the MTsystem that produced it (MT system codes canbe found in the Corpus Composition sectionabove).
The first column of Exhibit 1contains a list of the users who participated inthe snap judgment exercise separated by whichtext-handling task they performed.
The users'responses of "Y" or "N" appear under eachof the translations' document identificationcodes by user.
The snap judgment scores foreach of the text handling tasks was calculated10as the percentage of "Ys" for the corpus of 15translations by all users performing that task.The user responses and results for the gistingexercise are shown in Exhibit 2.
In the gistingexercise, each user was asked to rate decisionpoints in a translation on a 1-5 scale.
The toprow of Exhibit 2 lists the seven documentsseen by the users by their documentidentification codes.
The first column ofExhibit 2 contains a list of users whoparticipated in the gisting exercise.
Userratings averaged for each translation appearunder each of the translation codes for each ofthe users.
The scores for each of thetranslations were calculated by totaling a user'sratings and dividing that total by the numberof decision points contained in the document.The user responses and results for the triageexercise are shown in Exhibit 3.
In the triageexercise, each user was asked to order threeseparate stacks of translations by theirrelevance to a problem statement.
The top rowof Exhibit 3 lists the 15 translations seen bythe users by their document identificationcodes.
The first column of Exhibit 3 containsa list of users who participated in the triageexercise.
User responses of ordinal numberrankings appear under each of the documentidentification codes by user.
Each of thecategory rankings was scored by comparing itsresults to that of a ground truth ranking of thesame translations.The user responses and results for theextraction exercise are shown in Exhibit 4.
Inthe extraction exercise, each user was asked toidentify named entities in each translation:persons, locations, organizations, dates, times,and money/percent.
This extraction exercisewas modeled after the "Named Entity" taskof the Message Understanding Conference(MUC) (Chinchor and Dungca, 1995).Exhibit 4 contains two charts.
The top row ofboth charts contain a list of users whoparticipated in the extraction exercise.
Thefirst column of both charts lists sevendocuments seen by the users by theirdocument identification codes.
In the topchart, recall scores appear under each of theusers for each translation.
In the bottom chart,precision scores appear under each of theusers for each translation.
Recall wascalculated by the number of possible namedentities in a translation the user identified.Precision was calculated by the number ofitems the user identified as being namedentities that were actually named entities.The user responses and results for thefiltering exercise are shown in Exhibit 5.
Inthe filtering exercise, each user was asked tolook at 15 documents to determine if adocument fit into any one of the threecategories of Crime, Economics, orGovernment and Politics, i.e., "YES" or "Y,"none of the three categories, i.e., "NO"  or"N," or if they could not make a decisioneither way, i.e., "CANNOT BEDETERMINED" or "CBD."
Exhibit 5contains two charts.
The top row of bothcharts lists the 15 translations seen by the usersby their document identification codes.
Thefirst column of both charts contains a list ofusers who participated in the filtering exercise.The users' responses of "Y," "N," or "CBD"appear under each of the translations'document identification codes by user.
Theresults of the filtering exercise were calculatedwith the measure of recall.
Recall wascalculated by the number of translateddocuments related to the three categories ofCrime, Economics, and Government andPolitics the user identified.The user responses and results for thedetection exercise are shown in Exhibit 6.
Inthe detection exercise, each user was asked tolook at 15 documents to determine if the.document belonged to the category of Crime(C), the category of Economics (E), thecategory of Government and Politics (G&P),none of the three categories, i.e., "NO"  or"N,"  or if they could not make a decisioneither way, i.e., "CANNOT BEDETERMINED" or "CBD."
Exhibit 6contains three charts.
The top row of all threecharts lists the 15 translations seen by the usersby their document identification codes.
Thefirst column of all three charts contains a listof users who participated in  the detectionexercise.
User responses of "C ,"  "E , ""G&P,"  "CBD,"  or "NOTA" appear undereach of the translations' documentidentification codes by user.
The results of thedetection exercise were calculated with themeasure of recall.
Recall was calculated by thenumber of translated ocuments related toeach of the three categories of Crime,Economics, and Government and Politics theuser identified.3.2 Mapping Results onto ToleranceScaleThe results of the snap judgment exercise areshown in Exhibit 7.
In the snap judgmentexercise each user was asked whether adocument was coherent enough that it could11Exhibit I - ;Snap Judgment ResultsI 205 IE 2070SY2GISTINGUser A 4.46 2.47User B 4.62 3.47User C 4.85 3AVERAGE 4.64 2.98MEAN(MEANS)  2.52ACCEPTABLE YES YES2.15 2.10 2.00 1.93 1.85NO NO NO NO NOExhibit, 2 - Gisting Resultsground Truth~Jser DJeer FL~,er GTOTAL DISTANCEAVG DISTANCEACCEPTABIUTY~RIME UOA=I.052070 2069 2050 2049 2082 2055 20511 2 3 4 5 6 74 I 6 5 7 3 23 I 2 5 CBD 6 42 I 6 4 3 5 76 3 7 2 6 6 78I 2.3333 0,6667 2 1.6667 2.66NO YES NO NO NO2',10 YESECONOMICS UOA=.8762056 2072 2023 2028I 2 3 4I 2 4 3I CBD 2 3I 2 3 40 20 0.667 0.666672 0.666672~{ES YES YES YESExhibit 3 - Triage ResultsRECALL - User H RECALL - User I RECALL - ~ J TOTAL RECALLEXTRACTION2082TY 87.4% 77.7% 77.9% 81%2051E 76.6% 70.5% 84.9% 77.3%20708Y2 63.9% 77.3% 57.0% 66.
I%2055P 69.2% 43.4% 72.0% 61.5%2050SY 57% 53% 57.6% .55.9%2049L 52.8% 57% 47.8% 52.5%2069PN 32,5% :~4.9% ,~1.2% 39.5%PRECISION - User H PRECISION - User I PRECISION - tbet J TOTAL PRECISION2055P 97.2% 97.6% 95.2% 96.6%2082TY 95.2% 100% 91.7% 95.6%2069PN 96.7% 81.7% 100% 92.8%20508Y 88,9% 95.8% 91.1% 91.9%2051E 81.1% 71.1% 92.4% 81.5%2070SY2 76.3% 74.6% 87.2% 79..4%2049L 75.5% 74.
I% 78.
I% 75,9%=GOVERNMENT & POLITICS UOA=.2362078 2046 2012 2004i 3 4 2!
2 3 4!
3 4 2I 2 3 40 1 1 20 0.33..33 0.3,3333 0.666667fES NO NO NOACCEPTABLE:YESAV(I~: YES62% YES m m mNONONONOYESYESAV(P): YES87.7% YESNONONOExhibit 4 - Extraction Results12- -  - -ACCEPTABLE YES YES YES YES NO NO NOExhib i t  5 - Fi l ter ing ResultsCRIME 20491.User N CUser 0 CUser Q CUser P C2050SY 2051E 2055P 2070SY2 2069PN 2082TY~ r  ~ c  c  c E E c c c c c Ec c c c c EC C C C C NOTAACCEPTABLE YESAV(R) 82.1% IYES YES YES YES NO NOuuUserCpGOV & POL IONACCEPTABLE2078L 2046PN 2012SYG&P CG&P | CBD EG&P NOTA I NOTA G&PG&P G&P G&P EG&P NOTA NOTA CBD50%YESAV(R)YES NO NOExhib i t  6 - Detect ion Results13be used to successfully complete their assignedtask exercise.Snap Judg~nentT~Exhibit 7 - Snap Judgment  ResultsThe bars in Exhibit 7 represent thepercentage of affirmatives for the corpus of 15texts by all users.The results for the user exercises needed becomputed in a way which allowed theircomparison across tasks, but which used l:hemetrics relevant o each task at the same time.We address the computation of each of thesein turn.Gisting.
Computing the acceptability cut-offfor gisting follows the general pattern, exceptthat the text scores are not recall or precision.Rather, since gisting judgments were elicitedwith an "adequacy" measure, each text for eachuser has an average of the scores for thedecision points in that text.
In turn, theaverage of these average scores gives thecutoff for acceptability for gisting, namely2.52 out of a minimum of one and maximumof 5.
By this means, 2 texts are identified asacceptable for gisting, indicated in Exhibit 2.Triage.
As shown in Exhibit 3, triagerequires the comparison of ordinal rankings,with ordinal rankings from the ground truthset.
Here, a uniformity of agreement measurewas established, defined as the mean of thestandard deviations for each text in eachproblem statement.
Then the mean for eachtext in the user ranking was compared to theground truth ranking, plus-or-minus theuniformity measure.
A text is acceptable if itmatches the ground truth within theuniformity measure.
Based on thiscomputation, 7 of 15, or 46.7%, of the textsare acceptable for gisting.Extraction.
Extraction was computed usingboth recall and precision measures.
As withfiltering and detection, average recall iscomputed (62%), which is used as the cut-offfor acceptability, and identifies 3 texts asacceptable.
Similarly, the average precision,87.7%, creates a cut-off at 4 texts.
To showextraction as a single value, the total acceptablein precision and in recall are averaged,equaling 3.5, or 50% of the texts in the 7-textset.
These are shown in Exhibit 4.Filtering.
For filtering, user responses arecomputed on two tables conforming to the.ground truth values for each text ("Y" or "N",I.e., whether the text was relevant o crime ornot).
The average recall over all users and all?
texts is 66.7% for Y and 75% for N. Theseaverages create for the Y and N chart therespective cutoff boundaries for "YES" (textoutput is acceptable for filtering) and "NO" (itis not).
The total number of YES's from the Yand N tables is 8 or 53% of the texts in thecorpus acceptable for filtering.
These resultsare illustrated in Exhibit 5.Detection.
As shown in Exhibit 6, there arethree tables in detection, corresponding to thethree domain areas of Crime, Economics, andGovernment and Politics.
As with filtering, theaverage recall is computed for each domainover all users and texts, and this averageestablishes the cut-off boundary ofacceptability of text outputs for detection.
Forthe Crime domain, the average is 82.1%, forEconomics 94%, and for Government andPolitics 50%.
The total number of texts thusidentified as acceptable is 10, or 67% textsacceptable for detection.Exhibit 8 shows the results of the taskexercises.Task ExercisesGISTING TRIAGE EXTRACTION FILTERING DETECTIONTamExhibit 8 - Task Exercises Results14At the inception of this project, weestablished a heuristic scale of task tolerance,based on common understanding of the natureof each of these tasks.
This scale - filtering,detection, triage, extraction, and gisting, morder of tolerance - was not a hypothesis perse; nevertheless, it is rather surprising that theresults vary from the heuristic significantly.The results showed detection to be the mosttolerant task, rather than filtering.
Thepresumption had been that the filtering task,which simply requires a "yes" if a documentis related to a specific topic or "no" if it isnot, could be performed with higher accuracythan the task of detection that requiresclassifying each document by subject matter.In fact, when precision measures are factoredin for filtering and detection (as they were forextraction), filtering appears to be even lesstolerant han extraction.
This outcome seemsplausible when we consider that detection isoften possible ven when only small quantitiesof key words can be found in a document.Also surprising, the triage task was lesstolerant of MT output then expected.
It wassupposed that the ability to rank relevance to aparticular problem could be done withsufficient keywords in otherwise unintelligibletext; rather, a greater depth of understanding isnecessary to successfully complete this task.4 Future ResearchThere are at least two evaluation techniquesthat can use the task tolerance scale to predictthe usefulness of an MT system for aparticular downstream task.
The set ofexercises used to elicit the task tolerancehierarchy reported here can also be used todetermine the position on the scale of aparticular system.
The system translates textsfrom the corpus for which ground truth hasalready been established, and the userexercises are performed on these translations.The result is a set of tasks for which thesystem's output appears to be suitable.
Thepre-existing scale can help to resolveambiguous results, or can be used to makescale-wide inferences from a subset of theexercises: it may be possible to perform justone exercise (e.g., triage) and infer the actualposition of the system on the scale by thedegree of acceptability above or below themlmmum acceptability for triage itself.A second technique offers more potentialfor rapid, inexpensive test and re-test.
Thisinvolves the development of a diagnostic testset (White and Taylor 1998, Taylor and White1998), derived from the same source as theproficiency scale itself.
For every task in theexercise results, there are "borderline" texts,that is, texts acceptable for one task but not forthe next less tolerant task.
These texts willexhibit translation phenomena (grammatical,lexical, orthographic, formatting, etc.)
whichare diagnostic of the difference betweensuitability at one tolerance level and another.The text will also contain phenomena that arenot diagnostic at this level but are at a lesstolerant level.
By characterizing thephenomena that occur in the border texts foreach task, it is possible to determine thephenomena diagnostic to each tolerance level.A pilot investigation of these translationphenomena (Taylor and White 1998, Doyon etal.
1999) categorized the translationphenomena in terms of pedagogy-baseddescriptions of the contrasts between Japaneseand English (Connor-Linton 1995).
Thischaracterization allows for the representationof several individual problem instances with asingle suite of pair-specific, controlled, sourcelanguage patterns designed to test MT systemsfor coverage of each phenomenon.
Thesepatterns may be tested by any MT system forthat language pair, and the results of the testwill indicate where that system falls on theproficiency scale by its successful coverage ofthe diagnostic patterns associated with thattolerance level.The purpose of the user exercises is toestablish a scale of MT tolerance for thedownstream text handling tasks.
However, thesame method can be used to determine theusefulness of a particular system for any ofthe tasks by performing these exercises withthe system to be tested.
It is possible, forexample, to isolate the performance of systemsin the set used here, though the sample sizefrom each system is too small to draw anyconclusions in this case.
We hope to performthis exercises with larger samples both tovalidate these findings and to executeevaluations on candidate MT systems.Among other validation steps in the futurewill be confirmation of the exercise approachfrom an empirical perspective (e.g., whether toinclude "cannot be determined" as a choice),and a validation of the ground truth in thetriage exercise.Finally, we continue to refine the applicationof the methodology to reduce time andincrease user acceptance.
In particular, wehave developed a web-based version of severalof the exercises to make the process easier forthe user and more automatic for scoring.155 ConclusionThe MT Functional Proficiency Scale projecthas not only demonstrated that it is possiblefor poor MT output o be of use for certaintext-handling tasks, but has also indicated thedifferent tolerances each such task has forpossibly poor MT output.This task-based methodology developed inthe MT Functional Proficiency Scale projectusing Japanese-to-English corpora shouldprove useful in evaluating other language pairsystems.
There is also potential for evaluatingother text-handling systems, such assummarization, information retrieval, gisting,and information extraction, in the context ofthe other tasks that might process their output.Task-based evaluations provide a direct wayfor understanding how text-handlJ:~ngtechnologies can interact with each other inend-to-end processes.
In the case of MTsystems, it is possible to predict he effectiveapplicability of MT systems whose outputseems far less than perfect.6 ReferencesAlbisser, D. (1993).
"Evaluation of MTSystems at Union Bank of Switzerland.
"Machine Translation 8-1/2: 25-28.Arnold, A., L. Sadler, and R.
Humphreys.(1993).
"Evaluation: an assessment.
"Machine Translation 8-1/2: 1-24.Chinchor, Nancy, and Gary Dungca.
(1995).
"Four Scorers and Seven Years Ago: TheSconng Method for MUC-6.
"Proceedings of Sixth MessageUnderstanding Conference (MUC-6).Columbia, MD.Church, Kenneth, and Eduard Hovy.
(1991).
"Good Applications for CrummyMachine Translation."
in J. Neal and S.Walter (eds.
), Natural Language ProcessingSystems Evaluation Workshop.
RomeLaboratory Report #RL-TR-91-362.
Pp.147-157.Connor-Linton, Jeff.
(1995).
"Cross-culturalcomparison of writing standards:American ESL and Japanese EFL."
WorldEnglishes, 14.1:99-115.
Oxford: BasilBlackwell.Doyon, Jennifer, Kathryn B. Taylor, and JohnS.
White.
(1999).
"Task-Based Evaluationfor Machine Translation."
Proceedings ofMachine Translation Summit VII '99.Singapore.Japanese Electronic Industry DevelopmentAssociation.
(1992).
"JEIDAMethodology and Criteria on MachineTranslation Evaluation."
Tokyo: JEIDA.Taylor, Kathryn B., and John S. White (1998).
"Predicting what MT is Good for: UserJudgments and Task Performance.
"Proceedings of Third Conference of theAssociation for Machine Translation in theAmericas, AMTA'98.
Philadelphia, PA.White, John S., and Kathryn B. Taylor.
(1998).
"A Task-Oriented Evaluation Metric forMachine Translation."
Proceedings ofLanguage Resources and EvaluationConference, LREC-98, Volume I.
21-27.Grenada, Spain.16
