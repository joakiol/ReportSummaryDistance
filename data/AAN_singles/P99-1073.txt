Designing a Task-Based Evaluation Methodologyfor a Spoken Machine Translation SystemKav i ta  ThomasLanguage Technologies Inst i tuteCarnegie Mellon University5000 Forbes AvenuePittsburgh,  PA 15213, USAkavita@cs, cmu.
eduAbst rac tIn this paper, I discuss issues pertinent o thedesign of a task-based evaluation methodologyfor a spoken machine translation (MT) sys-tem processing human to human communica-tion rather than human to machine communi-cation.
I claim that system mediated human tohuman communication requires new evaluationcriteria and metrics based on goal complexityand the speaker's prioritization of goals.1 In t roduct ionTask-based evaluations for spoken language sys-tems focus on evaluating whether the speaker'stask is achieved, rather than evaluating utter-ance translation accuracy or other aspects ofsystem performance.
Our MT project focuseson the travel reservation domain and facilitateson-line translation of speech between clients andtravel agents arranging travel plans.
Our priorevaluations (Gates et al, 1996) have focusedon end-to-end translation accuracy at the ut-terance level (i.e., fraction of utterances trans-lated perfectly, acceptably, and unacceptably).While this method of evaluation conveys trans-lation accuracy, it does not give any informationabout how many of the client's travel arrange-ment goals have been conveyed, nor does it takeinto account the complexity of the speaker'sgoals and task, or the priority that they assignto their goals; for example, the same end-to-endscore for two dialogues may hide the fact thatin one dialogue the speakers were able to com-municate their most important goals while inthe other they were only able to communicatesuccessfully the less important goals.One common approach to evaluating spokenlanguage systems focusing on human-machinedialogue is to compare system responses to cor-rect reference answers; however, as discussedby (Walker et al, 1997), the set of referenceanswers for any particular user query is tiedto the system's dialogue strategy.
Evaluationmethods independent of dialogue strategy havefocused on measuring the extent to which sys-tems for interactive problem solving aid usersvia log-file evaluations (Polifroni et al, 1992),quantifying repair attempts via turn correctionratio, tracking user detection and correction ofsystem errors (Hirschman and Pao, 1993), andconsidering transaction success (Shriberg et al,1992).
(Danieli and Gerbino, 1995) measurethe dialogue module's ability to recover frompartial failures of recognition or understanding(i.e., implicit recovery) and inappropriate utter-ance ratio; (Simpson and Fraser, 1993) discussapplying turn correction ratio, transaction suc-cess, and contextual appropriateness to dialogueevaluations, and (Hirschman et ah, 1990) dis-cuss using task completion time as a black boxevaluation metric.Current literature on task-based evaluationmethodologies for spoken language systems pri-marily focuses on human-computer interactionsrather than system-mediated human-human in-teractions.
For a multilingual MT system,speakers communicate via the system, whichtranslates their responses and generates the out-put in the target language via speech synthesis.Measuring solution quality (Sikorski and Allen,1995), transaction success, or contextual appro-priateness is meaningless, ince we are not in-terested in measuring how efficient ravel agentsare in responding to clients' queries, but rather,how well the system conveys the speakers' goals.Likewise, task completion time will not cap-ture task success for MT dialogues ince it isdependent on dialogue strategies and speakerstyles.
Task-based evaluation methodologies for569MT systems must focus on whether goals arecommunicated, rather than whether they areachieved.2 Goa ls  o f  a Task -Based  Eva luat ionMethodo logy  for an MT SystemThe goal of a task-based evaluation for an MTsystem is to convey whether speakers' goalswere translated correctly.
An advantage of fo-cusing on goal translation is that it allows us tocompare dialogues where the speakers employdifferent dialogue strategies.
In our project, wefocus on three issues in goal communication:(1) distinction of goals based on subgoal com-plexity, (2) distinction of goals based on thespeaker's prioritization, and (3) distinction ofgoals based on domain.3 Pr io r i t i za t ion  o f  GoalsWhile we want to evaluate whether speakers'important goals are translated correctly, this issometimes difficult to ascertain, since not onlymust the speaker's goals be concisely describ-able and circumscribable, but also they mustnot change while she is attempting to achieveher task.
Speakers usually have a prioritizationof goals that cannot be predicted in advance andwhich differs between speakers; for example, ifone client wants to book a trip to Tokyo, it maybe imperative for him to book the flight ticketsat the least, while reserving rooms in a hotelmight be of secondary importance, and findingout about sights in Tokyo might be of lowestpriority.
However, his goals could be prioritizedin the opposite order, or could change if he findsone goal too difficult to communicate and aban-dons it in frustration.If we insist on eschewing unreliability issuesinherent in asking the client about the priorityof his goals after the dialogue has terminated(and he has perhaps forgotten his earlier prior-ity assignment), we cannot rely on an invariantprioritization of goals across speakers or acrossa dialogue.
The only way we can predict thespeaker's goals at the time he is trying to com-municate them is in cases where his goals are notcommunicated and he attempts to repair them.We can distinguish between cases in which.goalcommunication succeeds or fails, and we cancount the number of repair attempts in bothcases.
The insight is that speakers will attemptto repair higher priority goals more than lowerpriority goals, which they will abandon sooner.The number of repair attempts per goal quan-tifies the speaker's priority per goal to some de-gree.We can capture this information in a sim-ple metric that distinguishes between goals thateventually succeed or fail with at least one re-pair attempt.
Goals that eventually succeedwith tg repair attempts can be given a scoreof 1/tg,  which has a maximum score of 1 whenthere is only one repair attempt, and decays to0 as the number of repair attempts goes to infin-ity.
Similarly, we can give a score of-(1 - 1/tg)to goals that are eventually abandoned with tgrepair attempts; this has a maximum of 0 whenthere is only a single repair attempt and goesto -1 as tg goes to infinity.
So the overall dia-logue score becomes the average over all goals ofthe difference between these two metrics, witha maximum score of 1 and a minimum score of--1.1 for successful goalscore(goa l )  = - (1 tg (1) - ~) for unsuccessful goalscore(d ia logue)  -- 1 n mgoals Z score(goal) (2)goals4 Complex i ty  o f  Goa lsAnother factor to be considered is goal com-plexity; clearly we want to distinguish betweendialogues with the same main goals but in whichsome have many subgoals while others have fewsubgoals with little elaboration.
For instance,one traveller going to Tokyo may be satisfiedwith simply specifying his departure and arrivaltimes for the outgoing and return laps of hisflight, while another may have the additionalsubgoals of wanting a two-day stopover in Lon-don, vegetarian meals, and aisle seating in thenon-smoking section.
In the metric above, bothgoals and subgoals are treated in the same way(i.e., the sum over goals includes ubgoals), andwe are not weighting their scores any differently.While many subgoals require that the maingoal they fall under be communicated for themto be communicated, it is also true that for somespeakers, communicating just the main goal andnot the subgoal may be a communication fail-ure.
For example, if it is crucial for a speaker570to get a stopover in London, even if his maingoal (requesting a return flight from New Yorkto Tokyo) is successfully communicated, hewillview the communication attempt a failure un-less the system communicates the stopover suc-cessfully also.
On the other hand, communi-cating the subgoal (e.g., a stopover in London),without communicating the main goal is non-sensical - the travel agent will not know whatto make of "a stopover in London" without theaccompanying main goal requesting the flight toTokyo.However, even if two dialogues have the samegoals and subgoals, the complexity of the trans-lation task may differ; for example, if in onedialogue (A) the speaker communicates a singlegoal or subgoal per speaker turn, while in theother (B) the speaker communicates the goaland all its subgoals in the same speaker turn,it is clear that the dialogue in which the entiregoal structure is conveyed in the same speakerturn will be the more difficult translation task.We need to be able to account for the averagegoal complexity per speaker turn in a dialogueand scale the above metric accordingly; if dia-logues A and B have the same score accordingto the given metric, we should boost the scoreof B to reflect that it has required a more rigor-ous translation effort.
A first attempt would beto simply multiply the score of the dialogue bythe average subgoal complexity per main goalper speaker turn in the dialogue, where Nmg isthe number of main goals in a speaker turn andNsg is the number of subgoals.
In the metricbelow, the average subgoal complexity is 1 forspeaker turns in which there are no subgoals,and increases as the number of subgoals in thespeaker turn increases.score'(dialogue) = score(dialogue) ?1 .Nsg + Nmgnumspkturns ~--~" \[ ~r--m~ \] (3) spkturns5 Our  Task -Based  Eva luat ionMethodo logyScoring a dialogue is a coding task; scorers willneed to be able to distinguish goals and subgoalsin the domain.
We want to minimize train-ing for scorers while maximizing agreement be-tween them.
To do so, we list a predefined setof main goals (e.g., making flight arrangementsor hotel bookings) and group together all sub-goals that pertain to these main goals in a two-level tree.
Although this formalization sacrificessubgoal complexity, we are unable to determinethis without predefining a subgoal hierarchy andwe want to avoid predefining subgoal priority,which is set by assigning a subgoal hierarchy.After familiarizing themselves with the set ofmain goals and their accompanying subgoals,scorers code a dialogue by distinguishing in aspeaker turn between the main goals and sub-goals, whether they are successfully communi-cated or not, and the number of repair attemptsin successive speaker turns.
Scorers must alsoindicate which domain each goal falls under; wedistinguish goals as in-domain (i.e., referring tothe travel-reservation domain), out-of-domain(i.e., unrelated to the task in any way), andcross-domain (i.e., discussing the weather, com-mon polite phrases, accepting, negating, open-ing or closing the dialogue, or asking for re-peats).The distinction between domains is impor-tant in that we can separate in-domain goalsfrom cross-domain goals; cross-domain goals of-ten serve a meta-level purpose in the dialogue.We can thus evaluate performance over all goalswhile maintaining a clear performance measurefor in-domain goals.
Scores should be calculatedseparately based on domain, since this will indi-cate system performance more specifically, andprovide a useful metric for grammar develop-ers to compare subsequent and current domainscores for dialogues from a given scenario.In a large scale evaluation, multiple pairs ofspeakers will be given the same scenario (i.e., aspecific task to try and accomplish; e.g., flyingto Frankfurt, arranging a stay there for 2 nights,sightseeing to the museums, then flying on toTokyo}; domain scores will then be calculatedand averaged over all speakers.Actual evaluation is performed on transcriptsof dialogues labelled with information from sys-tem logs; this enables us to see the original ut-terance (human transcription} and evaluate thecorrectness of the target output.
If we wishto, log-file evaluations also permit us to eval-uate the system in a glass-box approach, evalu-ating individual system components separately(Simpson and Fraser, 1993).5716 Conc lus ions  and  Future  WorkThis work describes an initial attempt to ac-count for some of the significant issues in a task-based evaluation methodology for an MT sys-tem.
Our choice of metric reflects separate do-main scores, factors in subgoal complexity andnormalizes all counts to allow for comparisonamong dialogues that differ in dialogue strat-egy, subgoal complexity, number of goals andspeaker-prioritization f goals.
The proposedmetric is a first attempt, and describes work inprogress; we have attempted to present he sim-plest possible metric as an initial approach.There are many issues that need to be ad-dressed; for instance, we do not take into ac-count optimality of translations.
Although weare interested in goal communication and notutterance translation quality, the disadvantageto the current approach is that our optimalitymeasure is binary, and does not give any infor-mation about how well-phrased the translatedtext is.
More significantly, we have not resolvedwhether to use metric (1) for both subgoals andgoals together, or to score them separately.
Theproposed metric does not reflect that commu-nicating main goals may be essential to com-municating their subgoals.
It also does not ac-count for the possible complexity introduced bymultiple main goals per speaker turn.
We alsodo not account for the possibility that in anunsuccessful dialogue, a speaker may becomemore frustrated as the dialogue proceeds, andher relative goal priorities may no longer be re-flected in the number of repair attempts.
Wemay also want to further distinguish in-domainscores based on sub-domain (e.g., flights, ho-tels, events).
Perhaps most importantly, we stillneed to conduct a full-scale evaluation with theabove metric with several scorers and speakerpairs across different versions of the system tobe able to provide actual results.7 AcknowledgementsI would like to thank my advisor Lori Levin,Alon Lavie, Monika Woszczyna, and Aleksan-dra Slavkovic for their help and suggestions withthis work.guage system.
In Proceeedings of the 1995AAAI Spring Symposium on Empirical Meth-ods in Discourse Interpretation and Genera-tion, pages 34-39.L.Hirschman, D.Dahl, D.P.McKay,L.M.Norton, M.C.Linebarger.
1990.
Be-yond class A: A proposal for automaticevaluation of discourse.
In Proceedings ofthe Speech and Natural Language Workshop,pages 109-113.L.Hirschman and C.Pao.
1993.
The cost of er-rors in a spoken language system.
In Pro-ceedings of the Third European Conferenceon Speech Communication and Technology,pages 1419-1422.J.Polifroni, L.Hirschman, S.Seneff, and V.Zue.1992.
Experiments in evaluating interactivespoken language systems.
In Proceedings ofthe DARPA Speech and NL Workshop, pages28-31.E.Shriberg, E.Wade, and P.Price.
1992.Human-machine problem solving using spo-ken language systems (sls): Factors affectingperformance and user satisfaction.
In Pro-ceedings of the DARPA Speech and NL Work-shop, pages 49-54.T.Sikorski and J.Allen.
1995.
A task-based evaluation of the TRAINS-95 dia-logue system.
Technical report, University ofRochester.A.Simpson, and N.A.Fraser.
1993.
Black boxand glass box evaluation of the SUNDIALsystem.
In Proceedings of the Third Euro-pean Conference on Speech Communicationand Technology, pages 1423-1426.M.Walker, D.J.Litman, C.A.Kamm, andA.Abella.
1997.
PARADISE: A frameworkfor evaluating spoken dialogue agents.
Tech-nical Report TR 97.26.1, AT and T TechnicalReports.D.Gates, A.Lavie, L.Levin, A.Waibel,M.Gavalda, L.Mayfield, M.Woszczyna,P.Zhan.
1996.
End-to-end Evaluation inJANUS: a Speech-to-speech TranslationSystem.
In Proceedings of the 12th Euro-pean Conference on Artificial Intelligence,Workshop on Dialogue, Budapest, Hungary.ReferencesM.Danieli and E.Gerbino.
1995.
Metrics forevaluating dialogue strategies in a spoken lan-572
