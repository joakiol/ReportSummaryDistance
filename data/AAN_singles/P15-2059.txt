Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 358?364,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDocument Level Time-anchoring for TimeLine ExtractionEgoitz Laparra, Itziar Aldabe, German RigauIXA NLP group, University of the Basque Country (UPV/EHU){egoitz.laparra,itziar.aldabe,german.rigau}@ehu.eusAbstractThis paper investigates the contributionof document level processing of time-anchors for TimeLine event extraction.We developed and tested two different sys-tems.
The first one is a baseline systemthat captures explicit time-anchors.
Thesecond one extends the baseline systemby also capturing implicit time relations.We have evaluated both approaches in theSemEval 2015 task 4 TimeLine: Cross-Document Event Ordering.
We empiri-cally demonstrate that the document-basedapproach obtains a much more completetime anchoring.
Moreover, this approachalmost doubles the performance of the sys-tems that participated in the task.1 IntroductionTemporal relation extraction has been the topic ofdifferent SemEval tasks (Verhagen et al, 2007;Verhagen et al, 2010; UzZaman et al, 2013;Llorens et al, 2015) and other challenges as the6th i2b2 NLP Challenge (Sun et al, 2013).
Thesetasks focused mainly on the temporal relations ofthe events with respect to other events or time ex-pressions, and their goals are to discover which ofthem occur before, after or simultaneously to oth-ers.
Recently, SemEval 2015 included a novel taskregarding temporal information extraction (Mi-nard et al, 2015).
The aim of SemEval 2015 task4 is to order in a TimeLine the events in which atarget entity is involved and presents some signifi-cant differences with respect to previous exercises.First, the temporal information must be recoveredfrom different sources in a cross-document way.Second, the TimeLines are focused on the eventsinvolving just a given entity.
Finally, unlike pre-vious challenges, SemEval 2015 task 4 requires aquite complete time anchoring.
This work focusesmainly on this latter point.
We show that the tem-poral relations that explicitly connect events andtime expressions are not enough to obtain a fulltime-anchor annotation and, consequently, pro-duce incomplete TimeLines.
We propose that fora complete time-anchoring the temporal analysismust be performed at a document level in order todiscover implicit temporal relations.
We present apreliminary approach that obtains, by far, the bestresults on the main track of SemEval 2015 task 4.2 Related workThe present work is closely related to previous ap-proaches involved in TempEval campaigns (Ver-hagen et al, 2007; Verhagen et al, 2010; Uz-Zaman et al, 2013; Llorens et al, 2015).
Inthese works, the problem can be seen as a clas-sification task for deciding the type of the tempo-ral link that connects two different events or anevent and a temporal expression.
For that reason,the task has been mainly addresed using super-vised techniques.
For example, (Mani et al, 2006;Mani et al, 2007) trained a MaxEnt classifier us-ing training data which were bootstrapped by ap-plying temporal closure.
(Chambers et al, 2007)focused on event-event relations using previouslylearned event attributes.
More recently, (D?Souzaand Ng, 2013) combined hand-coded rules withsome semantic and discourse features.
(Laokulratet al, 2013) obtained the best results on TempE-val 2013 annotating sentences with predicate-rolestructures, while (Mirza and Tonelli, 2014) affirmthat using a simple feature set results in better per-formances.However, recent works like (Chambers et al,2014) have pointed out that these tasks coverjust a part of all the temporal relations that canbe inferred from the documents.
Furthermore,time-anchoring is just a part of the works pre-sented above.
Our approach aims to extend thesestrategies and it is based on other research lines358involving the extraction of implicit information(Palmer et al, 1986; Whittemore et al, 1991;Tetreault, 2002).
Particularly, we are inspired byrecent works on Implicit Semantic Role Labelling(ISRL) (Gerber and Chai, 2012) and very speciallyon the work by (Blanco and Moldovan, 2014) whoadapted the ideas about ISRL to focus on modi-fiers, including arguments of time, instead of corearguments or roles.
As the SemEval 2015 task 4does not include any training data we decided todevelop a deterministic algorithm of the type of(Laparra and Rigau, 2013) for ISRL.3 TimeLine: Cross-Document EventOrderingIn the SemEval task 4 TimeLine: Cross-DocumentEvent Ordering (Minard et al, 2015), given a setof documents and a target entity, the aim is to builda TimeLine by detecting the events in which theentity is involved and anchoring these events tonormalized times.
Thus, a TimeLine is a collec-tion of ordered events in time relevant for a partic-ular entity.
TimeLines contain relevant events inwhich the target entity participates as ARG0 (i.eagent) or ARG1 (i.e.
patient) as defined in Prop-Bank (Palmer et al, 2005).1The target entities canbe people, organization, product or financial enti-ties and the annotation of time anchors is based onTimeML.For example, given the entity Steve Jobs, aTimeLine contains the events with the associatedordering in the TimeLine and the time anchor:1 2004 18135-7-fighting2 2005-06-05 1664-2-keynote...4 2011-08-24 18315-2-step downThe dataset used for the task is composed of ar-ticles from Wikinews.
The trial data consists of 30documents about ?Apple Inc.?
and gold standardTimeLines for six target entities.
The test corpusconsists of 3 sets of 30 documents around threetopics and 38 target entities.
The topics are ?Air-bus and Boeing?, ?General Motors, Chrysler andFord?
and ?Stock Market?.The evaluation used in the task is based on themetric previously introduced in TempEval-3 (Uz-Zaman et al, 2013).
The metric captures the tem-1For more information consult http://tinyurl.com/owyuybbporal awareness of an annotation (UzZaman andAllen, 2011) based on temporal closure graphs.In order to calculate the precision, recall and F1score, the TimeLines are first transformed into agraph representation.
For that, the time anchorsare represented as TIMEX3 and the events are re-lated to the corresponding TIMEX3 by means ofthe SIMULTANEOUS relation type.
In addition,BEFORE relation types are created to representthat one event happens before another one and SI-MULTANEOUS relation types to refer to eventshappening at the same time.
The official scoresare based on the micro-average of F1 scores.The main track of the task (Track A) consistsof building TimeLines providing only the raw textsources.
Two systems participated in the task.
Theorganisers also defined a Track B where gold eventmentions were given.
In this case, two differentsystems sent results.
For both tracks, a sub-trackin which the events are not associated to a timeanchor was also presented.In this work, we focus on the main track of thetask.
We believe the main track is the most chal-lenging one as no annotated data is provided.
In-deed, WHUNLP 1 was the best run and achievedan F1 of 7.28%.Three runs were submitted.
The WHUNLPteam used the Stanford CoreNLP and they applieda rule-based approach to extract the entities andtheir predicates.
They also performed temporalreasoning.2The remaining two runs were submit-ted using the SPINOZA VU system (Caselli et al,2015).
They performed entity resolution, event de-tection, event-participant linking, coreference res-olution, factuality profiling and temporal process-ing at document and cross-document level.
Then,the TimeLine extractor built a global timeline be-tween all events and temporal expressions regard-less of the target entities and then it extracted thetarget entities for the TimeLines.
The participantsalso presented an out of the competition systemwhich anchors events to temporal expressions ap-pearing not only in the same sentence but also inthe previous and following sentences.4 Baseline TimeLine extractionIn this section we present a system that buildsTimeLines which contain events with explicittime-anchors.
We have defined a three step pro-2Unfortunately, the task participants did not submit a pa-per with the description of the system.359cess to build TimeLines.
Given a set of documentsand a target entity, the system first obtains theevents in which the entity is involved.
Second, itobtains the time-anchors for each of these events.Finally, it sorts the events according to their time-anchors.
For steps 1 and 2 we apply a pipeline oftools (cf.
section 4.1) that provides annotations atdifferent levels.4.1 NLP processingDetecting mentions of events, entities and time ex-pressions in text requires the combination of vari-ous Natural Language Processing (NLP) modules.We apply a generic pipeline of linguistic tools thatincludes Named-Entity Recognition (NER) andDisambiguation (NED), Co-reference Resolution(CR), Semantic Role Labelling (SRL), Time Ex-pressions Identification (TEI) and Normalization(TEN), and Temporal Relation Extraction (TRE).The NLP processing is based on the NewsReaderpipeline (Agerri et al, 2014a), version 2.1.
Next,we present the different tools in our pipeline.Named-Entity Recognition (NER) and Dis-ambiguation (NED): We perform NER using theixa-pipe-nerc that is part of IXA pipes (Agerri etal., 2014b).
The module provides very fast modelswith high performances, obtaining 84.53 in F1 onCoNLL tasks.
Our NED module is based on DB-pedia Spotlight (Daiber et al, 2013).
We have cre-ated a NED client to query the DBpedia Spotlightserver for the Named entities detected by the ixa-pipe-nerc module.
Using the best parameter com-bination, the best results obtained by this moduleon the TAC 2011 dataset were 79.77 in precisionand 60.67 in recall.
The best performance on theAIDA dataset is 79.67 in precision and 76.94 inrecall.Coreference Resolution (CR): In this case, weuse a coreference module that is loosely based onthe Stanford Multi Sieve Pass sytem (Lee et al,2011).
The system consists of a number of rule-based sieves that are applied in a deterministicmanner.
The system scores 56.4 F1 on CoNLL2011 task, around 3 points worse than the systemby (Lee et al, 2011).Semantic Role Labelling (SRL): SRL is per-formed using the system included in the MATE-tools (Bj?orkelund et al, 2009).
This system re-ported on the CoNLL 2009 Shared Task a labelledsemantic F1 of 85.63 for English.Time Expression Identification (TEI) andNormalization (TEN): We use the time modulefrom TextPro suite (Pianta et al, 2008) to capturethe tokens corresponding to temporal expressionsand to normalize them following TIDES specifica-tion.
This module is trained on TempEval3 data.The average results for English is: 83.81% preci-sion, 75.94% recall and 79.61% F1 values.Time Relation Extraction (TRE): We ap-ply the temporal relation extractor module fromTextPro to extract and classify temporal relationsbetween an event and a time expression.
Thismodule is trained using yamcha tool on the Tem-pEval3 data.
The result for relation classificationon the corpus of TempEval3 is: 58.8% precision,58.2% recall and 58.5% F1.4.2 TimeLine extractionOur TimeLine extraction system uses the linguis-tic information provided by the pipeline.
The pro-cess to extract the target entities, the events andtime-anchors can be described as follows:(1) Target entity identification: The target en-tities are identified by the NED module.
As theycan be expressed in several forms, we use theredirect links contained in DBpedia to extend thesearch of the events involving those target enti-ties.
For example, if the target entity is Toyotathe system would also include events involving theentities Toyota Motor Company or Toyota MotorCorp.
In addition, as the NED does not alwaysprovide a link to DBpedia, we also consider thematching of the wordform of the head of the argu-ment with the head of the target entity.
(2) Event selection: We use the output of theSRL module to extract the events that occur in adocument.
Given a target entity, we combine theoutput of the NER, NED, CR and SRL to obtainthose events that have the target entity as filler oftheir ARG0 or ARG1.
We also set some con-straints to select certain events according to thespecification of the SemEval task.
That is, we onlyreturn those events that are not negated and are notaccompanied by modal verbs except will.
(3) Time-anchoring: We extract the time-anchors from the output of the TRE and SRL.From the TRE, we extract as time-anchors thoserelations between events and time-expressionsidentified as SIMULTANEOUS.
From the SRL,we extract as time-anchors those ARG-TMP re-lated to time expressions.
In both cases we use thetime-expression returned by the TEI module.
The360tests performed on the trial data show that the bestchoice for time-anchoring is combining both op-tions.
For each time anchor we normalize the timeexpression using the output of the TEN module.The TimeLine extraction process described fol-lowing this approach builds TimeLines for eventswith explicit time-anchors.
We call this systemBTE and it can be seen as a baseline since we be-lieve that the temporal analysis should be carriedout at document level.
Section 5 presents our strat-egy for improving the time-anchoring carried outby our baseline system.5 Document level time-anchoringThe explicit time anchors provided by the NLPtools presented in Section 4.1 do not cover the fullset of events involving a particular entity.
That is,most of the events do not have an explicit time an-chor and therefore are not captured as part of theTimeLine of that entity.
Thus, we need to recoverthe time-anchors that appear implicitly in the text.In this preliminary work, we propose a simplestrategy that tries to capture implicit time-anchorswhile maintaining the coherence of the temporalinformation in the document.
As said in Section2, this strategy follows previous works on ImplicitSemantic Role Labelling.The rationale behind the algorithm 1 is that bydefault the events of an entity that appear in a doc-ument tend to occur at the same time as previousevents involving the same entity, except stated ex-plicitly.
For example, in Figure 1 all the eventsinvolving Steve Jobs, like gave and announced,are anchored to the same time-expression Mon-day although this only happens explicitly for thefirst event gave.
The example also shows how forother events that occur in different times the time-anchor is also mentioned explicitly, like for thoseevents that involve the entities Tiger and Mac OSX Leopard.Algorithm 1 starts from the annotation obtainedby the tools described in Section 4.1.
For a par-ticular entity a list of events (eventList) is cre-ated sorted by its occurrence in the text.
Then,for each event in this list the system checks if thatevent has already a time-anchor (eAnchor).
Ifthis is the case, the time-anchor is included in thelist of default time-anchors (defaultAnchor) forthe following events of the entity with the sameverb tense (eTense).
If the event does not havean explicit time-anchor but the system has founda time-anchor for a previous event belonging tothe same tense (defaultAnchor[eTense]), thistime-anchor is also assigned to the current event(eAnchor).
If none of the previous conditions sat-isfy, the algorithm anchors the event to the Docu-ment Creation Time (DCT) and sets this time-expression as the default time-anchor for the fol-lowing events with the same tense.Algorithm 1 Implicit Time-anchoring1: eventList = sorted list of events of an entity2: for event in eventList do3: eAnchor = time anchor of event4: eTense = verb tense of event5: if eAnchor not NULL then6: defaultAnchor[eTense] = eAnchor7: else if defaultAnchor[eTense] notNULL then8: eAnchor = defaultAnchor[eTense]9: else10: eAnchor = DCT11: defaultAnchor[eTense] = DCT12: end if13: end forNote that the algorithm 1 strongly depends onthe tense of the events.
As this information can beonly recovered from verbal predicates, this strat-egy cannot be applied to events described by nom-inal predicates.
For these cases just explicit time-anchors are taken into account.The TimeLine is built ordering the events ac-cording to the time-anchors obtained both explic-itly and implicitly.
We call this system DLT.6 ExperimentsWe have evaluated our two TimeLine extractors onthe main track of the SemEval 2015 task 4.
Twosystems participated in this track, WHUNLP andSPINOZAVU, with three runs in total.
Their per-formances in terms of Precision (P), Recall (R)and F1-score (F1) are presented in Table 6.
Wealso present in italics additional results of bothsystems.
On the one hand, the results of a cor-rected run of the WHUNLP system provided bythe SemEval organizers.
On the other hand, theresults of an out of the competition version ofthe SPINOZAVU team explained in (Caselli et al,2015).
The best run is obtained by the correctedversion of WHUNLP 1 with an F1 of 7.85%.
Thelow figures obtained show the intrinsic difficultyof the task, specially in terms of Recall.361Figure 1: Example of document-level time-anchoring.Table 6 also contains the results obtained by oursystems.
We present two different runs.
On theone hand, we present the results obtained usingjust the explicit time-anchors provided by BTE.As it can be seen, the results obtained by thisrun are similar to those obtained by WHUNLP 1.On the other hand, the results of the implicittime-anchoring approach (DLT) outperforms byfar our baseline and all previous systems appliedto the task.
To check that these results are notbiased by the time-relation extractor we use inour pipeline (TimePro), we reproduce the perfor-mances of BTE and DLT using another system toobtain the time-relations.
For this purpose we haveused CAEVO by (Chambers et al, 2014).
The re-sults obtained in this case show that the improve-ment obtained by our proposal is quite similar, re-gardless of the time-relation extractor chosen.System P R F1SPINOZAVU-RUN-1 7.95 1.96 3.15SPINOZAVU-RUN-2 8.16 0.56 1.05WHUNLP 1 14.10 4.90 7.28OC SPINOZA VU - - 7.12WHUNLP 1 14.59 5.37 7.85BTE 26.42 4.44 7.60DLT 20.67 10.95 14.31BTE caevo 17.56 4.86 7.61DLT caevo 17.02 12.09 14.13Table 1: Results on the SemEval-2015 taskThe figures in Table 6 seem to prove our hy-pothesis.
In order to obtain a full time-anchoringannotation, the temporal analysis must be carriedout at a document level.
The TimeLine extractoralmost doubles the performance by just includinga straightforward strategy as the one described inSection 5.
As expected, Table 6 shows that thisimprovement is much more significant in terms ofRecall.7 Conclusion and future-workIn this work we have shown that explicit tempo-ral relations are not enough to obtain a full time-anchor annotation of events.
We have proved theneed of a temporal analysis at document level.For that, we have proposed a simple strategy thatacquires implicit relations and it obtains a morecomplete time-anchoring.3The approach has beenevaluated on the TimeLine extraction task and theresults show that the performance can be doubledwhen using implicit relations.
As future work, weplan to explore in more detail this research lineby applying more sophisticated approaches in thetemporal analysis at document level.However, this is not the only research line thatwe want to go in depth.
The errors that the toolsof the pipeline are producing have a direct impacton the final result of our TimeLine extractors.
Ina preliminary analysis, we have noticed that this isspecially critical when detecting the events givena target entity.
Our pipeline does not detect allmentions of the target entities.
That is why we areplanning an in-depth error analysis of the pipelinein order to find the best strategy to improve on thelinguist analyses and the TimeLine extraction.8 AcknowledgmentWe are grateful to the anonymous reviewersfor their insightful comments.
This work hasbeen partially funded by SKaTer (TIN2012-38584-C06-02) and NewsReader (FP7-ICT-2011-8-316404), as well as the READERS project withthe financial support of MINECO (CHIST-ERAREADERS project - PCIN-2013-002-C02-01).3Publicly available at http://adimen.si.ehu.es/web/DLT362ReferencesRodrigo Agerri, Itziar Aldabe, Zuhaitz Beloki,Egoitz Laparra, Maddalen Lopez de Lacalle,German Rigau, Aitor Soroa, Antske Fokkens,Ruben Izquierdo, Marieke van Erp, Piek Vossen,Christian Girardi, and Anne-Lyse Minard.2014a.
Event detection, version 2.
NewsreaderDeliverable 4.2.2. http://www.newsreader-project.eu/files/2012/12/NWR-D4-2-2.pdf.Rodrigo Agerri, Josu Bermudez, and German Rigau.2014b.
IXA pipeline: Efficient and Ready to UseMultilingual NLP tools.
In Proceedings of the NinthInternational Conference on Language Resourcesand Evaluation (LREC-2014).
00013.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Compu-tational Natural Language Learning: Shared Task,CoNLL ?09, pages 43?48, Boulder, Colorado, USA.Eduardo Blanco and Dan Moldovan.
2014.
Leverag-ing verb-argument structures to infer semantic re-lations.
In Proceedings of the 14th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 145?154, Gothenburg,Sweden.Tommaso Caselli, Antske Fokkens, Roser Morante,and Piek Vossen.
2015.
SPINOZA VU: An nlppipeline for cross document timelines.
In Proceed-ings of the 9th International Workshop on SemanticEvaluation (SemEval 2015), pages 786?790, Den-ver, Colorado, June 4-5.Nathanael Chambers, Shan Wang, and Dan Juraf-sky.
2007.
Classifying temporal relations betweenevents.
In Proceedings of the 45th Annual Meetingof the ACL on Interactive Poster and DemonstrationSessions, ACL?07, pages 173?176, Prague, CzechRepublic.Nathanael Chambers, Taylor Cassidy, Bill McDowell,and Steven Bethard.
2014.
Dense event orderingwith a multi-pass architecture.
Transactions of theAssociation for Computational Linguistics, 2:273?284.Joachim Daiber, Max Jakob, Chris Hokamp, andPablo N. Mendes.
2013.
Improving efficiency andaccuracy in multilingual entity extraction.
In Pro-ceedings of the 9th International Conference on Se-mantic Systems (I-Semantics).Jennifer D?Souza and Vincent Ng.
2013.
Classifyingtemporal relations with rich linguistic knowledge.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NACL?13, pages 918?927, Atlanta, Georgia.Matthew Gerber and Joyce Chai.
2012.
Semantic rolelabeling of implicit arguments for nominal predi-cates.
Computational Linguistics, 38(4):755?798,December.Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-ruoka, and Takashi Chikayama.
2013.
Uttime:Temporal relation classification using deep syntacticfeatures.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 88?92,Atlanta, Georgia, USA.Egoitz Laparra and German Rigau.
2013.
Impar: Adeterministic algorithm for implicit semantic role la-belling.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL 2013), pages 33?41.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, CONLL Shared Task ?11, Portland, Oregon.Hector Llorens, Nathanael Chambers, Naushad UzZa-man, Nasrin Mostafazadeh, James Allen, and JamesPustejovsky.
2015.
Semeval-2015 task 5: Qa tem-peval - evaluating temporal information understand-ing with question answering.
In Proceedings of the9th International Workshop on Semantic Evaluation(SemEval 2015), pages 792?800, Denver, Colorado,June.Inderjeet Mani, Marc Verhagen, Ben Wellner,Chong Min Lee, and James Pustejovsky.
2006.
Ma-chine learning of temporal relations.
In Proceedingsof the 21st International Conference on Compu-tational Linguistics and the 44th Annual Meetingof the Association for Computational Linguistics,ACL?06, pages 753?760, Sydney, Australia.Inderjeet Mani, Ben Wellner, Marc Verhagen, andJames Pustejovsky.
2007.
Three approaches tolearning tlinks in timeml.
Technical report.Anne-Lyse Minard, Manuela Speranza, Eneko Agirre,Itziar Aldabe, Marieke van Erp, Bernardo Magnini,German Rigau, and Ruben Urizar.
2015.
Semeval-2015 task 4: Timeline: Cross-document event order-ing.
In Proceedings of the 9th International Work-shop on Semantic Evaluation (SemEval 2015), pages778?786, Denver, Colorado, June 4?5.Paramita Mirza and Sara Tonelli.
2014.
Classifyingtemporal relations with simple features.
In Proceed-ings of the 14th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 308?317, Gothenburg, Sweden, April.
Asso-ciation for Computational Linguistics.Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-man, Lynette Hirschman, Marcia Linebarger, andJohn Dowding.
1986.
Recovering implicit infor-mation.
In Proceedings of the 24th annual meetingon Association for Computational Linguistics, ACL?86, pages 10?19, New York, New York, USA.363Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106, March.Emanuele Pianta, Christian Girardi, and RobertoZanoli.
2008.
The textpro tool suite.
In Proceed-ings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC?08), Mar-rakech, Morocco, may.Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner.2013.
Evaluating temporal relations in clinicaltext: 2012 i2b2 Challenge.
Journal of the Amer-ican Medical Informatics Association, 20(5):806?813, September.Joel R. Tetreault.
2002.
Implicit role reference.
In In-ternational Symposium on Reference Resolution forNatural Language Processing, pages 109?115, Ali-cante, Spain.Naushad UzZaman and James Allen.
2011.
Temporalevaluation.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 351?356, Portland, Oregon, USA.Naushad UzZaman, Hector Llorens, Leon Derczyn-ski, James Allen, Marc Verhagen, and James Puste-jovsky.
2013.
Semeval-2013 task 1: Tempeval-3:Evaluating time expressions, events, and temporalrelations.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 2:Proceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013), SemEval?13, pages 1?9, Atlanta, Georgia, USA.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporalrelation identification.
In Proceedings of the 4th In-ternational Workshop on Semantic Evaluations, Se-mEval ?07, pages 75?80, Prague, Czech Republic.Marc Verhagen, Roser Saur?
?, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, SemEval ?10,pages 57?62, Los Angeles, California.Greg Whittemore, Melissa Macpherson, and GregCarlson.
1991.
Event-building through role-fillingand anaphora resolution.
In Proceedings of the 29thannual meeting on Association for ComputationalLinguistics, ACL ?91, pages 17?24, Berkeley, Cal-ifornia, USA.364
