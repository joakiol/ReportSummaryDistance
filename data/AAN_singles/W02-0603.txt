Unsupervised Discovery of MorphemesMathias Creutz and Krista LagusNeural Networks Research CentreHelsinki University of TechnologyP.O.Box 9800, FIN-02015 HUT, Finland{Mathias.Creutz, Krista.Lagus}@hut.fiAbstractWe present two methods for unsupervisedsegmentation of words into morpheme-like units.
The model utilized is espe-cially suited for languages with a richmorphology, such as Finnish.
The firstmethod is based on the Minimum Descrip-tion Length (MDL) principle and worksonline.
In the second method, Max-imum Likelihood (ML) optimization isused.
The quality of the segmentations ismeasured using an evaluation method thatcompares the segmentations produced toan existing morphological analysis.
Ex-periments on both Finnish and Englishcorpora show that the presented methodsperform well compared to a current state-of-the-art system.1 IntroductionAccording to linguistic theory, morphemes are con-sidered to be the smallest meaning-bearing ele-ments of language, and they can be defined in alanguage-independent manner.
However, no ade-quate language-independent definition of the wordas a unit has been agreed upon (Karlsson, 1998,p.
83).
If effective methods can be devised for theunsupervised discovery of morphemes, they couldaid the formulation of a linguistic theory of mor-phology for a new language.It seems that even approximative automated mor-phological analysis would be beneficial for manynatural language applications dealing with large vo-cabularies.
For example, in text retrieval it is cus-tomary to preprocess texts by returning words totheir base forms, especially for morphologically richlanguages.Moreover, in large vocabulary speech recognition,predictive models of language are typically used forselecting the most plausible words suggested by anacoustic speech recognizer (see, e.g., Bellegarda,2000).
Consider, for example the estimation of thestandard n-gram model, which entails the estima-tion of the probabilities of all sequences of n words.When the vocabulary is very large, say 100 000words, the basic problems in the estimation of thelanguage model are: (1) If words are used as ba-sic representational units in the language model, thenumber of basic units is very high and the estimatedword n-grams are poor due to sparse data.
(2) Dueto the high number of possible word forms, manyperfectly valid word forms will not be observed atall in the training data, even in large amounts of text.These problems are particularly severe for languageswith rich morphology, such as Finnish and Turkish.For example, in Finnish, a single verb may appear inthousands of different forms (Karlsson, 1987).The utilization of morphemes as basic representa-tional units in a statistical language model instead ofwords seems a promising course.
Even a rough mor-phological segmentation could then be sufficient.On the other hand, the construction of a comprehen-sive morphological analyzer for a language basedon linguistic theory requires a considerable amountof work by experts.
This is both slow and expen-sive and therefore not applicable to all languages.July 2002, pp.
21-30.
Association for Computational Linguistics.ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,Morphological and Phonological Learning: Proceedings of the 6th Workshop of theTable 1: The morphological structure of the Finnishword for ?also for [the] coffee drinker?.Word kahvinjuojallekinMorphs kahvi n juo ja lle kinTransl.
coffee of drink -er for alsoThe problem is further compounded as languagesevolve, new words appear and grammatical changestake place.
Consequently, it is important to developmethods that are able to discover a morphology fora language based on unsupervised analysis of largeamounts of data.As the morphology discovery from untagged cor-pora is a computationally hard problem, in practiceone must make some assumptions about the struc-ture of words.
The appropriate specific assumptionsare somewhat language-dedependent.
For example,for English it may be useful to assume that wordsconsist of a stem, often followed by a suffix and pos-sibly preceded by a prefix.
By contrast, a Finnishword typically consists of a stem followed by multi-ple suffixes.
In addition, compound words are com-mon, containing an alternation of stems and suf-fixes, e.g., the word kahvinjuojallekin (Engl.
?also for [the] coffee drinker?
; cf.
Table 1)1.
More-over, one may ask, whether a morphologically com-plex word exhibits some hierarchical structure, orwhether it is merely a flat concatenation of stemsand suffices.1.1 Previous Work on UnsupervisedSegmentationMany existing morphology discovery algorithmsconcentrate on identifying prefixes, suffixes andstems, i.e., assume a rather simple inflectional mor-phology.De?jean (1998) concentrates on the problem offinding the list of frequent affixes for a languagerather than attempting to produce a morphologicalanalysis of each word.
Following the work of ZelligHarris he identifies possible morpheme boundariesby looking at the number of possible letters follow-ing a given sequence of letters, and then utilizes fre-quency limits for accepting morphemes.1For a comprehensive view of Finnish morphology, see(Karlsson, 1987).Goldsmith (2000) concentrates on stem+suffix-languages, in particular Indo-European languages,and tries to produce output that would match asclosely as possible with the analysis given by a hu-man morphologist.
He further assumes that stemsform groups that he calls signatures, and each sig-nature shares a set of possible affixes.
He applies anMDL criterion for model optimization.The previously discussed approaches consideronly individual words without regard to their con-texts, or to their semantic content.
In a different ap-proach, Schone and Jurafsky (2000) utilize the con-text of each term to obtain a semantic representa-tion for it using LSA.
The division to morphemesis then accepted only when the stem and stem+affixare sufficiently similar semantically.
Their methodis shown to improve on the performance of Gold-smith?s Linguistica on CELEX, a morphologicallyanalyzed English corpus.In the related field of text segmentation, one cansometimes obtain morphemes.
Some of the ap-proaches remove spaces from text and try to identifyword boundaries utilizing e.g.
entropy-based mea-sures, as in (Redlich, 1993).Word induction from natural language text with-out word boundaries is also studied in (Deligneand Bimbot, 1997; Hua, 2000), where MDL-basedmodel optimization measures are used.
Viterbi orthe forward-backward algorithm (an EM algorithm)is used for improving the segmentation of the cor-pus2.Also de Marcken (1995; 1996) studies the prob-lem of learning a lexicon, but instead of optimiz-ing the cost of the whole corpus, as in (Redlich,1993; Hua, 2000), de Marcken starts with sentences.Spaces are included as any other characters.Utterances are also analyzed in (Kit and Wilks,1999) where optimal segmentation for an utteranceis sought so that the compression effect over the seg-ments is maximal.
The compression effect is mea-sured in what the authors call Description LengthGain, defined as the relative reduction in entropy.The Viterbi algorithm is used for searching for theoptimal segmentation given a model.
The input ut-2The regular EM procedure only maximizes the likelihoodof the data.
To follow the MDL approach where model cost isalso optimized, Hua includes the model cost as a penalty termon pure ML probabilities.terances include spaces and punctuation as ordinarycharacters.
The method is evaluated in terms of pre-cision and recall on word boundary prediction.Brent presents a general, modular probabilisticmodel structure for word discovery (Brent, 1999).He uses a minimum representation length criterionfor model optimization and applies an incremental,greedy search algorithm which is suitable for on-linelearning such that children might employ.1.2 Our ApproachIn this work, we use a model where words may con-sist of lengthy sequences of segments.
This modelis especially suitable for languages with agglutina-tive morphological structure.
We call the segmentsmorphs and at this point no distinction is made be-tween stems and affixes.The practical purpose of the segmentation isto provide a vocabulary of language units that issmaller and generalizes better than a vocabularyconsisting of words as they appear in text.
Such avocabulary could be utilized in statistical languagemodeling, e.g., for speech recognition.
Moreover,one could assume that such a discovered morph vo-cabulary would correspond rather closely to linguis-tic morphemes of the language.We examine two methods for unsupervised learn-ing of the model, presented in Sections 2 and 3.
Thecost function for the first method is derived from theMinimum Description Length principle from classicinformation theory (Rissanen, 1989), which simul-taneously measures the goodness of the representa-tion and the model complexity.
Including a modelcomplexity term generally improves generalizationby inhibiting overlearning, a problem especially se-vere for sparse data.
An incremental (online) searchalgorithm is utilized that applies a hierarchical split-ting strategy for words.
In the second method thecost function is defined as the maximum likelihoodof the data given the model.
Sequential splitting isapplied and a batch learning algorithm is utilized.In Section 4, we develop a method for evaluat-ing the quality of the morph segmentations producedby the unsupervised segmentation methods.
Eventhough the morph segmentations obtained are not in-tended to correspond exactly to the morphemes oflinguistic theory, a basis for comparison is providedby existing, linguistically motivated morphologicalanalyses of the words.Both segmentation methods are applied to thesegmentation of both Finnish and English words.In Section 5, we compare the results obtained fromour methods to results produced by Goldsmith?s Lin-guistica on the same data.2 Method 1: Recursive Segmentation andMDL CostThe task is to find the optimal segmentation of thesource text into morphs.
One can think of this asconstructing a model of the data in which the modelconsists of a vocabulary of morphs, i.e.
the code-book and the data is the sequence of text.
We try tofind a set of morphs that is concise, and moreovergives a concise representation for the data.
This isachieved by utilizing an MDL cost function.2.1 Model Cost Using MDLThe total cost consists of two parts: the cost of thesource text in this model and the cost of the code-book.
Let M be the morph codebook (the vocab-ulary of morph types) and D = m1m2 .
.
.mn thesequence of morph tokens that makes up the stringof words.
We then define the total cost C asC = Cost(Source text) + Cost(Codebook)=?tokens?
log p(mi) +?typesk ?
l(mj)The cost of the source text is thus the negative log-likelihood of the morph, summed over all the morphtokens that comprise the source text.
The cost of thecodebook is simply the length in bits needed to rep-resent each morph separately as a string of charac-ters, summed over the morphs in the codebook.
Thelength in characters of the morph mj is denoted byl(mj) and k is the number of bits needed to code acharacter (we have used a value of 5 since that is suf-ficient for coding 32 lower-case letters).
For p(mi)we use the ML estimate, i.e., the token count of midivided by the total count of morph tokens.2.2 Search AlgorithmThe online search algorithm works by incremen-tally suggesting changes that could improve the costfunction.
Each time a new word token is readfrom the input, different ways of segmenting it intomorphs are evaluated, and the one with minimumcost is selected.Recursive segmentation.
The search for the opti-mal morph segmentation proceeds recursively.
First,the word as a whole is considered to be a morph andadded to the codebook.
Next, every possible split ofthe word into two parts is evaluated.The algorithm selects the split (or no split) thatyields the minimum total cost.
In case of no split,the processing of the word is finished and the nextword is read from input.
Otherwise, the search for asplit is performed recursively on the two segments.The order of splits can be represented as a binary treefor each word, where the leafs represent the morphsmaking up the word, and the tree structure describesthe ordering of the splits.During model search, an overall hierarchical datastructure is used for keeping track of the currentsegmentation of every word type encountered sofar.
Let us assume that we have seen seven in-stances of linja-auton (Engl.
?of [the] bus?
)and two instances of autonkuljettajalla-kaan (Engl.
?not even by/at/with [the] car driver?
).Figure 1 then shows a possible structure used forrepresenting the segmentations of the data.
Eachchunk is provided with an occurrence count of thechunk in the data set and the split location in thischunk.
A zero split location denotes a leaf node, i.e.,a morph.
The occurrence counts flow down throughthe hierachical structure, so that the count of a childalways equals the sum of the counts of its parents.The occurrence counts of the leaf nodes are used forcomputing the relative frequencies of the morphs.To find out the morph sequence that a word consistsof, we look up the chunk that is identical to the word,and trace the split indices recursively until we reachthe leafs, which are the morphs.Note that the hierarchical structure is used onlyduring model search: It is not part of the final model,and accordingly no cost is associated with any othernodes than the leaf nodes.Adding and removing morphs.
Adding newmorphs to the codebook increases the codebookcost.
Consequently, a new word token will tend tobe split into morphs already listed in the codebook,which may lead to local optima.
To better escape lo-cal optima, each time a new word token is encoun-13:20:210:20:25:2kuljettajallakaankuljettajallakuljettaja llakaan0:26:70:70:9linja?n4:90:9linja?autonautonautoautonkuljettajallakaanFigure 1: Hierarchical structure of the segmenta-tion of the words linja-auton and autonkul-jettajallakaan.
The boxes represent chunks.Boxes with bold text are morphs, and are part of thecodebook.
The numbers above each box are the splitlocation (to the left of the colon sign) and the occur-rence count of the chunk (to the right of the colonsign).tered, it is resegmented, whether or not this word hasbeen observed before.
If the word has been observed(i.e.
the corresponding chunk is found in the hierar-chical structure), we first remove the chunk and de-crease the counts of all its children.
Chunks withzero count are removed (remember that removal ofleaf nodes corresponds to removal of morphs fromthe codebook).
Next, we increase the count of theobserved word chunk by one and re-insert it as anunsplit chunk.
Finally, we apply the recursive split-ting to the chunk, which may lead to a new, differentsegmentation of the word.?Dreaming?.
Due to the online learning, as thenumber of processed words increases, the qualityof the set of morphs in the codebook gradually im-proves.
Consequently, words encountered in the be-ginning of the input data, and not observed since,may have a sub-optimal segmentation in the newmodel, since at some point more suitable morphshave emerged in the codebook.
We have thereforeintroduced a ?dreaming?
stage: At regular intervalsthe system stops reading words from the input, andinstead iterates over the words already encounteredin random order.
These words are resegmented andthus compressed further, if possible.
Dreaming con-tinues for a limited time or until no considerable de-crease in the total cost can be observed.
Figure 2shows the development of the average cost per wordas a function of the increasing amount of source text.0 20 40 60 80 10020253035404550Corpus: Finnish newspaper text 100 000 wordsNumber of words read [x 1000 words]Averagecostperword[bits]Figure 2: Development of the average word costwhen processing newspaper text.
Dreaming, i.e., there-processing of the words encountered so far, takesplace five times, which can be seen as sudden dropson the curve.3 Method 2: Sequential Segmentation andML Cost3.1 Model Cost Using MLIn this case, we use as cost function the likelihoodof the data, i.e., P (data|model).
Thus, the modelcost is not included.
This corresponds to Maximum-Likelihood (ML) learning.
The cost is thenCost(Source text) =?morph tokens?
log p(mi), (1)where the summation is over all morph tokens in thesource data.
As before, for p(mi) we use the MLestimate, i.e., the token count of mi divided by thetotal count of morph tokens.3.2 Search AlgorithmIn this case, we utilize batch learning where an EM-like (Expectation-Maximization) algorithm is usedfor optimizing the model.
Moreover, splitting is notrecursive but proceeds linearly.1.
Initialize segmentation by splitting words intomorphs at random intervals, starting from thebeginning of the word.
The lengths of intervalsare sampled from the Poisson distribution with?
= 5.5.
If the interval is larger than the num-ber of letters in the remaining word segment,the splitting ends.2.
Repeat for a number of iterations:(a) Estimate morph probabilities for the givensplitting.
(b) Given the current set of morphs and theirprobabilities, re-segment the text using theViterbi algorithm for finding the segmen-tation with lowest cost for each word.
(c) If not the last iteration: Evaluate the seg-mentation of a word against rejection cri-teria.
If the proposed segmentation is notaccepted, segment this word randomly (asin the Initialization step).Note that the possibility of introducing a randomsegmentation at step (c) is the only thing that allowsfor the addition of new morphs.
(In the cost functiontheir cost would be infinite, due to ML probabilityestimates).
In fact, without this step the algorithmseems to get seriously stuck in suboptimal solutions.Rejection criteria.
(1) Rare morphs.
Reject thesegmentation of a word if the segmentation containsa morph that was used in only one word type in theprevious iteration.
This is motivated by the fact thatextremely rare morphs are often incorrect.
(2) Se-quences of one-letter morphs.
Reject the segmenta-tion if it contains two or more one-letter morphs ina sequence.
For instance, accept the segmentationhalua + n (Engl.
?I want?, i.e.
present stem ofthe verb ?to want?
followed by the ending for the firstperson singular), but reject the segmentation halu+ a + n (stem of the noun ?desire?
followed bya strange sequence of endings).
Long sequences ofone-letter morphs are usually a sign of a very badlocal optimum that may even get worse in future it-erations, in case too much probability mass is trans-ferred onto these short morphs3.3Nevertheless, for Finnish there do exist some one-lettermorphemes that can occur in a sequence.
However, these mor-phemes can be thought of as a group that belongs together: e.g.,4 Evaluation MeasuresWe wish to evaluate the method quantitatively fromthe following perspectives: (1) correspondence withlinguistic morphemes, (2) efficiency of compressionof the data, and (3) computational efficiency.
The ef-ficiency of compression can be evaluated as the totaldescription length of the corpus and the codebook(the MDL cost function).
The computational effi-ciency of the algorithm can be estimated from therunning time and memory consumption of the pro-gram.
However, the linguistic evaluation is in gen-eral not so straightforward.4.1 Linguistic Evaluation ProcedureIf a corpus with marked morpheme boundaries isavailable, the linguistic evaluation can be computedas the precision and recall of the segmentation.
Un-fortunately, we did not have such data sets at our dis-posal, and for Finnish such do not even exist.
In ad-dition, it is not always clear exactly where the mor-pheme boundary should be placed.
Several alterna-tives may be possible, cf.
Engl.
hope + d vs. hop+ ed, (past tense of to hope).Instead, we utilized an existing tool for providinga morphological analysis, although not a segmenta-tion, of words, based on the two-level morphologyof Koskenniemi (1983).
The analyzer is a finite-statetransducer that reads a word form as input and out-puts the base form of the word together with gram-matical tags.
Sample analyses are shown in Figure 3.The tag set consists of tags corresponding tomorphological affixes and other tags, for example,part-of-speech tags.
We preprocessed the analysesby removing other tags than those correspondingto affixes, and further split compound base forms(marked using the # character by the analyzer) intotheir constituents.
As a result, we obtained for eachword a sequence of labels that corresponds well toa linguistic morphemic analysis of the word.
A la-bel can often be considered to correspond to a singleword segment, and the labels appear in the order ofthe segments.The following step consists in retrieving the seg-mentation produced by one of the unsupervised seg-mentation algorithms, and trying to align this seg-the Finnish talo + j + a (plural partitive of ?house?
); canalso be thought of as talo + ja.Input OutputWord Base form Tagseasily EASY <DER:ly> ADVbigger BIG A CMPhours?
HOUR N PL GENauton AUTO N SG GENpuutaloja PUU#TALO N PL PTVtehnyt TEHDA?
V ACT PCP2 SGFigure 3: Morphological analyses for some Englishand Finnish word forms.
The Finnish words are au-ton (car?s), puutaloja ([some] wooden houses)and tehnyt ([has] done).
The tags are A (adjec-tive), ACT (active voice), ADV (adverb), CMP (com-parative), GEN (genitive), N (noun), PCP2 (2nd par-ticiple), PL (plural), PTV (partitive), SG (singular),V (verb), and <DER:ly> (-ly derivative).mentation with the desired morphemic label se-quence (cf.
Figure 4).A good segmentation algorithm will producemorphs that align gracefully with the correct mor-phemic labels, preferably producing a one-to-onemapping.
A one-to-many mapping from morphsto labels is also acceptable, when a morph forms acommon entity, such as the suffix -ja in puutaloja,which contains both the plural and partitive element.By contrast, a many-to-one mapping from morphsto a label is a sign of excessive splitting, e.g., t +alo for talo (cf.
English h + ouse for house).Correct labels BIG CMPMorph sequence bigg erCorrect labels HOUR PL GENMorph sequence hour s ?Correct labels PUU TALO PL PTVMorph sequence puu t alo jaFigure 4: Alignment of obtained morph sequenceswith their respective correct morphemic analyses.We assume that the segmentation algorithm hassplit the word bigger into the morphs bigg + er,hours?
into hour + s + ?
and puutaloja intopuu + t + alo + ja.Alignment procedure.
We align the morph se-quence with the morphemic label sequence usingdynamic programming, namely Viterbi alignment,to find the best sequence of mappings betweenmorphs and morphemic labels.
Each possible pairof morph/morphemic label has a distance associatedwith it.
For each segmented word, the algorithmsearches for the alignment that minimizes the to-tal alignment distance for the word.
The distanced(M,L) for a pair of morph M and label L is givenby:d(M,L) = ?
logcM,LcM, (2)where cM,L is the number of word tokens in whichthe morph M has been aligned with the label L; andcM is the number of word tokens that contain themorph M in their segmentation.
The distance mea-sure can be thought of as the negative logarithm of aconditional probability P (L|M).
This indicates theprobability that a morph M is a realisation of a mor-pheme represented by the label L. Put another way,if the unsupervised segmentation algorithm discov-ers morphs that are allomorphs of real morphemes, aparticular allomorph will ideally always be alignedwith the same (correct) morphemic label, whichleads to a high probability P (L|M), and a short dis-tance d(M,L)4.
In contrast, if the segmentation al-gorithm does not discover meaningful morphs, eachof the segments will be aligned with a number of dif-ferent morphemic labels throughout the corpus, andas a consequence, the probabilities will be low andthe distances high.We then utilize the EM algorithm for iterativelyimproving the alignment.
The initial alignment thatis used for computing initial distance values is ob-tained through a string matching procedure: Stringmatching is efficient for aligning the stem of theword with the base form (e.g., the morph puu withthe label PUU, and the morphs t + alo with thelabel TALO).
The suffix morphs that do not matchwell with the base form labels will end up alignedsomehow with the morphological tags (e.g., themorph ja with the labels PL + PTV).4This holds especially for allomorphs of ?stem morphemes?,e.g., it is possible to identify the English morpheme easy witha probability of one from both its allomorphs: easy and easi.However, suffixes, in particular, can have several meanings,e.g., the English suffix s can mean either the plural of nounsor the third person singular of the present tense of verbs.Comparison of methods.
In order to compare twosegmentation algorithms, the segmentation of eachis aligned with the linguistic morpheme labels, andthe total distance of the alignment is computed.Shorter total distance indicates better segmentation.However, one should note that the distance mea-sure used favors long morphs.
If a particular ?seg-mentation?
algorithm does not split one single wordof the corpus, the total distance can be zero.
In sucha situation, the single morph that a word is com-posed of is aligned with all morphemic labels of theword.
The morph M , i.e., the word, is unique, whichmeans that all probabilities P (L|M) are equal toone: e.g., the morph puutaloja is always alignedwith the labels PUU + TALO + PL + PTV and noother labels, which yields the probabilities P (PUU |puutaloja) = P (TALO | puutaloja) = P (PL |puutaloja) = P (PTV | puutaloja) = 1.Therefore, part of the corpus should be used astraining data, and the rest as test data.
Both data setsare segmented using the unsupervised segmentationalgorithms.
The training set is then used for estimat-ing the distance values d(M,L).
These values areused when the test set is aligned.
The better seg-mentation algorithm is the one that yields a betteralignment distance for the test set.For morph/label pairs that were never observed inthe training set, a maximum distance value is as-signed.
A good segmentation algorithm will findsegments that are good building blocks of entirelynew word forms, and thus the maximum distancevalues will occur only rarely.5 Experiments and ResultsWe compared the two proposed methods as well asGoldsmith?s program Linguistica5 on both Finnishand English corpora.
The Finnish corpus consistedof newspaper text from CSC6.
A morphosyntac-tic analysis of the text was performed using theConexor FDG parser7.
All characters were con-verted to lower case, and words containing othercharacters than a through z and the Scandinavianletters a?, a?
and o?
were removed.
Other than mor-phemic tags were removed from the morphological5http://humanities.uchicago.edu/faculty/goldsmith/Linguist-ica2000/6http://www.csc.fi/kielipankki/7http://www.conexor.fi/analyses of the words.
The remaining tags corre-spond to inflectional affixes (i.e.
endings and mark-ers) and clitics.
Unfortunately the parser does notdistinguish derivational affixes.
The first 100 000word tokens were used as training data, and the fol-lowing 100 000 word tokens were used as test data.The test data contained 34 821 word types.The English corpus consisted of mainly newspa-per text from the Brown corpus8.
A morphologi-cal analysis of the words was performed using theLingsoft ENGTWOL analyzer9.
In case of multi-ple alternative morphological analyses, the shortestanalysis was selected.
All characters were convertedto lower case, and words containing other charactersthan a through z, an apostrophe or a hyphen wereremoved.
Other than morphemic tags were removedfrom the morphological analyses of the words.
Theremaining tags correspond to inflectional or deriva-tional affixes.
A set of 100 000 word tokens from thecorpus sections Press Reportage and Press Editorialwere used as training data.
A separate set of 100 000word tokens from the sections Press Editorial, PressReviews, Religion, and Skills Hobbies were used astest data.
The test data contained 12 053 word types.Test results for the three methods and the two lan-guages are shown in Table 2.
We observe differenttendencies for Finnish and English.
For Finnish,there is a correlation between the compression ofthe corpus and the linguistic generalization capac-ity to new word forms.
The Recursive splitting withthe MDL cost function is clearly superior to the Se-quential splitting with ML cost, which in turn is su-perior to Linguistica.
The Recursive MDL methodis best in terms of data compression: it produces thesmallest morph lexicon (codebook), and the code-book naturally occupies a small part of the total cost.It is best also in terms of the linguistic measure, thetotal alignment distance on test data.
Linguistica, onthe other hand, employs a more restricted segmenta-tion, which leads to a larger codebook and to the factthat the codebook occupies a large part of the totalMDL cost.
This also appears to lead to a poor gen-eralization ability to new word forms.
The linguis-tic alignment distance is the highest, and so is thepercentage of aligned morph/morphemic label pairs8The Brown corpus is available at the Linguistic Data Con-sortium at http://www.ldc.upenn.edu/9http://www.lingsoft.fi/that were never observed in the training set.
On theother hand, Linguistica is the fastest program10.Also for English, the Recursive MDL methodachieves the best alignment, but here Linguisticaachieves nearly the same result.
The rate of com-pression follows the same pattern as for Finnish,in that Linguistica produces a much larger morphlexicon than the methods presented in this pa-per.
In spite of this fact, the percentage of unseenmorph/morphemic label pairs is about the same forall three methods.
This suggests that in a morpho-logically poor language such as English a restrictivesegmentation method, such as Linguistica, can com-pensate for new word forms ?
that it does not rec-ognize at all ?
with old, familiar words, that it ?getsjust right?.
In contrast, the methods presented in thispaper produce a morph lexicon that is smaller andable to generalize better to new word forms but hassomewhat lower accuracy for already observed wordforms.Visual inspection of a sample of words.
In anattempt to analyze the segmentations more thor-oughly, we randomly picked 1000 different wordsfrom the Finnish test set.
The total number of occur-rences of these words constitute about 2.5% of thewhole set.
We inspected the segmentation of eachword visually and classified it into one of three cat-egories: (1) correct and complete segmentation (i.e.,all relevant morpheme boundaries were identified),(2) correct but incomplete segmentation (i.e., not allrelevant morpheme boundaries were identified, butno proposed boundary was incorrect), (3) incorrectsegmentation (i.e., some proposed boundary did notcorrespond to an actual morpheme boundary).The results of the inspection for each of the threesegmentation methods are shown in Table 3.
TheRecursive MDL method performs best and segmentsabout half of the words correctly.
The SequentialML method comes second and Linguistica third witha share of 43% correctly segmented words.
Whenconsidering the incomplete and incorrect segmenta-tions the methods behave differently.
The RecursiveMDL method leaves very common word forms un-split, and often produces excessive splitting for rare10Note, however, that the computing time comparison withLinguistica is only approximate since it was a compiled pro-gram run on Windows whereas the two other methods were im-plemented as Perl scripts run on Linux.Table 2: Test results for the Finnish and English corpus.
Method names are abbreviated: Recursive seg-mentation and MDL cost (Rec.
MDL), Sequential segmentation and ML cost (Seq.
ML), and Linguistica(Ling.).
The total MDL cost measures the compression of the corpus.
However, the cost is computed accord-ing to Equation (1), which favors the Recursive MDL method.
The final number of morphs in the codebook(#morphs in codebook) is a measure of the size of the morph ?vocabulary?.
The relative codebook costgives the share of the total MDL cost that goes into coding the codebook.
The alignment distance is the totaldistance computed over the sequence of morph/morphemic label pairs in the test data.
The unseen alignedpairs is the percentage of all aligned morph/label pairs in the test set that were never observed in the trainingset.
This gives an indication of the generalization capacity of the method to new word forms.Language Finnish EnglishMethod Rec.
MDL Seq.
ML Ling.
Rec.
MDL Seq.
ML Ling.Total MDL cost [bits] 2.09M 2.27M 2.88M 1.26M 1.34M 1.44M#morphs in codebook 6302 10 977 22 075 3836 4888 8153Relative codebook cost 10.16% 15.27% 36.81% 9.42% 10.90% 19.14%Alignment distance 768k 817k 1111k 313k 444k 332kUnseen aligned pairs 23.64% 20.20% 37.22% 18.75% 19.67% 20.94%Time [sec] 620 390 180 130 80 30Table 3: Estimate of accuracy of morpheme bound-ary detection based on visual inspection of a sampleof 2500 Finnish word tokens.Method Correct Incomplete IncorrectRec.
MDL 49.6% 29.7% 20.6%Seq.
ML 47.3% 15.3% 37.4%Linguistica 43.1% 24.1% 32.8%words.
The Sequential ML method is more prone toexcessive splitting, even for words that are not rare.Linguistica, on the other hand, employs a more con-servative splitting strategy, but makes incorrect seg-mentations for many common word forms.The behaviour of the methods is illustrated by ex-ample segmentations in Table 4.
Often the Recur-sive MDL method produces complete and correctsegmentations.
However, both it and the SequentialML method can produce excessive splitting, as isshown for the latter, e.g.
affecti + on + at+ e. In contrast, Linguistica refrains from splittingwords when they should be split, e.g., the Finnishcompound words in the table.6 Discussion of the ModelRegarding the model, there is always room for im-provement.
In particular, the current model doesnot allow representation of contextual dependencies,i.e., that some morphs appear only in particular con-texts (allomorphy).
Moreover, languages have rulesregarding the ordering of stems and affixes (morpho-tax).
However, the current model has no way of rep-resenting such contextual dependencies.7 ConclusionsIn the experiments the online method with the MDLcost function and recursive splitting appeared mostsuccessful especially for Finnish, whereas for En-glish the compared methods were rather equal inperformance.
This is likely to be partially due tothe model structure of the presented methods whichis especially suitable for languages such as Finnish.However, there is still room for considerable im-provement in the model structure, especially regard-ing the representation of contextual dependencies.Considering the two examined model optimiza-tion methods, the Recursive MDL method per-formed consistently somewhat better.
Whether thisis due to the cost function or the splitting strategycannot be deduced based on these experiments.
Inthe future, we intend to extend the latter method toutilize an MDL-like cost function.Table 4: Some English and Finnish word segmentations produced by the three methods.
The Finnish wordsare ela?inla?a?ka?ri (veterinarian, lit.
animal doctor), ela?inmuseo (zoological museum, lit.
animalmuseum), ela?inpuisto (zoological park, lit.
animal park), and ela?intarha (zoo, lit.
animal garden).The suffixes -lle, -n, -on, and -sta are linguistically correct.
(Note that in the Sequential ML methodthe rejection criteria mentioned are not applied on the last round of Viterbi segmentation.
This is why twoone letter morphs appear in a sequence in the segmentation ela?in + tarh + a + n.)Recursive MDL Sequential ML Linguisticaaffect affect affectaffect + ing affect + ing affect + ingaffect + ing + ly affect + ing + ly affect + ing + lyaffect + ion affecti + on affect + ionaffect + ion + ate affecti + on + at + e affect + ion + ateaffect + ion + s affecti + on + s affect + ion + saffect + s affect + s affect + sela?in + la?a?ka?ri ela?in + la?a?ka?ri ela?inla?a?ka?riela?in + la?a?ka?ri + lle ela?in + la?a?ka?ri + lle ela?inla?a?ka?ri + lleela?in + museo + n ela?in + museo + n ela?inmuseo + nela?in + museo + on ela?in + museo + on ela?inmuseo + onela?in + puisto + n ela?in + puisto + n ela?inpuisto + nela?in + puisto + sta ela?in + puisto + sta ela?inpuisto + staela?in + tar + han ela?in + tarh + a + n ela?intarh + anReferencesJerome Bellegarda.
2000.
Exploiting latent semantic in-formation in statistical language modeling.
Proceed-ings of the IEEE, 88(8):1279?1296.Michael R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discovery.Machine Learning, 34:71?105.Carl de Marcken.
1995.
The unsupervised acquisitionof a lexicon from continuous speech.
Technical Re-port A.I.
Memo 1558, MIT Artificial Intelligence Lab.,Cambridge, Massachusetts.Carl de Marcken.
1996.
Linguistic structure as compo-sition and perturbation.
In Meeting of the Associationfor Computational Linguistics.Herve?
De?jean.
1998.
Morphemes as necessary con-cept for structures discovery from untagged corpora.In Workshop on Paradigms and Grounding in NaturalLanguage Learning, pages 295?299, Adelaide, Jan.22.Sabine Deligne and Fre?de?ric Bimbot.
1997.
Inference ofvariable-length linguistic and acoustic units by multi-grams.
Speech Communication, 23:223?241.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Yu Hua.
2000.
Unsupervised word induction using MDLcriterion.
In Proceedings of ISCSL, Beijing.Fred Karlsson.
1987.
Finnish Grammar.
WSOY, Juva,second edition.Fred Karlsson.
1998.
Yleinen kielitiede.Yliopistopaino/Helsinki University Press.Chunyu Kit and Yorick Wilks.
1999.
Unsupervisedlearning of word boundary with description lengthgain.
In Proceedings of CoNLL99 ACL Workshop,Bergen.Kimmo Koskenniemi.
1983.
Two-level morphology:A general computational model for word-form recog-nition and production.
Ph.D. thesis, University ofHelsinki.A.
Norman Redlich.
1993.
Redundancy reduction as astrategy for unsupervised learning.
Neural Computa-tion, 5:289?304.Jorma Rissanen.
1989.
Stochastic complexity in statis-tical inquiry.
World Scientific Series in Computer Sci-ence, 15:79?93.Patrick Schone and Daniel Jurafsky.
2000.
Knowledge-free induction of morphology using latent semanticanalysis.
In Proceedings of CoNLL-2000 and LLL-2000, pages 67?72, Lisbon.
