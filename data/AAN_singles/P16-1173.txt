Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1837?1847,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPhrase Structure Annotation and Parsing for Learner EnglishRyo NagataKonan University8-9-1 Okamoto, HigashinadaKobe, Hyogo 658-8501, Japannagata-acl @hyogo-u.ac.jp.Keisuke SakaguchiJohns Hopkins University3400 North Charles StreetBaltimore, MD, 21218, USAkeisuke@cs.jhu.eduAbstractThere has been almost no work on phrasestructure annotation and parsing speciallydesigned for learner English despite thefact that they are useful for representingthe structural characteristics of learner En-glish.
To address this problem, in this pa-per, we first propose a phrase structure an-notation scheme for learner English andannotate two different learner corpora us-ing it.
Second, we show their usefulness,reporting on (a) inter-annotator agreementrate, (b) characteristic CFG rules in thecorpora, and (c) parsing performance onthem.
In addition, we explore methodsto improve phrase structure parsing forlearner English (achieving an F -measureof 0.878).
Finally, we release the fullannotation guidelines, the annotated data,and the improved parser model for learnerEnglish to the public.1 IntroductionLearner corpora have been essential for NLP tasksrelated to learner language such as grammaticalerror correction.
They are normally annotatedwith linguistic properties.
In the beginning, at-tention was mainly focused on grammatical errorannotation (Izumi et al, 2004; D?
?az-Negrillo etal., 2009; Dale and Kilgarriff, 2011; Ng et al,2013).
Recently, it has been expanded to gram-matical annotation ?
first, Part-Of-Speech (POS)tagging (D?
?az-Negrillo et al, 2009; Nagata et al,2011) and then syntactic annotation (Kepser et al,2004; Dickinson and Ragheb, 2009; Ragheb andDickinson, 2012; Ragheb and Dickinson, 2013);syntactic annotation for learner corpora is now in-tensively studied.
Among a variety of studies, aseries of work by Ragheb and Dickinson (Dick-inson and Ragheb, 2009; Ragheb and Dickinson,2012; Ragheb and Dickinson, 2013) is importantin that they proposed a dependency annotationscheme, theoretically and empirically evaluated it,and revealed its theoretical problems, which givesa good starting point to those who wish to developa new annotation scheme for learner corpora.
Re-searchers including Foster (2004) and Ott andZiai (2010) have even started using dependency-annotated learner corpora to develop dependencyparsers for learner language.Although research on syntactic analysis forlearner corpora has been making great progress asnoted above, it is not yet complete.
There are atleast three limitations in the previous work: (i) asfar as we are aware, there has been almost no workon phrase structure annotation specially designedfor learner corpora; (ii) there are no publicly avail-able learner corpora annotated with syntax; (iii)phrase structure parsing performance on learnerEnglish has not yet been reported.The first limitation is that there exists no phrasestructure annotation scheme specially designed forlearner English.
As related work, Foster (2007a;2007b) and Foster and Andersen (2009) propose amethod for creating a pseudo-learner corpus by ar-tificially generating errors in a native corpus withphrase structures.
However, the resulting corpusdoes not capture various error patterns in learnerEnglish.Concerning the second limitation, a corpusgreatly increases in value when it is available tothe public as has been seen in other domains.
Nev-ertheless, whether dependency or phrase structure,there seems to be no publicly available learner cor-pora annotated with syntax.The above two limitations cause the thirdone that phrase structure parsing performance onleaner English has not yet been reported.
Forthis reason, Cahill (2015) demonstrates how ac-1837curately an existing parser performs on a pseudo-learner corpus (section 23 of WSJ with errors arti-ficially generated by Foster and Andersen (2009)?smethod).
Cahill et al (2014) show the perfor-mance of a phrase structure parser augmented byself-training on students?
essays, many of whichare presumably written by native speakers of En-glish.
Tetreault et al (2010) partially show phrasestructure parsing performance concerning prepo-sition usage in learner English, concluding that itis effective in extracting features for prepositionerror correction.
We need to reveal full parsingperformance to be able to confirm that this is truefor other syntactic categories and whether or notwe should use phrase structure parsing to facilitaterelated tasks such as grammatical error correctionand automated essay scoring.Here, we emphasize that phrase structure anno-tation has at least two advantages over dependencyannotation1.
First of all, it can directly encode in-formation about word order.
This is particularlyimportant because learner corpora often containerrors in word order.
For example, phrase struc-ture parsing will reveal in which phrases errors inword order tend to occur as we will partly do inSect.
3.
Second of all, phrase structure rather ab-stractly represents syntactic information in termsof phrase-to-phrase relations.
This means that thecharacteristics of learner English are representedby means of phrase-to-phrase relations (e.g., con-text free grammar (CFG) rules) or even as trees.Take as an example, one of the characteristic treeswe found in the corpora we have created:SNP VP?
ADJPAs we will discuss in Sect.
3, this tree suggeststhe mother tongue interference that the copula isnot necessary in adjective predicates in certain lan-guages.
It would be linguistically interesting to re-veal what CFG rules we need to add to, or subtractfrom, the native CFG rule set to be able to generatelearner English.
This is our primary motivation forthis work although our other motivations includedeveloping a parser for learner English.In view of this background, we address theabove problems in this paper.
Our contributions1We are not arguing that phrase structure annotation isbetter than dependency annotation; they both have their ownadvantages, and thus both should be explored.are three-fold.
First, we present a phrase struc-ture annotation scheme for dealing with learnerEnglish consistently and reliably.
For this, we pro-pose five principles which can be applied to creat-ing a novel annotation scheme for learner corpora.Second, we evaluate the usefulness of the anno-tation scheme by annotating learner corpora us-ing it.
To be precise, we report on inter-annotatoragreement rate and characteristic CFG rules in thecorpora, and take the first step to revealing phrasestructure parsing performance on learner English.In addition, we explore methods to improve phrasestructure parsing for learner English.
Finally, werelease the full annotation guidelines, the anno-tated corpora, and the improved parser model tothe public.The rest of this paper is structured as follows.Sect.
2 describes the annotation scheme.
Sect.
3explores the annotated learner corpora.
Sect.
4evaluates parsing performance using it.2 Phrase Structure Annotation Scheme2.1 General PrinciplesThe annotation scheme is designed to consistentlyretrieve the structure in the target text that is clos-est to the writer?s intention.
The following are thefive principles we created to achieve it:(P1) Consistency-first principle(P2) Minimal rule set principle(P3) Locally superficially-oriented principle(P4) Minimum edit distance principle(P5) Intuition principle(P1) states that the most important thing in ourannotation scheme is consistency.
It is a trade-offbetween quality and quantity of information; de-tailed rules that are too complicated make anno-tation unmanageable yet they may bring out valu-able information in learner corpora.
Corpus anno-tation will be useless if it is inconsistent and un-reliable no matter how precisely the rules can de-scribe linguistic phenomena.
Therefore, this prin-ciple favors consistency over completeness.
Oncewe annotate a corpus consistently, we consideradding further detailed information to it.
(P2) also has to do with consistency.
Thesmaller the number of rules is, the easier it be-comes to practice the rules.
Considering this, if wehave several candidates for describing a new lin-guistic phenomenon particular to learner English,we will choose the one that minimizes the numberof modifications to the existing rule set.
Note that1838this applies to the entire rule set; an addition of arule may change the existing rule set.
(P3) is used to determine the tag of a giventoken or phrase.
As several researchers (D?
?az-Negrillo et al, 2009; Dickinson and Ragheb,2009; Nagata et al, 2011; Ragheb and Dickin-son, 2012) point out, there are two ways of per-forming annotation, according to either superficial(morphological) or contextual (distributional) evi-dence.
For example, in the sentence *My univer-sity life is enjoy., the word enjoy can be interpretedas a verb according to its morphological form oras an adjective (enjoyable) or a noun (enjoyment)according to its context.
As the principle itselfconstrues, our annotation scheme favors superfi-cial evidence over distributional.
This is becausethe interpretation of superficial evidence has muchless ambiguity and (P3) can determine the tag of agiven token by itself as seen in the above example.Distributional information is also partly encodedin our annotation scheme as we discuss in Sub-sect.
2.2.
(P4) regulates how to reconstruct a correct formof a given sentence containing errors, which helpsto determine its phrase structure.
The problem isthat often one can think of several candidates aspossible corrections, which can become a sourceof inconsistency.
(P4) gives a clear solution to thisproblem.
It selects the one that minimizes the editdistance from the original sentence.
Note that theedit distances for deletion, addition, and replace-ment are one, one, and two (deletion and addition),respectively in our definition.For the cases to which these four principles donot apply, the fifth and final principle (P5) al-lows annotators to use their intuition.
It should benoted, however, that the five principles apply in theabove order to avoid unnecessary inconsistency.2.2 Annotation RulesOur annotation scheme is based on the POS-tagging and shallow-parsing annotation guidelinesfor learner English (Nagata et al, 2011), whichin turn are based on the Penn Treebank II-stylebracketing guidelines (Bies et al, 1995) (whichwill be referred to as PTB-II, hereafter).
This natu-rally leads us to adopt the PTB-II tag set in ours; anexception is that we exclude the function tags andnull elements from our present annotation schemefor annotation efficiency2.
Accordingly, we revise2We will most likely include them in a future version.the above guidelines to be able to describe phrasestructures characteristic of learner English.The difficulties in syntactic annotation oflearner English mainly lie in the fact that gram-matical errors appear in learner English.
Gram-matical errors are often classified into three typesas in Izumi et al (2004): omission, insertion, andreplacement type errors.
In addition, we includeother common error types (word order errors andfragments) in the error types to be able to describelearners?
characteristics more precisely.
The fol-lowing discuss how to deal with these five errortypes based on the five principles.2.2.1 Omission Type ErrorsThis type of error is an error where a necessaryword is missing.
For example, some kind of deter-miner is missing in the sentence *I am student.The existing annotation rules in PTB-II can han-dle most omission type errors.
For instance, thePTB-II rule set would parse the above example as?
(S (NP I) (VP am (NP student).)).?
Note that syn-tactic tags for irrelevant parts are omitted in thisexample (and hereafter).A missing head word may be more problematic.Take as an example the sentence *I busy.
where averb is missing.
The omission prevents the rule S?
NP VP from applying to it.
If we created a newrule for every head-omission with no limitation, itwould undesirably increase the number of rules,which violates (P2).To handle head-omissions, we propose a func-tion tag -ERR.
It denotes that a head is missing inthe phrase in question.
The function tag makes itpossible to apply the PTB-II rule set to sentencescontaining head-omissions as in:SNPIVP-ERR?
ADJPbusyWe need to reconstruct a correct form of a givensentence to determine whether or not a head wordis missing.
We use Principle (P4) for solving theproblem as discussed in Sect.
2.1.
For instance,the sentence *I want to happy.
can be correctedas either I want to be happy.
(edit distance is one;an addition of a word) or I want happiness.
(three;two deletions and an addition).
Following (P4), weselect the first correction that minimizes the edit1839distance, resulting in:SNPIVPwantVPTOtoVP-ERR?
ADJPhappy2.2.2 Insertion Type ErrorsAn insertion type error is an error where an extraword is used incorrectly.
For example, the wordabout is an extra word in the sentence *She dis-cussed about it.Insertion type errors are more problematic thanomission type errors.
It is not trivial how to an-notate an erroneous extra word.
On the one hand,one can argue that the extra word about is a prepo-sition from its morphological form.
On the otherhand, one can also argue that it is not, because theverb discuss takes no preposition.
As with this ex-ample, insertion type errors involve an ambiguitybetween superficial and distributional categories.Principles (P2) and (P3) together solve the am-biguity.
According to (P3), one should alwaysstick to the superficial evidence.
For example, theextra word about should be tagged as a prepo-sition.
After this, PTB-II applies to the rest ofthe sentence, which satisfies (P2).
As a result,one would obtain the parse ?
(S (NP She) (VP dis-cussed (PP (IN about) (NP it)))).).
?Insertion type errors pose a more vital problemin some cases.
Take as an example the sentence*It makes me to happy.
where the word to is erro-neous.
As before, one can rather straightforwardlytag it as a preposition, giving the POS sequence:*It/PRP makes/VBZ me/PRP to/TO happy/JJ ./.However, none of the PTB-II rules applies to thePOS sequence TO JJ to make a phrase.
Thismeans that we have to create a new rule for suchcases.
There are at most three possibilities ofgrouping the words in question to make a phrase:to happyme tome to happyIntuitively, the first one seems to be the most ac-ceptable.
To be precise, the second one assumesa postposition, contrary to the English preposi-tion system.
The third one assumes a wholenew rule generating a phrase from a personal pro-noun, a preposition, and an adjective into a phrase.Thus, they cause significant modifications to PTB-II, which violates (P2).
In contrast, a preposi-tion normally constitutes a prepositional phrasewith another phrase (although not normally withan adjective phrase).
Moreover, the first groupingwould produce for the rest of the words the per-fect phrase structure corresponding to the correctsentence without the preposition to:SNPme?TOtoADJPhappywhich satisfies (P2) unlike the second and thirdones.
Accordingly, we select the first one.All we have to do now is to name the phraseto happy.
There is an ambiguity between PP andADJP, both of which can introduce the parent S.The fact that a preposition constitutes a preposi-tional phrase with another phrase leads us to selectPP for the phrase.
Furthermore, the tag of a phraseis normally determined by the POS of one of theimmediate constituents, if any, that is entitled to bea head (i.e., the headedness).
Considering this, weselect PP in this case, which would give the parseto the entire sentence as follows:?
(S (NP It) (VPmakes (S (NP me) (PP (TO to) (ADJP happy)))).
)?In summary, for insertion errors to which PTB-II do not apply, we determine their phrase struc-tures as follows: (i) intuitively group words into aphrase, minimizing the number of new rules added(it is often helpful to examine whether an existingrule is partially applicable to the words in ques-tion); (ii) name the resulting phrase by the POS ofone of the immediate children that is entitled to bea head.2.2.3 Replacement Type ErrorsA replacement type error is an error where a wordshould be replaced with another word.
For exam-ple, in the sentence: *I often study English con-versation., the verb study should be replaced witha more appropriate verb such as practice.To handle replacement type errors systemati-cally, we introduce a concept called POS class,which is a grouping of POS categories defined as1840Class MembersNoun NN, NNS, NNP, NNPSVerb VB, VBP, VBZ, VBDAdjective JJ, JJR, JJSAdverb RB, RBR, RBSParticiple VBN, VBGTable 1: POS class.in Table 1; POS tags that are not shown in Table 1form a POS class by itself.
If the replacement inquestion is within the same POS class, it is anno-tated following Principles (P2) and (P3).
Namely,the erroneous word is tagged according to its su-perficial form and the rest of the sentence is anno-tated by the original rule set, which avoids creatingnew rules3.
If the replacement in question is fromone POS class to another, we will need to take spe-cial care because of the ambiguity between super-ficial and distributional POS categories.
For ex-ample, consider the sentence *I went to the see.where the word see is used as a noun, which isnot allowed in the standard English, and the inten-tion of the learner is likely to be sea (from the sur-rounding context).
Thus, the word see is ambigu-ous between a verb and a noun in the sentence.To avoid the ambiguity, we adopt a two layer-annotation scheme (D?
?az-Negrillo et al, 2009; Na-gata et al, 2011; Ragheb and Dickinson, 2012) toinclude both POSs.
In our annotation scheme, weuse a special tag (CE) for the replacement errorand encode the two POSs as its attribute values asin CE:VB:NN.
Then we can use the distributionalPOS tag to annotate the rest of the sentence.
Forexample, the above example sentence would givea tree:SNPIVPVPwentPPto NPthe CE:VB:NNsee3This means that spelling and morphological errors arenot directly coded in our annotation scheme as in He/PRPhas/VBZ a/DT books/NNS.2.2.4 Errors in Word OrderErrors in word order often appear in learner En-glish.
A typical example would be the reverseof the subject-object order: *This place like myfriends.
(correctly, My friends like this place.
).Principles (P2) and (P3) again play an impor-tant role in handling errors in word order.
We firstdetermine the POS tag of each word according toits morphological form.
This is rather straightfor-ward because errors in word order do not affectthe morphological form.
Then we determine thewhole structure based on the resulting POS tags,following Principle (P2); if rules in PTB-II applyto the sentence in question, we parse it accordingto them just as in the above example sentence: ?
(S(NP This place) (VP like (NPmy friends)).)?
Evenif any of the existing rules do not apply to a partof the sequence of the given POS tags, we stick toPrinciple (P3) as much as possible.
In other words,we determine partial phrase structures accordingto the given POS sequence to which the existingrule set applies.
Then we use the XP-ORD tag toput them together into a phrase.
As an example,consider the sentence *I ate lunch was delicious.
(correctly, The lunch I ate was delicious.).
Ac-cording to the superficial forms and local contexts,the phrase I ate lunch would form an S:SNPIVPate lunchHowever, the relations of the S to the rest of theconstituents are not clear.
Here, we use the XP-ORD tag to combine the S with the rest together:SXP-ORDSNPIVPate lunchVPwas ADJPdelicious2.2.5 FragmentsIn learner corpora, sentences are sometimes in-complete.
They are called fragments (e.g., missingmain clause: Because I like it.
).Fortunately, there exists already a tag for frag-ments in PTB-II: FRAG.
Accordingly, we use1841it in our annotation scheme as well.
For ex-ample, the above example would give the parse?
(FRAG (SBAR Because (S (NP I (VP like (NPit))))).)?
An exception is incomplete sentenceswhich are defined as S in the bracketing guide-lines for biomedical texts (Warner et al, 2012).We tag such incomplete sentences as S followingthe convention.
For example, an adjective phrasecan form an S (e.g., (S (ADVP Beautiful)!
)).2.2.6 Unknown Words and PhrasesThere are cases where one cannot tell the tag ofa given word.
We use the UK tag for such words(e.g., Everyone is don/UK).Even if its tag is unknown, it is somehow clearin some cases that the unknown word is the headword of the phrase just as in the above example.In that case, we use the UP tag so that it satis-fies the rule about the headedness of a phrase wehave introduced in Subsect.
2.2.2.
Based on this,the above example would give the parse ?
(S (NPeveryone) (VP is (UP (UK don ))).
)?For a phrase whose head word is unknown dueto some error(s) in it, we use the XP tag insteadof the UP tag.
As a special case of XP, we usethe XP-ORD tag to denote the information that wecannot determine the head of the phrase becauseof an error in word order.3 Corpus AnnotationWe selected the Konan-JIEM (KJ) learner cor-pus (Nagata et al, 2011) (beginning to interme-diate levels) as our target data.
It is manually an-notated with POSs, chunks, and grammatical er-rors, which helps annotators to select correct tags.We also included in the target data a part of theessays in ICNALE (Ishikawa, 2011) consisting ofa variety of learners (beginning to advanced lev-els4) in Asia (China, Indonesia, Japan, Korea, Tai-wan, Thailand, Hong Kong, Singapore, Pakistan,Philippines).
Table 2 shows the statistics on thetwo learner corpora.Two professional annotators5participated in theannotation process.
One of them first annotatedthe KJ data and double-checked the results.
Be-tween the first and second checks, we discussed4The details about the proficiency levels are availablein http://language.sakura.ne.jp/icnale/about.html5The annotators, whose mother tongue is Japanese, havea good command of English.
They have engaged in corpusannotation including phrase structure annotation for around20 years.the results with the annotator.
We revised the an-notation scheme based on the discussion whichresulted in the present version.
Then the secondannotator annotated a part of the KJ data to eval-uate the consistency between the two annotators.We took out 11 texts (955 tokens) as a develop-ment set.
The second annotator annotated it us-ing the revised annotation scheme where she con-sulted the first annotator if necessary.
After this,we provided her with the differences between theresults of the two annotators.
Finally, the first an-notator annotated the data in ICNALE while thesecond independently another part of the KJ dataand a part of the ICNALE data (59 texts, 12,052tokens in total), which were treated as a test set.Table 3 shows inter-annotator agreement mea-sured in recall, precision, F -measure, com-plete match rate, and chance-corrected mea-sure (Skj?rholt, 2014).
We used the EVALBtool6with the Collins (1997)?s evaluation parame-ter where we regarded the annotation results of thefirst annotator as the gold standard set.
We alsoused the syn-agreement tool7to calculate chance-corrected measure.
It turns out that the agreementis very high.
Even in the test set, they achievean F -measure of 0.928 and a chance-correctedmeasure of 0.982.
This shows that our annota-tion scheme enabled the annotators to consistentlyrecognize the phrase structures in the learner cor-pora in which grammatical errors frequently ap-pear.
The comparison between the results of thetwo annotators shows the major sources of the dis-agreements.
One of them is annotation concerningadverbial phrases.
In PTB-II, an adverbial phrasebetween the subject NP and the main verb is al-lowed to be a constituent of the VP (e.g., (S (NPI) (VP (ADVP often) go))) and also of the S (e.g.,(S (NP I) (ADVP often) (VP go))).
Another ma-jor source is the tag FRAG (fragments); the anno-tators disagreed on distinguishing between FRAGand S in some cases.The high agreement shows that the annotationscheme provides an effective way of consistentlyannotating learner corpora with phrase structures.However, one might argue that the annotation doesnot represent the characteristics of learner Englishwell because it favors consistency (and rather sim-ple annotation rules) over completeness.To see if the annotation results represent the6http://nlp.cs.nyu.edu/evalb/7https://github.com/arnsholt/syn-agreement1842Corpus # essays # sentences # tokens # errors/token # errors/sentenceKJ 233 3,260 30,517 0.15 1.4ICNALE 134 1,930 33,913 0.08 1.4Table 2: Statistics on annotated learner corpora.Set R P F CMR CCMDevelopment 0.981 0.981 0.981 0.913 0.995Test 0.919 0.927 0.928 0.549 0.982Table 3: Inter-annotator agreement measured in Recall (R), Precision (P ), F -measure (F ), CompleteMatch Rate (CMR), and Chance-Corrected Measure (CCM).characteristics of learner English, we extractedcharacteristic CFG rules from them.
The basicidea is that we compare the CFG rules obtainedfrom them with those from a native corpus (thePenn Treebank-II)8; we select as characteristicCFG rules those that often appear in the learnercorpora and not in the native corpus.
To formal-ize the extraction procedures, we denote a CFGrule and its conditional probability as A ?
B andp(B|A), respectively.
Then we define the score forA ?
B by s(A ?
B) = logpL(B|A)pN(B|A)where wedistinguish between learner and native corpora bythe subscripts L and N , respectively.
We estimatep(B|A) by expected likelihood estimation.
Notethat we remove the function tags to reduce the dif-ferences in the syntactic tags in both corpora whenwe calculate the score.Table 4 shows the top 10 characteristic CFGrules sorted in descending and ascending or-der according to their scores, which correspondto overused and underused rules in the learnercorpora, respectively.
Note that Table 4 ex-cludes rules consisting of only terminal and/or pre-terminal symbols to focus on the structural char-acteristics.
Also, it excludes rules containing aQuantifier Phrase (QP; e.g., (NP (QP 100 million)dollars)), which frequently appear and is one ofthe characteristics in the native corpus.In the overused column, CFG rules often con-tain the ?
element.
At first sight, this does notseem so surprising because ?
never appears in thenative corpus.
However, the rules actually show inwhich syntactic environment missing heads tend8To confirm that the extracted characteristics are not influ-enced by the differences in the domains of the two corpora,we also compared the learner data with the native speakersub-corpus in ICNALE that is in the same domain.
It turnedout that the extracted CFG rules, were very similar to thoseshown in Table 4.to occur.
For example, the CFG rule PP ?
?S shows that prepositions tend to be missing inthe prepositional phrase governing an S as in *Iam good doing this, which we had not realizedbefore this investigation.
More interestingly, theCFG rule VP ?
?
ADJP reveals that an adjectivephrase can form a verb phrase without a verb inlearner English.
Looking into the annotated datashows that the copula is missing in predicative ad-jectives as in the tree:SNPIVP?
ADJPbusyThis suggests the transfer of the linguistic systemthat the copula is not necessary or may be omittedin predicate adjectives in certain languages such asJapanese and Chinese.
Similarly, the rule VP ?
?NP shows in which environment a verb taking theobject tends to be missing.
Out of the 28 instances,18 (64%) are in a subordinate clause, which im-plies that learners tend to omit a verb when morethan one verb appear in a sentence.The second rule S ?
XP VP .
implies that thesubject NP cannot be recognized because of acombination of grammatical errors (c.f., S ?
NPVP .).
The corpus data show that 21% of XP inS ?
XP VP .
are actually XP-ORD concerningan error in a relative clause just as shown in thetree in Subsect.
2.2.4.
Some of the learners appar-ently have problems in appropriately using relativeclauses in the subject position.
It seems that thestructure of the relative clause containing anotherverb before the main verb confuses them.Most of the underused CFG rules are those thatintroduce rather complex structures.
For exam-1843Overuse Score Underuse ScorePP ?
?
NP 9.0 NP ?
NP , NP , -4.6S ?
XP VP .
7.2 S ?
NP NP -2.7PP ?
IN IN S 6.7 S ?
NP VP .
?
-2.6S ?
XP .
6.6 ADVP ?
NP RBR -2.5VP ?
?
ADJP 6.5 S ?
S , NP VP .
-2.4VP ?
?
NP 6.3 NP ?
NP , SBAR -2.4SBAR ?
IN NN TO S 6.1 SBAR ?
WHPP S -2.3PP ?
?
S 6.1 VP ?
VBD SBAR -2.2S ?
ADVP NP ADVP VP .
5.8 S ?
NP PRN VP .
-2.2PP ?
IN TO NP 5.7 S ?
PP , NP VP .
?
-2.1Table 4: Characteristic CFG rules.ple, the eighth rule VP ?
VBD SBAR implies astructure such as He thought that ?
?
?
.
The under-used CFG rules are a piece of the evidence that thispopulation of learners of English cannot use suchcomplex structures as fluently as native speakersdo.
Considering this, it will be useful feedbackto provide them with the rules (transformed intointerpretable forms).
As in this example, phrasestructure annotation should be useful not only forsecond language acquisition research but also forlanguage learning assistance.4 Parsing Performance EvaluationWe tested the following two state-of-the-artparsers on the annotated data: Stanford StatisticalNatural Language Parser (ver.2.0.3) (de Marneffeet al, 2006) and Charniak-Johnson parser (Char-niak and Johnson, 2005).
We gave the tokenizedsentences to them as their inputs.
We used againthe EVALB tool with the Collins (1997)?s evalua-tion parameter.Table 5 shows the results.
To our surprise, bothparsers perform very well on the learner corporadespite the fact that it contains a number of gram-matical errors and also syntactic tags that are notdefined in PTB-II.
Their performance is compara-ble to, or even better than, that on the Penn Tree-bank (reported in Petrov (2010)).To achieve further improvement, we augmentedthe Charniak-Johnson parser with the learner data.We first retrained its parser model using the 2-21 sections of Penn Treebank Wall Street Journal(hereafter, WSJ) as training data and its 24 sec-tion as development data, following the settingsshown in Charniak and Johnson (2005).
We thenadded the learner corpora to the training data usingsix-fold cross validation.
We split it into six parts,Parser R P F CMRStanford 0.812 0.832 0.822 0.398Charniak-Johnson 0.845 0.865 0.855 0.465Table 5: Parsing performance on learner English.each of which approximately consisted of 61 es-says, used one sixth as test data, another one sixthas development data instead of the 24 section, andretrained the parser model using the developmentdata and the training data consisting of the remain-ing four-sixths part of the learner data and the 2-21sections of WSJ.
We also conducted experimentswhere we copied the four sixths of the learner datan times (1 ?
n ?
50) and added them to the train-ing data to increase its weight in retraining.Figure 1 shows the results.
The simple additionof the learner data (n = 1) already outperforms theparser trained only on the 2-21 sections of WSJ(n = 0) in both recall and precision, achievingan F -measure of 0.866 and a complete match rateof 0.515.
The augmented parser model particu-larly works well on recognizing erroneous frag-ments in the learner data; F -measure improvedto 0.796 (n = 1) from 0.683 (n = 0) in thesentences containing fragments (i.e., FRAG) (46out of the 111 sentences that were originally er-roneously parsed made even a complete match).
Itwas also robust against spelling errors.
The perfor-mance further improves as the weight n increases(up to F = 0.878 when n = 24), which shows theeffectiveness of using learner corpus data as train-ing data.Figure 2 shows the parsing performance of theCharniak-Johnson parser in each sub-corpus of18440.80.850.90  5  10  15  20  25  30  35  40  45  50PerformanceWeight n for learner corporaRecallPrecisionFigure 1: Relation between learner corpus size intraining data and parsing performance.ICNALE (classified by country code9).
In most ofthe sub-corpora, the parser achieves an F -measureof 0.800 or better.
By contrast, it performs muchworse on the Korean sub-corpus.
The major rea-son for this is that it contains a number of wordorder errors (i.e., XP-ORD); to be precise, 27 in-stances compared to zero to two instances in theother sub-corpora.
Similarly, FRAG is a sourceof parsing errors in the Thai sub-corpus.
We needfurther investigation to determine whether the dif-ferences in parsing performance are due to thewriters?
mother tongue or other factors (e.g., pro-ficiency).We can summarize the findings as follows: (1)the state-of-the-art phrase structure parsers for na-tive English are effective even in parsing learnerEnglish; (2) they are successfully augmented bylearner corpus data; (3) the evaluation results sup-port the previous report (Tetreault et al, 2010) thatthey are effective in extracting parse features forgrammatical error correction (and probably for re-lated NLP tasks such as automated essay scoring);(4) however, performance may vary depending onthe writer?s mother tongue and/or other factors,which we need further investigation to confirm.5 ConclusionsThis paper explored phrase structure annotationand parsing specially designed for learner English.Sect.
3 showed the usefulness of our phrase struc-ture annotation scheme and the learner corporaannotated using it.
The annotation results exhib-ited high consistency.
They also shed light on (atleast, part of) the characteristics of the learners of9Ideally, it would be better to use sub-corpora classifiedby their mother tongues.
Unfortunately, however, only coun-try codes are provided in ICNALE.0.50.60.70.80.91HKG JPN CHN PHL SIN TWN PAK IDN THA KORF-measureCountry codeFigure 2: Parsing performance in each sub-corpus.English.
Sect.
4 further reported on the perfor-mance of the two state-of-the-art parsers on theannotated corpus, suggesting that they are accu-rate for providing NLP applications with phrasestructures in learner English.
All these findingssupport the effectiveness of our phrase structureannotation scheme for learner English.
It wouldbe much more difficult to conduct similar analy-ses and investigations without the phrase structureannotation scheme and a learner corpus annotatedbased on it.
The annotation guidelines, the anno-tated data, and the parsing model for learner En-glish created in this work are now available to thepublic10.In our future work, we will evaluate parsingperformance on other learner corpora such asICLE (Granger et al, 2009) consisting of a widevariety of learner Englishes.
We will also extendphrase structure annotation, especially working onfunction tags.AcknowledgmentsWe would like to thank Shin?ichiro Ishikawa, whocreated the ICNALE corpus, for providing us withthe data and Arne Sk?rholt for the assistance torun the syn-agreement tool.
We would also liketo thank the three anonymous reviewers for theirvaluable feedback.
This work was partly sup-ported by Grant-in-Aid for Young Scientists (B)Grant Number JP26750091.10We released the Konan-JIEM corpus with phrase struc-tures on March 2015, which is available at http://www.gsk.or.jp/en/catalog/gsk2015-a/.
We anno-tated the existing ICNALE, which was created by Dr.Ishikawa and his colleagues, with phrase structures.
We re-leased the data on Jun 2016, which is available at http://language.sakura.ne.jp/icnale/1845ReferencesAnn Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre.
1995.
Bracketing guidelines for Tree-bank II-style Penn treebank project.Aoife Cahill, Binod Gyawali, and James V. Bruno.2014.
Self-training for parsing learner text.
In Proc.of 1st Joint Workshop on Statistical Parsing of Mor-phologically Rich Languages and Syntactic Analysisof Non-Canonical Languages, pages 66?73.Aoife Cahill.
2015.
Parsing learner text: to Shoehornor not to Shoehorn.
In Proc.
of 9th Linguistic Anno-tation Workshop, pages 144?147.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine N-best parsing and MaxEnt discriminativereranking.
In Proc.
of 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
of 35th An-nual Meeting of the Association for ComputationalLinguistics, pages 16?23.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proc.of 13th European Workshop on Natural LanguageGeneration, pages 242?249.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProc.
of 5th International Conference on LanguageResources and Evaluation, pages 449?445.Ana D?
?az-Negrillo, Detmar Meurers, Salvador Valera,and Holger Wunsch.
2009.
Towards interlanguagePOS annotation for effective learner corpora in SLAand FLT.
Language Forum, 36(1?2):139?154.Markus Dickinson and Marwa Ragheb.
2009.
Depen-dency annotation for learner corpora.
In Proc.
of8th Workshop on Treebanks and Linguistic Theories,pages 59?70.Jennifer Foster and ?istein E. Andersen.
2009.
Gen-ERRate: Generating errors for use in grammaticalerror detection.
In Proc.
of 4th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations, pages 82?90.Jennifer Foster.
2004.
Parsing ungrammatical input:An evaluation procedure.
In Proc.
of 4th Interna-tional Conference on Language Resources and Eval-uation, pages 2039?2042.Jennifer Foster.
2007a.
Treebanks gone bad: gen-erating a treebank of ungrammatical English.
In2007 Workshop on Analytics for Noisy UnstructuredData, pages 39?46, Jan.Jennifer Foster.
2007b.
Treebanks gone bad: Parserevaluation and retraining using a treebank of un-grammatical sentences.
International Journal onDocument Analysis and Recognition, 10(3):129?145, Dec.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English v2.
Presses universitaires de Lou-vain, Louvain.Shinichiro Ishikawa, 2011.
A new horizon in learnercorpus studies: The aim of the ICNALE project,pages 3?11.
University of Strathclyde Publishing,Glasgow.Emi Izumi, Toyomi Saiga, Thepchai Supnithi, Kiy-otaka Uchimoto, and Hitoshi Isahara.
2004.
TheNICT JLE Corpus: Exploiting the language learn-ers?
speech database for research and education.
In-ternational Journal of The Computer, the Internetand Management, 12(2):119?125.Stephan Kepser, Ilona Steiner, and Wolfgang Sterne-feld.
2004.
Annotating and querying a treebank ofsuboptimal structures.
In Proc.
of 3rd Workshop onTreebanks and Linguistic Theories, pages 63?74.Ryo Nagata, Edward Whittaker, and Vera Shein-man.
2011.
Creating a manually error-tagged andshallow-parsed learner corpus.
In Proc.
of 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1210?1219.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.In Proc.
17th Conference on Computational NaturalLanguage Learning: Shared Task, pages 1?12.Niels Ott and Ramon Ziai.
2010.
Evaluating depen-dency parsing performance on German learner lan-guage.
In Proc.
of 9th International Workshop onTreebanks and Linguistic Theories, pages 175?186.Slav Petrov.
2010.
Products of random latent vari-able grammars.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 19?27.Marwa Ragheb and Markus Dickinson.
2012.
Defin-ing syntax for learner language annotation.
In Proc.of 24th International Conference on ComputationalLinguistics, pages 965?974.Marwa Ragheb and Markus Dickinson.
2013.
Inter-annotator agreement for dependency annotation oflearner language.
In Proc.
of 8th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations, pages 169?179.Arne Skj?rholt.
2014.
A chance-corrected measureof inter-annotator agreement for syntax.
In Proc.
of52nd Annual Meeting of the Association for Compu-tational Linguistics, pages 934?944.1846Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In Proc.
of 48nd Annual Meet-ing of the Association for Computational LinguisticsShort Papers, pages 353?358.Colin Warner, Arrick Lanfranchi, Tim O?Gorman,Amanda Howard, Kevin Gould, and Michael Regan.2012.
Bracketing biomedical text: An addendum toPenn Treebank II guidelines.1847
