Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 109?116,Prague, June 2007. c?2007 Association for Computational LinguisticsCognate identification and alignment using practical orthographiesMichael CysouwMax Planck Institute for EvolutionaryAnthropology, Leipzigcysouw@eva.mpg.deHagen JungMax Planck Institute for EvolutionaryAnthropology, Leipzigjung@eva.mpg.deAbstractWe use an iterative process of multi-gramalignment between associated words in dif-ferent languages in an attempt to identifycognates.
To maximise the amount of data,we use practical orthographies instead ofconsistently coded phonetic transcriptions.First results indicate that using practical or-thographies can be useful, the more so whendealing with large amounts of data.1 IntroductionThe comparison of lexemes across languages is apowerful method to investigate the historical rela-tions between languages.
A central prerequisite forany interpretation of historical relatedness is to es-tablish lexical cognates, i.e.
lexemes in differentlanguages that are of shared descend (in contrast tosimilarity by chance).
If a pair of lexemes in two dif-ferent languages stem from the same origin, this canbe due to the fact that both languages derive from acommon ancestor language, but it can also be causedby influence from one language on another (or influ-ence on both language from a third language).
Todecide whether cognates are indicative of a commonancestor language (?vertical transmission?)
or due tolanguage influence (?horizontal transmission?)
is adifficult problem with no shortcuts.
We do not thinkthat one kind of cognacy is more interesting that an-other.
Both loans (be it from a substrate or a super-strate) and lexemes derived from a shared ancestorare indicative of the history of a language, and bothshould be acknowledged in the unravelling of lin-guistic (pre)history.In this paper, we approach the identification ofcognate lexemes on the basis of large parallel lex-ica between languages.
This approach is an explicitattempt to reverse the ?Swadesh-style?
wordlistmethod.
In the Swadesh-style approach, first mean-ings are selected that are assumed to be less proneto borrowing, then cognates are identified in thoselists, and these cognates are then interpreted as in-dicative of shared descend.
In contrast, we proposeto first identify (possible) cognates among all avail-able information, then divide these cognates intostrata, and then interpret these strata in historicalterms.
(Because of limitations of space, we willonly deal with the first step, the identification of cog-nates, in this paper.)
This is of course exactly theroute of the traditional historical-comparative ap-proach to language comparison.
However, we thinkthat much can be gained by applying computationalapproaches to this approach.A major problem arises when dealing with largequantities of lexical material from many differentlanguages.
In most cases it will be difficult (or verycostly and time consuming in the least) to use co-herent and consistent phonetic transcriptions of allavailable information.
Even if we would have dictio-naries with phonetic transcriptions for all languagesthat we are interested in, this would not necessarilyhelp, as the details of phonetic transcription are nor-mally not consistent across different authors.
In thispaper, we will therefore attempt to deal with unpro-cessed material in practical orthographies.
This willof course pose problems for history-ridden orthogra-phies like in English or French.
However, we belevethat for most of the world?s languages the practical109orthographies are not as inconsistent as those (be-cause they are much younger) and might very wellbe useful for linguistic purposes.In this paper, we will first discuss the data used inthis investigation.
Then we will describe the algo-rithm that we used to infer alignments between wordpairs.
Finally, we will discuss a few of the resultsusing this algorithm on large wordlists in practicalorthography.2 ResourcesIn this study we used parallel wordlists thatwe extracted from the Intercontinental Dictio-nary Series (IDS) database, currently underdevelopment at the Max Planck Institute forEvolutionary Anthropology in Leipzig (seehttp://www.eva.mpg.de/lingua/files/ids.html formore information).
The IDS wordlists containmore than thousand entries of basic words fromeach language, and many entries contain alternativewordforms.
At this time, there are only a fewbasic transcription languages (English, Frenchand Portuguese) and some Caucasian languagesavailable.
We choose some of them for the purposeof the present study and preprocessed the data.To compare languages, we chose only word pairsthat were available and non-compound in bothlanguages.
For all words that occurred several timesin the whole collection of a language, we acceptedonly one randomly choosen wordform and left outall others.
We also deleted content in brackets orin between other special characters.
If, after thesepreparation, a wordform is still longer than twelveUTF-8 characters, we disregard these for reasonsof computational efficiency.
After this, we are stillleft with a large number of about 900 word pairs foreach pair of languages.3 AlignmentAn alignment of two words wa and wb is a bijectiveand maintained ordered one-to-one correspondencefrom all subsequences sa of the word wa with wa =concat(sa1 , sa2 , .
.
.
, sak) to all subsequences sb ofthe word wb with wb = concat(sb1 , sb2 , .
.
.
, sbk).
Itis possible that one of the associated subsequencesis the empty word .
In general one may constructa distance measure from such a linked sequence oftwo given words by assigning a cost for each singlelink of the alignment.
There are many such align-ment/cost functions described in the literature, andthey are often used to calculate a distance measurebetween two sequences of characters (Inkpen et al,2005).
A measurement regularly used for linguisticsequences is the Levenshtein distance, or a modi-fications of it.
Other distance measures detect, forexample, the longest common subsequences or thelongest increasing subsequences.It is our special interest to use multi-charactermappings for calculating a distance between twowords.
Therefore, we adapt and extend the Leven-shtein measurement.
First, we allow for mappingof any arbitrary string length (not just strings of onecharacter as in Levenshtein) and, second, we assigna continuous cost between 0 and 1 for every map-ping.Our algorithm consist basically of two steps.
Inthe first step, all possible subsequence pairs betweenassociated words are considered, and a cost functionis extracted for every multi-gram pair from their co-occurrences in the whole wordlist.
In a second step,this cost function is used to infer an alignment be-tween whole words.
On the basis of this alignmenta new cost function is established for all multi-grampairs.
This second step can be iterated until the costfunction stabilizes.3.1 Cost of an multi-gram pairFor every pair of subsequences sai and sbj we countthe number of co-occurrences.
The subsequencessai and sbj co-occur when they are found in two as-sociated words wa and wb from a language wordlistof two languages La and Lb.
We then use a sim-ple Dice coefficient as a cost function between allpossible subsequences.
For computational reasons,it is necessary to limit the size of the multi-gramsconsidered.
We decided to limit the multi-gramsize to a number of maximally four UTF-8 char-acters.
Still, in the first step of our algorithm,there is a very large set of such subsequence pairsbecause all possible combinations are considered.When an alignment is inferred in the iterative pro-cess, only the aligned subsequences are counted asco-occurrences, so the number of possible combi-nations is considerably lower.
Further, to preventlow frequent co-occurrences to have a dispropor-110tional impact, we added an attestation threshold of2% of the wordlist size for two subsequences to beaccepted for the alignment process.3.2 Alignment of wordsAn alignment of two words is a complete orderedlinking of subsequences.
We annotate it in thefollowing way (vertical dashes delimit the subse-quences; note that subsequences may be empty):( | w | ool)(wers | t~ | )There is a huge amount of possible combinationsof aligned subsequences.
On the basis of the costfunction, a distance is established for every wordpair alignment.
The summation of all multi-grammapping costs represents the distance of the align-ment.
Because we are dealing with multi-grams ofvariable length, alternative alignments of the sameword pair will consist of a different number of sub-sequences.
So, simple summation would lead to dis-tances out of the range from 0 to 1.
To counteractthis, we normalized the word distance.
We weightedeach subsequence relative to the number of charac-ters in the subsequence.
For example, the mappingof w and t~ in the example above would be multi-plied by 310 , because w and t~ have together 3 char-acters and the complete words have in total 10 char-acters.To make use of efficient divide and conquer solv-ing strategies and to get meaningful linguistic state-ments with the base of the calculated best align-ments, we decided to look for a special subset ofbest alignments.
As (Kondrak, 2002) pointed out,there are some situations in which the considerationof local alignment gets the required results.
If onlya part of a word aligning sequence is of high simi-larity then sometimes a linguistic justification of thewhole word similarity is given.
Those alignmentscontain the lowest cost multi-gram pairs, but are notnecessarily of best similarity in total.To illustrate the difference between local andglobal alignment, consider an example that showsdifferent results, depending whether the total sum ofmulti-gram similarities is taken or the best local one.Look at the two words ?abc?
and ?????
and a part ofits multi-gram cost function in Table 1.
The sum-mation of the costs would prefer alignment A2, ascan be seen in Table 2.
But we prefer A1, becauseit contains the subsequence pair (ab | ??)
with themulti-gram 1 multi-gram 2 costab ??
0.1bc ??
0.3a ?
0.4c ?
0.8.........Table 1: Costs for constructed subsequence pairs(ordered by cost)Index Alignment DistanceA2 (a | bc)(?
| ??)
0.4 + 0.3 = 0.7A1 (ab | c)(??
| ?)
0.1 + 0.8 = 0.9.........Table 2: Alignments with distancelowest cost.With these assumptions, we composed a fast andeasy method to find the best alignment.
We pre-fer alignments where some links are very good,but the rest might not be.
We assume that wordsare more related to each other, if there are suchhighly rated pairs.
This approach can also be foundin other string based comparing methods like, forexample, the Longest Common Increasing Subse-quence method, which calculates the longest equalmulti-gram and neglects the rest of the word.
Wefirst order all possible multi-gram mappings by theircosts and pick the subsequence pair with the low-est cost.
Starting from this mapping seed, we lookfor mappings for the rest of the word pair, both be-fore and after the initial mapped subsequence.
Forboth these suffixes and prefixes, we again search forthe subsequence with the lowest cost.
This processis re-applied until the whole words are mapped.
Ifthere is more than one optimal linking subsequencepair, then all possible alignments are considered.
Inthis way, we do not restrict, in contrast to Kondrak,which position for the multi-gram mapping will bepreferred for the local alignment.
The algorithmruns in O(n6).
It takes O(n4) time for all combina-tions of different multi-gram pairs within O(n) stepsin O(n) iterations.1114 Experimental EvaluationAs mentioned above, we applied our model to sometest data from the IDS database.
For later anal-yses, we also constructed some random wordlists.With these we are able to say something about howsignificant our results are.
To make these randomwordlists we remap each word wa from La to an ar-bitrarily chosen word wb from collection Lb.
Thisnew mapped word was adjusted to the size of theoriginally associated word from Lb.
The adjustmentworks by stretching or shrinking the new word to therequired length by doubling the word several timesand cutting of the overlaying head or tail afterwards.In this way, we controlled for word length and multi-gram frequencies.
This randomization process wasperformed five times from La to Lb, and five thetimes from Lb to La, and the results were averagedover all these ten cases.For the calculation process, we stored all lists inSQL tables.
We first built a preprocessed work-ing table with the lexemes from the languages to becompared, and afterwards we constructed the result-ing tables that hold all the results:?
compare table: the word pairs, their alignmentsand alignment goodness;?
subsequence table: the subsequence pairsfound and their co-occurrence coefficients;?
random compare table: pseudo random wordpairs like the compare table;?
random subsequence table: the subsequencepairs found from random compare table.Table 3 consists of the best alignments for word pairsof English and French after 30 iterations, and Table4 shows the best alignments for the comparison ofEnglish and Hunzib (a Caucasian language).
Firstnote that our algorithm works independently of theorthography used.
We do not assume that the sameUTF-8 characters in the two languages are identi-cal.
The fact that ?c?
is mapped between Englishclan and French clan is a result of the statistical dis-tribution of these characters in the two languages.This orthography-independence means that we canapply our algorithm without modifications to cyrillicscripts as shown with the English-Hunzib compari-son.
Second, we payed close attention to the fact thatthe word similarity values are comparable amongdifferent language comparisons.
This means that itis highly significant that the highest word similar-ities between English and French are much higherthan those between English and Hunzib (actually,the alignments between English and Hunzib are non-sensical, but more about that later).
Further, our al-gorithm finds vowel-consonant multi-grams in somecases (e.g.
see Table 5).
As far as we can see, thereare not linguistically meaningful and should be con-sidered an artifact of our current approach.
We hopeto fine-tune the algorithm in the future to prevent thisbehavior.Our method finds alignments, but also the subse-quences in the alignments are of interest.
The bestmapped multi-grams between English and Frenchare illustrated in Table 5.
Strangely, the highestranked ones are a few vowel+consonant bigrams,that occur not very often.
Since the Dice coefficientdepends on the size of the investigated collection, weassumed a minimum frequency of co-occurrences ineach calculation step of 2% of the collection size(which is 20 cases in the English-French compari-son).
The high-ranked bigrams are all just above thisthreshold.
Therefore, we might argue that all the bi-grams from the top of the list are a side-effect of thecollection size itself.Following these bigrams are many one-to-one matches of all alphabetic characters except?j,k,q,w,x,y,z?.
These mappings are found withoutassuming any similarity based on the UTF-8 encod-ing of the characters.
What we actually find here isa mapping for the orthography of the stratum of theFrench loan words in English.
As can be seen in thehistogram in Figure 1, the mapping between multi-grams falls off dramatically after these links.112English French Alignment similaritytribe,clan tribu,clan ( | c | | l | | an | ) ( | c | | l | | an | ) 0.955872long long ( | l | | on | | g | ) ( | l | | on | | g | ) 0.925542lion lion ( | l | | i | | on | ) ( | l | | i | | on | ) 0.916239canoe canoe,pirogue ( | c | | an | | o | | e | ) ( | c | | an | | o | | e | ) 0.911236famine famine,disette ( | f | | a | | m | | in | | e | ) ( | f | | a | | m | | in | | e | ) 0.910465innocent innocent ( | in | | n | | o | | c | | e | | n | | t | ) ( | in | | n | | o | | c | | e | | n | | t | ) 0.908913prison,jail prison ( | p | | r | | i | | s | | on | ) ( | p | | r | | i | | s | | on | ) 0.9089poncho poncho ( | p | | on | | c | | h | | o | ) ( | p | | on | | c | | h | | o | ) 0.907496sure,certain su?r,certain ( | c | | e | | r | | t | | a | | in | ) ( | c | | e | | r | | t | | a | | in | ) 0.905022tapioca,manioc manioc ( | m | | an | | i | | o | | c | ) ( | m | | an | | i | | o | | c | ) 0.904811............Table 3: English-French best rated alignments after 30 iterationsEnglish Hunzib Alignment similarityjewel avg~ar,akut ( | j | | e | | w | | e | | l | ) ( |  | | a | v | g~ | | a | | r | ) 0.507094see nacIa ( | s | | e | | e | ) ( | n | | a | cI | a | ) 0.489442grease,fat maa (g | r | | e | a | s | | e | ) ( | m | | a | |  | | a | ) 0.464667heaven gIalan ( | h | | e | | a | | v | | e | | n | ) (g | I | | a | | l | |  | | a | | n | ) 0.445626ocean akean ( | o | | c | | e | a | n | ) (a | k | | e | | a | | n | ) 0.419629pocket kisa,ibi (p | o | | c | | k | | e | t) ( | k | | i | | s | | a | ) 0.410143sweep lalIa ( | s | w | e | | e | p) (l |  | | a | lI | a | ) 0.395264measure masa ( | m | | e | a | s | ur | e | ) ( | m | | a | | s | | a | ) 0.393806flower g~akI (flo | w | | e | | r | ) ( | g~ | | a | | k | I) 0.391867rebuke,scold ak~a (r | e | | b | | u | k | e | ) ( | a | | k | | ~ | | a | ) 0.387163......... .
.
.Table 4: English-Hunzib best rated alignments after 30 iterations113E F freq dicear ar 21 1in in 26 1on on 22 1an an 22 1m m 80 0.92786n n 188 0.92161c c 120 0.91815p p 78 0.91798r r 277 0.91665f f 35 0.90647l l 132 0.90534v v 26 0.90346t t 165 0.8719b b 44 0.86301s s 126 0.85915d d 66 0.82913o o 192 0.82325e e 417 0.81479a a 229 0.81367g g 34 0.79683h h 53 0.7856i i 183 0.75961u u 94 0.69546............Table 5: Best English (E) and French (F) multi-grammappings after 30 iterations.The character-independence of our method is il-lustrated by the character mapping between Englishand Russian in Table 6.
Shown in the table are onlythe highest ranked orthographic mappings.
Againwe see an almost complete alphabetic linkage, prob-ably caused by the French loanwords shared by bothEnglish and Russian.With this approach, we are also able to find somevestiges of sound changes, as illustrated by the char-acter mapping between Spanish and Portuguese inTable 7.
Shown here are only the highest rankednon-identical multi-grams.
The dice coefficients ofthe pairs ?h??
?ll?, ?f???h?
show the results of soundchanges that were dramatically enough to be repre-sented in the orthography.
The pairs ?c??
?
?z?
and?n???n??
show difference in orthographic convention(though the best pair should have been ?nh?
?
?n??
).0.0 0.2 0.4 0.6 0.8 1.0010204030dice coefficientnumber of mappedmultigrams]Figure 1: Histogram of dice-coefficients forEnglish-French multi-gram mappings.E R freq dicer r 184 0.88874745n n 115 0.8461936l l 104 0.79646295s s 114 0.7927922t t 165 0.7701921m m 47 0.7699933o o 184 0.7510106k t~ 21 0.74458015p p 50 0.7388723i i 102 0.7034591a a 221 0.6866478u u 40 0.6449104c k 77 0.6251676e e 219 0.59066784b b 32 0.525643w v 46 0.46787763d d 42 0.381996............Table 6: Best English (E) and Russian (R) multi-gram mappings after 30 iterations.114P S freq dice............c?
z 20 0.6316202h ll 20 0.4552776f h 34 0.43381172n n?
24 0.37720457a?
n 33 0.31106696h h 23 0.23646937v b 32 0.2165933t h 29 0.2127131z c 24 0.15424858o e 305 0.12838262............Table 7: Spanish (S) and Portuguese (P) multi-grammappings after 30 iterations.
Only thehighest ranking non-identical mappings areshownA promising indicator for cognate identification isthe comparison of word alignment similarities withthe similarities between randomly associated wordpairs.
We generated pseudo random word pairs asdescribed above.
Therefore we caluclate for eachword from one language one coeffiecent value forthe linkage with the assocciated word and a sec-ond avarage value for the linkage with some ran-dom words.
In Figure 2 we plot these two valuesfor all words of English and all words of French (af-ter 30 iterations) against each other.
Each dot repre-sents a word.
The x-axis shows the similarity coef-ficient between the real words and the y-axis showsthe similarity coefficient from the comparison withthe pseudo random words.
As can be seen, manyof the actual similarities are more to the right of they = x line indicating more than chance frequencysimilarity.In contrast, in comparing English with Hunzib inFigure 3 there is only a slight tendency of stretchingof the scatterplot.
So one could conclude that En-glish and Hunzib have probably no cognates at all,although there are some strongly related word pairs.However, some slight stretching will always be seen,because of the usage of an algorithm with iterations.Such a process will always strengthen some randomcoefficientpseudorandomcoefficient0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Figure 2: English-French similarities for wordalignments plotted against the similaritieswith random language entries.tendencies.The iterative process is illustrated in Figure 4.Shown here are the alignment similarities for allword pairs between French and Portuguese.
Afterthe first round of alignment, there is only a slightstretch in the scatterplot.
Already after the seconditeration, the plot is stretched strongly.
In the furtheriterations the situation changes only slightly.
Appar-ently, two rounds of alignment and reassignment ofthe cost function suffice for convergence.5 ConclusionThe big advantage of using original orthographiesin the study of linguistic relationships is that muchmore information is readily available.
Because ofthe wealth of available data, we can use computa-tional approaches for the comparison of wordlists.In principle, the kind of approach that we havesketched out in this paper can just as well be usedfor the comparison of complete dictionaries.
Thecomparison of real wordlists with randomly shuf-fled wordlists indicated that even on purely statis-tical grounds it might be possible to separate mean-ingful alignments from random alignments.The most promising result of our investigation is1150.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0coefficientpseudorandomcoefficientFigure 3: English-Hunzib similarities for wordalignments plotted against the similaritieswith random language entries.that we were able to find cognates even without anyknowledge about the orthographic conventions usedin the languages that were compared.
In the com-parison English-French and English-Russian thereappear to be many French loanwords among thewell-aligned wordpairs.
If this impresion holds, weare in fact only able to infer the stratum of Frenchinfluence in European languages.
An interestingnext step would then be to redo the analyses af-ter removing this stratum from the data and lookfor deeper strata in the lexicon.
As shown by theSpanish-Portuguese comparison, sound changes canbe picked up by our approach as long as the changeshave left a trace in the orthography.ReferencesDiana Inkpen, Oana Frunza, and Grzegorz Kondrak.2005.
Automatic identification of cognates and falsefriends in french and english.
In RANLP-2005, Bul-garia, pages 251?257, September.Grzegorz Kondrak.
2002.
Algorithms for language re-construction.
Ph.D. thesis, University of Toronto.0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0coefficientpseudorandomcoefficientiteration 00.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0coefficientpseudorandomcoefficientiteration 10.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0coefficientpseudorandomcoefficientiteration 20.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0coefficientpseudorandomcoefficientiteration 29Figure 4: Plots of four iterations after 1, 2, 3 and 30rounds of the French-Portuguese compar-ison.
The coefficients are plotted againstcoefficients that were build with random-ized language entries.116
