A Question Answer System Based on Confirmed KnowledgeDeveloped by Using Mails Posted to a Mailing ListRyo NishimuraRyukoku UniversitySeta, Otsu, Shiga,520-2194, Japant030461a@ryukoku-u.jpYasuhiko WatanabeRyukoku UniversitySeta, Otsu, Shiga,520-2194, Japanwatanabe@rins.ryukoku.ac.jpYoshihiro OkadaRyukoku UniversitySeta, Otsu, Shiga,520-2194, Japanokada@rins.ryukoku.ac.jpAbstractIn this paper, we report a QA systemwhich can answer how type questionsbased on the confirmed knowledge basewhich was developed by using mailsposted to a mailing list.
We first dis-cuss a problem of developing a knowl-edge base by using natural language doc-uments: wrong information in naturallanguage documents.
Then, we describea method of detecting wrong informa-tion in mails posted to a mailing list anddeveloping a knowledge base by usingthese mails.
Finally, we show that ques-tion and answer mails posted to a mailinglist can be used as a knowledge base fora QA system.1 IntroductionBecause of the improvement of NLP, research ac-tivities which utilize natural language documentsas a knowledge base become popular, such as QAtrack on TREC (TREC) and NTCIR (NTCIR).However, there are a few QA systems which as-sumed the user model where the user asks how typequestion, in other words, how to do something andhow to cope with some problem (Kuro 00) (Kiyota02) (Mihara 05).
This is because we have severaldifficulties in developing a QA system which an-swers how type questions.
We focus attention totwo problems below.First problem is the difficulty of extracting evi-dential sentences.
It is difficult to extract evidentialsentences for answering how type questions onlyby using linguistic clues, such as, common contentwords and phrases.
To solve this problem, (Kuro00) and (Kiyota 02) proposed methods of collect-ing knowledge for answering questions from FAQdocuments and technical manuals by using the doc-ument structure, such as, a dictionary-like struc-ture and if-then format description.
However, thesekinds of documents requires the considerable costof developing and maintenance.
As a result, it isimportant to investigate a method of extracting ev-idential sentences for answering how type ques-tions from natural language documents at low cost.To solve this problem, (Watanabe 04) proposed amethod of developing a knowledge base by usingmails posted to a mailing list (ML).
We have thefollowing advantages when we develop knowledgebase by using mails posted to a mailing list.?
it is easy to collect question and answer mailsin a specific domain, and?
there is some expectation that information isupdated by participantsNext problem is wrong information.
It is almostinevitable that natural language documents, espe-cially web documents, contain wrong information.For example, (DA1?1) is opposed by (QR1?1?1).
(Q1) How I set up my wheel mouse for the netscapenavigator?
(DA1?1) You can find a setup guide in the Dec. is-sue of SD magazine.
(QR1?1?1) I can not use it although I modified/usr/lib/netscape/ja/Netscape accord-ing to the guide.Wrong information is a central problem of devel-oping a knowledge base by using natural language31documents.
As a result, it is important to investi-gate a method of detecting and correcting wrong in-formation in natural language documents.
(Watan-abe 05) reported a method of detecting wrong in-formation in question and answer mails posted to amailing list.
In (Watanabe 05), wrong informationin the mails can be detected by using mails whichML participants submitted for correcting wrong in-formation in the previous mails.
Then, the systemgives one of the following confirmation labels toeach set of question and their answer mails:positive label shows the information described ina set of a question and its answer mail is con-firmed by the following mails,negative label shows the information is opposedby the following mails, andother label shows the information is not yet con-firmed.Our knowledge base, on which our QA systembases, is composed of these labeled sets of a ques-tion and its answer mail.
Finally, we describe a QAsystem: It finds question mails which are similarto user?s question and shows the results to the user.The similarity between user?s question and a ques-tion mail is calculated by matching of user?s ques-tion and the significant sentence extracted from thequestion mail.
A user can easily choose and accessinformation for solving problems by using the sig-nificant sentences and these confirmation labels.2 Confirmed knowledge base developedby using mails posted to a mailing listThere are mailing lists to which question and an-swer mails are posted frequently.
For example, inVine Users ML, several kinds of question and an-swer mails are posted by participants who are in-terested in Vine Linux 1.
We reported that mailsposted to these kinds of mailing lists have the fol-lowing features.1.
Answer mails can be classified into threetypes: (1) direct answer (DA) mail, (2) ques-tioner?s reply (QR) mail, and (3) the others.Direct answer mails are direct answers to theoriginal question.
Questioner?s reply mails1Vine Linux is a linux distribution with a customizedJapanese environment.are questioner?s answers to the direct answermails.2.
Question and answer mails do not have a firmstructure because questions and their answersare described in various ways.
Because of nofirm structure, it is difficult to extract preciseinformation from mails posted to a mailing listin the same way as (Kuro 00) and (Kiyota 02)did.3.
A mail posted to ML generally has a signifi-cant sentence.
For example, a significant sen-tence of a question mail has the following fea-tures:(a) it often includes nouns and unregisteredwords which are used in the mail subject.
(b) it is often quoted in the answer mails.
(c) it often includes the typical expressions,such as,(ga / shikasi (but / however)) + ?
?
?
+ mashita /masen / shouka / imasu (can / cannot / whether /current situation is) + .
(ex) Bluefish de nihongo font ga hyouji dekimasen.
(I cannot see Japanese fonts on Bluefish.
)(d) it often occurs near the beginning.Taking account of these features, (Watanabe 05)proposed a method of extracting significant sen-tences from question mails, their DA mails, and QRmails by using surface clues.
Furthermore, (Watan-abe 05) proposed a method of detecting wrong in-formation in a set of a question mail and its DAmail by using the QR mail.For evaluating our method, (Watanabe 05) se-lected 100 examples of question mails in VineUsers ML.
They have 121 DA mails.
Each set ofthe question and their DA mails has one QR mail.First, we examined whether the results of deter-mining the confirmation labels were good or not.The results are shown in Table 1.
Table 2 showsthe type and number of incorrect confirmation.
Thereasons of the failures were as follows:?
there were many significant sentences whichdid not include the clue expressions.?
there were many sentences which were notsignificant sentences but included the clue ex-pressions.32Table 1: Results of determining confirmation labelstype correct incorrect totalpositive 35 18 53negative 10 4 14other 48 6 54Table 2: Type and number of incorrect confirmationincorrect type and number of correct answersconfirmation positive negative other totalpositive ?
4 14 18negative 2 ?
2 4other 4 2 ?
6Table 3: Results of determining confirmation labelsto the proper sets of a question and its DA maillabeling result positive negative other totalcorrect 29 8 27 64failure 4 4 15 23?
some question mails were submitted not forasking questions, but for giving some news,notices, and reports to the participants.
Inthese cases, there were no answer in the DAmail and no sentence in the QR mail for con-firming the previous mails.?
questioner?s answer was described in severalsentences and one of them was extracted, and?
misspelling.Next, we examined whether these significantsentences and the confirmation labels were helpfulin choosing and accessing information for solvingproblems.
In other words, we examined whether?
there was good connection between the signif-icant sentences or not, and?
the confirmation label was proper or not.For example, (Q2) and (DA2?1) in Figure 1 havethe same topic, however, (DA2?2) has a differ-ent topic.
In this case, (DA2?1) is a good answerto question (Q2).
A user can access the docu-ment from which (DA2?1) was extracted and ob-tain more detailed information.
As a result, the setof (Q2) and (DA2?1) was determined as correct.On the contrary, the set of (Q2) and (DA2?1) wasa failure.
In this experiment, 87 sets of a questionand its DA mail were determined as correct and 34sets were failures.
The reasons of the failures wereas follows:(Q2) vedit ha, sonzai shinai file wo hirakou to suru to corewo haki masuka.
(Does vedit terminate when we open anew file?
)(DA2?1) hai, core dump shimasu.
(Yes, it terminates.
)(DA2?2) shourai, GNOME ha install go sugu tsukaeru nodesu ka?
(In near future, can I use GNOME justafter the installation?
)(Q3) sound no settei de komatte imasu.
(I have much troublein setting sound configuration.
)(DA3?1) mazuha, sndconfig wo jikkou shitemitekudasai.
(First, please try ?sndconfig?.
)(QR3?1?1) kore de umaku ikimashita.
(I did well.
)(DA3?2) sndconfig de, shiawase ni narimashita.
(I tried?sndconfig?
and became happy.
)(Q4) ES1868 no sound card wo tsukatte imasu ga, oto ga ook-isugite komatte imasu.
(My trouble is that sound cardES1868 makes a too loud noise.
)(DA4?1) xmixer wo tsukatte kudasai.
(Please use xmixer.
)(QR4?1?1) xmixer mo xplaycd mo tsukaemasen.
(I can-not use xmixer and xplaycd, too.
)Figure 1: Examples of the significant sentence ex-traction?
wrong significant sentences extracted fromquestion mails, and?
wrong significant sentences extracted fromDA mails.Failures which were caused by wrong significantsentences extracted from question mails were notserious.
This is because there is not much likeli-hood of matching user?s question and wrong sig-nificant sentence extracted from question mails.On the other hand, failures which were causedby wrong significant sentences extracted from DAmails were serious.
In these cases, significant sen-tences in the question mails were successfully ex-tracted and there is likelihood of matching user?squestion and the significant sentence extractedfrom question mails.
Therefore, the precision ofthe significant sentence extraction was emphasizedin this task.Next, we examined whether proper confirmationlabels were given to these 87 good sets of a questionand its DA mail or not, and then, we found thatproper confirmation labels were given to 64 sets inthem.
The result was shown in Table 3.We discuss some example sets of significant sen-tences in detail.
Question (Q3) in Figure 1 hastwo answers, (DA3?1) and (DA3?2).
(DA3?1) is33Figure 3: A QA example which was generated by our systemPositiveNegativeOthersPositiveNegativeOthersQA processor Knowledge BaseUser InterfaceQuestion InputOutputInputAnalyzerSimilarityCalculatorSynonymDictionaryMailsposted to MLFigure 2: System overviewa suggestion to the questioner of (Q3) and (DA3?2) explains answerer?s experience.
The point to benoticed is (QR3?1?1).
(QR3?1?1) contains a clueexpression, ?umaku ikimashita (did well)?, whichgives a positive label to the set of (Q3) and (DA3?1).
It guarantees the information quality of (DA3?1) and let the user choose and access the answermail from which (DA3?1) was extracted.
(DA4?1) in Figure 1 which was extracted froma DA mail has wrong information.
Then, the ques-tioner of (Q4) confirmed whether the given infor-mation was helpful or not, and then, posted (QR4?1?1) in order to point out and correct the wronginformation in (DA4?1).
In this experiment, wefound 16 cases where the questioners posted replymails in order to correct the wrong information, andthe system found 10 cases in them and gave nega-tive labels to the sets of the question and its DAmail.3 QA system using mails posted to amailing list3.1 Outline of the QA systemFigure 2 shows the overview of our system.
A usercan ask a question to the system in a natural lan-guage.
Then, the system retrieves similar questionsfrom mails posted to a mailing list, and shows theuser the significant sentences which were extractedthe similar question and their answer mails.
Ac-cording to the confirmation labels, the sets of thesimilar question and their answer mails were classi-fied into three groups, positive, negative, and other,and shown in three tabs (Figure 3).
A user can eas-ily choose and access information for solving prob-lems by using the significant sentences and the con-34firmation labels.
The system consists of the follow-ing modules:Knowledge Base It consists of?
question and answer mails (50846 mails),?
significant sentences (26334 sentences: 8964,13094, and 4276 sentences were extractedfrom question, DA, and QR mails, respec-tively),?
confirmation labels (4276 labels were given to3613 sets of a question and its DA mail), and?
synonym dictionary (519 words).QA processor It consists of input analyzer andsimilarity calculator.Input analyzer transforms user?s question into adependency structure by using JUMAN(Kuro 98)and KNP(Kuro 94).Similarity calculator calculates the similarity be-tween user?s question and a significant sentence ina question mail posted to a mailing list by compar-ing their common content words and dependencytrees in the next way:The weight of a common content word t whichoccurs in user?s question ?
and significant sentenceS?
in the mails M?
(i = 1 ?
?
?N ) is:wWORD(t?M?)
= t?(t?
S?)
?Nd?
(t)where t?(t?
S?)
denotes the number of times con-tent word t occurs in significant sentence S?, Ndenotes the number of significant sentences, andd?
(t) denotes the number of significant sentencesin which content word t occurs.
Next, the weightof a common modifier-head relation in user?s ques-tion ?
and significant sentence S?
in question mailM?
is:wLINK(l?M?)
= wWORD(?d?ier (l)?M?)+wWORD(head(l)?M?
)where ?di?ier (l) and head(l) denote a modifierand a head of modifier-head relation l, respectively.Therefore, the similarity score between user?squestion ?
and significant sentence S?
of ques-tion mail M?, SCO?E(?
M ?
), is set to the to-tal weight of common content words and modifier-head relations which occur user?s question ?
andsignificant sentence S?
of question mail M?, that is,SCO?E(?
M ?)
= SCO?EWORD( ?
M ?)+SCO?ELINK(?
M ?
)where the elements of set T?
and set L?
are commoncontent words and modifier-head relations in user?squestion ?
and significant sentence S?
in questionmail M?, respectively.When the number of common content wordswhich occur in user?s question ?
and significantsentence S?
in question mail M?
is more than one,the similarity calculator calculates the similarityscore and sends it to the user interface.User Interface Users can access to the systemvia a WWW browser by using CGI based HTMLforms.
User interface put the answers in order ofthe similarity scores.3.2 EvaluationFor evaluating our method, we gave 32 questions inFigure 4 to the system.
These questions were basedon question mails posted to Linux Users ML.
Theresult of our method was compared with the resultof full text retrievalTest 1 by examined first answerTest 2 by examined first three answersTest 3 by examined first five answersTable 4 (a) shows the number of questions whichwere given the proper answer.
Table 4 (b) showsthe number of proper answers.
Table 4 (c) showsthe number and type of confirmation labels whichwere given to proper answers.In Test 1, our system answered question 2, 6, 7,8, 13, 14, 15, 19, and 24.
In contrast, the full textretrieval system answered question 2, 5, 7, 19, and32.
Both system answered question 2, 7 and 19,however, the answers were different.
This is be-cause several solutions of a problem are often sentto a mailing list and the systems found differentbut proper answers.
In all the tests, the results ofour method were better than those of full text re-trieval.
Our system answered more questions andfound more proper answers than the full text re-trieval system did.
Furthermore, it is much easierto choose and access information for solving prob-lems by using the answers of our QA system than35(1) I cannot get IP address again from DHCP server.
(2) I cannot make a sound on Linux.
(3) I have a problem when I start up X Window System.
(4) Tell me how to restore HDD partition to its normal con-dition.
(5) Where is the configuration file for giving SSI permissionto Apache ?
(6) I cannot login into proftpd.
(7) I cannot input kanji characters.
(8) Please tell me how to build a Linux router with two NICcards.
(9) CGI cannot be executed on Apache 1.39.
(10) The timer gets out of order after the restart.
(11) Please tell me how to show error messages in English.
(12) NFS server does not go.
(13) Please tell me how to use MO drive.
(14) Do you know how to monitor traffic load on networks.
(15) Please tell me how to specify kanji code on Emacs.
(16) I cannot input \ on X Window System.
(17) Please tell me how to extract characters from PDF files.
(18) It takes me a lot of time to login.
(19) I cannot use lpr to print files.
(20) Please tell me how to stop making a backup file onEmacs.
(21) Please tell me how to acquire a screen shot on X window.
(22) Can I boot linux without a rescue disk?
(23) Pcmcia drivers are loaded, but, a network card is notrecognized.
(24) I cannot execute PPxP.
(25) I am looking for FTP server in which I can use chmodcommand.
(26) I do not know how to create a Makefile.
(27) Please tell me how to refuse the specific user login.
(28) When I tried to start Webmin on Vine Linux 2.5, theconnection to localhost:10000 was denied.
(29) I have installed a video capture card in my DIY machine,but, I cannot watch TV programs by using xawtv.
(30) I want to convert a Latex document to a Microsoft Worddocument.
(31) Can you recommend me an application for monitoringresources?
(32) I cannot mount a CD-ROM drive.Figure 4: 32 questions which were given to the sys-tem for the evaluationby using the answers of the full text retrieval sys-tem.Both systems could not answer question 4, ?Tellme how to restore HDD partition to its normal con-dition?.
However, the systems found an answer inwhich the way of saving files on a broken HDDpartition was mentioned.
Interestingly, this answermay satisfy a questioner because, in such cases, ourdesire is to save files on the broken HDD partition.In this way, it often happens that there are gaps be-tween what a questioner wants to know and the an-swer, in several aspects, such as concreteness, ex-pression and assumption.
To overcome the gaps, itis important to investigate a dialogue system whichTable 4: Results of finding a similar question bymatching of user?s question and a significant sen-tenceTest 1 Test 2 Test 3our method 9 15 17full text retrieval 5 5 8(a) the number of questions whichis given the proper answerTest 1 Test 2 Test 3our method 9 25 42full text retrieval 5 9 15(b) the number of proper answerspositive negative other positive & negativeTest 1 2 2 5 0Test 2 9 4 12 0Test 3 10 5 25 2(c) the number and type of labelsgiven to proper answerscan communicate with the questioner.ReferencesTREC (Text REtrieval Conference) : http://trec.nist.gov/NTCIR (NII-NACSIS Test Collection for IR Systems) project:http://research.nii.ac.jp/ntcir/index-en.htmlKurohashi and Higasa: Dialogue Helpsystem based on Flexi-ble Matching of User Query with Natural Language Knowl-edge Base, 1st ACL SIGdial Workshop on Discourse andDialogue, pp.141-149, (2000).Kiyota, Kurohashi, and Kido: ?Dialog Navigator?
A QuestionAnswering System based on Large Text Knowledge Base,COLING02, pp.460-466, (2002).Kurohashi and Nagao: A syntactic analysis method of longJapanese sentences based on the detection of conjunctivestructures, Computational Linguistics, 20(4),pp.507-534,(1994).Kurohashi and Nagao: JUMAN Manual version 3.6 (inJapanese), Nagao Lab., Kyoto University, (1998).Mihara, fujii, and Ishikawa: Helpdesk-oriented Question An-swering Focusing on Actions (in Japanese), 11th Conven-tion of NLP, pp.
1096?1099, (2005).Watanabe, Sono, Yokomizo, and Okada: A Question AnswerSystem Using Mails Posted to a Mailing List, ACM Do-cEng 2004, pp.67-73, (2004).Watanabe, Nishimura, and Okada: Confirmed Knowledge Ac-quisition Using Mails Posted to a Mailing List, IJCNLP05,(2005).36
