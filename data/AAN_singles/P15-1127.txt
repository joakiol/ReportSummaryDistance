Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1311?1320,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsScalable Semantic Parsing with Partial OntologiesEunsol Choi Tom Kwiatkowski?Luke ZettlemoyerComputer Science & EngineeringUniversity of Washingtoneunsol@cs.washington.edu, tomkwiat@google.com, lsz@cs.washington.eduAbstractWe consider the problem of building scal-able semantic parsers for Freebase, andpresent a new approach for learning to dopartial analyses that ground as much of theinput text as possible without requiring thatall content words be mapped to Freebaseconcepts.
We study this problem on twonewly introduced large-scale noun phrasedatasets, and present a new semantic pars-ing model and semi-supervised learningapproach for reasoning with partial onto-logical support.
Experiments demonstratestrong performance on two tasks: refer-ring expression resolution and entity at-tribute extraction.
In both cases, the par-tial analyses allow us to improve precisionover strong baselines, while parsing manyphrases that would be ignored by existingtechniques.1 IntroductionRecently, significant progress has been made inlearning semantic parsers for large knowledgebases (KBs) such as Freebase (FB) (Cai and Yates,2013; Berant et al, 2013; Kwiatkowski et al,2013; Reddy et al, 2014).
Although these methodscan build general purpose meaning representations,they are typically evaluated on question answeringtasks and are designed to only parse questions thathave complete ontological coverage, in the sensethat there exists a logical form that can be executedagainst Freebase to get the correct answer.1In thispaper, we instead consider the problem of learningsemantic parsers for open domain text containing?Now at Google, NY.1To ensure all questions are answerable, the data is man-ually filtered.
For example, the WebQuestions dataset intro-duced by Berant et al (2013) contains only the 7% of theoriginally gathered questions.WikipediaHaitian human rights activistsArt museums and galleries in New YorkSchool buildings completed in 1897Olympic gymnasts of NorwayAppos.the capital of quake-hit Sichuan Provincea major coal producing provincethe relaxed seaside capital of MozambiqueFigure 1: Example noun phrases from Wikipediacategory labels and appositives in newswire text.concepts that may or may not be representable us-ing the Freebase ontology.Even very large knowledge bases have two typesof incompleteness that provide challenges for se-mantic parsing algorithms.
They (1) have partialontologies that cannot represent the meaning ofmany English phrases and (2) are typically miss-ing many facts.
For example, consider the phrasesin Figure 1.
They include subjective or otherwiseunmodeled phrases such as ?relaxed?
and ?quake-hit.?
Freebase, despite being large-scale, containsa limited set of concepts that cannot represent themeaning of these phrases.
They also refer to enti-ties that may be missing key facts.
For example, arecent study (West et al, 2014) showed that over70% of people in FB have no birth place, and 99%have no ethnicity.
In our work, we introduce a newsemantic parsing approach that explicitly modelsontological incompleteness and is robust to miss-ing facts, with the goal of recovering as much of asentence?s meaning as the ontology supports.
Weargue that this will enable the application of se-mantic parsers to a range of new tasks, such asinformation extraction (IE), where phrases rarelyhave full ontological support and new facts mustbe added to the KB.Because existing semantic parsing datasets havebeen filtered to limit incompleteness, we introducetwo new corpora that pair complex noun phraseswith one or more entities that they describe.
The1311(a)Wikipediacategoryx : Symphonic Poems by Jean Sibeliuse : {The Bard, Finlandia,Pohjola?s Daughter, En Saga, Spring Song, Tapiola... }l0: ?x.Symphonic(x) ?
Poems(x) ?
by(JeanSibelius, x)y : ?x.composition.form(x, Symphonicpoems) ?
composer(JeanSibelius, x)x : Defunct Korean football clubse : { Goyang KB Kookmin Bank FC,Hallelujah FC, Kyungsung FC }l0: ?x.defunct(x) ?
korean(x) ?
football(x) ?
clubs(x)y : ?x.OpenType[defunct](x) ?
OpenRel(x, KOREA) ?
football clubs(x))(b)Apposx : a driving force behind the projecte : Germanyl0: ?x.driving(x) ?
force(x) ?
behind(x, theproject)y : ?x.OpenType[driving force](x) ?
OpenRel[behind](x, OpenEntity[the project])x : an EU outpost in the Mediterraneane : Maltal0: ?x.outpost(x) ?
EU(x) ?
in(x, theMediterranean)y : ?x.OpenRel(x, EU) ?
OpenType[outpost](x) ?
contained by(x, MediterraneanSea)Figure 2: Examples of noun phrases x, from the Wikipedia category and apposition datasets, paired withthe set of entities e they describe, their underspecified logical form l0, and their final logical form y.first new dataset contains 365,000 Wikipedia cate-gory labels (Figure 1, top), each paired with the listof the associated Wikipedia entity pages.
The sec-ond has 67,000 noun phrases paired with a singlenamed entity, extracted from the appositive con-structions in KBP 2009 newswire text (Figure 1,bottom).2This new data is both large scale, andunique in the focus on noun phrases.
Noun phrasescontain a number of challenging compositionalphenomena, including implicit relations and noun-noun modifiers (e.g.
see Gerber and Chai (2010)).To better model text with only partial ontologi-cal support, we present a new semantic parser thatbuilds logical forms with concepts from a targetontology and open concepts that are introducedwhen there is no appropriate concept match inthe target ontology.
Figure 2 shows examples ofthe meanings that we extract.
Only the first ofthese examples can be fully represented using Free-base, all other examples require explicit modelingof open concepts.
To build these logical forms,we follow recent work for Combinatory Categori-cal Grammar (CCG) semantic parsing with Free-base (Kwiatkowski et al, 2013), extended to modelwhen open concepts should be used.
We developa two-stage learning algorithm: we first computebroad coverage lexical statistics over all of the data,which are then incorporated as features in a fullparsing model.
The parsing model is tuned on ahand-labeled data set with gold analyses.Experiments demonstrate the benefits of the newapproach.
It significantly outperforms strong base-2All new data is available on the authors?
websites.lines on both a referring expression resolution task,where much like in the QA setting we directly eval-uate if we recover the correct logical form for eachinput noun phrase, and on entity attribute extrac-tion, where individual facts are extracted from thegroundable part of the logical form.
We also seethat modeling incompleteness significantly boostsprecision; we are able to more effectively deter-mine which words should not be mapped to KBconcepts.
When run on all of the Wikipedia cat-egory data, we estimate that the learned modelwould discover 12 million new facts that could beadded to Freebase with 72% precision.2 OverviewSemantic Parsing with Open Concepts Ourgoal is to learn to map noun phrase referring ex-pressions x to logical forms y that describe theirmeaning.
In this work, y is built using both con-cepts from a knowledge base K and open conceptsthat lie outside of the scope of K. For example,in Figure 2 the phrase ?Defunct Korean footballclubs?
is modeled using a logical form y that con-tains the K concept football clubs(x) as wellas the open concepts OpenType[defunct](x).In this paper we describe a new method for learn-ing the mapping from x to y from corpora of refer-ring expression noun phrases, paired with a sets ofentities e that these referring expressions describe.Figure 2 shows examples of these data drawn fromtwo sources.Tasks We introduce two new datasets (Sec.
3)that pair referring noun phrases x with one or more1312entities e that they describe.
These data supportevaluation for two tasks: referring expression reso-lution and information extraction.In referring expression resolution, the parser isgiven x and is used to predict the referring expres-sion logical form y that describes e. Since themajority of our data cannot be fully modeled withFreebase, we evaluate each y against a hand labeledgold standard instead of trying to extract e from K.The entity attribute extraction task also involvesmapping phrases x to logical forms y, with thegoal of adding new facts to the knowledge base K.To do this, we assume each x is additionally pairedwith an set of entities e. We also define an entityattribute to be a literal in y that uses only conceptsfrom K. Finally, we extract, for each entity ine, all of the attributes listed in y.
For example,the first logical form y in Figure 2 has twoentity attributes: composer(JeanSibelius, x)and composition.form(x, Symphonic poems)which can be added to K for the entities{TheBard, Finlandia}.Model and Learning Our approach extendsthe two-stage semantic parser introduced byKwiatkowski et al(2013).
We use CCG to builddomain-independent logical forms l0and then in-troduce a new method for reasoning about howto map this intermediary representation onto bothopen concepts and K concepts (Sec.
4).To learn this model, we assume access to datawith two different types of annotations.
The firstcontains noun phrase descriptions x and describedentity sets e (as in Figure 2), which can be eas-ily gathered at scale with no manual data labelingeffort.
However, this data, in general, has signif-icant amount of knowledge base incompleteness;many described concepts and entity attributes willbe missing from K (see Sec.
3 for more details).Therefore, to support effective learning, we willalso use a small hand-labeled dataset containingx, e, a gold logical form y, an intermediary CCGlogical form l0, and a mapping from words in x toconstants inK and open concepts.
Our full learningapproach (Sec.
5) estimates a linear model on thesmall labeled dataset, with broad coverage featuresderived from the larger dataset.3 DataWe gathered two new datasets that pair complexnoun phrases with one or more Freebase entities.The Wikipedia category dataset contains365,504 Wikipedia category names paired with thelist of entities in that category.3Table 1 shows thedetails of this dataset and examples are given inFigure 2.
For each development and test data, werandomly select 500 categories consisted of 3-10words and describing fewer than 100 entities.The apposition dataset is a large set of com-plex noun phrases paired with named entities, ex-tracted from appositive constructions such as ?Gus-tav Bayer, a former Olympic gymnast for Norway.
?For this example, we extract the entity ?GustavBayer?
and pair it with the noun phrase ?a formerOlympic gymnast for Norway.?
To identify apposi-tive constructions, we ran the Stanford dependencyparser on the newswire section of the KBP 2009source corpus,4and selected noun phrases com-posed of 3 to 10 words, starting with an article, andpaired with a named entity that is in Freebase.This procedure of identifying complex entity de-scriptions allows for information extraction froma wide range of sources.
However, it is also noisyand challenging.
The dependency parser makes er-rors, for example ?the next day against the UnitedStates, Spain?
is falsely detected as an apposition.Furthermore, addressing context and co-referenceis often necessary.
For example, ?Puerto Montt, acity south of the capital?
or ?the company?s par-ent, Shenhua Group?
requires reference resolution.We gathered 67 thousand appositions, which willbe released to support future work, and randomlyselected 300 for testing.Measuring Incompleteness To study theamount of incompleteness in this data, we handlabeled logical forms for 500 Wikipedia categoriesin the development set.
Examples of annotationsare given in the rows labeled y in Figure 2.
Weuse these to measure the schema and fact coverageof Freebase.
Many of the entities in this datasetdo not have the Freebase attributes described bythe category phrases.
When a concept is not inFreebase, we annotate it as OpenType or OpenRel,as shown in Figure 2.
On average, each Wikipediacategory name describes 2.58 Freebase attributes,and 0.39 concepts that cannot be mapped to FB.Overall, 27.2% of the phrases contain conceptsthat do not exist in the Freebase schema.3Compiled by the YAGO project, available at: www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/4http://www.nist.gov/tac/2009/1313entire set dev test# categories 365,504 500 500# words per category 4.1 4.4 4.3# unique words 84,996 1,100 1,063# entities per category 19.9 19.1 18.7# entities 2,813,631 9,511 9,281# entity-category pairs 7,292,326 9,549 9,331Table 1: Wikipedia category data statistics.entire set test set# appositions 66,924 300# unique words 25,472 817# words per apposition 5.73 5.93Table 2: Appositive data statistics.Each category may have multiple correct logicalforms.
For example, ?Hotels?
can be mapped to:hotel(x), accomodation.type(x, hotel), orbuilding function(x, hotel).
There are alsogenuine ambiguities in meaning.
For example,?People from Bordeaux?
can be interpreted aspeople(x) ?
place lived(x, Bordeaux) orpeople(x) ?
place of birth(x, Bordeaux).We made a best effort attempt to gather as manycorrect logical forms as possible, finding onaverage 1.8 logical forms per noun phrase.
Therewere 97 unique binary relations, and 247 uniqueunary attributes in the annotation.Given these logical forms, we also measuredfactual coverage.
For the 72.8% of phrases thatcan be completely represented using Freebase, weexecuted the logical forms and compared the resultto the labeled entity set.
In total, 56% of the queriesreturned no entities and those that did return resultshave on average 15% overlap with the Wikipediaentity set.
We also measured how often attributesfrom the labeled logical forms were assigned to theWikipedia entities in FB, finding that only 33.6%were present.
Given this rate, we estimate that it ispossible to add 12 million new facts into FB fromthe 7 million entity-category pairs.4 Mapping Text to MeaningWe adopt a two-stage semantic parsing ap-proach (Kwiatkowski et al, 2013).
We first usea CCG parser to define a set CCG(x) of possiblelogical forms l0.
Then we will choose the logicalform l0that closely matches the linguistic struc-ture of the input text x, according to a learnedlinear model, and use an ontological match stepthat defines a set of transformations ONT(l0,K) tomap this meaning to a Freebase query y.
Figure 2shows examples of x, l0and y.
In this section wedescribe our approach with the more detailed ex-ample derivation in Figure 3.
We also describe theparameterization of a linear model that scores eachderivation.CCG parsing We use a CCG (Steedman, 1996)semantic parser (Kwiatkowski et al, 2013) to gen-erate an underspecified logical form l0.
Figure 3ashows an example parse.
The constants Former,Municipalities, in, Brandenburgh in l0are nottied to the target knowledge base, causing the logi-cal form to be underspecified.
They can be replacedwith Freebase constants in the later ontology match-ing step.Ontological Matching The ontological matchstep has structural match and constant match com-ponents.
Structural match operators can collapseor expand sub-expressions in the logical formsto match equivalent typed concepts in the targetknowledge base.
We adopt existing structuralmatch operators (Kwiatkowski et al, 2013) andrefer readers to that work for details.Constant match operators replace underspeci-fied constants in the underspecified logical forml0with concepts from the target knowledge base.There are four constant match operations used inFigure 3.
The first two constant matches, shown be-low, match underspecified constants with constantsof the same type from Freebase.in?
location.containedbyBrandenburgh?
BRANDENBURGHHowever, because we are modeling the semanticsof phrases that are not covered by the Freebaseschema, we also require the following two constantmatches:Former(x)?
OpenTypemunicipalities(x)?
OpenRel(x, Municipality)Here, the word ?former?
has been associated witha placeholder typing predicate since Freebase hasno way of expressing end dates of administrativedivisions.
There is also no Freebase type repre-senting the concept ?municipalities.?
However, thisword is associated with an entity in Freebase.
Sincethere is no suitable linking predicate for the entityMunicipality, we introduce a placeholder link-ing predicate OpenRel in the step from l2?
l3.Our constant match operators can also introduceplaceholder entities OpenEntity when there is nogood match in Freebase.1314(a) CCG parse builds an underspecified semantic representation of the sentence.Former municipalities in BrandenburghN/N N N\N/NP NP?f?x.f(x) ?
former(x) ?x.municipalities(x) ?f?x?y.f(y) ?
in(y, x) Brandenburg> >N N\N?x.former(x) ?municipalities(x) ?f?y.f(y) ?
in(y,Brandenburg)<Nl0= ?x.former(x) ?municipalities(x) ?
in(x,Brandenburg)(b) Constant matches replace underspecified constants with Freebase conceptsl0= ?x.former(x) ?municipalities(x) ?
in(x,Brandenburg)l1= ?x.former(x) ?municipalities(x) ?
in(x, Brandenburg)l2= ?x.former(x) ?municipalities(x) ?
location.containedby(x, Brandenburg)l3= ?x.former(x) ?
OpenRel(x, Municipality) ?
location.containedby(x, Brandenburg)l4= ?x.OpenType(x) ?
OpenRel(x, Municipality) ?
location.containedby(x, Brandenburg)Figure 3: Derivation of the analysis for ?Former municipalities in Brandenburgh?.
This analysis containsa placeholder type and a placeholder relation as described in Section 4.We also allow the creation of typing predicatesfrom matched entities through the introduction oflinking predicates.
For example, there is no nativetype associated with the word ?actor?
in Freebase.Instead we create a typing predicate by matchingthe word to a Freebase entity Actor using Free-base API and allowing the introduction of linkedpredicates such as person.profession :actor(x)?
person.profession(x, Actor)Scoring Full Parses Our goal in this paper is tolearn a function from the phrase x to the correctanalysis y.
We score each parse using a linearmodel with features that signal attributes of theunderspecified parse ?pand those that signal at-tributes of the ontological match ?ont.
Since themodel factors over the two stages of parser, we splitthe prediction problem similarly.
First, we selectthe maximum scoring underspecified logical form:l?= arg maxl?CCG(x)(?p?
?p(l))and then we select the highest scoring Freebaseanalysis y?that can be built from l?
:y?= arg maxr?ONT(l?,K)(?ont?
?ont(r))We describe an approach to learning the parametervectors ?pand ?ontbelow.5 LearningWe introduce a learning approach that first collatesaggregate statistics from the 7 million Wikipediaentity-category pairs and existing facts in FB, andthen uses a small labeled training set to tune theweights for features that incorporate these statistics.Wikipedia CategoryWars involving the Grand Duchy of LituaniaEntity AttributeBattleOfGrunwald type(x, military.conflict)GollubWar type(x, military.conflict)BattleOfGrunwald time.event.loc(x, Grunwald).
.
.
.
.
.Entity RelationBattleOfGrunwald military conflict.combatantsGollubWar time.event.start timeBattleOfGrunwald military conflict.commanders.
.
.
.
.
.Figure 4: Labeled entities are associated with at-tributes and relations.Broad Coverage Lexical Statistics EachWikipedia category is associated with a numberof entities, most of which exist in FB.
We usethese entities to extract relations and attributes inFB associated with that category.
For example,in Figure 4 the category ?Wars involving theGrand Duchy of Lithuania?
is associated withthe relation military conflict.combatantsand the attribute type(x, military.conflict)multiple times, because they are present in many ofthe category?s entities.
For each of the sub-phrasesin the category name we count these associationsover the entire Wikipedia category set.We use these counts to calculate PointwiseMutual Information (PMI) between words andFreebase attributes or relations.
We choose PMI toavoid overcompensating common words, attributes,or relations.
For example, the word ?Wars?
is seenwith the incorrect analysis type(x, time.event)more frequently than the correct analysistype(x, military.conflict).
However, PMIpenalizes the attribute type(x, time.event) for1315its popularity and the correct analysis is preferred.As PMI has a tendency to emphasize rare counts,we chose PMI squared, which takes the squaredvalue of the co-occurence count (PMI2(a, b) =logcount(a?b)2count(a)?count(b)), as a feature.Structural KB Statistics Existing semanticparsers typically make use of type constraintsto limit the space of possible logical forms.These strong type constraints are not fea-sible when the knowledge base is incom-plete.
For example, in Freebase the relationmilitary conflict.combatants expects an en-tity of type military conflict.combatant asits object.
However, many countries that have beeninvolved in wars are not assigned this type.We instead calculate type overlap statistics forall Freebase entities, to find likely missing types.For example, including the fact that the object ofmilitary conflict.combatants is very oftenof type location.country.Learning from Labeled Data We train eachhalf of the prediction problem separately, as de-fined in Section 4, using the labeled training dataintroduced in Section 3.
We use structured max-margin perceptrons to learn feature weights forboth the underspecified parse and the ontologicalmatch step following (Kwiatkowski et al, 2013).The aggregate statistics collected from 7 millioncategory-entity pairs produce very useful lexicalfeatures.
We integrate these statistics into our linearmodel by summing their values for each derivationand treating them as a feature.
All of the other fea-tures described in Section 6 are not word specificand are therefore far less sparse.6 FeaturesWe include a number of features that enable softtype checking on the output logical form, describedfirst below, along with other features that measuredifferent aspects of the analysis.Coherency features For example, con-sider the phrase ?The UK home city ofthe Queen,?
with Freebase logical formy = ?x.home(QEII, x) ?
in(x, UK) ?
city(x).Each of the relations has expected types fortheir argument: the relation ?home?
expectsa subject of type ?person?
and an objectof type ?location?.
Each type in Freebaselives in a hierarchy, so the type city implies{location, administrative division, .
.
.
}.The next four features test agreement of thesetypes on different parts of the output logical form.Relation arguments trigger a feature if theirtype is in the set of types expected by the relation.QEII is a person so this feature is triggered for therelation-argument application in home(QEII, x).Relation-relation pairs can share variable argu-ments.
For example, the variable x is the objectof ?home?
and the subject of ?in?.
Each relationexpects a set of types of x.
We have features tosignal if: these sets are disjoint; one set subsumesthe other; and the PMI between the highest levelexpected type (described in Section 5) if the setsare disjoint.
In the example given here, the type?location?
expected by ?in?
subsumes the type?city?
expected by ?home?
so the second featurefires.
We treat types such as city(x) as unaryrelations and include them in this feature set.Type domain measures compatibility among do-mains in Freebase.
Freebase is split into high-leveldomains and some of these are relevant, such as?football?
and ?sports?.
We identify those by count-ing their co-occurrences.
This becomes an indica-tor feature that signals their co-occurrence in y.Named entity type features test if the entity ethat we are extracting attributes for have Freebasetype ?person?, ?location?
or ?organization?.
If itdoes, we have a feature indicating if y defines aset of the same type.
This features is not used inthe referring expression task presented in Section 7since we cannot assume access to the entities thatare described.CCG parse feature signals which lexical itemswere used in the CCG parse.
Another feature firesif capitalized words map to named entities.String similarity features signal exact stringmatch, stemmed string match, and length weightedstring edit distance between a phrase in the sen-tence and the name of the Freebase element it wasmatched on.
We also use the Freebase search APIto generate scores for phrase, entity pairs and in-clude the log of this score as a features.Lexical PMI feature includes the lexical Point-wise Mutual Information described in Section 5.Freebase constant features signal the use oflinking predicates, as defined in Section 4, andthe log frequency count of the Freebase attributesacross all entities in the Wikipedia category set.1316Other features indicate the use of OpenRel,OpenEntity, OpenType in y and count repetitionsof Freebase concepts in y.7 Experimental SetupKnowledge base We use the Jan. 26, 2014 Free-base dump.
After pruning binary predicates takingnumeric values, it contains 9351 binary predicates,2754 unary predicates, and 1.2 billion assertions.Pruning and Feature Initialization We per-form beam search at each semantic parsing stage,using the Freebase search API to determine candi-date named entities (10 per phrase), binary predi-cates (300 per phrase), and unary predicates (500per phrase).
The ontology matching stage consid-ers the highest scored underspecified parse.The features are initialized to prefer well-typedlogical forms.
Type checking features are initiallyset to -2 for mismatch.
Features signalling incom-patible topic domains and repetition are initializedas -10.
All other initial feature weights are set to 1.Datasets and Annotation We evaluate on theWikipedia category and appositive datasets intro-duced in Sec.
3.
On the Wikipedia developmentdata, we annotated 500 logical forms, underspeci-fied logical forms and constant mappings for ontol-ogy matching.
The Wikipedia test data is composedof 500 unseen categories.
We did not train on theappositive dataset, as it contains challenges suchas co-reference and parsing errors as described inSec.
3.
Instead, we chose 300 randomly selected ex-amples for evaluation, and ran on the model trainedon the Wikipedia development data.Evaluation Metrics We report five-fold crossvalidation for development but ran the final modelonce on the test data, manually scoring the output.For evaluation on the referring expression resolu-tion performance (as defined in Sec.
2), we includeaccuracy for the final logical form (Exact Match).We also evaluate precision and recall for predictingindividual literals in this logical form on the devel-opment set.
To control for missing facts, we didnot evaluate the set of returned entities.To evaluate entity attribute extraction perfor-mance (as defined in Sec.
2), we identified threeclasses of predictions.
Extractions can be correct,benign, or false.
Correct attributes are actuallydescribed in the phrase, benign extraction maynot have been described but are still true, andfalse extractions are not true.
For example, ifSystemExact Partial MatchMatch P R F1KCAZ13 1.4 9.6 6.3 7.0IE Baseline 6.8 37.0 23.3 28.6NoPMI 11.0 23.7 20.8 21.6NoOpenSchema 13.7 35.8 30.0 31.1NoTyping 9.6 37.6 29.3 31.8Our Approach 15.9 39.3 33.5 35.1with Gold NE 20.8 46.6 40.5 42.3Table 3: Referring expression resolution perfor-mance on the development set on gold references.Data System Exact Match AccuracyWikipediaIE Baseline 21.8%Our Approach 28.4%ApposIE Baseline 0.0%Our Approach 4.7%Table 4: Manual evaluation for referring expressionresolution on the test sets.the phrase ?the capital of the communist-rulednation?
is mapped to the pair of attributescapital of administrative division(x) ,location(x), the first is correct and the second isbenign.
Other incorrect facts would be false.On the development set, we report precision andrecall against the union of the FB attributes in ourannotations without adjusting for benign extrac-tions or the fact that the annotations are not com-plete.
For the test sets, we computed precision(P) where benign extractions are considered to bewrong, as well as an adjusted precision metric (P*)where benign extractions are counted as correct.
Aswe do not have full test set annotations, we cannotreport recall.
Finally, we report the average numberof facts extracted per noun phrase (fact #).Comparison Systems We compare performanceto a number of ablated versions of the full system,where we have removed the open-constant ontologymatching operators (NoOpenSchema), the PMI fea-tures (NoPMI), or the type checking features (No-Typing).
For the referring expression resolutiontask, we excluded the named entity type feature, asthis assumes typing information about the entitywe are extracting attributes for.We report results without the PMI features andthe open schema matching operators (KCAZ13),which is a reimplementation of a recent FreebaseQA model (Kwiatkowski et al, 2013).
We alsolearn with gold named entity linking (Gold NE).For the entity attribute extraction, we built a su-pervised learning baseline that combines the outputof two discrete SVMs, one for predicting unary re-lations and one for binary relations.
Each classifier1317System Top n P R F1 fact #IE Baseline - 37.3 26.5 30.6 1.6Our Approach1 44.2 32.8 37.7 1.92 36.9 38.0 37.5 2.63 30.7 42.7 35.7 3.64 27.0 44.7 33.6 4.25 23.7 47.2 31.6 5.110 15.9 52.0 24.3 8.5Table 5: Entity attribute extraction performance onthe Wikipedia category development set.Data System P P* fact #WikipediaIE Baseline 56.7 58.7 1.6Our Approach 61.2 72.6 2.0ApposIE Baseline 4.9 13.9 1.3Our Approach 33.2 61.4 0.9Table 6: Manual evaluation for entity attribute ex-traction on the test sets.is trained using the annotated Wikipedia categories.This dataset contains hundreds of unary and bi-nary relations, which the IE baseline can predict.Each classifier is further anchored on a specificword, and includes n-gram and POS context fea-tures around that word, following features fromMintz et al(2009).
To predict binary relations, weused named entities as anchors.
For unary attributeswe anchored on all possible nouns and adjectives.The final logical form includes the best relationpredicted by each classifier.
We use the StanfordCoreNLP5toolkit for tokenization, named entityrecognition, and part-of-speech tagging.8 ResultsTables 3 and 4 show performance on the referringexpression resolution task.
Tables 5 and 6 showperformance on the extraction task.
Reported pre-cision is lower on the labeled development set thanon the test set, where predicted logical forms aremanually evaluated.
This reflects the fact that, de-spite our best attempts, the development set labelsare incomplete, as discussed in Section 3.Referring expression resolution The systemsretrieve the full meaning with 28.4% accuracy onthe Wikipedia test set, and 15.9% on the develop-ment set.
The gold named entity input improvesperformance by modest amounts.
This suggeststhat the errors stem from ontology mismatches, aswe will describe in more detail later in the qualita-tive analysis.
We also see that all of the ablations5http://nlp.stanford.edu/software/corenlp.htmlhurt performance, and that the KCAZ13 model per-forms extremely poorly.
The independent classifierbaseline performs well at the sub-clause level, butfails to form a full logical form of the referringexpression.
Partial grounding and broad-coveragedata statistics are essential for this problem.Entity attribute extraction In the two test sets,the approach achieves high benign precision lev-els (P*) of 72.6 and 61.4.
However, the apposi-tives data is significantly more challenging, and themodel misses many of the true facts that could beextracted.
Many errors comes in the early stages ofthe pipeline, which can be attributed at least in partto both (1) the higher levels of noise in the inputdata (see Section 3), and (2) the fact that the CCGparser was developed on the Wikipedia category la-bels.
While the IE baseline performs reasonably onthe Wikipedia test data, its performance degradessignificantly on appositions.
As it is trained to pre-dict pre-determined relations, it does not generalizeto different domains.For the development set, Table 5 also shows theprecision-recall trade off for the set of Freebaseattributes that appear in the top-n predicted logicalforms.
Precision drops quickly but recall can beimproved significantly, showing that the model canproduce many of the labeled facts.Qualitative evaluation We sampled 100 errorsfrom the Wikipedia test set for qualitative analy-sis.
10% came from entity linking.
About 30%come from choosing a superset or subset of thedesired meaning, for example by mapping ?novel?to book.
About 10% of the errors are from do-main ambiguity, such as mapping ?stage actor?
tofilm.film actor.
10% of the cases are from spu-rious string similarity, such as mapping ?Hungarianexpatriates?
to nationality(x, Hungary).
15%of the failures were due to incorrect underspecifiedlogical forms and, finally, about 10% of the errorswere because the typing features encouraged com-pound nouns to be split into separate attributes.
Onthe apposition dataset, 65% of errors stems fromparsing, either in apposition detection or CCG pars-ing.
Better modeling the complex attachment deci-sions for the noun phrases in the apposition datasetremains an area for future work.One advantage of our approach, especially incomparison to classifier based models like the IEbaseline, is the ability to predict previously unseenrelations.
Counting only the correctly predicted1318triples, we see that over 40% of the unique rela-tions we predict is not in the development set; ourmodel learns to generalize based on the learnedPMI features and other lexical cues.Finally, our approach extracted 2.0 entity at-tributes per Wikipedia phrase and 0.9 per appo-sition on average.
This matches our intuition thatthe apposition dataset contains many more wordsthat cannot be modeled with concepts in Freebase.9 Related WorkRecent work has begun to study the problem ofknowledge base incompleteness and reasoning withopen concepts.
Joshi et al (2014) describes anapproach for mapping short search queries to asingle Freebase relation, that benefits from model-ing schema incompleteness.
Additionally, Krish-namurthy et al (2012; 2014) present a semanticparser that builds partial meaning representationswith Freebase for information extraction applica-tions.
This is similar in spirit to the approach wepresent here, however they focus on a small, fixed,set of binary relations while we aim to represent asmuch of the text as possible using the entire Free-base ontology.
Krishnamurthy and Mitchell (2015)have also studied semantic parsing with open con-cepts via matrix factorization.
They use Freebaseentities but do not include Freebase concepts.The problem of building complete sentence anal-yses using all of the Freebase ontology has re-cently received attention within the context of ques-tion answering systems (Cai and Yates, 2013;Kwiatkowski et al, 2013; Berant et al, 2013; Be-rant and Liang, 2014; Reddy et al, 2014).
Sincethey do not model KB incompleteness, these mod-els will not work well on data that cannot be fullymodeled by Freebase.
In section 7, we report re-sults using one of these systems to provide a refer-ence point for our approach.
There has also beenother work on Freebase question answering (Yaoand Van Durme, 2014; Bordes et al, 2014; Wanget al, 2014) that directly searches the facts in theKB to find answers without explicitly modelingcompositional semantic structure.
Therefore, thesemethods will suffer when facts are missing.The syntactic and semantic structure of nounphrases has been extensively studied.
For example,work on NomBank (Meyers et al, 2004; Gerberand Chai, 2010) focus on the challenge of modelingimplicit arguments introduced by nominal predi-cates.
In a manual study, we discovered that the65% of our noun phrases contain implicit relations.We build on insights from Vadas and Curran (2008),who studied how to model the syntactic structureof noun phrases in CCGBank.
While we are, to thebest of our knowledge, the first to study compoundnoun phrases for semantic parsing to knowledge-bases, semantic parsers for noun phrase referringexpressions have been built for visual referring ex-pression (FitzGerald et al, 2013).There has been little work on IE from compoundnoun phrases.
Most existing IE algorithms extracta single relation, usually represented as a verb thatholds between a pair of named entities, for exam-ple with supervised learning techniques (Freitag,1998) or via distant supervision (Mintz et al, 2009;Riedel et al, 2013; Hoffmann et al, 2011).
We aimto go beyond relations between entity pairs, and toretrieve full semantics of noun phrases, extractingunary and binary relations for a single entity.
Anotable exception to this trend is the ReNoun sys-tem (Yahya et al, 2014) which models noun phrasestructure for open information extraction.
Theyreport that 97% of the attributes in Freebase arecommonly expressed as noun phrases.
However,unlike our work, they considered open informationextraction and did not ground the extractions in anexternal KB.10 ConclusionIn this paper, we present a semantic parsing ap-proach with knowledge base incompleteness, ap-plied to the problem of information extraction fromnoun phrases.
When run on all of the Wikipediacategory data, the approach would extract up to 12million new Freebase facts at 72% precision.There is significant potential for improving theparsing models, as well as better optimizing theprecision recall trade-off for the extracted facts.
Itwould also be interesting to gather data with com-positional phenomena, such as negation and dis-junction, and study its impact on the performanceof the semantic parser.AcknowledgmentsThe research was supported in part by DARPA, un-der the DEFT program through the AFRL (FA8750-13-2-0019) and the CSSG (N11AP20020), as wellas the NSF (IIS-1115966, IIS-1252835) and a giftfrom Google, Inc.
The authors thank Xiao Lingfor providing the apposition data, and Yoav Artzi,Nicholas FitzGerald, Mike Lewis, Sameer Singhand Mark Yatskar for discussion and comments.1319ReferencesJonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of the Empir-ical Methods in Natural Language Processing.Antoine Bordes, Jason Weston, and Sumit Chopra.2014.
Question answering with subgraph embed-dings.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, Doha, Qatar.
Association for ComputationalLinguistics.Qingqing Cai and Alexandar Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics.Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-moyer.
2013.
Learning distributions over logicalforms for referring expression generation.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing.Dayne Freitag.
1998.
Toward general-purpose learn-ing for information extraction.
In Proceedings of the36th Annual Meeting of the Association for Compu-tational Linguistics.Matthew Gerber and Joyce Y Chai.
2010.
Beyondnombank: a study of implicit arguments for nomi-nal predicates.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics.Association for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Dan Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of the Con-ference of the Association of Computational Linguis-tics.Mandar Joshi, Uma Sawat, and Soumen Chakrabarti.2014.
Knowledge graph and corpus driven segmen-tation and answer inference for telegraphic entity-seeking queries.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.
Association for Computational Linguistics.Jayant Krishnamurthy and Tom M Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning.
Associationfor Computational Linguistics.Jayant Krishnamurthy and Tom M. Mitchell.
2014.Joint syntactic and semantic parsing with combi-natory categorial grammar.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Jayant Krishnamurthy and Tom Mitchell.
2015.
Learn-ing a compositional semantics for freebase with anopne predicate vocabulary.
Transactions of the As-sociation for Computational Linguistics.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
Annotating noun ar-gument structure for nombank.
In LREC, volume 4.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of theJoint Conference of the Annual Meeting of the Asso-ciation for Computational Linguistics.
Associationfor Computational Linguistics.Siva Reddy, Mirella Lapata, and Mark Steedman.
2014.Large-scale semantic parsing without question-answer pairs.
Transactions of the Association forComputational Linguistics, 2.Sebastian Riedel, Limin Yao, Benjamin M. Marlin, andAndrew McCallum.
2013.
Relation extraction withmatrix factorization and universal schemas.
In JointHuman Language Technology Conference/AnnualMeeting of the North American Chapter of the As-sociation for Computational Linguistics.Mark Steedman.
1996.
Surface Structure and Interpre-tation.
The MIT Press.David Vadas and James R. Curran.
2008.
Parsing nounphrase structure with ccg.
In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics.Zhenghao Wang, Shengquan Yan, Huaming Wang,and Xuedong Huang.
2014.
An overview of mi-crosoft deep qa system on stanford webquestionsbenchmark.
Technical Report MSR-TR-2014-121,September.Robert West, Evgeniy Gabrilovich, Kevin Murphy,Shaohua Sun, Rahul Gupta, and Dekang Lin.
2014.Knowledge base completion via search-based ques-tion answering.
In Proceedings of World Wide WebConference.Mohamed Yahya, Steven Euijong Whang, RahulGupta, and Alon Halevy.
2014.
Renoun: Fact ex-traction for nominal attributes.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.1320
