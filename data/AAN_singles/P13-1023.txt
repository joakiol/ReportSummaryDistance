Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228?238,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUniversal Conceptual Cognitive Annotation (UCCA)Omri Abend?Institute of Computer ScienceThe Hebrew Universityomria01@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceThe Hebrew Universityarir@cs.huji.ac.ilAbstractSyntactic structures, by their nature, re-flect first and foremost the formal con-structions used for expressing meanings.This renders them sensitive to formal vari-ation both within and across languages,and limits their value to semantic ap-plications.
We present UCCA, a novelmulti-layered framework for semantic rep-resentation that aims to accommodate thesemantic distinctions expressed throughlinguistic utterances.
We demonstrateUCCA?s portability across domains andlanguages, and its relative insensitivityto meaning-preserving syntactic variation.We also show that UCCA can be ef-fectively and quickly learned by annota-tors with no linguistic background, anddescribe the compilation of a UCCA-annotated corpus.1 IntroductionSyntactic structures are mainly committed to rep-resenting the formal patterns of a language, andonly indirectly reflect semantic distinctions.
Forinstance, while virtually all syntactic annotationschemes are sensitive to the structural differencebetween (a) ?John took a shower?
and (b) ?Johnshowered?, they seldom distinguish between (a)and the markedly different (c) ?John took mybook?.
In fact, the annotations of (a) and (c) areidentical under the most widely-used schemes forEnglish, the Penn Treebank (PTB) (Marcus et al,1993) and CoNLL-style dependencies (Surdeanuet al, 2008) (see Figure 1).?
Omri Abend is grateful to the Azrieli Foundation forthe award of an Azrieli Fellowship.Underscoring the semantic similarity between(a) and (b) can assist semantic applications.
Oneexample is machine translation to target languagesthat do not express this structural distinction (e.g.,both (a) and (b) would be translated to the sameGerman sentence ?John duschte?).
Question An-swering applications can also benefit from dis-tinguishing between (a) and (c), as this knowl-edge would help them recognize ?my book?
as amuch more plausible answer than ?a shower?
tothe question ?what did John take?
?.This paper presents a novel approach to gram-matical representation that annotates semantic dis-tinctions and aims to abstract away from specificsyntactic constructions.
We call our approach Uni-versal Conceptual Cognitive Annotation (UCCA).The word ?cognitive?
refers to the type of cate-gories UCCA uses and its theoretical underpin-nings, and ?conceptual?
stands in contrast to ?syn-tactic?.
The word ?universal?
refers to UCCA?scapability to accommodate a highly rich set of se-mantic distinctions, and its aim to ultimately pro-vide all the necessary semantic information forlearning grammar.
In order to accommodate thisrich set of distinctions, UCCA is built as a multi-layered structure, which allows for its open-endedextension.
This paper focuses on the foundationallayer of UCCA, a coarse-grained layer that rep-resents some of the most important relations ex-pressed through linguistic utterances, including ar-gument structure of verbs, nouns and adjectives,and the inter-relations between them (Section 2).UCCA is supported by extensive typologi-cal cross-linguistic evidence and accords withthe leading Cognitive Linguistics theories.
Webuild primarily on Basic Linguistic Theory (BLT)(Dixon, 2005; 2010a; 2010b; 2012), a typologicalapproach to grammar successfully used for the de-228scription of a wide variety of languages.
BLT usessemantic similarity as its main criterion for cate-gorizing constructions both within and across lan-guages.
UCCA takes a similar approach, therebycreating a set of distinctions that is motivatedcross-linguistically.
We demonstrate UCCA?s rel-ative insensitivity to paraphrasing and to cross-linguistic variation in Section 4.UCCA is exceptional in (1) being a semanticscheme that abstracts away from specific syntacticforms and is not defined relative to a specific do-main or language, (2) providing a coarse-grainedrepresentation which allows for open-ended ex-tension, and (3) using cognitively-motivated cat-egories.
An extensive comparison of UCCA to ex-isting approaches to syntactic and semantic repre-sentation, focusing on the major resources avail-able for English, is found in Section 5.This paper also describes the compilation of aUCCA-annotated corpus.
We provide a quanti-tative assessment of the annotation quality.
Ourresults show a quick learning curve and no sub-stantial difference in the performance of annota-tors with and without background in linguistics.This is an advantage of UCCA over its syntacticcounterparts that usually need annotators with ex-tensive background in linguistics (see Section 3).We note that UCCA?s approach that advocatesautomatic learning of syntax from semantic super-vision stands in contrast to the traditional view ofgenerative grammar (Clark and Lappin, 2010).2 The UCCA Scheme2.1 The FormalismUCCA uses directed acyclic graphs (DAGs) torepresent its semantic structures.
The atomicmeaning-bearing units are placed at the leaves ofthe DAG and are called terminals.
In the founda-tional layer, terminals are words and multi-wordchunks, although this definition can be extendedto include arbitrary morphemes.The nodes of the graph are called units.
A unitmay be either (i) a terminal or (ii) several ele-ments jointly viewed as a single entity accordingto some semantic or cognitive consideration.
Inmany cases, a non-terminal unit is comprised of asingle relation and the units it applies to (its argu-ments), although in some cases it may also containsecondary relations.
Hierarchy is formed by usingunits as arguments or relations in other units.Categories are annotated over the graph?s edges,and represent the descendant unit?s role in formingthe semantics of the parent unit.
Therefore, the in-ternal structure of a unit is represented by its out-bound edges and their categories, while the rolesa unit plays in the relations it participates in arerepresented by its inbound edges.We note that UCCA?s structures reflect a singleinterpretation of the text.
Several discretely dif-ferent interpretations (e.g., high vs. low PP at-tachments) may therefore yield several differentUCCA annotations.UCCA is a multi-layered formalism, whereeach layer specifies the relations it encodes.
Thequestion of which relations will be annotated(equivalently, which units will be formed) is de-termined by the layer in question.
For example,consider ?John kicked his ball?, and assume ourcurrent layer encodes the relations expressed by?kicked?
and by ?his?.
In that case, the unit ?his?has a single argument1 (?ball?
), while ?kicked?has two (?John?
and ?his ball?).
Therefore, theunits of the sentence are the terminals (which arealways units), ?his ball?
and ?John kicked hisball?.
The latter two are units by virtue of express-ing a relation along with its arguments.
See Fig-ure 2(a) for a graph representation of this example.For a brief comparison of the UCCA formalismwith other dependency annotations see Section 5.2.2 The UCCA Foundational LayerThe foundational layer is designed to cover theentire text, so that each word participates in atleast one unit.
It focuses on argument structuresof verbal, nominal and adjectival predicates andthe inter-relations between them.
Argument struc-ture phenomena are considered basic by many ap-proaches to semantic and grammatical representa-tion, and have a high applicative value, as demon-strated by their extensive use in NLP.The foundational layer views the text as a col-lection of Scenes.
A Scene can describe somemovement or action, or a temporally persistentstate.
It generally has a temporal and a spatial di-mension, which can be specific to a particular timeand place, but can also describe a schematizedevent which refers to many events by highlight-ing a common meaning component.
For example,the Scene ?John loves bananas?
is a schematizedevent, which refers to John?s disposition towardsbananas without making any temporal or spatial1The anaphoric aspects of ?his?
are not considered part ofthe current layer (see Section 2.3).229John took a shower -ROOT-ROOTSBJOBJNMOD(a)John showered -ROOT-ROOTSBJ(b)John took my book -ROOT-ROOTSBJOBJNMOD(c)Figure 1: CoNLL-style dependency annotations.
Note that (a) and (c), which have different semantics but superficially similarsyntax, have the same annotation.Abb.
Category Short DefinitionScene ElementsP Process The main relation of a Scene that evolves in time (usually an action or movement).S State The main relation of a Scene that does not evolve in time.A Participant A participant in a Scene in a broad sense (including locations, abstract entities and Scenes servingas arguments).D Adverbial A secondary relation in a Scene (including temporal relations).Elements of Non-Scene UnitsC Center Necessary for the conceptualization of the parent unit.E Elaborator A non-Scene relation which applies to a single Center.N Connector A non-Scene relation which applies to two or more Centers, highlighting a common feature.R Relator All other types of non-Scene relations.
Two varieties: (1) Rs that relate a C to some super-ordinaterelation, and (2) Rs that relate two Cs pertaining to different aspects of the parent unit.Inter-Scene RelationsH ParallelSceneA Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive).L Linker A relation between two or more Hs (e.g., ?when?, ?if?, ?in order to?
).G Ground A relation between the speech event and the uttered Scene (e.g., ?surprisingly?, ?in my opinion?
).OtherF Function Does not introduce a relation or participant.
Required by the structural pattern it appears in.Table 1: The complete set of categories in UCCA?s foundational layer.specifications.
The definition of a Scene is moti-vated cross-linguistically and is similar to the se-mantic aspect of the definition of a ?clause?
in Ba-sic Linguistic Theory2.Table 1 provides a concise description of thecategories used by the foundational layer3.
Weturn to a brief description of them.Simple Scenes.
Every Scene contains one mainrelation, which is the anchor of the Scene, the mostimportant relation it describes (similar to frame-evoking lexical units in FrameNet (Baker et al,1998)).
We distinguish between static Scenes, thatdescribe a temporally persistent state, and proces-sual Scenes that describe a temporally evolvingevent, usually a movement or an action.
The mainrelation receives the category State (S) in static andProcess (P) in processual Scenes.
We note thatthe S-P distinction is introduced here mostly forpractical purposes, and that both categories can beviewed as sub-categories of the more abstract cat-egory Main Relation.A Scene contains one or more Participants (A).2As UCCA annotates categories on its edges, Scene nodesbear no special indication.
They can be identified by examin-ing the labels on their outgoing edges (see below).3Repeated here with minor changes from (Abend andRappoport, 2013), which focuses on the categories them-selves.This category subsumes concrete and abstract par-ticipants as well as embedded Scenes (see be-low).
Scenes may also contain secondary rela-tions, which are marked as Adverbials (D).The above categories are indifferent to the syn-tactic category of the Scene-evoking unit, be it averb, a noun, an adjective or a preposition.
For in-stance, in the Scene ?The book is in the garden?,?is in?
is the S, while ?the book?
and ?the garden?are As.
In ?Tomatoes are red?, the main static re-lation is ?are red?, while ?Tomatoes?
is an A.The foundational layer designates a separate setof categories to units that do not evoke a Scene.Centers (C) are the sub-units of a non-Scene unitthat are necessary for the unit to be conceptualizedand determine its semantic type.
There can be oneor more Cs in a non-Scene unit4.Other sub-units of non-Scene units are catego-rized into three types.
First, units that apply to asingle C are annotated as Elaborators (E).
For in-stance, ?big?
in ?big dogs?
is an E, while ?dogs?
isa C. We also mark determiners as Es in this coarse-grained layer5.
Second, relations that relate two or4By allowing several Cs we avoid the difficulties incurredby the common single head assumption.
In some cases theCs are inferred from context and can be implicit.5Several Es that apply to a single C are often placed in230more Cs, highlighting a common feature or role(usually coordination), are called Connectors (N).See an example in Figure 2(b).Relators (R) cover all other types of relationsbetween two or more Cs.
Rs appear in two mainvarieties.
In one, Rs relate a single entity to asuper-ordinate relation.
For instance, in ?I heardnoise in the kitchen?, ?in?
relates ?the kitchen?to the Scene it is situated in.
In the other, Rs re-late two units pertaining to different aspects of thesame entity.
For instance, in ?bottom of the sea?,?of?
relates ?bottom?
and ?the sea?, two units thatrefer to different aspects of the same entity.Some units do not introduce a new relation orentity into the Scene, and are only part of the for-mal pattern in which they are situated.
Such unitsare marked as Functions (F).
For example, in thesentence ?it is customary for John to come late?,the ?it?
does not refer to any specific entity or re-lation and is therefore an F.Two example annotations of simple Scenes aregiven in Figure 2(a) and Figure 2(b).More complex cases.
UCCA allows units toparticipate in more than one relation.
This is a nat-ural requirement given the wealth of distinctionsUCCA is designed to accommodate.
Already inthe foundational layer of UCCA, the need arisesfor multiple parents.
For instance, in ?John askedMary to join him?, ?Mary?
is a Participant of boththe ?asking?
and the ?joining?
Scenes.In some cases, an entity or relation is prominentin the interpretation of the Scene, but is not men-tioned explicitly anywhere in the text.
We marksuch entities as Implicit Units.
Implicit units areidentical to terminals, except that they do not cor-respond to a stretch of text.
For example, ?playinggames is fun?
has an implicit A which correspondsto the people playing the game.UCCA annotates inter-Scene relations (linkage)and, following Basic Linguistic Theory, distin-guishes between three major types of linkage.First, a Scene can be an A in another Scene.
Forinstance, in ?John said he must leave?, ?he mustleave?
is an A inside the Scene evoked by ?said?.Second, a Scene may be an E of an entity in an-other Scene.
For instance, in ?the film we saw yes-terday was wonderful?, ?film we saw yesterday?
isa Scene that serves as an E of ?film?, which is bothan A in the Scene and the Center of an A in thea flat structure.
In general, the coarse-grained foundationallayer does not try to resolve fine scope issues.JohnAkickedPhisEballCA(a)JohnCandNMaryCAboughtPaEsofaCAtogetherD(b)the filmAweAsawPyesterdayDEAwasFwonderfulCSE C(c)Figure 2: Examples of UCCA annotation graphs.Scene evoked by ?wonderful?
(see Figure 2(c)).A third type of linkage covers all other cases,e.g., temporal, causal and conditional inter-Scenerelations.
The linked Scenes in such cases aremarked as Parallel Scenes (H).
The units speci-fying the relation between Hs are marked as Link-ers (L)6.
As with other relations in UCCA, Linkersand the Scenes they link are bound by a unit.Unlike common practice in grammatical anno-tation, linkage relations in UCCA can cross sen-tence boundaries, as can relations represented inother layers (e.g., coreference).
UCCA thereforeannotates texts comprised of several paragraphsand not individual sentences (see Section 3).Example sentences.
Following are completeannotations of two abbreviated example sentencesfrom our corpus (see Section 3).
?Golf became a passion for his oldest daughter:she took daily lessons and became very good,reaching the Connecticut Golf Championship.
?This sentence contains four Scenes, evoked by?became a passion?, ?took daily lessons?, ?be-came very good?
and ?reaching?.
The individualScenes are annotated as follows:1.
?GolfA [becameE aE passionC]P [forR hisEoldestE daughterC]A?6It is equally plausible to include Linkers for the other twolinkage types.
This is not included in the current layer.2312.
?sheA [tookF [dailyE lessonsC]C]P ?3.
?sheA ... [becameE [veryE goodC]C]S?4.
?sheA ... reachingP [theE ConnecticutEGolfE ChampionshipC ]A?There is only one explicit Linker in this sen-tence (?and?
), which links Scenes (2) and (3).None of the Scenes is an A or an E in the other, andthey are therefore all marked as Parallel Scenes.We also note that in the case of the light verbconstruction ?took lessons?
and the copula clauses?became good?
and ?became a passion?, the verbis not the Center of the main relation, but ratherthe following noun or adjective.
We also note thatthe unit ?she?
is an A in Scenes (2), (3) and (4).We turn to our second example:?Cukor encouraged the studio toaccept her demands.
?This sentence contains three Scenes, evoked by?encouraged?, ?accept?
and ?demands?:1.
CukorA encouragedP [theE studioC]A [toR[accept her demands]C ]A2.
[the studio]A ... acceptP [her demands]A3.
herA demandsP IMPAScenes (2) and (3) act as Participants in Scenes(1) and (2) respectively.
In Scene (2), there isan implicit Participant which corresponds to what-ever was demanded.
Note that ?her demands?
is aScene, despite being a noun phrase.2.3 UCCA?s Multi-layered StructureAdditional layers may refine existing relations orotherwise annotate a complementary set of dis-tinctions.
For instance, a refinement layer cancategorize linkage relations according to their se-mantic types (e.g., temporal, purposive, causal) orprovide tense distinctions for verbs.
Another im-mediate extension to UCCA?s foundational layercan be the annotation of coreference relations.
Re-call the example ?John kicked his ball?.
A coref-erence layer would annotate a relation between?John?
and ?his?
by introducing a new node whosedescendants are these two units.
The fact thatthis node represents a coreference relation wouldbe represented by a label on the edge connectingthem to the coreference node.There are three common ways to extend an an-notation graph.
First, by adding a relation that re-lates previously established units.
This is done byintroducing a new node whose descendants are therelated units.
Second, by adding an intermediatePassage #1 2 3 4 5 6# Sents.
8 20 23 14 13 15# Tokens 259 360 343 322 316 393ITA 67.3 74.1 71.2 73.5 77.8 81.1Vs.
Gold 72.4 76.7 75.5 75.7 79.5 84.2Correction 93.7Table 2: The upper part of the table presents the number ofsentences and the number of tokens in the first passages usedfor the annotator training.
The middle part presents the av-erage F-scores obtained by the annotators throughout thesepassages.
The first row presents the average F-score whencomparing the annotations of the different annotators amongthemselves.
The second row presents the average F-scorewhen comparing them to a ?gold standard?.
The bottom rowshows the average F-score between an annotated passage ofa trained annotator and its manual correction by an expert.
Itis higher due to conforming analyses (see text).
All F-scoresare in percents.unit between a parent unit and some of its sub-units.
For instance, consider ?he replied foolishly?and ?he foolishly replied?.
A layer focusing onAdverbial scope may refine the flat Scene structureassigned by the foundational layer, expressing thescope of ?foolishly?
over the relation ?replied?
inthe first case, and over the entire Scene in the sec-ond.
Third, by adding sub-units to a terminal.
Forinstance, consider ?gave up?, an expression whichthe foundational layer considers atomic.
A layerthat annotates tense can break the expression into?gave?
and ?up?, in order to annotate ?gave?
as thetense-bearing unit.Although a more complete discussion of the for-malism is beyond the scope of this paper, we notethat the formalism is designed to allow differentannotation layers to be defined and annotated in-dependently of one another, in order to facilitateUCCA?s construction through a community effort.3 A UCCA-Annotated CorpusThe annotated text is mostly based on EnglishWikipedia articles for celebrities.
We have chosenthis genre as it is an inclusive and diverse domain,which is still accessible to annotators from variedbackgrounds.For the annotation process, we designed and im-plemented a web application tailored for UCCA?sannotation.
A sample of the corpus containingroughly 5K tokens, as well as the annotation ap-plication can be found in our website7.UCCA?s annotations are not confined to a sin-gle sentence.
The annotation is therefore carriedout in passages of 300-400 tokens.
After its an-7www.cs.huji.ac.il/?omria01232notation, a passage is manually corrected beforebeing inserted into the repository.The section of the corpus annotated thus farcontains 56890 tokens in 148 annotated passages(average length of 385 tokens).
Each passage con-tains 450 units on average and 42.2 Scenes.
EachScene contains an average of 2 Participants and 0.3Adverbials.
15% of the Scenes are static (containan S as the main relation) and the rest are dynamic(containing a P).
The average number of tokens ina Scene (excluding punctuation) is 10.7.
18.3%of the Scenes are Participants in another Scene,11.4% are Elaborator Scenes and the remainingare Parallel Scenes.
A passage contains an aver-age of 11.2 Linkers.Inter-annotator agreement.
We employ 4 an-notators with varying levels of background in lin-guistics.
Two of the annotators have no back-ground in linguistics, one took an introductorycourse and one holds a Bachelor?s degree in lin-guistics.
The training process of the annotatorslasted 30?40 hours, which includes the time re-quired for them to get acquainted with the webapplication.
As this was the first large-scale trialwith the UCCA scheme, some modifications to thescheme were made during the annotator?s training.We therefore expect the training process to be evenfaster in later distributions.There is no standard evaluation measure forcomparing two grammatical annotations in theform of labeled DAGs.
We therefore convertedUCCA to constituency trees8 and, following stan-dard practice, computed the number of brackets inboth trees that match in both span and label.
Wederive an F-score from these counts.Table 2 presents the inter-annotator agreementin the training phase.
The four annotators weregiven the same passage in each of these cases.
Inaddition, a ?gold standard?
was annotated by theauthors of this paper.
The table presents the av-erage F-score between the annotators, as well asthe average F-score when comparing to the goldstandard.
Results show that although it repre-sents complex hierarchical structures, the UCCAscheme is learned quickly and effectively.We also examined the influence of prior linguis-tic background on the results.
In the first passagethere was a substantial advantage to the annotators8In cases a unit had multiple parents, we discarded all butone of its incoming edges.
This resulted in discarding 1.9%of the edges.
We applied a simple normalization procedure tothe resulting trees.who had prior training in linguistics.
The obtainedF-scores when comparing to a gold standard, or-dered decreasingly according to the annotator?sacquaintance with linguistics, were 78%, 74.4%,69.5% and 67.8%.
However, this performance gapquickly vanished.
Indeed, the obtained F-scores,again compared to a gold standard and averagedover the next five training passages, were (by thesame order) 78.6%, 77.3%, 79.2% and 78%.This is an advantage of UCCA over other syn-tactic annotation schemes that normally requirehighly proficient annotators.
For instance, boththe PTB and the Prague Dependency Treebank(Bo?hmova?
et al, 2003) employed annotators withextensive linguistic background.
Similar findingsto ours were reported in the PropBank project,which successfully employed annotators with var-ious levels of linguistic background.
We viewthis as a major advantage of semantic annotationschemes over their syntactic counterparts, espe-cially given the huge amount of manual labor re-quired for large syntactic annotation projects.The UCCA interface allows for multiple non-contradictory (?conforming?)
analyses of a stretchof text.
It assumes that in some cases there ismore than one acceptable option, each highlight-ing a different aspect of meaning of the analyzedutterance (see below).
This makes the computa-tion of inter-annotator agreement fairly difficult.It also suggests that the above evaluation is exces-sively strict, as it does not take into account suchconforming analyses.
To address this issue, weconducted another experiment where an expert an-notator corrected the produced annotations.
Com-paring the corrected versions to the originals, wefound that F-scores are typically in the range of90%?95%.
An average taken over a sample ofpassages annotated by all four annotators yieldedan F-score of 93.7%.It is difficult to compare the above results to theinter-annotator agreement of other projects for tworeasons.
First, many existing schemes are basedon other annotation schemes or heavily rely onautomatic tools for providing partial annotations.Second, some of the most prominent annotationprojects do not provide reliable inter-annotatoragreement scores (Artstein and Poesio, 2008).A recent work that did report inter-annotatoragreement in terms of bracketing F-score is an an-notation project of the PTB?s noun phrases withmore elaborate syntactic structure (Vadas and Cur-233ran, 2011).
They report an agreement of 88.3% ina scenario where their two annotators worked sep-arately.
Note that this task is much more limitedin scope than UCCA (annotates noun phrases in-stead of complete passages in UCCA; uses 2 cat-egories instead of 12 in UCCA).
Nevertheless, theobtained inter-annotator agreement is comparable.Disagreement examples.
Here we discuss twomajor types of disagreements that recurred in thetraining process.
The first is the distinction be-tween Elaborators and Centers.
In most cases thisdistinction is straightforward, particularly whereone sub-unit determines the semantic type of theparent unit, while its siblings add more informa-tion to it (e.g., ?truckE companyC?
is a type of acompany and not of a truck).
Some structures donot nicely fall into this pattern.
One such case iswith apposition.
In the example ?the Fox dramaGlory days?, both ?the Fox drama?
and ?Glorydays?
are reasonable candidates for being a Cen-ter, which results in disagreements.Another case is the distinction between Scenesand non-Scene relations.
Consider the example?
[John?s portrayal of the character] has been de-scribed as ...?.
The sentence obviously containstwo scenes, one in which John portrays a charac-ter and another where someone describes John?sdoings.
Its internal structure is therefore ?John?sAportrayalP [of the character]A?.
However, thesyntactic structure of this unit leads annotators attimes into analyzing the subject as a non-Scene re-lation whose C is ?portrayal?.Static relations tend to be more ambiguous be-tween a Scene and a non-Scene interpretation.Consider ?Jane Smith (ne?e Ross)?.
It is not at allclear whether ?ne?e Ross?
should be annotated as aScene or not.
Even if we do assume it is a Scene,it is not clear whether the Scene it evokes is herScene of birth, which is dynamic, or a static Scenewhich can be paraphrased as ?originally namedRoss?.
This leads to several conforming analyses,each expressing a somewhat different conceptual-ization of the Scene.
This central notion will bemore elaborately addressed in future work.We note that all of these disagreements can beeasily resolved by introducing an additional layerfocusing on the construction in question.4 UCCA?s Benefits to Semantic TasksUCCA?s relative insensitivity to syntactic formshas potential benefits for a wide variety of seman-tic tasks.
This section briefly demonstrates thesebenefits through a number of examples.Recall the example ?John took a shower?
(Sec-tion 1).
UCCA annotates the sentence as a sin-gle Scene, with a single Participant and a proces-sual main relation: ?JohnA [tookF [aE showerC]C]P ?.
The paraphrase ?John showered?
is anno-tated similarly: ?JohnA showeredP ?.
The struc-ture is also preserved under translation to otherlanguages, such as German (?JohnA duschteP ?,where ?duschte?
is a verb), or Portuguese ?JohnA[tomouF banhoC]P ?
(literally, John took shower).In all of these cases, UCCA annotates the exampleas a Scene with an A and a P, whose Center is aword expressing the notion of showering.Another example is the sentence ?John doesnot have any money?.
The foundational layerof UCCA annotates negation units as Ds, whichyields the annotation ?JohnA [doesF ]S- notD[haveC]-S [anyE moneyC]A?
(where ?does ...have?
is a discontiguous unit)9.
This sentence canbe paraphrased as ?JohnA hasP noD moneyA?.UCCA reflects the similarity of these two sen-tences, as it annotates both cases as a single Scenewhich has two Participants and a negation.
A syn-tactic scheme would normally annotate ?no?
in thesecond sentence as a modifier of ?money?, and?not?
as a negation of ?have?.The value of UCCA?s annotation can again beseen in translation to languages that have only oneof these forms.
For instance, the German transla-tion of this sentence, ?JohnA hatS keinD GeldA?,is a literal translation of ?John has no money?.
TheHebrew translation of this sentence is ?eyn le johnkesef?
(literally, ?there-is-no to John money?
).The main relation here is therefore ?eyn?
(there-is-no) which will be annotated as S. This yieldsthe annotation ?eynS [leR JohnC]A kesefA?.The UCCA annotation in all of these cases iscomposed of two Participants and a State.
In En-glish and German, the negative polarity unit is rep-resented as a D. The negative polarity of the He-brew ?eyn?
is represented in a more detailed layer.As a third example, consider the two sentences?There are children playing in the park?
and ?Chil-dren are playing in the park?.
The two sentenceshave a similar meaning but substantially differentsyntactic structures.
The first contains two clauses,an existential main clause (headed by ?there are?
)9The foundational layer places ?not?
in the Scene level toavoid resolving fine scope issues (see Section 2)234and a subordinate clause (?playing in the park?
).The second contains a simple clause headed by?playing?.
While the parse trees of these sentencesare very different, their UCCA annotation in thefoundational layer differ only in terms of Functionunits: ?ChildrenA [areF playingC]P [inR theEparkC]A?
and ?ThereF areF childrenA [playing]P[inR theE parkC]A?10.Aside from machine translation, a great vari-ety of semantic tasks can benefit from a schemethat is relatively insensitive to syntactic variation.Examples include text simplification (e.g., for sec-ond language teaching) (Siddharthan, 2006), para-phrase detection (Dolan et al, 2004), summariza-tion (Knight and Marcu, 2000), and question an-swering (Wang et al, 2007).5 Related WorkIn this section we compare UCCA to some of themajor approaches to grammatical representation inNLP.
We focus on English, which is the most stud-ied language and the focus of this paper.Syntactic annotation schemes come in manyforms, from lexical categories such as POS tagsto intricate hierarchical structures.
Some for-malisms focus particularly on syntactic distinc-tions, while others model the syntax-semantics in-terface as well (Kaplan and Bresnan, 1981; Pollardand Sag, 1994; Joshi and Schabes, 1997; Steed-man, 2001; Sag, 2010, inter alia).
UCCA divergesfrom these approaches in aiming to abstract awayfrom specific syntactic forms and to only representsemantic distinctions.
Put differently, UCCA ad-vocates an approach that treats syntax as a hiddenlayer when learning the mapping between formand meaning, while existing syntactic approachesaim to model it manually and explicitly.UCCA does not build on any other annotationlayers and therefore implicitly assumes that se-mantic annotation can be learned directly.
Recentwork suggests that indeed structured predictionmethods have reached sufficient maturity to allowdirect learning of semantic distinctions.
Examplesinclude Naradowsky et al (2012) for semantic rolelabeling and Kwiatkowski et al (2010) for seman-tic parsing to logical forms.
While structured pre-diction for the task of predicting tree structuresis already well established (e.g., (Suzuki et al,10The two sentences are somewhat different in terms oftheir information structure (Van Valin Jr., 2005), which is rep-resented in a more detailed UCCA layer.2009)), recent work has also successfully tackledthe task of predicting semantic structures in theform of DAGs (Jones et al, 2012).The most prominent annotation scheme in NLPfor English syntax is the Penn Treebank.
Manysyntactic schemes are built or derived from it.
Anincreasingly popular alternative to the PTB aredependency structures, which are usually repre-sented as trees whose nodes are the words of thesentence (Ivanova et al, 2012).
Such represen-tations are limited due to their inability to natu-rally represent constructions that have more thanone head, or in which the identity of the headis not clear.
They also face difficulties in repre-senting units that participate in multiple relations.UCCA proposes a different formalism that ad-dresses these problems by introducing a new nodefor every relation (cf.
(Sangati and Mazza, 2009)).Several annotated corpora offer a joint syntac-tic and semantic representation.
Examples in-clude the Groningen Meaning bank (Basile et al,2012), Treebank Semantics (Butler and Yoshi-moto, 2012) and the Lingo Redwoods treebank(Oepen et al, 2004).
UCCA diverges from theseprojects in aiming to abstract away from syntac-tic variation, and is therefore less coupled with aspecific syntactic theory.A different strand of work addresses the con-struction of an interlingual representation, oftenwith a motivation of applying it to machine trans-lation.
Examples include the UNL project (Uchidaand Zhu, 2001), the IAMTC project (Dorr et al,2010) and the AMR project (Banarescu et al,2012).
These projects share with UCCA theiremphasis on cross-linguistically valid annotations,but diverge from UCCA in three important re-spects.
First, UCCA emphasizes the notion ofa multi-layer structure where the basic layers aremaximally coarse-grained, in contrast to the aboveworks that use far more elaborate representations.Second, from a theoretical point of view, UCCAdiffers from these works in aiming to representconceptual semantics, building on works in Cog-nitive Linguistics (e.g., (Langacker, 2008)).
Third,unlike interlingua that generally define abstractrepresentations that may correspond to several dif-ferent texts, UCCA incorporates the text into itsstructure, thereby facilitating learning.Semantic role labeling (SRL) schemes bearsimilarity to the foundational layer, due to theirfocus on argument structure.
The leading SRL ap-235proaches are PropBank (Palmer et al, 2005) andNomBank (Meyers et al, 2004) on the one hand,and FrameNet (Baker et al, 1998) on the other.
Atthis point, all these schemes provide a more fine-grained set of categories than UCCA.PropBank and NomBank are built on top of thePTB annotation, and provide for each verb (Prop-Bank) and noun (NomBank), a delineation of theirarguments and their categorization into semanticroles.
Their structures therefore follow the syn-tax of English quite closely.
UCCA is generallyless tailored to the syntax of English (e.g., see sec-ondary verbs (Dixon, 2005)).Furthermore, PropBank and NomBank do notannotate the internal structure of their arguments.Indeed, the construction of the commonly used se-mantic dependencies derived from these schemes(Surdeanu et al, 2008) required a set of syntactichead percolation rules to be used.
These rules aresomewhat arbitrary (Schwartz et al, 2011), do notsupport multiple heads, and often reflect syntac-tic rather than semantic considerations (e.g., ?mil-lions?
is the head of ?millions of dollars?, while?dollars?
is the head of ?five million dollars?
).Another difference is that PropBank and Nom-Bank each annotate only a subset of predicates,while UCCA is more inclusive.
This differenceis most apparent in cases where a single complexpredicate contains both nominal and verbal com-ponents (e.g., ?limit access?, ?take a shower?).
Inaddition, neither PropBank nor Nomabnk addresscopula clauses, despite their frequency.
Finally,unlike PropBank and NomBank, UCCA?s founda-tional layer annotates linkage relations.In order to quantify the similarity betweenUCCA and PropBank, we annotated 30 sentencesfrom the PropBank corpus with their UCCA anno-tations and converted the outcome to PropBank-style annotations11.
We obtained an unlabeledF-score of 89.4% when comparing to PropBank,which indicates that PropBank-style annotationsare generally derivable from UCCA?s.
The dis-agreement between the schemes reflects both an-notation conventions and principle differences,some of which were discussed above.The FrameNet project (Baker et al, 1998)11The experiment was conducted on the first 30 sentencesof section 02.
The identity of the predicates was determinedaccording to the PropBank annotation.
We applied a simpleconversion procedure that uses half a dozen rules that are notconditioned on any lexical item.
We used a strict evaluationthat requires an exact match in the argument?s boundaries.proposes a comprehensive approach to semanticroles.
It defines a lexical database of Frames, eachcontaining a set of possible frame elements andtheir semantic roles.
It bears similarity to UCCAboth in its use of Frames, which are a context-independent abstraction of UCCA?s Scenes, andin its emphasis on semantic rather than distribu-tional considerations.
However, despite these sim-ilarities, FrameNet focuses on constructing a lex-ical resource covering specific cases of interest,and does not provide a fully annotated corpus ofnaturally occurring text.
UCCA?s foundationallayer can be seen as a complementary effort toFrameNet, as it focuses on high-coverage, coarse-grained annotation, while FrameNet is more fine-grained at the expense of coverage.6 ConclusionThis paper presented Universal Conceptual Cog-nitive Annotation (UCCA), a novel frameworkfor semantic representation.
We described thefoundational layer of UCCA and the compilationof a UCCA-annotated corpus.
We demonstratedUCCA?s relative insensitivity to paraphrases andcross-linguistic syntactic variation.
We also dis-cussed UCCA?s accessibility to annotators with nobackground in linguistics, which can alleviate thealmost prohibitive annotation costs of large syn-tactic annotation projects.UCCA?s representation is guided by conceptualnotions and has its roots in the Cognitive Linguis-tics tradition and specifically in Cognitive Gram-mar (Langacker, 2008).
These theories representthe meaning of an utterance according to the men-tal representations it evokes and not according toits reference in the world.
Future work will ex-plore options to further reduce manual annotation,possibly by combining texts with visual inputsduring training.We are currently attempting to construct aparser for UCCA and to apply it to several seman-tic tasks, notably English-French machine trans-lation.
Future work will also discuss UCCA?sportability across domains.
We intend to showthat UCCA, which is less sensitive to the idiosyn-crasies of a specific domain, can be easily adaptedto highly dynamic domains such as social media.Acknowledgements.
We would like to thankTomer Eshet for partnering in the development ofthe web application and to Amit Beka for his helpwith UCCA?s software and development set.236ReferencesOmri Abend and Ari Rappoport.
2013.
UCCA: Asemantics-based grammatical annotation scheme.
InIWCS ?13, pages 1?12.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley Framenet project.
In ACL-COLING ?98, pages 86?90.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, andNathan Schneider.
2012.
Abstract mean-ing representation (AMR) 1.0 specification.http://www.isi.edu/natural-language/people/amr-guidelines-10-31-12.pdf.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semanticallyannotated corpus.
In LREC ?12, pages 3196?3200.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and BarboraHladka?.
2003.
The Prague Dependency Treebank.Treebanks, pages 103?127.Alistair Butler and Kei Yoshimoto.
2012.
Bankingmeaning representations from treebanks.
LinguisticIssues in Language Technology, 7(1).Alexander Clark and Shalom Lappin.
2010.
LinguisticNativism and the Poverty of the Stimulus.
Wiley-Blackwell.Robert M. W. Dixon.
2005.
A Semantic Approach ToEnglish Grammar.
Oxford University Press.Robert M. W. Dixon.
2010a.
Basic Linguistic Theory:Methodology, volume 1.
Oxford University Press.Robert M. W. Dixon.
2010b.
Basic Linguistic Theory:Grammatical Topics, volume 2.
Oxford UniversityPress.Robert M. W. Dixon.
2012.
Basic Linguistic The-ory: Further Grammatical Topics, volume 3.
Ox-ford University Press.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InCOLING ?04, pages 350?356.Bonnie Dorr, Rebecca Passonneau, David Farwell, Re-becca Green, Nizar Habash, Stephen Helmreich, Ed-ward Hovy, Lori Levin, Keith Miller, Teruko Mi-tamura, Owen Rambow, and Advaith Siddharthan.2010.
Interlingual annotation of parallel text cor-pora: A new framework for annotation and evalu-ation.
Natural Language Engineering, 16(3):197?243.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, andDan Flickinger.
2012. Who did what to whom?
:A contrastive study of syntacto-semantic dependen-cies.
In LAW ?12, pages 2?11.Bevan Jones, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.Semantics-based machine translation with hyper-edge replacement grammars.
In COLING ?12,pages 1359?1376.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
Handbook Of Formal Lan-guages, 3:69?123.Ronald M. Kaplan and Joan Bresnan.
1981.
Lexical-Functional Grammar: A Formal System For Gram-matical Representation.
Massachusetts Institute OfTechnology, Center For Cognitive Science.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization ?
step one: Sentence compres-sion.
In AAAI ?00, pages 703?710.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In EMNLP ?10, pages 1223?1233.R.W.
Langacker.
2008.
Cognitive Grammar: A BasicIntroduction.
Oxford University Press, USA.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational Linguistics, 19(2):313?330.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
Annotating noun argu-ment structure for Nombank.
In LREC ?04, pages803?806.Jason Naradowsky, Sebastian Riedel, and David Smith.2012.
Improving NLP through marginalization ofhidden syntactic structure.
In EMNLP ?12, pages810?820.Stephan Oepen, Dan Flickinger, Kristina Toutanova,and Christopher D Manning.
2004.
Lingo red-woods.
Research on Language and Computation,2(4):575?596.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):145?159.Carl Pollard and Ivan A.
Sag.
1994.
Head-drivenPhrase Structure Grammar.
University Of ChicagoPress.Ivan A Sag.
2010.
Sign-based construction gram-mar: An informal synopsis.
Sign-based Construc-tion Grammar.
CSLI Publications, Stanford, pages39?170.237Federico Sangati and Chiara Mazza.
2009.
An En-glish dependency treebank a` la Tesnie`re.
In TLT ?09,pages 173?184.Roy Schwartz, Omri Abend, Roi Reichart, and AriRappoport.
2011.
Neutralizing linguistically prob-lematic annotations in unsupervised dependencyparsing evaluation.
In ACL-HLT ?11, pages 663?672.Advaith Siddharthan.
2006.
Syntactic simplificationand text cohesion.
Research on Language & Com-putation, 4(1):77?109.Mark Steedman.
2001.
The Syntactic Process.
MITPress.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In CoNLL ?08,pages 159?177.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An empirical study of semi-supervised structured conditional models for depen-dency parsing.
In EMNLP ?09, pages 551?560.Hiroshi Uchida and Meiying Zhu.
2001.
The uni-versal networking language beyond machine trans-lation.
In International Symposium on Language inCyberspace, pages 26?27.David Vadas and James R Curran.
2011.
Parsing nounphrases in the Penn Treebank.
Computational Lin-guistics, 37(4):753?809.Robert D. Van Valin Jr. 2005.
Exploring The Syntax-semantics Interface.
Cambridge University Press.Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
A quasi-synchronous grammar for QA.
In EMNLP-CoNLL?07, pages 22?32.238
