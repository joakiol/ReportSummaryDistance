Proceedings of the Third Workshop on Statistical Machine Translation, pages 151?154,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImproving Word Alignment with Language Model Based Confidence ScoresNguyen Bach, Qin Gao, Stephan VogelInterACT, Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{nbach, qing, vogel+}@cs.cmu.eduAbstractThis paper describes the statistical machine trans-lation systems submitted to the ACL-WMT 2008shared translation task.
Systems were submitted fortwo translation directions: English?Spanish andSpanish?English.
Using sentence pair confidencescores estimated with source and target languagemodels, improvements are observed on the News-Commentary test sets.
Genre-dependent sentencepair confidence score and integration of sentencepair confidence score into phrase table are also in-vestigated.1 IntroductionWord alignment models are a crucial component in sta-tistical machine translation systems.
When estimatingthe parameters of the word alignment models, the sen-tence pair probability is an important factor in the objec-tive function and is approximated by the empirical prob-ability.
The empirical probability for each sentence pairis estimated by maximum likelihood estimation over thetraining data (Brown et al, 1993).
Due to the limitation oftraining data, most sentence pairs occur only once, whichmakes the empirical probability almost uniform.
This isa rather weak approximation of the true distribution.In this paper, we investigate the methods of weightingsentence pairs using language models, and extended thegeneral weighting method to genre-dependent weight.
Amethod of integrating the weight directly into the phrasetable is also explored.2 The Baseline Phrase-Based MT SystemThe ACL-WMT08 organizers provided Europarl andNews-Commentary parallel corpora for English ?
Span-ish.
Detailed corpus statistics is given in Table 1.
Follow-ing the guidelines of the workshop we built baseline sys-tems, using the lower-cased Europarl parallel corpus (re-stricting sentence length to 40 words), GIZA++ (Och andNey, 2003), Moses (Koehn et al, 2007), and the SRI LMtoolkit (Stolcke, 2002) to build 5-gram LMs.
Since noNews development sets were available we chose News-Commentary sets as replacements.
We used test-2006(E06) and nc-devtest2007 (NCd) as development sets forEuroparl and News-Commentary; test-2007 (E07) andnc-test2007 (NCt) as held-out evaluation sets.English SpanishEuroparl (E)sentence pairs 1,258,778unique sent.
pairs 1,235,134avg.
sentence length 27.9 29.0# words 35.14 M 36.54 Mvocabulary 108.7 K 164.8 KNews-Commentary (NC)sentence pairs 64,308unique sent.
pairs 64,205avg.
sentence length 24.0 27.4# words 1.54 M 1.76 Mvocabulary 44.2 K 56.9 KTable 1: Statistics of English?Spanish Europarl and News-Commentary corporaTo improve the baseline performance we trained sys-tems on all true-cased training data with sentence lengthup to 100.
We used two language models, a 5-gram LMbuild from the Europarl corpus and a 3-gram LM buildfrom the News-Commentary data.
Instead of interpolat-ing the two language models, we explicitly used them inthe decoder and optimized their weights via minimum-error-rate (MER) training (Och, 2003).
To shorten thetraining time, a multi-threaded GIZA++ version was usedto utilize multi-processor servers (Gao and Vogel, 2008).Other parameters were the same as the baseline sys-tem.
Table 2 shows results in lowercase BLEU (Pap-ineni et al, 2002) for both the baseline (B) and the im-proved baseline systems (B5) on development and held-151out evaluation sets.
We observed significant gains for theNews-Commentary test sets.
Our improved baseline sys-tems obtained a comparable performance with the bestEnglish?Spanish systems in 2007 (Callison-Burch et al,2007).Pairs Europarl NCE06 E07 NCd NCtEn?Es B 33.00 32.21 31.84 30.56B5 33.33 32.25 35.10 34.08Es?En B 33.08 33.23 31.18 31.34B5 33.26 33.23 36.06 35.56Table 2: NIST-BLEU scores of baseline and improved baselinesystems experiments on English?Spanish3 Weighting Sentence Pairs3.1 Problem DefinitionThe quality of word alignment is crucial for the perfor-mance of the machine translation system.In the well-known so-called IBM word alignmentmodels (Brown et al, 1993), re-estimating the model pa-rameters depends on the empirical probability P?
(ek, fk)for each sentence pair (ek, fk).
During the EM train-ing, all counts of events, e.g.
word pair counts, distortionmodel counts, etc., are weighted by P?
(ek, fk).
For ex-ample, in IBM Model 1 the lexicon probability of sourceword f given target word e is calculated as (Och and Ney,2003):p(f |e) =?k c(f |e; ek, fk)?k,f c(f |e; ek, fk)(1)c(f |e; ek, fk) =?ek,fkP?
(ek, fk)?aP (a|ek, fk) ?
(2)?j?
(f , fkj )?
(e, ekaj )Therefore, the distribution of P?
(ek, fk) will affect thealignment results.
In Eqn.
2, P?
(ek, fk) determineshow much the alignments of sentence pair (ek, fk) con-tribute to the model parameters.
It will be helpful ifthe P?
(ek, fk) can approximate the true distribution ofP (ek, fk).Consider that we are drawing sentence pairs from agiven data source, and each unique sentence pair (ek, fk)has a probability P (ek, fk) to be observed.
If the trainingcorpora size is infinite, the normalized frequency of eachunique sentence pair will converge to P (ek, fk).
In thatcase, equally assigning a number to each occurrence of(ek, fk) and normalizing it will be valid.
However, theassumption is invalid if the data source is finite.
As wecan observe in the training corpora, most sentences occuronly one time, and thus P?
(ek, fk) will be uniform.To get a more informative P?
(ek, fk), we exploredmethods of weighting sentence pairs.
We investigatedthree sets of features: sentence pair confidence (sc),genre-dependent sentence pair confidence (gdsc) andphrase alignment confidence (pc) scores.
These featureswere calculated over an entire training corpus and couldbe easily integrated into the phrase-based machine trans-lation system.3.2 Sentence Pair ConfidenceWe can hardly compute the joint probability of P (ek, fk)without knowing the conditional probability P (ek|fk)which is estimated during the alignment process.
There-fore, to estimate P (ek, fk) before alignment, we make anassumption that P?
(ek, fk) = P (ek)P (fk), which meansthe two sides of sentence pair are independent of eachother.
P (ek) and P (fk) can be obtained by using lan-guage models.
P (ek) or P (fk), however, can be smallwhen the sentence is long.
Consequently, long sentencepairs will be assigned low scores and have negligible ef-fect on the training process.
Given limited training data,ignoring these long sentences may hurt the alignment re-sult.
To compensate this, we normalize the probability bythe sentence length.
We propose the following methodto weighting sentence pairs in the corpora.
We trainedlanguage models for source and target language, and theaverage log likelihood (AVG-LL) of each sentence pairwas calculated by applying the corresponding languagemodel.
For each sentence pair (ek, fk), the AVG-LLL(ek, fk) isL(ek) = 1|ek|?eki ?ek logP (eki |h)L(fk) = 1|fk|?fkj ?fk logP (fkj |h)L(ek, fk) = [L(ek) + L(fk)]/2(3)where P (eki |h) and P (fkj |h) are ngram probabilities.The sentence pair confidence score is then given by:sc(ek, fk) = exp(L(ek, fk)).
(4)3.3 Genre-Dependent Sentence Pair ConfidenceGenre adaptation is one of the major challenges in statis-tical machine translation since translation models sufferfrom data sparseness (Koehn and Schroeder, 2007).
Toovercome these problems previous works have focusedon explicitly modeling topics and on using multiple lan-guage and translation models.
Using a mixture of topic-dependent Viterbi alignments was proposed in (Civeraand Juan, 2007).
Language and translation model adap-tation to Europarl and News-Commentary have been ex-plored in (Paulik et al, 2007).Given the sentence pair weighting method, it is pos-sible to adopt genre-specific language models into the152weighting process.
The genre-dependent sentence pairconfidence gdsc simulates weighting the training sen-tences again from different data sources, thus, givengenre g, it can be formulated as:gdsc(ek, fk) = sc(ek, fk|g) (5)where P (eki |h) and P (fkj |h) are estimated by genre-specific language models.The score generally represents the likelihood of thesentence pair to be in a specific genre.
Thus, if both sidesof the sentence pair show a high probability accordingto the genre-specific language models, alignments in thepair should be more possible to occur in that particulardomain, and put more weight may contribute to a betteralignment for that genre.3.4 Phrase Alignment ConfidenceSo far the confidence scores are used only in the train-ing of the word alignment models.
Tracking from whichsentence pairs each phrase pair was extracted, we can usethe sentence level confidence scores to assign confidencescores to the phrase pairs.
Let S(e?, f?)
denote the set ofsentences pairs from which the phrase pair (e?, f?)
was ex-tracted.
We calculate then a phrase alignment confidencescore pc as:pc(e?, f?)
= exp?(ek,fk)?S(e?,f?)
log sc(ek, fk)|S(e?, f?
)| (6)This score is used as an additional feature of the phrasepair.
The feature weight is estimated in MER training.4 Experimental ResultsThe first step in validating the proposed approach wasto check if the different language models do assign dif-ferent weights to the sentence pairs in the training cor-pora.
Using the different language models NC (News-Commentary), EP (Europarl), NC+EP (both NC and EP)the genre-specific sentence pair confidence scores werecalculated.
Figure 1 shows the distributions of the dif-ferences in these scores across the two corpora.
As ex-pected, the language model build from the NC corpus as-signs - on average - higher weights to sentence pairs in theNC corpus and lower weights to sentence pairs in the EPcorpus (Figure 1a).
The opposite is true for the EP LM.When comparing the scores calculated from the NC LMand the combined NC+EP LM we still see a clear sep-aration (Figure 1b).
No marked difference can be seenbetween using the EP LM and the NC+EP LM (Figure1c), which again is expected, as the NC corpus is verysmall compared to the EP corpus.The next step was to retrain the word alignment mod-els using sentences weights according to the various con-?0.06 ?0.04 ?0.02 0 0.02 0.04 0.0600.0050.010.015(a) Difference in Weight (NC?EP)Proportionin Corpora?0.06 ?0.04 ?0.02 0 0.02 0.04 0.0600.0050.010.0150.02(b) Difference in Weight (NC?NE)Proportionin Corpora?0.06 ?0.04 ?0.02 0 0.02 0.04 0.0600.0050.010.0150.02(c) Difference in Weight (NE?EP)Proportionin CorporaEuropal DataNews Commentary DataEuropal DataNews Commentary DataEuropal DataNews Commentary DataFigure 1: Histogram of weight differences genre specific con-fidence scores on NC and EP training corporafidence scores.
Table 3 shows training and test set per-plexities for IBM model 4 for both training directions.Not only do we see a drop in training set perplexities,but also in test set perplexities.
Using the genre specificconfidence scores leads to lower perplexities on the cor-responding test set, which means that using the proposedmethod does lead to small, but consistent adjustments inthe alignment models.Uniform NC+EP NC EPtrain En?Es 46.76 42.36 42.97 44.47Es?En 70.18 62.81 62.95 65.86testNC(En?Es) 53.04 53.44 51.09 55.94EP(En?Es) 91.13 90.89 91.84 90.77NC(Es?En) 81.39 81.28 78.23 80.33EP(Es?En) 126.56 125.96 123.23 122.11Table 3: IBM model 4 training and test set perplexities usinggenre specific sentence pair confidence scores.In the final step the specific alignment models wereused to generate various phrase tables, which were thenused in translation experiments.
Results are shown in Ta-ble 4.
We report lower-cased Bleu scores.
We used nc-dev2007 (NCt1) as an additional held-out evaluation set.Bold cells indicate highest scores.As we can see from the results, improvements are ob-tained by using sentence pair confidence scores.
Us-ing confidence scores calculated from the EP LM gaveoverall the best performance.
While we observe only asmall improvement on Europarl sets, improvements onNews-Commentary sets are more pronounced, especiallyon held-out evaluation sets NCt and NCt1.
The exper-iments do not give evidence that genre-dependent con-fidence can improve over using the general confidence153Test SetE06 E07 NCd NCt NCt1Es?EnB5 33.26 33.23 36.06 35.56 35.64NC+EP 33.23 32.29 36.12 35.47 35.97NC 33.43 33.39 36.14 35.27 35.68EP 33.36 33.39 36.16 35.63 36.17En?EsB5 33.33 32.25 35.10 34.08 34.43NC+EP 33.23 32.29 35.12 34.56 34.89NC 33.30 32.27 34.91 34.07 34.29EP 33.08 32.29 35.05 34.52 35.03Table 4: Translation results (NIST-BLEU) using gdsc with dif-ferent genre-specific language models for Es?En systemsscore.
As the News-Commentary language model wastrained on a very small amount of data further work isrequired to study this in more detail.Test SetE06 E07 NCd NCt NCt1Es?EnB5 33.26 33.23 36.06 35.56 35.64NC+EP+pc 33.54 33.39 36.07 35.38 35.85NC+pc 33.17 33.31 35.96 35.74 36.04EP+pc 33.44 32.87 36.22 35.63 36.09En?EsB5 33.33 32.25 35.10 34.08 34.43NC+EP+pc 33.28 32.45 34.82 33.68 33.86NC+pc 33.13 32.47 34.01 34.34 34.98EP+pc 32.97 32.20 34.26 33.99 34.34Table 5: Translation results (NIST-BLEU) using pc with differ-ent genre-specific language models for Es?En systemsTable 5 shows experiments results in NIST-BLEU us-ing pc score as an additional feature on phrase tablesin Es?En systems.
We observed that across develop-ment and held-out sets the gains from pc are inconsistent,therefore our submissions are selected from the B5+EPsystem.5 ConclusionIn the ACL-WMT 2008, our major innovations are meth-ods to estimate sentence pair confidence via languagemodels.
We proposed to use source and target languagemodels to weight the sentence pairs.
We developed sen-tence pair confidence (sc), genre-dependent sentence pairconfidence (gdsc) and phrase alignment confidence (pc)scores.
Our experimental results shown that we had a bet-ter word alignment and translation performance by usinggdsc.
We did not observe consistent improvements byusing phrase pair confidence scores in our systems.AcknowledgmentsThis work is in part supported by the US DARPA under theGALE program.
Any opinions, findings, and conclusions orrecommendations expressed in this material are those of the au-thors and do not necessarily reflect the views of DARPA.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisti-cal machine translation: Parameter estimation.
In Computa-tional Linguistics, volume 19(2), pages 263?331.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-) evalua-tion of machine translation.
In Proc.
of the ACL 2007 SecondWorkshop on Statistical Machine Translation, Prague, CzechRepublic.Jorge Civera and Alfons Juan.
2007.
Domain adaptation in sta-tistical translation with mixture modelling.
In Proc.
of theACL 2007 Second Workshop on Statistical Machine Transla-tion, Prague, Czech Republic.Qin Gao and Stephan Vogel.
2008.
Parallel implementationsof word alignment tool.
In Proc.
of the ACL 2008 Soft-ware Engineering, Testing, and Quality Assurance Work-shop, Columbus, Ohio, USA.Philipp Koehn and Josh Schroeder.
2007.
Experiments in do-main adaptation for statistical machine translation.
In Proc.of the ACL 2007 Second Workshop on Statistical MachineTranslation, Prague, Czech Republic.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-drej Bojar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machine transla-tion.
In Proc.
of the 45th Annual Meeting of the Associationfor Computational Linguistics, demo sessions, pages 177?180, Prague, Czech Republic, June.Franz J. Och and Hermann Ney.
2003.
A systematic compar-ison of various statistical alignment models.
In Computa-tional Linguistics, volume 1:29, pages 19?51.Franz Josef Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Erhard Hinrichs and Dan Roth,editors, Proceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation of ma-chine translation.
In Proc.
of the 40th Annual Conf.
of theAssociation for Computational Linguistics (ACL 02), pages311?318, Philadelphia, PA, July.Matthias Paulik, Kay Rottmann, Jan Niehues, Silja Hildebrand,and Stephan Vogel.
2007.
The ISL phrase-based mt systemfor the 2007 ACL workshop on statistical machine transla-tion.
In In Proc.
of the ACL 2007 Second Workshop on Sta-tistical Machine Translation, Prague, Czech Republic.Andreas Stolcke.
2002.
SRILM ?
An extensible language mod-eling toolkit.
In Proc.
Intl.
Conf.
on Spoken Language Pro-cessing, volume 2, pages 901?904, Denver.154
