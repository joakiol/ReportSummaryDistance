Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 19?24,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsTraining Data Modification for SMTConsidering Groups of Synonymous SentencesHideki KASHIOKASpoken Language Communication Research Laboratories, ATR2-2-2 Hikaridai, Keihanna Science CityKyoto, 619-0288, Japanhideki.kashioka@atr.jpAbstractGenerally speaking, statistical machinetranslation systems would be able to attainbetter performance with more training sets.Unfortunately, well-organized training setsare rarely available in the real world.
Con-sequently, it is necessary to focus on modi-fying the training set to obtain highaccuracy for an SMT system.
If the SMTsystem trained the translation model, thetranslation pair would have a low probabil-ity when there are many variations for tar-get sentences from a single source sentence.If we decreased the number of variationsfor the translation pair, we could constructa superior translation model.
This paper de-scribes the effects of modification on thetraining corpus when consideration is givento synonymous sentence groups.
We at-tempt three types of modification: com-pression of the training set, replacement ofsource and target sentences with a selectedsentence from the synonymous sentencegroup, and replacement of the sentence ononly one side with the selected sentencefrom the synonymous sentence group.
As aresult, we achieve improved performancewith the replacement of source-side sen-tences.1 IntroductionRecently, many researchers have focused their in-terest on statistical machine translation (SMT) sys-tems, with particular attention given to models anddecoding algorithms.
The quantity of the trainingcorpus has received less attention, although ofcourse the earlier reports do address the quantityissue.
In most cases, the larger the training corpusbecomes, the higher accuracy is achieved.
Usually,the quantity problem of the training corpus is dis-cussed in relation to the size of the training corpusand system performance; therefore, researchersstudy line graphs that indicate the relationship be-tween accuracy and training corpus size.On the other hand, needless to say, a single sen-tence in the source language can be used to trans-late several sentences in the target language.
Suchvarious possibilities for translation make MT sys-tem development and evaluation very difficult.Consequently, here we employ multiple referencesto evaluate MT systems like BLEU (Papineni et al,2002) and NIST (Doddington, 2002).
Moreover,such variations in translation have a negative effecton training in SMT because when several sen-tences of input-side language are translated into theexactly equivalent output-side sentences, the prob-ability of correct translation decreases due to thelarge number of possible pairs of expressions.Therefore, if we can restrain or modify the trainingcorpus, the SMT system might achieve high accu-racy.As an example of modification, different out-put-side sentences paired with the exactly equiva-lent input-side sentences are replaced with onetarget sentence.
These sentence replacements arerequired for synonymous sentence sets.
Kashioka(2004) discussed synonymous sets of sentences.Here, we employ a method to group them as a wayof modifying the training corpus for use with SMT.This paper focuses on how to control the corpuswhile giving consideration to synonymous sen-tence groups.192 Target CorpusIn this paper, we use a multilingual parallel corpuscalled BTEC (Takezawa et al, 2002) for our ex-periments.
BTEC was used in IWSLT (Akiba et al,2004).
This parallel corpus is a collection of Japa-nese sentences and their translations into English,Korean and Chinese that are often found in phrasebooks for foreign tourists.
These parallel sentencescover a number of situations (e.g., hotel reserva-tions, troubleshooting) for Japanese going abroad,and most of the sentences are rather short.
Sincethe scope of its topics is quite limited, some verysimilar sentences can be found in the corpus, mak-ing BTEC appropriate for modification with com-pression or replacement of sentences.
We use onlya part of BTEC for training data in our experiments.The training data we employ contain 152,170Japanese sentences, with each sentence combinedwith English and Chinese translations.
In Japanese,each sentence has 8.1 words on average, and themaximum sentence length is 150 words.
In English,each sentence contains an average of 7.4 words,with a maximum sentence length of 117 words.
InChinese, each sentence has an average of 6.7words and maximum length of 122 words.
Somesentences appear twice or more in the training cor-pus.
In total, our data include 94,268 differentJapanese sentences, 87,061 different Chinese sen-tences, and 91,750 different English sentences.Therefore, there are some sentence pairs that con-sist of exactly the same sentence in one languagebut a different sentence in another language, as Fig.1 shows.
This relationship can help in finding thesynonymous sentence group.The test data contain 510 sentences from differ-ent training sets in the BTEC.
Each source sen-tence in the test data has 15 target sentences forevaluations.
For the evaluation, we do not use anyspecial process for the grouping process.
Conse-quently, our results can be compared with those ofother MT systems.Figure 1.
Sample sentence pairs3 Modification MethodWhen an SMT system learns the translation model,variations in the translated sentences of the pair arecritical for determining whether the system obtainsa good model.
If the same sentence appears twicein the input-side language and these sentencesform pairs with two different target sentences inthe output-side language, then broadly speakingthe translation model defines almost the sameprobability for these two target sentences.In our model, the translation system features theability to generate an output sentence with somevariations; however, for the system to generate themost appropriate output sentence, sufficient infor-mation is required.
Thus, it is difficult to prepare asufficiently large training corpus.3.1 Synonymous Sentence GroupKashioka (2004) reported two steps for making asynonymous sentence group.
The first is a con-catenation step, and the second is a decompositionstep.
In this paper, to form a synonymous sentencegroup, we performed only the concatenation step,which has a very simple idea.
When the expression?Exp_A1?
in language A is translated into the ex-pressions ?Exp_B1, Exp_BB2, ..., Exp_Bn?
in lan-guage B, that set of expressions form onesynonymous group.
Furthermore, when the sen-tence ?Exp_A2?
in language A is translated into thesentences ?Exp_B1, Exp_Bn+1, ..., Exp_Bm?
in lan-guage B, ?Exp_B1, Exp_Bn+1,  ..., Exp_Bm (n < m)?form one synonymous group.
In this situation,?Exp_A1?
and ?Exp_A2?
form a synonymousgroup because both ?Exp_A1?
and ?Exp_A2?
havea relationship with the translation pairs of?Exp_B1.?
Thus, ?Exp_A1, Exp_A2?
in language Aand ?Exp_B1, ..., Exp_Bm?
in language B form asynonymous group.
If other language informationis available, we can extend this synonymous groupusing information on translation pairs for otherlanguages.In this paper, we evaluate an EJ/JE system and aCJ/JC system, and our target data include threelanguages, i.e., Japanese, English, and Chinese.We make synonymous sentence groups in two dif-ferent environments.
One is a group using Japaneseand English data, and other is a group that usesJapanese and Chinese data.S1 ?
T1S2 ?
T1S1 ?
T2S3 ?
T1The JE group contained 72,808 synonymous sentencegroups, and the JC group contained 83,910 synonymoussentence groups as shown in Table 1.20# of Groups # of Sent per GroupJE 72,808 2.1JC 83,910 1.8Table 1 Statistics used in BTEC data3.2 ModificationWe prepared the three types of modifications fortraining data.1.
Compress the training corpus based on thesynonymous sentence group (Fig.
2).2.
Replace the input and output sides?
sen-tences with the selected sentence, consider-ing the synonymous sentence group (Fig.
3).3.
Replace one side?s sentences with a se-lected sentence, considering the synony-mous sentence group (Figs.
4, 5).We describe these modifications in more detailin the following subsections.3.2.1 Modification with CompressionHere, a training corpus is constructed with severalgroups of synonymous sentences.
Then, eachgroup keeps only one pair of sentences and theother pairs are removed from each group, therebydecreasing the total number of sentences and nar-rowing the variation of expressions.
Figure 2shows an example of modification in this way.
Inthe figure, S1, S2, and S3 indicate the input-sidesentences while T1 and T2 indicate the output-sidesentences.
The left-hand side box shows a syn-onymous sentence group in the original trainingcorpus, where four sentence pairs construct onesynonymous sentence group.
The right-hand sidebox shows a part of the modified training corpus.In this case, we keep the S1 and T1 sentences, andthis resulting pair comprises a modified trainingcorpus.The selection of what sentences to keep is an im-portant issue.
In our current experiment, we selectthe most frequent sentence in each side?s languagefrom within each group.
In Fig.
2, S1 appearedtwice, while S2 and S3 appeared only once in theinput-side language.
As for the output-side lan-guage, T1 appeared three times and T2 appearedonce.
Thus, we keep the pair consisting of S1 andT1.
When attempting to separately select the mostfrequent sentence in each language, we may notfind suitable pairs in the original training corpus;however, we can make a new pair with the ex-tracted sentences for the modified training corpus.S1?T1S2?T1S1?T2S3?T1?
S1?T1Figure 2.
Modification sample for compression3.2.2 Modification of replacing the sentencesof both sidesIn the compression stage, the total number of sen-tences in the modified training corpus is decreased,and it is clear that fewer sentences in the trainingcorpus leads to diminished accuracy.
In order tomake a comparison between the original trainingcorpus and a modified training corpus with thesame number of sentences, we extract one pair ofsentences from each group, and each pair appearsin the modified training corpus in the same numberof sentences.
Figure 3 shows an example of thismodification.
The original training data are thesame as in Fig.
2.
Then we extract S1 and T1 bythe same process from each side with this group,and replacing all of the input-side sentences withS1 in this group.
The output side follows the sameprocess.
In this case, the modified training corpusconsists of four pairs of S1 and T1.S1?T1S2?T1S1?T2S3?T1?S1?T1S1?T1S1?T1S1?T1Figure 3.
Sample modifications for replacement ofboth sentences3.2.3 Modification to replace only one side?ssentenceWith the previous two modifications, the lan-guage variations in both sides decrease.
Next, wepropose the third modification, which narrows therange of one side?s variations.The sentences of one side are replaced with theselected sentence from that group.
The sentence forreplacement is selected by following the sameprocess used in the previous modifications.
As aresult, two modified training corpora are available21as shown in Figs.
4 and 5.
Figure 4 illustrates theoutput side?s decreasing variation, while Fig.
5shows the input side?s decreasing variation.Figure 4.
Modification example of replacing theoutput side?s sentenceFigure 5.
Modification example of replacing theinput side?s sentence4 SMT System and Evaluation methodIn this section, we describe the SMT systems usedin these experiments.
The SMT systems?
decoderis a graph-based decoder (Ueffing et al, 2002;Zhang et al, 2004).
The first pass of the decodergenerates a word-graph, a compact representationof alternative translation candidates, using a beamsearch based on the scores of the lexicon and lan-guage models.
In the second pass, an A* searchtraverses the graph.
The edges of the word-graph,or the phrase translation candidates, are generatedby the list of word translations obtained from theinverted lexicon model.
The phrase translationsextracted from the Viterbi alignments of the train-ing corpus also constitute the edges.
Similarly, theedges are also created from dynamically extractedphrase translations from the bilingual sentences(Watanabe and Sumita, 2003).
The decoder usedthe IBM Model 4 with a trigram language modeland a five-gram part-of-speech language model.Training of the IBM model 4 was implemented bythe GIZA++ package (Och and Ney, 2003).
Allparameters in training and decoding were the samefor all experiments.
Most systems with this trainingcan be expected to achieve better accuracy whenwe run the parameter tuning processes.
However,our purpose is to compare the difference in resultscaused by modifying the training corpus.We performed experiments for JE/EJ and JC/CJsystems and four types of training corpora:1) Original BTEC corpus;2) Compressed BTEC corpus (see 3.2.1);3) Replace both languages (see 3.2.2);4) Replace one side language (see 3.2.3)4-1) replacement on the input side4-2) replacement on the output side.For the evaluation, we use BLEU, NIST, WER,and PER as follows:S1?T1S2?T1S1?T2S3?T1?S1?T1S2?T1S1?T1S3?T1BLEU: A weighted geometric mean of the n-gram matches between test and referencesentences multiplied by a brevity penaltythat penalizes short translation sentences.S1?T1S2?T1S1?T2S3?T1?S1?T1S1?T1S1?T2S1?T1NIST: An arithmetic mean of the n-grammatches between test and reference sen-tences multiplied by a length factor, whichagain penalizes short translation sentences.mWER (Niessen et al, 2000): Multiple refer-ence word-error rate, which computes theedit distance (minimum number of inser-tions, deletions, and substitutions) betweentest and reference sentences.mPER: Multiple reference position-independentword-error rate, which computes the editdistance without considering the word order.5 Experimental ResultsIn this section, we show the experimental resultsfor the JE/EJ and JC/CJ systems.5.1 EJ/JE-system-based JE groupTables 2 and 3 show the evaluation results for theEJ/JE system.EJ BLEU NIST mWER mPEROriginal 0.36 3.73 0.55 0.51Compress 0.47 5.83 0.47 0.44Replace Both 0.42 5.71 0.50 0.47Replace J.
0.44 2.98 0.60 0.58Replace E. 0.48 6.05 0.44 0.41Table 2.
Evaluation results for EJ SystemJE BLEU NIST mWER mPEROriginal 0.46 3.96 0.52 0.49Compress 0.53 8.53 0.42 0.38Replace Both 0.49 8.10 0.46 0.41Replace J.
0.54 8.64 0.42 0.38Replace E. 0.51 6.10 0.52 0.49Table 3.
Evaluation results for JE system22Modification of the training data is based on thesynonymous sentence group with the JE pair.The EJ system performed at 0.55 in mWER withthe original data set, and the system replacing theJapanese side achieved the best performance of0.44 in mWER.
The system then gained 0.11 inmWER.
On the other hand, the system replacingthe English side lost 0.05 in mWER.
The mPERscore also indicates a similar result.
For the BLEUand NIST scores, the system replacing the Japa-nese side also attained the best performance.The JE system attained a score of 0.52 in mWERwith the original data set, while the system withEnglish on the replacement side gave the best per-formance of 0.42 in mWER, a gain of 0.10.
On theother hand, the system with Japanese on the re-placement side showed no change in mWER, andthe case of compression achieved good perform-ance.
The ratios of mWER and mPER are nearlythe same for replacing Japanese.
Thus, in both di-rections replacement of the input-side languagederives a positive effect for translation modeling.5.2 CJ/JC system-based JC groupTables 4 and 5 show the evaluation results for theEJ/JE system based on the group with a JC lan-guage pair.CJ BLEU NIST mWER mPEROriginal 0.51 6.22 0.41 0.38Compress 0.52 6.43 0.43 0.40Replace both 0.53 5.99 0.40 0.37Replace J.
0.50 5.98 0.41 0.39Replace C.  0.51 6.22 0.41 0.38Table 4.
Evaluation results for CJ based on the JClanguage pairJC BLEU NIST mWER mPEROriginal 0.56 8.45 0.38 0.34Compress 0.55 8.22 0.41 0.36Replace both 0.56 8.32 0.39 0.35Replace J.
0.56 8.25 0.40 0.36Replace C. 0.57 8.33 0.38 0.35Table 5.
Evaluation results for JC based on the JClanguage pairThe CJ system achieved a score of 0.41 inmWER with the original data set, with the othercases similar to the original; we could not find alarge difference among the training corpus modifi-cations.
Furthermore, the JC system performed at0.38 in mWER with the original data, although theother cases?
results were not as good.
These resultsseem unusual considering the EJ/JE system, indi-cating that they derive from the features of theChinese part of the BTEC corpus.6 DiscussionOur EJ/JE experiment indicated that the systemwith input-side language replacement achievedbetter performance than that with output-side lan-guage replacement.
This is a reasonable result be-cause the system learns the translation model withfewer variations for input-side language.In the experiment on the CJ/JC system based onthe JC group, we did not provide an outline of theEJ/JE system due to the features of BTEC.
Initially,BTEC data were created from pairs of Japaneseand English sentences in the travel domain.
Japa-nese-English translation pairs have variation asshown in Fig.
1.
However, when Chinese data wastranslated, BTEC was controlled so that the sameJapanese sentence has only one Chinese sentence.Accordingly, there is no variation in Chinese sen-tences for the pair with the same Japanese sentence.Therefore, the original training data would be simi-lar to the situation of replacing Chinese.
Moreover,replacing the Japanese data was almost to the sameas replacing both sets of data.
Considering this fea-ture of the training corpus, i.e.
the results for theCJ/JC system based on the group with JC languagepairs, there are few differences between keepingthe original data and replacing the Chinese data, orbetween replacing both side?s data and replacingonly the Japanese data.
These results demonstratethe correctness of the hypothesis that reducing theinput side?s language variation makes learningmodels more effective.Currently, our modifications only roughly proc-ess sentence pairs, though the process of makinggroups is very simple.
Sometimes a group mayinclude sentences or words that have slightly dif-ferent meanings, such as.
fukuro (bag), kamibukuro(paper bag), shoppingu baggu (shopping bag),tesagebukuro (tote bag), and biniiru bukuro (plas-tic bag).
In this case if we select tesagebukurofrom the Japanese side and ?paper bag?
from theEnglish side, we have an incorrect word pair in thetranslation model.
To handle such a problem, wewould have to arrange a method to select the sen-23tences from a group.
This problem is discussed inImamura et al (2003).
As one solution to thisproblem, we borrowed the measures of literalness,context freedom, and word translation stability inthe sentence-selection process.In some cases, the group includes sentences withdifferent meanings, and this problem was men-tioned in Kashioka (2004).
In an attempt to solvethe problem, he performed a secondary decomposi-tion step to produce a synonymous group.
How-ever, in the current training corpus, eachsynonymous group before the decomposition stepis small, so there would not be enough differencefor modifications after the decomposition step.The replacement of a sentence could be calledparaphrasing.
Shimohata et al (2004) reported aparaphrasing effect in MT systems, where if eachgroup would have the same meaning, the variationin the phrases that appeared in the other groupswould reduce the probability.
Therefore, consider-ing our results in light of their discussion, if thetraining corpus could be modified with the modulefor paraphrasing in order to control phrases, wecould achieve better performance.7 ConclusionThis paper described the modification of a trainingset based on a synonymous sentence group for astatistical machine translation system in order toattain better performance.
In an EJ/JE system, weconfirmed a positive effect by replacing the input-side language.
Because the Chinese data was spe-cific in our modification, we observed an inconclu-sive result for the modification in the CJ/JC systembased on the synonymous sentence group with a JClanguage pair.
However, there was still some effecton the characteristics of the training corpus.
In thispaper, the modifications of the training set arebased on the synonymous sentence group, and wereplace the sentence with rough processing.
If weparaphrased the training set and controlled thephrase pair, we could achieve better performancewith the same training set.AcknowledgementsThis research was supported in part by the NationalInstitute of Information and CommunicationsTechnology.ReferencesYasuhiro AKIBA, Marcello FEDERICO, NorikoKANDO, Hiromi NAKAIWA, Michael PAUL, andJun'ichi TSUJII, 2004.
Overview of the IWSLT04Evaluation Campaign, In Proc.
of IWSLT04, 1 ?
12.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the HLT Conference, SanDiego, California.Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto,2003.
Automatic Construction of Machine TranslationKnowledge Using Translation Literalness, in Proc.
ofEACL 2003, 155 ?
162.Hideki Kashioka, 2004.
Grouping Synonymous Sen-tences from a Parallel Corpus.
In Proc.
of LREC 2004,391 - 394.Sonja Niessen, Franz J. Och, Gregor Leusch, andHermann Ney.
2000.
An evaluation tool for machinetranslation: Fast evaluation for machine translationresearch.
In Proc.of LREC 2000, 39 ?
45.Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19 - 51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
of ACL 2002,311?318.Mitsuo Shimohata, Eiichiro Sumita, and Yuji Matsu-moto, 2004.
Building a Paraphrase Corpus for SpeechTranslation.
In Proc.
of LREC 2004, 1407 - 1410.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a broad-coverage bilingual corpus for speechtranslation of travel conversations in the real world,In Proc.
of LREC 2002, 147?152.Nicola Ueffing, Franz Josef Och, and Hermann Ney.2002.
Generation of word graphs in statistical ma-chine translation.
In Proc.
of the Conference onEmpirical Methods for Natural Language Proc-essing (EMNLP02), 156 ?
163.Taro Watanabe and Eiichiro Sumita.
2003.
Example-based decoding for statistical machine translation.
InMachine Translation Summit IX, 410 ?
417.Ruiqiang Zhang, Genichiro Kikui, Hirofumi Yamamoto,Frank Soong, Taro Watanabe and Wai Kit Lo, 2004.A Unified Approach in Speech-to-Speech Translation:Integrating Features of Speech recognition and Ma-chine Translation, In Proc.
of COLING 2004, 1168 -1174.24
