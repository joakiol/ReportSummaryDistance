Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 71?79,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsIdentifying Emotions, Intentions, and Attitudes in TextUsing a Game with a PurposeLisa PearlDepartment of Cognitive SciencesUniversity of California, Irvine3151 Social Science PlazaIrvine, CA 92697, USAlpearl@uci.eduMark SteyversDepartment of Cognitive SciencesUniversity of California, Irvine3151 Social Science PlazaIrvine, CA 92697, USAmsteyver@uci.eduAbstractSubtle social information is available in textsuch as a speaker?s emotional state, intentions,and attitude, but current information extrac-tion systems are unable to extract this infor-mation at the level that humans can.
We de-scribe a methodology for creating databasesof messages annotated with social informationbased on interactive games between humanstrying to generate and interpret messages for anumber of different social information types.We then present some classification resultsachieved by using a small-scale database cre-ated with this methodology.1 IntroductionA focus of much information extraction researchhas been identifying surface-level semantic content(e.g., identifying who did what to whom when).In recent years, research on sentiment analysis andopinion mining has recognized that more subtle in-formation can be communicated via linguistic fea-tures in the text (see Pang and Lee (2008) for a re-view), such as whether text (e.g., a movie review)is positive or negative (Turney 2002, Pang, Lee,and Vaithyanathan 2002, Dave, Lawrence, and Pen-nock 2003, Wiebe et al 2004, Kennedy and Inkpen2006, Agarwal, Biadsy, and Mckeown 2009, Greeneand Resnik 2009, among many others).
However,other subtle information available in text, such as aspeaker?s emotional states (e.g., anger, embarrass-ment), intentions (e.g., persuasion, deception), andattitudes (e.g., disbelief, confidence), has not beenexplored as much, though there has been some workin detecting emotion (e.g., Subasic and Huettner2001, Alm, Roth, and Sproat 2005, Nicolov et al2006, Abbasi 2007) and detecting deception (e.g.,Annolli, Balconi, and Ciceri 2002, Zhou et al 2004,Gupta and Skillicorn 2006, Zhou and Sung 2008).This latter kind of social information is useful foridentifying the ?tone?
of a message, i.e., for un-derstanding the underlying intention behind a mes-sage?s creation, and also for predicting how thismessage will be interpreted by humans reading it.A technical barrier to extracting this kind of socialinformation is that there are currently no large-scaletext databases that are annotated with social infor-mation from which to learn the relevant linguisticcues.
That is, there are few examples of social in-formation ?ground truth?
- text annotated with hu-man perceptions of the social information containedwithin the text.
Given the success of sentiment anal-ysis, we believe this social information could alsobe retrievable once the relevant linguistic cues areidentified.One way to create the necessary annotated datais to draw from computational social science (Lazeret al 2009), and make use of human-based com-putation (Kosurokoff 2001, von Ahn 2006, amongothers) since humans are used to transmitting so-cial information through language.
In this paper,we describe a methodology for creating this kindof database, and then present the results from asmall-scale database created using this methodol-ogy1.
In addition, we show one example of us-1The database can be obtained by downloading itfrom http://www.socsci.uci.edu/?lpearl/CoLaLab/projects.htmlor contacting Lisa Pearl at lpearl@uci.edu.71ing this database by training a Sparse MultinomialLogistic Regression classifier (Krishnapuram et al2005) on these data.2 Reliable databases of social information2.1 The need for databasesIn general, reliable databases are required to de-velop reliable machine learning algorithms.
Unfor-tunately, very few databases annotated with socialinformation exist, and the few that do are small insize.
A recent addition to the Linguistic Data Con-sortium demonstrates this: The Language Under-standing Annotation Corpus (LUAC) by Diab et al(2009) includes text annotated with committed be-lief, which ?distinguishes between statements whichassert belief or opinion, those which contain spec-ulation, and statements which convey fact or oth-erwise do not convey belief.?
This is meant to aidin determining which beliefs can be ascribed to acommunicator and how strongly the communicatorholds those beliefs.
Nonetheless, this is still a smallsample of the possible social information containedin text.
Moreover, the LUAC contains only about9000 words across two languages (6949 English,2183 Arabic), which is small compared to the cor-pora generally available for natural language pro-cessing (e.g., the English Gigaword corpus (Graff2003) contains 1756504 words).Another tack taken by researchers has been to useopen-source data that are likely to demonstrate cer-tain social information by happenstance, e.g., onlinegaming forums with games that happen to involvethe intent to deceive (e.g., Zhou and Sung 2008:Mafia game forums).
While these data sets are largerin size, they do not have the breadth of coverage interms of what social information they can capturebecause, by nature, the games only explicitly involveone kind of social information (e.g., intentions: de-ception); other social information cannot reliably beattributed to the text.
In general, real world data setspresent the problem of ground truth, i.e., knowingfor certain which emotions, intentions, and attitudesare conveyed by a particular message.However, people can often detect social informa-tion conveyed through text (perhaps parsing it asthe ?tone?
of the message).
For example, considerthe following message: ?Come on...you have to buythis.?
From only the text itself, we can readily in-fer that the speaker intends to persuade the listener.Human-based computation can leverage this abilityfrom the population, and use it to construct a reli-able database of social information.
Interestingly,groups of humans are sometimes capable of pro-ducing much more precise and reliable results thanany particular individual in the group.
For example,Steyvers et al (2009) has shown that such ?wisdomof crowds?
phenomena occur in many knowledgedomains, including human memory, problem solv-ing, and prediction.
In addition, Snow et al (2008)have demonstrated that a relatively small number ofnon-expert annotations in natural language tasks canachieve the same results as expert annotation.2.2 Games with a purposeOne approach is to use a game with a purpose(GWAP) (von Ahn and Dabbish 2004, von Ahn2006, von Ahn, Kedia, and Blum 2006) that is de-signed to encourage people to provide the infor-mation needed in the database.
GWAPs are cur-rently being used to accumulate information aboutmany things that humans find easy to identify (seehttp://www.gwap.com/gwap/ for several examples),such as objects in images (von Ahn and Dabbish2004), the musical style of songs, impressions ofsights and sounds in videos, and common sense re-lationships between concepts (von Ahn, Kedia, andBlum 2006).
In addition, as the collected data comesfrom and is vetted by a large number of participants,we can gauge which messages are reliable examplesof particular social information and which are con-fusing examples.2.3 A GWAP for social information in textWe designed a GWAP to create a database of mes-sages annotated with social information, where un-paid participants provide knowledge about the socialinformation in text.
The GWAP encourages partici-pants to both generate messages that reflect specificsocial information and to label messages created byother participants as reflecting specific social infor-mation.
Participants are given points for every mes-sage they create that is correctly labeled by anotherparticipant, and for every message created by an-other participant that they correctly label.Message generators were instructed to generate a72message expressing some particular social informa-tion type (such as persuading), and were allowed touse a displayed picture as context to guide their mes-sage, so they would not need to rely completely ontheir own imaginations.
All context pictures usedin our GWAP were meant to be generic enoughthat they could be a basis for a message express-ing a variety of social information types.
Contextpictures were randomly assigned when participantswere asked to generate messages; this meant that, forexample, a picture could be used to generate a per-suasive message and be used again later to generatea deceptive message.
Generators were also warnednot to use ?taboo?
words that would make the socialinformation too easy to guess 2, but were encour-aged to express the social information as clearly aspossible.
The generator was told that if another par-ticipant perceived the correct social information typefrom the message, the generator would be rewardedwith game points.Message annotators were instructed to guesswhich social information type was being expressedby the displayed message.
They were also shownthe image the generator used as context for the mes-sage, and were rewarded with points for successfuldetection of the intended social information.As an example of the GWAP in action, one par-ticipant might generate the message ?Won?t you con-sider joining our campaign?
It?s for a good cause.
?for the social information of persuading; a differ-ent participant would see this message and might la-bel it as an example of persuading.
A participantcan only label a message with one social informa-tion type (e.g., a participant could not choose bothpersuading and formal for the same message).3With enough game players, many messages arecreated that clearly reflect different social informa-tion.
Without any of the participants necessarily2Taboo words were chosen as morphological variants of thesocial information type description.
For example, persuade,persuades, persuaded, and persuading were considered taboowords for ?persuading?.
Future versions of the GWAP couldallow the taboo word list to be influenced by which words areoften associated with a particular social information type.3We note that this is a restriction that might be relaxed infuture versions of the GWAP.
For instance, participants mightdecide whether a message expresses a social information typeor not from their perspective, so the task is more like binaryclassification for each social information type.having expert knowledge or training, we expect thatthe cumulative knowledge to be quite reliable (forexample, see Steyvers et al (2009) and work by vonAhn (von Ahn and Dabbish 2004, von Ahn 2006,von Ahn, Kedia, and Blum 2006) for other success-ful cases involving the ?wisdom of the crowds?, andSnow et al (2008) for non-expert annotation in nat-ural language tasks such as affect recognition).
Be-cause the same text can be evaluated by many differ-ent people, this can reduce the effect of idiosyncraticresponses from a few individuals.An advantage of this kind of database is that manydifferent kinds of social information can be gen-erated and labeled by the participants so that thedatabase contains examples of many different kindsof social information in text, even if only a singlelabel is given to a particular message (perhaps ex-pressing that message?s most obvious social infor-mation from the perspective of the labeler).
We cangauge how clearly a message reflects social informa-tion by how often it is labeled by others as reflect-ing that social information.
In addition, by the verynature of the GWAP, we can also assess which so-cial information is easily confused by humans, e.g.,politeness with embarrassment, or confidence withdeception.
This can aid the development of modelsthat extract social information and could also iden-tify messages likely to be ambiguous to humans.2.4 A GWAP studyBelow we report data from an offline GWAP that in-volves eight types of social information indicative ofseveral social aspects that we thought would be ofinterest: politeness (indicates emotional state, atti-tude), rudeness (indicates emotional state, attitude),embarrassment (indicates emotional state), formal-ity (indicates attitude), persuading (indicates intent),deception (indicates intent), confidence (indicatesemotional state, attitude), and disbelief (indicatesattitude).
Fifty eight English-speaking adults par-ticipated in the GWAP, consisting of a mix of un-dergraduate students, graduate students, the authors,friends of the students, and friends of the authors,in order to simulate the varied mix of participants inan online GWAP.
The undergraduate students werecompensated with course credit.
Together, these 58participants created 1176 messages and made 3198annotations.
Note that a participant would label73more messages than that participant would be askedto generate, and more than one participant would la-bel the same message (though no participant wouldlabel a message that s/he created, nor would any par-ticipant label the same message more than once).Participants were encouraged to play the GWAPmultiple times if they were inclined, to simulate theexperience of playing a favorite game.
There was nolimit on message length, though most participantstended to keep messages fairly brief.
Some samplemessages (with the participants?
own spelling andpunctuation) that were correctly and incorrectly la-beled are shown in Table 1.Social Information MessageGeneratedLabeleddeceptiondeception?Oh yeah...your hair looks reallygreat like that...yup, I love it...it,uh, really suits you...?embarrassmentembarrassment?Oh... we?re not dating.
I wouldnever date him... he?s like abrother to me..?disbeliefdisbelief?Are you and him reallyfriends?
?rudenesspersuading?James, Bree doesn?t like you.She never did and never will!
?deceptionpersuading?I wasn?t going to take anythingfrom your storeroom, I swear!Really, I won?t try to get insideagain!
?politenessdeception?Your orange hair matches yoursweater nicely?Table 1: Sample messages from the offline GWAP.The GWAP as currently designed allows us togauge two interesting aspects of social informationtransmission via text.
First, we can assess our non-expert participants?
performance.
Second, we canassess the messages themselves.For the participants, we can gauge their accuracyas message generators by measuring how often amessage they created was successfully perceived asexpressing the intended social information type (thatis, their ?expressive accuracy?).
On average, mes-sage generators were able to generate reliable mes-sages 56% of the time.
Figure 1 displays the expres-sive accuracy of participants, while also showinghow many messages participants generated.
Mostparticipants created less than 30 messages, and wereaccurate more than half the time.Figure 1: Expressive accuracy of GWAP participants.At the same time, we can also gauge the accu-racy of the participants as non-expert annotators bymeasuring how often a participant perceived the in-tended social information (that is, their ?perceptiveaccuracy?).
On average, annotators were able to per-ceive the intended social information 58% of thetime.
Figure 2 displays the perceptive accuracy,while also showing how many messages partici-pants annotated.
Most participants annotated around20 messages or between 80 and 100 messages andwere accurate more than half the time.
Averageinter-annotator agreement was 0.44, calculated us-ing Fleiss?
Kappa (Fleiss 1971), suggesting moder-ate agreement.Figure 2: Perceptive accuracy of GWAP participants.Turning to the messages, we can gauge how oftenmessages were able to successfully express a par-ticular social information type, and how often theywere confused as expressing some other type.
Table2 shows a confusion matrix of social information de-74rived from this database.deceptionpolitenessrudenessembarrassmentconfidencedisbeliefformalitypersuadingdeception .37 .07 .10 .03 .09 .10 .04 .20politeness .05 .53 .05 .02 .03 .01 .20 .10rudeness .04 .01 .78 .02 .04 .04 .03 .03embarrassment .07 .09 .05 .56 .02 .13 .05 .03confidence .04 .04 .03 .01 .67 .05 .02 .13disbelief .10 .05 .05 .04 .07 .62 .02 .06formality .02 .34 .04 .02 .06 .03 .39 .10persuading .09 .06 .03 .01 .12 .03 .04 .61Table 2: Confusion matrix for the human participants.The rows represent the intended social information for amessage while the columns represent the labeled socialinformation, averaged over messages and participants.The matrix shows the likelihood that a messagewill be labeled as expressing specific social infor-mation (in a column), given that it has been gener-ated with specific social information in mind (in arow), averaged over messages and participants.
Inother words, we show the probability distributionp(labeled|generated).
The diagonal probabilitiesindicate how often a message?s social informationwas correctly labeled for each social informationtype; this shows how often social information trans-mission was successful.
Messages were perceivedcorrectly by human participants about 57% of thetime.
More particular observations about the data inTable 2 are that people are more likely to correctlyidentify a message expressing rudeness (p = .78)and confidence (p = .67) and less likely to correctlyidentify a message expressing deception (p = .37)or formality (p = .39).
Also, we can see that adeceptive message can often be mistaken for a per-suading message (p = 0.20), a formal message mis-taken for a polite message (p = 0.34), a messageexpressing disbelief mistaken for a message express-ing deception (p = .10), and a persuading messagemistaken for a deceptive message (p = .09) or con-fidence (p = .12), among other observations.
Someof these may be expected, e.g., confusing confidencewith persuading since someone who is trying to per-suade will likely be confident about the topic, orformality with politeness since many formal expres-sions are used to indicate politeness (e.g., ?if youwould be so kind?).
Others may be unexpected apriori, such as mistaking disbelief for deception.2.5 Human reliability and message reliabilityGiven that humans were believed to be good at iden-tifying social information in text, the low percep-tive accuracy rates for participants and low anno-tation accuracy rates for messages may seem unex-pected.
However, we believe it indicates that somemessages are better than others at expressing socialinformation in a way obvious to humans.
That is,messages confusing to human participants (e.g., thelower three examples in Table 1, as well as the con-fusing messages represented by the probabilities inTable 2) would be consistently mislabeled.It may be that some messages are created suchthat many annotators agree with each other, but theyall perceive a social information type other than theone intended.4 In a similar vein, messages withlow inter-annotator agreement may simply be poorlygenerated messages that should be removed from thedatabase.
To this end, we can assess how often ma-jority annotator agreement correlates with percep-tion of the message?s intended social informationtype.
Table 3 shows the confusion matrix for mes-sages where over 50% of the annotators agreed witheach other on which social information type was in-tended, and at least two annotators labeled the mes-sage.
A total of 866 messages satisfied these criteria.The confusion matrix, as before, shows the like-lihood that a message will be labeled as express-ing specific social information (in a column), giventhat it has been generated with specific social in-formation in mind (in a row), averaged over mes-sages and participants.
The diagonal probabilitiesindicate how often a message?s social informationwas correctly labeled for each social informationtype; this shows how often social information trans-mission was successful.
The messages in this sub-set were perceived correctly by human participantsabout 71% of the time, a significant improvementover 57%.
This demonstrates how even a modestpooling of non-expert opinion can significantly in-4Messages consistently perceived as expressing a differentsocial information type than intended should perhaps be con-sidered as actually expressing that social information type ratherthan the intended one.75deceptionpolitenessrudenessembarrassmentconfidencedisbeliefformalitypersuadingdeception .45 .05 .10 .01 .07 .07 .03 .21politeness .03 .71 .03 .00 .01 .00 .13 .09rudeness .03 .00 .92 .00 .01 .02 .02 .00embarrassment .04 .08 .05 .69 .00 .11 .01 .02confidence .01 .04 .02 .01 .82 .01 .01 .09disbelief .05 .03 .02 .02 .05 .82 .00 .02formality .02 .34 .02 .01 .03 .03 .46 .10persuading .03 .05 .01 .00 .05 .03 .01 .82Table 3: Confusion matrix for the human participants,where the majority of participants agreed on a message?sintended social information and at least two participantslabeled the message.
The rows represent the intended so-cial information for a message while the columns repre-sent the labeled social information, averaged over mes-sages and participants.crease the accuracy of social information identifica-tion in text.We can observe similar trends to what we saw inTable 2, in many cases sharpened from what theywere previously.
People are still more likely to iden-tify messages expressing rudeness (p = .92) andconfidence (p = .82), though they are also now morelikely to accurately identify persuading (p = .82).The ability to identify politeness (p = .71) and em-barrassment (p = .69) has also improved, thougha polite message can still be mistaken for a formalmessage (p = .13).
Formality (p = .46) and de-ception (p = .45) remain more difficult to iden-tify, with formal messages mistaken for politeness(p = .34) and deceptive messages mistaken for per-suading (p = .21) and rudeness (p=.10) 5.
Note,however, that messages of disbelief and persuad-ing are now rarely mistaken for deceptive messages(p = .05 and p = .03, respectively).
It is likelythen that the confusions arising in this data set aremore representative of the actual confusion humansencounter when perceiving these social information5We note that people?s precision on deceptive messages washigher: 0.67.
That is, when they labeled a message as deceptive,it was deceptive 2/3 of the time.
However, the probabilities inTable 3 represent deceptive message recall, i.e., how well theywere able to label all deceptive messages as deceptive.types.Identifying messages likely to be misperceived byhumans is useful for two reasons.
First, from a cog-nitive standpoint, we can identify what features ofthose messages are the source of the confusion ifthe messages are consistently misperceived, whichtells us what linguistic cues humans are (mistakenly)keying into.
This then leads to designing better ma-chine learning algorithms that do not key into thosemisleading cues.
Second, this aids the design of cog-nitive systems that predict how a message is likelyto be interpreted by humans, and can warn a humanreader if a message?s intent is likely to be interpretedincorrectly.3 Training a classifier with the databaseTo demonstrate the utility of the created database fordeveloping computational approaches to social in-formation identification in text, we applied a SparseMultinomial Logistic Regression (SMLR) classifier(Krishnapuram et al 2005) to the the subset of mes-sages where two or more participants labeled themessage and more than 50% of the participants per-ceived the intended social information type.
Thissubset consisted of 624 messages (these messagesmake up the messages in the diagonals of table 3).While we realize that there are many other machinelearning techniques that could be used, we thoughtthis classifier would be a reasonable one to startwith to demonstrate the utility of the database.
As afirst pass measure for identifying diagnostic linguis-tic cues, we examined a number of fairly shallowfeatures:?
unigrams, bigrams, and trigrams?
number of word types, word tokens, and sen-tences?
number of exclamation marks, questionsmarks, and punctuation marks?
average sentence and word length?
word type to word token ratio?
average word log frequency for words appear-ing more than once in the database76The use of shallow linguistic features seemed areasonable first investigation as prior research in-volving linguistic cues for identifying informationin text has often used word-level cues.
For exam-ple, positive and negative affect words (e.g., excel-lent vs. poor) have been used in sentiment analysisto summarize whether a document is positive or neg-ative (Turney 2002, Pang, Lee, and Vaithyanathan2002, among others).
In deception detection re-search, informative word-level cues include count-ing first and third person pronoun usage (e.g., me vs.them) (Anolli, Balconi, and Ciceri 2002), and notingthe number of ?exception words?
(e.g., but, except,without) (Gupta and Skillicorn 2006).
In addition,informative shallow text properties have also beenidentified (Zhou et al 2004), such as (a) number ofverbs, words, noun phrases, and sentences, (b) aver-age sentence and word length, and (c) word type toword token ratio.The SMLR classifier model was trained to pro-duce the label (one of eight) corresponding to thegenerated social information using all the text fea-tures as input.
Using a 10-fold cross-validation pro-cedure, the model was trained on 90% of the mes-sages and tested on the remaining 10%.
The sparseclassifier favors a small number of features in theregression solution and sets the weight of a largefraction of features to zero.
Some of the non-zeroweights learned by the model for each social infor-mation type are listed below (though each type hasother features that also had non-zero weights).
Posi-tive weights indicate positive correlations while neg-ative weights indicate negative correlations.
Cuesthat are negatively correlated are italicized.
Bigramsand trigrams are indicated by + in between the rele-vant words (e.g., no+way).
BEGIN and END indi-cate the beginning and the end of the message, re-spectively.?
deception: #-of-question-marks (-0.5),actually (1.4), at+all (0.6), if (0.8), me (-0.9),my (-0.2), not (1.6), of+course (1.1), trying+to(0.8), you+END (1.0)?
politeness: BEGIN+please (2.1), help (2.1),may+i (1.2), nice (2.3), nicely+END (1.1),so+sorry (1.5), would+you+like (1.0)?
rudeness: annoying (1.2), good (-1.1), great(-0.6), hurry+up (1.0), loud (2.7), mean (0.9),pretty (-2.0), ugly (1.6)?
embarrassment: BEGIN+oh (2.0),can?t+believe (1.0), can?t+believe+i (0.6),forgot (2.1), good (-.9), my (2.0), oh (1.1)?
confidence: i+believe (2.1), i+know (2.4),positive (3.5), really+good (2.9), sure (3.3),the+best (2.5), think (-0.8)?
disbelief: #-of-question-marks (2.4),BEGIN+are (3.8), like (-0.6), never (1.4),no+way (3.0), shocked (1.1), such+a (1.1)?
formality: #-of-exclamation-marks (-0.8),BEGIN+excuse (2.1), don?t (-0.8), miss (4.1),mr (3.7), please (2.7), sir (5.1), very+nice (1.0)?
persuading: BEGIN+if+you (2.3), buy (1.3),come (3.5), have+to (1.6), we+can (1.3),would+look (2.9), you+should (3.4)Some of the feature-label correlations discoveredby the model fit with our intuitions about the so-cial information types.
For example, deceptive mes-sages are negatively correlated with some of thefirst person pronouns (me, my), in accordance withAnolli, Balconi, and Ciceri (2002)?s results.
Sev-eral polite and formal words appear correlated withpolite and formal messages respectively (may+i,nice, so+sorry, would+you+like; BEGIN+excuse,miss, mr, sir), and formal messages tend not to in-clude exclamation points.
Negative words tend tobe associated with rude messages (annoying, loud,mean, ugly), while positive words tend to be asso-ciated with confident messages (really+good, sure,the+best).
Messages conveying disbelief tend tohave more question marks and contain expressionsof surprise (never, no+way, shocked), and persua-sive messages tend to contain coercive expressions(come, have+to, you+should).
As this is a relativelysmall data set, these cues are unlikely to be defini-tive ?
however, it is promising for the approach as awhole that the classifier can identify these cues usingfairly shallow linguistic analyses.We can also examine the classifier?s ability to la-bel messages, given the features it has deemed di-agnostic for each social information type (i.e., thosefeatures it gave non-zero weight).
For each message77in the dataset, the classifier predicted what the in-tended social information type was.
A correct pre-diction for a message?s type matches the intendedtype for the message.
A confusion matrix for theclassifier based on the messages from the 624 mes-sage test set is shown in Table 4.
Overall, the clas-sifier was able to correctly label 59% of the mes-sages.
This is 12% less than humans were able tocorrectly label, but far better than chance perfor-mance (13%) and the performance of a simple al-gorithm that chooses the most frequent data type inthe training set (17%).The classifier shows some patterns similar to thehuman participants: (1) deception and formality areharder to detect than other social information types,(2) confidence and embarrassment are easier to de-tect than other social information types, and (3) for-mality is often mistaken for politeness (p = .26).However, some differences from the human partici-pants are that deception is often mistaken for rude-ness (p = .19) and politeness is often confused withrudeness and embarrassment, in addition to formal-ity (all p = .12).deceptionpolitenessrudenessembarrassmentconfidencedisbeliefformalitypersuadingdeception .36 .08 .19 .08 .08 .09 .06 .08politeness .05 .49 .12 .12 .05 .01 .12 .05rudeness .06 .06 .63 .04 .07 .07 .01 .07embarrassment .02 .01 .11 .76 .06 .03 .01 .00confidence .06 .01 .04 .08 .68 .02 .03 .08disbelief .08 .03 .08 .02 .09 .56 .02 .12formality .00 .26 .06 .03 .00 .06 .43 .15persuading .05 .06 .09 .03 .11 .03 .02 .61Table 4: Confusion matrix for the machine learning clas-sifier.
The rows represent the intended social informationfor a message while the columns represent the labeled so-cial information.As the classifier?s behavior was similar to hu-man behavior in some cases, and the classifier usedonly these shallow linguistic features to make itsdecision, this suggests that humans may be key-ing into some of these shallower linguistic featureswhen deciding a message?s social information con-tent.
Given this, a classifier trained on such linguis-tic features may be able to predict which messagesare likely to be ambiguous to humans.4 ConclusionWe have described a methodology using GWAPsto create a database containing messages labeledwith social information such as emotions, inten-tions, and attitudes, which can be valuable to theinformation extraction research community.
Hav-ing implemented this methodology on a small scale,we discovered that non-expert annotators were ableto identify the social information of interest fairlywell when their collective perceptions were com-bined.
However, we also noted that certain socialinformation types are easily confusable by humans.We also used the database created by the GWAPto investigate shallow linguistic cues to social in-formation in text and attempt to automatically la-bel messages as expressing particular social infor-mation.
The fact that the social information typeswe used in our GWAP can be identified automati-cally with some success suggests that these socialinformation types are useful to pursue, though ofcourse there are many other emotional states, atti-tudes, and intentions that could be explored in fu-ture work.
In addition, other classifiers, particularlythose using deeper-level properties like phrase struc-ture, may be able to identify more subtle cues tosocial information in text.
We also foresee extend-ing the GWAP methodology to create large-scaledatabases both in English and in other languages inorder to continue fostering the development of com-putational approaches to social information identifi-cation.AcknowledgmentsThis paper has benefited from discussion and ad-vice from Padhraic Smyth, Pierre Isabelle, and threeanonymous reviewers.
In addition, this work issupported by NSF grant BCS-0843896 to LP andCORCL grant MI 14B-2009-2010 to LP and MS.ReferencesAbbasi, A.
2007.
Affect intensity analysis of dark webforums.
Proceedings of Intelligence and Security In-formatics (ISI): 282-288.Agarwal, A., Biadsy, F., and Mckeown, K. 2009.
Con-textual Phrase-Level Polarity Analysis using Lexical78Affect Scoring and Syntactic N-grams.
Proceedingsof the 12th Conference of the European Chapter of theACL, Athens, Green: 24-32.Alm, C. O., Roth, D., and Sproat, R. 2005.
Emo-tions from text: Machine learning for text-based emo-tion prediction.
Proceedings of the Human Lan-guage Technology Conference and the Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP).Anolli, L., Balconi, M., and Ciceri, R. 2002.
De-ceptive Miscommunication Theory (DeMiT): A NewModel for the Analysis of Deceptive Communication.In Anolli, L., Ciceri, R. and Rivs, G.
(eds)., Say notto say: new perspectives on miscommunication.
IOSPress: 73-100.Dave, K., Lawrence, S., and Pennock, D. 2003.
Min-ing the peanut gallery: Opinion extraction and seman-tic classication of product reviews, Proceedings ofWWW: 519-528.Diab, M., Dorr, B., Levin, L., Mitamura, T., Passonneau,R., Rambow, O., and Ramshaw, L. 2009.
LanguageUnderstanding Annotation Corpus.
LDC, Philadel-phia.Fleiss, J. L. 1971.
Measuring nominal scale agreementamong many raters.
Psychological Bulletin 76(5):378382.Graff, D. 2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia.Greene, S. and Resnik, P. 2009.
More than Words:Syntactic Packaging and Implicit Sentiment.
HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the ACL, Boulder,Colorado: 503-511.Gupta, S. and Skillicorn, D. 2006.
Improving a TextualDeception Detection Model, Proceedings of the 2006conference of the Center for Advanced Studies on Col-laborative research.
Toronto, Canada.Kennedy, A. and Inkpen, D. 2006.
Sentiment clas-sication of movie reviews using contextual valenceshifters.
Computational Intelligence, 22: 110-125.Kosorukoff, A.
2001.
Human-based Genetic Algorithm.IEEE Transactions on Systems, Man, and Cybernetics,SMC-2001: 3464-3469.Krishnapuram, B., Figueiredo, M., Carin, L., andHartemink, A.
2005.
Sparse Multinomial Logistic Re-gression: Fast Algorithms and Generalization Bounds.IEEE Transactions on Pattern Analysis and MachineIntelligence, 27: 957-968.Lazer, D., Pentland, A., Adamic, L., Aral, S., Barabsi,A., Brewer, D., Christakis, N., Contractor, N., Fowler,J., Gutmann, M., Jebara, T., King, G., Macy, M., Roy,D., and Val Alstyne, M. 2009.
Computational SocialScience, Science, 323: 721-723.Nicolov, N., Salvetti, F., Liberman, M., and Martin, J.
H.(eds.)
2006.
AAAI Symposium on Computational Ap-proaches to Analysing Weblogs (AAAI-CAAW).
AAAIPress.Pang, B. and Lee, L. 2008.
Opinion Mining and Sen-timent Analysis.
Foundations and Trends in Informa-tion Retrieval 2(1-2): 1-135.Pang, B., Lee, L., and Vaithyanathan, S. 2002.
Thumbsup?
Sentiment Classification using Machine Learn-ing Techniques.
Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP): 79-86.Snow, R., O?Connor, B., Jurafsky, D., and Ng, A.
2008.Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Language Tasks.
Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP), 254-263.Steyvers, M., Lee, M., Miller, B., and Hemmer, P. 2009.The Wisdom of Crowds in the Recollection of OrderInformation.
In J. Lafferty, C. Williams (Eds.)
Ad-vances in Neural Information Processing Systems, 23,MIT Press.Subasic, P. and Huettner A.
2001.
A?ect analysis of textusing fuzzy semantic typing.
IEEE Transactions onFuzzy Systems, 9: 483-496.Turney, P. 2002.
Thumbs Up or Thumbs Down?
Seman-tic Orientation Applied to Unsupervised Classificationof Reviews.
Proceedings of the Association for Com-putational Linguistics (ACL): 417-424.von Ahn, L. 2006.
Games With A Purpose.
IEEE Com-puter Magazine, June 2006: 96-98.von Ahn, L. and Dabbish, L. 2004.
Labeling Im-ages with a Computer Game.
Proceedings of theSIGCHI Conference on Human Factors in ComputingSystems (Association for Computing Machinery, NewYork, 2004): 319-326.von Ahn, L., Kedia, M. and Blum, M. 2006.
Verbosity:A Game for Collecting Common-Sense Facts, In Pro-ceedings of the SIGCHI conference on Human Factorsin computing systems, Montral, Quebec, Canada.Wiebe, J.M., Wilson, T., Bruce, R., Bell, M., and Martin,M.
2004.
Learning subjective language.
Computa-tional Linguistics, 30: 277-308.Zhou, L., Burgoon, J., Nunamaker, J., and Twitchell, D.2004.
Automating linguistics-based cues for detect-ing deception in text-based asynchronous computer-mediated communication.
Group Decision and Nego-tiation, 13: 81-106.Zhou, L. and Sung, Y.
2008.
Cues to deception in on-line Chinese groups.
Proceedings of the 41st AnnualHawaii international Conference on System Sciences,146.
Washington, DC: IEEE Computer Society.79
