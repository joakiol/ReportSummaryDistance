Bootstrapping toponym classifiersDavid A. Smith and Gideon S. MannCenter for Language and Speech ProcessingComputer Science Department, Johns Hopkins UniversityBaltimore, MD 21218, USA{dasmith,gsm}@cs.jhu.eduAbstractWe present minimally supervised methods fortraining and testing geographic name disam-biguation (GND) systems.
We train data-drivenplace name classifiers using toponyms alreadydisambiguated in the training text ?
by suchexisting cues as ?Nashville, Tenn.?
or ?Spring-field, MA?
?
and test the system on textswhere these cues have been stripped out andon hand-tagged historical texts.
We experimenton three English-language corpora of varyingprovenance and complexity: newsfeed from the1990s, personal narratives from the 19th cen-tury American west, and memoirs and recordsof the U.S. Civil War.
Disambiguation accu-racy ranges from 87% for news to 69% forsome historical collections.1 Scope and Prior WorkWe present minimally supervised methods for trainingand testing geographic name disambiguation (GND) sys-tems.
We train data-driven place name classifiers usingtoponyms already disambiguated in the training text ?by such existing cues as ?Nashville, Tenn.?
or ?Spring-field, MA?
?
and test the system on text where thesecues have been stripped out and on hand-tagged histori-cal texts.As in early work with such named-entity recognitionsystems as Nominator (Wacholder et al, 1997), muchprevious work in GND has relied on heuristic rules (Ol-ligschlaeger and Hauptmann, 1999; Kanada, 1999) andsuch culturally specific and knowledge intensive tech-niques as postal codes, addresses, and telephone num-bers (McCurley, 2001).
In previous work, we used theheuristic technique of calculating weighted centroids ofgeographic focus in documents (Smith and Crane, 2001).Sites closer to the centroid were weighted more heavilythan sites far away unless they had some countervailingimportance such as being a world capital.News texts offer two principal advantages for boot-strapping geocoding applications.
Just as journalisticstyle prefers identifying persons by full name and title onfirst mention, place names, when not of major cities, areoften first mentioned followed by the name of their state,province, or country.
Even if a toponym is strictly unam-biguous, it may still be labelled to provide the reader withsome ?backoff?
recognition.
Although there is only oneplace in the world named ?Wye Mills?, an author wouldstill usually append ?Maryland?
to it so that a reader whodoesn?t recognize the place name can still situate it withina rough area.
In any case, the goal is to generalize fromthe kinds of contexts in which writers use a disambiguat-ing label to one in which they do not.Since news stories also tend to be relatively short andfocused on a single topic, we can also exploit the heuristicof ?one sense per discourse?
: unless otherwise indicated?
e.g., by a different state label ?
subsequent mentionsof the toponym in the story can be identified with the first,unambiguous reference.
News stories often also have to-ponyms in their datelines that are disambiguated.
Ournews training corpus consists of two years (1989-90) ofAP wire and two months (October, November, 1998) ofTopic Detection and Tracking (TDT) data.
The test setis the December, 1998, TDT data.
See table 1 for thenumbers of toponyms in the corpora.In contrast to news texts, historical documents exhibita higher density of geographical reference and level ofambiguity.
To test the performance of our minimally-supervised classifiers in a particularly challenging do-main, we test it on a corpus of historical documents whereall place names have been marked and disambiguated.
Aswith news texts, we initially train and test our classifierson raw text.
The range of geographic reference in thesetexts is somewhat similar to American news text: the cor-pus comprises the Personal Memoirs of Ulysses S. Grantand two nineteenth-century books of travel about Califor-nia and Minnesota from the Library of Congress?
Amer-ican Memory project.1 In all, we thus have about 600pages of tagged historical text.2 Experimental SetupDividing the corpora in training and test data, we trainNaive Bayes classifiers on all examples of disambiguatedtoponyms in the training set.
Although it is not uncom-mon for two places in the same state, for example, toshare a name, we define disambiguation for purposes ofthese experiments as finding the correct U.S. state or for-eign country.
This asymmetry is reflected in U.S. newsand historical text of the training data, where toponymsare specified by U.S. states or by foreign countries.
Wethen run the classifiers on the test text with disambiguat-ing labels, such as state or country names that immedi-ately follow the city name, removed.Since not all toponyms in the test set will have beenseen in training, we also train backoff classifiers to guessthe states and countries related to a story.
If, for exam-ple, we cannot find a classifier for ?Oxford?, but cantell that a story is about Mississippi, we will still beable to disambiguate.
We use a gazetteer to restrict theset of candidate states and countries for a given placename.
In trying to disambiguate ?Portland?, we wouldthus consider Oregon, Maine, and England, among otheroptions, but not Maryland.
As in the word sense dis-ambiguation task as usually defined, we are classifyingnames and not clustering them.
This approach is prac-tical for geographic names, for which broad-coveragegazetteers exist, though less so for personal names (Mannand Yarowsky, 2003).
System performance is measuredwith reference to the naive baseline where each ambigu-ous toponym is guessed to be the most commonly oc-curring place.
London, England, would thus alwaysbe guessed rather than London, Ontario.
Bootstrappingmethods similar to ours have been shown to be compet-itive in word sense disambiguation (Yarowsky and Flo-rian, 2003; Yarowsky, 1995).3 Difficulty of the TaskOur ability to disambiguate place names should beweighed against the ease or difficulty of the task.
In aworld where most toponyms referred unambiguously toone place, we would not be impressed by near-perfectperformance.Before considering how toponyms are used in text, wecan examine the inherent ambiguity of place names in1Our annotated data also includes disambiguated texts ofHerodotus?
Histories and Caesar?s Gallic War, but toponyms inthe ancient (especially Greek) world do not show enough ambi-guity with personal names or with each other to be interesting.Corpus Train Test TaggedNews 80,366 1464 0Am.
Mem.
11,877 3782 342Civ.
War 59,994 787 4153Table 1: Experimental corpora with toponym counts inunsupervised training and test and hand-tagged test sec-tions.Continent % places % namesw/mult.
names w/mult.
placesN.
& Cent.
America 11.5 57.1Oceania 6.9 29.2South America 11.6 25.0Asia 32.7 20.3Africa 27.0 18.2Europe 18.2 16.6Table 2: Places with multiple names and names appliedto more than one place in the Getty Thesaurus of Geo-graphic Namesisolation.
The Getty Thesaurus of Geographic Names,with over a million toponyms, not only synthesizes manycontemporary gazetteers but also contains a wealth of his-torical names.
In table 2, we summarize for each conti-nent the proportion of places that have multiple namesand of names that can refer to more than one place.
Al-though these proportions are dependent on the namesand places selected for inclusion in this gazetteer, therelative rankings are suggestive.
In areas with morecopious historical records?such as Asia, Africa, andEurope?a place may be called by many names overtime, but individual names are often distinct.
With theincreasing tempo of settlement in modern times, how-ever, many places may be called by the same name, par-ticularly by nostalgic colonists in the New World.
Otherambiguities arise when people and places share names.Very few Greek and Latin place names are also personalnames.2 This is less true of Britain, where surnames(and surnames used as given names) are often taken fromplace names; in America, the confusion grows as numer-ous towns are named after prominent or obscure peo-ple.
What may be called a lack of imagination in themany 41 Oxfords, 73 Springfields, 91 Washingtons, and97 Georgetowns seems to plague the very area ?
NorthAmerica ?
covered by our corpora.If, however, one Washington or Portland predominatesin actual usage, things are not as bad as they seem.
At the2In Herodotus, for example, the only ambiguities betweenpeople and places are for foreign names such as ?Ninus?, thename used of Nineveh and of its mythical king.Corpus H(class) H(class|name) % ambig.News 6.453 0.241 12.71Am.
Mem.
4.519 0.525 18.81Civ.
War 4.323 0.489 18.49Table 3: Entropy (H) of the state/country classificationtaskvery worst, for a baseline system, one can always guessthe most predominant referent.
We quantify the level ofuncertainty in our corpora using entropy and average con-ditional entropy.
As stated above, we have simplified thedisambiguation problem to finding the state or country towhich a place belongs.
For our training corpora, we canthus measure the entropy of the classification and the av-erage conditional entropy of the classification given thespecific place name (table 3).
These entropies were cal-culated using unsmoothed relative frequencies.
The con-ditional entropy, not surprisingly, is fairly low, given thatthe percentage of toponyms that refer to more than oneplace in the training data is quite low.
Since training datado not perfectly predict test data, however, we have tosmooth these probabilities and entropy goes up.4 EvaluationWe evaluate our system?s performance on geographicname disambiguation using two tasks.
For the first task,we use the same sort of untagged raw text used in train-ing.
We simply find the toponyms with disambiguatinglabels ?
e.g., ?Portland, Maine?
?, remove the labels,and see if the system can restore them from context.
Forthe second task, we use texts all of whose toponyms havebeen marked and disambiguated.
The earlier heuristicsystem described in (Smith and Crane, 2001) was run onthe texts and all disambiguation choices were reviewedby a human editor.Table 4 shows the results of these experiments.
Thebaseline accuracy was briefly mentioned above: if a to-ponym has been seen in training, select the state or coun-try with which it was most frequently associated.
If a sitewas not seen, select the most frequent state or countryfrom among the candidates in the gazetteer.
The columnsfor ?seen?
and ?new?
provide separate accuracy rates fortoponyms that were seen in training and for those thatwere not.
Finally, the overall accuracy of the trained sys-tem is reported.
For the American Memory and Civil Warcorpora, we report results on the hand-tagged as well asthe raw text.Not surprisingly, in light of its lower conditional en-tropy, disambiguation in news text was the most accurate,at 87.38%.
Not only was the system accurate on news textoverall, but it degraded the least for unseen toponyms.The relative accuracy on the American Memory and CivilCorpus Baseline Seen New OverallNews 86.36 87.10 69.72 87.38Am.
Mem.
68.48 74.60 46.34 69.57(tagged) 80.12 91.74 10.61 77.19Civ.
War 78.27 77.23 33.33 78.65(tagged) 21.94 71.07 9.38 21.82Table 4: Disambiguation accuracy (%) on test corpora.Hand-tagged data were available for the American Mem-ory and Civil War corpora.War texts is also consistent with the entropies presentedabove.
The classifier shows a more marked degradationwhen disambiguating toponyms not seen in training.The accuracy of the classifier on restoring states andcountries in raw text is significantly, but not considerably,higher than the baseline.
It seems that many of toponymsmentioned in text might be only loosely connected to thesurrounding discourse.
An obituary, for example, mightmention that the deceased left a brother, John Doe, of Ar-lington, Texas.
Without tagging our test sets to mark suchtangential statements, it would be hard to weigh errors insuch cases appropriately.Although accuracy on the hand-tagged data from theAmerican memory corpus was better than for the rawtext, performance on the Civil War tagged data (Grant?sMemoirs) was abysmal.
Most of this error seems camefrom toponyms unseen in training, for with the accuracywas 9.38%.
In both sets of tagged text, moreover, the fullclassifier performed below baseline accuracy due to prob-lems with unseen toponyms.
The back-off state modelsare clearly inadequate for the minute topographical refer-ences Grant makes in his descriptions of campaigns.
In-cluding proximity to other places mentioned is probablythe best way to overcome this difficulty.
These problemssuggest that we need to more robustly generalize from thekinds of environments with labelled toponyms to thosewithout.5 ConclusionsLack of labelled training or test data is the bane of manyword sense disambiguation efforts.
For geographic namedisambiguation, we can extract training and test instancesfrom contexts where the toponyms are disambiguated bythe document?s author.
Tagging accuracy is quite good,especially for news texts, which have a lower entropyin the disambiguation task.
In real applications, how-ever, we do not usually need to disambiguate toponymsthat already have state or country labels; we need to dis-ambiguate unmarked place names.
We investigated theability of our classifier to generalize by evaluating onhand-corrected texts with all toponyms marked and dis-ambiguated.
The mixed results show that more gener-alization power is needed in our models, particularly theback-off models that handle toponyms unseen in training.In future work, we hope to try further methods fromWSD such as decision lists and transformation-basedlearning on the GND task.
In any event, we hope thatthis should improve the accuracy on toponyms seen intraining.
As for disambiguating unseen toponyms, incor-porating our prior work on heuristic proximity-base dis-ambiguation into the probabilistic framework would be anatural extension.
A fully hand-corrected test corpus ofnews text would also provide us with more robust evi-dence for classifier generalization.Evidence learned by classifiers to disambiguate to-ponyms includes the names of prominent people and in-dustries in a particular place, as well as the topics anddates of current and historical events, and the titles ofnewspapers (see figures 1 and 2).
In our news trainingcorpus, for example, Hawaii was most strongly collo-cated with ?lava?
and Poland with ?solidarity?
(case wasignored).
In addition to their use for GND, such associa-tions should be useful in their own right for event detec-tion (Smith, 2002), personal name disambiguation, andaugmenting the information in gazetteers.References[Kanada1999] Yasusi Kanada.
1999.
A method of geo-graphical name extraction from Japanese text for the-matic geographical search.
In Proceedings of theEighth International Conference on Information andKnowledge Management, pages 46?54, Kansas City,Missouri, November.
[Mann and Yarowsky2003] Gideon S. Mann and DavidYarowsky.
2003.
Unsupervised personal name disam-biguation.
In CoNLL, Edmonton, Alberta.
(to appear).
[McCurley2001] Kevin S. McCurley.
2001.
Geospatialmapping and navigation of the web.
In Proceedings ofthe Tenth International WWW Conference, pages 221?229, Hong Kong, 1?5 May.
[Olligschlaeger and Hauptmann1999] Andreas M. Ol-ligschlaeger and Alexander G. Hauptmann.
1999.Multimodal information systems and GIS: The In-formedia digital video library.
In Proceedings of theESRI User Conference, San Diego, California, July.
[Smith and Crane2001] David A. Smith and GregoryCrane.
2001.
Disambiguating geographic names ina historical digital library.
In Proceedings of ECDL,pages 127?136, Darmstadt, 4-9 September.
[Smith2002] David A. Smith.
2002.
Detecting andbrowsing events in unstructured text.
In Proceedingsof the 25th Annual ACM SIGIR Conference, pages 73?80, Tampere, Finland, August.
[Wacholder et al1997] Nina Wacholder, Yael Ravin, andMisook Choi.
1997.
Disambiguation of proper namesin text.
In Proceedings of the Fifth Conference onApplied Natural Language Processing, pages 202?208, Washington, DC, April.
Association for Compu-tational Linguistics.
[Yarowsky and Florian2003] David Yarowsky and RaduFlorian.
2003.
Evaluating sense disambiguation per-formance across diverse parameter spaces.
Journal ofNatural Language Engineering, 9(1).
[Yarowsky1995] David Yarowsky.
1995.
Unsuper-vised word sense disambiguation rivaling supervisedmehtods.
In Proceedings of the 33rd Annual Meet-ing of the Association for Computational Linguistics,pages 189?196.NASHVILLE , Tenn - Singer Marie Osmond willreceive the 1988 Roy Acuff Community ServiceAward from the Country Music Foundation.
She willbe honored for her work as national chairwoman ofthe Osmond Foundation ...
The honor is named for aGrand Ole Opry star known as ?the king of countrymusic?.NASHVILLE , Tenn - The home of country musicis singing the blues after the sale of its last locallyowned music publising company to CBS Records.Tree International Publishing, ranked as Billboardmagazine ?s No.
1 country music publisher forthe last 16 years, is being sold to New York-basedCBS for a reported $45 million to $50 million, TheTennessean reported today.NASHVILLE , Tenn - Country music entertainerJohnny Cash was scheduled to be released from Bap-tist Hospital Tuesday, two weeks after undergoingheart bypass surgery, a hospital spokeswoman saidMonday ...Figure 1: Documents with Dateline of Nashville, havingstrong collocation country musicPORTLAND, Ore - Federal court hearing on whetherto permit logging on timber tracts where northernspotted owl nests.GRANTS PASS, Ore - ... ?As more and more federallands are set aside for spotted owls and other typesof wildlife and recreation areas, the land availablefor perpetual commercial timber management de-creases?...SEATTLE - Interior Secretary Manuel Lujan saysfederal law should allow economic considerations tobe taken into account in deciding whether to protectspecies like the northern spotted owl...SAN FRANCISCO - Environmental groups can suethe government to try to stop logging of old-growth firnear spotted owl nests in western Oregon, a federalappeals court ruled Tuesday...PORTLAND, Ore - Environmentalists trying toprotect the northern spotted owl cheered a federaljudge?s decision halting logging on five timber tracts...WASHINGTON - Are the spotted owls that live in theancient forest of the Northwest really endangered orare they being victimized by the miniature radio trans-mitters that scientists use to track their movements?SEATTLE - A federal court extended a ban Thursdayon U.S Forest Servi ce plans to sell nearly 1 billionboard feet of ancient timber from nine nationa l forestsin two states where the northern spotted owl lives.Figure 2: A sample of new stories with the keyword spot-ted owl, most are Oregon/Washington
