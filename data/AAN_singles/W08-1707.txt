Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 49?56Manchester, August 2008Designing Testsuites for Grammar-based Systems in ApplicationsValeria de PaivaPalo Alto Research Center3333 Coyote Hill Rd.Palo Alto, CA 94304 USAvaleria.paiva@gmail.comTracy Holloway KingPalo Alto Research Center3333 Coyote Hill Rd.Palo Alto, CA 94304 USAthking@parc.comAbstractIn complex grammar-based systems, evensmall changes may have an unforeseeableimpact on overall system performance.
Re-gression testing of the system and its com-ponents becomes crucial for the grammarengineers developing the system.
As partof this regression testing, the testsuitesthemselves must be designed to accuratelyassess coverage and progress and to helprapidly identify problems.
We describea system of passage-query pairs dividedinto three types of phenomenon-based test-suites (sanity, query, basic correct).
Theseallow for rapid development and for spe-cific coverage assessment.
In addition,real-world testsuites allow for overall per-formance and coverage assessment.
Thesetestsuites are used in conjunction with themore traditional representation-based re-gression testsuites used by grammar engi-neers.1 IntroductionIn complex grammar-based systems, even smallchanges may have an unforeseeable impact onoverall system performance.1 Systematic regres-sion testing helps grammar engineers to trackprogress, and to recognize and correct shortcom-ings in linguistic rule sets.
It is also an essential toolc 2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1We would like to thank Rowan Nairn for his design andimplementation of the regression platform that runs these test-suites.
We would also like to thank the PARC Natural Lan-guage Theory and Technology group for their work with thesetestsuites and their comments on this paper.for assessing overall system status in terms of taskand runtime performance.As discussed in (Chatzichrisafis et al, 2007),regression testing for grammar-based systems in-volves two phases.
The first includes systematictesting of the grammar rule sets during their de-velopment.
This is the part of regression testingthat grammar engineers are generally most familiarwith.
The second phase involves the deploymentof the grammar in a system and the regression test-ing of the grammar as a part of the whole system.This allows the grammar engineer to see whetherchanges have any effect on the system, positive ornegative.
In addition, the results of regression test-ing in the system allow a level of abstraction awayfrom the details of the grammar output, which canease maintenance of the regression testsuites sothat the grammar engineers do not need to changethe gold standard annotation every time an interme-diate level of representation changes.In this paper, we focus on the design of testsuitesfor grammar-based systems, using a question-answering system as a model.
In particular, we areinterested in what types of testsuites allow for rapiddevelopment and efficient debugging.1.1 The Question-Answering SystemTo anchor the discussion, we focus on regressiontestsuites designed for a grammar-based question-answering system (Bobrow et al, 2007).
TheBridge system uses the XLE (Crouch et al, 2008)parser to produce syntactic structures and then theXLE ordered rewrite system to produce linguisticsemantics (Crouch and King, 2006) and abstractknowledge representations.
Abstract knowledgerepresentations for passages and queries are pro-cessed by an entailment and contradiction detec-tion system which determines whether the query is49entailed, contradicted, or neither by the passage.Entailment and contradiction detection betweenpassages and queries is a task well suited to regres-sion testing.
There are generally only two or threepossible answers given a passage and a query: en-tails, contradicts or neither (or in the looser case:relevant or irrelevant).
Wh-questions (section 5.1)receive a YES answer if an alignment is found be-tween the wh-word in the query and an appropriatepart of the passage representation; in this case, theproposed alignment is returned as well as the YESanswer.
This is particularly important for who andwhat questions where more than one entity in thepassage might align with the wh-word.From the standpoint of regression testing, twoimportant aspects of the question-answering appli-cation are:(1) The correct answer for a given pair is indepen-dent of the representations used by the systemand even of which system is used.
(2) The passage-query pairs with answers can beconstructed by someone who does not knowthe details of the system.The first aspect means that even drastic changes inrepresentation will not result in having to updatethe regression suites.
This contrasts sharply withregressions run against representative output whichrequire either that the gold standard be updated orthat the mapping from the output to that standard beupdated.
The second aspect means that externallydeveloped testsuites (e.g.
FraCaS (Cooper et al,1996), Pascal RTE (Sekine et al, 2007)) can eas-ily be incorporated into the regression testing andthat grammar engineers can rapidly add new test-suites, even if they do not have experience with theinternal structure of the system.
These aspects alsomean that such passage-query pairs can be usedfor cross-system comparisons of coverage (Bos,2008).1.2 Testsuite TypesIn the regression testsuites designed for thequestion-answering system, the passage-querypair testsuites are divided into two main types:those that focus on single phenomena (section 2)and those that use real-world passages (section3).
The phenomenon-based testsuites allow thegrammar engineer to track the behavior of thesystem with respect to a given construction, suchas implicativity, noun-noun compounds, temporalexpressions, or comparatives.
In contrast, thereal-world passages allow the grammar engineerto see how the system will behave when appliedto real data, including data which the system willencounter in applications.
Such sentences tend tostress the system in terms of basic performance(e.g.
efficiency and memory requirements forprocessing of long sentences) and in terms ofinteractions of different phenomena (e.g.
coordi-nation ambiguity interacting with implicativity).In addition to the passage-query pairs, the sys-tem includes regression over representations atseveral levels of analysis (section 4).
These arelimited in number, focus only on core phenomena,and are not gold standard representations but in-stead the best structure of the ones produced.
Theseare used to detect whether unintentional changeswere introduced to the representations (e.g.
newfeatures were accidentally created).2 Phenomenon SetsReal-world sentences involve analysis of multipleinteracting phenomena.
Longer sentences tend tohave more diverse sets of phenomena and hencea higher chance of containing a construction thatthe system does not handle well.
This can lead tofrustration for grammar engineers trying to trackprogress; fixing a major piece of the system canhave little or no effect on a testsuite of real-worldexamples.
To alleviate this frustration, we haveextensive sets of hand-crafted test examples thatare focused as much as possible on single phe-nomenon.
These include externally developed test-suites such as the FraCaS (Cooper et al, 1996) andHP testsuites (Nerbonne et al, 1988).
Focused test-suites are also good for quickly diagnosing prob-lems.
If all the broken examples are in the deverbaltestsuite, for example, it gives grammar engineersa good idea of where to look for bugs.The majority of the testsuites are organized bysyntactic and semantic phenomena and are de-signed to test all known variants of that phe-nomenon (see (Cohen et al, 2008) on the needto use testsuites designed to test system coverageas well as real-world corpora).
For the question-answering system, these include topics such asanaphora, appositives, copulars, negation, dever-bal nouns and adjectives, implicatives and factives,temporals, cardinality and quantifiers, compara-tives, possessives, context introducing nouns, andpertainyms.
These categories align with many of50those cited by (Bos, 2008) in his discussion of se-mantic parser coverage.
Some example passage-query pairs for deverbal nouns are shown in (3).
(3) a. P: Ed?s abdication of the throne was wel-come.Q: Ed abdicated the throne.A: YESb.
P: Ed?s abdication was welcome.Q: Ed abdicated.A: YESc.
P: Ed is an abdicator.Q: Ed abdicated.A: YESEach of the phenomena has three sets of test-suites associated with it.
Sanity sets (section 2.1)match a passage against itself.
The motivation be-hind this is that a passage should generally entailitself and that if the system cannot capture this en-tailment, something is wrong.
Query sets (sec-tion 2.2) match the passage against query versionsof the passage.
The simplest form of this is tohave a polarity question formed from the passage.More complex versions involve negative polarityquestions, questions with different adjuncts or ar-gument structures, and questions with synonyms orantonyms.
Basic correct sets (section 2.3) are se-lected passage-query pairs in which the system isknown to obtain the correct answer for the correctreason.
The idea behind these sets is that they canbe run immediately by the grammar engineer af-ter making any changes and the results should be100% correct: any mistakes indicates a problem in-troduced by the grammar engineer?s changes.2.1 Sanity SetsThe entailment and contradiction detection partof the system is tested in isolation by matchingqueries against themselves.
Some example sanitypairs from the copula testsuite are shown in (4).
(4) a. P: A boy is tall.Q: A boy is tall.A: YESb.
P: A girl was the hero.Q: A girl was the hero.A: YESc.
P: The boy is in the garden.Q: The boy is in the garden.A: YESd.
P: The boy is not in the garden.Q: The boy is not in the garden.A: YESNote that queries in the question-answering sys-tem do not have to be syntactically interrogative.This allows the sanity pairs to be processed bythe same mechanism that processes passage-querypairs with syntactically interrogative queries.The sanity check testsuites are largely composedof simple, hand-crafted examples of all the syntac-tic and semantic patterns that the system is knownto cover.
This minimal check ensures that at leastidentical representations trigger an entailment.2.2 Query SetsThe query sets form the bulk of the regressionsets.
The query sets comprise passages of the typesfound in the sanity sets, but with more complexqueries.
The simplest form of these is to form thepolarity question from the passage, as in (5).
Morecomplex queries can be formed by switching thepolarity from the passage to the query, as in (6).
(5) a. P: A boy is tall.Q: Is a boy tall?A: YESb.
P: A girl was the hero.Q: Was a girl the hero?A: YES(6) P: The boy is not in the garden.Q: Is the boy in the garden?A: NOTo form more complex pairs, adjuncts and ar-gument structure can be altered from the passageto the query.
These have to be checked carefullyto ensure that the correct answer is coded for thepair since entailment relations are highly sensitiveto such changes.
Some examples are shown in(7).
Alternations such as those in (7c) are crucialfor testing implicativity, which plays a key role inquestion answering.
(7) a. P: An older man hopped.Q: A man hopped.A: YESb.
P: John broke the box.Q: The box broke.A: YES51c.
P: Ed admitted that Mary arrived.Q: Mary arrived.A: YESA similar type of alteration of the query is tosubstitute synonyms for items in the passage, as in(8).
This is currently done less systematically in thetestsuites but helps determine lexical coverage.
(8) a. P: Some governments ignore historicalfacts.Q: Some governments ignore the facts ofhistory.A: YESb.
P: The boys bought some candy.Q: The boys purchased some candy.A: YESIn addition to the testsuites created by thequestion-answering system developers, the querysets include externally developed pairs, such asthose created for FraCaS (Cooper et al, 1996).These testsuites also involve handcrafted passage-query pairs, but the fact that they were developedoutside of the system helps to detect gaps in sys-tem coverage.
In addition, some of the FraCaSpairs involve multi-sentence passages.
Since thesentences in these passages are very short, they areappropriate for inclusion in the phenomenon-basedtestsuites.
Some externally developed testsuitessuch as the HP testsuite (Nerbonne et al, 1988) donot involve passage-query pairs but the same tech-niques used by the grammar engineers to create thesanity and the query sets are applied to these test-suites as well.2.3 Basic Correct SetsA subset of the query sets described above are usedto form a core set of basic correct testsuites.
Thesetestsuites contain passage-query pairs that the de-velopers have determined the system is answeringcorrectly for the correct reason.Since these testsuites are run each time the gram-mar engineer makes a change to the system be-fore checking the changes into the version controlrepository, it is essential that the basic correct test-suites can be run quickly.
Each pair is processedrapidly because the query sets are composed ofsimple passages that focus on a given phenomenon.In addition, only one or two representatives of anygiven construction is included in the basic correctset; that is, the sanity sets and query sets may con-tain many pairs testing copular constructions withadjectival complements, but only a small subset ofthese are included in the basic correct set.
In thequestion-answering system, 375 passage-querypairs are in the basic correct sets; it takes less thansix minutes to run the full set on standard machines.In addition, since the basic correct sets are dividedby phenomena, developers can first run those test-suites which relate directly to the phenomena theyhave been working on.Examining the basic correct sets gives anoverview of the expected base coverage of thesystem.
In addition, since all of the pairs areworking for the correct reason when they areadded to the basic correct set, any breakage is asign that an error has been introduced into thesystem.
It is important to fix these immediately sothat grammar engineers working on other parts ofthe system can use the basic correct sets to assessthe impact of their changes on the system.3 Real-world SetsThe ultimate goal of the system is to work on real-world texts used in the application.
So, tests ofthose texts are important for assessing progress onnaturally occurring data.
These testsuites are cre-ated by extracting sentences from the corpora ex-pected to be used in the run-time system, e.g.
news-paper text or the Wikipedia.2 Queries are then cre-ated by hand for these sentences.
Once the systemis being used by non-developers, queries posed bythose users can be incorporated into the testsuites toensure that the real-world sets have an appropriaterange of queries.
Currently, the system uses a com-bination of hand-crafted queries and queries fromthe RTE data which were hand-crafted, but not bythe question-answering system developers.
Someexamples are shown in (9).
(9) a. P: The interest of the automotive industryincreases and the first amplifier project, afour-channel output module for the Ger-man car manufacturer, Porsche, is fin-ished.Q: Porsche is a German car manufacturer.A: YESb.
P: The Royal Navy servicemen being heldcaptive by Iran are expected to be freed to-2If the application involves corpora containing ungram-matical input (e.g.
email messages), it is important to includeboth real-world and phenomenon sets for such data.52day.Q: British servicemen detainedA: YESc.
P: ?I guess you have to expect this ina growing community,?
said MardelleKean, who lives across the street fromJohn Joseph Famalaro, charged in thedeath of Denise A. Huber, who was 23when she disappeared in 1991.Q: John J. Famalaro is accused of havingkilled Denise A. Huber.A: YESThese real-world passages are not generally use-ful for debugging during the development cycle.However, they serve to track progress over time,to see where remaining gaps may be, and to pro-vide an indication of system performance in appli-cations.
For example, the passage-query pairs canbe roughly divided as to those using just linguis-tic meaning, those using logical reasoning, thoserequiring plausible reasoning, and finally those re-quiring world knowledge.
Although the bound-aries between these are not always clear (Sekine etal., 2007), having a rough division helps in guidingdevelopment.4 Regression on RepresentationsThere has been significant work on regression test-ing of a system?s output representations (Nerbonneet al, 1988; Cooper et al, 1996; Lehmann et al,1996; Oepen et al, 1998; Oepen et al, 2002): de-signing of the testsuites, running and maintainingthem, and tracking the results over time.
As men-tioned in the previous discussion, for a complexsystem such as a question-answering system, hav-ing regression testing that depends on the perfor-mance of the system rather than on details of therepresentations has significant advantages for de-velopment because the regression testsuites do nothave to be redone whenever there is a change to thesystem and because the gold standard items (i.e.,the passage-query pairs with answers) can be cre-ated by those less familiar with the details of thesystem.However, having a small but representative setof banked representations at each major level ofsystem output has proven useful for detecting un-intended changes that may not immediately disturbthe passage-query pairs.3 This is especially the case3In addition to running regression tests against representa-with the sanity sets and the most basic query sets:with these the query is identical to or very closelyresembles the passage so that changes to the repre-sentation on the passage side will also be in the rep-resentation on the query side and hence may not bedetected as erroneous by the entailment and contra-diction detection.For the question-answering system, 1200 sen-tences covering basic syntactic and semantic typesform a testsuite for representations.
The best rep-resentation currently produced by the system isstored for the syntax, the linguistic semantics, andthe abstract knowledge representation levels.
Toallow for greater stability over time and less sen-sitivity to minor feature changes in the rule sets, itis possible to bank only the most important featuresin the representations may, e.g.
the core predicate-argument structure.
The banked representationsare then compared with the output of the systemafter any changes are made.
Any differences areexamined to see whether they are intentional.
Ifthey were intended, then new representations needto be banked for the ones that have changed (see(Rose?n et al, 2005) for ways to speed up this pro-cess by use of discrimants).
If the differences werenot intended, then the developer knows which con-structions were affected by their changes and canmore easily determine where in the system the er-ror might have been introduced.5 Discussion and ConclusionsThe testsuites discussed above are continually un-der development.
We believe that the basic ideasbehind these testsuites should be applicable toother grammar-based systems used in applications.The passage-query pairs are most applicable toquestion-answering and search/retrieval systems,but aspects of the approach can apply to other sys-tems.Some issues that remain for the testsuites dis-cussed above are extending the use of wh-questionsin passage-query pairs, the division between devel-opment and test sets, and the incorporation of con-text into the testing.5.1 Wh-questionsThe testsuites as described have not yet been sys-tematically extended to wh-questions.
The querytions, the syntax, semantics, and abstract knowledge represen-tation have type declarations (Crouch and King, 2008) whichhelp to detect malformed representations.53sets can be easily extended to involve some substi-tution of wh-phrases for arguments and adjuncts inthe passage, as in (10).
(10) a. P: John broke the box.Q: Who broke the box?b.
P: John broke the box.Q: What did John break?c.
P: John broke the box.Q: What broke?d.
P: John broke the box.Q: What did John do?e.
P: We went to John?s party last night.Q: Who went to John?s party?There is a long-standing issue as to how to eval-uate responses to wh-questions (see (Voorhees andTice, 2000a; Voorhees and Tice, 2000b) and theTREC question-answering task web pages for dis-cussion and data).
For example, in (10a) most peo-ple would agree that the answer should be John, al-though there may be less agreement as to whetherJohn broke the box.
is an appropriate answer.
In(10b) and (10c) there is an issue as to whether theanswer should be box or the box and how to assesspartial answers.
This becomes more of an issue asthe passages become more complicated, e.g.
withheavily modified nominals that serve as potentialanswers.
While for (10d) the passage is a good an-swer to the question, for (10e) presumably the an-swer should be a list of names, not simply ?we?.Obtaining such lists and deciding how completeand appropriate they are is challenging.
Since mostquestion-answering systems are not constrained topolarity questions, it is important to assess per-formance on wh-questions as the system develops.Other, even more complicated questions, for exam-ple how to questions are also currently out of thescope of our testsuites.5.2 Development vs. TestingFor development and evaluation of systems, test-suites are usually divided into development sets,which the system developers examine in detail, andtest sets, which represent data unseen by the de-velopers.4 To a limited extent, the real-world sets4The usual division is between training, development, andtest sets, with the training set generally being much larger thanthe development and test sets.
For rule based systems, thetraining/development distinction is often irrelevant, and so aserve as a form of test set since they reflect the per-formance of the system on real data and are of-ten not examined in detail for why any given pairfails to parse.
However, the testsuites describedabove are all treated as development sets.
There areno reserved phenomenon-based testsuites for blindtesting of the system?s performance on each phe-nomenon, although there are real-world testsuitesreserved as test sets.If a given testsuite was created all at once, a ran-dom sampling of it could be held out as a test set.However, since there are often only a few pairs perconstruction or lexical item, it is unclear whetherthis approach would give a fair view of system cov-erage.
In addition, for rule-based systems suchas the syntax and semantics used in the question-answering system, the pairs are often constructedbased on the rules and lexicons as they were beingdeveloped.
As such, they more closely match thecoverage of the system than if it were possible torandomly select such pairs from external sources.As a system is used in an application, a test set ofunseen, application-specific data becomes increas-ingly necessary.
Such sets can be created from theuse of the application: for example, queries andreturned answers with judgments as to correctnesscan provide seeds for test sets, as well as for ex-tending the phenomenon-based and real-world de-velopment testsuites.5.3 ContextThe real sentences that a question-answering sys-tem would use to answer questions appear in alarger textual and metadata context.
This contextprovides information as to the resolution of pro-nouns, temporal expressions such as today and thismorning, ellipsis, etc.
The passage-query pairs inthe testsuites do not accurately reflect how well thesystem handles the integration of context.
Smalltwo sentence passages can be used to, for example,test anaphora resolution, as shown in (11).
(11) P: Mary hopped.
Then, she skipped.Q: Did Mary skip?A: YESEven in this isolated example, the answer can beconstrued as being UNKNOWN since it is possible,although unlikely, that she resolves to some otherentity.
This type of problem is pervasive in usingdistinction is made between those sets used in the developmentof the system and those unseen sets used to test and evaluatethe system?s performance.54simple passage-query pairs for system regressiontesting.A further issue with testing phenomena linked tocontext, such as anaphora resolution, is that theyare usually very complex and can result in signifi-cant ambiguity.
When used on real-world texts, ef-ficiency can be a serious issue which this type ofmore isolated testing does not systematically ex-plore.
As a result of this, the anaphora testsuitesmust be more carefully constructed to take advan-tage of isolated, simpler pairs when possible butto also contain progressively more complicated ex-amples that eventually become real-world pairs.5.4 Summary ConclusionsIn complex grammar-based systems, even smallchanges may have an unforeseeable impact on sys-tem performance.
Regression testing of the systemand its components becomes crucial for the gram-mar engineers developing the system.A key part of regression testing is the testsuitesthemselves, which must be designed to accuratelyassess coverage and progress and to help to rapidlyidentify problems.
For broad-coverage grammars,such as those used in open domain applications likeconsumer search and question answering, testsuitedesign is particularly important to ensure adequatecoverage of basic linguistic (e.g.
syntactic and se-mantic) phenomena as well as application specificphenomena (e.g.
interpretation of markup, incor-poration of metadata).We described a system of passage-query pairsdivided into three types of phenomenon-based test-suites (sanity, query, basic correct).
These allowfor rapid development and specific coverage as-sessment.
In addition, real-world testsuites allowfor overall performance and coverage assessment.More work is needed to find a systematic way toprovide ?stepping stones?
in terms of complexitybetween phenomenon-based and real-world test-suites.These testsuites are used in conjunction with themore traditional representation-based regressiontestsuites used by grammar engineers.
Theserepresentation-based testsuites use the samephenomenon-based approach in order to assesscoverage and pinpoint problems as efficiently aspossible.ReferencesBobrow, Daniel G., Bob Cheslow, Cleo Condoravdi,Lauri Karttunen, Tracy Holloway King, RowanNairn, Valeria de Paiva, Charlotte Price, and An-nie Zaenen.
2007.
PARC?s bridge and ques-tion answering system.
In King, Tracy Hollowayand Emily M. Bender, editors, Grammar Engineer-ing Across Frameworks, pages 46?66.
CSLI Publica-tions.Bos, Johan.
2008.
Let?s not argue about semantics.
InProceedings of LREC.Chatzichrisafis, Nikos, Dick Crouch, Tracy HollowayKing, Rowan Nairn, Manny Rayner, and Mari-anne Santaholma.
2007.
Regression testingfor grammar-based systems.
In King, Tracy Hol-loway and Emily M. Bender, editors, Proceedingsof the Grammar Engineering Across Frameworks(GEAF07) Workshop, pages 128?143.
CSLI Publica-tions.Cohen, K. Bretonnel, William A. Baumgartner Jr., andLawrence Hunter.
2008.
Software testing and thenaturally occurring data assumption in natural lan-guage processing.
In Software Engineering, Testing,and Quality Assurance for Natural Language Pro-cessing, pages 23?30.
Association for ComputationalLinguistics.Cooper, Robin, Dick Crouch, Jan van Eijck, ChrisFox, Josef van Genabith, Jan Jaspars, Hans Kamp,David Milward, Manfred Pinkal, Massimo Poesio,and Steve Pulman.
1996.
Using the framework.FraCas: A Framework for Computational Semantics(LRE 62-051).Crouch, Dick and Tracy Holloway King.
2006.
Seman-tics via f-structure rewriting.
In Butt, Miriam andTracy Holloway King, editors, LFG06 Proceedings,pages 145?165.
CSLI Publications.Crouch, Dick and Tracy Holloway King.
2008.
Type-checking in formally non-typed systems.
In SoftwareEngineering, Testing, and Quality Assurance for Nat-ural Language Processing, pages 3?4.
Associationfor Computational Linguistics.Crouch, Dick, Mary Dalrymple, Ron Ka-plan, Tracy King, John Maxwell, and PaulaNewman.
2008.
XLE documentation.http://www2.parc.com/isl/groups/nltt/xle/doc/.Lehmann, Sabine, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein,Kirsten Falkedal, Frederik Fouvry, Dominique Esti-val, Eva Dauphin, Herve?
Compagnion, Judith Baur,Lorna Balkan, and Doug Arnold.
1996.
TSNLP ?Test Suites for Natural Language Processing.
In Pro-ceedings of COLING 1996.Nerbonne, John, Dan Flickinger, and Tom Wasow.1988.
The HP Labs natural language evaluationtool.
In Proceedings of the Workshop on Evaluationof Natural Language Processing Systems.55Oepen, Stephan, Klaus Netter, and Judith Klein.
1998.TSNLP ?
Test Suites for Natural Language Process-ing.
In Nerbonne, John, editor, Linguistic Databases,pages 13?36.
CSLI.Oepen, Stephan, Dan Flickinger, Kristina Toutanova,and Chris D. Manning.
2002.
LinGO Redwoods.
arich and dynamic treebank for HPSG.
In Proceed-ings of The First Workshop on Treebanks and Lin-guistic Theories, pages 139?149.Rose?n, Victoria, Koenraad de Smedt, Helge Dyvik, andPaul Meurer.
2005.
TREPIL: Developing methodsand tools for multilevel treebank construction.
InProceedings of The Fourth Workshop on Treebanksand Linguistic Theories.Sekine, Satoshi, Kentaro Inui, Ido Dagan, Bill Dolan,Danilo Giampiccolo, and Bernardo Magnini, editors.2007.
Proceedings of the ACL-PASCAL Workshop onTextual Entailment and Paraphrasing.
Associationfor Computational Linguistics, Prague, June.Voorhees, Ellen and Dawn Tice.
2000a.
Building aquestion answering test collection.
In Proceedingsof SIGIR-2000, pages 200?207.Voorhees, Ellen and Dawn Tice.
2000b.
The TREC-8question answering track evaluation.
In Proceedings8th Text REtrieval Conference (TREC-8), pages 83?105.56
