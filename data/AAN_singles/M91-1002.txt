MUC-3 EVALUATION METRIC SNancy Chinchor, Ph .D .Science Applications International Corporatio n10260 Campus Point Drive, M/S 1 2San Diego, CA 9212 1(619) 458-2728INTRODUCTIONPurpos eThe MUC-3 evaluation metrics are measures of performance for the MUC- 3template fill task.
Obtaining summary measures of performance necessitates the los sof information about many details of performance .
The utility of summary measuresfor comparison of performance over time and across systems should outweigh thi sloss of detail .
The template fill task is complex because of the varying nature of th efills for each slot and the interdependencies of the slots .
The evaluation metrics usedin MUC-3 were adapted from traditional measures in information retrieval and signa lprocesing and were still evolving to fit the more complex data extraction task of MUC-3 when the evaluation was performed .
The scoring of the template fill task and th ecalculation of the metrics used in MUC-3 will be described here .
This description i smeant to assist in the analysis of the MUC-3 results and in the further evolution ofthe evaluation metrics .Metric sThe measures of performance chosen for use in MUC-3 were recall, precision ,fallout, and overgeneration .Recall, precision, and fallout were adapted based ontheir use in information retrieval .
Overgeneration was developed as a measure forMUC-3 .
Recall is a measure of the completeness of the template fill .
Precision is ameasure of the accuracy of the fill .
Fallout is a measure of the false alarm rate fo rthe slots which can be filled from finite sets of slot fillers .Overgeneration is ameasure of spurious generation .These measures will be described in greater detai lbelow .SCORE REPORTA semi-automated scoring system was developed for MUC-3 .The scoringsystem displayed the answer key templates, the response templates, and the message susing a flexibly customized emacs interface .
During scoring, the user was asked toenter the score for displayed mismatches between the key and the respons etemplates .
Fills could generally be scored as matches, partial matches, or mismatches .Depending on the type of slot fill, the scoring system may or may not have allowe dfull credit to be given .The interactive scoring was carried out following well -defined scoring guidelines .Depending on the scoring guidelines, full, partial, or n ocredit may have been allowed for each mismatch .After the interactive scoring wa scomplete, the scoring system produced an official score report containing templateby template score reports and a summary score report for the official record .
Asample summary score report produced for human comparison against the keyappears in Figure 1 .
The following sections discuss the contents of the score report .17* * * TOTAL SLOT SCORES * * *SLOTPOS ACTICOR PAR INCIICR IPAISPU MIS NONIREC PRE OVG FALtemplate-id118 1151114001 001 14 391 97 991incident-date114 1101 90 10 101 .31 101 0441 83 860incident-type118 1141112111 011 0401 95 9900category90 1091 88001 001 21271 98 81 19 14indiv-perps106 611 59021 1001 0 45 501 56 970org-perps71 681 58011 1501 9 12 481 82 85 1 3perp-confidence71 681 56121 1211 9 12 481 80 83 132phys-target-ids59 571 54301 1431 02 771 94 970phys-target-num41 411 39021 001 00 771 95 950phys-target-types59 571 52411 1141 02 771 92 9500human-target-ids145 1311129201 3321 2 14 231 90 992human-target-num94 881 79621 061 17 231 87 931human-target-types145 1311126231 2421 2 14 231 88 9720target-nationality35 191 17201 321 0 16 1031 51 9500instrument-types25 221 16101 001 58 881 66 75 230incident-location118 1131 88 2411 011 0501 85 880phys-effects41 441 37301 831 41 891 94 8890human-effects56 541 43221 1021 89 811' 78 81 151MATCHED ONLY1464 140211257 61 271171 371 62 119 8261 88 924MATCHED/MISSING1506 140211257 61 271171 371 62 161 8571 85 924ALL TEMPLATES1506 142011257 61 271171 371 80 161 8611 85 916SET FILLS ONLY640 6181547 1691 68 151 49 68 5161 87 9080Figure 1 : Summary Score ReportScoring Categorie sIndividual slot fills in the response were scored as correct, partially correct ,incorrect, noncommittal, spurious, or missing.
A response was correct if it was th esame as the key, partially correct if it partially approximated the key, and incorrec tif it was not the same as the key .
If the key and response were both blank, th eresponse was scored as noncommittal .
If the key was blank but the slot was filled, theresponse was scored as spurious .
If the response was blank and the key was not, th eresponse was scored as missing.
Figure 2 summarizes the scoring categories relatingthem to the corresponding columns in the score report .Category Criteria ColumnCorrect response = key CORPartial response= key PARIncorrect response ~ key INCNoncommittal keyand response are both blank NONSpurious key is blank and response is not SPUMissing response is blank and key is not MISFigure 2 :Scoring Categories1 8The summary score report rows show the totals for each of the categories ove rall templates.
The slots are listed on the left hand side and the totals for each slot ove rall templates are given in the labeled columns .For example, the total number o fphysical targets correctly identified was 54 .
The number appears in the phys-target-ids row and the COR column of the summary score report .
Note that the bottom fourrows of the score report are not slot scores but rather global summary rows describe din a later section .During scoring, the scoring system automatically scored matches as correc tand some partially matching hierarchically organized items as partially correct .However, many of the mismatches were interactively scored by the user .
To reflectthe number of items interactively scored as correct or partially correct, two column slabeled ICR and IPA were provided .The first two columns in the score report contain the number of possible slo tfills (POS) and the actual number of slot fills (ACT) .
The number of possible slot fill sis the number of slots fills in the key plus the number of optional slot fills in the ke ythat were matched in the response .
The number of possible slot fills for each systemdiffers depending on the optional fills given by the system .
The number of actualfills given is the number of slot fillers in the response .
The numbers in the possibleand actual columns are used to calculate the metrics .Calculation of Metric sThe metrics were calculated for each slot and for the summary rows .
Thecalculations were based on information in the columns of the score report as well a son some tallies kept internally by the scoring system .
The first three metrics show nin the score report are recall, precision, and overgeneration .These were calculate dfor each slot and were based on information contained in the score report .Recall is a measure of completeness and was calculated as follows .correct + (partial x 0.5)possibleFor example, recall for the human-target-ids slot was calculated as follows .REC COR +(PAR x 0 .5)POS_ 129 + (2 x 0 .5 )145130145= 0.90recall =1 9Precision is a measure of the accuracy of the attempted fills and was calculate das follows .correct + ( partial x 0 .5 )precision =actualFor example, the precision for the phys-target-num slot was calculated a sfollows .PRE=COR + ( PAR X 0 .5)ACT39 + ( 0 x 0 .5)4 13 94 1= 0.95Overgeneration is a measure of spurious generation and was calculated a sfollows .overgeneration = spuriou sFor example, the amount of overgeneration in the category slot was calculate das follows .OVG = ACT'2 1109= 0 .1 9Fallout is a measure of the false alarm rate .
The number of false alarms couldonly be measured for slots for which we knew the number of possible incorrectresponses .A subset of the slots in the template fill task were filled from finite sets .The rest of the slots are filled from possibly infinite sets .Fallout measures werecalculated for the finite set fill slots as follows .fallout = Incorrect + spuriouspossible incorrectwhere "possible incorrect" is the number of possible incorrect answers which coul dbe given in the response .
The number of possible incorrect is not shown in the scor ereport but a tally is kept internally by the scoring system .
The method for keepin gthis tally of possible incorrect has evolved during the course of the evaluation .actua l20In order to describe this evolution, a simple calculation of fallout for a singl eslot in a single template will be given .The instrument type slot has 16 possibl efillers .If the key contains the filler GUN and the response contains the fille rGRENADE, then fallout would b eFAL =	 INC +SPUpossible incorrec t11 5= 0 .07The number of possible incorrect is the cardinality of the set of possibl eanswers minus the number correct in the key which is 16 - 1, or 15 .In phase one, the fallout measure assumed that the system was essentiall ychoosing a subset of the finite set of possible fills when it gave a response .
Forexample, if the key for the instrument type slot contained GUN and GRENADE and theresponse contained BOMB, GRENADE, and CUTTING DEVICE, the phase one fallout woul dbeFAL =	 INC + SPUpossible incorrec t1 +116 - 221 4= 0.14The number of possible incorrect was the cardinality of the set minus the tota lnumber of slot fills given in the key .During phase two, it was noticed that this simple approach to fallout was infact erroneous for several reasons.
Some finite set slots allowed multiple uses of setmembers due to cross-referencing requirements .
For example, the slot fill CIVILIA Nmight be used multiple times in specifying the human target type for differen thuman targets .CIVILIAN: "MARIO FLORES "CIVILIAN : "JOSE RODRIGUEZ "Further complications arose when alternatives were given in the key for eac hsuch slot fill .
In order to solve all of these problems, the calculation of the possiblecorrect for the slot fills was revised to coincide more closely with the calculation use din information retrieval .Each separate slot fill item is now thought of as bein gchosen from the entire finite set of possible fill items .21In general, the number of possible incorrect is given by the followin gformula.E(IUI - Ikeyvall )keyva lwhere keyval stands for each of the key values including blanks, IUI is th ecardinality of the finite set U of possible slot fillers, and Ikeyvall is the number of keyvalues corresponding to the response .
If there are alternative key values for aresponse, then Ikeyvall > 1 .
If the key is blank, then there are no corresponding keyvalues and the contribution to the number of possible incorrect is the cardinality o fthe finite set .Returning to our example of instrument types with the key containing GU Nand GRENADE and the response containing BOMB, GRENADE, and CUTTING DEVICE ,fallout will be recalculated using the new method of determining the possibl eincorrect .
The number of possible incorrect is calculated by summing over the slotfills .
For GUN, the number of possible incorrect is the cardinality of the set, which i s16, minus the number of slot fill alternatives given in the key, which in this case i s1 .
For GRENADE, the number of possible incorrect is also 15 .
So the number ofpossible incorrect for this slot is 15 + 15, or 30 .
Since there is 1 incorrect and 1spurious response, fallout is 2/30, or 7% .
In phase one, fallout was 14% for this sameexample.If there are alternatives to a single slot fill in the key, the contribution to thenumber of possible incorrect by that slot fill is the cardinality of the finite set minu sthe number of alternatives given .
For example, if the key is GUN/GRENADE, thenumber of possible incorrect is 16 - 2, or 14 .If the key is blank, the number of possible incorrect is the cardinality of thefinite set .
For example, if the instrument type slot is blank in the key and th eresponse is GUN and GRENADE, then the fallout i sFAL =	 INC + SPUpossible incorrec t0 +21 621 6= 0 .1 3Notice that if the number of spurious responses is great enough, fallout can bemore than 100% .Meaning of Metric sRecall is a measure of completeness in the sense that it measures the amount o frelevant data extracted relative to the total available.
It is the true positive rate .
Amnemonic for recall can be constructed by imagining that you have been asked t oread the entire answer key, then fill in templates with all that you hav e22"remembered" or "recalled ."
Your score would be the total correctly recalled out o fthe total possible .Precision is the accuracy with which a system extracts data.
It is the amountof relevant data relative to the total put in by the system .
A mnemonic for precisionis to imagine that each time a system fills a slot it is throwing a dart at a dartboard .All of the bull's-eyes are correct .
Precision is a measure of the number of bull's-eyesrelative to the number of darts thrown .
Precision can also be described as thetendency of a system to avoid assigning bad fillers as it assigns more good fillers .Fallout is a measure of the false positive rate .
It is the tendency of the systemto assign incorrect fillers as the number of potential incorrect fillers increases .
So,for a mnemonic, if you are imagining the dartboard again, fallout measures th enumber of darts that "fall outside" of the bull's-eye relative to the size of the are aoutside the bull's-eye .Fallout can only be assigned for slots with a calculabl enumber of possible incorrect .Only some of the slots have a finite set of slot fill sassociated with them .The others have fills that come from potentially infinite set sand hence cannot be assigned a fallout score .Overgeneration is a measure of spurious generation .It is the amount ofspurious fillers assigned in relation to the total assigned .
Overgeneration wa scalculated to deter overgeneration as an approach to higher scores .
A mnemonic -forovergeneration can be constructed by imagining that required fills and extra fill sare in a box.
Overgeneration is represented by the area that the extra fills take up i nrelation to the total area .Summary Score sThe last four rows of the score report in Figure 1 are summary score rows .
Inphase one, there was one summary score row that represented the totals of thecolumns for the scoring categories including possible and actual .
The metrics werethen calculated based on those totals and appeared in the appropriate columns in th elower righthand portion of the chart .
The summary metrics are always calculate dfrom the items in the summary totals and are never the result of averaging th emetrics for the slots .In phase two, it was decided that the scoring system should keep the interna ltallies needed to supply several summary score rows, only one of which would be thetotal of the slot scores shown in the columns of the score report .
The scoring of slotsin the missing and spurious templates was the issue which gave rise to multipl esummary rows .In phase one, spurious templates were scored as spurious in th etemplate id slot only.
The spurious slot fillers aside from the template id slot fille rwere not scored as spurious .
Missing templates, however, were scored in the templat eid slot and in the individual missing slots .
This method of scoring did not penalize a smuch for overpopulating the database as it did for underpopulating it .In phase two, we wanted to find out how the systems scored if overpopulatin gand underpopulating the database were treated equally .
Two summary rows wereadded, one of which scored spurious and missing in the template id only and th eother of which scored spurious and missing templates for all of the spurious andmissing slot fills.
The official scores were still taken from the same summary row a sin phase one, but the other two rows were there for analysis .23The global summary rows are listed on the score report in order of strictnes sbased on the scoring of missing and spurious templates .
The MATCHED ONLY row hasmissing and spurious templates only scored in the template id slot .
This row containsthe least strict of the scores for the system .
The MATCHED/MISSING row contains theofficial test results .
The missing template slots are scored as missing.
The spurioustemplates are scored only in the template id slot .
The totals in this row are the total sof the tallies in the columns as shown.
The ALL TEMPLATES row has missing templat eslots scored as missing and spurious template slots scored as spurious.
This rowcontains the strictest scores for the system .A fourth summary row was added to allow analysis of system performance o nonly the set fill slots .
The SET FILLS ONLY row contains totals for slots with finite se tfills only .
A global fallout score is calculated for these slots and given in the fallou tcolumn of this row .CONCLUSIONS AND FURTHER RESEARCHThe evaluation metrics for MUC-3 had utility for system development and fo rthe reporting and analysis of the results of the evaluation.
The metrics were adapte dfrom simpler task models and were still evolving when the evaluation wa sperformed .
There has been consistent agreement on the necessity of basi cmeasurements of completeness, accuracy, false alarm rate, and overgeneration .These measurements were accomplished through the metrics of recall, precision ,fallout, and overgeneration as defined for MUC-3 .The global summary score sprovide several different views of system performance .
However, further analysis o fthe current results is possible based on the information in the official score reports .The template by template scores are officially reported and can be used as a basis forfurther analysis .For example, performance at the message level can be calculatedfrom the template by template scores for the systems .While the metrics of recall, precision, fallout, and overgeneration have bee ndefined for MUC-3, further research into the metrics and their implementation need sto be done.
Additional measurements may be required .
More refined definitions ofthe current measurements are probably needed .
The complexities of optional fills ,alternatives in the key, partial credit, and distribution of partial credit over ke yvalues, to name a few, still need to be examined more closely with considerationgiven to their effects on the metrics.These complexities have made it difficult t ofully test the scoring system software and require more attention to be paid t odetecting and isolating subtle errors .A different treatment of the slots will need tobe attempted.
For example, the template id slot is unique among the slots and will b ekept separate when the summary measures are calculated in the future.A singleoverall measure of performance may be possible in the future once the roles o frecall and precision are more fully determined.
All of these avenues of furthe rresearch have been opened up by the definition of a set of metrics for MUC-3 and th edevelopment of a scoring system embodying those metrics .ACKNOWLEDGEMENT SMany of the scoring issues were debated and resolved in consultation wit hmembers of the Program Committee .
David Lewis provided technical guidance .
PeteHalverson developed the scoring system.
The participants provided feedback.
BethSundheim was a sounding board for many of the issues that arose during th eevolution of the MUC-3 scoring .24
