Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197?200,New York, June 2006. c?2006 Association for Computational LinguisticsComparing the roles of textual, acoustic and spoken-language featureson spontaneous-conversation summarizationXiaodan Zhu Gerald PennDepartment of Computer Science, University of Toronto10 Kings College Rd., Toronto, Canada{xzhu, gpenn} @cs.toronto.eduAbstractThis paper is concerned with thesummarization of spontaneousconversations.
Compared with broadcastnews, which has received intensive study,spontaneous conversations have been lessaddressed in the literature.
Previouswork has focused on textual featuresextracted from transcripts.
This paperexplores and compares the effectivenessof both textual features and speech-related features.
The experiments showthat these features incrementally improvesummarization performance.
We also findthat speech disfluencies, which  have beenremoved as noise in previous work, helpidentify important utterances, while thestructural feature is less effective than itis in broadcast news.1 IntroductionSpontaneous conversations are a very importanttype of speech data.
Distilling importantinformation from them has commercial and otherimportance.
Compared with broadcast news, whichhas received the most intensive studies (Hori andFurui, 2003; Christensen et al 2004; Maskey andHirschberg, 2005), spontaneous conversations havebeen less addressed in the literature.Spontaneous conversations are different frombroadcast news in several aspects: (1) spontaneousconversations are often less well formedlinguistically, e.g., containing more speechdisfluencies and false starts; (2) the distribution ofimportant utterances in spontaneous conversationscould be different from that in broadcast news, e.g.,the beginning part of news often containsimportant information, but in conversations,information may be more evenly distributed; (3)conversations often contain discourse clues, e.g.,question-answer pairs and speakers?
information,which can be utilized to keep the summarycoherent; (4) word error rates (WERs) from speechrecognition are usually much higher inspontaneous conversations.Previous work on spontaneous-conversationsummarization has mainly focused on textualfeatures (Zechner, 2001; Gurevych and Strube,2004), while speech-related features have not beenexplored for this type of speech source.
This paperexplores and compares the effectiveness of bothtextual features and speech-related features.
Theexperiments show that these features incrementallyimprove summarization performance.
We alsodiscuss problems (1) and (2) mentioned above.
For(1), Zechner (2001) proposes to detect and removefalse starts and speech disfluencies from transcripts,in order to make the text-format summary conciseand more readable.
Nevertheless, it is not alwaysnecessary to remove them.
One reason is thatoriginal utterances are often more desired to ensurecomprehensibility and naturalness if the summariesare to be delivered as excerpts of audio (see section2), in order to avoid the impact of WER.
Second,disfluencies are not necessarily noise; instead, theyshow regularities in a number of dimensions(Shriberg, 1994), and correlate with many factorsincluding topic difficulty (Bortfeld et al 2001).Rather than removing them, we explore the effectsof disfluencies on summarization, which, to ourknowledge, has not yet been addressed in theliterature.
Our experiments show that they improvesummarization performance.To discuss problem (2), we explore and compareboth textual features and speech-related features,as they are explored in broadcast news (Maskeyand Hirschberg, 2005).
The experiments show thatthe structural feature (e.g.
utterance position) isless effective for summarizing spontaneousconversations than it is in broadcast news.
MMR197and lexical features are the best.
Speech-relatedfeatures follow.
The structural feature is leasteffective.
We do not discuss problem (3) and (4) inthis paper.
For problem (3), a similar idea has beenproposed to summarize online blogs anddiscussions.
Problem (4) has been partiallyaddressed by (Zechner & Waibel, 2000); but it hasnot been studied together with acoustic features.2 Utterance-extraction-basedsummarizationStill at its early stage, current research on speechsummarization targets a less ambitious goal:conducting extractive, single-document, generic,and surface-level-feature-based summarization.The pieces to be extracted could correspond towords (Koumpis, 2002; Hori and Furui, 2003).
Theextracts could be utterances, too.
Utteranceselection is useful.
First, it could be a preliminarystage applied before word extraction, as proposedby Kikuchi et al (2003) in their two-stagesummarizer.
Second, with utterance-level extracts,one can play the corresponding audio to users, aswith the speech-to-speech summarizer discussed inFurui et al (2003).
The advantage of outputtingaudio segments rather than transcripts is that itavoids the impact of WERs caused by automaticspeech recognition (ASR).
We will focus onutterance-level extraction, which at present appearsto be the only way to ensure comprehensibility andnaturalness if the summaries are to be delivered asexcerpts of audio themselves.Previous work on spontaneous conversationsmainly focuses on using textual features.
Gurevych& Strube (2004) develop a shallow knowledge-based approach.
The noun portion of WordNet isused as a knowledge source.
The noun senses weremanually disambiguated rather than automatically.Zechner (2001) applies maximum marginalrelevance (MMR) to select utterances forspontaneous conversation transcripts.3 Classification based utteranceextractionSpontaneous conversations contain moreinformation than textual features.
To utilize thesefeatures, we reformulate the utterance selectiontask as a binary classification problem, anutterance is either labeled as ?1?
(in-summary) or?0?
(not-in-summary).
Two state-of-the-artclassifiers, support vector machine (SVM) andlogistic regression (LR), are used.
SVM seeks anoptimal separating hyperplane, where the margin ismaximal.
In our experiments, we use the OSU-SVM package.
Logistic regression (LR) is indeed asoftmax linear regression, which models theposterior probabilities of the class label with thesoftmax of linear functions of feature vectors.
Forthe binary classification that we require in ourexperiments, the model format is simple.3.1 FeaturesThe features explored in this paper include:(1) MMR score: the score calculated with MMR(Zechner, 2001) for each utterance.
(2) Lexicon features: number of named entities,and utterance length (number of words).
Thenumber of named entities includes: person-name number, location-name number,organization-name number, and the totalnumber.
Named entities are annotatedautomatically with a dictionary.
(3) Structural features: a value is assigned toindicate whether a given utterance is in the first,middle, or last one-third of the conversation.Another Boolean value is assigned to indicatewhether this utterance is adjacent to a speakerturn or not.
(4) Prosodic features: we use basic prosody: themaximum, minimum, average and range ofenergy, as well as those of fundamentalfrequency, normalized by speakers.
All thesefeatures are automatically extracted.
(5) Spoken-language features: the spoken-languagefeatures include number of repetitions, filledpauses, and the total number of them.Disfluencies adjacent to a speaker turn are notcounted, because they are normally used tocoordinate interaction among speakers.Repetitions and pauses are detected in the sameway as described in Zechner (2001).4 Experimental results4.1 Experiment settingsThe data used for our experiments come fromSWITCHBOARD.
We randomly select 27conversations, containing around 3660 utterances.The important utterances of each conversation are198manually annotated.
We use f-score and theROUGE score as evaluation metrics.
Ten-foldcross validation is applied to obtain the resultspresented in this section.4.2 Summarization performance4.2.1 F-scoreTable-1 shows the f-score of logistic regression(LR) based summarizers, under differentcompression ratios, and with incremental featuresused.10% 15% 20% 25% 30%(1)  MMR .246 .309 .346 .355 .368(2) (1) +lexicon .293 .338 .373 .380 .394(3) (2)+structure .334 .366 .400 .409 .404(4) (3)+acoustic .336 .364 .388 .410 .415(5) (4)+spoken language .333 .376 .410 .431 .422Table 1. f-score of LR summarizers using incremental featuresBelow is the f-score of SVM-based summarizer:10% 15% 20% 25% 30%(1) MMR .246 .309 .346 .355 .368(2) (1) +lexicon .281 .338 .354 .358 .377(3) (2)+structural .326 .371 .401 .409 .408(4) (3)+acoustic .337 .380 .400 .422 .418(5) (4)+spoken language .353 .380 .416 .424 .423Table 2. f-score of SVM summarizers using incremental featuresBoth tables show that the performance ofsummarizers improved, in general, with morefeatures used.
The use of lexicon and structuralfeatures outperforms MMR, and the speech-relatedfeatures, acoustic features and spoken languagefeatures produce additional improvements.4.2.2 ROUGEThe following tables provide the ROUGE-1 scores:10% 15% 20% 25% 30%(1) MMR .585 .563 .523 .492 .467(2) (1) +lexicon .602 .579 .543 .506 .476(3) (2)+structure .621 .591 .553 .516 .482(4) (3)+acoustic .619 .594 .554 .519 .485(5) (4)+spoken language .619 .600 .566 .530 .492Table 3.
ROUGE-1 of LR summarizers using incremental features10% 15% 20% 25% 30%(1) MMR .585 .563 .523 .492 .467(2) (1) +lexicon .604 .581 .542 .504 .577(3) (2)+structure .617 .600 .563 .523 .490(4) (3)+acoustic .629 .610 .573 .533 .496(5)(4)+spoken language .628 .611 .576 .535 .502Table 4.
ROUGE-1 of SVM summarizers using incremental featuresThe ROUGE-1 scores show similar tendencies tothe f-scores: the rich features improvesummarization performance over the baselineMMR summarizers.
Other ROUGE scores likeROUGE-L show the same tendency, but are notpresented here due to the space limit.Both the f-score and ROUGE indicate that, ingeneral, rich features incrementally improvesummarization performance.4.3 Comparison of featuresTo study the effectiveness of individual features,the receiver operating characteristic (ROC) curvesof these features are presented in Figure-1 below.The larger the area under a curve is, the better theperformance of this feature is.
To be more exact,the definition for the y-coordinate (sensitivity) andthe x-coordinate (1-specificity) is:ratenegtivetrueFPTNTNyspecificitratepositivetrueFNTPTPysensitivit=+==+=where TP, FN, TN and FP are true positive, falsenegative, true negative, and false positive,respectively.Figure-1.
ROC curves for individual featuresLexicon and MMR features are the best twoindividual features, followed by spoken-languageand acoustic features.
The structural feature is leasteffective.Let us first revisit the problem (2) discussedabove in the introduction.
The effectiveness of thestructural feature is less significant than it is inbroadcast news.
According to the ROC curvespresented in Christensen et al (2004), thestructural feature (utterance position) is one of thebest features for summarizing read news stories,and is less effective when news stories containspontaneous speech.
Both their ROC curves coverlarger area than the structural feature here in figure1, that is, the structure feature is less effective forsummarizing spontaneous conversation than it is inbroadcast news.
This reflects, to some extent, that199information is more evenly distributed inspontaneous conversations.Now let us turn to the role of speech disfluencies,which are very common in spontaneousconversations.
Previous work detects and removesdisfluencies as noise.
Indeed, disfluencies showregularities in a number of dimensions (Shriberg,1994).
They correlate with many factors includingthe topic difficulty (Bortfeld et al 2001).
Tables 1-4 above show that they improve summarizationperformance when added upon other features.Figure-1 shows that when used individually, theyare better than the structural feature, and also betterthan acoustic features at the left 1/3 part of thefigure, where the summary contains relativelyfewer utterances.
Disfluencies, e.g., pauses, areoften inserted when speakers have word-searchingproblem, e.g., a problem finding topic-specifickeywords:Speaker A: with all the uh sulfur and all that otherstuff they're dumping out into the atmosphere.The above example is taken from a conversationthat discusses pollution.
The speaker inserts a filledpause uh in front of the word sulfur.
Pauses are notrandomly inserted.
To show this, we remove themfrom transcripts.
Section-2 of SWITCHBOARD(about 870 dialogues and 189,000 utterances) isused for this experiment.
Then we insert thesepauses back randomly, or insert them back at theiroriginal places, and compare the difference.
Forboth cases, we consider a window with 4 wordsafter each filled pause.
We average the tf.idf scoresof the words in each of these windows.
Then, forall speaker-inserted pauses, we obtain a set ofaveraged tf.idf scores.
And for all randomly-inserted pauses, we have another set.
The mean ofthe former set (5.79 in table 5) is statisticallyhigher than that of the latter set (5.70 in table 5).We can adjust the window size to 3, 2 and 1, andthen get the following table.Window size 1 2 3 4Insert Randomly 5.69 5.69 5.70 5.70Mean oftf.idf score Insert by speaker  5.72 5.82 5.81 5.79Difference is significant?
(t-test, p<0.05) Yes Yes Yes YesTable 5.
Average tf.idf scores of words following filled pauses.The above table shows that instead of randomlyinserting pauses, real speakers insert them in frontof words with higher tf.idf scores.
This helpsexplain why disfluencies work.5 ConclusionsPrevious work on summarizing spontaneousconversations has mainly focused on textualfeatures.
This paper explores and compares bothtextual and speech-related features.
Theexperiments show that these features incrementallyimprove summarization performance.
We also findthat speech disfluencies, which are removed asnoise in previous work, help identify importantutterances, while the structural feature is lesseffective than it is in broadcast news.6 ReferencesBortfeld, H., Leon, S.D., Bloom, J.E., Schober, M.F., &Brennan, S.E.
2001.
Disfluency Rates in Conversation:Effects of Age, Relationship, Topic Role, and Gender.Language and Speech, 44(2): 123-147Christensen, H., Kolluru, B., Gotoh, Y., Renals, S., 2004.From text summarisation to style-specificsummarisation for broadcast news.
Proc.
ECIR-2004.Furui, S., Kikuichi T. Shinnaka Y., and Hori C. 2003.Speech-to-speech and speech to text summarization,.First International workshop on LanguageUnderstanding and Agents for Real World Interaction,2003.Gurevych I. and Strube M. 2004.
Semantic SimilarityApplied to Spoken Dialogue Summarization.
COLING-2004.Hori C. and Furui S., 2003.
A New Approach to AutomaticSpeech Summarization IEEE Transactions onMultimedia, Vol.
5, NO.
3, September 2003,Kikuchi T., Furui S. and Hori C., 2003.
Automatic SpeechSummarization Based on Sentence Extraction andCompaction, Proc.
ICASSP-2003.Koumpis K., 2002.
Automatic Voicemail Summarisationfor Mobile Messaging Ph.D. Thesis, University ofSheffield, UK, 2002.Maskey, S.R., Hirschberg, J.
"Comparing Lexial,Acoustic/Prosodic, Discourse and Structural Featuresfor Speech Summarization", Eurospeech 2005.Shriberg, E.E.
(1994).
Preliminaries to a Theory of SpeechDisfluencies.
Ph.D. thesis, University of California atBerkeley.Zechner K. and Waibel A., 2000.
Minimizing word errorrate in textual summaries of spoken language.
NAACL-2000.Zechner K., 2001.
Automatic Summarization of SpokenDialogues in Unrestricted Domains.
Ph.D. thesis,Carnegie Mellon University, November 2001.200
