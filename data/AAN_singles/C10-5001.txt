Coling 2008: Kernel Engineering for Fast and Easy Design of Natural Language Applications?Tutorial notes, pages 1?91,Beijing, August 2010Kernel Engineering for Fast and EasyDesign of Natural Language ApplicationsAlessandro MoschittiDepartment of Information Engineering and Computer ScienceUniversity of TrentoEmail: moschitti@disi.unitn.itThe 23rd International Conference on Computational Linguistics August 22, 2010 Beijing, ChinaSchedule?
14:00 - 15:30 First part?
15:30 - 16:00 Coffee break?
16:00 - 17:30 Second part1Outline (1)?
Motivation?
Kernel-Based Machines?
Perceptron?
Support Vector Machines?
Kernel Definition?
Kernel Trick?
Mercer?s conditions?
Kernel operators?
Basic Kernels?
Linear Kernel?
Polynomial Kernel?
Lexical KernelOutline (2)?
Structural Kernels?
String and Word Sequence Kernels?
Tree Kernels?
Subtree, Syntactic, Partial Tree Kernels?
Applied Examples of Structural Kernels?
Semantic Role Labeling (SRL)?
Question Classification (QC)?
SVM-Light-TK?
Experiments in classroom with SRL and QC?
Inspection of the input, output, and model files2Outline (3)?
Kernel Engineering?
Structure Transformation?
Syntactic Semantic Tree kernels?
Kernel Combinations?
Kernels on Object Pairs?
Kernels for re-ranking?
Practical Question and Answer Classifier based on     SVM-Light-TK?
Combining Kernels?
Conclusion and Future WorkMotivation (1)?
Feature design most difficult aspect in designing alearning system?
complex and difficult phase, e.g., structural featurerepresentation:?
deep knowledge and intuitions are required?
design problems when the phenomenon isdescribed by many features3Motivation (2)?
Kernel methods alleviate such problems?
Structures represented in terms of substructures?
High dimensional feature spaces?
Implicit and abstract feature spaces?
Generate high number of features?
Support Vector Machines ?select?
the relevant features?
Automatic Feature engineering side-effectPart I: Kernel Methods Theory4A simple classification problem: Text CategorizationSport   CnPolitic     C1EconomicC2.
.
.
.
.
.
.
.
.
.
.BushdeclareswarWonderfulTottiYesterdaymatchBerlusconiacquiresInzaghibeforeelectionsl iiil tiText Classification Problem?
Given:?
a set of target categories:?
the set T of documents,definef : T  ?
2C?
VSM (Salton89?)?
Features are dimensions of a Vector Space.?
Documents and Categories are vectors of feature weights.?
d is assigned to        if?
?d ?
?Ci> th?C = C1,..,Cn{ }iC5More in detail?
In Text Categorization documents are wordvectors?
The dot product            counts the number offeatures in common?
This provides a sort of similarity??
(dx) =?x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)buy       acquisition     stocks          sell     marketzx?????
(dz) =?z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)buy   company            stocks          sellLinear Classifier?f (?x ) =?x ?
?w + b = 0,?x ,?w ?
?n,b ?
??
The equation of a hyperplane is?
is the vector representing the classifying example?
is the gradient of the hyperplane?
The classification function isx?w?
( ) sign( ( ))h x f x=6?
Mapping vectors in a space where they are linearly separablexxxxooooThe main idea of Kernel Functions)(xx??
??)x(?)x(?)x(?)x(?)(o?
)(o?)(o?)(o?
?A mapping example?
Given two masses m1 and m2 , one is constrained?
Apply a force fa to the mass m1?
Experiments?
Features m1 , m2 and  fa?
We want to learn a classifier that tells when a mass m1 will get far away from m222121),,(rmmCrmmf =?
If we consider the Gravitational Newton Law?
we need to find when f(m1 , m2 , r) < fa7A mapping example (2)))(),...,(()(),...,(11xxxxxxnn????
???
=?=?
The gravitational law is not linear so we need to change space)ln,ln,ln,(ln),,,(),,,(2121rmmfzyxkrmmfaa=?zyxcrmmCrmmf 2ln2lnlnln),,(ln2121?++=?++=(ln m1,ln m2,-2ln r)?
(x,y,z)- ln fa + ln C = 0, we can decide without error if the mass will get far away or not?
As0lnln2lnlnln21=?+??
Crmmfa?
We need the hyperplaneA kernel-based Machine Perceptron training??w0?
?0 ;b0?
0;k ?
0;R?
max1?
i?
l||?xi||dofor i =  1 to ?if yi(?wk?
?xi+ bk) ?
0 then?wk +1=?wk+?yi?xibk +1= bk+?yiR2k = k +1endifendforwhile an error is foundreturn k,(?wk,bk)89Novikoff?s TheoremLet S be a non-trivial training-set and letLet us suppose there is a vector           andwith ?
> 0.
Then the maximum number of errors of the perceptron is:* *, || || 1 =w w* *( , ) , 1,..., ,i iy b i l?+ ?
=w x2*2,Rt ??
?= ?
??
?1max || || .ii lR x?
?=10?
In each step of perceptron only training data is added with a certain weight?
So the classification function?
Note that data only appears in the scalar productDual Representation for Classification?
?w = ?jj=1..?
?yj?xj?sgn(?w ?
?x + b) = sgn ?jj=1..??yj?xj?
?x + b???????
?Dual Representation for Learning?
as well as the updating function?
The learning rate      only affects the re-scaling of the hyperplane, it does not affect the algorithm, so we can fix 1.?
= ?
?
if yi( ?
jj=1..??
y j ?
x j ?
?
x i + b) ?
0 then ?
i =?
i +?11?
We can rewrite the classification function as?
As well as the updating functionDual Perceptron algorithm and Kernel functions?h(x) = sgn(?w ?
?
?(?
x ) + b? )
= sgn( ?
jj=1..??yj?(?
xj) ?
?(?
x ) + b? )
== sgn( ?ji=1..?
?yjk(?xj,?x ) + b?
)?if yi?jj=1..?
?yjk(?xj,?xi) + b??????????
0 allora ?i=?i+?Support Vector Machines?
Hard-margin SVMs?
Soft-margin SVMs12Which hyperplane do we choose?Classifier with a Maximum MarginVar1Var2MarginMarginIDEA 1: Select the hyperplane with maximum margin13Support VectorsVar1Var2MarginSupport VectorsSupport Vector MachinesVar1Var2 kbxw ?=+?
?
?kbxw =+???0=+?
bxw?
?kkw?The margin is equal to 2 kw14Support Vector MachinesVar1Var2 kbxw ?=+?
?
?kbxw =+???0=+?
bxw?
?kkw?The margin is equal to 2 kwWe need to solve?max2 k||?w ||?w ?
?x + b ?
+k,   if?x is positive?w ?
?x + b ?
?k,   if?x is negativeSupport Vector MachinesVar1Var2 1w x b?
+ = ??
?1w x b?
+ =?
?0=+?
bxw?
?11w?There is a scale for which k=1.The problem transforms in:?max2||?w ||?w ?
?x + b ?
+1,  if?x is positive?w ?
?x + b ?
?1,  if?x is negative15Final Formulation??
?max2||?w ||?w ?
?xi+ b ?
+1,  yi=1?w ?
?xi+ b ?
?1,  yi= -1?max2||?w ||yi(?w ?
?xi+ b) ?1?min||?w ||2yi(?w ?
?xi+ b) ?1?min||?w ||22yi(?w ?
?xi+ b) ?1?????
?Optimization Problem?
Optimal Hyperplane:?
Minimize?
Subject to?
The dual problem is simplerlibxwywwii,...,1,1))((21)(2=?+?=????
?16Lagrangian DefinitionDual Optimization Problem17Dual Transformation?
To solve the dual problem we need to evaluate:?
Given the Lagrangian associated with our problem?
Let us impose the derivatives to 0, with respect to   w?Dual Transformation (cont?d)?
and wrt b?
Then we substituted them in the objective function18The Final Dual Optimization ProblemKhun-Tucker Theorem?
Necessary and sufficient conditions to optimality19Properties coming from constraints?
Lagrange constraints:?
Karush-Kuhn-Tucker constraints?
Support Vectors have     not null?
To evaluate b, we can apply the following equation?aii=1l?yi= 0,?w = ?ii=1l?yi?xilibwxyiii,...,1,0]1)([ ==?+????
?i?Soft Margin SVMsVar1Var2 1w x b?
+ = ??
?1w x b?
+ =?
?0=+?
bxw??11w?i?
slack variables are addedSome errors are allowed but they should penalize the objective functioni?20Soft Margin SVMsVar1Var2 1w x b?
+ = ??
?1w x b?
+ =?
?0=+?
bxw??11w?i?
The new constraints areThe objective function penalizes the incorrect classified examplesC is the trade-off between margin and the error?yi(?w ?
?xi+ b) ?1??i?
?xiwhere  ?i?
0?min12||?w ||2+C ?ii?Dual formulation?
By deriving wrt?
?w ,?
?
and b21Partial DerivativesSubstitution in the objective function?
of Kroneckerij?22Final dual optimization problemSoft Margin Support Vector Machines?
The algorithm tries to keep ?i low and maximize the margin?
NB: The number of error is not directly minimized (NP-completeproblem); the distances from the hyperplane are minimized?
If C?
?, the solution tends to the one of the hard-margin algorithm?
Attention !!!
: if C = 0 we get          = 0, since?
If C increases the number of error decreases.
When C tends toinfinite the number of errors must be 0, i.e.
the hard-marginformulation|||| w?
?min12||?w ||2+C ?ii?
?yi(?w ?
?xi+ b) ?1??i??xi?i?
0?yib ?1??i?
?xi23Robusteness of Soft vs. Hard Margin SVMsi?Var1Var20=+?
bxw???iVar1Var20=+?
bxw?
?Soft Margin SVM Hard Margin SVMKernels in Support Vector Machines?
In Soft Margin SVMs we maximize:?
By using kernel functions we rewrite the problem as:24Kernel Function Definition?
Kernels are the product of mapping functionssuch as?
?x ?
?n,?
?
(?
x ) = (?1(?x ),?2(?x ),...,?m(?x )) ?
?mThe Kernel Gram Matrix?
With KM-based learning, the sole information used from the training data set is the Kernel Gram Matrix?
If the kernel is valid, K is symmetric definite-positive .25Valid KernelsValid Kernels cont?d?
If the matrix is positive semi-definite then we can find a mapping ?
implementing the kernel function26Mercer?s Theorem (finite space)?
Let us consider?K =  K(?xi,?xj)( )i, j=1n?
K symmetric ?
?
V:                      for Takagi factorization of acomplex-symmetric matrix, where:?
?
is the diagonal matrix of the eigenvalues ?t of K?
are the eigenvectors, i.e.
the columns of V?
Let us assume lambda values non-negative?K = V??V?
?vt=  vti( )i =1n??
: ?
xi?
?tvti( )t =1n?
?n, i =1,..,nMercer?s Theorem (sufficient conditions)??
(?xi) ?
?
(?xj) = ?tvtit=1n?vtj= V?
?V( )ij= Kij= K(?xi,?xj)?
Therefore,?
which implies that K is a kernel function27Mercer?s Theorem (necessary conditions)?
?z2=?z ?
?z = ??V?vs?
?V?vs=?vs' V ?
?
?V?vs=?vs' K?vs=?vs' ?s?vs= ?s?vs2< 0?
Suppose we have negative eigenvalues ?s and eigenvectors       the following point?
has the following norm:this contradicts the geometry of the space.??vs?
?z = vsi?
(?xi)i=1n?= vsi?tvti( )t=i=1n??
?V?vsIs it a valid kernel??
It may not be a kernel so we can use M?
?M28Valid Kernel operations?
k(x,z) = k1(x,z)+k2(x,z)?
k(x,z) = k1(x,z)*k2(x,z)?
k(x,z) = ?
k1(x,z)?
k(x,z) = f(x)f(z)?
k(x,z) = k1(?(x),?(z))?
k(x,z) = x'BzBasic Kernels for unstructured data?
Linear Kernel?
Polynomial Kernel?
Lexical kernel?
String Kernel29Linear Kernel?
In Text Categorization documents are wordvectors?
The dot product            counts the number offeatures in common?
This provides a sort of similarity??
(dx) =?x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)buy       acquisition     stocks          sell     marketzx?????
(dz) =?z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)buy   company            stocks          sellFeature Conjunction (polynomial Kernel)?
The initial vectors are mapped in a higher space?
More expressive, as            encodesStock+Market vs. Downtown+Market features?
We can smartly compute the scalar product as)1,2,2,2,,(),(2121222121xxxxxxxx ?><?),()1()1(1222)1,2,2,2,,()1,2,2,2,,()()(22221122112121222221212121222121212221zxKzxzxzxzxzxzzxxzxzxzzzzzzxxxxxxzxPoly??????=+?=++==+++++==?==???
)(21xx30Document SimilarityindustrytelephonemarketcompanyproductDoc 1 Doc 2Lexical Semantic Kernel [CoNLL 2005]?
The document similarity is the SK function:?
where s is any similarity function between words,e.g.
WordNet [Basili et al,2005] similarity or LSA[Cristianini et al, 2002]?
Good results when training data is small ?SK(d1,d2) = s(w1,w2)w1?d1,w2?d2?31Using character sequenceszx?????
("bank") = ?
x = (0,..,1,..,0,..,1,..,0,......1,..,0,..,1,..,0,..,1,..,0)?
counts the number of common substringsbank       ank           bnk          bk          b??
("rank") = ?
z = (1,..,0,..,0,..,1,..,0,......0,..,1,..,0,..,1,..,0,..,1)rank               ank                  rnk          rk            r?
?x ?
?z = ?
("bank") ?
?
("rank") = k("bank","rank")String Kernel?
Given two strings, the number of matchesbetween their substrings is evaluated?
E.g.
Bank and Rank?
B, a, n, k, Ba, Ban, Bank, Bk, an, ank, nk,..?
R, a , n , k, Ra, Ran, Rank, Rk, an, ank, nk,..?
String kernel over sentences and texts?
Huge space but there are efficient algorithms32Formal Definition,  where,  wherei1 +1Kernel between Bank and Rank33An example of string kernel computationEfficient Evaluation?
Dynamic Programming technique?
Evaluate the spectrum string kernels?
Substrings of size p?
Sum the contribution of the different spectra34Efficient EvaluationAn example: SK(?Gatta?,?Cata?)?
First, evaluate the SK with size p=1, i.e.
?a?,?a?,?t?,?t?,?a?,?a??
Store this in the table?SKp=135Evaluating DP2?
Evaluate the weight of the string of size p in casea character will be matched?
This is done by multiplying the double summationby the number of substrings of size p-1Evaluating the Predictive DP on strings of size 2 (second row)?
Let?s consider substrings of size 2 and suppose that:?
we have matched the first ?a??
we will match the next character that we will add to the two strings?
We compute the weights of matches above at different stringpositions with some not-yet known character ????
If the match occurs immediately after ?a?
the weight will be ?1+1x ?1+1 = ?4 and we store just ?2 in the DP entry in [?a?,?a?
]36Evaluating the DP wrt different positions (second row)?
If the match for ?gatta?
occurs after ?t?
the weight will be ?1+2(x ?2 = ?5) since the substring for it will be with ?a????
We write such prediction in the entry [?a?,?t?]?
Same rationale for a match after the second ?t?
: we havethe substring ?a????
(matching with ?a??
from ?catta?)
fora weight of ?3+1  (x ?2)Evaluating the DP wrt different positions (third row)?
If the match occurs after ?t?
of ?cata?, the weight will be ?2+1(x ?2 = ?5 ) since it will be with the string ?a??
?, with a weightof ?3?
If the match occurs after ?t?
of both ?gatta?
and ?cata?, thereare two ways to compose substring of size two: ?a???
withweight ?4 or ?t??
with weight ?2 ?
the total is ?2+?437Evaluating the DP wrt different positions (third row)?
The final case is a match after the last ?t?
of both ?cat?
and?gatta??
There are three possible substrings of ?gatta?:?
?a???
?, ?t??
?, ?t??
for ?gatta?
with weight ?3 , ?2 or ?, respectively.?
There are two possible substrings of ?cata??
?a??
?, ?t??
with weight ?2 and ??
Their match gives weights: ?5 , ?3, ?2  ?
by summing: ?5 + ?3 + ?2Evaluating SK of size 2 using DP2?
The number (weight) ofsubstrings of size 2 between?gat?
and ?cat?
is ?4 = ?2([?a?,?a?]
entry of DP) x ?2(costof one character), where a =?t?
and   b = ?t?.?
Between ?gatta?
and ?cata?
is?7 + ?5 + ?4, i.e the matches of?a?
?a?, ?t?a?, ?ta?
with?a?a?
and ?ta?.
?SKp= 238Tree kernels?
Subtree, Subset Tree, Partial Tree kernels?
Efficient computationExample of a parse tree?
?John delivers a talk in Rome?S ?
N VPVP ?
V NP PPPP ?
IN NN ?
RomeNRomeSNNPD NVPV Johnindeliversa talkPPIN39The Syntactic Tree Kernel (STK)[Collins and Duffy, 2002]NPD NVPVdeliversa    talkThe overall fragment set40The overall fragment setNPDVPa Children are not dividedExplicit kernel spacezx?????
(Tx) =?x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0)?
counts the number of common substructures??
(Tz) =?z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0)41Efficient evaluation of the scalar product?
?x ?
?z = ?
(Tx) ?
?
(Tz) = K(Tx,Tz) ==nx?Tx??
(nx,nz)nz?Tz?Efficient evaluation of the scalar product?
[Collins and Duffy, ACL 2002] evaluate ?
in O(n2):??
(nx,nz) = 0,  if the productions are different else?
(nx,nz) =1,   if pre - terminals else?
(nx,nz) = (1+ ?
(ch(nx, j),ch(nz, j)))j=1nc(nx)??
?x ?
?z = ?
(Tx) ?
?
(Tz) = K(Tx,Tz) ==nx?Tx??
(nx,nz)nz?Tz?42Other Adjustments?
Normalization??
(nx,nz) = ?,    if pre - terminals else?
(nx,nz) = ?
(1+ ?
(ch(nx, j),ch(nz, j)))j=1nc(nx)??
?K (Tx,Tz) =K(Tx,Tz)K(Tx,Tx) ?K(Tz,Tz)?
Decay factorSubTree (ST) Kernel [Vishwanathan and Smola, 2002]NPDNatalkD Na   talkNPD NVPVdeliversatalkVdelivers43Evaluation??
(nx,nz) = 0,  if the productions are different else?
(nx,nz) =1,   if pre - terminals else?
(nx,nz) = (1+ ?
(ch(nx, j),ch(nz, j)))j=1nc(nx)??
Given the equation for the SST kernelEvaluation??
(nx,nz) = 0,  if the productions are different else?
(nx,nz) =1,   if pre - terminals else?
(nx,nz) = ?
(ch(nx, j),ch(nz, j))j=1nc(nx)??
Given the equation for the SST kernel44Fast Evaluation of STK [Moschitti, EACL 2006]where P(nx) and P(nz) are the production rules usedat nodes nx and nz?K(Tx,Tz) =  ?
(nx,nz)nx,nz?NP?NP = nx,nz?
Tx?Tz:?
(nx,nz) ?
0{ }== nx,nz?
Tx?Tz: P(nx) = P(nz){ },Algorithm45Observations?
We order the production rules used in Tx and Tz,  at loading time?
At learning time we may evaluate NP in|Tx|+|Tz | running time?
If Tx and Tz are generated by only one productionrule ?
O(|Tx|?|Tz | )?Observations?
We order the production rules used in Tx and Tz,  at loading time?
At learning time we may evaluate NP in|Tx|+|Tz | running time?
If Tx and Tz are generated by only one productionrule ?
O(|Tx|?|Tz | )?Very Unlikely!!!
!46Labeled Ordered Tree KernelNPD NVPVgivesa   talkNPD NVPVa    talkNPD NVPa    talkNPD NVPaNPDVPaNPDVPNPNVPNPNNP NPD N DNP ?VP?
SST satisfies the constraint ?remove 0 or allchildren at a time?.?
If we relax such constraint we get more generalsubstructures [Kashima and Koyanagi, 2002]Weighting Problems?
Both matched pairs give thesame contribution.?
Gap based weighting isneeded.?
A novel efficient evaluationhas to be definedNPD NVPVgivesa   talkNPD NVPVa    talkNPD NVPVgivesa   talkgivesJJgoodNPD NVPVgivesa   talkJJbad47Partial Trees, [Moschitti, ECML 2006]NPD NVPVbroughta    catNPD NVPVa    catNPD NVPa    catNPD NVPaNPDVPaNPDVPNPNVPNPNNP NPD N DNP ?VP?
SST + String Kernel with weighted gaps onNodes?
childrenPartial Tree Kernel?
By adding two decay factors we obtain:48Efficient Evaluation (1)?
In [Taylor and Cristianini, 2004 book], sequence kernels withweighted gaps are factorized with respect to differentsubsequence sizes.?
We treat children as sequences and apply the same theoryDpEfficient Evaluation (2)?
The complexity of finding the subsequences is?
Therefore the overall complexity iswhere ?
is the maximum branching factor (p = ?
)49Running Time of Tree Kernel FunctionsSVM-light-TK Software?
Encodes ST, SST and combination kernelsin SVM-light [Joachims, 1999]?
Available at http://dit.unitn.it/~moschitt/?
Tree forests, vector sets?
The new SVM-Light-TK toolkit will be releasedasap50Data Format?
?What does Html stand for???
1  |BT| (SBARQ (WHNP (WP What))(SQ (AUX does)(NP (NNP S.O.S.
))(VP (VB stand)(PP (IN for))))(.
?
))|BT|    (BOW (What *)(does *)(S.O.S.
*)(stand *)(for *)(?
*))|BT|    (BOP (WP *)(AUX *)(NNP *)(VB *)(IN *)(.
*))|BT|   (PAS (ARG0 (R-A1 (What *)))(ARG1 (A1 (S.O.S.
NNP)))(ARG2 (rel stand)))|ET| 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 66:1 152:1 274:1 333:1|BV| 2:1 21:1.4421347148614654E-4 23:1 31:1 36:1 39:1 41:1 46:1 49:1 52:1 66:1 152:1 246:1 333:1 392:1 |EV|Basic Commands?
Training and classification?
./svm_learn -t 5 -C T train.dat model?
./svm_classify test.dat model?
Learning with a vector sequence?
./svm_learn -t 5 -C V train.dat model?
Learning with the sum of vector and kernelsequences?
./svm_learn -t 5 -C + train.dat model51Part II: Kernel Methods for Practical ApplicationsKernel Engineering approaches?
Basic Combinations?
Canonical Mappings, e.g.
object transformations?
Merging of Kernels52Kernel Combinations an example?
Kernel Combinations:333333,,pTreepTreePTreeppTreeTreePTreepTreePTreepTreePTreeKKKKKKKKKKKKKKKK?
?=+?=?=+?=?+?+ ?
?kernel Tree featuresflat    of  kernel  polynomial  3TreepKKObject Transformation [Moschitti et al CLJ 2008]?
Canonical Mapping, ?M()?
object transformation,?
e. g. a syntactic parse tree into a verb subcategorization frame tree.?
Feature Extraction, ?E()?
maps the canonical structure in all its fragments?
different fragment spaces, e. g. ST, SST and PT.
),()()())(())(()()(),(2121212121SSKSSOOOOOOKEEEMEME=?=?=?= ??
?????
?53Predicate Argument Classification?
In an event:?
target words describe relation among different entities?
the participants are often seen as predicate's arguments.?
Example:Paul gives a talk in RomePredicate Argument Classification?
In an event:?
target words describe relation among different entities?
the participants are often seen as predicate's arguments.?
Example:[ Arg0 Paul] [ predicate gives ] [ Arg1 a talk] [ ArgM in Rome]54Predicate-Argument Feature RepresentationGiven a sentence, a predicate p:1.?
Derive the sentence parse tree2.?
For each node pair <Np,Nx>a.?
Extract a feature representation set Fb.?
If Nx exactly covers the Arg-i, F is one of its positive examplesc.?
F is a negative example otherwiseVector Representation for the linear kernelPhrase TypePredicateWordHead WordParse TreePathVoice Activeosition Right55Kernel Engineering: Tree TailoringPAT Kernel [Moschitti, ACL 2004]SNNPD NVPV Paulindeliversa    talkPPIN   NPjjFv,arg.0formalNstyleArg.
0a) SNNPD NVPV Paulindeliversa    talkPPIN   NPjjformalNstyleFv,arg.1 b) SNNPD NVPV Paulindeliversa    talkPPIN   NPjjformalNstyle Arg.
1Fv,arg.Mc)Arg.M?
These are Semantic Structures?
Given the sentence:[ Arg0 Paul] [ predicate delivers] [ Arg1 a talk] [ ArgM in formal Style]56In other words we consider?NPD NVPVdeliversa    talkSNPaulinPPIN   NPjjformalNstyle Arg.
1Sub-Categorization Kernel (SCF) [Moschitti, ACL 2004]SNNPD NVPV Paulindeliversa    talkPPIN   NPjjformalNstyleArg.
1Arg.
MArg.
0Predicate57Experiments on Gold Standard Trees?
PropBank and PennTree bank?
about 53,700 sentences?
Sections from 2 to 21 train., 23 test., 1 and 22 dev.?
Arguments from Arg0 to Arg5, ArgA and ArgM fora total of 122,774 and 7,359?
FrameNet and Collins?
automatic trees?
24,558 sentences from the 40 frames of Senseval 3?
18 roles (same names are mapped together)?
Only verbs?
70% for training and 30% for testingArgument Classification with Poly Kernel58PropBank ResultsArgument Classification on PAT using different Tree Fragment Extractor0.750.780.800.830.850.880 10 20 30 40 50 60 70 80 90 100% Training DataAccuracy---ST SSTLinear PT59FrameNet Results?
ProbBank arguments vs. Semantic RolesKernel Engineering: Node marking60Marking Boundary nodesNode Marking Effect61Different tailoring and markingCMSTMMSTExperiments?
PropBank and PennTree bank?
about 53,700 sentences?
Charniak trees from CoNLL 2005?
Boundary detection:?
Section 2 training?
Section 24 testing?
PAF and MPAF62Number of examples/nodes of Section 2Predicate Argument Feature (PAF) vs.
Marked PAF (MPAF) [Moschitti et al ACL-ws-2005]63Merging of Kernels [ECIR 2007]: Question/Answer Classification?
Syntactic/Semantic Tree Kernel?
Kernel Combinations?
ExperimentsMerging of Kernels [Bloehdorn & Moschitti, ECIR2007 & CIKM 2007]64Merging of KernelsNPD NVPVgivesa   talkNgoodNPD NVPVgivesa   talkNsolidDelta Evaluation is very simple65Question Classification?
Definition: What does HTML stand for??
Description: What's the final line in the Edgar Allan Poe poem "The Raven"??
Entity: What foods can cause allergic reaction in people??
Human: Who won the Nobel Peace Prize in 1992??
Location: Where is the Statue of Liberty??
Manner: How did Bob Marley die??
Numeric: When was Martin Luther King Jr.
born??
Organization: What company makes Bentley cars?Question Classifier based on Tree Kernels?
Question dataset (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/)[Lin and Roth, 2005])?
Distributed on 6 categories: Abbreviations, Descriptions, Entity, Human, Location, and Numeric.?
Fixed split 5500 training and 500 test questions?
Cross-validation (10-folds)?
Using the whole question parse trees?
Constituent parsing?
Example?What is an offer of direct stock purchase plan ??66Kernels?
BOW, POS are obtained with a simple tree, e.g.?
PT (parse tree)?
PAS (predicate argument structure)?BOXis What an offer an* * * * *67Question classificationSimilarity based on WordNet68Question Classification with S/STKMultiple Kernel Combinations69TASK: Question/Answer Classification [Moschitti, CIKM 2008]?
The classifier detects if a pair (question andanswer) is correct or not?
A representation for the pair is needed?
The classifier can be used to re-rank the output ofa basic QA systemDataset 2: TREC data?
138 TREC 2001 test questions labeled as?description??
2,256 sentences, extracted from the best rankedparagraphs (using a basic QA system based onLucene search engine on TREC dataset)?
216 of which labeled as correct by one annotator70Dataset 2: TREC data?
138 TREC 2001 test questions labeled as?description??
2,256 sentences, extracted from the best rankedparagraphs (using a basic QA system based onLucene search engine on TREC dataset)?
216 of which labeled as correct by one annotatorA question is linked to many answers: all its derivedpairs cannot be shared by training and test setsBags of words (BOW) and POS-tags (POS)?
To save time, apply STK to these trees:?BOXis What an offer of* * * * *?BOXVBZ WHNP DT NN IN* * * * *71Word and POS Sequences?
What is an offer of??
(word sequence, WSK)?
What_is_offer?
What_is?
WHNP VBZ DT NN IN?
(POS sequence, POSSK)?
WHNP_VBZ_NN?
WHNP_NN_INSyntactic Parse Trees (PT)72Predicate Argument Structure for Partial Tree Kernel (PASPTK)?
[ARG1 Antigens] were [AM?TMP originally] [rel defined] [ARG2 as non-self molecules].?
[ARG0 Researchers] [rel describe] [ARG1 antigens][ARG2 as foreignmolecules] [ARGM?LOC in the body]Kernels and Combinations?
Exploiting the property: k(x,z) = k1(x,z)+k2(x,z)?
BOW, POS, WSK, POSSK, PT, PASPTK?
BOW+POS, BOW+PT, PT+POS, ?73Results on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel TypeResults on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel Type74Results on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel TypeResults on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel Type75Results on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel TypeResults on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel Type76Results on TREC Data (5 folds cross validation)2022242628303234363840BOW POS POS_SK WSK PTPAS_SSTK PAS_PTKBOW+POS BOW+PTPOS_SK+PTWSK+PTPOS_SK+PT+PAS_SSTKPOS_SK+PT+PAS_PTKF1-measureKernel TypeBOW ?
24 POSSK+STK+PAS_PTK?
39?62 % of improvementKernels for Re-ranking77Re-ranking Framework?
Local classifier generates the most likely set ofhypotheses.?
These are used to build annotation pairs,           .?
positive instances if hi more correct than hj,?
A binary classifier decides if hi is more accuratethan hj.?
Each candidate annotation hi is described by astructural representation?hi, hjRe-ranking frameworkLocal Model78Syntactic Parsing Re-ranking?
Pairs of parse trees (Collins and Duffy, 2002)Re-ranking concept labeling[Dinarelli et al 2009]?
I have a problem with my monitorhi: I NULL have NULL a NULL problem PROBLEM-B with NULL my NULL monitor HW-Bhj: I NULL have NULL a NULL problem HW-Bwith NULL my NULL monitor79Flat tree representation  (cross-language structure)Multilevel Tree80Enriched Multilevel Tree?
FST CER from 23.2 to 16.01Re-ranking for Named-Entity Recognition [Vien et al 2010]?
CRF F1 from 84.86 to 88.1681Re-ranking Predicate Argument Structures[Moschitti et al CoNLL 2006]?
SVMs F1 from 75.89 to 77.25Conclusions?
Kernel methods and SVMs are useful tools to designlanguage applications?
Kernel design still requires some level of expertise?
Engineering approaches to tree kernels?
Basic Combinations?
Canonical Mappings, e.g.?
Node Marking?
Merging of kernels in more complex kernels?
Easy modeling produces state-of-the-art accuracy in manytasks, RTE, SRL, QC, NER, RE?
SVM-Light-TK efficient tool to use them82Future (on going work)?
Once we have found the right kernel, are we satisfied??
What about knowing the most relevant features??
Can we speed up learning/classification at real-applicationscenario level??
The answer is reverse kernel engineering:?
[Pighin&Moschitti, CoNLL2009, EMNLP2009, CoNLL2010]?
Mine the most relevant fragments according to SVMs gradient?
Use the linear space?
Software for reverse kernel engineering available in thenext  monthsThank you83References?
Alessandro Moschitti and Silvia Quarteroni, Linguistic Kernels for Answer Re-ranking inQuestion Answering Systems, Information and Processing Management, ELSEVIER,2010.?
Yashar Mehdad, Alessandro Moschitti and Fabio Massimo Zanzotto.
Syntactic/Semantic Structures for Textual Entailment Recognition.
Human Language Technology- North American chapter of the Association for Computational Linguistics (HLT-NAACL), 2010, Los Angeles, Calfornia.?
Daniele Pighin and Alessandro Moschitti.
On Reverse Feature Engineering of SyntacticTree Kernels.
In Proceedings of the 2010 Conference on Natural Language Learning,Upsala, Sweden, July 2010.
Association for Computational Linguistics.?
Thi Truc Vien Nguyen, Alessandro Moschitti and Giuseppe Riccardi.
Kernel-basedReranking for Entity Extraction.
In proceedings of the 23rd International Conference onComputational Linguistics (COLING), August 2010, Beijing, China.References?
Alessandro Moschitti.
Syntactic and semantic kernels for short text pair categorization.In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL2009), pages 576?584, Athens, Greece, March 2009.
Association for ComputationalLinguistics.?
Truc-Vien Nguyen, Alessandro Moschitti, and Giuseppe Riccardi.
Convolution kernelson constituent, dependency and sequential structures for relation extraction.
InProceedings of the Conference on Empirical Methods in Natural Language Processing,pages 1378?1387, Singapore, August 2009.
Association for Computational Linguistics.?
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi.
Re-ranking modelsbased-on small training data for spoken language understanding.
In Proceedings of theConference on Empirical Methods in Natural Language Processing, pages 1076?1085,Singapore, August 2009.
Association for Computational Linguistics.?
Alessandra Giordani and Alessandro Moschitti.
Syntactic Structural Kernels for NaturalLanguage Interfaces to Databases.
In ECML/PKDD, pages 391?406, Bled, Slovenia,2009.84References?
Alessandro Moschitti, Daniele Pighin and Roberto Basili.
Tree Kernels for SemanticRole Labeling, Special Issue on Semantic Role Labeling, Computational LinguisticsJournal.
March 2008.?
Fabio Massimo Zanzotto, Marco Pennacchiotti and Alessandro Moschitti, A MachineLearning Approach to Textual Entailment Recognition, Special Issue on TextualEntailment Recognition, Natural Language Engineering, Cambridge University Press.,2008?
Mona Diab, Alessandro Moschitti, Daniele Pighin, Semantic Role Labeling Systems forArabic Language using Kernel Methods.
In proceedings of the 46th Conference of theAssociation for Computational Linguistics (ACL'08).
Main Paper Section.
Columbus,OH, USA, June 2008.?
Alessandro Moschitti, Silvia Quarteroni, Kernels on Linguistic Structures for AnswerExtraction.
In proceedings of the 46th Conference of the Association for ComputationalLinguistics (ACL'08).
Short Paper Section.
Columbus, OH, USA, June 2008.References?
Yannick Versley, Simone Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern,Jason Smith, Xiaofeng Yang and Alessandro Moschitti, BART: A Modular Toolkit forCoreference Resolution, In Proceedings of the Conference on Language Resourcesand Evaluation, Marrakech, Marocco, 2008.?
Alessandro Moschitti, Kernel Methods, Syntax and Semantics for Relational TextCategorization.
In proceeding of ACM 17th Conference on Information and KnowledgeManagement (CIKM).
Napa Valley, California, 2008.?
Bonaventura Coppola, Alessandro Moschitti, and Giuseppe Riccardi.
Shallow semanticparsing for spoken language understanding.
In Proceedings of HLT-NAACL ShortPapers, pages 85?88, Boulder, Colorado, June 2009.
Association for ComputationalLinguistics.?
Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels forRelational Learning from Texts, Proceedings of The 24th Annual InternationalConference on Machine Learning  (ICML 2007).85References?
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, Proceedings of the 45th Conference of the Association for Computational Linguistics (ACL), Prague, June 2007.?
Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for Relational Learning from Texts, Proceedings of The 24th Annual International Conference on Machine Learning  (ICML 2007), Corvallis, OR, USA.?
Daniele Pighin, Alessandro Moschitti and Roberto Basili, RTV: Tree Kernels for Thematic Role Classification, Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-4), English Semantic Labeling, Prague, June 2007.?
Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semanitc Kernels for Text Classification, to appear in the 29th European Conference on Information Retrieval (ECIR), April 2007, Rome, Italy.?
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007References?
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar,Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification,Proceedings of the 45th Conference of the Association for Computational Linguistics(ACL), Prague, June 2007.?
Alessandro Moschitti, Giuseppe Riccardi, Christian Raymond, Spoken LanguageUnderstanding with Kernels for Syntactic/Semantic Structures, Proceedings of IEEEAutomatic Speech Recognition and Understanding Workshop (ASRU2007), Kyoto,Japan, December 2007?
Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and SemanticKernels for Text Classification, to appear in the 29th European Conference onInformation Retrieval (ECIR), April 2007, Rome, Italy.?
Stephan Bloehdorn, Alessandro Moschitti: Structure and semantics for expressive textkernels.
In proceeding of ACM 16th Conference on Information and   KnowledgeManagement (CIKM-short paper) 2007: 861-864, Portugal.86References?
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium onComputational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.?
Alessandro Moschitti, Efficient Convolution Kernels for Dependency and ConstituentSyntactic Trees.
In Proceedings of the 17th European Conference on MachineLearning, Berlin, Germany, 2006.?
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,Fast On-line Kernel Learning for Trees, International Conference on Data Mining(ICDM) 2006 (short paper).?
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, Alessandro Moschitti, SemanticKernels for Text Classification based on Topological Measures of Feature Similarity.
InProceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), HongKong, 18-22 December 2006.
(short paper).References?
Roberto Basili, Marco Cammisa and Alessandro Moschitti, A Semantic Kernel toclassify texts with very few training examples, in Informatica, an international journal ofComputing and Informatics, 2006.?
Fabio Massimo Zanzotto and Alessandro Moschitti, Automatic learning of textualentailments with cross-pair similarities.
In Proceedings of COLING-ACL, Sydney,Australia, 2006.?
Ana-Maria Giuglea and Alessandro Moschitti, Semantic Role Labeling via FrameNet,VerbNet and PropBank.
In Proceedings of COLING-ACL, Sydney, Australia, 2006.?
Alessandro Moschitti, Making tree kernels practical for natural language learning.
InProceedings of the Eleventh International Conference on European Association forComputational Linguistics, Trento, Italy, 2006.?
Alessandro Moschitti, Daniele Pighin and Roberto Basili.
Semantic Role Labeling viaTree Kernel joint inference.
In Proceedings of the 10th Conference on ComputationalNatural Language Learning, New York, USA, 2006.87References?
Roberto Basili, Marco Cammisa and Alessandro Moschitti, Effective use of Wordnetsemantics via kernel-based learning.
In Proceedings of the 9th Conference onComputational Natural Language Learning (CoNLL 2005), Ann Arbor (MI), USA, 2005?
Alessandro Moschitti, A study on Convolution Kernel for Shallow Semantic Parsing.
Inproceedings of the 42-th Conference on Association for Computational Linguistic(ACL-2004), Barcelona, Spain, 2004.?
Alessandro Moschitti and Cosmin Adrian Bejan, A Semantic Kernel for PredicateArgument Classification.
In proceedings of the Eighth Conference on ComputationalNatural Language Learning (CoNLL-2004), Boston, MA, USA, 2004.An introductory book on SVMs, Kernel methods and Text Categorization88Non-exhaustive reference list from other authors?
V. Vapnik.
The Nature of Statistical Learning Theory.
Springer, 1995.?
P. Bartlett and J. Shawe-Taylor, 1998.
Advances in Kernel Methods -Support Vector Learning, chapter Generalization Performance of Support Vector Machines and other Pattern Classifiers.
MIT Press.?
David Haussler.
1999.
Convolution kernels on discrete structures.Technical report, Dept.
of Computer Science, University of California atSanta Cruz.?
Lodhi, Huma, Craig Saunders, John Shawe Taylor, Nello Cristianini, and Chris Watkins.
Text classification using string kernels.
JMLR,2000?
Sch?lkopf, Bernhard and Alexander J. Smola.
2001.
Learning withKernels: Support Vector Machines, Regularization, Optimization, andBeyond.
MIT Press, Cambridge, MA, USA.Non-exhaustive reference list from other authors?
N. Cristianini and J. Shawe-Taylor, An introduction to support vectormachines (and other kernel-based learning methods) CambridgeUniversity Press, 2002?
M. Collins and N. Duffy, New ranking algorithms for parsing andtagging: Kernels over discrete structures, and the voted perceptron.
InACL02, 2002.?
Hisashi Kashima and Teruo Koyanagi.
2002.
Kernels for semi-structured data.
In Proceedings of ICML?02.?
S.V.N.
Vishwanathan and A.J.
Smola.
Fast kernels on strings and trees.
In Proceedings of NIPS, 2002.?
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders.
2003.
Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
D. Zelenko, C. Aone, and A. Richardella.
Kernel methods for relation extraction.
JMLR, 3:1083?1106, 2003.89Non-exhaustive reference list from other authors?
Taku Kudo and Yuji Matsumoto.
2003.
Fast methods for kernel-based text analysis.
In Proceedings of ACL?03.?
Dell Zhang and Wee Sun Lee.
2003.
Question classification using support vector machines.
In Proceedings of SIGIR?03, pages 26?32.?
Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
Using LTAG Based Features in Parse Reranking.
In Proceedings of EMNLP?03, 2003?
C. Cumby and D. Roth.
Kernel Methods for Relational Learning.
In Proceedings of ICML 2003, pages 107?114, Washington, DC, USA, 2003.?
J. Shawe-Taylor and N. Cristianini.
Kernel Methods for Pattern Analysis.
Cambridge University Press, 2004.?
A. Culotta and J. Sorensen.
Dependency tree kernels for relation extraction.
In Proceedings of the 42nd Annual Meeting on ACL, Barcelona, Spain, 2004.Non-exhaustive reference list from other authors?
Kristina Toutanova, Penka Markova, and Christopher Manning.
The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.
In Proceedings of EMNLP 2004.?
Jun Suzuki and Hideki Isozaki.
2005.
Sequence and Tree Kernels with Statistical Feature Mining.
In Proceedings of NIPS?05.?
Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.
Boosting based parse reranking with subtree features.
In Proceedings of ACL?05.?
R. C. Bunescu and R. J. Mooney.
Subsequence kernels for relation extraction.
In Proceedings of NIPS, 2005.?
R. C. Bunescu and R. J. Mooney.
A shortest path dependency kernel for relation extraction.
In Proceedings of EMNLP, pages 724?731, 2005.?
S. Zhao and R. Grishman.
Extracting relations with integrated information using kernel methods.
In Proceedings of the 43rd Meeting of the ACL, pages 419?426, Ann Arbor, Michigan, USA, 2005.90Non-exhaustive reference list from other authors?
J. Kazama and K. Torisawa.
Speeding up Training with Tree Kernels for Node Relation Labeling.
In Proceedings of EMNLP 2005, pages 137?144, Toronto, Canada, 2005.?
M. Zhang, J. Zhang, J. Su, , and G. Zhou.
A composite kernel to extract relations between entities with both flat and structured features.
In Proceedings of COLING-ACL 2006, pages 825?832, 2006.?
M. Zhang, G. Zhou, and A. Aw.
Exploring syntactic structured features over parse trees for relation extraction using kernel methods.
Information Processing and Management, 44(2):825?832, 2006.?
G. Zhou, M. Zhang, D. Ji, and Q. Zhu.
Tree kernel-based relation extraction with context-sensitive structured parse tree information.
In Proceedings of EMNLP-CoNLL 2007, pages 728?736, 2007.Non-exhaustive reference list from other authors?
Ivan Titov and James Henderson.
Porting statistical parsers with data-defined kernels.
In Proceedings of CoNLL-X, 2006?
Min Zhang, Jie Zhang, and Jian Su.
2006.
Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel.
In Proceedings of NAACL.?
M. Wang.
A re-examination of dependency path kernels for relation extraction.
In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP, 2008.91
