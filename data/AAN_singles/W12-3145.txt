Proceedings of the 7th Workshop on Statistical Machine Translation, pages 356?361,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsKriya - The SFU System for Translation Task at WMT-12Majid Razmara and Baskaran Sankaran and Ann Clifton and Anoop SarkarSchool of Computing ScienceSimon Fraser University8888 University DriveBurnaby BC.
V5A 1S6.
Canada{razmara, baskaran, aca69, anoop}@cs.sfu.caAbstractThis paper describes our submissions for theWMT-12 translation task using Kriya - our hi-erarchical phrase-based system.
We submittedsystems in French-English and English-Czechlanguage pairs.
In addition to the baseline sys-tem following the standard MT pipeline, wetried ensemble decoding for French-English.The ensemble decoding method improved theBLEU score by 0.4 points over the baselinein newstest-2011.
For English-Czech, we seg-mented the Czech side of the corpora andtrained two different segmented models in ad-dition to our baseline system.1 Baseline SystemsOur shared task submissions are trained in the hier-archical phrase-based model (Chiang, 2007) frame-work.
Specifically, we use Kriya (Sankaran et al,2012) - our in-house Hiero-style system for trainingand decoding.
We now briefly explain the baselinesystems in French-English and English-Czech lan-guage pairs.We use GIZA++ for word alignments and theMoses (Koehn et al, 2007) phrase-extractor for ex-tracting the initial phrases.
The translation modelsare trained using the rule extraction module in Kriya.In both cases, we pre-processed the training data byrunning it through the usual pre-processing pipelineof tokenization and lowercasing.For French-English baseline system, we traineda simplified hierarchical phrase-based model wherethe right-hand side can have at most one non-terminal (denoted as 1NT) instead of the usual twonon-terminal (2NT) model.
In our earlier experi-ments we found the 1NT model to perform com-parably to the 2NT model for close language pairssuch as French-English (Sankaran et al, 2012) at thesame time resulting in a smaller model.
We used theshared-task training data consisting of Europarl (v7),News commentary and UN documents for trainingthe translation models having a total of 15 M sen-tence pairs (we did not use the Fr-En Giga paral-lel corpus for the training).
We trained a 5-gramlanguage model for English using the English Gi-gaword (v4).For English-Czech, we trained a standard Hieromodel that has up to two non-terminals on the right-hand side.
We used the Europarl (v7), news com-mentary and CzEng (v0.9) corpora having 7.95Msentence pairs for training translation models.
Wetrained a 5-gram language model using the Czechside of the parallel corpora and did not use the Czechmonolingual corpus.The baseline systems use the following 8 stan-dard Hiero features: rule probabilities p(e|f) andp(f |e); lexical weights pl(e|f) and pl(f |e); wordpenalty, phrase penalty, language model and gluerule penalty.1.1 LM Integration in KriyaThe kriya decoder is based on a modified CYK al-gorithm similar to that of Chiang (2007).
We usea novel approach in computing the language model(LM) scores in Kriya, which deserves a mentionhere.The CKY decoder in Hiero-style systems canfreely combine target hypotheses generated in inter-356mediate cells with hierarchical rules in the highercells.
Thus the generation of the target hypothesesare fragmented and out of order in Hiero, comparedto the left to right order preferred by n-gram lan-guage models.This leads to challenges in estimating LM scoresfor partial target hypotheses and this is typically ad-dressed by adding a sentence initial marker (<s>)to the beginning of each derivation path.1 Thus thelanguage model scores for the hypothesis in the in-termediate cell are approximated, with the true lan-guage model score (taking into account sentenceboundaries) being computed in the last cell thatspans the entire source sentence.Kriya uses a novel idea for computing LM scores:for each of the target hypothesis fragment, it findsthe best position for the fragment in the final sen-tence and uses the corresponding score.
Specifi-cally, we compute three different scores correspond-ing to the three states where the fragment can endup in the final sentence, viz.
sentence initial, middleand final and choose the best score.
Thus given afragment tf consisting of a sequence of target to-kens, we compute LM scores for (i) <s> tf , (ii)tf and (iii) tf </s> and use the best score (only)for pruning.2 While this increases the number ofLM queries, we exploit the language model state in-formation in KenLM (Heafield, 2011) to optimizethe queries by saving the scores for the unchangedstates.
Our earlier experiments showed significantreduction in search errors due to this approach, inaddition to a small but consistent increase in BLEUscore (Sankaran et al, 2012).2 French-English SystemIn addition to the baseline system, we also trainedseparate systems for News and Non-News genresfor applying ensemble decoding (Razmara et al,2012).
The news genre system was trained only us-ing the news-commentary corpus (about 137K sen-1Alternately systems add sentence boundary markers (<s>and </s>) to the training data so that they are explicitly presentin the translation and language models.
While this can speedup the decoding as the cube pruning is more aggressive, it alsolimits the applicability of rules having the boundary contexts.2This ensures the the LM score estimates are never underes-timated for pruning.
We retain the LM score for fragment (caseii) for estimating the score for the full candidate sentence later.tence pairs) and the non-news genre system wastrained on the Europarl and UN documents data(14.8M sentence pairs).
The ensemble decodingframework combines the models of these two sys-tems dynamically when decoding the testset.
Theidea is to effectively use the small amount of newsgenre data in order to maximize the performance onthe news-based testsets.
In the following sections,we explain in broader detail how this system combi-nation technique works as well as the details of thisexperiment and the evaluation results.2.1 Ensemble DecodingIn the ensemble decoding framework we view trans-lation task as a domain mixing problem involvingnews and non-news genres.
The official trainingdata is from two major sources: news-commentarydata and Europarl/UN data and we hope to exploitthe distinctive nature of the two genres.
Given thatthe news data is smaller comparing to parliamen-tary proceedings data, we could tune the ensembledecoding to appropriately boost the weight for thenews genre mode during decoding.
The ensembledecoding approach (Razmara et al, 2012) takes ad-vantage of multiple translation models with the goalof constructing a system that outperforms all thecomponent models.
The key strength of this systemcombination method is that the systems are com-bined dynamically at decode time.
This enables thedecoder to pick the best hypotheses for each span ofthe input.In ensemble decoding, given a number of transla-tion systems which are already trained and tuned, allof the hypotheses from component models are usedin order to translate a sentence.
The scores of suchrules are combined in the decoder (i.e.
CKY) usingvarious mixture operations to assign a single score tothem.
Depending on the mixture operation used forcombining the scores, we would get different mix-ture scores.Ensemble decoding extends the log-linear frame-work which is found in state-of-the-art machinetranslation systems.
Specifically, the probability ofa phrase-pair (e?, f?)
in the ensemble model is:p(e?
| f?)
?
exp(w1 ?
?1?
??
?1st model?
w2 ?
?2?
??
?2nd model?
?
?
?)357where?
denotes the mixture operation between twoor more model scores.Mixture operations receive two or more scores(probabilities) and return the mixture score (prob-ability).
In this section, we explore different optionsfor this mixture operation.Weighted Sum (wsum): in wsum the ensembleprobability is proportional to the weighted sumof all individual model probabilities.p(e?
| f?)
?M?m?m exp(wm ?
?m)where m denotes the index of component mod-els, M is the total number of them and ?i is theweight for component i.Weighted Max (wmax): where the ensemble scoreis the weighted max of all model scores.p(e?
| f?)
?
maxm(?m exp(wm ?
?m))Product (prod): in prod, the probability of the en-semble model or a rule is computed as the prod-uct of the probabilities of all components (orequally the sum of log-probabilities).
Whenusing this mixture operation, ensemble de-coding would be a generalization of the log-linear framework over multiple models.
Prod-uct models can also make use of weights tocontrol the contribution of each component.These models are generally known as Logarith-mic Opinion Pools (LOPs) where:p(e?
| f?)
?
exp(M?m?m wm ?
?m)Model Switching: in model switching, each cell inthe CKY chart gets populated only by rulesfrom one of the models and the other mod-els?
rules are discarded.
This is based on thehypothesis that each component model is anexpert on different parts of sentence.
In thismethod, we need to define a binary indicatorfunction ?(f?
,m) for each span and componentmodel.?(f?
,m) =??
?1, m = argmaxn?M?(f?
, n)0, otherwiseThe criteria for choosing a model for each cell,?(f?
, n), could be based on:Max: for each cell, the model that has thehighest weighted top-rule score wins:?(f?
, n) = ?n maxe(wn ?
?n(e?, f?
))Sum: Instead of comparing only the score ofthe top rules, the model with the high-est weighted sum of the probability ofthe rules wins (taking into account thettl(translation table limit) limit on thenumber of rules suggested by each modelfor each cell):?(f?
, n) = ?n?e?exp(wn ?
?n(e?, f?
))The probability of each phrase-pair (e?, f?)
iscomputed as:p(e?
| f?)
=?m?(f?
,m) pm(e?
| f?
)Since log-linear models usually look for the bestderivation, they do not need to normalize the scoresto form probabilities.
Therefore, the scores that dif-ferent models assign to each phrase-pair may not bein the same scale.
Therefore, mixing their scoresmight wash out the information in one (or some)of the models.
We applied a heuristic to deal withthis problem where the scores are normalized overa shorter list.
So the list of rules coming from eachmodel for a certain cell in the CKY chart is normal-ized before getting mixed with other phrase-tablerules.
However, experiments showed using normal-ized scores hurts the BLEU score radically.
So weuse the normalized scores only for pruning and formixing the actual scores are used.As a more principled way, we used a toolkit,CONDOR (Vanden Berghen and Bersini, 2005), tooptimize the weights of our component models ona dev-set.
CONDOR, which is publicly available, isa direct optimizer based on Powell?s algorithm thatdoes not require explicit gradient information for theobjective function.2.2 Experiments and ResultsAs mentioned earlier all the experiments reportedfor French-English use a simpler Hiero translation358Method Devset Test-11 Test-12Baseline Hiero 26.03 27.63 28.15News data 24.02 26.47 26.27Non-news data 26.09 27.87 28.15Ensemble PROD 25.66 28.25 28.09Table 1: French-English BLEU scores.
Best performingsetting is shown in Boldface.model having at most one non-terminal (1NT) on theright-hand side.
We use 7567 sentence pairs fromnews-tests 2008 through 2010 for tuning and usenews-test 2011 for testing in addition to the 2012test data.
The feature weights were tuned usingMERT (Och, 2003) and we report the devset (IBM)BLEU scores and the testset BLEU scores computedusing the official evaluation script (mteval-v11b.pl).The results for the French-English experimentsare reported in Table 1.
We note that both baselineHiero model and the model trained from the non-news genre get comparable BLEU scores.
The newsgenre model however gets a lesser BLEU score andthis is to be expected due to the very small trainingdata available for this genre.Table 2 shows the results of applying various mix-ture operations on the devset and testset, both in nor-malized (denoted by Norm.)
and un-normalized set-tings (denoted by Base).
We present results for thesemixture operations using uniform weights (i.e.
un-tuned weights) and for PROD we also present theresults using the weights optimized by CONDOR.Most of the mixture operations outperform the Test-11 BLEU of the baseline models (shown in Table 1)even with uniform (untuned) weights.
We took thebest performing operation (i.e.
PROD) and tuned itscomponent weights using our optimizer which leadto 0.26 points improvement over its uniform-weightversion.The last row in Table 1 reports the BLEU scorefor this mixture operation with the tuned weightson the Test-12 dataset and it is marginally less thanthe baseline model.
While this is disappointing, thisalso runs counter to our empirical results from otherdatasets.
We are currently investigating this aspectas we hope to improve the robustness and applicabil-ity of our ensemble approach for different datasetsand language pairs.Mix.
Operation Weights Base Norm.WMAX uniform 27.67 27.94WSUM uniform 27.72 27.95SWITCHMAX uniform 27.96 26.21SWITCHSUM uniform 27.98 27.98PROD uniform 27.99 28.09PROD optimized 28.25 28.11Table 2: Applying ensemble decoding with different mix-ture operations on the Test-11 dataset.
Best performingsetting is shown in Boldface.3 English-Czech System3.1 Morpheme Segmented ModelFor English-Czech, we additionally experimentedusing morphologically segmented versions of theCzech side of the parallel data, since previouswork (Clifton and Sarkar, 2011) has shown that seg-mentation of morphologically rich languages canaid translation.
To derive the segmentation, webuilt an unsupervised morphological segmentationmodel using the Morfessor toolkit (Creutz and La-gus, 2007).Morfessor uses minimum description length cri-teria to train a HMM-based segmentation model.Varying the perplexity threshold in Morfessor doesnot segment more word types, but rather over-segments the same word types.
We hand tuned themodel parameters over training data size and per-plexity; these control the granularity and coverage ofthe segmentations.
Specifically, we trained differentsegmenter models on varying sets of most frequentwords and different perplexities and identified twosets that performed best based on a separate held-out set.
These two sets correspond to 500k most fre-quent words and a perplexity of 50 (denoted SM1)and 10k most frequent words and a perplexity of 20(denoted SM2).
We then used these two models tosegment the entire data set and generate two differ-ent segmented training sets.
These models had thebest combination of segmentation coverage of thetraining data and largest segments, since we foundempirically that smaller segments were less mean-ingful in the translation model.
The SM2 segmenta-tion segmented more words than SM1, but more fre-quently segmented words into single-character units.359For example, the Czech word ?dlaebn???
is brokeninto the useful components ?dlaeb + n???
by SM1, butis oversegmented into ?dl + a + e + b + n???
by SM2.However, SM1 fails to find a segmentation at all forthe related word ?dlaebn?
?mi?, while SM2 breaks itup similiarly with an additional suffix: ?dl + a + e +b + n??
+ mi?.With these segmentation models, we segmentedthe target side of the training and dev data beforetraining the translation model.
Similarly, we alsotrain segmented language models corresponding tothe two sets SM1 and SM2.
The MERT tuning stepuses the segmented dev-set reference to evaluate thesegmented hypotheses generated by the decoder foroptimizing the weights for the BLEU score.
How-ever for evaluating the test-set, we stitched the seg-ments in the decoder output back into unsegmentedforms in a post-processing step, before performingevaluation against the original unsegmented refer-ences.
The hypotheses generated by the decodercan have incomplete dangling segments where oneor more prefixes and/or suffixes are missing.
Whilethese dangling segments could be handled in a dif-ferent way, we use a simple heuristic of ignoring thesegment marker ?+?
by just removing the segmentmarker.
In next section, we report the results of us-ing the unsegmented model as well as its segmentedcounterparts.3.2 Experiments and ResultsIn the English-Czech experiments, we used the samedatasets for the dev and test sets as in French-English experiments (dev: news-tests 2008, 2009,2010 with 7567 sentence pairs and test: news-test2011 with 3003 sentence pairs).
Similarly,MERT (Och, 2003) has been used to tune the featureweights and we report the BLEU scores of two test-sets computed using the official evaluation script(mteval-v11b.pl).Table 3.2 shows the results of different segmenta-tion schemes on the WMT-11 and WMT-12 test-sets.SM1 slightly outperformed the other two models inTest-11, however the unsegmented model performedbest in Test-12, though marginally.
We are currentlyinvestigating this and are also considering the pos-sibility employing the idea of morpheme predictionin the post-decoding step in combination with thismorpheme-based translation as suggested by CliftonSegmentation Test-11 Test-12Baseline Hiero 14.65 12.40SM1 : 500k-ppl50 14.75 12.34SM2 : 10k-ppl20 14.57 12.34Table 3: The English-Czech results for different segmen-tation settings.
Best performing setting is shown in Bold-face.and Sarkar (2011).4 ConclusionWe submitted systems in two language pairs French-English and English-Czech for WMT-12 sharedtask.
In French-English, we experimented the en-semble decoding framework that effectively utilizesthe small amount of news genre data to improve theperformance in the testset belonging to the samegenre.
We obtained a moderate gain of 0.4 BLEUpoints with the ensemble decoding over the baselinesystem in newstest-2011.
For newstest-2012, it per-forms comparably to that of the baseline and we arepresently investigating the lack of improvement innewstest-2012.
For Cz-En, We found that the BLEUscores do not substantially differ from each otherand also the minor differences are not consistent forTest-11 and Test-12.ReferencesDavid Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, pages 32?42.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing, 4(1):3:1?3:34, February.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, Richard360Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 177?180.
Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of theAnnual Meeting of Association of Computational Lin-guistics, pages 160?167.Majid Razmara, George Foster, Baskaran Sankaran, andAnoop Sarkar.
2012.
Mixing multiple translationmodels in statistical machine translation.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics, Jeju, Republic of Korea,July.
Association for Computational Linguistics.
Toappear.Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.2012.
Kriya an end-to-end hierarchical phrase-basedmt system.
The Prague Bulletin of Mathematical Lin-guistics, 97(97):83?98, April.Frank Vanden Berghen and Hugues Bersini.
2005.
CON-DOR, a new parallel, constrained extension of pow-ell?s UOBYQA algorithm: Experimental results andcomparison with the DFO algorithm.
Journal of Com-putational and Applied Mathematics, 181:157?175,September.361
