Proceedings of the 43rd Annual Meeting of the ACL, pages 411?418,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsImproving Name Tagging byReference Resolution and Relation DetectionHeng Ji Ralph GrishmanDepartment of Computer ScienceNew York UniversityNew York, NY, 10003, USAhengji@cs.nyu.edu grishman@cs.nyu.eduAbstractInformation extraction systems incorpo-rate multiple stages of linguistic analysis.Although errors are typically compoundedfrom stage to stage, it is possible to re-duce the errors in one stage by harnessingthe results of the other stages.
We dem-onstrate this by using the results ofcoreference analysis and relation extrac-tion to reduce the errors produced by aChinese name tagger.
We use an N-bestapproach to generate multiple hypothesesand have them re-ranked by subsequentstages of processing.
We obtainedthereby a reduction of 24% in spuriousand incorrect name tags, and a reductionof 14% in missed tags.1 IntroductionSystems which extract relations or events from adocument typically perform a number of types oflinguistic analysis in preparation for informationextraction.
These include name identification andclassification, parsing (or partial parsing), semanticclassification of noun phrases, and coreferenceanalysis.
These tasks are reflected in the evalua-tion tasks introduced for MUC-6 (named entity,coreference, template element) and MUC-7 (tem-plate relation).In most extraction systems, these stages ofanalysis are arranged sequentially, with each stageusing the results of prior stages and generating asingle analysis that gets enriched by each stage.This provides a simple modular organization forthe extraction system.Unfortunately, each stage also introduces a cer-tain level of error into the analysis.
Furthermore,these errors are compounded ?
for example, errorsin name recognition may lead to errors in parsing.The net result is that the final output (relations orevents) may be quite inaccurate.This paper considers how interactions betweenthe stages can be exploited to reduce the error rate.For example, the results of coreference analysis orrelation identification may be helpful in name clas-sification, and the results of relation or event ex-traction may be helpful in coreference.Such interactions are not easily exploited in asimple sequential model ?
if name classificationis performed at the beginning of the pipeline, itcannot make use of the results of subsequent stages.It may even be difficult to use this information im-plicitly, by using features which are also used inlater stages, because the representation used in theinitial stages is too limited.To address these limitations, some recent sys-tems have used more parallel designs, in which asingle classifier (incorporating a wide range of fea-tures) encompasses what were previously severalseparate stages (Kambhatla, 2004; Zelenko et al,2004).
This can reduce the compounding of errorsof the sequential design.
However, it leads to avery large feature space and makes it difficult toselect linguistically appropriate features for par-ticular analysis tasks.
Furthermore, because thesedecisions are being made in parallel, it becomesmuch harder to express interactions between thelevels of analysis based on linguistic intuitions.411In order to capture these interactions more ex-plicitly, we have employed a sequential design inwhich multiple hypotheses are forwarded fromeach stage to the next, with hypotheses being rer-anked and pruned using the information from laterstages.
We shall apply this design to show hownamed entity classification can be improved by?feedback?
from coreference analysis and relationextraction.
We shall show that this approach cancapture these interactions in a natural and efficientmanner, yielding a substantial improvement inname identification and classification.2 Prior WorkA wide variety of trainable models have been ap-plied to the name tagging task, including HMMs(Bikel et al, 1997), maximum entropy models(Borthwick, 1999), support vector machines(SVMs), and conditional random fields.
Peoplehave spent considerable effort in engineering ap-propriate features to improve performance; most ofthese involve internal name structure or the imme-diate local context of the name.Some other named entity systems have exploredglobal information for name tagging.
(Borthwick,1999) made a second tagging pass which uses in-formation on token sequences tagged in the firstpass; (Chieu and Ng, 2002) used as features infor-mation about features assigned to other instancesof the same token.Recently, in (Ji and Grishman, 2004) we pro-posed a name tagging method which applied anSVM based on coreference information to filter thenames with low confidence, and used coreferencerules to correct and recover some names.
One limi-tation of this method is that in the process of dis-carding many incorrect names, it also discardedsome correct names.
We attempted to recoversome of these names by heuristic rules which arequite language specific.
In addition, this single-hypothesis method placed an upper bound on recall.Traditional statistical name tagging methodshave generated a single name hypothesis.
BBNproposed the N-Best algorithm for speech recogni-tion in (Chow and Schwartz, 1989).
Since then N-Best methods have been widely used by other re-searchers (Collins, 2002; Zhai et al, 2004).In this paper, we tried to combine the advan-tages of the prior work, and incorporate broaderknowledge into a more general re-ranking model.3 Task and TerminologyOur experiments were conducted in the context ofthe ACE Information Extraction evaluations, andwe will use the terminology of these evaluations:entity:  an object or a set of objects in one of thesemantic categories of interestmention:  a reference to an entity (typically, a nounphrase)name mention:  a reference by name to an entitynominal mention:  a reference by a common nounor noun phrase to an entityrelation:  one of a specified set of relationships be-tween a pair of entitiesThe 2004 ACE evaluation had 7 types of entities,of which the most common were PER (persons),ORG (organizations), and GPE (?geo-political enti-ties?
?
locations which are also political units, suchas countries, counties, and cities).
There were 7types of relations, with 23 subtypes.
Examples ofthese relations are ?the CEO of Microsoft?
(an em-ploy-exec relation), ?Fred?s wife?
(a family rela-tion), and ?a military base in Germany?
(a locatedrelation).In this paper we look at the problem of identify-ing name mentions in Chinese text and classifyingthem as persons, organizations, or GPEs.
BecauseChinese has neither capitalization nor overt wordboundaries, it poses particular problems for nameidentification.4 Baseline System4.1 Baseline Name TaggerOur baseline name tagger consists of a HMM tag-ger augmented with a set of post-processing rules.The HMM tagger generally follows the Nymblemodel (Bikel et al 1997), but with multiple hy-potheses as output and a larger number of states(12) to handle name prefixes and suffixes, andtransliterated foreign names separately.
It operateson the output of a word segmenter from TsinghuaUniversity.Within each of the name class states, a statisticalbigram model is employed, with the usual one-word-per-state emission.
The various probabilitiesinvolve word co-occurrence, word features, andclass probabilities.
Then it uses A* search decod-ing to generate multiple hypotheses.
Since theseprobabilities are estimated based on observations412seen in a corpus, ?back-off models?
are used toreflect the strength of support for a given statistic,as for the Nymble system.We also add post-processing rules to correctsome omissions and systematic errors using namelists (for example, a list of all Chinese last names;lists of organization and location suffixes) and par-ticular contextual patterns (for example, verbs oc-curring with people?s names).
They also deal withabbreviations and nested organization names.The HMM tagger also computes the margin ?the difference between the log probabilities of thetop two hypotheses.
This is used as a rough meas-ure of confidence in the top hypothesis (see sec-tions 5.3 and 6.2, below).The name tagger used for these experimentsidentifies the three main ACE entity types: Person(PER), Organization (ORG), and GPE (names ofthe other ACE types are identified by a separatecomponent of our system, not involved in the ex-periments reported here).4.2 Nominal Mention TaggerOur nominal mention tagger (noun group recog-nizer) is a maximum entropy tagger trained on theChinese TreeBank from the University of Pennsyl-vania, supplemented by list matching.4.3  Reference ResolverOur baseline reference resolver goes through twosuccessive stages: first, coreference rules will iden-tify some high-confidence positive and negativemention pairs, in training data and test data; thenthe remaining samples will be used as input of amaximum entropy tagger.
The features used in thistagger involve distance, string matching, lexicalinformation, position, semantics, etc.
We separatethe task into different classifiers for different men-tion types (name / noun / pronoun).
Then we in-corporate the results from the relation tagger toadjust the probabilities from the classifiers.
Finallywe apply a clustering algorithm to combine theminto entities (sets of coreferring mentions).4.4 Relation TaggerThe relation tagger uses a k-nearest-neighbor algo-rithm.
For both training and test, we consider allpairs of entity mentions where there is at most oneother mention between the heads of the two men-tions of interest1.
Each training / test example con-sists of the pair of mentions and the sequence ofintervening words.
Associated with each trainingexample is either one of the ACE relation types orno relation at all.
We defined a distance metric be-tween two examples based on?
whether the heads of the mentions match?
whether the ACE types of the heads of the mentionsmatch (for example, both are people or both are or-ganizations)?
whether the intervening words matchTo tag a test example, we find the k nearesttraining examples (where k = 3) and use the dis-tance to weight each neighbor, then select the mostcommon class in the weighted neighbor set.To provide a crude measure of the confidence ofour relation tagger, we define two thresholds, Dnearand Dfar.
If the average distance d to the nearestneighbors d < Dnear, we consider this a definite re-lation.
If Dnear < d < Dfar, we consider this a possi-ble relation.
If d > Dfar, the tagger assumes that norelation exists (regardless of the class of the nearestneighbor).5 Information from Coreference and Re-lationsOur system is processing a document consisting ofmultiple sentences.
For each sentence, the namerecognizer generates multiple hypotheses, each ofwhich is an NE tagging of the entire sentence.
Thenames in the hypothesis, plus the nouns in thecategories of interest constitute the mention set forthat hypothesis.
Coreference resolution links thesementions, assigning each to an entity.
In symbols:Si  is the i-th sentence in the document.Hi  is the hypotheses set for Sihij  is the j-th hypothesis in SiMij  is the mention set for hijmijk  is the k-th mention in Mijeijk  is the entity which mijk belongs to according tothe current reference resolution results5.1 Coreference FeaturesFor each mention we compute seven quantitiesbased on the results of name tagging and referenceresolution:1 This constraint is relaxed for parallel structures such as ?mention1, mention2,[and] mention3?.?
; in such cases there can be more than one intervening men-tion.413CorefNumijk  is the number of mentions in eijkWeightSumijk  is the sum of all the link weights be-tween mijk and other mentions in eijk , 0.8 forname-name coreference; 0.5 for apposition;0.3 for other name-nominal coreferenceFirstMentionijk  is 1 if mijk is the first name mentionin the entity; otherwise 0Headijk  is 1 if mijk includes the head word of name;otherwise 0Withoutidiomijk  is 1 if mijk is not part of an idiom;otherwise 0PERContextijk  is the number of PER context wordsaround a PER name such as a title or an ac-tion verb involving a PERORGSuffixijk  is 1 if ORG mijk includes a suffix word;otherwise 0The first three capture evidence of the correct-ness of a name provided by reference resolution;for example, a name which is coreferenced withmore other mentions is more likely to be correct.The last four capture local or name-internal evi-dence; for instance, that an organization name in-cludes an explicit, organization-indicating suffix.We then compute, for each of these seven quan-tities, the sum over all mentions k in a sentence,obtaining values for CorefNumij, WeightSumij, etc.
:CorefNum CorefNumij ijkk= ?
etc.Finally, we determine, for a given sentence andhypothesis, for each of these seven quantities,whether this quantity achieves the maximum of itsvalues for this hypothesis:BestCorefNumij ?CorefNumij = maxq CorefNumiq   etc.We will use these properties of the hypothesis asfeatures in assessing the quality of a hypothesis.5.2 Relation Word ClustersIn addition to using relation information forreranking name hypotheses, we used the relationtraining corpus to build word clusters which couldmore directly improve name tagging.
Name tag-gers rely heavily on words in the immediate con-text to identify and classify names; for example,specific job titles, occupations, or family relationscan be used to identify people names.
Such wordsare learned individually from the name tagger?straining corpus.
If we can provide the name taggerwith clusters of related words, the tagger will beable to generalize from the examples in the trainingcorpus to other words in the cluster.The set of ACE relations includes several in-volving employment, social, and family relations.We gathered the words appearing as an argumentof one of these relations in the training corpus,eliminated low-frequency terms and manually ed-ited the ten resulting clusters to remove inappro-priate terms.
These were then combined with lists(of titles, organization name suffixes, location suf-fixes) used in the baseline tagger.5.3 Relation FeaturesBecause the performance of our relation taggeris not as good as our coreference resolver, we haveused the results of relation detection in a relativelysimple way to enhance name detection.
The basicintuition is that a name which has been correctlyidentified is more likely to participate in a relationthan one which has been erroneously identified.For a given range of margins (from the HMM),the probability that a name in the first hypothesis iscorrect is shown in the following table, for namesparticipating and not participating in a relation:Margin In Relation(%) Not in Relation(%)<4 90.7 55.3<3 89.0 50.1<2 86.9 42.2<1.5 81.3 28.9<1.2 78.8 23.1<1 75.7 19.0<0.5 66.5 14.3Table 1 Probability of a name being correctTable 1 confirms that names participating in re-lations are much more likely to be correct thannames that do not participate in relations.
We alsosee, not surprisingly, that these probabilities arestrongly affected by the HMM hypothesis margin(the difference in log probabilities) between thefirst hypothesis and the second hypothesis.
So it isnatural to use participation in a relation (coupledwith a margin value) as a valuable feature for re-ranking name hypotheses.Let mijk be the k-th name mention for hypothe-sis hij of sentence; then we define:414Inrelationijk  = 1 if mijk  is in a definite relation= 0 if mijk is in a possible relation= -1 if mijk is not in a relationInrelation Inrelationij ijkk= ?Mostrelated Inrelation Inrelationij ij q iq?
=( max )Finally, to capture the interaction with the margin,we let zi  = the margin for sentence Si and dividethe range of values of zi into six intervals Mar1, ?Mar6.
And we define the hypothesis ranking in-formation: FirstHypothesisij = 1 if j =1; otherwise 0.We will use as features for ranking hij the con-junction of Mostrelatedij, zi ?
Marp (p = 1, ?, 6),and FirstHypothesisij .6 Using the Information from Corefer-ence and Relations6.1 Word Clustering based on RelationsAs we described in section 5.2, we can generateword clusters based on relation information.
If aword is not part of a relation cluster, we consider itan independent (1-word) cluster.The Nymble name tagger (Bikel et al, 1999) re-lies on a multi-level linear interpolation model forbackoff.
We extended this model by adding a levelfrom word to cluster, so as to estimate more reli-able probabilities for words in these clusters.
Table2 shows the extended backoff model for each ofthe three probabilities used by Nymble.TransitionProbabilityFirst-WordEmissionProbabilityNon-First-WordEmissionProbabilityP(NC2|NC1,<w1, f1>)P(<w2,f2>|NC1, NC2)P(<w2,f2>|<w1,f1>, NC2)P(<Cluster2,f2>|NC1, NC2)P(<Cluster2,f2>|<w1,f1>, NC2)P(NC2|NC1,<Cluster1,f1>)P(<Cluster2,f2>|<+begin+, other>,NC2)P(<Cluster2,f2>|<Cluster1,f1>,NC2)P(NC2|NC1) P(<Cluster2, f2>|NC2)P(NC2)  P(Cluster2|NC2) * P(f2|NC2)1/#(nameclasses)1/#(cluster)  *  1/#(word features)Table2 Extended Backoff Model6.2 Pre-pruning by MarginThe HMM tagger produces the N best hypothesesfor each sentence.2  In order to decide when weneed to rely on global (coreference and relation)information for name tagging, we want to havesome assessment of the confidence that the nametagger has in the first hypothesis.
In this paper, weuse the margin for this purpose.
A large marginindicates greater confidence that the first hypothe-sis is correct.3  So if the margin of a sentence isabove a threshold, we select the first hypothesis,dropping the others and by-passing the reranking.6.3 Re-ranking based on CoreferenceWe described in section 5.1, above, the coreferencefeatures which will be used for reranking the hy-potheses after pre-pruning.
A maximum entropymodel for re-ranking these hypotheses is thentrained and applied as follows:Training1.
Use K-fold cross-validation to generate multi-ple name tagging hypotheses for each docu-ment in the training data Dtrain (in each of the Kiterations, we use K-1 subsets to train theHMM and then generate hypotheses from theKth subset).2.
For each document d in Dtrain, where d includesn sentences S1?SnFor i = 1?n, let m = the number of hy-potheses for Si(1) Pre-prune the candidate hypotheses us-ing the HMM margin(2) For each hypothesis hij, j = 1?m(a) Compare hij with the key, set theprediction Valueij ?Best?
or ?NotBest?
(b) Run the Coreference Resolver onhij and the best hypothesis for eachof the other sentences, generateentity results for each candidatename in hij(c) Generate a coreference feature vec-tor Vij for hij(d) Output Vij and Valueij2 We set different N = 5, 10, 20 or 30 for different margin ranges, by cross-validation checking the training data about the ranking position of the besthypothesis for each sentence.
With this N, optimal reranking (selecting the besthypothesis among the N best) would yield Precision = 96.9 Recall = 94.5 F =95.7 on our test corpus.3 Similar methods based on HMM margins were used by (Scheffer et al, 2001).4153.
Train Maxent Re-ranking system on all Vij andValueijTest1.
Run the baseline name tagger to generate mul-tiple name tagging hypotheses for each docu-ment in the test data Dtest2.
For each document d in Dtest, where d includesn sentences S1?Sn(1) Initialize: Dynamic input of coreference re-solver H = {hi-best | i = 1?n, hi-best is thecurrent best hypothesis for Si}(2) For i = 1?n, assume m = the number ofhypotheses  for Si(a) Pre-prune the candidate hypotheses us-ing the HMM margin(b) For each hypothesis hij, j = 1?m?
hi-best = hij?
Run the Coreference Resolver on H,generate entity results for each namecandidate in hij?
Generate a coreference feature vec-tor Vij for hij?
Run Maxent Re-ranking system onVij, produce Probij of ?Best?
value(c) hi-best = the hypothesis with highestProbij of ?Best?
value, update H andoutput hi-best6.4 Re-ranking based on RelationsFrom the above first-stage re-ranking by corefer-ence, for each hypothesis we got the probability ofits being the best one.
By using these results andrelation information we proceed to a second-stagere-ranking.
As we described in section 5.3, the in-formation of ?in relation or not?
can be used to-gether with margin as another important measureof confidence.In addition, we apply the mechanism of weightedvoting among hypotheses (Zhai et al, 2004) as anadditional feature in this second-stage re-ranking.This approach allows all hypotheses to vote on apossible name output.
A recognized name is con-sidered correct only when it occurs in more than 30percent of the hypotheses (weighted by their prob-ability).In our experiments we use the probability pro-duced by the HMM, probij , for hypothesis hij .
Wenormalize this probability weight as:Wprobprobijijiqq= ?exp( )exp( )For each name mention mijk in hij , we define:Occur mq ijk( )  = 1 if mijk occurs in hq= 0 otherwiseThen we count its voting value as follows:Votingijk  is 1 if W Occur miq q ijkq??
( ) >0.3;otherwise 0.The voting value of hij is:Voting Votingij ijkk= ?Finally we define the following voting feature:BestVoting Voting Votingij ij q iq?
=( max )This feature is used, together with the featuresdescribed at the end of section 5.3 and the prob-ability score from the first stage, for the second-stage maxent re-ranking model.One appeal of the above two re-ranking algo-rithms is its flexibility in incorporating featuresinto a learning model: essentially any coreferenceor relation features which might be useful in dis-criminating good from bad structures can be in-cluded.7 System PipelineCombining all the methods presented above, theflow of our final system is shown in figure 1.8 Evaluation Results8.1 Training and Test DataWe took 346 documents from the 2004 ACE train-ing corpus and official test set, including bothbroadcast news and newswire, as our blind test set.To train our name tagger, we used the Beijing Uni-versity Insititute of Computational Linguistics cor-pus ?
2978 documents from the People?s Daily in1998 ?
and 667 texts in the training corpus for the2003 & 2004 ACE evaluation.
Our reference re-solver is trained on these 667 ACE texts.
The rela-tion tagger is trained on 546 ACE 2004 texts, fromwhich we also extracted the relation clusters.
Thetest set included 11715 names: 3551 persons, 5100GPEs and 3064 organizations.416Figure 1  System Flow8.2 Overall Performance ComparisonTable 3 shows the performance of the baseline sys-tem; Table 4 is the system with relation word clus-ters; Table 5 is the system with both relationclusters and re-ranking based on coreference fea-tures; and Table 6 is the whole system with sec-ond-stage re-ranking using relations.The results indicate that relation word clustershelp to improve the precision and recall of mostname types.
Although the overall gain in F-score issmall (0.7%), we believe further gain can beachieved if the relation corpus is enlarged in thefuture.
The re-ranking using the coreference fea-tures had the largest impact, improving precisionand recall consistently for all types.
Compared toour system in (Ji and Grishman, 2004), it helps todistinguish the good and bad hypotheses withoutany loss of recall.
The second-stage re-ranking us-ing the relation participation feature yielded asmall further gain in F score for each type, improv-ing precision at a slight cost in recall.The overall system achieves a 24.1% relative re-duction on the spurious and incorrect tags, and14.3% reduction in the missing rate over a state-of-the-art baseline HMM trained on the same material.Furthermore, it helps to disambiguate many nametype errors: the number of cases of type confusionin name classification was reduced from 191 to102.Name Precision Recall FPER 88.6 89.2 88.9GPE 88.1 84.9 86.5ORG 88.8 87.3 88.0ALL 88.4 86.7 87.5Table 3 Baseline Name TaggerName Precision Recall FPER 89.4 90.1 89.7GPE 88.9 85.8 89.4ORG 88.7 87.4 88.0ALL 89.0 87.4 88.2Table 4 Baseline + Word Clustering by RelationName Precision Recall FPER 90.1 91.2 90.5GPE 89.7 86.8 88.2ORG 90.6 89.8 90.2ALL 90.0 88.8 89.4Table 5 Baseline + Word Clustering by Relation +Re-ranking by CoreferenceName Precision Recall FPER 90.7 91.0 90.8GPE 91.2 86.9 89.0ORG 91.7 89.1 90.4ALL 91.2 88.6 89.9Table 6 Baseline + Word Clustering by Relation +Re-ranking by Coreference +Re-ranking by RelationIn order to check how robust these methods are,we conducted significance testing (sign test) on the346 documents.
We split them into 5 folders, 70documents in each of the first four folders and 66in the fifth folder.
We found that each enhance-ment (word clusters, coreference reranking, rela-tion reranking) produced an improvement in Fscore for each folder, allowing us to reject the hy-pothesis that these improvements were random at a95% confidence level.
The overall F-measure im-provements (using all enhancements) for the 5folders were: 2.3%, 1.6%, 2.1%, 3.5%, and 2.1%.HMM Name Tagger, wordclustering based on rela-tions, pruned by marginMultiple namehypothesesMaxent Re-rankingby coreferenceSingle namehypothesisPost-processingby heuristic rulesInputNominalMentionTaggerNominalMentionsRelationTaggerMaxent Re-rankingby relationCoreferenceResolver4179 ConclusionThis paper explored methods for exploiting theinteraction of analysis components in an informa-tion extraction system to reduce the error rate ofindividual components.
The ACE task hierarchyprovided a good opportunity to explore these inter-actions, including the one presented here betweenreference resolution/relation detection and nametagging.
We demonstrated its effectiveness forChinese name tagging, obtaining an absolute im-provement of 2.4% in F-measure (a reduction of19% in the (1 ?
F) error rate).
These methods arequite low-cost because we don?t need any extraresources or components compared to the baselineinformation extraction system.Because no language-specific rules are involvedand no additional training resources are required,we expect that the approach described here can bestraightforwardly applied to other languages.
Itshould also be possible to extend this re-rankingframework to other levels of analysis in informa-tion extraction ?- for example, to use event detec-tion to improve name tagging; to incorporatesubtype tagging results to improve name tagging;and to combine name tagging, reference resolutionand relation detection to improve nominal mentiontagging.
For Chinese (and other languages withoutovert word segmentation) it could also be extendedto do character-based name tagging, keeping mul-tiple segmentations among the N-Best hypotheses.Also, as information extraction is extended to cap-ture cross-document information, we should expectfurther improvements in performance of the earlierstages of analysis, including in particular nameidentification.For some levels of analysis, such as name tag-ging, it will be natural to apply lattice techniques toorganize the multiple hypotheses, at some gain inefficiency.AcknowledgementsThis research was supported by the Defense Ad-vanced Research Projects Agency under GrantN66001-04-1-8920 from SPAWAR San Diego,and by the National Science Foundation underGrant 03-25657.
This paper does not necessarilyreflect the position or the policy of the U.S. Gov-ernment.ReferencesDaniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance Learning Name-finder.
Proc.
FifthConf.
on Applied Natural Language Processing,Washington, D.C.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. Disser-tation, Dept.
of Computer Science, New YorkUniversity.Hai Leong Chieu and Hwee Tou Ng.
2002.
Named En-tity Recognition: A Maximum Entropy Approach Us-ing Global Information.
Proc.
: 17th Int?l Conf.
onComputational Linguistics (COLING 2002), Taipei,Taiwan.Yen-Lu Chow and Richard Schwartz.
1989.
The N-BestAlgorithm: An efficient Procedure for Finding Top NSentence Hypotheses.
Proc.
DARPA Speech andNatural Language WorkshopMichael Collins.
2002.
Ranking Algorithms for Named-Entity Extraction: Boosting and the Voted Percep-tron.
Proc.
ACL 2002Heng Ji and Ralph Grishman.
2004.
Applying Corefer-ence to Improve Name Recognition.
Proc.
ACL 2004Workshop on Reference Resolution and Its Applica-tions, Barcelona, SpainN.
Kambhatla.
2004.
Combining Lexical, Syntactic, andSemantic Features with Maximum Entropy Modelsfor Extracting Relations.
Proc.
ACL 2004.Tobias Scheffer, Christian Decomain, and StefanWrobel.
2001.
Active Hidden Markov Models for In-formation Extraction.
Proc.
Int?l Symposium on In-telligent Data Analysis (IDA-2001).Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets.2004.
Binary Integer Programming for InformationExtraction.
ACE Evaluation Meeting, September2004, Alexandria, VA.Lufeng Zhai, Pascale Fung, Richard Schwartz, MarineCarpuat, and Dekai Wu.
2004.
Using N-best Lists forNamed Entity Recognition from Chinese Speech.Proc.
NAACL 2004 (Short Papers)418
