...
:?!.,:~.
:  ~:~i~::'~:::i~~i~ ~:i~.!!?.::!~!:!~i,i-~.
?
.:;:..
: ~::":N~RTELHORTI'I~ RH TELECOMRecognition ofspontaneous speechPeter StubleyNorte\[ OpenSpeechAbstractCurrent speech recognition systems are capable of performing complex tasksfor co-operative users by determining their requirements through aconversation.
Most systems have been constructed without attempting toaccurately model spontaneous speech.
Some components, such as the parser,can be easily made robust o some of the artifacts of conversational speech.Others, such as the pronunciation models, simply ignore the possibility thatincomplete words can occur.
This results in some recognition errors, and maycause the application to begin to perform the wrong the action.
Typically,however, the next several conversation turns can identify and correct he error.This talk gives a brief overview of state-of-the-art of spoken language systemsand describes how some of the components are affected by artifacts ofspontaneous speech.Large bodies of accurately transcribed spontaneous speech are required tolearn the properties of spontaneous events.
: ;:i:!
fi~!~i i,~i O Ut l i  n eN~RTELNORTHERN TELECOM?
Components of a speech recognition system.?
Modeling speech:- acoust i c  mode ls .- p ronunc ia t ion  mode ls .- l anguage mode ls .?
Natural language understanding.?
Discourse management.?
Effects of spontaneous speech.Norte\[ OpenSpeechI ~ode\]~ I"~ I \i.I Soarc !
\\ ,  ' , I Manager l I / -"~ Under- ~ , ' '  / \[ standing\] \] i / ?i J Appli- \]1 /; \] cation I f /  : , , i /N~RTELNORTHERN TELECOMNorte\[ OpenSpeechThe search engine matches the user's speech to the most likely path throughthe search graph.
The search graph is specified by the acoustic models, andthe language model (where the language model includes pronunciation).
Themost likely path corresponds toa word sequence.
The matching is usuallyaccomplished using a coarse quantization of the speech spectrum and somevariation of the Viterbi algorithm (dynamic programming).This word sequence is passed to the discourse manager who in turn passes it tothe understanding component.
The meaning is extracted and returned to thediscourse manager.
The discourse manager takes the appropriate action andtells the user the result.X3ib;delmcj Speechl ag~ ~i~';i ~i~;~i~i!~;i;:~ii  !~:!~: ,,: ~!,; :::.
: ...N~RTELNORTHERN TELECOM?
Acoust i c  mode ls .- Model speech sounds, typically phone-based models.?
P ronunc ia t ion .- How words are pronounced, typically concatenations ofacoustic models as defined by the lexicon.?
Language mode l .- How words may be connected together, typically statisticalfor large applications.NorteL OpenSpeechThe acoustic models are usually some variation of hidden Markov models(HMMs).
The state sequence helps to model the quasi-stationarity of speechwith discrete jumps from one type of statistics to another (such as a transitionfrom a fricative to a vowel).
Each acoustic model typically corresponds toaphone in a particular context.
Acoustic models are trained from a large corpusof transcribed data.Words models are constructed by determining the pronunciation f each wordin a lexicon.
The string of phonemes i  mapped to a sequence of acousticmodels - the resulting chain of models becomes the model for the word.
Withthis approach, models can be constructed for each word without actuallyhaving training data specifically for that word.The language model describes how words may be connected together.
Themost common language models for large applications are purely statitistical,with the most common ones being the defined by the previous everal words.Bigram models give the probability of each word depending on the previousword and trigram models give the probability of each word depending on theprevious two words.
Language models are trained on large corpora of text aswell as the transcribed data used to train acoustic models.~ 'ii.~ U  .... ~g'~;~:~ :naturalN~RTELNORTHERN TELECOMExtract meaning from natural language in a normalizedmanner.- Typically some variation of CFG rules.Robust parsers do not require complete parses.- Robust to ungrammatical speech and recognition errors.ilnm tlNorte\[ OpenSpeechThe NLU component attempts to extract he meaning from the recognizedword string.
This is typically accomplished by matching CFG rules to buildparse trees.
For example, a number can be represented by a digit followed byanother number or simply a digit by itself.
Recursively applying this rule canrepresent any number.In most cases, a robust parser is used.
A robust parser is similar to a normalparser, but does not require a complete parse to succeed.
Instead, a forest ofparse trees is found that represent the highest level rules that could be found.With this approach, words that do not fit any rule (because of ungrammaticalspeech or misrecognitions) are simply left out of the parse, and the parserreturns what it can find.
For the above example, since the parser only has arule for a number, it will extract only "one five six four" and ignore "I'll,""take," and "please.
"15~.~ ~!~i!~i}}i~~ilaln a g Q me n tN~RTELNOHHERH TELECOM~Hutomated Broker.
~____~pen-ended Iow may I help y u~ L_ prompt I~wen~-fi,,e ~e~.L.~d hree thousand~ ollars.s made you rich~ l Action of "findoowat's the pric~ idgets Inc.~~ Nortel?
jJ~Wow.
Sell al~Qmy shares.
J/remembered, remembered.INorte\[ OpenSpeechThe discourse manager is responsible for carrying on the conversation with theuser.
Ambiguous or unclear information is clarified by the DM throughout thedialogue by:?
asking the user what they meant or prompting for the missing information.?
confirming the most likely interpretation.In a spontaneous dialogue, the discourse manager is required to infer thingsbased on the history of the conversation.
For example, if the previous requestwas for the price of a stock and the subsequent request gives only a stock, themost likely interpretation is that the user also wants the price of the subsequentstock.The discourse manager also interacts with the actual application, such as a database, voice mail/e-mail server, etc.
Thus, for each application, the discoursemanager must understand the requirements of the application, how to expressthe user's request to the application, and how to interpret the response from theapplication.6N RTELNORTHE?N TELECOM?
Phrases may frequently be ungrammatical.- Sentence fragments, sloppy usage.?
Examples:- Reserve tickets two people ten o'clock show.- Gotta go now.?
Natura l  language may a lso  include idiomaticexpressions.?
Examples :- "That's cool!"
likely has nothing to do with temperature.Nor te \ [  OpenSpeechPeople frequently speak in sentence fragments or use innovative constructionsfor effect.
The meaning is clear, but no self-respecting grammar textbookwould permit it.
Robust parsing solves many of these cases.Although the number of rules in the grammar can be increased to include all ofthe possible variations, a number of new problems will be introduced:?
computational complexity of a large number of rules.?
maintaining and debugging the rules is very difficult.
New rules will likelyhave unforeseen consequences and may conflict with existing rules.?
determining and writing all of the rules in the first place is time-consuming.Writing rules is similar to writing a program in any other computer language.Unless they are carefully designed, large programs are brittle and difficult omake bullet-proof.Natural anguage also includes many other effects, such as idiomaticexpressions and puns.
Some of these will change with time.
Extracting themeaning from expressions containing these can be difficult.
In practice,particularly given the state of today's voice synthesizers, people realize thatthey are speaking to a machine and will adjust heir language accordingly.The sentence fragments will remain, but some of these effects will be muchrarer than they will be in conversation with another human.
Thanks to StarTrek, people are used to computers interpreting idiomatic expressions literally.7N RTELNORTffERN TELECOM?
Res tar ts ,  cor rect ions ,  and  f i l l ed  pauses .- Can result in incomplete words.?
But recognition engines typically only model complete words.- Restarts are, by definition, rare events and thus always havelow probability.?
Events with low probability are frequently misrecognized.?
Examples :- I'll take the red no the black one.- It's uh the one on the uh left.- Give me fir- no twenty.Nor te \ [  OpenSpeechFilled pauses are perhaps the easiest to deal with.
Words representing thefilled pauses (such as ah, um, uh) can be added to the recognizer's lexicon andincorporated into the language model.Restarts and corrections that do not include incomplete words also affect onlythe language model.
The presence of an indicator word or phrase followed bya phrase similar to the correction can be incorporated in the language model.The biggest difficulty is that statistical language models typically have limitedhistories, and thus the fact that the following phrase is similar to the precedingphrase is usually lost.Incomplete words are the most difficult o handle.
The lexicon only containscomplete words.
Incomplete words can be permitted but this typically resultsin an explosion of the number of paths in the search graph and manyincomplete words will be easily confused with other words.
Including themeverywhere is likely to make things worse, both in terms of accuracy andspeed.nta  n e o u s N RTEL NOETHERN TELECOM?
Spontaneous events are not completely random.- For example, breaks for incomplete words are likely to occuronly at (or near) syllable boundaries.?
They are almost always rare events in any givenword sequence.- Unlikely events are difficult to model accurately.IAccurately transcribed ata are required to Iunderstand these events.
INorte\[ OpenSpeechSpontaneous events are not completely random.
They are also affected by thespeaker's tyle - some people have few filled pauses, others many.Spontaneous events often appear to be mostly uncorrelated with the actualword sequence.
As a result, they do no fit well into typical statistical languagemodels.
Since they are not modeled well by the statistical model, they alwaysappear to be unlikely.The probability of spontaneous events at any particular place is low; theprobability of spontaneous events occurring during any conversation is high.With large bodies of accurately transcribed ata, models that attempt toincorporate spontaneous events can be constructed.
The more accuratelyspontaneous events can be modeled, the better they will be recognized.~ ~ ~ "  , .
.
, .
; ~  ~ .
~ ; ~ ~  ~- .
.
:  ..... :.
.N~RTELNO~THE2N TELECOMUh, mis- norecognition of uh spon-spontaneous speechNortel OpenSpeechBibliography and references?
Deller, J.R., Proakis, J.G., and Hansen, J.H.L., Discrete-time processing ofspeech signals, MacMillan, 1993.?
Lee, C-H, Soong, F.K., and Paliwal, K.K., Automatic speech and speakerrecognition, advanced topics, Kluwer Academic Publishers, 1996.?
O' Shaughnessy, D., Speech Communication, Addison-Wesley, 1987.?
Rabiner, L.R., "A tutorial on hidden Markov models and selectedapplications inspeech recognition," Proceedings of the IEEE, Vol.
77, No.
2,February 1989, pp.
257-285.10
