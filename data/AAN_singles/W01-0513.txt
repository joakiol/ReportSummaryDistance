Is Knowledge-Free Induction of Multiword UnitDictionary Headwords a Solved Problem?Patrick Schone and Daniel JurafskyUniversity of Colorado, Boulder CO 80309{schone, jurafsky}@cs.colorado.eduAbstractWe seek a knowledge-free method for inducingmultiword units from text corpora for use asmachine-readable dictionary headwords.
Weprovide two major evaluations of nine existingcollocation-finders and  illustrate the continuingneed for improvement.
We use Latent SemanticAnalysis to make modest gains in performance, butwe show the significant challenges encountered  intrying this approach.1 IntroductionA multiword unit (MWU) is a connectedcollocation: a sequence of neighboring words?whose exact and unambiguous meaning orconnotation cannot be derived from the meaning orconnotation of its components?
(Choueka, 1988).
Inother words, MWUs are typically non-compositionalat some linguistic level.
For example, phonologicalnon-compositionality has been observed (Finke &Weibel, 1997; Gregory, et al 1999) where wordslike ?got?
[g<t] and ?to?
[tu] change phonetically to?gotta?
[g<rF] when combined.
We have interest ininducing headwords for machine-readabledictionaries (MRDs), so our interest is in semanticrather than phonological non-compositionality.
Asan example of semantic non-compositionality,consider ?compact disk?
: one could not deduce thatit was a music medium by only considering thesemantics of ?compact?
and ?disk.
?MWUs may also be non-substitutable and/ornon-modifiable (Manning and Sch?tze, 1999).
Non-substitutability implies that substituting a word ofthe MWU with its synonym should no longerconvey the same original content: ?compact disk?does not readily imply ?densely-packed disk.?
Non-modifiability, on the other hand, suggests onecannot modify the MWU?s structure and still conveythe same content: ?compact disk?
does not signify?disk that is compact.
?MWU dictionary headwords generally satisfy atleast one of these constraints.
For example, acompositional phrase would typically be excludedfrom a hard-copy dictionary since its constituentwords would already be listed.
These strategiesallow hard-copy dictionaries to remain compact.As mentioned, we wish to find MWU headwordsfor machine-readable dictionaries (MRDs).Although space is not an issue in MRDs, we desireto follow the lexicographic practice of reducingredundancy.
As Sproat indicated, "simplyexpanding the dictionary to encompass every wordone is ever likely to encounter is wrong: it fails totake advantage of regularities" (1992, p. xiii).
Ourgoal is to identify an automatic, knowledge-freealgorithm that finds all and only those collocationswhere it is necessary to supply a definition.?Knowledge-free?
means that the process shouldproceed without human input (other than, perhaps,indicating whitespace and punctuation).This seems like a solved problem.
Manycollocation-finders exist, so one might suspect thatmost could suffice for finding MWU dictionaryheadwords.
To verify this, we evaluate nineexisting collocation-finders to see which bestidentifies valid headwords.
We evaluate using twocompletely separate gold standards: (1) WordNetand (2) a compendium of Internet dictionaries.Although web-based resources are dynamic andhave better coverage than WordNet (especially foracronyms and names), we show that WordNet-basedscores are comparable to those using InternetMRDs.
Yet the evaluations indicate that significantimprovement is still needed in MWU-induction.As an attempt to improve MWU headwordinduction, we introduce several algorithms usingLatent Semantic Analysis (LSA).
LSA is atechnique which automatically induces semanticrelationships between words.
We use LSA to try toeliminate proposed MWUs which are semanticallycompositional.
Unfortunately, this does not help.Yet when we use LSA to identify substitutable delimiters.
This suggests that in a language withMWUs, we do show modest performance gains.
whitespace, one might prefer to begin at the word2 Previous ApproachesFor decades, researchers have explored varioustechniques for identifying interesting collocations.There have essentially been three separate kinds ofapproaches for accomplishing this task.
Theseapproaches could be broadly classified into (1)segmentation-based, (2) word-based and knowledge-driven, or (3) word-based and probabilistic.
We willillustrate strategies that have been attempted in eachof the approaches.
Since we assume knowledge ofwhitespace, and since many of the first and all of thesecond categories rely upon human input, we will bemost interested in the third category.2.1 Segmentation-driven StrategiesSome researchers view MWU-finding as a naturalby-product of segmentation.
One can regard text asa stream of symbols and segmentation as a means ofplacing delimiters in that stream so as to separatelogical groupings of symbols from one another.
Asegmentation process may find that a symbol streamshould not be delimited even though subcomponentsof the stream have been seen elsewhere.
In suchcases, these larger units may be MWUs.The principal work on segmentation has focusedeither on identifying words in phonetic streams(Saffran, et.
al, 1996; Brent, 1996; de Marcken,1996) or on tokenizing Asian and Indian languagesthat do not normally include word delimiters in theirorthography (Sproat, et al 1996; Ponte and Croft1996; Shimohata, 1997; Teahan, et al, 2000; andmany others).
Such efforts have employed variousstrategies for segmentation, including the use ofhidden Markov models, minimum descriptionlength, dictionary-based approaches, probabilisticautomata, transformation-based learning, and textcompression.
Some of these approaches requiresignificant sources of human knowledge, thoughothers, especially those that follow datacompression or HMM schemes, do not.These approaches could be applied to languageswhere word delimiters exist (such as in Europeanlanguages delimited by the space character).However, in such languages, it seems more prudentto simply take advantage of delimiters rather thanintroducing potential errors by trying to find wordboundaries while ignoring knowledge of thelevel and identify appropriate word combinations.2.2 Word-based, knowledge-driven StrategiesSome researchers start with words and proposeMWU induction methods that make use of parts ofspeech, lexicons, syntax or other linguistic structure(Justeson and Katz, 1995; Jacquemin, et al, 1997;Daille, 1996).
For example, Justeson and Katzindicated that the patterns NOUN  NOUN and ADJNOUN are very typical of MWUs.
Daille alsosuggests that in French, technical MWUs followpatterns such as ?NOUN de NOUN" (1996, p. 50).To find word combinations that satisfy such patternsin both of these situations necessitates the use of alexicon equipped with part of speech tags.
Since weare interested in knowledge-free induction ofMWUs, these approaches are less directly related toour work.
Furthermore, we are not really interestedin identifying constructs such as general nounphrases as the above rules might generate, butrather, in finding only those collocations that onewould typically need to define.2.3 Word-based, Probabilistic ApproachesThe third category assumes at most whitespaceand punctuation knowledge and attempts to inferMWUs using word combination probabilities.Table 1 (see next page) shows nine commonly-usedprobabilistic MWU-induction approaches.
In thetable,  f  and P  signify frequency and probabilityX Xof a word X.
A variable XY indicates a word bigramand   indicates its expected frequency at random.XYAn overbar signifies a variable?s complement.
Formore details, one can consult the original sources aswell as Ferreira and Pereira (1999) and Manningand Sch?tze (1999).3 Lexical AccessPrior to applying the algorithms, we lemmatizeusing a weakly-informed tokenizer that knows onlythat whitespace and punctuation separate words.Punctuation can either be discarded or treated aswords.
Since we are equally interested in findingunits like ?Dr.?
and ?U.
S.,?
we opt to treatpunctuation as words.Once we tokenize, we use Church?s (1995) suffixarray approach to identify word n-grams that occurat least T times (for T=10).
We then rank-order thePX|YMIXYMZ PrZ|YMIZY2log[PX PYPX PY]fY[PXYPXY]fXY [PXYPXY]fXYMi{X,X}j{Y,Y}(fij 	 ij )2ijfXY 	 XYXY (1	(XY/N))fXY 	 XYfXY (1	(fXY/N))Table 1: Probabilistic ApproachesMETHOD FORMULAFrequency(Guiliano, 1964)fXYPointwise MutualInformation (MI)(Fano, 1961;Church and Hanks,1990)log  (P  / P P )2 XY X YSelectionalAssociation(Resnik, 1996)SymmetricConditionalProbability(Ferreira andPereira, 1999)P  / P PXY X Y2Dice Formula(Dice, 1945) 2 f / (f +f )XY X YLog-likelihood(Dunning, 1993; (Daille, 1996).
Since we need knowledge-poorDaille, 1996) induction, we cannot use human-suggested filteringChi-squared ($ )2(Church and Gale,1991)Z-Score(Smadja, 1993;Fontenelle, et al,1994)Student?s t-Score(Church andHanks, 1990)n-gram list in accordance to each probabilisticalgorithm.
This task is non-trivial since mostalgorithms were originally suited for finding two-word collocations.
We must therefore decide howto expand the algorithms to identify general n-grams(say, C=w w ...w ).
We can either generalize or1 2 napproximate.
Since generalizing requiresexponential compute time and memory for severalof the algorithms, approximation is an attractivealternative.One approximation redefines X and Y to be,respectively, the word sequences w w ...w  and1 2 iw w ...w  where i is chosen to maximize P P .i+1 i+2 n, X YThis has a natural interpretation of being theexpected probability of concatenating the two mostprobable substrings in order to form the larger unit.Since it can be computed rapidly with low memorycosts, we use this approximation.Two additional issues need addressing beforeevaluation.
The first regards document sourcing.
Ifan n-gram appears in multiple sources (eg.,Congressional Record versus Associated Press),  itslikelihood of accuracy should increase.
This isparticularly true if we are looking for MWUheadwords for a general versus specializeddictionary.
Phrases that appear in one source mayin fact be general MWUs, but frequently, they aretext-specific units.
Hence, precision gained byexcluding single-source n-grams may be worthlosses in recall.
We will measure this trade-off.Second, evaluating with punctuation as words andapplying no filtering mechanism may unfairly biasagainst some algorithms.
Pre- or post-processing ofn-grams with a linguistic filter has shown toimprove some induction algorithms?
performancerules as in Section 2.2.
Yet we can filter by pruningn-grams whose beginning or ending word is amongthe top N most frequent words.
This unfortunatelyeliminates acronyms like ?U.
S.?
and phrasal verbslike ?throw up.?
However, discarding some wordsmay be worthwhile if the final list of n-grams isricher in terms of MRD headwords.
We thereforeevaluate with such an automatic filter, arbitrarily(and without optimization) choosing  N=75.4 Evaluating PerformanceA natural scoring standard is to select a languageand evaluate against headwords from existingdictionaries in that language.
Others have usedsimilar standards (Daille, 1996), but to ourknowledge, none to the extent described here.
Weevaluate thousands of hypothesized units from anunconstrained corpus.
Furthermore, we use twoseparate evaluation gold standards: (1) WordNet(Miller, et al 1990) and (2) a collection of InternetMRDs.
Using  two gold standards helps validMWUs.
It also provides evaluation using both staticand dynamic resources.
We choose to evaluate inEnglish  due  to the wealth of  linguistic resources.Rank ZScore $2 SCP Dice MutualInfo.SelectAssoc.LogLike.
TScore Freq1 IwoJimaBuenosAiresBuenosAiresBuenosAiresIwoJimaUnitedStatesUnitedStatesUnitedStatesUnitedStates2bonafideIwoJimaIwoJimaIwoJimabonafideHouseofRepre-sentativesLosAngelesLosAngelesLosAngeles4 BurkinaFaso Suu Kyi Suu Kyi Suu KyiWoundedKneeLosAngelesNewYorkNewYorkNewYork8 SatanicVerses Sault Ste Sault Ste Sault SteHubbleSpaceTelescopemycolleaguesSovietUnionmycolleaguesmycolleagues16 KuKluxKuKluxKuKluxKuKluxalmamaterH .
R SocialSecurityHighSchoolHighSchool32Pledge ofAllegiancePledge ofAllegiancePledge ofAllegiancePledge ofAllegianceCoca -Cola War IIHouse ofRepre-sentativesWednesday* * * *64 Telephone& amp ;TelegraphTelephone& amp ;TelegraphTelephone& amp ;TelegraphInternalRevenuePlannedParent-hoodPrimeMinister * * *realestateNewJersey128 PrimeMinisterPrimeMinisterPrimeMinisterSalmanRushdieSault Ste.
MariebothsidesAt thesame timeWallStreettermcare256 LehmanHuttonLehmanHuttonLehmanHuttontongue -in -cheeko ?
clock At thesamedel Mar allovergrandjury512La Habra La Habra La Habracompens-atory andpunitive20th -CenturyMondaynightdayslater80percentGreatNorthern1024 telephoneinterviewtelephoneinterviewtelephoneinterviewFood andAgricultureSheriff ?sdeputiesSouthDakotaCountyJailwhereyou300millionTable 2: Outputs from each algorithm at different sorted ranksThe ?
* *?
and ?
* * *?
are actual units.In particular, we use a randomly-selected corpus the first five columns as ?information-like.
?consisting of a 6.7 million word subset of  the TREC Similarly, since the last four columns sharedatabases (DARPA, 1993-1997).
properties of the frequency approach, we will referTable 2 illustrates a sample of rank-ordered output to them as ?frequency-like.
?from each of the different algorithms (following the One?s application may dictate which set ofcross-source, filtered paradigm described in section algorithms to use.
Our gold standard selection3).
Note that algorithms in the first four columns reflects our interest in general word dictionaries, soproduce results that are similar to each other as do results we obtain may differ from results we mightthose in the last four columns.
Although the mutual have obtained using terminology lexicons.information results seem to be almost in a class of If our gold standard contains K MWUs withtheir own, they actually are similar overall to the corpus frequencies satisfying threshold (T=10), ourfirst four sets of results; therefore, we will refer to figure of merit (FOM) is given by1K MKi1 Pi ,little or even negative impact.
On the other hand,where P  (precision at i) equals i/H , and H  is thei i inumber of hypothesized MWUs required to find thei  correct MWU.
This FOM corresponds to areathunder a precision-recall curve.4.1 WordNet-based EvaluationWordNet has definite advantages as an evaluationresource.
It has in excess of 50,000 MWUs, is freelyaccessible, widely used, and is in electronic form.Yet, it obviously cannot contain every MWU.
Forinstance, our corpus contains 177,331 n-grams (for2n10) satisfying T10, but WordNet containsonly 2610 of these.
It is unclear, therefore, ifalgorithms are wrong when they propose  MWUsthat are not in WordNet.
We will assume they arewrong but with a special caveat for proper nouns.WordNet includes few proper noun MWUs.
Yetseveral algorithms produce large numbers of propernouns.
This biases against them.
One could contendthat all proper nouns MWUs are valid,  but wedisagree.
Although such may be MWUs, they arenot necessarily MRD headwords; one would notinclude every proper noun  in a dictionary, butrather, those needing definitions.
To overcome this,we will have two scoring modes.
The first, ?S?mode (standing for some) discards any proposedcapitalized n-gram whose uncapitalized version isnot in WordNet.
The second mode ?N?
(for none)disregards all capitalized n-grams.Table 3 illustrates algorithmic performance ascompared to the 2610 MWUs from WordNet.
Thefirst double column illustrates ?out-of-the-box?performance on all 177,331 possible n-grams.
Thesecond double column shows cross-sourcing: onlyhypothesizing MWUs that appear in at least twoseparate datasets  (124,952 in all), but beingevaluated against all of the 2610 valid units.
Doublecolumns 3 and 4 show effects from high-frequencyfiltering the n-grams of the first and second columns(reporting only 29,716 and 17,720 n-grams)respectively.As Table 3 suggests, for every condition, theinformation-like algorithms seem to perform best atidentifying valid, general MWU headwords.Moreover, they are enhanced when cross-sourcingis considered; but since much of their strengthcomes from identifying proper nouns, filtering hasthe frequency-like approaches are independent ofdata source.
They also improve significantly withfiltering.
Overall, though, after the algorithms arejudged, even the best score of 0.265 is far short ofthe maximum possible, namely 1.0.Table 3: WordNet-based scoresProb (1) (2) (3) (4)algo- WordNet WordNet WordNet WordNetrithm cross- +Filter cross-source source+FilterS N S N S N S NZscore .222 .146 .263 .193 .220 .129 .265 .173SCP .221 .145 .262 .192 .220 .129 .265 .173Chi-sqr .222 .146 .263 .193 .220 .129 .265 .173Dice .242 .167 .265 .199 .230 .142 .256 .172MI .191 .122 .245 .169 .185 .111 .233 .151SA .057 .051 .058 .053 .182 .125 .202 .143Loglike .049 .050 .068 .064 .118 .095 .177 .129T-score .050 .051 .050 .052 .150 .109 .160 .118Freq .035 .037 .034 .037 .144 .105 .152 .1124.2   Web-based EvaluationSince WordNet is static and cannot report on all ofa corpus?
n-grams, one may expect differentperformance by using a more all-encompassing,dynamic resource.
The Internet houses dynamicresources which can judge practically every inducedn-gram.
With permission and sufficient time, onecan repeatedly query websites that host largecollections of MRDs and evaluate each n-gram.Having approval, we queried: (1) onelook.com,(2) acronymfinder.com, and (3) infoplease.com.
Thefirst website interfaces with over 600 electronicdictionaries.
The second is devoted to identifyingproper acronyms.
The third focuses on world factssuch as historical figures and organization names.To minimize disruption to websites by reducingthe total number of queries needed for evaluation,we use an evaluation approach from the informationretrieval community (Sparck-Jones and vanRijsbergen, 1975).
Each algorithm reports its top5000 MWU choices and  the union of these choices(45192 possible n-grams) is looked up on theInternet.
Valid MWUs identified at any website areassumed to be the only valid units in the data.
{Xi}ni1 {Xi}ni1cos(X,Y)X #Y||X|| ||Y|| .Algorithms are then evaluated based on this showed how one could compute latent semanticcollection.
Although this strategy for evaluation is vectors for any word in a corpus (Schone andnot flawless, it is reasonable and makes dynamic Jurafsky, 2000).
Using the same approach, weevaluation tractable.
Table 4 shows the algorithms?
compute semantic vectors for every proposed wordperformance (including proper nouns).
n-gram C=X X ...X   Since LSA involves wordThough Internet dictionaries and WordNet are counts, we can also compute semantic vectorscompletely separate ?gold standards,?
results aresurprisingly consistent.
One can conclude thatWordNet may safely be used as a gold standard infuture MWU headword evaluations.
Also,Table 4 Performance on Internet dataProb (1) (2) (3) (4)algorithm Internet Internet Internet Internetcross- +Filter cross-source source+FilterZ-Score .165 .260 .169 .269SCP .166 .259 .170 .270Chi-sqr .166 .260 .170 .270Dice .183 .258 .187 .267MI .139 .234 .140 .234SA .027 .033 .107 .194Log Like .023 .043 .087 .162T-score .025 .027 .110 .142Freq .016 .017 .104 .134one can see that Z-scores, $ , and2SCP have virtually identical results and seem to bestidentify MWU headwords (particularly if propernouns are desired).
Yet there is still significantroom for improvement.5 Improvement strategiesCan performance be improved?
Numerousstrategies could be explored.
An idea we discusshere tries using induced semantics to rescore theoutput of the best algorithm (filtered, cross-sourcedZscore) and eliminate semantically compositional ormodifiable MWU hypotheses.Deerwester, et al(1990) introduced LatentSemantic Analysis (LSA) as a computationaltechnique for inducing semantic relationshipsbetween words and documents.
It forms high-dimensional vectors using word counts and usessingular value decomposition to project thosevectors into an optimal k-dimensional, ?semantic?subspace (see Landauer, et al 1998).Following an approach from Sch?tze (1993), we1 2 n.(denoted by) for C?s subcomponents.
These caneither  include  (           ) or exclude (             )  C?scounts.
We seek to see if induced semantics canhelp eliminate incorrectly-chosen MWUs.
As willbe shown, the effort using semantics in this naturehas a very small payoff for the expended cost.5.1    Non-compositionalityNon-compositionality is a key component of validMWUs, so we may desire to emphasize n-grams thatare semantically non-compositional.
Suppose wewanted to determine if C (defined above) were non-compositional.
Then given some meaning function,, C should satisfy an equation like:g(  (C) , h( (X ),...,(X ) )  )0,           (1)1 nwhere h combines the semantics of C?ssubcomponents and g measures semanticdifferences.
If C were a bigram, then if g(a,b) isdefined to be |a-b|, if h(c,d) is the sum of c and d,and if (e) is set to -log P , then equation (1) wouldebecome the pointwise mutual information of thebigram.
If g(a,b) were defined to be (a-b)/b , and if?h(a,b)=ab/N and  (X)=f  , we essentially get Z-Xscores.
These formulations suggest that several ofthe probabilistic algorithms we have seen includenon-compositionality measures already.
However,since the probabilistic algorithms rely only ondistributional information obtained by consideringjuxtaposed words,  they tend to incorporate asignificant amount of non-semantic informationsuch as syntax.
Can semantic-only rescoring help?To find out, we must select g, h, and .
Since wewant to eliminate MWUs that are compositional, wewant h?s output to correlate well with C when thereis compositionality and correlate poorly otherwise.Frequently, LSA vectors are correlated using thecosine between them:A large cosine indicates strong correlation, so largevalues for g(a,b)=1-|cos(a,b)| should signal weakcorrelation or non-compositionality.
h couldMni1 wi aicoscos(Xi,Y)mink{Xi,Y}cos(Xi ,Y)	?k1k.represent a weighted vector sum of the components?
required for this task.
This seems to be a significantsemantic vectors with weights (w ) set to either 1.0 component.
Yet there is still another: maybeior the reciprocal of the words?
frequencies.
semantic compositionality is not always bad.Table 5 indicates several results using these Interestingly, this is often the case.
Considersettings.
As the first four rows indicate and as vice_president, organized crime, anddesired, non-compositionality is more apparent for Marine_Corps.
Although these are MWUs, one* (i.e., the vectors derived from excluding C?sXcounts) than for.
Yet, performance overall isXhorrible, particularly considering we are rescoringZ-score output whose score was 0.269.
Rescoringcaused five-fold degradation!Table 5: Equation 1 settingsg(a,b) h(a) (X) w Score oniInternet1-|cos(a,b)|X 1 0.05171/fi 0.0473*X 1 0.05981/fi* 0.0523|cos(a,b)|X 1 0.1741/fi 0.169*X 1 0.1311/fi* 0.128What happens if we instead emphasizecompositionality?
Rows 5-8 illustrate the effect:there is a significant recovery in performance.
Themost reasonable explanation for this is that ifMWUs and their components are stronglycorrelated, the components may rarely occur exceptin context with the MWU.
It takes about 20 hoursto compute the* for each possible n-gramXcombination.
Since the probabilistic algorithmsalready identify n-grams that share strongdistributional properties with their components, itseems imprudent to exhaust resources on this LSA-based strategy for non-compositionality.These findings warrant some discussion.
Why didnon-compositionality fail?
Certainly there is thepossibility that better choices for g, h, and  couldyield improvements.
We actually spent monthstrying to find an optimal combination as well as astrategy for coupling LSA-based scores with the Z-scores, but without avail.
Another possibility:although LSA can find semantic relationships, itmay not make semantic decisions at the levelwould still expect that the first is related topresident, the second relates to crime, and the lastrelates to Marine.
Similarly, tokens such asJohns_Hopkins and Elvis are anaphors forJohns_Hopkins_University and Elvis_Presley, sothey should have similar meanings.This begs the question: can induced semanticshelp at all?
The answer is ?yes.?
The key is usingLSA where it does best: finding things that aresimilar ?
or substitutable.5.2 Non-substitutivityFor every collocation C=X X ..X X X ..X , we1 2 i-1 i+1 niattempt to find other similar patterns in the data,X X ..X YX ..X .
If X  and Y are semantically1 2 i-1 i+1 n irelated, chances are that C is substitutable.Since LSA excels at finding semantic correlations,we can compareandto see if C isXi Ysubstitutable.
We use our earlier approach (Schoneand Jurafsky, 2000) for performing the comparison;namely, for every word W, we compute cos()w, Rfor 200 randomly chosen words, R. This allows forcomputation of a correlaton mean (? )
and standardWdeviation (1 ) between W  and other words.
AsWbefore, we then compute a normalized cosine score(      ) between words of interest, defined byWith this set-up, we now look for substitutivity.Note that phrases may be substitutable and still beheadword if their substitute phrases are themselvesMWUs.
For example, dioxide in carbon_dioxide issemantically similar to monoxide incarbon_monoxide.
Moreover, there are otherimportant instances of valid substitutivity:& AbbreviationsAlAlbert   <   Al_GoreAlbert_Gore& Morphological similaritiesRicoRican <  Puerto_RicoPuerto_Rican& Taxonomic relationshipsbachelormaster<bachelor_?_s_degreemaster_?_s_degree.Figure 1: Precision-recall curve for rescoringHowever, guilty and innocent are semanticallyrelated, but pleaded_guilty and pleaded_innocentare not MWUs.
We would like to emphasize only n-grams whose substitutes are valid MWUs.To show how we do this using LSA, suppose wewant to rescore a list L whose entries are potentialMWUs.
For every entry X in L, we seek out allother entries whose sorted order is less than somemaximum value (such as 5000) that have all but oneword in common.
For example, suppose X is?bachelor_?_s_degree.?
The only other entry thatmatches in all but one word is ?master_?_s_degree.
?If the semantic vectors for ?bachelor?
and ?master?have a normalized cosine score greater than athreshold of 2.0, we then say that the two MWUsare in each others substitution set.
To rescore, weassign a new score to each entry in substitution set.Each element in the substitution set gets the samescore.
The score is derived using a combination ofthe previous Z-scores for each element in thesubstitution set.
The combining function may be anaveraging, or a computation of the median, themaximum, or something else.
The maximumoutperforms the average and the median on our data.By applying in to our data, we observe a small butvisible improvement of 1.3% absolute to .282 (seeFig.
1).
It is also possible that other improvementscould be gained using other combining strategies.6 ConclusionsThis paper identifies several new results in the areaof MWU-finding.
We saw that MWU headwordevaluations using WordNet provide similar resultsto those obtained from far more extensive web-based resources.
Thus, one could safely useWordNet as a gold standard for future evaluations.We also noted that information-like algorithms,particularly Z-scores, SCP, and $2, seem to performbest at finding MRD headwords regardless offiltering mechanism, but that improvements are stillneeded.
We proposed two new LSA-basedapproaches which attempted to address issues ofnon-compositionality and non-substitutivity.Apparently,  either current algorithms alreadycapture much non-compositionality or LSA-basedmodels of non-compositionality are of little help.LSA does help somewhat as a model ofsubstitutivity.
However, LSA-based gains are smallcompared to the effort required to obtain them.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their comments and insights.ReferencesAcronymFinder.com(2000-1).
http://www.acronymfinder.com.
Searches between March 2000 and April 2001.Brent, M.R.
and Cartwright, T.A.
(1996).
Distributionalregularity and phonotactic constraints are useful forsegmentation.
Cognition, 61, 93-125.Choueka, Y.
(1988).
Looking for needles in a haystackor locating interesting collocation expressions in largetextual databases.
Proceedings of the RIAO, pp.
38-43.Church, K.W.
(1995).
N-grams.
Tutorial at ACL, ?95.MIT, Cambridge, MA.Church, K.W., & Gale, W.A.
(1991).
Concordances forparallel text.
Proc.
of the 7  Annual Conference of thethUW Center for ITE New OED & Text Research, pp.40-62, Oxford.Church, K.W., & Hanks, P. (1990).
Word associationnorms, mutual information and lexicography.Computational Linguistics, Vol.
16, No.
1, pp.
22-29.Daille, B.
(1996).
?Study and Implementation ofCombined Techniques from Automatic Extraction ofTerminology?
Chap.
3 of "The Balancing Act":Combining Symbolic and Statistical Approaches toLanguage (Klavans, J., Resnik, P.
(eds.
)), pp.
49-66DARPA (1993-1997).
DARPA text collections: A.P.Material, 1988-1990, Ziff Communications Corpus,1989, Congressional Record of the 103  Congress,rdand Los Angeles Times.Deerwester, S., S.T.
Dumais, G.W.
Furnas, T.K.Landauer, and R. Harshman.
(1990) Indexing byLatent Semantic Analysis.
Journal of the AmericanSociety of Information Science, Vol.
41de Marcken, C. (1996) Unsupervised LanguageAcquisition, Ph.D., MIT Manning, C.D., Sch?tze, H. (1999) Foundations ofDias, G., S.
Guillor?, J.G.
Pereira Lopes (1999).
Statistical Natural Language Processing, MIT Press,Language independent automatic acquisition of rigid Cambridge, MA, 1999.multiword units from unrestricted text corpora.
TALN, Mikheev, A., Finch, S. (1997).
Collocation lattices andCarg?se.
maximum entropy models.
WVLC, Hong Kong.Dice, L.R.
(1945).
Measures of the amount of ecologicassociations between species.
Journal of Ecology, 26,1945.Dunning, T (1993).
Accurate methods for the statistics ofsurprise and coincidence.
Computational Linguistics.Vol.
19, No.
1.Fano, R. (1961).
Transmission of Information.
MITPress,   Cambridge, MA.Finke, M. and Weibel, A.
(1997) Speaking modedependent pronunciation modeling in large vocabularyconversational speech recognition.
Eurospeech-97.Ferreira da Silva, J., Pereira Lopes, G. (1999).
A localmaxima method and a fair dispersion normalization forextracting multi-word units from corpora.
SixthMeeting on Mathematics of Language, pp.
369-381.Fontenelle, T., Br?ls, W., Thomas, L., Vanallemeersch,T., Jansen, J.
(1994).
DECIDE, MLAP-Project 93-19,deliverable D-1a: Survey of collocation extractiontools.
Tech.
Report, Univ.
of Liege, Liege, Belgium.Giuliano, V. E. (1964) "The interpretation of wordassociations."
In M.E.
Stevens et al (Eds.)
Statisticalassociation methods for mechanized documentation,pp.
25-32.
National Bureau of StandardsMiscellaneous Publication 269, Dec. 15, 1965.Gregory, M. L., Raymond, W.D., Bell, A., Fosler-Lussier, E., Jurafsky, D. (1999).
The effects ofcollocational strength and contextual predictability inlexical production.
CLS99, University of Chicago.Heid, U.
(1994).
On ways words work together.
Euralex-99.Hindle, D. (1990).
Noun classification from predicate-argument structures.
Proceedings of the AnnualMeeting of the ACL, pp.
268-275.InfoPlease.com (2000-1).
http://www.infoplease.com.Searches between March 2000 and April 2001.Jacquemin, C., Klavans, J.L., & Tzoukermann, E. (1997).Expansion of multi-word terms for indexing andretrieval using morphology and syntax.
Proc.
of ACL1997, Madrid, pp.
24-31.Justeson, J.S.
and S.M.Katz (1995).
Technicalterminology: some linguistic properties and analgorithm for identification in text.
Natural LanguageEngineering 1:9-27.Kilgariff, A., & Rose, T. (1998).
Metrics for corpussimilarity & homogeneity.
Manuscript, ITRI,University of Brighton.Landauer, T.K., P.W.
Foltz, and D. Laham.
(1998)Introduction to Latent Semantic Analysis.
DiscourseProcesses.
Vol.
25, pp.
259-284.Miller, G.
(1990).
?WordNet: An on-line lexicaldatabase,?
International Journal of Lexicography, 3(4).OneLook.com (2000-1).
http://www.onelook.com.Searches between March 2000 and April 2001.Ponte, J.M., Croft, B.W.
(1996).
Useg: A Retargetableword segmentation procedure for information retrieval.Symposium on Document Analysis and InformationRetrieval ?96.
Technical Report TR96-2, University ofMassachusetts.Resnik, P. (1996).
Selectional constraints: aninformation-theoretic model and its computationalrealization.
Cognition.
Vol.
61, pp.
127-159.Saffran, J.R., Newport, E.L., and Aslin, R.N.
(1996).Word segmentation: the role of distributional cues.Journal of Memory and Language, Vol.
25, pp.
606-621.Schone, P. and D. Jurafsky.
(2000) Knowledge-freeinduction of morphology using latent semanticanalysis.
Proc.
of the Computational NaturalLanguage Learning Conference, Lisbon, pp.
67-72.Sch?tze, H. (1993) Distributed syntactic representationswith an application to part-of-speech tagging.Proceedings of the IEEE International Conference onNeural Networks, pp.
1504-1509.Shimohata, S., Sugio, T., Nagata, J.
(1997).
Retrievingcollocations by co-occurrences and word orderconstraints.
Proceedings of the 35  Annual Mtg.
of thethAssoc.
for Computational Linguistics.
Madrid.Morgan-Kauffman Publishers, San Francisco.
Pp.476-481.Smadja, F. (1993).
Retrieving collocations from text:Xtract.
Computational Linguistics, 19:143-177.Sparck-Jones, K., C. van Rijsbergen (1975) Report on theneed for and provision of an ?ideal?
informationretrieval text collection, British Library Research andDevelopment Report, 5266, Computer Laboratory,University of Cambridge.Sproat R, Shih, C. (1990) A statistical method for findingword boundaries in Chinese text.
ComputerProcessing of Chinese & Oriental Languages, Vol.
4,No.
4.Sproat, R. (1992) Morphology and Computation.
MITPress, Cambridge, MA.Sproat, R.W., Shih, C., Gale, W., Chang, N. (1996) Astochastic finite-state word segmentation algorithm forChinese.
Computational Linguistics, Vol.
22, #3.Teahan, W.J., Yingyin, W. McNab, R, Witten, I.H.(2000).
A Compression-based algorithm for Chineseword segmentation.
ACL Vol.
26, No.
3, pp.
375-394.
