Proceedings of the 7th Workshop on Statistical Machine Translation, pages 292?297,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSyntax-aware Phrase-based Statistical Machine Translation:System DescriptionUlrich GermannUniversity of TorontoToronto, Ontario, Canadagermann@cs.toronto.eduAbstractWe present a variant of phrase-based SMT thatuses source-side parsing and a constituent re-ordering model based on word alignments inthe word-aligned training corpus to predict hi-erarchical block-wise reordering of the input.Multiple possible translation orders are rep-resented compactly in a source order lattice.This source order lattice is then annotated withphrase-level translations to form a lattice of to-kens in the target language.
Various featurefunctions are combined in a log-linear fashionto evaluate paths through that lattice.1 IntroductionDealing with word order differences is one of themajor challenges in automatic translation betweenhuman languages.
With its moderate context sen-sitivity and reliance on n-gram language models,phrase-based statistical machine translation (PB-SMT) (Koehn et al, 2003) is usually quite goodat performing small word order changes ?
forinstance, the inversion of adjective and noun inEnglish-to-French translation and vice versa.
How-ever, it regularly fails to execute word order changesover long distances, as they are required, for exam-ple, to accommodate the substantial differences inthe word order in subordinate clauses between Ger-man and English, or to cope with the phenomenonof the ?sentence bracket?
(Satzklammer) in Germanmain clauses, in which the finite part of the verbcomplex and additional elements (separable pre-fixes, participles, infinitives, etc.)
form a bracket thatencloses most of the arguments and other adverbialconstituents, as shown in Fig.
1.
In order to keep de-coding complexity in check, phrase-based decoderssuch as the Moses system (Koehn et al, 2007) rou-tinely limit the maximum distance for word orderchanges to six or seven word positions, thus rul-ing out, a priori, word order changes necessary toachieve good and fluent translations.As is generally acknowledged, word order dif-ferences are not entirely arbitrary.
By and largethey follow syntactic structure.
An analysis ofword-aligned French-English data by Fox (2002)showed that word alignment links rarely cross syn-tactic boundaries.
Wu?s (1997) Inversion Transac-tion Grammar (ITG), assumes that word order dif-ferences can be accounted for by hierarchical inver-sion of adjacent blocks of text.
Yamada and Knight(2001) present a stochastic model for transformingEnglish parse trees into Japanese word sequenceswithin a source-channel framework for Japanese-to-English translation.
Collins et al (2005) per-form heuristic word re-ordering from German intoEnglish word order based on German parse treeswith a particular focus on the aforementioned drasticword order differences between German and Englishclause structure.Building on Chiang (2007), several systems underactive development (e.g., Weese et al, 2011; Dyeret al, 2010) rely on synchronous context-free gram-mars to deal with word order differences.
In essence,these systems parse the input while synchronouslybuilding a parse tree in the translation target lan-guage, using probabilities of the source and targettrees as well as correspondence probabilities to eval-uate translation hypotheses.292?Dieser1 Vorschlag2 wird3 sicherlich4 im5 Ausschu?6 gr?ndlich7 diskutiert8 werden9 m?ssen10 .?
?This1 proposal2 will3 certainly4 have10 to10 be9 discussed8 toroughly7 in5 the5 commission6.
?Figure 1: The sentence bracket (Satzklammer) in German.The system presented in this paper takes a slightlydifferent route and is closer to the approach takenby Collins et al (2005): we parse only monolin-gually on the source side, re-order, and then trans-late.
However, unlike Collins et al we do not use aseries of rules to perform the transformations (nor dowe re-order the training data on the source side), buttry to learn reordering rules from the word-alignedcorpus with the original word order on both sides.Moreover, we do not commit to a single parse anda single re-ordering of the source at translation timebut consider multiple parse alternatives to create alattice of possible translation orders.
Each vertex inthe lattice corresponds to a specific subset of sourcewords translated up to that point.Individual edges and sequences of edges in thislattice are annotated with word- and phrase1-leveltranslations extracted from the word-aligned train-ing corpus, in the same way as phrase tables for PB-SMT are constructed2 .
An optimal path through thelattice is determined by dynamic programming, con-sidering a variety of feature functions combined in alog-linear fashion.In the following, we first describe the individ-ual processing steps in more detail and then try toshed some light on the system?s performance in thisyear?s shared task.
Due to space limitations, manydetails will have to be skipped.2 System Description2.1 Grammatical frameworkThe central idea underlying this work is that gram-mar constrains word reordering: we are allowed topermute siblings in a CFG tree, or the governor andits dependents in a dependency structure, but we arenot allowed to break phrase coherence by moving1?Phrase?
being any contiguous sequence of words in this con-text, as in PBSMT.2Except that we do not pre-compute phrase tables but constructthem dynamically on the fly using suffix arrays, as suggestedby Callison-Burch et al (2005).words out of their respective sub-tree.
Obviously,we need to be careful in the precise formulation ofour grammar, so as not to over-constrain word orderoptions.
For example, the German parse tree for thephrase ein1 [zu hoher]2 Preis3 in Fig.
2 below rulesout the proper word order of its English translation[too high]2 a1 price3.NPDetein1N?AP[zu hoher]2NPreis3Figure 2: X-bar syntax can be too restrictive.
This treedoes not allow the word order of the English translation[too high]2 a1 price3.In her analysis of phrasal cohesion in transla-tion, Fox (2002) pointed out that phrasal cohesionis greater with respect to dependency structure thanwith respect to constituent structure.
We thereforedecided to rely on the segmentation granularity in-herent in dependency parses.2.2 ParsingFor parsing, we developed our own hybrid left-corner dependency parser for German.
In many re-spects, it is inspired by the work on dependencyparsing by Eisner (1996) (edge factorization) andMcDonald et al (2005) (choice of features for edgescores).
From the generative point of view, we canimagine the following generative process: We startwith the root word of the sentence.
A Markov pro-cess then generates this word?s immediate depen-dents from left to right, at some point placing thehead word itself.
The dependents (but not the headword) are then expanded recursively in the samefashion.
At parse time we process the input left toright, deciding for each word what its governors are,or whether it governs some items to its left or right.293Since each word has exactly one governor (bar theroot word), we renormalize edge scores by marginal-izing over the potential governors.
If the word is po-tentially the left corner of a sub-tree, we establish anew rule (akin to a dotted rule in an Earley parser)and add it to the parse chart.
For potential gover-nors to the left, we scan the parse chart for partialproductions that end immediately before the wordin question and extend them by the word in ques-tion.
Whenever we add an item to a partial produc-tion that is ?past or reaching its head?
(i.e., the spancovered by the rule includes the sub-tree?s root or thenewly added item is the root), we treat the sub-treeas a new item in a bottom-up fashion, i.e., determinepotential governors outside of the span covered, adda new rule if the sub-tree could be the left corner of alarger sub-tree, etc.
In addition to the joint probabil-ity of all individual edges, we also consider the costof adding an item to a partial production.
To reduceparse complexity, we use a beam to limit the numberof potential governors that are considered for eachitem.
Unlike conventional CFGs, the set of ?rules?in this grammar is not finite; rules are generated onthe fly by a Markov process.
This adds robustness;we can always attach an item (token or sub-tree) toone of its immediate neighbors.2.3 Construction of a source order lattice (SOL)Rows and columns in the parse chart correspond tothe start and end positions of parse spans in the sen-tence.
Each cell contains zero or more productionrules that correspond to different segmentations ofthe respective span into sub-spans that may be re-ordered during translation.
Based on the underly-ing part-of-speech tags, we retrieve similar syntacticconfigurations from the word-aligned, source-side-parsed training corpus.For each example retrieved from the training cor-pus, we determine, from the word alignment infor-mation in the training corpus, the order in which thedependents and the head word are translated.
To re-duce noise from alignment errors, each example isweighted by the joint lexical translation probabilityof the words immediately involved in the produc-tion (i.e., the head and its dependents, but not grand-children).
Thus, examples with unlikely word align-ments count less than examples that have highlyprobable word alignments.
If exact matches for theproduction rule in question cannot be found in thecorpus (which happens frequently), we fall back ona factorized model that maps from source to targetpositions based on the part-of-speech of the depen-dent in question and its governor.
Words that are partof the verb complex (auxiliaries, separable prefixes,the ?lexical head?, etc.)
are grouped together and re-ceive special treatment.
(This is currently work inprogress; at this point, we translate only the lexicalhead, but ignore negation and auxiliaries.
)For each of the top N segmentations suggestedby the parser, translation order probabilities arecomputed on the basis of the weighted occurrencecounts, and used to set the edge weights in a lat-tice of possible translation orders, which we call theSource Order Lattice (SOL).
Each vertex in this lat-tice corresponds to a specific set of source wordstranslated so far.
(In principle, the number of ver-tices in this lattice is exponential in the length of theinput sentence; in practice, since we consider only asmall number of possibilities, their number is quitemanageable.)
For each chunk of text in the sug-gested order of translation, we increase the weightof the edge between the vertex representing the setof words translated so far and the vertex represent-ing the set of words translated after this chunk hasbeen translated by the probability of translating thechunk in question at this particular point in the trans-lation process.
Edges representing two or more con-secutive words (with the exception of those repre-senting a verb complex) are recursively replaced bylocal SOLs, until each edge corresponds to a singleword in the source sentence.2.4 Constructing a target word latticeThe global SOL thus constructed is then transformedinto a Target Word Lattice (TWL), while maintain-ing underlying alignment information.
Each individ-ual edge or sequence of adjacent edges correspond-ing to a contiguous sequence of words in the sourcesentence is replaced by a lattice that encodes therange of possible translations for the respective wordor phrase.
Translations are extracted from the word-aligned bilingual training corpus with the phrase-extraction method that is commonly used in phrase-based SMT.
As it is done in the Joshua system(Weese et al, 2011), we extract phrase translationson the fly from the word-aligned bilingual corpus294using suffix arrays instead of using pre-computedphrase tables.2.5 SearchOnce constructed, the TWL is searched with dy-namic programming with a beam search.
Hypothe-ses are scored by a log-linear combination of the fol-lowing feature functions.
Feature values are normal-ized by hypothesis length unless noted otherwise, tosafeguard against growth of cumulative feature val-ues at different rates as the length of a hypothesis in-creases, and to keep hypotheses of different lengthsmutually comparable.?
Distortion probabilities from the SOL as de-scribed above.?
Relative phrase translation frequenciesbased on counts in the training corpus.?
Lexical translation probabilities: forward(p (target | source); normalized by targetlength) and backward (p (source | target);normalized by source length).
Lexical transla-tion probabilities are based on alignment linkcounts in the word-aligned corpus.?
N -gram language model probability as esti-mated with the SRILM toolkit.?
Fluency.
Simple length-based normalizationof joint n-gram probabilities is problematic.
Itentices the decoder to ?throw in?
additional,highly frequent words to increase the languagemodel score.
Inversely, lack of normalizationprovides an incentive to keep translation hy-potheses as short as possible, even at the ex-pense of fluency.
This fluency feature func-tion computes the ratio of the language modelprobability of each proposed target word incontext and its unigram probability.
Rewards(p (wi |wi?k+1 .
.
.
wi?1) > p (wi)) and penal-ties (p (wi |wi?k+1 .
.
.
wi?1) < p (wi)) re-ceive different weights in the log-linear com-bination.
Rewards are normalized by tar-get length; penalties by the number of sourcewords translated.
The rationale between thedifferent forms of normalization is this: if wedon?t normalize rewards by hypothesis length,we have an incentive to pad the translation withhighly frequent tokens (commas, ?the?)
wher-ever their probability in context is higher thantheir simple unigram probability.
Awkwardlyplaced tokens, on the other hand, should alwaystrigger a penalty, and the system should notbe allowed to soften the blow by adding morepoorly, but not quite as poorly placed tokens.Normalization of penalties by covered sourcelength is an acknowledgement of the fact thatin longer sentences, the probability of havingpoints of disfluency increases.
We use two re-ward/penalty pairs sets of fluency feature func-tions.
One operates on surface forms, the otherone on part-of-speech tag sequences.?
Cumulative probability density of observedn-gram counts.
This feature function penal-izes n-grams that do not occur as often as theyshould (even if observed), based on prior obser-vation, and rewards those that do.
Consider thefollowing sequence of words in English:can you areThe sequence can you is fairly frequent, and sois you are.
However, can you are is not.
Withstandard n-gram back-off models, the model,upon not finding the full context can you forare, will back off to the context you and thusassign an inappropriately high probability top (are | can you).The n-gram cdf feature models the event as aBernoulli experiment.
Suppose, for example,that p (are | you) = .01, and we have observedcan you 1000 times, but have never seen canyou are.
Then the expected count of observa-tions is 10 andcdf (0 | 1000; .01) = (1?
.01)1000 ?
.0000433 Training and tuningThe system was trained on the German-English partof Europarl corpus (v.5).
The language model forEnglish was trained on all monolingual data avail-able for WMT-2010.
We true-cased, but did notlower-case the data.
Word alignment was performedwith multi-threaded Giza++ (Gao and Vogel, 2008).In order to bootstrap training data for our parser,we parsed the German side of the Europarl corpus295with the Berkeley Parser (Petrov et al, 2006; Petrovand Klein, 2007) and converted the CFG structuresto dependency structures using simple hand-writtenheuristics to identify the head in each phrase, simi-lar to those used by Magerman (1995) and Collins(1996).
This head was then selected as the gover-nor of the respective phrase.
Part-of-speech taggingand lemmatization on the English side as well as theGerman development and test data was performedwith the tool TreeTagger (Schmid, 1995).For tuning the model parameters, we tried to ap-ply pairwise rank optimization (PRO) (Hopkins andMay, 2011), but we were not able to achieve resultsthat beat our hand-tuned parameter settings.4 EvaluationUnfortunately, with a BLEU score of .121, (.150 af-ter several bug fixes in the program code), our sys-tem performed extremely poorly in the shared task.We have since tried to track down the reasons for thepoor performance, but have not been able to find acompelling explanation for it.A partial explanation may lie in the fact that weused only the Europarl data for training.3 However,our system also lags far behind a baseline Mosessystem trained on the same subset of data used forour system, which achieves a BLEU score of .184.Since our feature functions are very similar tothose used in MOSES, we suspect that better tuningof the feature weights might close the gap.
We arecurrently in the process of implementing and test-ing other parameter tuning methods (in addition tomanual tuning and PRO), specifically lattice-basedminimum error rate training (Macherey et al, 2008)and batch MIRA (Cherry and Foster, 2012).5 ConclusionWe have presented a variant of PBSMT that usessyntactic information from source-side parses in or-der to account better for word-order differences inGerman-to-English machine translation, while pre-serving the advantages of PBSMT.
Several compo-nents were developed from scratch, such as a depen-dency parser for German and a reordering model forparse constituents, as well as several novel variants3Participation in the shared task was a short term decision, andwe did not have the time to re-train our system.of n-gram based fluency measures.
While our re-sults for this year?s shared task are certainly disap-pointing, we nevertheless believe that we are on theright track.
We are not ready to give up quite yet.ReferencesCallison-Burch, Chris, Colin Bannard, and JoshSchroeder.
2005.
?Scaling phrase-based statisticalmachine translation to larger corpora and longerphrases.?
43rd Annual Meeting of the Associationfor Computational Linguistics (ACL ?05), 255?262.
Ann Arbor, Michigan.Cherry, Colin and George Foster.
2012.
?Batchtuning strategies for statistical machine transla-tion.?
2012 Meeting of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies.
Montr?al,Queb?c, Canada.Chiang, David.
2007.
?Hierarchical phrase-basedtranslation.?
Computational Linguistics, 33(2):1?28.Collins, Michael, Philipp Koehn, and Kuc?erov?Ivona.
2005.
?Clause restructuring for statisti-cal machine translation.?
43rd Annual Meetingof the Association for Computational Linguistics(ACL ?05).
Ann Arbor, MI, USA.Collins, Michael John.
1996.
?A new statisticalparser based on bigram lexical dependencies.
?Proceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics, 184?191.
Santa Cruz, California, USA.Dyer, Chris, Adam Lopez, Juri Ganitkevitch,Jonathan Weese, Ferhan Ture, Phil Blunsom,Hendra Setiawan, Vladimir Eidelman, and PhilipResnik.
2010.
?cdec: A decoder, alignment, andlearning framework for finite-state and context-free translation models.?
Proceedings of theACL 2010 System Demonstrations, 7?12.
Upp-sala, Sweden.Eisner, Jason M. 1996.
?Three new probabilis-tic models for dependency parsing: An explo-ration.?
The 16th International Conference onComputational Linguistics (COLING ?96), 340?345.
Copenhagen, Denmark.Fox, Heidi J.
2002.
?Phrasal cohesion and statis-tical machine translation.?
Conference on Em-296pirical Methods in Natural Language Processing(EMNLP ?02), 304?311.
Philadelphia, PA.Gao, Qin and Stephan Vogel.
2008.
?Parallel im-plementations of word alignment tool.?
Workshopon Software Engineering, Testing, and Quality As-surance for Natural Language Processing, 49?57.Columbus, Ohio.Hopkins, Mark and Jonathan May.
2011.
?Tuningas ranking.?
Conference on Empirical Methods inNatural Language Processing (EMNLP ?11).
Ed-inburgh, UK.Koehn, Philipp, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, OndrejBojar, Alexandra Constantin, and Evan Herbst.2007.
?Moses: Open source toolkit for statis-tical machine translation.?
45th Annual Meet-ing of the Association for Computational Linguis-tics (ACL ?07): Demonstration Session.
Prague,Czech Republic.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
?Statistical phrase-based transla-tion.?
Human Language Technology Conferenceof the North American Chapter of the Associationfor Computational Linguistics (HLT-NAACL ?03),48?54.
Edmonton, AB, Canada.Macherey, Wolfgang, Franz Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
?Lattice-based min-imum error rate training for statistical machinetranslation.?
Conference on Empirical Methodsin Natural Language Processing (EMNLP ?08),725?734.
Honolulu, Hawaii.Magerman, David M. 1995.
?Statistical decision-tree models for parsing.?
Proceedings of the An-nual Meeting of the ACL, 276?283.McDonald, Ryan, Koby Crammer, and FernandoPereira.
2005.
?Online large-margin training ofdependency parsers.?
Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL?05), 91?98.
Ann Arbor,Michigan.Petrov, Slav, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
?Learning accurate, compact,and interpretable tree annotation.?
Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, 433?440.
Sydney, Australia.Petrov, Slav and Dan Klein.
2007.
?Improved in-ference for unlexicalized parsing.?
Human Lan-guage Technologies 2007: The Conference ofthe North American Chapter of the Associationfor Computational Linguistics; Proceedings ofthe Main Conference, 404?411.
Rochester, NewYork.Schmid, Helmut.
1995.
?Improvements in part-of-speech tagging with an application to German.?
InProceedings of the ACL SIGDAT-Workshop, 47?50.
Dublin, Ireland.Weese, Jonathan, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.?Joshua 3.0: Syntax-based machine translationwith the Thrax grammar extractor.?
Proceed-ings of the Sixth Workshop on Statistical MachineTranslation, 478?484.
Edinburgh, Scotland.Wu, Dekai.
1997.
?Stochastic inversion transduc-tion grammars and bilingual parsing of parallelcorpora.?
Computational Linguistics, 23(3):377?403.Yamada, Kenji and Kevin Knight.
2001.
?A syntax-based statistical translation model.?
39th An-nual Meeting of the Association for Computa-tional Linguistics (ACL ?01).
Toulouse, France.297
