INFORMATION STATES AS F IRST  CLASS C IT IZENSJorgen Vi l ladsenCentre for Language Technology, Univers i ty of CopenhagenNjalsgade 80, DK-2300 Copenhagen S, DenmarkInternet:  jv@cst .ku.dkABSTRACTThe information state of an agent is changed whena text (in natural anguage) is processed.
Themeaning of a text can be taken to be this informa-tion state change potential.
The inference of a con-sequence make explicit something already implicitin the premises - -  i.e.
that no information statechange occurs if the (assumed) consequence t xt isprocessed after the (given) premise texts have beenprocessed.
Elementary logic (i.e.
first-order logic)can be used as a logical representation languagefor texts, but the notion of a information state (aset of possibilities - -  namely first-order models) isnot available from the object language (belongs tothe meta language).
This means that texts withother texts as parts (e.g.
propositional ttitudeswith embedded sentences) cannot be treated di-rectly.
Traditional intensional logics (i.e.
modallogic) allow (via modal operators) access to theinformation states from the object language, butthe access is limited and interference with (exten-sional) notions like (standard) identity, variablesetc.
is introduced.
This does not mean that theideas present in intensional logics will not work(possibly improved by adding a notion of partial-ity), but rather that often a formalisation i thesimple type theory (with sorts for entities and in-dices making information states first class citizens- -  like individuals) is more comprehensible, flexi-ble and logically well-behaved.INTRODUCTIONClassical first-order logic (hereafter called elemen-tary logic) is often used as logical representa-tion language.
For instance, elementary logic hasproven very useful when formalising mathemati-cal structures like in axiomatic set theory, num-ber theory etc.
Also, in natural language process-ing (NLP) systems, "toy" examples are easily for-malised in elementary logic:Every man lies.
John is a man.So, John lies.
(1)vx(man(x) lie(x)), man(John)zi (john) (2)303The formalisation is judged adequate since themodel theory of elementary logic is in correspon-dence with intuitions (when some logical maturityis gained and some logical innocence is lost) - -moreover the proof theory gives a reasonable no-tion of entailment for the "toy" examples.Extending this success story to linguisticallymore complicated cases is difficult.
Two problem-atic topics are:AnaphoraIt must be explained how, in a text, a dependentmanages to pick up a referent that was introducedby its antecedent.Every man lies.
John is a man.So, he lies.
(3)At t i tude  reportsPropositional ttitudes involves reports about cog-nition (belief/knowledge), perception etc.Mary believes that every man lies.John is a man.So, Mary believes that John lies.
(4)It is a characteristic that if one starts with the"toy" examples in elementary logic it is very dif-ficult to make progress for the above-mentionedproblematic topics.
Much of the work on thefirst three topics comes from the last decade - -in case of the last topic pioneering work by Hin-tikka, Kripke and Montague started in the sixties.The aim of this paper is to show that by takingan abstract notion of information states as start-ing point the "toy" examples and the limitationsof elementary logic are better understood.
We ar-gue that information states are to be taken seriousin logic-based approaches to NLP.
Furthermore,we think that information states can be regardedas sets of possibilities (structural aspects can beadded, but should not be taken as stand-alone).Information states are at the meta-level onlywhen elementary logic is used.
Information statesare still mainly at the meta-level when intensionallogics (e.g.
modal logic) are used, but some ma-nipulations are available at the object level.This limited access is problematic in connec-tion with (extensional) notions like (standard)identity, variables etc.
Information states can beput at object level by using a so-called simple typetheory (a classical higher-order logic based on thesimply typed A-calculus) - -  this gives a very ele-gant framework for NLP applications.The point is not that elementary or the vari-ous intensional logics are wrong - -  on the contrarythey include many important ideas - -  but for thepurpose of understanding, integrating and imple-menting a formalisation one is better off with asimple type theory (stronger type theories are pos-sible, of course).AGENTS AND TEXTSConsider an agent processing the texts t l , .
.
.
,  tn-By processing we mean that the agent ac-cepts the information conveyed by the texts.
Thetexts are assumed to be declarative (purely infor-mative) and unambiguous (uniquely informative).The texts are processed one by one (dynamically)- -  not considered as a whole (statically).
The dy-namic interpretation of texts seems more realisticthan the static interpretation.By a text we consider (complete) discourses- -  although as examples we use only single (com-plete) sentences.
We take the completeness tomean that the order of the texts is irrelevant.
Ingeneral texts have expressions as parts whose or-der is important - -  the completeness requirementonly means that the (top level) texts are completeunits.INFORMATION STATESWe first consider an abstract notion of an infor-mation state (often called a knowledge state or abelief state).
The initial information state I0 isassumed known (or assumed irrelevant).
Changesare of the information states of the agent as fol-lows:I0 r1'I1 r2, I2 r3 .
.
.
r%i  nwhere r/ is the change in the information statewhen the text t / i s  processed.An obvious approach is to identify informationstates with the set of texts already processed - -hence nothing lost.
Some improvements are pos-sible (normalisation and the like).
Since the textsare concrete objects they are easy to treat compu-tationally.
We call this approach the syntacticalapproach.An orthogonal approach (the semantical ap-proach) identifies information states with sets ofpossibilities.
This is the approach followed here.304Note that a possibility need not be a so-called"possible world" - -  partiality and similar notionscan be introduced, see Muskens (1989).A combination of the two approaches mightbe the optimal solution.
Many of these aspectsare discussed in Konolige (1986).Observe that the universal and empty sets areunderstood as opposites: the empty set of possi-bility and the universal set of texts represent the(absolute) inconsistent information state; and theuniversal set of possibility and the empty set oftexts represent he (absolute) initial informationstate.
Other notions of consistency and initialitycan be defined.A partial order on information states ("gettingbetter informed") is easy obtained.
For the syn-tactical approach this is trivial - -  more texts makeone better informed.
For the semantical pproachone could introduce previously eliminated possi-bilities in the information state, but we assumeeliminative information state changes: r(I) C Ifor all I (this does not necessarily hold for non-monotonic logics / belief revision / anaphora(?
)- -  see Groenendijk and Stokhof (1991) for furtherdetails).Given the texts t l , .
.
.
, t~ the agent is askedwhether a text t can be inferred; i.e.
whether pro-cessing t after processing t l , .
.
.
, t~  would changethe information state or not:Here r is the identity function.ELEMENTARY LOGICWhen elementary logic is used as logical represen-tation language for texts, information states areidentified with sets of models.Let the formulas ?1 , .
.
.
,  On, ?
be the transla-tions of the texts t l , .
.
.
, tn , t .
The informationstate when tl .
.
.
.
,tk has been processed is theset of all models in which ?1, .
.
.
,  ?n are all true.Q,  ?
.
.
, tn  entails t if the model set correspond-ing to the processing of Q , .
.
.
,  t,, does not changewhen t is processed.
I.e.
alternatively, consider aparticular model M - -  if ?1,- .
.
,  &n are all true inM then ?
must be true in M as well (this is theusual formulation of entailment).Hence, although any proof theory for elemen-tary logic matches the notion of entailment for"toy" example texts, the notion of informationstates is purely a notion of the model theory(hence in the meta-language; not available fromthe object language).
This is problematic whentexts have other texts as parts, like the embeddedsentence in propositional attitudes, since a directformalisation i elementary logic is ruled out.TRADIT IONAL APPROACHWhen traditional intensional ogics (e.g.
modallogics) are used as logical representation languagesfor texts, information states are identified withsets of possible worlds relative to a model M =(W,...), where W is the considered set of possibleworlds.The information state when t l , .
.
.
, tk  hasbeen processed is, relative to a model, the set ofpossible worlds in which ?1,.--, ek are all true.The truth definition for a formula ?
allows formodal operators, say g), such that if ?
is (3?
thenis true in the possible worlds We C_ W if ?
istrue in the possible worlds We _C W, where We --fv(W?)
for some function f?~ : :P(W) --* :P(W)(hence U = (W, fv, .
.
. )
) .For the usual modal operator \[\] the functionf:: reduces to a relation R:~ : W ?
W such that:W?
-- fo(W,) - U {w?
I Ro(w~,, w?
)}w~EWebBy introducing more modal operators the informa-tion states can be manipulated further (a small setof "permutational" and "quantificational" modaloperators would suffice - -  compare combinatorylogic and variable-free formulations of predicatelogic).
However, the information states as well asthe possible worlds are never directly accessiblefrom the object language.Another complication is that the fv functioncannot be specified in the object language directly(although equivalent object language formulas canoften be found - -  of.
the correspondence theory formodal logic).Perhaps the most annoying complication isthe possible interference with (extensional) no-tions like (standard) identity, where Leibniz's Lawfails (for non-modally closed formulas) - -  seeMuskens (1989) for examples.
If variables arepresent he inference rule of V-Introduction failsin a similar way.SIMPLE TYPE  THEORYThe above-mentioned complications becomes evenmore evident if elementary logic is replaced by asimple type theory while keeping the modal oper-ators (cf.
Montague's Intensional Logic).
The ~-calculus in the simple type theory allows for an el-egant compositionality methodology (category totype correspondence over the two algebras).
Oftenthe higher-order logic (quantificational power) fa-cilities of the simple type theory are not necessary- -  or so-called general models are sufficient.The complication regarding variables men-tioned above manifests itself in the way that /3-reduction does not hold for the A-calculus (again,305see Muskens (1989) and references herein).
Evenmore damaging: The (simply typed!)
A-calculus isnot Church-Rosser (due to the limited a-renamingcapabilities of the modal operators).What seems needed is a logical representationlanguage in which the information states are ex-plicit manipulable, like the individuals in elemen-tary logic.
This point of view is forcefully defendedby Cresswell (1990), where the possibilities of theinformation states are optimised using the well-known technique of indexing.
Hence we obtain anontology of entities and indices.In recent papers we have presented and dis-cussed a categorial grammar formalism capableof (in a strict compositional way) parsing andtranslating natural language texts, see Villadsen(1991a,b,c).
The resulting formulas are terms in amany-sorted simple type theory.
An example of atranslation (simplified):Mary believes that John lies.
(5))~i.believe(i, Mary, ()~j.lie(j, John))) (6)Adding partiality along the lines in Muskens(1989) is currently under investigation.ACKNOWLEDGMENTSReports work done while at Department of Com-puter Science, Technical University of Denmark.REFERENCESM.
J. Cresswell (1990).
Entities and Indices.Kluwer Academic Publishers.J.
Groenendijk and M. Stokhof (1991).
Two Theo-ries of Dynamic Semantics.
In J. van Eijck, editor,Logics in AI - 91, Amsterdam.
Springer-Verlag(Lecture Notes in Computer Science 478).K.
Konolige (1986) A Deduction Model of Belief.Pitman.R.
Muskens (1989).
Meaning and Partiality.
PhDthesis, University of Amsterdam.J.
Villadsen (1991a).
Combinatory CategorialGrammar for Intensional Fragment of NaturalLanguage.
In B. Mayoh, editor, ScandinavianConference on Artificial Intelligence- 91, Roskilde.IOS Press.J.
Villadsen (1991b).
Categorial Grammar and In-tensionality.
In Annual Meeting of the Danish As-sociation for Computational Linguistics - 91, Aal-borg.
Department of Computational Linguistics,Arhus Business School.J.
Villadsen (1991c).
Anaphora and Intensional-ity in Classical Logic.
In Nordic ComputationalLinguistics Conference - 91, Bergen.
To appear.
