Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
33?41, Prague, June 2007. c?2007 Association for Computational LinguisticsLearning Unsupervised SVM Classifier for Answer Selection in WebQuestion AnsweringYouzheng Wu, Ruiqiang Zhang, Xinhui Hu, and Hideki KashiokaNational Institute of Information and Communications Technology (NICT),ATR Spoken Language Communication Research Labs.2-2-2 Hikaridai ?Keihanna Science City?
Kyoto 619-0288 Japan{Youzheng.wu,Ruiqiang.zhang,Xinhui.hu,Hideki.kashioka}@atr.jpAbstractPrevious machine learning techniques foranswer selection in question answering(QA) have required question-answer train-ing pairs.
It has been too expensive andlabor-intensive, however, to collect thesetraining pairs.
This paper presents a novelunsupervised support vector machine (U-SVM) classifier for answer selection, whichis independent of language and does not re-quire hand-tagged training pairs.
The keyideas are the following: 1. unsupervisedlearning of training data for the classifier byclustering web search results; and 2. select-ing the correct answer from the candidatesby classifying the question.
The compara-tive experiments demonstrate that the pro-posed approach significantly outperformsthe retrieval-based model (Retrieval-M), thesupervised SVM classifier (S-SVM), and thepattern-based model (Pattern-M) for answerselection.
Moreover, the cross-model com-parison showed that the performance rank-ing of these models was: U-SVM > Pattern-M > S-SVM > Retrieval-M.1 IntroductionThe purpose of answer selection in QA is to se-lect the exact answer to the question from the ex-tracted candidate answers.
In recent years, manysupervised machine learning techniques for answerselection in open-domain question answering havebeen investigated in some pioneering studies [Itty-cheriah et al 2001; Ng et al 2001; Suzuki et al2002; Sasaki, et al 2005; and Echihabi et al 2003].Compared with retrieval-based [Yang et al 2003],pattern-based [Ravichandran et al 2002 and Soub-botin et al 2002], and deep NLP-based [Moldovanet al 2002, Hovy et al 2001; and Pasca et al 2001]answer selection, machine learning techniques aremore effective in constructing QA components fromscratch.
These techniques suffer, however, from theproblem of requiring an adequate number of hand-tagged question-answer training pairs.
It is too ex-pensive and labor intensive to collect such trainingpairs for supervised machine learning techniques.To tackle this knowledge acquisition bottleneck,this paper presents an unsupervised SVM classifierfor answer selection, which is independent of lan-guage and question type, and avoids the need forhand-tagged question-answer pairs.
The key ideasare as follows:1.
Regarding answer selection as a kind of classi-fication task and adopting an SVM classifier;2.
Applying unsupervised learning of pseudo-training data for the SVM classifier by cluster-ing web search results;3.
Training the SVM classifier by using threetypes of features extracted from the pseudo-training data; and4.
Selecting the correct answer from the candidateanswers by classifying the question.
Note thatthis means classifying a question into one ofthe clusters learned by clustering web searchresults.
Therefore, our classifying the question33Figure 1: Web Question Answering Architectureis different from conventional question classifi-cation (QC) [Li et al 2002] that determines theanswer type of the question.The proposed approach is fully unsupervised andstarts only from a user question.
It does not requirerichly annotated corpora or any deep linguistic tools.To the best of our knowledge, no research on thiskind of study we discuss here has been reported.Figure 1 illustrates the architecture of our web QAapproach.
The S-SVM and Pattern-M models areincluded for comparison.Because the focus of this paper just evaluates theanswer selection part, our approach requires knowl-edge of the answer type to the question in order tofind candidate answers, and that the answer must bea NE for convenience in candidate extraction.
Ex-periments using Chinese versions of the TREC 2004and 2005 test data sets show that our approach sig-nificantly outperforms the S-SVM for answer selec-tion, with a top 1 score improvement of more than20%.
Results obtained with the test data set in [Wuet al 2004] show that the U-SVM increases thetop 1/mrr 5/top 5 scores by 5.95%/6.06%/8.68%as compared with the Pattern-M.
Moreover, ourcross-model comparison demonstrates that the per-formance ranking of all models considered is: U-SVM > Pattern-M > S-SVM > Retrieval-M.2 Comparison among ModelsRelated researches on answer selection in QA can beclassified into four categories.
The retrieval-basedmodel [Yang et al 2003] selects a correct answerfrom the candidates according to the distance be-tween a candidate and all question keywords.
Thismodel does not work, however, if the question andthe answer-bearing sentences do not match on thesurface.
The pattern-based model [Ravichandranet al 2002 and Soubbotin et al 2002] first clas-sifies the question into predefined categories, andthen extracts the exact answer by using answer pat-terns learned off-line.
Although the pattern-basedmodel can obtain high precision for some prede-fined types of questions, it is difficult to define ques-tion types in advance for open-domain question an-swering.
Furthermore, this model is not suitable forall types of questions.
The deep NLP-based model[Moldovan et al 2002; Hovy et al 2001; and Pascaet al 2001] usually parses the user question and ananswer-bearing sentence into a semantic represen-tation, and then semantically matches them to findthe answer.
This model has performed very well atTREC workshops, but it heavily depends on high-performance NLP tools, which are time consumingand labor intensive for many languages.
Finally, themachine learning-based model has also been inves-tigated.
current models of this type are based on su-pervised approaches [Ittycheriah et al 2001; Ng etal.
2001; Suzuki et al 2002; and Sasaki et al 2005]that are heavily dependent on hand-tagged question-answer training pairs, which not readily available.In response to this situation, this paper presentsthe U-SVM for answer selection in open-domainweb question answering system.
Our U-SVM hasthe following advantages over supervised machinelearning techniques.
First, the U-SVM classifiesquestions into a question-dependent set of clusters,and the answer is the name of a question cluster.In contrast, most previous models have classifiedcandidates into positive and negative.
Second, theU-SVM automatically learns the unique question-dependent clusters and the pseudo-training for each34Table 1: Comparison of Various Machine Learning TechniquesSystem Model Key Idea Training Data[Ittycheriah et al 2001] ME Classifier Classifying candidates into positiveand negative5,000 EnglishQ-A pairs[Suzuki et al 2002] SVM Classifier Classifying candidates into positiveand negative1358 JapaneseQ-A pairs[Echihabi et al 2003] N-C Model Selecting correct answer by aligningquestion with sentences90,000 EnglishQ-A pairs[Sasaki et al 2005] ME Classifier Classifying words in sentences into an-swer and non-answer words2,000 JapaneseQ-A pairsOur U-SVM Model SVM Classifier Classifying question into a set ofquestion-dependent clustersNo Q-A pairsquestion.
This differs from the supervised tech-niques, in which a large number of hand-taggedtraining pairs are shared by all of the test ques-tions.
In addition, supervised techniques indepen-dently process the answer-bearing sentences, so theanswers to the questions may not always be ex-tractable because of algorithmic limitations.
On theother hand, the U-SVM can use the interdependencebetween answer-bearing sentences to select the an-swer to a question.Table 1 compares the key idea and training dataused in the U-SVM with those used in the supervisedmachine learning techniques.
Here, ME means themaximum entropy model, and N-C means the noisy-channel model.3 The U-SVMThe essence of the U-SVM is to regard answer selec-tion as a kind of text categorization-like classifica-tion task, but with no training data available.
In theU-SVM, the steps of ?clustering web search results?,?classifying the question?, and ?training SVM clas-sifier?
play very important roles.3.1 Clustering Web Search ResultsWeb search results, such as snippets returned byGoogle, usually include a mixture of multiplesubtopics (called clusters in this paper) related tothe user question.
To group the web search resultsinto clusters, we assume that the candidate answer ineach Google snippet can represent the ?signature?
ofits cluster.
In other words, the Google snippets con-taining the same candidate are regarded as alignedsnippets, and thus belong to the same cluster.
Websearch results are clustered in two phases.?
A first-stage Google search (FGS) is ap-plied to extract n candidate answers{c1, c2, .
.
.
, cn} from the top m Googlesnippets {s1, s2, .
.
.
, sm} by a NER tool[Wu et al 2005].
Those snippets containingthe candidates {ci} and at least one ques-tion keyword {qi} are retained.
Finally,the retained snippets {s1, s2, .
.
.
, sm} areclustered into n clusters {C1, C2, .
.
.
, Cn}by clustering web search results, that is,If a snippet includes L different candidates,the snippet belongs to L different clusters.If the candidates of different snippets arethe same, these snippets belong to the sameclusters.Consequently, the number of clusters {Ci} isfully determined by the number of candidates{ci}, and the cluster name of a cluster Ci is thecandidate answer ci.
Up to this point, we haveobtained clusters and sample snippets for eachcluster that will be used as training data for theSVM classifier.
Because this training data islearned automatically, rather than hand-tagged,we call it pseudo-training data.?
A second-stage Google search (SGS) is ap-plied to resolve data sparseness in the pseudo-training samples learned through the FGS.
TheFGS data may have very few training snip-pets in some clusters, so more snippets mustbe collected.
Note that this step just learns new35Google snippets into the clusters learned by theFGS, but does not add new clusters.For each candidate answer ci:Combine the original query q = {qi} andthe candidate ci to form a new query q?
={q, ci}.Submit q?
to Google and download the top50 Google snippets.Retain the snippets containing the candi-date ci and at least one keyword qi.Group the retained snippets into n clustersto form the new pseudo-training data.EndHere, we give an example illustrating the prin-ciple of clustering web search results in theFGS.
In submitting TREC 2004 test question 1.1?when was the first Crip gang started??
to Google(http://www.google.com/apis), we extract n(= 8)different candidates from the top m(= 30) Googlesnippets.
The Google snippets containing the samecandidates are aligned snippets, and thus the 12 re-tained snippets are grouped into 8 clusters, as listedin Table 2.
This table roughly indicates that the snip-pets with the same candidate answers contain thesame sub-meanings, so these snippets are consideredas aligned snippets.
For example, all Google snip-pets that contain the candidate answer 1969 expressthe time of establishment of ?the first Crip gang?.In summary, the U-SVM uses the result of ?clus-tering web search results?
as the pseudo-trainingdata of the SVM classifier, and then classifies userquestion into one of the clusters for answer selec-tion.
On the one hand, the clusters and their namesare based on candidate answers to question; on theother hand, candidates are dependent on question.Therefore, the clusters are question-dependent.3.2 Classifying QuestionUsing the pseudo-training data obtained by cluster-ing web search results to train the SVM classifier,we classify user questions into a set of question-dependent clusters and assume that the correct an-swer is the name of the question cluster that is as-signed by the trained U-SVM classifier.
For theabove example, if the U-SVM classifier, trained onthe pseudo-training data listed in Table 2, classifiesthe above test question into a cluster whose name is1969, then the cluster name 1969 is the answer tothe question.This paper selects LIBSVM toolkit1 to implementthe SVM classifier.
The kernel is the radical basisfunction with the parameter ?
= 0.001 in the exper-iments.3.3 Feature ExtractionTo classify the question into a question-dependentset of clusters, the U-SVM classifier extracts threetypes of features.?
A similarity-based feature set (SBFS) isextracted from the Google snippets.
The SBFSattempts to capture the word overlap betweena question and a snippet.
The possible valuesrange from 0 to 1.SBFS Featurespercentage of matched keywords (KWs)percentage of mismatched KWspercentage of matched bi-grams of KWspercentage of matched thesaurusesnormalized distance between candidate andKWsTo compute the matched thesaurus feature, weadopt TONGYICICILIN 2 in the experiments.?
A Boolean match-based feature set (BMFS) isalso extracted from the Google snippets.
TheBMFS attempts to capture the specific key-word Boolean matches between a question anda snippet.
The possible values are true or false.BMFS Featuresperson names are matched or notlocation names are matched or notorganization names are matched or nottime words are matched or notnumber words are matched or notroot verb is matched or notcandidate has or does not have bi-gram insnippet matching bi-gram in questioncandidate has or does not have desirednamed entity type?
A window-based word feature set (WWFS)is a set of words consisting of the words1http://www.csie.ntu.edu.tw/ cjlin/libsvm/2A Chinese Thesaurus Lexicon36Table 2: Clustering Web Search ResultsCluster Name Google Snippet1969 It is believed that the first Crip gang was formed in late 1969.
During this time inLos Angeles there were ...... the first Bloods and Crips gangs started forming in Los Angeles in late 1969, theIsland Bloods sprung up in north Pomona ...... formed by 16 year old Raymond Lee Washington in 1969.
Williams joinedWashington in 1971 ... had come to be called the Crips.
It was initially started toeliminate all street gangs ...August 8, 2005 High Country News ?
August 8, 2005: The Gangs of Zion2004 2004 main 1 Crips 1.1 FACTOID When was the first Crip gang started?
1.2 FAC-TOID What does the name mean or come...1972 One of the first-known and publicized killings by Crip gang members occurred atthe Hollywood Bowl in March 1972.1971 Williams joined Washington in 1971, forming the westside faction of what hadcome to be called the Crips.The Crips gang formed as a kind of community watchdog group in 1971 after thedemise of the Black Panthers.
...... formed by 16 year old Raymond Lee Washington in 1969.
Williams joinedWashington in 1971 ... had come to be called the Crips.
It was initially started toeliminate all street gangs ...1982 Oceanside police first started documenting gangs in 1982, when five known gangswere operating in the city: the Posole Locos...mid-1990s Street Locos; Deep Valley Bloods and Deep Valley Crips.
By the mid-1990s, gangviolence had ...1970s The Blood gangs started up as opposition to the Crips gangs, also in the 1970s, andthe rivalry stands to this day ...preceding {wi?5, .
.
.
, wi?1} and following{wi+1, .
.
.
, wi+5} the candidate answer.
TheWWFS features can be regarded as a kind ofrelevant snippets-based question keywords ex-pansion.
By extracting the WWFS feature set,the feature space in the U-SVM becomes ques-tion dependent, which may be more suitable forclassifying the question.
The number of classi-fication features in the S-SVM must be fixed,however, because all questions share the sametraining data.
This is one difference betweenthe U-SVM and the supervised SVM classifierfor answer selection.
Each word feature in theWWFS is weighted using its ISF value.ISF (wj , Ci) =N(wj , Ci) + 0.5N(wj) + 0.5(1)where N(wj) is the total number of thesnippets containing word feature wj , andN(wj , Ci) is the number of snippets in clusterCi containing word feature wj .When constructing question vector, we assumethat the question is an ideal question that con-tains all the extracted WWFS words.
There-fore, the values of the WWFS word features inquestion vector are 1.
Similarly, the values ofthe SBFS and BMFS features in question vec-tor are also estimated by self-similarity calcu-lation.4 Experiments4.1 Data SetsFor the experiments, no English named entity recog-nition (NER) tool is in our hand at the time ofthe experiments; therefore, we validate the U-SVM37in terms of Chinese web QA using three test datasets, which will be published with this paper3.
Al-though the U-SVM is independent of the questiontypes, for convenience in candidate extraction, onlythose questions whose answers are named entitiesare selected.
The three test data sets are CTREC04,CTREC05 and CTEST05.
CTREC04 is a set of178 Chinese questions translated from TREC 2004FACTOID testing questions.
CTREC05 is a set of279 Chinese questions translated from TREC 2005FACTOID testing questions.
CTEST05 is a set of178 Chinese questions found in [Wu et al 2004]that are similar to TREC testing questions exceptthat they are written in Chinese.
Figure 2 breaksdown the types of questions (manually assigned) inthe CTREC04 and CTREC05 data sets.
Here, PER,LOC, ORG, TIM, NUM, and CR refer to questionswhose answers are a person, location, organization,time, number, and book or movie, respectively.Figure 2: Statistics of CTEST05To collect the question-answer training data forthe S-SVM, we submitted 807 Chinese questions toGoogle and extracted the candidates for each ques-tion from the top 50 Google snippets.
We then man-ually selected the snippets containing the correctanswers as positive snippets, and designated all ofthe other snippets as negative snippets.
Finally, wecollected 807 hand-tagged Chinese question-answerpairs as the training data of S-SVM called CTRAIN-DATA.4.2 Evaluation MethodIn the experiments, the top m(= 50) Google snip-pets are adopted to extract candidates by using a3Currently no public testing question set for simplified Chi-nese QA is available.Chinese NER tool [Wu et al 2005].
The number ofthe candidates extracted from the top m(= 50) snip-pets, n, is adaptive for different questions but it doesnot exceed 30.
The results are evaluated in termsof two scores, top n and mrr 5.
Here, top n is therate at which at least one correct answer is includedin the top n answers, while mrr 5 is the average re-ciprocal rank (1/n) of the highest rank n(n ?
5) ofa correct answer to each question.4.3 U-SVM vs. Retrieval-MThe Retrieval-M selects the candidate with the short-est distances to all question keywords as the cor-rect answer.
In this experiment, the Retrieval-Mis implemented based on the snippets returned byGoogle, while the U-SVM is based on the SGS data,the SBFS and BMFS feature.
Table 3 summarizesthe comparative performance.Table 3: Comparison of Retrieval-M and U-SVMRetrieval-M U-SVMtop 1 27.84% 53.61%CTREC04 mrr 5 43.67% 66.25%top 5 71.13% 88.66%top 1 34.00% 50.00%CTREC05 mrr 5 48.20% 62.38%top 5 71.33% 82.67%The table shows that the U-SVM greatly improvesthe performance of the Retrieval-M: the top 1 im-provements for CTREC04 and CTREC05 are about25.8% and 16.0%, respectively.
This experimentdemonstrates that the assumptions used here in clus-tering web search results and in classifying the ques-tion are effective in many cases, and that the U-SVMbenefits from these assumptions.4.4 U-SVM vs. S-SVMTo explore the effectiveness of our unsupervisedmodel as compared with the supervised model, weconduct a cross-model comparison of the S-SVMand the U-SVM with the SBFS and BMFS featuresets.
The U-SVM results are compared with the S-SVM results for CTREC04 and CTREC05 in Ta-bles 4 and 5, respectively.
The S-SVM is trainedon CTRAINDATA.These tables show the following:38Table 4: Comparison of U-SVM and S-SVM onCTREC04FGS SGStop 1 S-SVM 30.93% 39.18%U-SVM 45.36% 53.61%mrr 1 S-SVM 45.36% 53.54%U-SVM 57.44% 66.25%top 5 S-SVM 71.13% 79.38%U-SVM 76.29% 88.66%Table 5: Comparison of U-SVM and S-SVM onCTREC05FGS SGStop 1 S-SVM 30.00% 33.33%U-SVM 48.00% 50.00%mrr 1 S-SVM 45.59% 48.67%U-SVM 58.01% 62.38%top 5 S-SVM 72.00% 74.67%U-SVM 75.33% 82.67%?
The proposed U-SVM significantly outper-forms the S-SVM for all measurements andall test data sets.
For the CTREC04 test dataset, the top1 improvements for the FGS andSGS data are about 14.5% and 14.4%, respec-tively.
For the CTREC05 test data set, the top1score for the FGS data increases from 30.0%to 48.0%, and the top 1 score for the SGS dataincreases from 33.3% to 50.0%.
Note that theSBFS and BMFS features here is fewer than thefeatures in [Ittycheriah et al 2001; Suzuki etal.
2002], but the comparison is still effectivebecause the models are compared in terms ofthe same features.
In the S-SVM, all questionsshare the same training data, while the U-SVMuses the unique pseudo-training data for eachquestion.
This is the main reason why the U-SVM performs better than the S-SVM does.?
The SGS data is greatly helpful for boththe U-SVM and the S-SVM.
Compared withthe FGS data, the top 1/mrr 5/top 5 im-provements for the S-SVM and the U-SVMon CTREC04 are 8.25%/8.18%/8.25% and7.25%/8.81%/12.37%.
The SGS can be re-garded as a kind of query expansion.
The rea-sons for this improvement are: the data sparse-ness in FGS data is partially resolved; and theuse of the Web to introduce data redundancyis helpful.
[Clarke et al 2001; Magnini et al2002; and Dumais et al 2002].In the S-SVM, all of the test questions share thesame hand-tagged training data, so the WWFS fea-tures cannot be easily used [Ittycheriah et al 2002;Suzuki, et al 2002].
Tables 6 and 7 comparethe performances of the U-SVM with the (SBFS +BMFS) features, the WWFS features, and combina-tion of three types of features for the CTREC04 andCTREC05 test data sets, respectively.Table 6: Performances of U-SVM for Different Fea-tures on CTREC04SBFS+BMFS WWFS Combinationtop 1 53.61% 46.39% 60.82%mrr 5 66.25% 59.19% 71.31%top 5 88.66% 81.44% 88.66%Table 7: Performances of U-SVM for Different Fea-tures on CTREC05SBFS+BMFS WWFS Combinationtop 1 50.00% 49.33% 57.33%mrr 5 62.38% 59.26% 65.61%top 5 82.67% 74.00% 80.00%These tables report that combining three typesof features can improve the performance ofthe U-SVM.
Using a combination of featureswith the CTREC04 test data set results in thebest performances: 60.82%/71.31%/88.66% fortop 1/mrr 5/top 5.
Similarly, as compared withusing the (SBFS + BMFS) and WWFS features, theimprovements from using a combination of featureswith the CTREC05 test data set are 7.33%/3.23%/-2.67% and 8.00%/6.35%/6.00%, respectively.
Theresults also demonstrate that the (SBFS + BMFS)features are more important than the WWFS fea-tures.These comparative experiments indicate that theU-SVM performs better than the S-SVM does, eventhough the U-SVM is an unsupervised technique andno hand-tagged training data is provided.
The aver-39age top 1 improvements for both test data sets areboth more than 20%.4.5 U-SVM vs. Pattern-M vs. S-SVMTo compare the U-SVM with the Pattern-M andthe S-SVM, we use the CTEST05 data set, shownin Figure 3.
The CTEST05 includes 14 differentquestion types, for example, Inventor Stuff (withquestion like ?Who invented telephone??
), Event-Day (with question like ?when is World Day for Wa-ter??
), and so on.
The Pattern-M uses the depen-dency syntactic answer patterns learned in [Wu etal.
2007] to extract the answer, and named entitiesare also used to filter noise from the candidates.Figure 3: Statistics of CTEST05Table 8 summarizes the performances of the U-SVM, Pattern-M, and S-SVM models on CTEST05.Table 8: Comparison of U-SVM, Pattern-M and S-SVM on CTEST05S-SVM Pattern-M U-SVMtop 1 44.89% 53.14% 59.09%mrr 5 56.49% 61.28% 67.34%top 5 74.43% 73.14% 81.82%The results in the table show that the U-SVMsignificantly outperforms the S-SVM and Pattern-M, while the S-SVM underperforms the Pattern-M.
Compared with the Pattern-M, the U-SVM in-creases the top 1/mrr 5/top 5 scores by 5.95%/6.06%/8.68%, respectively.
The reasons may lie inthe following:?
The Chinese dependency parser influences de-pendency syntactic answer-pattern extraction,and thus degrades the performance of thePattern-M model.?
The imperfection of Google snippets affectspattern matching, and thus adversely influencesthe Pattern-M model.
From the cross-modelcomparison, we conclude that the performanceranking of these models is: U-SVM > Pattern-M > S-SVM > Retrieval-M.5 Conclusion and Future WorkThis paper presents an unsupervised machine learn-ing technique (called the U-SVM) for answer selec-tion that is validated in Chinese open-domain webQA.
Regarding answer selection as a kind of classifi-cation task, the U-SVM automatically learns clustersand pseudo-training data for each cluster by cluster-ing web search results.
It then selects the correctanswer from the candidates according to classifyingthe question.
The contribution of this paper is thatit presents an unsupervised machine learning tech-nique for web QA that starts with only a user ques-tion.
The results of our experiments with three testdata sets are encouraging.
As compared with theS-SVM, the top 1 performances of the U-SVM forthe CTREC04 and CTREC05 data sets are signifi-cantly improved, at more than 20%.
Moreover, theU-SVM performs better than the Retrieval-M andthe Pattern-M.These experiments have only validated the U-SVM on named entity types of questions that ac-count for about 82% of all TREC2004 and 2005FACTOID test questions.
In fact, our technique isindependent of question types only if the candidatescan be extracted.
In the future, we will explore theeffectiveness of our technique for the other types ofquestions.
The web search results clustering in theU-SVM defines that a candidate in a Google snip-pet can represent the ?signature?
of its cluster.
Thisdefinition, however, is not always effective.
To fil-ter noise in the pseudo-training data, we will extractrelations between the candidates and the keywordsas the cluster signatures of Google snippets.
More-over, applying the U-SVM to QA systems in otherlanguages, like English and Japanese, will also beincluded in our future work.40ReferencesAbdessamad Echihabi, and Daniel Marcu.
2003.
ANoisy-Channel Approach to Question Answering.
InProc.
of ACL-2003, Japan.Abraham Ittycheriah, Salim Roukos.
2002.
IBM?s Sta-tistical Question Answering System-TREC 11.
In Proc.of TREC-11, Gaithersburg, Maryland.Bernardo Magnini, Matteo Negri, Roberto Prevete,Hristo Tanev.
2002.
Is It the Right Answer?
Exploit-ing Web Redundancy for Answer Validation.
In Proc.of ACL-2002, Philadelphia, pp.
425 432.Charles L. A. Clarke, Gordon V. Cormack, Thomas R.Lynam.
Exploiting Redundancy in Question Answer-ing In Proc.
of SIGIR-2001, pp 358?365, 2001.Christopher Pinchak, Dekang Lin.
2006.
A ProbabilisticAnswer Type Model.
In Proc.
of EACL-2006, Trento,Italy, pp.
393-400.Dan Moldovan, Sanda Harabagiu, Roxana Girju, et al2002.
LCC Tools for Question Answering.
NIST Spe-cial Publication: SP 500-251, TREC-2002.Deepak Ravichandran, Eduard Hovy.
2002.
LearningSurface Text Patterns for a Question Answering Sys-tem.
In Proc.
of the 40th ACL, Philadelphia, July2002.Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin.
2001.
TheUse of External Knowledge of Factoid QA.
In Proc.of TREC 2001, Gaithersburg, MD, U.S.A., November13-16, 2001.Hui Yang, Tat-Seng Chua.
2003.
QUALIFIER: QuestionAnswering by Lexical Fabric and External Resources.In Proc.
of EACL-2003, page 363-370.Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia.
2001.Question Answering Using a Large Text Database: AMachine Learning Approach.
In Proc.
of EMNLP-2001, pp66-73 (2001).Jun Suzuki, Yutaka Sasaki, Eisaku Maeda.
2002.
SVMAnswer Selection for Open-Domain Question Answer-ing.
In Proc.
of Coling-2002, pp.
974 980 (2002).Marius Pasca.
2001.
A Relational and Logic Represen-tation for Open-Domain Textual Question Answering.In Proc.
of ACL (Companion Volume) 2001: 37-42.Martin M. Soubbotin, Sergei M. Soubbotin.
2002.
Use ofPatterns for Detection of Likely Answer Strings: A Sys-tematic Approach.
In Proc.
of TREC-2002, Gaithers-burg, Maryland, November 2002.Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,and Andre Ng.
Web Question Answering: Is MoreAlways Better?.
In Proc.
SIGIR-2002, pp 291?298,2002.Xin Li, and Dan Roth.
2002.
Learning Question Classi-fication.
In Proc.
of the 19th International Conferenceon Computational Linguistics, Taibai, 2002.Youzheng Wu, Hideki Kashioka, Jun Zhao.
2007.
Us-ing Clustering Approaches to Open-domain QuestionAnswering.
In Proc.
of CICLING-2007, Mexico City,Mexico, pp506 517, 2007.Youzheng Wu, Jun Zhao and Bo Xu.
2005.
ChineseNamed Entity Recognition Model Based on MultipleFeatures.
In Proc.
of HLT/EMNLP-2005, Vancouver,Canada, pp.427-434.Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu.2004.
Building an Evaluation Platform for ChineseQuestion Answering Systems.
In Proc.
of the FirstNCIRCS, China, December, 2004.Yutaka Sasaki.
2005.
Question Answering as Question-Biased Term Extraction: A New Approach towardMultilingual QA.
In Proc.
of ACL-2005, pp.215-222.41
