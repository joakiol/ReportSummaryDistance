Proceedings of ACL-08: HLT, pages 613?621,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEnhancing Performance of Lexicalised GrammarsRebecca Dridan?, Valia Kordoni?, Jeremy Nicholson??
?Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany?Dept of Computer Science and Software Engineering and NICTA, University of Melbourne, Australia{rdrid,kordoni}@coli.uni-sb.de, jeremymn@csse.unimelb.edu.auAbstractThis paper describes how external resourcescan be used to improve parser performance forheavily lexicalised grammars, looking at bothrobustness and efficiency.
In terms of robust-ness, we try using different types of externaldata to increase lexical coverage, and find thatsimple POS tags have the most effect, increas-ing coverage on unseen data by up to 45%.
Wealso show that filtering lexical items in a su-pertagging manner is very effective in increas-ing efficiency.
Even using vanilla POS tags weachieve some efficiency gains, but when us-ing detailed lexical types as supertags weman-age to halve parsing time with minimal loss ofcoverage or precision.1 IntroductionHeavily lexicalised grammars have been used in ap-plications such as machine translation and informa-tion extraction because they can produce semanticstructures which provide more information than lessinformed parsers.
In particular, because of the struc-tural and semantic information attached to lexiconitems, these grammars do well at describing com-plex relationships, like non-projectivity and centerembedding.
However, the cost of this additional in-formation sometimes makes deep parsers that usethese grammars impractical.
Firstly because, if theinformation is not available, the parsers may fail toproduce an analysis, a failure of robustness.
Sec-ondly, the effect of analysing the extra informationcan slow the parser down, causing efficiency prob-lems.
This paper describes experiments aimed atimproving parser performance in these two areas, byannotating the input given to one such deep parser,the PET parser (Callmeier, 2000), which uses lex-icalised grammars developed under the HPSG for-malism (Pollard and Sag, 1994).2 BackgroundIn all heavily lexicalised formalisms, such as LTAG,CCG, LFG and HPSG, the lexicon plays a key rolein parsing.
But a lexicon can never hope to containall words in open domain text, and so lexical cover-age is a central issue in boosting parser robustness.Some systems use heuristics based on numbers, cap-italisation and perhaps morphology to guess the cat-egory of the unknown word (van Noord and Mal-ouf, 2004), while others have focused on automati-cally expanding the lexicon (Baldwin, 2005; Hock-enmaier et al, 2002; O?Donovan et al, 2005).
An-other method, described in Section 4, uses externalresources such as part-of-speech (POS) tags to selectgeneric lexical entries for out-of-vocabulary words.In all cases, we lose some of the depth of informa-tion the hand-crafted lexicon would provide, but ananalysis is still produced, though possibly less thanfully specified.The central position of these detailed lexiconscauses problems, not only of robustness, but also ofefficiency and ambiguity.
Many words may havefive, six or more lexicon entries associated withthem, and this can lead to an enormous search spacefor the parser.
Various means of filtering this searchspace have been attempted.
Kiefer et al (1999) de-scribes a method of filtering lexical items by specify-ing and checking for required prefixes and particles613which is particularly effective for German, but alsoapplicable to English.
Other research has looked atusing dependencies to restrict the parsing process(Sagae et al, 2007), but the most well known fil-tering method is supertagging.
Originally describedby Bangalore and Joshi (1994) for use in LTAG pars-ing, it has also been used very successfully for CCG(Clark, 2002).
Supertagging is the process of assign-ing probable ?supertags?
to words before parsing torestrict parser ambiguity, where a supertag is a tagthat includes more specific information than the typ-ical POS tags.
The supertags used in each formal-ism differ, being elementary trees in LTAG and CCGcategories for CCG.
Section 3.2 describes an exper-iment akin to supertagging for HPSG, where the su-pertags are HPSG lexical types.
Unlike elementarytrees and CCG categories, which are predominantlysyntactic categories, the HPSG lexical types containa lot of semantic information, as well as syntactic.In the case study we describe here, the tools,grammars and treebanks we use are taken fromwork carried out in the DELPH-IN1 collaboration.This research is based on using HPSG along withMinimal Recursion Semantics (MRS: Copestake etal.
(2001)) as a platform to develop deep naturallanguage processing tools, with a focus on multi-linguality.
The grammars are designed to be bi-directional (used for generation as well as parsing)and so contain very specific linguistic information.In this work, we focus on techniques to improveparsing, not generation, but, as all the methods in-volve pre-processing and do not change the gram-mar itself, we do not affect the generation capabil-ities of the grammars.
We use two of the DELPH-IN wide-coverage grammars: the English ResourceGrammar (ERG: Copestake and Flickinger (2000))and a German grammar, GG (Mu?ller and Kasper,2000; Crysmann, 2003).
We also use the PET parser,and the [incr tsdb()] system profiler and treebankingtool (Oepen, 2001) for evaluation.3 Parser RestrictionAn exhaustive parser, such as PET, by default pro-duces every parse licensed by the grammar.
How-ever, in many application scenarios, this is unnec-essary and time consuming.
The benefits of us-1http://wiki.delph-in.net/ing a deep parser with a lexicalised grammar arethe precision and depth of the analysis produced,but this depth comes from making many fine dis-tinctions which greatly increases the parser searchspace, making parsing slow.
By restricting the lexi-cal items considered during parsing, we improve theefficiency of a parser with a possible trade-off of los-ing correct parses.
For example, the noun phrasereading of The dog barks is a correct parse, althoughunlikely.
By blocking the use of barks as a nounin this case, we lose this reading.
This may be anacceptable trade-off in some applications that canmake use of the detailed information, but only if itcan be delivered in reasonable time.
An exampleof such an application is the real-time speech trans-lation system developed in the Verbmobil project(Wahlster, 2000), which integrated deep parsing re-sults, where available, into its appointment schedul-ing and travel planning dialogues.
In these exper-iments we look at two methods of restricting theparser, first by using POS tags and then using lexicaltypes.
To control the trade-off between efficiencyand precision, we vary which lexical items are re-stricted according to a likelihood threshold from therespective taggers.
Only open class words are re-stricted, since it is the gross distinctions between, forinstance, noun and verb that we would like to utilise.Any differences between categories for closed classwords are more subtle and we feel the parser is bestleft to make these distinctions without restriction.The data set used for these experiments is the jh5section of the treebank released with the ERG.
Thistext consists of edited written English in the domainof Norwegian hiking instructions from the LOGONproject (Oepen et al, 2004).3.1 Part of Speech TagsWe use TreeTagger (Schmid, 1994) to produce POStags and then open class words are restricted if thePOS tagger assigned a tag with a probability overa certain threshold.
A lower threshold will lead tofaster parsing, but at the expense of losing more cor-rect parses.
We experiment with various thresholds,and results are shown in Table 1.
Since a gold stan-dard treebank for our data set was available, it waspossible to evaluate the accuracy of the parser.
Eval-uation of deep parsing results is often reported onlyin terms of coverage (number of sentences which re-614Threshold Coverage Precision Timegold 93.5% 92.2% N/Aunrestricted 93.3% 92.4% 0.67s1.00 90.7% 91.9% 0.59s0.98 88.8% 89.3% 0.49s0.95 88.4% 89.5% 0.48s0.90 86.4% 88.5% 0.44s0.80 84.3% 87.0% 0.43s0.60 81.5% 87.3% 0.39sTable 1: Results obtained when restricting the parser lex-icon according to the POS tag, where words are restrictedaccording to a threshold of POS probabilities.ceive an analysis), because, since the hand-craftedgrammars are optimised for precision over cover-age, the analyses are assumed to be correct.
How-ever, in this experiment, we are potentially ?dilut-ing?
the precision of the grammar by using externalresources to remove parses and so it is important thatwe have some idea of how the accuracy is affected.In the table, precision is the percentage of sentencesthat, having produced at least one parse, produced acorrect parse.
A parse was judged to be correct if itexactly matched the gold standard tree in all aspects,syntactic and semantic.The results show quite clearly how the coveragedrops as the average parse time per sentence drops.In hybrid applications that can back-off to less infor-mative analyses, this may be a reasonable trade-off,enabling detailed analyses in shorter times wherepossible, and using the shallower analyses other-wise.3.2 Lexical TypesAnother option for restricting the parser is to use thelexical types used by the grammar itself, in a simi-lar method to that described by Prins and van Noord(2003).
This could be considered a form of supertag-ging as used in LTAG and CCG.
Restricting by lex-ical types should have the effect of reducing ambi-guity further than POS tags can do, since one POStag could still allow the use of multiple lexical itemswith compatible lexical types.
On the other hand, itcould be considered more difficult to tag accurately,since there are many more lexical types than POStags (almost 900 in the ERG) and less training datais available.Configuration Coverage Precision Timegold 93.5% 92.2% N/Aunrestricted 93.3% 92.4% 0.67s0.98 with POS 93.5% 91.9% 0.63s0.95 with POS 93.1% 92.4% 0.48s0.90 with POS 92.9% 92.3% 0.37s0.80 with POS 91.8% 91.8% 0.31s0.60 with POS 86.2% 93.5% 0.21s0.98 no POS 92.9% 92.3% 0.62s0.95 no POS 90.9% 91.0% 0.48s0.90 no POS 87.7% 89.2% 0.42s0.80 no POS 79.7% 84.6% 0.33s0.60 no POS 67.0% 84.2% 0.23sTable 2: Results obtained when restricting the parser lex-icon according to the predicted lexical type, where wordsare restricted according to a threshold of tag probabilities.Two models, with and without POS tags as features, wereused.While POS taggers such as TreeTagger are com-mon, and there some supertaggers are available, no-tably that of Clark and Curran (2007) for CCG,no standard supertagger exists for HPSG.
Conse-quently, we developed a Maximum Entropy modelfor supertagging using the OpenNLP implementa-tion.2 Similarly to Zhang and Kordoni (2006), wetook training data from the gold?standard lexicaltypes in the treebank associated with ERG (in ourcase, the July-07 version).
For each token, we ex-tracted features in two ways.
One used features onlyfrom the input string itself: four characters from thebeginning and end of the target word token, and twowords of context (where available) either side of thetarget.
The second used the features from the first,along with POS tags given by TreeTagger for thecontext tokens.We held back the jh5 section of the treebank fortesting the Maximum Entropy model.
Again, thelexical items that were to be restricted were con-trolled by a threshold, in this case the probabil-ity given by the maximum entropy model.
Table2 shows the results achieved by these two models,with the unrestricted results and the gold standardprovided for comparison.Here we see the same trends of falling coverage2http://maxent.sourceforge.net/615with falling time for both models, with the POStagged model consistently outperforming the word-form model.
To give a clearer picture of the com-parative performance of all three experiments, Fig-ure 1 shows how the results vary with time for bothmodels, and for the POS tag restricted experiment.Here we can see that the coverage and precision ofthe lexical type restriction experiment that uses theword-form model is just above that of the POS re-stricted one.
However the POS tagged model clearlyoutperforms both, showing minimal loss of coverageor precision at a threshold which halved the averageparsing time.
At the lowest parsing time, we seethat precision of the POS tagged model even goesup.
This can be explained by noting that coveragehere goes down, and obviously we are losing moreincorrect parses than correct parses.This echoes the main result from Prins and vanNoord (2003), that filtering the lexical categoriesused by the parser can significantly reduce parsingtime, while maintaining, or even improving, preci-sion.
The main differences between our method andthat of Prins and van Noord are the training data andthe tagging model.
The key feature of their exper-iment was the use of ?unsupervised?
training data,that is, the uncorrected output of their parser.
In thisexperiment, we used gold standard training data, butmuch less of it (just under 200 000 words) and stillachieved a very good precision.
It would be inter-esting to see what amount of unsupervised parseroutput we would require to achieve the same levelof precision.
The other difference was the taggingmodel, maximum entropy versus Hidden MarkovModel (HMM).
We selected maximum entropy be-cause Zhang and Kordoni (2006) had shown thatthey got better results using a maximum entropy tag-ger instead of a HMM one when predicting lexicaltypes, albeit for a slightly different purpose.
It is notpossible to directly compare results between our ex-periments and those in Prins and van Noord, becauseof different languages, data sets and hardware, but itis worth noting that parsing times are much lower inour setup, perhaps more so than can be attributed to4 years hardware improvement.
While the range ofsentence lengths appears to be very similar betweenthe data sets, one possible reason for this could bethe very large number of lexical categories used intheir ALPINO system.657075808590950.2 0.3 0.4 0.5 0.6 0.7Average time per sentence (seconds)CoverageGold standardPOS tags3333333Lexical types (no POS model)++++++Lexical types (with POS model)22 2 222Unrestricted?
?75808590950.2 0.3 0.4 0.5 0.6 0.7Average time per sentence (seconds)PrecisionGold standardPOS tags3 333333Lexical types (no POS model)+ +++++Lexical types (with POS model)22 2 2 22Unrestricted?
?Figure 1: Coverage and precision varying with time forthe three restriction experiments.
Gold standard and un-restricted results shown for comparison.While this experiment is similar to that of Clarkand Curran (2007), it differs in that their supertag-ger assign categories to every word, while we lookup every word in the lexicon and the tagger is used tofilter what the lexicon returns, only if the tagger con-fidence is sufficiently high.
As Table 2 shows, whenwe use the tags for which the tagger had a low confi-dence, we lose significant coverage.
In order to runas a supertagger rather than a filter, the tagger wouldneed to be much more accurate.
While we can lookat multi-tagging as an option, we believe much moretraining data would be needed to achieve a sufficientlevel of tag accuracy.Increasing efficiency is important for enablingthese heavily lexicalised grammars to bring the ben-efits of their deep analyses to applications, but simi-616larly important is robustness.
The following sectionis aimed at addressing this issue of robustness, againby using external information.4 Unknown Word HandlingThe lexical information available to the parser iswhat makes the depth of the analysis possible, andthe default configuration of the parser uses an all-or-nothing approach, where a parse is not producedif all the lexical information is not available.
How-ever, in order to increase robustness, it is possible touse underspecified lexical information where a fullyspecified lexical item is not available.
One methodof doing this, built in to the PET parser, is to usePOS tags to select generic lexical items, and henceallow a (less than fully specified) parse to be built.The six data sets used for these experiments werechosen to give a range of languages and genres.Four sets are English text: jh5 described in Sec-tion 3; trec consisting of questions from TREC andincluded in the treebanks released with the ERG;a00 which is taken from the BNC and consists offactsheets and newsletters; and depbank, the 700sentences of the Briscoe and Carroll version of Dep-Bank (Briscoe and Carroll, 2006) taken from theWall Street Journal.
The last two data sets are Ger-man text: clef700 consisting of German questionstaken from the CLEF competition and eiche564 asample of sentences taken from a treebank parsedwith the German HPSG grammar, GG and consist-ing of transcribed German speech data concerningappointment scheduling from the Verbmobil project.Vital statistics of these data sets are described in Ta-ble 3.We used TreeTagger to POS tag the six data sets,with the tagger configured to assign multiple tags,where the probability of the less likely tags was atleast half that of the most likely tag.
The data wasinput using a PET input chart (PIC), which allowsPOS tags to be assigned to each token, and thenparsed each with the PET parser.3 All English datasets used the July-07 CVS version of the ERG andthe German sets used the September 2007 versionof GG.
Unlike the experiments described in Sec-tion 3, adding POS tags in this way will have noeffect on sentences which the parser is already able3Subversion revision 384LanguageNumberofSentencesAve.SentenceLengthjh5 English 464 14.2trec English 693 6.9a00 English 423 17.2depbank English 700 21.5clef German 700 7.5eiche564 German 564 11.5Table 3: Data sets used in input annotation experiments.to parse.
The POS tags will only be considered whenthe parser has no lexicon entry for a given word, andhence can only increase coverage.
Results are shownin Table 4, comparing the coverage over each set tothat obtained without using POS tags to handle un-known words.
Coverage here is defined as the per-centage of sentences with at least one parse.These results show very clearly one of the poten-tial drawbacks of using a highly lexicalised gram-mar formalism like HPSG: unknown words are oneof the main causes of parse failure, as quantified inBaldwin et al (2004) and Nicholson et al (2008).In the results here, we see that for jh5, trec andeiche564, adding unknown word handling made al-most no difference, since the grammars (specificallythe lexicons) have been tuned for these data sets.
Onthe other hand, over unseen texts, adding unknownword handling made a dramatic difference to thecoverage.
This motivates strategies like the POS tagannotation used here, as well as the work on deeplexical acquisition (DLA) described in Zhang andKordoni (2006) and Baldwin (2005), since no gram-mar could ever hope to cover all words used withina language.As mentioned in Section 3, coverage is not theonly evaluation metric that should be considered,particularly when adding potentially less precise in-formation to the parsing process (in this case POStags).
Since the primary effect of adding POS tagsis shown with those data sets for which we do nothave gold standard treebanks, evaluating accuracyin this case is more difficult.
However, in order togive some idea of the effects on precision, a sampleof 100 sentences from the a00 data set was evaluatedfor accuracy, for this and the following experiments.617In this instance, we found there was only a slightdrop in precision, where the original analyses had aprecision of 82% and the precision of the analyseswhen POS tags were used was 80%.Since the parser has the means to accept namedentity (NE) information in the input, we also ex-perimented with using generic lexical items gener-ated from NE data.
We used SProUT (Becker et al,2002) to tag the data sets and used PET?s inbuilt NEhandling mechanism to add NE items to the input,associated with the appropriate word tokens.
Thisworks slightly differently from the POS annotationmechanism, in that NE items are considered by theparser, even when the associated words are in thelexicon.
This has the effect of increasing the numberof analyses produced for sentences that already havea full lexical span, but could also increase coverageby enabling parses to be produced where there is nolexical span, or where no parse was possible becausea token was not recognised as part of a name.
In or-der to isolate the effect of the NE data, we ran oneexperiment where the input was annotated only withthe SProUT data, and another where the POS tagswere also added.
These results are also in Table 4.Again, we see coverage increases in the three un-seen data sets, a00, depbank and clef, but not to thesame extent as the POS tags.
Examining the re-sults in more detail, we find that the increases comealmost exclusively from sentences without lexicalspan, rather than in sentences where a token waspreviously not recognised as part of a name.
Thismeans that the NE tagger is operating almost like aPOS tagger that only tags proper nouns, and as thePOS tagger tags proper nouns quite accurately, wefind the NE tagger gives no benefit here.
When ex-amining the precision over our sample evaluation setfrom a00, we find that using the NE data alone addsno correct parses, while using NE data with POStags actually removes correct parses when comparedwith POS alone, since the (in these cases, incorrect)NE data is preferred over the POS tags.
It is possiblethat another named entity tagger would give betterresults, and this may be looked at in future experi-ments.Other forms of external information might also beused to increase lexical coverage.
Zhang and Kor-doni (2006) reported a 20% coverage increase overbaseline using a lexical type predictor for unknownwords, and so we explored this avenue.
The samemaximum entropy tagger used in Section 3 was usedand each open class word was tagged with its mostlikely lexical type, as predicted by the maximum en-tropy model.
Table 5 shows the results, with thebaseline and POS annotated results for comparison.As with the previous experiments, we see a cover-age increase in those data sets which are consideredunseen text for these grammars.
Again it is clearthat the use of POS tags as features obviously im-proves the maximum entropy model, since this sec-ond model has almost 10% better coverage on ourunseen texts.
However, lexical types do not appearto be as effective for increasing lexical coverage asthe POS tags.
One difference between the POS andlexical type taggers is that the POS tagger could pro-duce multiple tags per word.
Therefore, for the nextexperiment, we altered the lexical type tagger so itcould also produce multiple tags.
As with the Tree-Tagger configuration we used for POS annotation,extra lexical type tags were produced if they were atleast half as probable as the most likely tag.
A lowerprobability threshold of 0.01 was set, so that hun-dreds of tags of equal likelihood were not producedin the case where the tagger was unable to make aninformed prediction.
The results with multiple tag-ging are also shown in Table 5.The multiple tagging version gives a coverage in-crease of between 2 and 10% over the single tag ver-sion of the tagger, but, at least for the English datasets, it is still less effective than straight-forwardPOS tagging.
For the German unseen data set, clef,we do start getting above what the POS tagger canachieve.
This may be in part because of the featuresused by the lexical type tagger ?
German, beinga more morphologically rich language, may benefitmore from the prefix and suffix features used in thetagger.In terms of precision measured on our sampleevaluation set, the single tag version of the lexicaltype tagger which used POS tag features achieveda very good precision of 87% where, of all the extrasentences that could now be parsed, only one did nothave a correct parse.
In an application where preci-sion is considered much more important than cover-age, this would be a good method of increasing cov-erage without loss of accuracy.
The single tag ver-sion that did not use POS tags in the model achieved618Baseline with POS NE only NE+POSjh5 93.1% 93.3% 93.1% 93.3%trec 97.1% 97.5% 97.4% 97.7%a00 50.1% 83.9% 53.0% 85.8%depbank 36.3% 76.9% 51.1% 80.4%clef 22.0% 67.7% 42.3% 75.3%eiche564 63.8% 63.8% 64.0% 64.0%Table 4: Parser coverage with baseline using no unknown word handling and unknown word handling using POS tags,SProUT named entity data as the only annotation, or SProUT tags in addition to POS annotation.Single Lexical Types Multiple Lexical TypesBaseline POS -POS +POS -POS +POSjh5 93.1% 93.3% 93.3% 93.3% 93.5% 93.5%trec 97.1% 97.5% 97.3% 97.4% 97.3% 97.4%a00 50.1% 83.9% 63.8% 72.6% 65.7% 78.5%depbank 36.3% 76.9% 51.7% 64.4% 53.9% 69.7%clef 22.0% 67.7% 59.9% 66.8% 69.7% 76.9%eiche564 63.8% 63.8% 63.8% 63.8% 63.8% 63.8%Table 5: Parser coverage using a lexical type predictor for unknown word handling.
The predictor was run in single tagmode, and then in multi-tag mode.
Two different tagging models were used, with and without POS tags as features.the same precision as with using only POS tags, butwithout the same increase in coverage.
On the otherhand, the multiple tagging versions, which at leaststarted approaching the coverage of the POS tag ex-periment, dropped to a precision of around 76%.From the results of Section 3, one might expectthat at least the lexical type method of handling un-known words might at least lead to quicker parsingthan when using POS tags, however POS tags areused differently in this situation.
When POS tagsare used to restrict the parser, any lexicon entry thatunifies with the generic part-of-speech lexical cate-gory can be used by the parser.
That is, when theword is restricted to, for example, a verb, any lexi-cal item with one of the numerous more specific verbcategories can be used.
In contrast, in these experi-ments, the lexicon plays no part.
The POS tag causesone underspecified lexical item (per POS tag) to beconsidered in parsing.
While these underspecifieditems may allow more analyses to be built than ifthe exact category was used, the main contributionto parsing time turned out to be the number of tagsassigned to each word, whether that was a POS tagor a lexical type.
The POS tagger assigned multipletags much less frequently than the multiple tagginglexical type tagger and so had a faster average pars-ing time.
The single tagging lexical type tagger hadonly slightly fewer tags assigned overall, and hencewas slightly faster, but at the expense of a signifi-cantly lower coverage.5 ConclusionThe work reported here shows the benefits that canbe gained by utilising external resources to anno-tate parser input in highly lexicalised grammar for-malisms.
Even something as simple and readilyavailable (for languages likely to have lexicalisedgrammars) as a POS tagger can massively increasethe parser coverage on unseen text.
While annotat-ing with named entity data or a lexical type supertag-ger were also found to increase coverage, the POStagger had the greatest effect with up to 45% cover-age increase on unseen text.In terms of efficiency, POS tags were also shownto speed up parsing by filtering unlikely lexiconitems, but better results were achieved in this caseby using a lexical type supertagger.
Again encour-aging the use of external resources, the supertaggingwas found to be much more effective when POS tags619were used to train the tagging model, and in this con-figuration, managed to halve the parsing time withminimal effect on coverage or precision.6 Further WorkA number of avenues of future research were sug-gested by the observations made during this work.In terms of robustness and increasing lexical cover-age, more work into using lexical types for unknownwords could be explored.
In light of the encourag-ing results for German, one area to look at is the ef-fect of different features for different languages.
Useof back-off models might also be worth consideringwhen the tagger probabilities are low.Different methods of using the supertagger couldalso be explored.
The experiment reported here usedthe single most probable type for restricting the lex-icon entries used by the parser.
Two extensions ofthis are obvious.
The first is to use multiple tagsover a certain threshold, by either inputting multi-ple types as was done for the unknown word han-dling, or by using a generic type that is compatiblewith all the predicted types over a certain threshold.The other possible direction to try is to not checkthe predicted type against the lexicon, but to simplyconstruct a lexical item from the most likely type,given a (high) threshold probability.
This would besimilar to the CCG supertagging mechanism and islikely to give generous speedups at the possible ex-pense of precision, but it would be illuminating todiscover how this trade-off plays out in our setup.ReferencesTimothy Baldwin, Emily M. Bender, Dan Flickinger, AraKim, and Stephan Oepen.
2004.
Road-testing theEnglish Resource Grammar over the British NationalCorpus.
In Proceedings of the Fourth InternationalConference on Language Resources and Evaluation(LREC 2004), pages 2047?50, Lisbon, Portugal.Timothy Baldwin.
2005.
Bootstrapping deep lexical re-sources: Resources for courses.
In Proceedings of theACL-SIGLEX 2005 Workshop on Deep Lexical Acqui-sition, pages 67?76, Ann Arbor, USA.Srinivas Bangalore and Aravind K. Joshi.
1994.
Dis-ambiguation of super parts of speech (or supertags):Almost parsing.
In Proceedings of the 15th COLINGConference, pages 154?160, Kyoto, Japan.Markus Becker, Witold Drozdzynski, Hans-UlrichKrieger, Jakub Piskorski, Ulrich Scha?fer, and FeiyuXu.
2002.
SProUT - Shallow Processing with TypedFeature Structures and Unification.
In Proceedings ofthe International Conference on NLP (ICON 2002),Mumbai, India.Ted Briscoe and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalised statistical parser on thePARC DepBank.
In Proceedings of the 44th AnnualMeeting of the ACL, pages 41?48, Sydney, Australia.Ulrich Callmeier.
2000.
PET - a platform for experi-mentation with efficient HPSG processing techniques.Natural Language Engineering, 6(1):99?107.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark.
2002.
Supertagging for combinatory cat-egorical grammar.
In Proceedings of the 6th Interna-tional Workshop on Tree Adjoining Grammar and Re-lated Frameworks, pages 101?106, Venice, Italy.Ann Copestake and Dan Flickinger.
2000.
An open-source grammar development environment and broad-coverage English grammar using HPSG.
In Proceed-ings of the Second conference on Language Resourcesand Evaluation (LREC-2000), Athens, Greece.Ann Copestake, Alex Lascarides, and Dan Flickinger.2001.
An algebra for semantic construction inconstraint-based grammars.
In Proceedings of the39th Annual Meeting of the ACL and 10th Conferenceof the EACL (ACL-EACL 2001), Toulouse, France.Berthold Crysmann.
2003.
On the efficient implemen-tation of German verb placement in HPSG.
In Pro-ceedings of RANLP 2003, pages 112?116, Borovets,Bulgaria.Julia Hockenmaier, Gann Bierner, and Jason Baldridge.2002.
Extending the coverage of a CCG system.
Re-search in Language and Computation.Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, andRob Malouf.
1999.
A bag of useful techniques for ef-ficient and robust parsing.
In Proceedings of the 37thAnnual Meeting of the ACL, pages 473?480, Mary-land, USA.Stefan Mu?ller and Walter Kasper.
2000.
HPSG analysisof German.
In Verbmobil: Foundations of Speech-to-Speech Translation, pages 238?253.
Springer, Berlin,Germany.Jeremy Nicholson, Valia Kordoni, Yi Zhang, TimothyBaldwin, and Rebecca Dridan.
2008.
Evaluating andextending the coverage of HPSG grammars.
In Pro-ceedings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC 2008), Mar-rakech, Morocco.620Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef vanGenabith, and Andy Way.
2005.
Large-scale induc-tion and evaluation of lexical resources from the Penn-II and Penn-III treebanks.
Computational Linguistics,31:pp 329?366.Stephan Oepen, Helge Dyvik, Jan Tore L?nning, ErikVelldal, Dorothee Beermann, John Carroll, DanFlickinger, Lars Hellan, Janne Bondi Johannessen,Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.2004.
Soma?
kapp-ete med trollet?
Towards MRS-based Norwegian?English machine translation.
InProceedings of the 10th International Conference onTheoretical and Methodological Issues in MachineTranslation, Baltimore, USA.Stephan Oepen.
2001.
[incr tsdb()] ?
competence andperformance laboratory.
User manual, ComputationalLinguistics, Saarland University, Saarbru?cken, Ger-many.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press,Chicago, USA.Robbert Prins and Gertjan van Noord.
2003.
Reinforcingparser preferences through tagging.
Traitement Au-tomatique des Langues, 44(3):121?139.Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii.
2007.HPSG parsing with shallow dependency constraints.In Proceedings of the 45th Annual Meeting of the ACL,pages 624?631, Prague, Czech Republic.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of Interna-tional Conference on New Methods in Language Pro-cessing, Manchester, UK.Gertjan van Noord and Robert Malouf.
2004.
Widecoverage parsing with stochastic attribute value gram-mars.
In IJCNLP-04 Workshop Beyond Shallow Anal-yses ?
Formalisms and statistical modelling for deepanalyses.Wolfgang Wahlster, editor.
2000.
Verbmobil: Foun-dations of Speech-to-Speech Translation.
Springer-Verlag, Berlin.Yi Zhang and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open texts processing.In Proceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC 2006),pages 275?280, Genoa, Italy.621
