Proceedings of the 12th European Workshop on Natural Language Generation, pages 174?182,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsThe TUNA-REG Challenge 2009: Overview and Evaluation ResultsAlbert GattComputing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKa.gatt@abdn.ac.ukAnja Belz Eric KowNatural Language Technology GroupUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukAbstractThe TUNA-REG?09 Challenge was oneof the shared-task evaluation competitionsat Generation Challenges 2009.
TUNA-REG?09 used data from the TUNA Cor-pus of paired representations of enti-ties and human-authored referring expres-sions.
The shared task was to create sys-tems that generate referring expressionsfor entities given representations of setsof entities and their properties.
Fourteams submitted six systems to TUNA-REG?09.
We evaluated the six systems andtwo sets of human-authored referring ex-pressions using several automatic intrinsicmeasures, a human-assessed intrinsic eval-uation and a human task performance ex-periment.
This report describes the TUNA-REG task and the evaluation methods used,and presents the evaluation results.1 IntroductionThis year?s run of the TUNA-REG Shared-TaskEvaluation Competition (STEC) is the third, andfinal, competition to involve the TUNA Corpus ofreferring expressions.
The TUNA Corpus was firstused in the Pilot Attribute Selection for Gener-ating Referring Expressions (ASGRE) Challenge(Belz and Gatt, 2007) which took place betweenMay and September 2007; and again for three ofthe shared tasks in Referring Expression Genera-tion (REG) Challenges 2008, which ran betweenSeptember 2007 and May 2008 (Gatt et al, 2008).This year?s TUNA Task replicates one of the threetasks from REG?08, the TUNA-REG Task.
It usesthe same test data, to enable direct comparisonagainst the 2008 results.
Four participating teamssubmitted 6 different systems this year; teams andtheir affiliations are shown in Table 1.Team ID AffiliationGRAPH Macquarie, Tilburg and Twente UniversitiesIS ICSI, University of CaliforniaNIL-UCM Universidad Complutense de MadridUSP University of Sa?o PaoloTable 1: TUNA-REG?09 Participants.2 DataEach file in the TUNA corpus1 consists of a sin-gle pairing of a domain (a representation of 7 en-tities and their attributes) and a human-authoreddescription for one of the entities (the target refer-ent).
Some domains represent sets of people, somerepresent items of furniture (see also Table 2).
Thedescriptions were collected in an online elicita-tion experiment which was advertised mainly ona website hosted at the University of Zurich WebExperimentation List2 (a web service for recruit-ing subjects for experiments), and in which partic-ipation was not controlled or monitored.
In theexperiment, participants were shown pictures ofthe entities in the given domain and were asked totype a description of the target referent (which washighlighted in the visual display).
The main condi-tion3 manipulated in the experiment was +/?LOC:in the +LOC condition, participants were told thatthey could refer to entities using any of their prop-erties (including their location on the screen).
Inthe ?LOC condition, they were discouraged fromdoing so, though not prevented.The XML format we have been using in theTUNA-REG STECs, shown in Figure 1, is a vari-ant of the original format of the TUNA corpus.The root TRIAL node has a unique ID and anindication of the +/ ?
LOC experimental condi-1http://www.csd.abdn.ac.uk/research/tuna/2http://genpsylab-wexlist.unizh.ch3The elicitation experiment had an additional independentvariable, manipulating whether descriptions were elicited in a?fault-critical?
or ?non-fault-critical?
condition.
For the sharedtasks this was ignored by collapsing all the data in these twoconditions.174tion.
The DOMAIN node contains 7 ENTITY nodes,which themselves contain a number of ATTRIBUTEnodes defining the possible properties of an en-tity in attribute-value notation.
The attributes in-clude properties such as an object?s colour ora person?s clothing, and the location of the im-age in the visual display which the DOMAIN rep-resents.
Each ENTITY node indicates whether itis the target referent or one of the six distrac-tors, and also has a pointer to the image that itrepresents.
The WORD-STRING is the actual de-scription typed by one of the human authors, theANNOTATED-WORD-STRING is the description withsubstrings annotated with the attributes they re-alise, while the ATTRIBUTE-SET contains the set ofattributes only.
The ANNOTATED-WORD-STRING andATTRIBUTE-SET nodes were provided in the train-ing and development data only, to show how sub-strings of a human-authored description mappedto attributes.<TRIAL CONDITION="+/-LOC" ID="..."><DOMAIN><ENTITY ID="..." TYPE="target" IMAGE="..."><ATTRIBUTE NAME="..." VALUE="..." />...</ENTITY><ENTITY ID="..." TYPE="distractor" IMAGE="..."><ATTRIBUTE NAME="..." VALUE="..." />...</ENTITY>...</DOMAIN><WORD-STRING>string describing the target referent</WORD-STRING><ANNOTATED-WORD-STRING>string in WORD-STRING annotatedwith attributes in ATTRIBUTE-SET</ANNOTATED-WORD-STRING><ATTRIBUTE-SET>set of domain attributes in the description</ATTRIBUTE-SET></TRIAL>Figure 1: XML format of corpus items.Apart from differences in the XML format, thedata used in the TUNA-REG Task also differs fromthe original TUNA corpus in that it has only the sin-gular referring expressions from the original cor-pus, and in that we have added to it the files ofimages of entities that the XML mark-up points to.The test set, which was constructed for the2008 run of the TUNA-REG Task, consists of 112items, each with a different domain paired withtwo human-authored descriptions.
The items aredistributed equally between furniture items andpeople, and between both experimental conditions(+/ ?
LOC).
In the following sections, the twosets of human descriptions will be referred to asHUMAN-1 and HUMAN-2.4 The numbers of filesin the training, development and test sets, as wellas in the people and furniture subdomains, areshown in Table 2.Furniture People AllTraining 319 274 593Development 80 68 148Test 56 56 112All 455 398 853Table 2: TUNA-REG data: subset sizes.3 The TUNA-REG TaskReferring Expression Generation (REG) has beenthe subject of intensive research in the NLG com-munity, giving rise to substantial consensus on theproblem definition, as well as the nature of the in-puts and outputs of REG algorithms.
Typically,such algorithms take as input a domain, consist-ing of entities and their attributes, together with anindication of which is the intended referent, andoutput a set of attributes true of the referent whichdistinguish it from other entities in the domain.The TUNA-REG task adds an additional stage (re-alisation) in which selected attributes are mappedto a natural language expression (usually a nounphrase).
Realisation has received far less attentionamong REG researchers than attribute selection.The TUNA-REG task is an ?end-to-end?
refer-ring expression generation task, in the sense thatit takes as input a representation of a set of enti-ties and their properties, and outputs a word stringwhich describes the target entity.
Participatingsystems were not constrained to have attribute se-lection as a separate module from realisation.In terms of the XML format, the items inthe test set distributed to participants consistedof a DOMAIN node and ATTRIBUTE-SET, and par-ticipating systems had to generate appropriateWORD-STRINGs.As with previous STECs involving the TUNAdata, we deliberately refrained from including inthe task definition any aim that would imply as-sumptions about quality (as would be the case ifwe had asked participants to aim to produce, say,minimal or uniquely distinguishing referring ex-pressions), and instead we simply listed the evalu-ation criteria that were going to be used (describedin Section 5).4Descriptions in each set are not all by the same author.175Evaluation criterion Type of evaluation Evaluation techniqueHumanlikeness Intrinsic/automatic Accuracy, String-edit distance, BLEU-3, NISTAdequacy/clarity Intrinsic/human Judgment of adequacy as rated by native speakersFluency Intrinsic/human Judgment of fluency as rated by native speakersReferential clarity Extrinsic/human Speed and accuracy in identification experimentTable 3: Overview of evaluation methods.4 Participating Teams and SystemsThis section briefly describes this year?s submis-sions.
Full descriptions of participating systemscan be found in the participants?
reports includedin this volume.IS: The submission of the IS team, IS-FP-GT, isbased on the idea that different writers use differ-ent styles of referring expressions, and that, there-fore, knowing the identity of the writer helps gen-erate REs similar to those in the corpus.
Theattribute-selection algorithm is an extended full-brevity algorithm which uses a nearest neighbourtechnique to select the attribute set (AS) most sim-ilar to a given writer?s previous ASs, or, in a casewhere no ASs by the given writer have previouslybeen seen, to select the AS that has the highest de-gree of similarity with all previously seen ASs byany writer.
If multiple ASs remain, the algorithmfirst selects the shortest, then the most represen-tative of the remaining REs, then the AS with thehighest-frequency attributes.
Individualised statis-tical models are used to convert the selected ASinto a surface-syntactic dependency tree which isthen converted to a word stirng with an existingrealiser.GRAPH: The GRAPH team reused their existinggraph-based attribute selection component, whichrepresents a domain as a weighted graph, and usesa cost function for attributes.
The team devel-oped a new realiser which uses a set of templatesderived from the descriptions in the TUNA cor-pus.
In order to build templates, certain subsetsof attributes were grouped together, individual at-tributes were replaced by their type, and a pre-ferred order for attributes was determined basedon frequencies of orderings.
During realisation,if a matching template exists, types are replacedwith the most frequent word string for each givenattribute; if no match exists, realisation is done bya simple rule-based method.NIL-UCM: The three systems submitted by thisgroup use a standard evolutionary algorithm forattribute selection where genotypes consist ofbinary-valued genes each representing the pres-ence or absence of a given attribute.
Realisationis done with a case-based reasoning (CBR) methodwhich retrieves the most similar previously seenASs for an input AS, in order of their similarityto the input.
(Sub)strings are then copied fromthe preferred retrieved case to create the outputword string.
One system, NIL-UCM-EvoCBR usesboth components as described above.
The othertwo systems, NIL-UCM-ValuesCBR and NIL-UCM-EvoTAP, replace one of the components with theteam?s corresponding component from REG?08.USP: The system submitted by this group, USP-EACH, is a frequency-based greedy attribute se-lection strategy which takes into account the +/?LOC attribute in the TUNA data.
Realisation wasdone using the surface realiser supplied to partici-pants in the ASGRE?07 Challenge.5 Evaluation Methods and ResultsWe used a range of different evaluation methods,including intrinsic and extrinsic,5 automaticallycomputed and human-evaluated, as shown in theoverview in Table 3.
Participants computed auto-matic intrinsic evaluation scores on the develop-ment set (using the teval program provided byus).
We performed all of the evaluations shownin Table 3 on the test data set.
For all measures,results were computed both (a) overall, using theentire test data set, and (b) by entity type, that is,computing separate values for outputs in the furni-ture and in the people domain.
Evaluation meth-ods for each evaluation type and correspondingevaluation results are presented in the followingthree sections.5.1 Automatic intrinsic evaluationsHumanlikeness, by which we mean the similar-ity of system outputs to sets of human-producedreference ?outputs?, was assessed using Accuracy,5Intrinsic evaluations assess properties of peer systems intheir own right, whereas extrinsic evaluations assess the effectof a peer system on something that is external to it, such as itseffect on human performance at some given task or the addedvalue it brings to an application.176All development data People FurnitureAccuracy SE BLEU Accuracy SE BLEU Accuracy SE BLEUIS-FP-GT 9.71% 4.313 0.297 4.41% 4.764 0.2263 15% 3.863 0.3684GRAPH ?
5.03 0.30 ?
5.15 0.33 ?
4.94 0.27NIL-UCM-EvoTAP 6% 5.41 0.20 3% 6.04 0.15 8% 4.87 0.24NIL-UCM-ValuesCBR 1% 5.86 0.19 1% 5.80 0.17 1% 5.91 0.20USP-EACH ?
6.03 0.19 ?
7.50 0.04 ?
4.78 0.31NIL-UCM-EvoCBR 3% 6.31 0.17 1% 6.94 0.16 4% 5.77 0.18Table 4: Participating teams?
self-reported automatic intrinsic scores on development data set with singlehuman-authored reference description (listed in order of overall mean SE score).All test data People FurnitureAcc SE BLEU NIST Acc SE BLEU NIST Acc SE BLEU NISTGRAPH 12.50 6.41 0.47 2.57 8.93 7.04 0.43 2.16 16.07 5.79 0.51 2.26IS-FP-GT 3.57 6.74 0.28 0.75 3.57 7.04 0.37 0.94 3.57 6.45 0.13 0.36NIL-UCM-EvoTAP 6.25 7.28 0.26 0.90 3.57 8.07 0.20 0.45 8.93 6.48 0.34 1.22USP-EACH 7.14 7.59 0.27 1.33 0.00 9.04 0.11 0.46 14.29 6.14 0.41 2.28NIL-UCM-ValuesCBR 2.68 7.71 0.27 1.69 3.57 8.07 0.23 0.94 1.79 7.34 0.28 1.99NIL-UCM-EvoCBR 2.68 8.02 0.26 1.97 0.00 9.07 0.19 1.65 5.36 6.96 0.35 1.69HUMAN-2 2.68 9.68 0.12 1.78 3.57 10.64 0.12 1.50 1.79 8.71 0.13 1.57HUMAN-1 2.68 9.68 0.12 1.68 3.57 10.64 0.12 1.41 1.79 8.71 0.12 1.49Table 5: Automatic intrinsic scores on test data set with two human-authored reference descriptions(listed in order of overall mean SE score).string-edit distance, BLEU-3 and NIST-5.
Accu-racy measures the percentage of cases where asystem?s output word string was identical to thecorresponding description in the corpus.
String-edit distance (SE) is the classic Levenshtein dis-tance measure and computes the minimal numberof insertions, deletions and substitutions requiredto transform one string into another.
We set thecost for insertions and deletions to 1, and that forsubstitutions to 2.
If two strings are identical, thenthis metric returns 0 (perfect match).
Otherwisethe value depends on the length of the two strings(the maximum value is the sum of the lengths).
Asan aggregate measure, we compute the mean ofpairwise SE scores.BLEU-x is an n-gram based string comparisonmeasure, originally proposed by Papineni et al(2001; 2002) for evaluation of Machine Transla-tion systems.
It computes the proportion of wordn-grams of length x and less that a system out-put shares with several reference outputs.
Settingx = 4 (i.e.
considering all n-grams of length ?
4)is standard, but because many of the TUNA de-scriptions are shorter than 4 tokens, we computeBLEU-3 instead.
BLEU ranges from 0 to 1.NIST is a version of BLEU, but where BLEUgives equal weight to all n-grams, NIST gives moreimportance to less frequent n-grams, which aretaken to be more informative.
The maximum NISTscore depends on the size of the test set.Unlike string-edit distance, BLEU and NIST areby definition aggregate measures (i.e.
a singlescore is obtained for a peer system based on theentire set of items to be compared, and this is notgenerally equal to the average of scores for indi-vidual items).Because the test data has two human-authoredreference descriptions per domain, the Accuracyand SE scores had to be computed slightly differ-ently to obtain test data scores (whereas BLEU andNIST are designed for multiple reference texts).For the test data only, therefore, Accuracy ex-presses the percentage of a system?s outputs thatmatch at least one of the reference outputs, and SEis the average of the two pairwise scores againstthe reference outputs.Results: Table 4 is an overview of the self-reported scores on the development set included inthe participants?
reports (not all participants reportAccuracy scores).
The corresponding scores forthe test data set as well as NIST scores for the testdata (all computed by us), are shown in Table 5.The table also includes the result of comparingthe two sets of human descriptions, HUMAN-1 andHUMAN-2, to each other using the same metrics(their scores are distinct only for non-commutativemeasures, i.e.
NIST and BLEU).We ran6 a one-way ANOVA for the SE scores.6We used SPSS for all statistical analyses and tests.177There was a main effect of SYSTEM on SE (F =10.938, p < .001).
A post-hoc Tukey HSD testwith ?
= .05 revealed a number of significant dif-ferences: all systems were significantly better thanthe human-authored descriptions, and GRAPH wasfurthermore significantly better than NIL-UCM-EvoCBR.We also computed the Kruskal-Wallis H valuefor the systems?
individual Accuracy scores, usinga chi square test to establish significance.
By thistest, the observed aggregate difference among theseven systems is significant at the .01 level (?27 =20.169).5.2 Human intrinsic evaluationThe TUNA?09 Challenge was the first TUNAshared-task competition to include an intrinsicevaluation involving human judgments of quality.Design: The intrinsic human evaluation in-volved descriptions for all 112 test data items fromall six submitted systems, as well as from the twosets of human-authored descriptions.7 Thus, eachof the 112 test set items was associated with 8different descriptions.
We used a Repeated LatinSquares design which ensures that each subjectsees descriptions from each system and for eachdomain the same number of times.
There werefourteen 8 ?
8 squares, and a total of 896 indi-vidual judgments in this evaluation, each systemreceiving 112 judgments (14 from each subject).Procedure: In each of the 112 trials, par-ticipants were shown a system output (i.e.
aWORD-STRING), together with its correspondingdomain, displayed as the set of corresponding im-ages on the screen.8 The intended (target) referentwas highlighted by a red frame surrounding it onthe screen.
They were asked to give two ratingsin answer to the following questions (the first forAdequacy, the second for Fluency):1.
How clear is this description?
Try to imaginesomeone who could see the same grid withthe same pictures, but didn?t know which ofthe pictures was the target.
How easily wouldthey be able to find it, based on the phrasegiven?7Note that we refer to all outputs, whether human orsystem-generated, as system outputs in what follows.8The on-screen display of images was very similar, al-though not identical, to that in the original TUNA elicitationexperiments.2.
How fluent is this description?
Here yourtask is to judge how well the phrase reads.Is it good, clear English?We did not use a rating scale (where integerscorrespond to different assessments of quality),because it is not generally considered appropriateto apply parametric methods of analysis to ordinaldata.
Instead, we asked subjects to give their judg-ments for Adequacy and Fluency for each item bymanipulating a slider like this:The slider pointer was placed in the center at thebeginning of each trial, as shown above.
The posi-tion of the slider selected by the subject mapped toan integer value between 1 and 100.
However, thescale was not visible to participants, whose taskwas to move the pointer to the left or right.
Thefurther to the right, the more positive the judgment(and the higher the value returned); the further tothe left, the more negative.Following instructions, subjects did two prac-tice examples, followed by the 112 test items inrandom order.
Subjects carried out the experi-ment over the internet, at a time and place of theirchoosing, and were allowed to interrupt and re-sume the experiment.
According to self-reportedtimings, subjects took between 25 and 60 minutesto complete the experiment (not counting breaks).Participants: We recruited eight native speak-ers of English from among post-graduate studentscurrently doing a Masters degree in a linguistics-related subject.9We recorded subjects?
gender, level of educa-tion, field of study, proficiency in English, vari-ety of English and colour vision.
Since all sub-jects were native English speakers, had normalcolour vision, and had comparable levels of ed-ucation and academic backgrounds, as indicatedabove, these variables are not included in the anal-yses reported below.Results: Table 6 displays the mean Fluency andAdequacy judgments obtained by each system.We conducted two separate 8 (SYSTEM) ?
2 (DO-MAIN) Univariate Analyses of Variance (ANOVAs)on Adequacy and Fluency, where DOMAIN ranges9MA Linguistics and MRes Speech, Language and Cog-nition at UCL; MA Applied Linguistics and MRes Psychol-ogy at Sussex; and MA Media-assisted Language Teachingat Brighton.178All test data People FurnitureAdequacy Fluency Adequacy Fluency Adequacy FluencyMean SD Mean SD Mean SD Mean SD Mean SD Mean SDGRAPH 84.11 21.07 85.81 17.52 85.30 18.10 87.70 14.42 82.91 23.78 83.93 20.11USP-EACH 77.72 28.33 84.20 20.27 81.04 26.48 81.82 24.47 74.41 29.93 86.57 14.79NIL-UCM-EvoTAP 76.16 28.34 61.95 26.13 78.66 27.48 59.13 29.78 73.66 29.22 64.77 21.79HUMAN-2 74.63 34.77 73.38 27.63 80.93 31.83 73.16 30.88 68.34 36.68 73.59 24.23NIL-UCM-ValuesCBR 72.34 33.93 59.41 33.94 68.18 37.37 46.23 34.92 76.50 29.86 72.59 27.43HUMAN-1 70.38 34.92 71.52 30.79 83.39 24.27 72.39 28.55 57.36 39.08 70.64 33.13NIL-UCM-EvoCBR 63.65 37.19 55.38 35.32 56.61 40.20 41.45 37.38 70.70 32.76 69.30 26.93IS-FP-GT 59.46 40.94 66.21 30.97 88.79 19.26 65.27 32.22 30.14 35.51 67.16 29.94Table 6: Human-assessed intrinsic scores on test data set, including the two sets of human-authoredreference descriptions (listed in order of overall mean Adequacy score).Adequacy FluencyGRAPH A GRAPH AUSP-EACH A B USP-EACH A BNIL-UCM-EvoTAP A B HUMAN-2 B CHUMAN-2 A B C HUMAN-1 C DNIL-UCM-ValuesCBR A B C IS-FP-GT C D EHUMAN-1 B C D NIL-UCM-EvoTAP D ENIL-UCM-EvoCBR C D NIL-UCM-ValuesCBR EIS-FP-GT D NIL-UCM-EvoCBR ETable 7: Homogeneous subsets for Adequacy and Fluency.
Systems which do not share a letter aresignificantly different at ?
= .05.over People and Furniture Items.
On Adequacy,there were main effects of SYSTEM (F (7, 880) =7.291, p < .001) and DOMAIN (F (1, 880) =29.133, p < .001), with a significant interac-tion between the two (F (7, 880) = 15.30, p <.001).
On Fluency, there were main effects ofSYSTEM (F (7, 880) = 18.14) and of DOMAIN(F (7, 880) = 17.20), again with a significantSYSTEM ?
DOMAIN interaction (F (7, 880) =5.60), all significant at p < .001.
Post-hoc Tukeycomparisons on both dependent measures yieldedthe homogeneous subsets displayed in Table 7.5.3 Extrinsic task-performance evaluationAs for earlier shared tasks involving the TUNAdata, we carried out a task-performance experi-ment in which subjects have the task of identifyingintended referents.Design: The extrinsic human evaluation in-volved descriptions for all 112 test data items fromall six submitted systems, as well as from the twosets of human-authored descriptions.
We used aRepeated Latin Squares design with fourteen 8?8squares, so again there were a total of 896 individ-ual judgments and each system received 112 judg-ments, however this time it was 7 from each sub-ject, as there were 16 participants; so half the par-ticipants did the first 56 items (the first 7 squares),and the other half the second 56 (the remaining 7squares).Procedure: In each of their 5 practice trials and56 real trials, participants were shown a systemoutput (i.e.
a WORD-STRING), together with its cor-responding domain, displayed as the set of corre-sponding images on the screen.
In this experimentthe intended referent was not highlighted in the on-screen display, and the participants?
task was toidentify the intended referent among the picturesby mouse-clicking on it.10In previous TUNA identification experiments(Belz and Gatt, 2007; Gatt et al, 2008), sub-jects had to read the description before identify-ing the intended referent.
In ASGRE?07 both de-scription and pictures were displayed at the sametime, yielding a single time measure that com-bined reading and identification times.
In REG?08,subjects first read the description and then calledup the pictures on the screen when they had fin-ished reading the description, which yielded sepa-rate reading and identification times.10Due to limitations related to the stimulus presentationsoftware, the images in this experiment were displayed instrict rows and columns, whereas the display grid in the web-based TUNA elicitation experiment and the intrinsic humanevalution experiment were slightly distorted.
This may haveaffected timings in those (very rare) cases where a descriptionexplicitly referenced the column a target referent was locatedin, as in the chair in column 1.179This year we tried out a version of the experi-ment where subjects listened to descriptions readout by a synthetic voice11 over headphones whilelooking at the pictures displayed on the screen.Stimulus presentation was carried out usingDMDX, a Win-32 software package for psycholin-guistic experiments involving time measurements(Forster and Forster, 2003).
Participants initiatedeach trial, which consisted of an initial warningbell and a fixation point flashed on the screen for1000ms.
Following this, the visual domain wasdisplayed, and the voice reading the descriptionwas initiated after a delay of 500ms.
We recordedtime in milliseconds from the start of display to themouse-click whereby a participant identified thetarget referent.
This is hereafter referred to as theidentification speed.
The analysis reported belowalso uses identification accuracy, the percentageof correctly identified target referents, as an addi-tional dependent variable.
Trials timed out after15, 000ms.Participants: The experiment was carried outby 16 participants recruited from among the fac-ulty and administrative staff of the University ofBrighton.
All participants carried out the experi-ment under supervision in the same quiet room onthe same laptop, in the same ambient conditions,with no interruptions.
All participants were nativespeakers, and we recorded type of post, whetherthey had normal colour vision and hearing, andwhether they were left or right-handed.Timeouts and outliers: None of the trialsreached time-out stage during the experiment.Outliers were defined as those identification timeswhich fell outside the mean ?2SD (standard de-viation) range.
44 data points (4.9%) out of a to-tal of 896 were identified as outliers by this defi-nition; these were replaced with the series mean(Ratliff, 1993).
The results reported for identi-fication speed below are based on these adjustedtimes.Results: Table 8 displays mean identificationspeed and identification accuracy per system.
Aunivariate ANOVA on identification speed revealedsignificant main effects of SYSTEM (F (7, 880) =4.04, p < .001) and DOMAIN (F (1, 880) =11We used the University of Edinburgh?s Festival speechgeneration system (Black et al, 1999) in combinationwith the nitech us slt arctic hts voice, a high-quality femaleAmerican voice.USP-EACH AGRAPH ANIL-UCM-EvoTAP A BIS-FP-GT A BNIL-UCM-ValuesCBR A BNIL-UCM-EvoCBR A BHUMAN-2 BHUMAN-1 BTable 9: Homogeneous subsets for IdentificationSpeed.
Systems which do not share a letter aresignificantly different at ?
= .05.11.53, p < .001), with a significant interaction(F (7, 880) = 6.02, p < .001).
Table 9 displayshomogeneous subsets obtained following pairwisecomparisons using a post-hoc Tukey HSD analysis.We treated identification accuracy as an indica-tor variable (indicating whether a participant cor-rectly identified a target referent or not in a giventrial).
A Kruskal-Wallis test showed a significantdifference between systems (?27 = 44.98; p <.001).5.4 CorrelationsTable 10 displays the correlations between theeight evaluation measures we used.
The num-bers are Pearson product-moment correlation co-efficients, calculated on the means (1 mean persystem on each measure).As regards the human-assessed intrinsic scores,there is no significant correlation between Ad-equacy and Fluency.
Among the automaticallycomputed intrinsic measures, the only significantcorrelation is between Accuracy and BLEU.
Forthe extrinsic identification performance measures,there is no significant correlation between Identi-fication Accuracy and Identification Speed.As for correlations across the two types(human-assessed and automatically computed) ofintrinsic measures, the only significant correla-tions are between Fluency and Accuracy, and be-tween Adequacy and Accuracy.
So, a systemwith a higher percentage of human-like outputs(as measured by Accurach) also tends to be scoredmore highly in terms of Fluency and Adequacy byhumans.We also found significant correlations betweenintrinsic and extrinsic measures: there was astrong and significant correlation between Iden-tification Accuracy and Adequacy, implying thatmore adequate system outputs allowed people toidentify target referents more correctly; there wasalso a significant (negative) correlation between180All test data People FurnitureID acc.
ID.
speed ID acc.
ID.
speed ID acc.
ID.
speed% Mean SD % Mean SD % Mean SDGRAPH 0.96 3069.16 878.89 0.95 3081.01 767.62 0.96 3057.31 984.60HUMAN-1 0.91 3517.58 1028.83 0.95 3323.76 764.59 0.88 3711.41 1214.55USP-EACH 0.90 3067.16 821.00 0.86 3262.79 865.61 0.95 2871.53 730.15NIL-UCM-EvoTAP 0.88 3159.41 910.65 0.88 3375.17 948.46 0.89 2943.65 824.17NIL-UCM-ValuesCBR 0.87 3262.53 974.55 0.80 3447.50 1003.21 0.93 3077.56 916.87HUMAN-2 0.83 3463.88 1001.29 0.89 3647.41 1045.95 0.77 3280.35 927.79NIL-UCM-EvoCBR 0.81 3362.22 892.45 0.75 3779.64 831.91 0.88 2944.80 748.69IS-FP-GT 0.68 3167.11 964.45 0.89 2980.30 750.78 0.46 3353.91 1114.68Table 8: Identification speed and accuracy per system.
Systems are displayed in descending order ofoverall identification accuracy.Human-assessed, intrinsic Extrinsic Auto-assessed, intrinsicFluency Adequacy ID Acc.
ID Speed Acc.
SE BLEU NISTFluency 1 0.68 0.50 -0.89* .85* -0.57 0.66 0.30Adequacy 0.68 1 0.95** -0.65 .83* -0.29 0.60 0.48Identification Accuracy 0.50 0.95** 1 -0.39 0.68 -0.01 0.49 0.60Identification Speed 0.89* -0.65 -0.39 1 -0.79 0.68 -0.51 0.06Accuracy 0.85* 0.83* 0.68 -0.79 1.00 -0.68 .859* 0.49SE -0.57 -0.29 -0.01 0.68 -0.68 1 -0.75 -0.07BLEU 0.66 0.60 0.49 -0.51 .86* -0.75 1 0.71NIST 0.30 0.48 0.60 0.06 0.49 -0.07 0.71 1Table 10: Correlations (Pearson?s r) between all evaluation measures.
(?significant at p ?
.05;?
?significant at p ?
.01)Fluency and Identification Speed, implying thatmore fluent descriptions led to faster identifica-tion.
While these results differ from previous find-ings (Belz and Gatt, 2008), in which no significantcorrelations were found between extrinsic mea-sures and automatic intrinsic metrics, it is worthnoting that significance in the results reported herewas only observed between human-assessed in-trinsic measures and the extrinsic ones.6 Concluding RemarksThe three editions of the TUNA STEC have at-tracted a substantial amount of interest.
In addi-tion to a sizeable body of new work on referringexpression generation, as another tangible out-come of these STECs we now have a wide rangeof different sets of system outputs for the same setof inputs.
A particularly valuable resource is thepairing of these outputs from the submitted sys-tems in each edition with evaluation data.As this was the last time we are running a STECwith the TUNA data, we will now make all datasets, documentation and evaluation software fromall TUNA STECs available to researchers.
We areplanning to add to these as many system outputsas we can, so that other researchers can performevaluations involving these.We are also planning to complete our evalua-tions of the evaluation methods we have devel-oped.
Among such experiments will be directcomparisons between the results of the three vari-ants of the identification experiment we have triedout, and a direct comparison between differentdesigns for human-assessed intrinsic evaluations(e.g.
comparing the slider design reported here topreference judgments and rating scales).Apart from the technological progress in REGwhich we hope the TUNA STECs have helpedachieve, perhaps the single most important scien-tific result is strong evidence for the importanceof extrinsic evaluations, as these do not necessar-ily agree with the results of much more commonlyused intrinsic types of evaluations.AcknowledgmentsWe thank our colleagues at the University ofBrighton who participated in the identification ex-periment, and the Masters students at UCL, Sus-sex and Brighton who participated in the qual-ity assessment experiment.
The evaluations werefunded by EPSRC (UK) grant EP/G03995X/1.ReferencesA.
Belz and A. Gatt.
2007.
The attribute selection forgre challenge: Overview and evaluation results.
In181Proceedings of UCNLG+MT: Language Generationand Machine Translation.A.
Belz and A. Gatt.
2008.
Intrinsic vs. extrinsic eval-uation measures for referring expression generation.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL?08),pages 197?200.A.
Black, P. Taylor, and R. Caley, 1999.
The Festi-val Speech Synthesis System: System Documenta-tion.
University of Edinburgh, 1.4 edition.K.
I. Forster and J. C. Forster.
2003.
DMDX: A win-dows display program with millisecond accuracy.Behavior Research Methods, Instruments, & Com-puters, 35(1):116?124.A.
Gatt, A. Belz, and Eric Kow.
2008.
The tuna chal-lenge 2008: Overview and evaluation results.
InProceedings of the 5th International Conference onNatural Language Generation (INLG?08).K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.BLEU: A method for automatic evaluation of ma-chine translation.
IBM research report, IBM Re-search Division.S.
Papineni, T. Roukos, W. Ward, and W. Zhu.
2002.Bleu: a. method for automatic evaluation of machinetranslation.
In Proc.
40th Annual Meeting of the As-sociation for Computational Linguistics (ACL-02),pages 311?318.R.
Ratliff.
1993.
Methods for dealing with reactiontime outliers.
Psychological Bulletin, 114(3):510?532.182
