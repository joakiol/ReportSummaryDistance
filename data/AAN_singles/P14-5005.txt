Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 25?30,Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational LinguisticsWINGS: Writing with Intelligent Guidance and SuggestionsXianjun Dai, Yuanchao Liu*, Xiaolong Wang, Bingquan LiuSchool of Computer Science and TechnologyHarbin Institute of Technology, China{xjdai, lyc, wangxl, liubq}@insun.hit.edu.cnAbstractWithout inspirations, writing may be afrustrating task for most people.
In this study,we designed and implemented WINGS, aChinese input method extended onIBus-Pinyin with intelligent writing assistance.In addition to supporting common Chineseinput, WINGS mainly attempts to spark users?inspirations by recommending both wordlevel and sentence level writing suggestions.The main strategies used by WINGS,including providing syntactically andsemantically related words based on wordvector representation and recommendingcontextually related sentences based on LDA,are discussed and described.
Experimentalresults suggest that WINGS can facilitateChinese writing in an effective and creativemanner.1 IntroductionWriting articles may be a challenging task, as weusually have trouble in finding the suitable wordsor suffer from lack of ideas.
Thus it may be veryhelpful if some writing reference information,e.g., words or sentences, can be recommendedwhile we are composing an article.On the one hand, for non-english users, e.g.,Chinese, the Chinese input method is our firsttool for interacting with a computer.
Nowadays,the most popular Chinese input methods arePinyin-based ones, such as Sougou Pinyin1 andGoogle Pinyin 2 .
These systems only presentaccurate results of Pinyin-to-Characterconversion.
Considering these systems?
lack ofsuggestions for related words, they hardlyprovide writers with substantial help in writing.On the other hand, try to meet the need of writingassistance, more and more systems facilitatingChinese writing have been available to the public,* Corresponding author1 http://pinyin.sogou.com2 http://www.google.com/intl/zh-CN/ime/pinyinsuch as WenXin Super Writing Assistant3 andBigWriter4, and among others.
However, due totheir shortcomings of building examples librarymanually and lack of corpus mining techniques,most of the time the suggestions made by thesesystems are not creative or contextual.Thus, in this paper, we present Writing withINtelligent Guidance and Suggestions (WINGS)5,a Chinese input method extended with intelligentwriting assistance.
Through WINGS, users canreceive intelligent, real-time writing suggestions,including both word level and sentence level.Different from existing Chinese writing assistants,WINGS mainly attempts to spark users?
writinginspirations from two aspects: providing diverserelated words to expand users?
minds andrecommending contextual sentences according totheir writing intentions.
Based on corpus miningwith Natural Language Processing techniques,e.g., word vector representation and LDA model,WINGS aims to facilitate Chinese writing in aneffective and creative manner.For example, when using WINGS to type?xuxurusheng?, a sequence of Chinese Pinyincharacters for ??????
(vivid/vividly), thePinyin-to-Character Module will generate ??????
and some other candidate Chinese words.Then the Words Recommending Modulegenerates word recommendations for ?????
?.
The recommended words are obtainedthrough calculating word similarities based onword vector representations as well as rule-basedstrategy (POS patterns).In the Sentences Recommending Module, wefirst use ?????
?
to retrieve examplesentences from sentences library.
Then the topicsimilarities between the local context and thecandidate sentences are evaluated for contextual3 http://www.xiesky.com4 http://www.zidongxiezuo.com/bigwriter_intro.php5 The DEB package for Ubuntu 64 and recorded video ofour system demonstration can be accessed at this URL:http://yunpan.cn/Qp4gM3HW446Rx (password:63b3)25Chinese Pinyin SequenceRecommended WordsRecommended SentencesPinyin-to-Character results (Original Words)Figure 1.
Screenshot of WINGS.sentence recommendations.At last in consideration of users?
feedback, weintroduce a User Feedback Module to our system.The recorded feedback data will in turn influencethe scores of words and sentences inRecommending Modules above.Figure 1 shows a screenshot of WINGS.2 Related Work2.1 Input MethodChinese input method is one of the mostimportant tools for Chinese PC users.
Nowadays,Pinyin-based input method is the most popularone.
The main strategy that Pinyin-based inputmethod uses is automatically converting Pinyinto Chinese characters (Chen and Lee, 2000).In recent years, more and more intelligentstrategies have been adopted by different inputmethods, such as Triivi 6 , an English inputmethod that attempts to increase writing speedby suggesting words and phrases, and PRIME(Komatsu et al., 2005), an English/Japaneseinput system that utilizes visited documents topredict the user?s next word to be input.In our system the basic process was Pinyin ?Characters (words) ?
Writing Suggestions(including words and sentences).
We mainlyfocused on writing suggestions from Characters(words) in this paper.
As the Pinyin-to-Characterwas the underlining work, we developed oursystem directly on the open source framework ofthe IBus (an intelligent input Bus for Linux andUnix OS) and IBus-Pinyin7 input method.2.2 Writing AssistantAs previously mentioned, several systems areavailable in supporting Chinese writing, such asWenXin Super Writing Assistant and Big Writer.6 http://baike.baidu.com/view/4849876.htm7 https://code.google.com/p/ibusThese systems are examples of a retrieval-basedwriting assistant, which is primarily based on alarge examples library and provides users with asearch function.In contrast, other writing assistants employspecial NLP strategies.
Liu et al.
(2011, 2012)proposed two computer writing assistants: onefor writing love letters and the other for blogwriting.
In these two systems, some specialtechniques were used, including text generation,synonym substitution, and concept expansion.PENS (Liu et al., 2000) and FLOW (Chen et al.,2012) are two writing assistants designed forstudents of English as a Foreign Language (EFL)practicing writing, which are mainly based onStatistical Machine Translation (SMT) strategies.Compared with the above mentioned systems,WINGS is closer to retrieval-based writingassistants in terms of function.
However, WINGScan provide more intelligent suggestions becauseof the introduction of NLP techniques, e.g., wordvector representation and topic model.2.3 Word Representations in Vector SpaceRecently, Mikolov et al.
(2013) proposed novelmodel architectures to compute continuousvector representations of words obtained fromvery large data sets.
The quality of theserepresentations was assessed through a wordsimilarity task, and according to their report, theword vectors provided state-of-the-artperformance for measuring syntactic andsemantic word similarities in their test set.
Theirresearch produced the open source toolword2vec8.In our system, we used word2vec to train theword vectors from a corpus we processedbeforehand.
For the Words RecommendingModule, these vectors were used to determine thesimilarity among different words.8 https://code.google.com/p/word2vec262.4 Latent Dirichlet AllocationThe topic model Latent Dirichlet Allocation(LDA) is a generative probabilistic model of acorpus.
In this model, documents are representedas random mixtures of latent topics, where eachtopic is characterized by the distribution ofwords (Blei et al., 2003).
Each document canthus be represented as a distribution of topics.Gibbs Sampling is a popular and efficientstrategy used for LDA parameter estimation andinference.
This technique is used inimplementing several open sourcing LDA tools,such as GibbsLDA++9 (Phan and Nguyen, 2007),which was used in this paper.In order to generate contextual sentencesuggestions, we ensured that the sentencesrecommended to the user were topic related tothe local context (5-10 words previously input)based on the LDA model.3 Overview of WINGSFigure 2 illustrates the overall architecture ofWINGS.StartPinyin to CharacterConvert pinyin to Chinese words(Original words)Words Recommending 11.
Calculate similarity between focusedoriginal word and the rest words in thedictionary2.
Get top 200 most similar words asthe candidate wordsWords and wordvectorsSentencesindexSentences Recommending 1Use the focused original orrecommended word to retrieve at most200 sentences by Clucene fromsentences index.S ntenc s andtheirtopic vectorSentences Recommending 21.
Infer the topic vector of the localcontext by Gibbs Sammpling.
Calculatethe KL divergence between the localcontext and candidate sentences.2.
The sentence has been used beforewill get a boost in score.1.
Select word or sentence as input2.
Save feedback(User Feedback)LDA train resultfor inferenceInput PinyinPinyin-Charactermapping data,etc.Words ands ntencesselected infoYESEndContinueNOWords Recommending 21.Boost in score: 1).Whether theoriginal and recommended wordmatch one of the specified patterns,such as A-N, V-N and etc.
2).
WhetherThe word has been used before2.
Re-rank candidate words.Figure 2.
Overall architecture of WINGS.3.1 System ArchitectureOur system is composed of four different9 http://gibbslda.sourceforge.netmodules: Pinyin-to-Character Module, WordsRecommending Module, SentencesRecommending Module, and User FeedbackModule.
The following sub-sections discussthese modules in detail.3.2 Pinyin-to-Character ModuleOur system is based on the open sourcing inputframework IBus and extended on theIBus-Pinyin input method.
Thus, thePinyin-to-Character module is adopted from theoriginal IBus-Pinyin system.
This moduleconverts the input Chinese Pinyin sequence intoa list of candidate Chinese words, which we referto as original words.3.3 Words Recommending Module?
Words vector representationsIn this preparatory step for wordrecommendation, words vector representationsare obtained using the word2vec tool.
This willbe described in detail in Section 4.?
Obtain the most related wordsOur system will obtain the focused originalword and calculate the cosine similaritiesbetween this word and the rest of the words inthe dictionary.
Thus, we can obtain the top 200most similar words according to their cosinevalues.
These words are referred to asrecommended words.
According to Mikolov etal.
(2013), these words are syntactically andsemantically similar to the original word.?
Re-rank the recommended wordsIn order to further improve word recommending,we introduce several special POS patterns (Table1).
If the POS of the original word and therecommended word satisfy one of the POSpatterns we specified, the score (based on thecosine similarity) of the recommended word willbe boosted.
In addition, the score of the wordselected by the user before will also be boosted.Therefore, these words will be ranked higher inthe recommended words list.POS oforiginal wordPOS ofrecommended wordN (noun) A (adjective)A (adjective) N (noun)N (noun) V (verb)Any POS Same with the original wordAny POS L (idiom)Table 1.
Special POS patterns.3.4 Sentences Recommending Module?
Sentences topic distributionIn this preparatory step for sentence27recommendation, sentences topic distributionvectors and other parameters are trained usingthe GibbsLDA++.
This step will be discussed inSection 4.?
Retrieve relative sentences via CLuceneThe focused original or recommended word willbe used to search the most related sentences inthe sentences index via CLucene10.
At most 200sentences will be taken as candidates, which willbe called recommended sentences.?
Re-rank the recommended sentencesTo ensure that the recommended sentences aretopic related to our local input context (5-10words previously input), we use Gibbs Samplingto infer the topic vector of the local context, andcalculate the KL divergence between the localcontext and each recommended sentence.
Finally,the recommended sentences will be re-rankedbased on their KL divergences value with respectto the local context and the boost score derivedfrom the feedback information.3.5 User Feedback ModuleThis module saves the users?
feedbackinformation, particularly the number of timeswhen users select the recommended words andsentences.
This information will be used as aboost factor for the Words and SentencesRecommending Modules.
Our reasons forintroducing this module are two-fold: the users?feedback reflects their preference, and at thesame time, this information can somewhatindicate the quality of the words and sentences.4 Data Pre-processingIn this section, the procedure of our datapre-processing is discussed in detail.
Firstly, ourraw corpus was crawled from DiYiFanWen11, aChinese writing website that includes all types ofwriting materials.
After extracting usefulcomposition examples from each raw html file,we merged all articles into a single file namedlarge corpus.
Finally, a total of 324,302 articleswere merged into the large corpus (with a totalsize of 320 MB).For words recommending, each of the articlesin our large corpus was segmented into words byICTCLAS 12  with POS tags.
Subsequently,word2vec tool was used on the words sequence(with useless symbols filtered).
Finally, thewords, their respective vector representations and10 http://sourceforge.net/projects/clucene11 http://www.diyifanwen.com12 http://ictclas.nlpir.orgmain POS tags were combined, and we builtthese data into one binary file.For sentences recommending, the large corpuswas segmented into sentences based on specialpunctuations.
Sentences that were either too longor too short were discarded.
Finally, 2,567,948sentences were left, which we called originalsentences.
An index was created on thesesentences using CLucene.
Moreover, wesegmented these original sentences and filteredthe punctuations and stop words.
Accordingly,these new sentences were named segmentedsentences.
We then ran GibbsLDA++ on thesegmented sentences, and the Gibbs samplingresult and topic vector of each sentence werethus obtained.
Finally, we built the originalsentence and their topic vectors into a binary file.The Gibbs sampling data used for inference waslikewise saved into a binary file.Table 2 lists all information on the resourcesof WINGS.Items InformationArticles corpus size 320 MBArticles total count 324,302Words total count 101,188Sentences total count 2,567,948Table 2.
Resources information.5 Experimental ResultsThis section discusses the experimental results ofWINGS.5.1 Words RecommendingThe top 20 recommended words for the sampleword ????
(teacher) are listed in Table 3.Compared with traditional methods (using Cilin,Hownet, and so forth.
), using the word vectors todetermine related words will identify morediverse and meaningful related words and thisquality of WINGS is shown in Table 4.
With thediversity of recommended words, writers?
mindscan be expanded easily.1-10: ??
(student), ??
(conduct class), ???
(Chinese class), ????
(with sincere wordsand earnest wishes), ????
(affability), ??
(guide), ??
(lecture), ??
(dais), ????
(patient), ??
(the whole class)11-20: ??
(finish class), ???
(remarks), ???
(math class), ???
(be absent-minded), ??
(ferule), ???
(class adviser), ????
(restless), ??
(remember), ????????
(excel one?s master), ??
(listen to)Table 3.
Top 20 recommended words for ????
(teacher).28Words about WordsPerson ?
?, ??
?, ?
?Quality ???
?, ???
?, ???
?Course ??
?, ??
?Teaching ?
?, ?
?, ?
?, ?
?Teaching facility ?
?, ?
?Student behaviour ?
?, ??
?, ???
?Special idiom ???????
?Others ?
?, ??
?Table 4.
Diversity of recommended words for????
(teacher).5.2 Sentences RecommendingBy introducing the topic model LDA, thesentences recommended by WINGS are related tothe topic of the local context.
Table 5 presentsthe top 5 recommended sentences for the word??????
(vivid/vividly) in two different localcontexts: one refers to characters in books; theother refers to statues and sculptures.
Mostsentences in the first group are related to the firstcontext, and most from the second group arerelated to the second context.In order to assess the performance of WINGSin sentence recommendation, the followingevaluation was implemented.
A total of 10Chinese words were randomly selected, and eachword was given two or three different localcontexts as above (contexts varied for differentwords).
Finally, we obtained a total of 24 groupsof data, each of which included an original word,a local context, and the top 10 sentencesrecommended by WINGS.
To avoid the influenceof personal preferences, 12 students were invitedto judge whether each sentence in the 24different groups was related to their respectivelocal context.
We believed that a sentence wasrelated to its context only when at least 70% ofthe evaluators agreed.
The Precision@10measure in Information Retrieval was used, andthe total average was 0.76, as shown in Table 6.Additionally, when we checked the sentenceswhich were judged not related to their respectivelocal context, we found that these sentences weregenerally too short after stop words removal, andas a result the topic distributions inferred fromGibbs Sampling were not that reliable.Context 1 is about characters in books:??
(story), ??
(character), ??
(image),??(works)1???????????????
(The characters of this book are depictedvividly)2????????????????????
(The characters of this book are depicted vividlyand the story is impressive narrative)3????????????
(The characters of this story are depictedvividly)4??????????????????????
(His works are full of plot twists, vividcharacters, and surprising endings)5??????????????????
(The characters in the book are depicted vividlyby Jing Zhuge)Context 2 is about statues and sculptures:??
(statue), ??
(sculpture), ??
(stoneinscription), ??(temple)1??????????????
(The walls are painted with mighty and vividdragons)2????????????????
(On both sides there are standing 18 vivid Arhatswith different manners)3?????????????????
(the Great Buddha Hall is grand and the statuesthere are vivid)4????????????
(Each statue is vivid and lifelike)5?????????????????????????
(On each of the eave angles there are 7 vividstatues of animals and birds with specialmeanings)Table 5.
Top 5 recommended sentences for ??????
(vivid/vividly) in two different localcontexts.LocalContextword1word2word3word4word5word6word7word8word9word101 0.9 0.3 0.9 0.6 0.7 0.8 0.6 0.8 1.0 0.92 0.4 0.7 1.0 0.9 0.9 0.7 1.0 0.5 0.9 0.53 0.9 N/A N/A N/A N/A 0.9 0.8 N/A N/A 0.7Average Precision@10 value of the 24 groups data                0.76Table 6.
Precision@10 value of each word under their respective context and the total average.295.3 Real Time PerformanceIn order to ensure the real time process for eachrecommendation, we used CLucene to index andretrieve sentences and memory cache strategy toreduce the time cost of fetching sentences?information.
Table 7 shows the average and maxresponding time of each recommendation ofrandomly selected 200 different words (Our testenvironment is 64-bit Ubuntu 12.04 LTS OS onPC with 4GB memory and 3.10GHz Dual-CoreCPU).Item Responding timeAverage 154 msMax 181 msTable 7.
The average and max responding timeof 200 different words?
recommending process6 Conclusion and Future WorkIn this paper, we presented WINGS, a Chineseinput method extended with writing assistancethat provides intelligent, real-time suggestionsfor writers.
Overall, our system providessyntactically and semantically related words, aswell as recommends contextually relatedsentences to users.
As for the large corpus, onwhich the recommended words and sentences arebased, and the corpus mining based on NLPtechniques (e.g., word vector representation andtopic model LDA), experimental results showthat our system is both helpful and meaningful.In addition, given that the writers?
feedback isrecorded, WINGS will become increasinglyeffective for users while in use.
Thus, we believethat WINGS will considerably benefit writers.In future work, we will conduct more userexperiments to understand the benefits of oursystem to their writing.
For example, we canintegrate WINGS into a crowdsourcing systemand analyze the improvement in our users?writing.
Moreover, our system may still beimproved further.
For example, we are interestedin adding a function similar to Google Suggest,which is based on the query log of the searchengine, in order to provide more valuablesuggestions for users.ReferencesDavid M. Blei, Andrew Y. Ng and Michael I. Jordan.2003.
Latent dirichlet allocation.
the Journal ofmachine Learning research, 3, pages 993-1022.Mei-Hua Chen, Shih-Ting Huang, Hung-Ting Hsieh,Ting-Hui Kao and Jason S. Chang.
2012.
FLOW: afirst-language-oriented writing assistant system.
InProceedings of the ACL 2012 SystemDemonstrations, pages 157-162.Zheng Chen and Kai-Fu Lee.
2000.
A new statisticalapproach to Chinese Pinyin input.
In Proceedingsof the 38th annual meeting on association forcomputational linguistics, pages 241-247.Hiroyuki Komatsu, Satoru Takabayash and ToshiyukiMasui.
2005.
Corpus-based predictive text input.
InProceedings of the 2005 international conferenceon active media technology, pages 75?80.Chien-Liang Liu, Chia-Hoang Lee, Ssu-Han Yu andChih-Wei Chen.
2011.
Computer assisted writingsystem.
Expert Systems with Applications, 38(1),pages 804-811.Chien-Liang Liu, Chia-Hoang Lee and Bo-Yuan Ding.2012.
Intelligent computer assisted blog writingsystem.
Expert Systems with Applications, 39(4),pages 4496-4504.Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun andChangning Huang.
2000.
PENS: A machine-aidedEnglish writing system for Chinese users.
InProceedings of the 38th Annual Meeting onAssociation for Computational Linguistics, pages529-536.Tomas Mikolov, Kai Chen, Greg Corrado and JeffreyDean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv:1301.3781.Xuan-Hieu Phan and Cam-Tu Nguyen.
2007.GibbsLDA++: A C/C++ implementation of latentDirichlet allocation (LDA).30
