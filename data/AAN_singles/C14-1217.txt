Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2303?2313, Dublin, Ireland, August 23-29 2014.Unsupervised Instance-Based Part of Speech InductionUsing Probable SubstitutesMehmet Ali Yatbaz Enis R?fat Sert Deniz YuretKoc?
University Artificial Intelligence Laboratory,?Istanbul{myatbaz,esert,dyuret}@ku.edu.trAbstractWe develop an instance (token) based extension of the state of the art word (type) based part-of-speech induction system introduced in (Yatbaz et al., 2012).
Each word instance is representedby a feature vector that combines information from the target word and probable substitutessampled from an n-gram model representing its context.
Modeling ambiguity using an instancebased model does not lead to significant gains in overall accuracy in part-of-speech tagging be-cause most words in running text are used in their most frequent class (e.g.
93.69% in thePenn Treebank).
However it is important to model ambiguity because most frequent words areambiguous and not modeling them correctly may negatively affect upstream tasks.
Our maincontribution is to show that an instance based model can achieve significantly higher accuracy onambiguous words at the cost of a slight degradation on unambiguous ones, maintaining a compa-rable overall accuracy.
On the Penn Treebank, the overall many-to-one accuracy of the system iswithin 1% of the state-of-the-art (80%), while on highly ambiguous words it is up to 70% better.On multilingual experiments our results are significantly better than or comparable to the bestpublished word or instance based systems on 15 out of 19 corpora in 15 languages.
The vectorrepresentations for words used in our system are available for download for further experiments.1 IntroductionUnsupervised part-of-speech (POS) induction aims to classify words into syntactic categories using un-labeled, plain text input.
The problem of induction is important for studying under-resourced languagesthat lack labeled corpora and high quality dictionaries.
It is also essential in modeling child languageacquisition because every child manages to induce syntactic categories without access to labeled sen-tences, labeled prototypes, or dictionary constraints (Ambridge and Lieven, 2011).
Categories inducedfrom data may point to shortcomings or inconsistencies of hand-labeled categories as discussed in Sec-tion 4.
Finally, the induced categories or the vector representations generated by the induction algorithmsmay improve natural language processing systems when used as additional features.Word-based POS induction systems classify different instances of a word in a single category (whichwe will refer to as the one-tag-per-word assumption).
Instance-based systems classify each occurence ofa word separately and can handle ambiguous words.Examples of word-based systems include ones that represent each word with the vector of neighboringwords (context vectors) and cluster them (Sch?utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), usea prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003),restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn,2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), orThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2303         !"
#$Figure 1: The accuracy comparison of word and instance based part-of-speech induction models as afunction of target word ambiguity (as measured by gold-standard-tag perplexity described in Section 3.3)on the Penn Treebank.construct word vector representations based on co-occurrences with contextual features (Yatbaz et al.,2012).The obvious limitation of the one-tag-per-word assumption is that instances of ambiguous words thathave more than one POS role are grouped into the same class.
For example, the word offer is tagged asNN(399), VB(105) and VBP(34)1in its 538 occurrences in the human labeled Wall Street Journal (WSJ)Section of the Penn Treebank (PTB) corpus (Marcus et al., 1999).
If all instances of offer are assignedto the most frequent tag NN, 36% (139/538) will be erroneously labeled.
In spite of this shortcoming,word-based POS induction systems generally do well because the one-tag-per-word assumption is mostlyaccurate: 93.69% of the word occurrences are tagged with their most frequent POS tag in the PTB(Toutanova et al., 2003).In order to handle ambiguous words, models without a strict one-tag-per-word assumption need togroup word instances into clusters according to their contexts.
Some of these instance-based models biaswords to have few tags using sparse priors in a Bayesian setting (Goldwater and Griffiths, 2007; John-son, 2007; Gao and Johnson, 2008), or posterior regularization (Ganchev et al., 2010).
Sch?utze (1993)represents the context of a word instance by concatenating context vectors of its left and right neigh-boring words, and clusters word instances.
Berg-Kirkpatrick et al.
(2010) use an EM algorithm wherethey replace the multinomial components with miniature logistic regressions and achieve the highestinstance-based accuracy on PTB.
Christodoulopoulos et al.
(2010) select prototypes of each clusterfrom the output of Brown (1992) and feed them to a HMM model that can handle prototypes as fea-tures (Haghighi and Klein, 2006).
However none of these models achieve results comparable to the bestword-based systems.In this work, we show that one can build an instance-based system that can perform significantlybetter on highly ambiguous words (see Figure 1) and yet is competitive with word-based systems inoverall accuracy.We follow the state of the art word-based system (Yatbaz et al., 2012) and use probable substitutes of aword instance as its contextual features.
The following examples illustrate how such paradigmatic (sub-stitute based) contextual features can capture the similarity between two contexts where a syntagmatic(neighbor based) representation would fail:1NN, VB and VBP are three POS tags from the Penn Treebank corpus and they correspond to singular noun, verb in baseform and non-3rdperson singular verb in present tense, respectively.
The numbers in parentheses are the frequencies.2304(1) ?Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov.
29.?director!
chairman (.8242), director (.0127), directors (.0127) .
.
.
(2) ?.
.
.
Joseph Corr was succeeded by Frank Lorenzo, chief of parent Texas Air.?chief!
chairman (.9945), president (.0031), directors (.0012) .
.
.Each sentence has the target words marked in bold (director and chief) and their likely substituteslisted with probabilities2in parentheses.
Note that the two contexts have no words in common, thereforesyntagmatic (neighbor based) contextual features will fail to capture their similarity.
However, paradig-matic features such as the top substitutes ?chairman?, ?directors?, etc.
clearly indicate the similarity andhelp place these two instances into the same category.Following (Globerson et al., 2007), we embed words and their contextual, orthographic, and morpho-logical features in a high dimensional Euclidean space that relates their joint probability to distance.
Incontrast to (Yatbaz et al., 2012) we build an instance-based POS induction system where each instancehas a vector representation that concatenates the word vector with the average of the contextual featurevectors.
We show that clustering of these instance vectors separate different roles of ambiguous wordswell, and achieve comparable or better performance than the best word-based systems in matching thegold tags on 19 corpora in 15 languages.
All the code that can be used to replicate our findings is availableat https://github.com/ai-ku/upos_2014.Section 2 describes the instance-based POS induction algorithm, Section 3 gives the results of ourexperiments, Section 4 compares the output of the induction system with the gold tags, and Section 5summarizes our contributions.2 AlgorithmIn this section, we describe the steps of our instance-based POS-induction algorithm:1.
Sample r substitutes for each word instance in the target corpus using an n-gram language model.2.
Construct r tuples for each instance where each tuple consists of a sampled substitute, the targetword, and the morphological and orthographical features of the target word (see Table 1).3.
Construct Euclidean embeddings of each word and each feature based on all tuples following Gle-berson et al.
(2007) and Maron et al.(2010).4.
Construct a vector representation for each instance by concatenating the embedding of the targetword with the average of its substitute embeddings.5.
Use k-means clustering to cluster the instance vectors where k is equal to the number of gold tags.Steps 1 and 2 construct a tuple representation for each instance.
Table 1 gives some example tuples forSentence (1) from the previous section.
In this example r = 3, so three substitutes are sampled for eachinstance as contextual features.
The sampling is with replacement from the substitute word distributionof a context given by the n-gram language model, so some substitute words may be repeated.
The targetword and its other features are identical for each of the r tuples representing a single instance.In step 3, we construct Euclidean embeddings for each unique word and feature value using the multi-variable version of the CODE algorithm described in (Globerson et al., 2007).
Given two categoricalvariables W and F , the CODE algorithm constructs Euclidean embeddings (vectors) for each of theirdistinct values in the same space.
The distance between the embedding of a w value,  (w), and theembedding of an f value,  (f), is related to their joint distribution p(w, f) as follows3p(w, f) =1Zp?(w)p?
(f)e d2w,f2Substitute probabilities are computed using a 4-gram language model.3(Globerson et al., 2007) describes several ways to relate distances to probabilities, the model used here is the marginal-marginal (MM) model.2305Word Subst Suf Cap NumVinken Makhlouf ?
T FVinken Makhlouf ?
T FVinken <unk> ?
T F61 20 ?
F T61 2000 ?
F T61 eleven ?
F Tyears years -s F Fyears years -s F Fyears years -s F FTable 1: The tuples constructed for the instances of ?Vinken?, ?61?
and ?years?
from Sentence (1).
Theelements of each tuple are the target word, sampled substitute, suffix, capitalization, and number features.where p?
represents empirical probabilities (frequencies from the training data), dw,fis the distance be-tween the embeddings  (w) and  (f) and Z =Pw,fp?(w)p?
(f)e d2w,fis a normalization constant.Starting with random vectors for each distinct value of w and f , we use stochastic gradient ascent tomove the embedding vectors around to maximize the likelihood given by this model.
Calculating thenormalization constant Z is the most expensive part of this procedure.
We solve this problem following(Maron et al., 2010) who suggest that a constant Z approximation can be used if the embedding vectorsare kept on the unit sphere.As Table 1 shows, considering the target word and its contextual, morphological and orthographicfeatures gives us more than two variables.
Yatbaz et al.
(2012) adopt the two variable CODE algorithmto this multi-variable case in an ad-hoc manner by considering the target word as w and all other featuresas distinct f values.
We implement the multi-variable extension of CODE suggested by (Globerson etal., 2007) (Section 6.2) which optimizes the following likelihood function:`( ,(1), .
.
.
,(K)) =KXi=1Xw,f(i)p?
(w, f(i)) log p(w, f(i))where w are the target words,   are the embeddings of target words, K is the number of different typesof features4, f(i)are the values of the i?th feature, and(i)are the embeddings for the values of the i?thfeature.
This extension can be seen as a set of K bivariate CODE models p(w, f(i)) that share the sametarget word embeddings  (w) but build their own feature embeddings(i)(f(i)).Step 4 constructs a vector representation for each word instance with the concatenation of its wordtype embedding and the average of its r substitute embeddings.
If the original embeddings are in ddimensional space, this results in a 2d dimensional vector representing an instance.Step 5 clusters these 2d dimensional instance vectors using a modified k-means algorithm with smartinitialization (Arthur and Vassilvitskii, 2007) and assigns each instance to one of k clusters.3 ExperimentsIn this section we present our instance-based POS induction experiments.
Section 3.1 describes the accu-racy metrics that we use to evaluate our results.
Section 3.2 details the test corpus and the experimentalparameters used in the English experiments and compares our results with previous work.
Section 3.3compares the performance of type and instance based systems on ambiguous words.
Finally, Section 3.4extends the language and corpus coverage by applying the best performing instance based models to 19corpora in 15 languages.4For example the number of features K = 4 in Table 1: Subst, Suf, Cap, and Num.2306Model MTO VMClark (2003) .712 .655Christodoulopoulos et al.
(2011) .728 .661Berg-Kirkpatrick et al.
(2010) .755 -Christodoulopoulos et al.
(2010) .761 .688Blunsom and Cohn (2011) .775 .697Yatbaz et al.
(2012) .8023 (.0070) .7207 (.0041)Instance based (Sec.
2) .7952 (.0030) .6908 (.0027)Table 2: Summary of results with MTO and VM scores for POS induction on the Penn Treebank.
Stan-dard errors are given in parentheses when available.
All the models incorporate orthographic and mor-phological features.
Berg-Kirkpatrick et al.
(2010) and Christodoulopoulos et al.
(2010) use instancebased models.3.1 EvaluationWe report many-to-one and V-measure scores for our experiments as suggested in (Christodoulopouloset al., 2010).
The many-to-one (MTO) evaluation maps each cluster to its most frequent gold tag andreports the percentage of correctly tagged instances.
The MTO score can be increased by simply increas-ing number of clusters, thus the number of clusters is fixed to match the number of gold tags in eachexperiment.
The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theory motivatedmetric that calculates the harmonic mean of completeness and homogeneity of the clusters.
Complete-ness of a cluster is maximized when all instances of a gold-tag are grouped into the same cluster and thehomogeneity is maximized when the members of a cluster belong to the same gold-tag.3.2 Experimental Settings and ResultsTo make a direct comparison with previously published results, the Wall Street Journal Section of thePenn Treebank was used as the test corpus (1,173,766 instances, 49,206 unique tokens) for Englishexperiments.
PTB uses 45 part-of-speech tags which we used as the gold standard for evaluation in ourexperiments.To compute substitutes in a given context we trained a language model using the ukWaC corpus (?2 billion tokens) constructed by crawling the ?.uk?
Internet domain (Ferraresi et al., 2008)5.
We usedSRILM (Stolcke, 2002) to build a 4-gram language model with interpolated Kneser-Ney discounting.Words that were observed less than 2 times in the language model training data were replaced by <unk>tags, which gave us a vocabulary size of 4,254,946.
The perplexity of the 4-gram language model onthe PTB is 303 and the unknown word rate is 0.008.
For computational efficiency only the top 100substitutes and their probabilities were computed for each position in the PTB using the FASTSUBSalgorithm (Yuret, 2012).
We use the same orthographic features defined in (Yatbaz et al., 2012) andgenerated morphological features using the unsupervised algorithm Morfessor (Creutz and Lagus, 2005).The experiments were run using the following default settings (unless otherwise stated): (1) eachword was kept with its original capitalization; (2) 90 substitutes sampled per instance; (3) the learningrate parameters for S-CODE were set to '0= 50, ?0= 0.2; (4) S-CODE convergence threshold, the log-likelihood difference between two consecutive iterations, was set to 0.001; (5) the S-CODE dimensionsand?Z were set to 25 and 0.166, respectively; (6) a modified k-means algorithm with smart initializationwas used (Arthur and Vassilvitskii, 2007); (7) the number of k-means restarts was set to 128 to improveclustering and reduce variance.Each experiment was repeated 10 times with different random seeds and the results are reported withstandard errors in parentheses or error bars in graphs.
Table 2 summarizes all the results reported in thissection and the ones we cite from the literature.5We use the Penn Treebank Tokenizer to make the training data compatible with PTB.23073.3 Word vs. Instance-Based InductionTable 2 shows that the overall many-to-one accuracy of our instance based induction system is compa-rable to (Yatbaz et al., 2012)6and significantly higher than the other published results on the Penn Tree-bank.
However Figure 1 in the introduction suggests that this summary hides the large difference in theanswers given by the different systems.
In this section we compare the performance of our instance-basedmodel to the word-based model of (Yatbaz et al., 2012) on word types at different levels of ambiguityusing the English Penn Treebank results.We propose the gold-tag perplexity of a word as a measure of its degree of ambiguity defined as:GP (w) = 2H(pw)= 2 Ptpw(t)log2pw(t)where w is a word, t is a tag, pwis the gold POS tag distribution of the word w and H(pw) is the entropyof the pwdistribution.
A GP of 1 for a word w indicates that w is always associated with the same POStag.
A word with N equally probable tags would have a GP of N .Figure 1 plots the gold-tag perplexity versus the smoothed MTO accuracy for the word-based andthe instance-based POS induction systems on the Penn Treebank.
To compose the plot, we found thebest mapping from the induced clusters to the gold-standard tags, then we computed the MTO accuracyfor each word using this mapping and plotted the MTO as a function of the word?s GP .
We used theNadaraya-Watson kernel regression estimate (Nadaraya, 1964; Watson, 1964) with normal kernel ofbandwidth 1.0 to obtain smooth regression lines.
The figure shows that the performance of the instance-based induction model does not degrade as much as the word-based model as the ambiguity of the wordsincrease.
However, only 14.94% of the instances in the PTB consists of words with GP greater than 1.5and 45.71% consists of words with GP exactly 1.
Thus, the overall accuracy numbers do not adequatelyreflect the improvement on highly ambiguous words.3.4 Multilingual ExperimentsFollowing Christodoulopoulos et al.
(2011), we extend our experiments to 8 languages from MULTEXT-East (Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Slovene and Serbian) (Erjavec, 2004)and 10 languages from the CoNLL-X shared task (Bulgarian, Czech, Danish, Dutch, German, Por-tuguese, Slovene, Spanish, Swedish and Turkish) (Buchholz and Marsi, 2006).To sample substitutes, we trained language models of Bulgarian, Czech, Estonian, Romanian, Danish,German, Dutch, Portuguese, Spanish, Swedish and Turkish with their corresponding TenTen corpora(Jakub??
?cek et al., 2013), and Hungarian, Slovene and Serbian with their corresponding Wikipedia dumpfiles7.
Serbian shares a common basis with Crotian and Bosnian therefore we trained 3 different languagemodels using Wikipedia dump files of Serbian together with these two languages and measured theperplexities on the MULTEXT-East Serbian corpus.
We chose the Croatian language model since itachieved the lowest perplexity score and unknown word ratio on MULTEXT-East Serbian corpus.
Weuse ukWaC corpora to train English language models.We used the default settings in Section 3.2 and incorporated only the orthographic features8.
Extractingunsupervised morphological features for languages with different characteristics would be of great value,but it is beyond the scope of this paper.
For each language the number of induced clusters is set to thenumber of tags in the gold-set.
To perform meaningful comparisons with the previous work we train andevaluate our models on the training section of MULTEXT-East9and CONLL-X languages (Lee et al.,2010).Table 3 presents the performance of our instance based model on 19 corpora in 15 languages togetherwith the corresponding best published results from?
(Yatbaz et al., 2012),?
(Blunsom and Cohn, 2011),6The difference is not statistically significant at p = 0.05.7Latest Wikipedia dump files are freely available at http://dumps.wikimedia.org/ and the text in the dump filescan be extracted using WP2TXT (http://wp2txt.rubyforge.org/)8All corpora (except German, Spanish and Swedish) label the punctuation marks with the same gold-tag therefore we addan extra punctuation feature for those languages.9Languages of MULTEXT-East corpora do not tag the punctuations, thus we add an extra tag for punctuations to the tag-setof these languages.2308Language TagsBestPublishedMTO VMInstanceBasedMTO / VMWSJEnglish 45 .802 / .721?.795 / .691MULTEXT-EastBulgarian 12+1 .665 / .556?.664 / .513Czech 12+1 .642 / .539?.705 / .511English 12+1 .733 / .633?.835 / .661Estonian 11+1 .644 / .533?.643 / .457Hungarian 12+1 .682 / .548?.647/ .459Romanian 14+1 .611 / .523?.660 / .528Slovene 12+1 .679 / .567?.667 / .451Serbian 12+1 .641 / .510?
.594/ .402CoNLL-XSharedTaskBulgarian 54 .704 / .596?
.751 / .583Czech 12 .701?/ .484?.701 / .486Danish 25 .761?/ .591?.761 / .584Dutch 13 .711?/ .547?.712 / .537German 54 .744?/ .630?
.749 / .618Portuguese 22 .785?/ .639?.782 / .607Slovene 29 .642?/ .539?
.638 / .469Spanish 47 .788?/ .632?.753/ .602Swedish 41 .682 / .589?
.681 / .546Turkish 30 .628 / .408?.637 / .401Table 3: The MTO and VM scores on 19 corpora in 15 languages together with number of inducedclusters.
Statistically significant results shown in bold (p < 0.05).?
(Christodoulopoulos et al., 2011) and?
(Clark, 2003).
All of the state-of-the-art systems in Table 3 areword-based and incorporate morphological features.Our MTO results are lower than the best systems on all of data-sets that use language models trainedon the Wikipedia corpora.
ukWaC and TenTen corpora are cleaner and tokenized better compared tothe Wikipedia corpora.
These corpora also have larger vocabulary sizes and lower out-of-vocabularyrates.
Thus language models trained on these corpora have much lower perplexities and generate bettersubstitutes than the Wikipedia based models.
Our model has lower VM scores in spite of good MTOscores on 14 corpora which is discussed in Section 4.Among the languages for which clean language model corpora were available, our model performscomparable to or significantly better than the best systems on most languages.
We show significantimprovements on MULTEXT-East Czech, Romanian, and CoNLL-X Bulgarian.
Our model achieves thestate-of-the-art MTO on MULTEXT-East English and scores comparable MTO with the best model onWSJ.
Our model shows comparable results on MULTEXT-East Bulgarian and Estonian, and CoNLL-X Czech, Danish, Dutch, German, Portuguese, Swedish and Turkish in terms of the MTO score.
Onereason for comparably low MTO on Spanish might be the absence of morphological features.4 DiscussionIn this section we perform further analysis on the clustering output of our model.
The example belowillustrates the advantage of the instance-based approach:(1) .
.
.
it will also offer buyers the option .
.
.Substitutes: give, help, attract(2) The offer is being launched .
.
.Substitutes: campaign, project, schemeThe word offer is a verb in the first sentence and a noun in the second one.
Clustering the wordembeddings can not distinguish the different occurrences of the words (Yatbaz et al., 2012).
On theother hand, the substitutes of offer in the two sentences can disambiguate the correct category of thecorresponding occurrences.
In our actual experiments our instance based representation distinguishesthe instances of offer as noun (cluster 26 and 12) and verb (cluster 35).To illustrate how words are distributed in the induced clusters, we compare the most frequent clustersof our model in Section 3 with the most frequent gold-tags of PTB in Figure 2.2309Figure 2: Each row corresponds to a gold tag and each column is an induced tag in the Hinton diagramabove.
The area of each square is proportional to the joint probability of the given tag and cluster.The low VM performance of our instance-based model compared to the state-of-the-art word-basedsystems on some languages is due to the completeness part of the VM score.
The Hinton diagram inFigure 2 shows that large gold-tag groups are split into several clusters based on the substitutability ofwords in that particular cluster (rows of the Hinton diagram).
For example, proper nouns (NNP) aresplit into three major clusters such that titles like Mr. or person names are in (40), nationality or countryrelated words like Japanese or U.S are in (22), and the rest of the proper nouns in cluster (30).The gold-tags of PTB, on the other hand, do not always respect whether words with the same tagare substitutable for one another.
Freudenthal et al.
(2005) argues, from the child language acquisitionperspective, that the standard linguistic definition of syntactic groups requires the substitutability ofwords in a syntactic category.
Word pairs that are placed in the same category in the PTB, such as ?Mr.
?and ?Friday?, ?be?
and ?run?, ?not?
and ?gladly?, ?of?
and ?into?
are clearly not generally substitutable.Another noteworty example of completeness error is that our model splits the punctuation mark (,)class of PTB into the clusters 15 and 43 based on the different usage patterns.
The majority of the(,) instances in cluster 15 are used in relative clauses, reported speech clauses or conjunctions whilecluster 43 generally consists of (,) instances that are used in non-essential clauses (ex: Time, the largestnewsweekly, .
.
.
).5 ContributionsOur main contributions can be summarized as follows:?
We introduced an instance based POS induction system that can handle ambiguous words and iscompetitive with the word-based systems in overall accuracy.?
We extended the S-CODE framework to handle more than two categorical variables.?
Our instance based system scores 79.5% many-to-one accuracy on the Penn Treebank and achievesresults that are significantly better than or comparable with the best published systems on 15 out of19 corpora in 15 languages.?
All our code and data, including the substitute distributions and word vectors for the PTB,2310MULTEXT-East and CoNLL-X shared task corpora are available at the authors?
website athttps://github.com/ai-ku/upos_2014.AcknowledgementsWe would like to thank Adam Kilgarriff and the Sketch Engine10team for making their corpora availableto us.
This work was supported in part by the Scientific and Technical Research Council of Turkey(T?UB?ITAK Project 112E277).ReferencesB.
Ambridge and E.V.M.
Lieven, 2011.
Child Language Acquisition: Contrasting Theoretical Approaches, chapter6.1.
Cambridge University Press.D.
Arthur and S. Vassilvitskii.
2007. k-means++: The advantages of careful seeding.
In Proceedings of theeighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027?1035.
Society for Industrial andApplied Mathematics.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phylogenetic grammar induction.
In Proceedings of the 48thAnnual Meeting of the Association for Computational Linguistics, pages 1288?1297, Uppsala, Sweden, July.Association for Computational Linguistics.Phil Blunsom and Trevor Cohn.
2011.
A hierarchical pitman-yor process hmm for unsupervised part of speechinduction.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-man Language Technologies, pages 865?874, Portland, Oregon, USA, June.
Association for ComputationalLinguistics.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
Comput.
Linguist., 18:467?479, December.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x shared task on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ?06, pages 149?164,Stroudsburg, PA, USA.
Association for Computational Linguistics.Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman.
2010.
Two decades of unsupervised posinduction: how far have we come?
In Proceedings of the 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 575?584, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman.
2011.
A bayesian mixture model for posinduction using multiple features.
In Proceedings of the 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 638?647, Edinburgh, Scotland, UK., July.
Association for Computational Linguis-tics.Alexander Clark.
2003.
Combining distributional and morphological information for part of speech induction.In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics -Volume 1, EACL ?03, pages 59?66, Stroudsburg, PA, USA.
Association for Computational Linguistics.Mathias Creutz and Krista Lagus.
2005.
Inducing the morphological lexicon of a natural language from unanno-tated text.
In Proceedings of AKRR?05, International and Interdisciplinary Conference on Adaptive KnowledgeRepresentation and Reasoning, pages 106?113, Espoo, Finland, June.Toma?z Erjavec.
2004.
MULTEXT-east version 3: Multilingual morphosyntactic specifications, lexicons andcorpora.
In Fourth International Conference on Language Resources and Evaluation, LREC?04, pages 1535?1538.
ELRA.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini.
2008.
Introducing and evaluating ukwac,a very large web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Canwe beat Google, pages 47?54.D.
Freudenthal, J.M.
Pine, and F. Gobet.
2005.
On the resolution of ambiguities in the extraction of syntacticcategories through chunking.
Cognitive Systems Research, 6(1):17?25.10https://www.sketchengine.co.uk2311Kuzman Ganchev, Jo?ao Grac?a, Jennifer Gillenwater, and Ben Taskar.
2010.
Posterior regularization for structuredlatent variable models.
J. Mach.
Learn.
Res., 99:2001?2049, August.Jianfeng Gao and Mark Johnson.
2008.
A comparison of bayesian estimators for unsupervised hidden markovmodel pos taggers.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing,EMNLP ?08, pages 344?352, Stroudsburg, PA, USA.
Association for Computational Linguistics.Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby.
2007.
Euclidean embedding of co-occurrence data.
J. Mach.
Learn.
Res., 8:2265?2295, December.Sharon Goldwater and Tom Griffiths.
2007.
A fully bayesian approach to unsupervised part-of-speech tagging.In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744?751,Prague, Czech Republic, June.
Association for Computational Linguistics.Aria Haghighi and Dan Klein.
2006.
Prototype-driven learning for sequence models.
In Proceedings of themain conference on Human Language Technology Conference of the North American Chapter of the Associa-tion of Computational Linguistics, HLT-NAACL ?06, pages 320?327, Stroudsburg, PA, USA.
Association forComputational Linguistics.Milo?s Jakub??
?cek, Adam Kilgarriff, Vojt?ech Kov?a?r, Pavel Rychl`y, and V?
?t Suchomel.
2013.
The tenten corpusfamily.
In International Conference on Corpus Linguistics, Lancaster.Mark Johnson.
2007.
Why doesn?t EM find good HMM POS-taggers?
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL), pages 296?305, Prague, Czech Republic, June.
Association for Computational Linguistics.Michael Lamar, Yariv Maron, and Elie Bienenstock.
2010a.
Latent-descriptor clustering for unsupervised posinduction.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,EMNLP ?10, pages 799?809, Stroudsburg, PA, USA.
Association for Computational Linguistics.Michael Lamar, Yariv Maron, Mark Johnson, and Elie Bienenstock.
2010b.
Svd and clustering for unsupervisedpos tagging.
In Proceedings of the ACL 2010 Conference Short Papers, pages 215?219, Uppsala, Sweden, July.Association for Computational Linguistics.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010.
Simple type-level unsupervised pos tagging.
InProceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ?10,pages 853?861, Stroudsburg, PA, USA.
Association for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor.
1999.
Treebank-3.
LinguisticData Consortium, Philadelphia.Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010.
Sphere embedding: An application to part-of-speechinduction.
In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, Advances inNeural Information Processing Systems 23, pages 1567?1575.Elizbar A Nadaraya.
1964.
On estimating regression.
Theory of Probability & Its Applications, 9(1):141?142.A.
Rosenberg and J. Hirschberg.
2007.
V-measure: A conditional entropy-based external cluster evaluationmeasure.
In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processingand Computational Natural Language Learning, pages 410?420.H.
Sch?utze and J. Pedersen.
1993.
A Vector Model for syntagmatic and paradigmatic relatedness.
In Proceedingsof the 9th Annual Conference of the University of Waterloo Centre for the New OED and Text Research, Oxford,England.Hinrich Sch?utze.
1995.
Distributional part-of-speech tagging.
In Proceedings of the seventh conference onEuropean chapter of the Association for Computational Linguistics, EACL ?95, pages 141?148, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Andreas Stolcke.
2002.
Srilm-an extensible language modeling toolkit.
In Proceedings International Conferenceon Spoken Language Processing, pages 257?286, November.Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.2312Geoffrey S Watson.
1964.
Smooth regression analysis.
Sankhy?a: The Indian Journal of Statistics, Series A, pages359?372.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.
Learning syntactic categories using paradigmatic rep-resentations of word context.
In Proceedings of the 2012 Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Language Learning, pages 940?951, Jeju Island, Korea, July.Association for Computational Linguistics.Deniz Yuret.
2012.
Fastsubs: An efficient and exact procedure for finding the most likely lexical substitutes basedon an n-gram language model.
Signal Processing Letters, IEEE, 19(11):725?728, Nov.2313
