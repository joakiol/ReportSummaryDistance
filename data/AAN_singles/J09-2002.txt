Exploiting Semantic Role Resourcesfor Preposition DisambiguationTom O?Hara?University of Maryland, Baltimore CountyJanyce Wiebe?
?University of PittsburghThis article describes how semantic role resources can be exploited for preposition disambigua-tion.
The main resources include the semantic role annotations provided by the Penn Treebankand FrameNet tagged corpora.
The resources also include the assertions contained in the Fac-totum knowledge base, as well as information from Cyc and Conceptual Graphs.
A commoninventory is derived from these in support of definition analysis, which is the motivation for thiswork.The disambiguation concentrates on relations indicated by prepositional phrases, and isframed as word-sense disambiguation for the preposition in question.
A new type of feature forword-sense disambiguation is introduced, usingWordNet hypernyms as collocations rather thanjust words.
Various experiments over the Penn Treebank and FrameNet data are presented, in-cluding prepositions classified separately versus together, and illustrating the effects of filtering.Similar experimentation is done over the Factotum data, including a method for inferring likelypreposition usage from corpora, as knowledge bases do not generally indicate how relationshipsare expressed in English (in contrast to the explicit annotations on this in the Penn Treebank andFrameNet).
Other experiments are included with the FrameNet data mapped into the commonrelation inventory developed for definition analysis, illustrating how preposition disambiguationmight be applied in lexical acquisition.1.
IntroductionEnglish prepositions convey important relations in text.
When used as verbal adjuncts,they are the principal means of conveying semantic roles for the supporting entitiesdescribed by the predicate.
Preposition disambiguation is a challenging problem.
First,prepositions are highly polysemous.
A typical collegiate dictionary has dozens ofsenses for each of the common prepositions.
Second, the senses of prepositions tendto be closely related to one another.
For instance, there are three duplicate role assign-ments among the twenty senses for of in The Preposition Project (Litkowski andHargraves 2006), a resource containing semantic annotations for common prepositions.?
Institute for Language and Information Technologies, Baltimore, MD 21250.
E-mail:tomohara@umbc.edu.??
Department of Computer Science, Pittsburgh, PA 15260.
E-mail: wiebe@cs.pitt.edu.Submission received: 7 August 2006; accepted for publication: 21 February 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 2Consider the disambiguation of the usages of on in the following sentences:(1) The cut should be blocked on procedural grounds.
(2) The industry already operates on very thin margins.The choice between the purpose and manner meanings for on in these sentences isdifficult.
The purpose meaning seems preferred for sentence 1, as grounds is a type ofjustification.
For sentence 2, the choice is even less clear, though the manner meaningseems preferred.This article presents a new method for disambiguating prepositions using infor-mation learned from annotated corpora as well as knowledge stored in declarativelexical resources.
The approach allows for better coverage and finer distinctions thanin previous work in preposition disambiguation.
For instance, a traditional approachwould involvemanually developing rules for on that specify the semantic type of objectsassociated with the different senses (e.g., time for temporal).
Instead, we infer this basedon lexical associations learned from annotated corpora.The motivation for preposition disambiguation is to support a system for lexicalacquisition (O?Hara 2005).
The focus of the system is to acquire distinguishing infor-mation for the concepts serving to define words.
Large-scale semantic lexicons mainlyemphasize the taxonomic relations among the underlying concepts (e.g., is-a and part-of ), and often lack sufficient differentiation among similar concepts (e.g., via attributesor functional relations such as is-used-for).
For example, in WordNet (Miller et al 1990),the standard lexical resource for natural language processing, the only relations forbeagle andAfghan are that they are both a type of hound.
Although the size difference canbe inferred from the definitions, it is not represented in the WordNet semantic network.In WordNet, words are grouped into synonym sets called synsets, which representthe underlying concepts and serve as nodes in a semantic network.
Synsets are orderedinto a hierarchy using the hypernym relation (i.e., is-a).
There are several other semanticrelations, such as part-whole, is-similar-to, and domain-of .
Nonetheless, in version 2.1 ofWordNet, about 30% of the synsets for noun entries are not explicitly distinguished fromsibling synsets via semantic relations.To address such coverage problems in lexicons, we have developed an empiricalapproach to lexical acquisition, building upon earlier knowledge-based approaches indictionary definition analysis (Wilks, Slator, and Guthrie 1996).
This involves a two-stepprocess: Definitions are first analyzed with a broad-coverage parser, and then the result-ing syntactic relationships are disambiguated using statistical classification.
A crucialpart of this process is the disambiguation of prepositions, exploiting online resourceswith semantic role usage information.
The main resources are the Penn Treebank(PTB; Marcus et al 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), twopopular corpora providing rich annotations on English text, such as the semantic rolesassociatedwith prepositional phrases in context.
In addition to the semantic role annota-tions from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to providetraining data for the relation classification.
In particular, the FactotumKB (Cassidy 2000)is used to provide additional training data for prepositions that are used to conveyparticular relationships.
Information on preposition usage is not explicitly encoded inFactotum, so a new corpus analysis technique is employed to infer the associations.Details on the lexical acquisition process, including application and evaluation, canbe found in O?Hara (2005).
This article focuses on the aspects of this method relevantto the processing of prepositions.
In particular, here we specifically address preposition152O?Hara and Wiebe Exploiting Resources for Preposition Disambiguationdisambiguation using semantic role annotations from PTB, FrameNet, and Factotum.In each case, classification experiments are presented using the respective resources astraining data with evaluation via 10-fold cross validation.This article is organized as follows.
Section 2 presents background information onthe relation inventories used during classification, including one developed specificallyfor definition analysis.
Section 3 discusses the relation classifiers in depth with resultsgiven for four different inventories.
Section 4 discusses related work in relation disam-biguation, and Section 5 presents our conclusions.2.
Semantic Relation InventoriesThe representation of natural language utterances often incorporates the notion ofsemantic roles, which are analogous to the slots in a frame-based representation.
Inparticular, there is an emphasis on the analysis of thematic roles, which serve to tiethe grammatical constituents of a sentence to the underlying semantic representation.Thematic roles are also called case roles, because in some languages the grammaticalconstituents are indicated by case inflections (e.g., ablative in Latin).
As used here, theterm ?semantic role?
refers to an arbitrary semantic relation, and the term ?thematicrole?
refers to a relation intended to capture the semantics of sentences (e.g., eventparticipation).Which semantic roles are used varies widely in Natural Language Processing(NLP).
Some systems use just a small number of very general roles, such as beneficiary.At the other extreme, some systems use quite specific roles tailored to a particulardomain, such as catalyst in the chemical sense.2.1 Background on Semantic RolesBruce (1975) presents an account of early case systems in NLP.
For the most part,those systems had limited case role inventories, along the lines of the cases defined byFillmore (1968).
Palmer (1990) discusses some of the more contentious issues regardingcase systems, including adequacy for representation, such as reliance solely upon caseinformation to determine semantics versus the use of additional inference mechanisms.Barker (1998) provides a comprehensive summary of case inventories in NLP, alongwith criteria for the qualitative evaluation of case systems (generality, completeness, anduniqueness).
Linguistic work on thematic roles tends to use a limited number of roles.Frawley (1992) presents a detailed discussion of twelve thematic roles and discusseshow they are realized in different languages.During the shift in emphasis away from systems that work in small, self-containeddomains to those that can handle open-ended domains, there has been a trend towardsthe use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996).
TheWordNet lexicon (Miller et al 1990) serves as one example of this.
A synset is definedin terms of its relations with any of the other 100,000+ synsets, rather than in terms of aset of features like [?ANIMATE].
There has also been a shift in focus from deep under-standing (e.g., story comprehension) facilitated by specially constructed KBs to shallowsurface-level analysis (e.g., text extraction) facilitated by corpus analysis.
Both trendsseem to be behind the increase in case inventories in two relatively recent resources,namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002),both of which define well over a hundred case roles.
However, provided that the caseroles are well structured in an inheritance hierarchy, both paraphrasability and coveragecan be addressed by the same inventory.153Computational Linguistics Volume 35, Number 22.2 Inventories Developed for Corpus AnnotationWith the emphasis on corpus analysis in computational linguistics, there has been ashift away from relying on explicitly-coded knowledge towards the use of knowledgeinferred from naturally occurring text, in particular text that has been annotated byhumans to indicate phenomena of interest.
For example, rather than manually devel-oping rules for preferring one sense of a word over another based on context, themost successful approaches have automatically learned the rules based on word-senseannotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds andCotton 2001).The Penn Treebank version II (Marcus et al 1994) provided the first large-scale setof case annotations for general-purpose text.
These are very general roles, followingFillmore (1968).
The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) projectcurrently provides the most comprehensive set of semantic roles annotations.
These areat a much finer granularity than those in PTB, making them quite useful for applicationslearning semantics from corpora.
Relation disambiguation experiments for both of theserole inventories are presented subsequently.2.2.1 Penn Treebank.
The original PTB (Marcus, Santorini, and Marcinkiewicz 1993) pro-vided syntactic annotations in the form of parse trees for text from theWall Street Journal.This resource is very popular in computational linguistics, particularly for inducingpart-of-speech taggers and parsers.
PTB version II (Marcus et al 1994) added 20 func-tional tags, including a few thematic roles such as temporal, direction, and purpose.
Thesecan be attached to any verb complement but normally occur with clauses, adverbs, andprepositions.For example, Figure 1 shows a parse tree using the extended annotation format.In addition to the usual syntactic constituents such as NP and VP, function tags areincluded.
For example, the second NP gives the subject.
This also shows that the firstprepositional phrase (PP) indicates the time frame, whereas the last PP indicates theSentence:In 1982, Sports & Recreation?s managers and certain passive investors purchased thecompany from Brunswick Corp. of Skokie, Ill.Parse:(S (PP-TMP In (NP 1982)), temporal extent(NP-SBJ grammatical subject(NP (NP (NP Sports) & (NP Recreation) ?s)managers)and (NP certain passive investors))(VP purchased(NP the company)(PP-CLR from closely related(NP (NP Brunswick Corp.)(PP-LOC of locative(NP (NP Skokie) , (NP Ill)))))) .
)Figure 1Penn Treebank II parse tree annotation sample.
The functional tags are shown in boldface.154O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 1Frequency of Penn Treebank II semantic role annotations.
Relative frequencies estimated over thecounts for unique assignments given in the PTB documentation (bkt tags.lst), and descriptionsbased on Bies et al (1995).
Omits low-frequency benefactive role.
The syntactic role annotationsgenerally have higher frequencies; for example, the subject role occurs 49% of the time (out ofabout 240,000 total annotations).Role Freq.
Descriptiontemporal .113 indicates when, how often, or how longlocative .075 place/setting of the eventdirection .026 starting or ending location (trajectory)manner .021 indicates manner, including instrumentpurpose .017 purpose or reasonextent .010 spatial extentlocation.
The second PP is tagged as closely-related, which is one of the miscellaneousPTB function tags that are more syntactic in nature: ?
[CLR] occupy somemiddle groundbetween arguments and adjunct?
(Bies et al 1995).
Frequency information for thesemantic role annotations is shown in Table 1.2.2.2 FrameNet.
FrameNet (Fillmore, Wooters, and Baker 2001) is striving to develop anEnglish lexicon with rich case structure information for the various contexts that wordscan occur in.
Each of these contexts is called a frame, and the semantic relations thatoccur in each frame are called frame elements (FE).
For example, in the communica-tion frame, there are frame elements for communicator, message, medium, and so forth.FrameNet annotations occur at the phrase level instead of the grammatical constituentlevel as in PTB.
Figure 2 shows an example.Table 2 displays the top 25 semantic roles by frequency of annotation.
This showsthat the semantic roles in FrameNet can be quite specific, as with the roles cognizer,evaluee, and addressee.
In all, there are over 780 roles annotated with over 288,000 taggedinstances.Sentence:Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling stand-alone workstations to communicate over public or private ISDN networks.Annotation:Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling?C FE=?Communicator?
PT=?NP?
?standalone workstations?/C?to ?C TARGET=?y??communicate?/C?
?C FE=?Medium?
PT=?PP?
?over public or private ISDN networks?/C?
.Figure 2FrameNet annotation sample.
The constituent (C) tags identify the phrases that have beenannotated.
The frame element (FE) attributes indicate the semantic roles, and the phrase type(PT) attributes indicate the traditional grammatical category for the phrase.
For simplicity, thisexample is formatted in the earlier FrameNet format, but the information is taken from thelatest annotations (lu5.xml).155Computational Linguistics Volume 35, Number 2Table 2Common FrameNet semantic roles.
The top 25 of 773 roles are shown, representing nearly half ofthe total annotations (about 290,000).
Descriptions based on FrameNet 1.3 frame documentation.Role Freq.
Descriptionagent .037 person performing the intentional acttheme .031 object being acted on, affected, etc.experiencer .029 being who has a physical experience, etc.goal .028 endpoint of the pathspeaker .028 individual that communicates the messagestimulus .026 entity that evokes responsemanner .025 manner of performing an action, etc.degree .024 degree to which event occursself-mover .023 volitional agent that movesmessage .021 the content that is communicatedpath .020 the trajectory of motion, etc.cognizer .018 person who perceives the eventsource .017 the beginning of the pathtime .016 the time at which the situation occursevaluee .016 thing about which a judgment has been madedescriptor .015 attributes, traits, etc.
of the entitybody-part .014 location on the body of the experiencercontent .014 situation or state-of-affairs that attention is focused ontopic .014 subject matter of the communicated message, etc.item .012 entity whose scalar property is specifiedtarget .011 entity which is hit by a projectilegarment .011 clothing wornaddressee .011 entity that receives a message from the communicatorprotagonist .011 person to whom a mental property is attributedcommunicator .010 the person who communicates a message2.3 OtherA recent semantic role resource that is starting to attract interest is the Proposition Bank(PropBank), developed at the University of Pennsylvania (Palmer, Gildea, and Kings-bury 2005).
It extends the Penn Treebank with information on verb subcategorization.The focus is on annotating all verb occurrences and all their argument realizations thatoccur in the Wall Street Journal, rather than select corpus examples as in FrameNet.Therefore, the role inventory is heavily verb-centric, for example, with the generic labelsarg0 through arg4 denoting the main verbal arguments to avoid misinterpretations.Verbal adjuncts are assigned roles based on PTB version II (e.g., argM-LOC and argM-TMP).
PropBank has been used as the training data in recent semantic role labelingcompetitions as part of the Conferences on Computational Natural Language Learn-ing (Carreras and Ma`rquez 2004, 2005).
Thus, it is likely to become as influential asFrameNet in computational semantics.The Preposition Project similarly adds information to an existing semantic roleresource, namely FrameNet.
It is being developed by CL Research (Litkowski andHargraves 2006) and endeavors to provide comprehensive syntactic and semantic in-formation on various usages of prepositions, which often are not represented wellin semantic lexicons (e.g., they are not included at all in WordNet).
The PrepositionProject uses the sense distinctions from the Oxford Dictionary of English and integratessyntactic information about prepositions from comprehensive grammar references.156O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation2.4 Inventories for Knowledge RepresentationThis section describes three case inventories: one developed for the Cyc KB (Lenat1995), one used to define Conceptual Graphs (Sowa 1984), and one for the FactotumKB (Cassidy 2000).
The first two are based on a traditional knowledge representationparadigm.
With respect to natural language processing, these approaches are morerepresentative of the earlier approaches in which deep understanding is the chief goal.Factotum is also based on a knowledge representation paradigm, but in a sense alsoreflects the empirical aspect of the corpus annotation approach, because the annotationswere developed to address the relations implicit in Roget?s Thesaurus.In this article, relation disambiguation experiments are only presented for Facto-tum, given that the others do not readily provide sufficient training data.
However, theother inventories are discussed because each provides relation types incorporated intothe inventory used below for the definition analysis (see Section 3.5).2.4.1 Cyc.
The Cyc system (Lenat 1995) is the most ambitious knowledge representationproject undertaken to date, in development since 1984.
The full Cyc KB is propri-etary, which has hindered its adoption in natural language processing.
However, toencourage broader usage, portions of the KB have been made freely available to thepublic.
For instance, there is an open-source version of the system called OpenCyc(www.opencyc.org), which covers the upper part of the KB and also includes the Cycinference engine, KB browser, and other tools.
In addition, researchers can obtain accessto ResearchCyc, which contains most of the KB except for proprietary information (e.g.,internal bookkeeping assertions).Cyc uses a wide range of role types: very general roles (e.g., beneficiary); commonlyoccurring situational roles (e.g., victim); and highly specialized roles (e.g., catalyst).
Ofthe 8,756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actor-slot) with 51 other semantic roles (i.e., other instances of role).
Table 3 shows the mostcommonly used event-based roles in the KB.2.4.2 Conceptual Graphs.
The Conceptual Graphs (CG) mechanism was introduced bySowa (1984) for knowledge representation as part of his Conceptual Structures theory.The original text listed two dozen or so thematic relations, such as destination andinitiator.
In all, 37 conceptual relations were defined.
This inventory formed the basisfor most work in Conceptual Graphs.
Recently, Sowa (1999) updated the inventory toallow for better hierarchical structuring and to incorporate the important thematic rolesidentified by Somers (1987).
Table 4 shows a sample of these roles, along with usageestimates based on corpus analysis (O?Hara 2005).2.4.3 Factotum.
The Factotum semantic network (Cassidy 2000) developed by Micra,Inc., makes explicit many of the relations in Roget?s Thesaurus.1 Outside of proprietaryresources such as Cyc, Factotum is themost comprehensive KBwith respect to functionalrelations, which are taken here to be non-hierarchical relations, excluding attributes.OpenCyc does include definitions of many non-hierarchical relations.
However, thereare not many instantiations (i.e., relationship assertions), because it concentrates on thehigher level of the ontology.1 Factotum is based on the public domain version of Roget?s Thesaurus.
The latter is freely available viaProject Gutenberg (http://promo.net/pg), thanks to Micra, Inc.157Computational Linguistics Volume 35, Number 2Table 3Most common event-based roles in OpenCyc.
Descriptions based on comments from theOpenCyc knowledge base (version 0.7).
Relative frequencies based on counts obtainedvia Cyc?s utility functions.Role Freq.
Descriptiondone-by .178 relates an event to its ?doer?performed-by .119 doer deliberately does actobject-of-state-change .081 object undergoes some kind of intrinsic change of stateobject-acted-on .057 object is altered or affected in eventoutputs-created .051 object comes into existence sometime during eventtransporter .044 object facilitating conveyance of transporteestransportees .044 object being movedto-location .041 where the moving object is found when event endsobject-removed .036 object removed from its previous locationinputs .036 pre-existing event participant destroyed or incorporatedinto a new entityproducts .035 object is one of the intended outputs of eventinputs-destroyed .035 object exists before event and is destroyed during eventfrom-location .034 where some moving-object in the move is found at thebeginningprimary-object-moving .033 object is in motion at some point during the event, andthis movement is focalseller .030 agent sells something in the exchangeobject-of-possession-transfer .030 rights to use object transferred from one agent to anothertransferred-thing .030 object is being moved, transferred, or exchanged in theevent transfersender-of-info .030 sender is an agent who is the source of informationtransferredinputs-committed .028 object exists before event and continues to existafterwards, and as a result of event, object becomesincorporated into something created during eventobject-emitted .026 object is emitted from the emitter during the emissioneventThe Factotum knowledge base is based on the 1911 version of Roget?s Thesaurusand specifies the relations that hold between the Roget categories and the words listedin each entry.
Factotum incorporates information from other resources as well.
Forinstance, the Unified Medical Language System (UMLS) formed the basis for the initialinventory of semantic relations, which was later revised during tagging.Figure 3 shows a sample from Factotum.
This illustrates that the basic Roget or-ganization is still used, although additional hierarchical levels have been added.
Therelations are contained within double braces (e.g., ?
{{has subtype}}?)
and generallyapply from the category to each word in the synonym list on the same line.
For example,the line with ?
{{result of}}?
indicates that conversion is the result of transforming,as shown in the semantic relation listing that would be extracted.
There are over 400different relations instantiated in the knowledge base, which has over 93,000 assertions.Some of these are quite specialized (e.g., has-brandname).
In addition, there are quite afew inverse relations, becausemost of the relations are not symmetrical.
Certain featuresof the knowledge representation are ignored during the relation extraction used later.For example, relation specifications can have qualifier prefixes, such as an ampersandto indicate that the relationship only sometimes holds.158O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 4Common semantic roles used in Conceptual Graphs.
Inventory and descriptions based onSowa (1999, pages 502?510).
The term situation is used in place of Sowa?s nexus (i.e., ?fact oftogetherness?
), which also covers spatial structures.
Freq.
gives estimated relative frequenciesfrom O?Hara (2005).Role Freq.
Descriptionagent .267 entity voluntarily initiating an actionattribute .155 entity that is a property of some objectcharacteristic .080 types of properties of entitiestheme .064 participant involved with but not changedpatient .061 participant undergoing structural changelocation .053 participant of a spatial situationpossession .035 entity owned by some animate beingpart .035 object that is a component of some objectorigin .035 source of a spatial or ambient situationexperiencer .035 animate goal of an experienceresult .032 inanimate goal of an actinstrument .027 resource used but not changedrecipient .019 animate goal of an actdestination .013 goal of a spatial processpoint-in-time .011 participant of a temporal situationpath .011 resource of a spatial or ambient situationaccompaniment .011 object participating with anothereffector .008 source involuntarily initiating an actionbeneficiary .008 entity benefiting from event completionmatter .005 resource that is changed by the eventmanner .005 entity that is a property of some processsource .003 present at beginning of activityresource .003 material necessary for situationproduct .003 present at end of activitymedium .003 resource for transmitting informationgoal .003 final cause which is purpose or benefitduration .003 resource of a temporal processbecause .003 situation causing another situationamount .003 a measure of some characteristicTable 5 shows themost common relations in terms of usage in the semantic network,and includes others that are used in the experiments discussed later.2 The relativefrequencies just reflect relationships explicitly labeled in the KB data file.
For instance,this does not account for implicit has-subtype relationships based on the hierarchicalorganization of the thesaural groups (e.g., ?simple-change, has-subtype, conversion?
).The functional relations are shown in boldface.
This excludes the meronym or part-whole relations (e.g., is-conceptual-part-of ), in line with their classification by Cruse(1986) as hierarchical relations.
The reason for concentrating on the functional relationsis that these are more akin to the roles tagged in PTB and FrameNet.The information in Factotum complements WordNet through the inclusion of morefunctional relations (e.g., non-hierarchical relations such as uses and is-function-of ).
Forcomparison purposes, Table 6 shows the semantic relation usage in WordNet version2 The database files and documentation for the semantic network are available from Micra, Inc., viaftp://micra.com/factotum.159Computational Linguistics Volume 35, Number 2Original data:A. ABSTRACT RELATION...A6 CHANGE (R140 TO R152)...A6.1 SIMPLE CHANGE (R140)...A6.1.4 CONVERSION (R144)#144.
Conversion.N.
{{has subtype(change, R140)}} conversion, transformation.
{{has case: @R7, initial state, final state}}.
{{has patient: @R3a, object, entity}}.
{{result of}} {{has subtype(process, A7.7)}} converting, transforming.
{{has subtype}} processing.transition.Extracted relationships:?change, has-subtype, conversion?
?change, has-subtype, transformation?
?conversion, has-case, initial state?
?conversion, has-case, final state?
?conversion, has-patient, object?
?conversion, has-patient, entity?
?conversion, is-result-of , converting?
?conversion, is-result-of , transforming?
?process, has-subtype, converting?
?process, has-subtype, transforming?
?conversion, has-subtype, processing?Figure 3Sample data from Factotum.
Based on version 0.56 of Factotum.2.1.
As can be seen from the table, the majority of the relations are hierarchical.3WordNet 2.1 averages just about 1.1 non-taxonomic properties per concept (includ-ing inverses but excluding hierarchical relations such as has-hypernym and is-member-meronym-of ).
OpenCyc provides a much higher average at 3.7 properties per concept,although with an emphasis on argument constraints and other usage restrictions.
Fac-totum averages 1.8 properties per concept, thus complementing WordNet in terms ofinformation content.42.5 Combining the Different Semantic Role InventoriesIt is difficult to provide precise comparisons of the five inventories just discussed.
This isdue both to the different nature of the inventories (e.g., developed for knowledge basesas opposed to being derived from natural language annotations) and due to the way the3 In WordNet, the is-similar-to relation for adjectives can be considered as hierarchical, as it links satellitesynsets to heads of adjective clusters (Miller 1998).
For example, the satellite synsets for ?thirsty?
and?rainless?
are both linked to the head synset for ?dry (vs.
wet).
?4 These figures are derived by counting the number of relations excluding the instance and subset onesand then dividing by the number of concepts (i.e., ratio of non-hierarchical relations to concepts).
Cyc?scomments and lexical assertions are also excluded, as these are implicit in Factotum and WordNet.WordNet?s is-derived-from relations are omitted as lexical in nature (the figure otherwise would be 1.6).160O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 5Common Factotum semantic roles.
These account for 80% of the instances.
Boldface relations areused in the experiments (Section 3.4.2).Relation Freq.
Descriptionhas-subtype .401 inverse of is-a relationis-property-of .077 object with given salient characteris-caused-by .034 force that is the origin of somethinghas-property .028 salient property of an objecthas-part .022 a part of a physical objecthas-high-intensity .018 intensifier for property or characteristichas-high-level .017 implication of activity (e.g., intelligence)is-antonym-of .016 generally used for lexical oppositionis-conceptual-part-of .015 parts of other entities (e.g., case relations)has-metaphor .014 non-literal reference to the wordcausesmental .013 motivation (causation in the mental realm)uses .012 a tool needing active manipulationis-performed-by .012 human actor for the eventperformshuman .011 human role in performing some activityis-function-of .011 artifact passively performing the functionhas-result .010 more specific type of causeshas-conceptual-part .010 generalization of has-partis-used-in .010 activity or desired effect for the entityis-part-of .010 distinguishes part from group membershipcauses .009 inverse of is-caused-byhas-method .009 method used to achieve some goalis-caused-bymental .009 inverse of causesmentalhas-consequence .008 causation due to a natural associationhas-commencement .007 state that commences with the actionis-location-of .007 absolute location of an objectrequires .004 object or sub-action needed for an actionis-studied-in .004 inquires into any field of studyis-topic-of .002 communication dealing with given subjectproduces .002 what an action yields, generates, etc.is-measured-by .002 instrument for measuring somethingis-job-of .001 occupation title for a job functionis-patient-of .001 action that the object participates inis-facilitated-by .001 object or sub-action aiding an actionis-biofunction-of .0003 biological function of parts of living thingswas-performed-by .0002 is-performed-by occurring in the pasthas-consequenceobject .0002 consequence for the patient of an actionis-facilitated-bymental .0001 trait that facilitates some human actionrelation listings were extracted (e.g., just including event-based roles from OpenCyc).As can be seen from Tables 2 and 3, FrameNet tends to refine the roles for agents (e.g.,communicator) compared to OpenCyc, which in contrast has more refinements of theobject role (e.g., object-removed).
The Concept Graphs inventory includes more emphasison specialization relations than the others, as can be seen from the top entries in Table 4(e.g., attribute).In the next section, we show how classifiers can be automatically developed forthe semantic role inventories just discussed.
For the application to dictionary defin-ition analysis, we need to combine the classifiers learned over PTB, FrameNet, andFactotum.
This can be done readily in a cascaded fashion with the classifier for themost specific relation inventory (i.e., FrameNet) being used first and then the otherclassifiers being applied in turn whenever the classification is inconclusive.
This would161Computational Linguistics Volume 35, Number 2Table 6Semantic relation usage in WordNet.
Relative frequencies for semantic relations in WordNet(173,570 total instances).
This table omits lexical relations, such as the is-derived-from relation(71,914 instances).
Frequencies based on analysis of database files for WordNet 2.1.Relation Freq.
Descriptionhas-hypernym .558 superset relationis-similar-to .130 similar adjective synsetis-member-meronym-of .071 constituent memberis-part-meronym-of .051 constituent partis-pertainym-of .046 noun that adjective pertains tois-antonym-of .046 opposing concepthas-topic-domain .038 topic domain for the synsetalso-see .019 related entry (for adjectives and verbs)has-verb-group .010 verb senses grouped by similarityhas-region-domain .008 region domain for the synsethas-attribute .007 related attribute category or valuehas-usage-domain .007 usage domain for the synsetis-substance-meronym-of .004 constituent substanceentails .002 action entailed by the verbcauses .001 action caused by the verbhas-participle .001 verb participlehave the advantage that new resources could be integrated into the combined relationclassifier with minimal effort.
However, the resulting role inventory would likely beheterogeneous and might be prone to inconsistent classifications.
In addition, the roleinventory could change whenever new annotation resources are incorporated, makingthe overall definition analysis system somewhat unpredictable.Alternatively, the annotations can be converted into a common inventory, and aseparate relation classifier induced over the resulting data.
This has the advantagethat the target relation-type inventory remains stable whenever new sources of relationannotations are introduced.
In addition, the classifier will likely be more accurate asthere are more examples per relation type on average.
The drawback, however, is thatannotations from new resources must first be mapped into the common inventorybefore incorporation.The latter approach is employed here.
The common inventory incorporates some ofthe general relation types defined by Gildea and Jurafsky (2002) for their experimentsin classifying semantic relations in FrameNet using a reduced relation inventory.
Theydefined 18 relations (including a special-case null role for expletives), as shown inTable 7.
These roles served as the starting point for the common relation inventorywe developed to support definition analysis (O?Hara 2005), with half of the roles usedas is and a few others mapped into similar roles.
In total, twenty-six relations aredefined, including a few roles based on the PTB, Cyc, and Conceptual Graphs inven-Table 7Abstract roles defined by Gildea and Jurafsky based on FrameNet.
Taken from Gildea andJurafsky (2002).agent cause degree experiencer force goalinstrument location manner null path patientpercept proposition result source state topic162O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 8Inventory of semantic relations for definition analysis.
This inventory is inspired by the roles inTable 7 and is primarily based on FrameNet (Fillmore, Wooters, and Baker 2001) and ConceptualGraphs (Sowa 1999); it also includes roles based on the PTB and Cyc inventories.Relation Descriptionaccompaniment entity that participates with another entityagent entity voluntarily performing an actionamount quantity used as a measure of some characteristicarea region in which the action takes placecategory general type or class of which the item is an instancecause non-agentive entity that produces an effectcharacteristic general properties of entitiescontext background for situation or predicationdirection either spatial source or goal (same as in PTB)distance spatial extent of motionduration period of time that the situation applies withinexperiencer entity undergoing some (non-voluntary) experiencegoal location that an affected entity ends up ininstrument entity or resource facilitating event occurrencelocation reference spatial location for situationmanner property of the underlying processmeans action taken to affect somethingmedium setting in which an affected entity is conveyedpart component of entity or situationpath trajectory which is neither a source nor a goalproduct entity present at end of event (same as Cyc products)recipient recipient of the resource(s)resource entity utilized during event (same as Cyc inputs)source initial position of an affected entitytheme entity somehow affected by the eventtime reference time for situationtories.
Table 8 shows this role inventory along with a description of each case.
Inaddition to traditional thematic relations, this includes a few specialization relations,which are relevant to definition analysis.
For example, characteristic corresponds to thegeneral relation from Conceptual Graphs for properties of entities; and category gen-eralizes the corresponding FrameNet role, which indicates category type, to subsumeother FrameNet roles related to categorization (e.g., topic).
Note that this inventory isnot meant to be definitive and has been developed primarily to address mappings fromFrameNet for the experiments discussed in Section 3.5.
Thus, it is likely that additionalroles will be required when additional sources of semantic relations are incorporated(e.g., Cyc).
Themappingswere producedmanually by reviewing the role descriptions inthe FrameNet documentation and checking prepositional usages for each to determinewhich of the common inventory roles might be most relevant.
As some of the roles withthe same name have frame-specific meanings, in a few cases this involved conflictingusages (e.g., body-part associated with both area and instrument), which were resolved infavor of the more common usage.55 See www.cs.nmsu.edu/~tomohara/cl-prep-article/relation-mapping.html for the mapping,covering cases occurring at least 50 times in FrameNet.163Computational Linguistics Volume 35, Number 23.
Preposition DisambiguationThis section presents the results of our experiments on the disambiguation of relationsindicated by prepositional phrases.
Results are given for PTB, FrameNet, and Factotum.The PTB roles are general: For example, for the preposition for, there are six distinctions(four, with low-frequency pruning).
The PTB role disambiguation experiments thusaddress a coarse form of sense distinction.
In contrast, the FrameNet distinctions arequite specific: there are 192 distinctions associatedwith for (21 with low-frequency prun-ing); and, there are 17 distinctions in Factotum (15 with low-frequency pruning).
OurFrameNet and Factotum role disambiguation experiments thus address fine-grainedsense distinctions.3.1 OverviewA straightforward approach for preposition disambiguation would be to use typicalword-sense disambiguation features, such as the parts-of-speech of surrounding wordsand, more importantly, collocations (e.g., lexical associations).
Although this can behighly accurate, it tends to overfit the data and to generalize poorly.
The latter is ofparticular concern here as the training data is taken from a different genre than theapplication data.
For example, the PTB data is from newspaper text (specifically, WallStreet Journal), but the lexical acquisition is based on dictionary definitions.
We firstdiscuss how class-based collocations address this problem and then present the featuresused in the experiments.Before getting into technical details, an informal example will be used to motivatethe use of hypernym collocations.
Consider the following purpose role examples, whichare similar to the first example from the introduction.
(3) This contention would justify dismissal of these actions onpurposeprudential grounds.
(4) Ramada?s stock rose 87.5 cents onpurpose the news.It turns out that grounds and news are often used as the prepositional object in PTBwhen the sense for on is purpose (or reason).
Thus, these words would likely be chosen ascollocations for this sense.
However, for the sake of generalization, it would be better tochoose theWordNet hypernym subject matter, as that subsumes both words.
This wouldthen allow the following sentence to be recognized as indicating purpose even thoughcensurewas not contained in the training data.
(5) Senator sets hearing onpurpose censure of Bush.3.1.1 Class-Based Collocations via Hypernyms.
To overcome data sparseness problems, aclass-based approach is used for the collocations, with WordNet synsets as the sourceof the word classes.
(Part-of-speech tags are a popular type of class-based feature usedin word sense disambiguation (WSD) to capture syntactic generalizations.)
Recall thatthe WordNet synset hierarchy can be viewed as a taxonomy of concepts.
Therefore, inaddition to using collocations in the form of other words, we use collocations in theform of semantic concepts.Word collocation features are derived by making two passes over the trainingdata (e.g., ?on?
sentences with correct role indicated).
The first pass tabulates the164O?Hara and Wiebe Exploiting Resources for Preposition Disambiguationco-occurrence counts for each of the context words (i.e., those in a window around thetarget word) paired with the classification value for the given training instance (e.g.,the preposition sense from the annotation).
These counts are used to derive conditionalprobability estimates of each class value given co-occurrence of the various potentialcollocates.
The words exceeding a certain threshold are collected into a list associatedwith the class value, making this a ?bag of words?
approach.
In the experiments dis-cussed below, a potential collocate (coll) is selected whenever the conditional probabilityfor the class (C) value exceeds the prior probability by a factor greater than 20%:6P(C|coll)?
P(C)P(C)?
.20 (1)That is, for a given potential collocation word (coll) to be treated as one of the ac-tual collocation words, the relative percent change of the class conditional probability(P(C|coll)) versus the prior probability for the class value (P(C)) must be 20% or higher.The second pass over the training data determines the value for the collocational featureof each classification category by checking whether the current context window has anyof the associated collocation words.
Note that for the test data, only the second pass ismade, using the collocation lists derived from the training data.In generalizing this to a class-based approach, the potential collocational words arereplaced with each of their hypernym ancestors fromWordNet.
The adjective hierarchyis relatively shallow, so it is augmented by treating is-similar-to as has-hypernym.
Forexample, the synset for ?arid?
and ?waterless?
is linked to the synset for ?dry (vs.wet).?
Adverbs would be included, but there is no hierarchy for them.
Because the co-occurring words are not sense-tagged, this is done for each synset serving as a differentsense of the word.
Likewise, in the case of multiple inheritance, each parent synset isused.
For example, given the co-occurring wordmoney, the counts would be updated asif each of the following tokens were seen (grouped by sense).1.
{ medium of exchange#1, monetary system#1, standard#1, criterion#1,measure#2, touchstone#1, reference point#1, point of reference#1, ref-erence#3, indicator#2, signal#1, signaling#1, sign#3, communication#2,social relation#1, relation#1, abstraction#6 }2.
{ wealth#4, property#2, belongings#1, holding#2, material possession#1,possession#2 }3.
{ currency#1, medium of exchange#1, monetary system#1, standard#1,criterion#1, measure#2, touchstone#1, reference point#1, point of -reference#1, reference#3, indicator#2, signal#1, signaling#1, sign#3,communication#2, social relation#1, relation#1, abstraction#6 }Thus, the word token money is replaced by 41 synset tokens.
Then, the same two-passprocess just described is performed over the text consisting of the replacement tokens.Although this introduces noise due to ambiguity, the conditional-probability selectionscheme (Wiebe, McKeever, and Bruce 1998) compensates by selecting hypernym synsetsthat tend to co-occur with specific roles.6 The 20% threshold is a heuristic that is fixed for all experiments.
We tested automatic threshold derivationfor Senseval-3 and found that the optimal percentage differed across training sets.
As values near 20%were common, it is left fixed rather than adding an additional feature-threshold refinement step.165Computational Linguistics Volume 35, Number 2Note that there is no preference in the system for choosing either specific or generalhypernyms.
Instead, they are inferred automatically based on the word to be disam-biguated (i.e., preposition for these experiments).
Hypernyms at the top levels of thehierarchy are less likely to be chosen, as they most likely occur with different senses forthe same word (as with relation#1 previously).
However, hypernyms at lower levelstend not to be chosen, as there might not be enough occurrences due to other co-occurring words.
For example, wealth#4 is unlikely to be chosen as a collocation forthe second sense of money, as only a few words map into it, unlike property#2.
Theconditional-probability selection scheme (i.e., Equation (1)) handles this automaticallywithout having to encode heuristics about hypernym rank, and so on.3.1.2 Classification Experiments.
A supervised approach for word-sense disambiguationis used following Bruce and Wiebe (1999).For each experiment, stratified 10-fold cross validation is used: The classifiers arerepeatedly trained on 90% of the data and tested on the remainder, with the test setsrandomly selected to form a partition.
The results described here were obtained usingthe settings in Figure 4, which are similar to the settings used by O?Hara et al (2004)in the third Senseval competition.
The top systems from recent Senseval competitions(Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD.
Words in the im-mediate context (Word?i) and their parts of speech (POS?i) are standard features.
Wordcollocations are also common, but there are various ways of organizing collocations intofeatures (Wiebe, McKeever, and Bruce 1998).
We use the simple approach of having asingle binary feature per sense (e.g., role) that is set true whenever any of the associatedcollocation words for that sense are encountered (i.e., per-class-binary).The main difference of our approach from more typical WSD systems (Mihalcea,Chklovski, and Kilgarriff 2004) concerns the hypernym collocations.
The collocationcontext section of Figure 4 shows that word collocations can occur anywhere in thesentence, whereas hypernym collocations must occur within five words of the targetFeatures:Prep: preposition being classifiedPOS?i: part-of-speech of word at offset iWord?i: stem of word at offset iWordCollr: context has word collocation for role rHypernymCollr: context has hypernym collocation for role rCollocation context:Word: anywhere in the sentenceHypernym: within 5 words of target prepositionCollocation selection:Frequency: f (word) > 1Conditional probability: P(C|coll) ?
.50Relative percent change: (P(C|coll)?
P(C))/P(C) ?
.20Organization: per-class-binaryModel selection:C4.5 Decision tree via Weka?s J4.8 classifier (Quinlan 1993; Witten and Frank 1999)Figure 4Feature settings used in preposition classification experiments.
Aspects that differ from a typical WSDsystem are italicized.166O?Hara and Wiebe Exploiting Resources for Preposition Disambiguationprepositions (i.e., a five-word context window).7 This reduced window size is usedto make the hypernym collocations more related to the prepositional object and themodified term.The feature settings in Figure 4 are used in three different configurations: word-based collocations alone, hypernym collocations alone, and both collocations together.Combining the two types generally produces the best results, because this balances thespecific clues provided by the word collocations with the generalized clues provided bythe hypernym collocations.Unlike the general case for WSD, the sense inventory is the same for all the wordsbeing disambiguated; therefore, a single classifier can be produced rather than indi-vidual classifiers.
This has the advantage of allowing more training data to be usedin the derivation of the clues indicative of each semantic role.
However, if there weresufficient annotations for particular preposition, then it would be advantageous to havea dedicated classifier.
For example, the prior probabilities for the roles would be basedon the usages for the given preposition.
Therefore, we perform experiments illustratingthe difference when disambiguating prepositions with a single classifier versus the useof separate classifiers.3.2 Penn Treebank Classification ExperimentsThe first set of experiments deals with preposition disambiguation using PTB.
Whenderiving training data from PTB via the parse tree annotations, the functional tags as-sociated with prepositional phrases are converted into preposition sense tags.
Considerthe following excerpt from the sample annotation for PTB shown earlier:(6)(S (PP-TMP In (NP 1982)), temporal extent(NP-SBJ grammatical subject(NP (NP (NP Sports) & (NP Recreation) ?s)managers) ...Treating temporal as the preposition sense yields the following annotation:(7) InTMP 1982, Sports & Recreation?s managers ...The relative frequencies of the roles in the PTB annotations for PPs are shown in Ta-ble 9.
As can be seen, several of the roles do not occur often with PPs (e.g., extent).
Thissomewhat skewed distribution makes for an easier classification task than the one forFrameNet.3.2.1 Illustration with ?at.?
As an illustration of the probabilities associated with class-based collocations, consider the differences in the prior versus class-based conditionalprobabilities for the semantic roles of the preposition at in the Penn Treebank (ver-sion II).
Table 10 shows the global probabilities for the roles assigned to at, along with7 This window size was chosen after estimating that on average the prepositional objects occur within2.3 ?
1.26 words of the preposition and that the average attachment site is within 3.0 ?
2.98 words.
Thesefigures were produced by analyzing the parse trees for the semantic role annotations in the PTB.167Computational Linguistics Volume 35, Number 2Table 9Penn Treebank semantic roles for PPs.
Omits low-frequency benefactive relation.
Freq.
is the relativefrequency of the role occurrence (36,476 total instances).
Example usages are taken fromthe corpus.Role Freq.
Examplelocative .472 workers at a factorytemporal .290 expired atmidnight Tuesdaydirection .149 has grown at a sluggish pacemanner .050 CDs aimed at individual investorspurpose .030 opened for tradingextent .008 declined by 14%conditional probabilities for these roles given that certain high-level WordNet synsetsoccur in the context.
In a context referring to a concrete concept (i.e., entity#1), thedifference in the probability distributions for the locative and temporal roles shows thatthe locative interpretation becomes even more likely.
In contrast, in a context referringto an abstract concept (i.e., abstraction#6), the difference in the probability distributionsfor the same roles shows that the temporal interpretation becomes more likely.
Therefore,these class-based lexical associations capture commonsense usages of the preposition at.3.2.2 Results.
The classification results for these prepositions in the Penn Treebank showthat this approach is very effective.
Table 11 shows the accuracy when disambiguatingthe 14 prepositions using a single classifier with 6 roles.
Table 11 also shows the per-class statistics, showing that there are difficulties tagging the manner role (e.g., lowestF-score).
For the single-classifier case, the overall accuracy is 89.3%, using Weka?s J4.8classifier (Witten and Frank 1999), which is an implementation of Quinlan?s (1993) C4.5decision tree learner.For comparison, Table 12 shows the results for individual classifiers created for theprepositions annotated in PTB.
A few prepositions only have small data sets, such asof which is used more for specialization relations (e.g., category) than thematic ones.This table is ordered by entropy, which measures the inherent ambiguity in the classesas given by the annotations.
Note that the Baseline column is the probability of the mostfrequent sense, which is a common estimate of the lower bound for classificationTable 10Prior and posterior probabilities of roles for ?at?
in the Penn Treebank.
P(R) is the relative frequency.P(R|S) is the probability of the relation given that the synset occurs in the immediate context ofat.
RPCR,S is the relative percentage change: (P(R|S)?
P(R))/P(R).Synsetentity#1 abstraction#6Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,Slocative 73.5 75.5 0.03 67.0 ?0.09temporal 23.9 22.5 ?0.06 30.6 0.28manner 2.0 1.5 ?0.25 2.0 0.00direction 0.6 0.4 ?0.33 0.4 ?0.33168O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 11Overall preposition disambiguation results over Penn Treebank roles.
A single classifier is used for allthe prepositions.
# Instances is the number of role annotations.
# Classes is the number of distinctroles.
Entropy measures non-uniformity of the role distributions.
Baseline is estimated by themost-frequent role.
The Word Only experiment uses just word collocations, Hypernym Only justuses hypernym collocations, and Both uses both types of collocations.
Accuracy is average forpercent correct over ten trials in cross validation.
STDEV is the standard deviation over the trials.Experiment Accuracy STDEVWord Collocations Only 88.1 0.88Hypernym Collocations Only 88.2 0.43Both Collocations 89.3 0.33Data Set Characteristics# Instances: 27,308# Classes: 6Entropy: 1.831Baseline: 49.2Word Only Hypernym Only BothClass Prec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
Fdirection .953 .969 .960 .952 .967 .959 .956 .965 .961extent .817 .839 .826 .854 .819 .834 .817 .846 .829locative .879 .967 .921 .889 .953 .920 .908 .932 .920manner .797 .607 .687 .790 .599 .680 .826 .558 .661purpose .854 .591 .695 .774 .712 .740 .793 .701 .744temporal .897 .776 .832 .879 .794 .834 .845 .852 .848Table 12Per-preposition disambiguation results over Penn Treebank roles.
A separate classifier is used for eachpreposition, excluding roles with less than 1% relative frequency.
Freq gives the prepositionfrequency, and Roles the number of senses.
Entropy measures data set uniformity, and Baselineselects most common role.
The Word and Hypernym columns show results when including justword and hypernym collocations respectively, whereas Both includes both types.
Each columnshows averages for percent correct over ten trials.
The Mean row averages the values of theindividual experiments.Prep Freq.
Roles Entropy Baseline Word Hypernym Boththrough 331 4 1.668 0.438 59.795 62.861 58.592by 1290 7 1.575 0.479 87.736 88.231 86.655as 220 3 1.565 0.405 95.113 96.377 96.165between 87 4 1.506 0.483 77.421 81.032 70.456of 30 3 1.325 0.567 63.182 82.424 65.606out 76 4 1.247 0.711 70.238 76.250 63.988for 1401 6 1.189 0.657 82.444 85.795 80.158on 1915 5 1.181 0.679 85.998 88.720 79.428in 14321 7 1.054 0.686 86.404 92.647 86.523throughout 59 2 0.998 0.525 61.487 35.949 63.923at 2825 5 0.981 0.735 84.178 90.265 85.561across 78 2 0.706 0.808 75.000 78.750 77.857from 1521 5 0.517 0.917 91.649 91.650 91.650to 3074 5 0.133 0.985 98.732 98.537 98.829Mean 1944.8 4.43 1.12 0.648 80.0 82.1 78.9169Computational Linguistics Volume 35, Number 2experiments.
When using preposition-specific classifiers, the hypernym collocationssurprisingly outperform the other configurations, most likely due to overfitting withword-based clues: 82.1% versus 80.0% for the word-only case.3.3 FrameNet Classification ExperimentsThe second set of experiments perform preposition disambiguation using FrameNet.A similar preposition word-sense disambiguation experiment is carried out over theFrameNet semantic role annotations involving prepositional phrases.
Consider the sam-ple annotation shown earlier:(8) Hewlett-Packard Co has rolled out a new range of ISDN connectivityenabling ?C FE=?Communicator?
PT=?NP?
?standalone workstations?/C?to ?C TARGET=?y??communicate?/C?
?C FE=?Medium?
PT=?PP?
?overpublic or private ISDN networks?/C?.The prepositional phrase annotation is isolated and treated as the sense of the preposi-tion.
This yields the following sense annotation:(9) Hewlett-Packard Co has rolled out a new range of ISDN connectivityenabling standalone workstations to communicate overMedium public orprivate ISDN networks.Table 13 shows the distribution of common roles assigned to prepositional phrases.
Thetopic role is the most frequent case not directly covered in PTB.3.3.1 Illustration with ?at.?
See Table 14 for the most frequent roles out of the 124 casesthat were assigned to at, along with the conditional probabilities for these roles giventhat certain high-level WordNet synsets occur in the context.
In a context referringto concrete entities, the role place becomes more prominent.
However, in an abstractcontext, the role time becomes more prominent.
Thus, similar behavior to that noted forPTB in Section 3.2.1 occurs with FrameNet.3.3.2 Results.
Table 15 shows the results of classification when all of the prepositionsare classified together.
Due to the exorbitant number of roles (641), the overall resultsare low.
However, the combined collocation approach still shows slight improvement(23.3% versus 23.1%).
The FrameNet inventory contains many low-frequency relationsTable 13Most common FrameNet semantic roles for PPs.
Relative frequencies for roles assigned toprepositional phrases in version 1.3 (66,038 instances), omitting cases below 0.01.Role Freq.
Role Freq.
Role Freq.goal .092 theme .022 whole .015path .071 manner .021 individuals .013source .043 area .018 location .012topic .040 reason .018 ground .012time .037 addressee .017 means .011place .033 stimulus .017 content .011170O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 14Prior and posterior probabilities of roles for ?at?
in FrameNet.
Only the top 5 of 641 applicable rolesare shown.
P(R) is the relative frequency for relation.
P(R|S) is the probability of the relation giventhat the synset occurs in the immediate context of at.
RPCR,S is the relative percentage change:(P(R|S) ?
P(R))/P(R).Synsetentity#1 abstraction#6Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,Splace 15.6 19.0 21.8 16.8 7.7time 12.0 11.5 ?4.2 15.1 25.8stimulus 6.6 5.0 ?24.2 6.6 0.0addressee 6.1 4.4 ?27.9 3.3 ?45.9goal 5.5 6.3 14.5 6.0 9.1Table 15Preposition disambiguation with all FrameNet roles.
All 641 roles are considered.
Entropymeasuresdata set uniformity, and Baseline selects most common role.Experiment Accuracy STDEVWord Collocations Only 23.078 0.472Hypernym Collocations Only 23.206 0.467Both Collocations 23.317 0.556Data Set Characteristics# Instances: 65,550# Classes: 641Entropy: 6.785Baseline: 9.3that complicate this type of classification.
By filtering out relations that occur in less than1% of the role occurrences for prepositional phrases, substantial improvement results,as shown in Table 16.
Even with filtering, the classification is challenging (e.g., 18 classeswith entropy 3.82).
Table 16 also shows the per-class statistics, indicating that the meansand place roles are posing difficulties for classification.Table 17 shows the results when using individual classifiers, ordered by entropy.This illustrates that the role distributions are more complicated than those for PTB,yielding higher entropy values on average.
In all, there are over 360 prepositionswith annotations, 92 with ten or more instances each.
(Several of the low-frequencycases are actually adverbs, such as anywhere, but are treated as prepositions during theannotation extraction.)
The results show that the word collocations produce slightlybetter results: 67.8 versus 66.0 for combined collocations.
Unlike the case with PTB,the single-classifier performance is below that of the individual classifiers.
This isdue to the fine-grained nature of the role inventory.
When all the roles are consideredtogether, prepositions are sometimes being incorrectly classified using roles that havenot been assigned to them in the training data.
This occurs when contextual clues arestronger for a commonly used role than for the appropriate one.
Given PTB?s small roleinventory, this problem does not occur in the corresponding experiments.3.4 Factotum Classification ExperimentsThe third set of experiments deals with preposition disambiguation using Factotum.Note that Factotum does not indicate the way the relationships are expressed in English.171Computational Linguistics Volume 35, Number 2Table 16Overall results for preposition disambiguation with common FrameNet roles.
Excludes roles with lessthan 1% relative frequency.
Entropymeasures data set uniformity, and Baseline selects mostcommon role.
Detailed per-class statistics are also included, averaged over the 10 folds.Experiment Accuracy STDEVWord Collocations Only 73.339 0.865Hypernym Collocations Only 73.437 0.594Both Collocations 73.544 0.856Data Set Characteristics# Instances: 32974# Classes: 18Entropy: 3.822Baseline: 18.4Word Only Hypernym Only BothClass Prec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
Faddressee .785 .332 .443 .818 .263 .386 .903 .298 .447area .618 .546 .578 .607 .533 .566 .640 .591 .613content .874 .618 .722 .895 .624 .734 .892 .639 .744goal .715 .766 .739 .704 .778 .739 .703 .790 .743ground .667 .386 .487 .684 .389 .494 .689 .449 .541individuals .972 .947 .959 .961 .945 .953 .938 .935 .936location .736 .524 .610 .741 .526 .612 .815 .557 .660manner .738 .484 .584 .748 .481 .584 .734 .497 .591means .487 .449 .464 .562 .361 .435 .524 .386 .441path .778 .851 .812 .777 .848 .811 .788 .849 .817place .475 .551 .510 .483 .549 .513 .474 .576 .519reason .803 .767 .784 .777 .773 .774 .769 .714 .738source .864 .980 .918 .865 .981 .919 .860 .978 .915stimulus .798 .798 .797 .795 .809 .802 .751 .752 .750theme .787 .811 .798 .725 .847 .779 .780 .865 .820time .585 .665 .622 .623 .687 .653 .643 .690 .664topic .831 .836 .833 .829 .842 .835 .856 .863 .859whole .818 .932 .871 .807 .932 .865 .819 .941 .875Similarly, WordNet does not indicate this, but it does include definition glosses.
Forexample,(10)Factotum:?drying, is-function-of , drier?WordNet:dryalter remove the moisture from and make drydryerappliance an appliance that removes moistureThese definition glosses might be useful in certain cases for inferring the relation markers(i.e., generalized case markers).
As is, Factotum cannot be used to provide training datafor learning how the relations are expressed in English.
This contrasts with corpus-based annotations, such as PTB (Marcus et al 1994) and FrameNet (Fillmore, Wooters,and Baker 2001), where the relationships are marked in context.3.4.1 Inferring Semantic Role Markers.
To overcome the lack of context in Factotum, therelation markers are inferred through corpus checks, in particular through proximitysearches involving the source and target terms from the relationship (i.e., ?source,172O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 17Per-preposition disambiguation results over FrameNet roles.
A separate classifier is used for eachpreposition, excluding roles with less than 1% relative frequency.
Freq gives the prepositionfrequency, and Roles the number of senses.
Entropy measures data set uniformity, and Baselineselects most common role.
The Word and Hypernym columns show results when including justword and hypernym collocations, respectively, whereas Both includes both types.
Each columnshows averages for percent correct over ten trials.
The Mean row averages the values of theindividual experiments.Prep Freq.
Roles Entropy Baseline Word Hypernym Bothwith 3758 25 4.201 19.6 59.970 57.809 61.924of 7339 22 4.188 12.8 85.747 84.663 85.965between 675 23 4.166 11.4 61.495 56.215 53.311under 286 26 4.045 25.5 29.567 33.040 33.691against 557 26 4.028 21.2 53.540 58.885 31.892for 2678 22 3.988 22.6 58.135 58.839 39.809by 3348 18 3.929 13.6 62.618 60.854 61.152on 3579 22 3.877 18.1 61.011 57.671 60.838at 2685 21 3.790 21.2 61.814 58.501 57.630in 6071 18 3.717 18.7 54.253 49.953 53.880as 1123 17 3.346 27.1 53.585 47.186 42.722to 4741 17 3.225 36.6 71.963 77.751 72.448behind 254 13 3.222 22.8 47.560 41.045 43.519over 1157 16 3.190 27.8 47.911 48.548 50.337after 349 16 2.837 45.8 62.230 65.395 61.944around 772 15 2.829 45.1 52.463 52.582 49.357from 3251 14 2.710 51.2 73.268 71.934 75.423round 389 12 2.633 34.7 46.531 50.733 49.393into 1923 14 2.208 62.9 79.175 77.366 80.846during 242 10 2.004 63.6 71.067 75.200 68.233like 570 9 1.938 62.3 82.554 79.784 85.666through 1358 10 1.905 66.0 77.800 77.798 79.963up 745 10 1.880 60.3 76.328 76.328 74.869off 647 9 1.830 63.8 90.545 86.854 90.423out 966 8 1.773 60.7 77.383 79.722 78.671across 894 11 1.763 67.6 80.291 80.095 80.099towards 673 10 1.754 67.9 65.681 71.171 65.517down 965 7 1.600 63.2 81.256 81.466 79.141along 723 9 1.597 72.5 87.281 86.862 86.590about 1894 8 1.488 72.2 83.214 76.663 83.899back 405 7 1.462 64.7 88.103 91.149 86.183past 275 9 1.268 78.9 85.683 86.423 85.573Mean 1727.9 14.8 2.762 43.8 67.813 67.453 65.966relation, target?).
For example, using AltaVista?s Boolean search,8 this can be done via?source NEAR target.
?Unfortunately, this technique would require detailed post-processing of the Websearch results, possibly including parsing, in order to extract the patterns.
As an ex-pedient, common prepositions9 are included in a series of proximity searches to find8 AltaVista?s Boolean search is available at www.altavista.com/sites/search/adv.9 The common prepositions are determined from the prepositional phrases assigned functionalannotations in the Penn Treebank (Marcus et al 1994).173Computational Linguistics Volume 35, Number 2the preposition occurring most frequently with the given terms.
For instance, given therelationship ?drying, is-function-of, drier?, the following searches would be performed.
(11) drying NEAR drier NEAR indrying NEAR drier NEAR to...drying NEAR drier NEAR ?around?To account for prepositions that occur frequently (e.g., of ), pointwise mutual infor-mation (MI) statistics (Manning and Schu?tze 1999, pages 66?68) are used in place of theraw frequency when rating the potential markers.
These are calculated as follows:MIprep = log2P(X,Y)P(X)?
P(Y)?
log2f (source NEAR target NEAR prep)f (source NEAR target)?
f (prep)(2)Such checks are done for the 25 most common prepositions to find the prepositionyielding the highest mutual information score.
For example, the top three markers forthe ?drying, is-function-of, drier?
relationship based on this metric are during, after, andwith.3.4.2 Method for Classifying Functional Relations.
Given the functional relationships inFactotum along with the inferred relation markers, machine-learning algorithms canbe used to infer what relation most likely applies to terms occurring together with aparticular marker.
Note that the main purpose of including the relation markers is toprovide clues for the particular type of relation.
Because the source term and targetterms might occur in other relationships, associations based on them alone might notbe as accurate.
In addition, the inclusion of these clue words (e.g., the prepositions)makes the task closer to what would be done in inferring the relations from free text.The task thus approximates preposition disambiguation, using the Factotum relationsas senses.Figure 5 gives the feature settings used in the experiments.
This is a version ofthe feature set used in the PTB and FrameNet experiments (see Figure 4), simplified toaccount for the lack of sentential context.
Figure 6 contains sample feature specificationsfrom the experiments discussed in the next section.
The top part shows the originalrelationships from Factotum; the first example indicates that connaturalize causes simi-larity.
Also included is the most likely relation marker inferred for each instance.
Thisshows that ?n/a?
is used whenever a preposition for a particular relationship cannot beinferred.
This happens in the first example because connaturalize is a rare term.The remaining parts of Figure 6 illustrate the feature values that would be derivedfor the three different experiment configurations, based on the inclusion of word and/orhypernym collocations.
In each case, the classification variable is given by relation.For brevity, the feature specification only includes collocation features for the mostfrequent relations.
Sample collocations are also shown for the relations (e.g., vulgar-ity for is-caused-by).
In the word collocation case, the occurrence of similarity is usedto determine that the is-caused-by feature (WC1) should be positive (i.e., ?1?)
for thefirst two instances.
Note that there is no corresponding hypernym collocation due toconditional probability filtering.
In addition, although new is not included as a wordcollocation, one of its hypernyms, namely Adj:early#2, is used to determine that thehas-consequence feature (HC3) should be positive in the last instance.174O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationContext:Source and target terms from relationship (?source, relation, target?
)Features:POSsource: part-of-speech of the source termPOStarget: part-of-speech of the target termPrep: preposition serving as relation marker (?n/a?
if not inferable)WordCollr: 1 iff context contains any word collocation for relation rHypernymCollr: 1 iff context contains any hypernym collocation for relation rCollocation selection:Frequency: f (word) > 1Relative percent change: (P(C|coll)?
P(C))/P(C) ?
.20Organization: per-class-binary groupingModel selection:Decision tree using Weka?s J4.8 classifier (Witten and Frank 1999)Figure 5Features used in Factotum role classification experiments.
Simplified version of Figure 4: Contextonly consists of the source and target terms.3.4.3 Results.
To make the task more similar to the PTB and FrameNet cases coveredpreviously, only the functional relations in Factotum are used.
These are determinedby removing the hierarchical relations (e.g., has-subtype and has-part) along with theattribute relations (e.g., is-property-of ).
In addition, in cases where there are inversefunctions (e.g., causes and is-caused-by), the most frequently occurring relation of eachinverse pair is used.
This is done because the relation marker inference approach doesnot account for argument order.
The boldface relations in the listing shown earlier inTable 5 are those used in the experiment.
Only single-word source and target terms areconsidered to simplify the WordNet hypernym lookup (i.e., no phrasals).
The resultingdata set has 5,959 training instances.
The data set alo includes the inferred relationmarkers (e.g., one preposition per training instance), thus introducing some noise.Figure 6 includes a few examples from this data set.
This shows that the originalrelationship ?similarity, is-caused-by, rhyme?
from Factotum is augmented with theby marker prior to classification.
Again, these markers are inferred via Web searchesinvolving the terms from the original relationship.Table 18 shows the results of the classification.
The combined use of both collocationtypes achieves the best overall accuracy at 71.2%, which is good considering that thebaseline of always choosing the most common relation (is-caused-by) is 24.2%.
This com-bination generalizes well by using hypernym collocations, while retaining specificityvia word collocations.
The classification task is difficult, as suggested by the numberof classes, entropy, and baseline values all being comparable to the filtered FrameNetexperiment (see Table 16).3.5 Common Relation Inventory Classification ExperimentsThe last set of experiments investigate preposition disambiguation using FrameNetmapped into a reduced semantic role inventory.
For the application to lexical acqui-sition, the semantic role annotations are converted into the common relation inventorydiscussed in Section 2.5.
To apply the common inventory to the FrameNet data, anno-tations using the 641 FrameNet relations (see Table 2) need to be mapped into those175Computational Linguistics Volume 35, Number 2Relationships from Factotum with inferred markers:Relationship Marker?similarity, is-caused-by, connaturalize?
n/a?similarity, is-caused-by, rhyme?
by?approximate, has-consequence, imprecise?
because?new, has-consequence, patented?
withWord collocations only:Relation POSs POSt Prep WC1 WC2 WC3 WC4 WC5 WC6 WC7is-caused-by NN VB n/a 1 0 0 0 0 0 0is-caused-by NN NN by 1 0 0 0 0 0 0has-consequence NN JJ because 0 0 0 0 0 0 0has-consequence JJ VBN with 0 0 0 0 0 0 0Sample collocations:is-caused-by {bitterness, evildoing, monochrome, similarity, vulgarity}has-consequence {abrogate, frequently, insufficiency, nonplus, ornament}Hypernym collocations only:Relation POSs POSt Prep HC1 HC2 HC3 HC4 HC5 HC6 HC7is-caused-by NN VB n/a 0 0 0 0 0 0 0is-caused-by NN NN by 0 0 0 0 0 0 0has-consequence NN JJ because 0 0 0 0 0 0 0has-consequence JJ VBN with 0 0 1 0 0 0 0Sample collocations:is-caused-by {N:hostility#3, N:inelegance#1, N:humorist#1}has-consequence {V:abolish#1, Adj:early#2, N:inability#1, V:write#2}Both collocations:Relation POSs POSt Prep WC1 ... WC7 HC1 HC2 HC3 ...is-caused-by NN VB n/a 1 ... 0 0 0 0 ...is-caused-by NN NN by 1 ... 0 0 0 0 ...has-consequence NN JJ because 0 ... 0 0 0 0 ...has-consequence JJ VBN with 0 ... 0 0 0 1 ...Legend:POSs & POSt are the parts of speech for the source and target terms; andWCr & HCr are the word and hypernym collocations as follows:1. is-caused-by 2. is-function-of 3. has-consequence 4. has-result5.
is-caused-bymental 6. is-performed-by 7. usesFigure 6Sample feature specifications for Factotum experiments.
Each relationship from Factotum isaugmented with one relational marker inferred via Web searches, as shown at top of figure.Three distinct sets of feature vectors are shown based on the type of collocation included,omitting features for low-frequency relations.176O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationTable 18Functional relation classification over Factotum.
This uses the relational source and target termswith inferred prepositions.
The accuracy figures are averages based on 10-fold cross validation.The gain in accuracy for the combined experiment versus the word experiment is statisticallysignificant at p < .01 (via a paired t-test).Experiment Accuracy STDEVWord Collocations Only 68.4 1.28Hypernym Collocations Only 53.9 1.66Both Collocations 71.2 1.78Data Set Characteristics# Instances: 5,959# Classes: 21Entropy: 3.504Baseline: 24.2using the 26 common relations shown in Table 8.
Results for the classification of theFrameNet data mapped into the common inventory are shown in Table 19.
As canbe seen, the performance is well above that of the full classification over FrameNetwithout filtering (see Table 15).
Although the low-frequency role filtering yields thehighest performance (see Table 16), this comes at the expense of having half of thetraining instances discarded.
Corpus annotations are a costly resource, so such wasteis undesirable.
Table 19 also shows the per-class statistics, indicating that the means,direction, and part roles are handled poorly by the classifier.
The latter two are due to therelatively small training examples for the roles in question, which can be addressedpartly by refining the mapping from FrameNet.
However, problems classifying themeans role occur with all classifiers discussed in this article, suggesting that that roleis too subtle to be classified with the feature set currently used.The results in Table 19 also illustrate that the reduced, common-role inventory hasan additional advantage of improving performance in the classification, compared to acascaded approach.
This occurs because several of the miscellaneous roles in FrameNetcover subtle distinctions that are not relevant for definition analysis (e.g., cognizer andaddressee).
The common inventory therefore strikes a balance between the overly generalroles in PTB, which are easy to classify, and the overly specialized roles in FrameNet,which are quite difficult to classify.
Nonetheless, a certain degree of classification diffi-culty is inevitable in order for the inventory to provide adequate coverage of the dif-ferent distinctions present in dictionary definitions.
Note that, by using the annotationsfrom PTB and FrameNet, the end result is a general-purpose classifier, not one tied intodictionary text.
Thus, it is useful for other tasks besides definition analysis.This classifier was used to disambiguate prepositions in the lexical acquisitionsystem we developed at NMSU (O?Hara 2005).
Evaluation of the resulting distinctionswas performed by having the output of the system rated by human judges.
Manu-ally corrected results were also evaluated by the same judges.
The overall ratings arenot high in both cases, suggesting that some of the distinctions being made are subtle.For instance, for ?counterintelligence achieved by deleting any information of value?from the definition of censoring, means is the preferred role for by, but manner is ac-ceptable.
Likewise, characteristic is the preferred role for of, but category is interpretable.Thus, the judges differed considerably on these cases.
However, as the ratings forthe uncorrected output were close to those for the corrected output, the approach ispromising to use for lexical acquisition.
If desired, the per-role accuracy results shownin Table 19 could be incorporated as confidence values assigned to particular relation-ships extracted from definitions (e.g., 81% for those with source but only 21% whenmeans used).177Computational Linguistics Volume 35, Number 24.
Related WorkThe main contribution of this article concerns the classification methodology (ratherthan the inventories for semantic roles), so we will only review other work relatedto this aspect.
First, we discuss similar work involving hypernyms.
Then, we addresspreposition classification proper.Scott and Matwin (1998) use WordNet hypernyms for text classification.
Theyinclude a numeric density feature for any synset that subsumes words appearing inthe document, potentially yielding hundreds of features.
In contrast, the hypernymcollocations discussed in Section 3.1.1 involve a binary feature for each of the relationsbeing classified, using indicative synsets based on the conditional probability test.
Thistest alleviates the need for their maximum height parameter to avoid overly generalhypernyms.
Their approach, as well as ours, considers all senses of a word, distrib-uting the alternative readings throughout the set of features.
In comparison, GildeaTable 19Results for preposition disambiguation with common roles.
The FrameNet annotations are mappedinto the common inventory from Table 8.
Entropymeasures data set uniformity, and Baselineselects most common role.
Detailed per-class statistics are also included, averaged over the10 folds.Experiment Accuracy STDEVWord Collocations Only 62.9 0.345Hypernym Collocations Only 62.6 0.487Both Collocations 63.1 0.639Data Set Characteristics# Instances: 59,615# Classes: 24Entropy: 4.191Baseline: 12.2Word Only Hypernym Only BothClass Prec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
Faccompaniment .630 .611 .619 .671 .605 .636 .628 .625 .626agent .623 .720 .667 .639 .726 .677 .616 .731 .668area .546 .475 .508 .541 .490 .514 .545 .501 .522category .694 .706 .699 .695 .700 .697 .714 .718 .716cause .554 .493 .521 .569 .498 .531 .540 .482 .509characteristic .595 .468 .523 .607 .474 .530 .584 .490 .532context .569 .404 .472 .577 .388 .463 .568 .423 .485direction .695 .171 .272 .701 .189 .294 .605 .169 .260duration .601 .465 .522 .589 .445 .503 .596 .429 .497experiencer .623 .354 .449 .606 .342 .435 .640 .378 .474goal .664 .683 .673 .662 .674 .668 .657 .680 .668instrument .406 .339 .367 .393 .337 .360 .405 .370 .385location .433 .557 .487 .427 .557 .483 .417 .553 .475manner .493 .489 .490 .483 .478 .479 .490 .481 .485means .235 .183 .205 .250 .183 .210 .254 .184 .212medium .519 .306 .382 .559 .328 .412 .529 .330 .403part .539 .289 .368 .582 .236 .323 .526 .301 .380path .705 .810 .753 .712 .813 .759 .706 .795 .748product .837 .750 .785 .868 .739 .788 .769 .783 .770recipient .661 .486 .559 .661 .493 .563 .642 .482 .549resource .613 .471 .530 .614 .458 .524 .618 .479 .539source .703 .936 .802 .697 .936 .799 .707 .937 .806theme .545 .660 .596 .511 .661 .576 .567 .637 .600time .619 .624 .621 .626 .612 .619 .628 .611 .619178O?Hara and Wiebe Exploiting Resources for Preposition Disambiguationand Jurafsky (2002) instead just select the first sense for their hypernym features forrelation classification.
They report marginal improvements using the features, whereasconfigurations with hypernym collocations usually perform best in our prepositiondisambiguation experiments.Mohit and Narayanan (2003) use WordNet hypernyms to generalize patterns forinformation extraction inferred from FrameNet annotations by distributing supportfrom terms co-occurring in annotations for frame elements to the terms for hypernyms.However, they do not incorporate a filtering stage, as with our conditional probabilitytest.
Mihalcea (2002) shows how hypernym information can be useful in deriving cluesfor unsupervised WSD.
Patterns for co-occurring words of a given sense are inducedfrom sense-tagged corpora.
Each pattern specifies templates for the co-occurring wordsin the immediate context window of the target word, as well as their correspondingsynsets if known (e.g., sense tagged or unambiguous), and similarly the hypernymsynsets if known.
To disambiguate a word, the patterns for each of its senses areevaluated in the context, and the sense with the most support is chosen.The work here addresses relation disambiguation specifically with respect to thoseindicated by prepositional phrases (i.e., preposition word-sense disambiguation).
Untilrecently, there has been little work on general-purpose preposition disambiguation.Litkowski (2002) and Srihari, Niu, and Li (2001) present approaches using manuallyderived rules.
Both approaches account for only a handful of prepositions; in contrast,for FrameNet we disambiguate 32 prepositions via individual classifiers and over 100prepositions via the combined classifier.
Liu and Soo (1993) present a heuristic approachfor relation disambiguation relying upon syntactic clues as well as occurrence of specificprepositions.
They assign roles to constituents of a sentence from corpus data providedthat sufficient instances are available.
Otherwise, a human trainer is used to answerquestions needed by the system for the assignment.
They report an 86% accuracy ratefor the assignment of roles to verbal arguments in about 5,000 processed sentences.Alam (2004) sketches out how the preposition over might be disambiguated into oneof a dozen roles using features based on the head and complement, such as whether thehead is amovement verb orwhether the complement refers to a duration.
These featuresform the basis for a manually-constructed decision tree, which is interpreted by hand inan evaluation over sentences from the British National Corpus (BNC), giving a precisionof 93.5%.
Boonthum, Toida, and Levinstein (2006), building upon the work of Alam,show how WordNet can be used to automate the determination of similar head andcomplement properties.
For example, if both the head and complement refer to people,with should be interpreted as accompaniment.
These features form the basis for adisambiguation system using manually constructed rules accounting for ten commonlyoccurring prepositions.
They report a precision of 79% with a recall of 76% over aninventory of seven roles in a post hoc evaluation that allows for partial correctness.There have been a fewmachine-learning approaches that are more similar to the ap-proach used here.
Gildea and Jurafsky (2002) perform relation disambiguation using theFrameNet annotations as training data.
They include lexical features for the headwordof the phrase and the predicating word for the entire annotated frame (e.g., the verbcorresponding to the frame under which the annotations are grouped).
They also useseveral features derived from the output of a parser, such as the constituent type of thephrase (e.g., NP), the grammatical function (e.g., subject), and a path feature listing part-of-speech tags from the target word to the phrase being tagged.
They report an accuracyof 78.5% with a baseline of 40.6% over the FrameNet semantic roles.
However, byconditioning the classification on the predicatingword, the range of roles for a particularclassification instance is more limited than in the experiments presented in this article.179Computational Linguistics Volume 35, Number 2Blaheta and Charniak (2000) use the PTB annotations for relation disambiguation.
Theyuse a few parser-derived features, such as the constituent labels for nearby nodes andpart-of-speech for parent and grandparent nodes.
They also include lexical features forthe head and alternative head (because prepositions are considered as the head by theirparser).
As their classifier tags all adjuncts, they include the nominal and adverbial roles,which are syntactic and more predictable than the roles occurring with prepositionalphrases.There have been recent workshops featuring competitions for semantic role tagging(Carreras and Ma`rquez 2004, 2005; Litkowski 2004).
A common approach is to tag allthe semantic roles in a sentence at the same time to account for dependencies, suchas via Hidden Markov Models.
To take advantage of accurate Support Vector Machineclassification, Pradhan et al (2005) instead use a postprocessing phrase based on trigrammodels of roles.
Their system incorporates a large variety of features, building upon sev-eral different preceding approaches, such as including extensions to the path featuresfrom Gildea and Jurafsky (2002).
Their lexical features include the predicate root word,headwords for the sentence constituents and PPs, as well as their first and last words.Koomen et al (2005) likewise use a large feature set.
They use an optimization phase tomaximize satisfaction of the constraints imposed by the PropBank data set, such as thenumber of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1).Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain thehypernyms selected to serve as collocations, building upon our earlier work (O?Haraand Wiebe 2003).
They report 87.7% accuracy in a setup similar to ours over PTB(i.e., a gain of 2 percentage points).
They use a different type of collocation featurethan ours: having a binary feature for each potential collocation rather than a singlefeature per class.
That is, they useOver-Range Binary rather than Per-Class Binary (Wiebe,McKeever, and Bruce 1998).
Moreover, they include several hundred of these features,rather than our seven (benefactive previously included), which is likely the main sourceof improvement.
Again, the per-class binary organization is a bag of words approach,so it works well only with a limited number of potential collocations.
Follow-up workof theirs (Ye and Baldwin 2007) fared well in the recent preposition disambiguationcompetition, held as part of SemEval-2007 (Litkowski and Hargraves 2007).
Thus, animmediate area for future work will be to incorporate such improved feature sets.
Wewill also investigate addressing sentential role constraints as in general semantic roletagging.5.
ConclusionThis article shows how to exploit semantic role resources for preposition disambigua-tion.
Information about two different types of semantic role resources is provided.
Theemphasis is on corpus-based resources providing annotations of naturally occurringtext.
The Penn Treebank (Marcus et al 1994) covers general roles for verbal adjuncts andFrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specificroles for all verbal arguments.
In addition, semantic role inventories from knowledgebases are investigated.
Cyc (Lehmann 1996) provides fine-grained role distinctions,Factotum (Cassidy 2000) includes a variety of functional relations, and work in Concep-tual Graphs (Sowa 1999) emphasizes roles for attributes.
Relations from both types ofresources are considered when developing the inventory of relations used for definitionanalysis, as shown in Table 8.The disambiguation concentrates on relations indicated by prepositional phrases,and is framed as word-sense disambiguation for the preposition in question.
A new180O?Hara and Wiebe Exploiting Resources for Preposition Disambiguationtype of feature for word-sense disambiguation is introduced, using WordNet hyper-nyms as collocations rather than just words, as is typically done.
The full feature set isshown in Figure 4.
Various experiments over the PTB and FrameNet data are presented,including prepositions classified separately versus together, and illustrating the effectsof filtering.
The main results in Tables 11 and 16 show that the combined use of wordand hypernym collocations generally achieves the best performance.
For relationshipsderived from knowledge bases, the prepositions and other relational markers need tobe inferred from corpora.
A method for doing this is demonstrated using Factotum,with results shown in Table 18.
In addition, to account for granularity differences in thesemantic role inventories, the relations are mapped into a common inventory that wasdeveloped based on the inventories discussed in the article.
This allows for improvedclassification in cases where inventories provide overly specialized relations, such asthose in FrameNet.
Classification results are shown in Table 19.The recent competitions on semantic relation labeling have highlighted the useful-ness of incorporating a variety of clues for general-purpose relation disambiguation(Carreras and Ma`rquez 2005).
Some of the techniques developed here for prepositiondisambiguation can likely help with relation disambiguation in general.
For instance,there are quite a few lexical features, such as in Pradhan et al (2005), which could beextended to use semantic classes as with our hypernym collocations.
In general it seemsthat, when lexical features are used in supervised machine learning, it is likely thatcorresponding class-based features based on hypernyms can be beneficial for improvedcoverage.Other aspects of this approach are geared specifically to our goal of supportinglexical acquisition from dictionaries, which was the motivation for the emphasis onpreposition disambiguation.
Isolating the preposition annotations allows the classifiersto be more readily tailored to definition analysis, especially because predicate framesare not assumed as with other FrameNet relation disambiguation.
Future work willinvestigate combining the general relation classifiers with preposition disambiguationclassifiers, such as is done in Ye and Baldwin (2006).
Future work will also investigateimprovements to the application to definition analysis.
Currently, FrameNet roles arealwaysmapped to the same common inventory role (e.g., place to location).
However, thisshould account for the frame of the annotation and perhaps other context information.Lastly, we will also look for more resources to exploit for preposition disambiguation(e.g., ResearchCyc).AcknowledgmentsThe experimentation for this article wasgreatly facilitated though the use ofcomputing resources at New Mexico StateUniversity.
We are also grateful for theextremely helpful comments providedby the anonymous reviewers.ReferencesAlam, Yukiko Sasaki.
2004.
Decision treesfor sense disambiguation of prepositions:Case of over.
In Proceedings of theComputational Lexical Semantics Workshop,pages 52?59, Boston, MA.Barker, Ken.
1998.
Semi-Automatic Recognitionof Semantic Relationships in English TechnicalTexts.
Ph.D. thesis, Department ofComputer Science, University of Ottawa.Bies, Ann, Mark Ferguson, Karen Katz,Robert MacIntyre, Victoria Tredinnick,Grace Kim, Mary Ann Marcinkiewicz,and Britta Schasberger.
1995.
Bracketingguidelines for Treebank II style:Penn Treebank project.
TechnicalReport MS-CIS-95-06, University ofPennsylvania.Blaheta, Don and Eugene Charniak.
2000.Assigning function tags to parsed text.In Proceedings of the 1st Annual Meetingof the North American Chapter of theAmerican Association for ComputationalLinguistics (NAACL-2000), pages234?240,Seattle, WA.181Computational Linguistics Volume 35, Number 2Boonthum, Chutima, Shunichi Toida, andIrwin B. Levinstein.
2006.
Prepositionsenses: Generalized disambiguationmodel.
In Proceedings of the SeventhInternational Conference on ComputationalLinguistics and Intelligent Text Processing(CICLing-2006), pages 196?207,Mexico City.Bruce, Bertram.
1975.
Case systems fornatural language.
Artificial Intelligence,6:327?360.Bruce, Rebecca and Janyce Wiebe.
1999.Decomposable modeling in naturallanguage processing.
ComputationalLinguistics, 25(2):195?208.Carreras, Xavier and Llu?
?s Ma`rquez.
2004.Introduction to the CoNLL-2004 sharedtask: Semantic role labeling.
In Proceedingsof the Eighth Conference on Natural LanguageLearning (CoNLL-2004), pages 89?97,Boston, MA.Carreras, Xavier and Llu?
?s Ma`rquez.
2005.Introduction to the CoNLL-2005 sharedtask: Semantic role labeling.
In Proceedingsof the Ninth Conference on Natural LanguageLearning (CoNLL-2005), pages 152?164,Ann Arbor, MI.Cassidy, Patrick J.
2000.
An investigationof the semantic relations in the Roget?sThesaurus: Preliminary results.
InProceedings of the First InternationalConference on Intelligent Text Processingand Computational Linguistics(CICLing-2000), pages 181?204,Mexico City.Cruse, David A.
1986.
Lexical Semantics.Cambridge University Press, Cambridge.Edmonds, Phil and Scott Cotton, editors.2001.
Proceedings of the Senseval 2 Workshop.Association for Computational Linguistics,Toulouse.Fillmore, Charles.
1968.
The case for case.In Emmon Bach and Rovert T. Harms,editors, Universals in Linguistic Theory.Holt, Rinehart and Winston, New York,pages 1?88.Fillmore, Charles J., Charles Wooters, andCollin F. Baker.
2001.
Building a largelexical databank which providesdeep semantics.
In Proceedings of thePacific Asian Conference on Language,Information and Computation, pages 3?25,Hong Kong.Frawley, William.
1992.
Linguistic Semantics.Lawrence Erlbaum Associates,Hillsdale, NJ.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Grozea, Cristian.
2004.
Finding optimalparameter settings for high performanceword sense disambiguation.
InSenseval-3: Third International Workshopon the Evaluation of Systems for theSemantic Analysis of Text, pages 125?128,Barcelona.Kilgarriff, Adam.
1998.
Senseval: An exercisein evaluating word sense disambiguationprograms.
In Proceedings of the FirstInternational Conference on LanguageResources and Evaluation (LREC ?98),pages 581?588, Granada.Koomen, Peter, Vasin Punyakanok, DanRoth, and Wen-tau Yih.
2005.
Generalizedinference with multiple semantic rolelabeling systems.
In Proceedings of theNinth Conference on Computational NaturalLanguage Learning (CoNLL), pages 181?184,Ann Arbor, MI.Lehmann, Fritz.
1996.
Big posets ofparticipatings and thematic roles.
InPeter W. Eklund, Gerard Ellis, andGraham Mann, editors, ConceptualStructures: Knowledge Representation asInterlingua, Springer-Verlag, Berlin,pages 50?74.Lenat, Douglas B.
1995.
Cyc: A large-scaleinvestment in knowledge infrastructure.Communications of the ACM, 38(11):33?38.Litkowski, Kenneth C. 2002.
Digraphanalysis of dictionary prepositiondefinitions.
In Proceedings of theSIGLEX/SENSEVAL Workshop on WordSense Disambiguation: Recent Successesand Future Directions, pages 9?16,Philadelphia, PA.Litkowski, Kenneth C. 2004.
Senseval-3 task:Automatic labeling of semantic roles.In Proceedings of Senseval-3: The ThirdInternational Workshop on the Evaluation ofSystems for the Semantic Analysis of Text,pages 9?12, Barcelona.Litkowski, Kenneth C. and Orin Hargraves.2006.
Coverage and inheritance in ThePreposition Project.
In Third ACL-SIGSEMWorkshop on Prepositions, pages 37?44,Trento.Litkowski, Kenneth C. and Orin Hargraves.2007.
SemEval-2007 task 06: Word-sensedisambiguation of prepositions.
InProceedings of the Fourth InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 24?29, Prague.Liu, Rey-Long and Von-Wun Soo.
1993.
Anempirical study on thematic knowledgeacquisition based on syntactic clues andheuristics.
In Proceedings of the 31st AnnualMeeting of the Association for Computational182O?Hara and Wiebe Exploiting Resources for Preposition DisambiguationLinguistics (ACL-93), pages 243?250,Columbus, OH.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press,Cambridge, MA.Marcus, Mitchell, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre,Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
ThePenn Treebank: Annotating predicateargument structure.
In Proceedingsof the ARPA Human LanguageTechnology Workshop, pages 110?115,Plainsboro, NJ.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Mihalcea, Rada.
2002.
Instance basedlearning with automatic featureselection applied to word sensedisambiguation.
In Proceedings ofthe 19th International Conference onComputational Linguistics (COLING-2002),pages 1?7, Taiwan.Mihalcea, Rada, Timothy Chklovski, andAdam Kilgarriff.
2004.
The Senseval-3English lexical sample task.
InSenseval-3: Third International Workshopon the Evaluation of Systems for theSemantic Analysis of Text, pages 25?28,Barcelona.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine Miller.
1990.
WordNet: Anon-line lexical database.
InternationalJournal of Lexicography, 3(4): SpecialIssue on WordNet.Miller, Katherine.
1998.
Modifiers inWordNet.
In Christiane Fellbaum,editor,WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA,pages 47?67.Mohit, Behrang and Srini Narayanan.2003.
Semantic extraction withwide-coverage lexical resources.
InCompanion Volume of the Proceedingsof HLT-NAACL 2003 - Short Papers,pages 64?66, Edmonton.O?Hara, Thomas P. 2005.
Empirical acquisitionof conceptual distinctions via dictionarydefinitions.
Ph.D. thesis, Departmentof Computer Science, New MexicoState University.O?Hara, Tom, Rebecca Bruce, Jeff Donner,and Janyce Wiebe.
2004.
Class-basedcollocations for word-sensedisambiguation.
In Proceedings ofSenseval-3: The Third InternationalWorkshop on the Evaluation of Systemsfor the Semantic Analysis of Text,pages 199?202, Barcelona.O?Hara, Tom and Janyce Wiebe.
2003.Classifying functional relations inFactotum via WordNet hypernymassociations.
In Proceedings of theFourth International Conference onIntelligent Text Processing andComputational Linguistics (CICLing-2003),pages 347?359, Mexico City.OpenCyc.
2002.
OpenCyc release 0.6b.Available at www.opencyc.org.Palmer, Martha, Dan Gildea, and PaulKingsbury.
2005.
The Proposition Bank:A corpus annotated with semantic roles.Computational Linguistics, 31(1):71?106.Palmer, Martha Stone.
1990.
SemanticProcessing for Finite Domains.
CambridgeUniversity Press, Cambridge.Pradhan, Sameer, Kadri Hacioglu, ValerieKrugler, Wayne Ward, James H. Martin,and Daniel Jurafsky.
2005.
Supportvector learning for semantic argumentclassification.Machine Learning,60(1?3):11?39.Quinlan, J. Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann,San Mateo, CA.Scott, Sam and Stan Matwin.
1998.
Textclassification using WordNet hypernyms.In Proceedings of the COLING/ACLWorkshop on Usage of WordNet in NaturalLanguage Processing Systems, pages 38?44,Montreal.Somers, Harold L. 1987.
Valency and Case inComputational Linguistics.
EdinburghUniversity Press, Scotland.Sowa, John F. 1984.
Conceptual Structures inMind and Machines.
Addison-Wesley,Reading, MA.Sowa, John F. 1999.
Knowledge Representation:Logical, Philosophical, and ComputationalFoundations.
Brooks Cole Publishing,Pacific Grove, CA.Srihari, Rohini, Cheng Niu, and Wei Li.2001.
A hybrid approach for namedentity and sub-type tagging.
InProceedings of the 6th Applied NaturalLanguage Processing Conference,pages 247?254, Seattle.Wiebe, Janyce, Kenneth McKeever, andRebecca F. Bruce.
1998.
Mappingcollocational properties into machinelearning features.
In Proceedings of the6th Workshop on Very Large Corpora(WVLC-98), pages 225?233, Montreal.183Computational Linguistics Volume 35, Number 2Wilks, Yorick, Brian M. Slator, and LouiseGuthrie.
1996.
Electric Words.
MIT Press,Cambridge, MA.Witten, Ian H. and Eibe Frank.
1999.Data Mining: Practical MachineLearning Tools and Techniques with JavaImplementations.
Morgan Kaufmann,San Francisco, CA.Ye, Patrick and Timothy Baldwin.
2006.Semantic role labeling of prepositionalphrases.
ACM Transactions on AsianLanguage Information Processing (TALIP),5(3):228?244.Ye, Patrick and Timothy Baldwin.2007.
MELB-YB: Preposition sensedisambiguation using rich semanticfeatures.
In Proceedings of the FourthInternational Workshop on SemanticEvaluations (SemEval-2007),pages 241?244, Prague.184This article has been cited by:1.
Timothy Baldwin, Valia Kordoni, Aline Villavicencio.
2009.
Prepositions in Applications:A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey andIntroduction to the Special Issue.
Computational Linguistics 35:2, 119-149.
[Citation] [PDF][PDF Plus]
