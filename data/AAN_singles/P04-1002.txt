Constructivist Development ofGrounded Construction GrammarsLuc SteelsUniversity of Brussels (VUB AI Lab)SONY Computer Science Lab - Paris6 Rue Amyot, 75005 Paris steels@arti.vub.ac.beAbstractThe paper reports on progress in building com-putational models of a constructivist approach tolanguage development.
It introduces a formalismfor construction grammars and learning strategiesbased on invention, abduction, and induction.
Ex-amples are drawn from experiments exercising themodel in situated language games played by embod-ied artificial agents.1 IntroductionThe constructivist approach to language learningproposes that ?children acquire linguistic compe-tence (...) only gradually, beginning with moreconcrete linguistic structures based on particularwords and morphemes, and then building up tomore abstract and productive structures based onvarious types of linguistic categories, schemas, andconstructions.?
(TomaselloBrooks, 1999), p. 161.The approach furthermore assumes that languagedevelopment is (i) grounded in cognition becauseprior to (or in a co-development with language)there is an understanding and conceptualisation ofscenes in terms of events, objects, roles that objectsplay in events, and perspectives on the event, and(ii) grounded in communication because languagelearning is intimately embedded in interactions withspecific communicative goals.
In contrast to thenativist position, defended, for example, by Pinker(Pinker, 1998), the constructivist approach does notassume that the semantic and syntactic categoriesas well as the linking rules (specifying for examplethat the agent of an action is linked to the subjectof a sentence) are universal and innate.
Rather, se-mantic and syntactic categories as well as the waythey are linked is built up in a gradual developmen-tal process, starting from quite specific ?verb-islandconstructions?.Although the constructivist approach appears toexplain a lot of the known empirical data about childlanguage acquisition, there is so far no worked outmodel that details how constructivist language de-velopment works concretely, i.e.
what kind of com-putational mechanisms are implied and how theywork together to achieve adult (or even child) levelcompetence.
Moreover only little work has beendone so far to build computational models for han-dling the sort of ?construction grammars?
assumedby this approach.
Both challenges inform the re-search discussed in this paper.2 Abductive LearningIn the constructivist literature, there is often the im-plicit assumption that grammatical development isthe result of observational learning, and several re-search efforts are going on to operationalise this ap-proach for acquiring grounded lexicons and gram-mars (see e.g.
(Roy, 2001)).
The agents are givenpairs with a real world situation, as perceived by thesensori-motor apparatus, and a language utterance.For example, an image of a ball is shown and at thesame time a stretch of speech containing the word?ball?.
Based on a generalisation process that usesstatistical pattern recognition algorithms or neuralnetworks, the learner then gradually extracts whatis common between the various situations in whichthe same word or construction is used, thus progres-sively building a grounded lexicon and grammar ofa language.The observational learning approach has hadsome success in learning words for objects and ac-quiring simple grammatical constructions, but thereseem to be two inherent limitations.
First, there isthe well known poverty of the stimulus argument,widely accepted in linguistics, which says that thereis not enough data in the sentences normally avail-able to the language learner to arrive at realisticlexicons and grammars, let alne learn at the sametime the categorisations and conceptualisations ofthe world implied by the language.
This has leadmany linguists to adopt the nativist position men-tioned earlier.
The nativist position could in princi-ple be integrated in an observational learning frame-work by introducing strong biases on the generali-sation process, incorporating the constraints of uni-versal grammar, but it has been difficult to identifyand operationalise enough of these constraints to doconcrete experiments in realistic settings.
Second,observational learning assumes that the languagesystem (lexicon and grammar) exists as a fixed staticsystem.
However, observations of language in useshows that language users constantly align their lan-guage conventions to suit the purposes of specificconversations (ClarkBrennan, 1991).
Natural lan-guages therefore appear more to be like complexadaptive systems, similar to living systems that con-stantly adapt and evolve.
This makes it difficultto rely exclusively on statistical generalisation.
Itdoes not capture the inherently creative nature oflanguage use.This paper explores an alternative approach,which assumes a much more active stance from lan-guage users based on the Peircian notion of abduc-tion (Fann, 1970).
The speaker first attempts touse constructions from his existing inventory to ex-press whatever he wants to express.
However whenthat fails or is judged unsatisfactory, the speakermay extend his existing repertoire by inventing newconstructions.
These new constructions should besuch that there is a high chance that the hearer maybe able to guess their meaning.
The hearer alsouses as much as possible constructions stored inhis own inventory to make sense of what is beingsaid.
But when there are unknown constructions,or the meanings do not fit with the situation beingtalked about, the hearer makes an educated guessabout what the meaning of the unknown languageconstructions could be, and adds them as new hy-potheses to his own inventory.
Abductive construc-tivist learning hence relies crucially on the fact thatboth agents have sufficient common ground, sharethe same situation, have established joint attention,and share communicative goals.
Both speaker andhearer use themselves as models of the other in or-der to guess how the other one will interpret a sen-tence or why the speaker says things in a particularway.Because both speaker and hearer are taking risksmaking abductive leaps, a third activity is needed,namely induction, not in the sense of statistical gen-eralisation as in observational learning but in thesense of Peirce (Fann, 1970): A hypothesis arrivedat by making educated guesses is tested againstfurther data coming from subsequent interactions.When a construction leads to a successful interac-tion, there is some evidence that this constructionis (or could become) part of the set of conventionsadopted by the group, and language users shouldtherefore prefer it in the future.
When the construc-tion fails, the language user should avoid it if alter-natives are available.Implementing these visions of language learn-ing and use is obviously an enormous challenge forcomputational linguistics.
It requires not only cog-nitive and communicative grounding, but also gram-mar formalisms and associated parsing and produc-tion algorithms which are extremely flexible, bothfrom the viewpoint of getting as far as possiblein the interpretation or production process despitemissing rules or incompatibilities in the inventoriesof speaker and hearer, and from the viewpoint ofsupporting continuous change.3 Language GamesThe research reported here uses a methodologicalapproach which is quite common in Artificial Liferesearch but still relatively novel in (computational)linguistics: Rather than attempting to develop sim-ulations that generate natural phenomena directly,as one does when using Newton?s equations to sim-ulate the trajectory of a ball falling from a tower,we engage in computational simulations and roboticexperiments that create (new) artificial phenomenathat have some of the characteristics of natural phe-nomena and hence are seen as explaining them.Specifically, we implement artificial agents withcomponents modeling certain cognitive operations(such as introducing a new syntactic category, com-puting an analogy between two events, etc.
), andthen see what language phenomena result if theseagents exercise these components in embodied situ-ated language games.
This way we can investigatevery precisely what causal factors may underly cer-tain phenomena and can focus on certain aspects of(grounded) language use without having to face thevast full complexity of real human languages.
Asurvey of work which follows a similar methodol-ogy is found in (CangelosiParisi, 2003).The artificial agents used in the experiments driv-ing our research observe real-world scenes throughtheir cameras.
The scenes consist of interactionsbetween puppets, as shown in figure 1.
Thesescenes enact common events like movement of peo-ple and objects, actions such as push or pull, giveor take, etc.
In order to achieve the cognitivegrounding assumed in constructivist language learn-ing, the scenes are processed by a battery of rela-tively standard machine vision algorithms that seg-ment objects based on color and movement, trackobjects in real-time, and compute a stream of low-level features indicating which objects are touch-ing, in which direction objects are moving, etc.These low-level features are input to an event-recognition system that uses an inventory of hier-archical event structures and matches them againstthe data streaming in from low-level vision, similarto the systems described in (SteelsBaillie, 2003).Figure 1: Scene enacted with puppets so that typicalinteractions between humans involving agency canbe perceived and described.In order to achieve the communicative ground-ing required for constructivist learning, agents gothrough scripts in which they play various languagegames, similar to the setups described in (Steels,2003).
These language games are deliberately quitesimilar to the kind of scenes and interactions used ina lot of child language research.
A language gameis a routinised interaction between two agents abouta shared situation in the world that involves the ex-change of symbols.
Agents take turns playing therole of speaker and hearer and give each other feed-back about the outcome of the game.
In the gamefurther used in this paper, one agent describes toanother agent an event that happened in the mostrecently experienced scene.
The game succeeds ifthe hearer agrees that the event being described oc-curred in the recent scene.4 The LexiconVisual processing and event recognition results ina world model in the form of a series of facts de-scribing the scene.
To play the description game, thespeaker selects one event as the topic and then seeksa series of facts which discriminate this event and itsobjects against the other events and objects in thecontext.
We use a standard predicate calculus-stylerepresentation for meanings.
A semantic structureconsists of a set of units where each unit has a ref-erent, which is the object or event to which the unitdraws attention, and a meaning, which is a set ofclauses constraining the referent.
A semantic struc-ture with one unit is for example written down asfollows:[1] unit1   ev1   fall(ev1,true), fall-1(ev1,obj1), ball(obj1)where unit1 is the unit, ev1 the referent, and fall(ev1,true), fall-1(ev1,obj1), ball(obj1) the meaning.
Thedifferent arguments of an event are decomposedinto different predicates.
For example, for ?Johngives a book to Mary?, there would be four clauses:give(ev1,true) for the event itself, give-1(ev1, John),for the one who gives, give-2(ev1,book1), for the ob-ject given, and give-3(ev1,Mary), for the recipient.This representation is more flexible and makes itpossible to add new components (like the mannerof an event) at any time.Syntactic structures mirror semantic structures.They also consist of units and the name of unitsare shared with semantic structures so that cross-reference between them is straightforward.
Theform aspects of the sentence are represented in adeclarative predicate calculus style, using the unitsas arguments.
For example, the following unit isconstrained as introducing the string ?fall?
:[2] unit1   string(unit1, ?fall?
)The rule formalism we have developed uses ideasfrom several existing formalisms, particularlyunification grammars and is most similar to theEmbodied Construction Grammars proposed in(BergenChang, 2003).
Lexical rules link parts ofsemantic structure with parts of syntactic structure.All rules are reversable.
When producing, theleft side of a rule is matched against the semanticstructure and, if there is a match, the right side isunified with the syntactic structure.
Converselywhen parsing, the right side is matched against thesyntactic structure and the left side unified with thesemantic structure.
Here is a lexical entry for theword ?fall?.
[3] ?unit   ?ev   fall(?ev,?state), fall-1(?ev,?obj) ?unit   string(?unit,?fall?
)It specifies that a unit whose meaning isfall(?ev,?state), fall-1(?ev,?obj) is expressed withthe string ?fall?.
Variables are written down with aquestion mark in front.
Their scope is restricted tothe structure or rule in which they appear and ruleapplication often implies the renaming of certainvariables to take care of the scope constraints.
Hereis a lexical entry for ?ball?
:[4] ?unit   ?obj   ball(?obj) ?unit   string(?unit,?ball?
)Lexicon lookup attempts to find the minimal setof rules that covers the total semantic structure.New units may get introduced (both in the syntacticand semantic structure) if the meaning of a unitis broken down in the lexicon into more than oneword.
Thus, the original semantic structure in [1]results after the application of the two rules [3]and [4] in the following syntactic and semanticstructures:[5] unit1   ev1   fall(ev1,true), fall-1(ev1,obj1)unit2   obj1   ball(obj1)?
?unit1   string(unit1, ?fall?
)unit2   string(unit2, ?ball?
)If this syntactic structure is rendered, it producesthe utterance ?fall ball?.
No syntax is implied yet.In the reverse direction, the parser starts with thetwo units forming the syntactic structure in [5]and application of the rules produces the followingsemantic structure:[6] unit1   ?ev   fall(?ev,?state), fall-1(?ev,?obj)unit2   ?obj1   ball(?obj1)The semantic structure in [6] now contains variablesfor the referent of each unit and for the variouspredicate-arguments in their meanings.
The inter-pretation process matches these variables againstthe facts in the world model.
If a single consistentseries of bindings can be found, then interpretationis successful.
For example, assume that the facts inthe meaning part of [1] are in the world model thenmatching [6] against them results in the bindings:[7] ?ev/ev1, ?state/true, ?obj/obj1, ?obj1/obj1When the same word or the same meaning iscovered by more than one rule, a choice needsto be made.
Competing rules may develop if anagent invented a new word for a particular meaningbut is later confronted with another word used bysomebody else for the same meaning.
Every rulehas a score and in production and parsing, ruleswith the highest score are preferred.When the speaker performs lexicon lookup andrules were found to cover the complete semanticstructure, no new rules are needed.
But when somepart is uncovered, the speaker should create a newrule.
We have experimented so far with a simplestrategy where agents lump together the uncoveredfacts in a unit and create a brand new word, consist-ing of a randomly chosen configuration of syllables.For example, if no word for ball(obj1) exists yet tocover the semantic structure in [1], a new rule suchas [4] can be constructed by the speaker and subse-quently used.
If there is no word at all for the wholesemantic structure in [1], a single word covering thewhole meaning will be created, giving the effect ofholophrases.The hearer first attempts to parse as far as pos-sible the given sentence, and then interprets the re-sulting semantic structure, possibly using joint at-tention or other means that may help to find the in-tended interpretation.
If this results in a unique setof bindings, the language game is deemed success-ful.
But if there were parts of the sentence whichwere not covered by any rule, then the hearer canuse abductive learning.
The first critical step is toguess as well as possible the meaning of the un-known word(s).
Thus suppose the sentence is ?fallball?, resulting in the semantic structure:[8] unit1   ?ev   fall(?ev,?state), fall-1(?ev,?obj)If this structure is matched, bindings for ?ev and?obj are found.
The agent can now try to find thepossible meaning of the unknown word ?ball?.
Hecan assume that this meaning must somehow helpin the interpretation process.
He therefore concep-tualises the same way as if he would be the speakerand constructs a distinctive description that drawsattention to the event in question, for example byconstraining the referent of ?obj with an additionalpredicate.
Although there are usually several waysin which obj1 differs from other objects in the con-text.
There is a considerable chance that the pred-icate ball is chosen and hence ball(?obj) is abduc-tively inferred as the meaning of ?ball?
resulting ina rule like [4].Agents use induction to test whether the rulesthey created by invention and abduction have beenadopted by the group.
Every rule has a score, whichis local to each agent.
When the speaker or hearerhas success with a particular rule, its score is in-creased and the score of competing rules is de-creased, thus implementing lateral inhibition.
Whenthere is a failure, the score of the rule that was usedis decreased.
Because the agents prefer rules withthe highest score, there is a positive feedback inthe system.
The more a word is used for a partic-ular meaning, the more success that word will have.Figure 2: Winner-take-all effect in words competingfor same meaning.
The x-axis plots language gamesand the y-axis the use frequency.Scores rise in all the agents for these words and soprogressively we see a winner-take-all effect withone word dominating for the expression of a par-ticular meaning (see figure 2).
Many experimentshave by now been performed showing that this kindof lateral inhibition dynamics allows a populationof agents to negotiate a shared inventory of form-meaning pairs for content words (Steels, 2003).5 SyntactisationThe reader may have noticed that the semanticstructure in [6] resulting from parsing the sentence?fall ball?, includes two variables which will bothget bound to the same object, namely ?obj, intro-duced by the predicate fall-1(?ev,?obj), and ?obj1, in-troduced by the predicate ball(?obj1).
We say that inthis case ?obj and ?obj1 form an equality.
Just fromparsing the two words, the hearer cannot know thatthe object involved in the fall event is the same asthe object introduced by ball.
He can only figurethis out when looking at the scene (i.e.
the worldmodel).
In fact, if there are several balls in thescene and only one of them is falling, there is noway to know which object is intended.
And even ifthe hearer can figure it out, it is still desirable thatthe speaker should provide extra-information aboutequalities to optimise the hearer?s interpretation ef-forts.A major thesis of the present paper is that resolv-ing equivalences between variables is the main mo-tor for the introduction of syntax.
To achieve it, theagents could, as a first approximation, use rules likethe following one, to be applied after all lexical ruleshave been applied:[9] ?unit1   ?ev1   fall-1(?ev1,?obj2)?unit2   ?obj2   ball(?obj2)?unit1   string(?unit1, ?fall?
)?unit2   string(?unit2, ?ball?
)This rule is formally equivalent to the lexical rulesdiscussed earlier in the sense that it links parts ofa semantic structure with parts of a syntactic struc-ture.
But now more than one unit is involved.
Rule[9] will do the job, because when unifying its rightside with the semantic structure (in parsing) ?obj2unifies with the variables ?obj (supplied by ?fall?
)and ?obj1 (supplied by ?ball?)
and this forces themto be equivalent.
Note that ?unit1 in [9] only con-tains those parts of the original meaning that involvethe variables which need to be made equal.The above rule works but is completely specific tothis case.
It is an example of the ad hoc ?verb-island?constructions reported in an early stage of child lan-guage development.
Obviously it is much more de-sirable to have a more general rule, which can beachieved by introducing syntactic and semantic cat-egories.
A semantic category (such as agent, perfec-tive, countable, male) is a categorisation of a con-ceptual relation, which is used to constrain the se-mantic side of grammatical rules.
A syntactic cate-gory (such as noun, verb, nominative) is a categori-sation of a word or a group of words, which canbe used to constrain the syntactic side of grammati-cal rules.
A rule using categories can be formed bytaking rule [9] above and turning all predicates orcontent words into semantic or syntactic categories.
[10] ?unit1   ?ev1   semcat1(?ev1,?obj2)?unit2   ?obj2   semcat2(?obj2)?unit1   syncat1 (?unit1)?unit2   syncat2(?unit2)The agent then needs to create sem-rules to cate-gorise a predicate as belonging to a semantic cate-gory, as in:[11] ?unit1   ?ev1   fall-1(?ev1,?obj2)?unit1   ?ev1   semcat1(?ev1,?obj1)and syn-rules to categorise a word as belonging to asyntactic category, as in:[12] ?unit1   string(?unit1,?fall?
)?unit1   ?ev1   syncat1(?unit1)These rules have arrows going only in one directionbecause they are only applied in one way.1 Duringproduction, the sem-rules are applied first, then thelexical rules, next the syn-rules and then the gram-1Actually if word morphology is integrated, syn-rules needto be bi-directional, but this topic is not discussed further heredue to space limitations.matical rules.
In parsing, the lexical rules are ap-plied first (in reverse direction), then the syn-rulesand the sem-rules, and only then the grammaticalrules (in reverse direction).
The complete syntacticand semantic structures for example [9] look as fol-lows:[13] unit1   ?ev1   fall(?ev1,?state), fall-1(?ev1,?obj),semcat1(?ev1,?obj)unit2   ?obj1   ball(?obj1), semcat2(?obj1)?
?unit1   string(unit1, ?fall?
), syncat-1(unit1)unit2   string(unit2, ?ball?
), syncat-2(unit2)The right side of rule [10] matches with this syntac-tic structure, and if the left side of rule [10] is unifiedwith the semantic structure in [13] the variable ?obj2unifies with ?obj and ?obj1, thus resolving the equal-ity before semantic interpretation (matching againstthe world model) starts.How can language users develop such rules?
Thespeaker can detect equalities that need to be re-solved by re-entrance: Before rendering a sentenceand communicating it to the hearer, the speaker re-parses his own sentence and interprets it against thefacts in his own world model.
If the resulting setof bindings contains variables that are bound to thesame object after interpretation, then these equali-ties are candidates for the construction of a rule andnew syntactic and semantic categories are made asa side effect.
Note how the speaker uses himself asa model of the hearer and fixes problems that thehearer might otherwise encounter.
The hearer candetect equalities by first interpreting the sentencebased on the constructions that are already part ofhis own inventory and the shared situation and priorjoint attention.
These equalities are candidates fornew rules to be constructed by the hearer, and theyagain involve the introduction of syntactic and se-mantic categories.
Note that syntactic and semanticcategories are always local to an agent.
The samelateral inhibition dynamics is used for grammaticalrules as for lexical rules, and so is also a positivefeedback loop leading to a winner-take-all effect forgrammatical rules.6 HierarchyNatural languages heavily use categories to tightenrule application, but they also introduce additionalsyntactic markings, such as word order, functionwords, affixes, morphological variation of wordforms, and stress or intonation patterns.
Thesemarkings are often used to signal to which categorycertain words belong.
They can be easily incorpo-rated in the formalism developed so far by addingadditional descriptors of the units in the syntacticstructure.
For example, rule [10] can be expandedwith word order constraints and the introduction ofa particle ?ba?
:[14] ?unit1   ?ev1   semcat1(?ev1,?obj2)?unit2   ?obj2   semcat2(?obj2)?unit1   syncat1 (?unit1)?unit2   syncat2(?unit2)?unit3   string (?unit3, ?ba?
)?unit4   syn-subunits (  ?unit1, ?unit2, ?unit3  ),preceeds(?unit2, ?unit3)Note that it was necessary to introduce a superunit?unit4 in order to express the word order constraintsbetween the ba-particle and the unit that introducesthe object.
Applying this rule as well as the syn-rules and sem-rules discussed earlier to the seman-tic structure in [5] yields:[13] unit1   ev1   fall(ev1,true), fall-1(ev1,obj),semcat1(ev1,obj)unit2   obj1   ball(obj1), semcat2(obj1)?
?unit1   string(unit1, ?fall?
), syncat-1(unit1)unit2   string(unit2, ?ball?
), syncat-2(unit2)unit3   string(unit3, ?ba?
)unit4   syn-subunits(  unit1,unit2,unit3  ),preceeds(unit2,unit3)When this syntactic structure is rendered, it pro-duces ?fall ball ba?, or equivalently ?ball ba fall?,because only the order between ?ball?
and ?ba?
isconstrained.Obviously the introduction of additional syntac-tic features makes the learning of grammatical rulesmore difficult.
Natural languages appear to havemeta-level strategies for invention and abduction.For example, a language (like Japanese) tends to useparticles for expressing the roles of objects in eventsand this usage is a strategy both for inventing the ex-pression of a new relation and for guessing what theuse of an unknown word in the sentence might be.Another language (like Swahili) uses morphologi-cal variations similar to Latin for the same purposeand thus has ended up with a rich set of affixes.
Inour experiments so far, we have implemented suchstrategies directly, so that invention and abductionis strongly constrained.
We still need to work outa formalism for describing these strategies as meta-rules and research the associated learning mecha-nisms.Figure 3: The graph shows the dependency structureas well as the phrase-structure emerging through theapplication of multiple rulesWhen the same word participates in severalrules, we automatically get the emergence ofhierarchical structures.
For example, suppose thattwo predicates are used to draw attention to obj1 in[5]: ball and red.
If the lexicon has two separatewords for each predicate, then the initial semanticstructure would introduce different variables so thatthe meaning after parsing ?fall ball ba red?
wouldbe:[15] fall(?ev,?state), fall-1(?ev,?obj), ball (?obj),red(?obj2)To resolve the equality between ?obj and ?obj2, thespeaker could create the following rule:[14] ?unit1   ?obj   semcat3(?obj)?unit2   ?obj   semcat4(?obj)?unit1   syncat3(?unit1)?unit2   syncat4(?unit2)?unit3   syn-subunits (  unit1,unit2  ), pre-ceeds(unit1,unit2)The predicate ball is declared to belong to semcat4and the word ?ball?
to syncat4.
The predicate redbelongs to semcat3 and the word ?red?
to syncat3.Rendering the syntactic structure after applicationof this rule gives the sentence ?fall red ball ba?.
Ahierarchical structure (figure 3) emerges because?ball?
participates in two rules.7 Re-useAgents obviously should not invent new conven-tions from scratch every time they need one, butrather use as much as possible existing categorisa-tions and hence existing rules.
This simple economyprinciple quickly leads to the kind of syntagmaticand paradigmatic regularities that one finds in natu-ral grammars.
For example, if the speaker wants toexpress that a block is falling, no new semantic orsyntactic categories or linking rules are needed butblock can simply be declared to belong to semcat4and ?block?
to syncat3 and rule [14] applies.Re-use should be driven by analogy.
In one ofthe largest experiments we have carried out so far,agents had a way to compute the similarity betweentwo event-structures by pairing the primitive opera-tions making up an event.
For example, a pick-upaction is decomposed into: an object moving intothe direction of another stationary object, the firstobject then touching the second object, and next thetwo objects moving together in (roughly) the oppo-site direction.
A put-down action has similar sub-events, except that their ordering is different.
Theroles of the objects involved (the hand, the objectbeing picked up) are identical and so their gram-matical marking could be re-used with very low riskof being misunderstood.
When a speaker reuses agrammatical marking for a particular semantic cate-gory, this gives a strong hint to the hearer what kindof analogy is expected.
By using these inventionand abduction strategies, semantic categories likeagent or patient gradually emerged in the artificialgrammars.
Figure 4 visualises the result of this ex-periment (after 700 games between 2 agents takingturns).
The x-axis (randomly) ranks the differentpredicate-argument relations, the y-axis their mark-ers.
Without re-use, every argument would have itsown marker.
Now several markers (such as ?va?
or?zu?)
cover more than one relation.Figure 4: More compact grammars result from re-use based on semantic analogies.8 ConclusionsThe paper reports significant steps towards the com-putational modeling of a constructivist approach tolanguage development.
It has introduced aspects ofa construction grammar formalism that is designedto handle the flexibility required for emergent de-veloping grammars.
It also proposed that invention,abduction, and induction are necessary and suffi-cient for language learning.
Much more technicalwork remains to be done but already significant ex-perimental results have been obtained with embod-ied agents playing situated language games.
Mostof the open questions concern under what circum-stances syntactic and semantic categories should bere-used.Research funded by Sony CSL with additional fund-ing from ESF-OMLL program, EU FET-ECAgents andCNRS OHLL.ReferencesBergen, B.K.
and N.C. Chang.
2003.
Embod-ied Construction Grammar in Simulation-BasedLanguage Understanding.
TR 02-004, ICSI,Berkeley.Cangelosi, and D. Parisi 2003.
Simulating the Evo-lution of Language.
Springer-Verlag, Berlin.Clark, H. and S. Brennan 1991.
Grounding in com-munication.
In: Resnick, L. J. Levine and S.Teasley (eds.)
Perspectives on Socially SharedCognition.
APA Books, Washington.
p. 127-149.Fann, K.T.
1970.
Peirce?s Theory of AbductionMartinus Nijhoff, The Hague.Roy, D. 2001.
Learning Visually Grounded Wordsand Syntax of Natural Spoken Language.
Evolu-tion of communication 4(1).Pinker, S. 1998.
Learnability and Cognition: Theacquisition of Argument Structure.
The MITPress, Cambridge Ma.Steels, L. 2003 Evolving grounded communicationfor robots.
Trends in Cognitive Science.
Volume7, Issue 7, July 2003 , pp.
308-312.Steels, L. and J-C. Baillie 2003.
Shared Ground-ing of Event Descriptions by Autonomous Robots.Journal of Robotics and Autonomous Systems43, 2003, pp.
163-173.Tomasello, M. and P.J.
Brooks 1999.
Early syntac-tic development: A Construction Grammar ap-proach In: Barrett, M.
(ed.)
(1999) The Develop-ment of Language Psychology Press, London.
pp.161-190.
