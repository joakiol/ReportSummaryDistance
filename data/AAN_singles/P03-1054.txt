Accurate Unlexicalized ParsingDan KleinComputer Science DepartmentStanford UniversityStanford, CA 94305-9040klein@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040manning@cs.stanford.eduAbstractWe demonstrate that an unlexicalized PCFG canparse much more accurately than previously shown,by making use of simple, linguistically motivatedstate splits, which break down false independenceassumptions latent in a vanilla treebank grammar.Indeed, its performance of 86.36% (LP/LR F1) isbetter than that of early lexicalized PCFG models,and surprisingly close to the current state-of-the-art.
This result has potential uses beyond establish-ing a strong lower bound on the maximum possi-ble accuracy of unlexicalized models: an unlexical-ized PCFG is much more compact, easier to repli-cate, and easier to interpret than more complex lex-ical models, and the parsing algorithms are simpler,more widely understood, of lower asymptotic com-plexity, and easier to optimize.In the early 1990s, as probabilistic methods sweptNLP, parsing work revived the investigation of prob-abilistic context-free grammars (PCFGs) (Booth andThomson, 1973; Baker, 1979).
However, early re-sults on the utility of PCFGs for parse disambigua-tion and language modeling were somewhat disap-pointing.
A conviction arose that lexicalized PCFGs(where head words annotate phrasal nodes) werethe key tool for high performance PCFG parsing.This approach was congruent with the great successof word n-gram models in speech recognition, anddrew strength from a broader interest in lexicalizedgrammars, as well as demonstrations that lexical de-pendencies were a key tool for resolving ambiguitiessuch as PP attachments (Ford et al, 1982; Hindle andRooth, 1993).
In the following decade, great successin terms of parse disambiguation and even languagemodeling was achieved by various lexicalized PCFGmodels (Magerman, 1995; Charniak, 1997; Collins,1999; Charniak, 2000; Charniak, 2001).However, several results have brought into ques-tion how large a role lexicalization plays in suchparsers.
Johnson (1998) showed that the perfor-mance of an unlexicalized PCFG over the Penn tree-bank could be improved enormously simply by an-notating each node by its parent category.
The Penntreebank covering PCFG is a poor tool for parsing be-cause the context-freedom assumptions it embodiesare far too strong, and weakening them in this waymakes the model much better.
More recently, Gildea(2001) discusses how taking the bilexical probabil-ities out of a good current lexicalized PCFG parserhurts performance hardly at all: by at most 0.5% fortest text from the same domain as the training data,and not at all for test text from a different domain.1But it is precisely these bilexical dependencies thatbacked the intuition that lexicalized PCFGs should bevery successful, for example in Hindle and Rooth?sdemonstration from PP attachment.
We take this as areflection of the fundamental sparseness of the lex-ical dependency information available in the PennTreebank.
As a speech person would say, one mil-lion words of training data just isn?t enough.
Evenfor topics central to the treebank?s Wall Street Jour-nal text, such as stocks, many very plausible depen-dencies occur only once, for example stocks stabi-lized, while many others occur not at all, for exam-ple stocks skyrocketed.2The best-performing lexicalized PCFGs have in-creasingly made use of subcategorization3 of the1There are minor differences, but all the current best-knownlexicalized PCFGs employ both monolexical statistics, whichdescribe the phrasal categories of arguments and adjuncts thatappear around a head lexical item, and bilexical statistics, or de-pendencies, which describe the likelihood of a head word takingas a dependent a phrase headed by a certain other word.2This observation motivates various class- or similarity-based approaches to combating sparseness, and this remains apromising avenue of work, but success in this area has provensomewhat elusive, and, at any rate, current lexicalized PCFGsdo simply use exact word matches if available, and interpolatewith syntactic category-based estimates when they are not.3In this paper we use the term subcategorization in the origi-nal general sense of Chomsky (1965), for where a syntactic cat-categories appearing in the Penn treebank.
Charniak(2000) shows the value his parser gains from parent-annotation of nodes, suggesting that this informa-tion is at least partly complementary to informationderivable from lexicalization, and Collins (1999)uses a range of linguistically motivated and care-fully hand-engineered subcategorizations to breakdown wrong context-freedom assumptions of thenaive Penn treebank covering PCFG, such as differ-entiating ?base NPs?
from noun phrases with phrasalmodifiers, and distinguishing sentences with emptysubjects from those where there is an overt subjectNP.
While he gives incomplete experimental resultsas to their efficacy, we can assume that these featureswere incorporated because of beneficial effects onparsing that were complementary to lexicalization.In this paper, we show that the parsing perfor-mance that can be achieved by an unlexicalizedPCFG is far higher than has previously been demon-strated, and is, indeed, much higher than communitywisdom has thought possible.
We describe severalsimple, linguistically motivated annotations whichdo much to close the gap between a vanilla PCFGand state-of-the-art lexicalized models.
Specifically,we construct an unlexicalized PCFG which outper-forms the lexicalized PCFGs of Magerman (1995)and Collins (1996) (though not more recent models,such as Charniak (1997) or Collins (1999)).One benefit of this result is a much-strengthenedlower bound on the capacity of an unlexicalizedPCFG.
To the extent that no such strong baseline hasbeen provided, the community has tended to greatlyoverestimate the beneficial effect of lexicalization inprobabilistic parsing, rather than looking criticallyat where lexicalized probabilities are both needed tomake the right decision and available in the trainingdata.
Secondly, this result affirms the value of lin-guistic analysis for feature discovery.
The result hasother uses and advantages: an unlexicalized PCFG iseasier to interpret, reason about, and improve thanthe more complex lexicalized models.
The grammarrepresentation is much more compact, no longer re-quiring large structures that store lexicalized proba-bilities.
The parsing algorithms have lower asymp-totic complexity4 and have much smaller grammaregory is divided into several subcategories, for example divid-ing verb phrases into finite and non-finite verb phrases, ratherthan in the modern restricted usage where the term refers onlyto the syntactic argument frames of predicators.4O(n3) vs. O(n5) for a naive implementation, or vs. O(n4)if using the clever approach of Eisner and Satta (1999).constants.
An unlexicalized PCFG parser is muchsimpler to build and optimize, including both stan-dard code optimization techniques and the investiga-tion of methods for search space pruning (Caraballoand Charniak, 1998; Charniak et al, 1998).It is not our goal to argue against the use of lex-icalized probabilities in high-performance probabi-listic parsing.
It has been comprehensively demon-strated that lexical dependencies are useful in re-solving major classes of sentence ambiguities, and aparser should make use of such information wherepossible.
We focus here on using unlexicalized,structural context because we feel that this infor-mation has been underexploited and underappreci-ated.
We see this investigation as only one part ofthe foundation for state-of-the-art parsing which em-ploys both lexical and structural conditioning.1 Experimental SetupTo facilitate comparison with previous work, wetrained our models on sections 2?21 of the WSJ sec-tion of the Penn treebank.
We used the first 20 files(393 sentences) of section 22 as a development set(devset).
This set is small enough that there is no-ticeable variance in individual results, but it allowedrapid search for good features via continually repars-ing the devset in a partially manual hill-climb.
All ofsection 23 was used as a test set for the final model.For each model, input trees were annotated or trans-formed in some way, as in Johnson (1998).
Givena set of transformed trees, we viewed the local treesas grammar rewrite rules in the standard way, andused (unsmoothed) maximum-likelihood estimatesfor rule probabilities.5 To parse the grammar, weused a simple array-based Java implementation ofa generalized CKY parser, which, for our final bestmodel, was able to exhaustively parse all sentencesin section 23 in 1GB of memory, taking approxi-mately 3 sec for average length sentences.65The tagging probabilities were smoothed to accommodateunknown words.
The quantity P(tag|word) was estimatedas follows: words were split into one of several categorieswordclass, based on capitalization, suffix, digit, and othercharacter features.
For each of these categories, we took themaximum-likelihood estimate of P(tag|wordclass).
This dis-tribution was used as a prior against which observed taggings,if any, were taken, giving P(tag|word) = [c(tag, word) +?
P(tag|wordclass)]/[c(word)+?].
This was then inverted togive P(word|tag).
The quality of this tagging model impactsall numbers; for example the raw treebank grammar?s devset F1is 72.62 with it and 72.09 without it.6The parser is available for download as open source at:http://nlp.stanford.edu/downloads/lex-parser.shtmlVP<VP:[VBZ].
.
.
PP><VP:[VBZ].
.
.
NP><VP:[VBZ]>VBZNPPPFigure 1: The v=1, h=1 markovization of VP ?
VBZ NP PP.2 Vertical and Horizontal MarkovizationThe traditional starting point for unlexicalized pars-ing is the raw n-ary treebank grammar read fromtraining trees (after removing functional tags andnull elements).
This basic grammar is imperfect intwo well-known ways.
First, the category symbolsare too coarse to adequately render the expansionsindependent of the contexts.
For example, subjectNP expansions are very different from object NP ex-pansions: a subject NP is 8.7 times more likely thanan object NP to expand as just a pronoun.
Havingseparate symbols for subject and object NPs allowsthis variation to be captured and used to improveparse scoring.
One way of capturing this kind ofexternal context is to use parent annotation, as pre-sented in Johnson (1998).
For example, NPs with Sparents (like subjects) will be marked NP?S, whileNPs with VP parents (like objects) will be NP?VP.The second basic deficiency is that many ruletypes have been seen only once (and therefore havetheir probabilities overestimated), and many ruleswhich occur in test sentences will never have beenseen in training (and therefore have their probabili-ties underestimated ?
see Collins (1999) for analy-sis).
Note that in parsing with the unsplit grammar,not having seen a rule doesn?t mean one gets a parsefailure, but rather a possibly very weird parse (Char-niak, 1996).
One successful method of combatingsparsity is to markovize the rules (Collins, 1999).
Inparticular, we follow that work in markovizing outfrom the head child, despite the grammar being un-lexicalized, because this seems the best way to cap-ture the traditional linguistic insight that phrases areorganized around a head (Radford, 1988).Both parent annotation (adding context) and RHSmarkovization (removing it) can be seen as two in-stances of the same idea.
In parsing, every node hasa vertical history, including the node itself, parent,grandparent, and so on.
A reasonable assumption isthat only the past v vertical ancestors matter to thecurrent expansion.
Similarly, only the previous hhorizontal ancestors matter (we assume that the headHorizontal Markov OrderVertical Order h = 0 h = 1 h ?
2 h = 2 h = ?v = 1 No annotation 71.27 72.5 73.46 72.96 72.62(854) (3119) (3863) (6207) (9657)v ?
2 Sel.
Parents 74.75 77.42 77.77 77.50 76.91(2285) (6564) (7619) (11398) (14247)v = 2 All Parents 74.68 77.42 77.81 77.50 76.81(2984) (7312) (8367) (12132) (14666)v ?
3 Sel.
GParents 76.50 78.59 79.07 78.97 78.54(4943) (12374) (13627) (19545) (20123)v = 3 All GParents 76.74 79.18 79.74 79.07 78.72(7797) (15740) (16994) (22886) (22002)Figure 2: Markovizations: F1 and grammar size.child always matters).
It is a historical accident thatthe default notion of a treebank PCFG grammar takesv = 1 (only the current node matters vertically) andh = ?
(rule right hand sides do not decompose atall).
On this view, it is unsurprising that increasingv and decreasing h have historically helped.As an example, consider the case of v = 1,h = 1.
If we start with the rule VP ?
VBZ NPPP PP, it will be broken into several stages, each abinary or unary rule, which conceptually representa head-outward generation of the right hand size, asshown in figure 1.
The bottom layer will be a unaryover the head declaring the goal: ?VP: [VBZ]?
?VBZ.
The square brackets indicate that the VBZ isthe head, while the angle brackets ?X?
indicates thatthe symbol ?X?
is an intermediate symbol (equiv-alently, an active or incomplete state).
The nextlayer up will generate the first rightward sibling ofthe head child: ?VP: [VBZ].
.
.
NP?
?
?VP: [VBZ]?NP.
Next, the PP is generated: ?VP: [VBZ].
.
.
PP?
?
?VP: [VBZ].
.
.
NP?
PP.
We would then branch off leftsiblings if there were any.7 Finally, we have anotherunary to finish the VP.
Note that while it is con-venient to think of this as a head-outward process,these are just PCFG rewrites, and so the actual scoresattached to each rule will correspond to a downwardgeneration order.Figure 2 presents a grid of horizontal and verti-cal markovizations of the grammar.
The raw tree-bank grammar corresponds to v = 1, h = ?
(theupper right corner), while the parent annotation in(Johnson, 1998) corresponds to v = 2, h = ?, andthe second-order model in Collins (1999), is broadlya smoothed version of v = 2, h = 2.
In addi-tion to exact nth-order models, we tried variable-7In our system, the last few right children carry over as pre-ceding context for the left children, distinct from common prac-tice.
We found this wrapped horizon to be beneficial, and italso unifies the infinite order model with the unmarkovized rawrules.Cumulative Indiv.Annotation Size F1 1 F1 1 F1Baseline (v ?
2, h ?
2) 7619 77.77 ?
?UNARY-INTERNAL 8065 78.32 0.55 0.55UNARY-DT 8066 78.48 0.71 0.17UNARY-RB 8069 78.86 1.09 0.43TAG-PA 8520 80.62 2.85 2.52SPLIT-IN 8541 81.19 3.42 2.12SPLIT-AUX 9034 81.66 3.89 0.57SPLIT-CC 9190 81.69 3.92 0.12SPLIT-% 9255 81.81 4.04 0.15TMP-NP 9594 82.25 4.48 1.07GAPPED-S 9741 82.28 4.51 0.17POSS-NP 9820 83.06 5.29 0.28SPLIT-VP 10499 85.72 7.95 1.36BASE-NP 11660 86.04 8.27 0.73DOMINATES-V 14097 86.91 9.14 1.42RIGHT-REC-NP 15276 87.04 9.27 1.94Figure 3: Size and devset performance of the cumulatively an-notated models, starting with the markovized baseline.
Theright two columns show the change in F1 from the baseline foreach annotation introduced, both cumulatively and for each sin-gle annotation applied to the baseline in isolation.history models similar in intent to those describedin Ron et al (1994).
For variable horizontal his-tories, we did not split intermediate states below 10occurrences of a symbol.
For example, if the symbol?VP: [VBZ].
.
.
PP PP?
were too rare, we would col-lapse it to ?VP: [VBZ].
.
.
PP?.
For vertical histories,we used a cutoff which included both frequency andmutual information between the history and the ex-pansions (this was not appropriate for the horizontalcase because MI is unreliable at such low counts).Figure 2 shows parsing accuracies as well as thenumber of symbols in each markovization.
Thesesymbol counts include all the intermediate stateswhich represent partially completed constituents.The general trend is that, in the absence of furtherannotation, more vertical annotation is better ?
evenexhaustive grandparent annotation.
This is not truefor horizontal markovization, where the variable-order second-order model was superior.
The bestentry, v = 3, h ?
2, has an F1 of 79.74, alreadya substantial improvement over the baseline.In the remaining sections, we discuss other an-notations which increasingly split the symbol space.Since we expressly do not smooth the grammar, notall splits are guaranteed to be beneficial, and not allsets of useful splits are guaranteed to co-exist well.In particular, while v = 3, h ?
2 markovization isgood on its own, it has a large number of states anddoes not tolerate further splitting well.
Therefore,we base all further exploration on the v ?
2, h ?
2ROOTS?ROOTNP?SNNRevenueVP?SVBDwasNP?VPQP$$CD444.9CDmillion,,S?VPVP?SVBGincludingNP?VPNP?NPJJnetNNinterest,,CONJPRBdownRBslightlyINfromNP?NPQP$$CD450.7CDmillion..Figure 4: An error which can be resolved with the UNARY-INTERNAL annotation (incorrect baseline parse shown).grammar.
Although it does not necessarily jump outof the grid at first glance, this point represents thebest compromise between a compact grammar anduseful markov histories.3 External vs. Internal AnnotationThe two major previous annotation strategies, par-ent annotation and head lexicalization, can be seenas instances of external and internal annotation, re-spectively.
Parent annotation lets us indicate animportant feature of the external environment of anode which influences the internal expansion of thatnode.
On the other hand, lexicalization is a (radi-cal) method of marking a distinctive aspect of theotherwise hidden internal contents of a node whichinfluence the external distribution.
Both kinds of an-notation can be useful.
To identify split states, weadd suffixes of the form -X to mark internal contentfeatures, and ?X to mark external features.To illustrate the difference, consider unary pro-ductions.
In the raw grammar, there are many unar-ies, and once any major category is constructed overa span, most others become constructible as well us-ing unary chains (see Klein and Manning (2001) fordiscussion).
Such chains are rare in real treebanktrees: unary rewrites only appear in very specificcontexts, for example S complements of verbs wherethe S has an empty, controlled subject.
Figure 4shows an erroneous output of the parser, using thebaseline markovized grammar.
Intuitively, there areseveral reasons this parse should be ruled out, butone is that the lower S slot, which is intended pri-marily for S complements of communication verbs,is not a unary rewrite position (such complementsusually have subjects).
It would therefore be naturalto annotate the trees so as to confine unary produc-tions to the contexts in which they are actually ap-propriate.
We tried two annotations.
First, UNARY-INTERNAL marks (with a -U) any nonterminal nodewhich has only one child.
In isolation, this resultedin an absolute gain of 0.55% (see figure 3).
Thesame sentence, parsed using only the baseline andUNARY-INTERNAL, is parsed correctly, because theVP rewrite in the incorrect parse ends with an S?VP-U with very low probability.8Alternately, UNARY-EXTERNAL, marked nodeswhich had no siblings with ?U.
It was similar toUNARY-INTERNAL in solo benefit (0.01% worse),but provided far less marginal benefit on top ofother later features (none at all on top of UNARY-INTERNAL for our top models), and was discarded.9One restricted place where external unary annota-tion was very useful, however, was at the pretermi-nal level, where internal annotation was meaning-less.
One distributionally salient tag conflation inthe Penn treebank is the identification of demonstra-tives (that, those) and regular determiners (the, a).Splitting DT tags based on whether they were onlychildren (UNARY-DT) captured this distinction.
Thesame external unary annotation was even more ef-fective when applied to adverbs (UNARY-RB), dis-tinguishing, for example, as well from also).
Be-yond these cases, unary tag marking was detrimen-tal.
The F1 after UNARY-INTERNAL, UNARY-DT,and UNARY-RB was 78.86%.4 Tag SplittingThe idea that part-of-speech tags are not fine-grainedenough to abstract away from specific-word be-haviour is a cornerstone of lexicalization.
TheUNARY-DT annotation, for example, showed that thedeterminers which occur alone are usefully distin-guished from those which occur with other nomi-nal material.
This marks the DT nodes with a singlebit about their immediate external context: whetherthere are sisters.
Given the success of parent anno-tation for nonterminals, it makes sense to parent an-notate tags, as well (TAG-PA).
In fact, as figure 3shows, exhaustively marking all preterminals withtheir parent category was the most effective singleannotation we tried.
Why should this be useful?Most tags have a canonical category.
For example,NNS tags occur under NP nodes (only 234 of 70855do not, mostly mistakes).
However, when a tag8Note that when we show such trees, we generally onlyshow one annotation on top of the baseline at a time.
More-over, we do not explicitly show the binarization implicit by thehorizontal markovization.9These two are not equivalent even given infinite data.VP?STOtoVP?VPVBseePP?VPINifNP?PPNNadvertisingNNSworksVP?STO?VPtoVP?VPVB?VPseeSBAR?VPIN?SBARifS?SBARNP?SNN?NPadvertisingVP?SVBZ?VPworks(a) (b)Figure 5: An error resolved with the TAG-PA annotation (of theIN tag): (a) the incorrect baseline parse and (b) the correct TAG-PA parse.
SPLIT-IN also resolves this error.somewhat regularly occurs in a non-canonical posi-tion, its distribution is usually distinct.
For example,the most common adverbs directly under ADVP arealso (1599) and now (544).
Under VP, they are n?t(3779) and not (922).
Under NP, only (215) and just(132), and so on.
TAG-PA brought F1 up substan-tially, to 80.62%.In addition to the adverb case, the Penn tag setconflates various grammatical distinctions that arecommonly made in traditional and generative gram-mar, and from which a parser could hope to get use-ful information.
For example, subordinating con-junctions (while, as, if ), complementizers (that, for),and prepositions (of, in, from) all get the tag IN.Many of these distinctions are captured by TAG-PA (subordinating conjunctions occur under S andprepositions under PP), but are not (both subor-dinating conjunctions and complementizers appearunder SBAR).
Also, there are exclusively noun-modifying prepositions (of ), predominantly verb-modifying ones (as), and so on.
The annotationSPLIT-IN does a linguistically motivated 6-way splitof the IN tag, and brought the total to 81.19%.Figure 5 shows an example error in the baselinewhich is equally well fixed by either TAG-PA orSPLIT-IN.
In this case, the more common nominaluse of works is preferred unless the IN tag is anno-tated to allow if to prefer S complements.We also got value from three other annotationswhich subcategorized tags for specific lexemes.First we split off auxiliary verbs with the SPLIT-AUX annotation, which appends ?BE to all formsof be and ?HAVE to all forms of have.10 More mi-norly, SPLIT-CC marked conjunction tags to indicate10This is an extended uniform version of the partial auxil-iary annotation of Charniak (1997), wherein all auxiliaries aremarked as AUX and a -G is added to gerund auxiliaries andgerund VPs.whether or not they were the strings [Bb]ut or &,each of which have distinctly different distributionsfrom other conjunctions.
Finally, we gave the per-cent sign (%) its own tag, in line with the dollar sign($) already having its own.
Together these three an-notations brought the F1 to 81.81%.5 What is an Unlexicalized Grammar?Around this point, we must address exactly what wemean by an unlexicalized PCFG.
To the extent thatwe go about subcategorizing POS categories, manyof them might come to represent a single word.
Onemight thus feel that the approach of this paper is towalk down a slippery slope, and that we are merelyarguing degrees.
However, we believe that there is afundamental qualitative distinction, grounded in lin-guistic practice, between what we see as permittedin an unlexicalized PCFG as against what one findsand hopes to exploit in lexicalized PCFGs.
The di-vision rests on the traditional distinction betweenfunction words (or closed-class words) and contentwords (or open class or lexical words).
It is stan-dard practice in linguistics, dating back decades,to annotate phrasal nodes with important function-word distinctions, for example to have a CP[for]or a PP[to], whereas content words are not part ofgrammatical structure, and one would not have spe-cial rules or constraints for an NP[stocks], for exam-ple.
We follow this approach in our model: variousclosed classes are subcategorized to better representimportant distinctions, and important features com-monly expressed by function words are annotatedonto phrasal nodes (such as whether a VP is finite,or a participle, or an infinitive clause).
However, nouse is made of lexical class words, to provide eithermonolexical or bilexical probabilities.11At any rate, we have kept ourselves honest by es-timating our models exclusively by maximum like-lihood estimation over our subcategorized gram-mar, without any form of interpolation or shrink-age to unsubcategorized categories (although we domarkovize rules, as explained above).
This effec-11It should be noted that we started with four tags in the Penntreebank tagset that rewrite as a single word: EX (there), WP$(whose), # (the pound sign), and TO), and some others suchas WP, POS, and some of the punctuation tags, which rewriteas barely more.
To the extent that we subcategorize tags, therewill be more such cases, but many of them already exist in othertag sets.
For instance, many tag sets, such as the Brown andCLAWS (c5) tagsets give a separate sets of tags to each form ofthe verbal auxiliaries be, do, and have, most of which rewrite asonly a single word (and any corresponding contractions).VP?STOtoVP?VPVBappearNP?VPNP?NPCDthreeNNStimesPP?NPINonNP?PPNNPCNNJJlastNNnightVP?STOtoVP?VPVBappearNP?VPNP?NPCDthreeNNStimesPP?NPINonNP?PPNNPCNNNP-TMP?VPJJlastNN?TMPnight(a) (b)Figure 6: An error resolved with the TMP-NP annotation: (a)the incorrect baseline parse and (b) the correct TMP-NP parse.tively means that the subcategories that we break offmust themselves be very frequent in the language.In such a framework, if we try to annotate cate-gories with any detailed lexical information, manysentences either entirely fail to parse, or have onlyextremely weird parses.
The resulting battle againstsparsity means that we can only afford to make a fewdistinctions which have major distributional impact.Even with the individual-lexeme annotations in thissection, the grammar still has only 9255 states com-pared to the 7619 of the baseline model.6 Annotations Already in the TreebankAt this point, one might wonder as to the wisdomof stripping off all treebank functional tags, onlyto heuristically add other such markings back in tothe grammar.
By and large, the treebank out-of-thepackage tags, such as PP-LOC or ADVP-TMP, havenegative utility.
Recall that the raw treebank gram-mar, with no annotation or markovization, had an F1of 72.62% on our development set.
With the func-tional annotation left in, this drops to 71.49%.
Theh ?
2, v ?
1 markovization baseline of 77.77%dropped even further, all the way to 72.87%, whenthese annotations were included.Nonetheless, some distinctions present in the rawtreebank trees were valuable.
For example, an NPwith an S parent could be either a temporal NP or asubject.
For the annotation TMP-NP, we retained theoriginal -TMP tags on NPs, and, furthermore, propa-gated the tag down to the tag of the head of the NP.This is illustrated in figure 6, which also shows anexample of its utility, clarifying that CNN last nightis not a plausible compound and facilitating the oth-erwise unusual high attachment of the smaller NP.TMP-NP brought the cumulative F1 to 82.25%.
Notethat this technique of pushing the functional tagsdown to preterminals might be useful more gener-ally; for example, locative PPs expand roughly theROOTS?ROOT??NP?SDTThisVP?SVBZisVP?VPVBpanicNP?VPNNbuying.!??ROOTS?ROOT??NP?SDTThisVP?S-VBFVBZisNP?VPNNpanicNNbuying.!??
(a) (b)Figure 7: An error resolved with the SPLIT-VP annotation: (a)the incorrect baseline parse and (b) the correct SPLIT-VP parse.same way as all other PPs (usually as IN NP), butthey do tend to have different prepositions below IN.A second kind of information in the originaltrees is the presence of empty elements.
FollowingCollins (1999), the annotation GAPPED-S marks Snodes which have an empty subject (i.e., raising andcontrol constructions).
This brought F1 to 82.28%.7 Head AnnotationThe notion that the head word of a constituent canaffect its behavior is a useful one.
However, oftenthe head tag is as good (or better) an indicator of howa constituent will behave.12 We found several headannotations to be particularly effective.
First, pos-sessive NPs have a very different distribution thanother NPs ?
in particular, NP ?
NP ?
rules are onlyused in the treebank when the leftmost child is pos-sessive (as opposed to other imaginable uses like forNew York lawyers, which is left flat).
To address this,POSS-NP marked all possessive NPs.
This broughtthe total F1 to 83.06%.
Second, the VP symbol isvery overloaded in the Penn treebank, most severelyin that there is no distinction between finite and in-finitival VPs.
An example of the damage this con-flation can do is given in figure 7, where one needsto capture the fact that present-tense verbs do notgenerally take bare infinitive VP complements.
Toallow the finite/non-finite distinction, and other verbtype distinctions, SPLIT-VP annotated all VP nodeswith their head tag, merging all finite forms to a sin-gle tag VBF.
In particular, this also accomplishedCharniak?s gerund-VP marking.
This was extremelyuseful, bringing the cumulative F1 to 85.72%, 2.66%absolute improvement (more than its solo improve-ment over the baseline).12This is part of the explanation of why (Charniak, 2000)finds that early generation of head tags as in (Collins, 1999)is so beneficial.
The rest of the benefit is presumably in theavailability of the tags for smoothing purposes.8 DistanceError analysis at this point suggested that many re-maining errors were attachment level and conjunc-tion scope.
While these kinds of errors are undoubt-edly profitable targets for lexical preference, mostattachment mistakes were overly high attachments,indicating that the overall right-branching tendencyof English was not being captured.
Indeed, this ten-dency is a difficult trend to capture in a PCFG be-cause often the high and low attachments involve thevery same rules.
Even if not, attachment height isnot modeled by a PCFG unless it is somehow ex-plicitly encoded into category labels.
More com-plex parsing models have indirectly overcome thisby modeling distance (rather than height).Linear distance is difficult to encode in a PCFG?
marking nodes with the size of their yields mas-sively multiplies the state space.13 Therefore, wewish to find indirect indicators that distinguish highattachments from low ones.
In the case of two PPsfollowing a NP, with the question of whether thesecond PP is a second modifier of the leftmost NPor should attach lower, inside the first PP, the im-portant distinction is usually that the lower site is anon-recursive base NP.
Collins (1999) captures thisnotion by introducing the notion of a base NP, inwhich any NP which dominates only preterminals ismarked with a -B.
Further, if an NP-B does not havea non-base NP parent, it is given one with a unaryproduction.
This was helpful, but substantially lesseffective than marking base NPs without introducingthe unary, whose presence actually erased a usefulinternal indicator ?
base NPs are more frequent insubject position than object position, for example.
Inisolation, the Collins method actually hurt the base-line (absolute cost to F1 of 0.37%), while skippingthe unary insertion added an absolute 0.73% to thebaseline, and brought the cumulative F1 to 86.04%.In the case of attachment of a PP to an NP ei-ther above or inside a relative clause, the high NPis distinct from the low one in that the already mod-ified one contains a verb (and the low one may bea base NP as well).
This is a partial explanation ofthe utility of verbal distance in Collins (1999).
To13The inability to encode distance naturally in a naive PCFGis somewhat ironic.
In the heart of any PCFG parser, the funda-mental table entry or chart item is a label over a span, for ex-ample an NP from position 0 to position 5.
The concrete use ofa grammar rule is to take two adjacent span-marked labels andcombine them (for example NP[0,5] and VP[5,12] into S[0,12]).Yet, only the labels are used to score the combination.Length ?
40 LP LR F1 Exact CB 0 CBMagerman (1995) 84.9 84.6 1.26 56.6Collins (1996) 86.3 85.8 1.14 59.9this paper 86.9 85.7 86.3 30.9 1.10 60.3Charniak (1997) 87.4 87.5 1.00 62.1Collins (1999) 88.7 88.6 0.90 67.1Length ?
100 LP LR F1 Exact CB 0 CBthis paper 86.3 85.1 85.7 28.8 1.31 57.2Figure 8: Results of the final model on the test set (section 23).capture this, DOMINATES-V marks all nodes whichdominate any verbal node (V*, MD) with a -V. Thisbrought the cumulative F1 to 86.91%.
We also triedmarking nodes which dominated prepositions and/orconjunctions, but these features did not help the cu-mulative hill-climb.The final distance/depth feature we used was anexplicit attempt to model depth, rather than usedistance and linear intervention as a proxy.
WithRIGHT-REC-NP, we marked all NPs which containedanother NP on their right periphery (i.e., as a right-most descendant).
This captured some further at-tachment trends, and brought us to a final develop-ment F1 of 87.04%.9 Final ResultsWe took the final model and used it to parse sec-tion 23 of the treebank.
Figure 8 shows the re-sults.
The test set F1 is 86.32% for ?
40 words,already higher than early lexicalized models, thoughof course lower than the state-of-the-art parsers.10 ConclusionThe advantages of unlexicalized grammars are clearenough ?
easy to estimate, easy to parse with, andtime- and space-efficient.
However, the dismal per-formance of basic unannotated unlexicalized gram-mars has generally rendered those advantages irrel-evant.
Here, we have shown that, surprisingly, themaximum-likelihood estimate of a compact unlexi-calized PCFG can parse on par with early lexicalizedparsers.
We do not want to argue that lexical se-lection is not a worthwhile component of a state-of-the-art parser ?
certain attachments, at least, requireit ?
though perhaps its necessity has been overstated.Rather, we have shown ways to improve parsing,some easier than lexicalization, and others of whichare orthogonal to it, and could presumably be usedto benefit lexicalized parsers as well.AcknowledgementsThis paper is based on work supported in part by theNational Science Foundation under Grant No.
IIS-0085896, and in part by an IBM Faculty PartnershipAward to the second author.ReferencesJames K. Baker.
1979.
Trainable grammars for speech recogni-tion.
In D. H. Klatt and J. J. Wolf, editors, Speech Communi-cation Papers for the 97th Meeting of the Acoustical Societyof America, pages 547?550.Taylor L. Booth and Richard A. Thomson.
1973.
Applyingprobability measures to abstract languages.
IEEE Transac-tions on Computers, C-22:442?450.Sharon A. Caraballo and Eugene Charniak.
1998.
New figuresof merit for best-first probabilistic chart parsing.
Computa-tional Linguistics, 24:275?298.Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998.Edge-based best-first chart parsing.
In Proceedings of theSixth Workshop on Very Large Corpora, pages 127?133.Eugene Charniak.
1996.
Tree-bank grammars.
In Proc.
ofthe 13th National Conference on Artificial Intelligence, pp.1031?1036.Eugene Charniak.
1997.
Statistical parsing with a context-freegrammar and word statistics.
In Proceedings of the 14th Na-tional Conference on Artificial Intelligence, pp.
598?603.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In NAACL 1, pages 132?139.Eugene Charniak.
2001.
Immediate-head parsing for languagemodels.
In ACL 39.Noam Chomsky.
1965.
Aspects of the Theory of Syntax.
MITPress, Cambridge, MA.Michael John Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In ACL 34, pages 184?191.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, Univ.
of Pennsylvania.Jason Eisner and Giorgio Satta.
1999.
Efficient parsing forbilexical context-free grammars and head-automaton gram-mars.
In ACL 37, pages 457?464.Marilyn Ford, Joan Bresnan, and Ronald M. Kaplan.
1982.
Acompetence-based theory of syntactic closure.
In Joan Bres-nan, editor, The Mental Representation of Grammatical Re-lations, pages 727?796.
MIT Press, Cambridge, MA.Daniel Gildea.
2001.
Corpus variation and parser performance.In 2001 Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Donald Hindle and Mats Rooth.
1993.
Structural ambiguity andlexical relations.
Computational Linguistics, 19(1):103?120.Mark Johnson.
1998.
PCFG models of linguistic tree represen-tations.
Computational Linguistics, 24:613?632.Dan Klein and Christopher D. Manning.
2001.
Parsing withtreebank grammars: Empirical bounds, theoretical models,and the structure of the Penn treebank.
In ACL 39/EACL 10.David M. Magerman.
1995.
Statistical decision-tree models forparsing.
In ACL 33, pages 276?283.Andrew Radford.
1988.
Transformational Grammar.
Cam-bridge University Press, Cambridge.Dana Ron, Yoram Singer, and Naftali Tishby.
1994.
The powerof amnesia.
Advances in Neural Information Processing Sys-tems, volume 6, pages 176?183.
Morgan Kaufmann.
