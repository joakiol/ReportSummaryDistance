Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 164?173,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsParsing Natural Language Queries for Life Science KnowledgeTadayoshi HaraNational Institute of Informatics2-1-2 Hitotsubashi, Chiyoda-ku,Tokyo 101-8430, JAPANharasan@nii.ac.jpYuka TateisiFaculty of Informatics, Kogakuin University1-24-2 Nishi-shinjuku, Shinjuku-ku,Tokyo 163-8677, JAPANyucca@cc.kogakuin.ac.jpJin-Dong KimDatabase Center for Life Science2-11-16 Yayoi, Bunkyo-ku,Tokyo 113-0032, JAPANjdkim@dbcls.rois.ac.jpYusuke MiyaoNational Institute of Informatics2-1-2 Hitotsubashi, Chiyoda-ku,Tokyo 101-8430, JAPANyusuke@nii.ac.jpAbstractThis paper presents our preliminary work onadaptation of parsing technology toward natu-ral language query processing for biomedicaldomain.
We built a small treebank of natu-ral language queries, and tested a state-of-the-art parser, the results of which revealed thata parser trained on Wall-Street-Journal arti-cles and Medline abstracts did not work wellon query sentences.
We then experimentedan adaptive learning technique, to seek thechance to improve the parsing performance onquery sentences.
Despite the small scale of theexperiments, the results are encouraging, en-lightening the direction for effective improve-ment.1 IntroductionRecent rapid progress of life science resulted in agreatly increased amount of life science knowledge,e.g.
genomics, proteomics, pathology, therapeutics,diagnostics, etc.
The knowledge is however scat-tered in pieces in diverse forms over a large numberof databases (DBs), e.g.
PubMed, Drugs.com, Ther-apy database, etc.
As more and more knowledge isdiscovered and accumulated in DBs, the need fortheir integration is growing, and corresponding ef-forts are emerging (BioMoby1, BioRDF2, etc.
).Meanwhile, the need for a query language withhigh expressive power is also growing, to cope with1http://www.biomoby.org/2http://esw.w3.org/HCLSIG BioRDF Subgroupthe complexity of accumulated knowledge.
For ex-ample, SPARQL3 is becoming an important querylanguage, as RDF4 is recognized as a standard in-teroperable encoding of information in databases.SPARQL queries are however not easy for humanusers to compose, due to its complex vocabulary,syntax and semantics.
We propose natural language(NL) query as a potential solution to the problem.Natural language, e.g.
English, is the most straight-forward language for human beings.
Extra trainingis not required for it, yet the expressive power isvery high.
If NL queries can be automatically trans-lated into SPARQL queries, human users can accesstheir desired knowledge without learning the com-plex query language of SPARQL.This paper presents our preliminary work forNL query processing, with focus on syntactic pars-ing.
We first build a small treebank of naturallanguage queries, which are from Genomics track(Hersh et al, 2004; Hersh et al, 2005; Hersh et al,2006; Hersh et al, 2007) topics (Section 2 and 3).The small treebank is then used to test the perfor-mance of a state-of-the-art parser, Enju (Ninomiyaet al, 2007; Hara et al, 2007) (Section 4).
Theresults show that a parser trained on Wall-Street-Journal (WSJ) articles and Medline abstracts willnot work well on query sentences.
Next, we ex-periment an adaptive learning technique, to seek thechance to improve the parsing performance on querysentences.
Despite the small scale of the experi-ments, the results enlighten directions for effective3http://www.w3.org/TR/rdf-sparql-query/4http://www.w3.org/RDF/164GTREC04 05 06 07Declarative 1 0 0 0Imperative 22 60 0 0Infinitive 1 0 0 0Interrogative- WP/WRB/WDT 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50- Non-wh 5 0 0 0NP 14 0 0 0Total 58 60 28 50Table 1: Distribution of sentence constructionsimprovement (Section 5).2 Syntactic Features of Query SentencesWhile it is reported that the state-of-art NLP tech-nology shows reasonable performance for IR orIE applications (Ohta et al, 2006), NLP technol-ogy has long been developed mostly for declara-tive sentences.
On the other hand, NL queries in-clude wide variety of sentence constructions suchas interrogative sentences, imperative sentences, andnoun phrases.
Table 1 shows the distribution of theconstructions of the 196 query sentences from thetopics of the ad hoc task of Genomics track 2004(GTREC04) and 2005 (GTREC05) in their narra-tive forms, and the queries for the passage retrievaltask of Genomics track 2006 (GTREC06) and 2007(GTREC07).GTREC04 set has a variety of sentence construc-tions, including noun phrases and infinitives, whichare not usually considered as full sentences.
In the2004 track, the queries were derived from interviewseliciting information needs of real biologists, with-out any control on the sentence constructions.GTREC05 consists only of imperative sentences.In the 2005 track, a set of templates were derivedfrom an analysis of the 2004 track and other knownbiologist information needs.
The derived templateswere used as the commands to find articles describ-ing biological interests such as methods or roles ofgenes.
Although the templates were in the form?Find articles describing ...?, actual obtained imper-atives begin with ?Describe the procedure or methodfor?
(12 sentences), ?Provide information about?
(36 sentences) or ?Provide information on?
(12 sen-tences).GTREC06 consists only of wh-questions where awh-word constitutes a noun phrase by itself (i.e.
itsSVPNPPPNPPPNP NP NP NPVB NNS IN NN IN NN[ ] Find articles abut function of FancD2Figure 1: The tree structure for an imperative sentencepart-of-speech is the WP in Penn Treebank (Marcuset al, 1994) POS tag set) or is an adverb (WRB).
Inthe 2006 track, the templates for the 2005 track werereformulated into the constructions of questions andwere then utilized for deriving the questions.
For ex-ample, the templates to find articles describing therole of a gene involved in a given disease is refor-mulated into the question ?What is the role of genein disease?
?GTREC07 consists only of wh-questions where awh-word serves as a pre-nominal modifier (WDT).In the 2007 track, unlike in those of last two years,questions were not categorized by the templates, butwere based on biologists?
information needs wherethe answers were lists of named entities of a giventype.
The obtained questions begin with ?what +entity type?
(45 sentences), ?which + entity type?
(4sentences), or ?In what + entity type?
(1 sentence).In contrast, the GENIA Treebank Corpus (Tateisiet al, 2005)5 is estimated to have no imperative sen-tences and only seven interrogative sentences (seeSection 5.2.2).
Thus, the sentence constructions inGTREC04?07 are very different from those in theGENIA treebank.3 Treebanking GTREC query sentencesWe built a treebank (with POS) on 196 query sen-tences following the guidelines of the GENIA Tree-bank (Tateisi and Tsujii, 2006).
The queries werefirst parsed using the Stanford Parser (Klein andManning, 2003), and manual correction was made5http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+Treebank165SBARQSQVPWHNP[i168]   NP[i169?i168]  NP[?i169] PPWDT NNS VBP   VBN   IN NNWhat toxicities are [ ] associated [ ] with cytarabineFigure 2: The tree structure for an interrogative sentenceby the second author.
We tried to follow the guide-line of the GENIA Treebank as closely as possible,but for the constructions that are rare in GENIA, weused the ATIS corpus in Penn Treebank (Bies et al,1995), which is also a collection of query sentences,for reference.Figure 1 shows the tree for an imperative sen-tence.
A leaf node with [ ] corresponds to a nullconstituent.
Figure 2 shows the tree for an inter-rogative sentence.
Coindexing is represented byassigning an ID to a node and a reference to theID to the node which is coindexed.
In Figure 2,WHNP[i168] means that the WHNP node is indexedas i168, NP[i169?i168] means that the NP node isindexed as i169 and coindexed to the i168 node, andNP[?i169] means that the node is coindexed to thei169 node.
In this sentence, which is a passive wh-question, it is assumed that the logical object (whattoxicities) of the verb (associate) is moved to thesubject position (the place of i169) and then movedto the sentence-initial position (the place of i168).As most of the query sentences are either impera-tive or interrogative, there are more null constituentscompared to the GENIA Corpus.
In the GTRECquery treebank, 184 / 196 (93.9%) sentences con-tained one or more null constituents, whereas in GE-NIA, 12,222 / 18,541 (65.9%) sentences did.
We ex-pected there are more sentences with multiple nullconstituents in GTREC compared to GENIA, due tothe frequency of passive interrogative sentences, buton the contrary the number of sentences containingmore than one null constituents are 65 (33.1%) inGTREC, and 6,367 (34.5%) in GENIA.
This may bedue to the frequency of relative clauses in GENIA.4 Parsing system and extraction ofimperative and question sentencesWe introduce the parser and the POS tagger whoseperformances are examined, and the extraction ofimperative or question sentences from GTREC tree-bank on which the performances are measured.4.1 HPSG parserThe Enju parser (Ninomiya et al, 2007)6 is a deepparser based on the HPSG formalism.
It producesan analysis of a sentence that includes the syntac-tic structure (i.e., parse tree) and the semantic struc-ture represented as a set of predicate-argument de-pendencies.
The grammar is based on the standardHPSG analysis of English (Pollard and Sag, 1994).The parser finds a best parse tree scored by a max-ent disambiguation model using a Cocke-Kasami-Younger (CKY) style algorithm.We used a toolkit distributed with the Enju parserfor training the parser with a Penn Treebank style(PTB-style) treebank.
The toolkit initially convertsthe PTB-style treebank into an HPSG treebank andthen trains the parser on it.
We used a toolkit dis-tributed with the Enju parser for extracting a HPSGlexicon from a PTB-style treebank.
The toolkit ini-tially converts the PTB-style treebank into an HPSGtreebank and then extracts the lexicon from it.The HPSG treebank converted from the test sec-tion was used as the gold-standard in the evaluation.As the evaluation metrics of the Enju parser, we usedlabeled and unlabeled precision/recall/F-score of thepredicate-argument dependencies produced by theparser.
A predicate-argument dependency is repre-sented as a tuple of ?wp, wa, r?, where wp is thepredicate word, wa is the argument word, and r isthe label of the predicate-argument relation, suchas verb-ARG1 (semantic subject of a verb) andprep-ARG1 (modifiee of a prepositional phrase).4.2 POS taggerThe Enju parser assumes that the input is alreadyPOS-tagged.
We use a tagger in (Tsuruoka et al,2005).
It has been shown to give a state-of-the-artaccuracy on the standard Penn WSJ data set and alsoon a different text genre (biomedical literature) whentrained on the combined data set of the WSJ data and6http://www-tsujii.is.s.u-tokyo.ac.jp/enju166the target genre (Tsuruoka et al, 2005).
Since ourtarget is biomedical domain, we utilize the taggeradapted to the domain as a baseline, which we call?the GENIA tagger?.4.3 Extracting imperative and questionsentences from GTREC treebankIn GTREC sentences, two major constructions ofsentences can be observed: imperative and questionsentences.
These two types of sentences have differ-ent sentence constructions and we will observe theimpact of each or both of these constructions on theperformances of parsing or POS-tagging.
In orderto do so, we collected imperative and question sen-tences from our GTREC treebank as follows:?
GTREC imperatives - Most of the impera-tive sentences in GTREC treebank begin withempty subjects ?
(NP-SBJ */-NONE-)?.
We ex-tracted such 82 imperative sentences.?
GTREC questions - Interrogative sentencesare annotated with the phrase label ?SBARQ?or ?SQ?, where ?SBARQ?
and ?SQ?
respec-tively denote a wh-question and an yes/no ques-tion.
We extracted 98 interrogative sentenceswhose top phrase labels were either of them.5 ExperimentsWe examine the POS-tagger and the parser for thesentences in the GTREC corpus.
They are adaptedto each of GTREC overall, imperatives, and ques-tions.
We then observe how the parsing or POS-tagging accuracies are improved and analyze whatis critical for parsing query sentences.5.1 Experimental settings5.1.1 Dividing corporaWe prepared experimental datasets for the follow-ing four domains:?
GENIA Corpus (GENIA) (18,541 sentences)Divided into three parts for training (14,849sentences), development test (1,850 sentences),and final test (1,842 sentences).?
GTREC overall (196 sentences)Divided into two parts: one for ten-folds crossvalidation test (17-18 ?
10 sentences) and theother for error analysis (17 sentences)Target GENIA tagger Adapted taggerGENIA 99.04% -GTREC (overall) 89.98% 96.54%GTREC (imperatives) 90.32% 97.30%GRREC (questions) 89.25% 94.77%Table 2: Accuracy of the POS tagger for each domain?
GTREC imperatives (82 sentences)Divided into two parts: one for ten-folds crossvalidation test (7-8 ?
10 sentences) and theother for error analysis (7 sentences)?
GTREC questions (98 sentences)Divided into two parts: one for ten-folds crossvalidation test (9 ?
10 sentences) and the otherfor error analysis (8 sentences)5.1.2 Adaptation of POS tagger and parserIn order to adapt the POS tagger and the parser toa target domain, we took the following methods.?
POS tagger - For the GTREC overall / impera-tives / questions, we replicated the training datafor 100,000 times and utilized the concatenatedreplicas and GENIA training data in (Tsuruokaet al, 2005) for training.
For POS tagger, thenumber of replicas of training data was deter-mined among 10n(n = 0, .
.
.
, 5) by testingthese numbers on development test sets in threeof ten datasets of cross validation.?
Enju parser - We used a toolkit in the Enjuparser (Hara et al, 2007).
As a baseline model,we utilized the model adapted to the GENIACorpus.
We then attempted to further adapt themodel to each domain.
In this paper, the base-line model is called ?the GENIA parser?.5.2 POS tagger and parser performancesTable 2 and 3 respectively show the POS tagging andthe parsing accuracies for the target domains, andFigure 3 and 4 respectively show the POS taggingand the parsing accuracies for the target domainsgiven by changing the size of the target training data.The POS tagger could output for each word eitherof one-best POS or POS candidates with probabili-ties, and the Enju parser could take either of the twooutput types.
The bracketed numbers in Table 3 and167Parser GENIA AdaptedPOS Gold GENIA tagger Adapted tagger Gold GENIA tagger Adapted taggerFor GENIA 88.54 88.07 (88.00) - - - -For GTREC overall 84.37 76.81 (72.43) 83.46 (81.96) 89.00 76.98 (74.44) 86.98 (85.42)For GTREC imperatives 85.19 78.54 (77.75) 85.71 (85.48) 89.42 74.40 (74.84) 88.97 (88.67)For GTREC questions 85.45 76.25 (67.27) 83.55 (80.46) 87.33 81.41 (71.90) 84.87 (82.70)[ using POS candidates with probabilities (using only one best POS) ]Table 3: Accuracy of the Enju parser for GTREC70758085900 20 40 60 80 100 120 140F-scoreCorpus size (sentences)70758085900 20 40 60F-scoreCorpus size (sentences)657075808590950 20 40 60 80F-scoreCorpus size (sentences)Adapted parser, gold POSAdapted parser, adapted tagger (prob.
)GENIA parser, adapted tagger (prob.
)Adapted parser, GENIA tagger (prob.
)Adapted parser, adapted tagger (1best)GENIA parser, adapted tagger (1best)Adapted parser, GENIA tagger (1best)For GTREC imperatives For GTREC questionsFor GTREC overallFigure 4: Parsing accuracy vs. corpus size8890929496980 50 100 150Accuracy(%)Corpus size (sentences)GTREC overallGTREC imperativesGTREC questionsFigure 3: POS tagging accuracy vs. corpus sizethe dashed lines in Figure 4 show the parsing accu-racies when we utilized one-best POS given by thePOS tagger, and the other numbers and lines showthe accuracies given by POS candidates with proba-bilities.
In the rest of this section, when we just say?POS tagger?, the tagger?s output is POS candidateswith probabilities.Table 4 and 5 respectively compare the types ofPOS tagging and parsing errors for each domainbetween before and after adapting the POS tagger,and Table 6 compares the types of parsing errors forCorrect ?
Error GENIA tagger Adapted taggerFor GTREC overall (17 sentences)NN ?
NNP 4 0.6VB ?
NN 4 0WDT ?
WP 4 0NN ?
JJ 1 1.9For GTREC imperative (seven sentences)FW ?
NNP / NN / JJ 7 4VB ?
NN 4 0NN ?
NNP 2 0For GTREC question (eight sentences)WDT ?
WP 3 0VB ?
VBP 2 1NNS ?
VBZ 2 0(The table shows only error types observed more thanonce for either of the taggers)Table 4: Tagging errors for each of the GTREC corporaeach domain between before and after adapting theparser.
The numbers of errors for the rightmost col-umn in each of the tables were given by the averageof the ten-folds cross validation results.In the following sections, we examine the im-pact of the performances of the POS taggers or theparsers on parsing the GTREC documents.168GENIA parserError types GENIA tagger Adapted taggerFor GTREC overall (17 sentences)Failure in detecting verb 12 0.2Root selection 6 0Range of NP 5 5PP-attachment 4 3Determiner / pronoun 4 1Range of verb subject 4 4Range of verb object 3 3Adjective / modifier noun 2 3For GTREC imperatives (seven sentences)Failure in detecting verb 8 0Root selection 4 0Range of NP 3 4PP-attachment 3 1.8Range of PP 2 2For GTREC questions (eight sentences)Range of coordination 5 3Determiner / pronoun 3 0PP-attachment 3 1Range of PP 2 2Subject for verb 2 1(The table shows only the types of parsing errors observed morethan once for either of the parsers)Table 5: Impact of adapting POS tagger on parsing errors5.2.1 Impact of POS tagger on parsingIn Table 2, for each of the GTREC corpora,the GENIA tagger dropped its tagging accuracy byaround nine points, and then recovered five to sevenpoints by the adaptation.
According to this behav-ior of the tagger, Table 3 shows that the GENIA andthe adapted parsers with the GENIA tagger droppedtheir parsing accuracies by 6?15 points in F-scorefrom the accuracies with the gold POS, and then re-covered the accuracies within two points below theaccuracies with the gold POS.
The performance ofthe POS tagger would thus critically affect the pars-ing accuracies.In Figure 3, we can observe that the POS taggingaccuracy for each corpus rapidly increased only forfirst 20?30 sentences, and after that the improvementspeed drastically declined.
Accordingly, in Figure 4,the line for the adapted parser with the adapted tag-ger (the line with triangle plots) rose rapidly for thefirst 20?30 sentences, and after that slowed down.We explored the tagging and parsing errors, andanalyze the cause of the initial accuracy jump andthe successive improvement depression.Gold POSError types GENIA parser Adapted parserFor GTREC overall (17 sentences)Range of NP 5 1.3Range of verb subject 3 2.6PP-attachment 3 2.7Whether verb takesobject & complement 3 2.9Range of verb object 2 1For GTREC imperatives (seven sentences)Range of NP 4 1.1PP-attachment 2 1.6Range of PP 2 0.3Preposition / modifier 2 2For GTREC questions (eight sentences)Coordination / conjunction 2 2.2Auxiliary / normal verb 2 2.6Failure in detecting verb 2 2.6(The table shows only the types of parsing errors observed morethan once for either of the parsers)Table 6: Impact of adapting parser on parsing errorsCause of initial accuracy jumpIn Table 4, ?VB ?
NN?
tagging errors wereobserved only in imperative sentences and drasti-cally decreased by the adaptation.
In a impera-tive sentence, a verb (VB) usually appears as thefirst word.
On the other hand, the GENIA taggerwas trained mainly on the declarative sentences andtherefore would often take the first word in a sen-tence as the subject of the sentence, that is, noun(NN).
When the parser received a wrong NN-tag fora verb, the parser would attempt to believe the infor-mation (?failure in detecting verb?
in Table 6) andcould then hardly choose the NN-tagged word as amain verb (?root selection?
in Table 6).
By adaptingthe tagger, the correct tag was given to the verb andthe parser could choose the verb as a main verb.
?WDT ?
WP?
tagging errors were observed onlyin the question sentences and also drastically de-creased.
For example, in the sentence ?What toxici-ties are associated with cytarabine?
?, ?What?
worksas a determiner (WDT) which takes ?toxicities?,while the GENIA tagger often took this ?What?
as apronoun (WP) making a phrase by itself.
This wouldbe because the training data for the GENIA taggerwould contain 682 WP ?what?
and only 27 WDT?what?.
WP ?what?
could not make a noun phraseby taking a next noun, and then the parsing of theparsing would corrupt (?determiner / pronoun?
inTable 5).
By adapting the tagger, ?WDT?
tag was169given to ?What?, and the parser correctly made aphrase ?What toxicities?.Since the variation of main verbs in GTREC im-peratives is very small (see Section 2) and that ofinterrogatives is also very small, in order to cor-rect the above two types of errors, we would requireonly small training data.
In addition, these types oferrors widely occurred among imperatives or ques-tions, the accuracy improvement by correcting theerrors was very large.
The initial rapid improvementwould thus occur.Cause of improvement depression?NN ?
NNP?
tagging errors would come fromthe description style of words.
In the GTRECqueries, technical terms, such as the names of dis-eases or proteins, sometimes begin with capital char-acters.
The GENIA tagger would take the capi-talized words not as a normal noun (NN) but as aproper noun (NNP).
By adaptation, the tagger wouldhave learned the capital usage for terms and the er-rors then decreased.However, in order to achieve such improvement,we would have to wait until a target capitalized termis added to the training corpus.
?FW ?
NNP / NN/ JJ?, ?NN ?
JJ?, and several other errors would besimilar to this type of errors in the point that, theywould be caused by the difference in annotation pol-icy or description style between the training data forthe GENIA tagger and the GTREC queries.
?VB ?
VBP?
errors were found in questions.
Forexample, ?affect?
in the question ?How do muta-tions in Sonic Hedgehog genes affect developmen-tal disorders??
was base form (VB), while the GE-NIA tagger took it as a present tense (VBP) sincethe GENIA tagger would be unfamiliar with suchverb behavior in questions.
By adaptation, the tag-ger would learn that verbs in the domain tend to takebase forms and the errors then decreased.However, the tagger model based on local contextfeatures could not substantially solve the problem.VBP of course could appear in question sentences.We observed that a verb to be VBP was tagged withVB by the adapted tagger.
In order to distinguishVB from VBP, we should capture longer distancedependencies between auxiliary and main verbs.In tagging, the fact that the above two types oferrors occupied most of the errors other than the er-rors involved in the initial jump, would be relatedto why the accuracy improvement got so slowly,which would lead to the improvement depression ofthe parsing performances.
With the POS candidateswith probabilities, the possibilities of correct POSswould increase, and therefore the parser would givehigher parsing performances than using only one-best POSs (see Table 3 and Figure 4).Anyway, the problems were not substantiallysolved.
For these tagging problems, just adding thetraining data would not work.
We might need re-construct the tagging system or re-consider the fea-ture designs of the model.5.2.2 Impact of parser itself on parsingFor the GTREC corpora, the GENIA parser withgold POSs lowered the parsing accuracy by morethan three points than for the GENIA Corpus, whilethe adaptation of the parser recovered a few pointsfor each domain (second and fifth column in Table3).
Figure 4 would also show that we could improvethe parser?s performance with more training data foreach domain.
For GTREC questions, the parsing ac-curacy dropped given the maximum size of the train-ing data.
Our training data is small and thereforesmall irregular might easily make accuracies drop orrise.
7 We might have to prepare more corpora forconfirming our observation.Table 6 would imply that the major errors for allof these three corpora seem not straightforwardly as-sociated with the properties specific to imperative orquestion sentences.
Actually, when we explored theparse results, errors on the sentence constructionsspecific to the two types of sentences would hardlybe observed.
(?Failure in detecting verb?
errors inGTREC questions came from other causes.)
Thiswould mean that the GENIA parser itself has poten-tial to parse the imperative or question sentences.The training data of the GENIA parser consistsof the WSJ Penn Treebank and the GENIA Corpus.As long as we searched with our extraction methodin Section 4.3, the WSJ and GENIA Corpus seemrespectively contain 115 and 0 imperative, and 4327This time we could not analyze which training data affectedthe decrease, because through the cross validation experimentseach sentence was forced to be once final test data.
However,we would like to find the reason for this accuracy decrease insome way.170and seven question sentences.
Unlike the POS tag-ger, the parser could convey more global sentenceconstructions from these sentences.Although the GENIA parser might understand thebasic constructions of imperative or question sen-tences, by adaptation of the parser to the GTRECcorpora, we could further learn more local construc-tion features specific to GTREC, such as word se-quence constructing a noun phrase, attachment pref-erence of prepositions or other modifiers.
The errorreduction in Table 6 would thus be observed.However, we also observed that several types oferrors were still mostly unsolved after the adapta-tion.
Choosing whether to add complements forverbs or not, and distinguishing coordinations fromconjunctions seems to be difficult for the parser.
Iftwo question sentences were concatenated by con-junctions into one sentence, the parser would tend tofail to analyze the sentence construction for the lat-ter sentence.
The remaining errors in Table 6 wouldimply that we should also re-consider the model de-signs or the framework itself for the parser in addi-tion to just increasing the training data.6 Related workSince domain adaptation has been an extensive re-search area in parsing research (Nivre et al, 2007),a lot of ideas have been proposed, including un-/semi-supervised approaches (Roark and Bacchiani,2003; Blitzer et al, 2006; Steedman et al, 2003;McClosky et al, 2006; Clegg and Shepherd, 2005;McClosky et al, 2010) and supervised approaches(Titov and Henderson, 2006; Hara et al, 2007).Their main focus was on adapting parsing modelstrained with a specific genre of text (in most casesPTB-WSJ) to other genres of text, such as biomed-ical research papers.
A major problem tackled insuch a task setting is the handling of unknown wordsand domain-specific ways of expressions.
However,as we explored, parsing NL queries involves a sig-nificantly different problem; even when all words ina sentence are known, the sentence has a very differ-ent construction from declarative sentences.Although sentence constructions have gained lit-tle attention, a notable exception is (Judge et al,2006).
They pointed out low accuracy of state-of-the-art parsers on questions, and proposed super-vised parser adaptation by manually creating a tree-bank of questions.
The question sentences are anno-tated with phrase structure trees in the PTB scheme,although function tags and empty categories areomitted.
An LFG parser trained on the treebank thenachieved a significant improvement in parsing ac-curacy.
(Rimell and Clark, 2008) also worked onquestion parsing.
They collected question sentencesfrom TREC 9-12, and annotated the sentences withPOSs and CCG (Steedman, 2000) lexical categories.They reported a significant improvement in CCGparsing without phrase structure annotations.On the other hand, (Judge et al, 2006) also im-plied that just increasing the training data would notbe enough.
We went further from their work, builta small but complete treebank for NL queries, andexplored what really occurred in HPSG parsing.7 ConclusionIn this paper, we explored the problem in parsingqueries.
We first attempted to build a treebank onqueries for biological knowledge and successfullyobtained 196 annotated GTREC queries.
We nextexamined the performances of the POS tagger andthe HPSG parser on the treebank.
In the experi-ments, we focused on the two dominant sentenceconstructions in our corpus: imperatives and ques-tions, extracted them from our corpus, and then alsoexamined the parser and tagger for them.The experimental results showed that the POStagger?s mis-tagging to main verbs in imperativesand wh-interrogatives in questions critically de-creased the parsing performances, and that oursmall corpus could drastically decrease such mis-tagging and consequently improve the parsing per-formances.
The experimental results also showedthat the parser itself could improve its own perfor-mance by increasing the training data.
On the otherhand, the experimental results suggested that thePOS tagger or the parser performance would stag-nate just by increasing the training data.In our future research, on the basis of our findings,we would like both to build more training data forqueries and to reconstruct the model or reconsiderthe feature design for the POS tagger and the parser.We would then incorporate the optimized parser andtagger into NL query processing applications.171ReferencesAnn Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for Treebank IIstyle ?
Penn Treebank project.
Technical report, De-partment of Linguistics, University of Pennsylvania.John Blitzer, Ryan Mcdonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 120?128, Sydney, Australia.A.
B. Clegg and A. Shepherd.
2005.
Evaluating and in-tegrating treebank parsers on a biomedical corpus.
InProceedings of the ACL 2005 Workshop on Software,Ann Arbor, Michigan.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Evaluating impact of re-training a lexical dis-ambiguation model on domain adaptation of an hpsgparser.
In Proceedings of 10th International Confer-ence on Parsing Technologies (IWPT 2007), pages 11?22.William R. Hersh, Ravi Teja Bhupatiraju, L. Ross,Aaron M. Cohen, Dale Kraemer, and Phoebe Johnson.2004.
TREC 2004 Genomics Track Overview.
In Pro-ceedings of the Thirteenth Text REtrieval Conference,TREC 2004.William R. Hersh, Aaron M. Cohen, Jianji Yang,Ravi Teja Bhupatiraju, Phoebe M. Roberts, andMarti A. Hearst.
2005.
TREC 2005 Genomics TrackOverview.
In Proceedings of the Fourteenth Text RE-trieval Conference, TREC 2005.William R. Hersh, Aaron M. Cohen, Phoebe M. Roberts,and Hari Krishna Rekapalli.
2006.
TREC 2006 Ge-nomics Track Overview.
In Proceedings of the Fif-teenth Text REtrieval Conference, TREC 2006.William R. Hersh, Aaron M. Cohen, Lynn Ruslen, andPhoebe M. Roberts.
2007.
TREC 2007 GenomicsTrack Overview.
In Proceedings of The Sixteenth TextREtrieval Conference, TREC 2007.John Judge, Aoife Cahill, and Josef van Genabith.2006.
Questionbank: Creating a Corpus of Parsing-Annotated Questions.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the ACL, pages 497?504.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert Macintyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.
InProceedings of ARPA Human Language TechnologyWorkshop.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 337?344, Sydney, Australia.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic Domain Adaptation for Parsing.
InProceedings of the 2010 Annual Conference of theNorth American Chapter of the ACL, pages 28?36, LosAngeles, California.Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,and Jun?ichi Tsujii.
2007.
A log-linear model with ann-gram reference distribution for accurate hpsg pars-ing.
In Proceedings of 10th International Conferenceon Parsing Technologies (IWPT 2007), pages 60?68.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on depen-dency parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007, pages 915?932,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Laura Rimell and Stephen Clark.
2008.
Adapting aLexicalized-Grammar Parser to Contrasting Domains.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 475?584.Brian Roark and Michiel Bacchiani.
2003.
Supervisedand unsupervised PCFG adaptation to novel domains.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 126?133, Edmonton, Canada.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In Pro-ceedings of the tenth conference on European chap-ter of the Association for Computational Linguistics,pages 331?338, Budapest, Hungary.Mark Steedman.
2000.
The Syntactic Process.
THEMIT Press.Yuka Tateisi and Jun?ichi Tsujii.
2006.
GENIA Anno-tation Guidelines for Treebanking.
Technical ReportTR-NLP-UT-2006-5, Tsujii Laboratory, University ofTokyo.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, andJun?ichi Tsujii.
2005.
Syntax Annotation for the GE-NIA corpus.
In Proceedings of the Second Interna-tional Joint Conference on Natural Language Process-172ing (IJCNLP 2005), Companion volume, pages 222?227.Ivan Titov and James Henderson.
2006.
Porting statis-tical parsers with data-defined kernels.
In Proceed-ings of the Tenth Conference on Computational Natu-ral Language Learning, pages 6?13, New York City.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advances inInformatics - 10th Panhellenic Conference on Infor-matics, volume LNCS 3746, pages 382?392, Volos,Greece, November.
ISSN 0302-9743.173
