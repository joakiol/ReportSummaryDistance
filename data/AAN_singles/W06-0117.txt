Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 122?125,Sydney, July 2006. c?2006 Association for Computational LinguisticsFrance Telecom R&D Beijing Word Segmenterfor Sighan Bakeoff 2006Wu LiuFrance Telecom R&DBeijingwu.liu@francetelecom.comHeng LiFrance Telecom R&D Bei-jingheng.li@francetelecom.comYuan DongBeijing University ofPosts and Telecommunicationsyuandong@bupt.edu.cnNan HeBeijing University of Postsand Telecommunicationshn.ft.pris@gmail.comHaitao LuoNortheastern University ofChinaluoht@ics.neu.edu.cnHaila WangFrance Telecom R&D Beijinghaila.wang@francetelecom.comAbstractThis paper presents two word segmenta-tion (WS) systems and a named entityrecognition (NER) system in FranceTelecom R&D Beijing.
The one systemof WS is for open tracks based on n-gram language model and another one isfor closed tracks based on maximum en-tropy approach.
The NER system uses ahybrid algorithm based on Class-basedlanguage model and rule-based knowl-edge.
These systems are all augmentedwith a set of post-processors.1 IntroductionThe FTRD team participated in MSRA Open,MSRA Closed and CityU Closed tracks of theWS bakeoff and MSRA Open track of the NERbakeoff, and achieved the state-of-the-art per-formance in these tracks.
Analysis of the resultsshows that each component of these systemscontributed to the scores.2 System Description2.1 MSRA Open track of WSThe system used in open track of WS is based onthe system (Li 2005) participated in the secondinternational WS bakeoff.
We mainly modify thefactoid detection rules and add the GKB (TheGrammatical Knowledge-base of ContemporaryChinese) dictionary.
The system also has a fewpostprocessors.
The main postprocessors includenamed entity recognizers and TBL (Transforma-tion-Based Learning) component.2.1.1 Basic systemIn our basic system, Chinese words can be cate-gorized into one of the following types: lexiconwords, morphological words, factoids, name en-tities.
These types of words were processed indifferent ways in our system, and were incorpo-rated into a unified statistical framework of thetrigram language model.
The details about thebasic system are reported in (Li 2005).2.1.2 Factoid detectionThe factoid rules used in the basic system weresummarized according to the MSRA trainingdata.
The Tokenization Guidelines of ChineseText (V5.0) was provided by MSRA in thisbakeoff.
We used the Guidelines to rewrite thefactoid rules, and the performance had the dis-tinct improvement.1222.1.3 Named entity identificationThe named entity recognizer is the one partici-pated in the NER bakeoff, as shown in figure 1.In the section 2.3, we will describe in detail.2.2 System Used in Close tracksThe system used in closed tracks of WS is basedon maximum entropy approach.
The system alsohas a few postprocessors.
The main postproces-sors include combining the separated words andTBL component.2.2.1 Basic systemThe basic system is similar to (Ng and Low,2004).
We used the Tsujii laboratory maximumentropy package v2.0 (http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/) to train our mod-els.
For CityU closed track, the basic features arethe same as (Ng and Low, 2004).
For MSRAclosed track, we used two sets of basic features.The one is similar to (Ng and Low, 2004) and wechange the window size of another one from 2 to3, so we trained two models for MSRA closedtrack and submitted two results.2.2.2 Post processingFirstly, we extracted one lexicon from each train-ing data.
For MSRA closed track, the postpro-cessor only combined the words which appearedin the lexicon but were separated in the test result.For CityU closed track, we firstly used the fac-toid tool provided by the open system of WS tocombine the separated factoid words, and thenwe used the lexicon to combine the separatedwords, at last the TBL was applied to the testresult.2.3 MSRA Open track of NERThe system used a hybrid algorithm which cancombine a class-based statistical model (Gao2004) with various types of rule-based knowl-edge very well.
All the words were categorizedinto three types: Lexicon words (LWs), Factoidwords (FTs), Named Entity (NEs).
Accordingly,three main components were included to identifyeach kind of named entities: basic word candi-dates, NE combination and Viterbi search, asshown in Figure 1.Figure 1 FTRD NE RecognizerThe recognizer was applied to open track of WSand we used it to participate in the MSRA opentrack of NER.
The system also had a TBL post-processor.2.4 TBLIn our system, the open source toolkit fnTBL(http://nlp.cs.jhu.edu/~rflorian/fntbl/index.html)is chosen.
Coping with word segmentation task,we utilized a method called ?LMR?
taggingwhich was the same as (Nianwen Xue and LibinShen 2003).
Two rule template sets were used inour system.
The complicated one had 40 tem-plates, which covered various kinds of wordsposition and tag position occurrence, i.e., consid-ering contextual information of words and tags.For example, rule ?pos_0 word_0 word_1word_2 => pos?
could generate rules containinginformation about current word, current word?stag, the next word and the word after next.
Theother rule template neglected tag information, ittook only contextual word information into ac-count.
For an instance, ?word_0 word_1 word_2=> pos?.
The task of WS applied the two ruletemplate sets, and the task of NER only appliedthe complicated one.
In the Section 3, we willcompare the two rule template sets.3 Evaluation3.1 Open tracks3.1.1 MSRA Open track of WSIn this open track, we used one lexicon of294,382 entries, which included the entries of42,430 MDWs (Morphological Derived Words)generated from the GKB dictionary, 12,487 PNs,22,907 LNs and 29,032 ONs, 10,414 four-character idioms, plus the word lists generatedfrom the training data provided by the secondinternational Chinese Word Segmentation bake-off and 80114 GKB words.
We also used thetraining data provided by the last bakeoff fortraining our trigram word-based language model.123Table 1 presents the results of this track.
Forcomparison, we also include in the table (Row 1)the results of basic system.
From Row 2 to Row11, it shows the relative contribution of eachcomponent and resource to the overall wordsegmentation performance.
The second columnshows the recall, the third column the precision,and the fourth column F-score.
The last two col-umns present the recall of the OOV words andthe recall of IV words, respectively.
(%)     R      P     F  Roov    Riv1.basicsystem 0.971 0.958 0.964 0.590 0.9842.1+newfactoid 0.966 0.958 0.962 0.642 0.9783.1+GKB lexicon 0.975 0.966 0.971 0.716 0.9844.3+newfactoid 0.971 0.967 0.969 0.768 0.9785.4+NE  0.971 0.973 0.972 0.838 0.9756.5+TBL 0.977 0.976 0.977 0.840 0.9827.5+newTBL 0.980 0.978 0.979 0.839 0.9858.4+TBL 0.977 0.970 0.974 0.769 0.9849.4+newTBL 0.980 0.971 0.975 0.769 0.98710.8+NE  0.977 0.976 0.977 0.840 0.98211.9+NE 0.979 0.978 0.979 0.841 0.984Table1: Our system results on Open tracksFrom Table 1 we can find that, in Row 1, thebasic system participated in the last bakeoff al-ready achieves quite good recall, but the recall ofOOV is not very good because it cannot correctlyidentify unknown words that are not in the lexi-con such as factoids and name entities (espe-cially the nested named entity) and new words(except factoids, named entities and words ab-stracted from training data).
In Row 2, we onlyrewrite the factoid rules according to the MSRAGuidelines, and the recall of OOV improves sig-nificantly while the recall of IV falls slightly.
Itshows that the factoid detection affects the recallof IV.
As shown in Table 1, the GKB lexicon hasmade significant and persistent progress in allperformance because the GKB lexicon is refinedand the words are conformed to the MSRA stan-dard.
We also find that the NE postprocessor canimprove the recall of OOV but affects slightlythe recall of IV in all experiments.
It shows thatour named entity recognition has make im-provement compared with that of last year.
Asshown in Table 1, TBL has made slightly butpersistent progress in all steps it applies to.
AfterTBL adaptation OOV recall stays almost un-changed, for the rules are derived from trainingcorpus, and no OOV words would meet the con-dition of applying them in theory, but IV recallimproves, which compensates the loss of IV re-call caused by NE post-process and the factoiddetection.
It is interesting comparing the per-formance of two TBL template sets, the firsttemplate set is simple and the threshold for gen-erating rules is 3 by default (called TBL in Table1), and the second is more complicated with a"0" threshold (called New TBL in Table 1).
Thenumber of rules generated is 1061 and 12135respectively.
Our experiments demonstrate thatmore precise rule template set with low thresholdalways leads to better performance, for theycould cover more situations, although a simplerule template set with high threshold does betterin OOV word recognition.3.1.2 MSRA Open track of NERIn the track, we used People's Daily 2000 corpus(Yu, 2003) for building our lexicon and trainingour model.Considering that organization names are ir-regular in their forms compared with personnames and location names, and there are manyabbreviations and anaphora, TBL adaptation maydegrade the performance of organization,   wesubmitted two results, as shown in Table 2.1+TBL1 means that TBL only adapt person andlocation results of basic system, the organizationperformance of basic system and 1+TBL1 wouldbe identical.
1+TBL2 means TBL adapt all threetypes of NE.
For comparison, we list (Column 2)the results of basic system.
The Row 2 to Row13 shows the recall, the precision, and the F-score of PN, LN, ON and total.
(%) 1.basic 1+TBL1 1+TBL2R 87.28 91.43 91.74P 90.63 92.56 92.77PNF 88.92 91.99 92.25R 80.18 87.39 89.74P 81.68 87.51 89.77LNF 80.92 87.45 89.76R 65.59 65.59 76.48P 73.80 73.80 75.44ONF 69.45 69.45 76.11R 79.31 83.99 87.53P 82.98 86.45 87.67TotalF 81.10 85.20 87.60124Table 2: MSRA Open track of NERTo our surprise, performance listed in Table 2demonstrates that applying TBL causes a dra-matic improvement in all three types of NE, es-pecially organization performance.
The greatsimilarity between training corpus and test cor-pus of MSRA may explain this.
For the inconsis-tency of standard between MSRA and PKU, therecall, especially of the ONs, is not very good.We did some effort in the standard adaptation,such as constraint the length and type of candi-date words in combining the named entities, butthe result is not very good.3.2 Closed tracksThe Table 3 and Table 4 present the results ofMSRA and CityU closed tracks respectively.
(%)     R      P     F  Roov    Riv1.basicsystem(2) 0.924 0.877 0.900 0.575 0.9362.1+training lexicon 0.955 0.953 0.954 0.575 0.9693.2+TBL 0.960 0.955 0.958 0.575 0.9734.basicsystem(3) 0.919 0.880 0.899 0.602 0.9305.4+training lexicon  0.950 0.954 0.952 0.602 0.9626.5+TBL 0.954 0.955 0.955 0.603 0.966Table 3: Our system results on MSRA Closed(%)     R      P     F  Roov    Riv1.basicsystem 0.947 0.916 0.931 0.716 0.9572.1+training lexicon 0.959 0.960 0.959 0.716 0.9693.2+TBL 0.969 0.964 0.967 0.716 0.9804.1+factoid tool 0.946 0.915 0.931 0.713 0.9565.4+training lexicon 0.958 0.959 0.959 0.713 0.9686.5+TBL 0.969 0.964 0.966 0.712 0.9806' 0.962 0.962 0.962 0.722 0.972Table 4: Our system results on CityU ClosedIn Table 3, the basic system (2) shows the win-dow size of the template is 2 and the basic sys-tem (3) is 3.
As is shown in the table, except theprecision and the recall of OOV, the performanceof window size with 2 outperforms that of win-dow size with 3.In Table 4, the system 6' is the one we submit-ted in this closed CityU track, but the system 6 isbetter than the system 6'.
In TBL training, wemade a mistake that the training data weren'tprocessed by factoid tool and lexicon combining.We also can find that the factoid tool doesn't im-prove the performance.
The system 6 isn't thebest one (system 3).Combining the separated words according totraining lexicon improved the performance ofboth MSRA and CITYU closed track.
In themeantime, TBL worked considerably well in allclosed tracks.4 ConclusionsThe evaluation results show that the performanceof NER need be improved in abbreviations rec-ognition and anaphora resolution.AcknowledgementsThe work reported here was a team effort.
Wethank Yonggang Xue, Duo Ji, Haitao Luo, NanHe and Xinnian Mao for their help in the ex-perimentation and evaluation of the system.
Wealso thank Prof. Shiwen Yu for the People'sDaily 2000 corpus (Yu 2003) and GKB (Yu2002) lexicon.ReferencesHeng Li, etc.
2005.
Chinese Word Segmentation inFTRD Beijing.
Proceedings of the Fourth SIGHANworkshop on Chinese Language Processing.Pages:150-154Hwee Tou Ng, Jin Kiat Low.
2004.
Chiense part-of-speech tagging: One-at-a-time or all-at-once?Word-based or character-based?.
Proceedings ofthe 2004 conference on Empirical Methods inNatural Language Processing.
Pages:277-284Jianfeng Gao, Mu Li, Andi Wu and Chang-NingHuang.
2004a.
Chinese word segmentation: apragmatic approach.
Microsoft Research TechnicalReport, MSR-TR-2004-123.Nianwen Xue, Libin Shen.
July 2003.
Chinese wordsegmentation as LMR tagging.
Proceedings of theSecond SIGHAN workshop on Chinese LanguageProcessing.
Pages:176-179.Shiwen Yu, etc.
2003.
Specification for Corpus Proc-essing at Peking University:Word Segmentation,POS Tagging and Phonetic Notation.
Journal ofChinese Language and Computing, 13(2) 121-158.Shiwen Yu, etc.
2002.
The Grammatical Knowledge-base of Contemporary Chinese --- A CompleteSpecification.
Tsinghua University Press.125
