Session 13: CSR SearchRichard SchwartzBBN Systems and TechnologiesCambr idge,  MA 02138ABSTRACTThis session had five papers related to different opics inCSR Search.
The topics ranged from integration of manyknowledge sources within a practical system, to differentsearch algorithms for real-time large vocabulary speechrecognition.1.
PapersThis section contains one or two paragraphs about each ofthe papers presented.
More details can be found in the papersthat follow.1.
Sadaoki Furui from N'IT presented a paper describingthe integration of several knowledge sources for a systemthat performed very large vocabulary recognition of namesand address for directory assistance.
Some experimentswere performed with directory listings for 70,000 customers.Knowledge sources at the phonological, lexical, and gram-matical evel were used to make the search feasible.
Asmaller scale system with 2,300 subscribers was used in tri-als in order to reduce computation and error rate.2.
Hy Murveit from SRI presented some new algorithmsfor recognition of continuous speech using continuous den-sities.
The first algorithm used a tree structure for the un-igram back-off part of the language model (which usuallyaccounts for most of the computation), and the usual bigramstructure for those few bigrams observed in the training.
Thesecond class of algorithms covered techniques for reducingthe observation computation for continuous densities.
Then,he presented a wide range of extensive xperiments radingoff different approaches for reducing computation and size.The result was that the recognition could run in about hreetimes real time with accuracy only a little bit worse than thatfor the best research conditions.
In addition, it could run inreal time with about hree times the error rate.3.
Doug Paul from MIT Lincoln Laboratory presented someimprovements in the Stack Decoder search.
The improve-ments were made in the fast match algorithms and in theimplementation f the search components.
For example,caching algorithms to reduce look-up costs, and quantiza-tion algorithms were employed to reduce size requirements.In addition, techniques for tree-clustering of different al-lophones of a phoneme, and techniques for incrementalspeaker adaptation were presented.4.
Julian OdeU from Cambridge University presented asearch algorithm in which all of the constraints were usedin a single time-synchronous pass over the dam.
Thus, allknowledge sources, including trigram language models andbetween-word coarticulation models were compiled ynami-cally into a tree.
While the search was somewhat expensive,it was quite interesting that it was possible at all, since thesize of the search space would be tremendous ff fully ex-panded.
This work showed that using the available knowl-edge as early as possible greatly reduces the computation.5.
Long Nguyen from BBN described experiments aimedat reducing the perceived search errors that might resultfrom using the N-best Search strategy.
The search algo-rithm, which was related to Progressive Search techniqueproposed by Murveit, built a lattice to cover a wide range ofchoices.
Then, this lattice of alternatives was decoded againusing trigrana and between-word triphone models.
The re-suit, however, showed that there were very few search errorscaused by the original N-best algorithm.
However, the newlattice search algorithm was faster than restoring the n-bestalternatives.
Finally, the other essential uses for the n-bestparadigm were reviewed.2.
ConclusionsSome general conclusions can be made from the variousattempts at improving the efficiency and accuracy of thesearch algorithms.First, while there are various tradeoffs that can be maderelated to pruning back the number of active hypotheses,or the size of the language model, etc, in general, thesecompromises quickly become damaging, in that they alsoincrease the word error rate.The more effective approaches make use of two general tech-niques: shared computation, and multiple-pass trategies.2.1.
Shared ComputationTwo effective ways to share computation are to use treestructures, and to perform bottom-up rocessing.Tree structures, both at the phonetic and language modeling385level reduce computation by a large factor since the compu-tation for the initial portions of similar words can be shared.By the time the computation gets to the ends of the words,most of the words have been eliminated.Bottom-up rocessing means that a system examines the in-put without regard to the surrounding context, and uses thesescores in ,various combinations depending on the global con-text.
Thus, the repeated scoring of the same acoustic eventsin different language model contexts is avoided.2.2.
Multiple-Pass StrategiesThere are several multi-pass earch strategies that have foundbeneficial use when real-time isdesired.
The problem is that,even though it would be nice to use all of the knowledgesources at once to obtain their full integration, this is just tooexpensive for the size of problems we are trying to handle,and the currently available hardware.
The single-pass searchemployed by Cambridge University certainly showed thatthere is much to be gained from efficient sharing, primar-ily through the use of dynamically compiled tree structures.However, at the current time, it seems unlikely that this ap-proach could be pushed all the way to real-time processing.The multiple-puss trategies discussed here include fastmatch algorithms, using vector quantization as an approxi-mation to eliminate most of the computation for Gaussians,use of N-best searches with reduced models followed byrescoring with more detailed models, and use of lattices inmuch the same way.
In addition, the use of the forward-backward search technique allows the later passes to makemore effective use of the pruning information derived fromearlier passes.The multiple-pass earch strategies often can save several or-ders of magnitude in search computation, thus making real-time conceivable.386
