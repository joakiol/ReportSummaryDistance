Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 549?553,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsUNRAVEL?A Decipherment ToolkitMalte Nuhn and Julian Schamper and Hermann NeyHuman Language Technology and Pattern RecognitionComputer Science Department, RWTH Aachen University, Aachen, Germany<surname>@cs.rwth-aachen.deAbstractIn this paper we present the UNRAVELtoolkit: It implements many of the recentlypublished works on decipherment, includ-ing decipherment for deterministic cipherslike e.g.
the ZODIAC-408 cipher and Parttwo of the BEALE ciphers, as well as deci-pherment of probabilistic ciphers and un-supervised training for machine transla-tion.
It also includes data and exampleconfiguration files so that the previouslypublished experiments are easy to repro-duce.1 IntroductionThe idea of applying decipherment techniques tothe problem of machine translation has driven re-search on decipherment in the recent time.
Eventhough the theoretical knowledge has been pub-lished in the form of papers there has not beenany release of software until now.
This made itvery difficult to follow upon the recent researchand to contribute new ideas.
With this publica-tion we want to share our implementation of twoimportant decipherment algorithms: Beam searchfor deterministic substitution ciphers and beamedEM training for probabilistic ciphers.
It is clearthat the field of decipherment is still under heavyresearch and that the true value of this release doesnot lie in the current implementations themselves,but rather in the opportunity for other researchersto contribute their ideas to the field.2 OverviewEnciphering a plaintext into a ciphertext can bedone using a myriad of encipherment methods.Each of these methods needs its own customizedtools and tweaks in order to be deciphered auto-matically.
The goal of UNRAVEL is not to providea solver for every single encipherment method, butrather to provide reusable tools that can be appliedto unsupervised learning for machine translation.UNRAVEL contains two tools: DET-UNRAVELfor decipherment of deterministic ciphers, andEM-UNRAVEL for EM decipherment for proba-bilistic substitution ciphers and simple machinetranslation tasks.
A comparison of both tools isgiven in Table 1.The code base is implemented in C++11 anduses many publicly available libraries: TheGOOGLE-GLOG logging library is used for all log-ging purposes, the GOOGLE-GFLAGS library isused for providing command line flags, and theGOOGLETEST library is used for unit testing andconsistency checks throughout the code base.Classes for compressed I/O, access toOpenFST (Allauzen et al., 2007), access toKENLM(Heafield, 2011), representing mappings,n-gram counts, vocabularies, lexicons, etc.
areshared across the code base.For building we use the GNU build system.
UN-RAVEL can be compiled using GCC, ICC, andCLANG on various Linux distributions and onMacOS X.
Scripts to download and compile nec-essary libraries are also included: This makes iteasy to install UNRAVEL and its dependencies indifferent computing environments.Also, configuration- and data files (if possiblefrom a license point of view) for various experi-ments (see Section 4.2 and Section 5.2) are dis-tributed.
Amongst others this includes setups forthe ZODIAC-408 and Part two of the BEALE ci-phers (deterministic ciphers), as well as the OPUScorpus and the VERBMOBIL corpus (probabilisticcipher/machine translation).3 Related WorkWe list the most important publications that lead tothe implementation of UNRAVEL: Regarding DET-549UNRAVEL, the following literature is relevant:Hart (1994) presents a tree search algorithm forsimple substitution ciphers with known word seg-mentations.
The idea of performing a tree searchand looking for mappings fulfilling consistencyconstraints was later adopted to n-gram based de-cipherment in an A* search approach presentedby Corlett and Penn (2010).
DET-UNRAVEL im-plements the beam search approach presented byNuhn et al.
(2013) together with the refinementspresented in (Nuhn et al., 2014).
The Bayesianapproach presented by Ravi and Knight (2011a) tobreak the ZODIAC-408 cipher is not implemented,but configuration and data to solve the ZODIAC-408 cipher with DET-UNRAVEL is included.
Alsoit is worth noting that Hauer et al.
(2014) providedfurther work towards homophonic deciphermentthat is not included in UNRAVEL.The EM training for the decipherment of prob-abilistic substitution ciphers, as first described byLee (2002) is implemented in EM-UNRAVEL to-gether with various improvements and extensions:The beam- and preselection search approxima-tions presented by Nuhn and Ney (2014), the con-text vector based candidate induction presentedby Nuhn et al.
(2012), as well as training of thesimplified machine translation model presented byRavi and Knight (2011b).4 Deterministic Ciphers: DET-UNRAVELGiven an input sequence fN1with tokens fnfroma vocabulary Vfand a language model of a tar-get language p(eN1) with the target tokens froma target vocabulary Ve, the task is to find amapping function ?
: Vf?
Veso that thelanguage model probability of the deciphermentp(?(f1)?
(f2) .
.
.
?
(fN)) is maximized.DET-UNRAVEL solves this optimization prob-lem using the beam search approach presented byNuhn et al.
(2013): The main idea is to structureall partial ?s into a search tree: If a cipher con-tains |Vf| unique symbols, then the search tree isof height |Vf|.
At each level a decision about then-th symbol is made.
The leaves of the tree formfull hypotheses.
Instead of traversing the wholesearch tree, beam search traverses the tree top tobottom and only keeps the most promising candi-dates at each level.
Table 2 shows the importantparameters of the algorithm.4.1 Implementation DetailsDuring search, our implementation keeps track ofall partial hypotheses in two arraysHsandHt.
Weuse two different data structures for the hypothe-ses in Hsand the hypotheses in Ht: Hscontainsthe full information of the current partial mapping?.
The candidates in the array Htare generatedby augmenting hypotheses from the array Hsbyjust one additional mapping decision f ?
e andthus we use a different data structure for these hy-potheses: They contain the current mapping deci-sion f ?
e and a pointer to the parent node inHs.
This saves memory in comparison to storingthe complete mapping at every point in time andis faster than storing the mapping as a tree, whichwould have to be traversed for every score estima-tion.The fact that only one additional decision ismade during the expansion process is also usedwhen calculating the scores for the new hypothe-sis: Only the additional terms of the final score forthe current partial hypothesis ?
are added to thepredecessor score (i.e.
the scheme is scorenew=scoreold+ ?, where scoreoldis independent of thecurrent decision f ?
e).The now scored hypotheses in Ht(our imple-mentation also includes the improved rest cost es-Aspect Deterministic Ciphers: DET-UNRAVEL Probabilistic Ciphers: EM-UNRAVELSearch Space Mappings ?
Substitution tables {p(f |e)}Training Beam search over all ?.
The order inwhich the decisions for ?
(f) for each fare made is based on the extension order.EM-training: In the E-step use beamsearch to obtain the most probable deci-pherments eI1for a given ciphertext se-quence fJ1.
Update {p(f |e)} in M-step.Decoding Apply ?
to cipher text.
Viterbi decoding using final {p(f |e)}.Experiments ZODIAC-408, pt.
two of BEALE ciphers OPUS, VERBMOBILTable 1: Comparison of DET-UNRAVEL and EM-UNRAVEL.550timation as described in (Nuhn et al., 2014)) arepruned using different pruning strategies: Thresh-old pruning?given the best hypothesis, add athreshold score and prune the hypotheses withscores lower than best hypothesis plus this thresh-old score?and histogram pruning?which onlykeeps the best Bhistohypothesis at every level ofthe search tree.
Further, the surviving hypothesesare checked whether they fulfill certain constraintsC(?)
like e.g.
enforcing 1-to-1 mappings duringsearch.Those hypotheses in Htthat survived the prun-ing step and the constraints check are converted tofull hypotheses so that they can be stored in Hs.Then, the search continues with the next cardinal-ity.The order in which decisions about the symbolsf ?
Vfare made during search (called extensionorder) can be computed using different strategies:We implement a simple frequency sorting heuris-tic, as well as a more advanced strategy that usesbeam search to find an improved enumeration off ?
Vf, as presented in (Nuhn et al., 2014).Our implementation expands the partial hy-potheses in Hsin parallel: The implementationhas been tested with up to 128 threads (on a 128core machine) with parallelization overhead ofless than 20%.4.2 ExperimentsThe configurations for decoding the ZODIAC-408cipher as well as Part two of the BEALE ciphers arealmost identical: For both setups we use an 8-gramcharacter language model trained on a subset ofthe English Gigaword corpus (Parker et al., 2011).We obtain n-gram counts (order 2 to 8) from theinput ciphers and pass these to DET-UNRAVEL.
Inboth cases we use the improved heuristic togetherwith the improved extension order as presented in(Nuhn et al., 2014).For the ZODIAC-408, using a beam sizeBhist=26 yields 52 out of 54 correct mappings.
For thePart two of the BEALE ciphers a much larger beamsize of Bhist= 10M yields 157 correct mappingsout of 185, resulting in an error rate on the stringof 762 symbols is 5.4%.5 Probabilistic Ciphers: EM-UNRAVELFor probabilistic ciphers, the goal is to find a prob-abilistic substitution table {p(f |e)} with normal-ization constraint ?e?fp(f |e) = 1.
Learningthis table is done iteratively using the EM algo-rithm (Dempster et al., 1977).Each iteration consists of two steps: Hypoth-esis generation (E-Step) and retraining the table{p(f |e)} using the posterior probability pj(e|fJ1)that any translation eI1of fJ1has the word e alignedto the source word fj(M-Step).From a higher level view, EM-UNRAVEL can beseen as a specialized word based MT decoder thatcan efficiently generate and organize all possibletranslations in the E-step, and efficiently retrainthe model {p(f |e)} on all these hypotheses in theM-step.5.1 Implementation DetailsIn contrast to DET-UNRAVEL, EM-UNRAVEL pro-cesses the input corpus sentence by sentence.
Foreach sentence, we build hypotheses eI1from left toright, one word at a time:First, the empty hypothesis is added to a setof currently active partial hypotheses.
Then, foreach partial hypothesis, a new source word is cho-sen such that local reordering constraints are ful-filled.
For this, a coverage vector (which encodesthe words that have already been translated) hasto be updated for each hypothesis.
Once the cur-rent source word to be translated next has beenchosen, hypotheses for all possible translations ofthis source word are generated and scored.
Af-ter having processed the entire set of partial hy-potheses, the set of newly generated hypotheses isName DescriptionPruningBhistHistogram pruning.
Only the best Bhisthypotheses are kept.BthresThreshold pruning.
Hypotheses withscores S worse than Sbest+Bthres, whereSbestis the score of the best hyptohesis,are pruned.ConstraintsC(?)
Substitution constraint.
Hypotheses notfulfilling the constraintC(?)
are discardedfrom search.Extension OrderVextExtension order.
Enumeration of the vo-cabulary Vfin which the search tree overall ?
is visited.BexthistHistogram Pruning for extension ordersearch.WextnWeight for n?gram language modellookahead score.Table 2: Important parameters of DET-UNRAVEL.551pruned: Here, the partial hypotheses are organizedand pruned with respect to their cardinality.
Foreach cardinality, we keep the Bhistobest scoringhypotheses.Similarly to DET-UNRAVEL, the previously de-scribed expansion and pruning step is imple-mented using two arrays Hsand Ht.
However,in EM-UNRAVEL the partial hypotheses in HsandHtuse the same data structures since?in contrastto DET-UNRAVEL?recombination of hypothesesis possible.In the case of large vocabularies it is not feasi-ble to keep track of all possible substitutions for agiven source word.
This step can also be approx-imated using the preselection technique by Nuhnand Ney (2014): Instead of adding hypotheses forall possible target words, only a small subset ofpossible successor hypotheses is generated: Theseare based on the current source word that is to betranslated, as well as the current language modelstate.Once the search is completed we compute pos-teriors on the resulting word graph and accumu-late those across all sentences in the corpus.
Hav-ing finished one pass over the corpus, the accumu-Name DescriptionPruningBhistHistogram pruning.
Only the best Bhisthypotheses are kept.Preselection SearchBlexcandLexical candidates.
Try only the bestBlexcandsubstitutions e for each word fbased on p(f |e)BLMcandLM candidates.
Try only the best BLMhistsuccessor words e with respect to the pre-vious hypothesis?
LM state.Translation ModelWjumpJump width.
Maximum jump size allowedin local reordering.CjumpJump cost.
Cost for non-monotonic tran-sitions.CinsInsertion cost.
Cost for insertions ofwords.MinsMaximum number of insertions per sen-tence.CdelDeletion cost.
Cost for deletions of words.MdelMaximum number of of deletions per sen-tence.Other?lexLexical smoothing parameter.NctxNumber of candidate translations allowedin lexicon generation in context vectorstep.Table 3: Important parameters of EM-UNRAVEL.lated posteriors are used to re-estimate {p(e|f)}and the next iteration of the EM algorithm begins.Also, with every new parameter table {p(e|f)},the Viterbi decoding of the source corpus is com-puted.While full EM training is feasible and givesgood results for the OPUS corpus, Nuhn et al.
(2012) suggest to include a context vector step inbetween EM iterations for large vocabulary tasks.Using the Viterbi decoding of the source se-quence from the last E-step and the corpus usedto train the LM, we create normalized context vec-tors for each word e and f .
The idea is that vec-tors for words e and f that are translations of eachother are similar.
For each word f ?
Vf, a set ofcandidates e ?
Vecan be computed.
These candi-dates are used to initialize a new lexicon, which isfurther refined using standard EM iterations after-wards.Both, EM training and the context vector stepare implemented in a parallel fashion (running ina single process).
Parallelization is done on a sen-tence level: We successfully used our implemen-tation with up to 128 cores.5.2 ExperimentsWe briefly mention experiments on two corpora:The OPUS corpus and the VERBMOBIL corpus.The OPUS corpus is a subtitle corpus of roughly100k running words.
Here the vocabulary sizeof the source language (Spanish) is 562 and thetarget language (English) contains 411 uniquewords.
Using a 3-gram language model UNRAVELachieves 19.5% BLEU on this task.The VERBMOBIL corpus contains roughly 600krunning words.
The target language vocabularysize is 3, 723 (English) and the source languagevocabulary size is 5, 964 (German).
Using a 3-gram language model and the context vector ap-proach, UNRAVEL achieves 15.5% BLEU.6 Download and LicenseUNRAVEL can be downloaded atwww.hltpr.rwth-aachen.de/unravel.UNRAVEL is distributed under a custom opensource license.
This includes free usage fornoncommercial purposes as long as any changesmade to the original software are publishedunder the terms of the same license.
The exactformulation is available at the download page forUNRAVEL.552We have chosen to keep this paper independentof actual implementation details such as method-and parameter names.
Please consult the READMEfiles and comments in UNRAVEL?s source code forimplementation details.7 ConclusionUNRAVEL is a flexible and efficient deciphermenttoolkit that is freely available to the scientific com-munity.
It implements algorithms for solving de-terministic and probabilistic substitution ciphers.We hope that this release sparks more interest-ing research on decipherment and its applicationsto machine translation.References[Allauzen et al.2007] Cyril Allauzen, Michael Riley,Johan Schalkwyk, Wojciech Skut, and MehryarMohri.
2007.
Openfst: A general and efficientweighted finite-state transducer library.
In JanHolub and Jan Zd?arek, editors, CIAA, volume 4783of Lecture Notes in Computer Science, pages 11?23.Springer.
[Corlett and Penn2010] Eric Corlett and Gerald Penn.2010.
An exact A* method for deciphering letter-substitution ciphers.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 1040?1047, Uppsala, Swe-den, July.
The Association for Computer Linguis-tics.
[Dempster et al.1977] Arthur P. Dempster, Nan M.Laird, and Donald B. Rubin.
1977.
Maximum like-lihood from incomplete data via the EM algorithm.Journal of the Royal Statistical Society, B, 39.
[Hart1994] George W Hart.
1994.
To decodeshort cryptograms.
Communications of the ACM,37(9):102?108.
[Hauer et al.2014] Bradley Hauer, Ryan Hayward, andGrzegorz Kondrak.
2014.
Solving substitution ci-phers with combined language models.
In Pro-ceedings of COLING 2014, the 25th InternationalConference on Computational Linguistics: Techni-cal Papers, pages 2314?2325.
Dublin City Univer-sity and Association for Computational Linguistics.
[Heafield2011] Kenneth Heafield.
2011.
KenLM:Faster and Smaller Language Model Queries.
InProceedings of the Sixth Workshop on StatisticalMachine Translation, pages 187?197, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.
[Lee2002] Dar-Shyang Lee.
2002.
Substitution deci-phering based on hmms with applications to com-pressed document processing.
Pattern Analysisand Machine Intelligence, IEEE Transactions on,24(12):1661?1666.
[Nuhn and Ney2014] Malte Nuhn and Hermann Ney.2014.
Em decipherment for large vocabularies.
InAnnual Meeting of the Assoc.
for ComputationalLinguistics, pages 759?764, Baltimore, MD, USA,June.
[Nuhn et al.2012] Malte Nuhn, Arne Mauser, and Her-mann Ney.
2012.
Deciphering foreign language bycombining language models and context vectors.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages156?164, Jeju, Republic of Korea, July.
Associationfor Computational Linguistics.
[Nuhn et al.2013] Malte Nuhn, Julian Schamper, andHermann Ney.
2013.
Beam search for solving sub-stitution ciphers.
In Annual Meeting of the Assoc.for Computational Linguistics, pages 1569?1576,Sofia, Bulgaria, August.
[Nuhn et al.2014] Malte Nuhn, Julian Schamper, andHermann Ney.
2014.
Improved decipherment ofhomophonic ciphers.
In Conference on EmpiricalMethods in Natural Language Processing, Doha,Qatar, October.
[Parker et al.2011] Robert Parker, David Graff, JunboKong, Ke Chen, and Kazuaki Maeda.
2011.
Englishgigaword fifth edition.
Linguistic Data Consortium,Philadelphia.
[Ravi and Knight2011a] Sujith Ravi and Kevin Knight.2011a.
Bayesian inference for Zodiac and other ho-mophonic ciphers.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 239?247, Portland, Ore-gon, June.
Association for Computational Linguis-tics.
[Ravi and Knight2011b] Sujith Ravi and Kevin Knight.2011b.
Deciphering foreign language.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 12?21, Portland, Oregon, USA,June.
Association for Computational Linguistics.553
