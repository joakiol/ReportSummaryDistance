Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477?485,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPStochastic Gradient Descent Training forL1-regularized Log-linear Models with Cumulative PenaltyYoshimasa Tsuruoka??
Jun?ichi Tsujii???
Sophia Ananiadou???
School of Computer Science, University of Manchester, UK?
National Centre for Text Mining (NaCTeM), UK?
Department of Computer Science, University of Tokyo, Japan{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.ukAbstractStochastic gradient descent (SGD) usesapproximate gradients estimated fromsubsets of the training data and updatesthe parameters in an online fashion.
Thislearning framework is attractive becauseit often requires much less training timein practice than batch training algorithms.However, L1-regularization, which is be-coming popular in natural language pro-cessing because of its ability to pro-duce compact models, cannot be effi-ciently applied in SGD training, due tothe large dimensions of feature vectorsand the fluctuations of approximate gra-dients.
We present a simple method tosolve these problems by penalizing theweights according to cumulative values forL1 penalty.
We evaluate the effectivenessof our method in three applications: textchunking, named entity recognition, andpart-of-speech tagging.
Experimental re-sults demonstrate that our method can pro-duce compact and accurate models muchmore quickly than a state-of-the-art quasi-Newton method for L1-regularized log-linear models.1 IntroductionLog-linear models (a.k.a maximum entropy mod-els) are one of the most widely-used probabilisticmodels in the field of natural language process-ing (NLP).
The applications range from simpleclassification tasks such as text classification andhistory-based tagging (Ratnaparkhi, 1996) to morecomplex structured prediction tasks such as part-of-speech (POS) tagging (Lafferty et al, 2001),syntactic parsing (Clark and Curran, 2004) and se-mantic role labeling (Toutanova et al, 2005).
Log-linear models have a major advantage over otherdiscriminative machine learning models such assupport vector machines?their probabilistic out-put allows the information on the confidence ofthe decision to be used by other components in thetext processing pipeline.The training of log-liner models is typically per-formed based on the maximum likelihood crite-rion, which aims to obtain the weights of the fea-tures that maximize the conditional likelihood ofthe training data.
In maximum likelihood training,regularization is normally needed to prevent themodel from overfitting the training data,The two most common regularization methodsare called L1 and L2 regularization.
L1 regular-ization penalizes the weight vector for its L1-norm(i.e.
the sum of the absolute values of the weights),whereas L2 regularization uses its L2-norm.
Thereis usually not a considerable difference betweenthe two methods in terms of the accuracy of theresulting model (Gao et al, 2007), but L1 regu-larization has a significant advantage in practice.Because many of the weights of the features be-come zero as a result of L1-regularized training,the size of the model can be much smaller than thatproduced by L2-regularization.
Compact modelsrequire less space on memory and storage, and en-able the application to start up quickly.
These mer-its can be of vital importance when the applicationis deployed in resource-tight environments such ascell-phones.A common way to train a large-scale L1-regularized model is to use a quasi-Newtonmethod.
Kazama and Tsujii (2003) describe amethod for training a L1-regularized log-linearmodel with a bound constrained version of theBFGS algorithm (Nocedal, 1980).
Andrew andGao (2007) present an algorithm called Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), which can work on the BFGS algorithmwithout bound constraints and achieve faster con-vergence.477An alternative approach to training a log-linearmodel is to use stochastic gradient descent (SGD)methods.
SGD uses approximate gradients esti-mated from subsets of the training data and up-dates the weights of the features in an onlinefashion?the weights are updated much more fre-quently than batch training algorithms.
This learn-ing framework is attracting attention because it of-ten requires much less training time in practicethan batch training algorithms, especially whenthe training data is large and redundant.
SGD wasrecently used for NLP tasks including machinetranslation (Tillmann and Zhang, 2006) and syn-tactic parsing (Smith and Eisner, 2008; Finkel etal., 2008).
Also, SGD is very easy to implementbecause it does not need to use the Hessian infor-mation on the objective function.
The implemen-tation could be as simple as the perceptron algo-rithm.Although SGD is a very attractive learningframework, the direct application of L1 regular-ization in this learning framework does not resultin efficient training.
The first problem is the inef-ficiency of applying the L1 penalty to the weightsof all features.
In NLP applications, the dimen-sion of the feature space tends to be very large?itcan easily become several millions, so the appli-cation of L1 penalty to all features significantlyslows down the weight updating process.
The sec-ond problem is that the naive application of L1penalty in SGD does not always lead to compactmodels, because the approximate gradient used ateach update is very noisy, so the weights of thefeatures can be easily moved away from zero bythose fluctuations.In this paper, we present a simple method forsolving these two problems in SGD learning.
Themain idea is to keep track of the total penalty andthe penalty that has been applied to each weight,so that the L1 penalty is applied based on the dif-ference between those cumulative values.
Thatway, the application of L1 penalty is needed onlyfor the features that are used in the current sample,and also the effect of noisy gradient is smoothedaway.We evaluate the effectiveness of our methodby using linear-chain conditional random fields(CRFs) and three traditional NLP tasks, namely,text chunking (shallow parsing), named entityrecognition, and POS tagging.
We show that ourenhanced SGD learning method can produce com-pact and accurate models much more quickly thanthe OWL-QN algorithm.This paper is organized as follows.
Section 2provides a general description of log-linear mod-els used in NLP.
Section 3 describes our stochasticgradient descent method for L1-regularized log-linear models.
Experimental results are presentedin Section 4.
Some related work is discussed inSection 5.
Section 6 gives some concluding re-marks.2 Log-Linear ModelsIn this section, we briefly describe log-linear mod-els used in NLP tasks and L1 regularization.A log-linear model defines the following prob-abilistic distribution over possible structure y forinput x:p(y|x) = 1Z(x) exp?iwifi(y,x),where fi(y,x) is a function indicating the occur-rence of feature i, wi is the weight of the feature,and Z(x) is a partition (normalization) function:Z(x) =?yexp?iwifi(y,x).If the structure is a sequence, the model is calleda linear-chain CRF model, and the marginal prob-abilities of the features and the partition functioncan be efficiently computed by using the forward-backward algorithm.
The model is used for a va-riety of sequence labeling tasks such as POS tag-ging, chunking, and named entity recognition.If the structure is a tree, the model is called atree CRF model, and the marginal probabilitiescan be computed by using the inside-outside algo-rithm.
The model can be used for tasks like syn-tactic parsing (Finkel et al, 2008) and semanticrole labeling (Cohn and Blunsom, 2005).2.1 TrainingThe weights of the features in a log-linear modelare optimized in such a way that they maximizethe regularized conditional log-likelihood of thetraining data:Lw =N?j=1log p(yj |xj ;w)?R(w), (1)where N is the number of training samples, yj isthe correct output for input xj , and R(w) is the478regularization term which prevents the model fromoverfitting the training data.
In the case of L1 reg-ularization, the term is defined as:R(w) = C?i|wi|,where C is the meta-parameter that controls thedegree of regularization, which is usually tuned bycross-validation or using the heldout data.In what follows, we denote by L(j,w)the conditional log-likelihood of each samplelog p(yj |xj ;w).
Equation 1 is rewritten as:Lw =N?j=1L(j,w)?
C?i|wi|.
(2)3 Stochastic Gradient DescentSGD uses a small randomly-selected subset of thetraining samples to approximate the gradient ofthe objective function given by Equation 2.
Thenumber of training samples used for this approx-imation is called the batch size.
When the batchsize is N , the SGD training simply translates intogradient descent (hence is very slow to converge).By using a small batch size, one can update theparameters more frequently than gradient descentand speed up the convergence.
The extreme caseis a batch size of 1, and it gives the maximumfrequency of updates and leads to a very simpleperceptron-like algorithm, which we adopt in thiswork.1Apart from using a single training sample toapproximate the gradient, the optimization proce-dure is the same as simple gradient descent,2 sothe weights of the features are updated at trainingsample j as follows:wk+1 = wk + ?k?
?w (L(j,w)?CN?i|wi|),where k is the iteration counter and ?k is the learn-ing rate, which is normally designed to decreaseas the iteration proceeds.
The actual learning ratescheduling methods used in our experiments aredescribed later in Section 3.3.1In the actual implementation, we randomly shuffled thetraining samples at the beginning of each pass, and thenpicked them up sequentially.2What we actually do here is gradient ascent, but we stickto the term ?gradient descent?.3.1 L1 regularizationThe update equation for the weight of each featurei is as follows:wik+1 = wik + ?k??wi(L(j,w)?
CN |wi|).The difficulty with L1 regularization is that thelast term on the right-hand side of the above equa-tion is not differentiable when the weight is zero.One straightforward solution to this problem is toconsider a subgradient at zero and use the follow-ing update equation:wik+1 = wik + ?k?L(j,w)?wi?
CN ?ksign(wki ),where sign(x) = 1 if x > 0, sign(x) = ?1 if x <0, and sign(x) = 0 if x = 0.
In this paper, we callthis weight updating method ?SGD-L1 (Naive)?.This naive method has two serious problems.The first problem is that, at each update, we needto perform the application of L1 penalty to all fea-tures, including the features that are not used inthe current training sample.
Since the dimensionof the feature space can be very large, it can sig-nificantly slow down the weight update process.The second problem is that it does not producea compact model, i.e.
most of the weights of thefeatures do not become zero as a result of train-ing.
Note that the weight of a feature does not be-come zero unless it happens to fall on zero exactly,which rarely happens in practice.Carpenter (2008) describes an alternative ap-proach.
The weight updating process is dividedinto two steps.
First, the weight is updated with-out considering the L1 penalty term.
Then, theL1 penalty is applied to the weight to the extentthat it does not change its sign.
In other words,the weight is clipped when it crosses zero.
Theirweight update procedure is as follows:wk+12i = wki + ?k?L(j,w)?wi???
?w=wk,if wk+12i > 0 thenwk+1i = max(0, wk+ 12i ?CN ?k),else if wk+12i < 0 thenwk+1i = min(0, wk+ 12i +CN ?k).In this paper, we call this update method ?SGD-L1 (Clipping)?.
It should be noted that this method479-0.1-0.0500.050.10  1000  2000  3000  4000  5000  6000WeightUpdatesFigure 1: An example of weight updates.is actually a special case of the FOLOS algorithm(Duchi and Singer, 2008) and the truncated gradi-ent method (Langford et al, 2009).The obvious advantage of using this method isthat we can expect many of the weights of thefeatures to become zero during training.
Anothermerit is that it allows us to perform the applica-tion of L1 penalty in a lazy fashion, so that wedo not need to update the weights of the featuresthat are not used in the current sample, which leadsto much faster training when the dimension of thefeature space is large.
See the aforementioned pa-pers for the details.
In this paper, we call this effi-cient implementation ?SGD-L1 (Clipping + Lazy-Update)?.3.2 L1 regularization with cumulativepenaltyUnfortunately, the clipping-at-zero approach doesnot solve all problems.
Still, we often end up withmany features whose weights are not zero.
Re-call that the gradient used in SGD is a crude ap-proximation to the true gradient and is very noisy.The weight of a feature is, therefore, easily movedaway from zero when the feature is used in thecurrent sample.Figure 1 gives an illustrative example in whichthe weight of a feature fails to become zero.
Thefigure shows how the weight of a feature changesduring training.
The weight goes up sharply whenit is used in the sample and then is pulled backtoward zero gradually by the L1 penalty.
There-fore, the weight fails to become zero if the featureis used toward the end of training, which is thecase in this example.
Note that the weight wouldbecome zero if the true (fluctuationless) gradientwere used?at each update the weight would goup a little and be pulled back to zero straightaway.Here, we present a different strategy for apply-ing the L1 penalty to the weights of the features.The key idea is to smooth out the effect of fluctu-ating gradients by considering the cumulative ef-fects from L1 penalty.Let uk be the absolute value of the total L1-penalty that each weight could have received upto the point.
Since the absolute value of the L1penalty does not depend on the weight and we areusing the same regularization constant C for allweights, it is simply accumulated as:uk =CNk?t=1?t.
(3)At each training sample, we update the weightsof the features that are used in the sample as fol-lows:wk+12i = wki + ?k?L(j,w)?wi???
?w=wk,if wk+12i > 0 thenwk+1i = max(0, wk+ 12i ?
(uk + qk?1i )),else if wk+12i < 0 thenwk+1i = min(0, wk+ 12i + (uk ?
qk?1i )),where qki is the total L1-penalty that wi has actu-ally received up to the point:qki =k?t=1(wt+1i ?
wt+ 12i ).
(4)This weight updating method penalizes theweight according to the difference between uk andqk?1i .
In effect, it forces the weight to receive thetotal L1 penalty that would have been applied ifthe weight had been updated by the true gradients,assuming that the current weight vector resides inthe same orthant as the true weight vector.It should be noted that this method is basi-cally equivalent to a ?SGD-L1 (Clipping + Lazy-Update)?
method if we were able to use the truegradients instead of the stochastic gradients.In this paper, we call this weight updatingmethod ?SGD-L1 (Cumulative)?.
The implemen-tation of this method is very simple.
Figure 2shows the whole SGD training algorithm with thisstrategy in pseudo-code.4801: procedure TRAIN(C)2: u?
03: Initialize wi and qi with zero for all i4: for k = 0 to MaxIterations5: ?
?
LEARNINGRATE(k)6: u?
u + ?C/N7: Select sample j randomly8: UPDATEWEIGHTS(j)9:10: procedure UPDATEWEIGHTS(j)11: for i ?
features used in sample j12: wi ?
wi + ?
?L(j,w)?wi13: APPLYPENALTY(i)14:15: procedure APPLYPENALTY(i)16: z ?
wi17: if wi > 0 then18: wi ?
max(0, wi ?
(u + qi))19: else if wi < 0 then20: wi ?
min(0, wi + (u?
qi))21: qi ?
qi + (wi ?
z)22:Figure 2: Stochastic gradient descent training withcumulative L1 penalty.
z is a temporary variable.3.3 Learning RateThe scheduling of learning rates often has a majorimpact on the convergence speed in SGD training.A typical choice of learning rate scheduling canbe found in (Collins et al, 2008):?k =?01 + k/N , (5)where ?0 is a constant.
Although this schedulingguarantees ultimate convergence, the actual speedof convergence can be poor in practice (Darkenand Moody, 1990).In this work, we also tested simple exponentialdecay:?k = ?0?
?k/N , (6)where ?
is a constant.
In our experiments, wefound this scheduling more practical than thatgiven in Equation 5.
This is mainly because ex-ponential decay sweeps the range of learning ratesmore smoothly?the learning rate given in Equa-tion 5 drops too fast at the beginning and tooslowly at the end.It should be noted that exponential decay is nota good choice from a theoretical point of view, be-cause it does not satisfy one of the necessary con-ditions for convergence?the sum of the learningrates must diverge to infinity (Spall, 2005).
How-ever, this is probably not a big issue for practition-ers because normally the training has to be termi-nated at a certain number of iterations in practice.34 ExperimentsWe evaluate the effectiveness our training algo-rithm using linear-chain CRF models and threeNLP tasks: text chunking, named entity recogni-tion, and POS tagging.To compare our algorithm with the state-of-the-art, we present the performance of the OWL-QNalgorithm on the same data.
We used the publiclyavailable OWL-QN optimizer developed by An-drew and Gao.4 The meta-parameters for learningwere left unchanged from the default settings ofthe software: the convergence tolerance was 1e-4;and the L-BFGS memory parameter was 10.4.1 Text ChunkingThe first set of experiments used the text chunk-ing data set provided for the CoNLL 2000 sharedtask.5 The training data consists of 8,936 sen-tences in which each token is annotated with the?IOB?
tags representing text chunks such as nounand verb phrases.
We separated 1,000 sentencesfrom the training data and used them as the held-out data.
The test data provided by the shared taskwas used only for the final accuracy report.The features used in this experiment were uni-grams and bigrams of neighboring words, and un-igrams, bigrams and trigrams of neighboring POStags.To avoid giving any advantage to our SGD al-gorithms over the OWL-QN algorithm in terms ofthe accuracy of the resulting model, the OWL-QNalgorithm was used when tuning the regularizationparameter C. The tuning was performed in such away that it maximized the likelihood of the heldoutdata.
The learning rate parameters for SGD werethen tuned in such a way that they maximized thevalue of the objective function in 30 passes.
Wefirst determined ?0 by testing 1.0, 0.5, 0.2, and 0.1.We then determined ?
by testing 0.9, 0.85, and 0.8with the fixed ?0.3This issue could also be sidestepped by, for example,adding a small O(1/k) term to the learning rate.4Available from the original developers?
websites:http://research.microsoft.com/en-us/people/galena/ orhttp://research.microsoft.com/en-us/um/people/jfgao/5http://www.cnts.ua.ac.be/conll2000/chunking/481Passes Lw/N # Features Time (sec) F-scoreOWL-QN 160 -1.583 18,109 598 93.62SGD-L1 (Naive) 30 -1.671 455,651 1,117 93.64SGD-L1 (Clipping + Lazy-Update) 30 -1.671 87,792 144 93.65SGD-L1 (Cumulative) 30 -1.653 28,189 149 93.68SGD-L1 (Cumulative + Exponential-Decay) 30 -1.622 23,584 148 93.66Table 1: CoNLL-2000 Chunking task.
Training time and accuracy of the trained model on the test data.-2.4-2.2-2-1.8-1.60  10  20  30  40  50ObjectivefunctionPassesOWL-QNSGD-L1 (Clipping)SGD-L1 (Cumulative)SGD-L1 (Cumulative + ED)Figure 3: CoNLL 2000 chunking task: Objective0500001000001500002000000  10  20  30  40  50#ActivefeaturesPassesOWL-QNSGD-L1 (Clipping)SGD-L1 (Cumulative)SGD-L1 (Cumulative + ED)Figure 4: CoNLL 2000 chunking task: Number ofactive features.Figures 3 and 4 show the training process ofthe model.
Each figure contains four curves repre-senting the results of the OWL-QN algorithm andthree SGD-based algorithms.
?SGD-L1 (Cumu-lative + ED)?
represents the results of our cumu-lative penalty-based method that uses exponentialdecay (ED) for learning rate scheduling.Figure 3 shows how the value of the objec-tive function changed as the training proceeded.SGD-based algorithms show much faster conver-gence than the OWL-QN algorithm.
Notice alsothat ?SGD-L1 (Cumulative)?
improves the objec-tive slightly faster than ?SGD-L1 (Clipping)?.
Theresult of ?SGD-L1 (Naive)?
is not shown in thisfigure, but the curve was almost identical to thatof ?SGD-L1 (Clipping)?.Figure 4 shows the numbers of active features(the features whose weight are not zero).
It isclearly seen that the clipping-at-zero approachfails to reduce the number of active features, whileour algorithms succeeded in reducing the numberof active features to the same level as OWL-QN.We then trained the models using the wholetraining data (including the heldout data) and eval-uated the accuracy of the chunker on the test data.The number of passes performed over the train-ing data in SGD was set to 30.
The results areshown in Table 1.
The second column shows thenumber of passes performed in the training.
Thethird column shows the final value of the objectivefunction per sample.
The fourth column showsthe number of resulting active features.
The fifthcolumn show the training time.
The last columnshows the f-score (harmonic mean of recall andprecision) of the chunking results.
There was nosignificant difference between the models in termsof accuracy.
The naive SGD training took muchlonger than OWL-QN because of the overhead ofapplying L1 penalty to all dimensions.Our SGD algorithms finished training in 150seconds on Xeon 2.13GHz processors.
TheCRF++ version 0.50, a popular CRF library de-veloped by Taku Kudo,6 is reported to take 4,021seconds on Xeon 3.0GHz processors to train themodel using a richer feature set.7 CRFsuite ver-sion 0.4, a much faster library for CRFs, is re-ported to take 382 seconds on Xeon 3.0GHz, usingthe same feature set as ours.8 Their library uses theOWL-QN algorithm for optimization.
Althoughdirect comparison of training times is not impor-6http://crfpp.sourceforge.net/7http://www.chokkan.org/software/crfsuite/benchmark.html8ditto482tant due to the differences in implementation andhardware platforms, these results demonstrate thatour algorithm can actually result in a very fast im-plementation of a CRF trainer.4.2 Named Entity RecognitionThe second set of experiments used the namedentity recognition data set provided for theBioNLP/NLPBA 2004 shared task (Kim et al,2004).9 The training data consist of 18,546 sen-tences in which each token is annotated with the?IOB?
tags representing biomedical named enti-ties such as the names of proteins and RNAs.The training and test data were preprocessedby the GENIA tagger,10 which provided POS tagsand chunk tags.
We did not use any information onthe named entity tags output by the GENIA tagger.For the features, we used unigrams of neighboringchunk tags, substrings (shorter than 10 characters)of the current word, and the shape of the word (e.g.?IL-2?
is converted into ?AA-#?
), on top of thefeatures used in the text chunking experiments.The results are shown in Figure 5 and Table2.
The trend in the results is the same as that ofthe text chunking task: our SGD algorithms showmuch faster convergence than the OWL-QN algo-rithm and produce compact models.Okanohara et al (2006) report an f-score of71.48 on the same data, using semi-Markov CRFs.4.3 Part-Of-Speech TaggingThe third set of experiments used the POS tag-ging data in the Penn Treebank (Marcus et al,1994).
Following (Collins, 2002), we used sec-tions 0-18 of the Wall Street Journal (WSJ) corpusfor training, sections 19-21 for development, andsections 22-24 for final evaluation.
The POS tagswere extracted from the parse trees in the corpus.All experiments for this work, including the tun-ing of features and parameters for regularization,were carried out using the training and develop-ment sets.
The test set was used only for the finalaccuracy report.It should be noted that training a CRF-basedPOS tagger using the whole WSJ corpus is not atrivial task and was once even deemed impracticalin previous studies.
For example, Wellner and Vi-lain (2006) abandoned maximum likelihood train-9The data is available for download at http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ERtask/report.html10http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/-3.8-3.6-3.4-3.2-3-2.8-2.6-2.4-2.20  10  20  30  40  50ObjectivefunctionPassesOWL-QNSGD-L1 (Clipping)SGD-L1 (Cumulative)SGD-L1 (Cumulative + ED)Figure 5: NLPBA 2004 named entity recognitiontask: Objective.-2.8-2.7-2.6-2.5-2.4-2.3-2.2-2.1-2-1.9-1.80  10  20  30  40  50ObjectivefunctionPassesOWL-QNSGD-L1 (Clipping)SGD-L1 (Cumulative)SGD-L1 (Cumulative + ED)Figure 6: POS tagging task: Objective.ing because it was ?prohibitive?
(7-8 days for sec-tions 0-18 of the WSJ corpus).For the features, we used unigrams and bigramsof neighboring words, prefixes and suffixes ofthe current word, and some characteristics of theword.
We also normalized the current word bylowering capital letters and converting all the nu-merals into ?#?, and used the normalized word as afeature.The results are shown in Figure 6 and Table 3.Again, the trend is the same.
Our algorithms fin-ished training in about 30 minutes, producing ac-curate models that are as compact as that producedby OWL-QN.Shen et al, (2007) report an accuracy of 97.33%on the same data set using a perceptron-based bidi-rectional tagging model.5 DiscussionAn alternative approach to producing compactmodels for log-linear models is to reformulate the483Passes Lw/N # Features Time (sec) F-scoreOWL-QN 161 -2.448 30,710 2,253 71.76SGD-L1 (Naive) 30 -2.537 1,032,962 4,528 71.20SGD-L1 (Clipping + Lazy-Update) 30 -2.538 279,886 585 71.20SGD-L1 (Cumulative) 30 -2.479 31,986 631 71.40SGD-L1 (Cumulative + Exponential-Decay) 30 -2.443 25,965 631 71.63Table 2: NLPBA 2004 Named entity recognition task.
Training time and accuracy of the trained modelon the test data.Passes Lw/N # Features Time (sec) AccuracyOWL-QN 124 -1.941 50,870 5,623 97.16%SGD-L1 (Naive) 30 -2.013 2,142,130 18,471 97.18%SGD-L1 (Clipping + Lazy-Update) 30 -2.013 323,199 1,680 97.18%SGD-L1 (Cumulative) 30 -1.987 62,043 1,777 97.19%SGD-L1 (Cumulative + Exponential-Decay) 30 -1.954 51,857 1,774 97.17%Table 3: POS tagging on the WSJ corpus.
Training time and accuracy of the trained model on the testdata.problem as a L1-constrained problem (Lee et al,2006), where the conditional log-likelihood of thetraining data is maximized under a fixed constraintof the L1-norm of the weight vector.
Duchi etal.
(2008) describe efficient algorithms for pro-jecting a weight vector onto the L1-ball.
AlthoughL1-regularized and L1-constrained learning algo-rithms are not directly comparable because the ob-jective functions are different, it would be inter-esting to compare the two approaches in termsof practicality.
It should be noted, however, thatthe efficient algorithm presented in (Duchi et al,2008) needs to employ a red-black tree and israther complex.In SGD learning, the need for tuning the meta-parameters for learning rate scheduling can be an-noying.
In the case of exponential decay, the set-ting of ?
= 0.85 turned out to be a good ruleof thumb in our experiments?it always producednear best results in 30 passes, but the other param-eter ?0 needed to be tuned.
It would be very usefulif those meta-parameters could be tuned in a fullyautomatic way.There are some sophisticated algorithms foradaptive learning rate scheduling in SGD learning(Vishwanathan et al, 2006; Huang et al, 2007).However, those algorithms use second-order infor-mation (i.e.
Hessian information) and thus needaccess to the weights of the features that are notused in the current sample, which should slowdown the weight updating process for the samereason discussed earlier.
It would be interestingto investigate whether those sophisticated learningscheduling algorithms can actually result in fasttraining in large-scale NLP tasks.6 ConclusionWe have presented a new variant of SGD that canefficiently train L1-regularized log-linear models.The algorithm is simple and extremely easy to im-plement.We have conducted experiments using CRFsand three NLP tasks, and demonstrated empiri-cally that our training algorithm can produce com-pact and accurate models much more quickly thana state-of-the-art quasi-Newton method for L1-regularization.AcknowledgmentsWe thank N. Okazaki, N. Yoshinaga, D.Okanohara and the anonymous reviewers for theiruseful comments and suggestions.
The work de-scribed in this paper has been funded by theBiotechnology and Biological Sciences ResearchCouncil (BBSRC; BB/E004431/1).
The researchteam is hosted by the JISC/BBSRC/EPSRC spon-sored National Centre for Text Mining.ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Pro-ceedings of ICML, pages 33?40.484Bob Carpenter.
2008.
Lazy sparse stochastic gradientdescent for regularized multinomial logistic regres-sion.
Technical report, Alias-i.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proceed-ings of COLING 2004, pages 103?110.Trevor Cohn and Philip Blunsom.
2005.
Semantic rolelabeling with tree conditional random fields.
In Pro-ceedings of CoNLL, pages 169?172.Michael Collins, Amir Globerson, Terry Koo, XavierCarreras, and Peter L. Bartlett.
2008.
Exponen-tiated gradient algorithms for conditional randomfields and max-margin markov networks.
The Jour-nal of Machine Learning Research (JMLR), 9:1775?1822.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1?8.Christian Darken and John Moody.
1990.
Note onlearning rate schedules for stochastic optimization.In Proceedings of NIPS, pages 832?838.Juhn Duchi and Yoram Singer.
2008.
Online andbatch learning using forward-looking subgradients.In NIPS Workshop: OPT 2008 Optimization for Ma-chine Learning.Juhn Duchi, Shai Shalev-Shwartz, Yoram Singer, andTushar Chandra.
2008.
Efficient projections ontothe l1-ball for learning in high dimensions.
In Pro-ceedings of ICML, pages 272?279.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proceedings of ACL-08:HLT, pages 959?967.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study ofparameter estimation methods for statistical naturallanguage processing.
In Proceedings of ACL, pages824?831.Han-Shen Huang, Yu-Ming Chang, and Chun-NanHsu.
2007.
Training conditional random fields byperiodic step size adaptation for large-scale text min-ing.
In Proceedings of ICDM, pages 511?516.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evalua-tion and extension of maximum entropy models withinequality constraints.
In Proceedings of EMNLP2003.J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recognitiontask at JNLPBA.
In Proceedings of the InternationalJoint Workshop on Natural Language Processing inBiomedicine and its Applications (JNLPBA), pages70?75.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML, pages 282?289.John Langford, Lihong Li, and Tong Zhang.
2009.Sparse online learning via truncated gradient.
TheJournal of Machine Learning Research (JMLR),10:777?801.Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y.Ng.
2006.
Efficient l1 regularized logistic regres-sion.
In Proceedings of AAAI-06, pages 401?408.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Jorge Nocedal.
1980.
Updating quasi-newton matriceswith limited storage.
Mathematics of Computation,35(151):773?782.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
2006.
Improvingthe scalability of semi-markov conditional randomfields for named entity recognition.
In Proceedingsof COLING/ACL, pages 465?472.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof EMNLP 1996, pages 133?142.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifi-cation.
In Proceedings of ACL, pages 760?767.David Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings ofEMNLP, pages 145?156.James C. Spall.
2005.
Introduction to StochasticSearch and Optimization.
Wiley-IEEE.Christoph Tillmann and Tong Zhang.
2006.
A discrim-inative global training algorithm for statistical MT.In Proceedings of COLING/ACL, pages 721?728.Kristina Toutanova, Aria Haghighi, and ChristopherManning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of ACL, pages 589?596.S.
V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.Schmidt, and Kevin P. Murphy.
2006.
Acceleratedtraining of conditional random fields with stochasticgradient methods.
In Proceedings of ICML, pages969?976.Ben Wellner and Marc Vilain.
2006.
Leveragingmachine readable dictionaries in discriminative se-quence models.
In Proceedings of LREC 2006.485
