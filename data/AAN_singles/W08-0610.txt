BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 71?79,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSpecies Disambiguation for Biomedical Term IdentificationXinglong Wang and Michael MatthewsSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh, EH8 9LW, UK{xwang,mmatsews}@inf.ed.ac.ukAbstractAn important task in information extraction(IE) from biomedical articles is term iden-tification (TI), which concerns linking en-tity mentions (e.g., terms denoting proteins)in text to unambiguous identifiers in stan-dard databases (e.g., RefSeq).
Previous workon TI has focused on species-specific docu-ments.
However, biomedical documents, es-pecially full-length articles, often talk aboutentities across a number of species, in whichcase resolving species ambiguity becomes anindispensable part of TI.
This paper de-scribes our rule-based and machine-learningbased approaches to species disambiguationand demonstrates that performance of TI canbe improved by over 20% if the correct speciesare known.
We also show that using thespecies predicted by the automatic species tag-gers can improve TI by a large margin.1 IntroductionThe exponential growth of the amount of scien-tific literature in the fields of biomedicine and ge-nomics has made it increasingly difficult for sci-entists to keep up with the state of the art.
TheTXM project (Alex et al, 2008a), a three-year projectwhich aims to produce software tools to aid cura-tion of biomedical papers, targets this problem andexploits natural language processing (NLP) technol-ogy in an attempt to automatically extract enrichedprotein-protein interactions (EPPI) and tissue expres-sions (TE) from biomedical text.A critical task in TXM is term identification (TI),the task of grounding mentions of biomedical namedentities to identifiers in referent databases.
TI can beseen as an intermediate task that builds on the pre-vious component in an information extraction (IE)pipeline, i.e., named entity recognition (NER), andprovides crucial information as input to the morecomplex module of relation extraction (RE).
Thestructure of the IE pipeline resembles a typical cu-ration process by human biologists.
For example,when curating protein-protein interactions (PPIs), acurator would first mark up the protein mentions intext, and then identify the mentions by finding theirunique identifiers from standard protein databasessuch as RefSeq,1 and finally curate pairs of IDs asPPIs.TI is a matching and disambiguation pro-cess (Wang and Matthews, 2008), and a primarysource of ambiguity lies in the model organisms ofthe terms.
In curation tasks, one often needs to dealwith collections of articles that involve entities of alarge variety of species.
For example, our collec-tion of articles from PubMed and PubMed Centralinvolve over 100 model organisms.
Also, it is of-ten the case that more than one species appear in thesame document, especially when the document is afull-length article.
In our dataset, 74% of the arti-cles concern more than one organism.
In many stan-dard databases, such as RefSeq and SwissProt, ho-molog proteins in different species, which often con-tain nearly identical synonym lists, are assigned dis-tinct identifiers.
This makes biomedical terms evenmore polysemous and hence species disambiguationbecomes crucial to TI.
For example, querying Ref-Seq2 with the protein mention plk1 resulted in 981http://www.ncbi.nlm.nih.gov/RefSeq/2The searches were carried out on November 5, 2007.71hits.
By adding a species to the query, e.g.
mouse,one can significantly reduce the number of results totwo.This paper describes our work on the task ofspecies disambiguation.
We also report the perfor-mance gain of a TI system from integration of vari-ous automatic species taggers.
The paper is organ-ised as follows.
Section 2 gives a brief overview ofrelated work.
Section 3 presents our methodologiesfor species disambiguation.
Section 4 describes arule-based TI system that we developed in the TXMproject, and the evaluation metrics.
This section alsoreports the evaluation results of the TI system withand without help from the species predicted by thetaggers.
We finally conclude in Section 5.2 Related WorkThe most relevant work to ours are the Gene Nor-malisation (GN) tasks (Morgan and Hirschman,2007; Hirschman et al, 2004) in the BioCreAtIvE I& II workshops (Hirschman et al, 2007; Hirschmanet al, 2005), which provided forums for exchang-ing thoughts and methodologies on tackling the taskof TI.
The data provided in the GN tasks, however,were species-specific, which means that the lexiconsand datasets were concerned with single model or-ganisms and thus species disambiguation was notrequired.
A few participating systems, however, in-tegrated a filter to rule out entities with erroneousspecies (Hanisch et al, 2005; Fluck et al, 2007),which were reported to be helpful.
Another differ-ence between our task and the BioCreAtIvE GN onesis that we carry out TI on entity level while GN ondocument level.It is worth mentioning that the protein-protein in-teraction task (IPS) in BioCreAtIvE II has taken intoaccount species ambiguity.
The IPS task resemblesthe work-flow of manual curation of PPIs in articlesinvolving multiple species, and to accomplish thetask, one would require a full pipeline of IE systems,including named entity recognition, term identifica-tion and relation extraction.
The best result for IPS(Krallinger et al, 2007) was fairly low at 28.85%F1, which reflects the difficulty of the task.
Someparticipants of IPS have reported (e.g., Grover et al,2007) that resolving species ambiguity was one ofthe biggest challenges.
Our analysis of the IPS train-ing data revealed that the interacting proteins in thiscorpus belong to over 60 species, and only 56.27%of them are human.As noted in previous work (Krauthammer and Ne-nadic, 2004; Chen et al, 2005; Krallinger et al,2007; Wang, 2007), determining the correct speciesfor the protein mentions is a very important step to-wards TI.
However, as far as we know, there hasbeen little work in species disambiguation and in towhat extent resolving species ambiguity can help TI.3 Species Disambiguation3.1 Data and OntologyThe species tagger was developed on the ITI TXMcorpora (Alex et al, 2008b), which were producedas part of the TXM project (Alex et al, 2008a).
Wecreated two corpora in slightly different domains,EPPI and TE.
The EPPI corpus consists of 217 full-text papers selected from PubMed and PubMed Cen-tral and domain experts annotated all documents forboth protein entities and PPIs, as well as extra (en-riched) information associated with the PPIs and nor-malisations of the proteins to publicly available on-tologies.
The TE corpus consists of 230 full-textpapers, in which entities such as proteins, tissues,genes and mRNAcDNAs were identified, and a newtissue expression relation was marked up.We used these corpora to develop a species tag-ging system.
As the biomedical entities in thedata were manually assigned with standard databaseidentifiers,3 it was straightforward to obtain theirspecies IDs through the mappings provided by En-trezGene and RefSeq.
In more detail, proteins, pro-tein complexes, genes and mRNAcDNAs in both EPPIand TE datasets were assigned with NCBI Taxon-omy IDs (TaxIDs)4 denoting their species.
TheEPPI and TE datasets have different distributions ofspecies.
The entities in the EPPI data belong to118 species with human being the most frequentat 51.98%.
In the TE data, the entities are across67 species and mouse was the most frequent at44.67%.5To calculate the inter-annotator-agreement, about40% of the documents were doubly annotated bydifferent annotators.
The averaged F1 scores of3In our data, genes are tagged with EntrezGene IDs, andproteins and mRNAcDNAs with RefSeq IDs.4http://www.ncbi.nlm.nih.gov/sites/entrez?db=Taxonomy5These figures were obtained from the training split of thedatasets.72EPPI devtest TE devtestP R F1 P R F1PreWd 81.88 1.87 3.65 91.49 1.63 3.21PreWd + Spread 63.85 14.17 23.19 77.84 17.97 29.20PreWd Sent 60.79 5.16 9.52 56.16 7.76 13.64PreWd Sent + Spread 39.74 50.54 44.49 31.71 46.68 37.76Prefix 98.98 3.07 5.96 77.93 2.97 5.72PreWd + Prefix 91.95 4.95 9.40 82.27 4.62 8.75PreWd + Prefix + Spread 68.46 17.49 27.87 77.77 21.26 33.39Table 1: Results (%) of the rule-based species tagger.species annotation on the doubly annotated EPPI andTE datasets are 86.45% and 95.11%, respectively,indicating that human annotators have high agree-ment when assigning species to biomedical entities.3.2 Detecting Species WordsWords referring to species, such as human, are im-portant indicators of the species of the nearby enti-ties.
We have developed a rule-based program thatdetects species words, which were used to help thespecies identification systems described in the fol-lowing sections.The species word tagger is a lexical look-upcomponent which applies to tokenised text andmarks content words such as human, murine andD.
melanogaster with their corresponding speciesTaxIDs.
In addition, rules written in an lxtransducegrammar6 are used to identify species prefixes (e.g.,?h?
for human, ?m?
for mouse).
For example, theterm mSos-1 would be assigned with a TaxID formouse.
Note that a species ?word?
may contain sev-eral words, for example, ?E.
coli?.
Please see (Wangand Grover, 2008) for more details on the speciesword tagger.3.3 Assigning Species to Entities3.3.1 Rule-based ApproachIt is intuitive that a species word that occurs nearan entity (e.g., ?mouse p53?)
is a strong indicator ofits species.
To assess this intuition, we developed aset of five rules using heuristics and species wordsdetected by the species word tagger.?
PreWd: If the word preceding an entity is a speciesword, assign the species indicated by that word tothe entity.6See http://www.ltg.ed.ac.uk/software/ltxml2 for details of the LT-XML 2 tools developed at theLTG group at Edinburgh University.?
PreWd Sent: If a species word that occurs to theleft of an entity and in the same sentence, assign thespecies indicated by that word to the entity.?
Prefix: If an entity has a species-indicating prefix,e.g., mSos-1, then tag the species to that entity.?
Spread: Spread the species of an entity e to all en-tities in the same document that have the same sur-face form with e. This rule must be used in conjunc-tion with the other rules.?
Majority Vote:7 Count the species words in a docu-ment and assign as a weight to each species the pro-portion of all species words in the document thatrefer to the species.8 Tag all entities in the docu-ment the species with the highest weight, defaultingto human in the case of a tie.Table 1 shows the results of species tagging whenthe above rules were applied.
As we can see, the pre-cision of the systems that rely solely on the previousspecies words or prefixes is very good but the recallis low.
The system that looks at the previous speciesword in the same sentence does better as measuredby F1.
In addition, spreading the species improvesboth systems but the overall results are still not sat-isfactory.It is slightly counter-intuitive that using a rulesuch as ?PreWd?
did not achieve perfect precision.Closer inspection revealed that most of the false pos-itives were due to a few problematic guidelines inthe annotation process.
For example,?
?The amounts of human and mouse CD200R ...?,where ?CD200R?
was tagged as mouse (10090) bythe system but the gold-standard answer was human(9606).
This was due to the fact that the annotationtool was not able to assign multiple correct species7TheMajority Vote rule was used by default in the TI system,which is described in Section 4.1.8For example, if there are N species words in a documentand Nhuman are associated with human, the human speciesweight is calculated as NhumanN .73BL EPPI TE Combined EPPI Model TE Model Combined ModelModel Model Model +Rules +Rules +RulesEPPI devtest 60.56 73.03 58.67 72.28 74.24 59.67 73.77TE devtest 30.22 67.15 69.82 67.20 67.53 70.14 67.47Overall 48.88 70.77 62.96 70.33 71.66 63.70 71.34Table 2: Accuracy (%) of the machine-learning based species tagger and the hybrid species tagger as tested on theEPPI and TE devtest datasets.
An ?Overall?
score is the micro-average of a system?s accuracy on both datasets.to a single entity.?
?...
wheat eIFiso4G ...?, where ?eIFiso4G?
wastagged as wheat (4565) but the annotator thoughtit was Triticum (4564).
In this case, TaxID 4565 isa species under genus 4564, and arguably is also acorrect answer.
Other similar cases include Xeno-pus vs. Xenopus tropicalis, and Rattus vs. Rattusnorvegicus, etc.
This is the main cause for the falsepositives as our system always predicts species in-stead of genus or TaxIDs of any other ranks, whichthe annotators occasionally employed.3.3.2 Machine Learning ApproachWe split the EPPI and TE datasets into trainingand development test (devtest) sets and developeda machine-learning (ML) based species tagger.
Us-ing the training splits, we trained a maximum en-tropy classifier9 using the following set of features,with respect to each entity occurrence.
The param-eter n was empirically developed using the trainingdatasets.?
leftContext The n word lemmas to the left of theentity, without position (n = 200).?
rightContext The n word lemmas to the right of theentity, without position (n = 200).?
leftSpeciesIDs The n species IDs, located to the leftof the entity and assigned by the species word tag-ger (n = 5).?
rightSpeciesIDs The n species IDs, located to theright of the entity and assigned by the species wordtagger (n = 5).?
leftNouns The n nouns to the left of the entity (withorder and n = 2).
This feature attempts to cap-ture cases where a noun preceding an entity indi-cates species, e.g., mouse protein p53.?
leftAdjs The n adjectives to the left of the entity(with order and n = 2).
This feature intends tocapture cases where an adjective preceding an en-tity indicates species, e.g., murine protein p53.9http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html?
leftSpeciesWords The n species word forms, identi-fied by the species word tagger, located to the leftof the entity (n = 5).?
rightSpeciesWords The n species word forms, iden-tified by the species word tagger, located to the rightof the entity (n = 5).?
firstLetter The first character of the entity itself.Sometimes the first letters of entities indicate theirspecies, e.g., hP53.?
documentSpeciesIDs All species IDs that occur inthe article in question.?
useStopWords If this feature is switched on then fil-ter out the words that appear in a pre-compiled stop-word list from the above features.
The list consistsof frequent common English words such as prepo-sitions (e.g., in).?
useStopPattern If this feature is switched on then fil-ter out the words consisting only of digits and punc-tuation characters.The results of the ML species tagger are shown inTable 2.
We measure the performance in accuracyinstead of F1 because the ML based tagger assigns aspecies tag to every entity occurrence, and thereforeprecision is equal to recall.
We tested four modelson the devtest portions of the EPPI and TE corpora:?
BL: a baseline system, which tags the devtest in-stances using the most frequent species occurringin the corresponding training dataset.
For example,human is the most frequent species in the EPPI train-ing data, and therefore all entities in the EPPI devtestdataset were tagged with human.?
EPPI Model: obtained by training the maxent clas-sifier on the EPPI training data.?
TE Model: obtained by training the maxent classi-fier on the TE training data.?
Combined Model: obtained by training the maxentclassifier on a joint dataset consisting of both theEPPI and TE training corpora.3.3.3 Hybrid ApproachAs we have shown, rules ?PreWd?
and ?Prefix?achieved very good precision but low recall, which74suggests that when these rules were applicable, it ishighly likely that they would get the correct species.Based on this observation, we combined the ML ap-proach and the rule-based approach in such a waythat the rules ?PreWd?
and ?Prefix?
were applied ontop of ML and override predictions made by ML.
Inother words, the rules act as a post-processor andcorrect the decisions made by the ML when verystrong species indicators such as previous specieswords or species prefixes are detected.
This shouldincrease precision and at the same time keep recallrelatively intact.
The hybrid systems were tested onthe same datasets and the results are shown in theright 3 columns in Table 2.We performed significance tests on the results inTable 2.
First, a Friedman test was used to deter-mine whether the 7 sets of results10 were signifi-cantly different, and then pairwise Wilcoxon SignedRank tests were employed to tell whether any sys-tem performed significantly better than others.
Onboth datasets, the 6 machine-learning models signif-icantly outperformed the baseline (p < 0.01).
OnEPPI devtest dataset, the EPPI models (with or with-out rules) and the Combined Models outperformedthe TE models (p < 0.05), while on TE dataset, theTE models and the Combined Models outperformedthe EPPI models (p < 0.05).
Also, applying thepost filtering rules did not significantly improve theML models, although it appears that adding the rulesconsistently increase the accuracy by a small mar-gin.4 Term Identification4.1 The TI systemThe TI system is composed of a matcher which de-termines a list of candidate identifiers and a rankerthat assigns a confidence value to each identifierthat is used to rank the candidates in order with themost likely identifiers occurring first.
The matcher isbased largely on the rule-based system described in(Wang and Matthews, 2008), but has been put into amore flexible framework that allows for defining andcustomising the rules in a configuration file.
In ad-dition, the system has been expanded to perform TIon additional entity types.
The rules for each entitywere developed using the training data and a visuali-10The Friedman test requires accuracy figures with respect toeach document in the datasets, which are not shown in Table 2.sation system that compared the synonym list for thetarget identifiers with the actual entity mentions andprovided visual feedback on the true positives andfalse positives resulting from candidate rules sets.Examples of some of the rules that can be incorpo-rated into the system are listed below.
A confidencevalue is assigned to each of the rules using heuristicsand passed to the ranking system.1.
LowerCase: Convert the entity mention to lower-case and look up the result in a lower case versionof the entity term database.2.
Norm: Normalise the mention11 and look up the re-sult in a normalised version of the term database.3.
Prefix: Add and/or remove a set of prefixes fromthe entity mention and look up the result in the en-tity term database.
The actual prefixes and whetherto add or remove them are specified in the configu-ration file.4.
Suffix: Add and/or remove a set of suffixes from theentity mention and look up the result in the entityterm database.
The actual suffixes and whether toadd or remove them are specified in the configura-tion file.5.
Porter: Compute the Porter stem of the entity men-tion and looked up the synonym in a Porter stemmedversion of the entity term database.The ranking system currently works by defining aset of confidence indicators for each entity, comput-ing the confidence for each indicator and then multi-plying each individual confidence together to deter-mine the overall identifier confidence.
The follow-ing indicators are currently used by the system.1.
Match: The confidence as determined by thematcher.2.
Species: The confidence that the species of the iden-tifier is the correct species.3.
Reference Count: Based on the number of liter-ature references12 associated with each identifier.The higher the reference count, the higher the con-fidence.11Normalising a string involves converting Greek charactersto English (e.g., ?
?alpha), converting to lowercase, changingsequential indicators to integer numerals (e.g., i, a, alpha?1,etc.)
and removing all spaces and punctuation.
For example,rab1, rab-1, rab?, rab I are all normalised to rab1.12The Reference Counts were obtained from EntrezGene andRefSeq databases.754.
Primary Name: Based on a determination that theentity mention is the primary name for the identi-fier.
This is based both on a name provided by thelexicon and a name derived from the synonym list.Among these, one of the most critical indicators isthe species confidence.
By default, this confidenceis set to the weight assigned to the species by theMajority Vote tagger (see Section 3.3.1).
When thespecies of an entity is tagged by an external speciestagger or by human annotators, the default confi-dence can be overridden.
This setting allows us tointegrate automatic species taggers, such as the onesdescribed in the previous section, for achieving bet-ter TI performance.
For example, suppose we wantto employ theHybrid species tagger.
To compute thespecies confidence, first the hybrid tagger is used topredict the most likely species and the Majority Votetagger is run at the same time.
If the species of anidentifier matches the species assigned by the hybridtagger, the species confidence is set to the weightgenerated by the hybrid tagger.
Otherwise, the con-fidence is set to the weight generated by theMajorityVote tagger.To assess how much species ambiguity accountsfor the overall ambiguity in biomedical entities, weestimated the averaged ambiguity rates for the pro-tein entities in the TXM datasets, without and withthe species information.
Suppose there are n uniqueprotein mentions in a dataset.
First, we look up theRefSeq database by exact match with every uniqueprotein mention mi, where i ?
{0..n ?
1}, andfor each mi we retrieve two lists of identifiers: Liand L?i, where Li consists of all identifiers and L?ionly contains the identifiers whose model organ-ism matches the manually tagged species of the pro-tein mention.
The ambiguity rates without and withspecies are computed byPn?1i=0 |Li|n andPn?1i=0 |L?i|n , re-spectively.
Table 3 shows the ambiguity rates on theEPPI and TE datasets.Protein Cnt ID Cnt AmbiguityEPPI 6,955 184,633 26.55EPPI species 6,955 17,357 2.50TE 8,539 103,016 12.06TE species 8539 12,705 1.49Table 3: Ambiguity in protein entities, with and withoutspecies information, in EPPI and TE datasets.4.2 Experiments on TXM DataTo identify whether species disambiguation can im-prove performance of TI, we ran the TI system onthe EPPI and TE data.
As shown in Tables 4 and 5,we tested the TI systems with or without help froma number of species tagging systems, including:?
Baseline: Run TI without species tags.13?
Gold Species: Run TI with manually tagged species.This is the upper-bound performance.?
Rule: Run TI with species predicted by the rule-based species tagger.?
ML(human/mouse): Run TI with the species that oc-curs most frequently in the training datasets (i.e.,human for EPPI and mouse for TE).?
ML(EPPI): Run TI with species predicted by the MLtagger trained on the EPPI training dataset.?
ML(EPPI)+Rule: Run TI with species predicted bythe hybrid system using both ML(EPPI) and therules.?
ML(TE): Run TI with species predicted by the MLtagger trained on the TE training dataset.?
ML(TE)+Rule: Run TI with species predicted by thehybrid system using both ML(TE) and the rules.?
ML(EPPI+TE): Run TI with species predicted by theML tagger trained on both EPPI and TE training data.?
ML(EPPI+TE)+Rule: Run TI with species predictedby the hybrid system using both ML(EPPI+TE) andthe rules.We score the systems using top n precision, wheren ?
{1, 5, 10, 15, 20}.
The argument for this evalua-tion scheme is that if a TI system is not good enoughin predicting a single identifier correctly, a ?bag?
ofIDs with the correct answer included would also behelpful.
The ?Avg.
Rank?
field denotes the averagedposition where the correct answer lies in, and thelower the value is, the better the TI system performs.For example, a TI system with an ?Avg.
Rank?
of 1would be ideal, as it would always return the correctID at the top of the list.
Note that in the TE data, notonly protein entities, but also genes, mRNAcDNA,and GOMOPs14 were tagged.On both datasets, using the gold standard speciesmuch improved accuracy of TI (e.g., 19.2% on EPPI13Note that the TI system already integrated a basic speciestagging system that uses the Majority Vote rule as described inSection 3.3.1.
Thus this is a fairly high ?baseline?.14GOMOP is a tag that denotes an entity being either a gene,or an mRNAcDNA, or a protein, which was used when the anno-tator could not determine what type the entity in question was.76Method Prec@1 Prec@5 Prec@10 Prec@15 Prec@20 Avg.
RankBaseline 54.31 73.45 76.44 77.90 78.51 5.82Gold Species 73.52 79.36 80.75 80.75 80.99 1.62Rule 54.99 73.72 76.45 77.91 78.52 5.79ML(human) 65.66 76.36 78.82 79.78 80.03 2.58ML(EPPI) 65.24 76.82 79.01 79.93 80.29 2.39ML(EPPI)+Rule 65.88 77.09 79.04 79.94 80.30 2.36ML(TE) 55.87 75.14 78.69 79.85 80.30 2.86ML(TE)+Rule 56.54 75.47 78.70 79.86 80.31 2.83ML(EPPI+TE) 64.55 76.48 78.53 79.83 80.38 2.49ML(EPPI+TE)+Rule 65.03 76.62 78.55 79.84 80.39 2.46Table 4: Results of TI on the EPPI dataset.
All figures, except ?Avg.
Rank?, are percentages.
This evaluation wascarried out on protein entities only.Method Prec@1 Prec@5 Prec@10 Prec@15 Prec@20 Avg.
RankBaseline 63.24 76.20 77.30 77.94 78.25 1.72Gold Species 71.82 78.03 78.34 78.40 78.41 1.29Rule 63.45 76.21 77.30 77.95 78.25 1.72ML(mouse) 58.76 75.40 77.25 77.92 78.24 1.90ML(EPPI) 66.59 76.53 77.23 77.76 78.12 1.68ML(EPPI)+Rule 66.85 76.54 77.24 77.76 78.12 1.67ML(TE) 66.12 76.25 77.32 77.81 78.11 1.70ML(TE)+Rule 66.37 76.25 77.32 77.81 78.11 1.70ML(EPPI+TE) 65.78 76.14 77.28 77.84 78.12 1.71ML(EPPI+TE)+Rule 66.03 76.14 77.29 77.84 78.12 1.70Table 5: Results of TI on the TE dataset.
All figures, except ?Avg.
Rank?, are percentages.
There are four entity typesin the TE data, i.e., protein, gene, mRNAcDNA and GOMOP.
The evaluation was carried out on all entity types.data).
Also, automatically predicted species tagswere proven to be helpful.
On the EPPI data, theML(EPPI)+Rule outperformed other systems.
Notethat the species distribution in the devtest dataset isstrongly biased to human, which explains why theML(human) system performed nearly as well.
How-ever, defaulting to human was not guaranteed to suc-ceed because one would not be able to know theprior species in a collection of unseen documents.Indeed, on the TE data, the system ML(mouse),which uses the most frequent species in the trainingdata, i.e.
mouse, as default, yielded poor results.4.3 Experiments on BioCreAtIvE DataTo assess the portability of the species tagging ap-proaches, an ?artificial?
dataset was created by join-ing the species-specific datasets from BioCreAtIvE1 & 2 GN tasks to form a corpus consisting of fourspecies.
In detail, four datasets were taken, threefrom BioCreAtIvE 1 task 1B (i.e., fly, mouse andyeast) and one from BioCreAtIvE 2 task GN (i.e., hu-man).
Assuming genes in each dataset are species-specific,15 we can train/test ML models for speciesdisambiguation and apply them to help TI.
This taskis more difficult than the original BioCreAtIvE GNtasks due to the additional ambiguity caused by mul-tiple model organisms.We first carried out experiments on species dis-ambiguation.
In addition to the TXM (i.e., the sys-tem uses ML(EPPI+TE)+Rule model) and the Major-ity Vote taggers, we trained the species tagger ona dataset comprising of the devtest sets from theBioCreAtIvE I & II GN tasks.
In more detail, we firstpre-processed the dataset and marked up gene enti-ties with an NER system (Alex et al, 2007; Grover etal., 2007).16 The entities were also tagged with the15This assumption is not strictly true because each datasetmay contain genes of other species, and it would be hard toassess how true it is as abstracts in the BioCreAtIvE GN datasetsare not normalised to an entity level.16The NER system was trained on BioCreAtIvE II GM train-ing and test datasets.77species as indicated by the source dataset where theywere drawn from, which were used as the ?Gold?species.
Using the same algorithm and feature set asdescribed in Section 3.3.2, a BC model was trained.human fly mouse yeastMajority Vote 82.35 78.43 71.69 85.12BC model 70.23 89.24 75.41 87.64TXM model 93.35 3.27 31.89 3.49Table 6: Accuracy (%) of the species disambiguationsystems as tested on the BioCreAtIvE I & II test data.The ?BC model?
was trained on the BioCreAtIvE de-vtest data, the ?TXM model?
was trained on the TXM EPPIand TE training data, and the ?Majority Vote?
was the de-fault species tagging system in the TI system (see Section3.3.1).As shown in Table 6, except on human, the TXMmodel yielded very disappointing results, whereasthe BC model did well overall.
This was becausethe TXM model was trained on a dataset where flyand yeast entities occur rarely with only 2% and 5%of the training instances belonging to these species,respectively, which again revealed the influence ofthe bias introduced in the training material to the MLmodels.System Precision Recall F1Gold 70.1 63.3 66.5Majority Vote 46.7 56.3 51.0TXM model 37.8 46.5 41.7BC model 45.8 56.1 50.4Table 7: Performance of TI with or without the automati-cally predicted species on the joint BioCreAtIvE GN testdataset.Using the species disambiguation models, we car-ried out TI experiments, using the same procedureas we did on the TXM data.
The results were ob-tained using the official BioCreAtIvE GN scorers17and are presented in Table 7.
Performance of TI as-sisted by all three species taggers were much behindthat of TI using the gold-standard species, whichshows species-tagging can potentially enhance TIperformance and there is much room for improving17We tested the TI system on the four original BioCreAtIvEGN datasets separately and the averaged performance was aboutthe median among the participating systems in the workshops.We did not optimise the TXM TI system on BioCreAtIvE, as ourpoint here is to measure the TI performance with or without helpfrom the automatic predicted species.the species disambiguation systems.
On the otherhand, it was disappointing that the ?Majority Vote?system, which did not use any external species tag-ger, achieved the best results, while TI with the ?BCmodel?
tagger yielded slightly worse results and theTXM model performed poorly.# Species # of Docs % of Docs1 96 26.202 121 32.793+ 153 41.19Table 8: # of species per document in the TXM data.One possible reason that the ?Majority Vote?
tag-ger yielded reasonably good result on the BioCre-AtIvE dataset, but unsatisfactory result on the TXMdatasets was due to the difference in documentlength in the two corpora: the BioCreAtIvE corpusis comprised of abstracts and the TXM corpora con-sist of only full-length articles.
In abstracts, authorsare inclined to only talk about the main biomedicalentities described in the paper, whereas in full arti-cles, they tend to describe a larger number of enti-ties, possibly in multiple species, for the purposes ofdescribing related work or comparison.
Recall thatthe ?Majority Vote?
rule outputs the species indicatedby the majority of the species words, which wouldobviously perform better on abstracts, where morelikely only one species is described, than on full-length articles.
Table 8 shows the number of speciesper document in the TXM data, where most docu-ments (i.e., 74%) involve more than one species, inwhich cases the ?Majority Vote?
would not be able totake obvious advantage.5 Conclusions and Future WorkThis paper presented a range of solutions to the taskof species disambiguation and evaluated their per-formance on the ITI TXM corpus, and on a jointdataset from BioCreAtIvE I & II GN tasks.
Weshowed that rule-based species tagging systems thatexploit heuristics, such as previous species words orspecies prefix, can achieve very high precision butlow recall.
ML species taggers, on the other hand,can achieve good overall performance, under thecondition that the species distributions in trainingand test datasets are not too distant.
Our best per-forming species tagger is a hybrid system that first78uses ML to predict species and then applies certainrules to correct errors.We also performed TI experiments with help fromspecies tags assigned by human annotators, or pre-dicted by the automatic species taggers.
On alldatasets, the gold-standard species tags improved TIperformance by a large margin: 19.21% on the EPPIdevtest set, 8.59% on the TE devtest set, and 23.4%on the BioCreAtIvE GN test datasets, which clearlyshows that species information is indeed very impor-tant for TI.
On the EPPI and TE datasets, the speciespredicted by the best-performing hybrid system im-proved TI by 11.57% and 3.61%, respectively.
Onthe combined dataset from BioCreAtIvE GN tasks,however, it did not work as well as expected.In the future we plan to work on better ways tointegrate the machine learning approaches and therules.
In particular, we would like to explore statis-tical relational learning, which may provide ways tointegrate rules as constraints into machine learningand may be able to alleviate the bias in the learntmodels.AcknowledgementsThe work was supported by the ITI Life SciencesText Mining programme.18ReferencesB.
Alex, B. Haddow, and C. Grover.
2007.
Recognisingnested named entities in biomedical text.
In Proceed-ings of BioNLP 2007, Prague, Czech Republic.B.
Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,M.
Matthews, S. Roebuck, R. Tobin, and X. Wang.2008a.
Assisted curation: does text mining reallyhelp?
In Proceedings of PSB.B.
Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,M.
Matthews, S. Roebuck, R. Tobin, and X. Wang.2008b.
The ITI TXM corpus: Tissue expressionand protein-protein interactions.
In Proceedings ofthe LREC Workshop on Building and Evaluating Re-sources for Biomedical Text Mining, Morocco.L.
Chen, H. Liu, and C. Friedman.
2005.
Gene name am-biguity of eukaryotic nomenclatures.
Bioinformatics,21(2):248?256.J.
Fluck, H. Mevissen, H. Dach, M. Oster, andM.
Hofmann-Apitius.
2007.
ProMiner: Recogni-tion of human gene and protein names using regularly18http://www.itilifesciences.comupdated disctionaries.
In Proceedings of the SecondBioCreative Challenge Evaluation Workshop.C.
Grover, B. Haddow, E. Klein, M. Matthews, L. A.Nielsen, R. Tobin, and X. Wang.
2007.
Adapting a re-lation extraction pipeline for the BioCreAtIvE II task.In Proceedings of the BioCreAtIvE II Workshop 2007,Madrid.D.
Hanisch, K. Fundel, H-T Mevissen, R Zimmer, andJ Fluck.
2005.
ProMiner: Organism-specific pro-tein name detection using approximate string match-ing.
BMC Bioinformatics, 6(Suppl 1):S14.L.
Hirschman, M. Colosimo, A. Morgan, J. Columbe, andA.
Yeh.
2004.
Task 1B: Gene list task BioCreAtIveworkshop.
In BioCreative: Critical Assessment for In-formation Extraction in Biology.L.
Hirschman, A. Yeh, C. Blaschke, and A. Valencia.2005.
Overview of BioCreAtIvE: critical assessmentof information extraction for biology.
BMC Bioinfor-matics, 6(Suppl1):S1.L.
Hirschman, M. Krallinger, and A. Valencia, edi-tors.
2007.
Second BioCreative Challenge EvaluationWorkshop.
Fundacio?n CNIO Carlos III, Madrid.M.
Krallinger, F. Leitner, and A. Valencia.
2007.
Assess-ment of the second BioCreative PPI task: Automaticextraction of protein-protein interactions.
In Proceed-ings of the BioCreAtIvE II Workshop 2007, pages 41?54, Madrid, Spain.M.
Krauthammer and G. Nenadic.
2004.
Term iden-tification in the biomedical literature.
Journal ofBiomedical Informatics (Special Issue on Named En-tity Recogntion in Biomedicine), 37(6):512?526.A.
A. Morgan and L. Hirschman.
2007.
Overview ofBioCreative II gene normalisation.
In Proceedings ofthe BioCreAtIvE II Workshop, Madrid.X.
Wang and C. Grover.
2008.
Learning the species ofbiomedical named entities from annotated corpora.
InProceedings LREC2008, Marrakech, Morocco.X.
Wang and M. Matthews.
2008.
Comparing usabil-ity of matching techniques for normalising biomedicalnamed entities.
In Proceedings of PSB.X.
Wang.
2007.
Rule-based protein term identificationwith help from automatic species tagging.
In Proceed-ings of CICLING 2007, pages 288?298, Mexico City.79
