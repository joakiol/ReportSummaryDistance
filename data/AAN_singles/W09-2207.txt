Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49?57,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn Analysis of Bootstrapping for the Recognition of Temporal ExpressionsJordi PovedaTALP Research CenterTechnical University of Catalonia (UPC)Barcelona, Spainjpoveda@lsi.upc.eduMihai SurdeanuNLP GroupStanford UniversityStanford, CAmihais@stanford.eduJordi TurmoTALP Research CenterTechnical University of Catalonia (UPC)Barcelona, Spainturmo@lsi.upc.eduAbstractWe present a semi-supervised (bootstrapping)approach to the extraction of time expressionmentions in large unlabelled corpora.
Becausethe only supervision is in the form of seedexamples, it becomes necessary to resort toheuristics to rank and filter out spurious pat-terns and candidate time expressions.
Theapplication of bootstrapping to time expres-sion recognition is, to the best of our knowl-edge, novel.
In this paper, we describe onesuch architecture for bootstrapping Informa-tion Extraction (IE) patterns ?suited to theextraction of entities, as opposed to events orrelations?
and summarize our experimentalfindings.
These point out to the fact that apattern set with a good increase in recall withrespect to the seeds is achievable within ourframework while, on the other side, the de-crease in precision in successive iterations issuccesfully controlled through the use of rank-ing and selection heuristics.
Experiments arestill underway to achieve the best use of theseheuristics and other parameters of the boot-strapping algorithm.1 IntroductionThe problem of time expression recognition refersto the identification in free-format natural languagetext of the occurrences of expressions that denotetime.
Time-denoting expressions appear in a greatdiversity of forms, beyond the most obvious ab-solute time or date references (e.g.
11pm, Febru-ary 14th, 2005): time references that anchor on an-other time (three hours after midnight, two weeks be-fore Christmas), expressions denoting durations (afew months), expressions denoting recurring times(every third month, twice in the hour), context-dependent times (today, last year), vague references(somewhere in the middle of June, the near future)or times that are indicated by an event (the day G.Bush was reelected).
This problem is a subpart ofa task called TERN (Temporal Expression Recog-nition and Normalization), where temporal expres-sions are first identified in text and then its intendedtemporal meaning is represented in a canonical for-mat.
TERN was first proposed as an independenttask in the 2004 edition of the ACE conferences1.The most widely used standard for the annotation oftemporal expressions is TIMEX (Ferro et al, 2005).The most common approach to temporal expres-sion recognition in the past has been the use ofhand-made grammars to capture the expressions (see(Wiebe et al, 1998; Filatova and Hovy, 2001; Sa-quete et al, 2004) for examples), which can thenbe easily expanded with additional attributes for thenormalization task, based on computing distanceand direction (past or future) with respect to a ref-erence time.
This approach achieves an F1-measureof approximately 85% for recognition and normal-ization.
The use of machine learning techniques ?mainly statistical?
for this task is a more recentdevelopment, either alongside the traditional hand-grammar approach to learn to distinguish specificdifficult cases (Mani and Wilson, 2000), or on itsown (Hacioglu et al, 2005).
The latter apply SVMsto the recognition task alone, using the output of sev-eral human-made taggers as additional features forthe classifier, and report an F1-measure of 87.8%.1http://www.nist.gov/speech/tests/ace/49Bootstrapping techniques have been used for suchdiverse NLP problems as: word sense disambigua-tion (Yarowsky, 1995), named entity classification(Collins and Singer, 1999), IE pattern acquisition(Riloff, 1996; Yangarber et al, 2000; Yangarber,2003; Stevenson and Greenwood, 2005), documentclassification (Surdeanu et al, 2006), fact extractionfrom the web (Pas?ca et al, 2006) and hyponymy re-lation extraction (Kozareva et al, 2008).
(Yarowsky, 1995) used bootstrapping to train de-cision list classifiers to disambiguate between twosenses of a word, achieving impressive classificationaccuracy.
(Collins and Singer, 1999) applied boot-strapping to extract rules for named entity (NE) clas-sification, seeding the sytem with a few handcraftedrules.
Their main innovation was to split trainingin two alternate stages: during one step, only con-textual rules are sought; during the second step, thenew contextual rules are used to tag further NEs andthese are used to produce new spelling rules.Bootstrapping approaches are employed in(Riloff, 1996), (Yangarber et al, 2000), (Yangarber,2003), and (Stevenson and Greenwood, 2005)in order to find IE patterns for domain-specificevent extraction.
(Pas?ca et al, 2006) employ abootstrapping process to extract general facts fromthe Web, viewed as two-term relationships (e.g[Donald Knuth, 1938] could be an instance ofa ?born in year?
relationship).
(Surdeanu et al,2006) used bootstrapping co-trained with an EMclassifier in order to perform topic classificationof documents based on the presence of certainlearned syntactic-semantic patterns.
In (Kozarevaet al, 2008), bootstrapping is applied to findingnew members of certain class of objects (i.e.
an?is-a?
relationship), by providing a member of therequired class as seed and using a ?such as?
type oftextual pattern to locate new instances.The recognition of temporal expressions is cru-cial for many applications in NLP, among them: IE,Question Answering (QA) and Automatic Summa-rization (for the temporal ordering of events).
Workon slightly supervised approaches such as bootstrap-ping is justified by the large availability of unla-belled corpora, as opposed to tagged ones, fromwhich to learn models for recognition.2 ArchitectureFigure 1 illustrates the building blocks of the algo-rithm and their interactions, along with input andoutput data.The inputs to the bootstrapping algorithm are theunlabelled training corpus and a file of seed ex-amples.
The unlabelled corpus is a large collec-tion of documents which has been tokenized, POStagged, lemmatized, and syntactically analyzed forbasic syntactic constituents (shallow parsing) andheadwords.
The second input is a set of seed exam-ples, consisting of a series of token sequences whichwe assume to be correct time expressions.
The seedsare supplied without additional features, and withoutcontext information.Our bootstrapping algorithm works with two al-ternative views of the same target data (time expres-sions), that is: patterns and examples (i.e.
an in-stance of a pattern in the corpus).
A pattern is a gen-eralized representation that can match any sequenceof tokens meeting the conditions expressed in thepattern (these can be morphological, semantic, syn-tactic and contextual).
An example is an actual can-didate occurrence of a time expression.
Patterns aregenerated from examples found in the corpus and,in its turn, new examples are found by searchingfor matches of new patterns.
Both patterns and ex-amples may carry contextual information, that is, awindow of tokens left and right of the candidate timeexpression.Output examples and output patterns are the out-puts of the bootstrapping process.
Both the set ofoutput examples and the set of output patterns areincreased with each new iteration, by adding the newcandidate examples (respectively, patterns) that havebeen ?accepted?
during the last iteration (i.e.
thosethat have passed the ranking and selection step).Initially, a single pass through the corpus is per-formed in order to find occurrences of the seeds inthe text.
Thus, we bootstrap an initial set of exam-ples.
From then on, the bootstrapping process con-sists of a succession of iterations with the followingsteps:1.
Ranking and selection of examples: Each ex-ample produced during any of the previous it-erations, 0 to i ?
1, is assigned a score (rank-ing).
The top n examples are selected to growthe set of output examples (selection) and will50Figure 1: Block diagram of bootstrapping algorithmbe used for the next step.
The details are givenin Section 4.2.2.
Generation of candidate patterns: Candidatepatterns for the current iteration are generatedfrom the selected examples of the previous step(discussed in Section 3).3.
Ranking and selection of candidate patterns:Each pattern from the current iteration is as-signed a score and the top m patterns are se-lected to grow the set of output patterns and tobe used in the next step (discussed in Section4.1).
This step also involves a process of analy-sis of subsumptions, performed simultaneouslywith selection, in which the set of selected pat-terns is examined and those that are subsumedby other patterns are discarded.4.
Search for instances of the selected patterns:The training corpus is traversed, in order tosearch for instances (matches) of the selectedpatterns, which, together with the accepted ex-amples from all previous iterations, will formthe set of candidate examples for iteration i+1.Also, in order to relax the matching of pat-terns to corpus tokens and of token forms amongthemselves, the matching of token forms is case-insensitive, and all the digits in a token are gen-eralized to a generic digit marker (for instance,?12-23-2006?
is internally rewritten as ?@@-@@-@@@@?
).Even though our architecture is built on a tradi-tional boostrapping approach, there are several ele-ments that are novel, at least in the context of tem-poral expression recognition: a) our pattern repre-sentation incorporates full syntax and distributionalsemantics in a unified model (see Section 3); b) ourpattern ranking/selection approach includes a sub-sumption model to limit redundancy; c) the formu-lae in our example ranking/selection approach aredesigned to work with variable-length expressionsthat incorporate a context.3 Pattern representationPatterns capture both the sequence of tokens thatintegrate a potential time expression (i.e.
a timeexpression mention), and information from the leftand right context where it occurs (up to a boundedlength).
Let us call prefix the part of the pattern thatrepresents the left context, infix the part that repre-sents a potential time expression mention and postfixthe part that represents the right context.The EBNF grammar that encodes our pattern rep-resentation is given in Figure 2.
Patterns are com-posed of multiple pattern elements (PEs).
A patternelement is the minimal unit that is matched againstthe tokens in the text, and a single pattern elementcan match to one or several tokens, depending onthe pattern element type.
A pattern is considered tomatch a sequence of tokens in the text when: first,all the PEs from the infix are matched (this gives thepotential time expression mention) and, second, allthe PEs from the prefix and the postfix are matched(this gives the left and right context information forthe new candidate example, respectively).
There-fore, patterns with a larger context window are morerestrictive, because all of the PEs in the prefix andthe postfix have to be matched (on top of the infix)for the pattern to yield a match.We distinguish among token-level generalizations51pattern ::= prefix SEP infix SEP postfix SEP(modifiers)*prefix ::= (pattern-elem)*infix ::= (pattern-elem)+postfix ::= (pattern-elem)*pattern-elem ::= FORM "(" token-form ")" |SEMCLASS "(" token-form ")" |POS "(" pos-tag ")" | LEMMA "(" lemma-form ")" |SYN "(" syn-type "," head ")" |SYN-SEM "(" syn-type "," head ")"modifiers ::= COMPLETE-PHRASEFigure 2: The EBNF Grammar for Patterns(i.e.
PEs) and chunk-level generalizations.
The for-mer have been generated from the features of a sin-gle token and will match to a single token in the text.The latter have been generated from and match to asequence of tokens in the text (e.g.
a basic syntacticchunk).
Patterns are built from the following typesof PEs (which can be seen in the grammar from Fig-ure 2):1.
Token form PEs: The more restrictive, onlymatch a given token form.2.
Semantic class PEs: Match tokens (sometimesmultiwords) that belong to a given semanticsimilarity class.
This concept is defined below.3.
POS tag PEs: Match tokens with a given POS.4.
Lemma PEs: Match tokens with a givenlemma.5.
Syntactic chunk PEs: Match a sequence of to-kens that is a syntactic chunk of a given type(e.g.
NP) and whose headword has the samelemma as indicated.6.
Generalized syntactic PEs: Same as the previ-ous, but the lemma of the headword may be anyin a given semantic similarity class.The semantic similarity class of a word is definedas the word itself plus a group of other semanti-cally similar words.
For computing these, we em-ploy Lin?s corpus of pairwise distributional similari-ties among words (nouns, verbs and adjectives) (Lin,1998), filtered to include only those words whosesimilarity value is above both an absolute (highestn) and relative (to the highest similarity value in theclass) threshold.
Even after filtering, Lin?s similari-ties can be ?noisy?, since the corpus has been con-structed relying on purely statistical means.
There-fore, we are employing in addition a set of manu-ally defined semantic classes (hardcoded lists) sen-sitive to our domain of temporal expressions, suchthat these lists ?override?
the Lin?s similarity cor-pus whenever the semantic class of a word presentin them is involved.
The manually defined semanticclasses include: the written form of cardinals; ordi-nals; days of the week (plus today, tomorrow andyesterday); months of the year; date trigger words(e.g.
day, week); time trigger words (e.g.
hour, sec-ond); frequency adverbs (e.g.
hourly, monthly); dateadjectives (e.g.
two- day, @@-week-long); and timeadjectives (e.g.
three-hour, @@-minute-long).We use a dynamic window for the amount of con-text that is encoded into a pattern, that is, we gen-erate all the possible patterns with the same infix,and anything between 0 and the specified length ofthe context window PEs in the prefix and the postfix,and let the selection step decide which variations getaccepted into the next iteration.The modifiers field in the pattern representa-tion has been devised as an extension mecha-nism.
Currently the only implemented mod-ifier is COMPLETE-PHRASE, which when at-tached to a pattern, ?rounds?
the instance (i.e.candidate time expression) captured by its infixto include the closest complete basic syntacticchunk (e.g.
?LEMMA(end) LEMMA(of) SEM-CLASS(January)?
would match ?the end of De-cember 2009?
instead of only ?end of December?against the text ?.
.
.
By the end of December 2009,. .
.
?).
This modifier was implemented in view of thefact that most temporal expressions correspond withwhole noun phrases or adverbial phrases.From the above types of PEs, we have built thefollowing types of patterns:1.
All-lemma patterns (including the prefix andpostfix).2.
All-semantic class patterns.3.
Combinations of token form with sem.
class.4.
Combinations of lemma with sem.
class.5.
All-POS tag patterns.6.
Combinations of token form with POS tag.7.
Combinations of lemma with POS tag.8.
All-syntactic chunk patterns.9.
All-generalized syntactic patterns.4 Ranking and selection of patterns andlearning examples4.1 PatternsFor the purposes of this section, let us define thecontrol set C as being formed by the seed examplesplus all the selected examples over the previous it-erations (only the infix considered, not the context).52Note that, except for the seed examples, this is onlyassumed correct, but cannot be guaranteed to be cor-rect (unsupervised).
In addition, let us define the in-stance set Ip of a candidate pattern p as the set ofall the instances of the pattern found in a fraction ofthe unlabelled corpus (only infix of the instance con-sidered).
Each candidate pattern pat is assigned twopartial scores:1.
A frequency-based score freq sc(p) that mea-sures the coverage of the pattern in (a sectionof) the unsupervised corpus:freq sc(p) = Card(Ip ?
C)2.
A precision score prec sc(p) that evaluates theprecision of the pattern in (a section of) the un-supervised corpus, measured against the con-trol set:prec sc(p) = Card(Ip?C)Card(Ip)These two scores are computed only against afraction of the unlabelled corpus for time effi-ciency.
There remains an issue with whether multi-sets (counting each repeated instance several times)or normal sets (counting them only once) should beused for the instance sets Ip.
Our experiments indi-cate that the best results are obtained by employingmultisets for the frequency-based score and normalsets for the precision score.Given the two partial scores above, we have triedthree different strategies for combining them:?
Multiplicative combination: ?1 log(1 +freq sc(p)) + ?2 log(2 + prec sc(p))?
The strategy suggested in (Collins and Singer,1999): Patterns are first filtered by imposinga threshold on their precision score.
Only forthose patterns that pass this first filter, their finalscore is considered to be their frequency-basedscore.?
The strategy suggested in (Riloff, 1996):{ prec sc(p) ?
log(freq sc(p)) if prec sc(p) ?
thr0 otherwise4.1.1 Analysis of subsumptionsIntertwined with the selection step, an analysis ofsubsumptions is performed among the selected pat-terns, and the patterns found to be subsumed by oth-ers in the set are discarded.
This is repeated until ei-ther a maximum ofm patterns with no subsumptionsamong them are selected, or the list of candidate pat-terns is exhausted, whichever happens first.
The pur-pose of this analysis of subsumptions is twofold: onthe one hand, it results in a cleaner output patternset by getting rid of redundant patterns; on the otherhand, it improves temporal efficiency by reducingthe number of patterns being handled in the last stepof the algorithm (i.e.
searching for new candidateexamples).In our scenario, a pattern p1 with instance set Ip1is subsumed by a pattern p2 with instance set Ip2if Ip1 ?
Ip2 .
We make a distinction among ?theo-retical?
and ?empirical?
subsumptions.
Theoreticalsubsumptions are those that can be justified based ontheoretical grounds alone, from observing the formof the patterns.
Empirical subsumptions are thosecases where in fact one pattern subsumes another ac-cording to the former definition, but this could onlybe detected by having calculated their respective in-stance sets a priori, which beats one of the purposesof the analysis of subsumptions ?namely, tempo-ral efficiency?.
We are only dealing with theoreti-cal subsumptions here.
A pattern theoretically sub-sumes another pattern when either of these condi-tions occur:?
The first pattern is identical to the second, ex-cept that the first has fewer contextual PEs inthe prefix and/or the postfix.?
Part or all of the PEs of the first pattern areidentical to the corresponding PEs in the sec-ond pattern, except for the fact that they areof a more general type (element-wise); the re-maining PEs are identical.
To this end, we havedefined a partial order of generality in the PEtypes (see section 3), as follows:FORM ?
LEMMA ?
SEMCLASS; FORM ?
POS;SYN ?
SYN-SEMC?
Both the above conditions (fewer contextualPEs and of a more general type) happen at thesame time.4.2 Learning ExamplesAn example is composed of the tokens which havebeen identified as a potential time expression (whichwe shall call the infix) plus a certain amount of leftand right context (from now on, the context) en-coded alongside the infix.
For ranking and selecting53examples, we first assign a score and select a num-ber n of distinct infixes and, in a second stage, weassign a score to each context of appearance of aninfix and select (at most) m contexts per infix.
Ourscoring system for the infixes is adapted from (Pas?caet al, 2006).
Each distinct infix receives three par-tial scores and the final score for the infix is a linearcombination of these, with the ?i being parameters:?1sim sc(ex) + ?2pc sc(ex) + ?3ctxt sc(ex)1.
A similarity-based score (sim sc(ex)), whichmeasures the semantic similarity (as per theLin?s similarity corpus (Lin, 1998)) of theinfix with respect to set of ?accepted?
outputexamples from all previous iterations plus theinitial seeds.
If w1, .
.
.
, wn are the tokens inthe infix (excluding stopwords); ej,1, .
.
.
, ej,mjare the tokens in the j-th example of the setE of seed plus output examples; and sv(x, y)represents a similarity value, the similaritySim(wi) of the i-th word of the infix wrtthe seeds and output is given by Sim(wi) =?|E|j=1 max(sv(wi, ej,1), .
.
.
, sv(wi, ej,mj )),and the similarity-based score of an in-fix containing n words is given by?ni=1 log(1+Sim(wi))n .2.
A phrase-completeness score (pc sc(ex)),which measures the likelihood that the infixis a complete time expression and not merelya part of one, over the entire set of candidateexample: count(INFIX)count(?INFIX?)3.
A context-based score (ctxt sc(ex)), intendedas a measure of the infix?s relevance.
For eachcontext (up to a length) where this infix appearsin the corpus, the frequency of the word withmaximum relative frequency (over the wordsin all the infix?s contexts) is taken.
The sumis then scaled by the relative frequency of thisparticular infix.Apart from the score associated with the infix,each example (i.e.
infix plus a context) receivestwo additional frequency scores for the left and rightcontext part of the example respectively.
Each ofthese is given by the relative frequency of the tokenwith maximum frequency of that context, computedover all the tokens that appear in all the contexts ofall the candidate examples.
For each selected infix,the m contexts with best score are selected.5 Experiments5.1 Experimental setupAs unsupervised data for our experiments, we usethe NW (newswire) category of LDC?s ACE 2005Unsupervised Data Pool, containing 456 Mbytes ofdata in 204K documents for a total of over 82 mil-lion tokens.
Simultaneously, we use a much smallerlabelled corpus (where the correct time expressionsare tagged) to measure the precision, recall and F1-measure of the pattern set learned by the bootstrap-ping process.
This is the ACE 2005 corpus, contain-ing 550 documents with 257K tokens and approx.4650 time expression mentions.
The labelled corpusis split in two halves: one half is used to obtain theinitial seed examples from among the time expres-sions found therein; the other half is used for eval-uation.
We are requiring that a pattern captures thetarget time expression mention exactly (no misalign-ment allowed at the boundaries), in order to count itas a precision or recall hit.We will also be interested in measuring the gainin recall, that is, the difference between the recallin the best iteration and the initial recall given bythe seeds.
Also important is the number of iter-ations after which the bootstrapping process con-verges.
In the case where the same F1- measuremark is achieved in two experimental settings, ear-lier convergence of the algorithm will be prefered.Otherwise, better F1 and gain in recall are the pri-mary goals.In order to start with a set of seeds with high pre-cision, we select them automatically, imposing thata seed time expression must have precision above acertain value (understood as the percentage, of allthe appearances of the sequence of tokens in the su-pervised corpus, those in which it is tagged as a cor-rect time expression).
In the experiments presentedbelow, this threshold for precision of the seeds is90% ?in the half of the supervised corpus reservedfor extraction of seeds?.
From those that pass thisfilter, the ones that appear with greater frequency areselected.
For time expressions that have an identi-cal digit pattern (e.g.
two dates ?
@@ December?or two years ?
@@@@?, where @ stands for anydigit), only one seed is taken.
This approach sim-ulates the human domain expert, which typically isthe first step in bootstrapping IE models54Unless specifically stated otherwise, all the exper-iments presented below share the following defaultsettings:?
Only the first 2.36 Mbytes of the unsupervisedcorpus are used (10 Mbytes after tokenizationand feature extraction), that is 0.5% of theavailable data.
This is to keep the executiontime of experiments low, where multiple exper-iments need to be run to optimize a certain pa-rameter.?
We use the Collins and Singer strategy (seesection 4.1) with a precision threshold of 0.50for sub-score combination in pattern selection.This strategy favours patterns with slightlyhigher precision.?
The maximum length of prefix and postfix is 1and 0 elements, respectively.
This was deter-mined experimentally.?
100 seed examples are used (out of a maximumof 605 available).?
In the ranking of examples, the ?i weights forthe three sub- scores for infixes are 0.5 forthe ?similarity-based score?, 0.25 for ?phrase-completeness?
and 0.25 for ?context-basedscore?.?
In the selection of examples, the maximumnumber of new infixes accepted per iteration is200, with a maximum of 50 different contextsper infix.
In the selection of patterns, the max-imum number of new accepted patterns per it-eration is 5000 (although this number is neverreached due to the analysis of subsumptions).?
In the selection of patterns, multisets are usedfor computing the instance set of a patternfor the frequency-based score and normal setsfor the precision score (determined experimen-tally).?
The POS tag type of generalization (pattern el-ement) has been deactivated, that is, neither all-POS patterns, nor patterns that are combina-tions of POS PEs with another are generated.After an analysis of errors, it was observed thatPOS generalizations (because of the fact thatthey are not lexicalized like, for instance, thesyntactic PEs with a given headword) give riseto a considerable number of precision errors.?
All patterns are generated with COMPLETE-PHRASE modifier automatically attached.
Itwas determined experimentally that it was bestto use this heuristic in all cases (see section 3).5.2 Variation of the number of seedsWe have performed experiments using 1, 5, 10, 20,50, 100, 200 and 500 seeds.
The general trends ob-served were as follows.
The final precision (whenthe bootstrapping converges) decreases more or lessmonotonically as the number of seeds increases, al-though there are slight fluctuations; besides, the dif-ference in this respect between using few seeds (20to 50) or more (100 to 200) is of only around 3%.However, a big leap can be observed in moving from200 to 500 seeds, where both the initial precision(of the seeds) and final precision (at point of con-vergence) drop by 10% wrt to using 200 seeds.
Thefinal recall increases monotonically as the numberof seeds increases?since more supervised informa-tion is provided?.
The final F1-measure first in-creases and then decreases with an increasing num-ber of seeds, with an optimum value being reachedsomewhere between the 50 and 100 seeds.The largest gain in recall (difference between re-call of the seeds and recall at the point of con-vergence) is achieved with 20 seeds, for a gainof 16.38% (initial recall is 20.08% and final is36.46%).
The best mark in F1-measure is achievedwith 100 seeds, after 6 iterations: 60.43% (the finalprecision is 69.29% and the final recall is 53.58%;the drop in precision is 6.5% and the gain in recall is14.28%).
Figure 3 shows a line plot of precision vsrecall for these experiments.
This experiment sug-gests that the problem of temporal expression recog-nition can be captured with minimal supervised in-formation (100 seeds) and larger amounts of unsu-pervised information.Figure 3: Effect of varying the number of seeds555.3 Variation of the type of generalizationsused in patternsIn these experiments, we have defined four differ-ents sets of generalizations (i.e.
types of pattern ele-ments among those specified in section 3) to evalu-ate how semantic and syntactic generalizations con-tribute to performance of the algorithm.
These fourexperiments are labelled as follows: NONE includesonly PEs of the LEMMA type; SYN includes PEsof the lemma type and of the not-generalized syn-tactic chunk (SYN) type; SEM includes PEs of thelemma type and of the semantic class (SEMCLASS)type, as well as combinations of lemma with SEM-CLASS PEs; and lastly, SYN+SEM includes every-thing that both SYN and SEM experiments include,plus PEs of the generalized syntactic chunk (SYN-SEMC) type.One can observe than neither type of generaliza-tion, syntactic or semantic, is specially ?effective?when used in isolation (only a 3.5% gain in recall inboth cases).
It is only the combination of both typesthat gives a good gain in recall (14.28% in the caseof this experiment).
Figure 4 shows a line plot of thisexperiment.
The figure indicates that the problem oftemporal expression recognition, even though appar-ently simple, requires both syntactic and semanticinformation for efficient modeling.Figure 4: Effect of using syntactic and/or semantic gen-eralizations5.4 Variation of the size of unsupervised datausedWe performed experiments using increasingamounts of unsupervised data for training in thebootstrapping: 1, 5, 10, 50 and 100 Mbytes ofpreprocessed corpus (tokenized and with featureextraction).
The amounts of plain text data areroughly a fifth part, respectively.
The objectiveof these experiments is to determine whetherperformance improves as the amount of trainingdata is increased.
The number of seeds passed tothe bootstrapping is 68.
The maximum number ofnew infixes (the part of an example that contains acandidate time expression) accepted per iterationhas been increased from 200 to 1000, because itwas observed that larger amounts of unsupervisedtraining data need a greater number of selection?slots?
in order to render an improvement (that is, amore ?reckless?
bootstrapping), otherwise they willfill up all the allowed selection slots.The observed effect is that both the drop in preci-sion (from the initial iteration to the point of conver-gence) and the gain in recall improve more or lessconsistently as a larger amount of training data istaken, or otherwise the same recall point is achievedin an earlier iteration.
These improvements are nev-ertheless slight, in the order of between 0.5% and2%.
The biggest improvement is observed in the 100Mbytes experiment, where recall after 5 iterations is6% better than in the 50 Mbytes experiment after 7iterations.
The drop in precision in the 100 Mbytesexperiment is 13.05%, for a gain in recall of 21.36%(final precision is 71.02%, final recall 52.84% andfinal F1 60.59%).
Figure 5 shows a line plot of thisexperiment.
This experiment indicates that increas-ing amounts of unsupervised data can be used to im-prove the performance of our model, but the task isnot trivial.Figure 5: Effect of varying the amount of unsupervisedtraining data6 Conclusions and future researchWe have presented a slightly supervised algorithmfor the extraction of IE patterns for the recognition56of time expressions, based on bootstrapping, whichintroduces a novel representation of patterns suitedto this task.
Our experiments show that with a rel-atively small amount of supervision (50 to 100 ini-tial correct examples or seeds) and using a combina-tion of syntactic and semantic generalizations, it ispossible to obtain an improvement of around 15%-20% in recall (with regard to the seeds) and F1-measure over 60% learning exclusively from unla-belled data.
Furthermore, using increasing amountsof unlabelled training data (of which there is plentyavailable) is a workable way to obtain small im-provements in performance, at the expense of train-ing time.
Our current focus is on addressing specificproblems that appear on inspection of the precisionerrors in test, which can improve both precision andrecall to a degree.
Future planned lines of researchinclude using WordNet for improving the semanticaspects of the algorithm (semantic classes and simi-larity), and studying forms of combining the patternsobtained in this semi-supervised approach with su-pervised learning.ReferencesM.
Collins and Y.
Singer.
1999.
Unsupervised mod-els for named entity classification.
In Proceedings ofthe Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very Large Cor-pora, pages 100?110, College Park, MD.
ACL.L.
Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-son.
2005.
Tides 2005 standard for the annotation oftemporal expressions.
Technical report, MITRE Cor-poration.E.
Filatova and E. Hovy.
2001.
Assigning time-stamps toevent-clauses.
In Proceedings of the 2001 ACL Work-shop on Temporal and Spatial Information Processing,pages 88?95.K.
Hacioglu, Y. Chen, and B. Douglas.
2005.
Automatictime expression labelling for english and chinese text.In Proc.
of the 6th International Conference on Intel-ligent Text Processing and Computational Linguistics(CICLing), pages 548?559.
Springer.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Seman-tic class learning from the web with hyponym patternlinkage graphs.
In Proc.
of the Association for Com-putational Linguistics 2008 (ACL-2008:HLT), pages1048?1056.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of the 17th InternationalConference on Computational Linguistics and the 36thAnnual Meeting of the Association for ComputationalLinguistics (COLING-ACL-98), pages 768?774, Mon-treal, Quebec.
ACL.I.
Mani and G. Wilson.
2000.
Robust temporal process-ing of news.
In Proceedings of the 38th Annual Meet-ing of the Association for Computational Linguistics,pages 69?76, Morristown, NJ, USA.
ACL.M.
Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Names and similarities on the web: Fact extrac-tion in the fast lane.
In Proceedings of the 21th In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the ACL, pages 809?816.ACL.E.
Riloff.
1996.
Automatically generating extraction pat-terns from untagged text.
In Proceedings of the Thir-teenth National Conference on Artificial Intelligence(AAAI-96), pages 1044?1049.
AAAI/MIT Press.E.
Saquete, R. Mun?oz, and P.
Mart??nez-Barco.
2004.Event ordering using terseo system.
In Proc.
of the9th International Conference on Application of Natu-ral Language to Information Systems (NLDB), pages39?50.
Springer.M.
Stevenson and M. Greenwood.
2005.
A semanticapproach to IE pattern induction.
In Proceedings ofthe 43rd Meeting of the Association for ComputationalLinguistics, pages 379?386.
ACL.M.
Surdeanu, J. Turmo, and A. Ageno.
2006.
A hybridapproach for the acquisition of information extractionpatterns.
In Proceedings of the EACL 2006 Workshopon Adaptive Text Extraction and Mining (ATEM 2006).ACL.J.
M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, andK.
J. McKeever.
1998.
An empirical approach to tem-poral reference resolution.
Journal of Artificial Intelli-gence Research, 9:247?293.R.
Yangarber, R. Grishman, P. Tapanainen, andS.
Hutunen.
2000.
Automatic acquisition of domainknowledge for information extraction.
In Proceedingsof the 18th International Conference of ComputationalLinguistics, pages 940?946.R.
Yangarber.
2003.
Counter-training in discovery ofsemantic patterns.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics.
ACL.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA.
ACL.57
