What?s There to Talk About?A Multi-Modal Model of Referring Behaviorin the Presence of Shared Visual InformationDarren GergleHuman-Computer Interaction InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburg, PA  USAdgergle+cs.cmu.eduAbstractThis paper describes the development ofa rule-based computational model thatdescribes how a feature-based representa-tion of shared visual information com-bines with linguistic cues to enable effec-tive reference resolution.
This work ex-plores a language-only model, a visual-only model, and an integrated model ofreference resolution and applies them to acorpus of transcribed task-oriented spo-ken dialogues.
Preliminary results from acorpus-based analysis suggest that inte-grating information from a shared visualenvironment can improve the perform-ance and quality of existing discourse-based models of reference resolution.1 IntroductionIn this paper, we present work in progress to-wards the development of a rule-based computa-tional model to describe how various forms ofshared visual information combine with linguis-tic cues to enable effective reference resolutionduring task-oriented collaboration.A number of recent studies have demonstratedthat linguistic patterns shift depending on thespeaker?s situational context.
Patterns of prox-imity markers (e.g., this/here vs. that/there)change according to whether speakers perceivethemselves to be physically co-present or remotefrom their partner (Byron & Stoia, 2005; Fussellet al, 2004; Levelt, 1989).
The use of particularforms of definite referring expressions (e.g., per-sonal pronouns vs. demonstrative pronouns vs.demonstrative descriptions) varies depending onthe local visual context in which they are con-structed (Byron et al, 2005a).
And people arefound to use shorter and syntactically simplerlanguage (Oviatt, 1997) and different surfacerealizations (Cassell & Stone, 2000) when ges-tures accompany their spoken language.More specifically, work examining dialoguepatterns in collaborative environments has dem-onstrated that pairs adapt their linguistic patternsbased on what they believe their partner can see(Brennan, 2005; Clark & Krych, 2004; Gergle etal., 2004; Kraut et al, 2003).
For example, whena speaker knows their partner can see their ac-tions but will incur a small delay before doing so,they increase the proportion of full NPs used(Gergle et al, 2004).
Similar work by Byron andcolleagues (2005b) demonstrates that the formsof referring expressions vary according to a part-ner?s proximity to visual objects of interest.Together this work suggests that the interlocu-tors?
shared visual context has a major impact ontheir patterns of referring behavior.
Yet, a num-ber of discourse-based models of reference pri-marily rely on linguistic information without re-gard to the surrounding visual environment (e.g.,see Brennan et al, 1987; Hobbs, 1978; Poesio etal., 2004; Strube, 1998; Tetreault, 2005).
Re-cently, multi-modal models have emerged thatintegrate visual information into the resolutionprocess.
However, many of these models are re-stricted by their simplifying assumption of com-munication via a command language.
Thus, theirapproaches apply to explicit interaction tech-niques but do not necessarily support more gen-eral communication in the presence of sharedvisual information (e.g., see Chai et al, 2005;Huls et al, 1995; Kehler, 2000).It is the goal of the work presented in this pa-per to explore the performance of language-based models of reference resolution in contextswhere speakers share a common visual space.
Inparticular, we examine three basic hypotheses7regarding the likely impact of linguistic and vis-ual salience on referring behavior.
The first hy-pothesis suggests that visual information is dis-regarded and that linguistic context provides suf-ficient information to describe referring behav-ior.
The second hypothesis suggests that visualsalience overrides any linguistic salience in gov-erning referring behavior.
Finally, the third hy-pothesis posits that a balance of linguistic andvisual salience is needed in order to account forpatterns of referring behavior.In the remainder of this paper, we begin bypresenting a brief discussion of the motivationfor this work.
We then describe three computa-tional models of referring behavior used to ex-plore the hypotheses described above, and thecorpus on which they have been evaluated.
Weconclude by presenting preliminary results anddiscussing future modeling plans.2 MotivationThere are several motivating factors for develop-ing a computational model of referring behaviorin shared visual contexts.
First, a model of refer-ring behavior that integrates a component ofshared visual information can be used to increasethe robustness of interactive agents that conversewith humans in real-world situated environ-ments.
Second, such a model can be applied tothe development of a range of technologies tosupport distributed group collaboration and me-diated communication.
Finally, such a model canbe used to provide a deeper theoretical under-standing of how humans make use of variousforms of shared visual information in their every-day communication.The development of an integrated multi-modalmodel of referring behavior can improve the per-formance of state-of-the-art computational mod-els of communication currently used to supportconversational interactions with an intelligentagent (Allen et al, 2005; Devault et al, 2005;Gorniak & Roy, 2004).
Many of these modelsrely on discourse state and prior linguistic con-tributions to successfully resolve references in agiven utterance.
However, recent technologicaladvances have created opportunities for human-human and human-agent interactions in a widevariety of contexts that include visual objects ofinterest.
Such systems may benefit from a data-driven model of how collaborative pairs adapttheir language in the presence (or absence) ofshared visual information.
A successful computa-tional model of referring behavior in the pres-ence of visual information could enable agents toemulate many elements of more natural and real-istic human conversational behavior.A computational model may also make valu-able contributions to research in the area of com-puter-mediated communication.
Video-mediatedcommunication systems, shared media spaces,and collaborative virtual environments are tech-nologies developed to support joint activitiesbetween geographically distributed groups.However, the visual information provided ineach of these technologies can vary drastically.The shared field of view can vary, views may bemisaligned between speaking partners, and de-lays of the sort generated by network congestionmay unintentionally disrupt critical informationrequired for successful communication (Brennan,2005; Gergle et al, 2004).
Our proposed modelcould be used along with a detailed task analysisto inform the design and development of suchtechnologies.
For instance, the model could in-form designers about the times when particularvisual elements need to be made more salient inorder to support effective communication.
Acomputational model that can account for visualsalience and understand its impact on conversa-tional coherence could inform the construction ofshared displays or dynamically restructure theenvironment as the discourse unfolds.A final motivation for this work is to furtherour theoretical understanding of the role sharedvisual information plays during communication.A number of behavioral studies have demon-strated the need for a more detailed theoreticalunderstanding of human referring behavior in thepresence of shared visual information.
They sug-gest that shared visual information of the taskobjects and surrounding workspace can signifi-cantly impact collaborative task performance andcommunication efficiency in task-oriented inter-actions (Kraut et al, 2003; Monk & Watts, 2000;Nardi et al, 1993; Whittaker, 2003).
For exam-ple, viewing a partner?s actions facilitates moni-toring of comprehension and enables efficientobject reference (Daly-Jones et al, 1998), chang-ing the amount of available visual informationimpacts information gathering and recovery fromambiguous help requests (Karsenty, 1999), andvarying the field of view that a remote helper hasof a co-worker?s environment influences per-formance and shapes communication patterns indirected physical tasks (Fussell et al, 2003).Having a computational description of theseprocesses can provide insight into why they oc-cur, can expose implicit and possibly inadequatesimplifying assumptions underlying existing8theoretical models, and can serve as a guide forfuture empirical research.3 Background and Related WorkA review of the computational linguistics lit-erature reveals a number of discourse modelsthat describe referring behaviors in written, andto a lesser extent, spoken discourse (for a recentreview see Tetreault, 2005).
These include mod-els based primarily on world knowledge (e.g.,Hobbs et al, 1993), syntax-based methods(Hobbs, 1978), and those that integrate a combi-nation of syntax, semantics and discourse struc-ture (e.g., Grosz et al, 1995; Strube, 1998;Tetreault, 2001).
The majority of these modelsare salience-based approaches where entities areranked according to their grammatical function,number of prior mentions, prosodic markers, etc.In typical language-based models of referenceresolution, the licensed referents are introducedthrough utterances in the prior linguistic context.Consider the following example drawn from thePUZZLE CORPUS1 whereby a ?Helper?
describes toa ?Worker?
how to construct an arrangement ofcolored blocks so they match a solution only theHelper has visual access to:(1)  Helper: Take the dark red piece.Helper: Overlap it over the orange halfway.In excerpt (1), the first utterance uses the defi-nite-NP ?the dark red piece,?
to introduce a newdiscourse entity.
This phrase specifies an actualpuzzle piece that has a color attribute of dark redand that the Helper wants the Worker to positionin their workspace.
Assuming the Worker hascorrectly heard the utterance, the Helper can nowexpect that entity to be a shared element as estab-lished by prior linguistic context.
As such, thispiece can subsequently be referred to using apronoun.
In this case, most models correctly li-cense the observed behavior as the Helper speci-fies the piece using ?it?
in the second utterance.3.1 A Drawback to Language-Only ModelsHowever, as described in Section 2, several be-havioral studies of task-oriented collaborationhave suggested that visual context plays a criticalrole in determining which objects are salientparts of a conversation.
The following examplefrom the same PUZZLE CORPUS?in this case froma task condition in which the pairs share a visualspace?demonstrates that it is not only the lin-guistic context that determines the potential ante-1The details of the PUZZLE CORPUS are described in ?.4.cedents for a pronoun, but also the physical con-text as well:(2)  Helper: Alright, take the dark orange block.Worker: OK.Worker: [ moved an incorrect piece ]Helper: Oh, that?s not it.In excerpt (2), both the linguistic and visualinformation provide entities that could be co-specified by a subsequent referent.
In this ex-cerpt, the first pronoun ?that,?
refers to the ?
[in-correct piece]?
that was physically moved intothe shared visual workspace but was not previ-ously mentioned.
While the second pronoun,?it,?
has as its antecedent the object co-specifiedby the definite-NP ?the dark orange block.?
Thisexample demonstrates that during task-orientedcollaborations both the linguistic and visual con-texts play central roles in enabling the conversa-tional pairs to make efficient use of communica-tion tactics such as pronominalization.3.2 Towards an Integrated ModelWhile most computational models of referenceresolution accurately resolve the pronoun in ex-cerpt (1), many fail at resolving one or more ofthe pronouns in excerpt (2).
In this rather trivialcase, if no method is available to generate poten-tial discourse entities from the shared visual en-vironment, then the model cannot correctly re-solve pronouns that have those objects as theirantecedents.This problem is compounded in real-worldand computer-mediated environments since thevisual information can take many forms.
For in-stance, pairs of interlocutors may have differentperspectives which result in different objects be-ing occluded for the speaker and for the listener.In geographically distributed collaborations aconversational partner may only see a subset ofthe visual space due to a limited field of viewprovided by a camera.
Similarly, the speed of thevisual update may be slowed by network conges-tion.Byron and colleagues recently performed apreliminary investigation of the role of sharedvisual information in a task-oriented, human-to-human collaborative virtual environment (Byronet al, 2005b).
They compared the results of alanguage-only model with a visual-only model,and developed a visual salience algorithm to rankthe visual objects according to recency, exposuretime, and visual uniqueness.
In a hand-processedevaluation, they found that a visual-only modelaccounted for 31.3% of the referring expressions,and that adding semantic restrictions (e.g., ?open9that?
could only match objects that could beopened, such as a door) increased performance to52.2%.
These values can be compared with alanguage-only model with semantic constraintsthat accounted for 58.2% of the referring expres-sions.While Byron?s visual-only model uses seman-tic selection restrictions to limit the number ofvisible entities that can be referenced, her modeldiffers from the work reported here in that it doesnot make simultaneous use of linguistic salienceinformation based on the discourse content.
So,for example, referring expressions cannot be re-solved to entities that have been mentioned butwhich are not visible.
Furthermore, all otherthings equal, it will not correctly resolve refer-ences to objects that are most salient based onthe linguistic context over the visual context.Therefore, in addition to language-only and vis-ual-only models, we explore the development ofan integrated model that uses both linguistic andvisual salience to support reference resolution.We also extend these models to a new task do-main that can elaborate on referential patterns inthe presence of various forms of shared visualinformation.
Finally, we make use of a corpusgathered from laboratory studies that allow us todecompose the various features of shared visualinformation in order to better understand theirindependent effects on referring behaviors.The following section provides an overview ofthe task paradigm used to collect the data for ourcorpus evaluation.
We describe the basic ex-perimental paradigm and detail how it can beused to examine the impact of various features ofa shared visual space on communication.4 The Puzzle Task CorpusThe corpus data used for the development of themodels in this paper come from a subset of datacollected over the past few years using a referen-tial communication task called the puzzle study(Gergle et al, 2004).In this task, pairs of participants are randomlyassigned to play the role of ?Helper?
or?Worker.?
It is the goal of the task for the Helperto successfully describe a configuration of piecesto the Worker, and for the Worker to correctlyarrange the pieces in their workspace.
The puzzlesolutions, which are only provided to the Helper,consist of four blocks selected from a larger setof eight.
The goal is to have the Worker correctlyplace the four solution pieces in the proper con-figuration as quickly as possible so that theymatch the target solution the Helper is viewing.Each participant was seated in a separate roomin front of a computer with a 21-inch display.The pairs communicated over a high-quality,full-duplex audio link with no delay.
The ex-perimental displays for the Worker and Helperare illustrated in Figure 1.Figure 1.
The Worker?s view (left) and theHelper?s view (right).The Worker?s screen (left) consists of a stag-ing area on the right hand side where the puzzlepieces are held, and a work area on the left handside where the puzzle is constructed.
TheHelper?s screen (right) shows the target solutionon the right, and a view of the Worker?s workarea in the left hand panel.
The advantage of thissetup is that it allows exploration of a number ofdifferent arrangements of the shared visualspace.
For instance, we have varied the propor-tion of the workspace that is visually shared withthe Helper in order to examine the impact of alimited field-of-view.
We have offset the spatialalignment between the two displays to simulatesettings of various video systems.
And we haveadded delays to the speed with which the Helperreceives visual feedback of the Worker?s actionsin order to simulate network congestion.Together, the data collected using the puzzleparadigm currently contains 64,430 words in theform of 10,640 contributions collected from over100 different pairs.
Preliminary estimates suggestthat these data include a rich collection of over5,500 referring expressions that were generatedacross a wide range of visual settings.
In this pa-per, we examine a small portion of the data inorder to assess the feasibility and potential con-tribution of the corpus for model development.4.1 Preliminary Corpus OverviewThe data collected using this paradigm includesan audio capture of the spoken conversation sur-rounding the task, written transcriptions of thespoken utterances, and a time-stamped record ofall the piece movements and their representativestate in the shared workspace (e.g., whether theyare visible to both the Helper and Worker).
From10these various streams of data we can parse andextract the units for inclusion in our models.For initial model development, we focus onmodeling two primary conditions from the PUZ-ZLE CORPUS.
The first is the ?No Shared VisualInformation?
condition where the Helper couldnot see the Worker?s workspace at all.
In thiscondition, the pair needs to successfully com-plete the tasks using only linguistic information.The second is the ?Shared Visual Information?condition, where the Helper receives immediatevisual feedback about the state of the Worker?swork area.
In this case, the pairs can make use ofboth linguistic information and shared visual in-formation in order to successfully complete thetask.As Table 1 demonstrates, we use a small ran-dom selection of data consisting of 10 dialoguesfrom each of the Shared Visual Information andNo Shared Visual Information conditions.
Eachof these dialogues was collected from a uniqueparticipant pair.
For this evaluation, we focusedprimarily on pronoun usage since this has beensuggested to be one of the major linguistic effi-ciencies gained when pairs have access to ashared visual space (Kraut et al, 2003).TaskConditionCorpusStatisticsDialogues Contri-butionsWords Pro-nounsNo SharedVisualInformation10 218 1181 30SharedVisualInformation10 174 938 39Total 20 392 2119 69Table 1.
Overview of the data used.5 Preliminary Model OverviewsThe models evaluated in this paper are basedon Centering Theory (Grosz et al, 1995; Grosz& Sidner, 1986) and the algorithms devised byBrennan and colleagues (1987) and adapted byTetreault (2001).
We examine a language-onlymodel based on Tetreault?s Left-Right Centering(LRC) model, a visual-only model that uses ameasure of visual salience to rank the objects inthe visual field as possible referential anchors,and an integrated model that balances the visualinformation along with the linguistic informationto generate a ranked list of possible anchors.5.1 The Language-Only ModelWe chose the LRC algorithm (Tetreault, 2001) toserve as the basis for our language-only model.
Ithas been shown to fare well on task-oriented spo-ken dialogues (Tetreault, 2005) and was easilyadapted to the PUZZLE CORPUS data.LRC uses grammatical function as a centralmechanism for resolving the antecedents of ana-phoric references.
It resolves referents by firstsearching in a left-to-right fashion within the cur-rent utterance for possible antecedents.
It thenmakes co-specification links when it finds anantecedent that adheres to the selectional restric-tions based on verb argument structure andagreement in terms of number and gender.
If amatch is not found the algorithm then searchesthe lists of possible antecedents in prior utter-ances in a similar fashion.The primary structure employed in the lan-guage-only model is a ranked entity list sorted bylinguistic salience.
To conserve space we do notreproduce the LRC algorithm in this paper andinstead refer readers to Tetreault?s original for-mulation (2001).
We determined order based onthe following precedence ranking:Subject  Direct Object  Indirect ObjectAny remaining ties (e.g., an utterance with twodirect objects) were resolved according to a left-to-right breadth-first traversal of the parse tree.5.2 The Visual-Only ModelAs the Worker moves pieces into their work-space, depending on whether or not the work-space is shared with the Helper, the objects be-come available for the Helper to see.
The visual-only model utilized an approach based on visualsalience.
This method captures the relevant vis-ual objects in the puzzle task and ranks them ac-cording to the recency with which they were ac-tive (as described below).Given the highly controlled visual environ-ment that makes up the PUZZLE CORPUS, we havecomplete access to the visual pieces and exacttiming information about when they becomevisible, are moved, or are removed from theshared workspace.
In the visual-only model, wemaintain an ordered list of entities that comprisethe shared visual space.
The entities are includedin the list if they are currently visible to both theHelper and Worker, and then ranked according tothe recency of their activation.22This allows for objects to be dynamically rearranged de-pending on when they were last ?touched?
by the Worker.115.3 The Integrated ModelWe used the salience list generated from the lan-guage-only model and integrated it with the onefrom the visual-only model.
The method of or-dering the integrated list resulted from generalperceptual psychology principles that suggestthat highly active visual objects attract an indi-vidual?s attentional processes (Scholl, 2001).In this preliminary implementation, we de-fined active objects as those objects that had re-cently moved within the shared workspace.These objects are added to the top of the linguis-tic-salience list which essentially rendered themas the focus of the joint activity.
However, peo-ple?s attention to static objects has a tendency tofade away over time.
Following prior work thatdemonstrated the utility of a visual decay func-tion (Byron et al, 2005b; Huls et al, 1995), weimplemented a three second threshold on thelifespan of a visual entity.
From the time sincethe object was last active, it remained on the listfor three seconds.
After the time expired, the ob-ject was removed and the list returned to its priorstate.
This mechanism was intended to capturethe notion that active objects are at the center ofshared attention in a collaborative task for a shortperiod of time.
After that the interlocutors revertto their recent linguistic history for the context ofan interaction.It should be noted that this is work in progressand a major avenue for future work is the devel-opment of a more theoretically grounded methodfor integrating linguistic salience informationwith visual salience information.5.4 Evaluation PlanTogether, the models described above allow us totest three basic hypotheses regarding the likelyimpact of linguistic and visual salience:Purely linguistic context.
One hypothesis isthat the visual information is completely disre-garded and the entities are salient purely basedon linguistic information.
While our prior workhas suggested this should not be the case, severalexisting computational models function only atthis level.Purely visual context.
A second possibility isthat the visual information completely overrideslinguistic salience.
Thus, visual informationdominates the discourse structure when it isavailable and relegates linguistic information to asubordinate role.
This too should be unlikelygiven the fact that not all discourse deals withexternal elements from the surrounding world.A balance of syntactic and visual context.
Athird hypothesis is that both linguistic entitiesand visual entities are required in order to accu-rately and perspicuously account for patterns ofobserved referring behavior.
Salient discourseentities result from some balance of linguisticsalience and visual salience.6 Preliminary ResultsIn order to investigate the hypotheses describedabove, we examined the performance of themodels using hand-processed evaluations of thePUZZLE CORPUS data.
The following presents theresults of the three different models on 10 trialsof the PUZZLE CORPUS in which the pairs had noshared visual space, and 10 trials from when thepairs had access to shared visual information rep-resenting the workspace.
Two experts performedqualitative coding of the referential anchors foreach pronoun in the corpus with an overallagreement of 88% (the remaining anomalieswere resolved after discussion).As demonstrated in Table 2, the language-onlymodel correctly resolved 70% of the referringexpressions when applied to the set of dialogueswhere only language could be used to solve thetask (i.e., the no shared visual information condi-tion).
However, when the same model was ap-plied to the dialogues from the task conditionswhere shared visual information was available, itonly resolved 41% of the referring expressionscorrectly.
This difference was significant, 2(1,N=69) = 5.72, p = .02.No Shared VisualInformationShared VisualInformationLanguageModel70.0%   (21 / 30) 41.0%   (16 / 39)VisualModeln/a 66.7%  (26 / 39)IntegratedModel70.0%  (21 / 30) 69.2%  (27 / 39)Table 2.
Results for all pronouns in the subsetof the PUZZLE CORPUS evaluated.In contrast, when the visual-only model wasapplied to the same data derived from the taskconditions in which the shared visual informationwas available, the algorithm correctly resolved66.7% of the referring expressions.
In compari-son to the 41% produced by the language-onlymodel.
This difference was also significant, 2(1,N=78) = 5.16, p = .02.
However, we did not findevidence of a difference between the perform-ance of the visual-only model on the visual taskconditions and the language-only model on the12language task conditions, 2(1, N=69) = .087, p =.77 (n.s.
).The integrated model with the decay functionalso performed reasonably well.
When the inte-grated model was evaluated on the data whereonly language could be used it effectively revertsback to a language-only model, therefore achiev-ing the same 70% performance.
Yet, when it wasapplied to the data from the cases when the pairshad access to the shared visual information itcorrectly resolved 69.2% of the referring expres-sions.
This was also better than the 41% exhib-ited by the language-only model, 2(1, N=78) =6.27, p = .012; however, it did not statisticallyoutperform the visual-only model on the samedata, 2(1, N=78) = .059, p = .81 (n.s.
).In general, we found that the language-onlymodel performed reasonably well on the dia-logues in which the pairs had no access to sharedvisual information.
However, when the samemodel was applied to the dialogues collectedfrom task conditions where the pairs had accessto shared visual information the performance ofthe language-only model was significantly re-duced.
However, both the visual-only model andthe integrated model significantly increased per-formance.
The goal of our current work is to finda better integrated model that can achieve sig-nificantly better performance than the visual-only model.
As a starting point for this investiga-tion, we present an error analysis below.6.1 Error AnalysisIn order to inform further development of themodel, we examined a number of failure caseswith the existing data.
The first thing to note wasthat a number of the pronouns used by the pairsreferred to larger visible structures in the work-space.
For example, the Worker would some-times state, ?like this?
?, and ask the Helper tocomment on the overall configuration of the puz-zle.
Table 3 presents the performance results ofthe models after removing all expressions thatdid not refer to pieces of the puzzle.No Shared VisualInformationShared VisualInformationLanguageModel77.7%  (21 / 27) 47.0%  (16 / 34)VisualModeln/a 76.4%  (26 / 34)IntegratedModel77.7%  (21 / 27) 79.4%  (27 / 34)Table 3.
Model performance results when re-stricted to piece referents.In the errors that remained, the language-onlymodel had a tendency to suffer from a number ofhigher-order referents such as events and actions.In addition, there were several errors that re-sulted from chaining errors where the initial ref-erent was misidentified.
As a result, all subse-quent chains of referents were incorrect.The visual-only model and the integratedmodel had a tendency to suffer from timing is-sues.
For instance, the pairs occasionally intro-duced a new visual entity with, ?this one??
How-ever, the piece did not appear in the workspaceuntil a short time after the utterance was made.In such cases, the object was not available as areferent on the object list.
In the future we planto investigate the temporal alignment betweenthe visual and linguistic streams.In other cases, problems simply resulted fromthe unique behaviors present when exploringhuman activities.
Take the following example,(3) Helper: There is an orange red that obscureshalf of it and it is to the left of itIn this excerpt, all of our models had troublecorrectly resolving the pronouns in the utterance.However, while this counts as a strike against themodel performance, the model actually presenteda true account of human behavior.
While themodel was confused, so was the Worker.
In thiscase, it took three more contributions from theHelper to unravel what was actually intended.7 Future WorkIn the future, we plan to extend this work inseveral ways.
First, we plan future studies to helpexpand our notion of visual salience.
Each of thevisual entities has an associated number of do-main-dependent features.
For example, they mayhave appearance features that contribute to over-all salience, become activated multiple times in ashort window of time, or be more or less salientdepending on nearby visual objects.
We intend toexplore these parameters in detail.Second, we plan to appreciably enhance theintegrated model.
It appears from both our initialdata analysis, as well as our qualitative examina-tion of the data, that the pairs make tradeoffs be-tween relying on the linguistic context and thevisual context.
Our current instantiation of theintegrated model could be enhanced by taking amore theoretical approach to integrating the in-formation from multiple streams.Finally, we plan to perform a large-scale com-putational evaluation of the entire PUZZLE CORPUSin order to examine a much wider range of visual13features such as limited field-of-views, delays inproviding the shared visual information, andvarious asymmetries in the interlocutors?
visualinformation.
In addition to this we plan to extendour model to a wider range of task domains inorder to explore the generality of its predictions.AcknowledgmentsThis research was funded in by an IBM Ph.D.Fellowship.
I would like to thank Carolyn Ros?and Bob Kraut for their support.ReferencesAllen, J., Ferguson, G., Swift, M., Stent, A., Stoness, S.,Galescu, L., et al (2005).
Two diverse systems built usinggeneric components for spoken dialogue.
In Proceedingsof Association for Computational Linguistics, CompanionVol., pp.
85-88.Brennan, S. E. (2005).
How conversation is shaped by vis-ual and spoken evidence.
In J. C. Trueswell & M. K. Ta-nenhaus (Eds.
), Approaches to studying world-situatedlanguage use: Bridging the language-as-product and lan-guage-as-action traditions (pp.
95-129).
Cambridge, MA:MIT Press.Brennan, S. E., Friedman, M. W., & Pollard, C. J.
(1987).
Acentering approach to pronouns.
In Proceedings of 25thAnnual Meeting of the Association for Computational Lin-guistics, pp.
155-162.Byron, D. K., Dalwani, A., Gerritsen, R., Keck, M., Mam-pilly, T., Sharma, V., et al (2005a).
Natural noun phrasevariation for interactive characters.
In Proceedings of 1stAnnual Artificial Intelligence and Interactive Digital En-tertainment Conference, pp.
15-20.
AAAI.Byron, D. K., Mampilly, T., Sharma, V., & Xu, T. (2005b).Utilizing visual attention for cross-modal coreference in-terpretation.
In Proceedings of Fifth International and In-terdisciplinary Conference on Modeling and Using Con-text (CONTEXT-05), pp.Byron, D. K., & Stoia, L. (2005).
An analysis of proximitymarkers in collaborative dialog.
In Proceedings of 41st an-nual meeting of the Chicago Linguistic Society, pp.
Chi-cago Linguistic Society.Cassell, J., & Stone, M. (2000).
Coordination and context-dependence in the generation of embodied conversation.
InProceedings of International Natural Language Genera-tion Conference, pp.
171-178.Chai, J. Y., Prasov, Z., Blaim, J., & Jin, R. (2005).
Linguis-tic theories in efficient multimodal reference resolution:An empirical investigation.
In Proceedings of IntelligentUser Interfaces, pp.
43-50.
NY: ACM Press.Clark, H. H., & Krych, M. A.
(2004).
Speaking while moni-toring addressees for understanding.
Journal of Memory &Language, 50(1), 62-81.Daly-Jones, O., Monk, A., & Watts, L. (1998).
Some advan-tages of video conferencing over high-quality audio con-ferencing: Fluency and awareness of attentional focus.
In-ternational Journal of Human-Computer Studies, 49, 21-58.Devault, D., Kariaeva, N., Kothari, A., Oved, I., & Stone,M.
(2005).
An information-state approach to collaborativereference.
In Proceedings of Association for Computa-tional Linguistics, Companion Vol., pp.Fussell, S. R., Setlock, L. D., & Kraut, R. E. (2003).
Effectsof head-mounted and scene-oriented video systems on re-mote collaboration on physical tasks.
In Proceedings ofHuman Factors in Computing Systems (CHI '03), pp.
513-520.
ACM Press.Fussell, S. R., Setlock, L. D., Yang, J., Ou, J., Mauer, E. M.,& Kramer, A.
(2004).
Gestures over video streams to sup-port remote collaboration on physical tasks.
Human-Computer Interaction, 19, 273-309.Gergle, D., Kraut, R. E., & Fussell, S. R. (2004).
Languageefficiency and visual technology: Minimizing collabora-tive effort with visual information.
Journal of Language &Social Psychology, 23(4), 491-517.Gorniak, P., & Roy, D. (2004).
Grounded semantic compo-sition for visual scenes.
Journal of Artificial IntelligenceResearch, 21, 429-470.Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995).
Center-ing: A framework for modeling the local coherence of dis-course.
Computational Linguistics, 21(2), 203-225.Grosz, B. J., & Sidner, C. L. (1986).
Attention, intentionsand the structure of discourse.
Computational Linguistics,12(3), 175-204.Hobbs, J. R. (1978).
Resolving pronoun references.
Lingua,44, 311-338.Hobbs, J. R., Stickel, M. E., Appelt, D. E., & Martin, P.(1993).
Interpretation as abduction.
Artificial Intelligence,63, 69-142.Huls, C., Bos, E., & Claassen, W. (1995).
Automatic refer-ent resolution of deictic and anaphoric expressions.
Com-putational Linguistics, 21(1), 59-79.Karsenty, L. (1999).
Cooperative work and shared context:An empirical study of comprehension problems in side byside and remote help dialogues.
Human-Computer Interac-tion, 14(3), 283-315.Kehler, A.
(2000).
Cognitive status and form of reference inmultimodal human-computer interaction.
In Proceedingsof American Association for Artificial Intelligence (AAAI2000), pp.
685-689.Kraut, R. E., Fussell, S. R., & Siegel, J.
(2003).
Visual in-formation as a conversational resource in collaborativephysical tasks.
Human Computer Interaction, 18, 13-49.Levelt, W. J. M. (1989).
Speaking: From intention to articu-lation.
Cambridge, MA: MIT Press.Monk, A., & Watts, L. (2000).
Peripheral participation invideo-mediated communication.
International Journal ofHuman-Computer Studies, 52(5), 933-958.Nardi, B., Schwartz, H., Kuchinsky, A., Leichner, R.,Whittaker, S., & Sclabassi, R. T. (1993).
Turning awayfrom talking heads: The use of video-as-data in neurosur-gery.
In Proceedings of Interchi '93, pp.
327-334.Oviatt, S. L. (1997).
Multimodal interactive maps: Design-ing for human performance.
Human-Computer Interaction,12, 93-129.Poesio, M., Stevenson, R., Di Eugenio, B., & Hitzeman, J.(2004).
Centering: A parametric theory and its instantia-tions.
Computational Linguistics, 30(3), 309-363.Scholl, B. J.
(2001).
Objects and attention: the state of theart.
Cognition, 80, 1-46.Strube, M. (1998).
Never look back: An alternative to cen-tering.
In Proceedings of 36th Annual Meeting of the Asso-ciation for Computational Linguistics, pp.
1251-1257.Tetreault, J. R. (2001).
A corpus-based evaluation of center-ing and pronoun resolution.
Computational Linguistics,27(4), 507-520.Tetreault, J. R. (2005).
Empirical evaluations of pronounresolution.
Unpublished doctoral thesis, University ofRochester, Rochester, NY.Whittaker, S. (2003).
Things to talk about when talkingabout things.
Human-Computer Interaction, 18, 149-170.14
