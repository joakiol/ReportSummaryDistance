Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 450?454, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsASVUniOfLeipzig: Sentiment Analysis in Twitter using Data-drivenMachine Learning TechniquesRobert RemusNatural Language Processing Group,Department of Computer Science,University of Leipzig, Germanyrremus@informatik.uni-leipzig.deAbstractThis paper describes University of Leipzig?sapproach to SemEval-2013 task 2B on Sen-timent Analysis in Twitter: message polar-ity classification.
Our system is designed tofunction as a baseline, to see what we canaccomplish with well-understood and purelydata-driven lexical features, simple general-izations as well as standard machine learningtechniques: We use one-against-one SupportVector Machines with asymmetric cost fac-tors and linear ?kernels?
as classifiers, worduni- and bigrams as features and additionallymodel negation of word uni- and bigrams inword n-gram feature space.
We consider gen-eralizations of URLs, user names, hash tags,repeated characters and expressions of laugh-ter.
Our method ranks 23 out of all 48 partic-ipating systems, achieving an averaged (pos-itive, negative) F-Score of 0.5456 and an av-eraged (positive, negative, neutral) F-Score of0.595, which is above median and average.1 IntroductionIn SemEval-2013?s task 2B on Sentiment Analysisin Twitter, given a Twitter message, i.e.
a tweet, thegoal is to classify whether this tweet is of positive,negative, or neutral polarity (Wilson et al 2013),i.e.
the task is a ternary polarity classification.Due to Twitter?s growing popularity, the availabil-ity of large amounts of data that go along with thatand the fact, that many people freely express theiropinion on virtually everything using Twitter, re-search on sentiment analysis in Twitter has receiveda lot of attention lately (Go et al 2009; Pak andParoubek, 2010).
Language is usually used casu-ally in Twitter and exhibits interesting properties.Therefore, some studies specifically address certainissues, e.g.
a tweet?s length limitation of 140 char-acters, some studies leverage certain language char-acteristics, e.g.
the presence of emoticons etc.Davidov et al(2010) identify various ?sentimenttypes?
defined by Twitter hash tags (e.g.
#bored)and smileys (e.g.
:S) using words, word n-grams,punctuation marks and patterns as features.
Bar-bosa and Feng (2010) map words to more generalrepresentations, i.e.
part of speech (POS) tags andthe words?
prior subjectivity and polarity.
Addi-tionally, they count the number of re-tweets, hashtags, replies, links etc.
They then combine the out-puts of 3 online sources of labeled but noisy and bi-ased Twitter data into a more robust classificationmodel.
Saif et al(2012) also address data sparsityvia word clustering methods, i.e.
semantic smooth-ing and sentiment-topics extraction.
Agarwal et al(2011) contrast a word unigram model, a tree ker-nel model and a model of various features, e.g.
POStag counts, summed up prior polarity scores, pres-ence or absence of capitalized text, all applied to bi-nary and ternary polarity classification.
Kouloumpiset al(2011) show that Twitter-specific feature engi-neering, e.g.
representing the presence or absenceof abbreviations and character repetitions improvesmodel quality.
Jiang et al(2011) focus on target-dependent polarity classification regarding a givenuser query.While various models and features have been pro-posed, word n-gram models proved to be competi-tive in many studies (Barbosa and Feng, 2010; Agar-450wal et al 2011; Saif et al 2012) yet are straight-forward to implement.
Moreover, word n-grammodels do not rely on hand-crafted and generally{genre, domain}-non-specific resources, e.g.
priorpolarity dictionaries like SentiWordNet (Esuli andSebastiani, 2006) or Subjectivity Lexicon (Wiebe etal., 2005).
In contrast, purely data-driven word n-gram models are domain-specific per se: they ?letthe data speak for themselves?.
Therefore we be-lieve that carefully designing such a baseline usingwell-understood and purely data-driven lexical fea-tures, simple generalizations as well as standard ma-chine learning techniques is a worthwhile endeavor.In the next Section we describe our system.
InSection 3 we discuss its results in SemEval-2013task 2B and finally conclude in Section 4.2 System DescriptionWe approach the ternary polarity classification viaone-against-one (Hsu and Lin, 2002) Support VectorMachines (SVMs) (Vapnik, 1995; Cortes and Vap-nik, 1995) using a linear ?kernel?
as implementedby LibSVM1.
To deal with the imbalanced class dis-tribution of positive (+), negative (?)
and neutral-or-objective (0) instances, we use asymmetric costfactors C+, C?, C0 that allow for penalizing falsepositives and false negatives differently inside theone-against-one SVMs.
While the majority class?C0 is set to 1.0, the minority classes?
C{+,?
}s areset as shown in (1)C{+,?}
=#(0-class instances)#({+,?
}-class instances)(1)similar to Morik et al(1999)?s suggestion.2.1 DataTo develop our system, we use all training data avail-able to us for training and all development data avail-able to us for testing, after removing 75 duplicatesfrom the training data and 2 duplicates from thedevelopment data.
Please note that 936 tweets ofthe originally provided training data and 3 tweets ofthe originally provided development data were not1http://www.csie.ntu.edu.tw/?cjlin/libsvm/available at our download time2.
Table 1 summa-rizes the used data?s class distribution after duplicateremoval.Data + ?
0 ?Training 3,263 1,278 4,132 8,673Development 384 197 472 1,053?
3,647 1,475 4,604 9,726Table 1: Class distribution of positive (+), negative (?
)and neutral-or-objective (0) instances in training and de-velopment data after duplicate removal.For sentence segmentation and tokenization of thedata we use OpenNLP3.
An example tweet of theprovided training data is shown in (1):(1) #nacamam @naca you have to try Sky-walk Deli on the 2nd floor of the Com-erica building on Monroe!
#bestlunchehttp://instagr.am/p/Rfv-RfTI-3/.2.2 Model SelectionTo select an appropriate model, we experiment withdifferent feature sets (cf.
Section 2.2.1) and differentcombinations of generalizations (cf.
Section 2.2.2).2.2.1 FeaturesWe consider the following feature sets:a. word unigramsb.
word unigrams plus negation modeling forword unigramsc.
word uni- and bigramsd.
word uni- and bigrams plus negation modelingfor word unigramse.
word uni- and bigrams plus negation modelingfor word uni- and bigramsWord uni- and bigrams are induced data-driven, i.e.directly extracted from the textual data.
We performno feature selection; neither stop words nor punc-tuation marks are removed.
We simply encode thepresence or absence of word n-grams.2Training data was downloaded on February 21, 2013, 9:18a.m.
and development data was downloaded on February 28,2013, 10:41 a.m. using the original download script.3http://opennlp.apache.org451Whether a word uni- or bigram is negated, i.e.appears inside of a negation scope (Wiegand et al2010), is detected by LingScope4 (Agarwal and Yu,2010), a state-of-the-art negation scope detectionbased on Conditional Random Fields (Lafferty etal., 2001).
We model the negation of word n-gramsin an augmented word n-gram feature space as de-tailedly described in Remus (2013): In this featurespace, each word n-gram is either represented aspresent ([1, 0]), absent ([0, 0]), present inside a nega-tion scope ([0, 1]) and present both inside and out-side a negation scope ([1, 1]).We trained a model for each feature set and chosethe one that yields the highest accuracy: word uni-and bigrams plus negation modeling for word uni-and bigrams.2.2.2 GeneralizationsTo account for Twitter?s typical language char-acteristics, we consider all possible combinationsof generalizations of the following character se-quences, inspired by (Montejo-Ra?ez et al 2012):a.
User names, that mark so-called mentions in aTweet, expressed by @username.b.
Hash tags, that mark keywords or topics in aTweet, expressed by #keyword.c.
URLs, that mark links to other web pages.d.
Twitpic URLs, that mark links to pictureshosted by twitpic.com.e.
Repeated Characters, e.g.
woooow.
We col-lapse characters re-occuring more than twice,e.g.
woooow is replaced by woow.f.
Expressions of laughter, e.g.
hahaha.
Wegeneralize derivatives of the ?base forms?haha, hehe, hihi and huhu.
A derivativemust contain the base form and may addition-ally contain arbitrary uppercased and lower-cased letters at its beginning and its end.
Wecollapse these derivatives.
E.g., hahahah andHAHAhaha and hahaaa are all replaced bytheir base form haha, eheheh and heheHEare all replaced by hehe etc.4http://sourceforge.net/projects/lingscope/User names, hash tags, URLs and Twitpic URLs aregeneralized by either simply removing them (modeI) or by replacing them with a single unique token(mode II), i.e.
by forming an equivalence class.
Re-peated characters and expressions of laughter aregeneralized by collapsing them as described above.There are 1 +?6k=1(6k)= 64 possible combina-tions of generalizations including no generalizationat all.
We trained a word uni- and bigram plus nega-tion modeling for word uni- and bigrams model (cf.Section 2.2.1) for each combination and both modeI and mode II and chose the one that yields the high-est accuracy: Generalization of URLs (mode I), re-peated characters and expressions of laughter.Although it may appear counterintuitive not togeneralize hash tags and user names, the trainingdata contains several re-occuring hash tags, that ac-tually convey sentiment, e.g.
#love, #cantwait,#excited.
Similarly, the training data con-tains several re-occuring mentions of ?celebrities?,that may hint at sentiment which is usually as-sociated with them, e.g.
@justinbieber or@MittRomney.3 Results & DiscussionTo train our final system, we use all available train-ing and development data (cf.
Table 1).
The SVM?s?base?
cost factor C is optimized via 10-fold crossvalidation, where in each fold 9/10th of the availabledata are used for training, the remaining 1/10th is usedfor testing.
C values are chosen from {2 ?
10?3, 2 ?10?2, 2 ?
10?1, 2 ?
100, 2 ?
101, 2 ?
102, 2 ?
103}.
In-ternally, the asymmetric cost factors C+, C?, C0 (cf.Section 2) are then set to C{+,?,0} := C ?
C{+,?,0}.The final system is then applied to both Twit-ter and SMS test data (cf.
Table 2).
Please noteTest Data + ?
0 ?Twitter 1,572 601 1,640 3,813SMS 492 394 1,208 2,094Table 2: Class distribution of positive (+), negative (?
)and neutral-or-objective (0) instances in Twitter and SMStesting data.that we only participate in the constrained setting ofSemEval-2013 task 2B (Wilson et al 2013) as wedid not use any additional training data.452Detailed evaluation results on Twitter test dataare shown in Table 3, results on SMS test data areshown in Table 4.
The ranks we achieved in the con-strained only-ranking and the full constrained andunconstrained-ranking are shown in Table 5.Class P R F+ 0.7307 0.5833 0.6487?
0.5795 0.3577 0.44240 0.6072 0.8098 0.6940+,?
0.6551 0.4705 0.5456+,?, 0 0.6391 0.5836 0.5950Table 3: Precision P , Recall R and F-Score F of Univer-sity of Leipzig?s approach to SemEval-2013 task 2B onTwitter test data distinguished by classes (+, ?, 0) andaverages of +,?
and +,?, 0.Class P R F+ 0.5161 0.5854 0.5486?
0.5174 0.3020 0.38140 0.7289 0.7881 0.7574+,?
0.5168 0.4437 0.4650+,?, 0 0.5875 0.5585 0.5625Table 4: Precision P , Recall R and F-Score F of Uni-versity of Leipzig?s approach to SemEval-2013 task 2Bon SMS test data distinguished by classes (+, ?, 0) andaverages of +,?
and +,?, 0.Test data Constr.
Un/constr.Twitter 18 of 35 23 of 48SMS 20 of 28 31 of 42Table 5: Ranks of University of Leipzig?s approach toSemEval-2013 task 2B on Twitter and SMS test data inthe constrained only (Constr.)
and the constrained andunconstrained setting (Un/constr.
).On Twitter test data our system achieved an av-eraged (+,?)
F-Score of 0.5456, which is abovethe average (0.5382) and above the median (0.5444).Our system ranks 23 out of 48 participating systemsin the full constrained and unconstrained-ranking.Averaging over over +,?, 0 it yields an F-Score of0.595.On SMS test data our system performs quitepoorly compared to other participating systems as (i)we did not adapt our model to the SMS data at all,e.g.
we did not consider more appropriate or othergeneralizations, and (ii) its class distribution is quitedifferent from our training data (cf.
Table 1 vs. 2).Our system achieved an averaged (+,?)
F-Score of0.465, which is below the average (0.5008) and be-low the median (0.5060).
Our system ranks 31 out of42 participating systems in the full constrained andunconstrained-ranking.
Averaging over over +,?, 0it yields an F-Score of 0.5625.4 ConclusionWe described University of Leipzig?s contributionto SemEval-2013 task 2B on Sentiment Analysis inTwitter.
We approached the message polarity classi-fication via well-understood and purely data-drivenlexical features, negation modeling, simple general-izations as well as standard machine learning tech-niques.
Despite being designed as a baseline, oursystem ranks midfield on both Twitter and SMS testdata.As even the state-of-the-art system achieves(+,?)
averaged F-Scores of 0.6902 and 0.6846on Twitter and SMS test data, respectively, polar-ity classification of tweets and short messages stillproves to be a difficult task that is far from beingsolved.
Future enhancements of our system includethe use of more data-driven features, e.g.
featuresthat model the distribution of abbreviations, punctu-ation marks or capitalized text as well as fine-tuningour generalization mechanism, e.g.
by (i) general-izing only low-frequency hash tags and usernames,but not generalizing high-frequency ones, (ii) gener-alizing acronyms that express laughter, such as lol(?laughing out loud?)
or rofl (?rolling on the floorlaughing?).ReferencesS.
Agarwal and H. Yu.
2010.
Biomedical negationscope detection with conditional random fields.
Jour-nal of the American Medical Informatics Association,17(6):696?701.A.
Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-sonneau.
2011.
Sentiment analysis of twitter data.
InProceedings of the Workshop on Languages in SocialMedia (LSM, pages 30?38.L.
Barbosa and J. Feng.
2010.
Robust sentiment detec-tion on twitter from biased and noisy data.
In Proceed-453ings of the 23rd International Conference on Compu-tational Linguistics (COLING), pages 36?44.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine Learning, 20(3):273?297.D.
Davidov, O. Tsur, and A. Rappoport.
2010.
Enhancedsentiment learning using twitter hashtags and smileys.In Proceedings of the 23rd International Conferenceon Computational Linguistics (COLING), pages 241?249.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: A pub-licly available lexical resource for opinion mining.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC), pages417?422.A.
Go, R. Bhayani, and L. Huang.
2009.
Twitter senti-ment classification using distant supervision.
CS224Nproject report, Stanford University.C.
Hsu and C. Lin.
2002.
A comparison of methodsfor multiclass support vector machines.
IEEE Trans-actions on Neural Networks, 13(2):415?425.L.
Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao.
2011.Target-dependent twitter sentiment classification.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages151?160.E.
Kouloumpis, T. Wilson, and J. Moore.
2011.
Twittersentiment analysis: The good the bad and the OMG.In Proceedings of the 5th International Conference onWeblogs and Social Media (ICWSM), pages 538?541.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe 18th International Conference on Machine Learn-ing (ICML), pages 282?289.A.
Montejo-Ra?ez, E. Mart?nez-Ca?mara, M.T.
Mart?n-Valdivia, and L.A. Urena-Lo?pez.
2012.
Random walkweighting over sentiwordnet for sentiment polarity de-tection on twitter.
In Proceedings of the 3rd Workshopon Computational Approaches to Subjectivity and Sen-timent Analysis (WASSA), pages 3?10.K.
Morik, P. Brockhausen, and T. Joachims.
1999.
Com-bining statistical learning with a knowledge-based ap-proach ?
a case study in intensive care monitoring.
InProceedings of the 16th International Conference onMachine Learning (ICML), pages 268?277.A.
Pak and P. Paroubek.
2010.
Twitter as a corpus forsentiment analysis and opinion mining.
In Proceed-ings of the 7th International Conference on LanguageResources and Evaluation (LREC).R.
Remus.
2013.
Negation modeling in machinelearning-based sentiment analysis.
In forthcoming.H.
Saif, Y.
He, and H. Alani.
2012.
Alleviating datasparsity for twitter sentiment analysis.
In Proceedingsof the 2nd Workshop on Making Sense of Microposts(#MSM).V.
Vapnik.
1995.
The Nature of Statistical Learning.Springer New York, NY.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 1(2):165?210.M.
Wiegand, A. Balahur, B. Roth, D. Klakow, andA.
Montoyo.
2010.
A survey on the role of negation insentiment analysis.
In Proceedings of the 2010 Work-shop on Negation and Speculation in Natural Lan-guage Processing (NeSp-NLP), pages 60?68.T.
Wilson, Z. Kozareva, P. Nakov, A. Ritter, S. Rosenthal,and V. Stoyanov.
2013.
SemEval-2013 task 2: Sen-timent analysis in twitter.
In Proceedings of the 7thInternational Workshop on Semantic Evaluation (Se-mEval).454
