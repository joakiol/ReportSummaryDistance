Proceedings of the 7th Workshop on Statistical Machine Translation, pages 382?387,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDFKI?s SMT System for WMT 2012David VilarGerman Research Center for Artificial Intelligence (DFKI GmbH)Language Technology LabBerlin, Germanydavid.vilar@dfki.deAbstractWe describe DFKI?s statistical based submis-sion to the 2012 WMT evaluation.
The sub-mission is based on the freely available ma-chine translation toolkit Jane, which supportsphrase-based and hierarchical phrase-basedtranslation models.
Different setups have beentested and combined using a sentence selec-tion method.1 IntroductionIn this paper we present DFKI?s submission forthe 2012 MT shared task based on statistical ap-proaches.
We use a variety of phrase-based and hi-erarchical phrase-based translation systems with dif-ferent configurations and enhancements and com-pare their performance.
The output of the systemsare later combined using a sentence selection mech-anism.
Somewhat disappointingly the sentence se-lection hardly improves over the best single system.DFKI participated in the German to English andEnglish to German translation tasks.
Technicalproblems however hindered a more complete systemfor this last translation direction.This paper is organized as follows: Section 2 re-ports on the different single systems that we built forthis shared task.
Section 3 describes the sentence se-lection mechanism used for combining the output ofthe different systems.
Section 4 concludes the paper.2 Single SystemsFor all our setups we used the Jane toolkit (Vi-lar et al, 2010a), which in its current version sup-ports both phrase-based and hierarchical phrase-based translation models.
In this Section we presentthe different settings that we used for the task.The bilingual training data used for training allsystems was the combination of the provided Eu-roparl and News data.
We also used two baseline 4-gram language models trained on the same Europarltraining data and on the enhanced News Commen-tary monolingual training data.
The newstest2010dataset was used for optimization of the systems.2.1 Phrase-based SystemThe first system is a baseline phrase-based systemtrained on the available bilingual training data.
Wordalignments is trained using GIZA++ (Och and Ney,2003), phrase extraction is performed with Jane us-ing standard settings, i.e.
maximum source phraselength 6, maximum target phrase length 12, countfeatures, etc.
Consult the Jane documentation formore details.
For reordering the standard distance-based reordering model is computed.
Scaling factorsare trained using MERT on n-best lists.2.1.1 Verb reorderingsFollowing (Popovic?
and Ney, 2006), for Germanto English translation, we perform verb reorderingby first POS-tagging the source sentence and after-wards applying hand-defined rules.
This includesrules for reordering verbs in subordinate clauses andparticiples.2.1.2 Moore LMMoore and Lewis (2010) propose a method forfiltering large quantities of out-of-domain language-model training data by comparing the cross-entropy382of an in-domain language model and an out-of-domain language model trained on a random sam-pling of the data.
We followed this approach to filterthe news-crawl corpora provided the organizers.
Byexperimenting on the development set we decidedto use a 4-gram language model trained on 15M fil-tered sentences (the original data comprising over30M sentences).2.2 Hierarchical SystemWe also trained a hierarchical system on the samedata as the phrase-based system, and also tried theadditional language model trained according to Sec-tion 2.1.2, as well as the verb reorderings describedin Section 2.1.1.2.2.1 Poor Man?s SyntaxVilar et al (2010b) propose a ?syntax-based?
ap-proach similar to (Venugopal et al, 2009), but us-ing automatic clustering methods instead of linguis-tic parsing for defining the non-terminals used in theresulting grammar.
The main idea of the method isto cluster the words (mimicking the concept of Part-of-Speech tagging), performing a phrase extractionpass using the word classes instead of the actualwords and performing another clustering on thephrase level (corresponding to the linguistic classesin a parse tree).2.2.2 Lightly-Supervised TrainingHuck et al (2011) propose to augment the mono-lingual training data by translating available addi-tional monolingual data with an existing translationsystem.
We adapt this approach by translating thedata selected according to Section 2.1.2 with thephrase-based translation system described in Sec-tion 2.1, and use this additional data to expand thebilingual data available for training the hierarchicalphrase-based system.12.3 Experimental ResultsTable 1 shows the results obtained for the Germanto English translation direction on the newstest2011dataset.
The baseline phrase-based system obtains a1The decision of which system to use to produce the addi-tional training material follows mainly a practical reason.
Asthe hierarchical model is more costly to train and at decodingtime, we chose the phrase-based system as the generating sys-tem.BLEU score of 18.2%.
The verb reorderings achievean improvement of 0.6% BLEU, and adding the ad-ditional language model obtains an additional 1.6%BLEU improvement.The hierarchical system baseline achieves a bet-ter BLEU score than the baseline PBT system, andis comparable to the PBT system with additional re-orderings.
In fact, adding the verb reorderings tothe hierarchical system slightly degrades its perfor-mance.
This indicates that the hierarchical model isable to reflect the verb reorderings necessary for thistranslation direction.
Adding the bigger languagemodel of Section 2.1.2 also obtains a nice improve-ment of 1.4% BLEU for this system.
On the otherhand and somewhat disappointingly, the lightly su-pervised training and the poor man?s syntax ap-proach are not able to improve translation quality.For the English to German translation directionwe encountered some technical problems, and wewere not able to perform as many experiments as forthe opposite direction.
The results are shown in Ta-ble 2 and show similar trends as for the German toEnglish direction, except that the hierarchical sys-tem in this case does not outperform the PBT base-line.3 Sentence SelectionIn this section we will describe the system combi-nation method based on sentence selection that weused for combining the output of the systems de-scribed in Section 2.
This approach was tried suc-cessfully in (Vilar et al, 2011).We use a log-linear model for computing thescores of the different translation hypotheses, gen-erated by all the systems described in Section 2, i.e.those listed in Tables 1 and 2.
The model scalingfactors are computed using a standard MERT runon the newstest2011 dataset, optimizing for BLEU.This is comparable to the usual approach used forrescoring n-best lists generated by a single system,and has been used previously for sentence selectionpurposes (see (Hildebrand and Vogel, 2008) whichuses a very similar approach to our own).
Note thatno system dependent features like translation prob-abilities were computed, as we wanted to keep thesystem general.We will list the features we compute for each of383System BLEU[%]PBT Baseline 18.2PBT + Reordering 18.8PBT + Reordering + Moore LM 20.4Hierarchical Baseline 18.7Hierarchical + Moore LM 20.1Hierarchical + Moore LM + Lightly Supervised 19.8Poor Man?s Syntax 18.6Hierarchical + Reordering 18.5Table 1: Translation results for the different single systems, German to English.System BLEU[%]PBT Baseline 12.4Hierarchical Baseline 11.6Hierarchical + Moore LM 13.1Poor Man?s Syntax 11.6Table 2: Translation results for the different single systems, English to Germanthe systems.
We have used features that try to focuson characteristics that humans may use to evaluate asystem.3.1 Cross System BLEUBLEU was introduced in (Papineni et al, 2002)and it has been shown to have a high correlationwith human judgement.
In spite of its shortcom-ings (Callison-Burch et al, 2006), it has been con-sidered the standard automatic measure in the devel-opment of SMT systems (with new measures beingadded to it, but not substituting it, see for e.g.
(Ceret al, 2010)).Of course, the main problem of using the BLEUscore as a feature for sentence selection in a real-life scenario is that we do not have the referencesavailable.
We overcame this issue by generatinga custom set of references for each system, usingthe other systems as gold translations.
This is ofcourse inexact, but n-grams that appear on the out-put of different systems can be expected to be moreprobable to be correct, and BLEU calculated thisway gives us a measure of this agreement.
This ap-proach can be considered related to n-gram poste-riors (Zens and Ney, 2006) or minimum Bayes riskdecoding (e.g.
(Ehling et al, 2007)) in the context ofn-best rescoring, but applied without prior weight-ing (unavailable directly) and more focused on theevaluation interpretation.We generated two features based on this idea.The first one is computed at the system level, i.e.
itis the same for each sentence produced by a sys-tem and serves as a kind of prior weight similarto the one used in other system combination meth-ods (e.g.
(Matusov et al, 2008)).
The other featurewas computed at the sentence level.
For this we usedthe smoothed version of BLEU proposed in (Lin andOch, 2004), again using the output of the rest ofthe systems as pseudo-reference.
As optimizationon BLEU often tends to generate short translations,we also include a word penalty feature.3.2 Error Analysis FeaturesIt is safe to assume that a human judge will tryto choose those translations which contain the leastamount of errors, both in terms of content and gram-maticality.
A classification of errors for machinetranslation systems has been proposed in (Vilar etal., 2006), and (Popovic?
and Ney, 2011) presentshow to compute a subset of these error categories au-tomatically.
The basic idea is to extend the familiarWord Error Rate (WER) and Position independent384word Error Rate (PER) measures on word and base-form2 levels to identify the different kind of errors.For our system we included following features:Extra Word Errors (EXTer) Extra words in thehypothesis not present in the references.Inflection Errors (hINFer) Words with wrong in-flection.
Computed comparing word-level er-rors and base-form-level errors.Lexical Errors (hLEXer) Wrong lexical choicesin the hypothesis with respect to the references.Reordering Errors (hRer) Wrong word order inthe hypothesis.Missing Words (MISer) Words present in the ref-erence that are missing in the hypothesis.All these features are computed using the opensource Hjerson3 tool (Popovic?, 2011), which alsooutputs the standard WER metric, which we addedas an additional feature.As was the case in Section 3.1, for computingthese measures we do not have a reference available,and thus we use the rest of the systems as pseudo-references.
This has the interesting effect that some?errors?
are actually beneficial for the performanceof the system.
For example, it is known that sys-tems optimised on the BLEU metric tend to produceshort hypotheses.
In this sense, the extra words con-sidered as errors by the EXTer measure may be ac-tually beneficial for the overall performance of thesystem.3.3 IBM1 ScoresIBM1-like scores on the sentence level are known toperform well for the rescoring of n-best lists froma single system (see e.g.
(Hasan et al, 2007)).
Ad-ditionally, they have been shown in (Popovic et al,2011) to correlate well with human judgement forevaluation purposes.
We thus include them as addi-tional features.2Computed using the TreeTagger tool (http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/)3The abbreviations for the features are taken over directlyfrom the output of the tool.De-En En-DeBest System 20.4 13.1Worst System 18.2 11.6Sentence Selection 20.9 13.3Table 3: Sentence selection results3.4 Additional Language ModelWe used a 5-gram language model trained on thewhole news-crawl corpus as an additional model forrescoring.
We used a different language model as theone described in Section 2.1.2 as not to favor thosesystems that already included it at decoding time.3.5 Experimental ResultsThe sentence selection improved a little bit over thebest single system for German to English transla-tion, but hardly so for English to German, as shownin Table 3.
For English to German this can be due tothe small amount of systems that were available forthe sentence selection system.
Note also that theseresults are measured on the same corpus the systemwas trained on, so we expect the improvement onunseen test data to be even smaller.
Nevertheless thesentence selection system constituted our final sub-mission for the MT task.4 ConclusionsFor this year?s evaluation DFKI used a statisticalsystem based around the Jane machine translationtoolkit (Vilar et al, 2010a), working in its twomodalities: phrase-based and hierarchical phrase-based models.
Different enhancements were triedin addition to the baseline configuration: POS-basedverb reordering, monolingual data selection, poorman?s syntax and lightly supervised training, withmixed results.A sentence selection mechanism has later beenapplied in order to combine the output of the dif-ferent configurations.
Although encouraging resultshad been obtained in (Vilar et al, 2011), for this taskwe found only a small improvement.
This may bedue to the strong similarity of the systems, as theyare basically trained on the same data.
In (Vilar etal., 2011) the training data was varied across the sys-tems, which may have produced a bigger variety in385the translation outputs that can be of advantage forthe selection mechanism.
This is an issue that shouldbe explored in more detail for further work.We also plan to do a comparison with systemcombination approaches where new hypotheses canbe generated (instead of selecting one from a pre-defined set), and study under which conditions eachapproach is more suited than the other.5 AcknowledgementsThis work was done with the support of theTaraXU?
Project4, financed by TSB Technologies-tiftung Berlin-Zukunftsfonds Berlin, co-financed bythe European Union-European fund for regional de-velopment.ReferencesChris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of Bleu inMachine Translation Research.
In Proc.
of the 11thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 249?256,Trento, Italy, April.Daniel Cer, Christopher D. Manning, and Daniel Juraf-sky.
2010.
The best lexical metric for phrase-basedstatistical mt system optimization.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 555?563,Los Angeles, CA, USA.Nicola Ehling, Richard Zens, and Hermann Ney.
2007.Minimum Bayes risk decoding for BLEU.
In AnnualMeeting of the Assoc.
for Computational Linguistics,pages 101?104, Prague, Czech Republic, June.Sas?a Hasan, Richard Zens, and Hermann Ney.
2007.
Arevery large N-best lists useful for SMT?
In HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics AnnualMeeting, pages 57?60, Rochester, NY, April.
Associa-tion for Computational Linguistics.A.S.
Hildebrand and S. Vogel.
2008.
Combination ofmachine translation systems via hypothesis selectionfrom combined n-best lists.
In MT at work: Proc.
ofthe Eighth Conference of the Association for MachineTranslation in the Americas, pages 254?261.Matthias Huck, David Vilar, Daniel Stein, and HermannNey.
2011.
Lightly-Supervised Training for Hier-archical Phrase-Based Machine Translation.
In The4http://taraxu.dfki.deEMNLP 2011 Workshop on Unsupervised Learning inNLP, Edinburgh, UK, July.Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE:a Method for Evaluating Automatic Evaluation Met-rics for Machine Translation.
In Proc.
of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Geneva, Switzerland.Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-erico, Muntsin Kolss, Young-Suk Lee, Jose B. Marino,Matthias Paulik, Salim Roukos, Holger Schwenk,and Hermann Ney.
2008.
System combination formachine translation of spoken and written language.IEEE Transactions on Audio, Speech and LanguageProcessing, 16(7):1222?1237, September.R.C.
Moore and W. Lewis.
2010.
Intelligent selection oflanguage model training data.
In Proceedings of theACL 2010 Conference Short Papers, pages 220?224.Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.Maja Popovic?
and Hermann Ney.
2006.
POS-based wordreorderings for statistical machine translation.
In In-ternational Conference on Language Resources andEvaluation, pages 1278?1283, Genoa, Italy, May.Maja Popovic?
and Hermann Ney.
2011.
Towards Au-tomatic Error Analysis of Machine Translation Out-put.
Computational Linguistics, 37(4):657?688, De-cember.Maja Popovic, David Vilar, Eleftherios Avramidis, andAljoscha Burchardt.
2011.
Evaluation without ref-erences: Ibm1 scores as evaluation metrics.
In Proc.of the Sixth Workshop on Statistical Machine Trans-lation, pages 99?103.
Association for ComputationalLinguistics, July.Maja Popovic?.
2011.
Hjerson: An Open Source Toolfor Automatic Error Classification of Machine Trans-lation Output.
The Prague Bulletin of MathematicalLinguistics, pages 59?68.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference Grammars:Softening Syntactic Constraints to Improve Statisti-cal Machine Translation.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 236?244, Boulder,Colorado, USA, June.386David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Machine Transla-tion Output.
In International Conference on LanguageResources and Evaluation, pages 697?702, Genoa,Italy, May.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010a.
Jane: Open Source Hierarchical Transla-tion, Extended with Reordering and Lexicon Models.In Proc.
of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, pages 262?270,Uppsala, Sweden, July.David Vilar, Daniel Stein, Stephan Peitz, and HermannNey.
2010b.
If I Only Had a Parser: Poor Man?sSyntax for Hierarchical Machine Translation.
In Inter-national Workshop on Spoken Language Translation,pages 345?352, Paris, France, December.David Vilar, Eleftherios Avramidis, Maja Popovic?, andSabine Hunsicker.
2011.
Dfki?s sc and mt submis-sions to iwslt 2011.
In International Workshop on Spo-ken Language Translation, San Francisco, CA, USA,December.R.
Zens and H. Ney.
2006.
N-gram Posterior Proba-bilities for Statistical Machine Translation.
In HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics AnnualMeeting (HLT-NAACL), Workshop on Statistical Ma-chine Translation, pages 72?77, New York City, June.387
