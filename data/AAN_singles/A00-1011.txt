REES: A Large-Scale Relation and Event Extraction SystemChinatsu AoneSRA International, Inc.4300 Fair Lakes CourtFairfax, VA 22033aonec@verdi.sra.comMila Ramos-SantacruzSRA International, Inc.4300 Fair Lakes CourtFairfax, VA 22033mila@verdi.sra.comAbstractThis paper reports on a large-scale, end-to-end relation and event extraction system.
Atpresent, the system extracts a total of 100types of relations and events, whichrepresents a much wider coverage than istypical of extraction systems.
The systemconsists of three specialized pattem-basedtagging modules, a high-precision co-reference resolution module, and aconfigurable template generation module.We report quantitative valuation results,analyze the results in detail, and discussfuture directions.IntroductionOne major goal of information extraction (IE)technology is to help users quickly identify avariety of relations and events and their keyplayers in a large volume of documents.
Incontrast with this goal, state-of-the-artinformation extraction systems, as shown in thevarious Message Understanding Conferences(MUCs), extract a small number of relations andevents.
For instance, the most recent MUC,MUC-7, called for the extraction of 3 relations(person-employer, maker-product, andorganization-location) and 1 event (spacecraftlaunches).
Our goal is to develop an IE systemwhich scales up to extract as many types ofrelations and events as possible with a minimumamount of porting effort combined with highaccuracy.
Currently, REES handles 100 types ofrelations and events, and it does so in a modular,configurable, and scalable manner.Below, Section 1 presents the ontologies ofrelations and events that we have developed.Section 2 describes REES' system architecture.Section 3 evaluates the system's performance,and offers a qualitative analysis of system errors.Section 4 discusses future directions.1 Relation and Event OntologiesAs the first step in building a large-scale relationand event extraction system, we developedontologies of the relations and events to beextracted.
These ontologies represent a widevariety of domains: political, financial, business,military, and life-related events and relations.
"Relations" covers what in MUC-7 are calledTemplate Elements (TEs) and TemplateRelations (TRs).
There are 39 types of relations.While MUC TE's only dealt with singularentities, REES extracts both singular and pluralentities (e.g., "five executives").
The TRrelations are shown in italic in the table below.RelationsPlace Relations 'Artifact RelationsPlace-Name&AliasesPlace-TypePlace-SubtypePlace-DescriptorPlace-CountryArtifact-Name&AliasesArtifact-TypeArtifact-SubtypeArtifact-DescriptorArtifact-MakerArtifact-OwnerOrganization Relations Person RelationsOrg-Name&AliasesOrg-DescriptorOrg-FoundationDateOrg-NationalityOrg-TickerSymbolOrg-LocationOrg-P arentOrgOrg-OwnerOrg-FounderOrg-StockMarketPerson-Name&AliasesPerson-TypePerson-SubtypePerson-DescriptorPerson-HonorificPerson-AgePerson-PhoneNumberPerson-NationalityPerson-AffiliationPerson-SiblingPerson-SpousePerson-ParentPerson-Grandparent76Person-OtherRelativePerson-BirthPlacePerson-BirthDateTable 1: Relation Ontology"Events" are extracted along with their eventparticipants, e.g., "who did what to whom whenand where?"
For example, for a BUYINGevent, REES extracts the buyer, the artifact, theseller, and the time and location of the BUYINGevent.
REES currently covers 61 types ofevents, as shown below.EventsVehicle TransactionVehicle departsVehicle arrivesSpacecraft launchVehicle crashPersonnel ChangeHireTerminate contractPromoteSucceedStart officeBuy artifactSell artifactImport artifactExport artifactGive moneyBusinessStart businessClose businessMake artifactAcquire companySell companySue organizationMerge companyCrime FinancialSexual assaultSteal moneySeize drugIndictArrestTryConvictSentenceJailCurrency moves upCurrency moves downStock moves upStock moves downStock market moves upStock market moves downStock index moves upStock index moves downPolitical ConflictNominateAppointElectExpel personReach agreementHold meetingImpose mbargoToppleFamilyDieMarryKillInjureHijack vehicleHold hostagesAttack targetFire weaponWeapon hitInvade landMove forcesRetreatSurrenderEvacuateTable 2: Event OntologyFigures 1 and 2 show sample relation and eventtemplates.
Figure 1 shows a Person-Affiliationrelation template for "Frank Ashley, aspokesman for Occidental Petroleum Corp.'"<PERSON AFFILIATION-AP8802230207-54> :=TYPE: PERSON AFFILIATIONPERSON: \[TE for"Frank Ashley"\]ORG: \[TE for "Occidental Petroleum"\]Figure 1: Example of Relation TemplateFigure 2 shows an Attack Target event templatefor the sentence "an Iraqi warplane attacked thefrigate Stark with missiles May 17, 1987.
"<ATTACK TARGET-AP8804160078-12>: =iTYPE: CONFLICTSUBTYPE: ATTACK TARGETATTACKER: \[TE for "an Iraqi warplane"\]TARGET: \[TE for "the frigate Stark"\]WEAPON: \[TE for "missiles"\]TIME: "May 17, 1987"PLACE: \[TE for "the gulf'\]COMMENT: "attacked"Figure 2: Example of Event Template2 System Architecture and ComponentsFigure 3 illustrates the REES systemarchitecture.
REES consists of three maincomponents: a tagging component (cf.
Section2.1), a co-reference resolution module (cf.Section 2.2), and a template generation module(cf.
Section 2.3).
Figure 3 also illustrates thatthe user may run REES from a Graphical UserInterface (GUI) called TemplateTool (cf.Section 2.4).2.1 Tagging ModulesThe tagging component consists of threemodules as shown in Figure 3: NameTagger,NPTagger and EventTagger.
Each module relieson the same pattern-based xtraction engine, butuses different sets of patterns.
The NameTaggerrecognizes names of people, organizations,places, and artifacts (currently only vehicles).77remplateroot / /v- ' : .
v "  .
.
.
.
.
.
.
.GUI interaction?Figure 3: The REES System ArchitectureThe NPTagger then takes the XML-taggedoutput of the NameTagger through two phases.First, it recognizes non-recursive Base NounPhrase (BNP) (our specifications for BNPresemble those in Ramshaw and Marcus 1995).Second, it recognizes complex NPs for onlythe four main semantic types of NPs, i.e.,Person, Organization, Location, and Artifact(vehicle, drug and weapon).
It makes post-modifier attachment decisions only for thoseNPs that are crucial to the extraction at hand.During this second phase, relations which canbe recognized locally (e.g., Age, Affiliation,Maker) are also recognized and stored usingthe XML attributes for the NPs.
For instance,the XML tag for "President of XYZ Corp."below holds an AFFILIATION attribute withthe ID for "XYZ Corp."<PNP ID="03" AFFILIATION="O4">President of<ENTITY ID="04">XYZ Corp.</ENTITY></PNP>Building upon the XML output of theNPTagger, the EventTagger ecognizesevents applying its lexicon-driven,syntactically-based generic patterns.
Thesepatterns tag events in the presence of atleast one of the arguments specified in thelexical entry for a predicate.
Subsequentpattems try to find additional arguments aswell as place and time adjunct informationfor the tagged event.
As an example of theEventTagger's generic patterns, considerthe simplified pattern below.
This pattemmatches on an event-denoting verb thatrequires a direct object of type weapon(e.g., "fire a gun")(&{AND $VP {ARG2_SYN=DO}{ARG2_SEM=WEAPON } }{AND $ARTIFACT {SUBTYPE=WEAPON} })1The important aspect of REES is itsdeclarative, lexicon-driven approach.
Thisapproach requires a lexicon entry for eachevent-denoting word, which is generally aI &=concatenation, AND=Boolean operator, $VPand SARTIFACT are macro references for complexphrases.71:1verb.
The lexicon entry specifies the syntacticand semantic restrictions on the verb'sarguments.
For instance, the following lexiconentry is for the verb "attack."
It indicates thatthe verb "attack" belongs to the CONFLICTontology and to the ATTACK_TARGET type.The first argument for the verb "attack" issemantically an organization, location, person,or artifact (ARGI_SEM), and syntactically asubject (ARGI_SYN).
The second argumentis semantically an organization, location,person or artifact, and syntactically a directobject.
The third argument is semantically aweapon and syntactically a prepositionalphrase introduced by the preposition "with".ATTACK { { {CATEGORY VERB}{ONTOLOGY CONFLICT}{TYPE ATTACK_TARGET}{ARGI_SEM {ORGANIZATION LOCATIONPERSON ARTIFACT} }{ARGI_SYN {SUBJECT} }{ARG2_SEM {ORGANIZATION LOCATIONPERSON ARTIFACT} }{ARG2_SYN {DO}{ARG3_SEM{WEAPON}{ARG3_SYN {WITH} } } }About 50 generic event extraction patterns,supported by lexical information as shownabove, allow extraction of events and theirarguments in cases like:An lraqi warplane attacked the frigate Starkwith missiles May 17, 1987.This generic, lexicon-driven event extractionapproach makes REES easily portable becausenew types of events can be extracted by justadding new verb entries to the lexicon.
Nonew patterns are required.
Moreover, thisapproach allows for easy customizationcapability: a person with no knowledge of thepattern language would be able to configurethe system to extract new events.While the tagging component is similar toother pattern-based IE systems (e.g., Appelt etal.
1995; Aone et al 1998, Yangarber andGrishman 1998), our EventTagger is moreportable through a lexicon-driven approach.2.2 Co-reference ResolutionAfter the tagging phase, REES sends the XMLoutput through a rule-based co-referenceresolution module that resolves:?
definite noun phrases of Organization,Person, and Location types, and?
singular person pronouns: he and she.Only "high-precision" rules are currentlyapplied to selected types of anaphora.
That is,we resolve only those cases of anaphora whoseantecedents the module can identify with highconfidence.
For example, the pronoun ruleslook for the antecedents only within 3sentences, and the definite NP rules relyheavily on the head noun matches.
Our high-precision approach results from ourobservation that unless the module is veryaccurate (above 80% precision), the co-reference module can hurt the overallextraction results by over-merging templates.2.3 Template Generation ModuleA typical template generation module is ahard-coded post-processing module which hasto be written for each type of template.
Bycontrast, our Template Generation module isunique as it uses declarative rules to generateand merge templates automatically so as toachieve portability.2.3.1 Declarative Template GenerationREES outputs the extracted information in theform of either MUC-style templates, asillustrated in Figure 1 and 2, or XML.
Acrucial part of a portable, scalable system is tobe able to output different ypes of relationsand events without changing the templategeneration code.
REES maps XML-taggedoutput of the co-reference module to templatesusing declarative template definitions, whichspecifies the template label (e.g.,ATTACK_TARGET), XML attribute names(e.g., ARGUMENT l), corresponding templateslot names (e.g., ATTACKER), and the typerestrictions on slot values (e.g., string).792.3.2 Event MergingOne of the challenges of event extraction is tobe able to recognize and merge those eventdescriptions which refer to the same event.The Template Generation module uses a set ofdeclarative, customizable rules to merge co-referring events into a single event.
Often, therules reflect pragmatic knowledge of the world.For example, consider the rule below for theDYING event ype.
This rule establishes thatif two die events have the same subject, thenthey refer to the same event (i.e., a personcannot die more than once).
{merge{EVENT 1 {AND {SUBTYPE DIE} {PERSON$foo}}{EVENT 2 {AND {SUBTYPE DIE} {PERSON$foo}}}2.4 Graphical User Interface (GUI)For some applications such as databasepopulation, the user may want to validate thesystem output.
REES is provided with a Java-based Graphical User Interface that allows theuser to run REES and display, delete, ormodify the system output.
As illustrated inFigure 4, the tool displays the templates on thebottom half of the screen, and the user canchoose which template to display.
The top halfof the screen displays the input document withextracted phrases in different colors.
The usercan select any slot value, and the tool willhighlight the portion of the input textresponsible for the slot value.
This feature isvery useful in efficiently verifying systemoutput.
Once the system's output has beenverified, the resulting templates can be savedand used to populate adatabase.3 System Evaluat ionThe table below shows the system's recall,precision, and F-Measure scores for thetraining set (200 texts) and the blind set (208texts) from about a dozen news sources.
Eachset contains at least 3 examples of each type ofrelations and events.
As we mentioned earlier,"relations" includes MUC-style TEs and TRs.Text Task Templates R P F-MSet in keysRel.
9955 76 74 75.35Train Events 2525 57 74 64.57Rel.
& 10707 74 74 73.95EventsRel.
8938 74 74 73.74Blind Events 2020 42 75 53.75Rel.
& 9526 69 74 71.39EventsTable 3: Evaluation ResultsThe blind set F-Measure for 31 types ofrelations (73.95%) exceeded our initial goal of70%.
While the blind set F-Measure for 61types of events was 53.75%, it is significant tonote that 26 types of events achieved an F-Measure over 70%, and 37 types over 60% (cf.Table 4).
For reference, though not exactlycomparable, the best-performing MUC-7system achieved 87% in TE, 76% in TR, and51% in event extraction.F-M in Event typesblind set90-100 2 : Buy artifact.
Marry80-89 9 : Succeed, Merge company, Kill,Surrender, Arrest, Convict, Sentence,Nominate, Expel.70-79 15 : Die, Sell artif~/ct, ExportArtifact, Hire, Start office, Makeartifact, Acquire company, Sueorganization, Stock Index movesdown, Steal money, Indict, Jail,Vehicle crash, Elect, Hold meeting.Table 4: Top-performing Event Types80Figure 4: TemplateToolRegarding relation extraction, the difference inthe score between the training and blind setswas very small.
In fact, the total F-Measure onthe blind set is less than 2 points lower thanthat of the training set.
It is also interesting tonote that for 8 of the 12 relation types wherethe F-Measure dropped more than 10 points,the training set includes less than 20 instances.In other words, there seems to be a naturalcorrelation between low number of instances inthe training set and low performance in theblind set.There was a significant drop between thetraining and blind sets in event extraction: 11points.
We believe that the main reason is thatthe total number of events in the training set isfairly low: 801 instances of 61 types of events(an average of 13/event), where 35 of the eventtypes had fewer than 10 instances.
In fact, 9out of the 14 event types which scored lowerthan 40% F-Measure had fewer than I0examples.
In comparison, there were 34,000instances of 39 types of relations in the trainingset.The contribution of the co-reference module isillustrated in the table below.
Co-referenceresolution consistently improves F-Measuresboth in training and blind sets.
Its impact islarger in relation than event extraction.Text set Task Co- No co-reference referencerules rulesRelations 75.35 72.54Training Events 64.57 63.62Relations 73.95 71.34& EventsRelations 73.74 72.03Blind Events 53.75 53.2271.39 69.86 Relations& EventsTable 5: Comparative results with and withoutco-reference rulesIn the next two sections, we analyze both falsepositives and false negatives.813.1 False Positives (or Precision Errors)REES produced precision errorsfollowing cases:?
Most of the errors were duein theto over-generation of templates.
These are mostlycases of co-referring noun phrases that thesystem failed to resolve.
For example:"Panama ... the nation ... this country.., hiscountry"Rules for the co-reference module are stillunder development, and at present REEShandles only limited types of plural nounphrase anaphora.Spurious events resulted from verbs inconditional constructions (e.g., "if ...then...") or from ambiguous predicates.For instance, "appoint" as a POLITICALevent vs. a PERSONNEL CHANGEevent.The subject of a verb was misidentified.This is particularly frequent in reducedrelative clauses.Kabul radio said the latest deaths broughtto 38 the number of  people killed in thethree car bomb explosions,(Wrong subject: "the number of people" asthe KILLER instead of the victim)3.2 False Negatives (or Recall Errors)Below, we list the most frequent recall errorsin the training set.?
Some event arguments are mentioned withevent nouns instead of event verbs.
Thecurrent system does not handle noun-basedevent extraction.India's acquisition last month of thenuclear submarine from the SovietUnion...(SELLER="Soviet Union" andTIME="last month'" come with the noun-based event "acquisition.")?
Pronouns "it" and "they," which carrylittle semantic information, are currentlynot resolved by the co-reference module.It also has bought hree late-1970s vintageICilo class Soviet submarines and two WestGerman HDW 209 subs(Missed BUYER=India because ofunresolved it.)?
Verb arguments are a conjunction of nounphrases.
The current system does nothandle coordination of verb arguments.Hezbollah killed 21 lsraelis and 43 ofLahad's oldiers(The system gets only the first object: 21Israelis.
)?
Ellipsis cases.
The current system does nothandle ellipsis.The two were sentenced to five-year prisonterms with hard labor by the state securitycourt...(Missed PERSON_SENTENCED fillbecause of unresolved the two.)?
The subject of the event is relatively farfrom the event-denoting verb:Vladislav Listyev, 38, who broughttelevision interview shows in the style ofPhil Donahue or Larry King to Russianviewers and pioneered hard-hittingtelevision journalism in the 1980s, wasshot in the heart by unknown assailantsand died immediately...(The system missed subject VladislavListyev for attack event shot)?
Missed ORG LOCATION relations forlocations that are part of the organization'sname.Larnaca General Hospital(Missed ORG_LOCATION TR for thisand Larnaca.
)We asked a person who is not involved in thedevelopment of REES to review the eventextraction output for the blind set.
This personreported that:?
In 35% of the cases where the REESsystem completely missed an event, it wasbecause the lexicon was missing thepredicate.
REES's event predicate lexiconis rather small at present (a total of 140verbs for 61 event types) and is mostlybased on the examples found in thetraining set,?
In 30% of the cases, the subject or objectwas elliptical.
The system does notcurrently handle ellipsis.82?
In 25% of the cases, syntactic/semanticargument structures were missing fromexisting lexical entries.It is quite encouraging that simply addingadditional predicates and predicate argumentstructures to the lexicon could significantlyincrease the blind set performance.4 Future DirectionsWe believe that improving co-referenceresolution and adding noun-based eventextraction capability are critical to achievingour ultimate goal of at least 80% F-Measurefor relations and 70% for events.4.1 Co-reference ResolutionAs discussed in Section 3.1 and 3.2, accurateco-reference r solution is crucial to improvingthe accuracy of extraction, both in terms ofrecall and precision.
In particular, weidentified two types of high-payoff co-reference r solution:?
definite noun phrase resolution, especiallyplural noun phrases?
3 rd person neutral pronouns "it" and"they.
"4.2 Noun-based Event ExtractionREES currently handles only verb-basedevents.
Noun-based event extraction addsmore complexity because:Nouns are often used in a generic, non-referential manner (e.g., "We see a mergeras being in the consumer's interest"), andWhen referential, nouns often refer toverb-based events, thus requiring noun-verb co-reference resolution ("An F-14crashed shortly after takeoff...
The crash").However, noun-based events are crucialbecause they often introduce additional keyinformation, as the underlined phrases belowindicate:While Bush's meetings with prominent anti-apartheid leaders uch as ArchbishopDesmond Tutu and Albertina Sisulu areimportant...We plan to develop a generic set of patterns fornoun-based event extraction to complement theset of generic verb-based extraction patterns.5 ConclusionsIn this paper, we reported on a fast, portable,large-scale event and relation extraction systemREES.
To the best of our knowledge, this isthe first attempt to develop an IE system whichcan extract such a wide range of relations andevents with high accuracy.
It performsparticularly well on relation extraction, and itachieves 70% or higher F-Measure for 26 typesof events already.
In addition, the design ofREES is highly portable for future addition ofnew relations and events.AcknowledgementsThis project would have not been possiblewithout the contributions of Arcel Castillo,Lauren Halverson, and Sandy Shinn.
Ourthanks also to Brandon Kennedy, whoprepared the hand-tagged data.ReferencesAone, Chinatsu, Lauren Halverson, Tom Hampton,and Mila Ramos-Santacruz.
1998.
"SRA:Description of the IE 2 System Used for MUC-7.
"In Proceedings ofthe 7thMessage UnderstandingConference (MUC-7).Appelt, Douglas E., Jerry R Hobbs, John Bear,David Israel, Megumi Kameyama, Andy Kehler,David Martin, Karen Myers, and Mabry Tyson.1995.
"SRI International FASTUS System: MUC-6 Test Results and Analysis."
In Proceedings ofthe 6 th Message Understanding Conference(MUC-6).Ramshaw, Lance A., and Mitchell P. Marcus.
1995.
"Text Chunking Using Transformation-BasedLearning".
In Proceedings of the 3 rd ACLWorkshop on Very Large Corpora (WVLC95).Yangarber, Roman and Ralph Grishman.
1998.
"NYU: Description of the Proteus~PET System asUsed for MUC-7 ST." In Proceedings of the 6 thMessage Understanding Conference (MUC-7).83
