UMASS/HUGHES: DESCRIPTION OF THE CIRCUS SYSTE MUSED FOR MUC-5 1W.
Lehnert, J. McCarthy, S. Soderland, E. Riloff, C .
Cardie, J. Peterson, F. FengUniversity of MassachusettsDepartment of Computer Scienc eBox 3461 0Amherst, MA 01003-461 0lehnert@cs .turtass .eduC.
Dolan, S. GoldmanHughes Research Laboratories3011 Malibu Canyon Road M/S RL9 6Malibu, CA 9026 5cpd@aic .hrl.hac .comINTRODUCTIO NThe primary goal of our effort is the development of robust and portable language processin gcapabilities for information extraction applications.
The system under evaluation here is based on languageprocessing components that have demonstrated strong performance capabilities in previous evaluation s[Lehnert et al.
1992a] .
Having demonstrated the general viability of these techniques, we are no wconcentrating on the practicality of our technology by creating trainable system components to replac ehand-coded data and manually-engineered software .Our general strategy is to automate the construction of domain-specific dictionaries and other language -related resources so that information extraction can be customized for specific applications with a minimalamount of human assistance.
We employ a hybrid system architecture that combines selective conceptextraction [Lehnert 1991] technologies developed at UMass with trainable classifier technologies develope dat Hughes [Dolan et al.
1991] .
Our MUC-5 system incorporates seven trainable language components tohandle (1) lexical recognition and part-of-speech tagging, (2) knowledge of semantic/syntactic interactions ,(3) semantic feature tagging, (4) noun phrase analysis, (5) limited coreference resolution, (6) domain objec trecognition, and (7) relational link recognition .
Our trainable components have been developed so domai nexperts who have no background in natural language or machine learning can train individual syste mcomponents in the space of a few hours.Many critical aspects of a complete information extraction are not appropriate for customization o rtrainable knowledge acquisition .
For example, our system uses low-level text specialists designed t orecognize dates, locations, revenue objects, and other common constructions that involve knowledge o fconventional language .
Resources of this type are portable across domains (although not all domains requireall specialists) and should be developed as shamble language resources.
The UMass/Hughes focus has beenon other aspects of information extraction that can benefit from corpus-based knowledge acquisition .
Forexample, in any given information extraction application, some sentences are more important than others ,and within a single sentence some phrases are more important than others.
When a dictionary is customizedfor a specific application, vocabulary coverage can be sensitive to the fact that a lot of words contribut elittle or no information to the final extraction task : full dictionary coverage is not needed for informatio nextraction applications .In this paper we will overview our hybrid architecture and trainable system components .
We will lookat examples taken from our official test runs, discuss the test results obtained in our official and optionaltest runs, and identify promising opportunities for additional research .1 This work was supported by the Department of Defense under Contract No .
MDA904-92-C-2390 .
Theviews expressed in this paper are those of the authors and should not be interpreted to represent opinions o rpolicies of the United States Government.277TRAINABLE LANGUAGE PROCESSIN GOur MUC-5 system relies on two major tools that support automated dictionary construction : (1) OTB ,a trainable part-of-speech tagger, and (2) AutoSlog, a dictionary construction tool that operates i nconjunction with the CIRCUS sentence analyzer .
We trained OTB for EJV on a subset of EJV texts andthen again for EME using only EME texts .
OTB is notable for the high hit rates it obtains on the basis ofrelatively little training.
We found that OTB attained overall hit rates of 97% after training on only 100 9sentences for EJV .
OTB crossed the 97% threshold in EME after only 621 training sentences .
Incrementa lOTB training requires human interaction with a point-and-click interface .
Our EJV training was completedafter 16 hours with the interface; our EME training required 10 hours.AutoS log is a dictionary construction tool that analyzes source texts in conjunction with associated keytemplates (or text annotations) in order to propose concept node (CN) definitions for CIRCUS [Riloff &Lehnert 1993; Riloff 1993] .
A special interface is then used for a manual review of the AutoSlogdefinitions in order to separate the good ones from the bad ones.
Of 3167 AutoSlog CN definition sproposed in response to 1100 EJV key templates, 944 (30%) were retained after manual inspection .
ForEME, AutoSlog proposed 2952 CN definitions in response to 1000 key templates and 2275 (77%) of thesewere retained after manual inspection .
After generalizing the original definitions with active/passivetransformations, verb tense generalizations, and singular/plural generalizations, our final EJV dictionar ycontained 3017 CN definitions and our final EME dictionary contained 4220 CN definitions.
It took 20hours to manually inspect and filter the full EJV dictionary ; the full EME dictionary was completed in 1 7hours.
The CIRCUS dictionary used in our official run was based exclusively on AutoSlog CN definitions .No hand-coded or manually altered definitions were added to the CN dictionary.When CIRCUS processes a sentence it can invoke a semantic feature tagger (MayTag) that dynamicallyassigns features to nouns and noun modifiers.
MayTag uses a feature taxonomy based on the semantics ofour target templates, and it dynamically assigns context-sensitive tags using a corpus-driven case-basedreasoning algorithm [Cardie 93] .
MayTag operates as an optional enhancement to CIRCUS sentenc eanalysis .
We ran CIRCUS with MayTag for EJV, but did not use it for EME (well return to a discussionof this and other domain differences later) .
MayTag was trained on 174 EJV sentences containing 559 1words (3060 open class words and 2531 closed class words) .
Our tests indicate that MayTag achieves a 74%hit rate on general semantic features (covering 14 possible tags) and a 75% hit rate on specific semanticfeatures (covering 42 additional tags) .
Interactive training for MayTag took 14 hours using a text editor.An important aspect of the MUC-5 task concerns information extraction at the level of noun phrases .Important set fill information is often found in modifiers, such as adjectives and prepositional phrases.
Part-of-speech tags help us identify basic noun phrase components, but higher-level processes are needed t odetermine if a prepositional phrase should be attached, how a conjunction should be scoped, or if a comm ashould be crossed .
Noun phrase recognition is a non-trivial problem at this higher level .
To address themore complicated aspects of noun phrase recognition, we use a trainable classifier that attempts to find thebest termination point for a relevant noun phrase .
This component was trained exclusively on the EJ Vcorpus and then used without alteration for both EJV and EME .
Experiments indicate that the noun phras eclassifier terminates EJV noun phrases perfectly 87% of the time .
7% of its noun phrases pick up spurioustext (they are extended too far), and 6% are truncated (they are not extended or extended far enough) .
Similarhit rates are found with EME test data: 86% for exact NP recognition, with 6% picking up spurious tex tand 8% being truncated .
The noun phrase classifier was trained on 1350 EJV noun phrases examined i ncontext.
It took 14 hours to manually mark these 1350 instances using a text editor .Before we can go from CIRCUS output to template instantiations, we create intermediate structure scalled memory tokens.
Memory tokens incorporate coreference decisions and structure relevant informationto facilitate template generation .
Memory tokens record source strings from the original input text, ontags, MayTag features, and pointers to concept nodes that extracted individual noun phrases.Discourse analysis contributes to critical decisions associated with memory tokens .
Here we find thegreatest challenges to trainable language systems.
Thus far, we have implemented one trainable componen tthat contributes to coreference resolution in limited contexts .
We isolate compound noun phrases that aresyntactically consistent with appositive constructions and pass these NP pairs on to a coreference classifier .Since adjacent NPs may be separated by a comma if they occur in a list or at a clause boundary, it is easy t o278confuse legitimate appositives with pairings of unrelated (but adjacent) NPs .
Appositive recognition i stherefore treated as a binary classification problem that can be handled with corpus-driven training .
For ourofficial MUC-5 runs we trained a classifier to handle appositive recognition using EJV development text sand then used the resulting classifier for both EJV and EME .
Our best test results with this classifie rshowed an 87% hit rate on EJV appositives.
It took 10 hours to manually classify 2276 training instancesfor the appositive classifier using a training interface .Our final tool, TTG, is responsible for the creation of template generators that map CIRCUS outputinto final template instantiations.
TTG template generators are responsible for the recognition and creatio nof domain objects as well as the insertion of relational links between domain objects .
TTG is corpus-drivenand requires no human intervention during training.
Application-specific access methods (pathing functions )must be hand-coded for a new domain but these can be added to TTG in a few days by a knowledgeabl etechnician working with adequate domain documentation.
Once these adjustments are in place, TTG use smemory tokens and key templates to train classifiers for template generation .
No further humanintervention is required to create the template generators, although additional testing, tuning and adjustment sare needed for optimal performance .Our hybrid architecture demonstrates how machine learning capabilities can be utilized to acquire man ydifferent kinds of knowledge from a corpus .
These same acquisition techniques also make it easy to exploi tthe resulting knowledge without additional knowledge engineering or sophisticated reasoning .
Theknowledge we can readily acquire from a corpus of representative texts is limited with respect to reusability ,but nevertheless cost-effective in a system development scenario predicated on customized software.
Thetrainable components used for both EJV and EME were completed after 101 hours of interactive work by ahuman-in-the-loop.
Moreover, most of our training interfaces can be effectively operated by domain experts :programming knowledge or familiarity with computational linguistics is generally not required .2 Near theend of this paper will report the results of a system development experiment that supports this claim .There will always be a need for some amount of manual programming during the system developmen tcycle for a new information extraction application .
Even so, significant amounts of system developmentthat used to rely on experienced programmers have been shifted over to trainable language components .
Theability to automate knowledge acquisition on the basis of key templates represents a significantredistribution of labor away from skilled knowledge engineers, who need access to domain knowledge ,directly to the domain experts themselves.
By putting domain experts into the role of the human-in-the-loopwe can reduce dependence on software technicians.
When significant amounts of system development workis being handled by automated knowledge acquisition and expert-assisted knowledge acquisition, it wil lbecome increasingly cost-effective to customize and maintain a variety of information extractio napplications.
We have only just begun to explore the range of possibilities associated with trainabl elanguage processing systems .The hybrid architecture underlying our official MUC-5 systems was less than six months old at th etime of the evaluation, and most of the trainable language components that we utilized were less than a yea rold.
Less than 24 person/months were expended for both of the EJV and EME systems, although thi sestimate is confounded by the fact that trainable components and their associated interfaces were beingdesigned, implemented, and tested by the same people responsible for our MUC-5 system development .
Thecreation of a trainable system component represents a one-time system development investment that can b eapplied to subsequent systems at much less overhead.Figure 1 outlines the basic flow-of-control through the major components of the UMass/Hughes MUC- 5system.
Note that most of the trainable components depend only on the texts from the development corpus .The concept node dictionary and the trainable template generator also rely on answer keys during training.
Inthe case of the concept node dictionary, we have been able to drive our dictionary construction process o nthe basis of annotated texts created by using a point-and-click text marking interface .
So the substantialoverhead associated with creating a large collection of key templates is not needed to support automate ddictionary construction .
However, we do not see how to support trainable template generation without a se tof key templates, so this one trainable component requires a significant investment with respect to labor .2 Some technical background is needed to train on.
Knowledge of our part-of-speech tags is needed for tha tinterface.279text fileNI/PREPROCESSORrSPECIALISTSTRAINABLE P-0- STAGGERpreprocessed text file4'TRAINABLE APPOSITIVERECOGNITIONTRAINABLE NPANALYZERCIRCUSSENTENCEANALYZERAUTOMATED CNDICTIONARYTRAINABLE SEMANTICTAGGER,memory tokensLEXICA LCOREFERENCETRAINABLE TEMPLATEGENERATOR1development textsand key templates4final template instantiationFigure 1 : System Architecture280THE OFFICIAL TEST RUN SOur official test runs were conducted on four DECstations running Allegro Common Lisp .
The runswent smoothly in EME but we encountered one fatal error in one of the EJV test sets.
Portions of ourofficial EJV and EME scare reports are shown in figures 2 and 3 .ERR SUB REC PRE UND OVGallobjects 77 25 26 54 65 2 8matched only 48 17 55 76 34 8textfiltering 6 5 9 4 3 5 6P&R 2P&R P&2 RF-measures135 .18144 .39129 .1 3Figure 2: Official EJV Score Report Summarie sERR SUB REC PRE UND OVGallobjects 77 24 31 39 59 4 8matched only 5 4 15 6 0 5 9 3 0 3 1textfiltering 8 5 7 6 15 2 3P&R 2P&R P&2 RF-measures134 .84137 .42132 .59 +Figure 3 : Official EME Score Report Summarie sTHE OPTIONAL TEST RUNSWe ran optional tests to see what sort of recall/precision trade-offs were available from the system .Since the template generator is a set of classifiers, and each classifier outputs a certainty associated with ahypothesized template fragment, we have many parameters that can be manipulated .
Raising the thresholdon the certainty for a hypothesis will, in most cases, increase precision and reduce recall .
In theexperiments reported here, we have varied the parameters over broad classes of discrimination trees .
Thereare three important classes of decision tree: (1) trees that filter the creation of objects based on string fills ,(2) trees that filter the creation of objects based on set fills, and (3) trees that hypothesize relations amon gobjects.
An example of the first class is the tree that filters the CIRCUS output for entity names in th eEJV domain .
An example of the second class is the tree that filters possible lithography objects based o nevidence of the type of lithography process .
The trees that hypothesize TIE_UP_RELATIONSHIP's andME_CAPABILITY's are examples of the third class.For these experiments we have varied the certainty thresholds for all trees of a given class.
Figure 4shows the trade-off achieved for EME .28 18070 ?
n.c 60 no~ 50?
.n ~0 40 n30?
n n20100 I I, II0 102030 4 0Recal lFigure 4 : Trade-off curve for EME8070c 600 500)4030a 2010001020304 0Recal lFigure 5 : Trade-off curve for EJVThis trade-off curve was achieved by varying, in concert the thresholds on all three classes o fdiscrimination tree from 0 .0 to 0.9 .
Figure 5 shows the trade-off curve achieved in EJV .
The differencebetween the two curves highlights difference between the two domains and between the systemconfigurations used for the two domains .
The EME curve shows a much more dramatic trade-off.
The EJVcurve shows that only modest varying of recall and precision is achievable .
Part of this is a reflection ofthe two domains.
In EJV, most relationships were found via two noun phrases that shared a common CNtrigger.
This method proved to be effective at detecting relationships.
Therefore the only real difference i nthe trade-off comes from varying the thresholds for the string-fill and set-fill trees, which generate theobjects that are then composed into relationships.
In EME, there not nearly as many shared triggers and sothe template generator must attempt intelligent guesses for relations.
The probabilistic guesses made inEME are much more amenable to threshold manipulation than the more structured information used in EJV .Also, in EJV the system ran with a slot masseur that embodied some domain knowledge .
In EJV, TTGwas configured to only hypothesize objects if the slot masseur had found a reasonable slot-fill or set-fill .This use of domain knowledge further limited the efficacy of changing certainty thresholds .28 2TRAINABLE INFORMATION EXTRACTION IN ACTIONBefore CIRCUS can tackle an input sentence, we have to pass the source text through a preprocesso rthat locates sentence boundaries and reworks the source text into a list structure .
The preprocessor replacespunctuation marks with special symbols and applies text processing specialists to pick up dates, locations ,and other objects of interest to the target domain .
We use the same preprocessing specialists for both EJ Vand EME : many specialists will apply to multiple domains .
A subset of the Gazetteer was used to supportthe location specialist, but no other MRDs are used by the preprocessing specialists .
We do not have aspecialist that attempts to recognize company names .Source Text :BRIDGESTONE SPORTS CO .
SAID FRIDAY IT HAS SET UP A JOINT VENTURE IN TAIWAN WITH ALOCAL CONCERN AND A JAPANESE TRADING HOUSE TO PRODUCE GOLF CLUBS TO BE SHIPPED T OJAPAN .Preprocessed Text :('START' BRIDGESTONE SPORTS CO= SAID "ON 241189 IT HAS SET UP A JOINT VENTURE IN @@ Taiwan WITH ALOCAL CONCERN AND A JAPANESE TRADING HOUSE TO PRODUCE GOLF CLUBS TO BE SHIPPED TO @@_Japa n'END' )Note that the date specialist had to consult the dateline of the source text in order to determine that"Friday" must refer to November 24, 1989 .
Once the preprocessor has completed its analysis, the OTB part -of-speech tagger identifies parts of speech:(*START* BRIDGESTONE SPORTS C0 SAID **ON_241189 IT HAS SET UP A JOIN Tstrtnmnmnoun verb $date$noun aux pasp ptcl art nmVENTURE IN @@ Taiwan WITH A LOCAL CONCERN AND A JAPANESE TRADIN Gnounprep $location$ prep art nmnounconj art nmnmHOUSE 10 PRODUCE GOLF CLUBS 10 BE SHIPPED TO@@_Japan *END* )nouninf verbnmnouninf aux paspprep$location$ stopon tagged 97.1% of the words in FJV 0592 correctly .
One error associated with "... A COMPANYACTIVE IN TRADING WITH TAIWAN ..
."
led to a truncated noun phrase when "active" was tagged as a hea dnoun instead of a nominative predicate .With part-of-speech tags in place, CIRCUS can begin selective concept extraction .
On this firstsentence from EJV 0592, CIRCUS triggers 18 CN definitions triggered by the words "said" (3 CNs), "set "(3 CNs), "venture" (9 CNs), "produce" (1 CN), and "shipped" (2 CNs) .
These CNs extract a number of ke ynoun phrases, and assign semantic features to these noun phrases based on soft constraints in the C Ndefinition .
Some of these features were recognized to be inconsistent with the slot fill and others wer edeemed acceptable.
Notice that different CNs picked up "BRIDGESTONE SPORTS CO ."
with incompatiblesemantic features (it was associated with both a joint venture and a joint venture parent feature) .extracted NPsrejected CN features:accepted CN features :BRIDGESTONE SPORTS CO=Iv-personjv-entity, jv-parent, j vRA JOINT VENTUREjv-person jv-entity compan yGOLF CLUBSIv prod serv, productionAs we can see from this sentence, CN feature types are not always reliable, and CIRCUS does notalways recognize the violation of a soft feature constraint.
An independent set of semantic features areobtained from MayTag .
In the first sentence of EJV 0592, MayTag only missed marking "golf clubs" as aproduct/service.
An independent set of semantic features are obtained from MayTag :283Ell&MayTag semantic featureshits & misse s*START *BRIDGESTONE((WS-JV-ENTITY) (WS-COMPANY-NAME)); correctSPORTS((WS-JV-ENTITY) (WS-COMPANY-NAME)); correctCO=((WS-JV-ENTITY) (WS-GENERIC-COMPANY)); correctSAIDIT((WS-ENTITY) NIL); correctHASSE TUPAJOINT((WS-JV-ENTITY) NIL); correctVENTURE((WS-JV-ENTITY) NIL); correc tI NJV-LOCATION((WS-LOCATION) NIL); correc tWITHALOCAL((WS-ENTITY) NIL); correctCONCERN((WS-JV-ENTITY) NIL); correctANDAJAPANESE((WS-NATIONALITY) NIL); correctTRADING((WS-PRODUCT-SERVICE) (WS-SALES)); correctHOUSE((WS-JV-ENTITY) NIL); correctTOPRODUCEGOLF((WS-ENTITY) NIL); incorrectCLUBS( (WS-ENTITY) NIL); incorrectTOBESHIPPE DTOJV-LOCATION((WS-LOCATION) NIL); correctIn addition to extracting some noun phrases and assigning semantic features to those noun phrases, wealso call the noun phrase classifier to see if any of the simple NPs picked up by the CN definitions shouldbe extended to longer NPs.
For this sentence, the noun phrase classifier extended only one NP : it decidedthat "A JOINT VENTURE" should be extended to pick up "A JOINT VENTURE IN TAIWAN WITH A LOCA LCONCERN AND A JAPANESE TRADING HOUSE".
The second prepositional phrase should not have bee nincluded - this is an NP expansion that was overextended.Each noun phrase extracted by a CIRCUS concept node will eventually be preserved in a memory tokenthat records the CN features, MayTag features, any NP extensions, and other information associated wit hCN definitions .
But before we look at the memory tokens, let's briefly review the other NPs that ar eextracted from the remainder of the text.
For each preprocessed sentence produced in response to EJV 0592 ,we will put the noun phrases extracted by CIRCUS into boldface and use underlines to indicate how th enoun phrase classifier extends some of these NPs.
*START* BRIDGESTONE SPORTS C0= SAID **0N_241189 11 HAS SET UP AJOINT VENTURE IN cR)	 Taiwan WITHALOCAL CONCERN AND A JAPANESE TRADING HOUSE TO PRODUCE GOLF CLUBS TO BE SHIPPED TO @@_Japan*END*) (*START* THE JOINT VENTURE $COMMA$ BRIDGESTONE SPORTS TAIWAN CO= $COMMA$ CAPITALIZE DAT $$20000000 TWD $COMMA$ WILL START PRODUCTION **DURING 0190 WITH PRODUCTION OFONAND METAL WOODCLUBS A MONTH *END*) (*START* THE MONTHLY OUTPUT WILL BE LATER RAISED T O&850000 UNITS $COMMA$ BRIDGESTON SPORTS OFFICIALS SAID *END*) (*START* THE NEW COMPAN Y$COMMA$ BASED IN KAOHSIUNG $COMMA$ SOUTHERN TAIWAN $COMMA$ IS OWNED %%75 BY BRIDGESTONESPORTS $COMMA$ %%15 BY UNION PRECISION CASTING CO= OF C 	 Taiwan AND THE REMAINDER BY 1ACACO= $COMMA$A COMPANYACTIVE IN TRADING WITH TAIWAN $COMMA$ THE OFFICIALS SAID *END*) (*START*BRIDGESTONE SPORTS HAS SO FAR BEEN ENTRUSTING PRODUCTION OF GOLF CLUB PARTSWITH UNIONPRECISION CASTING AND OTHER TAIWAN COMPANIES *END*) (*START* WITH THE ESTABLISHMENT OF TH ETAIWAN UNIT $COMMA$ THE JAPANESE SPORTS GOODS MAKER PLANS TO INCREASE PRODUCTION OFLUXURY CLUBS INJanan *END*)284As far as our CN dictionary coverage is concerned, we were able to identify all of the relevant nounphrases needed with the exception of "A LOCAL CONCERN AND A JAPANESE TRADING HOUSE' which shouldhave been picked up by a JV parent CN.
In fact, our AutoSlog dictionary had two such definitions in plac efor exactly this type of construction, but neither definition was able to complete its instantiation because ofa previously unknown problem with time stamps inside CIRCUS .
This was a processing failure - not adictionary failure.Trainable noun phrase analysis processes 13 of the 17 NP instances marked above correctly.
Three ofthe NPs were expanded too far, and one was expanded but not quite far enough due to a tagging error b yon ("a company active ..
.").
An inspection of the 13 correct instances reveals that 7 of these would hav ebeen correctly terminated by simple heuristics based on part-of-speech tags .
It is important to note that th etrainable NP analyzer had to deduce these more "obvious" heuristics in the same way that it deduce sdecisions for more complicated decisions .
It is encouraging to see that straightforward heuristics can b eacquired automatically by trainable classifiers .
When our analyzer makes a mistake, it generally happen swith the more complicated noun phrases (which is where hand-coded heuristics tend to break down as well) .After the noun phrase classifier has attempted to find the best termination points for the relevant NPs ,we then call the coreference classifier to consider pairs of adjacent NPs separated by a comma.
In this textwe find three such appositive candidates (the second of which contains an extended NP that was not properl yterminated):THE JOINT VENTURE, BRIDGESTONE SPORTS TAIWAN CO.TAGA CO., A COMPANY ACTIVETHE NEW COMPANY, BASED IN KAOHSIUNG, SOUTHERN TAIWA NIn the third case, the location specialist failed to recognize either Kaohsiung or Southern Taiwan a snames of locations .
On the other hand, the fragment "based in Kaohsiung" was recognized as a locatio ndescription and therefore reformatted it as "THE NEW COMPANY (%BASED-IN% KAOHSIUNG), SOUTHER NTAIWAN" which set up the entire construct as an appositive candidate .
The coreference classifier then wenton to accept each of these three instances as valid appositive constructions .
This was the right decision inthe first two cases, but wrong in the third.
If full location recognition had been working, this last instanc ewould have never been handed to the coreference classifier in the first place .The coreference classifier tells us when adjacent noun phrases should be merged into a single memor ytoken .
We also invoke some hand-coded heuristics for coreference decisions that can be handled on the basi sof lexical features alone.
These heuristics determine that Bridgestone Sports Co. is coreferent withBridgestone Sports, and that "THE JOINT VENTURE,, BRIDGESTONE SPORTS TAIWAN CO." is coreferent with " AJOINT VENTURE IN TAIWAN ..." Our lexical coreference heuristics are nevertheless very conservative, so theyfail to merge our four product service instances in spite of the fact that "clubs" appears in three of thes estring fills .
In effect, we pass the following memory token output to TTG:5 recognized companies (#4 and #5 should have been merged) :lirTAGA CO=" aka "A COMPANY ACTIVE "2) "UNION PRECISION CASTING CO= OF @if) TAIWAN "3r BRIDGESTONE SPORTS CO." aka "BRIDGGSTONE SPORTS"4 THE NEW COMPANY%BASED-IN% KAOHSIUNG)" aka "SOUTHERN TAIWAN "5 THE JOINT VENTURE aka "BRIDGESTONE SPORTS TAIWAN CO=" aka"A JOINT VENTURE IN @@_T AIWAN WITH A LOCAL CONCERN AND A JAPANESE TRADING HOUSE "4 product service strings (all of these should have been merged) :(6 'GOLF CLUBS""&&2tNN IRON AND METAL WOOD CLUBS"8 "GOLF CLUB PARTS WITH UNION PRECISION CASTING AND OTHER TAIWAN COMPANIES "9 'LUXURY CLUBS IN (@JAPAN'1 ownership and 2 percent objects :(10) "$$20000000 TWD"We failed to extract "the remainder by .
.." for the third ownership objec t(11)"%%15"because our percentage specialist was not watching for verbal referents(12) "%%75"in a percentage context - this could be fixed with an adjustment to th especialist.285.
When TTG receives memory tokens as input, the object existence classifiers try to filter out spuriousinformation picked up by overzealous CN definitions .
Unfortunately, in the case of 0592, TTG filtered ou ttwo good memory tokens: (#1 describing the parent Tago Co.), and (#5 describing the joint venture) .
It wasparticularly damaging to throw away #5 because that memory token contained the correct company nam e(Bridgestone Sports Taiwan Co.) .
Of the 3 remaining memory tokens describing companies, TTG correctl yidentified the two parent companies on the basis of semantic features, but then it was forced to pick up #4as the child company.
Our pathing function was smart enough to know that -nm NEW COMPANY' was probabl ynot a good company name, but that left us with ?sotmIERN TAIWAN" for the company name.
So a failure thatstarted with location recognition led to a mistake in trainable appositive recognition, which then combine dwith a failure in lexical coreference recognition and a filtering error by TTG in order to give us a join tventure named "SOUTHERN TAIWAN" instead of "BRIDGESTON SPORTS TAIWAN ."
Overly aggressive filteringby TTG resulted in the loss of our 4 product service memory tokens .Our CN instantiations do not explicitly represent relational information, but CNs that share a commo ntrigger word can be counted on to link two CN instantiations in some kind of a relationship .
Triggerfamilies can reliably tell us when two entities are related, but they can't tell us what that relationship is .
Werelied on "TTG to deduce specific relationships on the basis of its training .
In cases like "75% B YBRIDGESTONE SPORTS", TTG had no trouble linking extracted percentage objects with companies .
But ourtrainable link recognition ran into more difficulties when trigger families contained multiple companie sAmong the features that TTG had available for discrimination where closed class features, such as memor ytoken types, semantic features, and CN patterns, and open class features (i .e.
trigger words) .
However,although there exist heuristics for discriminating relationships based on particular words, the combinatio nof the algorithms used (ID3) and the amount of data (600 stories) failed to induce these heuristics .
The maybe other algorithms, however, that used the same or less data and external knowledge to derive suc hheuristics from the training data .The processing for EME proceeds very similarly to EJV, with the exception that MayTag is not usedin our EME configuration, and in the EME system we used our standard CN mechanism and an additionalkeyword CN (KCN) mechanism.
The KCN mechanism was used to recognize specific types of processing,equipment, and devices that have one or only a few possible manifestations.
Below we see the OTB tag sfor the first sentence, all of which are correct .
In fact, for EME text 2789568, OTB had 100% hit rate .
*start* **DURING_2Q91$COMMA$Nikon Corp= $LPAREN$&&773 1strt$date$puncnmnoun puncnoun$RPAREN$ plans to market the NSR-1755EX8A $COMMA$ a new steppe rpuncverbinf verbart nounpuncart nmnounintended foruse inthe production of&&64- Mbit DRAMs*end*gaspprep noun prep art nounprepnmnm nounstopThe memory token structure below illustrates the processing of the text prior to TTG.
Two NPs areidentified as the same entity, "Nikon" and "Nikon Corp." The two NPs are merged into one memory tokenbased on name merging heuristics .
The second NP demonstrates how multiple recognition mechanisms ca nadd robustness to the processing .
"Nikon Corp." is picked up by both a CN triggered off of "plans tomarket" and by two KCNs, one that looks for "Corp ."
and another that looks for the lead NP in the story.
(TOKEN(TYPE (ME-ENTITY) )(SUBTYPE NIL)(RELATION NIL )(SLOT-FILLS(TYPE COMPANY )(NAME ( :SYM-LIST NIKON CORP=)) )(NPS(NP 2 1 (NIKON )(CNS %ME-ENTITY-NAME-SUBJECT-VERB-AND-DO-STEPPER%) )(NP 0 3 (NIKON CORP= )(CNS %ME-ENTITY-NAME-SUBJECT-VERB-AND-INFINITIVE-PLANS-TO-MARKET% )(KCNS %KEYWORD-ME-ENTITY-CORP= %%LEAD-NP%))))286Unfortunately, our system did not get any lithography objects for this story .
On our list of things toget to if time permitted was creating a lithography object for an otherwise orphaned stepper .
We would hav eonly gotten one lithography object since we merged all mentions of "stepper" into one memory token .We created a synthetic version of the system that inserted a lithography memory token corresponding toeach stepper.
One was discard by TTG and another was created because there were two different equipmen tobjects attached to the remaining lithography object .
The features that TTG used to hypothesize a ne wME_CAPABILITY are illustrative of one of the weaknesses of this particular method .
TTG used thefollowing features to decide not to generate an ME_CAPABILI TY developer .FEATURE RELATION CERTAINTY AMER FEATURE0.36The process is not X-RAY 0.23The entity is not triggered off "developed" 0.14The process is not CVD 0.03The process is not LITHOGRAPHY of UKN type 0.04The process is not ETCHING 0.06The entity is not triggered off "from" 0.12All of the features are negative, and the absence of each feature reduces the certainty that the relatio nholds, because each feature's presence, broadly speaking, is positive evidence of a relation.
Therefore, thenode of the decision tree that is found is a grouping of cases that have no particular positive evidence tosupport the relation, but also no negative evidence .
With the relation threshold set at 0 .3, this yields anegative identification of a relation .
However, there are strong indications of a relation here .
For example ,the trigger "plans to market" is good evidence of a relation, however, the nature decision tree algorithm s(recursively splitting the training data) causes us to loose that feature (in favor of other, better features) .The following set of features shows what TTG used to generate an ME CAPABILITY distributor .FEATURE RELATION CERTAINTY AFTER FEATURE0.38The process is not packaging 0.47The entity is not in a PP 0.58A CN marked the entity as an entity 0 .55The process is not layering type sputtering 0.40Again, we do not see here the features that we would expect, given the text .
A human generating rule swould say that "plans to market" is a good indication of a ME_CAPABILITY distributor .DICTIONARY CONSTRUCTION BY DOMAIN EXPERT SSites participating in the recent message understanding conferences have increasingly focused theirresearch on developing methods for automated knowledge acquisition and tools for human-assiste dknowledge engineering .
However, it is important to remember that the ultimate users of these tools will bedomain experts, not natural language processing researchers .
Domain experts have extensive knowledgeabout the task and the domain, but will have little or no background in linguistics or text processing.
Tool sthat assume familiarity with computational linguistics will be of limited use in practical developmen tscenarios .To investigate practical dictionary construction, we conducted an experiment with government analysts .We wanted to demonstrate that domain experts with no background in text processing could successfully us ethe AutoSlog dictionary construction tool [Riloff and Lehnert 1993] .
We compared the dictionariesconstructed by the government analysts with a dictionary constructed by a UMass researcher .
The results ofthe experiment suggest that domain experts can successfully use AutoSlog with only minimal training andachieve performance levels comparable to NLP researchers.,287AutoSlog is a system that automatically constructs a dictionary for information extraction tasks .
Givena training corpus, AutoSlog proposes domain-specific concept node definitions that CIRCUS [Lehnert1991] uses to extract information from text.
However, many of the definitions proposed by AutoS logshould not be retained in the permanent dictionary because they are useless or too risky .
We therefore relyon a human-in-the-loop to manually skim the defmitions proposed by AutoSlog and separate the good onesfrom the bad ones.Two government analysts agreed to be the subjects of our experiment.
Both analysts had generatedtemplates for the joint ventures domain, so they were experts with the EJV domain and the template-fillin gtask.
Neither analyst had any background in linguistics or text processing and had no previous experiencewith our system .
Before they began using the AutoSlog interface, we gave them a 1 .5 hour tutorial toexplain how AutoSlog works and how to use the interface .
The tutorial included some examples tohighlight important issues and general decision-making advice .
Finally, we gave each analyst a set of 1575concept node definitions to review.
These included definitions to extract 8 types of information : jv-entities ,facilities, person names, product/service descriptions, ownership percentages, total revenue amounts ,revenue rate amounts, and ownership capitalization amounts .We did not give the analysts all of the concept node definitions proposed by AutoSlog for the EJVdomain .
AutoSlog actually proposed 3167 concept node definitions, but the analysts were only available fortwo days and we did not expect them to be able to review 3167 definitions in this limited time frame .
Sowe created an "abridged" version of the dictionary by eliminating jv-entity and product/service patterns tha tappeared only infrequently in the corpus.3 The resulting "abridged" dictionary contained 1575 concept nod edefinitions .We compared the analysts' dictionaries with the dictionary generated by UMass for the final Tipste revaluation .
However, the official UMass dictionary was based on the complete set of 3167 definition soriginally proposed by AutoSlog as well as definitions that were spawned by AutoSlog's optiona lgeneralization modules .
We did not use the generalization modules in this experiment, due to tim econstraints.
To create a comparable UMass dictionary, we removed all of the "generalized" definitions fromthe UMass dictionary as well as the definitions that were not among the 1575 given to the analysts .
Theresulting UMass dictionary was a much smaller subset of the official UMass dictionary .Analyst A took approximately 12.0 hours and Analyst B took approximately 10 .6 hours to filter theirrespective dictionaries.
Figure 6 shows the number of definitions that each analyst kept, separated by types .For comparison's sake, we also show the breakdown for the smaller UMass dictionary .CN Type # proposed byAutoS log# kept(UMass)# kept(Analyst A)# kept(Analyst B)entity 688 311 357 423facility 80 20 16 55ownership-percent 174 91 117 9 1person 243 119 149 52pmd sery 316 76 152 44revenue-rate 19 14 12 1 6revenue-total 30 22 15 26total-capitalization 25 14 13 22TOTAL 1575 667 831 729Figure 6 : Comparative dictionary size s3While processing the training corpus, AutoSlog keeps track of the number of times that it proposes eac hdefinition (it may propose a definition more than once if the same pattern appears multiple times in th ecorpus) .
We removed all jv-entity definitions that were proposed < 2 times and all product/servic edefinitions that were proposed < 3 times .
We eliminated jv-entity and product/service definitions onl ybecause the sheer number of these definitions overwhelmed the other types .288We compared the dictionaries constructed by the analysts with the UMass dictionary in the followin gmanner.
We took the official UMass/Hughes system, removed the official UMass dictionary, and replaced i twith a new dictionary (the smaller UMass dictionary or an analysts' dictionary) .
One complication is thatthe UMass/Hughes system includes two modules, TTG and Maytag, that use the concept node dictionaryduring training.
In a clean experimental design, we should ideally retrain these components for each ne wdictionary.
We did retrain the template generator (TTG), but we did not retrain Maytag .
We expect that thi sshould not have a significant impact on the relative performances of the dictionaries, but we are not certai nof its exact impact.
Finally, we scored each new version of the UMass/Hughes system on the Tips3 tes tset .
Figure 7 shows the results for each dictionary .TIPS3 Recall Precision P&R ERRUMass/Hughes 18 51 27.06 83Analyst A 19 47 27.39 83Analyst B 20 47 27.89 83Figure 7 : Comparative scores for Tips3The F-measures (P&R) were extremely close across all 3 dictionaries .
In fact, both analysts' dictionariesachieved slightly higher F-measures than the UMass dictionary .
The error rates (ERR) for all threedictionaries were identical .
But we do see some variation in the recall and precision scores .
We also seevariations when we score the three parts of Tips3 separately (see Figure 8) .TIPS3/Partl Recall Precision P&R ERRUMass/Hughes 18 51 27.04 83Analyst A 20 48 28.00 82Analyst B 22 47 29.69 _8 1TIPS3/Part2 Recall Precision P&R ERRUMass/Hughes 17 52 26.03 84Analyst A 18 48 25 .92 84Analyst B 20 47 27 .75 83TIPS3/Part3 Recall Precision P&R ERRUMass/Hughes 20 50 28 .12 82Analyst A 20 46 27.96 82Analyst B 17 48 _25 .25 _84Figure 8 : Comparative scores for Partl, Part2, and Part 3In general, the analysts' dictionaries achieved slightly higher recall but lower precision than the UMas sdictionary.
We hypothesize that this is because the UMass researcher was not very familiar with the corpu sand was therefore somewhat conservative about keeping definitions .
The analysts were much more familia rwith the corpus and were probably more willing to keep definitions for patterns that they had seen before .There is usually a trade-off involved in making these decisions : a liberal strategy will often result in highe rrecall but lower precision whereas a conservative strategy may result in lower recall but higher precision .It is interesting to note that even though there was great variation across the individual dictionaries (se eFigure 6), the resulting scores were very similar .
This may be because some definitions can contribute adisproportionate amount of performance if they are frequently triggered by a given test set .
If the thre edictionaries were in agreement on that subset of the dictionary that is most heavily used, those definitionscould dominate overall system performance.
Some dictionary defmitions are more important than others .To summarize, this experiment suggests that domain experts can successfully use AutoSlog to builddomain-specific dictionaries for information extraction .
With only 1 .5. hours of training, two domainexperts constructed dictionaries that achieved performance comparable to a dictionary constructed by aUMass researcher.
Although this was only a small experiment, the results lend credibility to the claim thatdomain experts can build effective dictionaries for information extraction .28 9WHAT WORKS AND WHAT NEEDS WORKWhen we look at individual texts and work up a walk through analysis of what is and is not working,we find that many of our trainable language components are working very well.
The dictionary coverageprovided by AutoSlog appears to be quite adequate .
OTB is operating reliably enough for subsequen tsentence analysis .
When we run into difficulties with our trainable components, we often find that many ofthese difficulties stem from a mismatch of training data with test data .
For example, when we trained thecoreference interface for appositive recognition, we eliminated from the training data all candidate pair sinvolving locations because the location specialist should be identifying locations for us .
If the coreferenceclassifier were operating in an ideal environment, it would never encounter unrecognized locations .Unfortunately, as we saw with EJV 0592, the location specialist does not trap all the locations, and this le dto a bad coreference decision .
In an earlier version of the coreference classifier we had trained it on imperfec tdata containing unrecognized locations, but as the location specialist improved, we felt that the training fo rthe coreference classifier was falling increasingly out of sync with the rest of the system so we updated it byeliminating all the location instances .
Then when the coreference classifier was confronted with a nunrecognized location, it failed to classify it correctly .
When upstream system components are continuallyevolving (as they were during our MUC-5 development cycle), it is difficult to synchronize downstrea mdependencies in training data.
A better system development cycle would stabilize upstream component sbefore training downstream components in order to maintain the best possible synchronization acrosstrainable components .TTG was able to add some value to the output of CIRCUS and subsequent discourse processing.
Inmodule tests, TTG typically added 6-12% of accuracy in identifying domain objects and relationships .
Thatadded value is measured against picking that most likely class (yes or no) for a particular domain objec t(e .g .
JV-ENTITY or ME-LITHOGRAPHY) or relationship (e .g .
JV-TIE-UP or ME-MICROELECTRONICS-CAPABILITY) .
However, TTG fell far below our expectations for correctlyfiltering and connecting the parser's output .
We find two reasons for this short fall .
First, some smal ldeficit can be attributed to the system development cycle since TTG sits at the end of the cycle of trainingand testing various modules .The second, and by far the dominant effect comes from the combination of the training algorithm (11)3)and the amount of data.
As mentioned previously, there are two types of features used by TTG: (1) closedclass (e.g .
token type, semantic features, and CN patterns) and (2) open class features (i .e .
CN triggerwords) .
Using open class features can be difficult, because most algorithms cannot detect reliabl ediscriminating features if there are too many features?reliable features cannot be separated from noise.Using trigger words in conjunction relations between memory token results in 3,000-5,000 binary features .With no noise suppression added to the algorithm and given a large number of features, ID3 will create verydeep decision trees that classify stories in the training set based on noise .We ran two sets of decision trees in deciding how to configure our system for the fmal test run.
MIN-TREES using only closed class features and no noise suppression and MAX-TREE using closed class andopen class features and a noise suppression rule.
The noise suppression was a termination condition on therecursion of the ID3 algorithm.
Recursion was terminated when all features resulted in creating a node tha tclassified examples from few than 10 different source texts .
Using closed class features rarely resulted in aterminal node that classified examples from fewer than 10 stories.
In all tests the MAX-trees performedbetter.
However, as a result of the noise suppression, no decision tree contained very many discrimination son a trigger.
The performance of the MAX-trees indicated that individual words are good discriminators ,however their scarcity in the decision trees indicates that we are not using the appropriate algorithm .
Webelieve that data-lean algorithms (such as explanation-based learning) in concert with shared knowledgebases might be effective.In attributing performance to various components, we measured 25 random texts in EME .
At thememory token stage we found that CIRCUS had extracted string-fills and set-fills with a recall/precision of68/54.
However our score output for those slots was 32/45 (measured only on the slots we attempted) .Even when the thresholds for TTG were lowered to 0 .0, so that all output came through, the recall was notanywhere near 68 .
Therefore it would appear that the difficult part of the template task is not finding goo dthings to put in the template, but figuring how to split and merge objects .
We do not (yet) have a trainablecomponent that handles splitting and merging decisions in general .
,290The EJV and EME systems that we tested in our official evaluation were in many ways incomplet esystems.
Although our upstream components were operating reasonably well, additional feedback cycleswere badly needed for other components operating downstream .
In particular, trainable coreference andtrainable template generation did not received the time and attention they deserve .
We are generallyencouraged by the success of our trainable components for part-of-speech tagging, dictionary generation ,noun phrase analysis, semantic feature tagging, and coreference based on appositive recognition .
But weencountered substantial difficulties with general coreference prior to template generation .
This appears to bethe greatest challenge remaining for trainable components supporting information extraction .
We knowfrom our earlier work in the domain of terrorism that coreference resolution can be reasonably well-managedon the basis of hand-coded heuristics [Lehnert et al.
1992b] .
But this type of solution does not port acros sdomains and therefore represents a significant system development bottleneck .
True portability will only beachieved with trainable coreference capabilities.We believe that trainable discourse analysis was the major stumbling block standing between ou rMUC-5 system and the performance levels attained by systems incorporating hand-coded discourse analysis .We remain optimistic that state-of-the-art performance will be obtained by corpus-driven machine learnin gtechniques but it is clear that more research is needed to meet this very important challenge .
To facilitateresearch in this area by other sites, UMass will make concept extraction training data (CIRCUS output) fo rthe full EJV and EME corpora available to research laboratories with intemet access .
When paired withMUC-5 key templates available from the Linguistic Data Consortium, this data will allow a wide range ofresearchers who may not be experts in natural language to tackle the challenge of trainable coreference an dtemplate generation as problems in machine learning .
We believe it is important for the NLP communityto encourage and support the involvement of a wider research community in our quest for practica linformation extraction technologies .BIBLIOGRAPHYCardie, C .
(1993) A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentenc eAnalysis.
Eleventh National Conference on Artificial Intelligence (AAAI-93) .
Washington, D .C .
pp.
798-803 .Dolan, C. P., Goldman, S .
R., Cuda, T. V., & Nakamura, A .
M. (1991).
Hughes Trainable Tex tSkimmer.
Description of the TTS System as Used for MUC-3 .
In B .
Sundheim (Ed .
), Third Messag eUnderstanding Conference (MUC-3) .
Naval Ocean Systems Center, San Diego California: MorganKaufmann.
pp.
155-162.Lehnert, W. (1991) Symbolic/Subsymbolic Sentence Analysis : Exploiting the Best of Two Worlds .Advances in Connectionist and Neural Computation Theory .
Vol.
I.
(ed: J. Pollack and J .
Barnden) AblexPublishing, Norwood, New Jersey.
pp.
135-164.Lehnert, W ., Cardie, C., Fisher, D., McCarthy, J ., Riloff, E., Soderland, S .
(1992a) University ofMassachusetts : MUC-4 Text Results and Analysis Proceedings of the Fourth Message Understandin gConference (MUC-4).
Morgan Kaufmann.
San Mateo, CA.
pp.
151-158.Lehner[, W., Cardie, C., Fisher, D., McCarthy, J ., Riloff, E ., Soderland, S .
(1992b) Description of theCIRCUS system as Used for MUC-4 .
Proceedings of the Fourth Message Understanding Conferenc e(MUC-4) .
Morgan Kaufmann .
San Mateo, CA .
pp.
282-288 .Quinlan, J .
R .
(1983) .
Learning Efficient Classification Procedures and Their Application to Chess En dGames.
In R. S. Michalski, J. G. Carbonell, & T .
M. Mitchell (Eds.
), Machine Learning: An Artificia lIntelligence Approach .
Morgan Kaufmann.
pp.
463-482 .Riloff, E. (1993) Automatically Constructing a Dictionary for Information Extraction Tasks .
EleventhNational Conference on Artificial Intelligence (AAAI-93) .
Washington, D.C. pp .
811-816 .Riloff E., and Lehnert, W. (1993) Automated Dictionary Construction for Information Extraction fro mText.
Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications .
IEEEComputer Society Press .
pp.
93-99 .291
