STOCHASTIC  REPRESENTATION OFCONCEPTUAL STRUCTURE IN  THE AT IS  TASKRoberto Pieraccini, Esther Levin, Chin-Hui LeeAT&T Bell Laboratories600 Mountain AvenueMurray Hill, N J, 07974ABSTRACTWe propose amodel for a statistical representation f the conceptualstructure in a restricted subset of spoken natural language.
The modelis used for segmenting a sentence into phrases and labeling them withconcept relations (or cases).
The model is trained using a corpus ofannotated transcribed sentences.
The performance of the model wasassessed on two tasks, including DARPA ATIS class A sentences.INTRODUCTIONThe goal of a speech understanding system is generally that oftranslating a sequence of acoustic measurements of the speech sig-nal into some form that represents the meaning conveyed by thesentence.
One of the knowledge representation paradigms, knownas semantic networks \[2\] establishes relations between conceptualentities using a graph structure.
These concept relations, or lin-guistic cases, can be used to label different parts of a sentencein order to obtain its interpretation.
The task itself defines theset of relevant cases.
For instance, for the task of assigning theorigin, the destination and the departure time of a flight, a con-venient representation is in terms of the following set of cases:C = {01,C2, C3, C4}, where Cl  = ORIGIN, C2 = DESTINATION,03 = DEPARTURE_TIME, and C4 = DUMMY.
The introductionof a DUMMY case is useful for covering all the parts of the sen-tence that are not relevant to the task.
A sentence like !
wouldlike to fly from Boston to Chicago next Saturday night can beanalyzed as:?
DUMMY: I would like to fly?
ORIGIN: from Boston?
DESTINATION: to Chicago?
DEPARTURE_TIME: next Saturday night.Note that although the first phrase (I  would like to fly) conveysimportant information, it is considered irrelevant o this partic-ular task, and therefore assigned to the DUMMY case.
Thesegmentation of a sentence into cases (conceptual segmentation)can be described by labeling each word in the sentence with theindex of the case it expresses.
In the example above, the con-ceptuM segmentation is represented by the following sequence oflabels:C = (cI,c2, c3...c12) (1)where:e l  = c2 .
.
.
.
.
e5 = 04  (2 )e 6 = c?
= C1c 8 ~ c 9 z C 2In this paper we tackle the problem of decoding the words con-stituting the spoken sentence and the corresponding sequence ofcase labels, from the speech signal.MAP DECODING OF CASESLet us denote byA = a , ,a2 .
.
.aN,  (3)the sequence of acoustic observations extracted from a spokensentence, byW = ~, ,w2.
.
.WM,  (4)the sequence of words constituting the sentence, and byC = c l , c2 .
.
.CM,  (5)the sequence of case labels, where ci takes its values from a pre-defined set of conceptual relations C = {C1,C2,.
.
.CK}.
Theproblem of finding W and C given A can be approached usingthe maximum a posteriori decoding (MAP).
Following this crite-rion we want to find the sequence of words ~V and the sequenceof cases C that maximizes the conditional probabilityP(~V, C JA)  = max P (W,C\ ]A) .
(6)WxCThis conditional probability can be written using the Bayes in-version formula as:P (W,C\ [A)  = P (A IW,C)P(W\ [C)P(C)P (A)  (7)In this formula P(C)  represents the a-priori probability of thesequence of cases, P (W I C) is the probability of a sentenceexpressing a given sequence of cases, and P (A  I W,C)  is theacoustic model.
We can reasonably assume that the acousticrepresentation f a word is independent of the conceptual relationit belongs to, hence:P (A lW,C)  = P (A IW) ,  (8)and this is the criterion that is usually maximized in stochasticbased speech recognizers, for instance those using hidden Markov121modeling \[1\] for the acoustic/phonetic decoding.
In this paperwe deal with the remaining termsP (W \[ C )P (C)= (9)MH P(w~lwi - , .
.
.w l ,C)P(Wl  l C)i=2MI I  P(c~ L c~_1 ...cl)e(c~)i=2We proceed by assuming that:P(wi I wi-t .
.
.wl ,  C) = (10)P(wi I ~',-~...
w~_., ci),andP(c i lc i _ t .
.
.c t )  = (11)P(ci l ei-1...ci-m).These are Markov processes of order n and m respectively, and ifn and m are large we don't lose any generality by making this as-sumption.
For practical purposes n and m should be small enoughto allow a reliable estimation of the probabilities from a finite setof data.
An additional assumption i  equation (10) is that a givenword in the sentence, used for expressing a certain case, is inde-pendent of the case of the preceding words.
Assuming that thesequence of words could be directly observed (for instance pro-viding a transcription of the uttered sentence), and the sequenceof cases is unknown, equations (10) and (11) describe a a hiddenMarkov process, where the states of the underlying model corre-spond to the cases, the observation probabilities of each state arerepresented by equation (10) in the form of state local (n + 1)-gram language models, and the transitions between the states aredescribed by equation (11).THE FROM-TO TASKA first evaluation of the model was performed based on a set of825 sentences artificially generated by a finite state grammar \[3\]using a vocabulary of 41 different words.
The sentences expressdifferent ways of making requests to travel between two cities.
Atypical example is:I want to travel into Boston and I am interested in flightsbetween Boston and WashingtonThe task consisted of identifying the origin and destination citiesof the flight.
The relevant cases of this task are then flight ori-gin and flight destination.
However the model has three states,ORIGIN, DESTINATION and DUMMY.
50 sentences, randomlyselected out of the 825, were used to estimate the parametersof the model, i.e.
the transition probabilities (equation 11) andthe state local language models (equation 10), with n = 1 andm = 1 (i.e.
the underlying Markov process was a 1 st order pro-cess and the state local language models were bi-grams).
Thetraining sentences were hand-labeled with the appropriate cases.The remaining 775 sentences were decoded using Viterbi decod-ing algorithm.
The performance was assessed by counting thenumber of sentences that were segmented assigning the correctwords (i.e.
the correct city names) to the DESTINATION andORIGIN states.
We observed that 7% of the sentences (55 out of775) had a wrong origin/destination assignment.
In some of thewrong segmentations one of the relevant states was missing, theother state containing both the real destination and origin cities.In other examples, similar to the sentences shown above, both thedestination and the origin states were assigned to the same cityname, that appeared twice in the sentence.
To improve the per-formance we imposed some additional constraints in the decodingprocedure.
For a given sentence the decoded state sequence wassearched among those sequences of states where both the originand destination states were visited only once (i.e.
when one ofthose states was left, the current partial path was not allowed toenter that state again).
In addition, the phrases assigned to theorigin and destination states had to include different city names.These constraints, representing a higher level a priori knowledgeof the task, were imposed in the Viterbi decoding by keepingtrack of the past sequence of states for each partial candidate so-lution, and duplicating the partial solutions when two (or more)candidates merged at the same state and showed conflicting con-straints.
This approach resulted in a substantial improvement ofthe performance.
Only one error was observed out of the 775 testsentences ( 0.13% error rate).
The same level of performance wasobtained in experiments using a 1-gram language model insideeach state, but increasing the number of states to five:ORIGIN, DESTINATION, DUMMY, FROM, TO.The last two states accounted for the expressions that usuallyprecede the origin and destination city names respectively.
Forexample the FROM state was associated to expressions of thekind: from, depart out of, leaving, etc., and the TO state was as-sociated to expressions like: to, going to, arriving into, etc.
Thisexperiment indicates that there is a tradeoff between the numberof states and the complexity (order) of the state language mod-els.
Expanding the set of states to reflect the linguistic structureof the sentences may result in a reduction of the number of pa-rameters to be estimated uring training, giving a more robustmodel.THE AT IS  TASKThe technique of case decoding is being applied to the classA sentences of the DARPA ATIS task.
A sentence of this taskcan be analyzed in terms of 7 general cases, that are QUERY,generally associated to the phrases expressing the kind of request,OBJECT expressing the object of the query, ATTRIBUTE thatdescribes ome attributes of the object, RESTRICTION describingthe restrictions on the values of the answer, Q_ATTR describingpossible attributes of the query, AND including connectives likeand, or, also, indicating that the sentence may have more thatone query.
Of course we include a DUMMY state like in the abovementioned examples.
For example, a sentence like:What type of economy fare could I get from San Francisco toDallas on the 25th of Aprilis segmented as:?
QUERY: What type of?
ATTRIBUTE: economy?
OBJECT: fare122?
Q_ATTR: could I get?
RESTRICTION: from San Francisco to Dallas on the 25thof AprilWe can further analyze some of the cases into more detailed con-ceptual relations, giving the following representation:s ATTRIBUTEo a_fare: economy?
RESTRICTIONo origin: from San Franciscoo destination: to DallasO date : on the 25th of AprilWe defined 44 different cases for describing the whole set of 547class A training sentences.
The complete list of cases is shown inTable l. The training sentences (covered by a vocabulary of 501words) were hand-labeled according to this set of states and thetransition probabilities and the state local bigram models wereestimated using the maximum likelihood criterion.
Table 2 showsexamples of the phrases used for estimating the bigram languagemodels for some of the defined states.
Considering the large num-ber of parameters to be estimated ( i.e.
the transition probabili-ties between the 44 states of the model and the 44 bigram modelsextended to the entire vocabulary of 501 words), and consideringthe small number of training sentences, this estimation poses ro-bustness problems.
One way to alleviate these problems consistsof grouping the words in the vocabulary into equivalence classes.For example all the' city names can be grouped in the same class,as well as the airport names, the numbers, the airline names, etc.The testing of the system was performed on the transcribedJun-90 and Feb-91 class A test sentences.
New words were allo-cated to a new-word category that was assigned a small probabil-ity within each state.
Table 3 reports the number of sentences,for each test set, that were correctly labeled by the case decoder,along with the statistics on the correctly assigned cases.
Table 4shows examples of correct segmentations from the FEB-91 testset.
It is interesting to notice the allocation of the connectiveand to different cases in sentences 1),3), and 4).
.Although sen-tences 1) and 3) contain similar expressions (between ... and ...),the system recognizes that in the first case the phrase refers toa period of time, while in the second case it refers to origin anddestination cases.
Moreover, sentence 3) shows that the conceptrelations origin and destination are not necessarily referred to theorigin and destination of a flight, but can be referred to otherevents, like ground transportation i this case.
This sensitivityto the context (to the value of the 0 BJ ECT in the example above)shown by certain cases must be taken into account by the modulethat will interpret he conceptual segmentation and generate theSQL query.
In sentence 4) the word and is clearly interpretedas connecting two distinct restrictions on the query.
The samephenomenon is shown in sentence 5) where the word or connectstwo alternative possible origins of the flight.
Table 5 shows ex-amples of incorrect segmentations from the FEB-91 test set.
Insentence 1) the phrase used for Eastern should be assigned tothe airline case.
The error is due to the fact that the word Easternwas not observed in the training set.
In sentence 2) the phrasethrough Dallas Fort Worth should have been labeled with the con-nect case, but this case has very few examples in the training setQUERYOBJECTATTRIBUTE attributea_datea_origina_destina_tirnea_airlinea_flcodea_aircrafta_classa_farea_stopa_atplacea _waya_restricta_tablea_bodyQ_AT T R'AND, DUMMYRESTRICTION dateorigindestintimeairlineflcodemealgroundaircraftclassfarestopatplacedept_timearvl_timewayrestricttablerangespeedbodydayconnectTable 1: The set of cases in the ATIS  taskQUERY I would likecan I have a list ofit give me a description ofOBJECT the flightsthe fare ona price on a ticketorigin arriving from Dallasfrom Atlanta airportbetween airport B WIdeparting Atlantadestin and Bostonarriving in San Franciscogoing to San Franciscoreturning to Atlantadept_time leaving after 1:00 pmthat depart in the afternoonway round- tripreturnthat are round-tripclass a class Q W ticketa 1st class ticketwhich have 1st class service availableTable 2: Examples of phrases assigned to cases in the training sen-tencesTEST Number of Sentences I Number of I Casessentences correct cases correctJUN-90 98 87 (88.7%) i 419 \]398 (95.0%)FEB-91 i 148 119 (80.4%) 713 671 (94.1%)Table 3: Results with two different test sets1231)Please list all flights between Baltimore and Atlantaon Tuesdays between 4 in the afternoon and 9 in the evenincDUMMY: PleaseQUERY: list allOBJECT: the flightsorigin: between Baltimoredestin: and Atlantaday: on Tuesdaystime: between 4 in the afternoon and 9 in the evening2) What's the cheapest round-trip airfare on Americanflight 1074 from Dallas to PhiladelphiaQUERY: What'sa_fare: the cheapesta_way: round-tripOBJECT: airfareairline: on Americanflcode: flight 1074origin: from Dallasdestin: to Philadelphia3) What kind of ground transportation is there between theairport and dowrltown AtlantaQUERY: What kind ofOBJECT: ground transportationQ_ATTR: is thereorigin: between the airportdestin: and downtown Atlanta4) What are the restrictions on the cheapest fare fromPittsburgh to Denver and from Denver to San FranciscoQUERY: What areOBJECT: the restrictionsfare: on the cheapest farei!origin: from Pittsburghdestin: to DenverAND: andorigin: from Denverdestin: to San Francisco5)Display flights from Oakland or San Francisco to DenverQ U E RY: DisplayOBJECT: flightsorigin: from OaklandAND: ororigin: San Franciscodestin: to DenverTable 4: Examples of correctly decoded test sentences from FEB-91test set1) What kind of aircraft is used for Eastern flight 205QUERY: What kind ofOBJECT: aircraftQ_ATTR: isI flcode: = used for Eastern flight 2052)1s there a flight from Denver through Dallas Fort Worthto PhiladelphiaQUERY: Is thereOBJECT: a flightorigin: from Denver through Dallas Fort Worthdestin: to Philadelphia3)Can you please tell me the type of plane that my clientwould be flying on from Baltimore to PittsburghDUMMY: Can you pleaseQUERY: tell me the type of plane that my client would beOBJECT: flying onorigin: from Baltimoredestin: to PittsburghTable 5: Examples of incorrectly decoded test sentences from FEB-91test setwith a consequent poor estimation of the parameters related toit.
The same problem, i.e.
inadequate training, is also the causeof the wrong segmentation of sentence 3.Future  WorkThe goal of the understanding system is to retrieve the infor-mation in the ATIS database.
In order to do this we are develop-ing a module that translates the conceptual representation f thesentence obtained with the described method into an SQL query.Since the ambiguity of the sentence is resolved by the conceptualsegmentation, this module implements a deterministic mapping.CONCLUSIONSWe proposed a very simple semantic grammar for the ATIStask.
The grammar was designed to be rich enough to handlemost queries, but limited in certain ways so as to facilitate pars-ing by very simple and well-understood HMM methods.
The ad-vantages of this approach are its straightforward integration withan HMM based speech recognizer, and its capability of learningfrom examples.
Even with an extremely small training set, thesystem was able to assign the correct analysis to more than 80%of the class A sentences in both the JUN-90 and FEB-91 test sets.The authors gratefully acknowledge the helpful advice andconsultation of Ken Church, Alexandra Gertner, A1 Gorin, Fer-nando Pereira, and Evelyne Tzoukerman.REFERENCES\[1\] Jelinek, F., "Continuous Speech Recognition by Statistical Meth-ods"Proceedings oflEEE, vol.
64, no.
4, pp.
532-556, 1976\[2\] Sowa, J., F. Conceptual Structures: Information Processing inMind and Machine, Addison-Wesley, Reading, MA, 1984.\[3\] Gertner, A. N., Gorin, A. L., Roe, D. B., " Adaptive LanguageAcquisition from a Subset of the Airline Reservation Task," paperin preparation.124
